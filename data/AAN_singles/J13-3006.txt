Selectional Preferences forSemantic Role ClassificationBen?at Zapirain?University of the Basque CountryEneko Agirre?
?University of the Basque CountryLlu?
?s Ma`rquez?Universitat Polite`cnica de CatalunyaMihai Surdeanu?University of ArizonaThis paper focuses on a well-known open issue in Semantic Role Classification (SRC) research:the limited influence and sparseness of lexical features.
We mitigate this problem using modelsthat integrate automatically learned selectional preferences (SP).
We explore a range of modelsbased on WordNet and distributional-similarity SPs.
Furthermore, we demonstrate that the SRCtask is better modeled by SP models centered on both verbs and prepositions, rather than verbsalone.
Our experiments with SP-based models in isolation indicate that they outperform a lexicalbaseline with 20 F1 points in domain and almost 40 F1 points out of domain.
Furthermore, weshow that a state-of-the-art SRC system extended with features based on selectional preferencesperforms significantly better, both in domain (17% error reduction) and out of domain (13%error reduction).
Finally, we show that in an end-to-end semantic role labeling system we obtainsmall but statistically significant improvements, even though our modified SRC model affectsonly approximately 4% of the argument candidates.
Our post hoc error analysis indicates thatthe SP-based features help mostly in situations where syntactic information is either incorrect orinsufficient to disambiguate the correct role.?
Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country.E-mail: benat.zapirain@ehu.es.??
Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country.E-mail: e.agirre@ehu.es.?
UPC Campus Nord (Omega building), Jordi Girona 1?3, 08034 Barcelona, Catalonia.E-mail: lluism@lsi.upc.edu.?
1040 E. 4th Street, Tucson, AZ 85721.
E-mail: msurdeanu@arizona.edu.Submission received: 14 November 2011; revised submission received: 31 May 2012; accepted for publication:15 August 2012.doi:10.1162/COLI a 00145?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 31.
IntroductionSemantic Role Labeling (SRL) is the problem of analyzing clause predicates in text byidentifying arguments and tagging them with semantic labels indicating the role theyplay with respect to the predicate.
Such sentence-level semantic analysis allows thedetermination of who did what to whom, when and where, and thus characterizes theparticipants and properties of the events established by the predicates.
For instance,consider the following sentence, in which the arguments of the predicate to send havebeen annotated with their respective semantic roles.1(1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal.Recognizing these event structures has been shown to be important for a broadspectrum of NLP applications.
Information extraction, summarization, questionanswering, machine translation, among others, can benefit from this shallow semanticanalysis at sentence level, which opens the door for exploiting the semantic relationsamong arguments (Boas 2002; Surdeanu et al2003; Narayanan and Harabagiu 2004;Melli et al2005; Moschitti et al2007; Higashinaka and Isozaki 2008; Surdeanu,Ciaramita, and Zaragoza 2011).
In Ma`rquez et al(2008) the reader can find a broadintroduction to SRL, covering several historical and definitional aspects of the problem,including also references to the main resources and systems.State-of-the-art systems leverage existing hand-tagged corpora (Fillmore,Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervisedmachine learning systems, and typically perform SRL in two sequential steps:argument identification and argument classification.
Whereas the former is mostly asyntactic recognition task, the latter usually requires semantic knowledge to be takeninto account.
The semantic knowledge that most current systems capture from text isbasically limited to the predicates and the lexical units contained in their arguments,including the argument head.
These ?lexical features?
tend to be sparse, especiallywhen the training corpus is small, and thus SRL systems are prone to overfit thetraining data and generalize poorly to new corpora (Pradhan, Ward, and Martin 2008).As a simplified example of the effect of sparsity, consider the following sentencesoccurring in an imaginary training data set for SRL:(2) [JFK]Patient was assassinated [in Dallas]Location(3) [John Lennon]Patient was assassinated [in New York]Location(4) [JFK]Patient was assassinated [in November]Temporal(5) [John Lennon]Patient was assassinated [in winter]TemporalAll four sentences share the same syntactic structure, so the lexical features (i.e., thewords Dallas, New York, November, and winter) represent the most relevant knowledgefor discriminating between the Location and Temporal adjunct labels in learning.1 For simplicity, in this paper we talk about arguments in the most general sense.
Unless noted otherwise,argument will refer to both core-arguments (Agent, Patient, Instrument, etc.)
and adjuncts (Manner,Temporal, Location, etc.
).632Zapirain et alSelectional Preferences for Semantic Role ClassificationThe problem is that, as in the following sentences, for the same predicate, one mayencounter similar expressions with new words like Texas or December, which theclassifiers cannot match with the lexical features seen during training, and thus becomeuseless for classification:(6) [Smith] was assassinated [in Texas](7) [Smith] was assassinated [in December]This problem is exacerbated when SRL systems are applied to texts coming fromnew domains where the number of new predicates and argument heads increasesconsiderably.
The CoNLL-2004 and 2005 evaluation exercises on semantic role labeling(Carreras and Ma`rquez 2004, 2005) reported a significant performance degradationof around 10 F1 points when applied to out-of-domain texts from the Brown corpus.Pradhan, Ward, and Martin (2008) showed that this performance degradation isessentially caused by the argument classification subtask, and suggested the lexicaldata sparseness as one of the main reasons.In this work, we will focus on Semantic Role Classification (SRC), and we will showthat selectional preferences (SP) are useful for generalizing lexical features, helpingfight sparseness and domain shifts, and improving SRC results.
Selectional preferencestry to model the kind of words that can fill a specific argument of a predicate, andhave been widely used in computational linguistics since the early days (Wilks 1975).Both semantic classes from existing lexical resources like WordNet (Resnik 1993b) anddistributional similarity based on corpora (Pantel and Lin 2000) have been successfullyused for acquiring selectional preferences, and in this work we have used several ofthose models.The contributions of this work to the field of SRL are the following:1.
We formalize and implement a method that applies several selectionalpreference models to Semantic Role Classification, introducing for the firsttime the use of selectional preferences for prepositions, in addition toselectional preferences for verbs.2.
We show that the selectional preference models are able to generalizelexical features and improve role classification performance in a controlledexperiment disconnected from a complete SRL system.
The positive effectis consistently observed in all variants of WordNet and distributionalsimilarity measures and is especially relevant for out-of-domain data.
Theseparate learning of SPs for verbs and prepositions contributessignificantly to the improvement of the results.3.
We integrate the information of several SP models in a state-of-the-art SRLsystem (SwiRL)2 and obtain significant improvements in semantic roleclassification and, as a consequence, in the end-to-end SRL task.
The keyfor the improvement lies in the combination of the predictions providedby SwiRL and the several role classification models based on selectionalpreferences.2 http://surdeanu.info/mihai/swirl/.633Computational Linguistics Volume 39, Number 34.
We present a manual analysis of the output of the combined roleclassification system.
By observing a set of real examples, we categorizedand quantified the situations in which SP models tend to help roleclassification.
By inspecting also a set of negative cases, this analysis alsosheds light on the limitations of the current approach and identifiesopportunities for further improvements.The use of selectional preferences for improving role classification was first pre-sented in Zapirain, Agirre, and Ma`rquez (2009), and later extended in Zapirain et al(2010) to a full-fledged SRC system.
In the current paper, we provide more detailedbackground information and details of the selectional preference models, as well ascomplementary experiments on the integration in a full-fledged system.
More impor-tantly, we incorporate a detailed analysis of the output of the system, comparing it withthat of a state-of-the-art SRC system not using SPs.The rest of the paper is organized as follows.
Section 2 provides background on theautomatic acquisition of selectional preference, and its recent relation to the semanticrole labeling problem.
In Section 3, the SP models investigated in this paper are ex-plained in all their variants.
The results of the SP models in laboratory conditions arepresented in Section 4.
Section 5 describes the method for integrating the SP models in astate-of-the-art SRL system and discusses the results obtained.
In Section 6 the qualita-tive analysis of the system output is presented, including a detailed discussion of severalexamples.
Finally, Section 7 concludes and outlines some directions for future research.2.
BackgroundThe simplest model for generating selectional preferences would be to collect all headsfilling each role of the target predicate.
This is akin to the lexical features used by currentSRL systems, and we refer to this model as the lexical model.
More concretely, thelexical model for verb-role selectional preferences consists of the list of words appearingas heads of the role arguments of the predicate verb.
This model can be extractedautomatically from the SRL training corpus using straightforward techniques.
Whenusing this model for role classification, it suffices to check whether the head word ofthe argument matches any of the words in the lexical model.
The lexical model is thebaseline for our other SP models, all of which build on that model.In order to generalize the lexical model, semantic classes can be used.
Although inprinciple any lexical resource listing semantic classes for nouns could be applied, mostof the literature has focused on the use of WordNet (Resnik 1993b).
In the WordNet-based model, the words occurring in the lexical model are projected over the semantichierarchy of WordNet, and the semantic classes which represent best those words areselected.
Given a new example, the SRC system has to check whether the new wordmatches any of those semantic classes.
For instance, in example sentences (2)?
(5), thesemantic class <time period> covers both training examples for Temporal (i.e., Novemberand winter), and <geographical area> covers the examples for Location.
When testwords Texas and December occur in Examples (6) and (7), the semantic classes to whichthey belong can be used to tag the first as Location and the second as Temporal.As an alternative to the use of WordNet, one can also apply automatically acquireddistributional similarity thesauri.
Distributional similarity methods analyze the co-occurrence patterns of words and are able to capture, for instance, that December is moreclosely related to November than to Dallas (Grefenstette 1992).
Distributional similarity istypically used on-line (i.e., given a pair of words, their similarity is computed on the go),634Zapirain et alSelectional Preferences for Semantic Role Classificationbut, in order to speed up its use, it has also been used to produce off-line a full thesauri,storing, for every word, the weighted list of all outstanding similar words (Lin 1998).In the Distributional similarity model, when test item Texas in Example (6) is to belabeled, the higher similarity to Dallas and New York, in contrast to the lower similarityto November and winter, would be used to label the argument with the Location role.The automatic acquisition of selectional preferences is a well-studied topic in NLP.Many methods using semantic classes and selectional preferences have been proposedand applied to a variety of syntactic?semantic ambiguity problems, including syntacticparsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez2008; Koo, Carreras, and Collins 2008; Agirre et al2011), word sense disambiguation(Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun res-olution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov andRoth 2009).
In addition, selectional preferences have been shown to be effective toimprove the quality of inference and information extraction rules (Pantel et al2007;Ritter, Mausam, and Etzioni 2010).
In some cases, the aforementioned papers do notmention selectional preferences, but all of them use some notion of preferring certainsemantic types over others in order to accomplish their respective task.In fact, one could use different notions of semantic types.
In one extreme, we wouldhave a small set of coarse semantic classes.
For instance, some authors have used the26 so-called ?semantic fields?
used to classify all nouns in WordNet (Agirre, Baldwin,and Martinez 2008; Agirre et al2011).
The classification could be more fine-grained, asdefined by the WordNet hierarchy (Resnik 1993b; Agirre and Martinez 2001; McCarthyand Carroll 2003), and other lexical resources could be used as well.
Other authors haveused automatically induced hierarchical word classes, clustered according to occurrenceinformation from corpora (Koo, Carreras, and Collins 2008; Ratinov and Roth 2009).On the other extreme, each word would be its own semantic class, as in the lexicalmodel, but one could also model selectional preference using distributional similarity(Grefenstette 1992; Lin 1998; Pantel and Lin 2000; Erk 2007; Bergsma, Lin, and Goebel2008).
In this paper we will focus on WordNet-based models that use the whole hierarchyand on distributional similarity models, and we will use the lexical model as baseline.2.1 WordNet-Based ModelsResnik (1993b) proposed the modeling of selectional preferences using semantic classesfrom WordNet and applied the model to tackle some ambiguity issues in syntax, suchas noun-compounds, coordination, and prepositional phrase attachment.
Given twoalternative structures, Resnik used selectional preferences to choose the attachmentmaximizing the fitness of the head to the selectional preferences of the attachmentpoints.
This is similar to our task, but in our case we compare the target head to the selec-tional preference models for each possible role label (i.e., given a verb and the head of anargument, we need to find the role with the selectional preference that fits the head best).In Resnik?s model, he first characterizes the restrictiveness of the selectional pref-erence of an argument position r of a governing predicate p, noted as R(p, r).
For that,given a set of classes C from the WordNet nominal hierarchies, he takes the relative en-tropy or Kullback-Leibler distance between the prior distribution P(C) and the posteriordistribution P(C|p, r):R(p, r) =?c?CP(c|p, r)logP(c|p, r)P(c)(1)635Computational Linguistics Volume 39, Number 3The priors can be computed from any corpora, computing frequencies of classesand using maximum likelihood estimates.
The frequencies for classes cannot be directlyobserved, but they can be estimated from the lexical frequencies of the nouns underthe class, as in Equation (2).
Note that in WordNet, hypernyms (?hyp?
for short)correspond to superclass relations, and therefore hyp(n) returns all superclasses ofnoun n.freq(c) =?
{n|c?hyp(n)}freq(n) (2)A complication arises because of the polysemy of nouns.
If each occurrence of anoun counted once in all classes that its senses belong to, polysemous nouns wouldaccount for more probability mass than monosemous nouns, even if they occurred thesame number of times.
As a solution, the frequency of polysemous nouns is split amongits senses uniformly.
For instance, the probability of the class <time period> can beestimated according to the frequencies of nouns like November, spring, and the rest ofnouns under it.
November has a single sense, so every occurrence counts as 1, but springhas six different senses, so each occurrence should only count as 0.16.
Note that withthis method we are implicitly dealing with the word sense ambiguity problem.
Whenencountering a polysemous noun as an argument of a verb, we record the occurrenceof all of its senses.
Given enough occurrences of nouns, the classes generalizing theintended sense of the nouns will gather more counts than competing classes.
In theexample, <time period> would have 1.16 compared with 0.16 <tool> (i.e., for the metalelastic device meaning of spring).
Researchers have used this fact to perform Word SenseDisambiguation using selectional preferences (Resnik 1993a; Agirre and Martinez 2001;McCarthy and Carroll 2003).The posterior probability can be computed similarly, but it takes into account occur-rences of the nouns in the required argument position of the predicate, and thus requiresa corpus annotated with roles.The selectional preference of a predicate p and role r for a head w0 of any potentialargument, noted as SPRes(p, r, w0), is formulated as follows:3SPRes(p, r, w0) = maxc0?hyp(w0 )P(c0|p, r)log P(c0|p,r)P(c0)R(p, r)(3)The numerator formalizes the goodness of fit for the best semantic class c0 thatcontains w0.
The hypernym (i.e., superclass) of w0 yielding the maximum value ischosen.
The denominator models how restrictive the selectional preference is for p andr, as modeled in Equation (1).Variations of Resnik?s idea to find a suitable level of generalization have beenexplored in later years.
Li and Abe (1998) applied the minimum-description lengthprinciple.
Alternatively, Clark and Weir (2002) devised a procedure to decide when aclass should be preferred rather than its children.Brockmann and Lapata (2003) compared several class-based models (includingResnik?s selectional preferences) on a syntactic plausibility judgment task for German.3 We slightly modified the notation of Resnik (1993b) in order to be coherent with the formulae presentedin this paper.636Zapirain et alSelectional Preferences for Semantic Role ClassificationThe models return weights for (verb, syntactic function, noun) triples, and correla-tion with human plausibility judgment is used for evaluation.
Resnik?s selectionalpreference scored best among WordNet-based methods (Li and Abe 1998; Clark andWeir 2002).
Despite its earlier publication, Resnik?s method is still the most popularrepresentative among WordNet-based methods (Pado?, Pado?, and Erk 2007; Erk, Pado?,and Pado?
2010; Baroni and Lenci 2010).
We also chose to use Resnik?s model in thispaper.One of the disadvantages of the WordNet-based models, compared with the distri-butional similarity models, is that they require that the heads are present in WordNet.This limitation can negatively influence the coverage of the model, and also its general-ization ability.2.2 Distributional Similarity ModelsDistributional similarity models assume that a word is characterized by the words itco-occurs with.
In the simplest model, co-occurring words are taken from a fixed-sizecontext window.
Each word w would be represented by the set of words that co-occurwith it, T(w).
In a more elaborate model, each word w would be represented as a vectorof words T(w) with weights, where Ti(w) corresponds to the weight of the ith word inthe vector.
The weights can be calculated following a simple frequency of co-occurrence,or using some other formula.Then, given two words w and w0, their similarity can be computed using any simi-larity measure between their co-occurrence sets or vectors.
For instance, early work byGrefenstette (1992) used the Jaccard similarity coefficient of the two sets T(w) and T(w0)(cf.
Equation (4) in Figure 1).
Lee (1999) reviews a wide range of similarity functions,including Jaccard and the cosine between two vectors T(w) and T(w0) (cf.
Equation (5)in Figure 1).In the context of lexical semantics, the similarity measure defined by Lin (1998)has been very successful.
This measure (cf.
Equation (6) in Figure 1) takes into accountsyntactic dependencies (d) in its co-occurrence model.
In this case, the set T(w) of co-occurrences of w contains pairs (d,v) of dependencies and words, representing the factsimJac(w, w0) =|T(w) ?
T(w0)||T(w) ?
T(w0)|(4)simcos(w, w0) =?ni=1Ti(w)Ti(w0)??ni=1Ti(w)2?
?ni=1Ti(w0)2(5)simLin(w, w0) =?
(d,v)?T(w)?T(w0 )(I(w, d, v) + I(w0, d, v))?
(d,v)?T(w) I(w, d, v) +?
(d,v)?T(w0 ) I(w0, d, v)(6)Figure 1Similarity measures used in the paper.
Jac and cos stand for Jaccard and cosine similarity metrics.T(w) is the set of words co-occurring with w, Ti(w) is the weight of the ith element of the vectorof words co-occurring with w, and I(w, d, v) is the mutual information between w and d, v.637Computational Linguistics Volume 39, Number 3that the corpus contains an occurrence of w having dependency d with v. For instance,if the corpus contains John loves Mary, then the pair (ncsubj, love) would be in the setT for John.
The measure uses information-theoretic principles, and I(w, d, v) representsthe information content of the triple (Lin 1998).Although the use of co-occurrence vectors for words to compute similarity has beenstandard practice, some authors have argued for more complex uses.
Schu?tze (1998)builds vectors for each context of occurrence of a word, combining the co-occurrencevectors for each word in the context.
The vectors for contexts were used to inducesenses and to improve information retrieval results.
Edmonds (1997) built a lexical co-occurrence network, and applied it to a lexical choice task.
Chakraborti et al(2007)used transitivity over co-occurrence relations, with good results on several classificationtasks.
Note that all these works use second order and higher order to refer to their method.In this paper, we will also use second order to refer to a new method which goes beyondthe usual co-occurrence vectors (cf.
Section 3.3).A full review of distributional models is out of the scope of this paper, as we are in-terested in showing that some of those models can be used successfully to improve SRC.Pado?
and Lapata (2007) present a review of distributional models for word similarity,and a study of several parameters that define a broad family of distributional similaritymodels, including Jaccard and Lin.
They provide publicly available software,4 whichwe have used in this paper, as explained in the next section.
Baroni and Lenci (2010)present a framework for extracting distributional information from corpora that can beused to build models for different tasks.Distributional similarity models were first used to tackle syntactic ambiguity.
Forinstance, Pantel and Lin (2000) obtained very good results on PP-attachment using thedistributional similarity measure defined by Lin (1998).
Distributional similarity wasused to overcome sparsity problems: Alongside the counts in the training data of thetarget words, the counts of words similar to the target ones were also used.
Althoughnot made explicit, Lin was actually using a distributional similarity model of selectionalpreferences.The application of distributional selectional preferences to semantic roles (as op-posed to syntactic functions) is more recent.
Gildea and Jurafsky (2002) are the only onesapplying selectional preferences in a real SRL task.
They used distributional clusteringand WordNet-based techniques on a SRL task on FrameNet roles.
They report a verysmall improvement of the overall performance when using distributional clusteringtechniques.
In this paper we present complementary experiments, with a different roleset and annotated corpus (PropBank), a wider range of selectional preference models,and the analysis of out-of-domain results.Other papers applying semantic preferences in the context of semantic roles rely onthe evaluation of artificial tasks or human plausibility judgments.
Erk (2007) introduceda distributional similarity?based model for selectional preferences, reminiscent of thatof Pantel and Lin (2000).
Her approach models the selectional preference SPsim(p, r, w0)of an argument position r of governing predicate p for a possible head-word w0 asfollows:SPsim(p, r, w0) =?w?Seen(p,r)sim(w0, w) ?
weight(p, r, w) (7)4 http://www.coli.uni-saarland.de/?pado/dv/dv.html.638Zapirain et alSelectional Preferences for Semantic Role Classificationwhere sim(w0, w) is the similarity between the seen and potential heads, Seen(p, r) is theset of heads of role r for predicate p seen in the training data set (as in the lexical model),and weight(p, r, w) is the weight of the seen head word w. Our distributional model forselectional preferences follows her formalization.Erk instantiated the basic model with several corpus-based distributional similaritymeasures, including Lin?s similarity, Jaccard, and cosine (Figure 1) among others, andseveral implementations of the weight function such as the frequency.
The quality ofeach model instantiation, alongside Resnik?s model and an expectation maximization(EM)-based clustering model, was tested in a pseudo-disambiguation task where thegoal was to distinguish an attested filler of the role and a randomly chosen word.
Theresults over 100 frame-specific roles showed that distributional similarities attain similarerror rates to Resnik?s model but better than EM-based clustering, with Lin?s formulahaving the smallest error rate.
Moreover, the coverage of distributional similarity mea-sures was much better than Resnik?s.
In a more recent paper, Erk, Pado?, and Pado?
(2010)extend the aforementioned work, including evaluation to human plausibility judgmentsand a model for inverse selectional preferences.In this paper we test similar techniques to those presented here, but we evaluateselectional preference models in a setting directly related to semantic role classification,namely, given a selectional preference model for a verb we find the role which fitsbest the given head word.
The problem is indeed qualitatively different from previouswork in that we do not have to choose among the head words competing for a role butamong selectional preferences of roles competing for a head word.More recent work on distributional selectional preference has explored the use ofdiscriminative models (Bergsma, Lin, and Goebel 2008) and topical models (O?
Se?aghdha2010; Ritter, Mausam, and Etzioni 2010).
These models would be a nice addition to thoseimplemented in this paper, and if effective, they would improve further our results withrespect to the baselines which don?t use selectional preferences.Contrary to WordNet-based models, distributional preferences do not rely on ahand-built resource.
Their coverage and generalization ability depend on the corpusfrom which the distributional similarity model was computed.
This fact makes thisapproach more versatile in domain adaptation scenarios, as more specific and test-setfocused generalization corpora could be used to modify, enrich, or even replace theoriginal corpus.2.3 PropBankIn this work we use the semantic roles defined in PropBank.
The Proposition Bank(Palmer, Gildea, and Kingsbury 2005) emerged as a primary resource for research inSRL.
It provides semantic role annotation for all verbs in the Penn Treebank corpus.PropBank takes a ?theory-neutral?
approach to the designation of core semantic roles.Each verb has a frameset listing its allowed role labelings in which the arguments aredesignated by number (starting from 0).
Each numbered argument is provided with anEnglish language description specific to that verb.
The most frequent roles are Arg0 andArg1 and, generally, Arg0 stands for the prototypical agent and Arg1 corresponds to theprototypical patient or theme of the proposition.
The rest of arguments (Arg2 to Arg5)do not generalize across verbs, that is, they have verb specific interpretations.Apart from the core numbered roles, there are 13 labels to designate adjuncts:AM-ADV (general-purpose), AM-CAU (cause), AM-DIR (direction), AM-DIS (dis-course marker), AM-EXT (extent), AM-LOC (location), AM-MNR (manner), AM-MOD639Computational Linguistics Volume 39, Number 3Table 1Example of verb-role lexical SP models for write, listed in alphabetical order.
Number of headsindicates the number of head words attested, Unique heads indicates the number of distincthead words attested, and Examples lists some of the heads in alphabetical order.Verb-role Number of Unique Examplesheads headswrite-Arg0 98 84 Angrist anyone baker ball bank Barlow Bates ...write-Arg1 97 69 abstract act analysis article asset bill book ...write-Arg2 7 7 bank commander hundred jaguar Kemp member ...write-AM-LOC 2 2 paper spacewrite-AM-TMP 1 1 month(modal verb), AM-NEG (negation marker), AM-PNC (purpose), AM-PRD (predication),AM-REC (reciprocal), and AM-TMP (temporal).3.
Selectional Preference Models for Argument ClassificationOur approach for applying selectional preferences to semantic role classification isdiscriminative.
That is, the SP-based models provide a score for every possible rolelabel given a verb (or preposition), the head word of the argument, and the selectionalpreferences for the verb (or preposition).
These scores can be used to directly assign themost probable role or to codify new features to train enriched semantic role classifiers.In this section we first present all the variants for acquiring selectional preferencesused in our study, and then present the method to apply them to semantic role classifi-cation.
We selected several variants that have been successful in some previous works.3.1 Lexical SP ModelIn order to implement the lexical model we gathered all heads w of arguments fillinga role r of a predicate p and obtained freq(p, r, w) from the corresponding training data(cf.
Section 4.1).
Table 1 shows a sample of the heads of arguments attested in thecorpus for the verb write.
The lexical SP model can be simply formalized as follows:SPlex(p, r, w0) = freq(p, r, w0) (8)3.2 WordNet-Based SP ModelsWe instantiated the model based on (Resnik 1993b) presented in the previous sec-tion (SPRes, cf.
Equation (3)) using the implementation of Agirre and Martinez (2001).Tables 2 and 3 show the synsets5 that generalize best the head words in Table 1for write-Arg0 and write-Arg1, according to the weight assigned to those synsets byEquation (1).
According to this model, and following basic intuition, the words attestedas being Arg0s of write are best generalized by semantic classes such as living things,5 The WordNet terminology for concepts is synset.
In this paper we use concept, synset, and semantic classinterchangeably.640Zapirain et alSelectional Preferences for Semantic Role ClassificationTable 2Excerpt from the selectional preferences for write-Arg0 according to SPRes, showing the synsetsthat generalize best the head words in Table 1.
Weight lists the weight assigned to those synsetsby Equation (1).
Description includes the words and glosses in the synset.Synset Weight Descriptionn#00002086 5.875 life form organism being living thing any living entityn#00001740 5.737 entity something anything having existence (living or nonliving)n#00009457 4.782 object physical object a physical (tangible and visible) entity;n#00004123 4.351 person individual someone somebody mortal human soula human being;Table 3Excerpt from the selectional preferences for write-Arg1 according to SPRes, showing the synsetsthat generalize best the head words in Table 1.
Weight lists the weight assigned to those synsetsby Equation (1).
Description includes the words and glosses in the synset.Synset Weight Descriptionn#00019671 7.956 communication something that is communicated between peopleor groupsn#04949838 4.257 message content subject matter substance what a communicationthat .
.
.n#00018916 3.848 relation an abstraction belonging to or characteristic of two entitiesn#00013018 3.574 abstraction a concept formed by extracting common featuresfrom examplesentities, physical objects, and human beings, whereas Arg1s by communication, mes-sage, relation, and abstraction.Resnik?s method performs well among Wordnet-based methods, but we realizedthat it tends to overgeneralize.
For instance, in Table 2, the concept for ?entity?
(one ofthe unique beginners of the WordNet hierarchy) has a high weight.
This means that ahead like ?grant?
would be assigned Arg0.
In fact, any noun which is under conceptn#00001740 (entity) but not under n#04949838 (message) would be assigned Arg0.
Thisobservation led us to speculate on an alternative method which would try to generalizeas little as possible.Our intuition is that general synsets can fit several selectional preferences at thesame time.
For instance, the <entity> class, as a superclass of most words, would be acorrect generalization for the selectional preferences of all agent, patient, and instrumentroles of a predicate like break.
On the contrary, specific concepts are usually more usefulfor characterizing selectional preferences, as in the <tool> class for the instrument roleof break.
The priority of using specific synsets over more general ones is, thus, justifiedin the sense that they may better represent the most relevant semantic characteristics ofthe selectional preferences.The alternative method (SPwn) is based on the depth of the concepts in the WordNethierarchy and the frequency of the nouns.
The use of the depth in hierarchies to modelthe specificity of concepts (the deeper the more specific) is not new (Rada et al1989;Sussna 1993; Agirre and Rigau 1996).
Our method tries to be conservative with respectto generalization: When we check which SP is a better fit for a given target head, wealways prefer the SP that contains the most specific generalization for the target head(the lowest synset which is a hypernym of the target word).641Computational Linguistics Volume 39, Number 3Table 4Excerpt from the selectional preferences for write-Arg0 according to SPwn, showing from deeperto shallower the synsets in WordNet which are connected to head words in Table 1.
Depth liststhe depth of synsets in WordNet.
Description includes the words and glosses in the synset.Synset Depth Freq.
Descriptionn#01967203 9 1 humanoid human being any living or extinct member of the .
.
.n#07603319 8 1 spy undercover agent a secret agent hired by a state to .
.
.n#07151308 8 1 woman a human female who does houseworkn#06183656 8 1 Federal Reserve the central bank of the USTable 5Excerpt from the selectional preferences for write-Arg1 according to SPwn, showing from deeperto shallower the synsets in WordNet which are connected to head words in Table 1.
Depth liststhe depth of synsets in WordNet.
Description includes the words and glosses in the synset.Synset Depth Freq.
Descriptionn#05403815 13 1 information formal accusation of a crimen#05401516 12 1 accusation accusal a formal charge of wrongdoing brought .
.
.n#04925620 11 1 charge complaint a pleading describing some wrong or offensen#04891230 11 1 memoir an account of the author?s personal experiencesMore concretely, we model selectional preferences as a multiset6 of synsets, storingall hypernyms of the heads seen in the training data for a certain role of a givenpredicate, that is:Smul(p, r) =?w?Seen(p,r)hyp(w) (9)where Seen(p, r) are all the argument heads for predicate p and role r, and hyp(w) returnsall the synsets and hypernyms of w, including hypernyms of hypernyms recursively upto the top synsets.For any given synset s, let d(s) be the depth of the synset in the WordNet hierarchy,and let 1Smul(p,r)(s) be the multiplicity function which returns how many times s is con-tained in the multiset Smul(p, r).
We define a partial order among synsets a, b ?
Smul(p, r)as follows: ord(a) > ord(b) iff d(a) > d(b) or d(a) = d(b) ?
1Smul(p,r)(a) > 1Smul(p,r)(b).Tables 4 and 5 show the most specific synsets (according to their depth) for write-Arg0and write-Arg1.We can then measure the goodness of fit of the selectional preference for a word asthe rank in the partial order of the first hypernym of the head that is also present in theselectional preference.
For that, we introduce SPwn(p, r, w), which following the previousnotation is defined as:SPwn(p, r, w) = arg maxs?hyp(w)?Smul(p,r)ord(s) (10)6 Multisets are similar to sets, but allow for repeated members.642Zapirain et alSelectional Preferences for Semantic Role ClassificationTable 6Most similar words for Texas and December according to Lin (1998).Texas Florida 0.249, Arizona 0.236, California 0.231, Georgia 0.221, Kansas 0.217,Minnesota 0.214, Missouri 0.214, Michigan 0.213, Colorado 0.208, NorthCarolina 0.207, Oklahoma 0.207, Arkansas 0.205, Alabama 0.205, Nebraska0.201, Tennessee 0.197, New Jersey 0.194, Illinois 0.189, Virginia 0.188,Kentucky 0.188, Wisconsin 0.188, Massachusetts 0.184, New York 0.183December June 0.341, October 0.340, November 0.333, April 0.330, February 0.329,September 0.328, July 0.323, January 0.322, August 0.317, may 0.305, March0.250, Spring 0.147, first quarter 0.135, mid-December 0.131, month 0.130,second quarter 0.129, mid-November 0.128, fall 0.125, summer 0.125,mid-October 0.121, autumn 0.121, year 0.121, third quarter 0.119In case of ties, the role coming first in alphabetical order would be returned.
Note that,similar to the Resnik model (cf.
Section 2.1), this model implicitly deals with the wordambiguity problem.As with any other approximation to measure specificity of concepts, the use ofdepth has some issues, as some deeply rooted stray synsets would take priority.
Forinstance, Table 4 shows that synset n#01967203 for human being is the deepest synset.
Inpractice, when we search the synsets of a target word in the SPwn models following Eq.
(10), the most specific synsets (specially stray synsets) are not found, and synsets higherin the hierarchy are used.3.3 Distributional SP ModelsAll our distributional SP models are based on Equation (7).
We have used several vari-ants for sim(w0, w), as presented subsequently, but in all cases, we used the frequencyfreq(p, r, w) as the weight in the equation.
Given the availability of public resources fordistributional similarity, rather than implementing sim(w0, w) afresh we used (1) the pre-compiled similarity measures by Lin (1998),7 and (2) the software for semantic spacesby Pado?
and Lapata (2007).In the first case, Lin computed the similarity numbers for an extensive vocabularybased on his own similarity formula (cf.
Equation (6) in Figure 1) run over a largeparsed corpus comprising journalism texts from different sources: WSJ (24 millionwords), San Jose Mercury (21 million words) and AP Newswire (19 million words).The resource includes, for each word in the vocabulary, its most similar words withthe similarity weight.
In order to get the similarity for two words, we can check theentry in the thesaurus for either word.
We will refer to this similarity measure assimpreLin.
Table 6 shows the most similar words for Texas and December according to thisresource.For the second case, we applied the software to the British National Corpus toextract co-occurrences, using the optimal parameters as described in Pado?
and Lapata(2007, page 179): word-based space, medium context, log-likelihood association, and7 http://www.cs.ualberta.ca/?lindek/downloads.htm.643Computational Linguistics Volume 39, Number 3Table 7Summary of distributional similarity measures used in this work.Similarity measure Sourcesimcos cosine BNCsimJac Jaccard BNCsimLin Lin BNCsimpreLin Lin Pre-computedsimpreLin?cos cosine (2nd order) Pre-computedsimpreLin?Jac Jaccard (2nd order) Pre-computed2,000 basis elements.
We tested Jaccard, cosine, and Lin?s measure for similarity, yieldingsimJac, simcos, and simLin, respectively.In addition to measuring the similarity of two words directly, that is, using the co-occurrence vectors of each word as in Section 2, we also tried a variant which we willcall second-order similarity.
In this case each word is represented by a vector whichcontains all similar words with weights, where those weights come from first ordersimilarity.
That is, in order to obtain the second-order vector for word w, we need tocompute its first order similarity with all other words in the vocabulary.
The second-order similarity of two words is then computed according to those vectors.
For this, wejust need to change the definition of T and T in the similarity formulas in Figure 1: NowT(w) would return the list of words which are taken to be similar to w, and T(w) wouldreturn the same list but as a vector with weights.This approximation is computationally expensive, as we need to compute thesquare matrix of similarities for all word pairs in the vocabulary, which is highly time-consuming.
Fortunately, the pre-computed similarity scores of Lin (1998) (which usesimLin) are readily available, and thus the second-order similarity vectors can be easilycomputed.
We used Jaccard and cosine to compute the similarity of the vectors, and wewill refer to these similarity measures as simpreLin?Jac and simpreLin?cos hereinafter.
Due to thecomputational complexity, we did not compute second order similarity for the semanticspace software of Pado?
and Lapata (2007).Table 7 summarizes all similarity measures used in this study, and the corpus orpre-computed similarity list used to build them.3.4 Selectional Preferences for PrepositionsAll the previously described models have been typically applied to verb-role selectionalpreferences for NP arguments.
Applying them to general semantic role labeling maynot be straightforward, however, and may require some extensions and adaptations.For instance, not all argument candidates are noun phrases.
Common arguments withother syntactic types include prepositional, adjectival, adverbial, and verb phrases.
Anycandidate argument without a nominal head cannot be directly treated by the modelsdescribed so far.644Zapirain et alSelectional Preferences for Semantic Role ClassificationTable 8Example of prep-role lexical models for the preposition from, listed in alphabetical order.Prep-role Number of Unique Examplesheads headsfrom-Arg0 32 30 Abramson agency association barrier cut ...from-Arg1 173 118 accident ad agency appraisal arbitrage ...from-Arg2 708 457 academy account acquisition activity ad ...from-Arg3 396 165 activity advertising agenda airport ...from-Arg4 5 5 europe Golenbock system Vizcaya westfrom-AM-ADV 19 17 action air air conception datum everyone ...from-AM-CAU 5 4 air air design experience exposurefrom-AM-DIR 79 71 agency alberta amendment america arson ...from-AM-LOC 20 17 agency area asia body bureau orlando ...from-AM-MNR 29 28 agency Carey company earnings floor ...from-AM-TMP 33 21 april august beginning bell day dec. half ...A particularly interesting case is that of prepositional phrases.8 Prepositions definerelations between the preposition attachment point and the preposition complement.Prepositions are ambiguous with respect to these relations, which allows us to talkabout preposition senses.
The Preposition Project (Litkowski and Hargraves 2005, 2006)is an effort that produced a detailed sense inventory for English prepositions, whichwas later used in a preposition sense disambiguation task at SemEval-2007 (Litkowskiand Hargraves 2007).
Sense labels are defined as semantic relations, similar to those ofsemantic role labels.
In a more recent work, Srikumar and Roth (2011) presented a jointmodel for extended semantic role labeling in which they show that determining thesense of the preposition is mutually related to the task of labeling the argument role ofthe prepositional phrase.
Following the previous work, we also think that prepositionsdefine implicit selectional preferences, and thus decided to explore the use of preposi-tional preferences with the aim of improving the selection of the appropriate semanticroles.
Addressing other arguments with non-nominal heads has been intentionally leftfor further work.The most straightforward way of including prepositional information in SP modelswould be to add the preposition as an extra parameter of the SP.
Initial experimentsrevealed sparseness problems with collecting the ?verb, preposition, NP-head, role?4-tuples from the training set.
A simpler approach consists of completely disregardingthe verb information while collecting the prepositional preferences.
That is, the selec-tional preference for a preposition p and role r is defined as the union of all nouns wfound as heads of noun phrases embedded in prepositional phrases headed by p andlabeled with semantic role r. Then, one can apply any of the variants described in theprevious sections to calculate SP(p, r, w).
Table 8 shows a sample of the lexical model forthe preposition from, organized according to the roles it plays.These simple prep-role preferences largely avoided the sparseness problem whilestill being able to capture relevant information to distinguish the appropriate roles inmany PP arguments.
In particular, they proved to be relevant to distinguish betweenadjuncts of the type ?
[in New York]Location?
vs.
?
[in Winter]Temporal.?
Nonetheless, we8 Prepositional phrase is the second most frequent type of syntactic constituent for semantic arguments(13%), after noun phrases (45%).645Computational Linguistics Volume 39, Number 3are aware that not taking into account verb information also introduces some lim-itations.
In particular, the simplification could damage the performance on PP corearguments, which are verb-dependent.9 For instance, our prepositional preferenceswould not be able to suggest appropriate roles for the following two PP arguments:?increase [ from seven cents a share]Arg3?
and ?receive [ from the funds]Arg2,?
becausethe two head nouns (cents and funds) are semantically very similar.
Assigning thecorrect roles in these cases clearly depends on the information carried by the verbs.Arg3 is the starting point for the predicate increase, whereas Arg2 refers to the source forreceive.Our perspective on making this simple definition of prep-role SPs was practical andjust a starting point to play with the argument preferences introduced by prepositions.A more complex model, distinguishing between prepositional phrases in adjunct andcore argument positions, should be able to model the linguistics better yet alviate thesparseness problem, and would hopefully produce better results.The combination scheme for applying verb-role and prep-role is also very simple.Depending on the syntactic type of the argument we apply one or the other model, bothin learning and testing: When the argument is a noun phrase, we use verb-role selectionalpreferences. When the argument is a prepositional phrase, we use prep-roleselectional preferences.We thus use a straightforward method to combine both kinds of SPs.
More complexpossibilities like doing mixtures of both SPs are left for future work.3.5 Role Classification with SP ModelsSelectional preference models can be directly used to perform role classification.
Givena target predicate p and noun phrase candidate argument with head w, we simply selectthe role r of the predicate which best fits the head according to the SP model.
Thisselection rule is formalized as:ROLE(p, w) = arg maxr?Roles(p)SP(p, r, w) (11)with Roles(p) being the set of all roles applicable to the predicate p, and SP(p, r, w)the goodness of fit of the selectional preference model for the head w, which can beinstantiated with all the variants mentioned in the previous subsections, includingthe lexical model (Equation (8)) WordNet-based SP models (Equations (3) and (10)),and distributional SP models (Equation (7)), using different similarity models as inTable 7.
Ties were broken returning the role coming first according to alphabeticalorder.
Note that in the case of SPwn (Equation 10) we need to use arg min rather thanarg max.9 The percentage of prepositional phrases in core argument position is 48%, slightly lower than in adjunctposition (52%).646Zapirain et alSelectional Preferences for Semantic Role ClassificationNote that if the candidate argument is a prepositional phrase with preposition p?and embedded NP head word w, the classification rule uses the prep-role SP model,that is:ROLE(p, p?, w) = arg maxr?Roles(p?
)SP(p?, r, w)4.
Experiments with Selectional Preferences in IsolationIn this section we evaluate the ability of selectional preference models to discriminateamong different roles.
For that, SP models will be used in isolation, according to the clas-sification rule in Equation (11), to predict role labels for a set of (predicate, argument-head)pairs.
That is, we are interested in the discriminative power of the semantic informationcarried by the SPs, factoring out any other feature commonly used by the state-of-the-art SRL systems.
The data sets used and the experimental results are presented in thefollowing.4.1 Data SetsThe data used in this work are the benchmark corpus provided by the CoNLL-2005shared task on SRL (Carreras and Ma`rquez 2005).
The data set, of over 1 million tokens,comprises PropBank Sections 02?21 for training, and Sections 24 and 23 for develop-ment and testing, respectively.
The Selectional Preferences implemented in this studyare not able to deal with non-nominal argument heads, such us those of NEG, DIS,MOD (i.e., SPs never predict NEG, DIS, or MOD roles); but, in order to replicate thesame evaluation conditions of typical PropBank-based SRL experiments all argumentsare evaluated.
That is, our SP models don?t return any prediction for those, and theevaluation penalizes them accordingly.The predicate?role?head triples (p, r, w) for generalizing the selectional preferencesare extracted from the arguments of the training set, yielding 71,240 triples, from which5,587 different predicate-role selectional preferences (p, r) are derived by instantiatingthe different models in Section 3.
Tables 9 and 10 show additional statistics about someof the most (and least) frequent verbs and prepositions in these tuples.The test set contains 4,134 pairs (covering 505 different predicates) to be classifiedinto the appropriate role label.
In order to study the behavior on out-of-domain data,we also tested on the PropBanked part of the Brown corpus (Marcus et al1994).
Thiscorpus contains 2,932 (p, w) pairs covering 491 different predicates.4.2 ResultsThe performance of each selectional preference model is evaluated by calculatingthe customary precision (P), recall (R), and F1 measures.10 For all experiments re-ported in this paper, we checked for statistical significance using bootstrap resampling(100 samples) coupled with one-tailed paired t-test (Noreen 1989).
We consider a resultsignificantly better than another if it passes this test at the 99% confidence interval.10 P = Correct/Predicted ?
100, R = Correct/Gold ?
100, where Correct is the number of correct predictions,Predicted is the number of predictions, and Gold is the total number of gold annotations.F1 = 2PR/(P + R) is the harmonic mean of P and R.647Computational Linguistics Volume 39, Number 3Table 9Statistics of the three most and least frequent verbs in the training set.
Role frame lists the typesof arguments seen in training for each verb; Heads indicates the total number of arguments forthe verb; Heads per role shows the average number of head words for each role; and Uniqueheads per role lists the average number of unique head words for each verb?s role.Verb Role frame Heads Heads Unique headsper role per rolesay Arg0,Arg1,Arg3,AM-ADV, AM-LOC, 7,488 1,069 371AM-MNR, AM-TMP, AM-LOC,AM-MNRhave Arg0,Arg1,AM-ADV,AM-LOC 3,487 498 189AM-MNR,AM-NEG,AM-TMPmake Arg0,Arg1,Arg2,AM-ADV 2,207 315 143AM-LOC,AM-MNR,AM-TMP... ... ... ... ...accrete Arg1 1 1 1accede Arg0 1 1 1absolve Arg0 1 1 1Table 10Statistics of the three most and least frequent prepositions in the training set.
Role frame liststhe types of arguments seen in training for each preposition; Heads indicates the total numberof arguments for the preposition; Heads per role shows the average number of head words foreach role; and Unique heads per role lists the average number of unique head words for eachpreposition?s role.Preposition Role frame Heads Heads Unique headsper role per rolein Arg0,Arg1,Arg2,Arg3,Arg4,Arg5 6,859 403 81AM-ADV,AM-CAU,AM-DIR,AM-DIS,AM-EXT,AM-LOC,AM-MNR,AM-NEG,AM-PNC,AM-PRD,AM-TMPto Arg0,Arg1,Arg2,Arg3,Arg4, 3,495 233 94AM-ADV,AM-CAU,AM-DIR,AM-DIS,AM-EXT,AM-LOC,AM-MNR,AM-PNC,AM-PRD,AM-TMPfor Arg0,Arg1,Arg2,Arg3,Arg4, 2,935 225 74AM-ADV,AM-CAU,AM-DIR,AM-DIS,AM-LOC,AM-MNR,AM-PNC,AM-TMP... ... ... ... ...beside Arg2, AM-LOC 2 1 1atop Arg2, AM-DIR 2 1 1aboard AM-LOC 1 1 1Tables 11 and 12 list the results of the various selectional preference models inisolation.
Table 11 shows the results for verb-role SPs, and Table 12 lists the resultsfor the combination of verb-role and preposition-role SPs as described in Section 3.4.11It is worth noting that the results of Tables 11 and 12 are calculated over exactly the11 Note that the results reported here are not identical to those we reported in Zapirain, Agirre, andMa`rquez (2009).
The differences are two-fold: (a) in our previous experiments we discarded roles suchas MOD, DIS, and NEG, whereas here we evaluate on all roles, and (b) our previous work used only thesubset of the data that could be mapped to VerbNet (around 50%), whereas here we inspect all tuples.648Zapirain et alSelectional Preferences for Semantic Role ClassificationTable 11Results for verb-role SPs in the development partition of WSJ, the test partition of WSJ, and theBrown corpus.
For each experiment, we show precision (P), recall (R), and F1.
Values in boldfacefont are the highest in the corresponding column.
F1 values marked with ?
are significantlylower than the highest F1 score in the same column.Verb-role SPsDevelopment WSJ Test Brown TestP R F1 P R F1 P R F1lexical 73.94 21.81 33.69?
70.75 26.66 39.43?
59.39 05.51 10.08?SPRes 43.65 35.70 39.28?
45.07 37.11 40.71?
36.34 27.58 31.33?SPwn 53.09 43.35 47.73?
55.44 45.58 50.03?
41.76 31.58 35.96?SPsimLin 53.88 44.35 48.65?
52.27 45.13 48.66?
48.30 32.08 38.56?SPsimJac 48.40 45.53 46.92?
48.85 46.38 47.58?
42.10 34.34 37.82?SPsimcos 52.37 49.26 50.77?
53.13 50.44 51.75?
43.24 35.27 38.85?SPsimpreLin60.29 59.54 59.91 59.93 59.38 59.65 50.79 48.39 49.56SPsimpreLin?Jac60.56 56.97 58.71 61.76 58.63 60.16 51.97 42.39 46.69?SPsimpreLin?cos60.22 56.64 58.37 61.12 58.12 59.63 51.92 42.35 46.65?Table 12Results for combined verb-role and prep-role SPs in the development partition of WSJ, the testpartition of WSJ, and the Brown corpus.
For each experiment, we show precision (P), recall (R),and F1.
Values in boldface font are the highest in the corresponding column.
F1 values markedwith ?
are significantly lower from the highest F1 score in the same column.Preposition-role and Verb-role SPsDevelopment WSJ Test Brown TestP R F1 P R F1 P R F1lexical 82.05 39.17 53.02?
82.98 43.77 57.31?
68.47 13.60 22.69?SPRes 63.72 53.09 57.93?
63.47 53.24 57.91?
55.12 44.15 49.03?SPwn 71.72 59.68 65.15?
65.70 63.88 64.78?
60.08 48.10 53.43?SPsimLin 63.84 54.58 58.85?
63.75 56.40 59.85?
54.27 39.96 46.04?SPsimJac 61.75 61.13 61.44?
61.83 61.40 61.61?
55.42 53.45 54.42?SPsimcos 64.81 64.17 64.49?
64.67 64.22 64.44?
56.56 54.54 55.53?SPsimpreLin67.78 67.10 67.44?
68.34 67.87 68.10?
58.43 56.35 57.37?SPsimpreLin?Jac69.90 69.20 69.55 70.82 70.33 70.57 62.37 60.15 61.24SPsimpreLin?cos69.47 68.78 69.12 70.28 69.80 70.04 62.36 60.14 61.23same example set.
PP arguments are treated by the verb-role SPs by just ignoring thepreposition and considering the head noun of the NP immediately embedded in the PP.It is worth mentioning that none of the SP models is able to predict the role whenfacing a head word missing from the model.
This is especially noticeable in the lexicalmodel, which can only return predictions for words seen in the training data and is649Computational Linguistics Volume 39, Number 3penalized in recall.
WordNet based models, which have a lower word coverage com-pared to distributional similarity?based models, are also penalized in recall.In both tables, the lexical row corresponds to the baseline lexical match method.The following rows correspond to the WordNet-based selectional preference models.The distributional models follow, including the results obtained by the three similarityformulas on the co-occurrences extracted from the BNC (simJac, simcos simLin), and theresults obtained when using Lin?s pre-computed similarities directly (simpreLin) and as asecond-order vector (simpreLin?Jac and simpreLin?cos).First and foremost, this experiment proves that splitting SPs into verb- andpreposition-role SPs yields better results.
The comparison of Tables 11 and 12 showsthat the improvements are seen for both precision and recall, but especially remarkablefor recall.
The overall F1 improvement is of up to 10 points.
Unless stated otherwise, therest of the analysis will focus on Table 12.As expected, the lexical baseline attains a very high precision in all data sets, whichunderscores the importance of the lexical head word features in argument classification.Its recall is quite low, however, especially in Brown, confirming and extending Pradhan,Ward, and Martin (2008), who also report a similar performance drop for argumentclassification on out-of-domain data.
All our selectional preference models improveover the lexical matching baseline in recall, with up to 24 absolute percentage pointsin the WSJ test data set and 47 absolute percentage points in the Brown corpus.
Thiscomes at the cost of reduced precision, but the overall F-score shows that all selectionalpreference models are well above the baseline, with up to 13 absolute percentagepoints on the WSJ data sets and 39 absolute percentage points on the Brown data set.The results, thus, show that selectional preferences are indeed alleviating the lexicalsparseness problem.12As an example, consider the following head words of potential arguments of theverb wear found in the test set: doctor, men, tie, shoe.
None of these nouns occurred asheads of arguments of wear in the training data, and thus the lexical feature wouldbe unable to predict any role for them.
Using selectional preferences, we successfullyassigned the A0 role to doctor and men, and the A1 role to tie and shoe.Regarding the selectional preference variants, WordNet-based and first-order distri-butional similarity models attain similar levels of precision, but the former have lowerrecall and F1.
The performance loss on recall can be explained by the limited lexicalcoverage of WordNet when compared with automatically generated thesauri.
Examplesof words missing in WordNet include abbreviations (e.g., Inc., Corp.) and brand names(e.g., Texaco, Sony).The comparison of the WordNet-based models indicates that our proposal for alighter method of WordNet-based selectional preference was successful, as our simplervariant performs better than Resnik?s method.
In manual analysis, we realized thatResnik?s model tends to always predict the most frequent roles whereas our modelcovers a wider role selection.
Resnik?s tendency to overgeneralize makes more frequentroles cover all the vocabulary, and the weighting system penalizes roles with feweroccurrences.12 We verified that the lexical model shows higher classification accuracy than all the more elaborate SPmodels on the subset of cases covered by both the lexical and the SP models.
In this situation, if we aimedat constructing the best role classifier with SPs alone we could devise a back-off strategy, in the style ofChambers and Jurafsky (2010), which uses the predictions of the lexical model when present and one ofthe SP models if not.
As presented in Section 5, however, our main goal is to integrate these SP modelsin a real end-to-end SRL system, so we keep their analysis as independent predictors for the moment.650Zapirain et alSelectional Preferences for Semantic Role ClassificationThe results for distributional models indicate that the SPs using Lin?s ready-madethesaurus (simpreLin) outperforms Pado?
and Lapata?s distributional similarity model (Pado?and Lapata 2007) calculated over the BNC (simLin) in both Tables 11 and 12.
This mightbe due to the larger size of the corpus used by Lin, but also by the fact that Lin used anewspaper corpus, compared with the balanced BNC corpus.
Further work would beneeded to be more conclusive, and, if successful, could improve further the results ofsome SP models.Among the three similarity metrics using Pado?
and Lapata?s software, the cosineseems to perform consistently better.
Regarding the comparison between first-order andsecond-order using pre-computed similarity models, the results indicate that second-order is best when using both the verb-role and prep-role models (cf.
Table 12), althoughthe results for verb-roles are mixed (cf.
Table 11).
Jaccard seems to provide slightly betterresults than cosine for second-order vectors.In summary, the use of separate verb-role and prep-role models produces the bestresults, and second-order similarity is highly competitive.
As far as we know, this isthe first time that prep-role models and second-order models are applied to selectionalpreference modeling.5.
Semantic Role Classification ExperimentsIn this section we advance the use of SP in SRL one step further and show that selec-tional preferences are able to effectively improve performance of a state-of-the-art SRLsystem.
More concretely, we integrate the information of selectional preference modelsin a SRL system and show significant improvements in role classification, especiallywhen applied to out-of-domain corpora.13We will use some of the selectional preference models presented in the previoussection.
We will focus on the combination of verb-role and prep-role models.
Regardingthe similarity models, we will choose the best two performing models from each ofthe three families that we tried, namely, the two WordNet models, the two best modelsbased on the BNC corpus (simJac,simcos), and the two best models based on Lin?s precom-puted similarity metrics (sim2Jac,sim2cos).
We left the exploration of other combinations forfuture work.5.1 Integrating Selectional Preferences in Role ClassificationFor these experiments, we modified the SwiRL SRL system, a state-of-the-art semanticrole labeling system (Surdeanu et al2007).
SwiRL ranked second among the systemsthat did not implement model combination at the CoNLL-2005 shared task and fifthoverall (Carreras and Ma`rquez 2005).
Because the focus of this section is on role classi-fication, we modified the SRC component of SwiRL to use gold argument boundaries,that is, we assume that semantic role identification works perfectly.
Nevertheless, for arealistic evaluation, all the features in the role classification model are generated usingactual syntactic trees generated by the Charniak parser (Charniak 2000).The key idea behind our approach is model combination: We generate a battery ofbase models using all resources available and we combine their outputs using multi-ple strategies.
Our pool of base models contains 13 different models: The first is the13 The data sets used for the experiments reported in this section are exactly the ones described inSection 4.1.651Computational Linguistics Volume 39, Number 3unmodified SwiRL SRC, the next six are the selected SP models from the previoussection, and the last six are variants of SwiRL SRC.
In each variant, the feature set ofthe unmodified SwiRL SRC model is extended with a single feature that models thechoice of a given SP, for example, SRC+SPres contains an extra feature that indicates thechoice of Resnik?s SP model.14We combine the outputs of these base models using two different strategies: (a)majority voting, which selects the label predicted by most models, and (b) meta-classification, which uses a supervised model to learn the strengths of each base model.For the meta-classification model, we opted for a binary classification approach: First,for each constituent we generate n data points, one for each distinct role label proposedby the pool of base models; then we use a binary meta-classifier to label each candidaterole as either correct or incorrect.
We trained the meta-classifier on the usual PropBanktraining partition, using 10-fold cross-validation to generate outputs for the basemodels that require the same training material.
At prediction time, for each candidateconstituent we selected the role label that was classified as correct with the highestconfidence.The binary meta-classifier uses the following set of features: Labels proposed by the base models, for example, the feature SRC+SPres=Arg0indicates that the SRC+SPres base model proposed the Arg0 label.
We add13 such features, one for each base model.
Intuitively, this feature allowsthe meta-classifier to learn the strengths of each base model with respectto role labels: SRC+SPres should be trusted for the Arg0 role, and so on. Boolean value indicating agreement with the majority vote, for example, thefeature Majority=true indicates that the majority of the base modelsproposed the same label as the one currently considered by themeta-classifier. Number of base models that proposed this data point?s label.
To reduce sparsity,for each number of base models, N, we generate N distinct featuresindicating that the number of base models that proposed this label islarger than k, where k ?
[0, N).
For example, if two base models proposedthe label under consideration, we generate the following two features:BaseModelNumber>0 and BaseModelNumber>1.
This feature provides finercontrol over the number of votes received by a label than the majorityvoter, for example, the meta-classifier can learn to trust a label if morethan two base models proposed it, even if the majority vote disagrees. List of actual base models that proposed this data point?s label.
We store adistinct feature for each base model that proposed the current label, andalso a concatenation of all these base model names.
The latter feature isdesigned to allow the meta-classifier to learn preferences for certaincombinations of base models.
For example, if two base models, SPres andSPwn, proposed the label under consideration, we generate three features:Base=SPres, Base=SPwn, and Base=SPres+SPwn.14 Adding more than one SP output as a feature in SwiRL?s SRC model did not improve performance indevelopment over the single-SP SRC model.
Our conjecture is that the large number of features in SRChas the potential to drown the SP-based features.
This may be accentuated when there are more SP-basedfeatures because their signal is divided among them due to their overlap.
We have also tried to add theinput features of the SP models directly to the SRC model but this also proved to be unsuccessful duringdevelopment.652Zapirain et alSelectional Preferences for Semantic Role ClassificationTable 13Results for the combination approaches.
Accuracy shows the overall results.
Core and Adjcontain F1 results restricted to the core numbered roles and adjuncts, respectively.
SRC isSwiRL?s standalone SRC model; +SPx stands for the SRC model extended with a feature given bythe corresponding SP model.
Values in boldface font are the highest in the correspondingcolumn.
Accuracy values marked with ?
are significantly lower than the highest accuracy scorein the same column.WSJ test Brown testAcc.
Core F1 Adj.
F1 Acc.
Core F1 Adj.
F1SRC 90.83?
93.25 81.31 79.52 84.42 57.76+SPRes 90.76?
93.17 81.08 79.86?
84.52 59.24+SPwn 90.56?
92.88 81.11 79.73?
84.26 59.69+SPsimJac 90.86?
93.37 80.30 79.83?
84.43 59.54+SPsimcos 90.87?
93.33 80.92 80.50?
85.14 60.16+SPsimpreLin?Jac90.95?
93.03 82.75 80.75?
85.62 59.63+SPsimpreLin?cos91.23?
93.78 80.56 80.48?
84.95 61.01Meta-classifier 92.43 94.62 84.00 81.94 86.25 63.36Voting 92.36 94.57 83.68 82.15 86.37 63.785.2 Results for Semantic Role ClassificationTable 13 compares the performance of both combination approaches against the stand-alone SRC model.
In the table, the SRC+SP?
models stand for SRC classifiers enhancedwith one feature from the corresponding SP.
The meta-classifier shown in the table com-bines the output of all the 13 base models introduced previously.
We implemented themeta-classifier using Support Vector Machines (SVMs)15 with a quadratic polynomialkernel, and C = 0.01 (tuned in the development set).16 Lastly, Table 13 shows the resultsof the voting strategy, over the same set of base models.In the columns we show overall classification accuracy and F1 results for both corearguments (Core) and adjunct arguments (Adj.).
Note that for the overall SRC scores, wereport classification accuracy, defined as ratio of correct predictions over total numberof arguments to be classified.
The reason for this is that the models in this section alwaysreturn a label for all arguments to be classified, and thus accuracy, precision, recall, andF1 are all equal.Table 13 indicates that four out of the six SRC+SP?
models perform better than thestandalone SRC model in domain (WSJ), and all of them outperform SRC out of domain(Brown).
The improvements are small, however, and, generally, not statistically signifi-cant.
On the other hand, the meta-classifier outperforms the original SRC model bothin domain (17.4% relative error reduction; 1.60 points of accuracy improvement) andout of domain (13.4% relative error reduction; 2.42 points of accuracy improvement),and the differences are statistically significant.
This experiment proves our claim thatSPs can be successfully used to improve semantic role classification.
It also underscoresthe fact that combining SRC and SPs is not trivial, however.
Our hypothesis is that this15 http://svmlight.joachims.org.16 We have also trained the meta-classifier with other learning algorithms (e.g., logistic regression withL2 regularization) and we obtained similar but slightly lower results.653Computational Linguistics Volume 39, Number 3is caused by the large performance disparity (20 F1 points in domain and 18 out ofdomain) between the original SRC model and the standalone SP methods.Interestingly, the meta-classifier performs only marginally better than the voting ap-proach in domain and slightly worse out of domain.
We believe that this is another effectof the above observation: Given the weaker SP-based features, the meta-classifier doesnot learn much beyond a majority vote, which is exactly what the simpler, unsuper-vised voting method models.
Nevertheless, regardless of the combination method, thisexperiment emphasizes that infusing SP information in the SRC task is beneficial.Table 13 also shows that our approach yields consistent improvements for bothcore and adjunct arguments.
Out of domain, we see a bigger accuracy improvementfor adjunct arguments (6.02 absolute points) vs. core arguments (1.83 points, for thevoting model).
This is to be expected, as most core arguments fall under the Arg0 andArg1 classes, which can typically be disambiguated based on syntactic information (i.e.,subject vs. object).
On the other hand, there are no syntactic hints for adjunct arguments,so the system learns to rely more on SP information in this case.Regarding the performance of individual combinations of SRC and SP methods(e.g., SRC+SPRes), the differences among SP models in Table 13 are much smallerthan in Table 12.
SPsimpreLin?cos and SPsimpreLin?Jacyield the best results in both cases, anddistributional methods are slightly stronger than WordNet-based methods.
SPRes andSPwn perform similarly when combined, with a small lead for Resnik?s method.
Thesmaller differences and changes in the rank among SP methods are due to the complexinteractions when combining SP models with the SRC system.Table 14Precision (P), recall (R), and F1 results per argument type for the standalone SRC model andthe meta-classifier, in the two test data sets (WSJ and Brown).
Due to space limitations, theAM- prefix has been dropped from the labels of all adjuncts.
When classifying all arguments(last row), the F1 score is an accuracy score because in this scenario P = R = F1.
We checked forstatistical significance for the overall F1 scores (All row).
Values in boldface font indicate thehighest F1 score in the corresponding row and block.
F1 values marked with ?
are significantlylower than the corresponding highest F1 score.WSJ test Brown testSRC Meta-classifier SRC Meta-classifierP R F1 P R F1 P R F1 P R F1Arg0 93.6 96.7 95.1 95.1 97.4 96.2 87.6 89.3 88.4 89.4 91.0 90.2Arg1 93.3 94.5 93.9 94.2 95.7 95.0 84.3 90.6 87.3 86.2 91.9 89.0Arg2 86.0 82.6 84.3 87.8 87.4 87.6 52.7 56.8 54.7 55.9 59.9 57.8Arg3 77.6 63.4 69.8 82.4 68.3 74.7 36.4 19.0 25.0 45.8 26.2 33.3Arg4 86.8 78.6 82.5 89.5 81.0 85.0 59.4 34.5 43.7 67.9 34.5 45.8Core 92.9 93.6 93.3 94.2 95.1 94.6 82.6 86.3 84.4 84.6 87.9 86.3ADV 58.5 51.4 54.7 64.4 52.3 57.7 45.1 24.3 31.6 51.9 25.7 34.4CAU 61.1 71.0 65.7 80.0 77.4 78.7 64.7 45.8 53.7 84.6 45.8 59.5DIR 46.2 25.0 32.4 68.8 45.8 55.0 64.7 45.8 53.7 73.9 44.5 55.6DIS 84.3 82.7 83.5 95.6 82.7 88.7 52.6 27.0 35.7 54.5 32.4 40.7EXT 50.0 12.5 20.0 50.0 12.5 20.0 0.0 0.0 0.0 0.0 0.0 0.0LOC 85.2 80.9 83.0 85.0 84.7 84.8 67.8 61.2 64.3 68.3 68.7 68.5MNR 55.8 54.1 55.0 68.9 61.7 65.1 47.4 38.9 42.7 59.2 49.3 53.8PNC 51.9 37.8 43.8 62.5 40.5 49.2 51.7 39.5 44.8 53.3 42.1 47.1TMP 93.6 95.9 94.7 92.8 95.9 94.4 79.0 78.1 78.5 84.1 83.2 83.7Adj 83.1 79.6 81.3 86.2 81.9 84.0 64.9 52.1 57.8 69.8 58.0 63.4All ?
?
90.8?
?
?
92.4 ?
?
79.5?
?
?
81.9654Zapirain et alSelectional Preferences for Semantic Role ClassificationLastly, Table 14 shows a breakdown of the results by argument type for the orig-inal SRC model and the meta-classifier (results are also presented over all numberedarguments, Core, adjuncts, and Adj).
This comparison emphasizes the previous obser-vation that SPs are more useful for arguments that are independent of syntax than forarguments that are usually tied to certain syntactic constructs (i.e., Arg0 and Arg1).
Forexample, in domain the meta-classifier improves Arg0 classification with 1.1 F1 points,but it boosts the classification performance for causative arguments (AM-CAU) with 13absolute points.
A similar behavior is observed out of domain.
For example, whereasArg0 classification is improved with 1.7 points, the classification of manner arguments(AM-MNR) is improved by 11 points.
All in all, with two exceptions, selectional prefer-ences improve classification accuracy for all argument types, both in and out of domain.The previous experiments showed that a meta-classifier (and a voting approach)over a battery of base models improves over the performance of each individual clas-sifier.
Given that half of our base models are all relatively minor changes of the sameoriginal classifier (SwiRL), however, it would be desirable to ensure that the overallperformance gain of the meta-classification system is due to the infusion of semanticinformation that is missing in the baseline SRC, and not to a regularization effect comingfrom the ensemble of classifiers.
The qualitative analysis presented in Section 6 willreinforce this hypothesis.5.3 Results for End-to-End Semantic Role LabelingLastly, we investigate the contribution of SPs in an end-to-end SRL system.
As discussedbefore, our approach focuses on argument classification, a subtask of complete SRL,because this component suffers in the presence of lexical data sparseness (Pradhan,Ward, and Martin 2008).
To understand the impact of SPs on the complete SRL task wecompared two SwiRL models: one that uses the original classification model (the SRCline in Table 13) and another that uses our meta-classifier model (the Meta-classifierline in Table 13).
To implement this experiment we had to modify the publicly down-loadable SwiRL model, which performs identification and classification jointly, using asingle multi-class model.
We changed this framework to a pipeline model, which firstperforms argument identification (i.e., is this constituent an argument or not?
), followedby argument classification (i.e., knowing that this constituent is an argument, what isits label?
).17 We used the same set of features as the original SwiRL system and theoriginal model to identify argument boundaries.
This pipeline model allowed us toeasily plug in different classification models, which offers a simple platform to evaluatethe contribution of SPs in an end-to-end SRL system.Table 15 compares the original SwiRL pipeline (SwiRL in the table) with the pipelinemodel where the classification component was replaced with the meta-classifier previ-ously introduced (SwiRL w/ meta).
The latter model backs off to the original classifi-cation model for candidates that are not covered by our current selectional preferences(i.e., are not noun phrases or prepositional phrases containing a noun phrase as thesecond child).
We report results for the test partitions of WSJ and Brown in the sametable.
Note that these results are not directly comparable with the results in Tables 13and 14, because in those initial experiments we used gold argument boundaries whereas17 This pipeline model performs slightly worse than the original SwiRL on the WSJ data and slightly betteron Brown.655Computational Linguistics Volume 39, Number 3Table 15Precision (P), recall (R), and F1 results per argument for the end-to-end semantic role labelingtask.
We compared two models: the original SwiRL model and the one where the classificationcomponent was replaced with the meta-classifier introduced at the beginning of the section.
Weused the official CoNLL-2005 shared-task scorer to produce these results.
We checked forstatistical significance for the overall F1 scores (All row).
Values in boldface font indicate thehighest F1 score in the corresponding row and block.
F1 values marked with ?
are significantlylower than the corresponding highest F1 score.WSJ test Brown testSwiRL SwiRL w/ meta SwiRL SwiRL w/ metaP R F1 P R F1 P R F1 P R F1Arg0 87.0 81.6 84.2 87.8 81.9 84.8 86.6 81.3 83.9 87.3 81.7 84.4Arg1 79.1 71.8 75.3 79.4 72.1 75.6 70.2 64.6 67.3 71.1 65.2 68.0Arg2 70.0 56.6 62.6 69.2 58.3 63.3 41.8 42.7 42.2 42.3 44.6 43.4Arg3 72.4 43.9 54.7 72.6 44.5 55.2 36.4 12.9 19.0 34.6 14.5 20.5Arg4 73.3 61.8 67.0 73.8 60.8 66.7 48.8 25.6 33.6 44.4 25.6 32.5ADV 59.4 50.6 54.6 59.5 50.0 54.4 49.0 38.2 42.9 49.9 38.5 43.5CAU 61.5 43.8 51.2 66.0 45.2 53.7 58.7 35.5 44.3 59.1 34.2 43.3DIR 44.7 20.0 27.6 50.0 22.6 30.9 59.0 27.2 37.2 61.3 25.9 36.5DIS 76.1 63.8 69.4 77.0 63.8 69.7 58.8 41.0 48.3 59.7 41.3 48.9EXT 72.7 50.0 59.3 72.7 50.0 59.3 20.0 8.1 11.5 21.4 8.1 11.8LOC 64.7 52.9 58.2 64.8 55.4 59.7 48.3 37.7 42.3 46.8 40.5 43.5MNR 59.1 52.0 55.3 61.4 51.7 56.2 53.8 47.3 50.3 55.9 48.3 51.8PNC 47.1 34.8 40.0 46.4 33.9 39.2 51.8 26.4 35.0 52.4 26.7 35.1TMP 78.7 71.4 74.9 78.4 71.5 73.8 59.7 60.6 60.2 61.0 61.2 61.1All 79.7 70.9 75.0?
80.0 71.3 75.4 71.8 64.2 67.8?
72.4 64.6 68.4Table 15 shows results for an end-to-end model, which includes predicted argumentboundaries.Table 15 shows that the use of selectional preferences improves overall results whenusing predicted argument boundaries as well.
Selectional preferences improve F1 scoresfor four out of five core arguments in both WSJ and Brown, for six out of nine modifierarguments in WSJ, and for seven out of nine modifier arguments in Brown.
Notably, theSPs improve results for the most common argument types (Arg0 and Arg1).
All in all,SPs yield a 0.4 F1 point improvement in WSJ and 0.6 F1 point improvement in Brown.These improvements are small but they are statistically significant.
We consider these re-sults encouraging, especially considering that only a small percentage of arguments areactually inspected by selectional preferences.
This analysis is summarized in Table 16,which lists how many argument candidates are inspected by the system in its differentstages.
The table indicates that the vast majority of argument candidates are filteredout by the argument identification component, which does not use SPs.
Because of this,even though approximately 50% of the role classification decisions can be reinforcedwith SPs, only 4.5% and 3.6% of the total number of argument candidates in WSJ andBrown, respectively, are actually inspected by the classification model that uses SPs.6.
Analysis and DiscussionWe conducted a complementary manual analysis to further verify the usefulness of thesemantic information provided by the selectional preferences.
We manually inspected100 randomly selected classification cases, 50 examples in which the meta-classifier is656Zapirain et alSelectional Preferences for Semantic Role ClassificationTable 16Counts for argument candidates for the two test partitions on the end-to-end semantic rolelabeling task.
The Predicted non-arguments line indicates how many candidate arguments areclassified as non-arguments by the argument identification classifier.
The Incompatible with SPsline indicates how many candidates were classified as arguments but cannot be modeled by ourcurrent SPs (i.e., they are not noun phrases or prepositional phrases containing a noun phrase asthe second child).
Lastly, the Compatible with SPs line lists how many candidates were bothclassified as likely arguments and can be modeled by the SPs.WSJ test Brown testPredicted non-arguments 158,310 184,958Incompatible with SPs 5,739 11,167Compatible with SPs 7,691 7,867Total 171,740 203,992correct and the baseline SRC (SwiRL) is wrong, and 50 where the meta-classifier choosesthe incorrect classifier and the SRC is right.
Interestingly, we observed that the majorityof cases have a clear linguistic interpretation, shedding light on the reasons why themeta-classifier using SP information manages to correct some erroneous predictions ofthe original SRC model, but also on the limitations of selectional preferences.Regarding the success of the meta-classifier, the studied cases generally correspondto low frequency verb?argument head pairs, in which the baseline SRC might havehad problems with generalization.
In 29 of the cases (?58%), the syntactic informationis not enough to disambiguate the proper role, tends to indicate a wrong role label,or it confuses the SRC because it contains errors.
Most of the semantically based SPpredictions are correct, however, so the meta-classifier does select the correct role label.In another 15 cases (?30%) the source of the baseline SRC error is not clear, but still,several SP models suggest the correct role, giving the opportunity to the meta-classifierto make the right choice.
Finally, in the remaining six cases (?12%) a ?chance effect?
isobserved: The failure of the baseline SRC model does not have a clear interpretation and,moreover, most SP predictions are actually wrong.
In these situations, several labels arepredicted with the same confidence, and the meta-classifier selects the correct one bychance.Figure 2 shows four real examples in which we see the importance of the infor-mation provided by the selectional preferences.
In example (a), the verb flash neveroccurs in training with the argument head word news.
The syntactic structure alonestrongly suggests Arg0, because the argument is an NP just to the left of a verb in activeform.
This is probably why the baseline SRC incorrectly predicts Arg0.
Some semanticinformation is needed to know that the word news is not the agent of the predicate(Arg0), but rather the theme (thing shining, Arg1).
Selectional preferences make thiswork perfectly, because all variants predict the correct label by signaling that news ismuch more compatible with flash in Arg1 position rather than Arg0.In example (b), the predicate promise expects a person as Arg1 (person promised to,Recipient) and an action as Arg2 (promised action, Theme).
Moreover, the presence ofArg2 is obligatory.
The syntactic structure is correct but does not provide the semantic(Arg1 should be a person) or structural information (the assignment of Arg1 would haverequired an additional Arg2) needed to select the appropriate role.
SwiRL does not haveit either, and it assigns the incorrect Arg1 label.
Most SP models correctly predict thatinvestigation is more similar to the heads of Arg2 arguments of promise than to theheads of Arg1 arguments, however.657Computational Linguistics Volume 39, Number 3(a) Several traders could be seen shaking their heads when (([the news]Arg0?Arg1)NP( flashed)VP)S .
(b) Italian President Francesco Cossiga (promised ([a quick investigation intowhether Olivetti broke Cocom rules]Arg1?Arg2)NP)VP.
(c) Annual payments (will more than double ([from (a year ago)NP]TMP?Arg3)PP toabout $240 million ?
?
?
)VP ?
?
?
(d) Procter & Gamble Co. plans to (begin ((testing (next month)NP)VP)S ([a superco.detergent that ?
?
?
washload]Arg0?Arg1)NP)VP .Figure 2Examples of incorrect SwiRL role assignments fixed by the meta-classifier.
In each sentence, theverb is emphasized in italics and the head word for the selectional preferences is boldfaced.
Theargument under focus is marked within square brackets.
x ?
y means that the incorrect label xassigned by the baseline SwiRL model is corrected into role label y by the combined system.Finally, examples also contain simplified syntactic annotations from the test set predictedsyntactic layer, which are used for the discussion in the text.In example (c) we see the application of prep-role selectional preferences.
In thatsentence, the baseline SRC is likely confused by the content word feature of the PP?from a year ago?
(Surdeanu et al2003).
In PropBank, ?year?
is a strong indicatorof a temporal adjunct (AM-TMP).
The predicate double, however, describes the Arg3argument as ?starting point?
of the action and it is usually introduced by the prepositionfrom.
This is very common also for other motion verbs (go, rise, etc.
), resulting in thefrom-Arg3 selectional preference containing a number of heads of temporal expressions,in particular many more instances of the word year than the from-AM-TMP selectionalpreference.
As a consequence, the majority of SP models predict the correct Arg3 label.Finally, example (d) highlights that selectional preferences increase robustness infront of parsing errors.
In this example, the NP ?a superco.
detergent?
is incorrectlyattached to ?begin?
instead of the predicate testing by the syntactic parser.
This producesmany incorrect features derived from syntax (syntactic frame, path, etc.)
that may con-fuse the baseline SRC model, which ends up producing an incorrect Arg0 assignment.Most of the SP models, however, predict that detergent is not a plausible Agent for test(?examiner?
), but instead it fits best with the Arg1 position (?examined?
).Nevertheless, selectional preferences have a significant limitation: They do notmodel syntactic structures, which often give strong hints for classification.
In fact, thevast majority of the situations where the meta-classifier performs worse than the origi-nal SRC model are cases that are syntax-driven, hence situations that are incompletelyaddressed by the current SP models.
Even though the SRC and the SRC+SP modelshave features that model syntax, they can be overwhelmed by the SP features andstandalone models, which leads to incorrect meta-classification results.
Figure 3 shows afew representative examples in this category.
In the first example in the figure, the meta-classifier changes the correctly assigned label Arg2 to Arg1, because most SP modelsfavor the Arg1 label for the argument ?test.?
In the PropBank training corpus, however,the argument following the verb fail is labeled Arg2 in 79% of the cases.
Because theSP models do not take into account syntax or positional information, this syntacticpreference is lost.
Similarly, SPs do not model the fact that the verb buy is seldompreceded by an Arg1 argument, or the argument immediately following the verb precedetends to be Arg1, hence the incorrect classifications in Figure 3 (b) and (c).
All these658Zapirain et alSelectional Preferences for Semantic Role Classification(a) Some ?circuit breakers?
installed after the October 1987 crash (failed ([their firsttest ]Arg2?Arg1)NP)VP...(b) Many fund managers argue that now?s ([the time]TMP?Arg1)NP (to buy)VP)S .
(c) Telephone volume was up sharply, but it was still at just half the level of theweekend (preceding ([Black Monday ]Arg1?TMP)NP)VP .Figure 3Examples of incorrect assignments by the meta-classifier.
In each sentence, the verb isemphasized in italics and the head word for the selectional preferences is boldfaced.
Theargument under focus is marked within square brackets.
x ?
y means that the correctx label assigned by the baseline model is wrongly converted into y by the meta-classifier.As in Figure 2, examples also contain simplified syntactic annotations taken from the testset predicted syntactic layer.examples are strong motivation for SP models that model both lexical and syntacticpreferences.
We will address such models in future work.7.
ConclusionsCurrent systems usually perform SRL in two pipelined steps: argument identificationand argument classification.
Whereas identification is mostly syntactic, classificationrequires semantic knowledge to be taken into account.
In this article we have shownthat the lexical heads seen in training data are too sparse to assign the correct role,and that selectional preferences are able to generalize those lexical heads.
In fact, weshow for the first time that the combination of the predictions of several selectionalpreference models with a state-of-the-art SRC system yields significant improvements inboth in-domain and out-of-domain test sets.
These improvements to role classificationtranslate into small but statistically significant improvements in an end-to-end semanticrole labeling system.
We find these results encouraging considering that in the completesemantic role labeling task only a small percentage of argument candidates are affectedby our modified role classification model.
The experiments were carried out over thewell-known CoNLL-2005 data set, based on PropBank.We applied several selectional preference models, based on WordNet and distribu-tional similarity.
Our experiments show that all models outperform the pure lexicalmatching approach, with distributional methods performing better that WordNet-basedmethods, and second-order similarity models being the best.
In addition to the tradi-tional selectional preferences for verbs, we introduce the use of selectional preferencesfor prepositions, which are applied to classifying prepositional phrases.
The combi-nation of both types of selectional preferences improves over the use of selectionalpreferences for verbs alone.The analysis performed over the cases where the base SRC system and the com-bined system differed showed that the selectional preferences are specially helpful whensyntactic information is either incorrect or insufficient to disambiguate the correct role.The analysis also highlighted that the limitations of selectional preferences for modelingsyntactic structures introduce some errors in the combined model.
Those errors couldbe addressed if the SP models included some syntactic information.Our research leaves the door open for tighter integration of semantic and syntacticinformation for Semantic Role Labeling.
We introduced selectional preferences in theSRC system as simple features, but models which extend syntactic structures with659Computational Linguistics Volume 39, Number 3selectional preferences (or vice versa) could overcome some of the errors that our systemintroduced.
Extending the use of selectional preferences to other syntactic types beyondnoun phrases and prepositional phrases would be also of interest.
In addition, themethod for combining selectional preferences for verbs and prepositions was naive,and we expect that a joint model of verb and preposition preferences for prepositionalphrases would improve results further.
Finally, individual selectional preference meth-ods could be improved and newer methods incorporated, which could further improvethe results.AcknowledgmentsThe authors would like to thank the threeanonymous reviewers for their detailedand insightful comments on the submittedversion of this manuscript, which helpedus to improve it significantly in this revision.This work was partially funded bythe Spanish Ministry of Science andInnovation through the projects OpenMT-2(TIN2009-14675-C03) and KNOW2(TIN2009-14715-C04-04).
It also receivedfinancial support from the SeventhFramework Programme of the EU(FP7/2007- 2013) under grant agreements247762 (FAUST) and 247914 (MOLTO).Mihai Surdeanu was supported by the AirForce Research Laboratory (AFRL) underprime contract no.
FA8750-09-C-0181.Any opinions, findings, and conclusionor recommendations expressed in thismaterial are those of the authors and donot necessarily reflect the view of theAir Force Research Laboratory (AFRL).ReferencesAgirre, Eneko, Timothy Baldwin, andDavid Martinez.
2008.
Improvingparsing and PP attachment performancewith sense information.
In Proceedingsof ACL-08: HLT, pages 317?325,Columbus, OH.Agirre, Eneko, Kepa Bengoetxea, KoldoGojenola, and Joakim Nivre.
2011.Improving dependency parsing withsemantic classes.
In Proceedings of the49th Annual Meeting of the Associationfor Computational Linguistics: HumanLanguage Technologies, pages 699?703,Portland, OR.Agirre, Eneko and David Martinez.
2001.Learning class-to-class selectionalpreferences.
In Proceedings of the 2001Workshop on Computational NaturalLanguage Learning (CoNLL-2001),pages 1?8, Toulouse.Agirre, Eneko and German Rigau.
1996.Word sense disambiguation usingconceptual density.
In Proceedings of the16th Conference on ComputationalLinguistics - Volume 1, COLING ?96,pages 16?22, Stroudsburg, PA.Baroni, Marco and Alessandro Lenci.2010.
Distributional memory: A generalframework for corpus-based semantics.Computational Linguistics, 36(4):673?721.Bergsma, Shane, Dekang Lin, and RandyGoebel.
2008.
Discriminative learning ofselectional preference from unlabeled text.In Proceedings of EMNLP, pages 59?68,Honolulu, HI.Boas, H. C. 2002.
Bilingual framenetdictionaries for machine translation.In Proceedings of the Third InternationalConference on Language Resources andEvaluation (LREC), pages 1,364?1,371,Las Palmas de Gran Canaria.Brockmann, Carsten and Mirella Lapata.2003.
Evaluating and combiningapproaches to selectional preferenceacquisition.
In Proceedings of the 10thConference of the European Chapter of theAssociation of Computational Linguistics(EACL-2003), pages 27?34, Budapest.Carreras, X. and L. Ma`rquez.
2004.Introduction to the CoNLL-2004Shared Task: Semantic Role Labeling.In Proceedings of the Eighth Conferenceon Computational Natural LanguageLearning (CoNLL-2004), pages 89?97,Boston, MA.Carreras, X. and L. Ma`rquez.
2005.Introduction to the CoNLL-2005Shared Task: Semantic Role Labeling.In Proceedings of the Ninth Conferenceon Computational Natural LanguageLearning (CoNLL-2005), pages 152?164,Ann Arbor, MI.Chakraborti, Sutanu, Nirmalie Wiratunga,Robert Lothian, and Stuart Watt.
2007.Acquiring word similarities with higherorder association mining.
In Proceedingsof the 7th International Conference onCase-Based Reasoning: Case-Based ReasoningResearch and Development, ICCBR ?07,pages 61?76, Berlin.660Zapirain et alSelectional Preferences for Semantic Role ClassificationChambers, Nathanael and Daniel Jurafsky.2010.
Improving the use of pseudo-wordsfor evaluating selectional preferences.
InProceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics,pages 445?453, Uppsala, Sweden.Charniak, E. 2000.
A maximum-entropyinspired parser.
In Proceedings of the 1stMeeting of the North American Chapterof the Association for ComputationalLinguistics (NAACL-2000), pages 132?139,Seattle, WA.Clark, Stephen and Stephen Weir.
2002.Class-based probability estimationusing a semantic hierarchy.
ComputationalLinguistics, 28(2):187?206.Edmonds, Philip.
1997.
Choosing the wordmost typical in context using a lexicalco-occurrence network.
In Proceedings of the35th Annual Meeting of the Association forComputational Linguistics and EighthConference of the European Chapter of theAssociation for Computational Linguistics,ACL ?98, pages 507?509, Stroudsburg, PA.Erk, Katrin.
2007.
A simple, similarity-basedmodel for selectional preferences.
InProceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics(ACL-2007), pages 216?223, Prague.Erk, Katrin, Sebastian Pado?, and Ulrike Pado?.2010.
A flexible, corpus-driven model ofregular and inverse selectional preferences.Computational Linguistics, 36(4):723?763.Fillmore, C. J., J. Ruppenhofer, and C. F.Baker.
2004.
FrameNet and representingthe link between semantic and syntacticrelations.
In Frontiers in Linguistics,volume I of Language and LinguisticsMonograph Series B.
Institute of Linguistics,Academia Sinica, Taipei, pages 19?59.Gildea, D. and D. Jurafsky.
2002.
Automaticlabeling of semantic roles.
ComputationalLinguistics, 28(3):245?288.Grefenstette, Gregory.
1992.
Sextant:Exploring unexplored contexts forsemantic extraction from syntacticanalysis.
In ACL?92, pages 324?326,Newark, DE.Higashinaka, Ryuichiro and Hideki Isozaki.2008.
Corpus-based question answeringfor why-questions.
In Proceedings of theThird International Joint Conference onNatural Language Processing (IJCNLP),pages 418?425, Hyderabad.Hindle, Donald.
1990.
Noun classificationfrom predicate-argument structures.
InProceedings of the 28th Annual Meeting of theAssociation for Computational Linguistics(ACL-1990), pages 268?275, Pittsburgh, PA.Koo, Terry, Xavier Carreras, and MichaelCollins.
2008.
Simple semi-superviseddependency parsing.
In Proceedingsof ACL-08: HLT, pages 595?603,Columbus, OH.Lee, Lillian.
1999.
Measures of distributionalsimilarity.
In 37th Annual Meeting of theAssociation for Computational Linguistics,pages 25?32, College Park, MD.Li, Hang and Naoki Abe.
1998.
Generalizingcase frames using a thesaurus and theMDL principle.
Computational Linguistics,24(2):217?244.Lin, Dekang.
1998.
Automatic retrievaland clustering of similar words.
InProceedings of the 36th Annual Meetingof the Association for ComputationalLinguistics and the 17th InternationalConference on Computational Linguistics(COLING-ACL-1998), pages 768?774,Montreal.Litkowski, K. C. and O. Hargraves.
2005.
Thepreposition project.
In Proceedings of theACL-SIGSEM Workshop on the LinguisticDimensions of Prepositions and their Use inComputational Linguistic Formalisms andApplications, pages 171?179, Colchester.Litkowski, K. C. and O. Hargraves.
2007.SemEval-2007 Task 06: Word-sensedisambiguation of prepositions.In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations(SemEval-2007), pages 24?29, Prague.Litkowski, Ken and Orin Hargraves.2006.
Coverage and inheritance in thepreposition project.
In Prepositions ?06:Proceedings of the Third ACL-SIGSEMWorkshop on Prepositions, pages 37?44,Trento.Marcus, Mitchell, Grace Kim, Mary AnnMarcinkiewicz, Robert MacIntyre,Ann Bies, Mark Ferguson, Karen Katz,and Britta Schasberger.
1994.
The PennTreebank: Annotating predicate argumentstructure.
In Proceedings of the Workshop onHuman Language Technology (HLT-94),pages 114?119, Plainsboro, NJ.Ma`rquez, Llu?
?s, Xavier Carreras, Kenneth C.Litkowski, and Suzanne Stevenson.
2008.Semantic role labeling: An introduction tothe special issue.
Computational Linguistics,34(2):145?159.McCarthy, Diana and John Carroll.
2003.Disambiguating nouns, verbs, andadjectives using automatically acquiredselectional preferences.
ComputationalLinguisties, 29:639?654.Melli, Gabor, Yang Wang, Yudong Liu,Mehdi M. Kashani, Zhongmin Shi,661Computational Linguistics Volume 39, Number 3Baohua Gu, Anoop Sarkar, and FredPopowich.
2005.
Description of SQUASH,the SFU question answering summaryhandler for the DUC-2005 summarizationtask.
In Proceedings of DocumentUnderstanding Workshop, HLT/EMNLPAnnual Meeting, Vancouver.Moschitti, Alessandro, Silvia Quarteroni,Roberto Basili, and Suresh Manandhar.2007.
Exploiting syntactic and shallowsemantic kernels for question/answerclassification.
In Proceedings of the45th Conference of the Association forComputational Linguistics (ACL),pages 776?783, Prague.Narayanan, S. and S. Harabagiu.
2004.Question answering based on semanticstructures.
In Proceedings of the 20thInternational Conference on ComputationalLinguistics (COLING), pages 693?701,Geneva.Noreen, E. W. 1989.
Computer-IntensiveMethods for Testing Hypotheses:An Introduction, Wiley.O?
Se?aghdha, Diarmuid.
2010.
Latentvariable models of selectional preference.In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics,pages 435?444, Uppsala.Pado?, Sebastian and Mirella Lapata.
2007.Dependency-based construction ofsemantic space models.
ComputationalLinguistics, 33(2):161?199.Pado?, Sebastian, Ulrike Pado?, and Katrin Erk.2007.
Flexible, corpus-based modellingof human plausibility judgements.
InProceedings of the 2007 Joint Conferenceon Empirical Methods in Natural LanguageProcessing and Computational NaturalLanguage Learning (EMNLP-CoNLL-2007),pages 400?409, Prague.Palmer, M., D. Gildea, and P. Kingsbury.2005.
The proposition bank: An annotatedcorpus of semantic roles.
ComputationalLinguistics, 31(1):71?105.Pantel, Patrick, Rahul Bhagat, BonaventuraCoppola, Timothy Chklovski, andEduard Hovy.
2007.
ISP: Learninginferential selectional preferences.In Human Language Technologies 2007:The Conference of the North AmericanChapter of the Association for ComputationalLinguistics; Proceedings of the MainConference, pages 564?571, Rochester, NY.Pantel, Patrick and Dekang Lin.
2000.
Anunsupervised approach to prepositionalphrase attachment using contextuallysimilar words.
In Proceedings of the 38thAnnual Conference of the Association ofComputational Linguistics (ACL-2000),pages 101?108, Hong Kong.Pradhan, S., W. Ward, and J. H. Martin.
2008.Towards robust semantic role labeling.Computational Linguistics, 34(2):289?310.Rada, R., H. Mili, E. Bicknell, and M. Blettner.1989.
Development and application of ametric on semantic nets.
IEEE Transactionson Systems, Man, and Cybernetics,19(1):17?30.Ratinov, Lev and Dan Roth.
2009.
Designchallenges and misconceptions in namedentity recognition.
In Proceedings of theThirteenth Conference on ComputationalNatural Language Learning (CoNLL-2009),pages 147?155, Boulder, CO.Resnik, Philip.
1993a.
Selection andInformation: A Class-Based Approach toLexical Relationships.
Ph.D. thesis,University of Pennsylvania.Resnik, Philip.
1993b.
Semantic classes andsyntactic ambiguity.
In Proceedings of theWorkshop on Human Language Technology,pages 278?283, Morristown, NJ.Ritter, Alan, Mausam, and Oren Etzioni.2010.
A latent Dirichlet alcation methodfor selectional preferences.
In Proceedings ofthe 48th Annual Meeting of the Association forComputational Linguistics, pages 424?434,Uppsala.Schu?tze, Hinrich.
1998.
Automatic wordsense discrimination.
ComputationalLinguistics, 24(1):97?123.Srikumar, V. and D. Roth.
2011.
A jointmodel for extended semantic role labeling.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMNLP), pages 129?139, Edinburgh.Surdeanu, M., S. Harabagiu, J. Williams,and P. Aarseth.
2003.
Using predicate-argument structures for informationextraction.
In Proceedings of the 41st AnnualMeeting of the Association for ComputationalLinguistics (ACL-2003), pages 8?15,Sapporo.Surdeanu, Mihai, Massimiliano Ciaramita,and Hugo Zaragoza.
2011.
Learning torank answers to non-factoid questionsfrom Web collections.
ComputationalLinguistics, 37(2):351?383.Surdeanu, Mihai, Llu?
?s Ma`rquez, XavierCarreras, and Pere R. Comas.
2007.Combination strategies for semantic rolelabeling.
Journal of Artificial IntelligenceResearch (JAIR), 29:105?151.Sussna, Michael.
1993.
Word sensedisambiguation for free-text indexingusing a massive semantic network.
InProceedings of the Second International662Zapirain et alSelectional Preferences for Semantic Role ClassificationConference on Information and KnowledgeManagement, CIKM ?93, pages 67?74,New York, NY.Wilks, Yorick.
1975.
Preference semantics.In E. L. Kaenan, editor, Formal Semantics ofNatural Language.
Cambridge UniversityPress, Cambridge, MA, pages 329?348.Zapirain, Ben?at, Eneko Agirre, and Llu??sMa`rquez.
2009.
Generalizing over lexicalfeatures: Selectional preferences forsemantic role classification.
In Proceedingsof the Joint Conference of the 47th AnnualMeeting of the Association for ComputationalLinguistics and the 4th International JointConference on Natural Language Processing(ACL-IJCNLP-2009), pages 73?76, Suntec.Zapirain, Ben?at, Eneko Agirre, Llu?
?sMa`rquez, and Mihai Surdeanu.
2010.Improving semantic role classificationwith selectional preferences.
In Proceedingsof the 11th Annual Conference of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL HLT2010), pages 373?376, Los Angeles, CA.663
