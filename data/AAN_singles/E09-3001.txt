Proceedings of the EACL 2009 Student Research Workshop, pages 1?9,Athens, Greece, 2 April 2009. c?2009 Association for Computational LinguisticsModelling Early Language Acquisition Skills:Towards a General Statistical Learning MechanismGuillaume AimettiUniversity of SheffieldSheffield, UKg.aimetti@dcs.shef.ac.ukAbstractThis paper reports the on-going research of athesis project investigating a computationalmodel of early language acquisition.
Themodel discovers word-like units from cross-modal input data and builds continuouslyevolving internal representations within a cog-nitive model of memory.
Current cognitivetheories suggest that young infants employgeneral statistical mechanisms that exploit thestatistical regularities within their environmentto acquire language skills.
The discovery oflexical units is modelled on this behaviour asthe system detects repeating patterns from thespeech signal and associates them to discreteabstract semantic tags.
In its current state, thealgorithm is a novel approach for segmentingspeech directly from the acoustic signal in anunsupervised manner, therefore liberating itfrom a pre-defined lexicon.
By the end of theproject, it is planned to have an architecturethat is capable of acquiring language andcommunicative skills in an online manner, andcarry out robust speech recognition.
Prelimi-nary results already show that this method iscapable of segmenting and building accurateinternal representations of important lexicalunits as ?emergent?
properties from cross-modal data.1 IntroductionConventional Automatic Speech Recognition(ASR) systems can achieve very accurate recog-nition results, particularly when used in their op-timal acoustic environment on examples withintheir stored vocabularies.
However, when takenout of their comfort zone accuracy significantlydeteriorates and does not come anywhere nearhuman speech processing abilities for even thesimplest of tasks.
This project investigates novelcomputational language acquisition techniquesthat attempt to model current cognitive theoriesin order to achieve a more robust speech recogni-tion system.Current cognitive theories suggest that oursurrounding environment is rich enough to ac-quire language through the use of simple statisti-cal processes, which can be applied to all oursenses.
The system under development aims tohelp clarify this theory, implementing a compu-tational model that is general across multiplemodalities and has not been pre-defined with anylinguistic knowledge.In its current form, the system is able to detectwords directly from the acoustic signal and in-crementally build internal representations withina memory architecture that is motivated by cog-nitive plausibility.
The algorithm proposed canbe split into two main processes, automatic seg-mentation and word discovery.
Automaticallysegmenting speech directly from the acousticsignal is made possible through the use of dy-namic programming (DP); we call this methodacoustic DP-ngram?s.
The second stage, keyword discovery (KWD), enables the model tohypothesise and build internal representations ofword classes that associates the discovered lexi-cal units with discrete abstract semantic tags.Cross-modal input is fed to the system throughthe interaction of a carer module as an ?audio?and ?visual?
stream.
The audio stream consists ofan acoustic signal representing an utterance,while the visual stream is a discrete abstract se-mantic tag referencing the presence of a keyword within the utterance.Initial test results show that there is significantpotential with the current algorithm, as it seg-ments in an unsupervised manner and does notrely on a predefined lexicon or acoustic phonemodels that constrain current ASR methods.1The rest of this paper is organized as follows.Section 2 reviews current developmental theoriesand computational models of early language ac-quisition.
In section 3, we present the currentimplementation of the system.
Preliminary ex-periments and results are described in sections 4and 5 respectively.
Conclusions and further workare discussed in sections 6 and 7 respectively.2 Background2.1 Current Developmental TheoriesThe ?nature?
vs. ?nurture?
debate has been foughtout for many years now; are we born with innatelanguage learning capabilities, or do we solelyuse the input from the environment to find struc-ture in language?Nativists believe that infants have an innatecapability for acquiring language.
It is their viewthat an infant can acquire linguistic structurewith little input and that it plays a minor role inthe speed and sequence with which they learnlanguage.
Noam Chomsky is one of the mostcited language acquisition nativists, claimingchildren can acquire language ?On relativelyslight exposure and without specific training?
(Chomsky, 1975, p.4).On the other hand, non-nativists argue that theinput contains much more structural informationand is not as full of errors as suggested by nativ-ists (Eimas et al, 1971; Best et al, 1988; Jusc-zyk et al, 1993; Saffran et al, 1996;Christiansen et al, 1998; Saffran et al, 1999;Saffran et al, 2000; Kirkham et al, 2002;Anderson et al, 2003; Seidenberg et al, 2002;Kuhl, 2004; Hannon and Trehub, 2005).Experiments by Saffran et al (1996, 1999)show that 8-month old infants use the statisticalinformation in speech as an aid for word segmen-tation with only two minutes of familiarisation.Inspired by these results, Kirkham et al(2002) suggest that the same statistical processesare also present in the visual domain.
Kirkham etal.
(2002) carried out experiments showing thatpreverbal infants are able to learn patterns of vis-ual stimuli with very short exposure.Other theories hypothesise that statistical andgrammatical processes are both used when learn-ing language (Seidenberg et al, 2002; Kuhl,2004).
The hypothesis is that newborns begin lifeusing statistical processes for simpler problems,such as learning the sounds of their native lan-guage and building a lexicon, whereas grammaris learnt via non-statistical methods later on.
Sei-denberg et al (2002) believe that learninggrammar begins when statistical learning ends.This has proven to be a very difficult boundaryto detect.2.2 Current Computational ModelsThere has been a lot of interest in trying to seg-ment speech in an unsupervised manner, there-fore liberating it from the required expert knowl-edge needed to predefine the lexical units forconventional ASR systems.
This has led speechrecognition researchers to delve into the cogni-tive sciences to try and gain an insight into howhumans achieve this without much difficulty andmodel it.Brent (1999) states that for a computationalalgorithm to be cognitively plausible it must:?
Start with no prior knowledge of generallanguage structure.?
Learn in a completely unsupervisedmanner.?
Segment incrementally.An automatic segmentation method similar tothat of the acoustic DP-ngram method is segmen-tal DTW.
Park & Glass (2008) have adapted dy-namic time warping (DTW) to find matchingacoustic patterns between two utterances.
Thediscovered units are then clustered, using an ad-jacency graph method, to describe the topic ofthe speech data.Statistical Word Discovery (SWD) (ten Boschand Cranen, 2007) and the Cross-channel EarlyLexical Learning (CELL) model (Roy and Pent-land, 2002), also similar methods to the one de-scribed in this paper, discover word-like unitsand then updating internal representationsthrough clustering processes.
The downfall of theCELL approach is that it assumes speech is ob-served as an array of phone probabilities.A more radical approach is Non-negative ma-trix factorization (NMF) (Stouten et al, 2008).NMF detects words from ?raw?
cross-modal in-put without any kind of segmentation during thewhole process, coding recurrent speech frag-ments into to ?word-like?
entities.
However, thefactorisation process removes all temporal in-formation.3 The Proposed System3.1 ACORNSThe computational model reported in this paperis being developed as part of a European projectcalled ACORNS (Acquisition of Communication2and Recognition Skills).
The ACORNS projectintends to design an artificial agent (LittleAcorns) that is capable of acquiring human ver-bal communication skills.
The main objective isto develop an end-to-end system that is biologi-cally plausible; restricting the computational andmathematical methods to those that model be-havioural data of human speech perception andproduction within five main areas:Front-end Processing: Research and devel-opment of new feature representations guided byphonetic and psycho-linguistic experiments.Pattern Discovery: Little Acorns (LA) willstart life without any prior knowledge of basicspeech units, discovering them from patternswithin the continuous input.Memory Organisation and Access: A mem-ory architecture that approaches cognitive plau-sibility is employed to store discovered units.Information Discovery and Integration: Ef-ficient and effective techniques for retrieving thepatterns stored in memory are being developed.Interaction and Communication: LA isgiven an innate need to grow his vocabulary andcommunicate with the environment.3.2 The Computational ModelThere are two key processes to the language ac-quisition model described in this paper; auto-matic segmentation and word discovery.
Theautomatic segmentation stage allows the systemto build a library of similar repeating speechfragments directly from the acoustic signal.
Thesecond stage associates these fragments with theobserved semantic tags to create distinct keyword classes.Automatic SegmentationThe acoustic DP-ngram algorithm reported inthis section is a modification of the precedingDP-ngram algorithm (Sankoff and Kruskal,1983; Nowell and Moore, 1995).
The originalDP-ngram model was developed by Sankoff andKruskal (1983) to find two similar portions ofgene sequences.
Nowell and Moore (1995) thenmodified this model to find repeated patternswithin a single phone transcription sequencethrough self-similarity.
Expanding on thesemethods, the author has developed a variant thatis able to segment speech, directly from theacoustic signal; automatically segmenting impor-tant lexical fragments by discovering ?similar?repeating patterns.
Speech is never the sametwice and therefore impossible to find exactrepetitions of importance (e.g.
phones, words orsentences).The use of DP allows this algorithm to ac-commodate temporal distortion through dynamictime warping (DTW).
The algorithm finds partialmatches, portions that are similar but not neces-sarily identical, taking into account noise, speedand different pronunciations of the speech.Traditional template based speech recognitionalgorithms using DP would compare two se-quences, the input speech vectors and a wordtemplate, penalising insertions, deletions andsubstitutions with negative scores.
Instead, thisalgorithm uses quality scores, positive and nega-tive, to reward matches and prevent anythingelse; resulting in longer, more meaningful sub-sequences.Figure 1: Acoustic DP-ngram Processes.Figure 1 displays the simplified architecture ofthe acoustic DP-ngram algorithm.
There are fourmain stages to the process:Stage 1: The ACORNS MFCC front-end isused to parameterise the raw speech signal of thetwo utterances being fed to the system.
The de-fault settings have been used to output a series of37-element feature vectors.
The front-end isbased on Mel-Frequency Coefficients (MFCC),which reflects the frequency sensitivity of theauditory system, to give 12 MFCC coefficients.A measure of the raw energy is added along with12 differential (?)
and 12 2nd differential (??)coefficients.
The front-end also allows the optionfor cepstral mean normalisation (CMN) and cep-stral mean and variance normalisation (CMVN).Stage 2: A local-match distance matrix is thencalculated by measuring the cosine distance be-SpeechUtterance 1 (Ui)Utterance 2 (Uj)Get Feature VectorsPre-ProcessingCreate Distance MatrixCalculate Quality ScoresFind Local AlignmentsDiscovered Lexical UnitsDP-ngramAlgorithm3tween each pair of frames ( )1 2,v v  from the twosequences, which is defined by:1 2 1 2 1 2( , ) ( . )
/ ( .
)TTd v v v v v v=  (1)Stage 3: The distance matrix is then used to cal-culate accumulative quality scores for successiveframe steps.
The recurrence defined in equation(2) is used to find all quality scores,i jq .In order to maximize on quality, substitutionscores must be positive and both insertion anddeletion scores must be negative as initialised inequation (3).
( )( )( )1, 1, 1,, 1 , 1 , 1,1, 1 1, 1 1, 1,,,1 ,1 ,max,0,.
.. .. .iji ji j i j i ji j i j i ji ji j i j i jaba bq s d qq s d qqq s d q???
?
??
?
??
?
?
?
?
?+ ?+ ?=+?????????
(2)where,,,,,,1.1    (Insertion score)1.1    (Deletion score)1.1    (Substitution score)frame-frame distanceAccumulative quality scoreiji jaba bi ji jsssdq?
?= ?= ?= +==(3)The recurrence in equation (2) stops past dissimi-larities causing global effects by setting all nega-tive scores to zero, starting a fresh new homolo-gous relationship between local alignments.Figure 2: Quality score matrix calculated from twodifferent utterances.
The plot also displays the optimallocal alignment.Figure 2 shows the plot of the quality scores cal-culated from two different utterances.
Theshaded areas show repeating structure; longerand more accurate fragments attain greater qual-ity scores, indicated by the darker areas withinthe plot.Applying a substitution score of 1 will causethe accumulative quality score to grow as a linearfunction.
The current settings defined by equa-tion (3) use a substitution score greater than 1,thus allowing local accumulative quality scoresto grow exponentially, giving longer alignmentsmore importance.By setting insertion and deletion scores to val-ues less than -1, the model will find closermatching acoustic repetitions; whereas a valuegreater than -1 and less than 0 allows the modelto find repeated patterns that are longer and lessaccurate, therefore allowing control over the tol-erance for temporal distortion.Stage 4: The final stage is to discover localalignments from within the quality score matrix.Backtracking pointers ( )bt  are maintained ateach step of the recursion:,( 1, ),        (Insertion)( , 1),        (Deletion)( 1, 1),   (Substitution)(0,0)             (Initial pointer)i ji ji jbti j????
?= ??
????
(4)When the quality scores have been calculatedthrough equation (2), it is possible to backtrackfrom the highest score to obtain the local align-ments in order of importance with equation (4).A threshold is set so that only local alignmentsabove a desired quality score are to be retrieved.Figure 2 presents the optimal local alignmentthat was discovered by the acoustic DP-ngramalgorithm for the utterances ?Ewan is shy?
and?Ewan sits on the couch?.The discovered repeated pattern (the dark linein figure 2) is [y uw ah n].
Start and stop timesare collected which allows the model to retrievethe local alignment from the original audio signalin full fidelity when required.Key Word DiscoveryThe milestone set for all systems developedwithin the ACORNS project is for LA to learn 10key words.
To carry out this task, the DP-ngramalgorithm has been modified with the addition ofa key word discovery (KWD) method that con-tinues the theme of a general statistical learningmechanism.
The acoustic DP-ngram algorithmexploits the co-occurrence of similar acousticpatterns within different utterances; whereas, the10 20 30 40 50 60 70 80102030405060708090100110 051015Utterance 2Utterance1Quality Score Matrix with Local Alignment4KWD method exploits the co-occurrence of theassociated discrete abstract semantic tags.
Thisallows the system to associate cross-modal re-peating patterns and build internal representa-tions of the key words.KWD is a simple approach that creates a classfor each key word (semantic tag) observed, inwhich all discovered exemplar units representingeach key word are stored.
With this list of epi-sodic segments we can perform a clusteringprocess to derive an ideal representation of eachkey word.For a single iteration of the DP-ngram algo-rithm, the current utterance ( )curUtt  is comparedwith another utterance in memory ( )nUtt .
KWDhypothesises whether the segments found withinthe two utterances are potential key words, bysimply comparing the associated semantic tags.There are three possible paths for a single itera-tion:1: If the tag ofcurUtt  has never been seen thencreate a new key word class and store the wholeutterance as an exemplar of it.
Do not carry outthe acoustic DP-ngram process and proceed tothe next utterance in memory 1( )nUtt + .2: If both utterances share the same tag thenproceed with the acoustic DP-ngram process andappend discovered local alignments to the keyword class representing that tag.
Proceed to thenext utterance in memory 1( )nUtt + .3: If both utterances contain different tags thendo not carry out acoustic DP-ngram?s and pro-ceed to the next utterance in memory 1( )nUtt + .By creating an exemplar list for each key wordclass we are able to carry out a clustering processthat allows us to create a model of the ideal rep-resentation.
Currently, the clustering process im-plemented simply calculates the ?centroid?
ex-emplar, finding the local alignment with theshortest distance from all the other local align-ments within the same class.
The ?centroid?
isupdated every time a new local alignment isadded, therefore the system is creating internalrepresentations that are continuously evolvingand becoming more accurate with experience.For recognition tasks the system can be set touse either the ?centroid?
exemplar or all thestored local alignments for each key word class.LA ArchitectureThe algorithm runs within a memory structure(fig.
3) developed with inspiration from currentcognitive theories of memory (Jones et al,2006).
The memory architecture works as fol-lows:Carer: The carer interacts with LA to con-tinuously feed the system with cross-modal input(acoustic & semantic).Figure 3: Little Acorns?
memory architecture.Perception: The stimulus is processed by the?perception?
module, converting the acoustic sig-nal into a representation similar to the humanauditory system.Short Term Memory (STM): The output ofthe ?perception?
module is stored in a limitedSTM which acts as a circular buffer to store npast utterances.
The n past utterances are com-pared with the current input to discover repeatedpatterns in an incremental fashion.
As a batchprocess LA can only run on a limited number ofutterances as the search space is unbound.
As anincremental process, LA could potentially handlean infinite number of utterances, thus making it amore cognitively plausible system.Long Term Memory (LTM): The ever in-creasing lists of discovered units for each keyword representation are stored in LTM.
Cluster-ing processes can then be applied to build andupdate internal representations.
The representa-tions stored within LTM are only pointers towhere the segment lies within the very long termmemory.Very Long Term Memory: The very longterm memory is used to store every observed ut-terance.
It is important to note that unless there isa pointer for a segment of speech within LTMthen the data cannot be retrieved.
But, futurework may be carried out to incorporate addi-tional ?sleeping?
processes on the data stored inVLTM to re-organise internal representations orcarry out additional analysis.4 ExperimentsAccuracy of experiments within the ACORNSproject is based on LA?s response to its carer.The correct response is for LA to predict the keyCARER STM/Working Memory Episodic BufferLTMInternal Represen-tationsVLTMEpisodic Memoryof all past eventsPerceptionFront-endprocessingLAMulti-ModalSensory DataResponse from LADP-ngram - Pattern DiscoveryRetrieval:MemoryAccessKWDInformationDiscoveryand Organi-sation5word tag associated with the current incomingutterance while only observing the speech signal.LA re-uses the acoustic DP-ngram algorithm tosolve this task in a similar manner to traditionalDP template based speech recognition.
The rec-ognition process is carried out by comparing ex-emplars, of discovered key words, against thecurrent incoming utterance and calculating aquality distance (as described in stage 3 of sec-tion 3.2).
Thus, the exemplar producing the high-est quality score, by finding the longest align-ment, is taken to be the match, with which wecan predict its associated visual tag.A number of different experiments have beencarried out:E1 - Optimal STM Window: This experi-ment finds the optimal utterance window lengthfor the system as an incremental process.
Vary-ing values of the utterance window length (from1 to 100) were used to obtain key word recogni-tion accuracy results across the same data set.E2 - Batch vs.
Incremental: The optimalwindow length chosen for the incremental im-plementation is compared against the batch im-plementation of the algorithm.E3 - Centroid vs. Exemplars: The KWDprocess stores a list of exemplars representingeach key word class.
For the recognition task wecan either use all the exemplars in each key wordlist or a single ?centroid?
exemplar that bestrepresents the list.
This experiment will comparethese two methods for representing internal rep-resentations of the key words.E4 ?
Speaker Dependency: The algorithm istested on its ability to handle the variation inspeech from different speakers with differentfeature vectors.1234HTK MFCC's (no norm)ACORNS MFCC's (no norm)ACORNS MFCC's (Cepstral Mean Norm)ACORNS MFCC's (Cepstral Mean and Variance Norm)VVVV====Using normalisation methods will reduce theinformation within the feature vectors, removingsome of the speaker variation.
Therefore, keyword detection should be more accurate for adata set of multiple speakers with normalisation.4.1 Test DataThe ACORNS English corpus is used for theabove experiments.
Sentences were created bycombining a carrier sentence with a keyword.
Atotal of 10 different carrier sentences, such as?Do you see the X?, ?Where is the X?, etc., whereX is a keyword, were combined with one of tendifferent keywords, such as ?Bottle?, ?Ball?, etc.This created 100 unique sentences which wererepeated 10 times and recorded with 4 differentspeakers (2 male and 2 female) to produce 4000utterances.In addition to the acoustic data, each utteranceis associated with an abstract semantic tag.
As anexample, the utterance ?What matches thisshoe?
will contain the tag referring to ?shoe?.The tag does not give any location or phoneticinformation about the key word within the utter-ance.E1 and E2 use a sub-set of 100 different utter-ances from a single speaker.
E3 is carried out ona sub-set of 200 utterances from a single speakerand the database used for E4 is a sub-set of 200utterances from all four speakers (2 male and 2female) presented in a random order.5 ResultsE1: LA was tested on 100 utterances with vary-ing utterance window lengths.
The plot in figure4 shows the total key word detection accuracyfor each window length used.
The x-axis displaysthe utterance window lengths (1?100) and the y-axis displays the total accuracy.The results are as expected.
Longer windowlengths achieve more accurate results.
This isbecause longer window lengths produce a largersearch space and therefore have more chance ofcapturing repeating events.
Shorter windowlengths are still able to build internal representa-tions, but over a longer period.Figure 4: Single speaker key word accuracy usingvarying utterance window lengths of 1-100.Accuracy results reach a maximum with an ut-terance window length of 21 and then stabilize ataround 58% (?1%).
From this we can conclude0 10 20 30 40 50 60 70 80 90 1000102030405060708090100Accuracy(%)Utterance Window LengthWord Detection Accuracy for varying windowlengths (1-100) over 100 utterances6that 21 is the minimum window length needed tobuild accurate internal representations of thewords within the test set, and will be used for allsubsequent experiments.E2:  The plot in figure 4 displays the total keyword detection accuracy for the different utter-ance window lengths and does not show thegradual word acquisition process.
Figure 5 com-pares the word detection accuracy of the system(y-axis) as a function of the number of utterancesobserved (x-axis).
Accuracy is recorded as thepercentage of correct replies for the last ten ob-servations.
The long discontinuous line in theplot shows the word detections accuracy for ran-domly guessing the key word.Figure 5: Word detection accuracy LA running as abatch and incremental process.
Results are plotted as afunction of the past 10 utterances observed.It can be seen from the plot in figure 5 that thesystem begins life with no word representations.At the beginning, the system hypothesises newword units from which it can begin to bootstrapits internal representations.As an incremental process, with the optimalwindow length, the system is able to captureenough repeating patterns and even begins tooutperform the batch process after 90 utterances.This is due to additional alignments discoveredby the batch process that are temporarily distort-ing a word representation, but the batch processwould ?catch up?
in time.Another important result to take into accountis that only comparing the current incoming ut-terance with the last observed utterance isenough to build word representations.
Althoughthis is very efficient, the problem is that there is agreater possibility that some words will never bediscovered if they are not present in adjacent ut-terances within the data set.E3: Currently the recognition process uses all thediscovered exemplars within each key wordclass.
This process causes the computationalcomplexity to increase exponentially.
It is alsonot suitable for an incremental process with thepotential of running on an infinite data set.To tackle this problem, recognition was car-ried out using the ?centroid?
exemplar of eachkey word class.
Figure 6 shows the word detec-tion accuracy as a function of utterances ob-served for both methods.Figure 6: Word detection accuracy using centroidsand complete exemplar list for recognition.The results show that the ?centroid?
method isquickly outperformed and that the word detectionaccuracy difference increases with experience.After 120 utterances performance seems togradually decline.
This is because the ?centroid?method cannot handle the variation in the acous-tic speech data.
Using all the discovered units forrecognition allows the system to reach an accu-racy of 90% at around 140 utterances, where itthen seems to stabilise at around 88%.E4: The addition of multiple speakers will addgreater variation to the acoustic signal, distortingpatterns of the same underlying unit.
Over the200 utterances observed, word detection accu-racy of the internal representations increases, butat a much slower rate than the single speaker ex-periments (fig.
7).The assumption that using normalisation meth-ods would achieve greater word detection accu-racy, by reducing speaker variation, does nothold true.
On reflection this comes as no sur-prise, as the system collects exemplar units witha larger relative fidelity for each speaker.This raises an important issue; the optimal ut-terance window length for the algorithm as anincremental process was calculated for a single0 10 20 30 40 50 60 70 80 90 1000102030405060708090100UttWin = 1UttWin = 21BatchRandomUtterances ObservedWord Detection AccuracyIncremental vs. Batch ProcessAccuracy(%)0 20 40 60 80 100 120 140 160 180 2000102030405060708090100All ExemplarsCentroidRandomAccuracy(%)Utterances ObservedWord Detection AccuracyCentroid vs.
Complete Exemplar List7speaker, therefore, increasing the search spacewill allow the model to find more repeating pat-terns from the same speaker.
Following thislogic, it could be hypothesised that the optimalsearch space should be four times the size usedfor one speaker and that it will take four times asmany observations to achieve the same accuracy.Figure 7: Total accuracy using different feature vec-tors after 200 observed utterances.6 ConclusionsPreliminary results indicate that the environmentis rich enough for word acquisition tasks.
Thepattern discovery and word learning algorithmimplemented within the LA memory architecturehas proven to be a successful approach for build-ing stable internal representations of word-likeunits.
The model approaches cognitive plausibil-ity by employing statistical processes that aregeneral across multiple modalities.
The incre-mental approach also shows that the model isstill able to learn correct word representationswith a very limited working memory model.Additionally to the acquisition of words andword-like units, the system is able to use the dis-covered tokens for speech recognition.
An im-portant property of this method, that differenti-ates it from conventional ASR systems, is that itdoes not rely on a pre-defined vocabulary, there-fore reducing language-dependency and out-of-dictionary errors.Another advantage of this system, comparedto systems such as NMF, is that it is able to givetemporal information of the whereabouts of im-portant repeating structure which can be used tocode the acoustic signal as a lossless compres-sion method.7 Discussion & Future WorkA key question driving this research is whethermodelling human language acquisition can helpcreate a more robust speech recognition system.Therefore further development of the proposedarchitecture will continue to be limited to cogni-tively plausible approaches and should exhibitsimilar developmental properties as early humanlanguage learners.
In its current state, the systemis fully operational and intends to be used as aplatform for further development and experi-ments.The experimental results are promising.
How-ever, it is clear to see that the model suffers fromspeaker-dependency issues.
The problem can besplit into two areas, front-end processing of theincoming acoustic signal and the representationof discovered lexical units in memory.Development is being carried out on variousclustering techniques that build constantly evolv-ing internal representations of internal lexicalclasses in an attempt to model speech variation.Additionally, a secondary update process, im-plemented as a re-occurring ?sleeping phase?
isbeing investigated.
This phase is going to allowthe memory organisation to re-structure itself bylooking at events over a longer history, whichcould be carried out as a batch process.The processing of prosodic cues, such asspeech rhythm and pitch intonation, will be in-corporated within the algorithm to increase thekey word detection accuracy and further exploitthe richness of the learners surrounding envi-ronment.
Adults, when speaking to infants, willhighlight words of importance through infantdirected speech (IDS).
During IDS adults placemore pitch variance on words that they want theinfant to attend to.Further experiments have been planned to seeif the model exhibits similar patterns of learningbehaviour as young multiple language learners.Experiments will be carried out with the multiplelanguages available in the ACORNS database(English, Finnish and Dutch).AcknowledgementThis research was funded by the EuropeanCommission, under contract number FP6-034362, in the ACORNS project (www.acorns-project.org).
The author would also like to thankProf.
Roger K. Moore for helping to shape thiswork.0102030405060708090100RandomHTK?nonormACORNS?cmnACORNS?nonormACORNS?cmvn1 2 3 4 5                           V V V V VAccuracy(%)Word Detection AccuracySpeaker-Dependency8ReferencesA.
Park and J. R. Glass.
2008.
Unsupervised PatternDiscovery in Speech.
Transactions on Audio,Speech and Language Processing, 16(1):186-197.C.
T. Best, G. W. McRoberts and N. M. Sithole.
1988.Examination of the perceptual re-organization forspeech contrasts: Zulu click discrimination by Eng-lish-speaking adults and infants.
Journal of Ex-perimental Psychology: Human Perceptionand Performance, 14:345-360.D.
M. Jones, R. W. Hughes and W. J. Macken.
2006.Perceptual Organization Masquerading as Phono-logical Storage: Further Support for a Perceptual-Gestural View of Short-Term Memory.
Journalof Memory and Language, 54:265-328.D.
Roy and A. Pentland.
2002.
Learning Words fromSights and Sounds: A Computational Model.
Cog-nitive Science, 26(1):113-146.D.
Sankoff and Kruskal J.
B.
1983.
Time Warps,String Edits, and Macromolecules: The The-ory and Practice of Sequence Comparison.Addison-Wesley Publishing Company, Inc.E.
E. Hannon and S. E. Trehub.
2005.
Turning in toMusical Rhythms: Infants Learn More readily thanAdults.
PNAS, 102(35):12639-12643.J.
L. Anderson, J. L. Morgan and K. S. White.
2003.A Statistical Basis for Speech Sound Discrimina-tion.
Language and Speech, 46(43):155-182.J.
R. Saffran, R. N. Aslin and E. L. Newport.
1996.Statistical Learning by 8-Month-Old Infants.
SCI-ENCE, 274:1926-1928.J.
R. Saffran, E. K. Johnson, R. N. Aslin and E. L.Newport.
1999.
Statistical Learning of Tone Se-quences by Human Infants and Adults.
Cognition,70(1):27-52.J.
R. Saffran, A. Senghashas and J. C. Trueswell.2000.
The Acquisition of Language by Children.PNAS, 98(23):12874-12875.L.
ten Bosch and B. Cranen.
2007.
A ComputationalModel for Unsupervised Word Discovery.
IN-TERSPEECH 2007, 1481-1484.M.
H. Christiansen, J. Allen and M. Seidenberg.
1998.Learning to Segment Speech using Multiple Cues.Language and Cognitive Processes, 13:221-268.M.
S. Seidenberg, M. C. MacDonald and J. R. Saf-fran.
2002.
Does Grammar Start Where StatisticsStop?.
SCIENCE, 298:552-554.M.
R. Brent.
1999.
Speech Segmentation and WordDiscovery: A Computational Perspective.
Trendsin Cognitive Sciences, 3(8):294-301.N.
Chomsky.
1975.
Reflections on Language.
NewYork: Pantheon Books.N.
Z. Kirkham, A. J. Slemmer and S. P. Johnson.2002.
Visual Statistical Learning in Infancy: Evi-dence for a Domain General Learning Mechanism.Cognition, 83:B35-B42.P.
D. Eimas, E. R. Siqueland, P. Jusczyk and J. Vigo-rito.
1971.
Speech Perception in Infants.
Science,171(3968):303-606.P.
K. Kuhl.
2004.
Early Language Acquisition: Crack-ing the Speech Code.
Nature, 5:831-843.P.
Nowell and R. K. Moore.
1995.
The Application ofDynamic Programming Techniques to Non-WordBased Topic Spotting.
EuroSpeech ?95, 1355-1358.P.
W. Jusczyk, A. D. Friederici, J. Wessels, V. Y.Svenkerud and A. M. Jusczyk.
1993.
Infants?
Sen-sitivity to the Sound Patterns of Native LanguageWords.
Journal of Memory & Language,32:402-420.V.
Stouten, K. Demuynck and H. Van hamme.
2008.Discovering Phone Patterns in Spoken Utterancesby Non-negative Matrix Factorisation.
IEEE Sig-nal Processing Letters, 131-134.9
