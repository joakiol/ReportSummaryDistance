IIIIIIIiIIIIIIIIIIIA Comparison of WordNet and Roget's Taxonomy forMeasuring Semantic SimilarityMichael L. Mc HaleIntelligent Information SystemsAir Force Research Laboratory525 Brooks Road13441 Rome, NY, USA,mchale@ai.rl.af.milAbstractThis paper presents the results of usingRoget's International Thesaurus as thetaxonomy in a semantic similaritymeasurement task.
Four similarity metricswere taken from the literature and applied toRoget's.
The experimental evaluationsuggests that the traditional edge countingapproach does surprisingly well (acorrelation of r=0.88 with a benchmark setof human similarity judgements, with anupper bound of r=0.90 for human subjectsperforming the same task.
)IntroductionThe study of semantic relatedness has been apart of artificial intelligence and psychology formany years.
Much of the early semanticrelatedness work in natural language processingcentered around the use of Roget's thesaurus(Yaworsky 92).
As WordNet (Miller 90) becameavailable, most of the new work used it (Agirre& Rigau 96, Resnik 95, Jiang & Conrath 97).This is understandable, as WordNet is freelyavailable, fairly large and was designed forcomputing.
Roget's remains, though, anattractive l xical resource for those with accessto it.
Its wide, shallow hierarchy is denselypopulated with nearly 200,000 words andphrases.
The relationships among the words arealso much richer than WordNet's IS-A or HAS-PART links.
The price paid for this richness is asomewhat unwieldy tool with ambiguous links.This paper presents an evaluation of Roget'sfor the task of measuring semantic similarity.This is done by using four metrics of semanticsimilarity found in the literature while usingRoget's International Thesaurus, third edition(Roget 1962) as the taxonomy.
Thus the resultscan be compared to those in the literature (thatused WordNet).
The end result is the ability tocompare the relative usefulness of Roget's andWordNet for this type of task.1 Semantic SimilarityEach metric of semantic similarity makesassumptions about he taxonomy in which itworks.
Generally, these assumptions go unstatedbut since they are important for theunderstanding of the results we obtain, we willcover them for each metric.
All the metricsassume a taxonomy with some semantic order.1.1 Distance Based SimilarityA common method of measuring semanticsimilarity is to consider the taxonomy as a tree,or lattice, in semantic space.
The distancebetween concepts within that space is then takenas a measurement of the semantic similarity.1.1.1 Edges as distanceIf all the edges (branches of the tree) are ofequal ength, then the number of interveningedges is a measure of the distance.
Themeasurement usually used (Rada et al 89) is theshortest path between concepts.
This, of course,relies on an ideal taxonomy with edges of equallength.
In taxonomies based on naturallanguages, the edges are not the same length.
InRoget's, for example, the distance (countingedges) between Intellect and Grammar is thesame as the distance between Grammar andPhrase Structure.
This does not seem intuitive.In general, the edges in this type of taxonomytend to grow shorter with depth.115IIIIIIIIIIIIIIIIIII1.1.2 Related Metn'csA number of different metrics related to distancehave used edges that have been modified tocorrect for the problem of non-uniformity.
Themodifications include the density of thesubhierarchies, the depth in the hierarchy wherethe word is found, the type of links, and theinformation content of the nodes subsuming theword.The use of density is based on theobservation that words in a more densely part ofthe hierarchy are more closely related thanwords in sparser areas (Agirre and Rigau 96).For density to be a valid metric, the hierarchymust be fairly complete or at least thedistribution of words in the hierarchy has toclosely reflect he distribution of words in thelanguage.
Neither of these conditions ever holdcompletely.
Furthermore, the observation aboutdensity may be an overgeneralization.
InRoget's, for instance, category 277 Ship/Boathas many more words (much denser) thancategory 372 Blueness.
That does not mean thatkayak is more closely related to tugboat than skyblue is to turquoise.
In fact, it does not evenmean that kayak is closer to Ship/Boat hanturquoise is to Blueness.Depth in the hierarchy is another attributeoften used.
It may be more useful in the deephierarchy of WordNet han it is in Roget's wherethe hierarchy is fairly flat and uniform.
All thewords in Roget's are at either level 6 or 7 in thehierarchy.The type of link in WordNet is explicit, inRoget's it is never clear but it consists of morethan IS-A and HAS-PART.
One such link isHAS-ATTRIBUTE.Some of the researchers that have used theabove metrics include Sussna (Sussna 93) whoweighted the edges by using the density of thesubhierarchy, the depth in the hierarchy and thetype of link.
Richardson and Smeaton(Richardson and Smeaton 95) used density,hierarchy depth and the information content ofthe concepts.
Jiang and Conrath (Jiang andConrath 95) used the number of edges andinformation content.
They all reportedimprovement in results compared to straightedge counting.116McHale (95) decomposed Roget's taxonomyand used five different metrics to show theusefulness of the various attributes of thetaxonomy.
Two of those metrics deal withdistance but only one is of interest to us for thistask; the number of intervening words.
Thenumber of intervening words ignores thehierarchy completely, treating it as a flat file.For the measurement to be an accurate metric,two conditions must be met.
Fi'i'st, the orderingof the words must be correct.
Second, either allthe words of the language must be represented(virtually impossible) or they must be evenlydistributed throughout the hierarchy I.
Since it isunlikely that either of these conditions hold forany taxonomy, the most that can be expected ofthis measurement is hat it might provide areasonable approximation of the distance(similar to density).
It is included here, notbecause the approximation is reasonable, butbecause it provides information that helpsexplain the other results.1.2 Information Based SimilarityGiven the above problems with distance relatedmeasures, Resnik (Resnik 95) decided to use justthe information content of the concepts andcompared the results to edge Counting andhuman replication of the same task.
Resnikdefines the similarity of two concepts as themaximum of the Information Content of theconcepts that subsume them in the taxonomy.The Information Content of a concept relies onthe probability of encountering an instance ofthe concept.
To compute this probability, Resnikused the relative frequency of occurrence ofeach word in the Brown Corpus 2.
Theprobabilities thus found should fairly wellapproximate he true values for othergeneralized texts.
The concept probabilities werethen computed from the occurrences as simplythe relative frequency of the concept.I This condition certainly does not hold true inWordNet where animals and plants represent adisproportionately large section of the hierarchy.2 Resnik used the semantic concordance (semcor) thatcomes with WordNet.
Semcor is derived from ahand-tagged subset of the Brown Corpus.
Hiscalculations were done using WordNet 1.5.IiIIIIIIIIIIIIFreq(c)(e) =NThe information content of each concept is thengiven by IC(c) = log .i ~(c), where ~(c) is theprobability.
Thus, more common words havelower information content.To replicate the metric using Roget's, thefrequency of occurrence of the words found inthe Brown Corpus was divided by the totalnumber of occurrences of the word in Roget's 3.From the information content of each concept,the information content for each node in theRoget hierarchy was computed.
These aresimply the minimum of the information contentof all the words beneath the node in thetaxonomy.
Therefore, the information content ofa parent node is never greater than any of itschildren.The metric of relatedness for two wordsaccording to Resnik is the information content ofthe lowest common ancestor for any of the wordsenses.
What this implies is that, for the purposeof measuring relatedness, each synset inWordNet or each semicolon group in Roget'swould have an information content equal to itsmost common member.
For example, the wordsdruid (Roget's Index number 1036.15) and pope(1036.8) would have an information contentequal to that of clergy (1036).
Clergy'sinformation content is based on the two mostcommon words below it in the hierarchy -brother and sister.
Thus druid would have aninformation content less than that of brother, asituation that I do not find intuitive since druidappears much less frequently than brother.Computationally, the easiest way to computethe information content of a word is tocompletely compute the values for the entirehierarchy a priori.
This involves approximately300,000 (200,000 words plus 100,000 nodes in3 The frequencies were computed for Roget's as thetotal frequency for each word divided by the numberof senses in Roget.
This gives us an approximation fthe information content for each concept.
Thefrequency data were taken from the MRCPsycholinguistic database available from the OxfordText Archive.117the hierarchy) computations for the entire Rogethierarchy.
This is sizeable overhead compared toedge counting which requires no a prioricomputations.
Of course, once the computationsare done they do not need to be recomputed untila new word is added to the hierarchy.
Since thevalues for information content bubble up fromthe words, each addition of a word wouldrequire that all the hierarchy above it berecomputed.Jiang and Conrath (Jiang and Conrath 97)also used information content to measuresemantic relatedness but they combined it withedge counting using a formula that also took intoconsideration local density, node depth and linktype.
They optimized the formula by using twoparameters, ct and ~, that controlled the degreeof how much the node depth and density factorscontributed tothe edge weighting computation.If t~----0 and 13=1, then their formula for thedistance between two concepts cl and c2simplifies toDist(cl,c2) = IC(c0 + IC(c2) - 2 X \[C(LS(cbc2))Where LS(cbc2) denotes the lowest super-ordinate ofcl and c2.2 EvaluationThe above metrics are used to rate the similarityof a set of word pairs.
The results are evaluatedby comparing them to a rating produced byhuman subjects.
Miller and Charles (199 l) gavea group of students thirty word pairs and askedthe students to rate them for "similarity inmeaning" on a scale from 0 (no similarity) to 4(perfect synonymy).
Resnik (1995) replicatedthe task with a different set of students andfound a correlation between the two ratings ofr=.9011 for the 28 word pairs tested.
Resnik,Jiang and Conrath (1997) and I all consider thisvalue to be a reasonable upper-bound towhatone should expect from a computational methodperforming the same task.Resnik also performed an evaluation of twocomputational methods both using WordNet 1.5.He evaluated simple edge counting (r=-.6645)and information content (r=.791 l).
Jiang andConrath improved on that some (r=-.8282) usinga version of their combined formula given aboveIIIIIiIIIIIIIIIIIthat had been empirically optimized forWordNet.Table I gives the results from Resnik (thefirst four columns) along with the ratings ofsemantic similarity for each word pair usinginformation content, he number of edges, thenumber of intervening words and Jiang andConrath's implified formula (e,--0, 13=1) withrespect to Roget's.
Both the number of edgesand the number of intervening words are givenin their raw form.
The correlation value for theedges was computed using (12 - Edges) where12 is the maximum number of edges.
Thecorrelation for intervening words was computedusing (199,427 - words).3 Synopsis of ResultsSimilarity MethodWordNetHuman judgements (replication)Information ContentEdge CountingJiang & ConrathRoget'sInformation ContentEdge CountingIntervening WordsJiang & ConrathCorrelationr=-.9015r=.7911r=-.6645r=-.8282r=.7900r=-.8862r=-.5734r=.79114 DiscussionInformation Content is very consistent betweenthe two hierarchies.
Resnik's correlation forWordNet was 0.7911 while the one conductedhere for Roget's was 0.7900.
This is remarkablein that he IC values for Roget's used theaverage number of occurrences for all the sensesof the words whereas for WordNet he numberof occurrences of the actual sense of the wordwas used.
This may be explainable by realizingthat in either case the numbers are justapproximations ofwhat he real values would befor any particular text.Jiang & Conrath's metric did just a littleworse using Roget's than the results they gaveusing WordNet but that may very well bebecause I was unable to optimize the values of ctand \[3 for Roget's.The harder esult to explain seems to beedge counting.
It does much better in the118shallow, uniform hierarchy of Roget's than itdoes in WordNet.
Why this is the case requiresfurther investigation.
Factors to consider includethe uniformity of edges, the maximum numberof edges in each hierarchy and the generalorganization of the two hierarchies.
I expect hatmajor factors are the fairly uniform nature ofRoget's hierarchy and the broader set ofsemantic relations allowed in Roget's.
Currently,it seems that Roget's captures the popularsimilarity of isolated word pairs better thanWordNet does.5 Related WorkAgirre and Rigau (Agirre and Rigau 1996) use aconceptual distance formula that was created toby sensitive to the length of the shortest path thatconnects the concepts involved, the depth of thehierarchy and the density of concepts in thehierarchy.
Their work was designed formeasuring words in context and is not directlyapplicable to the isolated word pairmeasurements done here.
Agirre and Rigau feelthat concepts in a dense part of the hierarchy arerelatively closer than those in a more sparseregion; a point which was covered above.
Tomeasure the distance, they use a conceptualdensity formula.
The Conceptual Density of aconcept, as they define it, is the ratio of areas;the area expected beneath the concept divided bythe area actually beneath it.Some of the results given in Table 1 seem tosupport he use of density.
The word pairsforest-graveyard and chord-smile both have anedge distance of 8.
The number of interveningwords for each pair are considerably different(296 and 3253 respectively).
For these particularword pairs the latter numbers more closelymatch the ranking iven by humans.
If oneconsiders density important then perhaps we canuse a different measure of density by computingthe number of intervening words per edge 4.
Thismetric was tested with the 28 word pairs and theresults were a slight improvement (r=.6472)over the number of intervening words but arestill well below that attained by simple edgecounting.4 Words/Edge is a metric of density analogous toPeople/Square Mile.IIII,,""" WordNeto ~?r o a "!
.n "Roger 'sIIiIIIIIIIcar-automobilegem-jeweljourney-voyageboy-ladcoast-shoreasylum-madhousemagician-wizardmidday-noonfurnace-stovefood-fruitbird-cockbird-cranetool-implementbrother-monkcrane-implementlad-brotherjourney-carmonk-oracle3.923.843.843.763.703.613.503.423.113.083.052.972.952.821.681.661.163.903.503.503.503.503.603.503.602.602.102.202.103.402.400.301.200.708.0414,936.758.4210.8116.6713.6712.391.715.019.319.316.082.972.972.940 .00i.i0 0.80 2.97food-rooster 0.89 i.i0 1.01coast-hill 0.87 0.70 6.23forest-graveyardmonk-slave0.600.700.840.550.002.97coast-forest 0.42 0.60 0.00lad-wizard 0.42 0.70 2.97chord-smile 0.13 0.10 2.35glass-magician 0.ii 0.10 1.01noon-strinu 0.00 0.00 rlngrooster-voyage0.080.08 0.00 0.00OM N 0~H O{411"o ?
~?
N g?
I~11, tr30 10.77 0 5 10.6830 13.23 0 1 12.4729 8.90 2 14 8.8929 12.91 0 1 12.3029 11.61 0 1 11.4029 11.16 0 2 11.0430 4.75 4 17 4.7530 15.77 0 2 13.1223 13.53 0 1 12.6627 0.02 4 369 0.0229 1.47 4 47 1.4727 1.47 4 919 1.4729 13.35 0 1 12.5424 9.89 2 2 9.8524 2.53 4 336 2.5326 0.00 10 15418 0.000 0.84 6 478 0.8424 0.00 12 12052 0.0018 0.00 12 25339 0.0026 0.00 10 14024 1.910 0.30 8 296 0.3027 0.00 12 29319 0.000 0.00 i0 4801 1.9126 0.00 12 64057 0.0020 0.00 8 3253 0.0022 0.00 12 82965 0.000 1.58 6 779 1.580 0.00 12 34780 0.00IIIITable 1.
Metric ResultsIConclusionThis paper presented the results of usingRoget's International Thesaurus as the taxonomyin a semantic similarity measurement task.
Foursimilarity metrics were taken from the literatureand applied to Roget's.
The experimentalevaluation suggests hat the traditional edgecounting approach does surprisingly well (acorrelation of r=0.8862 with a benchmark set ofhuman similarity judgements, with an upper119bound of r=0.9015 for human subjectsperforming the same task.
)The results hould provide incentive to thosewishing to understand the effect of variousattributes on metrics for semantic relatednessacross hierarchies.
Further investigation of whythis dramatic improvement in edge countingoccurs in the shallow, uniform hierarchy ofRoget's needs to be conducted.
The resultsshould prove beneficial to those doing researchwith Roget's, WordNet and other semanticbased hierarchies.AcknowledgementsThis research was sponsored in part by AFOSRunder RL-2300C601.ReferencesAgirre, E. and G. Rigau (1996) Word SenseDisambiguation Using Conceptual Density.In Proceedings ofthe 16" InternationalConference on Computational Linguistics(Coling '96), Copenhagen, Denmark, 1996.Jiang, JJ.
and D.W. Conrath (1997) "SemanticSimilarity Based on Corpus Statistics and?
Lexical Taxonomy", in Proceedings ofROCLING X (1997) InternationalConference on Research in ComputationalLinguistics, Taiwan, 1997.?
Me Hale, M. L. (1995) Combining Machine-Readable Lexical Resources with aPrinciple-Based Parser, Ph.D. Dissertation,Syracuse University, NY.
Available fromUMI.Miller, G. and W.G.
Charles (1991) "ContextualCorrelates of Semantic Similarity",Language and Cognitive Processes, Vol.
6,No.
1, 1-28.i2oMiller, G. (1990) "Five papers on WordNet".Special Issue of International Journal ofLexicography 3(4).Rada, R., H. Mili, E. Bicknell, and M. Bletner(1989) "Development and Application of aMetric on Semantic Nets".
IEEETransactions on Systems, Man andCybernetics, Vol.
19, No.
1, 17-30.Resnik, P. (1995) "Using Information Content oEvaluate Semantic Similarity in aTaxonomy", Proceedings of the 14 ~International Joint Conference on ArtificialIntelligence, Vol.
1,448-453, Montreal,August 1995.Richardson, R. and A.F.
Smeaton (1995) UsingWordNet in a Knowledge-Based Approachto Information Retrieval.
Working PaperCA-0395, School of Computer Applications,Dublin City University, Ireland.Roget (1962) Roget's International Thesaurus,Third Edition.
Berrey, L.V.
and G.
Carruth(eds.
), Thomas Y. Crowell Co.: New York.Yaworsky, D. (1992) Word-SenseDisambiguation Using Statistical Models ofRoger's Categories Trained on LargeCorpora.
Proceedings of the 15"`International Conference on ComputationalLinguistics (Coling '92).
Nantes, France.
