Proceedings of NAACL HLT 2009: Short Papers, pages 33?36,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsText Categorization from Category Name via Lexical ReferenceLibby BarakDepartment of Computer ScienceUniversity of TorontoToronto, Canada M5S 1A4libbyb@cs.toronto.eduIdo Dagan and Eyal ShnarchDepartment of Computer ScienceBar-Ilan UniversityRamat-Gan 52900, Israel{dagan, shey}@cs.biu.ac.ilAbstractRequiring only category names as user inputis a highly attractive, yet hardly explored, set-ting for text categorization.
Earlier bootstrap-ping results relied on similarity in LSA space,which captures rather coarse contextual sim-ilarity.
We suggest improving this scheme byidentifying concrete references to the categoryname?s meaning, obtaining a special variant oflexical expansion.1 IntroductionTopical Text Categorization (TC), the task of clas-sifying documents by pre-defined topics, is mostcommonly addressed as a supervised learning task.However, the supervised setting requires a substan-tial amount of manually labeled documents, whichis often impractical in real-life settings.Keyword-based TC methods (see Section 2) aimat a more practical setting.
Each category is rep-resented by a list of characteristic keywords, whichshould capture the category meaning.
Classifica-tion is then based on measuring similarity betweenthe category keywords and the classified documents,typically followed by a bootstrapping step.
Themanual effort is thus reduced to providing a key-word list per category, which was partly automatedin some works through clustering.The keyword-based approach still requires non-negligible manual work in creating a representativekeyword list per category.
(Gliozzo et al, 2005)succeeded eliminating this requirement by using thecategory name alone as the initial keyword, yet ob-taining superior performance within the keyword-based approach.
This was achieved by measur-ing similarity between category names and docu-ments in Latent Semantic space (LSA), which im-plicitly captures contextual similarities for the cate-gory name through unsupervised dimensionality re-duction.
Requiring only category names as user in-put seems very attractive, particularly when labeledtraining data is too costly while modest performance(relative to supervised methods) is still useful.The goal of our research is to further improve thescheme of text categorization from category name,which was hardly explored in prior work.
When an-alyzing the behavior of the LSA representation of(Gliozzo et al, 2005) we noticed that it capturestwo types of similarities between the category nameand document terms.
One type regards words whichrefer specifically to the category name?s meaning,such as pitcher for the category Baseball.
How-ever, typical context words for the category which donot necessarily imply its specific meaning, like sta-dium, also come up as similar to baseball in LSAspace.
This limits the method?s precision, due tofalse-positive classifications of contextually-relateddocuments that do not discuss the specific categorytopic (such as other sports documents wrongly clas-sified to Baseball).
This behavior is quite typicalfor query expansion methods, which expand a querywith contextually correlated terms.We propose a novel scheme that models sepa-rately these two types of similarity.
For one, itidentifies words that are likely to refer specificallyto the category name?s meaning (Glickman et al,2006), based on certain relations in WordNet and33Wikipedia.
In tandem, we assess the general contex-tual fit of the category topic using an LSA model,to overcome lexical ambiguity and passing refer-ences.
The evaluations show that tracing lexicalreferences indeed increases classification precision,which in turn improves the eventual classifier ob-tained through bootstrapping.2 Background: Keyword-based TextCategorizationThe majority of keyword-based TC methods fit thegeneral bootstrapping scheme outlined in Figure 1,which is cast in terms of a vector-space model.
Thesimplest version for step 1 is manual generation ofthe keyword lists (McCallum and Nigam, 1999).
(Ko and Seo, 2004; Liu et al, 2004) partly auto-mated this step, using clustering to generate candi-date keywords.
These methods employed a standardterm-space representation in step 2.As described in Section 1, the keyword list in(Gliozzo et al, 2005) consisted of the category namealone.
This was accompanied by representing thecategory names and documents (step 2) in LSAspace, obtained through cooccurrence-based dimen-sionality reduction.
In this space, words that tendto cooccur together, or occur in similar contexts, arerepresented by similar vectors.
Thus, vector similar-ity in LSA space (in step 3) captures implicitly thesimilarity between the category name and contextu-ally related words within the classified documents.Step 3 yields an initial similarity-based classifi-cation that assigns a single (most similar) categoryto each document, with Sim(c, d) typically beingthe cosine between the corresponding vectors.
Thisclassification is used, in the subsequent bootstrap-ping step, to train a standard supervised classifier(either single- or multi-class), yielding the eventualclassifier for the category set.3 Integrating Reference and ContextOur goal is to augment the coarse contextual simi-larity measurement in earlier work with the identifi-cation of concrete references to the category name?smeaning.
We were mostly inspired by (Glickman etal., 2006), which coined the term lexical referenceto denote concrete references in text to the specificmeaning of a given term.
They further showed thatInput: set of categories and unlabeled documentsOutput: a classifier1.
Acquire a keyword list per category2.
Represent each category c and document das vectors in a common space3.
For each document dCatSim(d) = argmaxc(Sim(c, d))4.
Train a supervised classifier on step (3) outputFigure 1: Keyword-based categorization schemeCategory name WordNet WikipediaCryptography decipher digital signatureMedicine cardiology biofeedback, homeopathyMacintosh Apple Mac, MacMotorcycle bike, cycle Honda XR600Table 1: Referring terms from WordNet and Wikipediaan entailing text (in the textual entailment setting)typically includes a concrete reference to each termin the entailed statement.
Analogously, we assumethat a relevant document for a category typically in-cludes concrete terms that refer specifically to thecategory name?s meaning.We thus extend the scheme in Figure 1 by cre-ating two vectors per category (in steps 1 and 2): areference vector ~cref in term space, consisting of re-ferring terms for the category name; and a contextvector ~ccon, representing the category name in LSAspace, as in (Gliozzo et al, 2005).
Step 3 then com-putes a combined similarity score for categories anddocuments based on the two vectors.3.1 References to category namesReferring terms are collected from WordNet andWikipedia, by utilizing relations that are likely tocorrespond to lexical reference.
Table 1 illustratesthat WordNet provides mostly referring terms ofgeneral terminology while Wikipedia provides morespecific terms.
While these resources were used pre-viously for text categorization, it was mostly for en-hancing document representation in supervised set-tings, e.g.
(Rodr?
?guez et al, 2000).WordNet.
Referring terms were found in Word-Net starting from relevant senses of the categoryname and transitively following relation types thatcorrespond to lexical reference.
To that end, we34specified for each category name those senses whichfit the category?s meaning, such as the outer spacesense for the category Space.1A category name sense is first expanded by itssynonyms and derivations, all of which are then ex-panded by their hyponyms.
When a term has nohyponyms it is expanded by its meronyms instead,since we observed that in such cases they often spec-ify unique components that imply the holonym?smeaning, such as Egypt for Middle East.
However,when a term is not a leaf in the hyponymy hierarchythen its meronyms often refer to generic sub-parts,such as door for car.
Finally, the hyponyms andmeronyms are expanded by their derivations.
Asa common heuristic, we considered only the mostfrequent senses (top 4) of referring terms, avoidinglow-ranked (rare) senses which are likely to intro-duce noise.Wikipedia.
We utilized a subset of a lexical ref-erence resource extracted from Wikipedia (anony-mous reference).
For each category name we ex-tracted referring terms of two types, capturing hy-ponyms and synonyms.
Terms of the first type areWikipedia page titles for which the first definitionsentence includes a syntactic ?is-a?
pattern whosecomplement is the category name, such as Chevroletfor the category Autos.
Terms of the second typeare extracted from Wikipedia?s redirect links, whichcapture synonyms such as x11 for Windows-X.The reference vector ~cref for a category consistsof the category name and all its referring terms,equally weighted.
The corresponding similarityfunction is Simref (c, d) = cos(~cref , ~dterm)), where~dterm is the document vector in term space.3.2 Incorporating context similarityOur key motivation is to utilize Simref as the ba-sis for classification in step 3 (Figure 1).
However,this may yield false positive classifications in twocases: (a) inappropriate sense of an ambiguous re-ferring term, e.g., the narcotic sense of drug shouldnot yield classification to Medicine; (b) a passingreference, e.g., an analogy to cars in a software doc-ument, should not yield classification to Autos.1We assume that it is reasonable to specify relevant sensesas part of the typically manual process of defining the set ofcategories and their names.
Otherwise, when expanding namesthrough all their senses F1-score dropped by about 2%.In both these cases the overall context in the docu-ment is expected to be atypical for the triggered cat-egory.
We therefore measure the contextual similar-ity between a category c and a document d utilizingLSA space, replicating the method in (Gliozzo etal., 2005): ~ccon and ~dLSA are taken as the LSA vec-tors of the category name and the document, respec-tively, yielding Simcon(c, d) = cos(~ccon, ~dLSA)).2The overall similarity score of step 3 is de-fined as Sim(c, d) = Simref (c, d) ?
Simcon(c, d).This formula fulfils the requirement of finding atleast one referring term in the document; otherwiseSimref (c, d) would be zero.
Simcon(c, d) is com-puted in the reduced LSA space and is thus prac-tically non-zero, and would downgrade Sim(c, d)when there is low contextual similarity between thecategory name and the document.
Documents forwhich Sim(c, d) = 0 for all categories are omitted.4 Results and ConclusionsWe tested our method on the two corpora used in(Gliozzo et al, 2005): 20-NewsGroups, classifiedby a single-class scheme (single category per doc-ument), and Reuters-10 3, of a multi-class scheme.As in their work, non-standard category names wereadjusted, such as Foreign exchange for Money-fx.4.1 Initial classificationTable 2 presents the results of the initial classifica-tion (step 3).
The first 4 lines refer to classificationbased on Simref alone.
As a baseline, includingonly the category name in the reference vector (Cat-Name) yields particularly low recall.
Expansion byWordNet is notably more powerful than by the auto-matically extracted Wikipedia resource; still, the lat-ter consistently provides a small marginal improve-ment when using both resources (Reference), indi-cating their complementary nature.As we hypothesized, the Reference modelachieves much better precision than the Contextmodel from (Gliozzo et al, 2005) alone (Simcon).For 20-NewsGroups the recall of Reference is lim-ited, due to partial coverage of our current expansion2The original method includes a Gaussian Mixture re-scaling step for Simcon, which wasn?t found helpful whencombined with Simref (as specified next).310 most frequent categories in Reuters-2157835Reuters-10 20 NewsgroupsMethod R P F1 R P F1CatName 0.22 0.67 0.33 0.19 0.55 0.28WordNet 0.67 0.78 0.72 0.29 0.56 0.38Wikipedia 0.24 0.68 0.35 0.22 0.57 0.31Reference 0.69 0.80 0.74 0.31 0.57 0.40Context 0.59 0.64 0.61 0.46 0.46 0.46Combined 0.71 0.82 0.76 0.32 0.58 0.41Table 2: Initial categorization results (step 3)Method Feature Reuters-10 20 NGSet R P F1 F1Reference TF-IDF 0.91 0.50 0.65 0.51LSA 0.89 0.67 0.76 0.56Context TF-IDF 0.84 0.48 0.61 0.48LSA 0.73 0.56 0.63 0.44Combined TF-IDF 0.92 0.50 0.65 0.52LSA 0.89 0.71 0.79 0.56Table 3: Final bootstrapping results (step 4)resources, yielding a lower F1.
Yet, its higher pre-cision pays off for the bootstrapping step (Section4.2).
Finally, when the two models are Combined asmall precision improvement is observed.4.2 Final bootstrapping resultsThe output of step 3 was fed as standard trainingfor a binary SVM classifier for each category (step4).
We used the default setting for SVM-light, apartfrom the j parameter which was set to the number ofcategories in each data set, as suggested by (Moriket al, 1999).
For Reuters-10, classification wasdetermined independently by the classifier of eachcategory, allowing multiple classes per document.For 20-NewsGroups, the category which yielded thehighest classification score was chosen (one-versus-all), fitting the single-class setting.
We experimentedwith two document representations for the super-vised step: either as vectors in tf-idf weighted termspace or as vectors in LSA space.Table 3 shows the final classification results.4First, we observe that for the noisy bootstrappingtraining data LSA document representation is usu-ally preferred.
Most importantly, our Reference andCombined models clearly improve over the earlier4Notice that P=R=F1 when all documents are classified toa single class, as in step 4 for 20-NewsGroups, while in step 3some documents are not classified, yielding distinct P/R/F1.Context.
Combining reference and context yieldssome improvement for Reuters-10, but not for 20-NewsGroups.
We noticed though that the actual ac-curacy of our method on 20-NewsGroups is notablyhigher than measured relative to the gold standard,due to its single-class scheme: in many cases, a doc-ument should truly belong to more than one cate-gory while that chosen by our algorithm was countedas false positive.
Future research is proposed to in-crease the method?s recall via broader coverage lexi-cal reference resources, and to improve its precisionthrough better context models than LSA, which wasfound rather noisy for quite a few categories.To conclude, the results support our main contri-bution ?
the benefit of identifying referring terms forthe category name over using noisier context mod-els alone.
Overall, our work highlights the potentialof text categorization from category names when la-beled training sets are not available, and indicatesimportant directions for further research.AcknowledgmentsThe authors would like to thank Carlo Strapparavaand Alfio Gliozzo for valuable discussions.
Thiswork was partially supported by the NEGEV project(www.negev-initiative.org).ReferencesO.
Glickman, E. Shnarch, and I. Dagan.
2006.
Lexicalreference: a semantic matching subtask.
In EMNLP.A.
Gliozzo, C. Strapparava, and I. Dagan.
2005.
Inves-tigating unsupervised learning for text categorizationbootstrapping.
In Proc.
of HLT/EMNLP.Y.
Ko and J. Seo.
2004.
Learning with unlabeled datafor text categorization using bootstrapping and featureprojection techniques.
In Proc.
of ACL.B.
Liu, X. Li, W. S. Lee, and P. S. Yu.
2004.
Text classi-fication by labeling words.
In Proc.
of AAAI.A.
McCallum and K. Nigam.
1999.
Text classificationby bootstrapping with keywords, EM and shrinkage.In ACL Workshop for Unsupervised Learning in NLP.K.
Morik, P. Brockhausen, and T. Joachims.
1999.
Com-bining statistical learning with a knowledge-based ap-proach - a case study in intensive care monitoring.
InProc.
of the 16th Int?l Conf.
on Machine Learning.M.
d. B.
Rodr?
?guez, J. M. Go?mez-Hidalgo, and B.
D?
?az-Agudo, 2000.
Using WordNet to complement traininginformation in text categorization, volume 189 of Cur-rent Issues in Linguistic Theory, pages 353?364.36
