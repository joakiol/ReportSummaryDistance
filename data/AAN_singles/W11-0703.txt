Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 12?19,Portland, Oregon, 23 June 2011. c?2011 Association for Computational LinguisticsWhat pushes their buttons?
Predicting comment polarity from the contentof political blog postsRamnath BalasubramanyanLanguage Technologies InstituteCarnegie Mellon Universityrbalasub@cs.cmu.eduWilliam W. CohenMachine Learning DepartmentCarnegie Mellon Universitywcohen@cs.cmu.eduDoug Pierce and David P. RedlawskPolitical Science DepartmentRutgers Universitydrpierce@eden.rutgers.edu, redlawsk@rutgers.eduAbstractPolitical blogs as a form of social media al-low for an uniquely interactive form of politi-cal discourse.
This is especially evident in fo-cused blogs with a strong ideological identity.We investigate techniques to identify topicswithin the context of the community, whichwhen discussed in a blog post evoke a dis-cernible positive or negative collective opin-ion from readers who respond to posts in com-ments.
This is done by using computationalmethods to assign sentiment polarity to blogcomments and learning community specificmodels that summarize issues tackled by blogsand predict the polarity based on the topicsdiscussed in a blog post.1 IntroductionRecent work in political psychology has made itclear that political decision-making is strongly influ-enced by emotion.
For instance, (Lodge and Taber,2000) propose a theory of ?motivated reasoning?, inwhich political information is processed in a waythat is determined, in part, by a quickly-computedemotional react to that information.
Strong exper-imental evidence for motivated reasoning (some-times called ?hot cognition?)
exists (Huang andPrice, 2001); (Redlawsk, 2002); (Redlawsk, 2006);(Isbell et al, 2006).
However, despite some recentproposals (Kim et al, 2008) it is unclear how tocomputationally model a person?s emotional reac-tion to news, and how to collect the data necessaryto fit such a model.
One problem is that emotionalreactions are different for different people - a fact ex-ploited in the use of political ?code words?
intendedto invoke a reaction in only a particular subset of theelectorate (a technique sometimes called ?dog whis-tle politics?
).In this paper, we evaluate the use of machinelearning methods to predict how members of a spe-cific political community will emotionally reactionto different types of news.
More specifically, we usea dataset of widely read (?A-list?)
political blogs,and attempt to predict the aggregate sentiment in thecomment section of blogs, as a function of the tex-tual content of the blog posting.
In this paper, weconsider only predicting polarity (positive and neg-ative feeling).
In contrast to work done traditionallyin sentiment analysis which focuses on determiningthe sentiment expressed in text, in this work, we fo-cus on the task of predicting the sentiment that ablock of text will evoke in readers, expressed in thecomment section, as a response to the blog post.This task is related to, but distinct from, severalother studies that have been made using commentsand discussions in political communities, or analy-sis of sentiment in comments - (Yano et al, 2009),(O?Connor et al, 2010), (Tumasjan et al, 2010).Below we discuss the methods used to address thevarious parts of this task.
First, we evaluate twomethods to automatically determine the commentpolarity: SentiWordNet (Baccianella and Sebastiani,2010) a general purpose resource that assigns sen-timent scores to entries in WordNet, and an auto-12mated corpus-specific technique based on pointwisemutual information.
The quality of the polarity as-sessments by these techniques are made by compar-ing them to hand annotated assessments on a smallnumber of blog posts.
Second, we consider twomethods for predicting comment polarity from postcontent: support vector machine classification, andsLDA, a topic-modeling-based approach.
Finally,we demonstrate that emotional reactions are indeedcommunity-specific, compare the accuracy of thisapproach to the more traditional approach of pre-dicting sentiment of a text from the text itself, andpresent our conclusions.2 DataIn this study, we use a collection of blog posts fromfive blogs: Carpetbagger(CB)1, Daily Kos(DK)2,Matthew Yglesias(MY)3, Red State(RS)4, and RightWing News(RWN)5, that focus on American politicsmade available by (Yano et al, 2009).
The postswere collected during November 2007 to October2008, which preceded the US presidential electionsheld in November 2008.
The blogs included in thedataset vary in political idealogy with blogs likeDaily Kos that are Democrat-leaning and blogs likeRed State tending to be much more conservative.Since we are interested in studying the responsesto blog posts, the corpus only contains posts wherethere have been at least one comment in the six daysafter the post was published.
It is important to notethat only the text in the blog posts and comments areused in this study.
All non-textual information likepictures, hyperlinks, videos etc.
are discarded.
Interms of text processing, for each blog, a vocabularyis created consisting of all terms that occur at least5 times in the blog.
Stopwords are eliminated us-ing a standard stopword list.
Each blog post is thenrepresented as a bag of words from the post.
Table2 shows statistics of the datasets.
Each dataset isstudied separately for the most part in the rest of thepaper.1http://www.thecarpetbaggerreport.com2http://www.dailykos.com/3http://yglesias.thinkprogress.org/4http://www.redstate.com/5http://rightwingnews.com/3 Labelling comments with sentimentpolarityThe first step in understanding the nature of poststhat evoke emotional responses is to get a measure ofthe polarity in the sentiment expressed in the com-ments section of a blog post.
The measure indicatesthe ability of the issues in the blog post and its treat-ment, to evoke strong emotions in readers.3.1 SentiWordNetIn the first stage of the study, we use SentiWord-Net (Baccianella and Sebastiani, 2010) which as-sociates a large number of words in WordNet witha positive, negative and objective score (summingup to 1).
Firstly, all the comments for a blog postin the comment section are aggregated and for thewords in the comments that are found in SentiWord-Net, the net positive and negative scores are com-puted.
Since SentiWordNet entries are associatedwith word senses and because we don?t performword sense disambiguation, the SentiWordNet po-larity of the most dominant word sense is used forwords in the comment section.
The sentiment in thecomment section is deemed to be positive if the netpositive score exceeds the negative score and nega-tive otherwise.
Therefore, each blog post is now as-sociated with a binary response variable indicatingthe polarity of the sentiment expressed in the com-ments.3.2 Using pointwise mutual informationA second technique to determine the sentiment po-larity of comments uses the principle of pointwisemutual information (PMI)(Turney, 2002).
We firstconstruct a seed list of positive and negative wordsby choosing the 100 topmost positive and negativewords from SentiWordNet and manually eliminat-ing words from this list that don?t pertain to senti-ment in our context.
(Appendix A has the list ofseed words used.)
This seed list is used to constructa larger set of positive and negative words by com-puting the PMI of the words in the seed lists withevery other word in the vocabulary.
It?s importantto note that this list is constructed for the specificcorpus that we work with.
Because every blog isprocessed separately, we construct a different senti-ment word list for each blog based on the statistics13Blog Pol align-ment#posts Vocabulary size Avg#wordsper postAvg #com-ments perpostAvg#words percommentsectionCarpetbagger(CB)liberal 1201 4998 170 31 1306Daily Kos (DK) liberal 2597 6400 103 198 3883Matthew Ygle-sias (MY)liberal 1813 4010 69 35 1420Red State (RS) conservative 2357 8029 158 28 806Right Wing Na-tion (RWN)conservative 1184 6205 185 33 1015Table 1: Dataset statisticsof word occurences.
Words in the vocabulary areranked by the difference in the average of the PMIwith positive and negative seed words.
The top 1000words in the resultant sorted list are treated as pos-itive words and the bottom 1000 words as negativewords.
The comment section of every post is taggedwith a positive or negative polarity as in the previoussection by computing the total positive and negativeword counts.Using the same seed word list, the procedure isperformed separately for each blog resulting in sen-timent polarity lists that are particular to the com-munity and idealogy associated with each blog.
Itshould be noted that while this method provides bet-ter estimates of comment sentiment polarity (as seenin Section 4), it involves more manual work in con-structing a seed set than the SentiWordNet methodwhich does not require any manual effort.3.3 Human labelsAs a third method that is accurate but expensive, wemanually labeled comments from approximately 30blog posts from each blog, with a positive or neg-ative label.
The guideline in labeling was to deter-mine if the sentiment in the comment section waspositive or negative to the subject of the post.
Thechief intention of this exercise is to determine thequality of the polarity assessments of the SentiWord-Net and PMI methods.
While it is possible to di-rectly use the assessments and train a classifier, theperformance of the classifier will be limited by thevery small number of training examples (30 insteadof thousands of examples).
The accuracy of the twoBlog SentiWordNet accuracy PMI accuracyCB 0.56 0.78DK 0.54 0.72MY 0.61 0.83RS 0.54 0.74RWN 0.64 0.84Table 2: Measuring accuracy of automatic comment po-larity detectionautomatic methods to determine comment polarityis shown in Table 2The better accuracy of the PMI method can be ex-plained by the fact that SentiWordNet is a generalpurpose list that is not customized for the domainwhich tends to make it noisy for text in the politi-cal domain.
The PMI technique corresponds moreclosely with the human labels but it requires a littlehuman effort in building the initial seed list of posi-tive and negative words.4 Predicting sentiment from blog contentWe now address the problem of using machinelearning techniques to predict the polarity of thecomments based on the blog post contents.4.1 SVMFirstly, we use support vector machines (SVM) toperform classification.
We frame the classificationtask as follows: The input features to the classifierare the words in the blog post i.e each blog post istreated as a bag of words and the output variable isthe binary comment polarity computed in the previ-14SentiWordNet PMIBlog SVM sLDA SVM sLDAcb 0.56 0.58 0.79 0.79dk 0.61 0.64 0.75 0.77my 0.67 0.59 0.87 0.87rs 0.53 0.55 0.74 0.76rwn 0.57 0.59 0.90 0.90Table 3: Accuracy: Using blog posts to predict commentsentiment polarityous section.
For our experiments, we used the SVM-Light package 6 with a simple linear kernel and eval-uated the classifier using 10 fold cross validation.Table 3 shows the accuracy of the classifier for thedifferent blogs and polarity measuring schemes.
Theerrors in classification can be attributed in part tothe inherent difficulty of the task due to the noise ofthe polarity labeling schemes and in part due to thedifficulty in obtaining a signal to predict commentpolarity from the body of the post.4.2 Supervised LDANext, we use Supervised LDA (sLDA) (Blei andMcAuliffe, 2008) to do the classification.
sLDA isa model that is an extension of Latent Dirichlet Al-location (LDA) (Blei et al, 2003) that models eachdocument as having an output variable in addition tothe document contents.
The output variable in theclassification case is modeled as an output of a lo-gistic regression model that uses the posterior topicdistribution of the LDA model as features.
In thistask, the output variable is +1 or -1 depending onthe polarity of the comment section.
In the experi-ments with sLDA, we set the number of topics as 15after experimenting with a range of topics and use10-fold cross validation.
The number of topics is setlower than it usually is with topic modeling, due tothe relatively short length and small number of doc-uments.The advantage of sLDA in this task is that we in-duce topics from the bodies of the blog posts thatserve to characterize the different issues that eachblog addresses.
In addition, the logistic regres-sion parameters indicate how each topic influencesthe output variable.
Table 4 shows the top 1 or 26http://svmlight.joachims.org/topics with the highest negative and positive logis-tic regression coefficients for each blog.
Inspect-ing the top words of the topics confirms our no-tions of the kinds of issues that appeal to the read-ers of each of the blogs.
For instance, in the top-ics induced from Daily Kos, a very liberal leaningblog, we see that the most negative topic (i.e.
thetopic that contributes the most to potential nega-tive comments) talks about the Bush adminstrationand Vice President Cheney, which was and remainsquite unpopular with people from the left.
The othernegative topic concerns the war in Iraq which wasalso very unpopular within people whose beliefs areliberal-leaning.
The most positive topic seeminglyfocuses on campaign funding.
Our conjecture forthe high comment polarity is the great success in thethen Democratic candidate Obama?s fund raising at-tempts during the presidential campaign.
In the sec-ond blog, Right Wing News, which is a conservativeblog, we see a different picture.
The most negativetopic deals with Islam and Muslim people which areissues that have tended to evoke negative reactionsfrom certain sections of people with conservativepolitical beliefs.
Global warming also evoked nega-tive comments which is consistent with the conser-vative viewpoint that there isn?t evidence to suggestthat greenhouse gases cause global warming.
Themost positive topic seems to be about anti-abortionissues which is an issue that frequently pops up inconservative political discourse.
Topics from theother blogs also seem to be in line with the standardpositions taken by liberal and conservatives on lead-ing issues in US politics like taxation, immigration,public health and the presidential campaign whichwas in full flow at the time the data was collected.Table 3 shows the accuracy of sLDA in predict-ing the comment polarity based on the blog posts.It can be seen from the table that sLDA performsmarginally better than SVM when trained on blogposts, even though documents are now representedin the lower dimensional topic space in contrast tothe high dimensional word space that was used withSVM.
sLDA provides the additional advantage ofproviding an overall summary of the corpus via thetopic tables it induces.15Blog Topic words Topic co-efficientCB* bush president news administration house white officials report fox governmentoffice military department public cheney john journal week pentagon national-0.79* huckabee giuliani romney mccain republican presidential religious campaign gopjohn party candidate mitt rudy mike conservative thompson support paul candidates0.48DK* bush administration congress law government court house intelligence white ex-ecutive committee time cheney federal course national act president congressionalinformation-1.54* iraq war bush troops news military american president iraqi starts maine cheersdays jeers mccain moreville rightnow day americans people-0.60* money health campaign foster energy district million people nrcc dccc care elec-tion time bill change funds don global federal economy0.62MY* iraq war american military iraqi government people troops bush security unitedforces world country surge presence political force maliki afghanistan-0.50* people care health don public immigration college political education school is-sue insurance social system policy real lot isn actually sense1.05RS* economy market people financial economic markets money world rate rates fed-eral mortgage government credit prices price term inflation reserve oil-0.30* tax government taxes money economic care people spending million jobs ameri-can energy health increase pay economy private free federal business0.61RWN* people muslim world country war american law muslims time police americarights free peace death city islamic government freedom united-0.68* democrats warming global vote election obama energy democratic change votesclimate people john gore political gas don voters party bill-0.39* people life women woman time own little love person children world live readbelieve god isn school feel mean0.47Table 4: Topics from sLDA and weightsSentiWordNet PMIBlog SVM sLDA SVM sLDAcb 0.66 0.56 0.79 0.79dk 0.72 0.59 0.74 0.73my 0.64 0.61 0.87 0.89rs 0.65 0.57 0.75 0.80rwn 0.65 0.60 0.90 0.90Table 5: Accuracy: Using comments to predict commentsentiment polarity4.3 Using comments to predict commentpolarityIn the previous experiments we were using the bod-ies of the blog posts to predict comment polarity.There are multiple factors which make this a diffi-cult task.
One major factor is the difficulty of learn-ing potentially noisy labels using automatic meth-ods.
More interestingly, we operate under the hy-pothesis that there is signal about comment polarityin the bodies of the blog posts.
To test this hypoth-esis, we train classifiers on the comment sectionsthemselves to predict comment polarity.
This servesto eliminate the effect of our hypothesis and focuson the inherent difficulty in learning the noisy la-bels.
Table 5 shows the results of these experiments.We see that once again, sLDA results are compara-ble to the accuracies reported by SVM and that PMIlabels are less noisier than the labels obtained using16Evaluating Trained on DK Trained on RWNDK 0.75/0.77 0.61/0.62RWN 0.74/0.71 0.90/0.90Table 6: Cross blog results: Accuracy using SVM/sLDASentiWordNet.
More importantly, we note that theaccuracy in predicting the comment polarity whilehigher than the accuracy in predicting the polarityfrom blog posts, is not significantly higher whichstrongly suggests that blog posts have quite a bit ofinformation regarding comment polarity.4.4 Cross blog experimentsThe effect of the nature of the blog on the classifier isexamined by training models on the blog posts froma conservative blog (RWN) using PMI-determinedpolarities as targets and by testing the model by run-ning liberal blog data (from DK) through it.
Simi-larly, we test RWN blog entries by training it on aclassifier trained on DK posts.
The results of the ex-periments are in Table 6.
For easy reference, thetable also includes the accuracies when blogs aretrained using posts from the same blog (obtainedfrom Table 3).
We see that the accuracy in predict-ing polarity degrades when blog posts are tested ona classifier trained on posts from a blog of oppositepolitical affiliation.
These results indicate that emo-tion is tied to the blog and community that one isinvolved in.4.5 ConclusionWe addressed the task of predicting the emotionalresponse that is induced in political discourses.
Tothis end, we tackled the tasks of determining the sen-timent polarity of comments in blogs and the task ofpredicting the polarity based on the content of theblog post.
Our approach also characterized the is-sues talked about in specific blog communities.
Ourexperiments show that the community specific PMImethod provides a more accurate picture of the sen-timent in comments than the generic SentiWordNettechnique.
We also see that the context of the com-munity is key as seen in the poor performance ofmodels trained on blogs from one end of the politi-cal spectrum in predicting the polarity of responsesto blog posts in communities on the other end of thespectrum.ReferencesAndrea Esuli Stefano Baccianella and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexi-cal resource for sentiment analysis and opinion min-ing.
In Proceedings of the Seventh conference onInternational Language Resources and Evaluation(LREC?10), Valletta, Malta, May.
European LanguageResources Association (ELRA).David Blei and Jon McAuliffe, 2008.
Supervised TopicModels, pages 121?128.
MIT Press, Cambridge, MA.D.
M Blei, A. Y Ng, and M. I Jordan.
2003.
Latentdirichlet alocation.
The Journal of Machine LearningResearch, 3:993?1022.Li-Ning Huang and Vincent Price.
2001.
Motivations,goals, information search, and memory about politicalcandidates.
Political Psychology, 22(4):pp.
665?692.Linda M. Isbell, Victor C. Ottati, and Kathleen C. Bruns.2006.
Affect and politics: Effects on judgment, pro-cessing, and information seeking.
In David Redlawsk,editor, Feeling Politics: Emotion in Political Infor-mation Processing.
Palgrave Macmillan, New York,USA.Sung-youn Kim, Charles S. Taber, and Milton Lodge.2008.
A Computational Model of the Citizen as Mo-tivated Reasoner: Modeling the Dynamics of the 2000Presidential Election.
SSRN eLibrary.Milton Lodge and Charles Taber, 2000.
Three Stepstoward a Theory of Motivated Political Reasoning.Cambridge University Press.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.
Fromtweets to polls: Linking text sentiment to public opin-ion time series.
In Proceedings of the InternationalAAAI Conference on Weblogs and Social Media.David P. Redlawsk.
2002.
Hot cognition or cool con-sideration?
testing the effects of motivated reasoningon political decision making.
The Journal of Politics,64(04):1021?1044.David Redlawsk.
2006.
Motivated reasoning, affect, andthe role of memory in voter decision-making.
In DavidRedlawsk, editor, Feeling Politics: Emotion in Politi-cal Information Processing.
Palgrave Macmillan, NewYork, USA.Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-ner, and Isabell M. Welpe.
2010.
Predicting elec-tions with twitter: What 140 characters reveal aboutpolitical sentiment.
In William W. Cohen and SamuelGosling, editors, ICWSM.
The AAAI Press.Peter D. Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th AnnualMeeting on Association for Computational Linguis-tics, ACL ?02, pages 417?424, Stroudsburg, PA, USA.Association for Computational Linguistics.17Tae.
Yano, William.
Cohen, and Noah A. Smith.
2009.Predicting response to political blog posts with topicmodels.
In Proceedings of the North American Associ-ation for Computational Linguistics Human LanguageTechnologies Conference.Appendix A18Positive wonderfulness, admirableness, admirability, wonderful, admirable, top-flight, splendid, first-class, fantabu-lous, excellent, good, balmy, mild, ennoble, dignified, amuse, agree, do good, benefit, vest, prefer, placate,pacify, mollify, lenify, gentle, conciliate, assuage, appease, filigree, dazzle, admiringly, character, preem-inence, note, eminence, distinction, radiance, amiability, bonheur, worship, adoration, divination, music,euphony, judiciousness, essentialness, essentiality, gain, crispness, urbanity, courtesy, decency, modesty,dedication, integrity, honourableness, honorableness, honor, goodness, good, morality, urbanity, tasteful-ness, elegance, elegance, healthfulness, nutritiveness, nutritiousness, wholesomeness, fineness, choiceness,loveliness, fairness, comeliness, beauteousness, picturesqueness, bluffness, good nature, character, props,joke, jocularity, jest, worthy, salubrious, healthy, virtuous, esthetic, artistic, aesthetic, spiffing, superlative,sterling, greatest, superb, brilliant, boss, banner, olympian, majestic, straightarrow, wide-eyed, round-eyed,dewy-eyed, childlike, righteous, answerable, nice, decent, diffident, respected, reputable, self-respecting,self-respectful, dignified, constructive, sweet, fabulous, fab, charming, admirable, idyllic, idealized, ide-alised, ennobling, dignifying, nice, incumbent, clean, lucky, intellectual, formidable, awing, awful, awe-some, awe-inspiring, amazing, important, joking, jocular, jocose, jesting, amicable, kind, genial, therapeu-tic, sanative, remedial, healing, curative, gracious, gainly, goody-goody, good, superb, solid, good, inspired,elysian, divine, worthy, quaint, discerning, golden, fortunate, blest, blessed, courteous, thorough, exhaus-tive, better, benign, pretty, piquant, engaging, attractive, well, veracious, right, grace, goodwill, belong,accommodate, serve, merit, deserve, shine, radiate, glow, beam, disillusion, disenchant, proclaim, laud,glorify, extol, exalt, cheer, consider, purify, enervate, recuperate, amusingly, dearly, dear, affectionately,thoroughly, soundly, well, simply, time, posterboard, fettle, mildness, clemency, successfulness, prosper-ity, wellbeing, well-being, upbeat, wholeness, haleness, purity, pureness, innocence, antithesis, serendipity,superordinate, superior, possible, pleaser, idolizer, idoliser, amoralistNegative tawdry, shoddy, cheapjack, scrimy, unsound, unfit, bad, sorry, sad, pitiful, lamentable, distressing, de-plorable, abject, unfortunate, inauspicious, humbug, trouble, inconvenience, disoblige, bother, smell, stink,reek, twinge, sting, prick, burn, sting, burn, bite, desensitize, desensitise, resent, begrudge, pity, compassion-ate, abreact, agonize, agonise, muddy, settle, moan, groan, impugn, repudiate, deny, reject, disapprove, snub,repel, rebuff, sting, stick, disapprove, refute, rebut, controvert, foul, curdle, smite, afflict, ease, comfort, ail,inflame, woefully, sadly, lamentably, deplorably, hard, unluckily, unfortunately, regrettably, alas, worst,throe, woe, suffering, inconvenience, incommodiousness, solacement, solace, dyspnoea, dyspnea, throe,shrew, ruffian, rowdy, roughneck, hooligan, bully, plonk, sullenness, moroseness, glumness, moodiness,malignity, malevolence, guilt, sorrow, ruefulness, rue, regret, dolour, dolor, dolefulness, gloating, gloat,weakness, self-torture, self-torment, suffering, hurt, distress, torment, curse, straits, pass, head, excoriation,canard, scurrility, billingsgate, scribble, scrawl, scratch, prejudice, preconception, bias, pill, onus, load, in-cumbrance, encumbrance, burden, poignancy, pathos, penalty, badness, bad, fault, demerit, hardness, moldi-ness, harshness, cruelty, cruelness, spitefulness, spite, nastiness, cattiness, bitchiness, malice, malevolency,malevolence, heinousness, barbarousness, barbarity, atrocity, atrociousness, illegitimacy, unnaturalness, dis-agreeableness, incongruousness, incongruity, ruggedness, hardness, unneighborliness, unfriendliness, dis-agreeableness, sadness, lugubriousness, gloominess, shlock, schlock, dreck, mongrel, bastard, shenanigan,roguishness, roguery, rascality, mischievousness, mischief-making, mischief, deviltry, devilry, devilment,shitwork, overexertion, overacting, hamming, shlep, schlep, worst, upset, scrofulous, sick, ill, sheltered,occult, trashy, rubbishy, undivided, worried, upset, disturbed, distressed, disquieted, troubled, unmanage-able, uncontrollable, mussy, messy, unsympathetic, invalidating, disconfirming, wretched, woeful, miser-able, execrable, deplorable, bush-league, bush, tinny, sleazy, punk, crummy, chintzy, cheesy, cheap, bum,inferior, indifferent, lowly, humble, insufficient, deficient, insubordinate, cross-grained, contrarious, spas-tic, spasmodic, convulsive, unaccepted, unacceptable, nonstandard, unsound, asocial, antisocial, feigned,broken-down, vicious, reprehensible, deplorable, criminal, condemnable, notorious, infamous, ill-famed,untreated, modified, limited, unmixed, unmingled, sheer, plain, cretinous, negative, imponderable, vexing,maddening, infuriating, exasperating, ungrateful, sore, painful, afflictive, harsh, unpeaceable, unforbearing,unpainted, underivative, scurrilous, opprobrious, abusive, verminous, outrageous, horrific, horrid, hideous,creepy, pestilent, pernicious, deadly, baneful, paranormal, grotty, nasty, awful, transcendental, preternatural,otherworldly, nonnatural, simulated, imitation, faux, false, fake, substitute, ersatz, strong, smart, wicked,terrible, severe, unpitying, ruthless, remorseless, pitiless, unlikeable, unlikable, unmourned, unlamented,rough, harsh, woeful, woebegone, lugubrious, heartsick, heartbroken, brokenhearted, bitterTable 7: Seed words used in the PMI technique19
