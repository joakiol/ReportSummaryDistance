Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 97?104,Sydney, July 2006. c?2006 Association for Computational LinguisticsAn Empirical Study of Chinese ChunkingWenliang Chen, Yujie Zhang, Hitoshi IsaharaComputational Linguistics GroupNational Institute of Information and Communications Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289{chenwl, yujie, isahara}@nict.go.jpAbstractIn this paper, we describe an empiricalstudy of Chinese chunking on a corpus,which is extracted from UPENN ChineseTreebank-4 (CTB4).
First, we comparethe performance of the state-of-the-art ma-chine learning models.
Then we proposetwo approaches in order to improve theperformance of Chinese chunking.
1) Wepropose an approach to resolve the spe-cial problems of Chinese chunking.
Thisapproach extends the chunk tags for ev-ery problem by a tag-extension function.2) We propose two novel voting meth-ods based on the characteristics of chunk-ing task.
Compared with traditional vot-ing methods, the proposed voting methodsconsider long distance information.
Theexperimental results show that the SVMsmodel outperforms the other models andthat our proposed approaches can improveperformance significantly.1 IntroductionChunking identifies the non-recursive cores ofvarious types of phrases in text, possibly as aprecursor to full parsing or information extrac-tion.
Steven P. Abney was the first personto introduce chunks for parsing(Abney, 1991).Ramshaw and Marcus(Ramshaw and Marcus,1995) first represented base noun phrase recog-nition as a machine learning problem.
In 2000,CoNLL-2000 introduced a shared task to tagmany kinds of phrases besides noun phrases inEnglish(Sang and Buchholz, 2000).
Addition-ally, many machine learning approaches, such asSupport Vector Machines (SVMs)(Vapnik, 1995),Conditional Random Fields (CRFs)(Lafferty etal., 2001), Memory-based Learning (MBL)(Parkand Zhang, 2003), Transformation-based Learn-ing (TBL)(Brill, 1995), and Hidden Markov Mod-els (HMMs)(Zhou et al, 2000), have been appliedto text chunking(Sang and Buchholz, 2000; Ham-merton et al, 2002).Chinese chunking is a difficult task, and muchwork has been done on this topic(Li et al, 2003a;Tan et al, 2005; Wu et al, 2005; Zhao et al,2000).
However, there are many different Chinesechunk definitions, which are derived from differ-ent data sets(Li et al, 2004; Zhang and Zhou,2002).
Therefore, comparing the performance ofprevious studies in Chinese chunking is very dif-ficult.
Furthermore, compared with the other lan-guages, there are some special problems for Chi-nese chunking(Li et al, 2004).In this paper, we extracted the chunking corpusfrom UPENN Chinese Treebank-4(CTB4).
Wepresented an empirical study of Chinese chunk-ing on this corpus.
First, we made an evaluationon the corpus to clarify the performance of state-of-the-art models in Chinese chunking.
Then weproposed two approaches in order to improve theperformance of Chinese chunking.
1) We pro-posed an approach to resolve the special prob-lems of Chinese chunking.
This approach ex-tended the chunk tags for every problem by a tag-extension function.
2) We proposed two novel vot-ing methods based on the characteristics of chunk-ing task.
Compared with traditional voting meth-ods, the proposed voting methods considered longdistance information.
The experimental resultsshowed the proposed approaches can improve theperformance of Chinese chunking significantly.The rest of this paper is as follows: Section 2describes the definitions of Chinese chunks.
Sec-97tion 3 simply introduces the models and featuresfor Chinese chunking.
Section 4 proposes a tag-extension method.
Section 5 proposes two newvoting approaches.
Section 6 explains the exper-imental results.
Finally, in section 7 we draw theconclusions.2 Definitions of Chinese ChunksWe defined the Chinese chunks based on the CTB4dataset1.
Many researchers have extracted thechunks from different versions of CTB(Tan et al,2005; Li et al, 2003b).
However, these studies didnot provide sufficient detail.
We developed a tool2to extract the corpus from CTB4 by modifying thetool Chunklink3.2.1 Chunk TypesHere we define 12 types of chunks4: ADJP, ADVP,CLP, DNP, DP, DVP, LCP, LST, NP, PP, QP,VP(Xue et al, 2000).
Table 1 provides definitionsof these chunks.Type DefinitionADJP Adjective PhraseADVP Adverbial PhraseCLP Classifier PhraseDNP DEG PhraseDP Determiner PhraseDVP DEV phraseLCP Localizer PhraseLST List MarkerNP Noun PhrasePP Prepositional PhraseQP Quantifier PhraseVP Verb PhraseTable 1: Definition of Chunks2.2 Data RepresentationTo represent the chunks clearly, we represent thedata with an IOB-based model as the CoNLL00shared task did, in which every word is to betagged with a chunk type label extended with I(inside a chunk), O (outside a chunk), and B (in-side a chunk, but also the first word of the chunk).1More detailed information athttp://www.cis.upenn.edu/ chinese/.2Tool is available athttp://www.nlplab.cn/chenwl/tools/chunklinkctb.txt.3Tool is available at http://ilk.uvt.nl/software.html#chunklink.4There are 15 types in the Upenn Chinese TreeBank.
Theother chunk types are FRAG, PRN, and UCP.Each chunk type could be extended with I or Btags.
For instance, NP could be represented astwo types of tags, B-NP or I-NP.
Therefore, wehave 25 types of chunk tags based on the IOB-based model.
Every word in a sentence will betagged with one of these chunk tags.
For in-stance, the sentence (word segmented and Part-of-Speech tagged) ?
?-NR(He) /??-VV(reached)/?
?-NR(Beijing) /?
?-NN(airport) /?/?
willbe tagged as follows:Example 1:S1: [NP?][VP??][NP??/??][O?
]S2: ?B-NP /?
?B-VP /?
?B-NP /?
?I-NP /?O /Here S1 denotes that the sentence is tagged withchunk types, and S2 denotes that the sentence istagged with chunk tags based on the IOB-basedmodel.With data representation, the problem of Chi-nese chunking can be regarded as a sequence tag-ging task.
That is to say, given a sequence oftokens (words pairing with Part-of-Speech tags),x = x1, x2, ..., xn, we need to generate a sequenceof chunk tags, y = y1, y2, ..., yn.2.3 Data SetCTB4 dataset consists of 838 files.
In the ex-periments, we used the first 728 files (FID fromchtb 001.fid to chtb 899.fid) as training data, andthe other 110 files (FID from chtb 900.fid tochtb 1078.fid) as testing data.
In the followingsections, we use the CTB4 Corpus to refer to theextracted data set.
Table 2 lists details on theCTB4 Corpus data used in this study.Training TestNum of Files 728 110Num of Sentences 9,878 5,290Num of Words 238,906 165,862Num of Phrases 141,426 101,449Table 2: Information of the CTB4 Corpus3 Chinese Chunking3.1 Models for Chinese ChunkingIn this paper, we applied four models, includ-ing SVMs, CRFs, TBL, and MBL, which haveachieved good performance in other languages.We only describe these models briefly since fulldetails are presented elsewhere(Kudo and Mat-sumoto, 2001; Sha and Pereira, 2003; Ramshawand Marcus, 1995; Sang, 2002).983.1.1 SVMsSupport Vector Machines (SVMs) is a pow-erful supervised learning paradigm based on theStructured Risk Minimization principle from com-putational learning theory(Vapnik, 1995).
Kudoand Matsumoto(Kudo and Matsumoto, 2000) ap-plied SVMs to English chunking and achievedthe best performance in the CoNLL00 sharedtask(Sang and Buchholz, 2000).
They created 231SVMs classifiers to predict the unique pairs ofchunk tags.The final decision was given by theirweighted voting.
Then the label sequence waschosen using a dynamic programming algorithm.Tan et al (Tan et al, 2004) applied SVMs toChinese chunking.
They used sigmoid functionsto extract probabilities from SVMs outputs as thepost-processing of classification.
In this paper, weused Yamcha (V0.33)5 in our experiments.3.1.2 CRFsConditional Random Fields is a powerful se-quence labeling model(Lafferty et al, 2001) thatcombine the advantages of both the generativemodel and the classification model.
Sha andPereira(Sha and Pereira, 2003) showed that state-of-the-art results can be achieved using CRFs inEnglish chunking.
CRFs allow us to utilize a largenumber of observation features as well as differ-ent state sequence based features and other fea-tures we want to add.
Tan et al (Tan et al, 2005)applied CRFs to Chinese chunking and their ex-perimental results showed that the CRFs approachprovided better performance than HMM.
In thispaper, we used MALLET (V0.3.2)6(McCallum,2002) to implement the CRF model.3.1.3 TBLTransformation based learning(TBL), first in-troduced by Eric Brill(Brill, 1995), is mainlybased on the idea of successively transforming thedata in order to correct the error.
The transforma-tion rules obtained are usually few , yet power-ful.
TBL was applied to Chinese chunking by Liet al(Li et al, 2004) and TBL provided good per-formance on their corpus.
In this paper, we usedfnTBL (V1.0)7 to implement the TBL model.5Yamcha is available athttp://chasen.org/ taku/software/yamcha/6MALLET is available athttp://mallet.cs.umass.edu/index.php/Main Page7fnTBL is available athttp://nlp.cs.jhu.edu/ rflorian/fntbl/index.html3.1.4 MBLMemory-based Learning (also called instancebased learning) is a non-parametric inductivelearning paradigm that stores training instances ina memory structure on which predictions of newinstances are based(Walter et al, 1999).
The simi-larity between the new instance X and example Yin memory is computed using a distance metric.Tjong Kim Sang(Sang, 2002) applied memory-based learning(MBL) to English chunking.
MBLperforms well for a variety of shallow parsingtasks, often yielding good results.
In this paper,we used TiMBL8(Daelemans et al, 2004) to im-plement the MBL model.3.2 FeaturesThe observations are based on features that areable to represent the difference between the twoevents.
We utilize both lexical and Part-Of-Speech(POS) information as the features.We use the lexical and POS information withina fixed window.
We also consider different combi-nations of them.
The features are listed as follows:?
WORD: uni-gram and bi-grams of words inan n window.?
POS: uni-gram and bi-grams of POS in an nwindow.?
WORD+POS: Both the features of WORDand POS.where n is a predefined number to denote windowsize.For instance, the WORD features at the 3rdposition (?
?-NR) in Example 1 (set n as 2):??
L2 ??
L1 ??
0 ??
R1 ?
R2?
(uni-gram) and ??
??
LB1??
??
B0??
??
RB1 ??
?
RB2?(bi-gram).
Thus featuresof WORD have 9 items(5 from uni-gram and4 from bi-grams).
In the similar way, fea-tures of POS also have 9 items and features ofWORD+POS have 18 items(9+9).4 Tag-ExtensionIn Chinese chunking, there are some difficult prob-lems, which are related to Special Terms, Noun-Noun Compounds, Named Entities Tagging andCoordination.
In this section, we propose an ap-proach to resolve these problems by extending thechunk tags.8TiMBL is available at http://ilk.uvt.nl/timbl/99In the current data representation, the chunktags are too generic to construct accurate models.Therefore, we define a tag-extension function fsin order to extend the chunk tags as follows:Te = fs(T,Q) = T ?Q (1)where, T denotes the original tag set, Q denotesthe problem set, and Te denotes the extended tagset.
For instance, we have an q problem(q ?
Q).Then we extend the chunk tags with q.
For NPRecognition, we have two new tags: B-NP-q andI-NP-q.
Here we name this approach as Tag-Extension.In the following three cases study, we demon-strate that how to use Tag-Extension to resolve thedifficult problems in NP Recognition.1) Special Terms: this kind of noun phrasesis special terms such as ?
?/ ??
(Life)/ ??
(Forbidden Zone)/ ?/?, which are bracketedwith the punctuation ?
?, ?, ?, ?, ?, ?
?.They are divided into two types: chunks with thesepunctuation and chunks without these punctua-tion.
For instance, ?
?/ ?
?/ ?
?/ ?/?
is anNP chunk (?B-NP/ ?
?I-NP/ ?
?I-NP/ ?I-NP/) while ??/??
(forever)/ ??(full-blown)/?(DE)/???
(Chinese Redbud)/?/?
is taggedas (?O/ ?
?O /?
?O/ ?O/ ???B-NP/?O/).
We extend the tags with SPE for SpecialTerms: B-NP-SPE and I-NP-SPE.2) Coordination: These problems are relatedto the conjunctions ??
(and), ?
(and), ?(or),?(and)?.
They can be divided into two types:chunks with conjunctions and chunks withoutconjunctions.
For instance, ???(HongKong)/?(and)/??(Macau)/?
is an NP chunk (?
?B-NP/ ?I-NP/ ?
?I-NP/), while in ???(least)/??
(salary)/ ?
(and)/ ???
(living mainte-nance)/?
it is difficult to tell whether ????
is ashared modifier or not, even for people.
We extendthe tags with COO for Coordination: B-NP-COOand I-NP-COO.3) Named Entities Tagging: Named Enti-ties(NE)(Sang and Meulder, 2003) are not dis-tinguished in CTB4, and they are all tagged as?NR?.
However, they play different roles inchunks, especial in noun phrases.
For instance,??
?-NR(Macau)/ ??-NN(Airport)?
and ??
?-NR(Hong Kong)/??-NN(Airport)?
vs ???
?-NR(Deng Xiaoping)/ ??-NN(Mr.)?
and ???
?-NR(Song Weiping) ?
?-NN(President)?.Here ????
and ????
are LOCATION, while?????
and ?????
are PERSON.
To investi-gate the effect of Named Entities, we use a LOCA-TION dictionary, which is generated from the PFRcorpus9 of ICL, Peking University, to tag locationwords in the CTB4 Corpus.
Then we extend thetags with LOC for this problem: B-NP-LOC andI-NP-LOC.From the above cases study, we know the stepsof Tag-Extension.
Firstly, identifying a specialproblem of chunking.
Secondly, extending thechunk tags via Equation (1).
Finally, replacing thetags of related tokens with new chunk tags.
AfterTag-Extension, we use new added chunk tags todescribe some special problems.5 Voting MethodsKudo and Matsumoto(Kudo and Matsumoto,2001) reported that they achieved higher accuracyby applying voting of systems that were trainedusing different data representations.
Tjong KimSang et al(Sang and Buchholz, 2000) reportedsimilar results by combining different systems.In order to provide better results, we also ap-ply the voting of basic systems, including SVMs,CRFs, MBL and TBL.
Depending on the charac-teristics in the chunking task, we propose two newvoting methods.
In these two voting methods, weconsider long distance information.In the weighted voting method, we can assigndifferent weights to the results of the individ-ual system(van Halteren et al, 1998).
However,it requires a larger amount of computational ca-pacity as the training data is divided and is re-peatedly used to obtain the voting weights.
Inthis paper, we give the same weight to all ba-sic systems in our voting methods.
Suppose, wehave K basic systems, the input sentence is x =x1, x2, ..., xn, and the results of K basic systemsare tj = t1j , t2j , ..., tnj , 1 ?
j ?
K. Then ourgoal is to gain a new result y = y1, y2, ..., yn byvoting.5.1 Basic VotingThis is traditional voting method, which is thesame as Uniform Weight in (Kudo and Mat-sumoto, 2001).
Here we name it as Basic Voting.For each position, we have K candidates from Kbasic systems.
After voting, we choose the candi-date with the most votes as the final result for eachposition.9More information at http://www.icl.pku.edu1005.2 Sent-based VotingIn this paper, we treat chunking as a sequence la-beling task.
Here we apply this idea in computingthe votes of one sentence instead of one word.
Wename it as Sent-based Voting.
For one sentence,we have K candidates, which are the tagged se-quences produced by K basic systems.
First, wevote on each position, as done in Basic Voting.Then we compute the votes of every candidate byaccumulating the votes of each position.
Finally,we choose the candidate with the most votes asthe final result for the sentence.
That is to say, wemake a decision based on the votes of the wholesentence instead of each position.5.3 Phrase-based VotingIn chunking, one phrase includes one or morewords, and the word tags in one phrase depend oneach other.
Therefore, we propose a novel vot-ing method based on phrases, and we compute thevotes of one phrase instead of one word or one sen-tence.
Here we name it as Phrase-based Voting.There are two steps in the Phrase-based Votingprocedure.
First, we segment one sentence intopieces.
Then we calculate the votes of the pieces.Table 3 is the algorithm of Phrase-based Voting,where F (tij , tik) is a binary function:F (tij , tik) ={1 : tij = tik0 : tij 6= tik (2)In the segmenting step, we seek the ?O?
or ?B-XP?
(XP can be replaced by any type of phrase)tags, in the results of basic systems.
Then we get anew piece if all K results have the ?O?
or ?B-XP?tags at the same position.In the voting step, the goal is to choose a resultfor each piece.
For each piece, we have K candi-dates.
First, we vote on each position within thepiece, as done in Basic Voting.
Then we accumu-late the votes of each position for every candidate.Finally, we pick the one, which has the most votes,as the final result for the piece.The difference in these three voting methods isthat we make the decisions in different ranges: Ba-sic Voting is at one word; Phrase-based Voting isin one piece; and Sent-based Voting is in one sen-tence.6 ExperimentsIn this section, we investigated the performance ofChinese chunking on the CTB4 Corpus.Input:Sequence: x = x1, ..., xn;K results: tj = t1j , ..., tnj , 1 ?
j ?
K.Output:Voted results: y = y1, y2, ..., ynSegmenting: Segment the sentence into pieces.Pieces[]=null; begin = 1For each i in (2, n){For each j in (1,K)if(tij is not ?O?
and ?B-XP?)
break;if(j > K){add new piece: p = xbegin, ..., xi?1 into Pieces;begin = i; }}Voting: Choose the result with the most votes for eachpiece: p = xbegin, ..., xend.Votes[K] = 0;For each k in (1,K)V otes[k] =?begin?i?end,1?j?KF (tij , tik) (3)kmax = argmax1?k?K(V otes[k]);Choose tbegin,kmax , ..., tend,kmax as the result forpiece p.Table 3: Algorithm of Phrase-based Voting6.1 Experimental SettingTo investigate the chunker sensitivity to the sizeof the training set, we generated different sizes oftraining sets, including 1%, 2%, 5%, 10%, 20%,50%, and 100% of the total training data.In our experiments, we used all the default pa-rameter settings of the packages.
Our SVMs andCRFs chunkers have a first-order Markov depen-dency between chunk tags.We evaluated the results as CONLL2000 share-task did.
The performance of the algorithm wasmeasured with two scores: precision P and recallR.
Precision measures how many chunks found bythe algorithm are correct and the recall rate con-tains the percentage of chunks defined in the cor-pus that were found by the chunking program.
Thetwo rates can be combined in one measure:F1 = 2?
P ?RR+ P (4)In this paper, we report the results with F1 score.6.2 Experimental Results6.2.1 POS vs. WORD+POSIn this experiment, we compared the perfor-mance of different feature representations, in-1017075808590950.01  0.02  0.05  0.1  0.2  0.5  1F1Size of Training dataSVM_WPSVM_PCRF_WPCRF_PFigure 1: Results of different featurescluding POS and WORD+ POS(See section 3.2),and set the window size as 2.
We also inves-tigated the effects of different sizes of trainingdata.
The SVMs and CRFs approaches were usedin the experiments because they provided goodperformance in chunking(Kudo and Matsumoto,2001)(Sha and Pereira, 2003).Figure 1 shows the experimental results, wherextics denotes the size of the training data, ?WP?refers to WORD+POS, ?P?
refers to POS.
We cansee from the figure that WORD+POS yielded bet-ter performance than POS in the most cases.
How-ever, when the size of training data was small,the performance was similar.
With WORD+POS,SVMs provided higher accuracy than CRFs inall training sizes.
However, with POS, CRFsyielded better performance than SVMs in largescale training sizes.
Furthermore, we found SVMswith WORD+POS provided 4.07% higher accu-racy than with POS, while CRFs provided 2.73%higher accuracy.6.2.2 Comparison of ModelsIn this experiment, we compared the perfor-mance of the models, including SVMs, CRFs,MBL, and TBL, in Chinese chunking.
In the ex-periments, we used the feature WORD+POS andset the window size as 2 for the first two mod-els.
For MBL, WORD features were within a one-window size, and POS features were within a two-window size.
We used the original data for TBLwithout any reformatting.Table 4 shows the comparative results of themodels.
We found that the SVMs approach wassuperior to the other ones.
It yielded results thatwere 0.72%, 1.51%, and 3.58% higher accuracythan respective CRFs, TBL, and MBL approaches.SVMs CRFs TBL MBLADJP 84.45 84.55 85.95 80.48ADVP 83.12 82.74 81.98 77.95CLP 5.26 0.00 0.00 3.70DNP 99.65 99.64 99.65 99.61DP 99.70 99.40 99.70 99.46DVP 96.77 92.89 99.61 99.41LCP 99.85 99.85 99.74 99.82LST 68.75 68.25 56.72 64.75NP 90.54 89.79 89.82 87.90PP 99.67 99.66 99.67 99.59QP 96.73 96.53 96.60 96.40VP 89.74 88.50 85.75 82.51+ 91.46 90.74 89.95 87.88Table 4: Comparative Results of ModelsMethod Precision Recall F1CRFs 91.47 90.01 90.74SVMs 92.03 90.91 91.46V1 91.97 90.66 91.31V2 92.32 90.93 91.62V3 92.40 90.97 91.68Table 5: Voting ResultsGiving more details for each category, the SVMsapproach provided the best results in ten cate-gories, the CRFs in one category, and the TBL infive categories.6.2.3 Comparison of Voting MethodsIn this section, we compared the performance ofthe voting methods of four basic systems, whichwere used in Section 6.2.2.
Table 5 shows theresults of the voting systems, where V1 refersto Basic Voting, V2 refers to Sent-based Voting,and V3 refers to Phrase-based Voting.
We foundthat Basic Voting provided slightly worse resultsthan SVMs.
However, by applying the Sent-based Voting method, we achieved higher accu-racy than any single system.
Furthermore, wewere able to achieve more higher accuracy by ap-plying Phrase-based Voting.
Phrase-based Votingprovided 0.22% and 0.94% higher accuracy thanrespective SVMs, CRFs approaches, the best twosingle systems.The results suggested that the Phrase-based Vot-ing method is quite suitable for chunking task.
ThePhrase-based Voting method considers one chunkas a voting unit instead of one word or one sen-tence.102SVMs CRFs TBL MBL V3NPR 90.62 89.72 89.89 87.77 90.92COO 90.61 89.78 90.05 87.80 91.03SPE 90.65 90.14 90.31 87.77 91.00LOC 90.53 89.83 89.69 87.78 90.86NPR* - - - - 91.13Table 6: Results of Tag-Extension in NP Recogni-tion6.2.4 Tag-ExtensionNP is the most important phrase in Chinesechunking and about 47% phrases in the CTB4 Cor-pus are NPs.
In this experiment, we presented theresults of Tag-Extension in NP Recognition.Table 6 shows the experimental results of Tag-Extension, where ?NPR?
refers to chunking with-out any extension, ?SPE?
refers to chunkingwith Special Terms Tag-Extension, ?COO?
refersto chunking with Coordination Tag-Extension,?LOC?
refers to chunking with LOCATION Tag-Extension, ?NPR*?
refers to voting of eight sys-tems(four of SPE and four of COO), and ?V3?refers to Phrase-based Voting method.For NP Recognition, SVMs also yielded thebest results.
But it was surprised that TBL pro-vided 0.17% higher accuracy than CRFs.
By ap-plying Phrase-based Voting, we achieved better re-sults, 0.30% higher accuracy than SVMs.From the table, we can see that the Tag-Extension approach can provide better results.
InCOO, TBL got the most improvement with 0.16%.And in SPE, TBL and CRFs got the same improve-ment with 0.42%.
We also found that Phrase-based Voting can improve the performance signif-icantly.
NPR* provided 0.51% higher than SVMs,the best single system.For LOC, the voting method helped to improvethe performance, provided at least 0.33% higheraccuracy than any single system.
But we alsofound that CRFs and MBL provided better resultswhile SVMs and TBL yielded worse results.
Thereason was that our NE tagging method was verysimple.
We believe NE tagging can be effectivein Chinese chunking, if we use a highly accurateNamed Entity Recognition system.7 ConclusionsIn this paper, we conducted an empirical study ofChinese chunking.
We compared the performanceof four models, SVMs, CRFs, MBL, and TBL.We also investigated the effects of using differentsizes of training data.
In order to provide higheraccuracy, we proposed two new voting methodsaccording to the characteristics of the chunkingtask.
We proposed the Tag-Extension approach toresolve the special problems of Chinese chunkingby extending the chunk tags.The experimental results showed that the SVMsmodel was superior to the other three models.We also found that part-of-speech tags played animportant role in Chinese chunking because thegap of the performance between WORD+POS andPOS was very small.We found that the proposed voting approachescan provide higher accuracy than any single sys-tem can.
In particular, the Phrase-based Voting ap-proach is more suitable for chunking task than theother two voting approaches.
Our experimentalresults also indicated that the Tag-Extension ap-proach can improve the performance significantly.ReferencesSteven P. Abney.
1991.
Parsing by chunks.
InRobert C. Berwick, Steven P. Abney, and CarolTenny, editors, Principle-Based Parsing: Computa-tion and Psycholinguistics, pages 257?278.
Kluwer,Dordrecht.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part of speech tagging.
Computational Lin-guistics, 21(4):543?565.Walter Daelemans, Jakub Zavrel, Ko van der Sloot,and Antal van den Bosch.
2004.
Timbl: Tilburgmemory-based learner v5.1.James Hammerton, Miles Osborne, Susan Armstrong,and Walter Daelemans.
2002.
Introduction to spe-cial issue on machine learning approaches to shallowparsing.
JMLR, 2(3):551?558.Taku Kudo and Yuji Matsumoto.
2000.
Use of sup-port vector learning for chunk identification.
In InProceedings of CoNLL-2000 and LLL-2000, pages142?144.Taku Kudo and Yuji Matsumoto.
2001.
Chunkingwith support vector machines.
In In Proceedings ofNAACL01.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In International Conference on Ma-chine Learning (ICML01).103Heng Li, Jonathan J. Webster, Chunyu Kit, and Tian-shun Yao.
2003a.
Transductive hmm based chi-nese text chunking.
In Proceedings of IEEE NLP-KE2003, pages 257?262, Beijing, China.Sujian Li, Qun Liu, and Zhifeng Yang.
2003b.
Chunk-ing parsing with maximum entropy principle (in chi-nese).
Chinese Journal of Computers, 26(12):1722?1727.Hongqiao Li, Changning Huang, Jianfeng Gao, and Xi-aozhong Fan.
2004.
Chinese chunking with anothertype of spec.
In The Third SIGHAN Workshop onChinese Language Processing.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Seong-Bae Park and Byoung-Tak Zhang.
2003.Text chunking by combining hand-crafted rules andmemory-based learning.
In ACL, pages 497?504.Lance Ramshaw and Mitch Marcus.
1995.
Textchunking using transformation-based learning.
InDavid Yarovsky and Kenneth Church, editors, Pro-ceedings of the Third Workshop on Very Large Cor-pora, pages 82?94, Somerset, New Jersey.
Associa-tion for Computational Linguistics.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.Introduction to the conll-2000 shared task: Chunk-ing.
In Proceedings of CoNLL-2000 and LLL2000,pages 127?132, Lisbin, Portugal.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the conll-2003 shared task:Language-independent named entity recognition.
InProceedings of CoNLL-2003.Erik F. Tjong Kim Sang.
2002.
Memory-based shal-low parsing.
JMLR, 2(3):559?594.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In Proceedings ofHLT-NAACL03.Yongmei Tan, Tianshun Yao, Qing Chen, and JingboZhu.
2004.
Chinese chunk identification using svmsplus sigmoid.
In IJCNLP, pages 527?536.Yongmei Tan, Tianshun Yao, Qing Chen, and JingboZhu.
2005.
Applying conditional random fieldsto chinese shallow parsing.
In Proceedings ofCICLing-2005, pages 167?176, Mexico City, Mex-ico.
Springer.Hans van Halteren, Jakub Zavrel, and Walter Daele-mans.
1998.
Improving data driven wordclass tag-ging by system combination.
In COLING-ACL,pages 491?497.V.
Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer-Verlag, New York.Daelemans Walter, Sabine Buchholz, and Jorn Veen-stra.
1999.
Memory-based shallow parsing.Shih-Hung Wu, Cheng-Wei Shih, Chia-Wei Wu,Tzong-Han Tsai, and Wen-Lian Hsu.
2005.
Ap-plying maximum entropy to robust chinese shallowparsing.
In Proceedings of ROCLING2005.Nianwen Xue, Fei Xia, Shizhe Huang, and AnthonyKroch.
2000.
The bracketing guidelines for thepenn chinese treebank.
Technical report, Universityof Pennsylvania.Yuqi Zhang and Qiang Zhou.
2002.
Chinese base-phrases chunking.
In Proceedings of The FirstSIGHAN Workshop on Chinese Language Process-ing.Tiejun Zhao, Muyun Yang, Fang Liu, Jianmin Yao, andHao Yu.
2000.
Statistics based hybrid approach tochinese base phrase identification.
In Proceedingsof Second Chinese Language Processing Workshop.GuoDong Zhou, Jian Su, and TongGuan Tey.
2000.Hybrid text chunking.
In Claire Cardie, WalterDaelemans, Claire Ne?dellec, and Erik Tjong KimSang, editors, Proceedings of the CoNLL00, Lis-bon, 2000, pages 163?165.
Association for Compu-tational Linguistics, Somerset, New Jersey.104
