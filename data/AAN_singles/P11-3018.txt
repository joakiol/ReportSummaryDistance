Proceedings of the ACL-HLT 2011 Student Session, pages 99?104,Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational LinguisticsPredicting Clicks in a Vocabulary Learning SystemAaron MichelonyBaskin School of EngineeringUniversity of California, Santa Cruz1156 High StreetSanta Cruz, CA 95060amichelo@soe.ucsc.eduAbstractWe consider the problem of predicting whichwords a student will click in a vocabularylearning system.
Often a language learnerwill find value in the ability to look up themeaning of an unknown word while readingan electronic document by clicking the word.Highlighting words likely to be unknown to areader is attractive due to drawing his or her at-tention to it and indicating that information isavailable.
However, this option is usually donemanually in vocabulary systems and onlineencyclopedias such as Wikipedia.
Furthur-more, it is never on a per-user basis.
This pa-per presents an automated way of highlight-ing words likely to be unknown to the specificuser.
We present related work in search engineranking, a description of the study used to col-lect click data, the experiment we performedusing the random forest machine learning al-gorithm and finish with a discussion of futurework.1 IntroductionWhen reading an article one occasionally encoun-ters an unknown word for which one would likethe definition.
For students learning or mastering alanguage, this can occur frequently.
Using a com-puterized learning system, it is possible to high-light words with which one would expect studentsto struggle.
The highlighting both draws attention tothe word and indicates that information about it isavailable.There are many applications of automaticallyhighlighting unknown words.
The first is, obviously,educational applications.
Another application is for-eign language acquisition.
Traditionally learners offoreign languages have had to look up unknownwords in a dictionary.
For reading on the computer,unknown words are generally entered into an onlinedictionary, which can be time-consuming.
The au-tomated highlighting of words could also be appliedin an online encyclopedia, such as Wikipedia.
Theproliferation of handheld computer devices for read-ing is another potential application, as some of theseuser interfaces may cause difficulty in the copyingand pasting of a word into a dictionary.
Given a fi-nite amount of resources available to improve defi-nitions for certain words, knowing which words arelikely to be clicked will help.
This can be used forcaching.In this paper, we explore applying machine learn-ing algorithms to classifying clicks in a vocabularylearning system.
The primary contribution of thiswork is to provide a list of features for machinelearning algorithms and their correlation with clicks.We analyze how the different features correlate withdifferent aspects of the vocabulary learning process.2 Related WorkThe previous work done in this area has mainly beenin the area of predicting clicks for web search rank-ing.
For search engine results, there have been sev-eral factors identified for why people click on cer-tain results over others.
One of the most impor-tant is position bias, which says that the presenta-tion order affects the probability of a user clickingon a result.
This is considered a ?fundamental prob-lem in click data?
(Craswell et al, 2008), and eye-99tracking experiments (Joachims et al, 2005) haveshown that click probability decays faster than ex-amination probability.There have been four hypotheses for how tomodel position bias:?
Baseline Hypothesis: There is no position bias.This may be useful for some applications but itdoes not fit with the data for how users click thetop results.?
Mixture Hypothesis: Users click based on rele-vance or at random.?
Examination Hypothesis: Each result has aprobability of being examined based on its po-sition and will be clicked if it is both examinedand relevant.?
Cascade Model: Users view search results fromtop to bottom and click on a result with a certainprobability.The cascade model has been shown to closely modelthe top-ranked results and the baseline model closelymatches how users click at lower-ranked results(Craswell et al, 2008).There has also been work done in predicting doc-ument keywords (Dog?an and Lu, 2010).
Their ap-proach is similar in that they use machine learningto recognize words that are important to a document.Our goals are complimentary, in that they are tryingto predict words that a user would use to search fora document and we are trying to predict words in adocument that a user would want more informationabout.
We revisit the comparison later in our discus-sion.3 Data DescriptionTo obtain click data, a study was conducted involv-ing middle-school students, of which 157 were inthe 7th grade and 17 were in the 8th grade.
90 stu-dents spoke Spanish as their primary language, 75spoke English as their primary language, 8 spokeother languages and 1 was unknown.
There were sixdocuments for which we obtained click data.
Eachdocument was either about science or was a fable.The science documents contained more advancedvocabulary whereas the fables were primarily writ-ten for English language learners.
In the study, thestudents took a vocabulary test, used the vocabu-lary system and then took another vocabulary testNumber Genre Words Students1 Science 2935 602 Science 2084 1383 Fable 667 234 Fable 513 225 Fable 397 166 Fable 105 5Table 1.
Document Informationwith the same words.
The highlighted words werechosen by a computer program using latent seman-tic analysis (Deerwester et al, 1990) and those re-sults were then manually edited by educators.
Thewords were highlighted identically for each student.Importantly, only nouns were highlighted and onlynouns were in the vocabulary test.
When the studentclicked on a highlighted word, they were shown def-initions for the word along with four images show-ing the word in context.
For example, if a studentclicked on the word ?crane?
which had the word?flying?
next to it, one of the images the studentwould see would be of a flying crane.
From Fig-ure 1 we see that there is a relation between the totalnumber of words in a document and the number ofclicks students made.0500100015002000250030000  0.05  0.1  0.15  0.2  0.25Document Length(Words)Ratio of Clicked Words to Highlighted WordsFigure 1.
Document Length Affects ClicksIt should be noted that there is a large class imbal-ance in the data.
For every click in document four,there are about 30 non-clicks.
The situation is evenmore imbalanced for the science documents.
For thesecond science document there are 100 non-clicksfor every click and for the first science documentthere are nearly 300 non-clicks for every click.100There was also no correlation seen between aword being on a quiz and being clicked.
This indi-cates that the students may not have used the systemas seriously as possible and introduced noise into theclick data.
This is further evidenced by the quizzes,which show that only about 10% of the quiz wordsthat students got wrong on the first test were actuallylearned.
However, we will show that we are able topredict clicks regardless.Figure 2, 3 and 4 show the relationship betweenthe mean age of acquisition of the words clicked on,STAR language scores and the number of clicks fordocument 2.
A second-degree polynomial was fit tothe data for each figure.
Students with STAR lan-guage scores above 300 are considered to have ba-sic ability, above 350 are proficient and above 400are advanced.
Age of acquisition scores are abstractand a score of 300 means a word was acquired at 4-6, 400 is 6-8 and 500 is 8-10 (Cortese and Fugett,2004).051015202530300  350  400  450  500ClicksMean Age of AcquisitionFigure 2.
Age of Acquisition vs Clicks4 Machine Learning MethodThe goal of our study is to predict student clicks ina vocabulary learning system.
We used the randomforest machine learning method, due to its success inthe Yahoo!
Learning to Rank Challenge (Chapelleand Chang, 2011).
This algorithm was tested usingthe Weka (Hall et al, 2009) machine learning soft-ware with the default settings.Random forest is an algorithm that classifies databy decision trees voting on a classification (Breiman,2001).
The forest chooses the class with the most051015202530250  300  350  400  450ClicksStar LanguageFigure 3.
STAR Language vs Clicks250300350400450300  350  400  450  500StarLanguageMean Age of AcquisitionFigure 4.
Age of Acquisition vs STAR Languagevotes.
Each tree in the forest is trained by first sam-pling a subset of the data, chosen randomly withreplacement, and then removing a large number offeatures.
The number of samples chosen is the samenumber as in the original dataset, which usually re-sults in about one-third of the original dataset leftout of the training set.
The tree is unpruned.
Ran-dom forest has the advantage that it does not overfitthe data.To implement this algorithm on our click data, weconstructed feature vectors consisting of both stu-dent features and word features.
Each word is eitherclicked or not clicked, so we were able to use a bi-nary classifier.1015 Evaluation5.1 FeaturesTo run our machine learning algorithms, we neededfeatures for them.
The features used are of twotypes: student features and word features.
The stu-dent features we used in our experiment were theSTAR (Standardized Testing and Reporting, a Cal-ifornia standardized test) language score and theCELDT (California English Language DevelopmentTest) overall score, which correlated highly witheach other.
There was a correlation of about -0.1between the STAR language score and total clicksacross all the documents.
Also available were theSTAR math score, CELDT reading, writing, speak-ing and listening scores, grade level and primary lan-guage.
These did not improve results and were notincluded in the experiment.We used and tested many word features, whichwere discovered to be more important than the stu-dent features.
First, we used the part-of-speech asa feature which was useful since only nouns werehighlighted in the study.
The part-of-speech taggerwe used was the Stanford Log-linear Part-of-SpeechTagger (Toutanova et al, 2003).
Second, variouspsycholinguistic variables were obtained from fivestudies (Wilson, 1988; Bird et al, 2001; Corteseand Fugett, 2004; Stadthagen-Gonzalez and Davis,2006; Cortese and Khanna, 2008).
The most use-ful was age of acquisition, which refers to ?the ageat which a word was learnt and has been proposedas a significant contributor to language and memoryprocesses?
(Stadthagen-Gonzalez and Davis, 2006).This was useful because it was available for the ma-jority of words and is a good proxy for the difficultyof a word.
Also useful was imageability, which is?the ease with which the word gives rise to a sen-sory mental image?
(Bird et al, 2001).
For ex-ample, these words are listed in decreasing orderof imageability: beach, vest, dirt, plea, equanimity.Third, we obtained the Google unigram frequencieswhich were also a proxy for the difficulty of a word.Fourth, we calculated click percentages for words,students and words, words in a document and spe-cific words in a document.
While these features cor-related very highly with clicks, we did not includethese in our experiment.
We instead would like tofocus on words for which we do not have click data.Fifth, the word position, which indicates the positionof the word in the document, was useful because po-sition bias was seen in our data.
Also important wasthe word instance, e.g.
whether the word is the first,second, third, etc.
time appearing in the document.After seeing a word three or four times, the clicksfor that word dropped off dramatically.There were also some other features that seemedinteresting but ultimately proved not useful.
Wegathered etymological data, such as the language oforigin and the date the word entered the English lan-guage; however these features did not help.
We werealso able to categorize the words using WordNet(Fellbaum, 1998), which can determine, for exam-ple, that a boat is an artifact and a lion is an animal.We tested for the categories of abstraction, artifact,living thing and animal but found no correlation be-tween clicks and these categories.5.2 Missing ValuesMany features were not available for every word inthe evaluation, such as age of acquisition.
We couldguess a value from available data, called imputation,or create separate models for each unique patternof missing features, called reduced-feature models.We decided to create reduced feature models due tothem being reported to consistently outperform im-putation (Saar-Tsechansky and Provost, 2007).5.3 Experimental Set-upWe ran our evaluation on document four, which hadclick data for 22 students.
We chose this docu-ment because it had the highest correlation betweena word being a quiz word and clicked, at 0.06, andthe correlation between the age of acquisition of aword and that word being a quiz word is high, at0.58.The algorithms were run with the following fea-tures: STAR language score, CELDT overall score,word position, word instance, document number,age of acquisition, imageability, Google frequency,stopword, and part-of-speech.
We did not includethe science text data as training data.
The trainingdata for a student consisted of his or her click datafor the other fables and all the other students?
clickdata for all the fables.1025.4 ResultsFrom Figure 2 we see the performance of randomforest.
We obtained similar performance with theother documents except document one.
We also notethat we also used a bayesian network and multi-boosting in Weka and obtained similar performanceto random forest.00.20.40.60.810  0.02  0.04  0.06  0.08  0.1  0.12  0.14TruepositiverateFalse positive rateRandom ForestFigure 5.
ROC Curve of Results6 DiscussionThere are several important issues to consider wheninterpreting these results.
First, we are trying tomaximize clicks when we should be trying to max-imize learning.
In the future we would like to iden-tify which clicks are more important than others andincorporate that into our model.
Second, across alldocuments of the study there was no correlation be-tween a word being on the quiz and being clicked.We would like to obtain click data from users ac-tively trying to learn and see how the results wouldbe affected and we speculate that the position biaseffect may be reduced in this case.
Third, this studyinvolved students who were using the system for thefirst time.
How these results translate to long-termuse of the program is unknown.The science texts are a challenge for the classifiersfor several reasons.
First, due to the relationship be-tween a document?s length and the number of clicks,there are relatively few words clicked.
Second, inthe study most of the more difficult words were nothighlighted.
This actually produced a slight negativecorrelation between age of acquisition and whetherthe word is a quiz word or not, whereas for the fa-ble documents there is a strong positive correlationbetween these two variables.
It raises the questionof how appropriate it is to include click data froma document with only one click out of 100 or 300non-clicks into the training set for a document withone click out of 30 non-clicks.
When the sciencedocuments were included in the training set for thefables, there was no difference in performance.The correlation between the word position andclicks is about -0.1.
This shows that position biasaffects vocabulary systems as well as search enginesand finding a good model to describe this is futurework.
The cascade model seems most appropri-ate, however the students tended to click in a non-linear order.
It remains to be seen whether this non-linearity holds for other populations of users.Previous work by Dog?an and Lu in predictingclick-words (Dog?an and Lu, 2010) built a learningsystem to predict click-words for documents in thefield of bioinformatics.
They claim that ?Our resultsshow that a word?s semantic type, location, POS,neighboring words and phrase information togethercould best determine if a word will be a click-word.
?They did report that if a word was in the title or ab-stract it was more likely to be a click-word, which issimilar to our finding that a word at the beginning ofthe document is more likely to be clicked.
However,it is not clear whether there is one underlying causefor both of these.
Certain features such as neigh-boring words do not seem applicable to our usage ingeneral, although it is something to be aware of forspecialized domains.
Their use of semantic typeswas interesting, though using WordNet we did notfind any preference for certain classes of nouns be-ing clicked over others.AcknowledgementsI would like to thank Yi Zhang for mentoring andproviding ideas.
I would also like to thank JudithScott, Kelly Stack, James Snook and other membersof the TecWave project.
I would also like to think theanonymous reviewers for their helpful comments.Part of this research is funded by National ScienceFoundation IIS-0713111 and the Institute of Educa-tion Science.
Any opinions, findings, conclusions orrecommendations expressed in this paper are thoseof the author, and do not necessarily reflect those ofthe sponsors.103ReferencesHelen Bird, Sue Franklin, and David Howard.
2001.
Ageof Acquisition and Imageability Ratings for a LargeSet of Words, Including Verbs and Function Words.Behavior Research Methods, Instruments, & Comput-ers, 33:73-79.Leo Breiman.
2001.
Random Forests.
Machine Learning45(1):5-32Olivier Chapelle and Yi Chang.
2011.
Yahoo!
Learningto Rank Challenge Overview.
JMLR: Workshop andConference Proceedings 14 1-24.Michael J. Cortese and April Fugett.
2004.
ImageabilityRatings for 3,000 Monosyllabic Words.
Behavior Re-search Methods, Instruments, and Computers, 36:384-387.Michael J. Cortese and Maya M. Khana.
2008.
Ageof Acquisition Ratings for 3,000 Monosyllabic Words.Behavior Research Methods, 40:791-794.Nick Craswell, Onno Zoeter, Michael Taylor, Bill Ram-sey.
2008.
An Experimental Comparison of ClickPosition-Bias Models.
First ACM International Con-ference on Web Search and Data Mining WSDM 2008.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, Richard Harshman.
1990.
In-dexing by Latent Semantic Analysis.
Journal of theAmerican Society for Information Science, 41(6):391-407.Rezarta I.
Dog?an and Zhiyong Lu.
2010.
Click-words:Learning to Predict Document Keywords from a UserPerspective.
Bioinformatics, 26, 2767-2775.Christine Fellbaum.
1998.
WordNet: An Electronic Lex-ical Database.
Bradford Books.Yoav Freund and Robert E. Shapire.
1995.
A Decision-Theoretic Generalization of on-Line Learning and anApplication to Boosting.
Journal of Computer andSystem Sciences, 55:119-139.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, Ian H. Witten 2009.The WEKA Data Mining Software: An Update.SIGKDD Explorations, Volume 11, Issue 1.Thorsten Joachims, Laura Granka, Bing Pan, HeleneHembrooke, Geri Gay.
2005.
Accurately Interpret-ing Clickthrough Data as Implicit Feedback.
Proceed-ings of the ACM Conference on Research and Devel-opment on Information Retrieval (SIGIR), 2005.Maytal Saar-Tsechansky and Foster Provost.
2007.Handling Missing Values when Applying ClassicationModels.
The Journal of Machine Learning Research,8:1625-1657.Hans Stadthagen-Gonzalez and Colin J. Davis.
2006.The Bristol Norms for Age of Acquisition, Imageabilityand Familiarity.
Behavior Research Methods, 38:598-605.Kristina Toutanova, Dan Klein, Christopher Manning,Yoram Singer.
2003.
Feature-Rich Part-of-SpeechTagging with a Cyclic Dependency Network.
Proceed-ings of HLT-NAACL 2003, 252-259.Michael D. Wilson.
1988.
The MRC Psycholinguis-tic Database: Machine Readable Dictionary, Version2.
Behavioural Research Methods, Instruments andComputers, 20(1):6-11.104
