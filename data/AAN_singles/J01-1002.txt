Integrating Prosodic and Lexical Cues forAutomatic Topic SegmentationG6khan Ttir*Bilkent UniversityAndreas  Stolcke tSRI InternationalDi lek Hakkani -Tt i r*Bilkent UniversityE l izabeth Shr iberg tSRI InternationalWe present aprobabilistic model that uses both prosodic and lexical cues for the automatic seg-mentation of speech into topically coherent units.
We propose two methods for combining lexicaland prosodic information using hidden Markov models and decision trees.
Lexical information isobtained from a speech recognizer, and prosodic features are extracted automatically from speechwaveforms.
We evaluate our approach on the Broadcast News corpus, using the DARPA-TDTevaluation metrics.
Results how that the prosodic model alone is competitive with word-basedsegmentation methods.
Furthermore, we achieve a significant reduction in error by combiningthe prosodic and word-based knowledge sources.1.
IntroductionTopic segmentation is the task of automatically dividing a stream of text or speech intotopically homogeneous blocks.
That is, given a sequence of (written or spoken) words,the aim of topic segmentation is to find the boundaries where topics change.
Figure 1gives an example of a topic change boundary from a broadcast news transcript.
Topicsegmentation is an important task for various language understanding applications,such as information extraction and retrieval, and text summarization.
In this paper,we present our work on automatic detection of topic boundaries from speech inputusing both prosodic and lexical information.Other automatic topic segmentation systems have focused on written text andhave depended mostly on lexical information.
This is problematic when segmentingspeech.
First, relying on word identities can propagate automatic speech recognizererrors to the topic segmenter.
Second, speech lacks typographic ues, as shown inFigure 1: there are no headers, paragraphs, entence punctuation marks, or capitalizedletters.
Speech itself, on the other hand, provides an additional, nonlexical knowledgesource through its durational, intonational, and energy characteristics, i.e., its prosody.Prosodic cues are known to be relevant o discourse structure in spontaneousspeech (cf.
Section 2.3) and can therefore be expected to play a role in indicating topictransitions.
Furthermore, prosodic cues, by their nature, are relatively unaffected byword identity, and should therefore improve the robustness of lexical topic segmenta-tion methods based on automatic speech recognition.?
Department of Computer Engineering, Bilkent University, Ankara, 06533, Turkey.
E-mail: {tur,hakkani}@cs.bilkent.edu.tr.
The research reported here was carried out while the authors wereInternational Fellows at SRI International.t Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA94025.
E-maih {stolcke,ees}@speech.sri.com.Computational Linguistics Volume 27, Number 1?
.. tens of thousands of people are homeless in northern china tonight after a powerfulearthquake hit an earthquake r gistering six point two on the richter scale at least fortyseven people are dead few pictures available from the region but we do know tem-peratures there will be very cold tonight minus seven degrees <TOPIC_CHANGE>peace talks expected to resume on monday in belfast northern ireland former u. s. sen-ator george mitchell is representing u. s. interests in the talks but it is another americancenter senator ather who was the focus of attention in northern ireland today here'sa.
b. c.'s richard gizbert he senator from america's best known irish catholic familyis in northern ireland today to talk about peace and reconciliation a peace processdoes not mean asking unionists or nationalists to change or discard their identity oraspirations...Figure 1An example of a topic boundary in a broadcast news transcript.Topic segmentation research based on prosodic information has generally reliedon hand-coded cues (with the notable exception of Hirschberg and Nakatani \[1998\]),or has not combined prosodic information with lexical cues (Litman and Passon-neau \[1995\] is one example where lexical information was combined with hand-codedprosodic features for a related task).
Therefore, the work presented here is the firstthat combines automatic extraction of both lexical and prosodic information for topicsegmentation.The general framework for combining lexical and prosodic cues for tagging speechwith various kinds of "hidden" structural information is a further development ofour earlier work on sentence segmentation and disfluency detection for spontaneousspeech (Shriberg, Bates, and Stolcke 1997; Stolcke and Shriberg 1996; Stolcke et al1998), conversational dialogue tagging (Stolcke et al 2000), and information extractionfrom broadcast news (Hakkani-T~ir et al 1999).In the next section, we review previous work on topic segmentation.
In Section 3,we describe our prosodic and language models as well as methods for combiningthem.
Section 4 reports our experimental procedures and results.
We close with somegeneral discussion (Section 5) and conclusions (Section 6).2.
Previous WorkWork on topic segmentation is generally based on two broad classes of cues.
On theone hand, one can exploit the fact that topics are correlated with topical content-wordusage, and that global shifts in word usage are indicative of changes in topic.
Quiteindependently, discourse cues, or linguistic devices such as discourse markers, cuephrases, syntactic onstructions, and prosodic signals are employed by speakers (orwriters) as generic indicators of endings or beginnings of topical segments.
Interest-ingly, most previous work has explored either one or the other type of cue, but onlyrarely both.
In automatic segmentation systems, word usage cues are often capturedby statistical language modeling and information retrieval techniques.
Discourse cues,on the other hand, are typically modeled with rule-based approaches or classifiersderived by machine learning techniques (such as decision trees).2.1 Approaches Based on Word UsageMost automatic topic segmentation work based on text sources has explored topicalword usage cues in one form or other.
Kozima (1993) used mutual similarity of wordsin a sequence of text as an indicator of text structure.
Reynar (1994) presented a methodthat finds topically similar regions in the text by graphically modeling the distribution32Ttir, Hakkani-Tiir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cuesof word repetitions.
The method of Hearst (1994, 1997) uses cosine similarity in a wordvector space as an indicator of topic similarity.More recently, the U.S. Defense Advanced Research Projects Agency (DARPA)initiated the Topic Detection and Tracking (TDT) program to further the state of theart in finding and following new topics in a stream of broadcast news stories.
Oneof the tasks in the TDT effort is segmenting a news stream into individual stories.Several of the participating systems rely essentially on word usage: Yamron et al(1998) model topics with unigram language models and their sequential structure withhidden Markov models (HMMs).
Ponte and Croft (1997) extract related word sets fortopic segments with the information retrieval technique of local context analysis, andthen compare the expanded word sets.2.2 Approaches Based on Discourse and Combined CuesPrevious work on both text and speech as found that cue phrases or discourse parti-cles (items uch as now or by the way), as well as other lexical cues, can provide valuableindicators of structural units in discourse (Grosz and Sidner 1986; Passonneau and Lit-man 1997, among others).In the TDT framework, the UMass HMM approach described in Allan et al (1998)uses an HMM that models the initial, middle, and final sentences of a topic segment,capitalizing on discourse cue words that indicate beginnings and ends of segments.Aligning the HMM to the data amounts to segmenting it.Beeferman, Berger, and Lafferty (1999) combined a large set of automatically se-lected lexical discourse cues in a maximum entropy model.
They also incorporatedtopical word usage into the model by building two statistical language models: onestatic (topic independent) and one that adapts its word predictions based on pastwords.
They showed that the log likelihood ratio of the two predictors behaves as anindicator of topic boundaries, and can thus be used as an additional feature in theexponential model classifier.2.3 Approaches Using Prosodic CuesProsodic cues form a subset of discourse cues in speech, reflecting systematic dura-tion, pitch, and energy patterns at topic changes and related locations of interest.
Alarge literature in linguistics and related fields has shown that topic boundaries (aswell as similar entities uch as paragraph boundaries in read speech, or discourse-level boundaries in spontaneous speech) are indicated prosodically in a manner thatis similar to sentence or utterance boundaries--only stronger.
Major shifts in topictypically show longer pauses, an extra-high F0 onset or "reset," a higher maximumaccent peak, greater ange in F0 and intensity (Brown, Currie, and Kenworthy 1980;Grosz and Hirschberg 1992; Nakajima and Allen 1993; Geluykens and Swerts 1993;Ayers 1994; Hirschberg and Nakatani 1996; Nakajima nd Tsukada 1997; Swerts 1997)and shifts in speaking rate (Brubaker 1972; Koopmans-van geinum and van Donzel1996; Hirschberg and Nakatani 1996).
Such cues are known to be salient for humanlisteners; in fact, subjects can perceive major discourse boundaries even if the speechitself is made unintelligible via spectral filtering (Swerts, Geluykens, and Terken 1992).Work in automatic extraction and computational modeling of these characteristicshas been more limited, with most of the work in computational prosody modelingdealing with boundaries at the sentence level or below.
However, there have beensome studies of discourse-level boundaries in a computational framework.
They differin various ways, such as type of data (monologue or dialogue, human-human orhuman-computer), type of features (prosodic and lexical versus prosodic only), whichfeatures are considered available (e.g., utterance boundaries or no boundaries), to33Computational Linguistics Volume 27, Number 1what extent features are automatically extractable and normalizable, and the machinelearning approach used.
Because of these vast difference, the overall results cannot becompared irectly to each other or to our work, but we describe three of the approachesbriefly here.An early study by Litman and Passonneau (1995) used hand-labeled prosodicboundaries and lexical information, but applied machine learning to a training corpusand tested on unseen data.
The researchers combined pause, duration, and hand-codedintonational boundary information with lexical information from cue phrases (such asand and so).
Additional knowledge sources included complex relations, such as coref-erence of noun phrases.
Work by Swerts and Ostendorf (1997) used prosodic featuresthat in principle could be extracted automatically, such as pitch range, to classify ut-terances from human-computer task-oriented dialogue into two categories: initial ornoninitial in the discourse segment.
The approach used CART-style decision trees tomodel the prosodic features, as well as various lexical features that, in principle, couldalso be estimated automatically.
In this case, utterances were presegmented, so the taskwas to classify segments rather than find boundaries in continuous peech; some of thefeatures included, such as type of boundary tone, may not be easy to extract robustlyacross speaking styles.
Finally, Hirschberg and Nakatani (1998) proposed a prosody-only front end for tasks such as audio browsing and playback, which could segmentcontinuous audio input into meaningful information units.
They used automaticallyextracted pitch, energy, and "other" features (such as the cross-correlation value usedby the pitch tracker in determining the estimate of F0) as inputs to CART-style trees,and aimed to predict major discourse-level boundaries.
They found various effects offrame window length and speakers, but concluded overall that prosodic cues couldbe useful for audio browsing applications.3.
The ApproachTopic segmentation i the paradigm used in this study and others (Allan et al 1998)proceeds in two phases.
In the first phase, the input is divided into contiguous tringsof words assumed to belong to the same topic.
We refer to this step as chopping.
For ex-ample, in textual input, the natural units for chopping are sentences (as can be inferredfrom punctuation and capitalization), since we can assume that topics do not change inmid sentence.
1 For continuous speech input, the choice of chopping criteria is less obvi-ous; we compare several possibilities in our experimental evaluation.
Here, for simplic-ity, we will use "sentence" to refer to units of chopping, regardless of the criterion used.In the second phase, the sentences are further grouped into contiguous tretchesbelonging to one topic, i.e., the sentence boundaries are classified into topic bound-aries and nontopic boundaries.
2 Topic segmentation is thus reduced to a boundaryclassification problem.
We will use B to denote the string of binary boundary classi-fications.
Furthermore, our two knowledge sources are the (chopped) word sequenceW and the stream of prosodic features F. Our approach aims to find the segmentationB with highest probability given the information in W and Fargmax P( BI W, F ) (1)Busing statistical modeling techniques.1 Similarly, it is sometimes assumed for topic segmentation purposes that topics change only atparagraph boundaries (Hearst 1997).2 We do not consider the problem of detecting recurring, discontinuous instances of the same topic, atask known as topic tracking in the TDT paradigm (Doddington 1998).34Ttir, Hakkani-Ttir, Stolcke, and Shriberg Integrating Prosodic and Lexical CuesIn the following subsections, we first describe the prosodic model of the depen-dency between prosody F and topic segmentation B; then, the language model relatingwords W and B; and finally, two approaches for combining the models.3.1 Prosodic ModelingThe job of the prosodic model is to estimate the posterior probability (or, alternatively,likelihood) of a topic change at a given word boundary, based on prosodic features ex-tracted from the data.
For the prosodic model to be effective, one must devise suitable,automatically extractable f atures.
Feature values extracted from a corpus can then beused in training probability estimators and to select a parsimonious subset of featuresfor modeling purposes.
We discuss each of these steps in turn in the following sections.3.1.1 Features.
We started with a large collection of features capturing two majoraspects of speech prosody, similar to our previous work (Shriberg, Bates, and Stolcke1997):Duration features: duration of pauses, duration of final vowels and finalrhymes, and versions of these features normalized both for phonedurations and speaker statistics.
3Pitch features: fundamental frequency (F0) patterns preceding andfollowing the boundary, F0 patterns across the boundary, and pitchrange relative to the speaker's baseline.
We processed the raw F0estimates (obtained with ESPS signal processing software from EntropicResearch Laboratory \[1993\]), with robustness-enhancing techniquesdeveloped by S6nmez et al (1998).We did not use amplitude- or energy-based features ince exploratory work showedthese to be much less reliable than duration and pitch and largely redundant given theabove features.
One reason for omitting energy features is that, unlike duration andpitch, energy-related measurements vary with channel characteristics.
Since channelproperties vary widely in broadcast news, features based on energy measures cancorrelate with shows, speakers, and so forth, rather than with the structural locationsin which we were interested.We included features that, based on the descriptive literature, should reflect breaksin the temporal and intonational contour.
We developed versions of such features thatcould be defined at each interword boundary, and that could be extracted by com-pletely automatic means (no human labeling).
Furthermore, the features were designedto be as independent of word identities as possible, for robustness to imperfect recog-nizer output.
A brief characterization f the informative features for the segmentationtask is given with our results in Section 4.6.
Since the focus here is on computationalmodeling we refer the reader to a companion paper (Shriberg et al 2000) for a detaileddescription of the acoustic processing and prosodic feature extraction.3.1.2 Decision Trees.
Any of a number of probabilistic lassifiers (such as neural net-works, exponential models, or naive Bayes networks) could be used as posterior prob-ability estimators.
As in past prosodic modeling work (Shriberg, Bates, and Stolcke1997), we chose CART-style decision trees (Breiman et al 1984), as implemented by3 The rhyme is the part of a syllable that comprises the nuclear phone (typically a vowel) and anyfollowing phones.
This is the part of the syllable most ypically affected by lengthening.35Computational Linguistics Volume 27, Number 1the IND package (Buntine and Caruana 1992), because of their ability to model featureinteractions, to deal with missing features, and to handle large amounts of trainingdata.
The foremost reason for our preference for decision trees, however, is that thelearned models can be inspected and diagnosed by human investigators.
This abilityis crucial for understanding what features are used and how, and for debugging thefeature xtraction process itself.
4Let Fi be the features extracted from a window around the ith potential topicboundary (chopping boundary), and let Bi be the boundary type (boundary/no-bound-ary) at that position.
We trained ecision trees to predict he ith boundary type, i.e., toestimate P(\]3ilFi, W).
The decision is only weakly conditioned on the word sequenceW, insofar as some of the prosodic features depend on the phonetic alignment of theword models (which we will denote with Wt).
We can thus expect he prosodic modelestimates to be robust o recognition errors.
The decision tree paradigm also allows usto add, and automatically select, other (nonprosodic) features that might be relevantto the task.3.1.3 Feature Selection.
The greedy nature of the decision tree learning algorithmimplies that larger initial feature sets can give worse results than smaller subsets.
Fur-thermore, it is desirable to remove redundant features for computational efficiencyand to simplify the interpretation f results.
For this purpose we developed an itera-rive feature selection "wrapper" algorithm (John, Kohavi, and Pfleger 1994) that findsuseful, task-specific feature subsets.
The algorithm combines elements of a brute-forcesearch with previously determined heuristics about good groupings of features.
Thealgorithm proceeds in two phases: In the first phase, the number of features i  reducedby leaving out one feature at a time during tree construction.
A feature whose removalincreases performance is marked as to be avoided.
The second phase then starts withthe reduced feature set and performs a beam search over all possible subsets to max-imize tree performance.We used entropy reduction in the overall tree (after cross-validation pruning) as ametric for comparing alternative f ature subsets.
Entropy reduction is the difference inentropy between the prior class distribution and the posterior distribution estimatedby the tree, as measured on a held-out set; it is a more fine-grained metric thanclassification accuracy, and is also more relevant to the model combination approachdescribed later.3.1.4 Training Data.
To train the prosodic model, we automatically aligned and ex-tracted features from 70 hours (about 700,000 words) of the Linguistic Data Consortium(LDC) 1997 Broadcast News (BN) corpus.
Topic boundary information determined byhuman labelers was extracted from the SGML markup that accompanies the wordtranscripts of this corpus.
The word transcripts were aligned automatically with theacoustic waveforms to obtain pause and duration information, using the SRI BroadcastNews recognizer (Sankar et al 1998).3.2 Lexical ModelingLexical information i  our topic segmenter is captured by statistical language models(LMs) embedded in an HMM.
The approach is an extension of the topic segmenter4 Interpreting large trees can be a daunting task.
However, the decision questions ear the tree root areusually interpretable, or, when nonsensical, usually indicate problems with the data.
Furthermore, asexplained inSection 4.6, we have developed simple statistics that give an overview of feature usagethroughout the tree.36Tfir, Hakkani-Tfir, Stolcke, and Shriberg Integrating Prosodic and Lexical CuesFigure 2Structure of the basic HMM developed by Dragon for the TDT Pilot Project.
The labels on thearrows indicate the transition probabilities.
TSP represents he topic switch penalty.developed by Dragon Systems for the TDT2 effort (Yamron et al 1998), which wasbased purely on topical word distributions.
We extend it to also capture lexical and(as described in Section 3.3) prosodic discourse cues.3.2.1 Model Structure.
The overall structure of the model is that of an HMM (Rabinerand Juang 1986) in which the states correspond to topic clusters Tj, and the obser-vations are sentences (or chopped units) W1 .
.
.
.
.
WN.
The resulting HMM, depictedin Figure 2, forms a complete graph, allowing for transitions between any two topicclusters.
Note that it is not necessary that the topic clusters correspond exactly to theactual topics to be located; for segmentation purposes, it is sufficient that two adjacentactual topics are unlikely to be mapped to the same induced cluster.
The observationlikelihoods for the HMM states, P(WilTj), represent the probability of generating agiven sentence Wi in a particular topic cluster Tj.We automatically constructed 100 topic cluster LMs, using the multipass k-meansalgorithm described in Yamron et al (1998).
Since the HMM emissions are meant omodel the topical usage of words, but not topic-specific syntactic structures, the LMs37Computational Linguistics Volume 27, Number 1consist of unigram distributions that exclude stopwords (high-frequency function andclosed-class words).
To account for unobserved words, we interpolate the topic-cluster-specific LMs with the global unigram LM obtained from the entire training data.
Theobservation likelihoods of the HMM states are then computed from these smoothedunigram LMs.All HMM transitions within the same topic cluster are given probability one,whereas all transitions between topics are set to a global topic switch penalty (TSP)that is optimized on held-out training data.
The TSP parameter allows trading offbetween false alarms and misses.
Once the HMM is trained, we use the Viterbi al-gorithm (Viterbi 1967; Rabiner and Juang 1986) to search for the best state sequenceand corresponding segmentation.
Note that the transition probabilities in the modelare not normalized to sum to one; this is convenient and permissible since the out-put of the Viterbi algorithm depends only on the relative weight of the transitionweights.We augmented the Dragon segmenter with additional states and transitions toalso capture lexical discourse cues.
In particular, we wanted to model the initial andfinal sentences in each topic segment, as these often contain formulaic phrases andkeywords used by broadcast speakers (From Washington, this is .
.
.
.
And  now .
.
.
).
Weadded two additional states, BEGIN and END, to the HMM (Figure 3) to model thesesentences.
Likelihoods for the BEGIN and END states are obtained as the unigramlanguage model probabilities of the initial and final sentences, respectively, of thetopic segments in the training data.
Note that a single BEGIN and END state areshared for all topics.
Best results were obtained by making traversal of these statesoptional in the HMM topology, presumably because some initial and final sentencesare better modeled by the topic-specific LMs.The resulting model thus effectively combines the Dragon and UMass HMM topicsegmentation approaches described in Allan et al (1998).
In preliminary experiments,we observed a 5% relative reduction in segmentation error with initial and final statesover the baseline HMM topology of Figure 2.
Therefore, all results reported later use anHMM topology with initial and final states.
Note that, since the topic-initial and topic-final states are optional, our training of the model is suboptimal.
Instead of labeling alltopic-initial and topic-final training sentences as data for the corresponding state, wewould expect further improvements by training the HMM in unsupervised fashionusing the Baum-Welch algorithm (Baum et al 1970; Rabiner and Juang 1986).3.2.2 Training Data.
Topic unigram language models were trained from the pooledTDT Pilot and TDT2 training data (Cieri et al 1999), covering transcriptions of broad-cast news from January 1992 through June 1994 and from January 1998 through Febru-ary 1998, respectively.
These corpora are similar in style, but do not overlap with the1997 LDC BN corpus from which we selected our prosodic training data and the eval-uaton test set.
For training the language models, we removed stories with fewer than300 and more than 3,000 words, leaving 19,916 stories with an average length of 538words (including stopwords).3.3 Model CombinationWe are now in a position to describe how lexical and prosodic information can becombined for topic segmentation.
As discussed before, the LMs in the HMM capturetopical word usage as well as lexical discourse cues at topic transitions, whereas adecision tree models prosodic discourse cues.
We expect hat these knowledge sourcesare largely independent, so their combination should yield significantly improvedperformance.38T~r, Hakkani-Ttir, Stolcke, and Shriberg Integrating Prosodic and Lexical CuesTSITSP1?TSPTSPTSPTSP1TSPTSPFigure 3Structure of an HMM with topic BEGIN and END states.
TSP represents he topic switchpenalty.Below we present wo approaches for building a combined statistical model thatperforms topic segmentation using all available knowledge sources.
For both ap-proaches it is convenient to associate a "boundary" pseudotoken with each potentialtopic boundary (i.e., with each sentence boundary).
Correspondingly, we introduce intothe HMM new states that emit these boundary tokens.
No other states emit boundarytokens; therefore ach sentence boundary must align with one of the boundary statesin the HMM.
As shown in Figure 4, there are two boundary states for each topiccluster, one representing a topic transition and the other representing a topic-internaltransition between sentences.
Unless otherwise noted, the observation likelihoods forthe boundary states are set to unity.The addition of boundary states allows us to compute the model's prediction oftopic changes as follows: Let B1,..
?, Bc denote the topic boundary states and, similarly,let N1,... ,  Nc denote the nontopic boundary states, where C is the number of topicclusters.
Using the forward-backward algorithm for HMMs (Rabiner and Juang 1986),we can compute P(qi = BflW) and P(qi = NjlW), the posterior probabilities that oneof these states is occupied at boundary i.
The model's prediction of a topic boundary39Computational Linguistics Volume 27, Number iTSPTSPTSPFigure 4Structure of the final HMM with fictitious boundary states used for combining language andprosodic models.
In the figure, states B1, B2, .
.
.
,  B100 represent the presence of a topicboundary, whereas tates N1, N2, .
.
.
,  N100 represent topic-internal sentence boundaries.
TSPis the topic switch penalty.is simply the sum over the corresponding state posteriors:cPHMM(Bi ~- yes\]W) = ~P(q i  = Bj lW) (2)j=lcPHMM(Bi = no lW ) = ~_~P(qi = N j lW)j=l= 1 - PHMM(Bi = yes\[W) (3)40Tiir, Hakkani-T(ir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues3.3.1 Model  Combination in the Decis ion Tree.
Decision trees allow the training ofa single classifier that takes both lexical and prosodic features as input, provided wecan compactly encode the lexical information for the decision tree.
We compute theposterior probability PHMM(Bi = yeslW) as shown above, to summarize the HMM'sbelief in a topic boundary based on all available lexical information W. The posteriorvalue is then used as an additional input feature to the prosodic decision tree, which istrained in the usual manner.
During testing, we declare a topic boundary whenever thetree's overall posterior estimate PDT(BilFi, W) exceeds ome threshold.
The thresholdmay be varied to trade off false alarms for miss errors, or to optimize an overall costfunction.Using HMM posteriors as decision tree features is similar in spirit to the knowl-edge source combination approaches used by Beeferman, Berger, and Lafferty (1999)and Reynar (1999), who also used the output of a topical word usage model as in-put to an overall classifier.
In previous work (Stolcke et al 1998) we used the presentapproach as one of the knowledge source combination strategies for sentence anddisfluency detection in spontaneous speech.3.3.2 Model  Combinat ion in the HMM.
An alternative approach to knowledge sourcecombination uses the HMM as the top-level model.
In this approach, the prosodicdecision tree is used to estimate likelihoods for the boundary states of the HMM, thusintegrating the prosodic evidence into the HMM's segmentation decisions.More formally, let Q =- (rl, ql .
.
.
.
.
ri, qi,...,rN, qN) be a state sequence throughthe HMM.
The model is constructed such that the states ri representing topic (orBEGIN/END) clusters alternate with the states qi representing boundary decisions.As in the baseline model, the likelihoods of the topic cluster states Tj account for thelexical observations:P(Wi\]ri ~- Tj) = P(WilTj) (4)as estimated by the unigram LMs.
Now, in addition, we let the likelihood of theboundary state at position i reflect he prosodic observation Fi.
Recall that, like Wi, Firefers to complete sentence units; specifically, Fi denotes the prosodic features of theith boundary between such units.P(Fi\]qi = Bj, W) = P(Fi\]Bi = yes, W)P(Fi\[qi = Nj, W) = P(FiIBi = no, W) j for all j = 1 .
.
.
.
, C (5)Using this construction, the product of all state likelihoods will give the overall ike-lihood, accounting for both lexical and prosodic observations:N N1-\[ P(Wilri) I I  P(Filqi, W) = P(W, FIQ ) (6)i=1 i=1Applying the Viterbi algorithm to the HMM will thus return the most likely segmen-tation conditioned on both words and prosody, which is our goal.Although decomposing the likelihoods as shown allows prosodic observations tobe conditioned on the words W, we use only the phonetic alignment information Wtfrom the word sequence W in our prosodic models, ignoring the word identities, soas to make them more robust o recognition errors.The likelihoods P(FilBi, Wt) for the boundary states can now be obtained from theprosodic decision tree.
Note that the decision tree estimates posteriors PDT(Bil\]2i, Wt).41Computational Linguistics Volume 27, Number 1These can be converted to likelihoods using Bayes rule as inP(Fi\[Bi, Wt) = P(FilWt)PDT (BilEG Wt)P(BiIWt) (7)The term P(FilWt) is a constant for all decisions Bi and can thus be ignored whenapplying the Viterbi algorithm.
Next, we approximate P(BilWt) ,~ P(Bi), justified by thefact that the Wt contains information about start and end times of phones and words,but not directly about word identities.
Instead of explicitly dividing the posteriors,1 we prefer to downsample the training set to make P(Bi = yes) = P(Bi = no) = ~.A beneficial side effect of this approach is that the decision tree models the lower-frequency events (topic boundaries) in greater detail than if presented with the raw,highly skewed class distribution.As is often the case when combining probabilistic models of different ypes, it isadvantageous to weight the contributions of the language models and the prosodictrees relative to each other.
We do so by introducing a tunable model combinationweight (MCW), and by using PDT(FilBi, Wt) MCW as the effective prosodic likelihoods.The value of MCW is optimized on held-out data.4.
Experiments and ResultsTo evaluate our topic segmentation models, we carried out experiments in the TDTparadigm.
We first describe our test data and the evaluation metrics used to comparemodel performance, then give the results we obtained with individual knowledgesources, followed by the results of the combined models.4.1 Test DataWe evaluated our system on three hours (6 shows, about 53,000 words) of the 1997LDC BN corpus.
The threshold for the model combination in the decision tree andthe topic switch penalty were optimized on the larger development training set of104 shows, which includes the prosodic model training data.
The MCW for the modelcombination in the HMM was optimized using a smaller held-out set of 10 shows ofabout 85,000 words total size, separate from the prosodic model training data.We used two test conditions: forced alignments using the true words, and recog-nized words as obtained by a simplified version of the SRI Broadcast News recognizer(Sankar et al 1998), with a word error rate of 30.5%.Our aim in these experiments was to use fully automatic recognition and pro-cessing wherever possible.
For practical reasons, we departed from this strategy intwo areas.
First, for word recognition, we used the acoustic waveform segmentationsprovided with the corpus (which also included the location of non_news material, suchas commercials and music).
Since current BN recognition systems perform this seg-mentation automatically with very good accuracy and with only a few percentagepoints penalty in word error rate (Sankar et al 1998), we felt the added complicationin experimental setup and evaluation was not justified.Second, for prosodic modeling, we used information from the corpus markupconcerning speaker changes and the identity of frequent speakers (e.g., news anchors).Automatic speaker segmentation and labeling is possible, although not without errors(Przybocki and Martin 1999).
Our use of speaker labels was motivated by the factthat meaningful prosodic features may require careful normalization by speaker, andunreliable speaker information would have made the analysis of prosodic featureusage much less meaningful.42Tfir, Hakkani-Ttir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues4.2 Evaluation MetricsWe have adopted the evaluation paradigm used by the TDT2--Topic Detection andTracking Phase 2 (Doddington 1998) program, allowing fair comparisons of variousapproaches both within this study and with respect to other ecent work.
Segmentationaccuracy was measured using TDT evaluation software from NIST, which implementsa variant of an evaluation metric suggested by Beeferman, Berger, and Lafferty (1999).The TDT segmentation metric is different from those used in most previous topicsegmentation work, and therefore merits some discussion.
It is designed to work ondata streams without any potential topic boundaries, uch as paragraph or sentenceboundaries, being given a priori.
It also gives proper partial credit to segmentationdecisions that are close to actual boundaries; for example, placing a boundary oneword from an actual boundary is considered a lesser error than if the hypothesizedboundary is off by, say, 100 words.The evaluation metric reflects the probability that two positions in the corpusprobed at random and separated by a distance of k words are correctly classified asbelonging to the same story or not.
If the two words belong to the same topic segment,but are erroneously claimed to be in different opic segments by the segmenter, thenthis will increase the system's false alarm probability.
Conversely, if the two words arein different opic segments, but are erroneously marked to be in the same segment,this will contribute to the miss probability.
The false alarm and miss rates are definedas averages over all possible probe positions with distance k.Formally, miss and false alarm rates are computed as 5PMis, = Y~s ~N--~lk d~yp (i, i + k) x (1 - dS~?f (i, i + k)) (8)E E Ns -k l ' '  ( i , i+k)) s i=1 , l -d~?
iv"N~-k  (1 ?
s ?
?
PFalseAla~m -- Y'~s A..M=I \ -- dShyp( l" i + k)) x d~ef(Z, ~ + k) (9)v-,N~-k as (i i + k) Es  Z.~i=l ~ref  \ "where the summation is over all broadcast shows s and word positions i in the testcorpus and whered sli,,={i if words i and j in show s are deemed by sys to be within the same storyotherwiseHere sys can be ref to denote the reference (correct) segmentation, or hyp to denote thesegmenter's decision.An analogous metric is defined for audio sources, where segmentation decisions(same or different topic) are probed at a time-based istance A:-T~ -A  -s -PMiss = GsJt=o ahyp(t,t+A) x (1-d~?f(t,t+A))dt (10)~'  ~G-A(1  s Jr=0 -dScd( t , t+A) )dtf  -a( .
PFa,,?A,a .... = Gsat=O ,l -d~yp( t , t+A) )  x d~?/(t , t+A)dt  (11)-G -A  -s  - ~s J,=o a~z(t,t+n)dt5 The definitions are those from Doddington (1998), but have been simplified and edited for clarity.43Computational Linguistics Volume 27, Number 1Table 1Segmentation error rates for various chopping criteria, using true words of the largerdevelopment data set.Chopping Criterion P Miss P FalseA larm C SegFIXED 0.5688 0.0639 0.2153TURN 0.6737 0.0436 0.2326SENTENCE 0.5469 0.0557 0.2030PAUSE 0.5111 0.0688 0.2002where the integration is over the entire duration of all stories of the shows in the testcorpus, and whereif times tl and t2 in show s are deemed by sys tobe within the same storyotherwiseWe used the same parameters as used in the official TDT2 evaluation: k = 50and A = 15 seconds.
Furthermore, again following NIST's evaluation procedure, wecombine miss and false alarm rates into a single segmentation cost metricCseg : CMiss X PMiss X P~eg + CFalseAlarm X PFalseAlarm ?
(1 - P~?9) (12)where the CMis~ = 1 is the cost of a miss, CFalseAlarm : 1 is the cost of a false alarm,and Pseg = 0.3 is the a priori probability of a segment being within an interval of kwords or A seconds on the TDT2 training corpus.
64.3 ChoppingUnlike written text, the output of the automatic speech recognizer contains no sentenceboundaries.
Therefore, chopping text into (pseudo)sentences is a nontrivial problemwhen processing speech.
Some presegmentation into roughly sentence-length units isnecessary since otherwise the observations associated with HMM states would com-prise too few words to give robust likelihoods of topic choice, causing poor perfor-mance.We investigated chopping criteria based on a fixed number of words (FIXED), atspeaker changes (TURN), at pauses (PAUSE), and, for reference, at actual sentenceboundaries (SENTENCE) obtained from the transcripts.
Table 1 gives the error ratesfor the four conditions, using the true word transcripts of the larger developmentdata set.
For the PAUSE condition, we empirically determined an optimal minimumpause duration threshold to use.
Specifically, we considered pauses exceeding 0.575of a second as potential topic boundaries in this (and all later) experiments.
For theFIXED condition, a block length of 10 words was found to work best.We conclude that a simple prosodic feature, pause duration, is an excellent criterionfor the chopping step, giving comparable or better performance than standard sentenceboundaries.
Therefore, we used pause duration as the chopping criterion in all furtherexperiments.6 Another parameter in the NIST evaluation is the deferral period, i.e., the amount of look-ahead beforea segmentation decision is made.
In all our experiments, we allowed unlimited deferral, effectivelyuntil the end of the news show being processed.44Tfir, Hakkani-Tiir, Stolcke, and Shriberg Integrating Prosodic and Lexical CuesTable 2Summary of error rates with the language model only (LM), the prosody model only (PM), thecombined ecision tree (CM-DT), and the combined HMM (CM-HMM).
(a) shows word-basederror metrics, 00) shows time-based error metrics.
In both cases a "chance" classifier that labelsall potential boundaries as nontopic would achieve 0.3 weighted segmentation cost.
(a) Error Rates on Forced Alignments Error Rates on Recognized WordsModel PMiss PFalseAlarm Cseg PMiss PFalseAlarm CsegChance 1.0 0.0 0.3 1.0 0.0 0.3LM 0.4847 0.0630 0.1895 0.4978 0.0577 0.1897PM 0.4130 0.0596 0.1657 0.4125 0.0705 0.1731CM-DT 0.4677 0.0260 0.1585 0.4891 0.0146 0.1569CM-HMM 0.3339 0.0536 0.1377 0.3748 0.0450 0.1438(b) Error Rates on Forced Alignments Error Rates on Recognized WordsModel  PMiss PFalseAlarm Cseg PMiss PFalseAlarm CsegChance 1.0 0.0 0.3 1.0 0.0 0.3LM 0.5260 0.0490 0.1921 0.5361 0.0415 0.1899PM 0.3503 0.0892 0.1675 0.3846 0.0737 0.1669CM-DT 0.5136 0.0210 0.1688 0.5426 0.0125 0.1715CM-HMM 0.3426 0.0496 0.1375 0.3746 0.0475 0.14564.4 Source-Specific Model TuningAs mentioned earlier, the segmentation models contain global parameters (the topictransition penalty of the HMM and the posterior threshold for the combined ecisiontree) to trade false alarms for miss errors.
Optimal settings for these parameters dependon characteristics of the source, in particular on the relative frequency of topic changes.Since broadcast news programs come from identified sources, it is useful and legitimateto optimize these parameters for each show type .
7 We therefore optimized the globalparameter for each model to minimize the segmentation cost on the training corpus(after training all other model parameters in a source-independent fashion).Compared to a baseline using source-independent global TSP and threshold, thesource-dependent models showed between 5% and 10% relative error reduction.
Allresults reported below use the source-dependent approach.4.5 Segmentation ResultsTable 2 shows the results for both individual knowledge sources (words and prosody),as well as for the combined models (decision tree and HMM).
It is worth notingthat the prosody-only results were obtained by running the combined HMM withoutlanguage model likelihoods; this approach gave better performance than using theprosodic decision trees directly as classifiers.Both word- and time-based metrics are given; they exhibit generally very similarresults.
Another dimension of the evaluation is the use of correct word transcripts(forced alignments) versus automatically recognized words.
Again, results along thisdimension are very similar, with some exceptions noted below.Comparing the individual knowledge sources, we observe that prosody alone doessomewhat better than the word-based HMM alone.
The types of errors made differ7 Shows in the 1997 BN corpus come from eight sources: ABC World News Tonight, CNN HeadlineNews, CNN Early Prime, PRI The World, CNN Prime News, CNN The World Today, C-SPAN PublicPolicy, and C-SPAN Washington Journal.
Six of these occurred in the test set.45Computational Linguistics Volume 27, Number 1consistently: the prosodic model has a higher false alarm rate, while the word-LMshave more miss errors.
The prosodic model shows more false alarms because regularsentence boundaries often show characteristics similar to those of topic boundaries.
Italso suggests that both models could be combined by letting the prosodic model selectcandidate topic boundaries that would then be filtered using lexical information.The combined models generally improve on the individual knowledge sources,In the word-based evaluation, the combined decision tree (DT) reduced overall seg-mentation cost by 19% over the language model on true words (17% on recognizedwords).
The combined HMM gave even better esults: 27% and 24% improvement inthe error rate over the language model for true and recognized words, respectively.Looking again at the breakdown of errors, we can see that the two model combina-tion approaches work quite differently: the combined DT has about the same miss rateas the LM, but a lower false alarms rate.
The combined HMM, by contrast, combinesa miss rate as low as (or lower than) that of the prosodic model with the lower falsealarm rate of the LM, suggesting that the functions of the two knowledge sources arecomplementary, as discussed above.
Furthermore, the different error patterns of thetwo combination approaches suggest hat further error reductions could be achievedby combining the two hybrid models.
9The trade-off between false alarms and miss probabilities is shown in more de-tail in Figure 5, which plots the two error metrics against each other.
Note that thefalse alarm rate does not reach one because the segmenter is constrained by the chop-ping algorithm: the pause criterion prevents the segmenter f om hypothesizing topicboundaries everywhere.4.6 Decision Tree for the Prosody-Only ModelFeature subset selection was run with an initial set of 73 potential features, which thealgorithm reduced to a set of 7 nonredundant features helpful for the topic segmen-tation task.
The full decision tree learned is shown in Figure 6.
We can identify fourdifferent kinds of features used in the tree, listed below.
For each feature type, we givethe feature names found in the tree and the relative feature usage, an approximatemeasure of feature importance (Shriberg, Bates, and Stolcke 1997).
Relative featureusage is computed as the relative frequency with which features of a given type arequeried in the tree, over a held-out est set...Pause duration (PhU_DUR, 42.7% usage).
This feature is the duration ofthe nonspeech interval occurring at the boundary.
The importance ofpause duration is underestimated here because, as explained earlier,pause durations are already used during the chopping process, so thatthe decision tree is applied only to boundaries exceeding a certainduration.
Separate xperiments using boundaries below our choppingthreshold show that the tree also distinguishes shorter pause durationsfor segmentation decisions.FO differences across the boundary (FOK_LRd~EAN_KBASELN andFOK_WRD_DIFF_MNMI~_NG, 35.9% usage).
These features compare the mean8 The exception is the time-based evaluation of the combined ecision tree.
We found that the posteriorprobability threshold optimized on the training set works poorly on the test set for this modelarchitecture and the time-based evaluation.
The threshold that is optimal on the test set achievesCsea = 0.1651.
Section 4.7 gives a possible xplanation for this result.9 Such a combination ofcombined models was suggested by one of the reviewers; we hope to pursue itin future research.46Ti.ir, Hakkani-Ttir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues.~..~,.Q<r.,g30.60.40.20.0* * LM\[\] \[\] PMa ~ CM-DT: CM-HMM0.2 0.4 0.6 0.8 1.0M iss  P robab i l i tyFigure 5False alarm versus miss probabilities (word-based metrics) for automatic topic segmentationfrom known words (forced alignments).
The segmenters used were a words-only HMM (LM),a prosody-only HMM (PM), a combined ecision tree (CM-DT), and a combined HMM(CM-HMM)..F0 of the word preceding the boundary (measured from voiced regionswithin that word) to either the speaker's estimated baseline F0(FOK_LR_MEAN_KBASELN) or to the mean F0 of the word following theboundary (FOK_WRD_DIFF_.MNMN_N).
Both features were computed based ona log-normal scaling of F0.
Other measures (such as minimum ormaximum F0 in the word or preceding window) as well as othernormalizations (based on F0 toplines, or non-log-based scalings) wereincluded in the initial feature set, but were not selected in thebest-performing tree.
The baseline feature captures a pitch range effect,and is useful at boundaries where the speaker changes (since range hereis compared only within-speaker).
The second feature captures therelative size of the pitch change at the boundary, but of course is notmeaningful at speaker boundaries.Turn features (TURN_F and TURN_TIME, 14.6% usage).
These featuresreflect he change of speakers.
TURN_F indicates whether a speaker47Computational Linguistics Volume 27, Number 1/zv~azz zviO48Tar, Hakkani-Ti~r, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues.change occurred at the boundary, while TURN_TIME measures the timepassed since the start of the current urn.Gender (GEN, 6.8% usage).
This feature indicates the speaker genderright before a potential boundary.Inspection of the tree reveals that the purely prosodic features (pause duration andF0 differences) are used as the prosody literature suggests.
The longer the observedpause, the more likely a boundary corresponds to a topic change.
Also, the closer aspeaker comes to his or her F0 baseline, or the larger the difference to the F0 followinga boundary, the more likely a topic change occurs.
These features thus correspondto the well-known phenomena of boundary tones and pitch reset that are generallyassociated with sentence boundaries (Vaissi6re 1983).
We found these indicators ofsentences boundaries to be particularly pronounced at topic boundaries.While turn and gender features are not prosodic features per se, they do interactclosely with them since prosodic measurements must be informed by and carefullynormalized for speaker identity and gender, and it is therefore natural to include themin a prosodic lassifier.
1?Not surprisingly, we find that turn boundaries are positivelycorrelated with topic boundaries, and that topic changes become more likely the longera turn has been going on.Interestingly, speaker gender is used by the decision tree for several reasons.
Onereason is stylistic differences between males and females in the use of F0 at topicboundaries.
This is true even after proper normalization, e.g., equatIng the gender-specific nontopic boundary distributions.
In addition, we found that nontopic pauses(i.e., chopping boundaries) are more likely to occur in male speech.
It could be thatmale speakers in BN are assigned longer topic segments on average, or that malespeakers are more prone to pausing in general, or that male speakers dominate thespontaneous speech portions, where pausing is naturally more frequent.
The detailsof this gender effect await further study.4.7 Decision Tree for the Combined ModelFigure 7 depicts the decision tree that combines the HMM language model topic deci-sions with prosodic features (see Section 3.3.1).
Again, we list the features used withtheir relative feature usages.1.
Language model posterior (POST_TOPIC, 49.3% usage).
This is theposterior probability P(Bi = yeslW) computed from the HMM.2.
Pause duration (PAU_DUR, 49.3% usage).
This feature is the same asdescribed for the prosody-only model.3.
FO differences across the boundary (FOK_WRD_DIFF_HILO_N andFOK_LR_MEAN_KBASELN, 1.4% usage).
These features are similar to thosefound for the prosody-only tree.
The only difference is that for the firstfeature, the comparison of FO values across the boundary is done bytaking the max imum FO of the previous word and the min imum FO ofthe following word, rather than the mean for both cases.10 For example, the features that measure F0 differences across boundaries do not make sense if thespeaker changes atthe boundary.
Accordingly, we made such features undefined for the decision treeat turn boundaries.49Computational Linguistics Volume 27, Number 1POST_TOPIC >= -0.083984r PAU_DUR >= 1058.3 PAU_DUR <82.5 ~PAU_DUR >= 82.5KBASELN < 0 .1977~R MEAN KBASELN >= 0.19777~_WRD_DIFF_HILO_N <-0 .02498~RD DIFF HILO N >=-0.024989Figure 7The decision tree of the combination model.The decision tree found for the combined task is smaller and uses fewer featuresthan the one trained with prosodic features only, for two reasons.
First, the LM poste-rior feature is found to be highly informative, superseding the selection of many of thelow-frequency features previously found.
Furthermore, as explained in Section 3.3.2,the prosody-only tree was trained on a downsampled dataset that equalizes the priorsfor topic and nontopic boundaries, as required for integration i to the HMM.
A wel-come side effect of this procedure is that it forces the tree to model the less frequentclass (topic boundaries) in much greater detail than if the tree were trained on the rawclass distribution, as is the case here.Because of its small size, the tree in Figure 7 is particularly easy to interpret.
Thetop-level split is based on the LM posterior.
The right branch handles cases wherewords are highly indicative of a topic boundary.
However, for short pauses, the treequeries further prosodic features to prevent false alarms.
Specifically, short pausesmust be accompanied both by an F0 close to the speaker's baseline and by a largeF0 reset o be deemed topic boundaries.
Conversely, if the LM posteriors are low (lefttop-level branch), but the pause is very long, the tree still outputs a topic boundary.4.8 Comparison of Model Combination ApproachesResults indicate that the model combination approach using an HMM as the top-levelmodel works better than the combined ecision tree.
While this result deserves moreinvestigation, we can offer some preliminary insights.We found it difficult to set the posterior probability thresholds for the combineddecision tree in a robust way.
As shown by the CM-DT curve in Figure 5, there is alarge jump in the false alarm/miss trade-off for the combined tree, in contrast to thecombined HMM approach, which controls the trade-off by a changing topic switchpenalty.
This occurs because posterior probabilities from the decision tree do not varysmoothly; rather, they vary in steps corresponding to the leaves of the tree.
The dis-50Tfir, Hakkani-Tiir, Stolcke, and Shriberg Integrating Prosodic and Lexical CuesTable 3Segmentation error rates with the language model only (LM), the combined HMM using allprosodic features (CM-HMM-all), the combined HMM using only pause duration and turnfeatures (CM-HMM-pause-turn), and using only pause duration, turn, and gender features(CM-HMM-pause-turn-gender).Model C~exLM 0.1895CM-HMM-pause-turn 0.1519CM-HMM-pause-turn-gender 0.1511CM-HMM-all 0.1377continuous character of the thresholded variable makes it hard to estimate a thresholdon the training data that performs robustly on the test data.
This could account forthe poor result on the time-based metrics for the combined tree (where the thresholdoptimized on the training data was far from optimal on the test set; see footnote 8).The same phenomenon is reflected in the fact that the prosody-only tree gave betterresults when embedded in an HMM without LM likelihoods than when used by itselfwith a posterior threshold.4.9 Contributions of Different Feature TypesWe saw in Section 4.6 that pause duration is by far the single most important feature inthe prosodic decision tree.
Furthermore, speaker changes are queried almost as oftenas the F0-related features.
Pause durations can be obtained using standard speech rec-ognizers, and are in fact used by many current TDT systems (see Section 4.10).
Speakerchanges are not prosodic features per se, and would be detected independently fromthe prosodic features proper.
To determine if prosodic measurements beyond pauseand speaker information improve topic segmentation accuracy, we tested systems thatconsisted of the HMM with the usual topic LMs, plus a decision tree that had ac-cess only to various subsets of pause- and speaker-related features, without usingany of the F0-based features.
Decision tree and HMM were combined as described inSection 3.3.2.Table 3 shows the results of the system using only topic language models (LM)as well as combined systems using all prosodic features (CM-HMM-all), only pauseduration and turn features (CM-HMM-pause-turn), and using only pause duration,turn, and gender features (CM-HMM-pause-turn-gender).
These results how that byusing only pause duration, turn, and gender features, it is indeed possible to obtainbetter esults (20% reduced segmentation cost) than with the lexical model alone, withgender making only a minor contribution.
However, we also see that a substantialfurther improvement (9% relative) is obtained by adding F0 features to the prosodicmodel.4.10 Results Compared to Other ApproachesBecause our work focused on the use of prosodic information and required detailedlinguistic annotations ( uch as sentence punctuation, turn boundaries, and speakerlabels), we used data from the LDC 1997 BN corpus to form the training set for theprosodic models and the (separate) test set used for evaluation.
This choice was crucialfor the research, but unfortunately complicates a quantitative comparison ofour resultsto other TDT segmentation systems.
The recent TDT2 evaluation used a different setof broadcast news data that postdated the material we used, and was generated bya different speech recognizer (although with a similar word error rate) (Cieri et al51Computational Linguistics Volume 27, Number 1Table 4Word-based segmentation error rates for different corpora.
Note that a hand-transcribed(forced alignment) version of the TDT2 test set was not available.Error Rates on Forced Alignments Error Rates on Recognized WordsTest Set PMis~ PFalseAlarm Cseg PMiss PFalseAlarrn CsegTDT2 NA NA NA 0.5509 0.0694 0.2139BN'97 0.4685 0.0817 0.1978 0.5128 0.0683 0.20171999).
Nevertheless we have attempted to calibrate our results with respect o theseTDT2 results, n We have not tried to compare our results to research outside the TDTevaluation framework.
In fact, other evaluation methodologies differ too much to allowmeaningful quantitative comparisons across publications.We wanted to ensure that the TDT2 evaluation test set was comparable in seg-mentation difficulty to our test set drawn from the 1997 BN corpus, and that the TDT2metrics behaved similarly on both sets.
To this end, we ran an early version of ourwords-only segmenter on both test sets.
As shown in Table 4, not only are the resultson recognized words quite close, but the optimal false alarm/miss trade-off is similaras well, indicating that the two corpora have roughly similar topic granularities.While the full prosodic omponent of our topic segmenter was not applied to theTDT2 test corpus, we can compare the performance of a simplified version of SRI'ssegmenter to other evaluation systems (Fiscus et al 1999).
The two best-performingsystems in the evaluation were those of CMU (Beeferman, Berger, and Lafferty 1999)with Cse9 = 0.1463, and Dragon (Yamron et al 1998; van Mulbregt et al 1999) withCse9 = 0.1579.
The SRI system achieved Cs~g = 0.1895.
All systems in the evaluation,including ours, used only information from words and pause durations determinedby a speech recognizer.A good reference to calibrate our performance is the Dragon system, from whichwe borrowed the lexical HMM segmentation framework.
Dragon made adjustmentsin its lexical modeling that account for the improvements relative to the basic HMMstructure on which our system is based.
As described by van Mulbregt et al (1999),a significant segmentation error reduction was obtained from optimizing the numberof topic clusters (kept fixed at 100 in our system).
Second, Dragon introduced moresupervision into the model training by building separate LMs for segments that hadbeen hand-labeled asnot related to news (such as sports and commercials) in the TDT2training corpus, which also resulted in substantial improvements.
Finally, Dragon usedsome of the TDT2 training data for tuning the model to the specifics of the TDT2corpus.In summary, the performance ofour combined lexical-prosodic system with Cs?9 =0.1438 is competitive with the best word-based systems reported to date.
More impor-tantly, since we found the prosodic and lexical knowledge sources to complement eachother, and since Dragon's improvements for TDT2 were confined to a better modelingof the lexical information, we would expect hat adding these improvements o ourcombined segmenter would lead to a significant improvement in the state of the art.11 Since our study was conducted, a third round of TDT benchmarks (TDT3) has taken place (NIST 1999).However, for TDT3, the topic segmentation evaluation metric was modified and the most recent resultsare thus not directly comparable with those from TDT2 or the present study,52Tiir, Hakkani-Ti.ir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues5.
DiscussionResults so far indicate that prosodic information provides an excellent source of in-formation for automatic topic segmentation, both by itself and in conjunction withlexical information.
Pause duration, a simple prosodic feature that is readily availableas a by-product of speech recognition, proved highly effective in the initial choppingphase, and was the most important feature used by prosodic decision trees.
Additional,pitch-based prosodic features are also effective as features in the decision tree.The results obtained with recognized words (at 30% word error rate) did not differgreatly from those obtained with correct word transcripts.
No significant degradationwas found with the words-only segmentation model, while the best combined modelexhibited about a 5% error increase with recognized words.
The lack of degradationon the words-only model may be partly due to the fact that the recognizer generallyoutputs fewer words than contained in the correct ranscripts, biasing the segmentertoward a lower false alarm rate.
Still, part of the appeal of prosodic segmentation isthat it is inherently robust o recognition errors.
This characteristic makes it even moreattractive for use in domains with higher error rates due to poor acoustic onditions ormore conversational speaking styles.
It is especially encouraging that the prosody-onlysegmenter achieved competitive performance.It was fairly straightforward tomodify the original Dragon HMM segmenter (Yam-ron et al 1998), which is based purely on topical word usage, to incorporate discoursecues, both lexical and prosodic.
The addition of these discourse cues proved highlyeffective, especially in the case of prosody.
The alternative knowledge source combi-nation approach, using HMM posterior probabilities as decision tree inputs, was alsoeffective, although less so than the HMM-based approach.
Note that the HMM-basedintegration, as implemented here, makes more stringent assumptions about the in-dependence of lexical and prosodic cues.
The combined decision tree, on the otherhand, has some ability to model dependencies between lexical and prosodic cues.
Thefact that the HMM-based combination approach gave the best results is thus indirectevidence that lexical and prosodic knowledge sources are indeed largely independent.Apart from the question of probabilistic independence, it seems that lexical andprosodic models are also complementary in the errors they make.
This is manifestedin the different distributions of miss and false alarm errors discussed in Section 4.5.It is also easy to find examples where the two models make complementary errors.Figure 8 shows two topic boundaries that are missed by one model but not the other.Several aspects of our model are preliminary or suboptimal in nature and can beimproved.
Even when testing on recognized words, we used parameters optimizedon forced alignments.
This is suboptimal but convenient, since it avoids the need torun word recognition on the relatively large training set.
Since results on recognizedwords are very similar to those on true words, we can conclude that not much was lostwith this expedient.
Also, we have not yet optimized the chopping stage relative tothe combined model (only relative to the words-only segmenter).
The use of prosodicfeatures other than pause duration for chopping should further improve the overallperformance.The improvement obtained with source-dependent topic switch penalties andposterior thresholds uggests that more comprehensive source-dependent modelingwould be beneficial.
In particular, both prosodic and lexical discourse cues are likelyto be somewhat source specific (e.g., because of different show formats and differentspeakers).
Given enough training data, it is straightforward to train source-dependentmodels.53Computational Linguistics Volume 27, Number 1(a) ?..
we have a severe thunderstorm watch two severe thunderstorm watchesand a tornado watch in effect he tornado watch in effect back here in easterncolorado the two severe thunderstorm watches here indiana over into ohiothose obviously associated with this line which is already been producingsome hail i'll be back in a moment we'll take a look at our forecast weathermap see if we can cool it off in the east will be very cold tonight minus sevendegrees <TOPIC_CHANGE>LM probability: 0.018713PM probability: 0.937276karen just walked in was in the computer and found out for me that nationalairport in washington d. c. did hit one hundred degrees today it's a recordhigh for them it's going to be uh hot again tomorrow but it will begin tocool off the que question is what time of day is this cold front going to moveby your house if you want to know how warm it's going to be tomorrowcomes through early in the day won't be that hot at all midday it'll still beinto the nineties but not as hot as it was today comes through late in the dayyou'll still be in the upper nineties but some relief is on the way ...(b) ... you know the if if the president has been unfaithful to his wife and atthis point you know i simply don't know any of the facts other than thebits and pieces that we hear and they're simply allegations at this point butbeing unfaithful to your wife isn't necessarily a crime lying in an affidavit isa crime inducing someone to lie in an affidavit is a crime but that occurredafter this apparent aping so i'll tell you there are going to be extremelythorny legal issues that will have to be sorted out white house spokesmanmike mccurry says the administration will cooperate in starr's investigation<TOPIC_CHANGE>LM probability: 1?000000PM probability: 0.134409cubans have been waiting for this day for a long time after months of plan-ning and preparation pope john paul the second will make his first visit tothe island nation this afternoon it is the first pilgrimage ver by a pope tocuba judy fortin joins us now from havana with more .
.
.
.Figure 8Examples of true topic boundaries where lexical and prosodic models make oppositedecisions?
(a) The prosodic model correctly predicts a topic change, the LM does not.
(b) TheLM predicts a topic change, the prosodic model does not.6.
Conc lus ionWe have presented a probabil istic approach to topic segmentat ion of speech, combin ingboth lexical and prosodic cues.
Topical word  usage and lexical discourse cues arerepresented by language models  embedded in an HMM.
Prosodic discourse cues,such as pause durat ions and pitch resets, are mode led  by a decision tree based onautomatical ly extracted acoustic features and al ignments.
Lexical and prosodic featurescan be combined either in the HMM or in the decision tree f ramework.Our  topic segmentat ion model  was evaluated on broadcast  news speech, andfound to give competit ive per formance (around 14% error according to the weightedTDT2 segmentat ion cost metric).
Notably, the segmentat ion accuracy of the prosodic54Tier, Hakkani-T(ir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cuesmodel alone is competit ive with a word-based segmenter, and a combined prosodic /lexical HMM achieves a substantial error reduction over the individual knowledgesources.AcknowledgmentsWe thank Becky Bates, Madelaine PlauchGZe'ev Rivlin, Ananth Sankar, and KemalS6nmez for invaluable assistance inpreparing the data for this study.
The paperwas greatly improved as a result ofcomments by Andy Kehler, MadelainePlauch6, and the anonymous reviewers.This research was supported by DARPAand NSF under NSF grant IRI-9619921 andDARPA contract no.
N66001-97-C-8544.
Theviews herein are those of the authors andshould not be interpreted as representingthe policies of the funding agencies.ReferencesAllan, J., J. Carbonell, G. Doddington,J.
Yamron, and Y. Yang.
1998.
Topicdetection and tracking pilot study: Finalreport.
In Proceedings ofDARPA BroadcastNews Transcription and UnderstandingWorkshop, pages 194-218, Lansdowne, VA,February.
Morgan Kaufmann.Ayers, Gayle M. 1994.
Discourse functionsof pitch range in spontaneous and readspeech.
In Working Papers in LinguisticsNo.
44.
Ohio State University, pages 1-49.Baum, Leonard E., Ted Petrie, GeorgeSoules, and Norman Weiss.
1970.
Amaximization technique occurring in thestatistical analysis of probabilisticfunctions in Markov chains.
The Annals ofMathematical Statistics, 41(1):164-171.Beeferman, Doug, Adam Berger, and JohnLafferty.
1999.
Statistical models for textsegmentation.
Machine Learning,34(1-3):177-210.
Special Issue on NaturalLanguage Learning.Breiman, L., J. H. Friedman, R. A. Olshen,and C. J.
Stone.
1984.
Classification andRegression Trees.
Wadsworth and Brooks,Pacific Grove, CA.Brown, G., K. L. Currie, and J. Kenworthy.1980.
Questions of Intonation.
UniversityPark Press, Baltimore.Brubaker, R. S. 1972.
Rate and pausecharacteristics of oral reading.
Journal ofPsycholinguistic Research, 1:141-147.Buntine, Wray and Rich Caruana, 1992.Introduction to IND Version 2.1 and RecursivePartitioning.
NASA Ames ResearchCenter, Moffett Field, CA, December.Cieri, Chris, David Graft, Mark Liberman,Nii Martey, and Stephanie Strassell.
1999.The TDT-2 text and speech corpus.
InProceedings ofDARPA Broadcast NewsWorkshop, pages 57-60, Herndon, VA,February.
Morgan Kaufmann.Doddington, George.
1998.
The TopicDetection and Tracking Phase 2 (TDT2)evaluation plan.
In Proceedings ofDARPABroadcast News Transcription andUnderstanding Workshop, pages 223-229,Lansdowne, VA, February.
MorganKaufmann.
Revised version availablefrom http://www.nist.gov/speech/tests/tdt/tdt98/.Entropic Research Laboratory, 1993.
ESPSVersion 5.0 Programs Manual.
Washington,D.C.
August.Fiscus, Jon, George Doddington, JohnGarofolo, and Alvin Martin.
1999.
NIST's1998 Topic Detection and Trackingevaluation (TDT2).
In Proceedings ofDARPA Broadcast News Workshop,pages 19-24, Herndon, VA, February.Morgan Kaufmann.Geluykens, R. and M. Swerts.
1993.
Localand global prosodic ues to discourseorganization i  dialogues.
In WorkingPapers 41, Proceedings ofESCA Workshop onProsody, pages 108-111, Lurid, Sweden.Grosz, B. and J. Hirschberg.
1992.
Someintonational characteristics of discoursestructure.
In John J. Ohala, Terrance M.Nearey, Bruce L. Derwing, Megan M.Hodge, and Grace E. Wiebe, editors,Proceedings ofthe International Conference onSpoken Language Processing, volume 1,pages 429432, Banff, Canada, October.Grosz, B. and C. Sidner.
1986.
Attention,intention, and the structure of discourse.Computational Linguistics, 12(3):175-204.Hakkani-Tiir, Dilek, GOkhan Tiir, AndreasStolcke, and Elizabeth Shriberg.
1999.Combining words and prosody forinformation extraction from speech.
InProceedings ofthe 6th European Conference onSpeech Communication a d Technology,volume 5, pages 1991-1994, Budapest,September.Hearst, Marti A.
1994.
Multi-paragraphsegmentation f expository text.
InProceedings ofthe 32nd Annual Meeting,pages 9-16, New Mexico State University,Las Cruces, NM, June.
Association forComputational Linguistics.Hearst, Marti A.
1997.
TextTiling:Segmenting text info multi-paragraphsubtopic passages.
ComputationalLinguistics, 23(1):33-64.55Computational Linguistics Volume 27, Number 1Hirschberg, Julia and Christine Nakatani.1996.
A prosodic analysis of discoursesegments in direction-giving monologues.In Proceedings ofthe 34th Annual Meeting,pages 286-293, Santa Cruz, CA, June.Association for ComputationalLinguistics.Hirschberg, Julia and Christine Nakatani.1998.
Acoustic indicators of topicsegmentation.
I  Robert H. Mannell andJordi Robert-Ribes, editors, Proceedings ofthe International Conference on SpokenLanguage Processing, pages 976-979,Sydney, December.
Australian SpeechScience and Technology Association.John, George H., Ron Kohavi, and KarlPfleger.
1994.
Irrelevant features and thesubset selection problem.
In William W.Cohen and Haym Hirsh, editors, MachineLearning: Proceedings ofthe llth InternationalConference, pages 121-129, San Francisco.Morgan Kaufmann.Koopmans-van Beinum, Florien J. andMonique E. van Donzel.
1996.Relationship between discourse structureand dynamic speech rate.
In H. TimothyBunnell and William Idsardi, editors,Proceedings ofthe International Conference onSpoken Language Processing, volume 3,pages 1724-1727, Philadelphia, October.Kozima, H. 1993.
Text segmentation basedon similarity between words.
InProceedings ofthe 31st Annual Meeting,pages 286-288, Ohio State University,Columbus, Ohio, June.
Association forComputational Linguistics.Litman, Diane J. and Rebecca J. Passonneau.1995.
Combining multiple knowledgesources for discourse segmentation.
IProceedings ofthe 33rd Annual Meeting,pages 108-115, MIT, Cambridge, MA,June.
Association for ComputationalLinguistics.Nakajima, Shin'ya and J. F. Allen.
1993.
Astudy on prosody and discourse structurein cooperative dialogues.
Phonetica,50:197-210.Nakajima, Shin'ya and Hajime Tsukada.1997.
Prosodic features of utterances intask-oriented dialogues.
In YoshinoriSagisaka, Nick Campbell, and NorioHiguchi, editors, Computing Prosody:Computational Models for ProcessingSpontaneous Speech.
Springer, New York,chapter 7, pages 81-94.NIST.
1999.
1999 Topic Detection andTracking Evaluation Project (TDT-3)Evaluation Project.
Speech Group,National Institute for Standards andTechnology, Gaithersburg, MD.
http://www.nist.gov/speech/tests/tdt/tdt99/.Passonneau, Rebecca J. and Diane J. Litman.1997.
Discourse segmentation byhumanand automated means.
ComputationalLinguistics, 23(1):103-139.Ponte, J. M. and W. B. Croft.
1997.
Textsegmentation bytopic.
In Proceedings oftheFirst European Conference on Research andAdvanced Technology for Digital Libraries,pages 120-129, Pisa, Italy.Przybocki, M. A. and A. F. Martin.
1999.The 1999 NIST speaker ecognitionevaluation, using summed two-channeltelephone data for speaker detection andspeaker tracking.
In Proceedings ofthe 6thEuropean Conference on SpeechCommunication a d Technology, volume 5,pages 2215-2218, Budapest, September.Rabiner, L. R. and B. H. Juang.
1986.
Anintroduction to hidden Markov models.IEEE ASSP Magazine, 3(1):4-16, January.Reynar, Jeffrey C. 1994.
An automaticmethod of finding topic boundaries.
InProceedings ofthe 32nd Annual Meeting,pages 331-333, New Mexico StateUniversity, Las Cruces, NM, June.Association for ComputationalLinguistics.Reynar, Jeffrey C. 1999.
Statistical modelsfor topic segmentation.
I  Proceedings ofthe 37th Annual Meeting, pages 357-364,University of Maryland, College Park,MD, June.
Association for ComputationalLinguistics.Sankar, Ananth, Fuliang Weng, Ze'ev Rivlin,Andreas Stolcke, and Ramana Rao Gadde.1998.
The development of SRI's 1997Broadcast News transcription system.
InProceedings DARPA Broadcast NewsTranscription and Understanding Workshop,pages 91-96, Lansdowne, VA, February.Morgan Kaufmann.Shriberg, Elizabeth, Rebecca Bates, andAndreas Stolcke.
1997.
A prosody-onlydecision-tree model for disfluencydetection.
In G. Kokkinakis, N. Fakotakis,and E. Dermatas, editors, Proceedings ofthe5th European Conference on SpeechCommunication a d Technology, volume 5,pages 2383-2386, Rhodes, Greece,September.Shriberg, Elizabeth, Andreas Stolcke, DilekHakkani-Ti~r, and G6khan Tiir.
2000.Prosody-based automatic segmentation fspeech into sentences and topics.
SpeechCommunication, 32(1-2), pages 127-154.Special Issue on Accessing Information inSpoken Audio.S6nmez, Kemal, Elizabeth Shriberg, LarryHeck, and Mitchel Weintraub.
1998.Modeling dynamic prosodic variation forspeaker verification.
In Robert H. Mannell56Tiir, Hakkani-Tiir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cuesand Jordi Robert?Ribes, editors,Proceedings ofthe International Conference onSpoken Language Processing, volume 7,pages 3189-3192, Sydney, December.Australian Speech Science andTechnology Association.Stolcke, Andreas, Klaus Ries, Noah Coccaro,Elizabeth Shriberg, Dan Jurafsky, PaulTaylor, Rachel Martin, Carol VanEss-Dykema, nd Marie Meteer.
2000.Dialogue act modeling for automatictagging and recognition of conversationalspeech.
Computational Linguistics,26(3):339-373.Stolcke, Andreas and Elizabeth Shriberg.1996.
Automatic linguistic segmentationof conversational speech.
In H. TimothyBunnell and William Idsardi, editors,Proceedings ofthe International Conference onSpoken Language Processing, volume 2,pages 1005-1008, Philadelphia, October.Stolcke, Andreas, Elizabeth Shriberg,Rebecca Bates, Mari Ostendorf, DilekHakkani, Madelaine PlauchG GOkhanTier, and Yu Lu.
1998.
Automaticdetection of sentence boundaries anddisfluencies based on recognized words.In Robert H. Mannell and JordiRobert-Ribes, editors, Proceedings oftheInternational Conference on Spoken LanguageProcessing, volume 5, pages 2247-2250,Sydney, December.
Australian SpeechScience and Technology Association.Swerts, M. 1997.
Prosodic features atdiscourse boundaries of differentstrength.
Journal of the Acoustical Society ofAmerica, 101:514-521.Swerts, M., R. Geluykens, and J. Terken.1992.
Prosodic correlates of discourseunits in spontaneous speech.
In John J.Ohala, Terrance M. Nearey, Bruce L.Derwing, Megan M. Hodge, and Grace E.Wiebe, editors, Proceedings oftheInternational Conference on Spoken LanguageProcessing, volume 1, pages 421-424,Banff, Canada, October.Swerts, M. and M. Ostendorf.
1997.
Prosodicand lexical indications of discoursestructure in human-machine interactions.Speech Communication, 22(1):25--41.Vaissi6re, Jacqueline.
1983.Language-independent prosodic features.In A. Cutler and D. R. Ladd, editors,Prosody: Models and Measurements.Springer, Berlin, chapter 5, pages 53-66.van Mulbregt, P., I. Carp, L. Gillick, S. Lowe,and J. Yamron.
1999.
Segmentation fautomatically transcribed broadcast newstext.
In Proceedings ofDARPA BroadcastNews Workshop, ages 77-80, Herndon,VA, February.
Morgan Kaufmann.Viterbi, A.
1967.
Error bounds forconvolutional codes and anasymptotically optimum decodingalgorithm.
IEEE Transactions on InformationTheory, 13:260-269.Yamron, J. P., I. Carp, L. Gillick, S. Lowe,and P. van Mulbregt.
1998.
A hiddenMarkov model approach to textsegmentation a d event racking.
InProceedings ofthe IEEE Conference onAcoustics, Speech, and Signal Processing,volume 1, pages 333-336, Seattle, WA,May.57
