JEP-TALN-RECITAL 2012, Atelier ILADI 2012: Interactions Langagi?res pour personnes Ag?es Dans les habitats Intelligents, pages 49?59,Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCPContribution ?
l?
?tude de la variabilit?
de la voix despersonnes ?g?es en reconnaissance automatique de la paroleFr?d?ric Aman, Michel Vacher, Solange Rossato, Fran?ois PortetLaboratoire d?Informatique de Grenoble (UMR 5217), ?quipe GETALP41 avenue des Math?matiques,BP 53 - 38041 Grenoble Cedex 9 - France{frederic.aman, michel.vacher, solange.rossato, francois.portet}@imag.frR?SUM?L?utilisation de la reconnaissance vocale pour l?assistance ?
la vie autonome se heurte ?
ladifficult?
d?utilisation des syst?mes de RAP qui ne sont pas pr?vus ?
la base pour la voix ?g?e.Pour caract?riser les diff?rences de comportement d?un syst?me de reconnaissance entre lespersonnes ?g?es et non-?g?es, nous avons ?tudi?
quels sont les phon?mes les moins bien reconnusen nous basant sur le corpus AD80 que nous avons enregistr?.
Les r?sultats montrent que certainsphon?mes tels que les plosives sont plus sp?cifiquement affect?s par l??ge.
De plus nous avonsrecueilli le corpus sp?cifique ERES38 afin d?adapter les mod?les acoustiques, avec pour r?sultatune diminution du taux d?erreur de mot de 15%.
Malgr?
la grande variabilit?
des performances,nous avons caract?ris?
comment la baisse des performances du syst?me de reconnaissanceautomatique de la parole peut ?tre corr?l?e avec la baisse d?autonomie des personnes ?g?es.ABSTRACTContribution to the study of elderly people?s voice variability in automatic speech recogni-tionUsing speech recognition to support ambient assisted living is impeded by the difficulty ofusing ASR systems that are not provided for the elderly voice.
To characterize these differencesin speech recognition performance, we studied phoneme categories which lead to the lowestrecognition rate in the elderly speakers with respect to the younger ones based on the AD80corpus that we recorded.
The results showed that some phonemes (such as plosives) are morespecifically affected by age than others.
Moreover, we collected the specific ERES38 corpus toadapt the ASR acoustic model to the elderly population which resulted in a 15% decrease ofthe word error rate.
Despite a great variability of performances, we characterized how lowerperformance of ASR systems can be correlated to the autonomy degradation of elderly people.MOTS-CL?S : reconnaissance automatique de parole, voix des personnes ?g?es, adaptationacoustique, assistance ?
la vie autonome.KEYWORDS: automatic speech recognition, ageing voice, acoustic adaptation, ambient assistedliving.491 IntroductionL?assistance ?
la vie autonome (ou Ambient Assisted Living - AAL) est devenu un enjeu important?tant donn?
la part croissante de la population ?g?e dans les pays industrialis?s.
Par contre,il reste encore ?
l?heure actuelle assez peu de projets qui prennent en compte une interactionvocale de la part de l?utilisateur, par exemple une commande vocale en domotique.
Les personnesqui b?n?ficieraient le plus de ces technologies seraient les personnes en perte d?autonomie, ?tantdonn?
que le langage naturel est le moyen le plus spontan?
de communication.Dans ce contexte, le projet CIRDO 1 auquel participe le LIG vise ?
favoriser l?autonomie et la priseen charge des personnes ?g?es par les aidants au travers d?e-lio, un produit de t?l?lien socialaugment?
et automatis?.
e-lio est un syst?me de communication en visiophonie s?adaptant audegr?
d?autonomie de son utilisateur.
e-lio permet l?acc?s ?
de nombreux services interactifs :visiophonie, t?l?phonie, messages, partage de photos, rappels automatiques, appels d?urgence,agenda partag?, plateforme domotique, entra?nement de la m?moire et de l?attention, etc.L?objectif du projet CIRDO est d?y int?grer un syst?me de Reconnaissance Automatique de laParole (RAP) qui inclura une d?tection des signaux de d?tresse ainsi que des commandes vocalesen compl?ment de la t?l?commande.L?utilisation de la reconnaissance vocale pour l?assistance ?
la vie autonome se heurte ?
la difficult?d?utilisation des syst?mes de RAP qui ne sont pas pr?vus ?
la base pour la voix ?g?e.
En effet, dufait de certaines caract?ristiques sp?cifiques de la voix ?g?e, un travail d?adaptation des syst?mesde RAP a d?
?tre r?alis?.
De fait, la parole ?g?e se caract?rise notamment par des tremblementsde la voix, une production impr?cise des consonnes, et une articulation plus lente (Ryan etBurk, 1974).
Du point de vue anatomique, des ?tudes ont montr?
des d?g?n?rescences li?es?
l?
?ge avec une atrophie des cordes vocales, une calcification des cartilages du larynx, et deschangements dans la musculature du larynx (Takeda et al, 2000; Mueller et al, 1984).
Deplus, des changements dans les capacit?s de contr?le moteur de la voix au niveau cognitifmodifient la production de la parole tout au long de la vie (Hooper et Cralidis, 2009).
D?autres?tudes (Georgila et al, 2008) ont montr?
que lors de l?interaction avec un syst?me de dialogue -incluant une RAP et une synth?se vocale - les personnes ?g?es utilisent, par rapport aux personnesjeunes, un vocabulaire plus riche et des phrases plus longues, et emploient plus fr?quement desexpressions d?interaction sociale telles que "au revoir" ou "merci", comme s?il s?agissait d?uneinteraction humain/humain.
Du fait que les mod?les acoustiques des syst?mes de RAP sontappris majoritairement sur de la voix non-?g?e, on observe donc une augmentation significativedu taux d?erreurs de mots pour la voix des personnes ?g?es par rapport ?
la voix des adultesnon-?g?s (Baba et al, 2004; Vipperla et al, 2008, 2010; Aman et al, 2012).Afin d?am?liorer le module de d?codage acoustico-phon?tique dans un syst?me de RAP et del?adapter ?
la voix des personnes ?g?es, une premi?re analyse a consist?
?
?tudier les phon?mesqui ?taient mal reconnus pour les personnes ?g?es.
Cette analyse, pr?sent?e dans la section 2, apermis d?extraire les phon?mes qui semblent plus probl?matiques ?
reconna?tre que d?autres lorsdu d?codage acoustico-phon?tique.
Nous avons r?alis?
une adaptation du mod?le acoustique,d?taill?e en section 3.
Nous montrons en section 4 que l?
?ge n?est pas le facteur d?terminant surla baisse des performances du syst?me de RAP, mais que le niveau de d?pendance de la personne?g?e joue un r?le important.
Nous concluons et pr?sentons les perspectives de recherche ensection 5.1. http://liris.cnrs.fr/cirdo/502 D?termination des phon?mes difficiles ?
reconna?tre2.1 Le corpus de test AD80Afin d?
?valuer le comportement du syst?me de RAP sur la voix ?g?e, nous avons utilis?
lecorpus Anondin-D?tresse 80 (AD80).
Ce corpus, enregistr?
par le laboratoire LIG, est sp?cifiqueau domaine de la domotique et ?
la d?tection d?appels de d?tresse.
Ce corpus est constitu?
del?enregistrement de 57 locuteurs (22 hommes et 35 femmes) ?g?s de 20 ?
94 ans.
Il a ?t?
demand?aux participants de lire une liste de 126 ?nonc?s courts de la vie quotidienne ou caract?ristiquesd?un appel de d?tresse (ex : "Il fait chaud" ou "Aidez-moi").
Dans le cadre de l?applicationenvisag?e, nous cherchons essentiellement ?
reconna?tre ce type d?
?nonc?s correspondant ?
desordres domotiques ou des appels ?
l?aide.Le corpus AD80 est constitu?
de deux groupes :- Le groupe voix non-?g?es, constitu?
de 21 locuteurs ?g?s de 20 ?
65 ans enregistr?s ?
Grenobledans notre laboratoire en 2004.
Tous les locuteurs ?taient actifs professionnellement ou ?tudiants.Cette premi?re partie a ?t?
enregistr?e lors d?
?tudes relatives ?
la reconnaissance d?appels ded?tresse dans un Habitat Intelligent pour la Sant?
(HIS) (Vacher et al, 2006), ce qui expliquepourquoi il s?agit d?
?nonc?s courts.
Le texte de ces ?nonc?s a ensuite ?t?
utilis?
pour des?valuations en milieu r?el en parole distante dans un appartement ?quip?
de microphones (Vacheret al, 2008).- Le groupe voix ?g?es, constitu?
de 36 locuteurs ?g?s de 62 ?
94 ans enregistr?s dans un h?pitalet ?
domicile ?
Grenoble en 2010 ainsi que dans un centre de r?tablissement et dans une maisonde retraite dans le d?partement du Gard en 2012.
Les locuteurs ?taient ?
la retraite, et pourcertains en situation de d?pendance.Au final, le corpus AD80 est form?
de 6 848 phrases annot?es, avec 2 heures et 3 minutesd?enregistrements audio (cf.
Table 1).Corpus AD80 Nombre locuteurs Age min-max Dur?e Nombre phrasesGroupe voix non-?g?es 21 20-65 38min 2646Groupe voix ?g?es 36 62-94 1h25min 4202Total 57 20-94 2h03min 6848TABLE 1 ?
Caract?ristiques du corpus AD802.2 Le syst?me de RAPLe syst?me de RAP choisi pour notre ?tude est Sphinx3 (Seymore et al, 1998).
Ce d?codeurutilise un mod?le acoustique d?pendant du contexte avec cha?nes de Markov cach?es 3 ?tats.
Lesvecteurs acoustiques sont compos?s de 13 coefficients MFCC, le delta et le double delta de chaquecoefficient.
Le mod?le acoustique que nous utilisons a ?t?
entra?n?
sur le corpus BREF120 (Lamelet al, 1991) qui est compos?
de 100 heures de parole annot?es enregistr?es aupr?s de 120locuteurs fran?ais.
Nous avons appel?
ce mod?le le mod?le acoustique g?n?rique.Un mod?le de langage sp?cialis?
a ?t?
utilis?.
Ce mod?le a ?t?
construit ?
partir des transcriptions51des phrases du corpus AD80, dont il a r?sult?
un mod?le de langage restreint, de type trigramme,avec un vocabulaire d?environ 160 mots.2.3 Comparaison des taux d?erreurs de mots et des scores d?alignementsforc?s entre les groupes voix ?g?es et voix non-?g?esAfin d?
?valuer l?effet de la voix ?g?e sur la performance de la RAP en utilisant le mod?le acoustiqueg?n?rique, nous avons compar?
le taux d?erreurs de mots (ou Word Error Rate - WER) entre lesgroupes.
Nous avons obtenu un WER de 7,33% pour le d?codage sur le groupe voix non-?g?es, etun WER de 27,56% pour le d?codage sur le groupe voix ?g?es.
Ainsi, nous avons observ?
uneimportante d?gradation du syst?me de RAP pour la voix des personnes ?g?es, avec une diff?renceabsolue de 20,23%.Pour aller plus loin dans l?analyse, nous avons r?alis?
un alignement forc?
sur ces deux groupes.L?alignement forc?
consiste ?
convertir les transcriptions de r?f?rence en suites de phon?mescal?s sur les donn?es audio en utilisant un dictionnaire phon?tique.
L?alignement forc?
a permisd?obtenir les scores d?alignement par phon?me.
Ceux-ci sont des scores de vraisemblance d?appar-tenance au phon?me normalement prononc?
pour la portion de signal consid?r?e.
Ce score peut?tre interpr?t?
comme une proximit?
avec la prononciation "standard", mod?lis?e par le mod?leacoustique g?n?rique.
Le score exprime le logarithme d?une vraisemblance, il est inf?rieur ou ?gal?
z?ro, et plus il est faible, plus le phon?me associ?
est ?loign?
du mod?le acoustique.Les scores sont pr?sent?s sur la Figure 1 selon la cat?gorie phon?mique.Plosivesnon-vois?esPlosivesvois?es Fricativesnon-vois?esFricativesvois?es NasalesetliquidesVoyellesferm?es Voyellesmoyennes Voyellesouvertes Voyellesnasales?3?2?10?104Scored?alignementforc?
Groupe voix non-?g?es Groupe voix ?g?esFIGURE 1 ?
Score d?alignement forc?
par cat?gorie phon?mique avec le mod?le acoustiqueg?n?rique pour les groupes voix non-?g?es et voix ?g?esPour le groupe voix non-?g?es, certains phon?mes montrent des valeurs plus faibles du scored?alignement, tels que les plosives ou les voyelles ouvertes.
D?autres sons, ?
l?inverse, sont plusproches des repr?sentations de mod?le acoustique : les fricatives.Pour le groupe voix ?g?es, les scores d?alignement sont moins ?lev?s que ceux obtenus pour legroupe voix non-?g?es.
En effet, le vieillissement provoque une moins bonne ma?trise du syst?me52articulatoire, et la prononciation des personnes ?g?es s?
?loigne donc du mod?le acoustique.
Lesconsonnes plosives et les voyelles nasales sont les phon?mes n?cessitant le plus de contr?lemoteur et sont les plus difficiles ?
articuler.
Ceci se traduit au niveau des scores d?alignement, lesplus bas sont obtenus pour les consonnes plosives et les voyelles nasales.Les ?carts de scores les plus importants par cat?gorie phon?mique sur le groupe voix ?g?es parrapport au groupe voix non-?g?es ont permis de caract?riser quels sont les phon?mes posant leplus de probl?mes pour la RAP des voix ?g?es.
Les diff?rences relatives de scores observ?es entreles deux groupes ont ?t?
calcul?es.
Les cat?gories phon?miques sont par ordre descendant dediff?rence : les voyelles nasales (-117,00%), les consonnes fricatives non-vois?es (-110,56%),les consonnes plosives non-vois?es (-105,72%), les consonnes fricatives vois?es (-87,86%), lesconsonnes plosives vois?es (-83,29%), les voyelles moyennes (-63,74%), les voyelles ouvertes(-53,21%), les voyelles ferm?es (-45,52%), et les consonnes nasales et liquides (-42,65%).
Ense basant sur les diff?rences relatives, les cat?gories phon?miques les plus affect?es pour laparole ?g?e sont les voyelles nasales, et les consonnes fricatives et plosives.
Aussi, nous pouvonsnoter que globalement les consonnes sont plus affect?es que les voyelles.
De plus, l?absence devoisement est le principal facteur de d?gradation, suivie par la modalit?
de r?alisation (plosiveset fricatives).
Ainsi, il serait possible que, en ce qui concerne les personnes ?g?es, les consonnesnon vois?es soient plus proches des consonnes vois?es.Ces r?sultats sont similaires ?
ceux obtenus par (Privat et al, 2004) qui ont trouv?
une d?gradationde la RAP entre voix ?g?e et voix non-?g?e avec une diff?rence relative tr?s proche de celle quenous avons obtenue pour chaque cat?gorie phon?mique, except?
pour les consonnes nasales etliquides et pour les voyelles nasales o?
leur syst?me ?tait moins performant que le notre.3 Adaptation acoustique3.1 Recueil du nouveau corpus ERES38?tant donn?e la baisse de performance du syst?me de RAP pour la voix ?g?es, nous avonsenregistr?
un nouveau corpus de parole de personnes ?g?es en vue de l?am?lioration du mod?leacoustique gr?ce ?
une m?thode d?adaptation acoustique.
Les principales ?tapes de l?
?tude sontr?sum?es Figure 2.                                                                                         Enregistrement corpusEvaluationERES38AD80 Voix ?g?e Voix non?
?g?eAdaptation acoustiqueFIGURE 2 ?
Principales ?tapes de l?
?tudeLe corpus constitu?
est un ensemble d?entretiens.
Chaque entrevue met en relation une per-53sonne ?g?e avec deux exp?rimentateurs dont l?un se fait l?interlocuteur privil?gi?.
Le mat?rielutilis?
pour les enregistrements ?tait un enregistreur TASCAM DR-100 avec un microphone ?condensateur unidirectionnel plac?
?
proximit?
et en direction de la personne ?g?e.
Une premi?repartie introductive permet de r?cup?rer les informations personnelles ainsi que les habitudeslinguistiques du locuteur.
Cette phase d?habituation avec le mat?riel d?enregistrement permetd?
?tablir le passage vers une parole un peu plus informelle et spontan?e pour recueillir le r?cit devie de la personne, incluant une description des activit?s quotidiennes et de leur habitat, un r?citd?accidents ?ventuels et des anecdotes.
Une activit?
de lecture est ?galement propos?e lors de cetentretien.
Le support choisi est un article de jardinage cr??
par les exp?rimentateurs dans le butde cibler les phon?mes probl?matiques.
Les plosives et fricatives non vois?es ont ?t?
introduitesde fa?on ?
se retrouver en contexte /a/, /i/ et /u/.Le corpus est constitu?
de 17 heures et 44 minutes d?enregistrements (cf.
Table 2) avec 24locuteurs (16 femmes et 8 hommes) dont l?
?ge varie de 68 ?
98 ans, incluant 48 minutes delectures par 22 locuteurs.
Ces locuteurs sont issus de structures sp?cifiques pour personnes ?g?es,foyers logements ou maisons de retraite.
Les entretiens ont ?t?
effectu?s avec des personnes plusou moins autonomes, sans d?ficience cognitive, parfois avec de s?rieuses difficult?s motrices,mais sans handicap lourd.Corpus ERES38 Nombre locuteurs Age min-max Dur?e Nombre phrasesLecture de texte 22 68-98 16h56min 300Parole spontan?e 24 68-98 48min 7300Total 24 68-98 17h44min 7600TABLE 2 ?
Caract?ristiques du corpus ERES38Les enregistrements des entretiens ont commenc?
?
?tre transcrits, et toutes les lectures ont?t?
transcrites et v?rifi?es.
Ces donn?es annot?es et structur?es constituent le corpus EntretiensRESidences 38 (ERES38).3.2 Adaptation MLLRLa m?thode d?adaptation de r?gression lin?aire du maximum de vraisemblance (Maximum Likeli-hood Linear Regression ou MLLR) a ?t?
utilis?e pour adapter le mod?le acoustique g?n?rique, apprissur BREF120, ?
la voix des personnes ?g?es.
L?adaptation a ?t?
faite globalement avec l?ensembledes lectures du corpus ERES38.
Nous avons ainsi obtenu un nouveau mod?le acoustique appel?mod?le acoustique adapt?
par MLLR.A partir du corpus AD80, un premier d?codage a ?t?
fait sur le groupe voix ?g?es en utilisant lemod?le acoustique g?n?rique.
Puis un second d?codage a ?t?
effectu?
sur ce m?me groupe avec lemod?le acoustique adapt?
par MLLR.
Le but ?tait de voir dans quelle mesure est la diff?rence deWER avec l?utilisation de l?un ou l?autre des mod?les, avec l?hypoth?se que le WER avec le groupevoix ?g?es issu du d?codage avec le mod?le acoustique adapt?
par MLLR serait proche du WERavec le groupe voix non-?g?es issu du d?codage avec le mod?le acoustique g?n?rique.
En outre,nous avons r?alis?
un d?codage avec le mod?le acoustique adapt?
par MLLR sur le groupe voixnon-?g?es afin de tester la sp?cificit?
de l?adaptation.54La Figure 3 montre que l?utilisation du mod?le acoustique adapt?
par MLLR a permis de r?duire leWER pour tous les locuteurs du groupe voix ?g?es.
Avec l?adaptation MLLR globale, le WER estde 11,95%.
Compar?
au WER de 27,56% sans adaptation, la diff?rence absolue est de -15,61%(diff?rence relative de -56,65%).
D?un point de vue applicatif, cela montre que l?on peut utiliserune base de parole ?g?e pour l?adaptation MLLR dont les locuteurs sont diff?rents de ceux de labase de test.
Cela d?montre que les voix des personnes ?g?es ont des caract?ristiques proprescommunes.
De plus, nous voyons que l?utilisation d?un corpus de petite taille (48 minutes delecture par 22 locuteurs du corpus ERES38) pour l?adaptation MLLR globale est suffisante pourdonner une am?lioration significative avec un WER de 11,95%, proche du WER de 7,33% trouv?dans le cas du d?codage de la parole non-?g?e.0102030400 10 20 30 40 50 60 70 80WER apr?s adaptation acoustiqueWER avant adaptation acoustiqueFIGURE 3 ?
WER avec adaptation acoustique MLLR en fonction du WER sans adaptation.
La droiterepr?sente la fonction identit?De plus, nous avons obtenu un WER de 10,39% lors du d?codage sur le groupe voix non-?g?esavec le mod?le acoustique adapt?
par MLLR.
Cela repr?sente une d?gradation relative de 43,75%compar?e au WER de 7,33% pour le d?codage avec le mod?le acoustique g?n?rique sur le m?megroupe.
Il est donc convenu que le mod?le acoustique adapt?
par MLLR est sp?cifique ?
la RAP surla population ?g?e.4 Relation entre l?autonomie des personnes ?g?es et la RAPLe WER issu du d?codage sur le groupe voix ?g?es avec le mod?le acoustique adapt?
par MLLR estrepr?sent?
en fonction de l?
?ge sur la Figure 4.
Nous observons par la dispersion des points deslocuteurs repr?sent?s que l?
?ge est un mauvais indicateur du WER.
De plus, nous avons calcul?la corr?lation de Pearson entre les variables "WER apr?s adaptation acoustique" et "Age".
Nousavons trouv?
un score de corr?lation de -0,053 (p = 0,759%) prouvant que le WER et l?
?gene sont pas corr?l?s.
En cons?quence, nous avons cherch?
si d?autres param?tres relatifs ?
lad?pendance peuvent ?tre des indicateurs de la performance du syst?me de RAP.Nous avons fait l?hypoth?se que la d?gradation physique et psychique affecte la production dela parole et donc les performances de la RAP.
Nous avons pris pour r?f?rence un test nationalrelatif ?
l?autonomie des personnes ?g?es : la grille AGGIR (Autonomie G?rontologie Groupes55051015202530354060 65 70 75 80 85 90 95WERapr?sadaptationacoustiqueAgeFIGURE 4 ?
WER apr?s adaptation acoustique MLLR en fonction de l?
?ge, avec la droite correspon-dant ?
la r?gression lin?aireIso-Ressources) 2.
Pour 29 locuteurs ?g?s en ?tablissement que nous avions enregistr?s pour laconstitution du corpus AD80, nous avons compl?t?
pour chacun d?entre eux une grille AGGIRavec l?aide du personnel soignant.La grille AGGIR est un outil d?
?valuation du degr?
de perte d?autonomie et de d?pendance, enterme de d?gradation physique et psychique, pour l?attribution de l?Allocation Personnalis?ed?Autonomie (APA), qui est une aide financi?re pour les personnes ?g?es d?pendantes en France.L?
?valuation est faite ?
partir de 17 variables.
Dix variables se r?f?rent ?
la perte d?autonomiephysique et psychique : coh?rence, orientation, toilette, habillage, alimentation, ?limination,transferts (se lever, se coucher, s?asseoir), d?placement ?
l?int?rieur, d?placement ?
l?ext?rieur, etcommunication ?
distance.
Sept variables se rapportent ?
la perte d?autonomie domestique etsociale : gestion personnelle de son budget et de ses biens, cuisine, m?nage, transports, achats,suivi du traitement, et activit?s de temps libre.
Chaque variable est cod?e par A (fait seul), B (faitpartiellement) ou C (ne fait pas).
Le score GIR (Groupe Iso-Ressources) est calcul?
?
partir desvariables afin de classer les personnes ?g?es dans un des six groupes : de GIR 1 (d?pendancetotale) ?
GIR 6 (autonomie totale).
Les personnes class?es de GIR 1 ?
GIR 4 sont autoris?es ?recevoir une aide financi?re selon leur degr?
de d?pendance.Nous avons regard?
si le score GIR est repr?sentatif de la performance de la RAP.
Le WER des 29participants test?s sont repr?sent?s en fonction de leur score GIR en Figure 5.
Quatre locuteurssont GIR 2, deux locuteurs sont GIR 3, quinze locuteurs sont GIR 4 et huit locuteurs sont GIR 6.Aucun locuteur n?est repr?sent?
en GIR 1 et GIR 5.
Nous observons en Figure 5 que le score GIRpourrait avoir une influence sur le WER, avec une baisse du WER en fonction de l?augmentationdu score GIR, sauf pour GIR 2.Du fait du faible nombre de locuteurs en GIR 2 et GIR 3, nous avons rassembl?
ces deux groupesen un groupe nomm?
GIR 2-3.
Puis nous avons avons r?alis?
une ANOVA sur GIR 2-3, GIR 4et GIR 6 pour v?rifier si le score GIR pourrait avoir un effet significatif sur le WER.
Nous avonstrouv?
les r?sultats suivants : F(DDL, DDLer ror) = F(2, 26) = 3,7 ; p < 0.05%, prouvant qu?il y aau moins une distribution dont la moyenne diff?re des autres moyennes.
Nous avons r?alis?
un2.
http ://vosdroits.service-public.fr/F1229.xhtml560510152025303540GIR 2 GIR 3 GIR 4 GIR 6WERapr?s adaptation acoustiqueScore GIRFIGURE 5 ?
WER avec adaptation acoustique MLLR en fonction du score GIRtest post-hoc de Bonferroni afin de caract?riser quels groupes sont significativement diff?rents dequels autres groupes.
Le test post-hoc a r?v?l?
que les groupes extr?mes GIR 2-3 et GIR 6 ont uneffet significativement diff?rent sur le WER, et que GIR 4 n?a pas une influence significativementdiff?rente sur le WER par rapport aux autres groupes.De plus, nous avons r?alis?
une ?tude pr?liminaire (que nous d?taillerons dans un prochainarticle) sur les corr?lations entre le WER et chacun des 17 param?tres de la grille AGGIR.
Ilsemblerait que les param?tres concernant le contr?le moteur des membres sup?rieurs et lacontinence pourrait ?tre les plus corr?l?s au WER.
En effet, ces param?tres pourraient ?trerepr?sentatifs d?une d?gradation physique g?n?ralis?e avanc?e affectant ?galement le contr?lede la voix, et donc diminueraient la performance du syst?me de RAP.
Une perspective pourrait?tre de permettre une pr?diction du WER en se basant sur les caract?ristiques de d?pendancedes personnes ?g?es.5 ConclusionCet article pr?sente notre ?tude sur le comportement d?un syst?me de RAP vis-?-vis de la voix ?g?e.
?tant donn?
l?absence de corpus contenant de la voix de personnes ?g?es de langue fran?aiseutilisable pour la cr?ation ou l?adaptation des mod?les, nous avons proc?d?
?
l?enregistrementdu corpus AD80.
A partir de ce corpus, nous avons analys?
quels ?taient les phon?mes pourla voix ?g?e posant le plus probl?me au syst?me de RAP.
Nous avons pu d?terminer que leur?loignement par rapport ?
la prononciation mod?lis?e par les mod?les acoustiques provoque uneaugmentation du WER du syst?me de RAP, avec une diff?rence absolue entre voix non-?g?e et?g?e de 20,23%.
Ensuite, nous avons proc?d?
?
l?adaptation du mod?le acoustique g?n?rique ?
lavoix des personnes ?g?es, gr?ce ?
la m?thode d?adaptation MLLR, ?
partir du corpus ERES38.
Lecas de l?adaptation MLLR globale est int?ressante car avec moins d?une heure d?enregistrements, ?partir de locuteurs diff?rents des locuteurs de test, nous avons obtenu des taux d?erreurs de motsproches du cas d?une reconnaissance avec le mod?le acoustique g?n?rique de parole non-?g?e,avec un WER de 11,95%, contre 27,56% avant adaptation.
De plus, nous avons montr?
que leWER n?est pas corr?l?
avec l?
?ge mais pourrait ?tre corr?l?
avec le niveau de d?pendance de la57personne ?g?e du fait d?une d?gradation physique g?n?rale.
La continuation de notre travail serade r?aliser une adaptation au locuteur et de montrer comment les diff?rents param?tres de lagrille AGGIR sont corr?l?s au WER.
La pr?diction du comportement du syst?me de RAP permettrade faciliter l?utilisation de ces nouvelles technologies dans la vie quotidienne des personnes ?g?esd?pendantes.
Aussi, nous allons ?tudier dans quelle mesure les sons non verbaux (inspirations,bruits de bouche) ainsi que les h?sitations et les d?fauts d?articulation sont plus fr?quents chezles personnes ?g?es, et une prochaine ?tape de notre travail sera de prendre en consid?ration cesph?nom?nes pour la construction des mod?les acoustiques et de langage.RemerciementsCette ?tude a ?t?
financ?e par l?Agence Nationale de la Recherche dans le cadre du projetCIRDO-Recherche Industrielle (ANR-2010-TECS-012).
Nous remercions particuli?rement RemusDugheanu, Juline le Grand, Yuko Sasa, Claude Aynaud et Quentin Lefol pour leur active contri-bution, ainsi que les diff?rentes personnes ?g?es et le personnel soignant qui ont accept?
departiciper aux enregistrements.R?f?rencesAMAN, F., VACHER, M., ROSSATO, S., DUGHEANU, R., PORTET, F., LE GRAND, J. et SASA, Y.
(2012).
?tude de la performance des mod?les acoustiques pour des voix de personnes ?g?es en vue del?adaptation des syst?mes de RAP.
In JEP, Grenoble, France.BABA, A., YOSHIZAWA, S., YAMADA, M., LEE, A. et SHIKANO, K. (2004).
Acoustic models of theelderly for large-vocabulary continuous speech recognition.
Electronics and Communications inJapan, Part 2, 87:49?57.GEORGILA, K., WOLTERS, M., KARAISKOS, V., KRONENTHAL, M., LOGIE, R., MAYO, N., MOORE, J.et WATSON, M. (2008).
A fully annotated corpus for studying the effect of cognitive ageingon users?
interactions with spoken dialogue systems.
In Proceedings of the 6th InternationalConference on Language Resources and Evaluation, Marrakech, Morocco.HOOPER, C. et CRALIDIS, A.
(2009).
Normal changes in the speech of older adults : You?ve stillgot what it takes ; it just takes a little longer !
Perspectives on Gerontology, 14:47?56.LAMEL, L., GAUVAIN, J. et ESKENAZI, M. (1991).
BREF, a large vocabulary spoken corpus forfrench.
In Proceedings of EUROSPEECH 91, volume 2, pages 505?508, Geneva, Switzerland.MUELLER, P., SWEENEY, R. et BARIBEAU, L. (1984).
Acoustic and morphologic study of thesenescent voice.
Ear, Nose, and Throat Journal, 63:71?75.PRIVAT, R., VIGOUROUX, N. et TRUILLET, P. (2004).
Etude de l?effet du vieillissement sur lesproductions langagi?res et sur les performances en reconnaissance automatique de la parole.Revue Parole, 31-32:281?318.RYAN, W. et BURK, K. (1974).
Perceptual and acoustic correlates in the speech of males.
Journalof Communication Disorders, 7:181?192.SEYMORE, K., STANLEY, C., DOH, S., ESKENAZI, M., GOUVEA, E., RAJ, B., RAVISHANKAR, M., RO-SENFELD, R., SIEGLER, M., STERN, R. et THAYER, E. (1998).
The 1997 CMU Sphinx-3 English58broadcast news transcription system.
In DARPA Broadcast News Transcription and UnderstandingWorkshop, Lansdowne, VA, USA.TAKEDA, N., THOMAS, G. et LUDLOW, C. (2000).
Aging effects on motor units in the humanthyroarytenoid muscle.
Laryngoscope, 110:1018?1025.VACHER, M., FLEURY, A., SERIGNAT, J., NOURY, N. et GLASSON, H. (2008).
Preliminary Evaluationof Speech/Sound Recognition for Telemedicine Application in a Real Environment.
In 9th Inter-national Conference on Speech Science and Speech Technology (InterSpeech 2008), volume 1, pages496?499, Brisbane Convention & Exhibition Centre (BCEC), Brisbane (Australia).
AustralasianSpeech Science and Technology Association (ASSTA).VACHER, M., SERIGNAT, J.-F., CHAILLOL, S., ISTRATE, D. et POPESCU, V. (2006).
Speech and SoundUse in Remote Monitoring System for Health Care.
In Faculty of INFORMATICS, M. U., ?diteur :9th International Conference on Text, Speech and Dialogue (TSD 2006), volume 4148 de LNCS -LNAI, pages 711?718, Faculty of Informatics, Masaryk University, Brno (Czech Republic).VIPPERLA, R., RENALS, S. et FRANKEL, J.
(2008).
Longitudinal study of ASR performance onageing voices.
Interspeech, pages 2550?2553.VIPPERLA, R., RENALS, S. et FRANKEL, J.
(2010).
Ageing voices : The Effect of Changes in VoiceParameters on ASR Performance.
EURASIP Journal on Audio, Speech, and Music Processing.59
