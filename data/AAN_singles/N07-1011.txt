Proceedings of NAACL HLT 2007, pages 81?88,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsFirst-Order Probabilistic Models for Coreference ResolutionAron Culotta and Michael Wick and Andrew McCallumDepartment of Computer ScienceUniversity of MassachusettsAmherst, MA 01003{culotta,mwick,mccallum}@cs.umass.eduAbstractTraditional noun phrase coreference res-olution systems represent features onlyof pairs of noun phrases.
In this paper,we propose a machine learning methodthat enables features over sets of nounphrases, resulting in a first-order proba-bilistic model for coreference.
We out-line a set of approximations that make thisapproach practical, and apply our methodto the ACE coreference dataset, achiev-ing a 45% error reduction over a com-parable method that only considers fea-tures of pairs of noun phrases.
This resultdemonstrates an example of how a first-order logic representation can be incorpo-rated into a probabilistic model and scaledefficiently.1 IntroductionNoun phrase coreference resolution is the problemof clustering noun phrases into anaphoric sets.
Astandard machine learning approach is to perform aset of independent binary classifications of the form?Is mention a coreferent with mention b?
?This approach of decomposing the problem intopairwise decisions presents at least two related diffi-culties.
First, it is not clear how best to convert theset of pairwise classifications into a disjoint cluster-ing of noun phrases.
The problem stems from thetransitivity constraints of coreference: If a and b arecoreferent, and b and c are coreferent, then a and cmust be coreferent.This problem has recently been addressed by anumber of researchers.
A simple approach is to per-form the transitive closure of the pairwise decisions.However, as shown in recent work (McCallum andWellner, 2003; Singla and Domingos, 2005), bet-ter performance can be obtained by performing rela-tional inference to directly consider the dependenceamong a set of predictions.
For example, McCal-lum and Wellner (2005) apply a graph partitioningalgorithm on a weighted, undirected graph in whichvertices are noun phrases and edges are weighted bythe pairwise score between noun phrases.A second and less studied difficulty is that thepairwise decomposition restricts the feature set toevidence about pairs of noun phrases only.
This re-striction can be detrimental if there exist features ofsets of noun phrases that cannot be captured by acombination of pairwise features.
As a simple exam-ple, consider prohibiting coreferent sets that consistonly of pronouns.
That is, we would like to requirethat there be at least one antecedent for a set of pro-nouns.
The pairwise decomposition does not makeit possible to capture this constraint.In general, we would like to construct arbitraryfeatures over a cluster of noun phrases using thefull expressivity of first-order logic.
Enabling thissort of flexible representation within a statisticalmodel has been the subject of a long line of researchon first-order probabilistic models (Gaifman, 1964;Halpern, 1990; Paskin, 2002; Poole, 2003; Richard-son and Domingos, 2006).Conceptually, a first-order probabilistic modelcan be described quite compactly.
A configura-tion of the world is represented by a set of predi-81HePresident BushLaura BushShe0.20.90.70.40.0010.6Figure 1: An example noun coreference graph inwhich vertices are noun phrases and edge weightsare proportional to the probability that the two nounsare coreferent.
Partitioning such a graph into disjointclusters corresponds to performing coreference res-olution on the noun phrases.cates, each of which has an associated real-valuedparameter.
The likelihood of each configuration ofthe world is proportional to a combination of theseweighted predicates.
In practice, however, enu-merating all possible configurations, or even all thepredicates of one configuration, can result in in-tractable combinatorial growth (de Salvo Braz et al,2005; Culotta and McCallum, 2006).In this paper, we present a practical method to per-form training and inference in first-order models ofcoreference.
We empirically validate our approachon the ACE coreference dataset, showing that thefirst-order features can lead to an 45% error reduc-tion.2 Pairwise ModelIn this section we briefly review the standard pair-wise coreference model.
Given a pair of nounphrases xij = {xi, xj}, let the binary random vari-able yij be 1 if xi and xj are coreferent.
Let F ={fk(xij , y)} be a set of features over xij .
For exam-ple, fk(xij , y) may indicate whether xi and xj havethe same gender or number.
Each feature fk has anassociated real-valued parameter ?k.
The pairwisemodel isp(yij |xij) =1Zxijexp?k?kfk(xij , yij)where Zxij is a normalizer that sums over the twosettings of yij .This is a maximum-entropy classifier (i.e.
logis-tic regression) in which p(yij |xij) is the probabilitythat xi and xj are coreferent.
To estimate ?
= {?k}from labeled training data, we perform gradient as-cent to maximize the log-likelihood of the labeleddata.Two critical decisions for this method are (1) howto sample the training data, and (2) how to combinethe pairwise predictions at test time.
Systems of-ten perform better when these decisions complementeach other.Given a data set in which noun phrases have beenmanually clustered, the training data can be cre-ated by simply enumerating over each pair of nounphrases xij , where yij is true if xi and xj are inthe same cluster.
However, this approach generatesa highly unbalanced training set, with negative ex-amples outnumbering positive examples.
Instead,Soon et al (2001) propose the following samplingmethod: Scan the document from left to right.
Com-pare each noun phrase xi to each preceding nounphrase xj , scanning from right to left.
For each pairxi, xj , create a training instance ?xij , yij?, where yijis 1 if xi and xj are coreferent.
The scan for xj ter-minates when a positive example is constructed, orthe beginning of the document is reached.
This re-sults in a training set that has been pruned of distantnoun phrase pairs.At testing time, we can construct an undirected,weighted graph in which vertices correspond tonoun phrases and edge weights are proportional top(yij |xij).
The problem is then to partition the graphinto clusters with high intra-cluster edge weights andlow inter-cluster edge weights.
An example of sucha graph is shown in Figure 1.Any partitioning method is applicable here; how-ever, perhaps most common for coreference is toperform greedy clustering guided by the word or-der of the document to complement the samplingmethod described above (Soon et al, 2001).
Moreprecisely, scan the document from left-to-right, as-signing each noun phrase xi to the same clusteras the closest preceding noun phrase xj for whichp(yij |xij) > ?, where ?
is some classificationthreshold (typically 0.5).
Note that this method con-trasts with standard greedy agglomerative cluster-ing, in which each noun phrase would be assignedto the most probable cluster according to p(yij |xij).82Choosing the closest preceding phrase is commonbecause nearby phrases are a priori more likely tobe coreferent.We refer to the training and inference methods de-scribed in this section as the Pairwise Model.3 First-Order Logic ModelWe propose augmenting the Pairwise Model toenable classification decisions over sets of nounphrases.Given a set of noun phrases xj = {xi}, let the bi-nary random variable yj be 1 if all the noun phrasesxi ?
xj are coreferent.
The features fk and weights?k are defined as before, but now the features canrepresent arbitrary attributes over the entire set xj .This allows us to use the full flexibility of first-orderlogic to construct features about sets of nouns.
TheFirst-Order Logic Model isp(yj |xj) =1Zxjexp?k?kfk(xj , yj)where Zxj is a normalizer that sums over the twosettings of yj .Note that this model gives us the representationalpower of recently proposed Markov logic networks(Richardson and Domingos, 2006); that is, we canconstruct arbitrary formulae in first-order logic tocharacterize the noun coreference task, and can learnweights for instantiations of these formulae.
How-ever, naively grounding the corresponding Markovlogic network results in a combinatorial explosion ofvariables.
Below we outline methods to scale train-ing and prediction with this representation.As in the Pairwise Model, we must decide how tosample training examples and how to combine inde-pendent classifications at testing time.
It is impor-tant to note that by moving to the First-Order LogicModel, the number of possible predictions has in-creased exponentially.
In the Pairwise Model, thenumber of possible y variables is O(|x|2), wherex is the set of noun phrases.
In the First-OrderLogic Model, the number of possible y variables isO(2|x|): There is a y variable for each possible el-ement of the powerset of x.
Of course, we do notenumerate this set; rather, we incrementally instan-tiate y variables as needed during prediction.A simple method to generate training examplesis to sample positive and negative cluster examplesuniformly at random from the training data.
Positiveexamples are generated by first sampling a true clus-ter, then sampling a subset of that cluster.
Negativeexamples are generated by sampling two positive ex-amples and merging them into the same cluster.At testing time, we perform standard greedy ag-glomerative clustering, where the score for eachmerger is proportional to the probability of thenewly formed clustering according to the model.Clustering terminates when there exists no addi-tional merge that improves the probability of theclustering.We refer to the system described in this section asFirst-Order Uniform.4 Error-driven and Rank-based trainingof the First-Order ModelIn this section we propose two enhancements tothe training procedure for the First-Order Uniformmodel.First, because each training example consists ofa subset of noun phrases, the number of possibletraining examples we can generate is exponential inthe number of noun phrases.
We propose an error-driven sampling method that generates training ex-amples from errors the model makes on the trainingdata.
The algorithm is as follows: Given initial pa-rameters ?, perform greedy agglomerative cluster-ing on training document i until an incorrect clusteris formed.
Update the parameter vector according tothis mistake, then repeat for the next training docu-ment.
This process is repeated for a fixed number ofiterations.Exactly how to update the parameter vector is ad-dressed by the second enhancement.
We proposemodifying the optimization criterion of training toperform ranking rather than classification of clus-ters.
Consider a training example cluster with a neg-ative label, indicating that not all of the noun phrasesit contains are coreferent.
A classification trainingalgorithm will ?penalize?
all the features associatedwith this cluster, since they correspond to a negativeexample.
However, because there may exists subsetsof the cluster that are coreferent, features represent-ing these positive subsets may be unjustly penalized.To address this problem, we propose constructingtraining examples consisting of one negative exam-83fcy12x2x1y23x3y13fcfcftFigure 2: An example noun coreference factor graphfor the Pairwise Model in which factors fc model thecoreference between two nouns, and ft enforce thetransitivity among related decisions.
The number ofy variables increases quadratically in the number ofx variables.ple and one ?nearby?
positive example.
In particular,when agglomerative clustering incorrectly mergestwo clusters, we select the resulting cluster as thenegative example, and select as the positive examplea cluster that can be created by merging other exist-ing clusters.1 We then update the weight vector sothat the positive example is assigned a higher scorethan the negative example.
This approach allowsthe update to only penalize the difference betweenthe two features of examples, thereby not penaliz-ing features representing any overlapping coreferentclusters.To implement this update, we use MIRA (Mar-gin Infused Relaxed Algorithm), a relaxed, onlinemaximum margin training algorithm (Crammer andSinger, 2003).
It updates the parameter vector withtwo constraints: (1) the positive example must havea higher score by a given margin, and (2) the changeto ?
should be minimal.
This second constraint isto reduce fluctuations in ?.
Let s+(?,xj) be theunnormalized score for the positive example ands?
(?,xk) be the unnormalized score of the neg-ative example.
Each update solves the following1Of the possible positive examples, we choose the one withthe highest probability under the current model to guard againstlarge fluctuations in parameter updatesfcy12x2x1y23x3y13fcfcfty123fcFigure 3: An example noun coreference factor graphfor the First-Order Model in which factors fc modelthe coreference between sets of nouns, and ft en-force the transitivity among related decisions.
Here,the additional node y123 indicates whether nouns{x1, x2, x3} are all coreferent.
The number of yvariables increases exponentially in the number ofx variables.quadratic program:?t+1 = argmin?||?t ?
?||2s.t.s+(?,xj) ?
s?
(?,xk) ?
1In this case, MIRA with a single constraint can beefficiently solved in one iteration of the Hildreth andD?Esopo method (Censor and Zenios, 1997).
Ad-ditionally, we average the parameters calculated ateach iteration to improve convergence.We refer to the system described in this section asFirst-Order MIRA.5 Probabilistic InterpretationIn this section, we describe the Pairwise and First-Order models in terms of the factor graphs they ap-proximate.For the Pairwise Model, a corresponding undi-rected graphical model can be defined asP (y|x) =1Zx?yij?yfc(yij , xij)?yij ,yjk?yft(yij , yj,k, yik, xij , xjk, xik)84where Zx is the input-dependent normalizer and fac-tor fc parameterizes the pairwise noun phrase com-patibility as fc(yij , xij) = exp(?k ?kfk(yij , xij)).Factor ft enforces the transitivity constraints byft(?)
= ??
if transitivity is not satisfied, 1 oth-erwise.
This is similar to the model presented inMcCallum and Wellner (2005).
A factor graph forthe Pairwise Model is presented in Figure 2 for threenoun phrases.For the First-Order model, an undirected graphi-cal model can be defined asP (y|x) =1Zx?yj?yfc(yj ,xj)?yj?yft(yj ,xj)where Zx is the input-dependent nor-malizer and factor fc parameterizes thecluster-wise noun phrase compatibility asfc(yj ,xj) = exp(?k ?kfk(yj , xj)).
Again,factor ft enforces the transitivity constraints byft(?)
= ??
if transitivity is not satisfied, 1 other-wise.
Here, transitivity is a bit more complicated,since it also requires that if yj = 1, then for anysubset xk ?
xj , yk = 1.
A factor graph for theFirst-Order Model is presented in Figure 3 for threenoun phrases.The methods described in Sections 2, 3 and 4 canbe viewed as estimating the parameters of each fac-tor fc independently.
This approach can thereforebe viewed as a type of piecewise approximation ofexact parameter estimation in these models (Suttonand McCallum, 2005).
Here, each fc is a ?piece?of the model trained independently.
These piecesare combined at prediction time using clustering al-gorithms to enforce transitivity.
Sutton and McCal-lum (2005) show that such a piecewise approxima-tion can be theoretically justified as minimizing anupper bound of the exact loss function.6 Experiments6.1 DataWe apply our approach to the noun coreference ACE2004 data, containing 443 news documents with28,135 noun phrases to be coreferenced.
336 doc-uments are used for training, and the remainder fortesting.
All entity types are candidates for corefer-ence (pronouns, named entities, and nominal enti-ties).
We use the true entity segmentation, and parseeach sentence in the corpus using a phrase-structuregrammar, as is common for this task.6.2 FeaturesWe follow Soon et al (2001) and Ng and Cardie(2002) to generate most of our features for the Pair-wise Model.
These include:?
Match features - Check whether gender, num-ber, head text, or entire phrase matches?
Mention type (pronoun, name, nominal)?
Aliases - Heuristically decide if one noun is theacronym of the other?
Apposition - Heuristically decide if one noun isin apposition to the other?
Relative Pronoun - Heuristically decide if onenoun is a relative pronoun referring to the other.?
Wordnet features - Use Wordnet to decide ifone noun is a hypernym, synonym, or antonymof another, or if they share a hypernym.?
Both speak - True if both contain an adjacentcontext word that is a synonym of ?said.?
Thisis a domain-specific feature that helps for manynewswire articles.?
Modifiers Match - for example, in the phrase?President Clinton?, ?President?
is a modifierof ?Clinton?.
This feature indicates if one nounis a modifier of the other, or they share a modi-fier.?
Substring - True if one noun is a substring ofthe other (e.g.
?Egypt?
and ?Egyptian?
).The First-OrderModel includes the following fea-tures:?
Enumerate each pair of noun phrases and com-pute the features listed above.
All-X is true ifall pairs share a featureX ,Most-True-X is trueif the majority of pairs share a feature X , andMost-False-X is true if most of the pairs do notshare feature X .85?
Use the output of the Pairwise Model for eachpair of nouns.
All-True is true if all pairs arepredicted to be coreferent, Most-True is true ifmost pairs are predicted to be coreferent, andMost-False is true if most pairs are predictedto not be coreferent.
Additionally, Max-Trueis true if the maximum pairwise score is abovethreshold, and Min-True if the minimum pair-wise score is above threshold.?
Cluster Size indicates the size of the cluster.?
Count how many phrases in the cluster areof each mention type (name, pronoun, nom-inal), number (singular/plural) and gender(male/female).
The features All-X and Most-True-X indicate how frequent each feature isin the cluster.
This feature can capture the softconstraint such that no cluster consists only ofpronouns.In addition to the listed features, we also includeconjunctions of size 2, for example ?Genders matchAND numbers match?.6.3 EvaluationWe use the B3 algorithm to evaluate the predictedcoreferent clusters (Amit and Baldwin, 1998).
B3is common in coreference evaluation and is similarto the precision and recall of coreferent links, ex-cept that systems are rewarded for singleton clus-ters.
For each noun phrase xi, let ci be the numberof mentions in xi?s predicted cluster that are in factcoreferent with xi (including xi itself).
Precision forxi is defined as ci divided by the number of nounphrases in xi?s cluster.
Recall for xi is defined asthe ci divided by the number of mentions in the goldstandard cluster for xi.
F1 is the harmonic mean ofrecall and precision.6.4 ResultsIn addition to Pairwise, First-Order Uniform, andFirst-Order MIRA, we also compare against Pair-wise MIRA, which differs from First-Order MIRAonly by the fact that it is restricted to pairwise fea-tures.Table 1 suggests both that first-order features anderror-driven training can greatly improve perfor-mance.
The First-OrderModel outperforms the Pair-F1 Prec RecFirst-Order MIRA 79.3 86.7 73.2Pairwise MIRA 72.5 92.0 59.8First-Order Uniform 69.2 79.0 61.5Pairwise 62.4 62.5 62.3Table 1: B3 results for ACE noun phrase corefer-ence.
FIRST-ORDER MIRA is our proposed modelthat takes advantage of first-order features of thedata and is trained with error-driven and rank-basedmethods.
We see that both the first-order featuresand the training enhancements improve performanceconsistently.wise Model in F1 measure for both standard train-ing and error-driven training.
We attribute some ofthis improvement to the capability of the First-Ordermodel to capture features of entire clusters that mayindicate some phrases are not coreferent.
Also, weattribute the gains from error-driven training to thefact that training examples are generated based onerrors made on the training data.
(However, weshould note that there are also small differences inthe feature sets used for error-driven and standardtraining results.
)Error analysis indicates that often noun xi is cor-rectly not merged with a cluster xj when xj has astrong internal coherence.
For example, if all 5 men-tions of France in a document are string identical,then the system will be extremely cautious of merg-ing a noun that is not equivalent to France into xj ,since this will turn off the ?All-String-Match?
fea-ture for cluster xj .To our knowledge, the best results on this datasetwere obtained by the meta-classification scheme ofNg (2005).
Although our train-test splits may differslightly, the best B-Cubed F1 score reported in Ng(2005) is 69.3%, which is considerably lower thanthe 79.3% obtained with our method.
Also note thatthe Pairwise baseline obtains results similar to thosein Ng and Cardie (2002).7 Related WorkThere has been a recent interest in training methodsthat enable the use of first-order features (Paskin,2002; Daume?
III and Marcu, 2005b; Richardsonand Domingos, 2006).
Perhaps the most related is86?learning as search optimization?
(LASO) (Daume?III and Marcu, 2005b; Daume?
III and Marcu,2005a).
Like the current paper, LASO is also anerror-driven training method that integrates predic-tion and training.
However, whereas we explic-itly use a ranking-based loss function, LASO usesa binary classification loss function that labels eachcandidate structure as correct or incorrect.
Thus,each LASO training example contains all candidatepredictions, whereas our training examples containonly the highest scoring incorrect prediction and thehighest scoring correct prediction.
Our experimentsshow the advantages of this ranking-based loss func-tion.
Additionally, we provide an empirical study toquantify the effects of different example generationand loss function decisions.Collins and Roark (2004) present an incrementalperceptron algorithm for parsing that uses ?early up-date?
to update the parameters when an error is en-countered.
Our method uses a similar ?early update?in that training examples are only generated for thefirst mistake made during prediction.
However, theydo not investigate rank-based loss functions.Others have attempted to train global scoringfunctions using Gibbs sampling (Finkel et al, 2005),message propagation, (Bunescu and Mooney, 2004;Sutton and McCallum, 2004), and integer linear pro-gramming (Roth and Yih, 2004).
The main distinc-tions of our approach are that it is simple to imple-ment, not computationally intensive, and adaptableto arbitrary loss functions.There have been a number of machine learningapproaches to coreference resolution, traditionallyfactored into classification decisions over pairs ofnouns (Soon et al, 2001; Ng and Cardie, 2002).Nicolae and Nicolae (2006) combine pairwise clas-sification with graph-cut algorithms.
Luo et al(2004) do enable features between mention-clusterpairs, but do not perform the error-driven and rank-ing enhancements proposed in our work.
Denis andBaldridge (2007) use a ranking loss function for pro-noun coreference; however the examples are stillpairs of pronouns, and the example generation is noterror driven.
Ng (2005) learns a meta-classifier tochoose the best prediction from the output of sev-eral coreference systems.
While in theory a meta-classifier can flexibly represent features, they do notexplore features using the full flexibility of first-order logic.
Also, their method is neither error-driven nor rank-based.McCallum and Wellner (2003) use a conditionalrandom field that factors into a product of pairwisedecisions about pairs of nouns.
These pairwise de-cisions are made collectively using relational infer-ence; however, as pointed out in Milch et al (2004),this model has limited representational power sinceit does not capture features of entities, only of pairsof mention.
Milch et al (2005) address these issuesby constructing a generative probabilistic model,where noun clusters are sampled from a generativeprocess.
Our current work has similar representa-tional flexibility as Milch et al (2005) but is discrim-inatively trained.8 Conclusions and Future WorkWe have presented learning and inference proce-dures for coreference models using first-order fea-tures.
By relying on sampling methods at trainingtime and approximate inference methods at testingtime, this approach can be made scalable.
This re-sults in a coreference model that can capture featuresover sets of noun phrases, rather than simply pairs ofnoun phrases.This is an example of a model with extremelyflexible representational power, but for which exactinference is intractable.
The simple approximationswe have described here have enabled this more flex-ible model to outperform a model that is simplifiedfor tractability.A short-term extension would be to consider fea-tures over entire clusterings, such as the number ofclusters.
This could be incorporated in a rankingscheme, as in Ng (2005).Future work will extend our approach to a widervariety of tasks.
The model we have described hereis specific to clustering tasks; however a similar for-mulation could be used to approach a number of lan-guage processing tasks, such as parsing and relationextraction.
These tasks could benefit from first-orderfeatures, and the present work can guide the approx-imations required in those domains.Additionally, we are investigating more sophis-ticated inference algorithms that will reduce thegreediness of the search procedures described here.87AcknowledgmentsWe thank Robert Hall for helpful contributions.
This workwas supported in part by the Defense Advanced ResearchProjects Agency (DARPA), through the Department of theInterior, NBC, Acquisition Services Division, under con-tract #NBCHD030010, in part by U.S. Government contract#NBCH040171 through a subcontract with BBNT SolutionsLLC, in part by The Central Intelligence Agency, the NationalSecurity Agency and National Science Foundation under NSFgrant #IIS-0326249, in part by Microsoft Live Labs, and in partby the Defense Advanced Research Projects Agency (DARPA)under contract #HR0011-06-C-0023.
Any opinions, findingsand conclusions or recommendations expressed in this mate-rial are the author(s)?
and do not necessarily reflect those of thesponsor.ReferencesB.
Amit and B. Baldwin.
1998.
Algorithms for scoring coref-erence chains.
In Proceedings of the Seventh Message Un-derstanding Conference (MUC7).Razvan Bunescu and Raymond J. Mooney.
2004.
Collectiveinformation extraction with relational markov networks.
InACL.Y.
Censor and S.A. Zenios.
1997.
Parallel optimization : the-ory, algorithms, and applications.
Oxford University Press.Michael Collins and Brian Roark.
2004.
Incremental parsingwith the perceptron algorithm.
In ACL.Koby Crammer and Yoram Singer.
2003.
Ultraconservativeonline algorithms for multiclass problems.
JMLR, 3:951?991.Aron Culotta and Andrew McCallum.
2006.
Tractable learn-ing and inference with high-order representations.
In ICMLWorkshop on Open Problems in Statistical Relational Learn-ing, Pittsburgh, PA.Hal Daume?
III and Daniel Marcu.
2005a.
A large-scale explo-ration of effective global features for a joint entity detectionand tracking model.
In HLT/EMNLP, Vancouver, Canada.Hal Daume?
III and Daniel Marcu.
2005b.
Learning as searchoptimization: Approximate large margin methods for struc-tured prediction.
In ICML, Bonn, Germany.Rodrigo de Salvo Braz, Eyal Amir, and Dan Roth.
2005.
Liftedfirst-order probabilistic inference.
In IJCAI, pages 1319?1325.Pascal Denis and Jason Baldridge.
2007.
A ranking approachto pronoun resolution.
In IJCAI.Jenny Rose Finkel, Trond Grenager, and Christopher Manning.2005.
Incorporating non-local information into informationextraction systems by gibbs sampling.
In ACL, pages 363?370.H.
Gaifman.
1964.
Concerning measures in first order calculi.Israel J.
Math, 2:1?18.J.
Y. Halpern.
1990.
An analysis of first-order logics of proba-bility.
Artificial Intelligence, 46:311?350.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kamb-hatla, and Salim Roukos.
2004.
A mention-synchronouscoreference resolution algorithm based on the Bell tree.
InACL, page 135.A.
McCallum and B. Wellner.
2003.
Toward conditional mod-els of identity uncertainty with application to proper nouncoreference.
In IJCAI Workshop on Information Integrationon the Web.Andrew McCallum and Ben Wellner.
2005.
Conditional mod-els of identity uncertainty with application to noun corefer-ence.
In Lawrence K. Saul, Yair Weiss, and Le?on Bottou,editors, NIPS17.
MIT Press, Cambridge, MA.Brian Milch, Bhaskara Marthi, and Stuart Russell.
2004.BLOG: Relational modeling with unknown objects.
InICML 2004 Workshop on Statistical Relational Learning andIts Connections to Other Fields.Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag,Daniel L. Ong, and Andrey Kolobov.
2005.
BLOG: Proba-bilistic models with unknown objects.
In IJCAI.Vincent Ng and Claire Cardie.
2002.
Improving machine learn-ing approaches to coreference resolution.
In ACL.Vincent Ng.
2005.
Machine learning for coreference resolu-tion: From local classification to global ranking.
In ACL.Cristina Nicolae and Gabriel Nicolae.
2006.
Bestcut: A graphalgorithm for coreference resolution.
In EMNLP, pages275?283, Sydney, Australia, July.
Association for Compu-tational Linguistics.Mark A. Paskin.
2002.
Maximum entropy probabilistic logic.Technical Report UCB/CSD-01-1161, University of Califor-nia, Berkeley.D.
Poole.
2003.
First-order probabilistic inference.
In IJCAI,pages 985?991, Acapulco, Mexico.
Morgan Kaufman.Matthew Richardson and Pedro Domingos.
2006.
Markovlogic networks.
Machine Learning, 62:107?136.D.
Roth and W. Yih.
2004.
A linear programming formulationfor global inference in natural language tasks.
In The 8thConference on Compuational Natural Language Learning,May.Parag Singla and Pedro Domingos.
2005.
Discriminative train-ing of markov logic networks.
In AAAI, Pittsburgh, PA.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim.2001.
A machine learning approach to coreference resolu-tion of noun phrases.
Comput.
Linguist., 27(4):521?544.Charles Sutton and Andrew McCallum.
2004.
Collective seg-mentation and labeling of distant entities in information ex-traction.
Technical Report TR # 04-49, University of Mas-sachusetts, July.Charles Sutton and Andrew McCallum.
2005.
Piecewise train-ing of undirected models.
In 21st Conference on Uncertaintyin Artificial Intelligence.88
