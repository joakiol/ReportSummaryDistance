Proceedings of the EACL 2009 Demonstrations Session, pages 53?56,Athens, Greece, 3 April 2009. c?2009 Association for Computational LinguisticsA text-based search interface for Multimedia DialecticsKaterina PastraInst.
for Language & Speech ProcessingAthens, Greecekpastra@ilsp.grEirini BaltaInst.
for Language & Speech ProcessingAthens, Greeceebalta@ilsp.grAbstractThe growing popularity of multimediadocuments requires language technologiesto approach automatic language analysisand generation from yet another perspec-tive: that of its use in multimodal commu-nication.
In this paper, we present a sup-port tool for COSMOROE, a theoreticalframework for modelling multimedia di-alectics.
The tool is a text-based search in-terface that facilitates the exploration of acorpus of audiovisual files, annotated withthe COSMOROE relations.1 IntroductionOnline multimedia content becomes more andmore accessible through digital TV, social net-working sites and searchable digital libraries ofphotographs and videos.
People of different agesand cultures attempt to make sense out of this dataand re-package it for their own needs, these beinginformative, educational and entertainment ones.Understanding and generation of multimedia dis-course requires knowledge and skills related to thenature of the interacting modalities and their se-mantic interplay for formulating the multimediamessage.Within such context, intelligent multimedia sys-tems are expected to parse/generate such messagesor at least assist humans in these tasks.
From an-other perspective, everyday human communica-tion is predominantly multimodal; as such, sim-ilarly intuitive human-computer/robot interactiondemands that intelligent systems master ?amongothers?
the semantic interplay between differ-ent media and modalities, i.e.
they are able touse/understand natural language and its referenceto objects and activities in the shared, situatedcommunication space.It was more than a decade ago, when the lackof a theory of how different media interact withone another was indicated (Whittaker and Walker,1991).
Recently, such theoretical framework hasbeen developed and used for annotating a corpusof audiovisual documents with the objective of us-ing such corpus for developing multimedia infor-mation processing tools (Pastra, 2008).
In this pa-per, we provide a brief overview of the theory andthe corresponding annotated corpus and presenta text-based search interface that has been devel-oped for the exploration and the automatic expan-sion/generalisation of the annotated semantic rela-tions.
This search interface is a support tool forthe theory and the related corpus and a first steptowards its computational exploitation.2 COSMOROEThe CrOSs-Media inteRactiOn rElations (COS-MOROE) framework describes multimedia di-alectics, i.e.
the semantic interplay betweenimages, language and body movements (Pastra,2008).
It uses an analogy to language discourseanalysis for ?talking?
about multimedia dialectics.It actually borrows terms that are widely used inlanguage analysis for describing a number of phe-nomena (e.g.
metonymy, adjuncts etc.)
and adoptsa message-formation perspective which is remi-niscent of structuralistic approaches in languagedescription.
While doing so, inherent character-istics of the different modalities (e.g.
exhaustivespecificity of images) are taken into consideration.COSMOROE is the result of a thorough,inter-disciplinary review of image-language andgesture-language interaction relations and charac-teristics, as described across a number of disci-plines from computational and semiotic perspec-tives.
It is also the result of observation and analy-sis of different types of corpora for different tasks.COSMOROE was tested for its coverage and de-scriptive power through the annotation of a corpusof TV travel documentaries.
Figure 1 presents theCOSMOROE relations.
There are three main rela-53tions: semantic equivalence, complementarity andindependence, each with each own subtypes.Figure 1: The COSMOROE cross-media relationsFor annotating a corpus with the COSMOROErelations, a multi-faceted annotation scheme isemployed.
COSMOROE relations link two ormore annotation facets, i.e.
the modalities oftwo or more different media.
Time offsetsof the transcribed speech, subtitles, graphic-textand scene text, body movements, gestures, shots(with foreground and background distinction) andkeyframe-regions are identified and included inCOSMOROE relations.
All visual data have beenlabelled by the annotators with one or two-wordaction or entity denoting tags.
These labels haveresulted from a process of watching only the vi-sual stream of the file.
The labelling followed acognitive categorisation approach, that builds onthe ?basic level theory?
of categorisation (Rosch,1978).
Currently, the annotated corpus consistsof 5 hours of TV travel documentaries in Greekand 5 hours of TV travel documentaries in En-glish.
Three hours of the Greek files have under-gone validation and a preliminary inter-annotatoragreement study has also been carried out (Pastra,2008).3 The COSMOROE Search InterfaceSuch rich semantically annotated multimedia cor-pus requires a support tool that will serve the fol-lowing:?
it will facilitate the active exploration andpresentation of the semantic interplay be-tween different modalities for any user, illus-trating the COSMOROE theory through spe-cific examples from real audiovisual data?
it will serve as simple search interface forgeneral users, taking advantage of the rich se-mantic annotation ?behind the scenes?
formore precise and intelligent retrieval of au-diovisual files?
it will allow for observation and educateddecision-taking on how one could proceedwith mining the corpus or using it as train-ing data for semantic multimedia processingapplications, and?
it will allow interfacing with semantic lexicalresources, computational lexicons, text pro-cessing components and cross-lingual infor-mation resources for automatically expand-ing and generalising the data (semantic rela-tions) one can mine from the corpus.We have developed such tool, the COSMOROEsearch interface.
The interface itself is actuallya text-based search engine, that indexes and re-trieves information from the COSMOROE anno-tated corpus.
The interface allows for both sim-ple search and advanced search, depending on thetype and needs of the users.
The advanced searchis designed for those who have a special interestin multimedia semantics and/or ones who want todevelop systems that will be trained on the COS-MOROE corpus.
This advanced version allowssearch in a text-based manner, in either of theseways:?
Search using single or multiword query terms(keywords) that are mentioned in the tran-scribed speech (or other text) of the videoor in the visual labels set of its visual-units,in order to find instantiations of different se-mantic relations in which they participate;?
Search using a pair of single or multi-word query terms (keywords) that are relatedthrough (un)specified semantic relations;?
Search for specific types of relations and findout how these are realized through actual in-stances in a certain multimedia context;?
Search for specific modality types (e.g.
spe-cific types of gestures, image-regions etc.
)and find out all the different relations inwhich they appear;54Figure 2 presents a search example, using theadvanced interface1.
The user has opted to searchfor all instances of the word ?bell?
appearing inthe visual label of keyframe regions and/or videoshots and in particular ones in which the bell isclearly shown either in the foreground or in thebackground.
In a similar way, the user can searchFigure 2: Search examplefor concepts present in the audio part of the video,through the use of the ?Transcribed Text?
option ormake a multiple selection.
Another possibility isto use a ?Query 2?
set, in conjunction, disjunctionor negation with ?Query 1?, in order to obtain therelations through which two categories of conceptsare associated.Multimedia relations can also be searched in-dependently of their content, simply denoting thedesired type.
Finally, the user can search for spe-cial types of visual-units, such as body move-ments, gestures, images, without defining the con-cept they denote.After executing the query, the user is presentedwith the list of the results, grouped by the semanticrelation in which the visual labels ?in the exam-ple case presented above?
participate.
Each hitis accompanied by its transcribed speech.
Indica-tion of the number of results found is given andthe user has also the option to save the results ofthe specific search session.
By clicking on indi-vidual hits in the result list, one may investigatethe corresponding relation particulars.Figure 3 shows such detailed view of one of theresults of the query shown in Figure 2.
All relation1Only part of the advanced search interface is depicted forthe screenshot to be intelligibleFigure 3: Example result - relation templatecomponents are presented, textual and visual ones.There are links to the video file from which the re-lation comes, at the specified time offsets.
Also,the user may watch the video clips of the modali-ties that participate in the relation (e.g.
a particu-lar gesture) and/or a static image (keyframe) of aparticipating image region (e.g.
a specific object)with the contour of the object highlighted.In this example, one may see that the word?monastery?, which was mentioned in the tran-scribed speech of the file, is grounded to the videosequence depicting a ?bell tower?
in the back-ground and to another image of a ?bell?, througha metonymic relation of type ?part for whole?.What is actually happening, from a semantic pointof view, is that although the video talks about a?monastery?, it never actually shows the building,it shows a characteristic part of it instead.
In thispage, the option to save these relation elements asa text file, is also provided.Last, a user may get a quantified profile of thecontents of the database (the annotated corpus) interms of number of relations per type, per lan-guage, per file, or even per file producer, numberof visual objects, gestures of different types, body55movements, word tokens, visual labels, frequen-cies of such data per file/set of files, as well as co-occurrence statistics on word-visual label pairs perrelation/file/language and other parameters.For the novice or general user, a simple inter-face is provided that allows the user to submita text query, with no other specifications.
Theresults consist of a hit list with thumbnails ofthe video-clips related to the query and the cor-responding transcribed utterance.
Individual hitslead to full viewing of the video clip.
Further de-tails on the hit, i.e.
information an advanced userwould get, are available following the advance-information link.
The use of semantic relations inmultimedia data, in this case, is hidden in the wayresults are sorted in the results list.
The sorting fol-lows a highly to less informative pattern relyingon whether the transcript words or visual labelsmatched to the query participate in cross-mediarelations or not, and in which relation.
Automat-ing the processing of audiovisual files for the ex-traction of cross-media semantics, in order to getthis type of ?intelligence?
in search and retrievalwithin digital video archives, is the ultimate ob-jective of the COSMOROE approach.3.1 Technical DetailsIn developing the COSMOROE search interface,specific application needs had to be taken intoconsideration.
The main goal was to develop atext-based search engine module capable of han-dling files in the .xml format and accessed by lo-cal and remote users.
The core implementationis actually a web application, mainly based onthe Apache Lucene2 search engine library.
Thischoice is supported by Lucene?s intrinsic charac-teristics, such as high-performance indexing andsearching, scalability and customization optionsand open source, cross-platform implementation,that render it one of the most suitable solutions fortext-based search.In particular, we exploited and further devel-oped the built-in features of Lucene, in order tomeet our design criteria:?
The relation specific .xml files were indexedin a way that retained their internal treestructure, while multilingual files can eas-ily be handled during indexing and searchingphases;2http://lucene.apache.org/?
The queries are formed in a text-like man-ner by the user, but are treated in a combinedway by the system, that enables a relationalsearch, enhanced with linguistic capabilities;?
The results are shown using custom sortingmethods, making them more presentable andeasily browsed by the user;?
Since Lucene is written in Java the applica-tion is basically platform-independent;?
The implementation of the Lucene search en-gine as a web application makes it easily ac-cessible to local and remote users, through asimple web browser page.During the results presentation phase, a specialissue had to be taken into consideration, that isvideo sharing.
Due to performance and securityreasons, the Red53 server is used, which is an opensource flash server, supporting secure streamingvideo.4 Conclusion: towards computationalmodelling of multimedia dialecticsThe COSMOROE search interface presented inthis paper is the first phase for supporting thecomputational modelling of multimedia dialectics.The tool aims at providing a user-friendly accessto the COSMOROE corpus, illustrating the theorythrough specific examples and providing an inter-face platform for reaching towards computationallinguistic resources and tools that will generaliseover the semantic information provided by the cor-pus.
Last, the tool illustrates the hidden intelli-gence one could achieve with cross-media seman-tics in search engines of the future.ReferencesK.
Pastra.
2008.
Cosmoroe: A cross-media rela-tions framework for modelling multimedia dialec-tics.
Multimedia Systems, 14:299?323.E.
Rosch.
1978.
Principles of categorization.
InE.
Rosch and B. Lloyd, editors, Cognition and Cat-egorization, chapter 2, pages 27?48.
Lawrence Erl-baum Associates.S.
Whittaker and M. Walker.
1991.
Toward a theoryof multi-modal interaction.
In Proceedings of theNational Conference on Artificial Intelligence Work-shop on Multi-modal Interaction.3http://osflash.org/red5/56
