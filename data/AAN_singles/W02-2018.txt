A comparison of algorithms for maximum entropy parameter estimationRobert MaloufAlfa-InformaticaRijksuniversiteit GroningenPostbus 7169700AS GroningenThe Netherlandsmalouf@let.rug.nlAbstractConditional maximum entropy (ME) models pro-vide a general purpose machine learning techniquewhich has been successfully applied to fields asdiverse as computer vision and econometrics, andwhich is used for a wide variety of classificationproblems in natural language processing.
However,the flexibility of ME models is not without cost.While parameter estimation for ME models is con-ceptually straightforward, in practice ME modelsfor typical natural language tasks are very large, andmay well contain many thousands of free parame-ters.
In this paper, we consider a number of algo-rithms for estimating the parameters of ME mod-els, including iterative scaling, gradient ascent, con-jugate gradient, and variable metric methods.
Sur-prisingly, the standardly used iterative scaling algo-rithms perform quite poorly in comparison to theothers, and for all of the test problems, a limited-memory variable metric algorithm outperformed theother choices.1 IntroductionMaximum entropy (ME) models, variously knownas log-linear, Gibbs, exponential, and multinomiallogit models, provide a general purpose machinelearning technique for classification and predictionwhich has been successfully applied to fields as di-verse as computer vision and econometrics.
In natu-ral language processing, recent years have seen MEtechniques used for sentence boundary detection,part of speech tagging, parse selection and ambigu-ity resolution, and stochastic attribute-value gram-mars, to name just a few applications (Abney, 1997;Berger et al, 1996; Ratnaparkhi, 1998; Johnson etal., 1999).A leading advantage of ME models is their flex-ibility: they allow stochastic rule systems to beaugmented with additional syntactic, semantic, andpragmatic features.
However, the richness of therepresentations is not without cost.
Even mod-est ME models can require considerable computa-tional resources and very large quantities of anno-tated training data in order to accurately estimatethe model?s parameters.
While parameter estima-tion for ME models is conceptually straightforward,in practice ME models for typical natural languagetasks are usually quite large, and frequently containhundreds of thousands of free parameters.
Estima-tion of such large models is not only expensive, butalso, due to sparsely distributed features, sensitiveto round-off errors.
Thus, highly efficient, accurate,scalable methods are required for estimating the pa-rameters of practical models.In this paper, we consider a number of algorithmsfor estimating the parameters of ME models, in-cluding Generalized Iterative Scaling and ImprovedIterative Scaling, as well as general purpose opti-mization techniques such as gradient ascent, conju-gate gradient, and variable metric methods.
Sur-prisingly, the widely used iterative scaling algo-rithms perform quite poorly, and for all of the testproblems, a limited memory variable metric algo-rithm outperformed the other choices.2 Maximum likelihood estimationSuppose we are given a probability distribution pover a set of events X which are characterized by ad dimensional feature vector function f : X ?
Rd .In addition, we have also a set of contexts W and afunction Y which partitions the members of X .
Inthe case of a stochastic context-free grammar, forexample, X might be the set of possible trees, thefeature vectors might represent the number of timeseach rule applied in the derivation of each tree, Wmight be the set of possible strings of words, andY (w) the set of trees whose yield is w ?W .
A con-ditional maximum entropy model q?
(x|w) for p hasthe parametric form (Berger et al, 1996; Chi, 1998;Johnson et al, 1999):q?
(x|w) =exp(?T f (x))?y?Y (w) exp(?T f (y))(1)where ?
is a d-dimensional parameter vector and?T f (x) is the inner product of the parameter vectorand a feature vector.Given the parametric form of an ME model in(1), fitting an ME model to a collection of trainingdata entails finding values for the parameter vector?
which minimize the Kullback-Leibler divergencebetween the model q?
and the empirical distribu-tion p:D(p||q?)
= ?w,xp(x,w) log p(x|w)q?
(x|w)or, equivalently, which maximize the log likelihood:L(?)
= ?w,xp(w,x) logq?
(x|w) (2)The gradient of the log likelihood function, or thevector of its first derivatives with respect to the pa-rameter ?
is:G(?)
= Ep[ f ]?Eq?
[ f ] (3)Since the likelihood function (2) is concave overthe parameter space, it has a global maximum wherethe gradient is zero.
Unfortunately, simply settingG(?)
= 0 and solving for ?
does not yield a closedform solution, so we proceed iteratively.
At eachstep, we adjust an estimate of the parameters ?
(k)to a new estimate ?
(k+1) based on the divergencebetween the estimated probability distribution q(k)and the empirical distribution p. We continue untilsuccessive improvements fail to yield a sufficientlylarge decrease in the divergence.While all parameter estimation algorithms wewill consider take the same general form, themethod for computing the updates ?
(k) at eachsearch step differs substantially.
As we shall see,this difference can have a dramatic impact on thenumber of updates required to reach convergence.2.1 Iterative ScalingOne popular method for iteratively refining themodel parameters is Generalized Iterative Scaling(GIS), due to Darroch and Ratcliff (1972).
Anextension of Iterative Proportional Fitting (Dem-ing and Stephan, 1940), GIS scales the probabil-ity distribution q(k) by a factor proportional to theratio of Ep[ f ] to Eq(k) [ f ], with the restriction that?
j f j(x) = C for each event x in the training data(a condition which can be easily satisfied by the ad-dition of a correction feature).
We can adapt GISto estimate the model parameters ?
rather than themodel probabilities q, yielding the update rule:?
(k) = log(Ep[ f ]Eq(k) [ f ]) 1CThe step size, and thus the rate of convergence,depends on the constant C: the larger the value ofC, the smaller the step size.
In case not all rows ofthe training data sum to a constant, the addition of acorrection feature effectively slows convergence tomatch the most difficult case.
To avoid this slowedconvergence and the need for a correction feature,Della Pietra et al (1997) propose an Improved Iter-ative Scaling (IIS) algorithm, whose update rule isthe solution to the equation:Ep[ f ] = ?w,xp(w)q(k)(x|w) f (x)exp(M(x)?
(k))where M(x) is the sum of the feature values for anevent x in the training data.
This is a polynomial inexp(?
(k)), and the solution can be found straight-forwardly using, for example, the Newton-Raphsonmethod.2.2 First order methodsIterative scaling algorithms have a long tradition instatistics and are still widely used for analysis ofcontingency tables.
Their primary strength is thaton each iteration they only require computation ofthe expected values Eq(k) .
They do not depend onevaluation of the gradient of the log-likelihood func-tion, which, depending on the distribution, could beprohibitively expensive.
In the case of ME models,however, the vector of expected values required byiterative scaling essentially is the gradient G. Thus,it makes sense to consider methods which use thegradient directly.The most obvious way of making explicit use ofthe gradient is by Cauchy?s method, or the methodof steepest ascent.
The gradient of a function is avector which points in the direction in which thefunction?s value increases most rapidly.
Since ourgoal is to maximize the log-likelihood function, anatural strategy is to shift our current estimate ofthe parameters in the direction of the gradient viathe update rule:?
(k) = ?(k)G(?
(k))where the step size ?
(k) is chosen to maximizeL(?
(k) + ?(k)).
Finding the optimal step size is itselfan optimization problem, though only in one dimen-sion and, in practice, only an approximate solutionis required to guarantee global convergence.Since the log-likelihood function is concave, themethod of steepest ascent is guaranteed to find theglobal maximum.
However, while the steps takenon each iteration are in a very narrow sense locallyoptimal, the global convergence rate of steepest as-cent is very poor.
Each new search direction is or-thogonal (or, if an approximate line search is used,nearly so) to the previous direction.
This leads toa characteristic ?zig-zag?
ascent, with convergenceslowing as the maximum is approached.One way of looking at the problem with steep-est ascent is that it considers the same search di-rections many times.
We would prefer an algo-rithm which considered each possible search direc-tion only once, in each iteration taking a step of ex-actly the right length in a direction orthogonal to allprevious search directions.
This intuition underliesconjugate gradient methods, which choose a searchdirection which is a linear combination of the steep-est ascent direction and the previous search direc-tion.
The step size is selected by an approximateline search, as in the steepest ascent method.
Sev-eral non-linear conjugate gradient methods, such asthe Fletcher-Reeves (cg-fr) and the Polak-Ribie`re-Positive (cf-prp) algorithms, have been proposed.While theoretically equivalent, they use slighly dif-ferent update rules and thus show different numericproperties.2.3 Second order methodsAnother way of looking at the problem with steep-est ascent is that while it takes into account the gra-dient of the log-likelihood function, it fails to takeinto account its curvature, or the gradient of the gra-dient.
The usefulness of the curvature is made clearif we consider a second-order Taylor series approx-imation of L(?
+ ?):L(?
+ ?)?
L(?
)+ ?T G(?
)+ 12?T H(?)?
(4)where H is Hessian matrix of the log-likelihoodfunction, the d ?
d matrix of its second partialderivatives with respect to ?.
If we set the deriva-tive of (4) to zero and solve for ?, we get the updaterule for Newton?s method:?
(k) = H?1(?(k))G(?
(k)) (5)Newton?s method converges very quickly (forquadratic objective functions, in one step), but it re-quires the computation of the inverse of the Hessianmatrix on each iteration.While the log-likelihood function for ME modelsin (2) is twice differentiable, for large scale prob-lems the evaluation of the Hessian matrix is com-putationally impractical, and Newton?s method isnot competitive with iterative scaling or first ordermethods.
Variable metric or quasi-Newton methodsavoid explicit evaluation of the Hessian by buildingup an approximation of it using successive evalua-tions of the gradient.
That is, we replace H?1(?
(k))in (5) with a local approximation of the inverse Hes-sian B(k):?
(k) = B(k)G(?
(k))with B(k) a symmatric, positive definite matrixwhich satisfies the equation:B(k)y(k) = ?
(k?1)where y(k) = G(?(k))?G(?
(k?1)).Variable metric methods also show excellent con-vergence properties and can be much more efficientthan using true Newton updates, but for large scaleproblems with hundreds of thousands of parame-ters, even storing the approximate Hessian is pro-hibitively expensive.
For such cases, we can applylimited memory variable metric methods, which im-plicitly approximate the Hessian matrix in the vicin-ity of the current estimate of ?
(k) using the previousm values of y(k) and ?(k).
Since in practical applica-tions values of m between 3 and 10 suffice, this canoffer a substantial savings in storage requirementsover variable metric methods, while still giving fa-vorable convergence properties.13 Comparing estimation techniquesThe performance of optimization algorithms ishighly dependent on the specific properties of theproblem to be solved.
Worst-case analysis typically1Space constraints preclude a more detailed discussion ofthese methods here.
For algorithmic details and theoreticalanalysis of first and second order methods, see, e.g., Nocedal(1997) or Nocedal and Wright (1999).does not reflect the actual behavior on actual prob-lems.
Therefore, in order to evaluate the perfor-mance of the optimization techniques sketched inprevious section when applied to the problem of pa-rameter estimation, we need to compare the perfor-mance of actual implementations on realistic datasets (Dolan and More?, 2002).Minka (2001) offers a comparison of iterativescaling with other algorithms for parameter esti-mation in logistic regression, a problem similar tothe one considered here, but it is difficult to trans-fer Minka?s results to ME models.
For one, heevaluates the algorithms with randomly generatedtraining data.
However, the performance and accu-racy of optimization algorithms can be sensitive tothe specific numerical properties of the function be-ing optimized; results based on random data mayor may not carry over to more realistic problems.And, the test problems Minka considers are rela-tively small (100?500 dimensions).
As we haveseen, though, algorithms which perform well forsmall and medium scale problems may not alwaysbe applicable to problems with many thousands ofdimensions.3.1 ImplementationAs a basis for the implementation, we have usedPETSc (the ?Portable, Extensible Toolkit for Sci-entific Computation?
), a software library designedto ease development of programs which solve largesystems of partial differential equations (Balay etal., 2001; Balay et al, 1997; Balay et al, 2002).PETSc offers data structures and routines for paral-lel and sequential storage, manipulation, and visu-alization of very large sparse matrices.For any of the estimation techniques, the most ex-pensive operation is computing the probability dis-tribution q and the expectations Eq[ f ] for each it-eration.
In order to make use of the facilities pro-vided by PETSc, we can store the training data asa (sparse) matrix F , with rows corresponding toevents and columns to features.
Then given a pa-rameter vector ?, the unnormalized probabilities q?
?are the matrix-vector product:q??
= expF?and the feature expectations are the transposedmatrix-vector product:Eq?
[ f ] = FT q?By expressing these computations as matrix-vectoroperations, we can take advantage of the high per-formance sparse matrix primitives of PETSc.For the comparison, we implemented both Gener-alized and Improved Iterative Scaling in C++ usingthe primitives provided by PETSc.
For the other op-timization techniques, we used TAO (the ?Toolkitfor Advanced Optimization?
), a library layered ontop of the foundation of PETSc for solving non-linear optimization problems (Benson et al, 2002).TAO offers the building blocks for writing optimiza-tion programs (such as line searches and conver-gence tests) as well as high-quality implementationsof standard optimization algorithms (including con-jugate gradient and variable metric methods).Before turning to the results of the comparison,two additional points need to be made.
First, inorder to assure a consistent comparison, we needto use the same stopping rule for each algorithm.For these experiments, we judged that convergencewas reached when the relative change in the log-likelihood between iterations fell below a predeter-mined threshold.
That is, each run was stoppedwhen:|L(?(k))?L(?(k?1))|L(?
(k))< ?
(6)where the relative tolerance ?
= 10?7.
For any par-ticular application, this may or may not be an appro-priate stopping rule, but is only used here for pur-poses of comparison.Finally, it should be noted that in the current im-plementation, we have not applied any of the possi-ble optimizations that appear in the literature (Laf-ferty and Suhm, 1996; Wu and Khudanpur, 2000;Lafferty et al, 2001) to speed up normalization ofthe probability distribution q.
These improvementstake advantage of a model?s structure to simplify theevaluation of the denominator in (1).
The particulardata sets examined here are unstructured, and suchoptimizations are unlikely to give any improvement.However, when these optimizations are appropriate,they will give a proportional speed-up to all of thealgorithms.
Thus, the use of such optimizations isindependent of the choice of parameter estimationmethod.3.2 ExperimentsTo compare the algorithms described in ?2, we ap-plied the implementation outlined in the previoussection to four training data sets (described in Table1) drawn from the domain of natural language pro-cessing.
The ?rules?
and ?lex?
datasets are examplesdataset classes contexts features non-zerosrules 29,602 2,525 246 732,384lex 42,509 2,547 135,182 3,930,406summary 24,044 12,022 198,467 396,626shallow 8,625,782 375,034 264,142 55,192,723Table 1: Datasets used in experimentsof stochastic attribute value grammars, one with asmall set of SCFG-like features, and with a verylarge set of fine-grained lexical features (Boumaet al, 2001).
The ?summary?
dataset is part of asentence extraction task (Osborne, to appear), andthe ?shallow?
dataset is drawn from a text chunkingapplication (Osborne, 2002).
These datasets varywidely in their size and composition, and are repre-sentative of the kinds of datasets typically encoun-tered in applying ME models to NLP classificationtasks.The results of applying each of the parameter es-timation algorithms to each of the datasets is sum-marized in Table 2.
For each run, we report the KLdivergence between the fitted model and the train-ing data at convergence, the prediction accuracy offitted model on a held-out test set (the fraction ofcontexts for which the event with the highest prob-ability under the model also had the highest proba-bility under the reference distribution), the numberof iterations required, the number of log-likelihoodand gradient evaluations required (algorithms whichuse a line search may require several function eval-uations per iteration), and the total elapsed time (inseconds).2There are a few things to observe about theseresults.
First, while IIS converges in fewer stepsthe GIS, it takes substantially more time.
At leastfor this implementation, the additional bookkeepingoverhead required by IIS more than cancels any im-provements in speed offered by accelerated conver-gence.
This may be a misleading conclusion, how-ever, since a more finely tuned implementation ofIIS may well take much less time per iteration thanthe one used for these experiments.
However, evenif each iteration of IIS could be made as fast as an2The reported time does not include the time required to in-put the training data, which is difficult to reproduce and whichis the same for all the algorithms being tested.
All tests wererun using one CPU of a dual processor 1700MHz Pentium 4with 2 gigabytes of main memory at the Center for High Per-formance Computing and Visualisation, University of Gronin-gen.iteration of GIS (which seems unlikely), the bene-fits of IIS over GIS would in these cases be quitemodest.Second, note that for three of the four datasets,the KL divergence at convergence is roughly thesame for all of the algorithms.
For the ?summary?dataset, however, they differ by up to two orders ofmagnitude.
This is an indication that the conver-gence test in (6) is sensitive to the rate of conver-gence and thus to the choice of algorithm.
Any de-gree of precision desired could be reached by anyof the algorithms, with the appropriate value of ?.However, GIS, say, would require many more itera-tions than reported in Table 2 to reach the precisionachieved by the limited memory variable metric al-gorithm.Third, the prediction accuracy is, in most cases,more or less the same for all of the algorithms.Some variability is to be expected?all of the datasets being considered here are badly ill-conditioned,and many different models will yield the same like-lihood.
In a few cases, however, the predictionaccuracy differs more substantially.
For the twoSAVG data sets (?rules?
and ?lex?
), GIS has a smalladvantage over the other methods.
More dramati-cally, both iterative scaling methods perform verypoorly on the ?shallow?
dataset.
In this case, thetraining data is very sparse.
Many features arenearly ?pseudo-minimal?
in the sense of Johnson etal.
(1999), and so receive weights approaching ?
?.Smoothing the reference probabilities would likelyimprove the results for all of the methods and re-duce the observed differences.
However, this doessuggest that gradient-based methods are robust tocertain problems with the training data.Finally, the most significant lesson to be drawnfrom these results is that, with the exception ofsteepest ascent, gradient-based methods outperformiterative scaling by a wide margin for almost all thedatasets, as measured by both number of functionevaluations and by the total elapsed time.
And, ineach case, the limited memory variable metric algo-Dataset Method KL Div.
Acc Iters Evals Timerules gis 5.124?10?2 47.00 1186 1187 16.68iis 5.079?10?2 43.82 917 918 31.36steepest ascent 5.065?10?2 44.88 224 350 4.80conjugate gradient (fr) 5.007?10?2 44.17 66 181 2.57conjugate gradient (prp) 5.013?10?2 46.29 59 142 1.93limited memory variable metric 5.007?10?2 44.52 72 81 1.13lex gis 1.573?10?3 46.74 363 364 31.69iis 1.487?10?3 42.15 235 236 95.09steepest ascent 3.341?10?3 42.92 980 1545 114.21conjugate gradient (fr) 1.377?10?3 43.30 148 408 30.36conjugate gradient (prp) 1.893?10?3 44.06 114 281 21.72limited memory variable metric 1.366?10?3 43.30 168 176 20.02summary gis 1.857?10?3 96.10 1424 1425 107.05iis 1.081?10?3 96.10 593 594 188.54steepest ascent 2.489?10?3 96.33 1094 3321 190.22conjugate gradient (fr) 9.053?10?5 95.87 157 849 49.48conjugate gradient (prp) 3.297?10?4 96.10 112 537 31.66limited memory variable metric 5.598?10?5 95.54 63 69 8.52shallow gis 3.314?10?2 14.19 3494 3495 21223.86iis 3.238?10?2 5.42 3264 3265 66855.92steepest ascent 7.303?10?2 26.74 3677 14527 85062.53conjugate gradient (fr) 2.585?10?2 24.72 1157 6823 39038.31conjugate gradient (prp) 3.534?10?2 24.72 536 2813 16251.12limited memory variable metric 3.024?10?2 23.82 403 421 2420.30Table 2: Results of comparison.rithm performs substantially better than any of thecompeting methods.4 ConclusionsIn this paper, we have described experiments com-paring the performance of a number of different al-gorithms for estimating the parameters of a con-ditional ME model.
The results show that vari-ants of iterative scaling, the algorithms which aremost widely used in the literature, perform quitepoorly when compared to general function opti-mization algorithms such as conjugate gradient andvariable metric methods.
And, more specifically,for the NLP classification tasks considered, the lim-ited memory variable metric algorithm of Bensonand More?
(2001) outperforms the other choices bya substantial margin.This conclusion has obvious consequences for thefield.
ME modeling is a commonly used machinelearning technique, and the application of improvedparameter estimation algorithms will it practical toconstruct larger, more complex models.
And, sincethe parameters of individual models can be esti-mated quite quickly, this will further open up thepossibility for more sophisticated model and featureselection techniques which compare large numbersof alternative model specifications.
This suggeststhat more comprehensive experiments to comparethe convergence rate and accuracy of various algo-rithms on a wider range of problems is called for.In addition, there is a larger lesson to be drawnfrom these results.
We typically think of computa-tional linguistics as being primarily a symbolic dis-cipline.
However, statistical natural language pro-cessing involves non-trivial numeric computations.As these results show, natural language processingcan take great advantage of the algorithms and soft-ware libraries developed by and for more quantita-tively oriented engineering and computational sci-ences.AcknowledgementsThe research of Dr. Malouf has been made possible bya fellowship of the Royal Netherlands Academy of Artsand Sciences and by the NWO PIONIER project Algo-rithms for Linguistic Processing.
Thanks also to StephenClark, Andreas Eisele, Detlef Prescher, Miles Osborne,and Gertjan van Noord for helpful comments and testdata.ReferencesSteven P. Abney.
1997.
Stochastic attribute-valuegrammars.
Computational Linguistics, 23:597?618.Satish Balay, William D. Gropp, Lois CurfmanMcInnes, and Barry F. Smith.
1997.
Efficienctmanagement of parallelism in object oriented nu-merical software libraries.
In E. Arge, A. M. Bru-aset, and H. P. Langtangen, editors, Modern Soft-ware Tools in Scientific Computing, pages 163?202.
Birkhauser Press.Satish Balay, Kris Buschelman, William D. Gropp,Dinesh Kaushik, Lois Curfman McInnes, andBarry F. Smith.
2001.
PETSc home page.http://www.mcs.anl.gov/petsc.Satish Balay, William D. Gropp, Lois CurfmanMcInnes, and Barry F. Smith.
2002.
PETSc usersmanual.
Technical Report ANL-95/11?Revision2.1.2, Argonne National Laboratory.Steven J. Benson and Jorge J.
More?.
2001.
A lim-ited memory variable metric method for boundconstrained minimization.
Preprint ANL/ACS-P909-0901, Argonne National Laboratory.Steven J. Benson, Lois Curfman McInnes, Jorge J.More?, and Jason Sarich.
2002.
TAO usersmanual.
Technical Report ANL/MCS-TM-242?Revision 1.4, Argonne National Laboratory.Adam Berger, Stephen Della Pietra, and VincentDella Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Compu-tational Linguistics, 22.Gosse Bouma, Gertjan van Noord, and Robert Mal-ouf.
2001.
Alpino: wide coverage computationalanalysis of Dutch.
In W. Daelemans, K. Sima?an,J.
Veenstra, and J. Zavrel, editors, ComputationalLinguistics in the Netherlands 2000, pages 45?59.
Rodolpi, Amsterdam.Zhiyi Chi.
1998.
Probability models for complexsystems.
Ph.D. thesis, Brown University.J.
Darroch and D. Ratcliff.
1972.
Generalized it-erative scaling for log-linear models.
Ann.
Math.Statistics, 43:1470?1480.Stephen Della Pietra, Vincent Della Pietra, andJohn Lafferty.
1997.
Inducing features of ran-dom fields.
IEEE Transactions on Pattern Analy-sis and Machine Intelligence, 19:380?393.W.E.
Deming and F.F.
Stephan.
1940.
On a leastsquares adjustment of a sampled frequency tablewhen the expected marginals are known.
Annalsof Mathematical Statistics, 11:427?444.Elizabeth D. Dolan and Jorge J.
More?.
2002.Benchmarking optimization software with per-formance profiles.
Mathematical Programming,91:201?213.Mark Johnson, Stuart Geman, Stephen Canon,Zhiyi Chi, and Stefan Riezler.
1999.
Estimatorsfor stochastic ?unification-based?
grammars.
InProceedings of the 37th Annual Meeting of theACL, pages 535?541, College Park, Maryland.John Lafferty and Bernhard Suhm.
1996.
Clusterexpansions and iterative scaling for maximum en-tropy language models.
In K. Hanson and R. Sil-ver, editors, Maximum Entropy and BayesianMethods.
Kluwer.John Lafferty, Fernando Pereira, and Andrew Mc-Callum.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In International Conference on Ma-chine Learning (ICML).Thomas P. Minka.
2001.
Algorithms formaximum-likelihood logistic regression.
Statis-tics Tech Report 758, CMU.Jorge Nocedal and Stephen J. Wright.
1999.
Nu-merical Optimization.
Springer, New York.Jorge Nocedal.
1997.
Large scale unconstrainedoptimization.
In A. Watson and I. Duff, editors,The State of the Art in Numerical Analysis, pages311?338.
Oxford University Press.Miles Osborne.
2002.
Shallow parsing using noisyand non-stationary training material.
Journal ofMachine Learning Research, 2:695?719.Miles Osborne.
to appear.
Using maximum entropyfor sentence extraction.
In Proceedings of theACL 2002 Workshop on Automatic Summariza-tion, Philadelphia.Adwait Ratnaparkhi.
1998.
Maximum entropymodels for natural language ambiguity resolu-tion.
Ph.D. thesis, University of Pennsylvania.Jun Wu and Sanjeev Khudanpur.
2000.
Efficienttraining methods for maximum entropy languagemodelling.
In Proceedings of ICSLP2000, vol-ume 3, pages 114?117, Beijing.
