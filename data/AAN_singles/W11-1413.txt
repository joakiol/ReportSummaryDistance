Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 105?110,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsGenerating Example Contexts to Illustrate a Target Word SenseJack Mostow Weisi DuanCarnegie Mellon University Carnegie Mellon UniversityRI-NSH 4103, 5000 Forbes Avenue Language Technologies InstitutePittsburgh, PA 15213-3890, USA Pittsburgh, PA 15213-3890, USAmostow@cs.cmu.edu wduan@cs.cmu.eduAbstractLearning a vocabulary word requires seeing itin multiple informative contexts.
We describea system to generate such contexts for a givenword sense.
Rather than attempt to do wordsense disambiguation on example contexts al-ready generated or selected from a corpus, wecompile information about the word sense intothe context generation process.
To evaluate thesense-appropriateness of the generated contextscompared to WordNet examples, three humanjudges chose which word sense(s) fit each ex-ample, blind to its source and intended sense.On average, one judge rated the generated ex-amples as sense-appropriate, compared to twojudges for the WordNet examples.
Althoughthe system?s precision was only half of Word-Net?s, its recall was actually higher thanWordNet?s, thanks to covering many senses forwhich WordNet lacks examples.1 IntroductionLearning word meaning from example contexts isan important aspect of vocabulary learning.
Con-texts give clues to semantics but also convey manyother lexical aspects, such as parts of speech, mor-phology, and pragmatics, which help enrich a per-son?s word knowledge base (Jenkins 1984; Nagy etal.
1985; Schatz 1986; Herman et al 1987; Nagyet al 1987; Schwanenflugel et al 1997; Kuhn andStahl 1998; Fukkink et al 2001).
Accordingly, onekey issue in vocabulary instruction is how to findor create good example contexts to help childrenlearn a particular sense of a word.
Hand-vettingautomatically generated contexts can be easier thanhand-crafting them from scratch (Mitkov et al2006; Liu et al 2009).This paper describes what we believe is the firstsystem to generate example contexts for a giventarget sense of a polysemous word.
Liu et al(2009) characterized good contexts for helpingchildren learn vocabulary and generated them for atarget part of speech, but not a given word sense.Pino and Eskenazi  (2009) addressed the polysemyissue, but in a system for selecting contexts ratherthan for generating them.
Generation can supplymore contexts for a given purpose, e.g.
teachingchildren, than WordNet or a fixed corpus contains.Section 2 describes a method to generate sense-targeted contexts.
Section 3 compares them toWordNet examples.
Section 4 concludes.2 ApproachAn obvious way to generate sense-targeted con-texts is to generate contexts containing the targetword, and use Word Sense Disambiguation (WSD)to select the ones that use the target word sense.However, without taking the target word sense intoaccount, the generation process may not output anycontexts that use it.
Instead, we model word sensesas topics and incorporate their sense indicators intothe generation process ?
words that imply a uniqueword sense when they co-occur with a target word.For example, retreat can mean ?a place of pri-vacy; a place affording peace and quiet.?
Indica-tors for this sense, in decreasing order of Pr(word |topic for target sense), include retreat, yoga, place,retreats, day, home, center, church, spiritual, life,city, time, lake, year, room, prayer, years, school,dog, park, beautiful, area, and stay.
Generatedcontexts include ?retreat in this bustling city?.Another sense of retreat (as defined in Word-Net) is ?
(military) a signal to begin a withdrawal105from a dangerous position,?
for which indicatorsinclude states, war, united, american, military,flag, president, world, bush, state, Israel, Iraq, in-ternational, national, policy, forces, foreign, na-tion, administration, power, security, iran, force,and Russia.
Generated contexts include ?militaryleaders believe that retreat?.We decompose our approach into two phases,summarized in Figure 1.
Section 2.1 describes theSense Indicator Extraction phase, which obtainsindicators for each WordNet synset of the targetword.
Section 2.2 describes the Context Genera-tion phase, which generates contexts that containthe target word and indicators for the target sense.Figure 1: overall work flow diagram2.1 Sense Indicator ExtractionKulkarni and Pedersen (2005) and Duan and Yates(2010) performed Sense Indicator Extraction, butthe indicators they extracted are not sense targeted.Content words in the definition and examples foreach sense are often good indicators for that sense,but we found that on their own they did poorly.One reason is that such indicators sometimes co-occur with a different sense.
But the main reasonis that there are so few of them that the word senseoften appears without any of them.
Thus we needmore (and if possible better) sense indicators.To obtain sense-targeted indicators for a targetword, we first assemble a corpus by issuing aGoogle query for each synset of the target word.The query lists the target word and all contentwords in the synset?s WordNet definition and ex-amples, and specifies a limit of 200 hits.
The re-sulting corpus contains a few hundred documents.To extract sense indicators from the corpus for aword, we adapt Latent Dirichlet Allocation (LDA)(Blei et al 2003).
LDA takes as input a corpus ofdocuments and an integer k, and outputs k latenttopics, each represented as a probability distribu-tion over the corpus vocabulary.
For k, we use thenumber of word senses.
To bias LDA to learn top-ics corresponding to the word senses, we use thecontent words in their WordNet definitions andexamples as seed words.After learning these topics and filtering out stopwords, we pick the 30 highest-probability wordsfor each topic as indicators for the correspondingword sense, filtering out any words that also indi-cate other senses.
We create a corpus for each tar-get word and run LDA on it.Having outlined the extraction process, we nowexplain in more detail how we learn the topics; themathematically faint-hearted may skip to Section2.2.
Formally, given corpus   with  documents,let   be the number of topics, and let    and    bethe parameters of the document and topic distribu-tions respectively.
LDA assumes this generativeprocess for each document    for a corpus  :1.
Choose            where2.
Choose            where3.
For each word      in    where,   is the number of words in(a) Choose a topic(b) Choose a topicwhereIn classical LDA, all   ?s are the same.
We al-low them to be different in order to use the seedwords as high confidence indicators of targetsenses to bias the hyper-parameters of their docu-ment distributions.For inference, we use Gibbs Sampling (Steyversand Griffiths 2006) with transition probabilityHere        denotes the topic assignments to allother words in the corpus except     ;is the number of times word   is assigned to topicin the whole corpus;          is the number ofwords assigned to topic   in the entire corpus;106is the count of tokens assigned to topicin document   ; and    and       are the hyper-parameters on     and      respectively in the twoDirichlet distributions.For each document    that contains seed wordsof some synset, we bias    toward the topic   forthat synset by making      larger; specifically, weset each      to 10 times the average value of   .This bias causes more words     in    to be as-signed to topic   because the words of    are likelyto be relevant to  .
These assignments then influ-ence the topic distribution of   so as to makelikelier to be assigned to   in any document     ,and thus shift the document distribution intowards  .
By this time we are back to the start ofthe loop where the document distribution ofis biased to  .
Thus this procedure can discovermore sense indicators for each sense.Our method is a variant of Labeled LDA (L-LDA) (Ramage 2009), which allows only labelsfor each document as topics.
In contrast, our va-riant allows all topics for each document, becauseit may use more than one sense of the target word.Allowing other senses provides additional flexibili-ty to discover appropriate sense indicators.The LDA method we use to obtain sense indica-tors fits naturally into the framework of bootstrap-ping WSD (Yarowsky 1995; Mihalcea 2002;Martinez et al 2008; Duan and Yates 2010), inwhich seeds are given for each target word, and thegoal is to disambiguate the target word by boot-strapping good sense indicators that can identifythe sense.
In contrast to WSD, our goal is to gen-erate contexts for each sense of the target word.2.2 Context GenerationTo generate sense-targeted contexts, we extend theVEGEMATIC context generation system (Liu etal.
2009).
VEGEMATIC generates contexts for agiven target word using the Google N-gram cor-pus.
Starting with a 5-gram that contains the targetword, VEGEMATIC extends it by concatenatingadditional 5-grams that overlap by 4 words on theleft or right.To satisfy various constraints on good contextsfor learning the meaning of a word, VEGEMATICuses various heuristic filters.
For example, to gen-erate contexts likely to be informative about theword meaning, VEGEMATIC prefers 5-grams thatcontain words related to the target word, i.e., thatoccur more often in its presence.
However, thiscriterion is not specific to a particular target sense.To make VEGEMATIC sense-targeted, wemodify this heuristic to prefer 5-grams that containsense indicators.
We assign the generated contextsto the senses whose sense indicators they contain.We discard contexts that contain sense indicatorsfor more than one sense.3 Experiments and EvaluationTo evaluate our method, we picked 8 target wordsfrom a list of polysemous vocabulary words usedin many domains and hence important for childrento learn (Beck et al 2002).
Four of them arenouns:  advantage (with 3 synsets), content (7),force (10), and retreat (7).
Four are verbs:  dash(6), decline (7), direct (13), and reduce (20).
Someof these words can have other parts of speech, butwe exclude those senses, leaving 73 senses in total.We use their definitions from WordNet becauseit is a widely used, comprehensive sense inventory.Some alternative sense inventories might be un-suitable.
For instance, children?s dictionaries maylack WordNet?s rare senses or hypernym relations.We generated contexts for these 73 word sensesas described in Section 2, typically 3 examples foreach word sense.
To reduce the evaluation burdenon our human judges, we chose just one context foreach word sense, and for words with more than 10senses we chose a random sample of them.
Toavoid unconscious bias, we chose random contextsrather than the best ones, which a human wouldlikelier pick if vetting the generated contexts byhand.
For comparison, we also evaluated WordNetexamples (23 in total) where available.We gave three native English-speaking college-educated judges the examples to evaluate indepen-dently, blind to their intended sense.
They filled ina table for each target word.
The left column listedthe examples (both generated and WordNet) inrandom order, one per row.
The top row gave theWordNet definition of each synset, one per col-umn.
Judges were told:  For each example, put a1 in the column for the sense that best fits howthe example uses the target word.
If more thanone sense fits, rank them 1, 2, etc.
Use the lasttwo columns only to say that none of the sensesfit, or you can't tell, and why.
(Only 10 such cas-es arose.
)We measured inter-rater reliability at two levels.107At the fine-grained level, we measured how wellthe judges agreed on which one sense fit the exam-ple best.
The value of Fleiss?
Kappa (Shrout andFleiss 1979) was 42%, considered moderate.
Atthe coarse-grained level, we measured how welljudges agreed on which sense(s) fit at all.
HereFleiss?
Kappa was 48%, also considered moderate.We evaluated the examples on three criteria.Yield is the percentage of intended senses forwhich we generate at least one example ?
whetherit fits or not.
For the 73 synsets, this percentage is92%.
Moreover, we typically generate 3 examplesfor a word sense.
In comparison, only 34% of thesynsets have even a single example in WordNet.
(Fine-grained) precision is the percentage ofexamples that the intended sense fits best accord-ing to the judges.
Human judges often disagree, sowe prorate this percentage by the percentage ofjudges who chose the intended sense as the best fit.The result is algebraically equivalent to computingprecision separately according to each judge, andthen averaging the results.
Precision for generatedexamples was 36% for those 23 synsets and 27%for all 67 synsets with generated examples.
Al-though we expected WordNet to be a gold stan-dard, its precision for the 23 synsets havingexamples was 52% ?
far less than 100%.This low precision suggests that the WordNetcontexts to illustrate different senses were oftennot informative enough for the judges to distin-guish them from all the other senses.
For example,the WordNet example reduce one?s standard ofliving is attached to the sense ?lessen and makemore modest.?
However, this sense is hard to dis-tinguish from ?lower in grade or rank or forcesomebody into an undignified situation.?
In fact,two judges did not choose the first sense, and oneof them chose the second sense as the best fit.Coarse-grained precision is similar, but based onhow often the intended sense fits the example atall, whether or not it fits best.
Coarse-grained pre-cision was 67% for the 23 WordNet examples,40% for the examples generated for those 23 syn-sets, and 33% for all 67 generated examples.Coarse-grained precision is important becausefine-grained semantic distinctions do not matter inillustrating a core sense of a word.
The problem ofhow to cluster fine-grained senses into coarsesenses is hard, especially if consensus is required(Navigli et al 2007).
Rather than attempt to identi-fy a single definitive partition of a target word?ssynsets into coarse senses, we implicitly define acoarse sense as the subset of synsets rated by ajudge as fitting a given example.
Thus the cluster-ing into coarse senses is not only judge-specific butexample-specific:   different, possibly overlappingsets of synsets may fit different examples.Recall is the percentage of synsets that fit theirgenerated examples.
Algebraically it is the productof precision and yield.
Fine-grained recall was25% for the generated examples, compared to only18% for the WordNet examples.
Coarse-grainedrecall was 30% for the generated examples, com-pared to 23% for the WordNet examples.Figure 2 shows how yield, inter-rater agreement,and coarse and fine precision for the 8 target wordsvary with their number of synsets.
With so fewwords, this analysis is suggestive, not conclusive.We plot all four metrics on the same [0,1] scale tosave space, but only the last two metrics have di-rectly comparable values,  However, it is still mea-ningful to compare how they vary.
Precision andinter-rater reliability generally appear to decreasewith the number of senses.
As polysemy increases,the judges have more ways to disagree with eachother and with our program.
Yield is mostly high,but might be lower for words with many senses,due to deficient document corpora for rare senses.Figure 2: Effects of increasing polysemyTable 1 compares the generated and WordNetexamples on various measures.
It compares preci-sion on the same 23 senses that have WordNet ex-amples.
It compares recall on all 73 senses.
Itcompares Kappa on the 23 WordNet examples andthe sample of generated examples the judges rated.Number of target word synsets0%10%20%30%40%50%60%70%80%90%100%0 5 10 15 20YieldFleiss'KappaPrecision(Coarse)Precision(Fine)108Generated WordNetYield 92% 34%Senses with examples 67 23Avg.
words in context 5.91 7.87Precision(same 23)Fine 36% 52%Coarse 40% 67%RecallFine  25% 18%Coarse 30% 23%Fleiss?KappaFine 0.43 0.39Coarse 0.48 0.49Table 1: Generated examples vs. WordNetErrors occur when 1) the corpus is missing aword sense; 2) LDA fails to find good sense indi-cators; or 3) Context Generation fails to generate asense-appropriate context.Our method succeeds when (1) the target senseoccurs in the corpus, (2) LDA finds good indica-tors for it, and (3) Context Generation uses them toconstruct a sense-appropriate context.
For exam-ple, the first sense of advantage is ?the quality ofhaving a superior or more favorable position,?
forwhich we obtain the sense indicators support, work,time, order, life, knowledge, mind, media, human,market, experience, nature, make, social, informa-tion, child, individual, cost, people, power, good,land, strategy, and company, and generate (amongothers) the context ?knowledge gave him an ad-vantage?.Errors occur when any of these 3 steps fails.Step 1 fails for the sense ?reduce in scope whileretaining essential elements?
of reduce because itis so general that no good example exists in thecorpus for it.
Step 2 fails for the sense of force in?the force of his eloquence easily persuaded them?because its sense indicators are men, made, great,page, man, time, general, day, found, side, called,and house.
None of these words are preciseenough to convey the sense.
Step 3 fails for thesense of advantage as ?
(tennis) first point scoredafter deuce,?
with sense indicators point, game,player, tennis, set, score, points, ball, court, ser-vice, serve, called, win, side, players, play, team,games, match, wins, won, net, deuce, line, oppo-nent, and turn.
This list looks suitably tennis-related.
However, the generated context ?theplayer has an advantage?
fits the first sense ofadvantage; here the indicator player for the tennissense is misleading.4 Contributions and LimitationsThis paper presents what we believe is the firstsystem for generating sense-appropriate contexts toillustrate different word senses even if they havethe same part of speech.
We define the problem ofgenerating sense-targeted contexts for vocabularylearning, factor it into Sense Indicator Extractionand Context Generation, and compare the resultingcontexts to WordNet in yield, precision, and recallaccording to human judges who decided, givendefinitions of all senses, which one(s) fit each con-text, without knowing its source or intended sense.This test is much more stringent than just decidingwhether a given word sense fits a given context.There are other possible baselines to compareagainst, such as Google snippets.
However, Googlesnippets fare poorly on criteria for teaching child-ren vocabulary (Liu et al under revision).
Anothershortcoming of this alternative is the inefficiencyof retrieving all contexts containing the target wordand filtering out the unsuitable ones.
Instead, wecompile constraints on suitability into a generatorthat constructs only contexts that satisfy them.Moreover, in contrast to retrieve-and-filter, ourconstructive method (concatenation of overlappingGoogle 5-grams) can generate novel contexts.There is ample room for future improvement.We specify word senses as WordNet synsets ratherthan as coarser-grain dictionary word senses morenatural for educators.
Our methods for target worddocument corpus construction, Sense IndicatorExtraction, and Context Generation are all fallible.On average, 1 of 3 human judges rated the result-ing contexts as sense-appropriate, half as many asfor WordNet examples.
However, thanks to highyield, their recall surpassed the percentage of syn-sets with WordNet examples.
The ultimate crite-rion for evaluating them will be their value intutorial interventions to help students learn vocabu-lary.AcknowledgmentsThis work was supported by the Institute of Educa-tion Sciences, U.S. Department of Education,through Grant R305A080157 to Carnegie MellonUniversity.
The opinions expressed are those of theauthors and do not necessarily represent the viewsof the Institute or the U.S. Department of Educa-tion.
We thank the reviewers and our judges.109ReferencesIsabel L.  Beck, Margaret G. Mckeown and LindaKucan.
2002.
Bringing Words to Life:  RobustVocabulary Instruction.
NY, Guilford.David Blei, Andrew Ng and Michael Jordan.
2003.Latent Dirichlet alocation.
Journal of MachineLearning Research 3: 993?1022.Weisi Duan and Alexander Yates.
2010.
ExtractingGlosses to Disambiguate Word Senses.
HumanLanguage Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, LosAngeles.Ruben G. Fukkink, Henk Blok and Kees De Glopper.2001.
Deriving word meaning from written context:A multicomponential skill.
Language Learning 51(3):477-496.Patricia A. Herman, Richard C. Anderson, P. DavidPearson and William E. Nagy.
1987.
Incidentalacquisition of word meaning from expositions withvaried text features.
Reading Research Quarterly22(3): 263-284.Joseph R. Jenkins, Marcy  Stein and Katherine Wysocki.1984.
Learning vocabulary through reading.American Educational Research Journal 21: 767-787.Melanie R. Kuhn and Steven A. Stahl.
1998.
Teachingchildren to learn word meaning from context: Asynthesis and some questions.
Journal of LiteracyResearch 30(1): 119-138.Anagha Kulkarni and Ted Pedersen.
2005.
Namediscrimination and email clustering usingunsupervised clustering and labeling of similarcontexts.
Proceedings of the Second IndianInternational Conference on Artificial Intelligence,Pune, India.Liu Liu, Jack Mostow and Greg Aist.
2009.
AutomatedGeneration of Example Contexts for HelpingChildren Learn Vocabulary.
Second ISCA Workshopon Speech and Language Technology in Education(SLaTE), Wroxall Abbey Estate, Warwickshire,England.Liu Liu, Jack Mostow and Gregory S. Aist.
underrevision.
Generating Example Contexts to HelpChildren Learn Word Meaning.
Journal of NaturalLanguage Engineering.David Martinez, Oier Lopez de Lacalle and EnekoAgirre.
2008.
On the use of automatically acquiredexamples for all-nouns word sense disambiguation.Journal of Artificial Intelligence Research 33: 79--107.Rada Mihalcea.
2002.
Bootstrapping large sense taggedcorpora.
Proceedings of the 3rd InternationalConference on Languages Resources and EvaluationsLREC 2002, Las Palmas, Spain.R.
Uslan Mitkov, Le An Ha and Nikiforos Karamanis.2006.
A computer-aided environment for generatingmultiple choice test items.
Natural LanguageEngineering 12(2): 177-194.William E. Nagy, Richard C. Anderson and Patricia A.Herman.
1987.
Learning Word Meanings fromContext during Normal Reading.
AmericanEducational Research Journal 24(2): 237-270.William E. Nagy, Patricia A. Herman and Richard C.Anderson.
1985.
Learning words from context.Reading Research Quarterly 20(2): 233-253.Roberto Navigli, Kenneth C. Litkowski and OrinHargraves.
2007.
Semeval-2007 task 07: Coarse-grained English all-words task.
Proceedings of the4th International Workshop on Semantic Evaluations,Association for Computational Linguistics: 30-35.Juan Pino and Maxine Eskenazi.
2009.
An Applicationof Latent Semantic Analysis to Word SenseDiscrimination for Words with Related and UnrelatedMeanings.
The 4th Workshop on Innovative Use ofNLP for Building Educational Applications,NAACL-HLT 2009 Workshops, Boulder, CO, USA.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009.
Labeled LDA: Asupervised topic model for credit attribution in multi-labeled corpora.
Proceedings of the 2009 Conferenceon Empirical Methods in Natural LanguageProcessing, Association for ComputationalLinguistics.Elinore K. Schatz and R. Scott Baldwin.
1986.
Contextclues are unreliable predictors of word meanings.Reading Research Quarterly 21: 439-453.Paula J. Schwanenflugel, Steven A. Stahl and ElisabethL.
Mcfalls.
1997.
Partial Word Knowledge andVocabulary Growth during Reading Comprehension.Journal of Literacy Research 29(4): 531-553.Patrick E. Shrout and Joseph L. Fleiss.
1979.
Intraclasscorrelations: Uses in assessing rater reliability.Psychological Bulletin 86(2): 420-428.Mark Steyvers and Tom Griffiths.
2006.
Probabilistictopic models.
Latent Semantic Analysis: A Road toMeaning.
T. Landauer, D. McNamara, S. Dennis andW.
Kintsch.
Hillsdale, NJ, Laurence Erlbaum.David Yarowsky.
1995.
Unsupervised WSD rivalingsupervised methods.
Proceedings of the 33rd AnnualMeeting of the Association for ComputationalLinguistics, Massachusetts Institute of Technology,Cambridge, MA.110
