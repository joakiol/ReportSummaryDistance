Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 200?207,New York, June 2006. c?2006 Association for Computational LinguisticsIdentifying and Analyzing Judgment OpinionsSoo-Min Kim and Eduard HovyUSC Information Sciences Institute4676 Admiralty Way, Marina del Rey, CA 90292{skim, hovy}@ISI.EDUAbstractIn this paper, we introduce a methodologyfor analyzing judgment opinions.
We de-fine a judgment opinion as consisting of avalence, a holder, and a topic.
We decom-pose the task of opinion analysis into fourparts: 1) recognizing the opinion; 2) iden-tifying the valence; 3) identifying theholder; and 4) identifying the topic.
In thispaper, we address the first three parts andevaluate our methodology using both in-trinsic and extrinsic measures.1 IntroductionRecently, many researchers and companies haveexplored the area of opinion detection and analysis.With the increased immersion of Internet users hascome a proliferation of opinions available on theweb.
Not only do we read more opinions from theweb, such as in daily news editorials, but also wepost more opinions through mechanisms such asgovernmental web sites, product review sites, newsgroup message boards and personal blogs.
Thisphenomenon has opened the door for massiveopinion collection, which has potential impact onvarious applications such as public opinion moni-toring and product review summary systems.Although in its infancy, many researchers haveworked in various facets of opinion analysis.
Panget al (2002) and Turney (2002) classified senti-ment polarity of reviews at the document level.Wiebe et al (1999) classified sentence level sub-jectivity using syntactic classes such as adjectives,pronouns and modal verbs as features.
Riloff andWiebe (2003) extracted subjective expressionsfrom sentences using a bootstrapping pattern learn-ing process.
Yu and Hatzivassiloglou (2003) iden-tified the polarity of opinion sentences usingsemantically oriented words.
These techniqueswere applied and examined in different domains,such as customer reviews (Hu and Liu 2004) andnews articles1.
These researchers use lists of opin-ion-bearing clue words and phrases, and then applyvarious additional techniques and refinements.Along with many opinion researchers, we par-ticipated in a large pilot study, sponsored by NIST,which concluded that it is very difficult to definewhat an opinion is in general.
Moreover, an ex-pression that is considered as an opinion in onedomain might not be an opinion in another.
Forexample, the statement ?The screen is very big?might be a positive review for a wide screen desk-top review, but it could be a mere fact in generalnewspaper text.
This implies that it is hard to applyopinion bearing words collected from one domainto an application for another domain.
One mighttherefore need to collect opinion clues within indi-vidual domains.
In case we cannot simply findtraining data from existing sources, such as newsarticle analysis, we need to manually annotate datafirst.Most opinions are of two kinds: 1) beliefs aboutthe world, with values such as true, false, possible,unlikely, etc.
; and 2) judgments about the world,with values such as good, bad, neutral, wise, fool-ish, virtuous, etc.
Statements like ?I believe that heis smart?
and ?Stock prices will rise soon?
are ex-amples of beliefs whereas ?I like the new policy onsocial security?
and ?Unfortunately this really washis year: despite a stagnant economy, he still wonhis re-election?
are examples of judgment opinions.However, judgment opinions and beliefs are notnecessarily mutually exclusive.
For example, ?Ithink it is an outrage?
or ?I believe that he issmart?
carry both a belief and a judgment.In the NIST pilot study, it was apparent thathuman annotators often disagreed on whether abelief statement was or was not an opinion.
How-ever, high annotator agreement was seen on judg-1 TREC novelty track 2003 and 2004200ment opinions.
In this paper, we therefore focusour analysis on judgment opinions only.
We hopethat future work yields a more precise definition ofbelief opinions on which human annotators canagree.We define a judgment opinion as consisting ofthree elements: a valence, a holder, and a topic.The valence, which applies specifically to judg-ment opinions and not beliefs, is the value of thejudgment.
In our framework, we consider the fol-lowing valences: positive, negative, and neutral.The holder of an opinion is the person, organiza-tion or group whose opinion is expressed.
Finally,the topic is the event or entity about which theopinion is held.In previous work, Choi et al (2005) identifyopinion holders (sources) using Conditional Ran-dom Fields (CRF) and extraction patterns.
Theydefine the opinion holder identification problem asa sequence tagging task: given a sequence of words( nxxx L21 ) in a sentence, they generate a se-quence of labels ( nyyy L21 ) indicating whetherthe word is a holder or not.
However, there aremany cases where multiple opinions are expressedin a sentence each with its own holder.
In thosecases, finding opinion holders for each individualexpression is necessary.
In the corpus they used,48.5% of the sentences which contain an opinionhave more than one opinion expression with multi-ple opinion holders.
This implies that multipleopinion expressions in a sentence occur signifi-cantly often.
A major challenge of our work istherefore not only to focus on sentence with onlyone opinion, but also to identify opinion holderswhen there is more than one opinion expressed in asentence.
For example, consider the sentence ?Inrelation to Bush?s axis of evil remarks, the GermanForeign Minister also said, Allies are not satellites,and the French Foreign Minister caustically criti-cized that the United States?
unilateral, simplisticworldview poses a new threat to the world?.
Here,?the German Foreign Minister?
should be theholder for the opinion ?Allies are not satellites?and ?the French Foreign Minister?
should be theholder for ?caustically criticized?.In this paper, we introduce a methodology foranalyzing judgment opinions.
We decompose thetask into four parts: 1) recognizing the opinion; 2)identifying the valence; 3) identifying the holder;and 4) identifying the topic.
For the purposes ofthis paper, we address the first three parts andleave the last for future work.
Opinions can be ex-tracted from various granularities such as a word, asentence, a text, or even multiple texts.
Each isimportant, but we focus our attention on word-level opinion detection (Section 2.1) and the detec-tion of opinions in short emails (Section 3).
Weevaluate our methodology using intrinsic and ex-trinsic measures.The remainder of the paper is organized as fol-lows.
In the next section, we describe our method-ology addressing the three steps described above,and in Section 4 we present our experimental re-sults.
We conclude with a discussion of futurework.2 Analysis of Judgment OpinionsIn this section, we first describe our methodologyfor detecting opinion bearing words and for identi-fying their valence, which is described in Section2.1.
Then, in Section 2.2, we describe our algo-rithm for identifying opinion holders.
In Section 3,we show how to use our methodology for detectingopinions in short emails.2.1 Detecting Opinion-Bearing Wordsand Identifying ValenceWe introduce an algorithm to classify a word asbeing positive, negative, or neutral classes.
Thisclassifier can be used for any set of words of inter-est and the resulting words with their valence tagscan help in developing new applications such as apublic opinion monitoring system.
We define anopinion-bearing word as a word that carries a posi-tive or negative sentiment directly such as ?good?,?bad?, ?foolish?, ?virtuous?, etc.
In other words,this is the smallest unit of opinion that can thereaf-ter be used as a clue for sentence-level or text-levelopinion detection.We treat word sentiment classification into Posi-tive, Negative, and Neutral as a three-way classifi-cation problem instead of a two-way classificationproblem of Positive and Negative.
By adding thethird class, Neutral, we can prevent the classifierfrom assigning either positive or negative senti-ment to weak opinion-bearing words.
For example,the word ?central?
that Hatzivassiloglou andMcKeown (1997) included as a positive adjectiveis not classified as positive in our system.
Instead201we mark it as ?neutral?
since it is a weak clue foran opinion.
If an unknown word has a strong rela-tionship with the neutral class, we can thereforeclassify it as neutral even if it has some small con-notation of Positive or Negative as well.Approach: We built a word sentiment classifierusing WordNet and three sets of positive, negative,and neutral words tagged by hand.
Our insight isthat synonyms of positive words tend to have posi-tive sentiment.
We expanded those manually se-lected seed words of each sentiment class bycollecting synonyms from WordNet.
However, wecannot simply assume that all the synonyms ofpositive words are positive since most words couldhave synonym relationships with all three senti-ment classes.
This requires us to calculate thecloseness of a given word to each category anddetermine the most probable class.
The followingformula describes our model for determining thecategory of a word:(1)            ).....,|(maxarg)|(maxarg 21 nccsynsynsyncPwcP ?where c is a category (Positive, Negative, or Neu-tral) and w is a given word; synn is a WordNetsynonym of the word w. We calculate this close-ness as follows;(2)   )|()(maxarg)|()(maxarg)|()(maxarg)|(maxarg1))(,(...3 2 1?====mkwsynsetfcountkcnccckcfPcPcsynsynsynsynPcPcwPcPwcPwhere kf  is the kth feature of class c which is also amember of the synonym set of the given word w.count(fk ,synset(w)) is the total number of occur-rences of the word feature fk in the synonym set ofword w. In section 4.1, we describe our manuallyannotated dataset which we used for seed wordsand for our evaluation.2.2 Identifying Opinion HoldersDespite successes in identifying opinion expres-sions and subjective words/phrases (See Section1), there has been less achievement on the factorsclosely related to subjectivity and polarity, such asidentifying the opinion holder.
However, our re-search indicates that without this information, it isdifficult, if not impossible, to define ?opinion?
ac-curately enough to obtain reasonable inter-annotator agreement.
Since these factors co-occurand mutually reinforce each other, the question?Who is the holder of this opinion??
is as impor-tant as ?Is this an opinion??
or ?What kind of opin-ion is expressed here?
?.In this section, we describe the automated iden-tification for opinion holders.
We define an opin-ion holder as an entity (person, organization,country, or special group of people) who expressesexplicitly or implicitly the opinion contained in thesentence.Previous work that is related to opinion holderidentification is (Bethard et al 2004) who identifyopinion propositions and holders.
However, theiropinion is restricted to propositional opinion andmostly to verbs.
Another related work is (Choi et al2005) who use the MPQA corpus2 to learn patternsof opinion sources using a graphical model andextraction pattern learning.
However, they have adifferent task definition from ours.
They define thetask as identifying opinion sources (holders) givena sentence, whereas we define it as identifyingopinion sources given an opinion expression in asentence.
We discussed their work in Section 1.Data: As training data, we used the MPQA cor-pus (Wilson and Wiebe, 2003), which containsnews articles manually annotated by 5 trained an-notators.
They annotated 10657 sentences from535 documents, in four different aspects: agent,expressive-subjectivity, on, and inside.
Expressive-subjectivity marks words and phrases that indi-rectly express a private state that is defined as aterm for opinions, evaluations, emotions, andspeculations.
The on annotation is used to markspeech events and direct expressions of privatestates.
As for the holder, we use the agent of theselected private states or speech events.
Whilethere are many possible ways to define what opin-ion means, intuitively, given an opinion, it is clearwhat the opinion holder means.
Table 1 shows anexample of the annotation.
In this example, weconsider the expression ?the U.S. government ?isthe source of evil?
in the world?
with an expres-2 http://www.cs.pitt.edu/~wiebe/pubs/ardasummer02/SentenceIraqi Vice President Taha Yassin Rama-dan, responding to Bush?s ?axis of evil?remark, said the U.S. government ?is thesource of evil?
in the world.Expressivesubjectivitythe U.S. government ?is the source of evil?in the worldStrength ExtremeSource Iraqi Vice President Taha Yassin RamadanTable 1: Annotation example202sive-subjectivity tag as an opinion of the holder?Iraqi Vice President Taha Yassin Ramadan?.Approach: Since more than one opinion may beexpressed in a sentence, we have to find an opinionholder for each opinion expression.
For example,in a sentence ?A thinks B?s criticism of T iswrong?, B is the holder of ?the criticism of T?,whereas A is the person who has an opinion thatB?s criticism is wrong.
Therefore, we define ourtask as finding an opinion holder, given an opinionexpression.
Our earlier work (ref suppressed) fo-cused on identifying opinion expressions withintext.
We employ that system in tandem with theone described here.To learn opinion holders automatically, we use aMaximum Entropy model.
Maximum Entropymodels implement the intuition that the best modelis the one that is consistent with the set of con-straints imposed by the evidence but otherwise isas uniform as possible (Berger et al 1996).
Thereare two ways to model the problem with ME: clas-sification and ranking.
Classification allocates eachholder candidate to one of a set of predefinedclasses while ranking selects a single candidate asanswer.
This means that classification modeling3can select many candidates as answers as long asthey are marked as true, and does not select anycandidate if every one is marked as false.
In con-trast, ranking always selects the most probablecandidate as an answer, which suits our task better.Our earlier experiments showed poor performancewith classification modeling, an experience alsoreported for Question Answering (Ravichandran etal.
2003).We modeled the problem to choose the mostprobable candidate that maximizes a given condi-tional probability distribution, given a set of holdercandidates h1h2.
.
.
hNand opinion expression e.The conditional probability P h h1h2.
.
.
hN, ecan be calculated based on K feature func-tions fkh , h1h2.
..hN, e .
We write a decision rulefor the ranking as follows:{ }{ } ]e),hhh(h,f?
[=e)],hhh|[P(hhK=kNkkhNh?=12121...argmax...argmaxEach k?
is a model parameter indicating theweight of its feature function.3 In our task, there are two classes: holder and non-holder.Figure 1 illustrates our holder identification sys-tem.
First, the system generates all possible holdercandidates, given a sentence and an opinion ex-pression <E>.
After parsing the sentence, it ex-tracts features such as the syntactic pathinformation between each candidate <H> and theexpression <E> and a distance between <H> and<E>.
Then it ranks holder candidates according tothe score obtained by the ME ranking model.
Fi-nally the system picks the candidate with the high-est score.
Below, we describe in turn how to selectholder candidates and how to select features for thetraining model.Holder Candidate Selection: Intuitively, onewould expect most opinion holders to be namedentities (PERSON or ORGANIZATION)4.
However,other common noun phrases can often be opinionholders, such as ?the leader?, ?three nations?, and?the Arab and Islamic world?.
Sometimes, pro-nouns like he, she, and they that refer to a PERSON,or it that refers to an ORGANIZATION or country,can be an opinion holder.
In our study, we considerall noun phrases, including common noun phrases,named entities, and pronouns, as holder candidates.Feature Selection: Our hypothesis is that thereexists a structural relation between a holder <H>and an expression <E> that can help to identifyopinion holders.
This relation may be representedby lexical-level patterns between <H> and <E>,but anchoring on surface words might run into thedata sparseness problem.
For example, if we seethe lexical pattern ?<H> recently criticized <E>?
inthe training data, it is impossible to match the ex-pression ?<H> yesterday condemned <E>?.
These,however, have the same syntactic features in our4 We use BBN?s named entity tagger IdentiFinder to collectnamed entities.Sentence             :   w1 w2 w3 w4 w5 w6 w7 w8 w9 ?
wnOpinion expression  <E>  :                  w6 w7 w8?
w2 ... w4 w5 w6 w7 w8 ?
w11 w12 w13 ?
w18 ?
w23 w24 w25 ..C1         C2            <E>                       C3                 C4                  C5givenCandidateholderselectionFeatureextraction:ParsingC1            C2    <E>          C3          C4          C5Rank the candidates byME model 1.C1   2.
C5   3.C3  4.C2  5.C4Pick the best candidate as a holder C1Figure 1: Overall system architecture203model.
We therefore selected structural featuresfrom a deep parse, using the Charniak parser.After parsing the sentence, we search for thelowest common parent node of the words in <H>and <E> respectively (<H> and <E> are mostlyexpressed with multiple words).
A lowest commonparent node is a non-terminal node in a parse treethat covers all the words in <H> and <E>.
Figure 2shows a parsed example of a sentence with theholder ?China?s official Xinhua news agency?
andthe opinion expression ?accusing?.
In this example,the lowest common parent of words in <H> is thebold NP and the lowest common parent of <E> isthe bold VBG.
We name these nodes Hhead andEhead respectively.
After finding these nodes, welabel them by subscript (e.g., NPH and VBGE) toindicate they cover <H> and <E>.
In order to seehow Hhead and Ehead are related to each other inthe parse tree, we define another node, HEhead,which covers both Hhead and Ehead.
In the exam-ple, HEhead is S at the top of the parse tree since itcovers both NPH and VBGE.
We also label S bysubscript as SHE.To express tree structure for ME training, weextract path information between <H> and <E>.
Inthe example, the complete path from Hhead toEhead is ?<H> NP S VP S S VP VBG <E>?.
How-ever, representing each complete path as a singlefeature produces so many different paths with lowfrequencies that the ME system would learnpoorly.
Therefore, we split the path into threeparts: HEpath, Hpath an Epath.
HEpath is definedas a path from HEhead to its left and right childnodes that are also parents of Hhead and Ehead.Hpath is a path from Hhead and one of its ancestornodes that is a child of HEhead.
Similarly, Epath isdefined as a path from Ehead to one of its ances-tors that is also a child of HEhead.
With this split-ting, the system can work when any of HEpath,Hpath or Epath appeared in the training data, evenif the entire path from <H> to <E> is unseen.
Table2 summarizes these concepts with two holder can-didate examples in the parse tree of Figure 2.We also include two non-structural features.
Thefirst is the type of the candidate, with values NP,PERSON, ORGANIZATION, and LOCATION.
Thesecond feature is the distance between <H> and<E>, counted in parse tree words.
This is moti-vated by the intuition that holder candidates tend tolie closer to their opinion expression.
All featuresare listed in Table 3.
We describe the performanceof the system in Section 4.Candidate 1 Candidate 2China?s official Xinu-hua news agency BushHhead NPH  NNPHEhead VBGE VBGEHEhead SHE VPHEHpath NPH NNPH NPH NPHNPH PPHEpath VBGE VPE SE SE VPE VBGE VPE SE SEHEpath SHE NPH VPE VPHE  PPH SETable 2: Heads and paths for the Figure 2 exampleFeatures DescriptionF1 Type of <H>F2 HEpathF3 HpathF4 EpathF5 Distance between <H> and <E>Table 3: Features for ME trainingNP ADVP VPS.NP JJNNP NNNNNNP POSRBVBD PPPP,NPINNNPNPINPPNPNNNP NPINSSofficialChina ?sXinhua news agencyalsoweighedinsundayonNNP POSchoiceBush ?sof NNSwordsVPVBGPPNPaccusingtheDT NNIN Spresidentof VPVBG NP PPorchestratingpublic opinionJJ NN In advance of possiblestrikes against the threecountries in an expansion ofthe war against terrorismFigure 2: A parsing example204Model 1?
Translate a German email to English?
Apply English opinion-bearing wordsModel 2?
Translate English opinion-bearing words toGerman?
Analyze a German email using the Germanopinion-bearing words.Table 4: Two models of German Email opinionanalysis system3 Applying our Methodology to GermanEmailsIn this section, we describe a German email analy-sis system into which we included the opinion-bearing words from Section 2.1 to detect opinionsexpressed in emails.
This system is part of a col-laboration with the EU-funded project QUALEG(Quality of Service and Legitimacy in eGovern-ment) which aims at enabling local governments tomanage their policies in a transparent and trustableway5.
For this purpose, local governments shouldbe able to measure the performance of the servicesthey offer, by assessing the satisfaction of its citi-zens.
This need makes a system that can monitorand analyze citizens?
emails essential.
The goal ofour system is to classify emails as neutral or asbearing a positive or negative opinion.To generate opinion bearing words, we ran theword sentiment classifier from Section 2.1 on 8011verbs to classify them into 807 positive, 785 nega-tive, and 6149 neutral.
For 19748 adjectives, thesystem classified them into 3254 positive, 303negative, and 16191 neutral.
Since our opinion-bearing words are in English and our target systemis in German, we also applied a statistical wordalignment technique, GIZA++ 6  (Och and Ney2000).
Running it on version two of the EuropeanParliament corpus, we obtained statistics for678,340 German-English word pairs and 577,362English-German word pairs.
Obtaining these twolists of translation pairs allows us to convert Eng-lish words to German, and German to English,without a full document translation system.
To util-ize our English opinion-bearing words in a Germanopinion analysis system, we developed two models,5 http://www.qualeg.eupm.net/my_spip/index.php6 http://www.fjoch.com/GIZA++.htmloutlined in Table 4, each of which is triggered atdifferent points in the system.In both models, however, we still need to decidehow to apply opinion-bearing words as clues todetermine the sentiment of a whole email.
Ourprevious work on sentence level sentiment classifi-cation (ref suppressed) shows that the presence ofany negative words is a reasonable indication of anegative sentence.
Since our emails are mostlyshort (the average number of words in each emailis 19.2) and we avoided collecting weak negativeopinion clue words, we hypothesize that our previ-ous sentence sentiment classification study workson the email sentiment analysis.
This implies thatan email is negative if it contains more than certainnumber of strong negative words.
We tune thisparameter using our training data.
Conversely, ifan email contains mostly positive opinion-bearingwords, we classify it as a positive email.
We assignneutral if an email does not contain any strongopinion-bearing words.Manually annotated email data was provided byour joint research site.
This data contains 71 emailsfrom citizens regarding a German festival.
26 ofthem contained negative complaints, for example,the lack of parking space, and 24 of them werepositive with complimentary comments to the or-ganization.
The rest of them were marked as?questions?
such as how to buy festival tickets,?only text?
of simple comments, ?fuzzy?, and ?dif-ficult?.
So, we carried system experiments on posi-tive and negative emails with precision and recall.We report system results in Section 4.4 Experiment ResultsIn this section, we evaluate the three systems de-scribed in Sections 2 and 3: detecting opinion-bearing words and identifying valence, identifyingopinion holders, and the German email opinionanalysis system.4.1 Detecting Opinion-bearing WordsWe described a word classification system to de-tect opinion-bearing words in Section 2.1.
To ex-amine its effectiveness, we annotated 2011 verbsand 1860 adjectives, which served as a gold stan-dard7.
These words were randomly selected from a7 Although nouns and adverbs may also be opinion-bearing,we focus only on verbs and adjectives for this study.205collection of 8011 English verbs and 19748 Eng-lish adjectives.
We use training data as seed wordsfor the WordNet expansion part of our algorithm(described in Section 2.1).
Table 5 shows the dis-tribution of each semantic class.
In both verb andadjective annotation, neutral class has much morewords than the positive or negative classes.We measured the precision, recall, and F-scoreof our system using 10-fold cross validation.
Table6 shows the results with 95% confidence bounds.Overall (combining positive, neutral and negative),our system achieved 77.7% ?
1.2% accuracy onverbs and 69.1% ?
2.1% accuracy on adjectives.The system has very high precision in the neutralcategory for both verbs (97.2%) and adjectives(89.5%), which we interpret to mean that our sys-tem is really good at filtering non-opinion bearingwords.
Recall is high in all cases but precision var-ies; very high for neutral and relatively high fornegative but low for positive.4.2 Opinion Holder IdentificationWe conducted experiments on 2822 <sentence;opinion expression; holder> triples and divided thedata set into 10 <training; test> sets for cross vali-dation.
For evaluation, we consider to match eitherfully or partially with the holder marked in the testdata.
The holder matches fully if it is a single entity(e.g., ?Bush?).
The holder matches partially whenit is part of the multiple entities that make up themarked holder.
For example, given a markedholder ?Michel Sidibe, Director of the Country andRegional Support Department of UNAIDS?, weconsider both ?Michel Sidibe?
and ?Director of theCountry and Regional Support Department ofUNAIDS?
as acceptable answers.Our experiments consist of two parts based onthe candidate selection method.
Besides the selec-tion method we described in Section 2.2, we alsoconducted a separate experiment by excluding pro-nouns from the candidate list.
With the secondmethod, the system always produces a non-pronoun holder as an answer.
This selectionmethod is useful in some Information Extractionapplication that only cares non-pronoun holders.We report accuracy (the percentage of correctanswers the system found in the test set) to evalu-ate our system.
We also report how many correctanswers were found within the top2 and top3 sys-tem answers.
Tables 7 and 8 show the system accu-racy with and without considering pronouns asalias candidates, respectively.
Table 8 mostlyshows lower accuracies than Table 7 because testdata often has only a non-pronoun entity as aholder and the system picks a pronoun as its an-swer.
Even if the pronoun refers the same entitymarked in the test data, the evaluation systemcounts it as wrong because it does not match thehand annotated holder.To evaluate the effectiveness of our system, weset the baseline as a system choosing the closestcandidate to the expression as a holder without theMaximum Entropy decision.
The baseline systemhad an accuracy of only 21.3% for candidate selec-tion over all noun phrases and 23.2% for candidateselection excluding pronouns.The results show that detecting opinion holdersis a hard problem, but adopting syntactic features(F2, F3, and F4) helps to improve the system.
Apromising avenue of future work is to investigatethe use of semantic features to eliminate nounPositive Negative Neutral TotalVerb 69 151 1791 2011Adjective 199 304 1357 1860Table 5: Word distribution in our gold standardPrecision Recall F-scoreV 20.5% ?
3.5% 82.4% ?
7.5% 32.3% ?
4.6%PA 32.4% ?
3.8% 75.5% ?
6.1% 45.1% ?
4.4%V 97.2% ?
0.6% 77.6% ?
1.4% 86.3% ?
0.7%XA 89.5% ?
1.7% 67.1% ?
2.7% 76.6% ?
2.1%V 37.8% ?
4.9% 76.2% ?
8.0% 50.1% ?
5.6%NA 60.0% ?
4.1% 78.5% ?
4.9% 67.8% ?
3.8%Table 6: Precision, recall, and F-score on word va-lence categorization for Positive (P), Negative (N)and Neutral (X) verbs (V) and adjectives (A) (with95% confidence intervals)Baseline F5 F15 F234 F12345Top1 23.2% 21.8% 41.6% 50.8% 52.7%Top2 39.7% 61.9% 66.3% 67.9%Top3 52.2% 72.5% 77.1% 77.8%Table 7: Opinion holder identification results(excluding pronouns from candidates)Baseline F5 F15 F234 F12345Top1 21.3% 18.9% 41.8% 47.9% 50.6%Top2 37.9% 61.6% 64.8% 66.7%Top3 51.2% 72.3% 75.3% 76.0%Table 8: Opinion holder identification results (Allnoun phrases as candidates)206phrases such as ?cheap energy subsidies?
or ?pos-sible strikes?
from the candidate set before we runour ME model, since they are less likely to be anopinion holder than noun phrases like ?three na-tions?
or ?Palestine people.
?4.3 German EmailsFor our experiment, we performed 7-fold crossvalidation on a set of 71 emails.
Table 9 shows theaverage precision, recall, and F-score.
Resultsshow that our system identifies negative emails(complaints) better than praise.
When we chose asystem parameter for the focus, we intended to findnegative emails rather than positive emails becauseofficials who receive these emails need to act tosolve problems when people complain but theyhave less need to react to compliments.
By high-lighting high recall of negative emails, we maymisclassify a neutral email as negative but there isalso less chance to neglect complaints.Category  Model1 Model2Precision 0.72 0.55Recall 0.40 0.65Positive(P)F-score 0.51 0.60Precision 0.55 0.61Recall 0.80 0.42Negative(N)F-score 0.65 0.50Table 9: German email opinion analysis system results5 Conclusion and Future WorkIn this paper, we presented a methodology for ana-lyzing judgment opinions, which we define asopinions consisting of a valence, a holder, and atopic.
We presented models for recognizing sen-tences containing judgment opinions, identifyingthe valence of the opinion, and identifying theholder of the opinion.
Remaining is to also finallyidentify the topic of the opinion.
Past tests withhuman annotators indicate that the accuracy ofidentifying valence, holder and topic is much in-creased when all three are being done simultane-ously.
We plan to investigate a joint model toverify this intuition.Our past work indicated that, for newspapertexts, it is feasible for annotators to identify judg-ment opinion sentences and for them to identifytheir holders and judgment valences.
It is encour-aging to see that we achieved good results on anew genre ?
emails sent from citizens to a city co-unsel ?
and in a new language, German.This paper presents a computational frameworkfor analyzing judgment opinions.
Even thoughthese are the most common opinions, it is a pitythat the research community remains unable to de-fine belief opinions (i.e., those opinions that havevalues such as true, false, possible, unlikely, etc.
)with high enough inter-annotator agreement.
Onlyonce we properly define belief opinion will we becapable of building a complete opinion analysissystem.ReferencesBerger, A, S. Della Pietra, and V. Della Pietra.
1996.
A MaximumEntropy Approach to Natural Language.
Computational Linguis-tics 22(1).Bethard, S., H. Yu, A. Thornton, V. Hatzivassiloglou, and D. Jurafsky.2004.
Automatic Extraction of Opinion Propositions and theirHolders.
AAAI Spring Symposium on Exploring Attitude and Affectin Text.Charniak, E. 2000.
A Maximum-Entropy-Inspired Parser.
Proc.
ofNAACL-2000.Choi, Y., C. Cardie, E. Riloff, and S. Patwardhan.
2005.
IdentifyingSources of Opinions with Conditional Random Fields and Extrac-tion Patterns.
Proc.
of Human Language Technology Confer-ence/Conference on Empirical Methods in Natural LanguageProcessing (HLT-EMNLP 2005).Esuli, A.  and F. Sebastiani.
2005.
Determining the semantic orienta-tion of terms through gloss classification.
Proc.
of CIKM-05, 14thACM International Conference on Information and KnowledgeManagement.Hatzivassiloglou, V. and McKeown, K. (1997).
Predicting the seman-tic orientation of adjectives.
Proc.
35th Annual Meeting of theAssoc.
for Computational Linguistics (ACL-EACL 97.Hu, M. and Liu, B.
2004.
Mining and summarizing customer reviews.Proc.
of KDD?04.
pp.168 - 177Och, F.J. 2002.
Yet Another MaxEnt Toolkit: YASMEThttp://wasserstoff.informatik.rwth-aachen.de/Colleag ues/och/Och, F.J and  Ney, H. 2000.
Improved statistical alignment models.Proc.
of ACL-2000, pp.
440?447, Hong Kong, China.Pang, B, L. Lee, and S. Vaithyanathan.
2002.
Thumbs up?
SentimentClassification using Machine Learning Techniques.
Proc.
ofEMNLP 2002.Ravichandran, D., E. Hovy, and F.J. Och.
2003.
Statistical QA - clas-sifier vs re-ranker: What?s the difference?
Proc.
of the ACL Work-shop on Multilingual Summarization and Question Answering.Riloff, E. and J. Wiebe.
2003.
Learning Extraction Patterns for Sub-jective Expressions.
Proc.
of EMNLP-03.Turney, P. 2002.
Thumbs Up or Thumbs Down?
Semantic OrientationApplied to Unsupervised Classification of Reviews.
Proc.
of the40th Annual Meeting of the ACL, 417?424.Wiebe, J, R. Bruce, and T. O?Hara.
1999.
Development and use of agold standard data set for subjectivity classifications.
Proc.
of the37th Annual Meeting of the Association for Computational Linguis-tics (ACL-99), 246?253.Wilson, T. and J. Wiebe.
2003.
Annotating Opinions in the WorldPress.
Proc.
of  ACL SIGDIAL-03.Yu, H. and V. Hatzivassiloglou.
2003.
Towards Answering OpinionQuestions: Separating Facts from Opinions and Identifying the Po-larity of Opinion Sentences.
Proc.
of EMNLP.207
