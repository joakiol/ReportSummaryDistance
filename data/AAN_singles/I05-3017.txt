The Second International Chinese Word Segmentation BakeoffThomas EmersonBasis Technology Corp.150 CambridgePark DriveCambridge, MA 02140tree@basistech.comAbstractThe second international Chinese wordsegmentation bakeoff was held in thesummer of 2005 to evaluate the currentstate of the art in word segmentation.Twenty three groups submitted 130 re-sult sets over two tracks and four differ-ent corpora.
We found that the technol-ogy has improved over the interveningtwo years, though the out-of-vocabularyproblem is still or paramount impor-tance.1!
IntroductionChinese is written without inter-word spaces, sofinding word-boundaries is an essential first stepin many natural language processing applica-tions including mono- and cross-lingual infor-mation retrieval and text-to-speech systems.
Thisword segmentation problem has been active areaof research in computational linguistics for al-most two decades and is a topic of active re-search around the world.
As the very notion of?word-hood?
in Chinese is hotly debated, so thedetermination of the correct division of a Chi-nese sentence into ?words?
can be very complex.In 2003 SIGHAN, the Special InterestGroup for Chinese Language Processing of theAssociation for Computational Linguistics(ACL) conducted the first International ChineseWord Segmentation Bakeoff (Sproat and Emer-son, 2003).
That competition was the first con-ducted outside of China and has become thebenchmark with which researchers evaluate theirsegmentation systems.
During the winter of2004 it was decided to hold a second evaluationto determine how the latest research has affectedsegmentation technology.2!
Details of the Contest2.1!
The CorporaFour corpora were used in the evaluation, twoeach using Simplified and Traditional Chinesecharacters.1 The Simplified Chinese corporawere provided by Beijing University and Micro-soft Research Beijing.
The Traditional Chinesecorpora were provided by Academia Sinica inTaiwan and the City University of Hong Kong.Each provider supplied separate training andtruth data sets.
Details on each corpus are pro-vided in Table!1.With one exception, all of the corpora wereprovided in a single character encoding.
We de-cided to provide all of the data in both Unicode(UTF-8 encoding) and the standard encodingused in each locale.
This would allow systemsthat use one or the other encoding to chose ap-propriately while ensuring consistent transcod-ing across all sites.
This conversion was prob-lematic in two cases:1.
The Academia Sinica corpus, providedin Unicode (UTF-16), contained char-acters found in Big Five Plus that are notfound in Microsoft?s CP950 or standardBig Five.
It also contained compatibilitycharacters that led to transcoding errorswhen converting from Unicode to BigFive Plus.
A detailed description of theseissues can be found on the Bakeoff 20051 A fifth (Simplified Chinese) corpus was provided by the University of Pennsylvania, but for numerous technical reasons itwas not used in the evaluation.
However, it has been made available (both training and truth data) on the SIGHAN websitealong with the other corpora.123pages on the SIGHAN website.
The dataalso included 11 instances of an invalidcharacter that could not be converted toBig Five Plus.2.
The City University of Hong Kong datawas initially supplied in Big Five/HKSCS.
We initially converted this toUnicode but found that there were char-acters appearing in Unicode IdeographExtension B, which many systems areunable to handle.
City University wasgracious enough to provide Unicodeversions for their files with all charactersin the Unicode BMP.
Specific details canbe found on the Bakeoff 2005 pages ofthe SIGHAN website.The truth data was provided in segmentedand unsegmented form by all of the providersexcept Academia Sinica, who only provided thesegmented truth files.
These were converted tounsegmented form using a simple Perl script.Unfortunately this script also removed spacesseparating non-Chinese (i.e., English) tokens.We had no expectation of correct segmentationon non-Chinese text, so the spaces were manu-ally removed between non-Chinese text in thetruth data prior to scoring.The Academia Sinica data separated tokensin both the training and truth data using a full-width space instead of one or more half-width(i.e., ASCII) spaces.
The scoring script wasmodified to ignore the type of space used so thatteams would not be penalized during scoring forusing a different separator.The segmentation standard used by eachprovider were made available to the participants,though late in the training period.
These stan-dards are either extremely terse (MSR), verbosebut in Chinese only (PKU, AS), or are verboseand moderately bilingual.
The PKU corpus usesa standard derived from GB 13715, the Chinesegovernment standard for text segmentation incomputer applications.
Similarly AS uses a Tai-wanese national standard for segmentation incomputer applications.
The CityU data was seg-mented using the LIVAC corpus standard, andthe MSR data to Microsoft's internal standard.The standards are available on the bakeoff website.The PKU data was edited by the organizersto remove a numeric identifier from the start ofeach line.
Unless otherwise noted in this paperno changes beyond transcoding were made tothe data furnished by contributors.2.2!
Rules and ProceduresThe bakeoff was run almost identically to thefirst described in Sproat and Emerson (2003):the detailed instructions provided to the partici-pants are available on the bakeoff website athttp://www.sighan.org/bakeoff2005/ .Groups (or ?sites?
as they were also called) in-terested in participating in the competition reg-istered on the SIGHAN website.
Only the pri-mary researcher for each group was asked toregister.
Registration was opened on June 1,Corpus Abbrev.
Encodings Training Size(Words/Types)Test Size(Words/Types)Academia Sinica(Taipei)AS Big Five Plus, Unicode 5.45M / 141K 122K / 19KBeijing University PK CP936, Unicode 1.1M / 55K 104K / 13KCity University ofHong KongCityU Big Five/HKSCS, Unicode 1.46M / 69K 41K / 9KMicrosoft Research(Beijing)MSR CP936, Unicode 2.37M / 88K 107K / 13KTable 1.
Corpus Information1242005 and allowed to continue through the timethe training data was made available on July 11.When a site registered they selected which cor-pus or corpora there were interested in using,and whether they would take part in the open orclosed tracks (described below.)
On July 11 thetraining data was made available on the Bakeoffwebsite for downloading: the same data wasused regardless of the tracks the sites registeredfor.
The web site did not allow a participant toID Site Contact Country AS PKU CityU MSR2 ICL, Beijing University Wuguang SHI ZH !3 Xiamen University Xiaodong SHI ZH "!
"!
"!
"!4 ITNLP Lab, Harbin Institute ofTechnologyWei JIANG ZH "!
"!
"!
"!5 France Telecom R&D Beijing Heng LI ZH "!
"!
"!
"!6 Information Retrieval Lab, HarbinInstitute of TechnologyHuipeng ZHANG ZH "!7 Dept.
of Linguistics, The Universityof Hong KongGuohong FU HK "!
"!
"!
"!8 Computer Science Dept., XiamenUniversityHua-lin Zeng ZH "!
"!9 Dept.
of Linguistics, The Ohio StateUniversityXiaofei LU US "12 Dept.
of Computer Science, TheUniversity of SheffieldYaoyong LI GB "!
"!
"!
"!13 Nanjing University Jiajun CHEN ZH "!
"!14 Stanford NL Group Huihsin TSENG US " " " "15 Nara Institute of Science and Tech-nologyMasayuki ASAHARA JP " " " "16 Academia Sinica Yu-Fang TSAI TW !
!19 National University of Singapore Hwee Tou NG SG !
!
!
!21 Kookmin University Seung-Shik KANG KO " " "23 US Dept.
of Defense Thomas Keenan US !
!24 Dept.
of Information Management,Tung Nan Institute of TechnologyJia-Lin TSAI TW "26 ICL, Peking University Huiming DUAN ZH "!27 Yahoo!
Inc. Aitao CHEN US "!
"!
"!
"!29 The Chinese University of HongKongTak Pang LAU HK " " "31 City University of Hong Kong Ka Po CHOW HK !
!33 City University of Hong Kong Chun Yu KIT HK " " "34 Institute of Computing Technology,Chinese Academy of SciencesShuangLong LI ZH "!
"!Table 2.
Participating Groups (" = closed test, !
= open test)125add a corpus to the set they initially selected,though at least one asked us via email to add oneand this was done manually.
Groups were givenuntil July 27 to train their systems, when thetesting data was released on the web site.
Theythen had two days to process the test corpora andreturn them to the organizer via email on Jul 29for scoring.
Each participant?s results wereposted to their section of the web site onAugust !6, and the summary results for all par-ticipants were made available to all groups onAugust 12.Two tracks were available for each corpus,open and closed:?
In the open tests participants could use anyexternal data in addition to the trainingcorpus to train their system.
This included,but was not limited to, external lexica,character set knowledge, part-of-speechinformation, etc.
Sites participating in anopen test were required to describe thisexternal data in their system description.?
In closed tests, participants were onlyallowed to use information found in thetraining data.
Absolutely no other data orinformation could be used beyond that inthe training document.
This includedknowledge of character sets, punctuationcharacters, etc.
These seemingly artificialrestrictions (when compared to ?realworld?
systems) were formulated to studyexactly how far one can get without sup-plemental information.Other obvious restrictions applied: groupscould not participate using corpora that they ortheir organization provided or that they had usedbefore or otherwise seen.Sites were allowed submit multiple runswithin a track, allowing them to compare variousapproaches.Scoring was done automatically using acombination of Perl and shell scripts.
Partici-pants were asked to submit their data using verystrict naming conventions to facilitate this: inonly a couple of instances were these not fol-lowed and human intervention was required.After the scoring was done the script would mailthe detailed results to the participant.
The scriptsused for scoring can be downloaded from theCorpus WordCountR P F OOV Roov RivAS 122,610 0.909 0.857 0.882 0.043 0.004 0.950CityU 40936 0.882 0.790 0.833 0.074 0.000 0.952MSR 106,873 0.955 0.912 0.933 0.026 0.000 0.981PKU 104,372 0.904 0.836 0.869 0.058 0.059 0.956Table 3: Baseline scores generated via maximal matching using only words from the training dataCorpus WordCountR P F OOV Roov RivAS 122,610 0.979 0.985 0.982 0.043 0.996 0.978CityU 40,936 0.988 0.991 0.989 0.074 0.997 0.988MSR 106,873 0.991 0.992 0.991 0.026 0.998 0.990PKU 104,372 0.985 0.988 0.987 0.058 0.994 0.985Table 4: Topline scores generated via maximal matching using only words from the testing data126Bakeoff 2005 web site.
It was provided to theparticipants to aid in the their data analysis.
Asnoted above, some of the training/truth data useda full-width space to separate tokens: the scoringscript was modified to ignore the differencesbetween full-width and half-width spaces.
Thisis the only case where the half-width/full-widthdistinction was ignored: a system that convertedtokens from full-width to half-width was penal-ized by the script.2.3!
Participating SitesThirty-six sites representing 10 countries ini-tially signed up for the bakeoff.
The People?sRepublic of China had the greatest number with17, followed by the United States (6), HongKong (5), Taiwan (3), six others with one each.Of these, 23 submitted results for scoring andsubsequently submitted a paper for these pro-ceedings.
A summary of participating groups andthe tracks for which they submitted results canbe found in Table!2 on the preceding page.
Alltogether 130 runs were submitted for scoring.3!
ResultsIn order to provide hypothetical best and worstcase results (i.e., we expect systems to do noworse than the base-line and to generally under-perform the top-line), we used a simple left-to-right maximal matching algorithm implementedin Perl to generate ?top-line?
and ?base-line?Participant Run ID Word Count R Cr P Cp F OOV Roov Riv15 b 122610 0.952 ?0.00122 0.951 ?0.00123 0.952 0.043 0.696 0.96315 a 122610 0.955 ?0.00118 0.939 ?0.00137 0.947 0.043 0.606 0.97114 122610 0.95 ?0.00124 0.943 ?0.00132 0.947 0.043 0.718 0.96027 122610 0.955 ?0.00118 0.934 ?0.00142 0.945 0.043 0.468 0.97812 122610 0.946 ?0.00129 0.942 ?0.00134 0.944 0.043 0.648 0.9597 122610 0.947 ?0.00128 0.934 ?0.00142 0.94 0.043 0.523 0.96615 c 122610 0.944 ?0.00131 0.934 ?0.00142 0.939 0.043 0.445 0.96733 122610 0.944 ?0.00131 0.902 ?0.00170 0.923 0.043 0.234 0.9765 122610 0.948 ?0.00127 0.900 ?0.00171 0.923 0.043 0.158 0.9834 122610 0.943 ?0.00132 0.895 ?0.00175 0.918 0.043 0.137 0.9793 122610 0.877 ?0.00188 0.796 ?0.00230 0.835 0.043 0.128 0.911Table 5.
Academia Sinica ?
Closed (italics indicate performance below baseline)Participant Run ID Word Count R Cr P Cp F OOV Roov Riv19 122610 0.962 ?0.00109 0.95 ?0.00124 0.956 0.043 0.684 0.97527 122610 0.958 ?0.00115 0.938 ?0.00138 0.948 0.043 0.506 0.97812 122610 0.949 ?0.00126 0.947 ?0.00128 0.948 0.043 0.686 0.9617 122610 0.955 ?0.00118 0.938 ?0.00138 0.946 0.043 0.579 0.97231 122610 0.943 ?0.00132 0.931 ?0.00145 0.937 0.043 0.531 0.9624 122610 0.952 ?0.00122 0.92 ?0.00155 0.936 0.043 0.354 0.9795 122610 0.952 ?0.00122 0.919 ?0.00156 0.935 0.043 0.311 0.9813 122610 0.004 ?0.00036 0.004 ?0.00036 0.004 0.043 0.085 0Table 6.
Academia Sinica ?
Open (italics indicate performance below baseline)127numbers.
This was done by generating word listsbased only on the vocabulary in each truth (top-line) and training (bottom-line) corpus andsegmenting the respective test corpora.
Theseresults are presented in Tables!3 and 4.All of the results comprise the followingdata: test recall (R), test precision (P), balancedF score (where F = 2PR/(P + R)), the out-of-vocabulary (OOV) rate on the test corpus, therecall on OOV words (Roov), and the recall onin-vocabulary words (Riv).
We use the usualdefinition of out-of-vocabulary words as the setof words occurring in the test corpus that are notin the training corpus.As in the previous evaluation, to test theconfidence level that two trials are significantlydifferent from each other we used the CentralLimit Theorem for Bernoulli trials (Grinsteadand Snell, 1997), assuming that the recall ratesfrom the various trials represents the probabilitythat a word will be successfully identified, andthat a binomial distribution is appropriate for theexperiment.
We calculated these values at the95% confidence interval with the formula ?2 !
(pParticipant Run ID Word Count R Cr P Cp F OOV Roov Riv14 40936 0.941 ?0.00233 0.946 ?0.00223 0.943 0.074 0.698 0.96115 a 40936 0.942 ?0.00231 0.941 ?0.00233 0.942 0.074 0.629 0.96715 b 40936 0.937 ?0.00240 0.946 ?0.00223 0.941 0.074 0.736 0.95327 40936 0.949 ?0.00217 0.931 ?0.00251 0.94 0.074 0.561 0.987 40936 0.944 ?0.00227 0.933 ?0.00247 0.939 0.074 0.626 0.96912 40936 0.931 ?0.00251 0.941 ?0.00233 0.936 0.074 0.657 0.95329 d 40936 0.937 ?0.00240 0.922 ?0.00265 0.929 0.074 0.698 0.95615 c 40936 0.915 ?0.00276 0.94 ?0.00235 0.928 0.074 0.598 0.9429 a 40936 0.938 ?0.00238 0.915 ?0.00276 0.927 0.074 0.658 0.96129 b 40936 0.936 ?0.00242 0.913 ?0.00279 0.925 0.074 0.656 0.95921 40936 0.917 ?0.00273 0.925 ?0.00260 0.921 0.074 0.539 0.94829 c 40936 0.925 ?0.00260 0.896 ?0.00302 0.91 0.074 0.639 0.9484 40936 0.934 ?0.00245 0.865 ?0.00338 0.898 0.074 0.248 0.9895 40936 0.932 ?0.00249 0.862 ?0.00341 0.895 0.074 0.215 0.9893 40936 0.814 ?0.00385 0.711 ?0.00448 0.759 0.074 0.227 0.86Table 7: City University of Hong Kong ?
Closed (italics indicate performance below baseline)Participant Run ID Word Count R Cr P Cp F OOV Roov Riv19 40936 0.967 ?0.00177 0.956 ?0.00203 0.962 0.074 0.806 0.9816 40936 0.958 ?0.00198 0.95 ?0.00215 0.954 0.074 0.775 0.97327 40936 0.952 ?0.00211 0.937 ?0.00240 0.945 0.074 0.608 0.987 40936 0.944 ?0.00227 0.938 ?0.00238 0.941 0.074 0.667 0.96612 40936 0.933 ?0.00247 0.94 ?0.00235 0.936 0.074 0.653 0.9554 40936 0.946 ?0.00223 0.898 ?0.00299 0.922 0.074 0.417 0.9895 40936 0.94 ?0.00235 0.901 ?0.00295 0.92 0.074 0.41 0.9823 40936 0.014 ?0.00116 0.013 ?0.00112 0.013 0.074 0.029 0.012Table 8: City University of Hong Kong ?
Open (italics indicate performance below baseline)128(1 - p)/n) where n is the number of words.
Thisvalue appears in subsequent tables under thecolumn cr.
We also calculate the confidence thatthe a character string segmented as a word isactually a word by treating p as the precisionrates of each system.
This is referred to as cp inthe result tables.
Two systems are then consid-ered to be statistically different (at a 95% confi-dence level) if one of their cr or cp are different.Tables 5?12 contain the results for each corpusand track (groups are referenced by their ID asfound in Table!2) ordered by F score.Participant Run ID Word Count R Cr P Cp F OOV Roov Riv14 106873 0.962 ?0.00117 0.966 ?0.00111 0.964 0.026 0.717 0.9687 106873 0.962 ?0.00117 0.962 ?0.00117 0.962 0.026 0.592 0.97227 a 106873 0.969 ?0.00106 0.952 ?0.00131 0.960 0.026 0.379 0.98527 b 106873 0.968 ?0.00108 0.953 ?0.00129 0.960 0.026 0.381 0.9844 106873 0.973 ?0.00099 0.945 ?0.00139 0.959 0.026 0.323 0.99115 b 106873 0.952 ?0.00131 0.964 ?0.00114 0.958 0.026 0.718 0.9585 106873 0.974 ?0.00097 0.940 ?0.00145 0.957 0.026 0.21 0.99513 106873 0.959 ?0.00121 0.956 ?0.00125 0.957 0.026 0.496 0.97212 106873 0.952 ?0.00131 0.960 ?0.00120 0.956 0.026 0.673 0.9624 6 106873 0.958 ?0.00123 0.952 ?0.00131 0.955 0.026 0.503 0.9724 7 106873 0.958 ?0.00123 0.952 ?0.00131 0.955 0.026 0.504 0.9724 4 106873 0.958 ?0.00123 0.949 ?0.00135 0.954 0.026 0.465 0.97224 5 106873 0.958 ?0.00123 0.951 ?0.00132 0.954 0.026 0.493 0.97124 3 106873 0.968 ?0.00108 0.938 ?0.00148 0.953 0.026 0.205 0.98933 106873 0.965 ?0.00112 0.935 ?0.00151 0.950 0.026 0.189 0.98615 a 106873 0.955 ?0.00127 0.942 ?0.00143 0.949 0.026 0.378 0.97121 106873 0.945 ?0.00139 0.949 ?0.00135 0.947 0.026 0.576 0.95524 0 106873 0.956 ?0.00125 0.938 ?0.00148 0.947 0.026 0.327 0.97334 106873 0.948 ?0.00136 0.942 ?0.00143 0.945 0.026 0.664 0.95524 2 106873 0.964 ?0.00114 0.924 ?0.00162 0.944 0.026 0.025 0.98915 c 106873 0.964 ?0.00114 0.923 ?0.00163 0.943 0.026 0.025 0.9924 1 106873 0.963 ?0.00115 0.924 ?0.00162 0.943 0.026 0.025 0.98929 a 106873 0.946 ?0.00138 0.933 ?0.00153 0.939 0.026 0.587 0.95629 b 106873 0.941 ?0.00144 0.932 ?0.00154 0.937 0.026 0.624 0.958 b 106873 0.957 ?0.00124 0.917 ?0.00169 0.936 0.026 0.025 0.9828 c 106873 0.955 ?0.00127 0.915 ?0.00171 0.935 0.026 0.025 0.9826 106873 0.937 ?0.00149 0.928 ?0.00158 0.932 0.026 0.457 0.953 106873 0.908 ?0.00177 0.927 ?0.00159 0.917 0.026 0.247 0.9268 a 106873 0.898 ?0.00185 0.896 ?0.00187 0.897 0.026 0.327 0.914Table 9: Microsoft Research ?
Closed (italics indicate performance below baseline)1294!
DiscussionAcross all of the corpora the best performingsystem, in terms of F score, achieved a 0.972,with an average of 0.918 and median of 0.941.As one would expect the best F score on theopen tests was higher than the best on the closedtests, 0.972 vs. 0.964, both on the MSR corpus.This result follows from the fact that systemstaking part on the open test can utilize moreinformation than those on the closed.
Also inter-esting to compare are the OOV recall rates be-tween the Open and Closed tracks.
The bestOOV recall in the open evaluation was 0.872compared to just 0.813 on the closed track.These data indicate that OOV handling is stillthe Achilles heel of segmentation systems, evenwhen the OOV rates are relatively small.
TheseOOV recall scores are better than those observedin the first bakeoff in 2003, with similar OOVvalues, which suggests that advances in un-known word recognition have occurred.
Never-theless OOV is still the most significant problemin segmentation systems.The best score on any track in the 2003bakeoff was F=0.961, while the best for thisevaluation was F=0.972, followed by 17 otherscores above 0.961.
This shows a general trendto a decrease in error rates, from 3.9% to 2.8%!These scores are still far below the theoretical0.99 level reflected in the topline and the highernumbers often reflected in the literature.
It isplain that one can construct a test set that anygiven system will achieve very high measures ofprecision and recall on, but these numbers mustviewed with caution as they may not scale toother applications or other problem sets.Three participants that used the scoringscript in their system evaluation observed differ-ent behavior from that of the organizers in theParticipant Run ID Word Count R Cr P Cp F OOV Roov Riv4 106873 0.98 ?0.00086 0.965 ?0.00112 0.972 0.026 0.59 0.9919 106873 0.969 ?0.00106 0.968 ?0.00108 0.968 0.026 0.736 0.9757 106873 0.969 ?0.00106 0.966 ?0.00111 0.967 0.026 0.612 0.97927 b 106873 0.971 ?0.00103 0.961 ?0.00118 0.966 0.026 0.512 0.9835 106873 0.975 ?0.00096 0.957 ?0.00124 0.966 0.026 0.453 0.98913 106873 0.959 ?0.00121 0.971 ?0.00103 0.965 0.026 0.785 0.96427 a 106873 0.97 ?0.00104 0.957 ?0.00124 0.963 0.026 0.466 0.98412 106873 0.95 ?0.00133 0.958 ?0.00123 0.954 0.026 0.648 0.95826 106873 0.925 ?0.00161 0.936 ?0.00150 0.930 0.026 0.617 0.9338 a 106873 0.94 ?0.00145 0.917 ?0.00169 0.928 0.026 0.239 0.95934 106873 0.916 ?0.00170 0.933 ?0.00153 0.924 0.026 0.705 0.9228 c 106873 0.928 ?0.00158 0.913 ?0.00172 0.920 0.026 0.355 0.9448 b 106873 0.923 ?0.00163 0.914 ?0.00172 0.918 0.026 0.354 0.9382 106873 0.913 ?0.00172 0.915 ?0.00171 0.914 0.026 0.725 0.9183 106873 0.921 ?0.00165 0.897 ?0.00186 0.909 0.026 0.562 0.938 d 106873 0.92 ?0.00166 0.889 ?0.00192 0.904 0.026 0.332 0.9368 e 106873 0.9 ?0.00184 0.861 ?0.00212 0.880 0.026 0.309 0.91627 c 106873 0.865 ?0.00209 0.844 ?0.00222 0.855 0.026 0.391 0.87823 106873 0.788 ?0.00250 0.818 ?0.00236 0.803 0.026 0.37 0.8Table 10: Microsoft Research ?
Open (italics indicate performance below baseline)130generation of the recall numbers, thereby af-fecting the F score.
We were unable to replicatethe behavior observed by the participant, norcould we determine a common set of softwareversions that might lead to the problem.
Weverified our computed scores on two differentoperating systems and two different hardwarearchitectures.
In each case the difference was inthe participants favor (i.e., resulted in an in-creased F score) though the impact was minimal.If there is an error in the scripts then it affects alldata sets identically, so we are confident in thescores as reported here.
Nevertheless, we hopethat further investigation will uncover the causeof the discrepancy so that it can be rectified inthe future.4.1!
Future DirectionsThis second bakeoff was an unqualified success,both in the number of systems represented and inthe demonstrable improvement in segmentationtechnology since 2003.
However, there are stillopen questions that future evaluations can at-tempt to answer, including: how well a systemtrained on one genre performs when faced withtext from a different register.
This will stressOOV handling in the extreme.
Consider a situa-tion where a system trained on PRC newswireParticipant Run ID Word Count R Cr P Cp F OOV Roov Riv27 104372 0.953 ?0.00131 0.946 ?0.00140 0.95 0.058 0.636 0.97214 104372 0.946 ?0.00140 0.954 ?0.00130 0.95 0.058 0.787 0.9566 a 104372 0.952 ?0.00132 0.945 ?0.00141 0.949 0.058 0.673 0.9696 b 104372 0.952 ?0.00132 0.943 ?0.00144 0.947 0.058 0.673 0.96913 104372 0.941 ?0.00146 0.95 ?0.00135 0.946 0.058 0.813 0.9497 104372 0.943 ?0.00144 0.944 ?0.00142 0.944 0.058 0.656 0.96115 b 104372 0.93 ?0.00158 0.951 ?0.00134 0.941 0.058 0.76 0.9414 104372 0.954 ?0.00130 0.927 ?0.00161 0.941 0.058 0.518 0.98134 104372 0.938 ?0.00149 0.942 ?0.00145 0.94 0.058 0.767 0.94815 a 104372 0.93 ?0.00158 0.938 ?0.00149 0.934 0.058 0.521 0.9555 104372 0.95 ?0.00135 0.919 ?0.00169 0.934 0.058 0.449 0.989 104372 0.922 ?0.00166 0.934 ?0.00154 0.928 0.058 0.728 0.93412 104372 0.919 ?0.00169 0.935 ?0.00153 0.927 0.058 0.593 0.93915 c 104372 0.904 ?0.00182 0.93 ?0.00158 0.917 0.058 0.325 0.9429 a 104372 0.926 ?0.00162 0.908 ?0.00179 0.917 0.058 0.535 0.9529 c 104372 0.918 ?0.00170 0.915 ?0.00173 0.917 0.058 0.621 0.93633 104372 0.929 ?0.00159 0.904 ?0.00182 0.916 0.058 0.252 0.97121 104372 0.9 ?0.00186 0.925 ?0.00163 0.912 0.058 0.389 0.93129 b 104372 0.917 ?0.00171 0.903 ?0.00183 0.91 0.058 0.6 0.9378 a 104372 0.906 ?0.00181 0.886 ?0.00197 0.896 0.058 0.29 0.9438 c 104372 0.907 ?0.00180 0.843 ?0.00225 0.874 0.058 0.082 0.9588 b 104372 0.906 ?0.00181 0.842 ?0.00226 0.873 0.058 0.081 0.9563 104372 0.843 ?0.00225 0.737 ?0.00273 0.786 0.058 0.153 0.885Table 11: Peking University ?
Closed (italics indicate performance below baseline)131text is given the Chinese translation of the Ara-bic al Jazeera newspaper.
A more detailedevaluation of different techniques for dealingwith certain constructs is also in order, findingthe right balance of learned and heuristic knowl-edge is paramount.
Tied to the accuracy per-formance of such hybrid systems is the runtimespeed: the trade-off between accuracy andthroughput is vitally important as more and moredata becomes computerized.
The overall effectsof the various segmentation standards on thecomparison of disparate systems has yet to bestudied.
In particular, a categorization of thedifferences in standards and the prevalence ofthe features reflected would be a worth whilestudy.
Xia (2000) compares the Penn ChineseTreebank?s standard with those used in Taiwanand China, and concludes that, ?most disagree-ments among these three guidelines do not makemuch difference in bracketing or sentence inter-pretation.?
This is probably not so transparentwhen evaluating segmentation accuracy, how-ever.No segmentation study has yet to examinethe handling of short strings where there is littlesurrounding context, as in search engine queries.Future evaluations should be designed to focuson these and other specific areas of interest.AcknowledgmentsThis bakeoff could not have taken place withoutthe following institutions who provided trainingand testing data:?
Institute of Linguistics, Academia Sinica,Taipei, Taiwan?
Institute for Computational Linguistics,Beijing University, Beijing, China?
Language Information Sciences ResearchCentre, City University of Hong Kong,Hong Kong SAR?
Microsoft Research Asia, Beijing, ChinaI would like to thank Gina Lavow and Chu-RenHuang for their organization of the fourthSIGHAN workshop of which this bakeoff isParticipant Run ID Word Count R Cr P Cp F OOV Roov Riv19 104372 0.968 ?0.00109 0.969 ?0.00107 0.969 0.058 0.838 0.9764 104372 0.968 ?0.00109 0.966 ?0.00112 0.967 0.058 0.826 0.97713 104372 0.964 ?0.00115 0.97 ?0.00106 0.967 0.058 0.864 0.9727 a 104372 0.964 ?0.00115 0.966 ?0.00112 0.965 0.058 0.841 0.9716 a 104372 0.961 ?0.00120 0.969 ?0.00107 0.965 0.058 0.872 0.9666 b 104372 0.961 ?0.00120 0.966 ?0.00112 0.963 0.058 0.869 0.9667 104372 0.959 ?0.00123 0.965 ?0.00114 0.962 0.058 0.853 0.9665 104372 0.964 ?0.00115 0.96 ?0.00121 0.962 0.058 0.788 0.9743 104372 0.959 ?0.00123 0.954 ?0.00130 0.957 0.058 0.776 0.9734 104372 0.944 ?0.00142 0.961 ?0.00120 0.952 0.058 0.869 0.94816 104372 0.945 ?0.00141 0.956 ?0.00127 0.951 0.058 0.79 0.95531 104372 0.952 ?0.00132 0.951 ?0.00134 0.951 0.058 0.784 0.9628 a 104372 0.943 ?0.00144 0.944 ?0.00142 0.943 0.058 0.737 0.95512 104372 0.932 ?0.00156 0.944 ?0.00142 0.938 0.058 0.755 0.9438 b 104372 0.886 ?0.00197 0.919 ?0.00169 0.902 0.058 0.561 0.90527 b 104372 0.877 ?0.00203 0.904 ?0.00182 0.89 0.058 0.72 0.88623 104372 0.781 ?0.00256 0.846 ?0.00223 0.813 0.058 0.628 0.791Table 12: Peking University ?
Open (italics indicate performance below baseline)132part, and John O?Neil for his comments on anearlier draft of this paper.
Finally I would alsolike to thank the participants for their interestand hard work in making this bakeoff a success.ReferencesCharles M. Grinstead and J. Laurie Snell.
1997.Introduction to Probability.
AmericanMathematical Society, Providence, RI, 2ndEdition.Richard Sproat and Thomas Emerson.
2003.
TheFirst International Chinese Word Segmenta-tion Bakeoff.
In Proceedings of the SecondSIGHAN Workshop on Chinese LanguageProcessing, July 11?12, 2003, Sapporo, Ja-pan.Fei Xia.
2000.
The Segmentation Guidelines forthe Penn Chinese Treebank (3.0).133
