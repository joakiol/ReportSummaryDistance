Robust Text Processing in Automated Information RetrievalTomek StrzalkowskiCourant Institute of Mathematical SciencesNew York University715 Broadway, rm.
704New York, NY 10003tomek@cs.nyu.eduAbstractWe report on the results of a series of experiments witha prototype text retrieval system which uses relativelyadvanced natural language processing techniques inorder to enhance the effectiveness of statistical docu-ment retrieval.
In this paper we show that large-scalenatural anguage processing (hundreds of millions ofwords and more) is not only required for a betterretrieval, but it is also doable, given appropriateresources.
In particular, we demonstrate hat the use ofsyntactic ompounds in the representation f databasedocuments as well as in the user queries, coupled withan appropriate term weighting strategy, can consider-ably improve the effectiveness of retrospective s arch.The experiments reported here were conducted on TIP-STER database in connection with the Text REtrievalConference series (TREC).
11 IntroductionThe task of information retrieval is to extractrelevant documents from a large collection of docu-ments in response to user queries.
When the docu-ments contain primarily unrestricted text (e.g., newspa-per articles, legal documents, etc.)
the relevance of adocument is established through 'full-text' retrieval.This has been usually accomplished by identifying keyterms in the documents (the process known as 'index-ing') which could then be matched against terms inqueries (Salton, 1989).
The effectiveness of any suchterm-based approach is directly related to the accuracywith which a set of terms represents the content of adocument, as well as how well it contrasts a givendocument with respect o other documents.
In otherwords, we are looking for a representation R such thatfor any text items D1 and D2, R(D1) = R(D2) iffmeaning(D1) = meaning(D2), at an appropriate levelof abstraction (which may depend on the types andcharacter of anticipated queries),' See (Harman, 1993) for a detailed description f TREC.The simplest word-based representations of con-tent are usually inadequate since single words arerarely specific enough for accurate discrimination, andtheir grouping is often accidental.
A better method is toidentify groups of words that create meaningfulphrases, especially if these phrases denote importantconcepts in the database domain.
For example, jointventure is an important term in the Wall Street Journal(WSJ henceforth) database, while neither joint nor ven-ture are important by themselves.
In fact, in a 800+MBytes database, both joint and venture would oftenbe dropped from the list of terms by the system becausetheir inverted document frequency (idj) weights weretoo low.
In large databases comprising hundreds ofthousands of documents he use of phrasal terms is notjust desirable, itbecomes necessary.To illustrate this point let us consider TRECTopic 104, an information request from which a data-base search query is to be built.
The reader may notevarious sections of this Topic, with <desc> correspond-ing to the user's original request, further elaborated in<narr>, and <con> consisting of expert-assignedphrases denoting key concepts to be considered.<top><num> Number: 104<dora> Domain: Law and Government<title> Topic: Catastrophic Health Insurance<desc> Description:Document will enumerate provisions of the U.S.
Catastrophic HealthInsurance Act of 1988, or the political/legal f lout from thatlegislation.<aaarr> Narrative:A relevant document will detail the content ofthe U.S. medicareact of 1988 which extended catastrophic illness benefitsto the elderly, with particular ttention tothe financing schemewhich led to a firestorm of protest and a Congressional retreat,or a relevant document will detail the political/legalconsequences of the catastrophic health insurance imbroglio andsubsequent efforts by Congress to provide similar coveragesthrough a less-controversial mechanism.<con> Concept(s):1.
Catastrophic Coverage Act of 1988, Medicare Part B,Health Care Financing Administration1682.
catastrophic-health program, catastrophic illness, catastrophiccare, acute care, long-term nursing home care3.
American Association of Retired Persons, AARP, senior citizen,National Committee to Preserve Social Security and Medicare</top>If the phrases are ignored altogether, 2 this query willproduce an output where the relevant documents arescattered as shown in the first table below which liststhe ranks and scores of relevant documents within thetop 100 retrieved ocuments.
On the other hand, if weinclude even simple phrases, such as catastrophic-health program, acute care, home care, and seniorcitizen, we can considerably sharpen the outcome ofthe search as seen in the second table)QUERY:104; NO.
RELEVANT:21REL DOCUMENT RANK (no phrases) RANK (phrases)WSJ890918-0173 2 5WSJ891004-0119 7 1WSJ870723-(K)64 8 8WSJ870213 -(X)53 10 12WsJgg0608-0121 14 7WSJ891005 -0005 15 4WSJ891009 -0009 35 18WSJ890920-0115 39 26WSJ890928-0184 40 61WSJ880609-0(~l 53 50WSJ891009-0188 73 46WSJ880705-OI94 97 95WSJ870601-0075 52WSJ891005-0001 72WSJ871028-0059 93A query obtained from the fields <rifle>, <desc>and <narr> will be, as may be expected, much weakerthan the one using <con> field, especially without hephrasal terms, because the narrative contains far fewerspecific terms while containing some that may provedistracting, e.g., firestorm.
In fact, Broglio and Croft(1993), and Broglio (personal communication, 1993)showed that the exclusion of the <con> field makes thequeries quite ineffective, while adding the <narr> fieldmakes them even worse as they lose precision by asmuch as 30%.
However, adding phrasal terms canimprove things considerably.
We return to this issuelater in the paper.An accurate syntactic analysis is an essentialprerequisite for selection of phrasal terms.
Various sta-tistical methods, e.g., based on word co-occurrences2 All single words (except he stopwords uch as articles orprepositions) am included in the query, including those making up thephrases.s Including extra terms in documents changes the way otherterms are weighted.
This issue is discussed later in this paper.and mutual information, as well as partial parsing tech-niques, are prone to high error rates (sometimes as highas 50%), turning out many unwanted associations.Therefore a good, fast parser is necessary, but it is byno means ufficienL While syntactic phrases are oftenbetter indicators of content than 'statistical phrases'where words are grouped solely on the basis of physi-cal proximity, e.g., "college junior" is not the same as"junior college" - -  the creation of compound termsmakes the term matching process more complex sincein addition to the usual problems of synonymy and sub-sumption, one must deal with their structure (e.g., "col-lege junior" is the same as "junior in college").For all kinds of terms that can be assigned to therepresentation of a document, e.g., words, syntacticphrases, fixed phrases, and proper names, various lev-els of "regularization" are needed to assure that syn-tactic or lexical variations of input do not obscureunderlying semantic uniformity.
Without actuallydoing semantic analysis, this kind of normalization canbe achieved through the following processes: 4(1) morphological stemming: e.g., retrieving isreduced to retriev;(2) lexicon-based word normalization: e.g.,retrieval is reduced to retrieve;(3) operator-argument representation of phrases:e.g., information retrieval, retrieving of infor-mation, and retrieve relevant information areall assigned the same representation,retrieve+information;(4) context-based term clustering into synonymyclasses and subsumption hierarchies: e.g., take-over is a kind of acquisition (in business), andFortran is a programming language.Introduction of compound terms complicates thetask of discovery of various semantic relationshipsamong them.
For example, the term natural languagecan often be considered to subsume any term denotinga specific human language, such as English.
Therefore,a query containing the former may be expected toretrieve documents containing the latter.
The same canbe said about language and English, unless language isin fact a part of the compound term programminglanguage in which case the association language -Fortran is appropriate.
This is a problem because (a) itis a standard practice to include both simple and com-pound terms in document representation, and (b) termassociations have thus far been computed primarily atword level (including fixed phrases) and therefore care4 An alternative, but less efficient method is to generate allvariants (lexical, syntactic, etc.)
of words~hrases in the queries(Sparck-Jones & Tait, 1984).169must be taken when such associations are used in termmatching.
This may prove particularly troublesome forsystems that attempt term clustering in order to create"meta-terms" to be used in document representation.2 Overall DesignWe have established the general architecture of aNLP-IR system, depicted schematically below, inwhich an advanced NLP module is inserted betweenthe textual input (new documents, user queries) and thedatabase search engine (in our case, NIST's PRISEsystem).
This design has already shown some promisein producing a better performance than the base statisti-cal system (Strzalkowski, 1993b).
We would like topoint out at the outset hat this system is completelyautomated, including the statistical core, and the naturallanguage processing components, and no human inter-vention or manual encoding is required.NIP: TAGGER PARSER termsIn our system the database text is first processedwith a sequence of programs that include a part-of-speech tagger, a lexicon-based morphological stemmerand a fast syntactic parser.
Subsequently certain typesof phrases are extracted from the parse trees and usedas compound indexing terms in addition to single-wordterms.
The extracted phrases are statistically analyzedas syntactic ontexts in order to discover a variety ofsimilarity links between smaller subphrases and wordsoccurring in them.
A further filtering process mapsthese similarity links onto semantic relations (generali-zation, specialization, synonymy, etc.)
after which theyare used to transform a user's request into a searchquery.The user's natural language request is alsoparsed, and all indexing terms occurring in it areidentified.
Certain highly ambiguous, usually single-word terms may be dropped, provided that they alsooccur as elements in some compound terms.
For exam-ple, "natural" may be deleted from a query already con-taining "natural language" because "natural" occurs inmany unrelated contexts: "natural number", "naturallogarithm", "natural approach", etc.
At the same time,other terms may be added, namely those which arelinked to some query term through admissible similar-ity relations.
For example, "unlawful activity" is addedto a query (TREC topic 055) containing the compoundterm "illegal activity" via a synonymy link between"illegal" and "unlawful".One of the observations made during the courseof TREC-2 was to note that removing low-qualityterms from the queries is at least as important (andoften more so) as adding synonyms and specializations.In some instances (e.g., routing runs) low-quality termshad to be removed (or inhibited) before similar termscould be added to the query or else the effect of queryexpansion was all but drowned out by the increasednoise.After the final query is constructed, the databasesearch follows, and a ranked list of documents isreturned.
It should be noted that all the processingsteps, those performed by the backbone system, andthose performed by the natural language processingcomponents, are fully automated, and no human inter-vention or manual encoding is required.3 Fast Parsing with TTP ParserT/'P (Tagged Text Parser) is a full-grammarparser based on the Linguistic String Grammardeveloped by Sager (1981).
It currently encompassesmost of the grammar productions and many of the res-trictions, but it is by no means complete.
Unlike a con-ventional parser, TYP's output is a regularizedrepresentation f each sentence which reflects its logi-cal predicate-argument structure, e.g., logical subjectand logical objects are identified epending upon themain verb subcategorization frame.
For example, theverb abide has, among others, a subcategorizationframe in which the object is a prepositional phrase withby, as in he'll abide by the court's decision, i.e.,ABIDE: subject NP object PREP by NPSubcategorization information is read from the on-lineOxford Advanced Learner's Dictionary (OALD) whichTI~P uses.Also unlike a conventional parser, TTP isequipped with a powerful skip-and-fit recoverymechanism that allows it to operate ffectively in theface of ill-formed input or under severe time pressure.A built-in timer regulates the amount of time allowedfor parsing any one sentence: if a parse is not returnedbefore the allotted time elapses, TTP enters the skip-ping mode in which it will try to "fit" the parse.
Whilein the skip-and-fit mode, the parser attempts to forciblyreduce incomplete constituents, possibly skimmingover portions of input in order to restart processing at anext unattempted constituent; in other words, it willfavor reduction over backtracking.
The result of thisstrategy is an approximate parse, partially fitted usingtop-down predictions.
In runs with approximately 130million words of TREC's Wall Street Journal and SanJose Mercury texts, the parser's speed averaged 30minutes per Megabyte or about 80 words per second,on a Sun SparcStationl0.
In addition, T IP  has beenshown to produce parse structures which are no worse170than those generated by full-scale linguistic parserswhen compared to hand-coded parse trees.
5Full details of TTP parser have been described inthe TREC-1 report (Strzalkowski, 1993a), as well as inother works (Strzalkowski, 1992; Strzalkowski &Scheyen, 1993).As may be expected, the skip-and-fit strategywill only be effective if the input skipping can be per-formed with a degree of determinism.
This means thatmost of the lexical level ambiguity must be removedfrom the input text, prior to parsing.
We achieve thisusing a stochastic parts of speech tagger to preprocessthe text prior to parsing.
In order to streamline the pro-cessing, we also perform morphological normalizationof words on the tagged text, before parsing.
This is pos-sible because the part-of-speech tags retain the infor-mation about each word's original form.
Thus the sen-tence The Soviets have been notified is transformed intothe/dt soviet/nps have/vbp be/vbn notify/vbn beforeparsing commences.
64 Head-Mod i f ie r  S t ruc turesSyntactic phrases extracted from T IP  parsestructures are represented as head-modifier pairs.
Thehead in such a pair is a central element of a phrase(main verb, main noun, etc.
), while the modifier is oneof the adjuncts or arguments of the head.
In the TRECexperiments reported here we extracted head-modifierword pairs only, i.e., nested pairs were not used eventhough this was warranted by the size of the database.Figure 1 shows all stages of the initial linguisticanalysis of a sample sentence from the WSJ database.The reader may note that the parser's output is apredicate-argument structure centered around the mainelements of various phrases.
For example, BE is themain predicate (modified by HAVE) with 2 arguments(subject, object) and 2 adjuncts (adv, sub_ord).INVADE is the predicate in the subordinate clause with2 arguments (subject, object).
The subject of BE is anoun phrase with PRESIDENT as the head element,two modifiers (FORMER, SOVIET) and a determiner(THE).
From this structure, we extract head-modifierpairs that become candidates for compound terms.
Ingeneral, the following types of pairs are considered: (1)a head noun of a noun phrase and its left adjective ornoun adjunct, (2) a head noun and the head of its rightadjunct, (3) the main verb of a clause and the head ofits object phrase, and (4) the head of the subject phrases Hand-coded parse trees were obtained from the University ofPennsylvania Treebank Project database.s The tags are read as follows: dt is determiner, nps is a propername, vbp is a tensed plural verb, vbn is a past participle.and the main verb.
These types of pairs account formost of the syntactic variants for relating two words (orsimple phrases) into pairs carrying compatible semanticcontent.
For example, the pair retrieve+informationwill be extracted from any of the following fragments:information retrieval system; retrieval of informationfrom databases; and information that can be retrievedby a user-controlled interactive search process.
Wealso attempted to identify and remove any terms whichwere explicitly negated in order to prevent matchesagainst their positive counterparts, either in the data-base or in the queries.One difficulty in obtaining head-modifier pairs ofhighest accuracy is the notorious ambiguity of nominalcompounds.
The pair extractor looks at the distributionstatistics of the compound terms to decide whether theassociation between any two words (nouns and adjec-fives) in a noun phrase is both syntactically valid andsemantically significant.
For example, we may acceptlanguage+natural and processing+language fromINPUT SENTENCEThe former Soviet president has been a local hero ever since a Rus-sian tank invaded Wisconsin.TAGGED SENTENCEThe~dr former/jj Soviet/./)" president/nn has/vbz beenlvbn aldt localljjherolnn ever/rb since/in a/dt Russian/jj tanklnn invaded/vbdWisconsin/np ./ erTAGGED & STEMMED SENTENCEthe~d1 former/jj soviet/jj president/nn have/vbz belvb, a/dt local/jjhero/nn ever/rb since~in a/dt russian/j~ tanklnn invadelvbdwiscensin/np ./ erPARSED SENTENCE\[assert\[\[perf \]HAVE\]I l\[verb \[BEI\]\[subject \[np \[n PRESIDENT\] [t_.pos THEI\[adj IFORMERll \[adj \[SOVIETI\]\]\]\[object \[np \[n HERO\] \[t..pos A\] \[adj \[LOCAL\]\]\]\]\[adv EVER\]\[sub_ord \[SINCE\[\[verb \[INVADE\]\]\[subject \[np \[n TANK\] \[t_.pos AI\[adj \[RUSSIAN\]l\]\]\[object \[np \[name \[WISCONSIN\]\]\]\]\]\]\]\]\]\]EXTRACTED TERMS & WEIGHTSpresident 2.623519 soviet 5.416102president+soviet 11.556747 president+former 14.594883hero 7.896426 hero+local 14.314775invade 8.435012 tank 6.848128tank+invade 17.402237 tank+mssian 16.030809russian 7.383342 wisconsin 7.785689Figure 1.
Stages of sentence processing.171natural language processing as correct, however,case+trading would make a mediocre term whenextracted from insider trading case.
On the other hand,it is important to extract rading+insider to be able tomatch documents containing phrases insider tradingsanctions act or insider trading activity.5 Term Weighting IssuesFinding a proper term weighting scheme is criti-cal in term-based retrieval since the rank of a documentis determined by the weights of the terms it shares withthe query.
One popular term weighting scheme, knownas ffidf, weights terms proportionately to their inverteddocument frequency scores and to their in-documentfrequencies (tO.
The in-document frequency factor isusually normalized by the document length, that is, it ismore significant for a term to occur in a short 100-wordabstract, han in a 5000-word article.
7A standard ff.idf weighting scheme (see Buckley,1993 for details) may be inappropriate for mixed termsets, consisting of ordinary concepts, proper names,and phrases, because:(1) It favors terms that occur fairly frequently in adocument, which supports only general-typequeries (e.g., "all you know about X").
Suchqueries were not typical in TREC.
(2) It attaches low weights to infrequent, highlyspecific terms, such as names and phrases,whose only occurrences in a document areoften decisive for relevance.
Note that suchterms cannot be reliably distinguished usingtheir distribution in the database as the sole fac-tor, and therefore syntactic and lexical informa-tion is required.
(3) It does not address the problem of inter-termdependencies arising when phrasal terms andtheir component single-word terms are allincluded in a document representation, i.e.,launch+satellite and satellite are not indepen-dent, and it is unclear whether they should becounted as two terms.In our post-TREC-2 experiments we considered(1) and (2) only.
We changed the weighting scheme sothat the phrases (but not the names, which we did notdistinguish in TREC-2) were more heavily weighted bytheir idf scores while the in-document frequency scoreswere replaced by logarithms multiplied by sufficientlylarge constants.
In addition, the top N highest-idfmatching terms (simple or compound) were counted7 This is not always true, for example when all occurrences of aterm are concentrated in a single section or a paragraph rather thanspread around the article.more toward the document score than the remainingterms.Schematically, these new weights for phrasal andhighly specific terms are obtained using the followingformula, while weights for most of the single-wordterms remain unchanged:weight (Ti)=( C1 *log (0c)+C 2* ot(N,i) *idfIn the above, ~t(N,i) is 1 for i <N and is 0 otherwise.The selection of a weighting formula was partly con-strained by the fact that document-length-normalized tfweights were precomputed at the indexing stage andcould not be altered without re-indexing of the entiredatabase.
The intuitive interpretation f the oL(N,i) fac-tor is as follows.
We restrict he maximum number ofterms on which a query is permitted to match a docu-ment to N highest weight terms, where N can be thesame for all queries or may vary from one query toanother.
Note that this is not the same as simply takingthe N top terms from each query.
Rather, for eachdocument for which there are M matching terms withthe query, only min(M,N) of them, namely those whichhave highest weights, will be considered when comput-ing the document score.
Moreover, only the globalimportance weights for terms are considered (such asidf), while local in-document frequency (eg., tO issuppressed by either taking a log or replacing it with aconstant.Changing the weighting scheme for compoundterms, along with other minor improvements ( uch asexpanding the stopword list for topics, or correcting afew parsing bugs) has lead to an overall increase ofprecision of more than 20% over our official TREC-2ad-hoc results.
Table 1 includes statistics of these newruns for 50 queries (numbered 101-150) against theWSJ database.
The gap between the precision levels incolumns txt2 and con reflects the difference in the qual-ity of the queries obtained from the narrative parts ofthe topics (txt2 = title + desc + narr), and thoseobtained primarily from expert's formulation (title +desc + con).
The column txt2+nlp represents theimprovement of txt2 queries thanks to NLP, with asmuch as 70% of the gap closed.
Similar improvementshave been obtained for other sets of queries.6 ConclusionsWe presented in some detail our natural languageinformation retrieval system consisting of an advancedNLP module and a 'pure' statistical core engine.
Whilemany problems remain to be resolved, including thequestion of adequacy of term-based representation fdocument content, we attempted to demonstrate thatthe architecture described here is nonetheless viable.
Inparticular, we demonstrated that natural language172Run txtl txt2 txt2+nlp con con+nipTot number of docs over all queriesRe/ 3929 3929 3929 3929 3929RelRet 2736 3025 3108 3332 3401%chg +9.0 +14.7 +21.8 +24.3Recall (interp) Precision Averages0.00 0.6874 0.7318 0.7201 0.7469 0.80630.10 0.4677 0.5293 0.5239 0.5726 0.61980.20 0.3785 0.4532 0.4751 0.4970 0.55660.30 0.3060 0.3707 0.4122 0.4193 0.47860.40 0.2675 0.3276 0.3541 0.3747 0.42570.50 0.2211 0.2815 0.3126 0.3271 0.38280.60 0.1765 0.2406 0.2752 0.2783 0.33800.70 0.1313 0.1783 0.2142 0.2267 0.28170.80 0.0828 0.1337 0.1605 0.1670 0.21640.90 0.0451 0.0818 0.1014 0.0959 0.14711.00 0.0094 0.0159 0.0194 0.0168 0.0474Average precision over all rel docsAvg 0.2309 \]0.2835 0.3070 0.3210 0.3759%chg \[ +22.8 +33.0 +39.0 +62.8Precision at N documents5 0.5000 0.5240 0.5200 0.5600 0.604010 0.4080 0.4600 0.4900 0.5020 0.5580100 0.2380 0.2790 0.2914 0.3084 0.3346R-Precision (after Rel)Exact 0.2671 0.3053 0.3332 0.3455 0.3950%chg +14.3 +24.7 +29.3 +47.9Table 1.
Run statistics for 50 ad-hoc queries against WSJ databasewith 1000 does retrieved per query: (1) txtl - single terms of <narr>and <desc> fields m this is the base ran; (2) txt2 - <hart> and <desc>fields with low weight erms removed; (3) txt2+nlp -<narr> and<desc> fields including syntactic phrase terms using the new weight-ing scheme; (4) con - <desc> and <con> fields with low weight termsremoved but with no NLP; and (5) con+nip - <dese> and <con>fields including phrases with the new weighting scheme.processing can now be done on a fairly large scale andthat its speed and robustness can match those of tradi-tional statistical programs uch as key-word indexingor statistical phrase extraction.
We suggest, with somecaution until more experiments are run, that naturallanguage processing can be very effective in creatingappropriate search queries out of a user's initialspecifications, which can be frequently imprecise orvague.AcknowledgementsThe author would like to thank Donna Harman ofNIST for making her PRISE system available for thisresearch.
We would also like to thank RalphWeischedel and Constantine Papageorgiou of BBN forproviding and assisting in the use of the part of speechtagger.
This paper is based upon work supported bythe Advanced Research Projects Agency under Con-tract N00014-90-J-1851 from the Office of NavalResearch, under Contract N00600-88-D-3717 fromPRC Inc., under ARPA's Tipster Phase-2 Contract 94-FI57900-000, and the National Science Foundationunder Grant IRI-93-02615.ReferencesBroglio, John and W. Bruce Croft.
1993.
"Query Pro-cessing for Retrieval from Large Text Bases.
"Human Language Technology, Proceedings of theworkshop, Princeton, NJ.
Morgan-Kaufmann, pp.353-357.Buckley, Chris.
1993.
"The Importance of ProperWeighting Methods."
Human Language Technol-ogy, Proceedings of the workshop, Princeton, NJ.Morgan-Kaufmann, pp.
349-352.Harman, Donna (ed.).
1993.
First Text REtr ievalConference.
NIST special publication 500-207.Sager, Naomi.
1981.
Natural  Language InformationProcessing.
Addison-Wesley.Sparck Jones, K. and J. I. Tait.
1984.
"Automaticsearch term variant generation."
Journal  o f  Docu-mentation, 40(1), pp.
50-66.Strzalkowski, Tomek.
1992.
"T IP :  A Fast and RobustParser for Natural Language."
Proceedings of the14th International Conference on ComputationalLinguistics (COLING), Nantes, France, July 1992.pp.
198-204.Strzalkowski, Tomek.
1993a.
"Natural Language Pro-cessing in Large-Scale Text Retrieval Tasks.
"Proceedings of the First Text REtrieval Conference(TREC-1), NIST Special Publication 500-207, pp.173-187.Strzalkowski, Tomek.
1993b.
"Robust Text Processingin Automated Information Retrieval."
Proc.
ofACL-sponsored workshop on Very Large Corpora.Ohio State Univ.
Columbus, June 22.Strzalkowski, Tomek, and Peter Scheyen.
1993.
"Evaluation of TTP Parser: a preliminary report.
"Proceedings of International Workshop on ParsingTechnologies (IWPT-93), Tilburg, Netherlands andDurbuy, Belgium, pp.
293-308.173
