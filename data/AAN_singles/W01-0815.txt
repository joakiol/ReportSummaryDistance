Evaluating text quality: judging output texts without a clear sourceAbstractWe consider how far two attributes oftext quality commonly used in MTevaluation ?
intelligibility and fidelity ?apply within NLG.
While the formerappears to transfer directly, the latterneeds to be completely re-interpreted.We make a crucial distinction betweenthe needs of symbolic authors andthose of end-readers.
We describe aform of textual feedback, based on acontrolled language used for specifyingsoftware requirements that appears wellsuited to authors?
needs, and anapproach for incrementally improvingthe fidelity of this feedback text to thecontent model.1 IntroductionProbably the most critical questions that need tobe addressed when evaluating automaticallygenerated texts are: does the text actually saywhat it?s supposed to say and is it fluent,coherent, clear and grammatical?
The answers tothese questions say something important abouthow good the target texts are and ?
perhapsmore to the point ?
how good the system thatgenerated them is.
There is no a priori reasonwhy the target texts should be any better orworse when they result from natural languagegeneration (NLG) or from machine translation(MT): indeed, they could result from the samelanguage generator.
Given this, it may be naturalto assume that NLG could appropriately adoptevaluation methods developed for its moremature sister, MT.
However, while this holdstrue for issues related to intelligibility (thesecond critical question), it does not apply asreadily to issues of fidelity (the first question).We go beyond our recent experience ofevaluating the AGILE system for producingmultilingual versions of software user manuals(Hartley, Scott et al, 2000; Kruijff et al, 2000)and raise some open questions about how best toevaluate the faithfulness of an output text withrespect to its input specification.2 Evaluating intelligibilityThe use of rating scales to assess theintelligibility of MT output has been widespreadsince the early days in the field.
Typically,monolingual raters assign a score to eachsentence in the output text.
However, this doesnot amount to an agreed methodology, since thenumber of points on the scale and theirdefinition have varied considerably.
Forexample, Carroll (1966) used a nine-point scalewhere point 1 was defined as ?hopelesslyunintelligible?
and point 9 as ?perfectly clearand intelligible?
; Nagao and colleagues (Nagaoet al, 1985), in contrast, used a five-point scale,while Arnold and his colleagues (Arnold et al,1994) suggest a four-point discrimination.
Inevaluating the intelligibility of the AGILE output,we asked professional translators and authorswho were native speakers of the languagesconcerned?Bulgarian, Czech and Russian?toscore individual text fragments on a four-pointscale.
The evaluators were also asked to give asummative assessment of the output?s suitabilityas the first draft of a manual.In a single pass, AGILE is capable ofgenerating several types of text, eachAnthony Hartley and Donia ScottInformation Technology Research Institute,University of BrightonUK{firstname.lastname}@itri.bton.ac.ukconstituting a section of a typical software usermanual?i.e., overview, short instructions, fullinstructions, and functional descriptions?andappearing in one of two styles (personal/director impersonal/indirect).
We evaluated all ofthese text types using the same method.
Theintelligibility evaluation was complemented byan assessment of the grammaticality of theoutput, conducted by independent nativespeakers trained in linguistics.
Following anapproach widely used in MT (e.g., Lehrbergerand Bourbeau, 1987), the judges referred to alist of error categories for their annotations.3 Evaluating fidelityIn MT, evaluating fidelity (or ?accuracy?
)entails a judgment about the extent to which twotexts ?say the same thing?.
Usually, the twotexts in question are the source (i.e., original)text and the (machine-)translated text and thejudges are expert translators who are againinvited to rate the relative information content ofpairs of sentences on an anchored scale (e.g.,Nagao et al, 1985).
But others (e.g., Caroll,1966) have also compared the informativenessof the machine translation and a humantranslation deemed to serve as a benchmark.Interestingly, both of these researchers found ahigh correlation between the intelligibilityevaluations and the fidelity evaluations, whichsuggests that it may be possible to infer fidelityfrom the (less costly) evaluation ofintelligibility.
However, at the current state-of-the-art this approach does not guarantee todetect cases where the translation is perfectlyfluent but also quite wrong.For NLG, the story is rather different.Lacking a source text, we are denied therelatively straightforward approach of detectingdiscrepancies between artifacts of the same type:texts.
The question is, instead, whether thegenerated text ?says the same thing?
as themessage ?
i.e., the model of the intendedsemantic content together with the pragmaticforce of the utterance.The message is clearly only availablethrough an external representation.
In translationgenerally, this external representation is thesource text and the task is commonlycharacterized as identifying the message ?which originates in the writer?s mental model ?in order to re-express it in the target language.
Inan NLG system, the one external representationthat is commonly available is the particulardomain model that serves as input to thegeneration system.
This model may have beenprovided directly by an artificial agent, such asan expert system.
Alternatively, it may havebeen constructed by a human agent as theintended instantiation of their mental model.Yet, whatever its origins, directly comparingthis intermediate representation to the outputtext is problematic.A recent survey of complete NLG systems(Cahill et al, 1999) found that half of the 18systems examined accepted input directly fromanother system1.
A typical example is theCaption Generation System (Mittal et al, 1998),which produces paragraph-sized captions toaccompany the complex graphics generated bySAGE (Roth et al, 1994).
The input to generationincludes definitions of the graphical constituentsthat are used to by SAGE to convey information:?spaces (e.g., charts, maps, tables), graphemes(e.g., labels, marks, bars), their properties (e.g.,color, shape) and encoders?the frames ofreference that enable their properties to beinterpreted/translated back to data values (e.g.,axes, graphical keys).
?2 For obvious reasons,this does not readily lend itself to directcomparison with the generated text caption.In the remaining half of the systemscovered, the domain model is constructed by theuser (usually a domain expert) through atechnique that has come to be known assymbolic authoring: the ?author?
uses aspecially-built knowledge editor to construct thesymbolic source of the target text.
These editorsare interfaces that allow authors to build thedomain model using a representation that ismore ?natural?
to them than the artificiallanguage of the knowledge base.3 The purposeof these representations is to provide feedbackintended to make the content of the domainmodel more available to casual inspection thanthe knowledge representation language of the1 By complete systems, we refer to systems that determineboth ?what to say?
and ?how to say it?, taking as input aspecification that is not a hand-crafted simulation of someintermediate representation.2 Mittal et al, 1998, pg.
438.3 See Scott, Power and Evans, 1998.domain model.
As such, they are obviouscandidates as the standard against which tomeasure the content of the texts that aregenerated from them.We first consider the case of feedbackpresented in graphical mode, and then the optionof textual feedback, using the WYSIWYMtechnology (Power and Scott, 1998; Scott,Power and Evans, 1998).
We go on to makerecommendations concerning the desirableproperties of the feedback text.4 Graphical representations of contentSymbolic authoring systems typically make useof graphical representations of the content of thedomain model?for example, conceptual graphs(Caldwell and Korelsky, 1994).
Once trained inthe language of the interface, the domainspecialist uses standard text-editing devices suchas menu selection and navigation with a cursor,together with standard text-editing actions (e.g.,select, copy, paste, delete) to create and edit thecontent specification of the text to be generatedin one or several selected languages.The user of AGILE, conceived to be aspecialist in the domain of the particularsoftware for which the manual is required (i.e.,CAD/CAM), models the procedures for how touse the software.
AGILE?s graphical userinterface (Hartley, Power et al, 2000) closelyresembles the interface that was developed foran earlier system, DRAFTER, which generatessoftware manuals in English and French (Pariset al, 1995).
The design of the interfacerepresents the components of the procedures(e.g., goals, methods, preconditions, sub-steps,side-effects) as differently coloured boxes.
Theuser builds a model of the procedures for usingthe software by constructing a series of nestedboxes and assigning labels to them via menusthat enable the selection of concepts from theunderlying domain ontology.4.1 The input specification for the userAs part of our evaluation of AGILE, we asked 18IT professionals4 to construct a number ofpredetermined content models of variousdegrees of complexity and to have the system4 There were six for each of the three Eastern Europeanlanguages; all had some (albeit limited) experience ofCAD/CAM systems and were fluent speakers of English.generate text from them in specified styles intheir native language.
Since the evaluation wasnot conducted in situ with real CAD/CAMsystem designers creating real draft manuals, weneeded to find a way to describe to theevaluators what domain models we wanted themto build.
Among the possible options were togive them a copy of either:?
the desired model as it would appear tothem in the interface (e.g., Figure 1);?
the target text that would be producedfrom the model (e.g., Figure 2);?
a ?pseudo-text?
that described the modelin a form of English that was closer tothe language of the AGILE interface thanto fluent English (e.g., Figure 3).Figure 1: Graphical display of content modelFigure 2: Target textFigure 3: Pseudo-text input specificationWe rejected the first option because itamounted to a task of replication which could beaccomplished successfully even without usershaving any real understanding of the meaning ofDraw a line by specifying its start and end points.To draw a lineSpecify the start point of the line.Specify the end point of the line.the model they were building.
Therefore, itwould shed no light on how users might be ableto build a graphical model externalising theirown mental model.We discarded the second because a text maynot necessarily make any explicit linguisticdistinction between different components of themodel?for example, between a precondition ona method and the first step in a methodconsisting of several steps5.
Thus, in general,target texts may not reflect every distinctionavailable in the underlying domain model(without this necessarily causing any confusionin the mind of the reader).
As a result of suchunderspecification, they are ill-suited to servingas a staring point from which a symbolic authorcould build a formal model.We opted, then, for providing our evaluatorswith a pseudo-text in which there was anexplicit and regular relationship between thecompo seudo-textua of thepseudoF4.2This pof jubetwe5 For exmake sumediumpluck thhours.
?We focused on (a), which was of coursemediated by (c); that is, we focused on the issueof creating an accurate model.
This is an easierissue than that of the fidelity of the output text tothe model (b), while the representations in (d)are too remote from one another to permit usefulcomparison.To measure the correspondence between theactual models and the desired/target models, weadopted the Generation String Accuracy (GSA)metric (Bangalore, Rambow and Whittaker,2000; Bangalore and Rambow, 2000) used inevaluating the output of a NLG system.
Itextends the simple Word Accuracy metricsuggested in the MT literature (Alshawi et al,1998), based on the string edit distance betweensome reference text and the output of thesystem.
As it stands, this metric fails to accountfor some of the special properties of the textgeneration task, which involves ordering wordtokens.
Thus, corrections may involve re-ordering tokens.
In order not to penalise amisplaced constituent twice?as both a deletionand an insertion?the generation accuracymetric treats the deletion (D) of a token fromone location and its insertion (I) at anotherlocation as a single movement (M).
Theremaining deletions, insertions, and substitutions(S) are counted separately.
Generation accuracyis given by the following equation, where R isthe number of (word) tokens in the reference nents of the procedures and their pl expression.
Figure 4 is one-texts used in the evaluation.Draw an arcFirst, start-tool the ARC command.M1.
Using the Windowsoperating system: choose the 3Points option from the Arcflyout on the Draw toolbar.M2.
Using the DOS or UNIXoperating system: igure 4: fragment of a typical pseudo-textEvaluating the fidelity of the outputarticular set-up afforded us the possibilitydging the fidelity of the ?translation?en the following representations:a) desired model and model producedb) model produced and output textc) pseudo-text and model producedd) pseudo-text and the output textample, between: ?To cook a goose: Before starting,re the goose has been plucked.
Put the goose in aoven for 1.5 hours.?
and ?To cook a goose: Firste goose.
Then put it in a medium oven for 1.5text.+++?= RSDIMAccuracyGeneration 1For Bangalore and his colleagues, thereference text is the desired text; it is a goldstandard given a priori by a corpus representingthe target output of the system.
The generationaccuracy of a string from the actual output of thesystem is computed on the basis of the numberof movements, substitutions, deletions andinsertions required to edit the string into thedesired form.In our case, the correspondence wasmeasured between models rather than texts, butwe found the metric ?portable?.
The tokens areno longer textual strings but semantic entities.Although this method provided a usefulquantitative measure of the closeness of the fitof the actual generated text to what wasintended, it is not without problems, some ofchoose the Arc option fromthe  Draw menu.choose 3 Points option.Specify the start point of the arc.which apply irrespective of whether the metric isapplied to texts or to semantic models.
Forexample, it does not capture qualitativedifferences between the generated object and thereference object, that is, it does not distinguishtrivial from serious mistakes.
Thus, representingan action as the first step in a procedure ratherthan as a precondition would have less impacton the end-reader?s ability to follow theinstructions than would representing a goal as aside-effect.65 Textual representations of contentOnce the model they represent becomesmoderately complex, graphical representationsprove to be difficult to interpret and unwieldy tovisualise and manipulate (Kim, 1990; Petre,1995).
WYSIWYM offers an alternative, textualmodality of feedback, which is more intuitiveand natural.
As we will discuss below, there is asense in which, in its current form, the feedbacktext may be too natural.5.1 Current status of  WYSIWYM feedbacktextThe main purpose of the text generated infeedback mode, as currently conceived, is toshow the symbolic author the possibilities forfurther expanding the model under development.As with AGILE?s box representation,clicking on a coloured ?anchor?
brings up amenu of legitimate fillers for that particular slotin the content representation.
Instantiating greenanchors is optional, but all red anchors must beinstantiated for a model to be potentiallycomplete (Figure 5).
Once this is the case,authors tend to switch to output mode, whichproduces a natural text reflecting the specifiedmodel and nothing else.Figure 5: fragment of a typical feedback text6 See Hartley et al(2000) for further discussion of thisissue and the results of the AGILE evaluation.In WYSIWYM systems the same generatoris used to produce both the feedback and outputtexts; this means that the feedback text can be asfluent as the output text.
In its currentinstantiations, this is precisely what is produced,even when the generator is capable of producingtexts of rather different styles for the differentpurposes.75.2 Feedback in a controlled languageThe motivation for generating a new type offeedback text comes from two sources.The first is the pseudo-texts that weconstructed by hand for the AGILE evaluation.As far as the form of the models actuallyconstructed is concerned, they provedconsistently reliable guides for the symbolicauthors.
Where they proved inadequate was intheir identification of multiple references to thesame domain model entity; several authorstended to create multiple instances of an entityrather than multiple pointers to a single instance.Let us now turn from the testing scenario, whereauthors have a defined target to hit, and considerinstead a production setting where the author isseeking to record a mental model.
It is a simplematter to have the system generate a secondfeedback text, complementing the present one,this time in the style of the pseudo-texts8 for thepurpose of describing unambiguously, ifrebarbatively, the state of a potentially completemodel.The second is Attempto Controlled English(ACE: Fuchs and Schwitter, 1996; Fuchs,Schwertel and Schwitter, 1999), which allowsdomain specialists to interactively formulatesoftware requirements specifications.
Thespecialists are required to learn a number ofcompositional rules which they must then applywhen writing their specifications.
These areparsed by the system.For all sentences that it accepts, the systemcreates a paraphrase (Figure 6) that indicates itsinterpretations by means of brackets.
Theseinterpretations concern phenomena likeanaphoric reference, conjunction anddisjunction, attachment of prepositional phrases,relative clauses and quantifier scope.
The user7 As, for example, in the ICONOCLAST system (seehttp://www.itri.bton.ac.uk/projects/iconoclast).8 Modulo the reference problems, for which a solution isindicated below1.
Do <red>this action</red> by using<green>this method</green>.2.
Schedule <red>this event</red> byusing <green>this method</green>.3.
Schedule the appointment by using<green>this method</green>.either accepts the interpretation or rephrases theinput to change it.Figure 6: ACE paraphrasesThe principle of making interpretationsexplicit appears to be good one in the NLGcontext too, especially for the personconstructing the domain model.
Moreover, inthe context where the output text is required tobe in a controlled language, the use ofWYSIWYM relieves the symbolic author of theburden of learning the specialized writing rulesof the given control language.Optimising the formulation of the controlledlanguage feedback is matter of iterativelyrevising it via the testing scenario, using GSA asthe metric, until authors consistently achievetotal fidelity of the models they construct withthe reference models.6 ConclusionsSo how can go about judging whether theproducts of NLG systems express the intendedmessage?
A first step towards this goal is toenable symbolic authors to satisfy themselvesthat they have built the domain model they hadin mind.
Graphical feedback is too difficult tointerpret, while natural language output that isoptimised for the end-reader may not show theunequivocal fidelity to the domain model thatthe symbolic author requires.We have suggested that textual feedback ina form close to a controlled language used forspecifying software requirements is a goodcandidate for this task.
We have further outlineda method for incrementally refining thiscontrolled language by monitoring symbolicauthors?
ability to construct reference domainmodels on the basis of controlled languagefeedback.
The trade-off between transparencyand naturalness in the output text intended forthe end-reader will involve design decisionsbased on, among other things, reader profiling.Assessing the fidelity of the end-reader text tothe model is also a necessary step, but not onethat can be conflated with or precede that ofvalidating the accuracy of the model withrespect to the author?s intentions.AcknowledgementsThe work described in the paper has beensupported by EC INCO-COPERNICUS projectPL961104 AGILE ?Automatic generation ofInstructions in Languages of Eastern Europe?.The authors express their gratitude to all thepartners of, and participants in, the AGILEproject, upon whose work this paper reports.ReferencesAlshawi, H., Bangalore, S. and Douglas, S. (1998).Automatic acquisition of hierarchical transductionmodels for machine translation.
Proceedings of the36th Annual Meeting of the Association forComputational Linguistics and the 17thInternational Conference on ComputationalLinguistics (COLING-ACL?98), Montreal,Canada, pp.
41 ?
47Arnold, D., Balkan, L., Lee Humphreys, R., Meijer,S.
and Sadler, L. (1994).
Machine translation: anintroductory guide.
Blackwell.Bangalore, S. and Rambow, O.
(2000).
Exploiting aHierarchical Model for Generation.
Proceedings ofthe 18th International Conference onComputational Linguistics (COLING?2000),Saarbruecken, Germany, pp.
42 ?
48.Bangalore, S., Rambow, O. and Whittaker, S. (2000).Evaluation Metrics for Generation.
Proceedings ofthe 1st International Conference on NaturalLanguage Generation, Mitzpe Ramon, Israel, pp.
1?
8.Cahill, L., Doran, C., Evans, R., Mellish, C., Paiva,D., Reape, M., Scott, D. and Tipper, N. (1999).
Insearch of a reference architecture for NLGsystems.
Proceedings of the 7th EuropeanWorkshop on Natural Language Generation(EWNLG'99), Toulouse, France, pp 77 ?
85.Caldwell, T. and Korelsky, T. (1994).
Bilingualgeneration of job descriptions from quasi-conceptual forms.
Proceedings of the FourthConference on Applied Natural LanguageProcessing (ANLP?94), pp.
1 ?
6.Input:The customer enters a card and a numeric personalcode.
If it is not valid then SM rejects the card.Paraphrase:The customer enters a card and [the customerenters] a numeric personal code.
If [the personalcode] is not valid then [SimpleMat] rejects the card.Carroll, J.B. (1966).
An experiment in evaluating thequality of translations.
In J.
Pierce.
Language andmachines: computers in translation andlinguistics.
Report by the Automatic LanguageProcessing Advisory Committee (ALPAC).Publication 1416.
National Academy of SciencesNational Research Council, pp.
67 ?
75.Fuchs, N.E.
and Schwitter, R. (1996).
AttemptoControlled English (ACE).
Proceedings of the 1stInternational Workshop on Controlled LanguageApplications (CLAW?96), Leuven, Belgium.Fuchs, N.E., Schwertel, U. and Schwitter, R. (1999).Attempto Controlled English (ACE) LanguageManual Version 3.0, Technical Report 99.03,Department of Computer Science, University ofZurich, August 1999.Hartley, A., Power, R., Scott, D. and Varbanov, S.(2000).
Design specification of the user interfacefor the AGILE final prototype.
Deliverable INTF2of INCO-COPERNICUS project PL961104AGILE: ?Automatic.
Generation of Instructions inLanguages of Eastern Europe?.
Available athttp://www.itri.bton.ac.uk.Hartley, A., Scott, D., Kruijff-Korbayova, I., Sharoff,S.
et al (2000).
Evaluation of the final prototype.Deliverable EVAL2 of INCO-COPERNICUSproject PL961104 AGILE: ?Automatic.
Generationof Instructions in Languages of Eastern Europe?.Available at http://www.itri.bton.ac.uk.Kim, Y.
(1990).
Effects of conceptual data modellingformalisms on user validation and analystmodelling of information requirements.
PhDthesis, University of Minnesota.Kruijff, G-J., Teich, E., Bateman, J., Kruijff-Korbayova, I. et al (2000).
Multilinguality in atext generation system for three Slavic languages.Proceedings of the 18th International Conferenceon Computational Linguistics (COLING?2000),Saarbruecken, Germany, pp.
474 ?
480.Lehrberger, J.
& Bourbeau, L. (1987) Machinetranslation: linguistic characterisitics of MTsystems and general methodology of evaluation.John Benjamins.Mittal, V.O, Moore, J., Carenini, G. and Roth, S.(1998).
Describing Complex Charts in NaturalLanguage: A Caption Generation System.Computational Linguistics, 24(3), pp.
431 ?
468.Nagao, M. Tsujii, J. and Nakamura, J.
(1985).
TheJapanese government project for machinetranslation.
Computational Linguistics, 11(2-3),pp.
91 ?
109.Paris, C., Vander Linden, K., Fischer, M., Hartley,T., Pemberton, L., Power, R. and Scott, D. (1995).A Support Tool for Writing MultilingualInstructions.
Proceedings of the FourteenthInternational Joint Conference in ArtificialIntelligence (IJCAI?95), pp.
1395 ?
1404.Petre, M. (1995).
Why looking isn?t always seeing:readership skills and graphical programming,Communications of the ACM, 38(6), pp.
33 ?
42.Power, R. and Scott, D. (1998) MultilingualAuthoring Using Feedback Texts.
Proceedings ofthe 36th Annual Meeting of the Association forComputational Linguistics and the 17thInternational Conference on ComputationalLinguistics, Montreal, Canada, pp.
1053 ?
1059.Roth, S.F., Kolojejchick, J., Mattis, J. and Goldstein,J.
(1994) Interactive graphics design usingautomatic presentation knowledge.
Proceedingsof CHI?94: Human Factors in ComputingSystems, Boston, M.A.Scott, D.R., Power, R., and Evans, R. (1998)Generation as a Solution to Its Own Problem.Proceedings of the 9th International Workshop onNatural Language Generation (INLG'98), Niagara-on-the-Lake, Canada, pp.
256 ?265.
