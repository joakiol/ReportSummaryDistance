MULTIPLE & SINGLE DOCUMENT SUMMARIZAT IONUSING DR-LINKMary McKennaDr.
Elizabeth Lid@TextWise  LLC2-212 Center  for Science and Techno logySyracuse, NY  13244l iz@textwise.corrg mary@textwise .comPhone: (315) 443-1989Research ProblemOur Tipster Phase III research objective for theSummarization task is to produce a single summaryacross multiple documents returned from a search onan information retrieval system.
An established setof metrics to evaluate the performance of our systemis not available in this field at present, so thisresearch is also developing a procedure to evaluatethe summaries we create.
We hope to uncover usefulmetrics and evaluation variables that can be used byothers working in this area.Automatic text summarization can mean manydifferent things.
A summary may be produced fromthe results of an information retrieval system query,or it may be created independent of any specifiedinformation eed.
A summary may represent a singledocument or a group of documents.
A summary maybe an extract of sentences or sections of text from thesource documents, or it may use only small fragmentsor even none of the actual wording from the sourcedocuments.
A summary may provide a generaloverview of document contents (indicative), or it mayact as a substitute for the actual document(informative).
Any evaluation methodology musttake these variations into account, and clearly specifythe type of summary the system is generating.We have chosen to develop indicative, querydependent summaries for both single and multipledocuments.
We are using metadata, phrases, andsometimes representative paragraphs in our multipledocument summaries.
We have further efined ourfinal evaluation framework by defining twoapplications for our multiple document summaries:query refinement summaries providing a thumbnailsketch of documents returned in response to a query,and topic overviews, supplying amuch more detailedmultiple document summary.TextWise/Tipster Research PlanThis Tipster esearch project began October 15th,1997.
The goal of the research project as originallyplanned was to produce multiple documentsummaries, using the documents returned from aquery using the DR-LINK information retrievalsystem.
As this research project was part of theTipster Phase III Text Summarization project, andTipster did not have a multiple document summaryevaluation track, our research plan was amended inJanuary of 1998.
We agreed to also create singledocument summaries in response to a query, in orderto participate in the formal Tipster evaluation(SUMMAC) in February of 1998.As of September, 1998, we are very close tocompleting this project.
Today a user can run ad hocqueries on the DR-LINK/Tipster SummarizationProject website.
Users can display single documentsumrnaries for any document in a results set.
Formultiple documents ummaries, two options areavailable.
A Thumbnail Sketch (brief summary) ofthe top 30 documents is automatically provided atthe top of the search results.
The user may also selectan option to create Detailed Summaries, specifyingthe number of documents (top-ranked 1-30) to beused in the summary.Test CollectionSUMMAC administrators selected training andtest queries for Tipster Phase III participants o use asa practice data set.
The queries are all from the TRECcollection.
200 of the top ranked documentsassociated with each query were also supplied to allparticipants.
These 200 documents contain bothrelevant and nonrelevant documents in response tothe query.
There were four data sources used in thetraining set: the Wall Street Journal, AssociatedPress, Federal Register (FR), and Department of215Energy (DOE) documents.
At the Tipster meeting inOctober 1997 it was decided that the FR and DOEdocuments were probably inappropriate for thesummarization task at hand.
At TextWise, weconstructed 20 databases using the Tipster-selecteddata for system development.
10 of these databasescontain all the documents from the retrieved sets,both relevant and nonrelevant, with FR and DOEdocuments removed.
The second set of databasescontain only relevant documents to a query, againwith FR and DOE documents removed.
The reasonfor the division is to determine what level of noisethe nonrelevant documents are contributing to thesummaries.We did not use the lengthy narrative descriptionsthat are part of the TREC queries in the practice dataset, as these are not representative of the length ofqueries used on our online service.
We used only theDescription sections of the queries as prepared byTREC.DR-LINK Modules Used in SummariesTo compose multiple and single documentsummaries, we used the output of the DR-LINKsystem \[1\].
DR-LINK is a natural languageinformation retrieval and analysis system whichreturns relevance ranked result sets in response to aquery.
DR-LINK document processing and indexingoutputs comprise the components of the summary.These outputs include: complex nominals, which areselected noun/adjective phrases (information system,running shoos); proper nouns with associatedcategories (Country: India; Company: AnalogDevices); subject fields which are metadata subjectcodes describing documents (InformationTechnology; Electricity/Electronics); and theselection of the most relevant section of a documentin response to a query.Selecting the 'Best' SFCs to Summarize aGroup of DocumentsSubject Field Codes (SFCs) were the firstcomponent of DR-LINK document tagging that weretested for use in summarizing multiple documents.SFCs are assigned when documents are indexedusing the DR-LINK document processor.
These aresubject codes that describe what a document is about.There are about 900 possible Subject Field Codes.An SFC must have a certain value threshold to beused as a descriptor to represent a given document.Example Document with Subject Field CodesNew Agent Orange Lawsuit Filed by VietnamVeteran --- A Wall Street Journal News Roundup.06/23/88 WALL STREET JOURNALA Vietnam veteran and his family filed a class-action suit in state court in Harris County, Texas,against seven chemical companies that manufacturedthe herbicide Agent Orange.The suit seeks more than $15 billion in damagesfor veterans who didn't discover they had beeninjured by the herbicide until after the massive AgentOrange litigation was settled May 7, 1984.The plaintiff Ronald Hartman, served in Vietnamfrom December 1967 to December 1968.
Last March,according to the complaint, Mr. Hartman wasdiagnosed as having lymphoma,  form of cancer thatplaintiffs in the Agent Orange case allege is causedby exposure to dioxin, a byproduct of the herbicide.The suit charges that the defendant chemicalcompanies, Diamond Shamrock Chemicals Co.; DowChemical Co.; Monsanto Co.; Uniroyal lnc.
;Hercules Inc.; Thompson Hayward Chemical Co., aunit of Harrisons & Crosfield PLC; and T.H.Agriculture & Nutrition Co. entered into a "willfulconspiracy in calloused and complete indifference tothe safety of those people who bravely served ourcountry in Vietnam.
"Michael Gordon, a New York lawyer whorepresents Diamond Shamrock, which has since splitinto several entities, dismissed the importance of thesuit and contended those veterans whose symptomsdeveloped after the settlement can also make claimsagainst the 1984 settlement fund, which has grownover the years to more than $200 million.Top 5 Subject Fields:Laws~Court ProceedingsBusiness PracticesUnited StatesChemical SubstancesLegal Decisions/JudgmentsFor our analysis, we listed the SFCs for the top10, 20 and 30 documents in response to a query, andsorted by both frequency and an alphabetical listing.The frequency list was used to present DR-LINK'scandidates for the best SFCs to represent a set ofdocuments.
The alphabetical list was used to providethe analysts with a list to choose the best SFCs tosummarize the document set.
We could have giventhe analysts the entire SFC list to choose from, butthat would have been problematic on two counts: 1)the list is too long to make the selection a reasonabletask, and more importantly, 2) the system has already216selected a certain number of SFCs to represent thedocument sets, and our intention is not to rewrite theSFC module, but to determine if this module ishelpfiil in summarizing a set of documents.The senior researcher first performed the SFCcoding task on a set of 30 documents to anticipateproblems and provide written directions to theanalysts.
Three analysts participated in the task ofchoosing the 'best' SFCs to represent a group ofdocuments.
The analysts were asked to select thebest SFCs to summarize the top 10 documents, thetop 20 documents, and the top 30 documents.Analysts were not provided with the query; documentSFCs are generated independent of a query.Analysts were given 30 full text documents for eachof 8 topics selected from the test collection: 125 (A),158 (B), 163 (C), 183 (D), 127 (E), 162 (F), 198 (G),200 (H).
In queries A-F, the top 30 documentscontained both relevant and non-relevant documents.In queries G and H, the top 30 documents were alljudged to be relevant o the query.
While the n isadmittedly very small here, relevant documents onlyfor G and H were used in order to determine ifanalysts would find it easier to agree on SFCs for arelevant set of documents as opposed to relevant andnon-relevant document sets.Analysts were provided with lists of SFCcandidates for the top 10, 20 and 30 documents,generated from the Tipster DR-LINK developmentsite.
The lists contained all the displayed SFCs usedin a given set \[I-I0\], \[1-20\], \[1-30\] of documents.SFCs were presented to the analysts in alphabeticalorder.
The length of these lists varied.
For the eighttopics used:Range of possible SFCs candidates for document setcontaining 10 documents: 10-16Range of possible SFCs candidates for document setcontaining 20 documents: 14-26Range of possible SFCs candidates for document setcontaining 30 documents: 17-31Analysts were asked to choose from zero to tenSFCs that best represented a given group ofdocuments.
The limit of ten was imposed becausethe list was to be part of an indicative, notinformative, summary.
The analysts were asked torank order these SFCs, although the rank orderexercise was meant to provide possibly usefialadditional information, rather than being central tothe effort.
The purpose of this task was to see if thehumans 1) agreed on what SFCs defined a set ofdocuments, and 2) were the human selections imilarto the fi~equency-ranked DR-LINK selections?lntercoder Reliability TestingThe data from this exercise were analyzed usingSPSS.
The first test was a pairwise comparison ofthe analysts - did they choose similar SFCs torepresent a given set of documents?
The Kappastatistic \[2\] was used for this test.
It is important tonote that these are clearly somewhat subjectivejudgments on the part of the coders.
We wanted touncover the consistency of these judgments.
To dothis, we had to take into account he probability thatagreements would happen by chance.
That is whythe Kappa statistic was used to judge intercoderreliability.All Kappa results in this report were statisticallysignificant - p < 0.01Average Kappa value for selecting the SFCs that bestsummarize the top 10 documents:Pairwise comparisons between humans 1, 2, & 3, andthen averaged:n=125 .49 .63 .48 Avg .53Average Kappa value for selecting the SFCs that bestsummarize the top 20 documents:Pairwise comparisons between humans 1, 2, & 3, andthen averaged:n=165 .56 .59.57Avg .57Average Kappa value for selecting the SFCs that bestsummarize the top 30 documents:Pairwise comparisons between humans 1, 2, & 3, andthen averaged:n=195.59.61.57Avg .59To interpret his result, we need to consult theguidelines first presented by Landis and Koch \[3\]0.0-0.20 = slight agreement0.21-0.40 = fair agreement0.41-0.60 = moderate agreement0.61-0.80 = substantial greement0.81-1.00 = almost perfect agreementWe found that our agreements among coders,selecting the best SFCs to summarize document sets,is right on the line between Moderate Agreement andSubstantial Agreement.
There are studies thatsuggest Kappa may be interpreted differentlydepending on the complexity of the coding task - thatis, a lower Kappa may be signaling strong agreementif the task is very complex.
However, as we are notable to judge the 'complexity' of this coding task, wewill be using the Landis and Koch guidelines for ourresults interpretation.
Another way of interpreting217this statistic is that a Kappa of, for example, 0.50,shows that there is a 50% agreement between codersabove what could have occurred by chance \[4\].The levels of agreement did not significantlychange for codes assigned to relevant and non-relevant document databases (A-F) versus relevantdocument only databases (G,H).
However, this maybe due to the small sample size.Our results comparing human coders with thecomputer (DR-LINK generated output) requiredsome data correction to account for the fact that inthe initial coding scheme, the computer was forced toa code of one (i.e., the computer always coded for thepresence of the SFC).
In the corrected ata scheme,the computer output was recoded so that alloccurrences of SFCs with a fi:equency of one werechanged to zero.Average Kappa value for selecting the SFCs that bestsummarize the top 10 documents:Pairwise comparisons between humans 1, 2, & 3, andthe computer, then averaged:n=125 .45 .33 .36 Avg .37Average Kappa value for selecting the SFCs that bestsummarize the top 20 documents:Pairwise comparisons between humans 1, 2, & 3, andthe computer, then averaged:n=165 .29.50.49Avg .43Average Kappa value for selecting the SFCs that bestsummarize the top 30 documents:Pairwise comparisons between humans 1, 2, & 3, andthe computer, then averaged:n=195.28.40.48Avg .39It is clear that the level of agreement between thehuman coders and the computer do not match thelevel of agreement between humans.
However, it isencouraging tonote the levels of agreement betweenhumans and the computer are still highly significant,or to use Landis & Koch's language, a fair tomoderate agreement.
These results reflect how muchagreement was reached among humans, andcomparing human selections with the with theautomatically produced SFC selections.
Theseresults do not demonstrate the extent o which the listof subject field codes, selected by either the humansor the computer module, actually represented the setof documents.From the human perspective, as the task becamemore difficult as more codes were introduced, thelevel of agreement with the computer showed littlechange (a better statistical analysis of this wouldrequire ven larger document sets).Most Relevant Paragraph SelectionFor the next module investigation, we examinedthe DR-LINK selection of the Most Relevant Section(MRS) of a document in response to a query.
DR-LINK processing divides documents into logicalsections.
The section that is most similar to the query,as chosen by a selection algorithm, is presented as theMost Relevant Section of the document to a user.
Inorder to adjust his algorithm to be used for multipledocument summaries, we chose a single paragraphwithin the Most Relevant Section to serve as thesummary text of the document.
We have named thisselection the Most Relevant Paragraph (MRP).
Weselect the Most Relevant Paragraph from the MostRelevant Section by simply using a list of queryterms and a stopword list to determine the mostappropriate paragraph within the MRS.
The MRSalgorithm has already performed most of the work,the MRP algorithm just refines this a step further.We use Most Relevant Paragraphs in both singleand multiple document summaries.
For the multipledocument summaries, we did not want to includerelevant paragraphs that were duplicates or nearduplicates; we wanted to avoid using repetitiousinformation as much as possible.
We removedduplicate and near duplicate paragraphs using a verysimple algorithm that computes imilarities amongsubstrings in the MRPs.
We require only onesubstring overlap to be found in order to declare amatch.
Duplicate paragraphs are noted, although notdisplayed, in the summary.
A link is provided to thefull text of the document containing the duplicateparagraph should a user want to investigate thatdocument (duplicate paragraphs may be from adifferent source or a later/earlier dition of a story, soit is important o retain the link to the duplicateparagraph document.
)The SUMMAC EvaluationAs noted above, our original project goal andfocus was to develop multiple document summaries.However, the Tipster SUMMAC evaluation did notinclude the evaluation of multiple documentsummaries.
Therefore, in order to participate in theSUMMAC evaluation, we briefly diverted all effortsto create single document summaries as required bythe formal evaluation process.218We participated in two of the three tasks for theSUMMAC evaluation: Ad Hoc - produce indicativesummaries to convey what the document is about;Question & Answer - produce informative summariesthat would serve as substitute for the originaldocument.
We did not participate in thecategorization task because the task was queryindependent and our current system is built aroundthe use of a query.We submitted results for 10% fixed length AdHoc summaries (summary could be no more than10% of original document size), 'best' length Ad Hocsummaries, and Question & Answer summaries(limited to 30% of original document size).
Oursubmission for Ad Hoc 'best' and Q&A were bothlimited to 30% of the document size.
Our submissionfor Ad Hoc Best and Q&A were identical.
Weparticipated in the Q&A task just to get an idea howwell our admittedly 'indicative' summaries fare whenbeing judged as 'informative' summaries.
(Theevaluation results made it clear that our indicativesummaries cannot serve as informative summaries!
)For the 10% (Brief) summaries we used the MostRelevant Paragraph, and if we needed to cut off inmid-sentence, an ellipses was used.
If no MRP waschosen, then we defaulted to displaying the leadparagraph.
If still under 10%, we then included thetop 3 Subject Field Codes for that document.
If  stillunder 10%, we used the top 5 complex nominals witha frequency greater than 1.
Finally, if still under10%, we listed as many proper nouns with afrequency greater than 1 from the People, Places, andCompany categories as possible.For the 30% (Best) and Q&A summaries, weincluded lead paragraph in addition to Most RelevantParagraph when these were not identical.
We usedthe top 3 subject filed codes (sorted by frequency), allcomplex nominals with a frequency greater than 1(again sorted by frequency), and all proper nounsfrom the People, Places, and Companies categories,provided length limitations did not prohibit heir use.Finally, we formatted the summaries usingdescriptive labels to explain to the user what iscontained in the summary.
These labels were LeadParagraph:, Most Relevant Paragraph: , SubjectAreas Mentioned:, and Frequent Phrases Mentioned:.Below is an example of summaries submitted fora query 257, used in both the Q&A and the Ad Hoctask.
The query asked about cigarette consumption ithe U.S. (We were not allowed to use titles in anysummaries.
)Example - 10% Adhoc Summary#AP880521-0203Since 1981, total U.S. cigarette consumption hasdropped more than 10 percent, and last yearAmericans consumed 575 billion cigarettes, or"~vieces, "the fewest since 1972.Subject Areas:Smoking~TobaccoCommerce~TradeGovernment PowersCommon Phrases:last yeartobacco support operationstobacco lawmakerstobacco programtrade billExample - 30% Adhoc Summary#AP880521-0203Lead Paragraph: The golden leaf of tobacco isoffering little shade from the heat generated by US.Surgeon General C. Everett Koop ~ declaration thatnicotine in cigarettes i addictive.Most Relevant Section: Since 1981, total U.S.cigarette consumption has dropped more than 10percent, and last year Americans consumed 575billion cigarettes, or "~vieces, "the fewest since 1972.Subject Areas Mentioned :Smoking~TobaccoCommerce~TradeGovernment PowersGovernmental InstitutionsFrequent Phrases Mentioned:last yeartobacco support operationstobacco lawmakerstobacco programtrade billnet outlaysPeople:Verner Grise R-R.1.John Chafee D-Ky.Places:District of ColumbiaNorth CarolinaCompanies: Commodity Credit CORPSUMMAC Evaluation ResultsFor the Ad Hoc task, participants were all veryclose to one another in performance.
TextWise wasthe only participant in the best quadrant for the F-score by Time on the fixed length summaries.
Webelieve this is due to the fact that we were the onlyparticipants that used lists and sentences in our219summaries; all other participants used only sentenceextracts.
Lists can be viewed very quickly.
Since thepoint of summaries i to save time, a well selected listcan save a lot of time.As mentioned above, we did not do well in theQuestion and Answer task by submitting identicalsummaries for both the Ad Hoc task and the Questionand Answer task.
The Question and Answer taskclearly requked more informative summaries, whichour system was not designed to create.
We wouldneed to do development work in this area to actuallyproduce informative summaries.For future development, we will continue todevelop mixed summaries, i.e., using both lists ofproper nouns, phrases, and subject areas, as well assummary sentences.
Improving our precisionselecting lists and sentences to summarize documentswill be the aim of any further development of oursingle document surnmaries.Proper Nouns & Complex Nominals Usedin Multiple Document SummariesAfter completing the SUMMAC evaluation work,we returaed to the creation of multiple documentsummaries.
The next DR-LINK modules elected foruse in the multiple document summaries were propernouns (PNs) and Complex Nominals (CNs)(noun/noun or adjective/noun phrases).
The DR-LINK document processing module tags propernouns and assigns one of 50 descriptive categoriesfor each proper noun in our indexes.
Complexnominals are bracketed as well.For the selection exercise, we ordered PNs andCNs using frequency of occurrence among a givendocument set.
We used only PNs and CNs with afrequency greater than one; using a frequency of oneto summarize a set of documents makes little sense.If a word or phrase is mentioned only once in a set often, twenty, or thirty documents, it is hardlyexemplary of the document set.We provided directions and materials for ourthree analysts, and asked them to choose the mostappropriate PNs and CNs to represent a given set ofdocuments, using our eight query test set.
Analystswere not provided with any lists to choose from.
ThePN and CN selection was done after the analyst readeach set of documents.We then compared the analysts elections.
Therewas very little intercoder consistency among analysts,except for the very top frequently occurring PNs andCNs.
We chose to use only this high frequencygroup of PNs and CNs in our final summaries.Frequency Cutoff figures are noted below.
Theranges below are not absolute; a different query maypresent a higher or lower number of PNs or CNs forany given frequency in a document set.Example ofFrequency Ranges fromCutoff Test QueriesPN rangefor 10 documents: 3 5-32PN rangefor 20 documents: 4 10-38PN range for30 documents: 5 7-42CN range for10 documents: 3 3-24CN range for20 documents: 4 4-37CN range for30 documents: 4 7-47An interesting outcome of this experiment wasthe varying numbers of PNs and CNs each analystused to represent a set of documents, even thoughthey were each presented with an identical set ofdirections and documents.
To smnmarize a set of 20documents, on average, Analyst One used 23 CNsand 25 PNs.
Analyst Two used 18 CNs and 15 PNs.Analyst Three used 6 CNs and 5 PNs.
The analystsbriefly described the task they had in mind when theywere composing their lists, The variance apparentamong the three analysts' interpretations of the word'summary' is not unlike the wide range ofinterpretations of what the word 'summary' may meanto any group of users.As a direct result of this observation, we havedecided to implement two types of multipledocument summaries.
A Thumbnail Sketch,consisting of the five most frequent PNs, CNs, andSFCs, allows for a quick check on the results.
Asecond, more comprehensive summary, the DetailedSummary, uses SFCs with a frequency of 3 orgreater, PNs and CNs with a frequency of 2 orgreater, and the most relevant paragraphs that do notcontain duplicate information.220Final EvaluationThe final evaluation for this project is aqualitative assessment ofthe usefulness of both typesof multiple document summaries.
The finalevaluation is still underway as this paper goes topress.
We are again using three analysts for theevaluation.
They are being asked to assess thefollowing for both the Thumbnail Sketches and theDetailed Summaries: Were the summaries usefulgiven the application description?
Did the summarymake sense?
Did the summary allow the user toaccomplish the information gathering task in a moreefficient manner?
Was there too much repetitiveinformation in the summary?
Were the mostimportant ideas or themes included, while trivialdetails excluded?
We are also soliciting generalfeedback as to each evaluator's opinions of thesummaries - what's missing, what other applicationsare appropriate, what application are unmet, etc.The purpose of the final evaluation is to assessour current system in order to direct future efforts.What can be said about our automatic summaries atthe completion of this project?
What areas needmore development effort?
What new directionsmight be pursued should we continue work in thisarea?ConclusionThis research has produced single documentsurnmaries, and two types of multiple documentsummaries, using the DR-LINK system.
We havediscovered little agreement in the researchcommunity regarding definitions of summaries orthe evaluation of summaries, although the Tipsterproject has certainly brought both issues to the fore.With this research project, we hope to have made auseful contribution to the early body of research onthe creation and evaluation of both single andmultiple document summaries.References\[1\] Liddy, E.D., Pai l  W., Yu, E.S.
& McKenna, M.1994.
Document Retrieval Using LinguisticKnowledge.
In Proceedings of the RIAO 94Conference Proceedings, 106-114.
Paris, France:JOUVE.\[2\] Carletta, Jean.
1996.
Assessing Agreement onClassification Tasks: The Kappa Statistic.Computational Linguistics.
22(2):249-254.\[3\] Landis, J.R. & Koch, G.G.
(1977) Themeasurement of observer agreement for categoricaldata.
Biometrics.
33, 159-174.\[4\] Krippendorff, K. (1980) Content Analysis: AnIntroduction to its methodology.
Sage: Newbury Park221
