BBN HARC and DELPHI  Results on theATIS Benchmarks  - February 1991S.
Austin, D. Ayuso, M. Bates, R. Bobrow, R. Ingria,J.
Makhoul, P. Placeway, and R. Schwartz, D. StallardBBN Systems and Technologies10 Moul ton  StreetCambridge, MA 02138ABSTRACTThis paper presents the test results of running BBN's HARC spokenlanguage system and DELPHI natural language understanding system onthe ATIS benchmarks.We give a brief system overview, and review the major changes that havebeen made in Delphi since the last DARPA SLS workshop.
We willbriefly discuss the development a d training process, and then present ourtest results and an analysis of their meaning.SYSTEM OVERVIEWDelphi is BBN's research NL system, which is based on aunification grarnrnar and which incorporates semantics into theunification framework.
Delphi s the NL component of the BBNHARC (Hear and Respond to Continuous Speech) system;integrated with the BYBLOS speech recognition system using anN-best architecture \[1,2\].Figure 1 shows the relationships among the components ofHARC, and their inputs and outputs.~ SpeechBYBLOSSpeechRecognitionN-bestDelphiNaturalLanguageProcessing  MRLDatabaseTranslatorSQLFigure 1: The BBN HARC SystemRECENT CHANGES IN  DELPHIThe BBN Delphi natural language understanding system whichwas reported in June, 1990 \[3\] has been changed and improved in anumber of ways:1.
The addition of statistical agenda capabilities to the parser.This achieved a considerable r duction i  parse times while atthe same time producing a desirable parse as the firstinterpretation i  most cases.
It is reported on in detailelsewhere in this volume \[4\].2.
A streamlined semantic processor.
This component ow uses"mapping units" to handle a number of phenomena thatwould otherwise result in a combinatorial explosion of rules.This allows the rules to be expressed more simply, with lesspossibility of forgetting to include a particular syntacticpattern.
It also makes possible a more general treatment ofthe kinds of metonomy which occur most frequently in theATIS domain.
Mapping units are described elsewhere in thisvolume \[5\].3.
An extended and improved ialogue component.
In addition tocovering domain-independent discourse phenomena, thiscomponent ow also utifizes a domain-dependent frame-likerepresentation of the discourse state, which makes it possiblefor Delphi to recognize implicit references toprior context aswell as explicit reference.
Implicit reference is frequent in theATIS domain (e.g., "Show the flights from Boston to Dallas.What meals are served?").4.
An N-best integration of speech recognition output withDelphi's NL processing.
Our initial results in using thisarchitecture for integration have been very positive.5.
A military application task.
We began to apply HARC to ademonstration task involved with military logisticalplanning.
This system, called DART (Dynamic AnalyticalReplanning Tool), our initial integration of speechunderstanding with it, and an outline of our plans to expandthat integration are described elsewhere in this volume \[6\].112SourceTotal # ofSentencesClass ASentences>3431Class AAnswers924Class D1Pairs861Class D1Answers108June '90 to Feb '91S R I 77 177 220 20 20TI - .w.
106 88 44MIT 1647 not classified not classifiedC M U 632 not classified not classifiedUp to June '90TI 776 551 551TI (June '90 test data) 93 90 90Total 64Figure 2: Common Training DataNL TRAININGTraining data for this phase of the SLS program was primarilythe 551 queries of training data that were available before theevaluation i June, 1990.
A summary of the training data is givenin figure 2.Figure 2 also shows that although over 3400 queries werecollected from all sources, fewer than 900 Class A queries withreference answers are available for training purposes, and only 64Class D1 dialogue pairs with reference answers are available.The data from MIT and CMU, although initially promisingbecause of its volume, proved not to be very useful, because thequeries were not classified (as Class A, Class D1, ete,), andreference SQL and answers were not provided.
This meant that itwas not possible during the development period to run thesequeries through our system and automatically determine whetherthe answers that were produced were correct or not.PERFORMANCEFigure 3 gives the results of BBN's performance on variousbenchmark NL and SLS tests, as of the February 19, 1991, thedate of the workshop.These results are comparable toDelphi's performance last Juneas reported by NIST \[7\].
Had the currrent scoring metric been inplace then, Delphi would have scored 57.8% on Class A.NotesS~stemTest set sizeDate submittedRI htWronNA (not answered~Wel hted ErrorScoreNL .
NL tS, tN, IN, tS,S Class A iClass A Class D1 Class D1 Class D1 Class A1 L_______ 1, 3 1, 3 2,4 1Delphi Delohi Delphi Delphi HARC145 S's2/7/9 1145 S's2/16/91 2/7/91 ~13/91 2/16/91145 S's2/7/91_',imp;.
.-~llil, am'l, :~%I, '~l .~ _iili,zl i l t ; ,  ~_~l,e.~.
I;tllEl,, ~lll,.
l ' ,  l l i l t : ,  " l ' t l t ;Figure 3: BBN's ATIS Benchmark Results, February 1991Notes:l. These results were scored by NIST before the workshop; these numbers reflect he rescoring NIST did after the workshop.2.
These results were submitted toNIST before the workshop, but were not scored.
The only change made to the system betweenthe first NL run and the second was to fix a minor bug in the SNOR translator which formats the input data for the parser.3.
The first class D1 test uncovered a problem in our system's backend translator, which was fixed for the second run.
See thediscussion section below for more information.4.
The difference between this and the previous run involved how to score pairs which gave NA for Q1.
See discussion below.113DISCUSSION NL Class D1There are several global points to make before discussing eachtest separately.As was the case last June, some problems howed up in thetest set itself.
Several queries that were not actually Class A wereincluded in the original test set; their removal resulted in the 145item test.
(More items may have been removed before the finalofficial scoring.)
Also, the reference answers for several querieshad to be augmented during the scoring, to account for ambiguitiesthat had gone unnoticed during the preparation f the test set.We believe that such problems are unavoidable, but minor andeasy to fix, so we do not recommend any major change in theevaluation methodology, but recommend that sufficient time forsites to check the reference answers be allocated in the schedule forthe next evaluation.A system which performs well on Class A but less well thanexpected on Class D might be using rather brittle techniques todeal with Class A which do not generalize effectively to discourse.It is also interesting tonote that this is not at all a problem for us.In fact, the best versions of our system did better on Class Dthan Class A, which is counterintuitive.
One would expect hatthe probability of getting aD1 pair correct is less than the productof the probability of getting a Class A sentence correct, becausenot only must two sentences be processed, but the processing ofthe second is likely to be harder than a simple Class A sentence,since it must involve reference resolution or other discourseprocessing.
This is best explained by the fact that the D1 test setwas short, rather easy, and involved more repetition of similarquery types than the Class A test.Out of Vocabulary WordsOne of the main reasons for the relatively high number of NAanswers to Class A utterances was simply vocabulary: Nineteenof the test utterances (13% of the test set) contained vocabularyoutside Delphi's lexicon.
The lack of training data clearly had animpact here, since one of the great benefits of training data isincreased vocabulary.It is worthy of note that since there is no control vocabularyamong the various systems, it is very difficult to meaningfullycompare the performance ofmultiple systems.
Using the officialdata presented, it is impossible to tell the difference between asystem that simply lacks some vocabulary entries and one that hasa larger lexicon but which cannot syntactically or semanticallyprocess many of the test utterances.NL  C lass  AThe only difference between the original score and the secondone is that a small problem in the formatting of SNOR input forthe parser was fixed.
The understanding component (syntax,semantics, and discourse processing), which is what the Class Atest is attempting tomeasure, was completely unchanged.The initial results of our D1 evaluation were shocking, but aquick investigation revealed several interesting facts:1.
Sixteen of the utterances that yielded a NA response were infact understood perfectly correctly by the syntactic, semantic,and discourse components of Delphi, and produced correctMRL expressions (refer to figure 1).
But there was a simplebug in the backend translator that turns MRL expressionsinto SQL expressions, and all 16 utterances tickled that bug.2.
Of those 16 utterances, 14 of them were extremely similar inwords, syntactic form, and semantic mport.
That is, 37% ofthe test pairs has this single form.
The fact that the test setwas significantly skewed toward one particular type of secondutterance normously magnified the effect of what wasactually avery small problem.3.
Fixing that one problem resulted in all 16 of those utterancesgoing through to SQL, and producing the correct answer.Because the problem was not in the language understandingcomponent of Delphi, because the test set was so skewed, andbecause the purpose of the D1 test at this stage was to test he testmethodology more than to test the dialogue systems, we fixed theproblem and resubmitted the results to NIST.
The resulting score(60.5%) is much more representative of the true capabilities ofourdialogue component than the original score.An additional problem with scoring D1 surfaced uring thisevaluation.
In a case where Q1 of a Q1-Q2 pair is not answered bythe system, what should be done with Q2?
We allowed oursystem to run Q2 as a context-independent query if possible, butexpected that the scoring software would treat it as NA, since it isnever possible to get a correct answer to a context-dependent queryff the context is not understood.
But the scoring package countedsuch answers as wrong.
The final run (66%) of our Class D testproduces NA for these cases.SLS  C lass  AIt is remarkable, and quite unexpected, that the score for thespeech test of Class A should be so close to the NL test on exactlythe same set of utterances.
This indicates that the N-best strategyfor integrating speech and NL processing seems to be working.Because the speech recognition component is currently producingabout 16.2% and a sentence rror rate of about 54.1% \[2\], it isobvious that the natural language component is making up forsome of the errors made by the speech recognition componentSome interesting results emerged from our analysis of how thespeech and NL components worked together.
The followingstatistics are from the original 148 utterance Class A test set(which was later educed to 145 by NIST after removal of 3 querieswhich were not actually Class A).In 58.8% of the cases, NL chose the 1-best utterance.
Of these,72.2% were correct speech ypotheses.14.9% had the correct speech ypothesis nthe N-best.12.6% didn't have the correct speech ypothesis n the N-best.114In 20.3% of the cases, NL chose one of the N-best utterances.26.7% of these were correct speech ypotheses.6.7% had the correct speech ypothesis n the N-best.66.7% didn't have the correct speech ypothesis n the N-best.In 20.9% of the cases, NL chose none of the utterances.Looking at the correctness of the answers produced by theHARC SLS system, we find the following (again, from 148 ClassA utterances):In 58.5% of the cases, NL chose the 1-best hypothesis.77.0% of these were T19.5% of these were F3.5% of these were NA.In 20.3% of the cases, NL chose one of the N-best utterances.50% of these were T30% of these were F20% of these were NA.In 20.9% of the cases, NL chose none of the N-best, so100% of these were NA.CONCLUSIONSAfter the last evaluation, our primary conclusion wasfollows \[3\], p 126.:"There is evidence that intra-speaker variability inlinguistic structure is fairly low, but that inter-speakervariability is very high.
In other words, a given speaker,at least in a single session, tends to use the same formsover and over again (e.g., "tickets flying"), and each newspeaker (at least so far:) tends to use locutions differentfrom previous peakers.asThis leads us to conclude that much more training datais needed in order to adequately prepare for evaluation..."Our experience in this evaluation only serves to underscore andreinforce our original conclusion.
Large amounts (severalthousand queries) of adequately prepared training data (classified,with reference SQL and reference answers) must be available intime for sites to use it for several months of development before atruly meaningful evaluation can be conducted.We have also developed some additional suggestions fordialogue valuation, which are detailed in a separate paper \[8\].ACKNOWLEDGEMENTSThe work reported here was supported by the AdvancedResearch Projects Agency and was monitored by the Office ofNaval Research under Contract No.
N00014-89-C-0008.
Theviews and conclusions contained in this document are those of theauthors and should not be interpreted as necessarily representingthe official poficies, either expressed or implied, of the DefenseAdvanced Research projects Agency or the United StatesGovernment.REFERENCES1.Chow, Y.L.., et al, "BYBLOS: The BBN Continuous SpeechRecognition System", IEEE International Conference on Acoustics,Speech, and Signal Processing, Dallas TX, April 1987, pp89-92, PaperNo.
3.7.2.
Kubala, F., et al, "BYBLOS Speech Recognition Benchmark Results",(in this proceedings).3.
Bates, M., Boisen, S., Ingria, R. and Stallard, D. "BBN ATIS SystemProgress Report - June 1990", Proceedings of the Speech and NaturalLanguage Workshop (June, 1990), Morgan Kaufmann Publishers, Inc.,1990.4.
Bobrow, R. "Statistical Agenda Parsing", (in this proceedings).5.
Bobrow, R., Ingfia, R., Stallard, D, "The 'Mapping Unit' Approach toSubcategofization", (in this proceedings).6.
Bates, M., Ellard, D., Peterson, P., and Shaked, V. "Using SpokenLanguage to Facilitate Military Transportation Planning", (in thisproceedings).7.
Pallett, D.S, et al "DARPA ATIS Test Results June 1990", inProceedings Speech and Natural Language Workshop, June 1990, MorganKaufmann Publishers, Inc., June, 1990.8.
Barns, M., and Ayuso, D., "A Proposal for Incremental DialogueEvaluation", (in this proceedings).115
