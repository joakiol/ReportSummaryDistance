Bilingual Co-Training for SentimentClassification of Chinese Product ReviewsXiaojun Wan?Peking UniversityThe lack of reliable Chinese sentiment resources limits research progress on Chinese sentimentclassification.
However, there are many freely available English sentiment resources on the Web.This article focuses on the problem of cross-lingual sentiment classification, which leveragesonly available English resources for Chinese sentiment classification.
We first investigate severalbasic methods (including lexicon-based methods and corpus-based methods) for cross-lingualsentiment classification by simply leveraging machine translation services to eliminate thelanguage gap, and then propose a bilingual co-training approach to make use of both the Englishview and the Chinese view based on additional unlabeled Chinese data.
Experimental resultson two test sets show the effectiveness of the proposed approach, which can outperform basicmethods and transductive methods.1.
IntroductionSentiment classification is the task of identifying the sentiment polarity of a given text,which is traditionally categorized as either positive or negative.
In recent years, senti-ment classification has drawn much attention in the natural language processing (NLP)field and it has many useful applications, such as opinion mining and summarization(Liu, Hu, and Cheng 2005; Ku, Liang, and Chen 2006; Titov and McDonald 2008).To date, a variety of lexicon-based and corpus-based methods have been developedfor sentiment classification.
The lexicon-based methods rely heavily on a sentimentlexicon containing positive terms and negative terms.
The corpus-based methods relyheavily on an annotated corpus for training a sentiment classifier.
The sentiment lexiconand corpus are considered the most valuable resources for the sentiment classificationtask.
However, such resources in different languages are rather unbalanced.
Becausemost previous work focuses on English sentiment classification, many annotated sen-timent lexica and corpora for English sentiment classification in various domains arefreely available on the Web.
However, the annotated resources for sentiment classifica-tion in many other languages are not abundant and it is time-consuming to manuallylabel a rich and reliable sentiment lexicon or corpus in those languages.
The challengebefore us is leveraging rich English resources for sentiment classification in otherlanguages.
In this study, we focus on the problem of English-to-Chinese cross-lingualsentiment classification, leveraging only English sentiment resources for sentiment clas-sification of Chinese product reviews, without using any Chinese sentiment resources.?
Institute of Computer Science and Technology, The MOE Key Laboratory of Computational Linguistics,Peking University, Beijing 100871, China.
E-mail: wanxiaojun@icst.pku.edu.cn.Submission received: 17 March 2010; revised submission received: 21 December 2010; accepted forpublication: 29 January 2011.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 3Note that this problem is not only defined for Chinese sentiment classification, butalso for various sentiment analysis tasks in other different languages.
The proposedapproach in this study can also be applied for generic cross-lingual text categorizationtasks.Pilot studies have been performed to make use of English resources for subjectivityclassification in Romanian (Mihalcea, Banea, and Wiebe 2007; Banea et al 2008), and themethods are very straightforward.
First, they use machine translation for translatingresources (such as a lexicon or corpus) between Romanian and English, and thenthey employ the lexicon-based or corpus-based method for subjectivity classification ineither Romanian or English.
Similar experiments have been performed for subjectivityclassification in Spanish (Banea et al 2008).
However, our empirical study shows thatsentiment classification performance using these methods is far from satisfactory be-cause the machine translation quality is not very good according to the recent NIST openmachine translation evaluation results, and thus a language gap between the originallanguage and the translated language still exists.In this study, we first investigate several basic methods for cross-lingual sentimentclassification, and then propose a bilingual co-training approach to improve theaccuracy of corpus-based polarity classification of Chinese product reviews.
UnlabeledChinese reviews can be fully leveraged in the proposed approach.
First, machinetranslation services are used to translate English training reviews into Chinese reviewsand also translate Chinese test reviews and additional unlabeled reviews into Englishreviews.
Then, we can view the classification problem in two different ways: theChinese view with only Chinese features and the English view with only Englishfeatures.
We then use the co-training approach to make full use of the two redundantviews of features.
The SVM classifier (Joachims 2002) is adopted as the basic classifierin the proposed approach.Three machine translation services (Google Translate, Yahoo Babel Fish, and MicrosoftBing Translate) are used for review translation in the experiments.
The experimentalresults on two test sets show that the proposed approach based on any machine transla-tion service can outperform a few popular baselines, including advanced transductivemethods.
We also find that the balanced growth of the positive and negative instancesat each iteration in the co-training algorithm is very important for the success of thealgorithm.The rest of this article is organized as follows: Section 2 discusses related work.Section 3 introduces several basic methods.
The proposed co-training approach is de-scribed in detail in Section 4.
Sections 5 and 6 present the evaluation set-up and results,respectively.
Lastly, we conclude this article and discuss future work in Section 7.2.
Related Work2.1 Sentiment ClassificationSentiment classification can be performed on words, sentences, or documents.
In thisarticle we focus on document-level sentiment classification, and research in this area hasfollowed a lexicon-based (i.e., rule-based) or a corpus-based (i.e., classification-based)approach.Lexicon-based methods involve deriving a sentiment measure for text based onsentiment lexica.
Turney (2002) predicts the sentiment orientation of a review as theaverage semantic orientation of the phrases in the review that contain adjectives or ad-verbs, which is known as the semantic orientation method.
Kim and Hovy (2004) build588Wan Bilingual Co-Training for Sentiment Classification of Chinese Product Reviewsthree models to assign a sentiment category to a given sentence by combining the indi-vidual sentiments of sentiment-bearing words.
Kanayama, Nasukawa, and Watanabe(2004) use the technique of deep language analysis for machine translation to extractsentiment units in text documents.
Kennedy and Inkpen (2006) determine the sentimentof a customer review by counting positive and negative terms and taking into accountcontextual valence shifters, such as negations and intensifiers.
Devitt and Ahmad (2007)explore a computable metric of positive or negative polarity in financial news text.Corpus-based methods consider the sentiment analysis task as a classification taskand they use a labeled corpus to train a sentiment classifier.
Since the work of Pang, Lee,and Vaithyanathan (2002), various classification models and linguistic features havebeen proposed to improve classification performance (Mullen and Collier 2004; Pangand Lee 2004; Read 2005; Wilson, Wiebe, and Hoffmann 2005).
More recently, McDonaldet al (2007) investigate a structured model for jointly classifying the sentiment of a textat varying levels of granularity.
Blitzer, Dredze, and Pereira (2007) investigate domainadaptation for sentiment classifiers, focusing on on-line reviews for different types ofproducts.
Andreevskaia and Bergler (2008) present a new system consisting of theensemble of a corpus-based classifier and a lexicon-based classifier with precision-basedvote weighting.
A non-negative matrix tri-factorization approach has been proposedfor sentiment classification, which learns from lexical prior knowledge in the formof domain-independent sentiment-laden terms in conjunction with domain-dependentunlabeled data and a few labeled data (Li, Zhang, and Sindhwani 2009).
Dasgupta andNg (2009) propose a semi-supervised approach to sentiment classification where theyfirst use spectral techniques to mine the unambiguous reviews and then exploit them toclassify the ambiguous reviews by a novel combination of active learning, transductivelearning, and ensemble learning.Chinese sentiment analysis has also been studied (Li and Sun 2007) and mostsuch work uses similar lexicon-based or corpus-based methods for Chinese sentimentclassification.To date, several pilot studies have been performed to leverage rich English re-sources for sentiment analysis in other languages.
Standard naive Bayes and SVMclassifiers have been applied for subjectivity classification in Romanian and Spanish(Mihalcea, Banea, and Wiebe 2007; Banea et al 2008), and the results show that auto-matic translation is a feasible alternative for the construction of resources and tools forsubjectivity analysis in a new target language.
Wan (2008) focuses on leveraging bothChinese and English lexica to improve Chinese sentiment analysis by using lexicon-based methods.
Wei and Pal (2010) apply structural correspondence learning (SCL)to minimize the noise introduced by machine translations.
In this study, we focus ondeveloping novel approaches to improve the corpus-based method for cross-lingualsentiment classification of Chinese product reviews.2.2 Cross-Domain Text ClassificationCross-domain text classification can be considered as a more general task than cross-lingual sentiment classification.
In this task, the labeled and unlabeled data come fromdifferent domains and their underlying distributions are often different from each other,which violates the basic assumption of traditional supervised learning.To date, many semi-supervised learning algorithms have been developed for ad-dressing the cross-domain text classification problem by transferring knowledge acrossdomains, and such algorithms include Transductive SVM (Joachims 1999), EM (Nigamet al 2000), EM-based naive Bayes classifier (Dai et al 2007a), Topic-bridged PLSA (Xue589Computational Linguistics Volume 37, Number 3et al 2008), Co-Clustering?based classification (Dai et al 2007b), and the two-stageapproach (Jiang and Zhai 2007).
Dai et al (2007b) use co-clustering as a bridge topropagate the class structure and knowledge from the in-domain to the out-of-domain.Jiang and Zhai (2007) look for a set of features generalizable across domains at the firstgeneralization stage, and then pick up useful features specific to the target domainat the second adaptation stage.
Daume?
III and Marcu (2006) introduce a statisticalformulation of this problem in terms of a simple mixture model.
In recent years, a fewmethods/algorithms have been proposed for cross-domain sentiment classification,including structural correspondence learning (Blitzer, Dredze, and Pereira 2007), cross-domain graph ranking (Wu et al 2009), and spectral feature alignment (Pan et al 2010).Moreover, several previous studies focus on the problem of cross-lingual text clas-sification, which can be considered a special case of cross-domain text classification.Bel, Koster, and Villegas (2003) empirically investigate three translation strategies forcross-lingual text categorization: document translation, terminology translation, andprofile-based translation.
A few novel models have been proposed to address theproblem?for example, the EM-based algorithm (Rigutini, Maggini, and Liu 2005),the information bottleneck approach (Ling et al 2008), multilingual domain models(Gliozzo and Strapparava 2005), and the structural correspondence learning approach(Prettenhofer and Stein 2010; Wei and Pal 2010).
Shi et al (2010) introduce a methodto transfer classification knowledge across languages by translating the model featuresand using an EM algorithm.
The most recent related work includes multilingual textcategorization based on multi-view learning (Amini, Usunier, and Goutte 2009; Aminiand Goutte 2010).
To the best of our knowledge, co-training has not yet been investi-gated for cross-domain or cross-lingual text classification.3.
The Basic MethodsA straightforward method for cross-lingual sentiment classification is to use machinetranslation for transferring lexica or corpora of reviews between English and Chinese,and then apply the lexicon-based or corpus-based method for sentiment classification ineither the English or Chinese language.
Therefore, the basic methods consist of two mainsteps: resource translation and sentiment classification.
According to different transla-tion directions and classification methods, four basic methods are introduced as follows.3.1 Lexicon-Based Method in English Language: LEX(EN)This method first translates Chinese reviews into English reviews, and then identifiesthe sentiment polarity of the translated English reviews based on English sentimentlexica, as illustrated in Figure 1.For any specific language, we employ the semantic-oriented approach used inWan (2008) to compute the semantic orientation value of a review.
The unsupervisedapproach is quite straightforward and it makes use of the following sentiment lexica:positive Lexicon (Positive Dic) containing terms expressing positive polarity, NegativeLexicon (Negative Dic) containing terms expressing negative polarity, NegationLexicon (Negation Dic) containing terms that are used to reverse the semantic polarityof a particular term, and Intensifier Lexicon (Intensifier Dic) containing terms thatare used to change the degree to which a term is positive or negative.
The semanticorientation value for a review is computed by summing the polarity values of all termsin the review, making use of both the word polarity defined in the positive and negativelexica and the contextual valence shifters defined in the negation and intensifier lexica.590Wan Bilingual Co-Training for Sentiment Classification of Chinese Product ReviewsFigure 1Framework of LEX(EN).For example, given a review of the image quality is not good, although the term good is apositive term, the use of the negation term not reverses the polarity orientation value,and the overall polarity orientation value of the review is negative.
Given a review ofthe image quality is very good, the use of the intensifier term very intensifies the polarityorientation value of good, and the overall polarity orientation value of the review ispositive.
In our study, the scope of a negation or intensifier term is simply determinedby using a distance window of two words.
We do not use a parser to determine thescope because the parsing results for the translated reviews are not reliable.Finally, if the semantic orientation value of a review is less than 0, the review islabeled as negative; otherwise, the review is labeled as positive.3.2 Lexicon-Based Method in Chinese Language: LEX(CN)This method first translates English sentiment lexica into Chinese lexica, and thenidentifies the sentiment polarity of Chinese reviews based on the translated Chineselexica, as illustrated in Figure 2.After we retrieve the four translated Chinese lexica, we apply the algorithm forsemantic orientation value computation used in Wan (2008) to predict the polarityorientation of the Chinese reviews.
Each Chinese review is first segmented into Chineseterms/words by using our in-house conditional random field (CRF)?based Chineseword segmentation tool, and then the polarity orientation value for the Chinese reviewis computed by summing the polarity values of all terms in the review.
The termsdefined in the negation lexicon are used to reverse the polarity values of the nearbyChinese terms, and the terms defined in the intensifier lexicon are used to intensify thepolarity values of the nearby Chinese terms.
The scope of a negation or intensifier termis also simply determined by using a distance window of two words.3.3 Corpus-Based Method in English Language: SVM(EN)As illustrated in Figure 3, we first learn a classifier based on labeled English reviews, andthen translate test Chinese reviews into English reviews.
Lastly, we use the classifierto classify the translated English reviews.
In this study, we use the widely used SVM591Computational Linguistics Volume 37, Number 3Figure 2Framework of LEX(CN).Figure 3Framework of SVM(EN).classifier for classification.
We also use a transductive variant of the SVM classifier formaking use of unlabeled Chinese reviews, which will be described in Section 5.5.
AllEnglish unigrams and bigrams are used as features, and the feature weight is simply setto term frequency.1 Finally, the sign of the prediction value of the classifier indicates thepolarity orientation of the review.3.4 Corpus-Based Method in Chinese Language: SVM(CN)As illustrated in Figure 4, we first translate labeled English reviews into Chinese re-views, and then learn a classifier based on the translated Chinese reviews with labels.1 Term frequency performs better than TK/IDF by our empirical analysis.592Wan Bilingual Co-Training for Sentiment Classification of Chinese Product ReviewsFigure 4Framework of SVM(CN).Lastly, we use the classifier to classify test Chinese reviews.
We also use the SVMclassifier (and a transductive variant) for classification, and all Chinese unigrams andbigrams are used as features.24.
The Bilingual Co-Training Method4.1 OverviewThese two basic corpus-based methods have been used in Banea et al (2008) forRomanian subjectivity analysis.
As shown in our later experiments, the two methodsdo not perform well for Chinese sentiment classification, because the term distributionsin the original reviews and the translated reviews are different.
One reason is attributedto machine translation.
Because current machine translation services cannot accuratelytranslate reviews, it is inevitable that they bring errors into the translated texts.
More-over, it may happen that different terms are used to express the same meaning in theoriginal texts and the translated texts, because each machine translation service usesparticular resources and corpora for model building.
The other reason is attributed toinherent domain difference.
The review sets in different languages are generally in verydifferent domains, because they are written by different users in different countries, andthe writing styles, lengths, and term usages of the reviews are very different.In order to address this problem, we propose to use the co-training approach tomake use of some amounts of unlabeled Chinese reviews to improve the classificationaccuracy.
The co-training approach can make full use of both the English features andthe Chinese features in a unified framework.
The framework of the proposed approachis illustrated in Figure 5.The framework consists of a training phase and a classification phase.
In the trainingphase, the input is the labeled English reviews and some amount of unlabeled Chinese2 For Chinese text, a unigram refers to a Chinese word and a bigram refers to two adjacent Chinese words.593Computational Linguistics Volume 37, Number 3Figure 5Framework of the bilingual co-training approach.reviews.3 The labeled English reviews are translated into labeled Chinese reviews andthe unlabeled Chinese reviews are translated into unlabeled English reviews by usingmachine translation services.
Therefore, each review is associated with an English ver-sion and a Chinese version.
The English features (i.e., all English unigrams and bigrams)and the Chinese features (i.e., all Chinese unigrams and bigrams) for each review areconsidered two different and redundant views of the review.
The co-training algorithmis then applied to learn two classifiers, and finally the two classifiers are combined intoa single sentiment classifier.
In the classification phase, each unlabeled Chinese review3 The unlabeled Chinese reviews used for co-training do not include the unlabeled Chinese reviews fortesting, that is, the Chinese reviews for testing are blind to the training phase.594Wan Bilingual Co-Training for Sentiment Classification of Chinese Product Reviewsfor testing is first translated into an English review, and then the learned classifier isapplied to predict the polarity orientation of the review as either positive or negative.4.2 The Co-Training AlgorithmThe co-training algorithm (Blum and Mitchell 1998) is a bootstrapping method; it startswith a set of labeled data, and increases the amount of annotated data using someamount of unlabeled data in an incremental way.
One important aspect of co-training isthat two conditionally independent views are required for co-training to work, but theindependence assumption can be relaxed.
In the past, co-training has been successfullyapplied to statistical parsing (Sarkar 2001), reference resolution (Ng and Cardie 2003),part-of-speech tagging (Clark, Curran, and Osborne 2003), word sense disambigua-tion (Mihalcea 2004), and e-mail classification (Kiritchenko and Matwin 2001).
Co-training has not yet been used for cross-domain or cross-lingual text categorization,however.The intuition behind the co-training algorithm is that if one classifier can confidentlypredict the class of an example, it can provide one more training example for the otherclassifier.
Of course, if this example happens to be easily classified by the first classifier,it does not mean that this example will be easily classified by the second classifier,so the second classifier will get useful information to improve itself, and vice versa(Kiritchenko and Matwin 2001).In the context of cross-lingual sentiment classification, each labeled English reviewor unlabeled Chinese review has two sets of features: English features and Chinese fea-tures.
Here, a review is used to indicate both its Chinese version and its English version,unless stated otherwise.
Now we describe the details of the co-training algorithm.The notations in the co-training algorithm are as follows: Fen and Fcn are redundantly sufficient sets of features, where Fen representsthe English features, and Fcn represents the Chinese features; L is a set of labeled training reviews; U is a set of unlabeled reviews; C is a basic classification algorithm, and Cen and Ccn represent twocomponent classifiers based on C; and p and n are positive integer numbers.The following steps loop for I iterations in the co-training algorithm:(1) Learn the first classifier Cen from L based on Fen.
(2) Use Cen to label reviews in U based on Fen.
(3) Choose p positive and n negative most confidently predicted reviews (Een)from U.
(4) Learn the second classifier Ccn from L based on Fcn.
(5) Use Ccn to label reviews in U based on Fcn.595Computational Linguistics Volume 37, Number 3(6) Choose p positive and n negative most confidently predicted reviews (Ecn)from U.
(7) Remove the reviews in Een?Ecn from U.
Note that the examples withconflicting labels are not included in Een?Ecn.
In other words, if anexample is in both Een and Ecn, but the labels for the example areconflicting, the example will be excluded from Een?Ecn.
(8) Add the reviews in Een?Ecn with the corresponding labels to L.From the point of view of the algorithm, the unlabeled reviews are added tothe model during the bootstrapping phase, and only for these reviews are the labelsobtained using an average of the normalized prediction values determined by thecomponent classifiers.
In the algorithm, p and n are two parameters controlling thegrowth size in the labeled data.
At each iteration, at most 2(p + n) reviews are addedinto L. The two parameters also maintain the class distribution in the labeled data bybalancing the parameter values of p and n at each iteration.
If p is similar to n, the growthis called balanced growth, otherwise the growth is called unbalanced growth.
Note thatthe co-training algorithm used in this study differs slightly from the original co-trainingalgorithm in that the original co-training algorithm is dependent on the sequence ofthe two component classifiers, whereas our co-training algorithm is independent ofthe classifier sequence.
Moreover, each classifier in our co-training algorithm not onlymakes use of a few examples confidently predicted by the other classifier, but also makesuse of a few examples confidently predicted by itself.In the co-training algorithm, a basic classification algorithm is required to constructCen and Ccn.
Typical text classifiers include Support Vector Machine (SVM), naive Bayes(NB), Maximum Entropy (ME), K-Nearest Neighbor (KNN), and so forth.
In this study,we adopt the widely used SVM classifier (Joachims 2002), as in the basic corpus-basedmethods.
Viewing input data as two sets of vectors in a feature space, SVM constructsa separating hyperplane in this space by maximizing the margin between the two datasets.
The output value of the SVM classifier for a review indicates the confidence levelof the review?s classification.
The sentiment polarity of a review is indicated by the signof the prediction value.
Note that we use all unigrams and bigrams in each language asfeatures and the feature weight is simply set to term frequency.In the training phase, the co-training algorithm learns two separate classifiers: Cenand Ccn.
Therefore, in the classification phase, we can obtain two prediction values for atest review.
We normalize the prediction values into [?1, 1] by dividing the maximumabsolute value.
Finally, the average of the normalized values is used as the overallprediction value of the review.4Several theoretical studies have been performed on co-training in the machinelearning field.
Blum and Mitchell (1998) prove that co-training can be successful if thetwo sufficient and redundant views are conditionally independent of each other.
Abney(2002) shows that weak dependence between the two views can also guarantee success-ful co-training.
Balcan, Blum, and Yang (2005) prove that a weaker assumption called?-expansion is sufficient for iterative co-training to succeed.
Wang and Zhou (2010) viewthe co-training process as a combinative label propagation over two views, and they4 Though this method of combining scores is unprincipled due to the fact that the scores themselves arenot calibrated, we found it worked well in practice.596Wan Bilingual Co-Training for Sentiment Classification of Chinese Product Reviewsprovide the sufficient and necessary condition for co-training to succeed.
As can beseen, the assumption about the dependence between the two views is much relaxed,which can guarantee that although the English features and the Chinese features arenot conditionally independent of each other, the use of the two views for co-trainingis acceptable.
In the extreme case, if both classifiers agree on all the unlabeled data,labeling the data does not create new information, and thus the co-training algorithmwill not work at all.
We will show in the experiments that the English classifier and theChinese classifier disagree on many unlabeled examples, which can also guarantee thesuccess of the co-training approach.5.
Evaluation Set-up5.1 English Sentiment ResourcesThe basic LEX(EN) and LEX(CN) methods require English sentiment lexica.
In thisstudy, we collected and used the following popular and publicly available Englishsentiment lexica,5 without any further filtering and labeling:Positive Dicen: 2,718 English positive terms (e.g., amazing, gorgeous) were collectedfrom a feature file6 containing the subjectivity clues used in the work (Wilson, Wiebe,and Hoffmann 2005; Wilson et al 2005).
The clues in this file were collected from anumber of sources.
Some were culled from manually developed resources (e.g., GeneralInquirer7 [Stone et al 1966]).
Others were identified automatically using both annotatedand unannotated data.
A majority of the clues were collected as part of the workreported in Riloff and Wiebe (2003).Negative Dicen: 4,910 English negative terms (e.g., boring, idiot) were collected fromthe same file.Negation Dicen: 88 negation terms (e.g., never, lack) were collected from a featurefile8 used in Wilson, Wiebe, and Hoffmann 2005; Wilson et al 2005.Intensifier Dicen: 244 intensifier terms (e.g., very, absolutely) were collected from afeature file9 used in Wilson, Wiebe, and Hoffmann 2005; Wilson et al 2005.We then used a large English-to-Chinese dictionary (LDC EC DIC2.010) with110,834 entries for projecting English lexica into Chinese lexica via term-to-term trans-lation.
If an English term corresponds to multiple Chinese terms, we simply use the firstChinese term for translation because the first one is the dominant translation.The basic SVM(EN), SVM(CN) methods and the co-training method require a la-beled English sentiment corpus.
In this study, we used the following popular Englishsentiment corpus:Training Set (Labeled English Reviews): There are many labeled English corporaavailable on the Web; we used the corpus constructed for multi-domain sentimentclassification (Blitzer, Drezde, and Perreira 2007),11 because the corpus was large-scale5 In this study, we focus on using a few popular English resources for comparative study, instead of tryingto collect and use all available resources.6 The file subjclueslen1-HLTEMNLP05.tff can be downloaded from http://www.cs.pitt.edu/mpqa/.7 http://www.wjh.harvard.edu/?inquirer/homecat.htm.8 The file valenceshifters.tff can be downloaded from http://www.cs.pitt.edu/mpqa/.9 The file intensifiers2.tff can be downloaded from http://www.cs.pitt.edu/mpqa/.10 http://projects.ldc.upenn.edu/Chinese/LDC ch.htm.11 http://www.cis.upenn.edu/?mdredze/datasets/sentiment/.597Computational Linguistics Volume 37, Number 3and it was within similar domains to the test set.
The data set consists of 8,000 Amazonproduct reviews (4,000 positive reviews + 4,000 negative reviews) for four differentproduct types: books, DVDs, electronics, and kitchen appliances.5.2 Chinese Review SetsThe following two data sets were collected and used as test sets in the experiments:IT168 Test Set (Labeled Chinese Reviews from IT168): In order to assess theperformance of the proposed approach, we collected and labeled 886 product reviews(451 positive reviews + 435 negative reviews) from the popular Chinese IT product Website IT168.12 The reviews focus on such products as mp3 players, mobile phones, digitalcameras, and laptop computers.360BUY Test Set (Labeled Chinese Reviews from 360BUY): In addition, we col-lected and labeled 930 product reviews (560 positive reviews + 370 negative reviews)from another popular Chinese online shopping Web site (360BUY).13 The reviews focuson such products as electronics and furniture.For these two test sets, two subjects participated in the annotation procedure.
Thepolarity tags of the reviews were first annotated by one subject and then checked by theother subject.
The conflicts were resolved by discussion.We also collected the following unlabeled Chinese review set for transductivemethods and the co-training method:Unlabeled Set (Unlabeled Chinese Reviews): We downloaded 2,000 additionalChinese product reviews from IT168 and used the reviews as the unlabeled set.14 Theunlabeled set and the IT168 test set were in the same domain and had similar underlyingfeature distributions, but the unlabeled set and the 360BUY test set may be in differentdomains.Note that the training set and the unlabeled set were used in the training phase,and the test set was blind to the training phase.
All these data sets are available uponrequest.5.3 Review TranslationFor all the data sets described in Sections 5.1 and 5.2, each Chinese review was translatedinto an English review, and each English review was translated into a Chinese review.15Therefore, each review has two views: the English view and the Chinese view.
A reviewis represented by both its English view and its Chinese view.Fortunately, machine translation techniques have been well developed in theNLP field (Lopez 2008), though the translation performance is far from satisfactory.A few commercial machine translation services can be publicly accessed, for ex-ample, Google Translate (GoogleTranslate),16 Yahoo Babel Fish (YahooTranslate),17 and12 http://www.it168.com.13 http://www.360buy.com.14 Only 1,000 unlabeled Chinese reviews were used in Wan (2009).15 We used the recently updated MT services for machine translation; we believe that the translation resultsare better than those in Wan (2009).16 http://translate.google.com/translate t.17 http://babelfish.yahoo.com/translate txt.598Wan Bilingual Co-Training for Sentiment Classification of Chinese Product ReviewsMicrosoft Bing Translate (MicrosoftTranslate).18 The three MT systems are considered tobe state-of-the-art commercial machine translation systems, and all three MT systemsprovide Chinese-to-English and English-to-Chinese translation services.
However, it isnot easy to accurately compare the translation performance of the three MT systemsbecause the three systems are updated frequently.
In the experiments, we adopt all ofthem for both English-to-Chinese translation and Chinese-to-English translation.Here are two examples of Chinese reviews and the corresponding translatedEnglish reviews, including the manual translation results (HumanTranslate):Positive Example: ,HumanTranslate: More functional and well used.GoogleTranslate: Or more functional, well used.YahooTranslate: The function are quite many, with also good.MicrosoftTranslate: or more, with gamers.Negative example: , 3000HumanTranslate: The price is too high and it should be below 3000.GoogleTranslate: Prices were too high, preferably below 3000.YahooTranslate: The price is excessively high, best below 3000.MicrosoftTranslate: The best price too high.
3000 following.Here are two examples of English reviews and the corresponding translatedChinese reviews:Positive Example: The book arrived as expected and was in great shape.
ThanksHumanTranslate: ,GoogleTranslate: ,YahooTranslate:MicrosoftTranslate: ,Negative example: We had to return this item for a refund.
It arrived and never worked.HumanTranslate: ,GoogleTranslate:YahooTranslate:MicrosoftTranslate: ,5.4 Evaluation MetricWe used the standard precision (P), recall (R), and F-measure (F) to measure the per-formance of positive and negative classes, and employed the accuracy metric (Acc) tomeasure the overall performance of each system.
The metrics are defined in the sameway as in generic text categorization tasks.5.5 Baseline MethodsIn the experiments, the proposed co-training approach (CoTrain) is compared with twogroups of baseline methods.18 http://www.microsofttranslator.com/.599Computational Linguistics Volume 37, Number 3The first group includes the following three monolingual baselines, which performsentiment classification of Chinese reviews based on Chinese resources.BaseCN1: This method is a monolingual baseline for Chinese sentiment classifica-tion, and it is lexicon-based.
It uses the most popular and publicly available Chinesesentiment lexica19 for Chinese sentiment classification by applying the same algorithmas LEX(CN) for semantic orientation value computation.
The four Chinese lexica werecollected as follows:Positive Diccn: 3,730 Chinese positive terms (e.g., /good-looking, /lucky) were collected from the Chinese Vocabulary for Sentiment Analysis (VSA)20released by HOWNET.Negative Diccn: 3,116 Chinese negative terms (e.g., /expensive, /clumsy) were collected from the Chinese VSA released by HOWNET.Negation Diccn: 13 negation terms (e.g., /be not, /be lack in) were collectedfrom related papers.Intensifier Diccn: 148 intensifier terms (e.g., /totally, /extremely) werecollected from the Chinese VSA released by HOWNET.BaseCN2: This method is a monolingual baseline based on supervised classificationin the Chinese language.
We downloaded a very large number of product reviewsand their associated tags from the popular Chinese online shopping Web site AmazonChina.21 The data set consists of 45,898 positive reviews and 24,146 negative reviews.The reviews are about various products such as consumer electronics, mobile phones,digital products, books, and so on.
The polarity tag of each review was automaticallyjudged by the number of the user-assigned stars attached to the review.
If the starnumber is equal to or less than two, the review is labeled as negative, and otherwisethe review is labeled as positive.
We adopt the inductive SVM classifier and use thelarge corpus for training.
Finally, the classifier is applied for sentiment classification ofChinese reviews.BaseCN3: This method is a monolingual baseline based on transductive classifica-tion in the Chinese language.
We adopt the transductive SVM classifier, and use theautomatically crawled corpus used in BaseCN2 and the unlabeled Chinese reviews fortraining.In addition, we perform five-fold cross-validation on each test set.
The method firstrandomly partitions the original test set into five subsets.
During each cross-validationprocess, a single subset is retained as the validation set, and the remaining four subsetsare used as the training set.
The inductive SVM classifier is trained on the training setand tested on the validation set.
The cross-validation process is then repeated five times,and the five results are then averaged.
Note that the results are produced by five differ-ent classification models that are different from other methods.
The performance of thecross-validation method can be seen as an upper bound for the monolingual methods,because the method uses human-labeled Chinese reviews for training, and moreover,the training reviews and the test reviews come from the same Web site and thus theyare in the same domain.
The method is denoted UpperBound(CrossValidation).The second group includes the following eleven cross-lingual baselines, whichperform sentiment classification of Chinese reviews based only on English resources.19 Very few Chinese sentiment lexica are freely available on the Web.20 http://www.keenage.com/html/e index.html.21 http://www.amazon.cn.600Wan Bilingual Co-Training for Sentiment Classification of Chinese Product ReviewsLEX(CN): This method uses the lexicon-based method in the Chinese language, asdescribed in Section 3.2.LEX(EN): This method uses the lexicon-based method in the English language, asdescribed in Section 3.1.SVM(CN): This method applies the inductive SVM with only Chinese features forsentiment classification in the Chinese view, as described in Section 3.4.
Only English-to-Chinese translation is needed.
The inductive SVM learner aims to build a decisionfunction based on the training set, and the unlabeled set is not used by this method.SVM(EN): This method applies the inductive SVM with only English features forsentiment classification in the English view, as described in Section 3.3.
Only Chinese-to-English translation is needed.
The unlabeled set is not used by this method.SVM(ENCN): This method combines the results of SVM(EN) and SVM(CN) byaveraging the prediction values of the two SVM classifiers in the same way as in theco-training approach.TSVM(CN): This method applies the transductive SVM with only Chinese featuresfor sentiment classification in the Chinese view.
Only English-to-Chinese translation isneeded.
The unlabeled set is used by this method.
Transductive SVM has been widelyused to treat partially labeled data in semi-supervised learning.
Different from inductiveSVM, it can leverage unlabeled data and try to separate both labeled and unlabeled datawith a maximum margin.
For more details, refer to Joachims (1999).TSVM(EN): This method applies the transductive SVM with only English featuresfor sentiment classification in the English view.
Only Chinese-to-English translation isneeded.
The unlabeled set is used by this method.TSVM(ENCN): This method combines the results of TSVM(EN) and TSVM(CN) byaveraging the prediction values of the two TSVM classifiers.SelfTrain(CN): This method uses the self-training algorithm in Mihalcea (2004) andthe unlabeled set for sentiment classification in the Chinese view.
The algorithm is asingle-view weakly supervised algorithm.
It starts with a set of labeled reviews, andbuilds a SVM classifier.
The classifier is then applied to the unlabeled reviews, and thep positive and n negative most confidently predicted reviews are added to the labeledset.
The classifier is then retrained on the new labeled set.
The process continues for Iiterations.
The parameters p, n, and I are defined in the same way as for the co-trainingalgorithm.SelfTrain(EN): This method uses the self-training algorithm and the unlabeled setfor sentiment classification in the English view.SelfTrain(ENCN): This method combines the results of SelfTrain(EN) and Self-Train(CN) by averaging the prediction values of the two self-training classifiers.
It isnoteworthy that SelfTrain(ENCN) differs from CoTrain in that there is no mutual inter-action between the English component classifier and the Chinese component classifierin SelfTrain(ENCN).Note that the three transductive methods and the three self-training methods arestrong baselines because they have been widely used for improving classification accu-racy by leveraging additional unlabeled examples.
We use the SVMLight toolkit22 withthe linear kernel and default parameter values for both inductive SVM classification andtransductive SVM classification.Though feature selection methods (e.g., Document Frequency [DF], InformationGain [IG], and Mutual Information [MI]) can be used for dimension reduction, we22 http://svmlight.joachims.org.601Computational Linguistics Volume 37, Number 3Table 1Results for monolingual methods on the IT168 test set.Positive Negative TotalMethod P R F P R F AccBaseCN1 0.681 0.929 0.786 0.882 0.549 0.677 0.743BaseCN2 0.716 0.945 0.815 0.914 0.611 0.733 0.781BaseCN3 0.724 0.942 0.819 0.913 0.628 0.744 0.788UpperBound 0.909 0.867 0.888 0.868 0.910 0.889 0.888(CrossValidation)use all the features in the experiments for comparative analysis because there is nosignificant performance improvement after applying the feature selection techniquesin our empirical study.6.
Evaluation Results6.1 Method ComparisonIn the experiments, we first compare the proposed co-training approach with the base-line methods.
The parameter values for CoTrain and SelfTrain are set as I = 80 andp = n = 5.
The three parameters are empirically set by considering the total number (i.e.,2,000) of the unlabeled Chinese reviews.
In our empirical study, the proposed approachcan perform well with a wide range of parameter values, which will be shown later.Tables 1 and 5 show the results for three monolingual baselines and the upperbound on the two test sets, respectively.
Tables 2 through 4 show the comparison resultsfor the cross-lingual methods based on the three machine translation services on theIT168 test set, respectively.
Tables 6 through 8 show the comparison results for the cross-lingual methods based on the three machine translation services on the 360BUY test set,respectively.
Note that we also present the classification results for the two componentclassifiers (Chinese component classifier Ccn and English component classifier Cen) ofour proposed co-training approach.
Tables 9 and 10 show the results of significancetests between CoTrain and the baseline methods on the two test sets, respectively.
Weadopt the sign test as a significance test because it is widely used in the field of text cate-gorization (Yang and Liu 1999).
In particular, we use an on-line service23 for performingsign tests in the experiments.
The p-values for sign tests are presented; the performancedifference between CoTrain and a baseline method is statistically significant at a 95%level if the p-value is smaller than 0.05.As can be seen in Tables 1 through 8, no matter which machine translation serviceis used, the proposed co-training approach (CoTrain) outperforms all baseline methodson the overall accuracy metric and most other metrics on the two test sets.
In particular,on the IT168 test set, the best accuracy is achieved by CoTrain with GoogleTranslate,and on the 360BUY test set, the best result is achieved by CoTrain with YahooTranslate.Even the two component classifiers in CoTrain can perform as well as or better than thebaseline methods.
As can be seen from Tables 9 and 10, the performance difference23 http://www.fon.hum.uva.nl/Service/Statistics/Sign Test.html.602Wan Bilingual Co-Training for Sentiment Classification of Chinese Product ReviewsTable 2Comparison results for cross-lingual methods on the IT168 test set with GoogleTranslate.Positive Negative TotalMethod P R F P R F AccLEX(CN) 0.615 0.772 0.684 0.678 0.499 0.575 0.638LEX(EN) 0.770 0.914 0.836 0.889 0.717 0.794 0.817SVM(CN) 0.735 0.843 0.785 0.808 0.685 0.741 0.765SVM(EN) 0.737 0.800 0.767 0.773 0.703 0.736 0.753SVM(ENCN) 0.758 0.856 0.804 0.828 0.717 0.768 0.788TSVM(CN) 0.735 0.732 0.733 0.723 0.726 0.725 0.729TSVM(EN) 0.816 0.847 0.831 0.835 0.802 0.818 0.825TSVM(ENCN) 0.817 0.840 0.828 0.829 0.805 0.817 0.823SelfTrain(CN) 0.742 0.747 0.745 0.736 0.731 0.734 0.739SelfTrain(EN) 0.801 0.847 0.823 0.831 0.782 0.806 0.815SelfTrain(ENCN) 0.804 0.836 0.820 0.823 0.789 0.805 0.813Ccn in CoTrain 0.828 0.834 0.831 0.826 0.821 0.824 0.827Cen in CoTrain 0.833 0.863 0.847 0.852 0.821 0.836 0.842CoTrain 0.858 0.882 0.870 0.874 0.848 0.861 0.866between CoTrain and any baseline method is always statistically significant whenGoogleTranslate or YahooTranslate is used for machine translation.
We can also see thatthe performance difference between CoTrain and any baseline method is almost alwaysstatistically significant when MicrosoftTranslate is used for machine translation, exceptfor the TSVM(CN) baseline on the IT168 test set and the TSVM(ENCN) and TSVM(CN)baselines on the 360BUY test set.Table 3Comparison results for cross-lingual methods on the IT168 test set with YahooTranslate.Positive Negative TotalMethod P R F P R F AccLEX(CN) 0.615 0.772 0.684 0.678 0.499 0.575 0.638LEX(EN) 0.759 0.874 0.812 0.845 0.713 0.773 0.795SVM(CN) 0.728 0.814 0.769 0.780 0.685 0.729 0.751SVM(EN) 0.699 0.736 0.717 0.710 0.671 0.690 0.704SVM(ENCN) 0.735 0.792 0.762 0.765 0.703 0.733 0.748TSVM(CN) 0.762 0.809 0.785 0.789 0.738 0.762 0.774TSVM(EN) 0.820 0.776 0.797 0.780 0.823 0.801 0.799TSVM(ENCN) 0.816 0.818 0.817 0.811 0.809 0.810 0.814SelfTrain(CN) 0.733 0.767 0.750 0.746 0.710 0.728 0.739SelfTrain(EN) 0.799 0.827 0.813 0.814 0.784 0.799 0.806SelfTrain(ENCN) 0.788 0.823 0.805 0.807 0.770 0.788 0.797Ccn in CoTrain 0.807 0.836 0.821 0.823 0.793 0.808 0.815Cen in CoTrain 0.831 0.831 0.831 0.825 0.825 0.825 0.828CoTrain 0.828 0.845 0.836 0.836 0.818 0.827 0.832603Computational Linguistics Volume 37, Number 3Table 4Comparison results for cross-lingual methods on the IT168 test set with MicrosoftTranslate.Positive Negative TotalMethod P R F P R F AccLEX(CN) 0.615 0.772 0.684 0.678 0.499 0.575 0.638LEX(EN) 0.744 0.909 0.818 0.878 0.676 0.764 0.795SVM(CN) 0.669 0.925 0.777 0.871 0.526 0.656 0.729SVM(EN) 0.702 0.855 0.771 0.806 0.623 0.703 0.742SVM(ENCN) 0.694 0.905 0.785 0.856 0.586 0.696 0.748TSVM(CN) 0.801 0.865 0.832 0.847 0.777 0.811 0.822TSVM(EN) 0.789 0.745 0.766 0.750 0.793 0.771 0.769TSVM(ENCN) 0.812 0.854 0.832 0.840 0.795 0.817 0.825SelfTrain(CN) 0.759 0.851 0.803 0.824 0.720 0.768 0.787SelfTrain(EN) 0.785 0.776 0.780 0.770 0.779 0.775 0.778SelfTrain(ENCN) 0.802 0.860 0.830 0.843 0.779 0.810 0.821Ccn in CoTrain 0.818 0.858 0.838 0.845 0.802 0.823 0.831Cen in CoTrain 0.803 0.820 0.811 0.809 0.791 0.800 0.806CoTrain 0.829 0.874 0.851 0.861 0.814 0.837 0.844Among the baselines, the best baseline is TSVM(ENCN).
Actually, TSVM(ENCN) isvery similar to CoTrain, and it combines the results of two classifiers in the same way.However, the co-training approach can train two more effective component classifiersthan those used in TSVM(ENCN).
As suggested from the tables, the accuracy values ofthe component classifiers (Ccn and Cen) in CoTrain are almost always higher than thoseof the corresponding TSVM(CN) and TSVM(EN), based on any machine translationservice.
The reason is that TSVM(CN) and TSVM(EN) leverage the unlabeled dataindependently, while the two component classifiers in the co-training approach leveragethe unlabeled data in a mutual way, and more useful knowledge in the unlabeled datacan be incorporated into the co-training approach.
We can also see that the co-trainingapproach outperforms the baseline self-training approach, which further demonstratesthe great importance of the mutual influence of the two views during the bootstrappingphase.As mentioned in Section 4.2, the English classifier and the Chinese classifier in theco-training approach are required to disagree on some unlabeled examples, and weTable 5Results for monolingual methods on the 360BUY test set.Positive Negative TotalMethod P R F P R F AccBaseCN1 0.747 0.845 0.793 0.707 0.568 0.630 0.734BaseCN2 0.752 0.927 0.830 0.829 0.538 0.652 0.772BaseCN3 0.761 0.927 0.836 0.835 0.559 0.670 0.781UpperBound 0.880 0.946 0.912 0.909 0.805 0.854 0.890(CrossValidation)604Wan Bilingual Co-Training for Sentiment Classification of Chinese Product ReviewsTable 6Comparison results for cross-lingual methods on the 360BUY test set with GoogleTranslate.Positive Negative TotalMethod P R F P R F AccLEX(CN) 0.714 0.895 0.794 0.741 0.457 0.565 0.720LEX(EN) 0.761 0.864 0.809 0.741 0.589 0.657 0.755SVM(CN) 0.791 0.825 0.808 0.717 0.670 0.693 0.763SVM(EN) 0.765 0.795 0.779 0.670 0.630 0.649 0.729SVM(ENCN) 0.801 0.834 0.817 0.732 0.686 0.709 0.775TSVM(CN) 0.784 0.877 0.828 0.773 0.635 0.697 0.781TSVM(EN) 0.824 0.818 0.821 0.727 0.735 0.731 0.785TSVM(ENCN) 0.826 0.857 0.841 0.771 0.727 0.748 0.805SelfTrain(CN) 0.791 0.861 0.825 0.757 0.657 0.703 0.780SelfTrain(EN) 0.797 0.813 0.805 0.708 0.686 0.697 0.762SelfTrain(ENCN) 0.814 0.850 0.831 0.757 0.705 0.730 0.792Ccn in CoTrain 0.849 0.877 0.863 0.804 0.765 0.784 0.832Cen in CoTrain 0.816 0.834 0.825 0.740 0.716 0.728 0.787CoTrain 0.846 0.884 0.865 0.812 0.757 0.783 0.833show the disagreement ratio between the two classifiers at each iteration in Figure 6.At each iteration in the co-training algorithm, we use the two classifiers to predictthe polarity tags of the unlabeled examples, respectively.
The disagreement ratio iscomputed by dividing the number of the consistently predicted examples by the sizeof the unlabeled set.
We can see from the figure that the disagreement ratio is alwayshigher than 20%, which guarantees the success of the co-training approach.Table 7Comparison results for cross-lingual methods on the 360BUY test set with YahooTranslate.Positive Negative TotalMethod P R F P R F AccLEX(CN) 0.714 0.895 0.794 0.741 0.457 0.565 0.720LEX(EN) 0.783 0.846 0.814 0.735 0.646 0.688 0.767SVM(CN) 0.798 0.816 0.807 0.711 0.686 0.699 0.765SVM(EN) 0.757 0.786 0.771 0.656 0.619 0.637 0.719SVM(ENCN) 0.793 0.834 0.813 0.727 0.670 0.698 0.769TSVM(CN) 0.806 0.854 0.829 0.757 0.689 0.721 0.788TSVM(EN) 0.822 0.839 0.830 0.749 0.724 0.736 0.794TSVM(ENCN) 0.832 0.877 0.854 0.797 0.732 0.763 0.819SelfTrain(CN) 0.804 0.848 0.825 0.749 0.686 0.717 0.784SelfTrain(EN) 0.804 0.830 0.817 0.730 0.695 0.712 0.776SelfTrain(ENCN) 0.820 0.861 0.840 0.772 0.714 0.742 0.802Ccn in CoTrain 0.857 0.857 0.857 0.784 0.784 0.784 0.828Cen in CoTrain 0.830 0.845 0.837 0.758 0.738 0.748 0.802CoTrain 0.869 0.866 0.868 0.798 0.803 0.801 0.841605Computational Linguistics Volume 37, Number 3Table 8Comparison results for cross-lingual methods on the 360BUY test set with MicrosoftTranslate.Positive Negative TotalMethod P R F P R F AccLEX(CN) 0.714 0.895 0.794 0.741 0.457 0.565 0.720LEX(EN) 0.749 0.864 0.803 0.732 0.562 0.636 0.744SVM(CN) 0.730 0.904 0.808 0.772 0.495 0.603 0.741SVM(EN) 0.727 0.795 0.759 0.638 0.549 0.590 0.697SVM(ENCN) 0.737 0.896 0.809 0.767 0.516 0.617 0.745TSVM(CN) 0.845 0.875 0.860 0.800 0.757 0.778 0.828TSVM(EN) 0.810 0.700 0.751 0.623 0.751 0.681 0.720TSVM(ENCN) 0.856 0.839 0.848 0.764 0.786 0.775 0.818SelfTrain(CN) 0.787 0.879 0.830 0.777 0.641 0.702 0.784SelfTrain(EN) 0.792 0.739 0.765 0.641 0.705 0.672 0.726SelfTrain(ENCN) 0.823 0.870 0.845 0.784 0.716 0.749 0.809Ccn in CoTrain 0.834 0.886 0.859 0.809 0.732 0.769 0.825Cen in CoTrain 0.800 0.779 0.789 0.678 0.705 0.691 0.749CoTrain 0.843 0.884 0.863 0.810 0.751 0.780 0.831For the three lexicon-based baseline methods (BaseCN1, LEX(CN), and LEX(EN)),the LEX(EN) method performs better than the BaseCN1 method, but the LEX(CN)method performs worse than the BaseCN1 method.
The reason is that the sentimentlexica used in LEX(CN) are automatically translated from the original English lexica,and the translation is very inaccurate because there are no contexts or clues for sensedisambiguation during the translation process.For the three monolingual baseline methods (BaseCN1, BaseCN2, and BaseCN3),the BaseCN2 and BaseCN3 methods outperform the BaseCN1 method.
However, theBaseCN2 and BaseCN3 methods cannot outperform the strong cross-lingual baselineTable 9p-values for sign tests between the results of CoTrain and baseline methods on the IT168 test set.GoogleTranslate YahooTranslate MicrosoftTranslateCoTrain vs. BaseCN1 2.85E-12 1.04E-06 1.82E-08CoTrain vs. BaseCN2 1.8E-07 0.00257 0.000182CoTrain vs. BaseCN3 1.27E-06 0.00922 0.000765CoTrain vs. LEX(CN) 6.09E-29 3.72E-21 1.61E-24CoTrain vs. LEX(EN) 0.0018 0.0276 0.00329CoTrain vs. SVM(CN) 1.26E-13 6.45E-10 2.7E-14CoTrain vs. SVM(EN) 2.15E-18 1.1E-17 3.46E-13CoTrain vs. SVM(ENCN) 2.08E-13 8.13E-12 1.05E-12CoTrain vs. TSVM(CN) 1.2E-19 1.07E-06 0.0624CoTrain vs. TSVM(EN) 1.41E-05 0.00311 2.17E-08CoTrain vs. TSVM(ENCN) 1.37E-08 0.0113 0.0396CoTrain vs. SelfTrain(CN) 7.07E-18 2.79E-11 6.53E-07CoTrain vs. SelfTrain(EN) 1.01E-07 0.0192 1.35E-07CoTrain vs. SelfTrain(ENCN) 6.4E-11 0.000194 0.000508606Wan Bilingual Co-Training for Sentiment Classification of Chinese Product ReviewsTable 10p-values for sign tests between the results of CoTrain and baseline methods on the 360BUYtest set.GoogleTranslate YahooTranslate MicrosoftTranslateCoTrain vs. BaseCN1 6.01E-08 4.45E-09 2.1E-07CoTrain vs. BaseCN2 0.000144 4.77E-05 0.000247CoTrain vs. BaseCN3 0.0009 0.000287 0.00139CoTrain vs. LEX(CN) 9.53E-10 7.15E-11 1.17E-09CoTrain vs. LEX(EN) 1.87E-05 1.64E-05 8.92E-07CoTrain vs. SVM(CN) 5.7E-08 2.91E-09 2.27E-11CoTrain vs. SVM(EN) 3.74E-15 5.77E-17 1.18E-20CoTrain vs. SVM(ENCN) 8.07E-09 3.76E-10 1.31E-12CoTrain vs. TSVM(CN) 2.38E-05 1.28E-05 0.838CoTrain vs. TSVM(EN) 4.27E-06 2.48E-05 1.78E-14CoTrain vs. TSVM(ENCN) 0.000306 0.00779 0.0884CoTrain vs. SelfTrain(CN) 1.08E-05 3.31E-06 5.64E-05CoTrain vs. SelfTrain(EN) 1.85E-10 2.48E-08 5.17E-14CoTrain vs. SelfTrain(ENCN) 4.52E-07 2.57E-05 0.00107methods (e.g., TSVM(ENCN), SelfTrain(ENCN)), because the Chinese training corpusis automatically collected without human checking and thus about 10% of the reviewsare mistakenly labeled.
Moreover, the corpus is collected from a different Web site, andthus the training set and the test set may be in different domains.
We also note that nomethods can outperform the monolingual upper bound (the cross-validation method),because it leverages in-domain human-labeled training set for model learning.Given any machine translation service, the transductive SVM classifiers can al-most always outperform the corresponding inductive SVM classifiers on the two testsets.
More specifically, the BaseCN3 method outperforms the BaseCN2 method; theTSVM(CN), TSVM(EN), and TSVM(ENCN) methods almost always outperform theSVM(CN), SVM(EN), and SVM(ENCN) methods, respectively, except that TSVM(CN)cannot outperform SVM(CN) on the IT168 test set with GoogleTranslate.
In most cases,SelfTrain(CN), SelfTrain(EN), and SelfTrain(ENCN) can outperform the SVM(CN),Figure 6The disagreement ratio between the two component classifiers on the remaining unlabeledexamples at each iteration for co-training (p = n = 5) with GoogleTranslate.607Computational Linguistics Volume 37, Number 3SVM(EN), and SVM(ENCN) methods, respectively.
The results demonstrate that theuse of unlabeled reviews is beneficial to the classification task.Overall, the use of unlabeled data and the combination of English and Chineseviews are beneficial to the final classification accuracy, and the co-training approachis more suitable for making use of the unlabeled Chinese reviews than the transductiveSVM and the self-training approach.Moreover, we find that the three machine translation services perform differently onthe two test sets, and no particular service can always outperform the other two serviceson the two test sets.
Although machine translation is very important in the proposedmethods, the quality of the three machine translation services offers no significantdifferences.6.2 Influences of Iteration Number (I)Figures 7 and 8 show the accuracy curves of the co-training approach and two strongbaselines (SVM(ENCN) and SelfTrain(ENCN)) with respect to different numbers ofiterations on the two test sets with GoogleTranslate, respectively.
The parameter valuesfor CoTrain and SelfTrain are set as p = n = 5.
The iteration number I varies from 1 to100.
When I is set to 1, both the co-training approach and the self-training approach de-generate into SVM(ENCN).
The accuracy curves of the component English and Chineseclassifiers learned in the co-training approach are also shown in the figures.
We omit thevery similar figures obtained with YahooTranslate and MicrosoftTranslate.We can see that the proposed co-training approach (CoTrain) can outperform thetwo strong baselines after a few iterations.
After a large number of iterations, theperformance of the co-training approach does not rise any more, because the algorithmruns out of all useful examples in the unlabeled set.
The performance finally has a slightdecline because some noisy training examples may be selected from the remainingunlabeled set.
Fortunately, the proposed approach performs well with a wide rangeof iteration values.We can also see that the two component classifiers show a similar trend to the co-training approach.
It is encouraging that either the component English classifier or thecomponent Chinese classifier alone can perform better than the strong baselines after aFigure 7Accuracy vs. number of iterations for co-training and baselines (p = n = 5) on the IT168 test setwith GoogleTranslate.608Wan Bilingual Co-Training for Sentiment Classification of Chinese Product ReviewsFigure 8Accuracy vs. number of iterations for co-training and baselines (p = n = 5) on the 360BUY testset with GoogleTranslate.few iterations.
The results show that the effectiveness of the co-training approach canbe attributed to the effectiveness of its two component classifiers.6.3 Influences of Growth Size (p, n)Figures 9 and 10 show how the growth size at each iteration (p positive and n negativeconfident examples) influences the accuracy of the proposed co-training approach onthe two test sets with GoogleTranslate, respectively.
In these experiments, we set p = n,which is considered a balanced growth.
When p differs very much from n, the growth isconsidered unbalanced.
Balanced growth of (2, 2), (5, 5), (10, 10), and (15, 15) examplesand unbalanced growth of (1, 5), (5, 1), (1, 10), and (10, 1) examples are comparedin the figures.
We omit the very similar figures obtained with YahooTranslate andMicrosoftTranslate.We can see that the performance of the co-training approach with balanced growthcan be improved after a few iterations.
The performance of the co-training approachwith larger p = n will rise more sharply, because the approach can make use of moreFigure 9Accuracy vs. different (p, n) for co-training on the IT168 test set with GoogleTranslate.609Computational Linguistics Volume 37, Number 3Figure 10Accuracy vs. different (p, n) for co-training on the 360BUY test set with GoogleTranslate.selected examples to improve the classifiers at each iteration.
Also, the performance ofthe co-training approach with larger p = n will become stable more quickly, because theapproach runs out of the limited examples in the unlabeled set more quickly.The performance of the co-training approaches with the unbalanced growth alwaysdeclines quite rapidly, however, because the selected unbalanced examples hurt theperformance at each iteration.
We also find that the more p is different from n, the fasterthe performance declines.
Actually, in the generic text categorization task, unbalancedtraining data will lead to poor classification results (Japkowicz and Stephen 2002).Overall, the growth size has a great impact on the final performance.
A balancedgrowth can lead to performance improvement, but an unbalanced growth can hurt thefinal performance.6.4 Influences of Feature SelectionIn these experiments, all features (unigrams + bigrams) are used.
As mentioned earlier,feature selection techniques are widely used for dimensionality reduction.
In this sec-tion, we conduct further experiments to investigate the influences of feature selectiontechniques on the classification results.
We use the simple but effective DF for featureselection.
Figures 11 and 12 show the comparison results of different feature sizes forthe co-training approach and two baselines on the two test sets with GoogleTranslate,respectively.
The feature size is measured as the proportion of the selected featuresagainst the total features (i.e., 100%), and we select 10%, 25%, and 50% features inthe experiments.
We omit the very similar figures obtained with YahooTranslate andMicrosoftTranslate.We can see from the figures that the feature selection technique has a very slightinfluence on the classification accuracy of each individual method.
This can be ex-plained by the fact that sentiment classification is different from topic-based text clas-sification, and the useful feature sets for the two classification tasks are very different.The popular feature selection techniques are helpful for topic-based text classification,but they cannot select good features for sentiment classification.
Though the featureselection techniques cannot improve the sentiment classification accuracy significantly,they can reduce the feature size to 10% while not significantly lowering the classificationaccuracy.
The large reduction of feature size can improve system efficiency.610Wan Bilingual Co-Training for Sentiment Classification of Chinese Product ReviewsFigure 11Influences of feature selection on the IT168 test set with GoogleTranslate.Figure 12Influences of feature selection on the 360Buy test set with GoogleTranslate.More importantly we can see that the TSVM and SelfTrain baselines always outper-form the inductive SVM baseline, and the co-training approach can always outperformall the three baselines with different feature sizes.
The results further demonstrate theeffectiveness and robustness of the proposed co-training approach.6.5 Influences of Different Training SetsIn the experiments, the training set provided by Blitzer, Dredze, and Pereira (2007)is a very balanced set (4,000 positive reviews + 4,000 negative reviews).
In this sec-tion, we sample the following two training sets from the original set: One trainingset consists of 4,000 positive reviews and randomly selected 2,000 negative reviews(#pos:#neg=2:1), and the other training set consists of 2,000 randomly selected positivereviews and 4,000 negative reviews (#pos:#neg=1:2).
The two sampled training sets arenot balanced.
The proposed co-training approach is compared with the three strongbaselines (SVM(ENCN), TSVM(ENCN), and SelfTrain(ENCN)) on the two training sets.Figures 13 and 14 show the comparison results on the two training sets, respectively.We can see that based on the two training sets, our proposed co-training approach can611Computational Linguistics Volume 37, Number 3Figure 13Comparison results based on one sampled training set (#pos:#neg=2:1) with GoogleTranslate.Figure 14Comparison results based on the second sampled training set (#pos:#neg=1:2) withGoogleTranslate.consistently outperform all three baselines, which further demonstrates the robustnessof our proposed approach.7.
Conclusion and DiscussionIn this article, we proposed to use the co-training approach to address the problemof cross-lingual sentiment classification.
The approach leverages only labeled Englishreviews and unlabeled Chinese reviews for Chinese sentiment classification.
First, thelabeled English reviews are translated into labeled Chinese reviews by using English-to-Chinese machine translation services, and the unlabeled Chinese reviews are trans-lated into unlabeled English reviews by using Chinese-to-English machine translationservices.
The English view and the Chinese view are considered two redundant views.Then, the co-training algorithm is employed to learn two component classifiers in thetwo views by mutually helping each other.
Finally, given a test Chinese review and itstranslated English review, the two classifiers are used to obtain two prediction values,and the final polarity tag of the review is decided by the average of the two predictionvalues.612Wan Bilingual Co-Training for Sentiment Classification of Chinese Product ReviewsIn the experiments, three machine translation services and two test sets are usedfor evaluation, and the evaluation results show the overall effectiveness and robustnessof the proposed co-training approach.
The approach can significantly outperform thelexicon-based baselines, the inductive classification baselines, the transductive classifi-cation baselines, and the self-training baselines.
We also find that the growth size (i.e.,the numbers of positive and negative examples selected in the labeled data) is a veryimportant factor in the proposed approach, which has great influence over the finalperformance.
In particular, a balanced growth leads to performance improvement, butan unbalanced growth hurts the final performance.Though we focus on English-to-Chinese cross-language sentiment classification inthis study, the proposed approach can be easily applied to cross-language sentimentclassification in other languages, because the three machine translation services covermany of the most frequently used language pairs.
For most western languages, featureextraction is very easy because word segmentation is not required.
However, for someAsian languages (e.g., Japanese, Korean), the step of word segmentation is requiredin order to split a text into words, and thus a word segmentation tool for the specificlanguage is necessary.
Fortunately, with the progress of NLP research, word segmenta-tion tools with good performance can be easily obtained for each specific language, andunigram/bigram features can be easily extracted after word segmentation.The feature distributions of the translated text and the natural text in the samelanguage are still different due to the inaccuracy of the machine translation serviceand the domain difference between the training set and the test set.
In future work,we will try to develop advanced methods to minimize the feature gap in the two reviewsets.
Moreover, we will translate both English and Chinese reviews into a few otherlanguages, and then exploit the multi-view learning techniques for making use of themultiple views in different languages.AcknowledgmentsThis work was supported by the NationalNatural Science Foundation of China underGrant No.
60873155, the Beijing NovaProgram under Grant No.
2008B03, and theMOE Program for New Century ExcellentTalents in University under Grant No.NCET-08-0006.
We are very grateful tothe anonymous reviewers for theirinsightful and constructive commentsand suggestions.ReferencesAbney, Steven P. 2002.
Bootstrapping.
InProceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics(ACL), pages 360?367, Philadelphia, PA.Andreevskaia, Alina and Sabine Bergler.2008.
When specialists and generalistswork together: Overcoming domaindependence in sentiment tagging.
InProceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics:Human Language Technologies (ACL-08:HLT), pages 290?298, Columbus, OH.Amini, Massih-Reza and Cyril Goutte.
2010.A co-classification approach to learningfrom multilingual corpora.
MachineLearning Journal, 79(1-2):105?121.Amini, Massih R., Nicolas Usunier, andCyril Goutte.
2009.
Learning from multiplepartially observed views?an applicationto multilingual text categorization.
InProceedings of the 23rd Annual Conferenceon Neural Information Processing Systems(NIPS), pages 28?36, Vancouver.Balcan, Maria-Florina, Avrim Blum, andKe Yang.
2005.
Co-training and expansion:Towards bridging theory and practice.In Proceedings of the Nineteenth AnnualConference on Neural Information ProcessingSystems (NIPS), pages 89?96, Vancouver.Banea, Carmen, Rada Mihalcea, JanyceWiebe, and Samer Hassan.
2008.Multilingual subjectivity analysis usingmachine translation.
In Proceedings of the2008 Conference on Empirical Methods inNatural Language Processing (EMNLP),pages 127?135, Waikiki, HI.Bel, Nuria, Cornelis H. A. Koster, andMarta Villegas.
2003.
Cross-lingual textcategorization.
In Proceedings of the Seventh613Computational Linguistics Volume 37, Number 3European Conference on Research andAdvanced Technology for Digital Libraries(ECDL), pages 126?139, Trondheim.Blitzer, John, Mark Dredze, and FernandoPereira.
2007.
Biographies, bollywood,boom-boxes and blenders: Domainadaptation for sentiment classification.In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics(ACL), pages 440?447, Prague.Blum, Avrim and Tom Mitchell.
1998.Combining labeled and unlabeleddata with co-training.
In Proceedingsof the Eleventh Annual Conference onComputational Learning Theory (COLT),pages 92?100, Madison, WI.Clark, Stephen, James R. Curran, andMiles Osborne.
2003.
BootstrappingPOS taggers using unlabelled data.In Proceedings of the 2003 Conference onComputational Natural Language Learning(CoNLL), pages 49?55, Edmonton.Dai, Wenyuan, Gui-Rong Xue, Qiang Yang,and Yong Yu.
2007a.
Transferring na?
?veBayes classifiers for text classification.In Proceedings of the 22nd AAAI Conferenceon Artificial Intelligence (AAAI),pages 540?545, Vancouver.Dai, Wenyuan, Gui-Rong Xue, Qiang Yang,and Yong Yu.
2007b.
Co-clusteringbased classification for out-of-domaindocuments.
In Proceedings of the 13thACM SIGKDD International Conference onKnowledge Discovery and Data Mining(KDD), pages 210?219, San Jose, CA.Dasgupta, Sajib and Vincent Ng.
2009.Mine the easy, classify the hard:A semi-supervised approach to automaticsentiment classification.
In Proceedings ofthe 47th Annual Meeting of the ACL and the4th IJCNLP of the AFNLP (ACL-IJCNLP),pages 701?709, Suntec.Daume?
III, Hal and Daniel Marcu.
2006.Domain adaptation for statisticalclassifiers.
Journal of Artificial IntelligenceResearch, 26(1):101?126.Devitt, Ann and Khurshid Ahmad.
2007.Sentiment polarity identification infinancial news: a cohesion-basedapproach.
In Proceedings of the 45thAnnual Meeting of the Association ofComputational Linguistics (ACL),pages 984?991, Prague.Gliozzo, Alfio and Carlo Strapparava.
2005.Cross language text categorization byacquiring multilingual domain modelsfrom comparable corpora.
In Proceedings ofthe ACL Workshop on Building and UsingParallel Texts, pages 9?16, Ann Arbor, MI.Japkowicz, Nathalie and Shaju Stephen.2002.
The class imbalance problem:A systematic study.
Intelligent DataAnalysis, 6(5): 429?449.Jiang, Jing and ChengXiang Zhai.
2007.A two-stage approach to domainadaptation for statistical classifiers.In Proceedings of the Sixteenth ACMConference on Information and KnowledgeManagement (CIKM), pages 401?410,Lisbon.Joachims, Thorsten.
1999.
Transductiveinference for text classification usingsupport vector machines.
In Proceedingsof the Sixteenth International Conference onMachine Learning (ICML), pages 200?209,Bled.Joachims, Thorsten.
2002.
Learning to ClassifyText Using Support Vector Machines:Methods, Theory and Algorithms.
KluwerAcademic Publishers, Norwell, MA.Kanayama, Hiroshi, Tetsuya Nasukawa, andHideo Watanabe.
2004.
Deeper sentimentanalysis using machine translationtechnology.
In Proceedings of the 20thInternational Conference on ComputationalLinguistics (COLING), pages 494?500,Geneva.Kennedy, Alistair and Diana Inkpen.2006.
Sentiment classification of moviereviews using contextual valenceshifters.
Computational Intelligence,22(2):110?125.Kim, Soo-Min and Eduard Hovy.
2004.Determining the sentiment of opinions.In Proceedings of the 20th InternationalConference on Computational Linguistics(COLING), pages 1367?1373, Geneva.Kiritchenko, Svetlana and Stan Matwin.2001.
Email classification with co-training.In Proceedings of the 2001 Conference of theCentre for Advanced Studies on CollaborativeResearch (CASCON), pages 192?201,Toronto.Ku, Lun-Wei, Yu-Ting Liang, andHsin-Hsi Chen.
2006.
Opinion extraction,summarization and tracking in newsand blog corpora.
In Proceedings ofthe AAAI-2006 Spring Symposium onComputational Approaches to AnalyzingWeblogs (AAAI-CAAW), pages 100?107,Stanford, CA.Li, Jun and Maosong Sun.
2007.
Experimentalstudy on sentiment classification ofChinese review using machine learningtechniques.
In Proceedings of theInternational Conference on Natural LanguageProcessing and Knowledge Engineering(NLP-KE), pages 393?400, Beijing.614Wan Bilingual Co-Training for Sentiment Classification of Chinese Product ReviewsLi, Tao, Yi Zhang, and Vikas Sindhwani.2009.
A non-negative matrixtri-factorization approach to sentimentclassification with lexical prior knowledge.In Proceedings of the 47th Annual Meeting ofthe ACL and the 4th IJCNLP of the AFNLP(ACL-IJCNLP), pages 244?252, Suntec.Ling, Xiao, Gui-Rong Xue, Wenyuan Dai,Yun Jiang, Qiang Yang, and Yong Yu.
2008.Can Chinese web pages be classified withEnglish data sources?
In Proceedings of the17th International Conference on World WideWeb (WWW), pages 969?978, Beijing.Liu, Bing, Minqing Hu, and Junsheng Cheng.2005.
Opinion observer: Analyzingand comparing opinions on the web.In Proceedings of the 14th InternationalConference on World Wide Web (WWW),pages 342?351, Chiba.Lopez, Adam.
2008.
Statistical machinetranslation.
ACM Computing Surveys,40(3), pages 1?49.McDonald, Ryan, Kerry Hannan, TylerNeylon, Mike Wells, and Jeff Reynar.
2007.Structured models for fine-to-coarsesentiment analysis.
In Proceedings of the45th Annual Meeting of the Association ofComputational Linguistics (ACL),pages 432?439, Prague.Mihalcea, Rada.
2004.
Co-trainingand self-training for word sensedisambiguation.
In Proceedings of theEighth Conference on ComputationalNatural Language Learning (CONLL),pages 33?40, Boston, MA.Mihalcea, Rada, Carmen Banea, andJanyce Wiebe.
2007.
Learning multilingualsubjective language via cross-lingualprojections.
In Proceedings of the 45thAnnual Meeting of the Association ofComputational Linguistics (ACL),pages 976?983, Prague.Mullen, Tony and Nigel Collier.
2004.Sentiment analysis using support vectormachines with diverse informationsources.
In Proceedings of the 2004Conference on Empirical Methods inNatural Language Processing (EMNLP),pages 412?418, Barcelona.Ng, Vincent and Claire Cardie.
2003.
Weaklysupervised natural language learningwithout redundant views.
In Proceedingsof the 2003 Human Language TechnologyConference of the North American Chapter ofthe Association for Computational Linguistics(HLT-NAACL), pages 94?101, Edmonton.Nigam, Kamal, Andrew K. McCallum,Sebastian Thrun, and Tom Mitchell.
2000.Text classification from labeled andunlabeled documents using EM.
MachineLearning, 39(2-3):103?134.Pan, Sinno J., Xiaochuan Ni, Jian-Tao Sun,Qiang Yang, and Zheng Chen.
2010.Cross-domain sentiment classification viaspectral feature alignment.
In Proceedingsof the 19th International Conference onWorld Wide Web (WWW), pages 751?760,Raleigh, NC.Pang, Bo, Lillian Lee, and ShivakumarVaithyanathan.
2002.
Thumbs up?Sentiment classification using machinelearning techniques.
In Proceedings of the2002 Conference on Empirical Methods inNatural Language Processing (EMNLP),pages 79?86, Philadelphia, PA.Pang, Bo and Lillian Lee.
2004.
A sentimentaleducation: Sentiment analysis usingsubjectivity summarization based onminimum cuts.
In Proceedings of the 42ndAnnual Meeting of the Association forComputational Linguistics (ACL),pages 271?278, Barcelona.Prettenhofer, Peter and Benno Stein.
2010.Cross-language text classfication usingstructural correspondence learning.
InProceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics(ACL), pages 1118?1127, Uppsala.Read, Jonathon.
2005.
Using emoticonsto reduce dependency in machinelearning techniques for sentimentclassification.
In Proceedings of the ACLStudent Research Workshop, pages 43?48,Ann Arbor, MI.Rigutini, Leonardo, Marco Maggini, andBing Liu.
2005.
An EM based trainingalgorithm for cross-language textcategorization.
In Proceedings of the 2005IEEE/WIC/ACM International Conferenceon Web Intelligence (WI), pages 529?535,Compiegne.Riloff, Ellen and Janyce Wiebe.
2003.Learning extraction patterns for subjectiveexpressions.
In Proceedings of the 2003Conference on Empirical Methods inNatural Language Processing (EMNLP),pages 105?112, Stroudsburg, PA.Sarkar, Anoop.
2001.
Applying co-trainingmethods to statistical parsing.
InProceedings of the Second Meeting of theNorth American Chapter of the Associationfor Computational Linguistics (NAACL),pages 175?182, Pittsburgh, PA.Shi, Lei, Rada Mihalcea, and Mingjun Tian.2010.
Cross language text classification bymodel translation and semi-supervisedlearning.
In Proceedings of the 2010Conference on Empirical Methods in615Computational Linguistics Volume 37, Number 3Natural Language Processing (EMNLP),pages 1057?1067, Cambridge, MA.Stone, Philip J., Dexter C. Dunphy, MarshallS.
Smith, and Daniel M. Ogilvie.
1966.
TheGeneral Inquirer: A Computer Approach toContent Analysis.
The MIT Press,Cambridge, MA.Titov, Ivan and Ryan McDonald.
2008.
Ajoint model of text and aspect ratings forsentiment summarization.
In Proceedings ofthe 46th Annual Meeting of the Association forComputational Linguistics: Human LanguageTechnologies (ACL-08: HLT), pages 308?316,Columbus, OH.Turney, Peter.
2002.
Thumbs up or thumbsdown?
Semantic orientation applied tounsupervised classification of reviews.In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 417?424, Philadelphia, PA.Wan, Xiaojun.
2008.
Using bilingualknowledge and ensemble techniques forunsupervised Chinese sentiment analysis.In Proceedings of the 2008 Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 553?561,Honolulu, HI.Wan, Xiaojun.
2009.
Co-Training forcross-lingual sentiment classification.In Proceedings of the 47th Annual Meeting ofthe ACL and the 4th IJCNLP of the AFNLP(ACL-IJCNLP), pages 235?243, Suntec.Wang, Wei and Zhi-Hua Zhou.
2010.
A newanalysis of co-training.
In Proceedings of the27th International Conference on MachineLearning (ICML), pages 1135?1142, Haifa.Wei, Bin and Christopher Pal.
2010.Cross-lingual adaptation: an experimenton sentiment classifications.
In Proceedingsof the 48th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 258?262, Uppsala.Wilson, Theresa, Janyce Wiebe, and PaulHoffmann.
2005.
Recognizing contextualpolarity in phrase-level sentiment analysis.In Proceedings of Human LanguageTechnology Conference and Conference onEmpirical Methods in Natural LanguageProcessing (HLT-EMNLP), pages 347?354,Vancouver.Wilson, Theresa, Paul Hoffmann, SwapnaSomasundaran, Jason Kessler, JanyceWiebe, Yejin Choi, Claire Cardie, EllenRiloff, and Siddharth Patwardhan.
2005.OpinionFinder: A system for subjectivityanalysis.
In Proceedings of HLP/EMNLP onInteractive Demonstrations, pages 34?35,Vancouver.Wu, Qiong, Songbo Tan, Haijun Zhai,Gang Zhang, Miyi Duan, and XueqiCheng.
2009.
SentiRank: Cross-domaingraph ranking for sentiment classification.In Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference onWeb Intelligence and Intelligent AgentTechnology, pages 309?314, Milan.Xue, Gui-Rong, Wenyuan Dai, Qiang Yang,and Yong Yu.
2008.
Topic-bridged PLSAfor cross-domain text classification.
InProceedings of the 31st Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval(SIGIR), pages 627?634, Singapore.Yang, Yiming and Xin Liu.
1999.
Are-examination of text categorizationmethods.
In Proceedings of the 22ndAnnual International ACM SIGIRConference on Research and Developmentin Information Retrieval (SIGIR),pages 42?49, Berkeley, CA.616
