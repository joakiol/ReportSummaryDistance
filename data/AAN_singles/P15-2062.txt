Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 377?383,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAn Empirical Study of Chinese Name Matching and ApplicationsNanyun Peng1and Mo Yu2and Mark Dredze11Human Language Technology Center of ExcellenceCenter for Language and Speech ProcessingJohns Hopkins University, Baltimore, MD, 212182Machine Intelligence and Translation LabHarbin Institute of Technology, Harbin, Chinanpeng1@jhu.edu, gflfof@gmail.com, mdredze@cs.jhu.eduAbstractMethods for name matching, an importantcomponent to support downstream taskssuch as entity linking and entity clustering,have focused on alphabetic languages, pri-marily English.
In contrast, logogram lan-guages such as Chinese remain untested.We evaluate methods for name matchingin Chinese, including both string match-ing and learning approaches.
Our ap-proach, based on new representations forChinese, improves both name matchingand a downstream entity clustering task.1 IntroductionA key technique in entity disambiguation is namematching: determining if two mention stringscould refer to the same entity.
The challengeof name matching lies in name variation, whichcan be attributed to many factors: nicknames,aliases, acronyms, and differences in translitera-tion, among others.
In light of these issues, exactstring match can lead to poor results.
Numerousdownstream tasks benefit from improved namematching: entity coreference (Strube et al, 2002),name transliteration (Knight and Graehl, 1998),identifying names for mining paraphrases (Barzi-lay and Lee, 2003), entity linking (Rao et al,2013) and entity clustering (Green et al, 2012).As a result, there have been numerous proposedname matching methods (Cohen et al, 2003), witha focus on person names.
Despite extensive explo-ration of this task, most work has focused on Indo-European languages in general and English in par-ticular.
These languages use alphabets as repre-sentations of written language.
In contrast, otherlanguages use logograms, which represent a wordor morpheme, the most popular being Chinesewhich uses hanzi (??).
This presents challengesfor name matching: a small number of hanzi repre-sent an entire name and there are tens of thousandsof hanzi in use.
Current methods remain largelyuntested in this setting, despite downstream tasksin Chinese that rely on name matching (Chen etal., 2010; Cassidy et al, 2011).
Martschat et al(2012) point out errors in coreference resolutiondue to Chinese name matching errors, which sug-gests that downstream tasks can benefit from im-provements in Chinese name matching techniques.This paper presents an analysis of new and ex-isting approaches to name matching in Chinese.The goal is to determine whether two Chinesestrings can refer to the same entity (person, orga-nization, location) based on the strings alone.
Themore general task of entity coreference (Soon etal., 2001), or entity clustering, includes the con-text of the mentions in determining coreference.
Incontrast, standalone name matching modules arecontext independent (Andrews et al, 2012; Greenet al, 2012).
In addition to showing name match-ing improvements on newly developed datasets ofmatched Chinese name pairs, we show improve-ments in a downstream Chinese entity clusteringtask by using our improved name matching sys-tem.
We call our name matching tool Mingpipe, aPython package that can be used as a standalonetool or integrated within a larger system.
We re-lease Mingpipe as well as several datasets to sup-port further work on this task.12 Name Matching MethodsName matching originated as part of research intorecord linkage in databases.
Initial work focused1The code and data for this paper are available at:https://github.com/hltcoe/mingpipe377on string matching techniques.
This work canbe organized into three major categories: 1) Pho-netic matching methods, e.g.
Soundex (Holmesand McCabe, 2002), double Metaphone (Philips,2000) etc.
; 2) Edit-distance based measures, e.g.Levenshtein distance (Levenshtein, 1966), Jaro-Winkler (Porter et al, 1997; Winkler, 1999),and 3) Token-based similarity, e.g.
soft TF-IDF(Bilenko et al, 2003).
Analyses comparing theseapproaches have not found consistent improve-ments of one method over another (Cohen et al,2003; Christen, 2006).
More recent work hasfocused on learning a string matching model onname pairs, such as probabilistic noisy channelmodels (Sukharev et al, 2014; Bilenko et al,2003).
The advantage of trained models is that,with sufficient training data, they can be tuned forspecific tasks.While many NLP tasks rely on name matching,research on name matching techniques themselveshas not been a major focus within the NLP com-munity.
Most downstream NLP systems have sim-ply employed a static edit distance module to de-cide whether two names can be matched (Chen etal., 2010; Cassidy et al, 2011; Martschat et al,2012).
An exception is work on training finitestate transducers for edit distance metrics (Ristadand Yianilos, 1998; Bouchard-C?ot?e et al, 2008;Dreyer et al, 2008; Cotterell et al, 2014).
Morerecently, Andrews et al (2012) presented a phylo-genetic model of string variation using transducersthat applies to pairs of names string (supervised)and unpaired collections (unsupervised).Beyond name matching in a single language,several papers have considered cross lingual namematching, where name strings are drawn fromtwo different languages, such as matching Arabicnames (El-Shishtawy, 2013) with English (Free-man et al, 2006; Green et al, 2012).
Addition-ally, name matching has been used as a componentin cross language entity linking (McNamee et al,2011a; McNamee et al, 2011b) and cross lingualentity clustering (Green et al, 2012).
However,little work has focused on logograms, with the ex-ception of Cheng et al (2011).
As we will demon-strate in ?
3, there are special challenges caused bythe logogram nature of Chinese.
We believe this isthe first evaluation of Chinese name matching.3 ChallengesNumerous factors cause name variations, includ-ing abbreviations, morphological derivations, his-Examples Notes???
v.s.
???
simplified v.s.
traditional??
v.s.
Abbreviation and traditional???????
v.s.
simplified??????
v.s.
Transliteration of Addis Ababa??????
in Mainland and Taiwan.
Dif-/ i2?ti?s1?i2?bei?b2 / ferent hanzi, similar pronuncia-v.s.
/ 2?ti?s1?2?bei?b2 / tions.????
v.s.
???
Transliteration of Florence in/ fo?luo?lu@n?s2 / Mainland and Hong Kong.
Dif-v.s.
/ fei?lEN?tsh8Y / ferent writing and dialects.???????
v.s.
???
Transliteration of Humphrey/ lu?fu?sW?xan?fu?laI / Rufus in Mainland and Hongv.s.
/ xan?lu?fu / Kong.
The first uses a literaltransliteration, while the seconddoes not.
Both reverse the nameorder (consistent with Chinesenames) and change the surnameto sound Chinese.Table 1: Challenges in Chinese name matching.torical sound or spelling change, loanword for-mation, translation, transliteration, or transcriptionerror (Andrews et al, 2012).
In addition to all theabove factors, Chinese name matching presentsunique challenges (Table 1):?
There are more than 50k Chinese characters.This can create a large number of parametersin character edit models, which can compli-cate parameter estimation.?
Chinese characters represent morphemes, notsounds.
Many characters can share a sin-gle pronunciation2, and many characters havesimilar sounds3.
This causes typos (mistak-ing characters with the same pronunciation)and introduces variability in transliteration(different characters chosen to represent thesame sound).?
Chinese has two writing systems (simplified,traditional) and two major dialects (Man-darin, Cantonese), with different pairings indifferent regions (see Table 2 for the threedominant regional combinations.)
This has asignificant impact on loanwords and translit-erations.2486 characters are pronounced / tCi / (regardless of tone).3e.g.
?
and ?
(different orthography) are pronouncedsimilar (/t?uAN/ and /t?AN /).378Region Writing System DialectHong Kong Traditional CantoneseMainland Simplified MandarinTaiwan Traditional MandarinTable 2: Regional variations for Chinese writingand dialect.4 MethodsWe evaluate several name matching methods,representative of the major approaches to namematching described above.String Matching We consider two commonstring matching algorithms: Levenshtein and Jaro-Winkler.
However, because of the issues men-tioned above we expect these to perform poorlywhen applied to Chinese strings.
We consider sev-eral transformations to improve these methods.First, we map all strings to a single writing sys-tem: simplified.
This is straightforward since tra-ditional Chinese characters have a many-to-onemapping to simplified characters.
Second, we con-sider a pronunciation based representation.
Weconvert characters to pinyin4, the official pho-netic system (and ISO standard) for transcribingMandarin pronunciations into the Latin alphabet.While pinyin is a common representation used inChinese entity disambiguation work (Feng et al,2004; Jiang et al, 2007), the pinyin for an en-tire entity is typically concatenated and treatedas a single string (?string-pinyin?).
However, thepinyin string itself has internal structure that maybe useful for name matching.
We consider twonew pinyin representations.
Since each Chinesecharacter corresponds to a pinyin, we take eachpinyin as a token corresponding to the Chinesecharacter.
We call this ?character-pinyin?.
Addi-tionally, every Mandarin syllable (represented bya pinyin) can be spelled with a combination of aninitial and a final segment.
Therefore, we spliteach pinyin token further into the initial and finalsegment.
We call this ?segmented-pinyin?5.Transducers We next consider methods that canbe trained on available Chinese name pairs.
Trans-ducers are common choices for learning edit dis-4Hong Kong has a romanization scheme more suitable forCantonese, but we found no improvements over using pinyin.Therefore, for simplicity we use pinyin throughout.5For example, the pinyin for ?
is segmented into / zh /and / ang /.tance metrics for strings, and they perform bet-ter than string similarity (Ristad and Yianilos,1998; Andrews et al, 2012; Cotterell et al, 2014).We use the probabilistic transducer of Cotterellet al (2014) to learn a stochastic edit distance.The model represent the conditional probabilityp(y|x; ?
), where y is a generated string based onediting x according to parameters ?.
At eachposition xi, one of four actions (copy, substi-tute, insert, delete) are taken to generate charac-ter yj.
The probability of each action dependson the string to the left of xi(x(i?N1):i), thestring to the right of xi(xi:(i+N2)), and gener-ated string to the left of yj(y(j?N3):j).
The vari-ables N1, N2, N3are the context size.
Note thatcharacters to the right of yjare excluded as theyare not yet generated.
Training maximizes theobserved data log-likelihood and EM is used tomarginalize over the latent edit actions.
Since thelarge number of Chinese characters make param-eter estimation prohibitive, we only train trans-ducers on the three pinyin representations: string-pinyin (28 characters), character-pinyin (384 char-acters), segmented-pinyin (59 characters).Name Matching as Classification An alternatelearning formulation considers name matching asa classification task (Mayfield et al, 2009; Zhanget al, 2010; Green et al, 2012).
Each string pairis an instance: a positive classification means thattwo strings can refer to the same name.
This al-lows for arbitrary and global features of the twostrings.
We use an SVM with a linear kernel.To learn possible edit rules for Chinese nameswe add features for pairs of n-grams.
For eachstring, we extract all n-grams (n=1,2,3) and alignn-grams between strings using the Hungarian al-gorithm.6Features correspond to the aligned n-gram pairs, as well as the unaligned n-grams.To reduce the number of parameters, we onlyinclude features which appear in positive train-ing examples.
These features are generated fortwo string representations: the simplified Chinesestring (simplified n-grams) and a pinyin repre-sentation (pinyin n-grams), so that we can in-corporate both orthographic features and phoneticfeatures.
We separately select the best perform-ing pinyin representation (string-pinyin, character-pinyin, segmented-pinyin) on development data6We found this performed much better than directly align-ing characters or tokens.
We also tried n-gram TF-IDF cosinesimilarity, but it degraded results (Cohen et al, 2003).379Feature Type Number of FeaturesSimplified n-grams ~10kPinyin n-grams ~9kJaccard similarity 6 ?
10TF-IDF similarity 2 ?
10Levenshtein distance 2 ?
10Other 7Table 3: Features for SVM learning.for each dataset.We measure Jaccard similarity between thetwo strings separately for 1,2,3-grams for eachstring representation.
An additional feature in-dicates no n-gram overlap.
The best performingLevenshtein distance metric is included as a fea-ture.
Finally, we include other features for severalname properties: the difference in character lengthand two indicators as to whether the first characterof the two strings match and if its a common Chi-nese last name.
Real valued features are binarized.Table 3 lists the feature templates we used inour SVM model and the corresponding number offeatures.5 Experiments5.1 DatasetWe constructed two datasets from Wikipedia.REDIRECT: We extracted webpage redirectsfrom Chinese Wikipedia pages that correspond toentities (person, organization, location); the pagetype is indicated in the page?s metadata.
Redi-rect links indicate queries that all lead to thesame page, such as ?Barack Hussein Obama?
and?Barack Obama?.
To remove redirects that are notentities (e.g.
?44th president?)
we removed entriesthat contain numerals and Latin characters, as wellas names that contain certain keywords.7The fi-nal dataset contains 13,730 pairs of person names,10,686 organizations and 5,152 locations, dividedinto35train,15development and15test.NAME GROUPS: Chinese Wikipedia contains ahandcrafted mapping between the entity name andvarious transliterations,8including for Mainland,Hong Kong and Taiwan.
We created two datasets:Mainland-Hong Kong (1288 people pairs, 357 lo-cations, 177 organizations), and Mainland-Taiwan(1500 people, 439 locations, 112 organizations).Data proportions are split as in REDIRECT.7Entries that contain??
(list), ??
(representative) , ??
(movement),??
(issue) and??
(wikipedia).8http://zh.wikipedia.org/wiki/Template:CGroupMethod Character prec@1 prec@3 MRRLevenshteinoriginal 0.773 0.838 0.821simplified 0.816 0.872 0.856string-pinyin 0.743 0.844 0.811character-pinyin 0.824 0.885 0.866segment-pinyin 0.797 0.877 0.849Jaro-Winkleroriginal 0.690 0.792 0.767simplified 0.741 0.821 0.803string-pinyin 0.741 0.818 0.800character-pinyin 0.751 0.831 0.813segment-pinyin 0.753 0.821 0.808Table 4: String matching on development data.5.2 EvaluationWe evaluated performance on a ranking task (thesetting of Andrews et al (2012)).
In each instance,the algorithm was given a query and a set of 11names from which to select the best match.
The11 names included a matching name as well as 10other names with some character overlap with thequery that are randomly chose from the same datasplit.
We evaluate using precision@1,3 and meanreciprocal rank (MRR).
Classifiers were trainedon the true pairs (positive) and negative examplesconstructed by pairing a name with 10 other namesthat have some character overlap with it.
The twoSVM parameters (the regularizer co-efficient Cand the instance weight w for positive examples),as well as the best pinyin representation, were se-lected using grid search on dev data.Results For string matching methods, simplifiedcharacters improve over the original characters forboth Levenshtein and Jaro-Winkler (Table 4).
Sur-prisingly, pinyin does not help over the simpli-fied characters.
Segmented pinyin improved overpinyin but did not do as well as the simplified char-acters.
Our method of character pinyin performedthe best overall, because it utilizes the phoneticinformation the pinyin encodes: all the differentcharacters that have the same pronunciation arereduced to the same pinyin representation.
Overall the representations, Levenshtein outperformedJaro-Winkler, consistent with previous work (Co-hen et al, 2003).Compared to the best string matching method(Levenshtein over pinyin characters), the trans-ducer improves for the two name group datasetsbut does worse on REDIRECT (Table 5).
Theheterogeneous nature of REDIRECT, includingvariation from aliases, nicknames, and long-distance re-ordering, may confuse the trans-ducer.
The SVM does best overall, improv-ing for all datasets over string matching and380Method Dataset prec@1 prec@3 MRRLevenshteinREDIRECT 0.820 0.868 0.859Mainland-Taiwan 0.867 0.903 0.897Mainland-Hong Kong 0.873 0.937 0.911TransducerREDIRECT 0.767 0.873 0.833Mainland-Taiwan 0.889 0.938 0.921Mainland-Hong Kong 0.925(?)0.989(?)0.954(?
)SVMREDIRECT 0.888(??)0.948(??)0.924(??
)Mainland-Taiwan 0.926 0.966(??)0.951(?
)Mainland-Hong Kongs 0.882 0.972 0.928Table 5: Results on test data.
* better thanLevenshtein; ** better than all other methods(p = 0.05)Features DatasetsREDIRECT Name GroupsALL 0.921 0.966- Jaccard similariy 0.908 0.929- Levenshtein 0.919 0.956- Simplified pairs 0.918 0.965- Pinyin pairs 0.920 0.960- Others 0.921 0.962Table 6: Ablation experiments on SVM featurestying or beating the transducer.
Differentpinyin representations (combined with the sim-plified representation) worked best on differ-ent datasets: character-pinyin for REDIRECT,segmented-pinyin for Mainland-Hongkong andstring-pinyin for Mainland-Taiwan.
To understandhow the features for SVM affect the final results,we conduct ablation tests for different group offeatures when trained on person names (only) foreach dataset (Table 6).
Overall, Jaccard featuresare the most effective.Error Analysis We annotated 100 randomlysampled REDIRECT development pairs incorrectlyclassified by the SVM.
We found three major typesof errors.
1) Matches requiring external knowl-edge (43% of errors), where there were nicknamesor aliases.
In these cases, the given name stringsare insufficient for determining the correct an-swer.
These types of errors are typically han-dled using alias lists.
2) Transliteration confusions(13%) resulting from different dialects, transliter-ation versus translation, or only part of a name be-ing transliterated.
3) Noisy data (19%): Wikipediaredirects include names in other languages (e.g.Japanese, Korean) or orthographically identicalstrings for different entities.
Finally, 25% of thetime the system simply got the wrong answer,Many of these cases are acronyms.5.3 Entity ClusteringWe evaluate the impact of our improved namematching on a downstream task: entity clusteringMethod Dev TestPrecision Recall F1 Precision Recall F1Exact match 84.55 57.46 68.42 63.95 65.44 64.69Jaro-winkler 84.87 58.35 69.15 70.79 66.21 68.42Levenshtein 83.16 61.13 70.46 69.56 67.27 68.40Transducer 90.33 74.92 81.90 73.59 63.70 68.29SVM 90.05 63.90 74.75 74.33 67.60 70.81Table 7: Results on Chinese entity clustering.
(cross document coreference resolution), wherethe goal is identify co-referent named mentionsacross documents.
Only a few studies have con-sidered Chinese entity clustering (Chen and Mar-tin, 2007), including the TAC KBP shared task,which has included clustering Chinese NIL men-tions (Ji et al, 2011).
We construct an entity clus-tering dataset from the TAC KBP entity linkingdata.
All of the 2012 Chinese data is used as de-velopment, and the 2013 data as test.
We use thesystem of Green et al (2012), which allows forthe inclusion of arbitrary name matching metrics.We follow their setup for training and evaluation(B3) and use TF-IDF context features.
We tunethe clustering cutoff for their hierarchical model,as well as the name matching threshold on the de-velopment data.
For the trainable name matchingmethods (transducer, SVM) we train the methodson the development data using cross-validation, aswell as tuning the representations and model pa-rameters.
We include an exact match baseline.Table 7 shows that on test data, our best method(SVM) improves over all previous methods byover 2 points.
The transducer makes strong gainson dev but not test, suggesting that parameter tun-ing overfit.
These results demonstrate the down-stream benefits of improved name matching.6 ConclusionOur results suggest several research directions.The remaining errors could be addressed with ad-ditional resources.
Alias lists could be learnedfrom data or derived from existing resources.Since the best pinyin representation varies bydataset, work could automatically determine themost effective representation, which may includedetermining the type of variation present in theproposed pair, as well as the associated dialect.Our name matching tool, Mingpipe, is imple-mented as a Python library.
We make Mingpipeand our datasets available to aid future research onthis topic.99https://github.com/hltcoe/mingpipe381ReferencesNicholas Andrews, Jason Eisner, and Mark Dredze.2012.
Name phylogeny: A generative model ofstring variation.
In Empirical Methods in NaturalLanguage Processing (EMNLP), pages 344?355.Regina Barzilay and Lillian Lee.
2003.
Learningto paraphrase: An unsupervised approach usingmultiple-sequence alignment.
In North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies (NAACL-HLT), pages 16?23.Mikhail Bilenko, Raymond Mooney, William Cohen,Pradeep Ravikumar, and Stephen Fienberg.
2003.Adaptive name matching in information integration.IEEE Intelligent Systems, 18(5):16?23.Alexandre Bouchard-C?ot?e, Percy Liang, Dan Klein,and Thomas L Griffiths.
2008.
A probabilistic ap-proach to language change.
In Advances in NeuralInformation Processing Systems (NIPS), pages 169?176.Taylor Cassidy, Zheng Chen, Javier Artiles, Heng Ji,Hongbo Deng, Lev-Arie Ratinov, Jing Zheng, Ji-awei Han, and Dan Roth.
2011.
Cuny-uiuc-sri tac-kbp2011 entity linking system description.
In TextAnalysis Conference (TAC).Ying Chen and James Martin.
2007.
Towards ro-bust unsupervised personal name disambiguation.In Empirical Methods in Natural Language Process-ing (EMNLP), pages 190?198.Ying Chen, Peng Jin, Wenjie Li, and Chu-Ren Huang.2010.
The chinese persons name disambiguationevaluation: Exploration of personal name disam-biguation in chinese news.
In CIPS-SIGHAN JointConference on Chinese Language Processing.Gang Cheng, Fei Wang, Haiyang Lv, and YinlingZhang.
2011.
A new matching algorithm for chi-nese place names.
In International Conference onGeoinformatics, pages 1?4.
IEEE.Peter Christen.
2006.
A comparison of personalname matching: Techniques and practical issues.In IEEE International Conference on Data MiningWorkshops, pages 290?294.William Cohen, Pradeep Ravikumar, and Stephen Fien-berg.
2003.
A comparison of string metrics formatching names and records.
In KDD Workshop onData Cleaning and Object Consolidation, pages 73?78.Ryan Cotterell, Nanyun Peng, and Jason Eisner.
2014.Stochastic contextual edit distance and probabilisticfsts.
In Association for Computational Linguistics(ACL), pages 625?630.Markus Dreyer, Jason R Smith, and Jason Eisner.2008.
Latent-variable modeling of string transduc-tions with finite-state methods.
In Empirical Meth-ods in Natural Language Processing (EMNLP),pages 1080?1089.Tarek El-Shishtawy.
2013.
A hybrid algo-rithm for matching arabic names.
arXiv preprintarXiv:1309.5657.Donghui Feng, Yajuan L?u, and Ming Zhou.
2004.A new approach for english-chinese named entityalignment.
In Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 372?379.Andrew T Freeman, Sherri L Condon, and Christo-pher M Ackerman.
2006.
Cross linguistic namematching in english and arabic: a one to many map-ping extension of the levenshtein edit distance algo-rithm.
In North American Chapter of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies (NAACL-HLT), pages 471?478.Spence Green, Nicholas Andrews, Matthew R. Gorm-ley, Mark Dredze, and Christopher D. Manning.2012.
Entity clustering across languages.
In NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT), pages 60?69.David Holmes and M Catherine McCabe.
2002.
Im-proving precision and recall for soundex retrieval.In International Conference on Information Tech-nology: Coding and Computing, pages 22?26.Heng Ji, Ralph Grishman, and Hoa Trang Dang.
2011.Overview of the tac 2011 knowledge base popula-tion track.
In Text Analytics Conference.Long Jiang, Ming Zhou, Lee-Feng Chien, and ChengNiu.
2007.
Named entity translation with webmining and transliteration.
In International JointConference on Artificial Intelligence (IJCAI), pages1629?1634.Kevin Knight and Jonathan Graehl.
1998.
Ma-chine transliteration.
Computational Linguistics,24(4):599?612.A Levenshtein.
1966.
Binary codes capable of cor-recting deletions, insertions and reversals.
SovietPhysics Doklady, 10(8):707?710.Sebastian Martschat, Jie Cai, Samuel Broscheit,?EvaM?ujdricza-Maydt, and Michael Strube.
2012.
Amultigraph model for coreference resolution.
InEmpirical Methods in Natural Language Processing(EMNLP) and the Conference on Natural LanguageLearning (CONLL), pages 100?106.James Mayfield, David Alexander, Bonnie J Dorr, Ja-son Eisner, Tamer Elsayed, Tim Finin, Clayton Fink,Marjorie Freedman, Nikesh Garera, Paul McNamee,et al 2009.
Cross-document coreference resolu-tion: A key technology for learning by reading.
InAAAI Spring Symposium: Learning by Reading andLearning to Read, pages 65?70.Paul McNamee, James Mayfield, Dawn Lawrie, Dou-glas W Oard, and David S Doermann.
2011a.Cross-language entity linking.
In International JointConference on Natural Language Processing (IJC-NLP), pages 255?263.382Paul McNamee, James Mayfield, Douglas W Oard,Tan Xu, Ke Wu, Veselin Stoyanov, and David Do-ermann.
2011b.
Cross-language entity linking inmaryland during a hurricane.
In Empirical Methodsin Natural Language Processing (EMNLP).L Philips.
2000.
The double metaphone search algo-rithm.
C/C++ Users Journal, 18(6).Edward H Porter, William E Winkler, et al 1997.
Ap-proximate string comparison and its effect on an ad-vanced record linkage system.
In Advanced recordlinkage system.
US Bureau of the Census, ResearchReport.Delip Rao, Paul McNamee, and Mark Dredze.
2013.Entity linking: Finding extracted entities in a knowl-edge base.
In Multi-source, Multilingual Informa-tion Extraction and Summarization, pages 93?115.Springer.Eric Sven Ristad and Peter N Yianilos.
1998.
Learningstring-edit distance.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 20(5):522?532.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational linguistics, 27(4):521?544.Michael Strube, Stefan Rapp, and Christoph M?uller.2002.
The influence of minimum edit distance onreference resolution.
In Empirical Methods in Natu-ral Language Processing (EMNLP), pages 312?319.Jeffrey Sukharev, Leonid Zhukov, and AlexandrinPopescul.
2014.
Learning alternative namespellings.
arXiv preprint arXiv:1405.2048.William E Winkler.
1999.
The state of record link-age and current research problems.
In Statistical Re-search Division, US Census Bureau.Wei Zhang, Jian Su, Chew Lim Tan, and Wen TingWang.
2010.
Entity linking leveraging: Automat-ically generated annotation.
In International Con-ference on Computational Linguistics (COLING),pages 1290?1298.383
