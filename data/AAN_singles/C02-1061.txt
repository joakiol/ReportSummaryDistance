Antonymy and Conceptual VectorsDidier Schwab, Mathieu Lafourcade and Violaine PrinceLIRMMLaboratoire d?informatique, de Robotiqueet de Microe?lectronique de MontpellierMONTPELLIER - FRANCE.
{schwab,lafourca,prince}@lirmm.frhttp://www.lirmm.fr/ ?
{schwab, lafourca, prince}AbstractFor meaning representations in NLP, we focusour attention on thematic aspects and concep-tual vectors.
The learning strategy of concep-tual vectors relies on a morphosyntaxic analy-sis of human usage dictionary definitions linkedto vector propagation.
This analysis currentlydoesn?t take into account negation phenomena.This work aims at studying the antonymy as-pects of negation, in the larger goal of its inte-gration into the thematic analysis.
We present amodel based on the idea of symmetry compat-ible with conceptual vectors.
Then, we defineantonymy functions which allows the construc-tion of an antonymous vector and the enumer-ation of its potentially antinomic lexical items.Finally, we introduce a measure which evaluateshow a given word is an acceptable antonym fora term.1 IntroductionResearch in meaning representation in NLP isan important problem still addressed throughseveral approaches.
The NLP team at LIRMMcurrently works on thematic and lexical disam-biguation text analysis (Laf01).
Therefore webuilt a system, with automated learning capa-bilities, based on conceptual vectors for mean-ing representation.
Vectors are supposed to en-code ?ideas?
associated to words or to expres-sions.
The conceptual vectors learning systemautomatically defines or revises its vectors ac-cording to the following procedure.
It takes, asan input, definitions in natural language con-tained in electronic dictionaries for human us-age.
These definitions are then fed to a morpho-syntactic parser that provides tagging and anal-ysis trees.
Trees are then used as an inputto a procedure that computes vectors usingtree geometry and syntactic functions.
Thus,a kernel of manually indexed terms is necessaryfor bootstrapping the analysis.
The transver-sal relationships1, such as synonymy (LP01),antonymy and hyperonymy, that are more orless explicitly mentioned in definitions can beused as a way to globally increase the coher-ence of vectors.
In this paper, we describe avectorial function of antonymy.
This can helpto improve the learning system by dealing withnegation and antonym tags, as they are oftenpresent in definition texts.
The antonymy func-tion can also help to find an opposite thema tobe used in all generative text applications: op-posite ideas research, paraphrase (by negationof the antonym), summary, etc.2 Conceptual VectorsWe represent thematic aspects of textual seg-ments (documents, paragraph, syntagms, etc)by conceptual vectors.
Vectors have been usedin information retrieval for long (SM83) andfor meaning representation by the LSI model(DDL+90) from latent semantic analysis (LSA)studies in psycholinguistics.
In computationallinguistics, (Cha90) proposes a formalism forthe projection of the linguistic notion of se-mantic field in a vectorial space, from whichour model is inspired.
From a set of elemen-tary concepts, it is possible to build vectors(conceptual vectors) and to associate them tolexical items2.
The hypothesis3 that considersa set of concepts as a generator to languagehas been long described in (Rog52).
Polysemicwords combine different vectors corresponding1well known as lexical functions (MCP95)2Lexical items are words or expressions which consti-tute lexical entries.
For instance, ?car ?
or ?white ant ?
arelexical items.
In the following we will (some what) usesometimes word or term to speak about a lexical item.3that we call thesaurus hypothesis.to different meanings.
This vector approachis based on known mathematical properties, itis thus possible to undertake well founded for-mal manipulations attached to reasonable lin-guistic interpretations.
Concepts are definedfrom a thesaurus (in our prototype applied toFrench, we have chosen (Lar92) where 873 con-cepts are identified).
To be consistent with thethesaurus hypothesis, we consider that this setconstitutes a generator family for the words andtheir meanings.
This familly is probably notfree (no proper vectorial base) and as such, anyword would project its meaning on it accordingto the following principle.
Let be C a finite setof n concepts, a conceptual vector V is a linearcombinaison of elements ci of C. For a meaningA, a vector V (A) is the description (in exten-sion) of activations of all concepts of C. For ex-ample, the different meanings of ?door ?
could beprojected on the following concepts (the CON-CEPT [intensity] are ordered by decreasing val-ues): V(?door ?)
= (OPENING[0.8], BARRIER[0.7],LIMIT [0.65], PROXIMITY [0.6], EXTERIOR[0.4], IN-TERIOR[0.39], .
.
.In practice, the larger C is, the finer the mean-ing descriptions are.
In return, the computingis less easy: for dense vectors4, the enumera-tion of activated concepts is long and difficultto evaluate.
We prefer to select the themati-cally closest terms, i.e., the neighbourhood.
Forinstance, the closest terms ordered by increas-ing distance to ?door ?
are: V(?door ?
)=?portal ?,?portiere?, ?opening?, ?gate?, ?barrier ?,.
.
.2.1 Angular DistanceLet us define Sim(A,B) as one of the similar-ity measures between two vectors A et B, of-ten used in information retrieval (Mor99).
Wecan express this function as: Sim(A,B) =cos(A?, B) = A?B?A???B?
with ???
as the scalarproduct.
We suppose here that vector com-ponents are positive or null.
Then, we definean angular distance DA between two vectors Aand B as DA(A,B) = arccos(Sim(A,B)).
In-tuitively, this function constitutes an evaluationof the thematic proximity and measures the an-gle between the two vectors.
We would gener-ally consider that, for a distance DA(A,B) ?
pi44Dense vectors are those which have very few nullcoordinates.
In practice, by construction, all vectors aredense.
(45 degrees) A and B are thematically close andshare many concepts.
For DA(A,B) ?
pi4 , thethematic proximity between A and B would beconsidered as loose.
Around pi2 , they have norelation.
DA is a real distance function.
It ver-ifies the properties of reflexivity, symmetry andtriangular inequality.
We have, for example,the following angles(values are in radian and de-grees).DA(V(?tit ?
), V(?tit ?
))=0 (0)DA(V(?tit ?
), V(?bird ?
))=0.55 (31)DA(V(?tit ?
), V(?sparrow ?
))=0.35 (20)DA(V(?tit ?
), V(?train ?
))=1.28 (73)DA(V(?tit ?
), V(?insect ?
))=0.57 (32)The first one has a straightforward interpreta-tion, as a ?tit ?
cannot be closer to anything elsethan itself.
The second and the third are notvery surprising since a ?tit ?
is a kind of ?sparrow ?which is a kind of ?bird ?.
A ?tit ?
has not muchin common with a ?train?, which explains a largeangle between them.
One can wonder why thereis 32 degrees angle between ?tit ?
and ?insect ?,which makes them rather close.
If we scruti-nise the definition of ?tit ?
from which its vectoris computed (Insectivourous passerine bird withcolorful feather.)
perhaps the interpretation ofthese values seems clearer.
In effect, the the-matic is by no way an ontological distance.2.2 Conceptual Vectors Construction.The conceptual vector construction is based ondefinitions from different sources (dictionaries,synonym lists, manual indexations, etc).
Defini-tions are parsed and the corresponding concep-tual vector is computed.
This analysis methodshapes, from existing conceptual vectors anddefinitions, new vectors.
It requires a bootstrapwith a kernel composed of pre-computed vec-tors.
This reduced set of initial vectors is man-ually indexed for the most frequent or difficultterms.
It constitutes a relevant lexical itemsbasis on which the learning can start and rely.One way to build an coherent learning systemis to take care of the semantic relations betweenitems.
Then, after some fine and cyclic compu-tation, we obtain a relevant conceptual vectorbasis.
At the moment of writing this article,our system counts more than 71000 items forFrench and more than 288000 vectors, in which2000 items are concerned by antonymy.
Theseitems are either defined through negative sen-tences, or because antonyms are directly in thedictionnary.
Example of a negative definition:?non-existence?
: property of what does not exist.Example of a definition stating antonym: ?love?
:antonyms: ?disgust ?, ?aversion?.3 Definition and Characterisation ofAntonymyWe propose a definition of antonymy compat-ible with the vectorial model used.
Two lexi-cal items are in antonymy relation if there isa symmetry between their semantic componentsrelatively to an axis.
For us, antonym construc-tion depends on the type of the medium thatsupports symmetry.
For a term, either we canhave several kinds of antonyms if several possi-bilities for symmetry exist, or we cannot havean obvious one if a medium for symmetry is notto be found.
We can distinguish different sortsof media: (i) a property that shows scalar val-ues (hot and cold which are symmetrical valuesof temperature), (ii) the true-false relevance orapplication of a property (e.g.
existence/non-existence) (iii) cultural symmetry or opposition(e.g.
sun/moon).From the point of view of lex-ical functions, if we compare synonymy andantonymy, we can say that synonymy is theresearch of resemblance with the test of sub-stitution (x is synonym of y if x may replacey), antonymy is the research of the symmetry,that comes down to investigating the existenceand nature of the symmetry medium.
We haveidentified three types of symmetry by relyingon (Lyo77), (Pal76) and (Mue97).
Each sym-metry type characterises one particular type ofantonymy.
In this paper, for the sake of clarityand precision, we expose only the complemen-tary antonymy.
The same method is used forthe other types of antonymy, only the list ofantonymous concepts are different.3.1 Complementary AntonymyThe complementary antonyms are couples likeevent/unevent, presence/absence.he is present ?
he is not absenthe is absent ?
he is not presenthe is not absent ?
he is presenthe is not present ?
he is absentIn logical terms, we would have:?x P (x)?
?Q(x) ?x ?P (x)?
Q(x)?x Q(x)?
?P (x) ?x ?Q(x)?
P (x)This corresponds to the exclusive disjunctionrelation.
In this frame, the assertion of oneof the terms implies the negation of the other.Complementary antonymy presents two kindsof symmetry, (i) a value symmetry in a booleansystem, as in the examples above and (ii) a sym-metry about the application of a property (blackis the absence of color, so it is ?opposed?
to allother colors or color combinaisons).4 Antonymy Functions4.1 Principles and Definitions.The aim of our work is to create a functionthat would improve the learning system by sim-ulating antonymy.
In the following, we will bemainly interested in antonym generation, whichgives a good satisfaction clue for these functions.We present a function which, for a given lex-ical item, gives the n closest antonyms as theneighbourhood function V provides the n clos-est items of a vector.
In order to know whichparticular meaning of the word we want to op-pose, we have to assess by what context mean-ing has to be constrained.
However, context isnot always sufficient to give a symmetry axisfor antonymy.
Let us consider the item ?father ?.In the ?family?
context, it can be opposite to?mother ?
or to ?children?
being therefore ambigu-ous because ?mother ?
and ?children?
are by no waysimilar items.
It should be useful, when contextcannot be used as a symmetry axis, to refinethe context with a conceptual vector which isconsidered as the referent.
In our example, weshould take as referent ?filiation?, and thus theantonym would be ?children?
or the specialisedsimilar terms (e.g.
?sons?
, ?daughters?)
?marriage?or ?masculine?
and thus the antonym would be?mother ?.The function AntiLexS returns the n closestantonyms of the word A in the context definedby C and in reference to the word R.AntiLexS(A,C,R, n)AntiLexR(A,C, n) = AntiLexS(A,C,C, n)AntiLexB(A,R, n) = AntiLexS(A,R,R, n)AntiLexA(A, n) = AntiLexS(A,A,A, n)The partial function AntiLexR has been de-fined to take care of the fact that in most cases,context is enough to determine a symmetry axis.AntiLexB is defined to determine a symmetryaxis rather than a context.
In practice, we haveAntiLexB = AntiLexR.
The last function isthe absolute antonymy function.
For polysemicwords, its usage is delicate because only oneword defines at the same time three things: theword we oppose, the context and the referent.This increases the probability to get unsatis-factory results.
However, given usage habits,we should admit that, practically, this functionwill be the most used.
It?s sequence process ispresented in picture 1.
We note Anti(A,C) theITEMSANTONYMOUSCONCEPTUAL VECTORCALCULATIONIDENTIFICATIONOF THE CLOSESTITEMSneighbourhoodCONCEPTUAL VECTORSstrong contextualisationCALCULATIONX, C, RX1, X2, ..., XnITEMSVAntiVECTORANTONYMOUSOF THEantiVcx, VcrVECTORSCORRESPONDINGOF THEFigure 1: run of the functions AntiLexantonymy function at the vector level.
Here,A is the vector we want to oppose and C thecontext vector.Items without antonyms: it is the caseof material objects like car, bottle, boat, etc.The question that raises is about the continu-ity the antonymy functions in the vector space.When symmetry is at stake, then fixed pointsor plans are always present.
We consider thecase of these objects, and in general, non op-posable terms, as belonging to the fixed spaceof the symmetry.
This allows to redirect thequestion of antonymy to the opposable proper-ties of the concerned object.
For instance, if wewant to compute the antonym of a ?motorcycle?,which is a ROAD TRANSPORT, its opposable prop-erties being NOISY and FAST, we consider its cat-egory (i.e.
ROAD TRANSPORT) as a fixed point,and we will look for a road transport (SILEN-CIOUS and SLOW ), something like a ?bicycle?
oran ?electric car ?.
With this method, thanks tothe fixed points of symmetry, opposed ?ideas?or antonyms, not obvious to the reader, couldbe discovered.4.2 Antonym vectors of concept listsAnti functions are context-dependent and can-not be free of concepts organisation.
Theyneed to identify for every concept and for ev-ery kind of antonymy, a vector considered asthe opposite.
We had to build a list of triples?concept, context, vector?.
This list is calledantonym vectors of concept list (AVC).4.2.1 AVC construction.The Antonym Vectors of Concepts list is manu-ally built only for the conceptual vectors of thegenerating set.
For any concept we can have theantonym vectors such as:AntiC(EXISTENCE, V ) = V (NON-EXISTENCE)AntiC(NON-EXISTENCE, V ) = V (EXISTENCE)AntiC(AGITATION, V ) = V (INERTIA)?
V (REST)AntiC(PLAY, V ) = V (PLAY)?VAntiC(ORDER, V (order) ?
V (disorder)) =V (DISORDER)AntiC(ORDER, V (classification) ?
V (order)) =V (CLASSIFICATION)As items, concepts can have, according tothe context, a different opposite vector evenif they are not polysemic.
For instance, DE-STRUCTION can have for antonyms PRESERVA-TION, CONSTRUCTION, REPARATION or PROTEC-TION.
So, we have defined for each concept, oneconceptual vector which allows the selection ofthe best antonym according to the situation.For example, the concept EXISTENCE has thevector NON-EXISTENCE for antonym for any con-text.
The concept DISORDER has the vector ofORDER for antonym in a context constituted bythe vectors of ORDER ?DISORDER5 and has CLAS-SIFICATION in a context constituted by CLASSI-FICATION and ORDER.The function AntiC(Ci, Vcontext) returns fora given concept Ci and the context defined byVcontext , the complementary antonym vector inthe list.4.3 Construction of the antonymvector: the Anti Function4.3.1 DefinitionsWe define the relative antonymy functionAntiR(A,C) which returns the opposite vec-tor of A in the context C and the absoluteantonymy function AntiA(A) = AntiR(A,A).The usage of AntiA is delicate because the lexi-cal item is considered as being its own context.We will see in 4.4.1 that this may cause realproblems because of sense selection.
We shouldstress now on the construction of the antonymvector from two conceptual vectors: Vitem, for5?
is the normalised sum V = A?B | vi = xi+yi?V ?the item we want to oppose and the other, Vc,for the context (referent).4.3.2 Construction of the AntonymVectorThe method is to focus on the salient notions inVitem and Vc.
If these notions can be opposedthen the antonym should have the inverse ideasin the same proportion.
That leads us to definethis function as follows:AntiR(Vitem, Vc) =?Ni=1 Pi ?AntiC(Ci, Vc)with Pi = V 1+CV (Vitem)itemi ?max(Vitemi , Vci)We crafted the definition of the weight P afterseveral experiments.
We noticed that the func-tion couldn?t be symmetric (we cannot reason-ably have AntiR(V(?hot ?),V(?temperature?))
=AntiR(V(?temperature?
),V(?hot ?))).
That is whywe introduce this power, to stress more on theideas present in the vector we want to oppose.We note also that the more conceptual6 the vec-tor is, the more important this power should be.That is why the power is the variation coeffi-cient7 which is a good clue for ?conceptuality?.To finish, we introduce this function max be-cause an idea presents in the item, even if thisidea is not present in the referent, has to be op-posed in the antonym.
For example, if we wantthe antonym of ?cold ?
in the ?temperature?
con-text, the weight of ?cold ?
has to be importanteven if it is not present in ?temperature?.4.4 Lexical Items and Vectors:Problem and SolutionsThe goal of the functions AntiLex is to returnantonym of a lexical item.
They are definedwith the Anti function.
So, we have to use toolswhich allow the passage between lexical itemsand vectors.
This transition is difficult becauseof polysemy, i.e.
how to choose the right relationbetween an item and a vector.
In other words,how to choose the good meaning of the word.4.4.1 Transition lexical items ?Conceptual VectorsAs said before, antonymy is relative to a con-text.
In some cases, this context cannot be suf-ficient to select a symmetry axis for antonymy.6In this paragraph, conceptual means: closeness of avector to a concept7The variation coefficient is SD(V )?
(V ) with SD as thestandart deviation and ?
as the arithmetic mean.To catch the searched meaning of the item and,if it is different from the context, to catch theselection of the meaning of the referent, we usethe strong contextualisation method.
It com-putes, for a given item, a vector.
In this vector,some meanings are favoured against others ac-cording to the context.
Like this, the contextvector is also contextualised.This contextualisation shows the problemcaused by the absolute antonymy functionAnti?R .
In this case, the method will computethe vector of the word item in the context item.This is not a problem if item has only one defini-tion because, in this case, the strong contextu-alisation has no effect.
Otherwise, the returnedconceptual vector will stress on the main idea itcontains which one is not necessary the appro-priate one.4.4.2 Transition Conceptual Vectors ?Lexical ItemsThis transition is easier.
We just have to com-pute the neighbourhood of the antonym vectorVant to obtain the items which are in thematicantonymy with Vitem.
With this method, wehave, for instance:V(AnticR(death, ?death ?
& ?life?
))=(LIFE 0.4)(?killer ?
0.449) (?murderer ?
0.467) (?blood sucker ?0.471) (?strige?
0.471) (?to die?
0.484) (?to live?
0.486)V(AnticR(life, ?death ?
& ?life?
))=(?death ?
0.336)(DEATH 0.357) (?murdered ?
0.367) (?killer ?
0.377)(C3:AGE OF LIFE 0.481) (?tyrannicide?
0.516) (?to kill ?0.579) (?dead ?
0.582)V(AntiCcA(LIFE))=(DEATH 0.034) (?death ?
0.427)(C3:AGE OF LIFE 0.551) (?killer ?
0.568) (?mudered ?0.588) (?tyrannicide?
0.699) (C2:HUMAN 0.737) (?tokill ?
0.748) (?dead ?
0.77)It is not important to contextualise the con-cept LIFE because we can consider that, for ev-ery context, the opposite vector is the same.In complementary antonymy, the closest itemis DEATH.
This result looks satisfactory.
We cansee that the distance between the antonymy vec-tor and DEATH is not null.
It is because ourmethod is not and cannot be an exact method.The goal of our function is to build the best(closest) antonymy vector it is possible to have.The construction of the generative vectors is thesecond explanation.
Generative vectors are in-terdependent.
Their construction is based on anontology.
To take care of this fact, we don?t haveboolean vectors, with which, we would have ex-actly the same vector.
The more polysemic theterm is, the farthest the closest item is, as wecan see it in the first two examples.We cannot consider, even if the potential ofantonymy measure is correct, the closest lexicalitem from Vanti as the antonym.
We have toconsider morphological features.
Simply speak-ing, if the antonym of a verb is wanted, the re-sult would be better if a verb is caught.4.5 Antonymy Evaluation MeasureBesides computing an antonym vector, it seemsrelevant to assess wether two lexical items canbe antonyms.
To give an answer to this ques-tion, we have created a measure of antonymyevaluation.
Let A and B be two vectors.The question is precisely to know if they canreasonably be antonyms in the context of C.The antonymy measure MantiEval is the an-gle between the sum of A and B and the sumof AnticR(A,C) and AnticR(B,C).
Thus, wehave:MantiEval = DA(A?B,AntiR(A,C)?AntiR(B,C))A+BABAnti(A,C)Anti(B,C)Anti(A,C)+Anti(B,C)Figure 2: 2D geometric representation of the antonymyevaluation measure MantiEvalThe antonymy measure is a pseudo-distance.It verifies the properties of reflexivity, symme-try and triangular inequality only for the subsetof items which doesn?t accept antonyms.
In thiscase, notwithstanding the noise level, the mea-sure is equal to the angular distance.
In thegeneral case, it doesn?t verify reflexivity.
Theconceptual vector components are positive andwe have the property: Distanti ?
[0, pi2 ].
Thesmaller the measure, the more ?antonyms?
thetwo lexical items are.
However, it would be amistake to consider that two synonyms would beat a distance of about pi2 .
Two lexical items atpi2 have not much in common8.
We would rathersee here the illustration that two antonymsshare some ideas, specifically those which arenot opposable or those which are opposable witha strong activation.
Only specific activated con-cepts would participate in the opposition.
Adistance of pi2 between two items should ratherbe interpreted as these two items do not sharemuch idea, a kind of anti-synonymy.
This re-sult confirms the fact that antonymy is not theexact inverse of synonymy but looks more like a?negative synonymy?
where items remains quiterelated.
To sum up, the antonym of w is nota word that doesn?t share ideas with w, but aword that opposes some features of w.4.5.1 ExamplesIn the following examples, the context has beenommited for clarity sake.
In these cases, thecontext is the sum of the vectors of the twoitems.MantiEval(EXISTENCE,NON-EXISTENCE) = 0.03MantiEvalC(?existence?, ?non-existence?)
= 0.44MantiEvalC(EXISTENCE, CAR) = 1.45MantiEvalC(?existence?, ?car ?)
= 1.06MantiEvalC(CAR, CAR) = 0.006MantiEvalC(?car ?, ?car ?)
= 0.407The above examples confirm what pre-sented.
Concepts EXISTENCE and NON-EXISTENCE are very strong antonyms in comple-mentary antonymy.
The effects of the polysemymay explain that the lexical items ?existence?
and?non-existence?
are less antonyms than their re-lated concepts.
In complementary antonymy,CAR is its own antonym.
The antonymy mea-sure between CAR and EXISTENCE is an exam-ple of our previous remark about vectors shar-ing few ideas and that around pi/2 this mea-sure is close to the angular distance (we haveDA(existence, car) = 1.464.).
We could con-sider of using this function to look in a concep-tual lexicon for the best antonyms.
However,the computation cost (around a minute on a P4at 1.3 GHz) would be prohibitive.8This case is mostly theorical, as there is no languagewhere two lexical items are without any possible relation.5 Action on learning and methodevaluationThe function is now used in the learning process.We can use the evaluation measure to show theincrease of coherence between terms:MantiEvalC new old?existence?, ?non-existence?
0.33 0.44?existence?, ?car ?
1.1 1.06?car ?, ?car ?
0.3 0, 407There is no change in concepts because they arenot learned.
In the opposite, the antonymy eval-uation measure is better on items.
The exempleshows that ?existence?
and ?non-existence?
havebeen largely modified.
Now, the two items arestronger antonyms than before and the vectorbasis is more coherent.
Of course, we can testthese results on the 71000 lexical items whichhave been modified more or less directly by theantonymy function.
We have run the test onabout 10% of the concerned items and found animprovement of the angular distance throughMantiEvalC ranking to 0.1 radian.6 ConclusionThis paper has presented a model of antonymyusing the formalism of conceptual vectors.
Ouraim was to be able: (1) to spot antonymy ifit was not given in definition and thus providean antonym as a result, (2) to use antonyms(discovered or given) to control or to ensure thecoherence of an item vector, build by learning,which could be corrupted.
In NLP, antonymy isa pivotal aspect, its major applications are the-matic analysis of texts, construction of large lex-ical databases and word sense disambiguation.We grounded our research on a computable lin-guisitic theory being tractable with vectors forcomputational sake.
This preliminary work onantonymy has also been conducted under thespotlight of symmetry, and allowed us to expressantonymy in terms of conceptual vectors.
Thesefunctions allow, from a vector and some contex-tual information, to compute an antonym vec-tor.
Some extensions have also been proposed sothat these functions may be defined and usablefrom lexical items.
A measure has been identi-fied to assess the level of antonymy between twoitems.
The antonym vector construction is nec-essary for the selection of opposed lexical itemsin text generation.
It also determines oppositeideas in some negation cases in analysis.Many improvements are still possible, thefirst of them being revision of the VAC lists.These lists have been manually constructed bya reduced group of persons and should widely bevalidated and expanded especially by linguists.We are currently working on possible improve-ments of results through learning on a corpora.ReferencesJacques Chauche?.
De?termination se?mantiqueen analyse structurelle : une expe?rience base?esur une de?finition de distance.
TAL Informa-tion, 1990.Scott C. Deerwester, Susan T. Dumais,Thomas K. Landauer, George W. Furnas, andRichard A. Harshman.
Indexing by latent se-mantic analysis.
Journal of the American So-ciety of Information Science, 41(6):391?407,1990.Mathieu Lafourcade.
Lexical sorting and lexicaltransfer by conceptual vectors.
In Proceedingof the First International Workshop on Mul-tiMedia Annotation, Tokyo, January 2001.Larousse.
The?saurus Larousse - des ide?es auxmots, des mots aux ide?es.
Larousse, 1992.Mathieu Lafourcade and Violaine Prince.
Syn-onymies et vecteurs conceptuels.
In actes deTALN?2001, Tours, France, July 2001.John Lyons.
Semantics.
Cambridge UniversityPress, 1977.Igor Mel?c?uk, Andre?
Clas, and Alain Polgue`re.Introduction a` la lexicologie explicative etcombinatoire.
Duculot, 1995.Emmanuel Morin.
Extraction de liensse?mantiques entre termes a` partir decorpus techniques.
PhD thesis, Universite?
deNantes, 1999.Victoria Lynn Muehleisen.
Antonymy and se-mantic range in english.
PhD thesis, North-western university, 1997.F.R.
Palmer.
Semantics : a new introduction.Cambridge University Press, 1976.P.
Roget.
Roget?s Thesaurus of English Wordsand Phrases.
Longman, London, 1852.Gerard Salton and Michael McGill.
Introduc-tion to Modern Information Retrieval.
Mc-GrawHill, 1983.
