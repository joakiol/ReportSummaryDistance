Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 72?81,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsUnsupervised Concept Annotation using Latent Dirichlet Allocation andSegmental MethodsNathalie Camelin, Boris Detienne, Ste?phane Huet, Dominique Quadri and Fabrice Lefe`vreLIA - University of Avignon, BP 9122884911 Avignon Cedex 09, France{nathalie.camelin,boris.detienne,stephane.huet,dominique.quadri,fabrice.lefevre}@univ-avignon.frAbstractTraining efficient statistical approaches fornatural language understanding generally re-quires data with segmental semantic annota-tions.
Unfortunately, building such resourcesis costly.
In this paper, we propose an ap-proach that produces annotations in an unsu-pervised way.
The first step is an implementa-tion of latent Dirichlet alocation that producesa set of topics with probabilities for each topicto be associated with a word in a sentence.This knowledge is then used as a bootstrap toinfer a segmentation of a word sentence intotopics using either integer linear optimisationor stochastic word alignment models (IBMmodels) to produce the final semantic anno-tation.
The relation between automatically-derived topics and task-dependent concepts isevaluated on a spoken dialogue task with anavailable reference annotation.1 IntroductionSpoken dialogue systems in the field of informationquery are basically used to interface a database withusers using speech.
When probabilistic models areused in such systems, good performance can only bereached at the price of collecting a lot of field data,which must be transcribed and annotated at the se-mantic level.
It becomes then possible to train effi-cient models in a supervised manner.
However, theannotation process is costly and as a consequencerepresents a real difficulty hindering the widespreaddevelopment of these systems.
Therefore any meansto avoid it would be profitable as portability to newtasks, domains or languages would be greatly facili-tated.To give a full description of the architecture of adialogue system is out of the scope of this paper.
In-stead we limit ourselves to briefly recall that oncea speech recognizer has transcribed the signal it iscommon (though avoidable for very simple tasks) touse a module dedicated to extract the meaning ofthe user?s queries.
This meaning representation isthen conveyed to an interaction manager that decidesupon the next best action to perform considering thecurrent user?s input and the dialogue history.
Oneof the very first steps to build the spoken languageunderstanding (SLU) module is the identification ofliteral concepts in the word sequence hypothesisedby the speech recogniser.
An example of a semanticrepresentation in terms of literal concept is given inFigure 1.
Once the concepts are identified they canbe further composed to form the overall meaning ofthe sentence, for instance by means of a tree repre-sentation based on hierarchical semantic frames.To address the issue of concept tagging severaltechniques are available.
Some of these techniquesnow classical rely on probabilistic models, that canbe either discriminative or generative.
Among these,the most efficiently studied this last decade are: hid-den Markov models, finite state transducers, max-imum entropy Markov models, support vector ma-chines, dynamic fields (CRF).
In (Hahn et al, 2010)it is shown that CRFs obtain the best performance ona tourist information retrieval task in French (ME-DIA (Bonneau-Maynard et al, 2005)), but also intwo other comparable corpora in Italian and Polish.To be able to apply any such technique, basic con-72words concept normalized valuedonnez-moi nullle refLink-coRef singulartarif object payment-amount-roompuisque connectProp implyje voudrais nullune chambre number-room 1qui cou?te object payment-amount-roompas plus de comparative-payment less thancinquante payment-amount-integer-room 50euros payment-unit euroFigure 1: Semantic concept representation for the query ?give me the rate since I?d like a room charged not more thanfifty euros?.cept units have to be defined by an expert.
In the bestcase, most of these concepts can be derived straight-forwardly from the pieces of information lurking inthe database tables (mainly table fields but not ex-clusively).
Some others are general (dialogic unitsbut also generic entities such as number, dates, etc).However, to provide an efficient and usable informa-tion to the reasoning modules (the dialogue managerin our case) concepts have to be fine-grained enoughand application-dependent (even general conceptsmight have to be tailored to peculiar uses).
To thatextent it seems out of reach to derive the conceptdefinitions using a fully automatic procedure.
Any-how the process can be bootstrapped, for instanceby induction of semantic classes such as in (Siu andMeng, 1999) or (Iosif et al, 2006).
Our assumptionhere is that the most time-consuming parts of con-cept inventory and data tagging could be obtained inan unsupervised way even though a final (but hope-fully minimal) manual procedure is still required totag the classes so as to manually correct automaticannotation.Unlike the previous attempts cited above whichdeveloped ad hoc approaches, we investigate herethe use of broad-spectrum knowledge extractionmethods.
The notion most related to that of conceptin SLU is the topic, as used in information retrievalsystems.
Anyhow for a long time, the topic detec-tion task was limited to associate a single topic toa document and thus was not fitted to our require-ments.
The recently proposed LDA technique al-lows to have a probabilistic representation of a doc-ument as a mixture of topics.
Then multiple topicscan co-occur inside a document and the same topiccan be repeated.
From these characteristics it is pos-sible to consider the application of LDA to unsu-pervised concept inventory and concept tagging forSLU.
A shortcoming is that LDA does not modelizeat all the sequentiality of the data.
To address this is-sue we propose to conclude the procedure with a fi-nal step to introduce specific constraints for a correctsegmentation of the data: the assignments of topicsproposed by LDA are modified to be more segmen-tally coherent.The paper is organised as follows.
Principlesof automatic induction of semantic classes are pre-sented in Section 2, followed by the presentation ofan induction system based on LDA.
The additionalstep of segmentation is presented in Section 3 withtwo variants: stochastic word alignment (GIZA) andinteger linear programming (ILP).
Then evaluationsand results are reported in Section 4 on the FrenchMEDIA dialogue task.2 Automatic induction of semantic classes2.1 Context modelingThe idea of automatic induction of semantic classesis based on the assumption that concepts often sharethe same context (syntactic or lexical).
Imple-mented systems are based on the observation of co-occurring words according to two different ways.The observation of consecutive words (bigrams ortrigrams) enables the generation of lexical com-pounds supposed to follow syntactic rules.
The com-parison of right and left contexts considering pairsof words enables to cluster words (and word com-pounds) into semantic classes.73In (Siu and Meng, 1999) and (Pargellis et al,2001), iterative systems are presented.
Their im-plementations differ in the metrics chosen to eval-uate the similarity during the generation of syntacticrules and semantic classes, but also in the numberof words taken into account in a word context andthe order of successive steps (which ones to gener-ate first: syntactic rules or semantic classes?).
Aniterative procedure is executed to obtain a sufficientset of rules in order to automatically extract knowl-edge from the data.While there may be still room for improvement inthese techniques we decided instead to investigategeneral knowledge extraction approaches in order toevaluate their potential.
For that purpose a globalstrategy based on an unsupervised machine learningtechnique is adopted in our work to produce seman-tic classes.2.2 Implementation of an automatic inductionsystem based on LDASeveral approaches are available for topic detectionin the context of knowledge extraction and informa-tion retrieval.
They all more or less rely on the pro-jection of the documents of interest in a semanticspace to extract meaningful information.
However,as the considered spaces (initial document wordsand latent semantics) are discrete the performanceof the proposed approaches for the topic extractiontasks are pretty unstable, and also greatly depend onthe quantity of data available.
In this work we weremotivated by the recent development of a very at-tractive technique with major distinct features suchas the detection of multiple topics in a single docu-ment.
LDA (Blei et al, 2003) is the first principleddescription of a Dirichlet-based model of mixturesof latent variables.
LDA will be used in our workto annotate the dialogue data in terms of topics inan unsupervised manner.
Then the relation betweenautomatic topics and expected concepts will be ad-dressed manually.Basically LDA is a generative probabilistic modelfor text documents.
LDA follows the assumptionthat a set of observations can be explained by latentvariables.
More specifically documents are repre-sented by a mixture of topics (latent variables) andtopics are characterized by distributions over words.The LDA parameters are {?, ?}.
?
represents theDirichlet parameters of K latent topic mixtures as?
= [?1, ?2, .
.
.
, ?K ].
?
is a matrix representing amultinomial distribution in the form of a conditionalprobability table ?k,w = P (w|k).
Based on this rep-resentation, LDA can estimate the probability of anew document d of N words d = [w1, w2, .
.
.
, wN ]using the following procedure.A topic mixture vector ?
is drawn from the Dirich-let distribution (with parameter ?).
The correspond-ing topic sequence ?
= [k1, k2, .
.
.
, kN ] is generatedfor the whole document accordingly to a multino-mial distribution (with parameter ?).
Finally eachword is generated by the word-topic multinomialdistribution (with parameter ?, that is p(wi|ki, ?
)).After this procedure, the joint probability of ?, ?
andd is then:p(?, ?, d|?, ?)
= p(?|?)N?i=1p(ki|?
)p(wi|ki, ?
)(1)To obtain the marginal probability of d, a final in-tegration over ?
and a summation over all possibletopics considering a word is necessary:p(d|?, ?)
=?p(?|?)??N?i=1?kip(ki|?
)p(wi|ki, ?)??
(2)The framework is comparable to that of probabilis-tic latent semantic analysis, but the topic multino-mial distribution in LDA is assumed to be sampledfrom a Dirichlet prior and is not linked to trainingdocuments.
This approach is illustrated in Figure 2.Training of the ?
and ?
parameters is possible us-ing a corpus of documents, with a fixed number oftopics to predict.
A variational inference procedureis described in (Blei et al, 2003) which alleviatesthe intractability due to the coupling between ?
and?
in the summation over the latent topics.
Once theparameters for the Dirichlet and multinomial distri-butions are available, topic scores can be derived forany given document or word sequence.In recent years, several studies have been carriedout in language processing based on LDA.
For in-stance, (Tam and Schultz, 2006) worked on unsuper-vised language model adaptation; (Celikyilmaz etal., 2010) ranked candidate passages in a question-answering system; (Phan et al, 2008) implementedLDA to classify short and sparse web texts.74LATENT DIRICHLET ALLOCATION?
k w?
?MNFigure 1: Graphical model representation of LDA.
The boxes are ?plates?
representing replicates.The outer plate represents documents, while the inner plate represents the repeated choiceof topics and words within a document.where p(zn | ?)
is simply ?
i for the unique i such that zin = 1.
Integrating over ?
and summing overz, we obtain the marginal distribution of a document:p(w | ?, ?)
=?p(?
| ?
)(N?n=1?znp(zn | ?)
p(wn |zn, ?
))d ?.
(3)Finally, taking the product of the marginal probabilities of single documents, we obtain the proba-bility of a corpus:p(D | ?, ?)
= M?d=1?p(?
d | ?
)(Nd?n=1?zdnp(zdn | ?
d)p(wdn |zdn, ?
))d ?
d .The LDA model is represented as a probabilistic graphical model in Figure 1.
As the figuremakes clear, there are three levels to the LDA representation.
The parameters ?
and ?
are corpus-level parameters, assumed to be sampled once in the process of generating a corpus.
The variables?
d are document-level variables, sampled once per document.
Finally, the variables zdn and wdn areword-level variables and are sampled once for each word in each document.It is important to distinguish LDA from a simple Dirichlet-multinomial clustering model.
Aclassical clustering model would involve a two-level model in which a Dirichlet is sampled oncefor a corpus, a multinomial clustering variable is selected once for each document in the corpus,and a set of words are selected for the document conditional on the cluster variable.
As with manyclustering models, such a model restricts a document to being associated with a single topic.
LDA,on the other hand, involves three levels, and notably the topic node is sampled repeatedly within thedocument.
Under this model, documents can be associated with multiple topics.Structures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling,where they are referred to as hierarchical models (Gelman et al, 1995), or more precisely as con-ditionally independent hierarchical models (Kass and Steffey, 1989).
Such models are also oftenreferred to as parametric empirical Bayes models, a term that refers not only to a particular modelstructure, but also to the methods used for estimating parameters in the model (Morris, 1983).
In-deed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameterssuch as ?
and ?
in simple implementations of LDA, but we also consider fuller Bayesian approachesas well.997Figure 2: Graphical representation for LDA variables(from (Blei et al, 2003)).
The grey circle is the only ob-servable variable.In our work LDA is employed to annotate eachuser?s utterance of a dialogue corpus with topic.
Ut-terances longer than one word are included in thetraining set as its sequence of words.
Once themodel has been trained, inference on data corpus as-signs the topic with the highest probability to eachword in a document.
This probability is computedfrom the probability of the topic to appear in the doc-ument and the probability of the word to be gener-ated by th opic.
As a con equence we obtain a fulltopic annotati n of the utterance.Notice that LDA considers a user utterance as abag of words.
This implies that each topic is as-signed to a word without any consideration for itsi m diate co text.
An additional segmental processis required if we want to introduce some context in-formation in the topic assignment.3 Segmental annotation3.1 Benefits of a segmental annotationThe segmental annotation of the data is not a strictrequirement for language understanding.
Up to quiterecently, most approaches for literal interpretationwere limited to lexical-concept relations; for in-stance this is the case of the Phoenix system (Ward,1991) based on the detection of keywords.
Howeverin an NLP perspective, the segmental approach al-lows to connect the various levels of sentence analy-sis (lexical, syntactic and semantic).
Even though, inorder to simplify its application, segments are gen-erally designed specifically for the semantic anno-tation and do not have any constraint on their rela-tion with the actual syntactic units (chunks, phrasalgroups, etc).
To get relieved of such constraints notonly simplifies the annotation process itself but asultimately the interpretation module is to be used in-side a spoken dialogue system, data will be noisyand generally bound the performance of the syn-tactic analysers (due to highly spontaneous and un-grammatical utterances from the users, combinedwith errors from the speech recognizer).Another interesting property of segmental ap-proach is to offer a convenient way to dissociate thedetection of a conceptual unit from the extraction ofits associated value.
The value corresponds to thenor alisation of the surface form (see last columnin 1); for instance if the segment ?not more than?is associated to the concept comparative-payment,its value is ?less than?.
The same value wouldbe associated to ?not exceeding?
or ?inferior to?.Value extraction requires a link between conceptsand words based on which the normalisation prob-lem can be addressed by means of regular expres-sions or concept-dependent language models (evenallowing integrated approaches such as describedin (Lefe`vre, 2007)).
In the case of global approaches(not segmental), value extraction must be dealt withdirectly at the level of the conceptual unit tagging,as in (Mairesse et al, 2009).
This additional level isvery complex (as some values may not be enumer-able, such as numbers and dates) and is only afford-able when the number of authorised values (for theenumerable cases) is low.To refine the LDA output, the topic-to-word align-ment is discarded and an automatic procedure isused to derive the best alignment between topics andwords.
While the underlying probabilistic modelsare pretty comparable, the major interest of this ap-pro ch is to eparate the tasks of detecting topics andaligning topics with words.
It is then possible to in-troduce additional constraints (such as locality, num-ber of segments, limits on repetitions etc) in the lat-ter task which would otherwise hinder topic detec-tion.
Conversely the alignment is self-coherent andable to question the associations proposed duringtopic detection with respect to its own constraintsonly.
Two approaches were designed to this pur-pose: one based on IBM alignment models and an-other one based on integer linear optimisation.753.2 Alignment with IBM models (GIZA)Once topic assignments for the documents in thecorpus have been proposed by LDA, a filtering pro-cess is done to keep only the most relevant topicsof each document.
The ?max most probable top-ics are kept according to the probability p(k|wi, d)that topic k generated the word wi of the documentd.
?max is a value fixed empirically according tothe expected set of topics in a sentence.
Then, theobtained topic sequences are disconnected from thewords.
At this point, the topic and word sequencescan be considered as a translation pair to producea word-topic parallel corpus.
These data can beused with classical approaches in machine transla-tion to align source and target sentences at the wordlevel.
Since these alignment models can align sev-eral words with a single topic, only the first occur-rence is kept for consecutive repetitions of the sametopic.
These models are expected to correct some er-rors made by LDA, and to assign in particular wordspreviously associated with discarded topics to morelikely ones.In our experiments the statistical word alignmenttoolkit GIZA++ (Och and Ney, 2003) is used totrain the so-called IBM models 1-4 as well as theHMM model.
To be able to train the most informa-tive IBM model 4, the following training pipelinewas considered: 5 iterations of IBM1, 5 iterationsof HMM, 3 iterations of IBM3 and 3 iterations ofIBM4.
The IBM4 model obtained at the last iter-ation is finally used to align words and topics.
Inorder to improve alignment, IBM models are usu-ally trained in both directions (words towards con-cepts and vice versa) and symmetrised by combin-ing them.
For this purpose, we resorted to the defaultsymmetrization heuristics used by MOSES, a widelyused machine translation system toolkit (Koehn etal., 2007).3.3 Alignment with Integer LinearProgramming (ILP)Another approach to the re-alignment of LDA out-puts is based on a general optimisation technique.ILP is a widely used tool for modelling and solv-ing combinatorial optimisation problems.
It broadlyaims at modelling a decision process as a set of equa-tions or inequations (called constraints) which arelinear with regards to so-called decision variables.An ILP is also composed of a linear objective func-tion.
Solving an ILP consists in assigning values todecision variables, such that all constraints are sat-isfied and the objective function is optimised.
Werefer to (Chen et al, 2010) for an overview of appli-cations and methods of ILP.We provide two ILP formulations for solving thetopic assignment problem related to a given docu-ment.
They both take as input data an ordered set dof words wi, i = 1...N , a set of K available topicsand, for each word wi ?
d and topic k = 1...K,the natural logarithm of the probability p(k|wi, d)that k is assigned to wi in the considered documentd.
Model [ILP ] simply finds the highest-probabilityassignment of one topic to each word in the doc-ument, such that at most ?max different topics areassigned.
[ILP ] : maxN?i=1K?k=1log(p(k|wi, d)) xik (3)?Kk=1 xik = 1 i (4)yk ?
xik ?
0 i, k (5)?Kk=1 yk ?
?max (6)xik ?
{0, 1} i, kyk ?
{0, 1} kIn this model, decision variable xik is equal to 1 iftopic k is assigned to word wi, and equal to 0 other-wise.
Constraints (4) ensure that exactly one topic isassigned to each word.
Decision variable yk is equalto 1 if topic k is used.
Constraints (5) force vari-able yk to take a value of 1 if at least one variablexik is not null.
Moreover, Constraints (6) limit thetotal number of topics used.
The objective function(3) merely states that we want to maximize the totalprobability of the assignment.
Through this model,our assignment problem is identified as a p-centreproblem (see (ReVelle and Eiselt, 2005) for a surveyon such location problems).Numerical experiments show that [ILP ] tends togive sparse assignments: most of the time, adja-cent words are assigned to different topics even ifthe total number of topics is correct.
To preventthis unnatural behaviour, we modified [ILP ] to con-sider groups of consecutive words instead of isolated76words.
Model [ILP seg] partitions the documentinto segments of consecutive words, and assigns onetopic to each segment, such that at most ?max seg-ments are created.
For the sake of convenience, wedenote by p?
(k|wij , d) =?jl=i log(p(k|wl, d)) thelogarithm of the probability that topic k is assignedto all words from i to j in the current document.
[ILP seg] : maxN?i=1N?j=iK?k=1p?
(k|wij , d) xijk (7)i?j=1N?l=iK?k=1xjlk = 1 i (8)N?i=1N?j=iK?k=1xijk ?
?max (9)xijk ?
{0, 1} i, j, kIn this model, decision variable xijk is equal to 1if topic k is assigned to all words from i to j, and0 otherwise.
Constraints (8) ensure that each wordbelongs to a segment that is assigned a topic.
Con-straints (9) limit the number of segments.
Due tothe small size of the instances considered in this pa-per, both [ILP ] and [ILP seg] are well solved by adirect application of an ILP solver.4 Evaluation and results4.1 MEDIA corpusThe MEDIA corpus is used to evaluate the pro-posed approach and to compare the various con-figurations.
MEDIA is a French corpus related tothe domain of tourism information and hotel book-ing (Bonneau-Maynard et al, 2005).
1,257 dia-logues were recorded from 250 speakers with a wiz-ard of Oz technique (a human agent mimics an auto-matic system).
This dataset contains 17k user utter-ances and 123,538 words, for a total of 2,470 distinctwords.The MEDIA data have been manually transcribedand semantically annotated.
The semantic annota-tion uses 75 concepts (e.g.
location, hotel-state,time-month.
.
.
).
Each concept is supported by a se-quence of words, the concept support.
The null con-cept is used to annotate every words segment thatdoes not support any of the 74 other concepts (anddoes not bear any information wrt the task).
On aver-age, a concept support contains 2.1 words, 3.4 con-cepts are included in a utterance and 32% of the ut-terances are restrained to a single word (generally?yes?
or ?no?).
Table 1 gives the proportions of ut-terances according to the number of concepts in theutterance.# concepts 1 2 3 [4,72]% utterances 49.4 14.1 7.9 28.6Table 1: Proportion of user utterances as a function of thenumber of concepts in the utterance.Notice that each utterance contains at least oneconcept (the null label being considered as a con-cept).
As shown in Table 2, some concepts are sup-ported by few segments.
For example, 33 conceptsare represented by less than 100 concept supports.Considering that, we can foresee that finding thesepoorly represented concepts will be hard for LDA.
[1,100[ [100,500[ [500,1k[ [1k,9k[ [9k,15k]33 21 6 14 1 (null)Table 2: Number of concepts according to their occur-rence range.4.2 Evaluation protocolUnlike previous studies, we chose a fully automaticway to evaluate the systems.
In (Siu and Meng,1999), a manual process is introduced to reject in-duced classes or rules that are not relevant to thetask and also to name the semantic classes with theappropriate label.
Thus, they were able to evaluatetheir semi-supervised annotation on the ATIS cor-pus.
In (Pargellis et al, 2001), the relevance of thegenerated semantic classes was manually evaluatedgiving a mark to each induced semantic rule.To evaluate the unsupervised procedure it is nec-essary to associate each induced topic with a MEDIAconcept.
To that purpose, the reference annotationis used to align topics with MEDIA concepts at theword level.
A co-occurrence matrix is computed andeach topic is associated with its most co-occurringconcept.As MEDIA reference concepts are very fine-grained, we also define a high-level concept hier-77archy containing 18 clusters of concepts.
For ex-ample, a high-level concept payment is created fromthe 4 concepts payment-meansOfPayment, payment-currency, payment-total-amount, payment-approx-amount; a high-level concept location correspondsto 12 concepts (location-country, location-district,location-street, .
.
.
).
Thus, two levels of conceptsare considered for the evaluation: high-level andfine-level.The evaluation is presented in terms of the classi-cal F-measure, defined as a combination of precisionand recall measures.
Two levels are also consideredto measure topic assignment quality:?
alignment corresponds to a full evaluationwhere each word is considered and associatedwith one topic;?
generation corresponds to the set of topics gen-erated for a turn (no order, no word-alignment).4.3 System descriptionsFour systems are evaluated in our experiments.
[LDA] is the result of the unsupervised learningof LDA models using GIBBSLDA++ tool1.
It as-signs the most probable topic to each word occur-rence in a document as described in Section 2.2.This approach requires prior estimation of the num-ber of clusters that are expected to be found in thedata.
To find an optimal number of clusters, we ad-justed the number K of topics around the 75 ref-erence concepts.
2k training iterations were madeusing default values for ?
and ?.
[GIZA] is the system based on the GIZA++toolkit2 which re-aligns for each sentence the topicsequence assigned by [LDA] to word sequence asdescribed in Section 3.2.
[ILP ] and [ILP seg] systems are the results ofthe ILP solver IBM ILOG CPLEX3 applied to themodels described in Section 3.3.For the three last systems, the value ?max has tobe fixed according to the desired concept annota-tion.
As on average a concept support contains 2.1words, ?max is defined empirically according to thenumber of words: with i = [[2, 4]]: ?max = i with1http://gibbslda.sourceforge.net/2http://code.google.com/p/giza-pp/3http://www-01.ibm.com/software/integration/optimization/cplex-optimizer/56 57 58 59 60 61 62 63 64 65 66 6750100150200FmeasureNumber of topicsGIZA ILP ILP_seg LDAFigure 3: F-measure of the high-level concept generationas a function of the number of topics.44 46 48 50 52 54 5650100150200FmeasureNumber of topicsGIZA ILP ILP_seg LDAFigure 4: F-measure of the high-level concept alignmentas a function of the number of topics.i = [[5, 10]] words: ?max = i?
2 and for utterancescontaining more than 10 words: ?max = i/2.For the sake of simplicity, single-word utterancesare processed separately with prior knowledge.
Citynames, months, days or answers (e.g.
?yes?, ?no?,?yeah?)
and numbers are identified in these one-word utterances.4.4 ResultsExamples of topics generated by [LDA], with K =100 topics, are shown in Table 3.Plots comparing the different systems imple-mented w.r.t.
the different evaluation levels in termsof F-measure are reported in Figures 3, 4, 5 and 6(high-level vs fine-level, alignment vs generation).The [LDA] system generates topics which are78Topic 0 Topic 13 Topic 18 Topic 35 Topic 33 Topic 43information time-date sightseeing politeness location answer-yeswords prob.
words prob.
words prob.
words prob.
words prob.
words prob.d?
0.28 du 0.16 de 0.30 au 0.31 de 0.30 oui 0.62plus 0.17 au 0.11 la 0.24 revoir 0.27 Paris 0.12 et 0.02informations 0.16 quinze 0.08 tour 0.02 madame 0.09 la 0.06 absolument 0.008autres 0.10 dix-huit 0.07 vue 0.02 merci 0.08 pre`s 0.06 autre 0.008de?tails 0.03 de?cembre 0.06 Eiffel 0.02 bonne 0.01 proche 0.05 donc 0.007obtenir 0.03 mars 0.06 sur 0.02 journe?e 0.01 Lyon 0.03 jour 0.005alors 0.01 dix-sept 0.04 mer 0.01 villes 0.004 aux 0.02 Notre-Dame 0.004souhaite 0.003 nuits 0.04 sauna 0.01 biento?t 0.003 gare 0.02 d?accord 0.004Table 3: Examples of topics discovered by LDA (K = 100).47 48 49 50 51 52 53 54 55 56 57 5850100150200FmeasureNumber of topicsGIZA ILP ILP_seg LDAFigure 5: F-measure of the fine-level concept generationas a function of the number of topics.correctly correlated with the high-level concepts.
Itcan be observed that the bag of 75 topics reachesan F-measure of 61.5% (Fig.
3).
When not enoughtopics are required from [LDA], induced topics aretoo wide to fit the fine-grained concept annotation ofMEDIA.
On the other hand if too many topics are re-quired, the performance of bag of high-level topicsstays the same while a substantial decrease of theF-measure is observed in the alignment evaluation(Fig.
4).
This effect can be explained by the auto-matic alignment method chosen to transpose topicsinto reference concepts.
Indeed, the increase of thenumber of topics makes them co-occur with manyconcepts, which often leads to assign them to themost frequent concept null in the studied corpus.From the high-level to fine-level concept evalua-tions, results globally decrease by 10%.
An addi-tional global loss of 10% is also observed for boththe generation and alignment scorings.
In the fine-34 36 38 40 42 44 46 4850100150200FmeasureNumber of topicsGIZA ILP ILP_seg LDAFigure 6: F-measure of the fine-level concept alignmentas a function of the number of topics.level evaluation, a maximum F-measure of 52.2%is observed for the generation of 75 topics (Fig.
5)whereas the F-measure decreases to 41.5% in thealignment evaluation (Fig.
6).To conclude on the [LDA] system, we can see thatit generates topics having a good correlation with thehigh-level concepts, seemingly the best representa-tion level between topics and concepts.
From theseresults it seems obvious that an additional step isneeded to obtain a more accurate segmental annota-tion, which is expected with the following systems.The [GIZA] system improves the results.
It isvery likely that the filtering process helps to dis-card the irrelevant topics.
Therefore, the automaticalignment between words and the filtered topics in-duced by [LDA] with IBM models seems more ro-bust when more topics (a higher value for K) is re-quired from [LDA], specifically in high-level con-cept alignment (Fig.
4).79Systems based on the ILP technique perform bet-ter than other systems whatever the evaluation.
Con-sidering [LDA] as the baseline, we can expect sig-nificant gains of performance.
For example, an F-measure of 66% is observed for the ILP systemsconsidering the high-level concept generation for 75topics (Figure 4), where the maximum for [LDA]was 61.5%, and an F-measure of 55% is observed(instead of 50.5% for [LDA]) considering the high-level concept alignment.No significant difference was finally measured be-tween both ILP models for the concept generationevaluations.
Even though [ILP seg] seems to ob-tain slightly better results in the alignment evalua-tion.
This could be expected since [ILP seg] intrin-sically yields alignments with grouped topics, closerto the reference alignment used for the evaluation.It is worth noticing that unlike [LDA] system be-haviour, the results of [ILP ] are not affected whenmore topics are generated by [LDA].
A large num-ber of topics enables [ILP ] to pick up the best topicfor a given segment among in a longer selection list.As for [LDA], the same losses are observed be-tween high-level and fine-level concepts and gener-ation and alignment paradigms.
Nevertheless, an F-measure of 54.8% is observed at the high-level con-cept in alignement evaluation (Figure 4) that corre-sponds to a precision of 56.2% and a recall of 53.5%,which is not so low considering a fully-automatichigh-level annotation system.5 Conclusions and perspectivesIn this paper an unsupervised approach for con-cept extraction and segmental annotation has beenproposed and evaluated.
Based on two steps(topic inventory and assignment with LDA, then re-segmentation with either IBM alignment models orILP) the technique has been shown to offer perfor-mance above 50% for the retrieval of reference con-cepts.
It confirms the applicability of the techniqueto practical tasks with an expected gain in data pro-duction.Future work will investigate the use of n-gramsto increase LDA accuracy to provide better hypothe-ses for the following segmentation method.
Besides,other levels of data representation will be examined(use of lemmas, a priori semantic classes like citynames.
.
. )
in order to better generalise on the data.ACKNOWLEDGEMENTSThis work is supported by the ANR funded projectPORT-MEDIA (www.port-media.org) and the LIAOptimNLP project (www.lia.univ-avignon.fr).ReferencesD.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
Latentdirichlet alocation.
The Journal of Machine LearningResearch, 3:993?1022.H.
Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,and D. Mostefa.
2005.
Semantic annotation of thefrench media dialog corpus.
In Proceedings of the 9thEuropean Conference on Speech Communication andTechnology.A.
Celikyilmaz, D. Hakkani-Tur, and G. Tur.
2010.
Ldabased similarity modeling for question answering.
InProceedings of the NAACL HLT 2010 Workshop on Se-mantic Search, pages 1?9.
Association for Computa-tional Linguistics.Der-San Chen, Robert G. Batson, and Yu Dang.
2010.Applied Integer Programming: Modeling and Solu-tion.
Wiley, January.Stefan Hahn, Marco Dinarelli, Christian Raymond, Fab-rice Lefvre, Patrick Lehnen, Renato De Mori, Alessan-dro Moschitti, Hermann Ney, and Giuseppe Riccardi.2010.
Comparing stochastic approaches to spokenlanguage understanding in multiple languages.
IEEETransactions on Audio, Speech and Language Pro-cessing, PP(99):1.E.
Iosif, A. Tegos, A. Pangos, E. Fosler-Lussier, andA.
Potamianos.
2006.
Unsupervised combination ofmetrics for semantic class induction.
In Proceedingsof the IEEE Spoken Language Technology Workshop,pages 86?89.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of ACL, Companion Volume, pages 177?180,Prague, Czech Republic.F.
Lefe`vre.
2007.
Dynamic bayesian networks anddiscriminative classifiers for multi-stage semantic in-terpretation.
In Proceedings of ICASSP, Honolulu,Hawai.F.
Mairesse, M.
Gas?ic?, F.
Jurc??
?c?ek, S. Keizer, B. Thom-son, K. Yu, and S. Young.
2009.
Spoken language80understanding from unaligned data using discrimina-tive classification models.
In Proceedings of ICASSP,Taipei, Taiwan.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.A.
Pargellis, E. Fosler-Lussier, A. Potamianos, and C.H.Lee.
2001.
Metrics for measuring domain indepen-dence of semantic classes.
In Proceedings of the 7thEuropean Conference on Speech Communication andTechnology.X.H.
Phan, L.M.
Nguyen, and S. Horiguchi.
2008.Learning to classify short and sparse text & web withhidden topics from large-scale data collections.
InProceeding of the 17th international conference onWorld Wide Web, pages 91?100.
ACM.C.
S. ReVelle and H. A. Eiselt.
2005.
Location analysis:A synthesis and survey.
European Journal of Opera-tional Research, 165(1):1?19, August.K.C.
Siu and H.M. Meng.
1999.
Semi-automatic acqui-sition of domain-specific semantic structures.
In Pro-ceedings of the 6th European Conference on SpeechCommunication and Technology.Y.C.
Tam and T. Schultz.
2006.
Unsupervised languagemodel adaptation using latent semantic marginals.
InProceedings of INTERSPEECH, pages 2206?2209.W Ward.
1991.
Understanding Spontaneous Speech.In Proceedings of ICASSP, pages 365?368, Toronto,Canada.81
