Proceedings of the Second Workshop on Statistical Machine Translation, pages 224?227,Prague, June 2007. c?2007 Association for Computational LinguisticsExperiments in Domain Adaptation for Statistical Machine TranslationPhilipp Koehn and Josh Schroederpkoehn@inf.ed.ac.uk, j.schroeder@ed.ac.ukSchool of InformaticsUniversity of Edinburgh2 Buccleuch Place, Edinburgh EH8 9LWScotland, United KingdomAbstractThe special challenge of the WMT 2007shared task was domain adaptation.
Wetook this opportunity to experiment withvarious ways of adapting a statistical ma-chine translation systems to a special do-main (here: news commentary), whenmost of the training data is from a dif-ferent domain (here: European Parliamentspeeches).
This paper also gives a descrip-tion of the submission of the University ofEdinburgh to the shared task.1 Our framework: the Moses MT systemThe open source Moses (Koehn et al, 2007) MTsystem was originally developed at the Universityof Edinburgh and received a major boost through a2007 Johns Hopkins workshop.
It is now used atseveral academic institutions as the basic infrastruc-ture for statistical machine translation research.The Moses system is an implementation of thephrase-based machine translation approach (Koehnet al, 2003).
In this approach, an input sentence isfirst split into text chunks (so-called phrases), whichare then mapped one-to-one to target phrases usinga large phrase translation table.
Phrases may be re-ordered, but typically a reordering limit (in our ex-periments a maximum movement over 6 words) isused.
See Figure 1 for an illustration.Phrase translation probabilities, reordering prob-abilities and language model probabilities are com-bined to give each possible sentence translation ascore.
The best-scoring translation is searched for bythe decoding algorithm and outputted by the systemas the best translation.
The different system compo-nents hi (phrase translation probabilities, languageFigure 1: Phrase-based statistical machine transla-tion model: Input is split into text chunks (phrases)which are mapped using a large phrase translationtable.
Phrases are mapped one-to-one, and may bereordered.model, etc.)
are combined in a log-linear model toobtain the score for the translation e for an input sen-tence f:score(e, f) = exp?i?i hi(e, f) (1)The weights of the components ?i are set by adiscriminative training method on held-out develop-ment data (Och, 2003).
The basic components usedin our experiments are: (a) two phrase translationprobabilities (both p(e|f) and p(f |e)), (b) two wordtranslation probabilities (both p(e|f) and p(f |e)),(c) phrase count, (d) output word count, (e) languagemodel, (f) distance-based reordering model, and (g)lexicalized reordering model.For a more detailed description of this model,please refer to (Koehn et al, 2005).2 Domain adaptionSince training data for statistical machine translationis typically collected opportunistically from wher-ever it is available, the application domain for a ma-chine translation system may be very different fromthe domain of the system?s training data.For the WMT 2007 shared task, the challenge wasto use a large amount of out-of-domain training data224(about 40 million words) combined with a muchsmaller amount of in-domain training data (about 1million words) to optimize translation performanceon that particular domain.
We carried out these ex-periments on French?English.2.1 Only out-of-domain training dataThe first baseline system is trained only on the out-of-domain Europarl corpus, which has the followingcorpus statistics:French EnglishSentences 1,257,419Words 37,489,556 33,787,8902.2 Only in-domain training dataThe second baseline system is trained only on thein-domain NewsCommentary corpus.
This corpusis much smaller:French EnglishSentences 42,884Words 1,198,041 1,018,5032.3 Combined training dataTo make use of all the training data, the straight-forward way is to simply concatenate the two train-ing corpora and use the combined data for bothtranslation model and language model training.
Inour situation, however, the out-of-domain trainingdata overwhelms the in-domain training data due tothe sheer relative size.
Hence, we do not expect thebest performance from this simplistic approach.2.4 In-domain language modelOne way to force a drift to the jargon of the targetdomain is the use of the language model.
In our nextsetup, we used only in-domain data for training thelanguage model.
This enables the system to use allthe translation knowledge from the combined cor-pus, but it gives a preference to word choices thatare dominant in the in-domain training data.2.5 Interpolated language modelEssentially, the goal of our subsequent approaches isto make use of all the training data, but to include apreference for the in-domain jargon by giving moreweight to the in-domain training data.
This and thenext approach explore methods to bias the languagemodel, while the final approach biases the transla-tion model.0.60.2 0.3 0.4 0.5 0.7 0.8157158159160161162163164weightperplexityFigure 2: Interpolating in-domain and out-of-domain language models: effect of interpolationweight on perplexity of LM on development set.We trained two language models, one for each theout-of-domain and the in-domain training data.
Lan-guage modeling software such as the SRILM toolkitwe used (Stolke, 2002) allows the interpolation ofthese language models.
When interpolating, we givethe out-of-domain language model a weight in re-spect to the in-domain language model.Since we want to obtain a language model thatgives us the best performance on the target domain,we set this weight so that the perplexity of the de-velopment set from that target domain is optimized.We searched for the optimal weight setting by sim-ply testing a set of weights and focusing on the mostpromising range of weights.Figure 2 displays all the weights we explored dur-ing this process and the corresponding perplexity ofthe resulting language model on the development set(nc-dev2007).
The optimal weight can be picked outeasily from this very smooth curve.2.6 Two language modelsThe log-linear modeling approach of statistical ma-chine translation enables a straight-forward combi-nation of the in-domain and out-of-domain languagemodels.
We included them as two separate fea-tures, whose weights are set with minimum errorrate training.
The relative weight for each model isset directly by optimizing translation performance.2.7 Two translation modelsFinally, besides biasing the language model to a spe-cific target domain, we may also bias the translationmodel.
Here, we take advantage of a feature of theMoses decoder?s factored translation model frame-work.
In factored translation models, the representa-225Method %BLEULarge out-of-domain training data 25.11Small in-domain training data 25.88Combined training data 26.69In-domain language model 27.46Interpolated language model 27.12Two language models 27.30Two translation models 27.64Table 1: Results of domain adaptation experimentstion of words is extended to a vector of factors (e.g.,surface form, lemma, POS, morphology).The mapping of an input phrase to an outputphrase is decomposed into several translation andgeneration steps, each using a different translationor generation table, respectively.
Such a decomposi-tion is called a decoding path.A more recent feature of the factored translationmodel framework is the possible use of multiple al-ternative decoding paths.
This alternate decodingpath model was developed by Birch et al (2007).For our purposes, we use two decoding paths, eachconsisting of only one translation step.
One decod-ing path is the in-domain translation table, and theother decoding path is the out-of-domain translationtable.
Again, respective weights are set with mini-mum error rate training.3 Domain adaptation resultsTable 1 shows results of our domain adaptation ex-periments on the development test set (nc-devtest-2007).
The results suggest that the language modelis a useful tool for domain adaptation.
While train-ing on all the data is essential for good performance,using an in-domain language model alone alreadygives fairly high performance (27.46).
The perfor-mance with the interpolated language model (27.12)and two language models (27.30) are similar.
Allperform better than the three baseline approaches.The results also suggest that higher performancecan be obtained by using two translation modelsthrough the Moses decoder?s alternative decodingpath framework.
We saw our best results under thiscondition (27.64).4 WMT 2007 shared task submissionsWe participated in all categories.
Given the four lan-guage pairs, with two translation directions and (ex-cept for Czech) two test domains, this required us tobuild 14 translation systems.We had access to a fairly large computer cluster tocarry out our experiments over the course of a fewweeks.
However, speed issues with the decoder andload issues on the crowded cluster caused us to takea few shortcuts.
Also, a bug crept in to our English?French experiments where we used the wrong deto-kenizer, resulting drop of 2?3 points in %BLEU.4.1 TuningMinimum error rate training is the most time-consuming aspects of the training process.
Due totime constraints, we did not carry out this step for allbut the Czech systems (a new language for us).
Forthe other systems, we re-used weight settings fromour last year?s submission.One of the most crucial outcomes of tuning is aproper weight setting for output length, which is es-pecially important for the BLEU score.
Since thetraining corpus and tokenization changed, our re-used weights are not always optimal in this respect.But only in one case we felt compelled to manuallyadjust the weight for the word count feature, sincethe original setup led to a output/reference length ra-tio of 0.88 on the development test set.4.2 Domain adaptationFor the Europarl test sets, we did not use any do-main adaptation techniques, but simply used eitherjust the Europarl training data or the combined data?
whatever gave the higher score on the develop-ment test set, although scores differed by only about0.1?0.2 %BLEU.In order to be able to re-use the old weights, wewere limited to domain adaptation methods that didnot change the number of components.
We decidedto use the interpolated language model method de-scribed in Section 2.5.
For the different languagepairs, optimal interpolation weights differed:Language pair Weight for Europarl LMFrench?English 0.43Spanish?English 0.41German?English 0.40English?French 0.51English?Spanish 0.42English?German 0.45226Language pair Europarl NewsCommentary%BLEU Length NIST %BLEU Length NISTFrench?English 32.66 0.96 7.94 28.27 1.03 7.50Spanish?English 33.26 1.00 7.82 34.17 1.06 8.35German?English 28.49 0.94 7.32 25.45 1.01 7.19Czech?English ?
?
?
22.68 0.98 6.96English?French 26.76 1.08 6.66 24.38 1.02 6.73English?Spanish 32.55 0.98 7.66 33.59 0.94 8.46English?German 20.59 0.97 6.18 17.06 1.00 6.04English?Czech ?
?
?
12.34 1.02 4.85Table 2: Test set performance of our systems: BLEU and NIST scores, and output/reference length ratio.4.3 Training and decoding parametersWe tried to improve performance by increasingsome of the limits imposed on the training and de-coding setup.
During training, long sentences areremoved from the training data to speed up theGIZA++ word alignment process.
Traditionally, weworked with a sentence length limit of 40.
We foundthat increasing this limit to about 80 gave better re-sults without causing undue problems with runningthe word alignment (GIZA++ increasingly fails andruns much slower with long sentences).We also tried to increase beam sizes and thelimit on the number of translation options per cov-erage span (ttable-limit).
This has shown to be suc-cessful in our experiments with Arabic?English andChinese?English systems.
Surprisingly, increasingthe maximum stack size to 1000 (from 200) andttable-limit to 100 (from 20) has barely any ef-fect on translation performance.
The %BLEU scorechanged only by less than 0.05, and often worsened.4.4 German?English systemThe German?English language pair is especiallychallenging due to the large differences in word or-der.
Collins et al (2005) suggest a method to reorderthe German input before translating using a set ofmanually crafted rules.
In our German?English sub-missions, this is done both to the training data andthe input to the machine translation system.5 ConclusionsOur submission to the WMT 2007 shared task is afairly straight-forward use of the Moses MT systemusing default parameters.
In a sense, we submitteda baseline performance of this system.
BLEU andNIST scores for all our systems on the test sets aredisplayed in Table 2.
Compared to other submittedsystems, these are very good scores, often the bestor second highest scores for these tasks.We made a special effort in two areas: We ex-plored domain adaptation methods for the News-Commentary test sets and we used reordering rulesfor the German?English language pair.AcknowledgmentsThis work was supported in part under the GALE programof the Defense Advanced Research Projects Agency, ContractNo.
HR0011-06-C-0022 and in part under the EuroMatrixproject funded by the European Commission (6th FrameworkProgramme).ReferencesBirch, A., Osborne, M., and Koehn, P. (2007).
CCG supertagsin factored statistical machine translation.
In Proceedingsof the Workshop on Statistical Machine Translation, Prague.Association for Computational Linguistics.Collins, M., Koehn, P., and Kucerova, I.
(2005).
Clause re-structuring for statistical machine translation.
In Proceed-ings of the 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 531?540, Ann Arbor,Michigan.
Association for Computational Linguistics.Koehn, P., Axelrod, A., Mayne, A.
B., Callison-Burch, C., Os-borne, M., and Talbot, D. (2005).
Edinburgh system descrip-tion for the 2005 IWSLT speech translation evaluation.
InProc.
of the International Workshop on Spoken LanguageTranslation.Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico,M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R.,Dyer, C., Bojar, O., Constantin, A., and Herbst, E. (2007).Moses: Open source toolkit for statistical machine transla-tion.
In Proceedings of the Annual Meeting of the Associa-tion for Computational Linguistics, demonstation session.Koehn, P., Och, F. J., and Marcu, D. (2003).
Statistical phrasebased translation.
In Proceedings of the Joint Conference onHuman Language Technologies and the Annual Meeting ofthe North American Chapter of the Association of Computa-tional Linguistics (HLT-NAACL).Och, F. J.
(2003).
Minimum error rate training in statisticalmachine translation.
In Hinrichs, E. and Roth, D., editors,Proceedings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167.Stolke, A.
(2002).
SRILM - an extensible language modelingtoolkit.
In Proceedings of the International Conference onSpoken Language Processing.227
