Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 54?61,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsExploring Large-Data Issues in the Curriculum:A Case Study with MapReduceJimmy LinThe iSchool, College of Information StudiesLaboratory for Computational Linguistics and Information ProcessingUniversity of Maryland, College Parkjimmylin@umd.eduAbstractThis paper describes the design of a pilot re-search and educational effort at the Univer-sity of Maryland centered around technologiesfor tackling Web-scale problems.
In the con-text of a ?cloud computing?
initiative lead byGoogle and IBM, students and researchers areprovided access to a computer cluster runningHadoop, an open-source Java implementationof Google?s MapReduce framework.
Thistechnology provides an opportunity for stu-dents to explore large-data issues in the con-text of a course organized around teams ofgraduate and undergraduate students, in whichthey tackle open research problems in the hu-man language technologies.
This design rep-resents one attempt to bridge traditional in-struction with real-world, large-data researchchallenges.1 IntroductionOver the past couple of decades, the field of compu-tational linguistics, and more broadly, human lan-guage technologies, has seen the emergence andlater dominance of empirical techniques and data-driven research.
Concomitant with this trend is therequirement of systems and algorithms to handlelarge quantities of data.
Banko and Brill (2001)were among the first to demonstrate the importanceof dataset size as a significant factor governing pre-diction accuracy in a supervised machine learningtask.
In fact, they argue that size of training setis perhaps more important than the choice of ma-chine learning algorithm itself.
Similarly, exper-iments in question answering have shown the ef-fectiveness of simple pattern-matching techniqueswhen applied to large quantities of data (Brill etal., 2001).
More recently, this line of argumenta-tion has been echoed in experiments with large-scalelanguage models.
Brants et al (2007) show thatfor statistical machine translation, a simple smooth-ing method (dubbed Stupid Backoff) approaches thequality of Kneser-Ney Smoothing as the amount oftraining data increases, and with the simple methodone can process significantly more data.Given these observations, it is important to in-tegrate discussions of large-data issues into anycourse on human language technology.
Most ex-isting courses focus on smaller-sized problems anddatasets that can be processed on students?
personalcomputers, making them ill-prepared to cope withthe vast quantities of data in operational environ-ments.
Even when larger datasets are leveraged inthe classroom, they are mostly used as static re-sources.
Thus, students experience a disconnect asthey transition from a learning environment to onewhere they work on real-world problems.Nevertheless, there are at least two major chal-lenges associated with explicit treatment of large-data issues in an HLT curriculum:?
The first concerns resources: it is unclear whereone might acquire the hardware to support ed-ucational activities, especially if such activitiesare in direct competition with research.?
The second involves complexities inherentlyassociated with parallel and distributed pro-cessing, currently the only practical solution tolarge-data problems.
For any course, it is diffi-54cult to retain focus on HLT-relevant problems,since the exploration of large-data issues ne-cessitates (time-consuming) forays into paralleland distributed computing.This paper presents a case study that grappleswith the issues outlined above.
Building on previ-ous experience with similar courses at the Univer-sity of Washington (Kimball et al, 2008), I presenta pilot ?cloud computing?
course currently under-way at the University of Maryland that leverages acollaboration with Google and IBM, through whichstudents are given access to hardware resources.
Tofurther alleviate the first issue, research is broughtinto alignment with education by structuring a team-oriented, project-focused course.
The core idea is toorganize teams of graduate and undergraduate stu-dents focused on tackling open research problems innatural language processing, information retrieval,and related areas.
Ph.D. students serve as leaderson projects related to their research, and are giventhe opportunity to serve as mentors to undergradu-ate and masters students.Google?s MapReduce programming framework isan elegant solution to the second issue raised above.By providing a functional abstraction that isolatesthe programmer from parallel and distributed pro-cessing issues, students can focus on solving theactual problem.
I first provide the context for thisacademic?industrial collaboration, and then moveon to describe the course setup.2 Cloud Computing and MapReduceIn October 2007, Google and IBM jointly an-nounced the Academic Cloud Computing Initiative,with the goal of helping both researchers and stu-dents address the challenges of ?Web-scale?
com-puting.
The initiative revolves around Google?sMapReduce programming paradigm (Dean andGhemawat, 2004), which represents a proven ap-proach to tackling data-intensive problems in a dis-tributed manner.
Six universities were involvedin the collaboration at the outset: Carnegie MellonUniversity, Massachusetts Institute of Technology,Stanford University, the University of California atBerkeley, the University of Maryland, and Univer-sity of Washington.
I am the lead faculty at the Uni-versity of Maryland on this project.As part of this initiative, IBM and Google havededicated a large cluster of several hundred ma-chines for use by faculty and students at the partic-ipating institutions.
The cluster takes advantage ofHadoop, an open-source implementation of MapRe-duce in Java.1 By making these resources available,Google and IBM hope to encourage faculty adop-tion of cloud computing in their research and alsointegration of the technology into the curriculum.MapReduce builds on the observation that manyinformation processing tasks have the same basicstructure: a computation is applied over a large num-ber of records (e.g., Web pages) to generate par-tial results, which are then aggregated in some fash-ion.
Naturally, the per-record computation and ag-gregation function vary according to task, but the ba-sic structure remains fixed.
Taking inspiration fromhigher-order functions in functional programming,MapReduce provides an abstraction at the point ofthese two operations.
Specifically, the programmerdefines a ?mapper?
and a ?reducer?
with the follow-ing signatures:map: (k1, v1)?
[(k2, v2)]reduce: (k2, [v2])?
[(k3, v3)]Key/value pairs form the basic data structure inMapReduce.
The mapper is applied to every inputkey/value pair to generate an arbitrary number of in-termediate key/value pairs.
The reducer is applied toall values associated with the same intermediate keyto generate output key/value pairs.
This two-stageprocessing structure is illustrated in Figure 1.Under the framework, a programmer need onlyprovide implementations of the mapper and reducer.On top of a distributed file system (Ghemawat et al,2003), the runtime transparently handles all otheraspects of execution, on clusters ranging from afew to a few thousand nodes.
The runtime is re-sponsible for scheduling map and reduce workerson commodity hardware assumed to be unreliable,and thus is tolerant to various faults through a num-ber of error recovery mechanisms.
The runtime alsomanages data distribution, including splitting the in-put across multiple map workers and the potentiallyvery large sorting problem between the map and re-duce phases whereby intermediate key/value pairsmust be grouped by key.1http://hadoop.apache.org/55inputinputinputinputmapmapmapmapinputinputinputinputBarrier:?group?values?by?keysreducereducereduceoutputoutputoutputFigure 1: Illustration of the MapReduce framework: the?mapper?
is applied to all input records, which generatesresults that are aggregated by the ?reducer?.The biggest advantage of MapReduce from a ped-agogical point of view is that it allows an HLTcourse to retain its focus on applications.
Divide-and-conquer algorithms running on multiple ma-chines are currently the only effective strategy fortackling Web-scale problems.
However, program-ming parallel and distributed systems is a difficulttopic for students to master.
Due to communica-tion and synchronization issues, concurrent opera-tions are notoriously challenging to reason about?unanticipated race conditions are hard to detect andeven harder to debug.
MapReduce allows the pro-grammer to offload these problems (no doubt im-portant, but irrelevant from the perspective of HLT)onto the runtime, which handles the complexities as-sociated with distributed processing on large clus-ters.
The functional abstraction allows a student tofocus on problem solving, not managing the detailsof error recovery, data distribution, etc.3 Course DesignThis paper describes a ?cloud computing?
course atthe University of Maryland being offered in Spring2008.
The core idea is to assemble small teams ofgraduate and undergraduate students to tackle re-search problems, primarily in the areas of informa-tion retrieval and natural language processing.
Ph.D.students serve as team leaders, overseeing smallgroups of masters and undergraduates on topics re-lated to their doctoral research.
The roles of ?teamleader?
and ?team member?
are explicitly assignedat the beginning of the semester, and are associatedwith different expectations and responsibilities.
Allcourse material and additional details are availableon the course homepage.23.1 Objectives and GoalsI identified a list of desired competencies for stu-dents to acquire and refine throughout the course:?
Understand and be able to articulate the chal-lenges associated with distributed solutions tolarge-scale problems, e.g., scheduling, loadbalancing, fault tolerance, memory and band-width limitations, etc.?
Understand and be able to explain the conceptsbehind MapReduce as one framework for ad-dressing the above issues.?
Understand and be able to express well-knownalgorithms (e.g., PageRank) in the MapReduceframework.?
Understand and be able to reason about engi-neering tradeoffs in alternative approaches toprocessing large datasets.?
Gain in-depth experience with one researchproblem in Web-scale information processing(broadly defined).With respect to the final bullet point, the studentsare expected to acquire the following abilities:?
Understand how current solutions to the par-ticular research problem can be cast into theMapReduce framework.?
Be able to explain what advantages the MapRe-duce framework provides over existing ap-proaches (or disadvantages if a MapReduceformulation turns out to be unsuitable for ex-pressing the problem).?
Articulate how adopting the MapReduceframework can potentially lead to advances inthe state of the art by enabling processing notpossible before.I assumed that all students have a strong foun-dation in computer science, which was operational-ized in having completed basic courses in algo-rithms, data structures, and programming languages2http://www.umiacs.umd.edu/?jimmylin/cloud-computing/56WeekMondayWednesday1 2 3HadoopBootCamp3 4 5 6Project Meetings:PhIProposalPresentations6 7 8 9PhaseIGtSk10 11 12Project Meetings:PhaseIIGuestSpeakers13 14 15Final ProjectPresentationsFigure 2: Overview of course schedule.
(in practice, this was trivially met for the graduatestudents, who all had undergraduate degrees in com-puter science).
I explicitly made the decision thatprevious courses in parallel programming, systems,or networks was not required.
Finally, prior experi-ence with natural language processing, informationretrieval, or related areas was not assumed.
How-ever, strong competency in Java programming was astrict requirement, as the Hadoop implementation ofMapReduce is based in Java.In the project-oriented setup, the team leaders(i.e., Ph.D. students) have additional roles to play.One of the goals of the course is to give them experi-ence in mentoring more junior colleagues and man-aging a team project.
As such, they were expected toacquire real-world skills in project organization andmanagement.3.2 Schedule and Major ComponentsAs designed, the course spans a standard fifteenweek semester, meeting twice a week (Monday andWednesday) for one hour and fifteen minutes eachsession.
The general setup is shown in Figure 2.
Asthis paper goes to press (mid-April), the course justconcluded Week 11.During the first three weeks, all students are im-mersed in a ?Hadoop boot camp?, where they areintroduced to the MapReduce programming frame-work.
Material was adapted from slides developedby Christophe Bisciglia and his colleagues fromGoogle, who have delivered similar content in var-ious formats.3 As it was assumed that all studentshad strong foundations in computer science, thepace of the lectures was brisk.
The themes of thefive boot camp sessions are listed below:?
Introduction to parallel/distributed processing?
From functional programming to MapReduceand the Google File System (GFS)?
?Hello World?
MapReduce lab?
Graph algorithms with MapReduce?
Information retrieval with MapReduceA brief overview of parallel and distributed pro-cessing provides a natural transition into abstrac-tions afforded by functional programming, the inspi-ration behind MapReduce.
That in turn provides thecontext to introduce MapReduce itself, along withthe distributed file system upon which it depends.The final two lectures focus on specific case stud-ies of MapReduce applied to graph analysis and in-formation retrieval.
The first covers graph searchand PageRank, while the second covers algorithmsfor information retrieval.
With the exception of the?Hello World?
lab session, all lecture content wasdelivered at the conceptual level, without specificreference to the Hadoop API and implementationdetails (see Section 5 for discussion).
The bootcamp is capped off with a programming exercise(implementation of PageRank) to ensure that stu-dents have a passing knowledge of MapReduce con-cepts in general and the Hadoop API in particular.Concurrent with the boot camp, team leaders areexpected to develop a detailed plan of research:what they hope to accomplish, specific tasks thatwould lead to the goals, and possible distribution ofthose tasks across team members.
I recommend thateach project be structured into two phases: the firstphase focusing on how existing solutions might berecast into the MapReduce framework, the secondphase focusing on interesting extensions enabled byMapReduce.
In addition to the detailed research3http://code.google.com/edu/parallel/57plan, the leaders are responsible for organizing intro-ductory material (papers, tutorials, etc.)
since teammembers are not expected to have any prior experi-ence with the research topic.The majority of the course is taken up by the re-search project itself.
The Monday class sessionsare devoted to the team project meetings, and theteam leader is given discretion on how this is man-aged.
Typical activities include evaluation of deliv-erables (code, experimental results, etc.)
from theprevious week and discussions of plans for the up-coming week, but other common uses of the meetingtime include whiteboard sessions and code review.During the project meetings I circulate from groupto group to track progress, offer helpful suggestions,and contribute substantially if possible.To the extent practical, the teams adopt standardbest practices for software development.
Studentsuse Eclipse as the development environment andtake advantage of a plug-in that provides a seamlessinterface to the Hadoop cluster.
Code is shared viaSubversion, with both project-specific repositoriesand a course-wide repository for common libraries.A wiki is also provided as a point of collaboration.Concurrent with the project meetings on Mon-days, a speaker series takes place on Wednesdays.Attendance for students is required, but otherwisethe talks are open to the public.
One of the goalsfor these invited talks is to build an active commu-nity of researchers interested in large datasets anddistributed processing.
Invited talks can be clas-sified into one of two types: infrastructure-focusedand application-focused.
Examples of the first in-clude alternative architectures for processing largedatasets and dynamic provisioning of computingservices.
Examples of the second include surveyof distributed data mining techniques and Web-scalesentiment analysis.
It is not a requirement for thetalks to focus on MapReduce per se?rather, an em-phasis on large-data issues is the thread that weavesall these presentations together.3.3 Student EvaluationAt the beginning of the course, students are assignedspecific roles (team leader or team member) andare evaluated according to different criteria (both ingrade components and relative weights).The team leaders are responsible for producingthe detailed research plan at the beginning of thesemester.
The entire team is responsible for threecheckpoint deliverables throughout the course: aninitial oral presentation outlining their plans, a shortinterim progress report at roughly the midpoint ofthe semester, and a final oral presentation accompa-nied by a written report at the end of the semester.On a weekly basis, I request from each stu-dent a status report delivered as a concise email: aparagraph-length outline of progress from the previ-ous week and plans for the following week.
This,coupled with my observations during each projectmeeting, provides the basis for continuous evalua-tion of student performance.4 Course ImplementationCurrently, 13 students (7 Ph.D., 3 masters, 3 under-graduates) are involved in the course, working onsix different projects.
Last fall, as planning wasunderway, Ph.D. students from the Laboratory forComputational Linguistics and Information Process-ing at the University of Maryland were recruitedas team leaders.
Three of them agreed, developingprojects around their doctoral research?these repre-sent cases with maximal alignment of research andeducational goals.
In addition, the availability of thisopportunity was announced on mailing lists, whichgenerated substantial interest.
Undergraduates wererecruited from the Computer Science honors pro-gram; since it is a requirement for those students tocomplete an honors project, this course provided asuitable vehicle for satisfying that requirement.Three elements are necessary for a successfulproject: interested students, an interesting researchproblem of appropriate scope, and the availabilityof data to support the work.
I served as a brokerfor all three elements, and eventually settled on fiveprojects that satisfied all the desiderata (one projectwas a later addition).
As there was more interestthan spaces available for team members, it was pos-sible to screen for suitable background and matchinginterests.
The six ongoing projects are as follows:?
Large-data statistical machine translation?
Construction of large latent-variable languagemodels?
Resolution of name mentions in large emailarchives58?
Network analysis for enhancing biomedicaltext retrieval?
Text-background separation in children?s pic-ture books?
High-throughput biological sequence align-ment and processingOf the six projects, four of them fall squarely inthe area of human language technology: the first twoare typical of problems in natural language process-ing, while the second two are problems in informa-tion retrieval.
The final two projects represent at-tempts to push the boundaries of the MapReduceparadigm, into image processing and computationalbiology, respectively.
Short project descriptions canbe found on the course homepage.5 Pedagogical DiscussionThe design of any course is an exercise in tradeoffs,and this pilot project is no exception.
In this section,I will attempt to justify course design decisions anddiscuss possible alternatives.At the outset, I explicitly decided against a ?tradi-tional?
course format that would involve carefully-paced delivery of content with structured exercises(e.g., problem sets or labs).
Such a design wouldperhaps be capped off with a multi-week finalproject.
The pioneering MapReduce course at theUniversity of Washington represents an example ofthis design (Kimball et al, 2008), combining sixweeks of standard classroom instruction with an op-tional four week final project.
As an alternative, I or-ganized my course around the research project.
Thischoice meant that the time devoted to direct instruc-tion on foundational concepts was very limited, i.e.,the three-week boot camp.One consequence of the boot-camp setup is somedisconnect between the lecture material and imple-mentation details.
Students were expected to rapidlytranslate high-level concepts into low-level pro-gramming constructs and API calls without muchguidance.
There was only one ?hands on?
sessionin the boot camp, focusing on more mundane is-sues such as installation, configuration, connectingto the server, etc.
Although that session also in-cluded overview of a simple Hadoop program, thatby no means was sufficient to yield in-depth under-standing of the framework.The intensity of the boot camp was mitigated bythe composition of the students.
Since students wereself-selected and further screened by me in terms oftheir computational background, they represent thehighest caliber of students at the university.
Further-more, due to the novel nature of the material, stu-dents were highly motivated to rapidly acquire what-ever knowledge was necessary outside the class-room.
In reality, the course design forced studentsto spend the first few weeks of the project simulta-neously learning about the research problem and thedetails of the Hadoop framework.
However, this didnot appear to be a problem.Another interesting design choice is the mixingof students with different backgrounds in the sameclassroom environment.
Obviously, the graduatestudents had stronger computer science backgroundsthan the undergraduates overall, and the team lead-ers had far more experience on the particular re-search problem than everyone else by design.
How-ever, this was less an issue than one would have ini-tially thought, partially due to the selection of thestudents.
Since MapReduce requires a different ap-proach to problem solving, significant learning wasrequired from everyone, independent of prior expe-rience.
In fact, prior knowledge of existing solutionsmay in some cases be limiting, since it precludes afresh approach to the problem.6 Course EvaluationHas the course succeeded?
Before this question canbe meaningfully answered, one needs to define mea-sures for quantifying success.
Note that the evalua-tion of the course is distinct from the evaluation ofstudent performance (covered in Section 3.3).
Giventhe explicit goal of integrating research and educa-tion, I propose the following evaluation criteria:?
Significance of research findings, as measuredby the number of publications that arise directlyor indirectly from this project.?
Placement of students, e.g., internships andpermanent positions, or admission to graduateprograms (for undergraduates).?
Number of projects with sustained research ac-tivities after the conclusion of the course.59?
Amount of additional research support fromother funding agencies (NSF, DARPA, etc.
)for which the projects provided preliminary re-sults.Here I provide an interim assessment, as this pa-per goes to press in mid-April.
Preliminary resultsfrom the projects have already yielded two sepa-rate publications: one on statistical machine trans-lation (Dyer et al, 2008), the other on informationretrieval (Elsayed et al, 2008).
In terms of studentplacement, I believe that experience from this coursehas made several students highly attractive to com-panies such as Google, Yahoo, and Amazon?bothfor permanent positions and summer internships.
Itis far too early to have measurable results with re-spect to the final two criteria, but otherwise prelim-inary assessment appears to support the overall suc-cess of this course.In addition to the above discussion, it is also worthmentioning that the course is emerging as a nexusof cloud computing on the Maryland campus (andbeyond), serving to connect multiple organizationsthat share in having large-data problems.
Already,the students are drawn from a variety of academicunits on campus:?
The iSchool?
Department of Computer Science?
Department of Linguistics?
Department of GeographyAnd cross-cut multiple research labs:?
The Institute for Advanced Computer Studies?
The Laboratory for Computational Linguisticsand Information Processing?
The Human-Computer Interaction Laboratory?
The Center for Bioinformatics and Computa-tional BiologyOff campus, there are ongoing collaborationswith the National Center for Biotechnology In-formation (NCBI) within the National Library ofMedicine (NLM).
Other information-based organi-zations around the Washington, D.C. area have alsoexpressed interest in cloud computing technology.7 ConclusionThis paper describes the design of an integrated re-search and educational initiative focused on tacklingWeb-scale problems in natural language processingand information retrieval using MapReduce.
Pre-liminary assessment indicates that this project rep-resents one viable approach to bridging classroominstruction and real-world research challenges.
Withthe advent of clusters composed of commodity ma-chines and ?rent-a-cluster?
services such as Ama-zon?s EC2,4 I believe that large-data issues can bepractically incorporated into an HLT curriculum at areasonable cost.AcknowledgmentsI would like to thank the generous hardware sup-port of IBM and Google via the Academic CloudComputing Initiative.
Specifically, thanks go outto Dennis Quan and Eugene Hung from IBM fortheir tireless support of our efforts.
This coursewould not have been possible without the participa-tion of 13 enthusiastic, dedicated students, for whichI feel blessed to have the opportunity to work with.In alphabetical order, they are: Christiam Camacho,George Caragea, Aaron Cordova, Chris Dyer, TamerElsayed, Denis Filimonov, Chang Hu, Greg Jablon-ski, Alan Jackoway, Punit Mehta, Alexander Mont,Michael Schatz, and Hua Wei.
Finally, I would liketo thank Esther and Kiri for their kind support.ReferencesMichele Banko and Eric Brill.
2001.
Scaling to very verylarge corpora for natural language disambiguation.
InProceedings of the 39th Annual Meeting of the As-sociation for Computational Linguistics (ACL 2001),pages 26?33, Toulouse, France.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 858?867, Prague, Czech Re-public.Eric Brill, Jimmy Lin, Michele Banko, Susan Dumais,and Andrew Ng.
2001.
Data-intensive question an-swering.
In Proceedings of the Tenth Text REtrieval4http://aws.amazon.com/ec260Conference (TREC 2001), pages 393?400, Gaithers-burg, Maryland.Jeffrey Dean and Sanjay Ghemawat.
2004.
MapReduce:Simplified data processing on large clusters.
In Pro-ceedings of the 6th Symposium on Operating SystemDesign and Implementation (OSDI 2004), pages 137?150, San Francisco, California.Chris Dyer, Aaron Cordova, Alex Mont, and Jimmy Lin.2008.
Fast, easy, and cheap: Construction of statisticalmachine translation models with MapReduce.
In Pro-ceedings of the Third Workshop on Statistical MachineTranslation at ACL 2008, Columbus, Ohio.Tamer Elsayed, Jimmy Lin, and Douglas Oard.
2008.Pairwise document similarity in large collections withMapReduce.
In Proceedings of the 46th Annual Meet-ing of the Association for Computational Linguistics(ACL 2008), Companion Volume, Columbus, Ohio.Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Le-ung.
2003.
The Google File System.
In Proceedingsof the 19th ACM Symposium on Operating SystemsPrinciples (SOSP-03), pages 29?43, Bolton Landing,New York.Aaron Kimball, Sierra Michels-Slettvet, and ChristopheBisciglia.
2008.
Cluster computing for Web-scaledata processing.
In Proceedings of the 39th ACMTechnical Symposium on Computer Science Education(SIGCSE 2008), pages 116?120, Portland, Oregon.61
