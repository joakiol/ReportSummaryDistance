Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 131?135,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsLink Type Based Pre-Cluster Pair Model for Coreference ResolutionYang Song?, Houfeng Wang?
and Jing Jiang?
?Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China?School of Information Systems, Singapore Management University, Singapore{ysong, wanghf}@pku.edu.cn, jingjiang@smu.edu.sgAbstractThis paper presents our participation in theCoNLL-2011 shared task, Modeling Unre-stricted Coreference in OntoNotes.
Corefer-ence resolution, as a difficult and challengingproblem in NLP, has attracted a lot of atten-tion in the research community for a long time.Its objective is to determine whether two men-tions in a piece of text refer to the same en-tity.
In our system, we implement mention de-tection and coreference resolution seperately.For mention detection, a simple classificationbased method combined with several effectivefeatures is developed.
For coreference resolu-tion, we propose a link type based pre-clusterpair model.
In this model, pre-clustering of allthe mentions in a single document is first per-formed.
Then for different link types, differentclassification models are trained to determinewheter two pre-clusters refer to the same en-tity.
The final clustering results are generatedby closest-first clustering method.
Official testresults for closed track reveal that our methodgives a MUC F-score of 59.95%, a B-cubedF-score of 63.23%, and a CEAF F-score of35.96% on development dataset.
When usinggold standard mention boundaries, we achieveMUC F-score of 55.48%, B-cubed F-score of61.29%, and CEAF F-score of 32.53%.1 IntroductionThe task of coreference resolution is to recognizeall the mentions (also known as noun phrases, in-cluding names, nominal mentions and pronouns)in a text and cluster them into equivalence classeswhere each quivalence class refers to a real-worldentity or abstract concept.
The CoNLL-2011 sharedtask1 uses OntoNotes2 as the evaluation corpus.
Thecoreference layer in OntoNotes constitutes one partof a multi-layer, integrated annotation of the shal-low semantic structures in the text with high inter-annotator agreement.
In addition to coreference,this data set is also tagged with syntactic trees, highcoverage verb and some noun propositions, partialverb and noun word senses, and 18 named entitytypes.
The main difference between OntoNotes andanother wellknown coreference dataset ACE is thatthe former does not label any singleton entity clus-ter, which has only one reference in the text.
We candelete all the singleton clusters as a postprocessingstep for the final results.
Alternatively, we can alsofirst train a classifier to separate singleton mentionsfrom the rest and apply this mention detection stepbefore coreference resolution.
In this work we adoptthe second strategy.In our paper, we use a traditional learning basedpair-wise model for this task.
For mention detec-tion, we first extract all the noun phrases in the textand then use a classification model combined withsome effective features to determine whether eachnoun phrase is actually a mention.
The features in-clude word features, POS features in the given nounphrase and its context, string matching feature inits context, SRL features, and named entity featuresamong others.
More details will be given in Sec-tion 3.
From our in-house experiments, the final F-scores for coreference resolution can be improvedby this mention detection part.
For coreference res-1http://conll.bbn.com2http://www.bbn.com/ontonotes/131Features describing ci or cjWords The first and last words of the given NP in ci ( or cj) , also including the words in thecontext with a window size 2POS Tags The part of speech tags corresponding to the wordsPronoun Y if mentions in ci( or cj) are pronouns; else NDefinite Y if mentions in ci( or cj) are definite NP; else NDemonstrative Y if mentions in ci( or cj) are demonstrative NP; else NNumber Singular or Plural, determined using a data file published by Bergsma and Lin (2006)Gender Male, Female, Neuter, or Unknown, determined using a data file published by Bergsmaand Lin (2006)Semantic Class Semantic Classes are given by OntoNotes for named entitiesMentino Type Common Noun Phrases or PronounsTable 1: The feature set describing ci or cj .olution, a traditinal pair-wise model is applied, inwhich we first use exact string matching to generatesome pre-clusters.
It should be noted that each pro-noun must be treated as a singleton pre-cluster, be-cause they are not like names or nominal mentions,which can be resolved effectively with exact stringmatching.
We then implement a classification basedpre-cluster pair model combined with several ef-fective coreference resolution features to determinewhether two pre-clusters refer to the same entity.
Fi-nally, we use closest-first clustering method to linkall the coreferential pre-clusters and generate the fi-nal cluster results.
As mentioned before, mentionshave three types: names, nominal mentions and pro-nouns.
Among them pronouns are very differentfrom names and nominal mentions, because they canonly supply limited information literally.
So we de-fine three kinds of link types for pre-cluster pairs:NP-NP link, NP-PRP link and PRP-PRP link.
(HereNP means Noun Phrases and PRP means Pronom-inal Phrases.)
One link represents one pre-clusterpair.
Intuitively, different link types tend to use dif-ferent features to determine whether this kind of linkis coreferential or not.
We implement three kindsof pre-cluster pair model based on three link types.Experimental results show that combined with out-puts from different link type based pre-cluster pairmodel can give better results than using an uni-fied classification model for three different kinds oflink types.
For all the classification models, we useopennlp.maxent3 package.The rest of this paper is organized as follows.
Sec-tion 2 describes our mention detection method.
Wediscuss our link type based pre-cluster pair modelfor coreference resolution in Section 3, evaluate it inSection 4, and conclude in Section 5.2 Mention DetectionWe select all the noun phrases tagged by theOntoNotes corpus as mention candidates and im-plement a classification-based model combinedwith several commonly used features to determinewhether a given noun phrase is a mention.
The fea-tures are given below:?
Word Features - They include the first word and thelast word in each given noun phrase.
We also usewords in the context of the noun phrase within awindow size of 2.?
POS Features - We use the part of speech tags ofeach word in the word features.?
Position Features - These features indicate wherethe given noun phrase appears in its sentence: be-gining, middle, or end.?
SRL Features - The Semantic Role of the givennoun phrase in its sentence.?
Verb Features - The verb related to the SemanticRole of the given noun phrase.3http://incubator.apache.org/opennlp/132Features describing the relationship between ci and cjDistance The minimum distance between mentions in ci and cjString Match Y if mentions are the same string; else NSubstring Match Y if one mention is a substring of another; else NLevenshtein Distance Levenshtein Distance between the mentionsNumber Agreement Y if the mentions agree in number; else NGender Agreement Y if the mentions agree in gender; else NN & G Agreement Y if mentions agree in both number and gender; else NBoth Pronouns Y if the mentions are both pronouns; else NVerb Agreement Y if the mentions have the same verb.SRL Agreement Y if the mentions have the same semantic rolePosition Agreement Y if the mentions have the same position (Beginning, Middle or End) in sentencesTable 2: The feature set describing the relationship between ci and cj .?
Entity Type Features - The named entity type for thegiven noun phrase.?
String Matching Features - True if there is anothernoun phrase wich has the same string as the givennoun phrase in the context.?
Definite NP Features - True if the given noun phraseis a definite noun phrase.?
Demonstrative NP Features - True if the given nounphrase is a demonstrative noun phrase.?
Pronoun Features - True if the given noun phrase isa pronoun.Intutively, common noun phrases and pronounsmight have different feature preferences.
So we trainclassification models for them respectively and usethe respective model to predicate for common nounphrases or pronouns.
Our mention detection modelcan give 52.9% recall, 80.77% precision and 63.93%F-score without gold standard mention boundarieson the development dataset.
When gold standardmention boundaries are used, the results are 53.41%recall, 80.8% precision and 64.31% F-score.
(By us-ing the gold standard mention boundaries, we meanwe use the gold standard noun phrase boundaries.
)3 Coreference ResolutionAfter getting the predicated mentions, we use someheuristic rules to cluster them with the purpose ofgenerating highly precise pre-clusters.
For this taskMetric Recall Precision F-scoreMUC 49.64% 67.18% 57.09%BCUBED 59.42% 70.99% 64.69%CEAF 45.68% 30.56% 36.63%AVERAGE 51.58% 56.24% 52.80%Table 3: Evaluation results on development dataset with-out gold mention boundariesMetric Recall Precision F-scoreMUC 48.94% 67.72% 56.82%BCUBED 58.52% 72.61% 64.81%CEAF 46.49% 30.45% 36.8%AVERAGE 51.32% 56.93% 52.81%Table 4: Evaluation results on development dataset withgold mention boundariesonly identity coreference is considered while attribu-tive NP and appositive construction are excluded.That means we cannot use these two importantheuristic rules to generate pre-clusters.
In our sys-tem, we just put all the mentions (names and nomi-nal mentions, except pronouns) which have the samestring into the identical pre-clusters.
With these pre-clusters and their coreferential results, we imple-ment a classification based pre-cluster pair model todetermine whether a given pair of pre-clusters re-fer to the same entity.
We follow Rahman and Ng(2009) to generate most of our features.
We alsoinclude some other features which intuitively seemeffective for coreference resolution.
These features133Metric Recall Precision F-scoreMUC 42.66% 53.7% 47.54%BCUBED 61.05% 74.32% 67.04%CEAF 40.54% 32.35% 35.99%AVERAGE 48.08% 53.46% 50.19%Table 5: Evaluation results on development datasetwith gold mention boundaries using unified classificationmodelMetric Recall Precision F-scoreMUC 53.73% 67.79% 59.95%BCUBED 60.65% 66.05% 63.23%CEAF 43.37% 30.71% 35.96%AVERAGE 52.58% 54.85% 53.05%Table 6: Evaluation results on test dataset without goldmention boundariesare shown in Table 1 and Table 2.
For simplicity, weuse ci and cj to represent pre-clusters i and j. Eachpre-cluster pair can be seen as a link.
We have threekinds of link types: NP-NP link, NP-PRP link andPRP-PRP link.
Different link types may have differ-ent feature preferences.
So we train the classifica-tion based pre-cluster pair model for each link typeseparately and use different models to predicate theresults.
With the predicating results for pre-clusterpairs, we use closest-first clustering to link them andform the final cluster results.4 Experimental ResultsWe present our evaluation results on developmentdataset for CoNLL-2011 shared Task in Table 3, Ta-ble 4 and Table 5.
Official test results are givenin Table 6 and Table 7.
Three different evaluationmetrics were used: MUC (Vilain et al, 1995), B3(Bagga and Baldwin, 1998) and CEAF (Luo, 2005).Finally, the average scores of these three metrics areused to rank the participating systems.
The differ-ence between Table 3 and Table 4 is whether goldstandard mention boundaries are given.
Here ?men-tion boundaries?
means a more broad concept thanthe mention definition we gave earlier.
We shouldalso detect real mentions from them.
From the ta-bles, we can see that the scores can be improved litt-tle by using gold standard mention boundaries.
Alsothe results from Table 5 tell us that combining differ-ent link-type based classification models performedMetric Recall Precision F-scoreMUC 46.66% 68.40% 55.48%BCUBED 54.40% 70.19% 61.29%CEAF 43.77% 25.88% 32.53%AVERAGE 48.28% 54.82% 49.77%Table 7: Evaluation results on test dataset with gold men-tion boundariesbetter than using an unified classification model.
Forofficial test results, our system did not perform aswell as we had expected.
Some possible reasons areas follows.
First, verbs that are coreferential with anoun phrase are also tagged in OntoNotes.
For ex-ample, ?grew ?
and ?the strong growth?
should belinked in the following case: ?Sales of passengercars grew 22%.
The strong growth followed year-to-year increases.?
But we cannot solve this kindof problem in our system.
Second, we should per-form feature selection to avoid some useless featuresharming the scores.
Meanwhile, we did not makefull use of the WordNet, PropBank and other back-ground knowledge sources as features to representpre-cluster pairs.5 ConclusionIn this paper, we present our system for CoNLL-2011 shared Task, Modeling Unrestricted Corefer-ence in OntoNotes.
First some heuristic rules areperformed to pre-cluster all the mentions.
And thenwe use a classification based pre-cluster pair modelcombined with several cluster level features.
Wehypothesize that the main reason why we did notachieve good results is that we did not carefully ex-amine the features and dropped the feature selec-tion procedure.
Specially, we did not make full useof background knowledge like WordNet, PropBank,etc.
In our future work, we will make up for theweakness and design a more reasonable model to ef-fectively combine all kinds of features.AcknowledgmentsThis research is supported by National Natu-ral Science Foundation of Chinese (No.60973053,No.91024009) and Research Fund for the Doc-toral Program of Higher Education of China(No.20090001110047).134ReferencesSameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel and Nianwen Xue.2011.
CoNLL-2011 Shared Task: Modeling Unre-stricted Coreference in OntoNotes.
In Proceedingsof the Fifteenth Conference on Computational NaturalLanguage Learning (CoNLL 2011), Portland, Oregon.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, and L.Hirschman.
1995.
A Model-Theoretic CoreferenceScoring Scheme.
In Proceedings of the Sixth MessageUnderstanding Conference (MUC-6), pages 4552, SanFrancisco, CA.
Morgan Kaufmann.Amit Bagga and Breck Baldwin.
1998.
Algorithms forScoring Coreference Chains.
In Proceedings of the 1stInternational Conference on Language Resources andEvaluation, Granada, Spain, pp.
563566.Xiaoqiang Luo.
2005.
On Coreference Resolution Per-formance Metrics.
In Proceedings of the Human Lan-guage Technology Conference and the 2005 Confer-ence on Empirical Methods in Natural Language Pro-cessing, Vancouver, B.C., Canada, pp.
2532.Vincent Ng.
2008.
Unsupervised Models for Corefer-ence Resolution.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pp.
640?649.Altaf Rahman and Vincent Ng.
2009.
Supervised Mod-els for Coreference Resolution.
In Proceedings ofthe 2009 Conference on Empirical Methods in Natu-ral Language Processing.Vincent Ng.
2010.
Supervised Noun Phrase CoreferenceResearch: The First Fifteen Years.
In Proceedings ofthe 48th Meeting of the Association for ComputationalLinguistics (ACL 2010), Uppsala, pages 1396-1411.Shane Bergsma and Dekang Lin.
2006.
BootstrappingPath-Based Pronoun Resolution.
In COLING?ACL2006, pages 33?40.135
