Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 885?894,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Discriminative Model for Joint Morphological Disambiguation andDependency ParsingJohn LeeDepartment of Chinese,Translation and LinguisticsCity University of Hong Kongjsylee@cityu.edu.hkJason Naradowsky, David A. SmithDepartment of Computer ScienceUniversity of Massachusetts, Amherst{narad,dasmith}@cs.umass.eduAbstractMost previous studies of morphological dis-ambiguation and dependency parsing havebeen pursued independently.
Morphologicaltaggers operate on n-grams and do not takeinto account syntactic relations; parsers usethe ?pipeline?
approach, assuming that mor-phological information has been separatelyobtained.However, in morphologically-rich languages,there is often considerable interaction betweenmorphology and syntax, such that neither canbe disambiguated without the other.
In this pa-per, we propose a discriminative model thatjointly infers morphological properties andsyntactic structures.
In evaluations on varioushighly-inflected languages, this joint modeloutperforms both a baseline tagger in morpho-logical disambiguation, and a pipeline parserin head selection.1 IntroductionTo date, studies of morphological analysis anddependency parsing have been pursued more orless independently.
Morphological taggers dis-ambiguate morphological attributes such as part-of-speech (POS) or case, without taking syntaxinto account (Hakkani-Tu?r et al, 2000; Hajic?
etal., 2001); dependency parsers commonly assumethe ?pipeline?
approach, relying on morphologi-cal information as part of the input (Buchholz andMarsi, 2006; Nivre et al, 2007).
This approachserves many languages well, especially those withless morphological ambiguity.
In English, for ex-ample, accuracy of POS tagging has risen above97% (Toutanova et al, 2003), and that of depen-dency parsing has reached the low nineties (Nivreet al, 2007).
For these languages, there may be littleto be gained to justify the computational cost of in-corporating syntactic inference during the morpho-logical tagging task; conversely, it is doubtful thaterrorful morphological information is a main causeof errors in English dependency parsing.However, the pipeline approach seems more prob-lematic for morphologically-rich languages withsubstantial interactions between morphology andsyntax (Tsarfaty, 2006).
Consider the Latin sen-tence, Una dies omnis potuit praecurrere amantis,?One day was able to make up for all the lovers?1.
Asshown in Table 1, the adjective omnis (?all?)
is am-biguous in number, gender, and case; there are sevenvalid analyses.
From the perspective of a finite-state morphological tagger, the most attractive anal-ysis is arguably the singular nominative, since omnisis immediately followed by the singular verb potuit(?could?).
Indeed, the baseline tagger used in thisstudy did make this decision.
Given its nominativecase, the pipeline parser assigned the verb potuit tobe its head; the two words form the typical subject-verb relation, agreeing in number.Unfortunately, as shown in Figure 1, the word om-nis in fact modifies the noun amantis, at the end ofthe sentence.
As a result, despite the distance be-tween them, they must agree in number, gender andcase, i.e., both must be plural masculine (or femi-nine) accusative.
The pipeline parser, acting on theinput that omnis is nominative, naturally did not see1Taken from poem 1.13 by Sextus Propertius, English trans-lation by Katz (2004).885Latin Una dies omnis potuit praecurrere amantisEnglish one day all could to surpass loversNumber sg pl sg pl sg sg pl sg - sg plGender f n m/f m/f m/f m/f/n m/f - - m/f/n m/fCase nom/ab nom/acc nom nom/acc nom gen acc - - gen accTable 1: The Latin sentence ?Una dies omnis potuit praecurrere amantis?, meaning ?One day was able to make upfor all the lovers?, shown with glosses and possible morphological analyses.
The correct analyses are shown in bold.The word omnis has 7 possible combinations of number, gender and case, while amantis has 5.
Disambiguation partlydepends on establishing amantis as the head of omnis, and so the two must agree in all three attributes.this agreement, and therefore did not consider thissyntactic relation likely.Such a dilemma is not uncommon in languageswith relatively free word order.
On the one hand,it appears difficult to improve morphological tag-ging accuracy on words like omnis without syntacticknowledge; on the other hand, a parser cannot reli-ably disambiguate syntax unless it has accurate mor-phological information, in this example the agree-ment in number, gender, and case.In this paper we propose to attack this chicken-and-egg problem with a discriminative model thatjointly infers morphological and syntactic propertiesof a sentence, given its words as input.
In eval-uations on various highly-inflected languages, themodel outperforms both a baseline tagger in mor-phological disambiguation, and a pipeline parser inhead selection.After a description of previous work (?2), thejoint model (?3) will be contrasted with the base-line pipeline model (?4).
Experimental results (?5-6) will then be presented, followed by conclusionsand future directions.2 Previous WorkSince space does not allow a full review of the vastliterature on morphological analysis and parsing, wefocus only on past research involving joint morpho-logical and syntactic inference (?2.1); we then dis-cuss Latin (?2.2), a language representative of thechallenges that motivated our approach.2.1 Joint Morphological and SyntacticInferenceMost previous work in morphological disambigua-tion, even when applied on morphologically com-plex languages with relatively free word order,potuitcoulddiesdayunaonepraecurrereto surpassamantisloversomnisallFigure 1: Dependency tree for the sentence ?Una diesomnis potuit praecurrere amantis?.
The word omnis isan adjective modifying the noun amantis.
This informa-tion is key to the morphological disambiguation of bothwords, as shown in Table 1.such as Turkish (Hakkani-Tu?r et al, 2000) andCzech (Hajic?
et al, 2001), did not consider syn-tactic relationships between words.
In the litera-ture on data-driven parsing, two recent studies at-tempted joint inference on morphology and syntax,and both considered phrase-structure trees for Mod-ern Hebrew (Cohen and Smith, 2007; Goldberg andTsarfaty, 2008).The primary focus of morphological processing inModern Hebrew is splitting orthographic words intomorphemes: clitics such as prepositions, pronouns,and the definite article must be separated from thecore word.
Each of the resulting morphemes is thentagged with an atomic ?part-of-speech?
to indicateword class and some morphological features.
Sim-ilarly, the English POS tags in the Penn Treebankcombine word class information with morphologi-886cal attributes such as ?plural?
or ?past tense?.Cohen and Smith (2007) separately train a dis-criminative conditional random field (CRF) for seg-mentation and tagging, and a generative probabilis-tic context-free grammar (PCFG) for parsing.
At de-coding time, the two models are combined as a prod-uct of experts.
Goldberg and Tsarfaty (2008) pro-pose a generative joint model.
This paper is the firstto use a fully discriminative model for joint morpho-logical and syntactic inference on dependency trees.2.2 LatinUnlike Modern Hebrew, Latin does not require ex-tensive morpheme segmentation2.
However, it doeshave a relatively free word order, and is also highlyinflected, with each word having up to nine morpho-logical attributes, listed in Table 2.
In addition to itsabsolute numbers of cases, moods, and tenses, Latinmorphology is fusional.
For instance, the suffix?is in omnis cannot be segmented into morphemesthat separately indicate gender, number, and case.According to the Latin morphological database en-coded in MORPHEUS (Crane, 1991), 30% of Latinnouns can be parsed as another part-of-speech, andon average each has 3.8 possible morphological in-terpretations.We know of only one previous attempt in data-driven dependency parsing for Latin (Bamman andCrane, 2008), with the goal of constructing a dy-namic lexicon for a digital library.
Parsing is per-formed using the usual pipeline approach, first withthe TreeTagger analyzer (Schmid, 1994) and thenwith a state-of-the-art dependency parser (McDon-ald et al, 2005).
Head selection accuracy was61.49%, and rose to 64.99% with oracle morpho-logical tags.
Of the nine morphological attributes,gender and especially case had the lowest accu-racy.
This observation echoes the findings forCzech (Smith et al, 2005), where case was also themost difficult to disambiguate.3 Joint ModelThis section describes a model that jointly infersmorphological and syntactic properties of a sen-tence.
It will be presented as a graphical model,2Except for enclitics such as -que, -ve, and -ne, but theirsegmentation is rather straightforward compared to Modern He-brew or other Semitic languages.Attribute ValuesPart-of- noun, verb, participle, adjective,speech adverb, conjunction, preposition,(POS) pronoun, numeral, interjection,exclamation, punctuationPerson first, second, thirdNumber singular, pluralTense present, imperfect, perfect,pluperfect, future perfect, futureMood indicative, subjunctive, infinitive,imperative, participle, gerund,gerundive, supineVoice active, passiveGender masculine, feminine, neuterCase nominative, genitive, dative,accusative, ablative, vocative,locativeDegree comparative, superlativeTable 2: Morphological attributes and values for Latin.Ancient Greek has the same attributes; Czech and Hun-garian lack some of them.
In all categories except POS,a value of null (?-?)
may also be assigned.
For example, anoun has ?-?
for the tense attribute.starting with the variables and then the factors,which represents constraints on the variables.
Letn be the number of words and m be the number ofpossible values for a morphological attribute.
Thevariables are:?
WORD: the n words w1,...,wn of the input sen-tence, all observed.?
TAG: O(nm) boolean variables3 Ta,i,v, corre-sponding to each value of the morphological at-tributes listed in Table 2.
Ta,i,v = true whenthe word wi has value v as its morphologicalattribute a.
In Figure 2, CASE3,acc is the short-hand representing the variable Tcase,3,acc.
It isset to true since the wordw3 has the accusativecase.?
LINK: O(n2) boolean variables Li,j corre-sponding to a possible link between each pair3The TAG variables were actually implemented as multino-mials, but are presented here as booleans for ease of understand-ing.887UNIGRAMCASE?UNIGRAMCASE?CASE?LINKCASE?LINKCASE?LINKCASE?LINKCASE    6,genCASE    3,genCASE    3,nom3,accCASEUNIGRAMCASE?UNIGRAMCASE?UNIGRAMCASE?CASE    2,...CASELINKCASE6,accCASE?BIGRAMCASE?BIGRAMTREE WORD?LINKWORDLINKCASE    5,...L L3,6 4,6Figure 2: The joint model (?3) depicted as a graphical model.
The variables, all boolean, are represented by circles andare bolded if their correct values are true.
Factors are represented by rectangles and are bolded if they fire.
For clarity,this graph shows only those variables and factors associated with one pair of words (i.e., w3=omnis and w6=amantis)and with one morphological attribute (i.e., case).
The variables L3,6, CASE3,acc and CASE6,acc are bolded, indicatingthat w3 and w6 are linked and both have the accusative case.
The ternary factor CASE-LINK, that connects to thesethree variable, therefore fires.of words4.
Li,j = true when there is a depen-dency link from the word wi to the word wj .
InFigure 2, the variable L3,6 is set to true sincethere is a dependency link between the wordsw3 and w6.We define a probability distribution over all joint as-signments A to the above variables,p(A) =1Z?kFk(A) (1)where Z is a normalizing constant.
The assign-ment A is subject to a hard constraint, representedin Figure 2 as TREE, requiring that the values ofthe LINK variables must yield a tree, which maybe non-projective.
The factors Fk(A) represent softconstraints evaluating various aspects of the ?good-ness?
of the tree structure implied by A.
We say afactor ?fires?
when all its neighboring variables are4Variables for link labels can be integrated in a straightfor-ward manner, if desired.true and it evaluates to a non-negative real num-ber; otherwise, it evaluates to 1 and has no effecton the product in equation (1).
Soft constraints inthe model are divided into local and link factors, towhich we now turn.3.1 Local FactorsThe local factors consult either one word or twoneighboring words, and their morphological at-tributes.
These factors express the desirability of theassignments of morphological attributes based on lo-cal context.
There are three types:?
TAG-UNIGRAM: There are O(nm) such unaryfactors, each instance of which is connected toa TAG variable.
The factor fires when Ta,i,vis true.
The features consist of the value vof the morphological attribute concerned, com-bined with the word identity of wi, with back-off using all suffixes of the word.
The CASE-UNIGRAM factors shown in Figure 2 are ex-amples of this family of factors.888?
TAG-BIGRAM: There are O(nm2) of such bi-nary factors, each connected to the TAG vari-ables of a pair of neighboring words.
The factorfires when Ta,i,v1 and Ta,i+1,v2 are both true.The CASE-BIGRAM factors shown in Figure 2are examples of this family of factors.?
TAG-CONSISTENCY: For each word, the TAGvariables representing the possible POS val-ues are connected to those representing the val-ues of other morphological attributes, yield-ing O(nm2) binary factors.
They fire whenTpos,i,v1 and Ta,i,v2 are both true.
These fac-tors are intended to discourage inconsistent as-signments, such as a non-null tense for a noun.It is clear that so far, none of these factors are awareof the morphological agreement between omnis andamantis, crucial for inferring their syntactic relation.We now turn our attention to link factors, whichserve this purpose.3.2 Link FactorsThe link factors consult all pairs of words, possiblyseparated by a long distance, that may have a de-pendency link.
These factors model the likelihoodof such a link based on the word identities and theirmorphological attributes:?
WORD-LINK: There areO(n2) such unary fac-tors, each connected to a LINK variable, asshown in Figure 2.
The factor fires when Li,jis true.
Features include various combina-tions of the word identities of the parent wi andchild wj , and 5-letter prefixes of these words,replicating the so-called ?basic features?
usedby McDonald et al (2005).?
POS-LINK: There are O(n2m2) such ternaryfactors, each connected to the variables Li,j ,Ti,pos,vi and Tj,pos,vj .
It fires when all three aretrue or, in other words, when the parent wordwi has POS vi, and the child wj has POS vj .Features replicate all the so-called ?basic fea-tures?
used by McDonald et al (2005) that in-volve POS.
These factors are not shown in Fig-ure 2, but would have exactly the same struc-ture as the CASE-LINK factors.Beyond these basic features, McDonald et al(2005) also utilize POS trigrams and POS 4-grams.
Both include the POS of two linkedwords, wi and wj .
The third component in thetrigrams is the POS of each word wk locatedbetween wi and wj , i < k < j.
The two ad-ditional components that make up the 4-gramsare subsets of the POS of words located to theimmediate left and right of wi and wj .If fully implemented in our joint model, thesefeatures would necessitate two separate fami-lies of link factors: O(n3m3) factors for thePOS trigrams, and O(n2m4) factors for thePOS 4-grams.
To avoid this substantial in-crease in model complexity, these features areinstead approximated: the POS of all wordsinvolved in the trigrams and 4-grams, exceptthose of wi and wj , are regarded as fixed, theirvalues being taken from the output of a mor-phological tagger (?4.1), rather than connectedto the appropriate TAG variables.
This approxi-mation allows these features to be incorporatedin the POS-LINK factors.?
MORPH-LINK: There are O(n2m2) suchternary factors, each connected to the variablesLi,j , Ti,a,vi and Tj,a,vj , for every attribute aother than POS.
The factor fires when all threevariables are true, and both vi and vj are non-null; i.e., it fires when the parent word wi hasvi as its morphological attribute a, and the childwj has vj .
Features include the combination ofvi and vj themselves, and agreement betweenthem.
The CASE-LINK factors in Figure 2 arean example of this family of factors.4 BaselinesTo ensure a meaningful comparison with the jointmodel, our two baselines are both implemented inthe same graphical model framework, and trainedwith the same machine-learning algorithm.
Roughlyspeaking, they divide up the variables and factors ofthe joint model and train them separately.
For mor-phological disambiguation, we use the baseline tag-ger described in ?4.1.
For dependency parsing, ourbaseline is a ?pipeline?
parser (?4.2) that infers syn-tax upon the output of the baseline tagger.8894.1 Baseline Morphological TaggerThe tagger is a graphical model with the WORDand TAG variables, connected by the local fac-tors TAG-UNIGRAM, TAG-BIGRAM, and TAG-CONSISTENCY, all used in the joint model (?3).4.2 Baseline Dependency ParserThe parser has no local factors, but has the samevariables as the joint model and the same featuresfrom all three families of link factors (?3).
However,since it takes as input the morphological attributespredicted by the tagger, the TAG variables are nowobserved.
This leads to a change in the structureof the link factors ?
all features from the POS-LINK factors now belong to the WORD-LINK fac-tors, since the POS of all words are observed.
Inshort, the features of the parser are a replication of(McDonald et al, 2005), but also extended beyondPOS to the other morphological attributes, with thefeatures in the MORPH-LINK factors incorporatedinto WORD-LINK for similar reasons.5 Experimental Set-up5.1 DataOur evaluation focused on the Latin DependencyTreebank (Bamman and Crane, 2006), created atthe Perseus Digital Library by tailoring the PragueDependency Treebank guidelines for the Latin lan-guage.
It consists of excerpts from works by eightLatin authors.
We randomly divided the 53K-wordtreebank into 10 folds of roughly equal sizes, with anaverage of 5314 words (347 sentences) per fold.
Weused one fold as the development set and performedcross-validation on the other nine.To measure how well our model generalizesto other highly-inflected, relatively free-word-orderlanguages, we considered Ancient Greek, Hungar-ian, and Czech.
Their respective datasets consist of8000 sentences from the Ancient Greek DependencyTreebank (Bamman et al, 2009), 5800 from theHungarian Szeged Dependency Treebank (Vincze etal., 2010), and a subset of 3100 from the Prague De-pendency Treebank (Bo?hmova?
et al, 2003).5.2 TrainingWe define each factor in (1) as a log-linear function:Fk(A) = exp?h?hfh(A,W, k) (2)Given an assignment A and words W , fh is anindicator function describing the presence or ab-sence of the feature, and ?h is the corresponding setof weights learned using stochastic gradient ascent,with the gradients inferred by loopy belief propaga-tion (Smith and Eisner, 2008).
The variance of theGaussian prior is set to 1.
The other two parametersin the training process, the number of belief propa-gation iterations and the number of training rounds,were tuned on the development set.5.3 DecodingThe output of the joint model is the assignment tothe TAG and LINK variables.
Loopy belief propaga-tion (BP) was used to calculate the posterior proba-bilities of these variables.
For TAG, we emit the tagwith the highest posterior probability as computedby sum-product BP.
We produced head attachmentsby first calculating the posteriors of the LINK vari-ables with BP and then passing them to an edge-factored tree decoder.
This is equivalent to mini-mum Bayes risk decoding (Goodman, 1996), whichis used by Cohen and Smith (2007) and Smith andEisner (2008).
This MBR decoding procedure en-forces the hard constraint that the output be a treebut sums over possible morphological assignments.55.4 Reducing Model ComplexityIn principle, the joint model should consider everypossible combination of morphological attributes forevery word.
In practice, to reduce the complexityof the model, we used a pre-existing morphologicaldatabase, MORPHEUS (Crane, 1991), to constrainthe range of possible values of the attributes listedin Table 2; more precisely, we add a hard constraint,requiring that assignments to the TAG variables becompatible with MORPHEUS.
This constraint signif-icantly reduces the value of m in the big-O notation5This approach to nuisance variables has also been usedeffectively for parsing with tree-substitution grammars, whereseveral derived trees may correspond to each derivation tree,and parsing with PCFGs with latent annotations.890Model Tagger Joint Tagger JointAttr.
?
all all non-null non-nullPOS 94.4 94.5 94.4 94.5Person 99.4 99.5 97.1 97.6Number 95.3 95.9 93.7 94.5Tense 98.0 98.2 93.2 93.9Mood 98.1 98.3 93.8 94.4Voice 98.5 98.6 95.3 95.7Gender 93.1 93.9 87.7 89.1Case 89.3 90.0 79.9 81.2Degree 99.9 99.9 86.4 90.8UAS 61.0 61.9 ?
?Table 3: Latin morphological disambiguation and pars-ing.
For some attributes, such as degree, a substan-tial portion of words have the null value.
The non-nullcolumns provides a sharper picture by excluding these?easy?
cases.
Note that POS is never null.for the number of variables and factors described in?3.
To illustrate the effect, the graphical model ofthe sentence in Table 1, whose six words are all cov-ered by the database, has 1,866 factors; without thebenefit of the database, the full model would have31,901 factors.The MORPHEUS database was automatically gen-erated from a list of stems, inflections, irregularforms and morphological rules.
It covers about 99%of the distinct words in the Latin Dependency Tree-bank.
At decoding time, for each fold, the databaseis further augmented with tags seen in training data.After this augmentation, an average of 44 words are?unseen?
in each fold.Similarly, we constructed morphological dictio-naries for Czech, Ancient Greek, and Hungarianfrom words that occurred at least five times in thetraining data; words that occurred fewer times wereunrestricted in the morphological attributes theycould take on.6 Experimental ResultsWe compare the performance of the pipeline model(?4) and the joint model (?3) on morphological dis-ambiguation and unlabeled dependency parsing.Model Tagger Joint Tagger JointAttr.
?
all all non-null non-nullPOS 95.5 95.7 95.5 95.7Person 98.4 98.8 93.5 95.6Number 91.2 92.3 87.0 88.4Tense 98.4 98.8 92.7 96.1Voice 98.5 98.7 93.2 95.8Gender 86.6 87.9 75.6 78.0Case 84.1 85.6 74.3 76.5Degree 97.9 98.0 90.1 90.1UAS 67.4 68.7 ?
?Table 4: Czech morphological disambiguation and pars-ing.
As with Latin, the model is least accurate withnoun/adjective categories of gender number, and case,particularly when considering only words whose truevalue is non-null for those attributes.
Joint inference withsyntactic features improves accuracy across the board.Model Tagger Joint Tagger JointAttr.
?
all all non-null non-nullPOS 94.9 95.7 94.9 95.7Person 98.7 99.0 92.2 94.6Number 97.4 97.9 96.5 97.1Tense 96.8 97.2 84.1 86.8Mood 97.9 98.3 91.4 93.2Voice 97.8 98.0 91.3 92.4Gender 95.4 96.1 90.7 91.9Case 95.9 96.3 92.0 92.6Degree 99.8 99.9 33.3 55.6UAS 68.0 70.5 ?
?Table 5: Ancient Greek morphological disambiguationand parsing.
Noun/adjective morphology is more accu-rate, but verbal morphology is more problematic.Model Tagger Joint Tagger JointAttr.
?
all all non-null non-nullPOS 95.8 95.8 95.8 95.8Person 98.5 98.6 94.9 94.1Number 97.4 97.5 96.8 96.6Tense 98.9 99.3 97.2 97.3Mood 98.7 99.2 95.8 97.3Case 96.7 97.0 94.5 94.9Degree 97.9 98.1 87.5 88.6UAS 78.2 78.8 ?
?Table 6: Hungarian morphological disambiguation andparsing.
The agglutinative morphological system makeslocal cues more effective, but syntactic information helpsin almost all categories.8916.1 Morphological DisambiguationAs seen in Table 3, the joint model outperforms6the baseline tagger in all attributes in Latin morpho-logical disambiguation.
Among words not coveredby the morphological database, accuracy in POS isslightly better, but lower for case, gender and num-ber.The joint model made the most gains on adjec-tives and participles.
Both parts-of-speech are par-ticularly ambiguous: according to MORPHEUS, 43%of the adjectives can be interpreted as another POS,most frequently nouns; while participles have an av-erage of 5.5 morphological interpretations.
Bothalso often have identical forms for different genders,numbers and cases.
In these situations, syntacticconsiderations help nudge the joint model to the cor-rect interpretations.Experiments on the other three languages bear outsimilar results: the joint model improves morpho-logical disambiguation.
The performance of Czech(Table 4) exhibits the closest analogue to Latin: gen-der, number, and case are much less accurately pre-dicted than are the other morphological attributes.Like Latin, Czech lacks definite and indefinite arti-cles to provide high-confidence cues for noun phraseboundaries.The Ancient Greek treebank comprises both ar-chaic texts, before the development of a definite ar-ticle, and later classic Greek, which has a definitearticle; Hungarian has both a definite and an indefi-nite article.
In both languages (Tables 5 and 6), nounand adjective gender, number, and case are moreaccurately predicted than in Czech and Latin.
Theverbal system of ancient Greek, in contrast, is morecomplex than that of the other languages, so mood,voice, and tense accuracy are lower.6.2 Dependency ParsingIn addition to morphological disambiguation, wealso measured the performance of the joint modelon dependency parsing of Latin and the other lan-guages.
The baseline pipeline parser (?4.2) yielded61.00% head selection accuracy (i.e., unlabeled at-tachment score, UAS), outperformed7 by the joint6The differences are statistically significant in all (p < 0.01by McNemar?s Test) but POS (p = 0.5).7Significant at p < e?11 by McNemar?s Test.model at 61.88%.
The joint model showed simi-lar improvements in Ancient Greek, Hungarian, andCzech.Wrong decisions made by the baseline tagger of-ten misled the pipeline parser.
For adjectives, the ex-ample shown in Table 1 and Figure 1 is a typical sce-nario, where an accusative adjective was tagged asnominative, and was then misanalyzed by the parseras modifying a verb (as a subject) rather than mod-ifying an accusative noun.
For participles modify-ing a noun, the wrong noun was often chosen basedon inaccurate morphological information.
In thesecases, the joint model, entertaining all morpholog-ical possibilities, was able to find the combinationof links and morphological analyses that are collec-tively more likely.The accuracy figures of our baselines are compa-rable, but not identical, to their counterparts reportedin (Bamman and Crane, 2008).
The differences maypartially be attributed to the different morphologi-cal tagger used, and the different learning algorithm,namely Margin Infused Relaxed Algorithm (MIRA)in (McDonald et al, 2005) rather than maximumlikelihood.
More importantly, the Latin DependencyTreebank has grown from about 30K at the time ofthe previous work to 53K at present, resulting in sig-nificantly different training and testing material.Gold Pipeline Parser When given perfect mor-phological information, the Latin parser performs at65.28% accuracy in head selection.
Despite the or-acle morphology, the head selection accuracy is stillbelow other languages.
This is hardly surprising,given the relatively small training set, and that the?the most difficult languages are those that combinea relatively free word order with a high degree of in-flection?, as observed at the recent dependency pars-ing shared task (Nivre et al, 2007); both of these arecharacteristics of Latin.A particularly troublesome structure is coordina-tion; the most frequent link errors all involve either aparent or a child as a conjunction.
In a list of words,all words and coordinators depend on the final coor-dinator.
Since the factors in our model consult onlyone link at a time, they do not sufficiently capturethis kind of structures.
Higher-order features, partic-ularly those concerned with links with grandparentsand siblings, have been shown to benefit dependency892parsing (Smith and Eisner, 2008) and may be able toaddress this issue.7 Conclusions and Future WorkWe have proposed a discriminative model thatjointly infers morphological properties and syntacticstructures.
In evaluations on various highly-inflectedlanguages, this joint model outperforms both a base-line tagger in morphological disambiguation, and apipeline parser in head selection.This model may be refined by incorporating richerfeatures and improved decoding.
In particular, wewould like to experiment with higher-order features(?6), and with maximum a posteriori decoding, viamax-product BP or (relaxed) integer linear program-ming.
Further evaluation on other morphologicalsystems would also be desirable.AcknowledgmentsWe thank David Bamman and Gregory Crane fortheir feedback and support.
Part of this researchwas performed by the first author while visitingPerseus Digital Library at Tufts University, un-der the grants A Reading Environment for Ara-bic and Islamic Culture, Department of Education(P017A060068-08) and The Dynamic Lexicon: Cy-berinfrastructure and the Automatic Analysis of His-torical Languages, National Endowment for the Hu-manities (PR-50013-08).
The latter two authorswere supported by Army prime contract #W911NF-07-1-0216 and University of Pennsylvania subaward#103-548106; by SRI International subcontract #27-001338 and ARFL prime contract #FA8750-09-C-0181; and by the Center for Intelligent InformationRetrieval.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethe authors?
and do not necessarily reflect those ofthe sponsors.ReferencesDavid Bamman and Gregory Crane.
2006.
The Designand Use of a Latin Dependency Treebank.
Proc.
Work-shop on Treebanks and Linguistic Theories (TLT).Prague, Czech Republic.David Bamman and Gregory Crane.
2008.
Building aDynamic Lexicon from a Digital Library.
Proc.
8thACM/IEEE-CS Joint Conference on Digital Libraries(JCDL 2008).
Pittsburgh, PA.David Bamman, Francesco Mambrini, and GregoryCrane.
2009.
An Ownership Model of Anno-tation: The Ancient Greek Dependency Treebank.Proc.
Workshop on Treebanks and Linguistic Theories(TLT).A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.2003.
The PDT: a 3-level Annotation Scenario.
InTreebanks: Building and Using Parsed Corpora, A.Abeille?
(ed).
Kluwer.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X Shared Task on Multilingual Dependency Parsing.Proc.
CoNLL.
New York, NY.Shay B. Cohen and Noah A. Smith.
2007.
Joint Morpho-logical and Syntactic Disambiguation.
Proc.
EMNLP-CoNLL.
Prague, Czech Republic.Gregory Crane.
1991.
Generating and Parsing ClassicalGreek.
Literary and Linguistic Computing 6(4):243?245.Yoav Goldberg and Reut Tsarfaty.
2008.
A Single Gen-erative Model for Joint Morphological Segmentationand Syntactic Parsing.
Proc.
ACL.
Columbus, OH.Joshua Goodman.
1996.
Parsing Algorithms and Met-rics.
Proc.
ACL.J.
Hajic?, P. Krbec, P.
Kve?ton?, K. Oliva, and V. Petkevic?.2001.
Serial Combination of Rules and Statistics: ACase Study in Czech Tagging.
Proc.
ACL.D.
Z. Hakkani-Tu?r, K. Oflazer, and G. Tu?r.
2000.
Statis-tical Morphological Disambiguation for AgglutinativeLanguages.
Proc.
COLING.Vincent Katz.
2004.
The Complete Elegies of SextusPropertius.
Princeton University Press, Princeton, NJ.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJana Hajic?.
2005.
Non-projective DependencyParsing using Spanning Tree Algorithms.
Proc.HLT/EMNLP.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online Large-Margin Training of DependencyParsers.
Proc.
ACL.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 Shared Task on De-pendency Parsing.
Proc.
CoNLL Shared Task Sessionof EMNLP-CoNLL.
Prague, Czech Republic.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging using Decision Trees.
Proc.
InternationalConference on New Methods in Language Processing.Manchester, UK.Noah A. Smith, David A. Smith and Roy W. Tromble.2005.
Context-Based Morphological Disambiguationwith Random Fields.
Proc.
HLT/EMNLP.
Vancouver,Canada.893David Smith and Jason Eisner.
2008.
Dependency Pars-ing by Belief Propagation.
Proc.
EMNLP.
Honolulu,Hawaii.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.Proc.
HLT-NAACL.
Edmonton, Canada.Reut Tsarfaty.
2006.
Integrated Morphological andSyntactic Disambiguation for Modern Hebrew.
Proc.COLING-ACL Student Research Workshop.Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgyMo?ra, Zolta?n Alexin, and Ja?nos Csirik.
2010.
Hun-garian Dependency Treebank.
Proc.
LREC.894
