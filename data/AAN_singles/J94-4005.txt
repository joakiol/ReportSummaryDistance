Training and Scaling Preference Functionsfor DisambiguationHiyan  A lshawi  ?AT&T Bell LaboratoriesDav id  Carter  tSRI InternationalWe present an automatic method for weighting the contributions ofpreference functions used indisambiguation.
I itial scaling factors are derived as the solution to a least squares minimizationproblem, and improvements are then made by hill climbing.
The method isapplied to disambiguat-ing sentences in the Air Travel Information System corpus, and the performance of the resultingscaling factors is compared with hand-tuned factors.
We then focus on one class of preferencefunction, those based on semantic lexical collocations.
Experimental results are presented show-ing that such functions vary considerably in selecting correct analyses.
In particular, we define afunction that performs ignificantly better than ones based on mutual information and likelihoodratios of lexical associations.1.
IntroductionThe importance ofgood preference functions for ranking competing analyses producedby language-processing systems grows as the coverage of these systems improves.Increasing coverage usually also increases the number of analyses for sentences previ-ously covered, bringing the danger of lower accuracy for these sentences.
Large scalerule-based analysis ystems have therefore tended to employ a collection of functionsto produce a score for sorting analyses in a preference order.
In this paper we addresstwo issues relating to the application of preference functions.1.1 Combining Multiple Preference FunctionsThe first problem we address is that of combining different functions, each of which issupposed to offer some contribution to selecting the best among a set of analyses of asentence.
Although multiple functions have been used in other systems (for example,McCord 1990; Hobbs and Bear 1990), little is typically said about how the functionsare combined to produce the overall score for an analysis, the weights presumablybeing determined by intuition or trial and error.
McCord (1993) gives very specificinformation about the weights he uses to combine preference functions, though theseweights are chosen by hand.
Selecting weights by hand, however, is a task for experts,which needs to be redone very time the system is applied to a new domain or corpus.Furthermore, there is no guarantee that the selected weights will achieve optimal oreven near-optimal performance.The speech-processing community, on the other hand, has a longer history of usingnumerical evaluation functions, and speech researchers have used schemes for scoringrecognition hypotheses that are similar to the one proposed here for disambiguation.
* AT&T Bell Laboratories, 600 Mountain Avenue, Murray Hill, NJ 07974, USA.
E-maihhiyan@research.at t.comj" SRI International, Cambridge Computer Science Research Centre, 23 Miller Yard, Cambridge CB2 1RQ,UK.
E-mail: dmc@cam.sri.com(~ 1994 Association for Computational LinguisticsComputational Linguistics Volume 20, Number 4For example, Ostendorf et al (1991) improve recognition performance by using a linearcombination of several scoring functions.
In their work the weights for the linearcombination are chosen to optimize a generalized mean of the rank of the correctword sequence.In our case, the problem is formulated as follows.
Each preference function isdefined as a numerical (possibly real-valued) function on representations correspond-ing to the sentence analyses.
A weighted sum of these functions is then used as theoverall measure to rank the possible analyses of a particular sentence.
We refer to thecoefficients, or weights, used in this linear combination as the "scaling factors" for thefunctions.
We determine these scaling factors automatically in order both to avoid theneed for expert hand tuning and to achieve performance that is at least locally optimal.We start with the solution to minimizing a squared-error cost function, a well-knowntechnique applied to many optimization and classification problems.
This solution isthen enhanced by application of a hill-climbing technique.1.2 Word Sense Collocation FunctionsUntil recently, the choice of the various functions used in rule-based systems wasmade mainly according to anecdotal information about the effectiveness of, for ex-ample, various attachment preference strategies.
There is now more empirical workcomparing such functions, particularly in the case of functions based on statisticalinformation about lexical or semantic ollocations.
Lexical collocation functions, espe-cially those determined statistically, have recently attracted considerable attention incomputational linguistics (Calzolari and Bindi 1990; Church and Hanks 1990; Sekineet al 1992; Hindle and Rooth 1993) mainly, though not exclusively, for use in dis-ambiguation.
These functions are typically derived by observing the occurrences oftuples (usually pairs or triples) that summarize relations present in an analysis of atext, or their surface occurrences.
For example, Hindle and Rooth (1993) and Resnikand Hearst (1993) give experimental results on the effectiveness of functions based onlexical associations, or lexical-class associations, atselecting appropriate prepositionalphrase attachments.We have experimented with a variety of specific functions that make use of collo-cations between word senses.
The results we present show that these functions varyconsiderably in disambiguation accuracy, but that the best collocation functions aremore effective than a function based on simple estimates of syntactic rule probabilities.In particular, the best collocation function performs ignificantly better than a relatedfunction that defines collocation strength in terms of mutual information, reducing theerror rate in a disambiguation task from approximately 30% to approximately 10%.We start by describing our experimental context and training data in Section 2.Then we address the issue of selecting scaling factors by presenting our optimizationprocedure in Section 3 and a comparison with manual scaling in Section 4.
Finally, wetake a close look at a set of semantic ollocation functions, defining them in Section 5and comparing their effectiveness atdisambiguation in Section 6.2.
The Experimental SetupDisambiguation TaskAll the experiments we describe here were done with the Core Language Engine(CLE), a primarily rule-based natural language-processing system (Alshawi 1992).More specifically, the work on optimizing preference factors and semantic olloca-tions was done as part of a project on spoken language translation i  which the CLEwas used for analysis and generation ofboth English and Swedish (AgnSs et al 1993).636Hiyan Alshawi and David Carter Training and Scaling Preference FunctionsThe work presented here is all concerned with the English analysis ide, though wesee no reason why its conclusions should not be applicable to Swedish or other naturallanguages.In our experiments we made use of the Air T, avel Information System (ATIS) cor-pus of transcribed speech sentences.
This application was chosen because the proposedmethod for automatic derivation of scaling factors requires a corpus of sentences thatare representative of the sublanguage, together with some independent measure of thecorrectness or plausibility of analyses of these sentences.
In addition, we had accessto a hand-parsed subcollection of the ATIS corpus built as part of the Penn Treebankproject (Marcus, Santorini, and Marcinkiewicz 1993).
Another eason for choosing ATISwas that it consists of several thousand sentences in a constrained discourse domain,which helped avoid sparseness problems in training collocation functions}In the various experiments, the alternatives we are choosing between are analysesexpressed in the version of quasi logical form (QLF) described by Alshawi and Crouch(1992).
QLFs express emantic ontent, but are derived compositionally from completesyntactic analyses of a sentence and therefore mirror much syntactic structure as well.However, the use of QLF analyses i not central to our method: the important thing isthat the representation used is rich enough to support avariety of preference functions.We have experimented with combinations of around 30 different functions and use20 of them in our spoken language translation system; the others contribute so littleto overall performance that their computational cost cannot be justified.
This defaultset of 20 was used throughout the scaling factor work described in Sections 3 and4.
It consists of 1 collocation-based function and 19 non-collocation-based ones.
Thework described in Section 6 involved substituting single alternative collocation-basedfunctions for the single one in the set of 20.Many (unscaled) preference functions imply return integers corresponding tocounts of particular constructs in the representation, such as the number of expressionscorresponding to adjuncts, unresolved ellipses, particular attachment configurations,or balanced conjunctions.
There are also some real-valued functions, including thesemantic ollocation functions discussed in Section 5.To illustrate how the system works, consider the ATIS sentence "Do I get dinneron this flight?"
The CLE assigns two analyses to this sentence; in one of them, QH,"on this flight" attaches high to "get," and in the other, QL, it attaches low to "dinner.
"Four functions return non-zero scores on these analyses.
Two of them, Lowl and Low2,prefer low attachment; the difference between them is an implementation detail thatcan be ignored here.
A third, SynRules, returns an estimate of the log probability ofthe syntactic rules used to construct the analysis.
A fourth, SemColl, is a semanticcollocation function.
The scores, after multiplying by scaling factors, are as shown inTable 1.
The SemColl function is the only one that prefers QH to QL.
Because thisfunction has a relatively large scaling factor, it is able to override the other four, whichall prefer QL for syntactic reasons.2.1 Training DataThe Penn Treebank contains around 650 ATIS trees, which we used during initial de-velopment of training and optimization software.
Some of the results in these initialtrials were encouraging, but most appeared to be below reasonable thresholds of sta-1 The hand-parsed sub-corpus was that on the ACL DCI CD-ROM 1 of September 1991.
The largercorpus, used for the bulk of the work reported here, consisted of 4615 class A and D sentences fromthe ATIS-2 training corpus.
These were all such sentences of up to 15 words that we had access to atthe time, excluding a set of randomly selected sentences that were set aside for other testing purposes.637Computational Linguistics Volume 20, Number 4Table 1Scaled preference scores for "Do I getdinner on this flight?
"Function Score Scoreon QH on QLLowi -9.08 -4.03Low2 -2.80 0.00SynRules -13.08 -12.78SemColl 24.32 3~38Total -0.64 -13.34tistical significance.
So, we concluded that it was worthwhile to produce more trainingdata.
For this purpose, we developed a semiautomatic mechanism for producing skele-tal constituent s ructure trees directly from QLF analyses proposed by our analyser.
Tomake these trees compatible with the treebank and to make them relatively insensi-tive to minor changes in semantic analysis, these QLF-induced trees consist simply ofnested constituents with two categories, A (argument) and P (predication), correspond-ing to constituents induced by QLF term and form expressions, respectively.
The treefor the example sentence used above is as follows:(p do(A I)get(A dinner)(P on(A thisflight) ) )The interactive software for producing the trees proposes constituents for confir-mation by a user and takes into account answers given, to minimize the number ofinteractive choices necessary.
Of the 4615 sentences in our training set, the CLE pro-duced an acceptable constituent s ructure for 4092 (about 89%).
A skeletal tree for eachof these 4092 sentences was created in this way and used in the various experimentswhose results are described below.
We do not directly address here the problems ofapplying preference functions to select he best analysis when none is completely cor-rect; we assume, based on our experience with the spoken language translator, thatfunctions and scaling factors trained on cases for which a completely correct analysisexists will also perform fairly well on cases for which one does not.2.2 Training ScoreEmploying treebank analyses in the training process required defining a measure ofthe "'degree of correctness" of a QLF analysis under the assumption that the phrase-structure analysis in the treebank is correct.
At first sight this might appear difficult,in that QLF is a logical formalism, but in fact it preserves much of the geometry ofconstituent s ructure.
Specifically, significant (typically BAR-2 level) constituents endto give rise to term (roughly argument) or form (roughly predication) QLF subexpres-sions, though the details do not matter here.
It is thus possible to associate segmentsof the input with such QLF subexpressions and to check whether such a segment isalso present as a constituent in the treebank analysis.
The issues raised by measuring638Hiyan Alshawi and David Carter Training and Scaling Preference FunctionsQLF correctness in terms of agreement with structures containing less informationthan those QLFs are discussed further at the end of Section 4.The training score functions we considered for a QLF q with respect to a treebanktree t were functions of the formscore(q, t) = al lQ N T I - a21Q \ T\] - a31T \ QI,where Q is the set of string segments induced by the term and form expressions ofq; Tis the set of constituents in t; al, a2, and a3 are positive constants; and the "\" operatordenotes et difference.
The idea is to reward the QLF for constituents in common withthe treebank and to penalize it for differences.
Trial and error led us to choosea1=1,  a2=10, a3=0,which penalizes hallucination of incorrect constituents (modeled by \[Q \ T\]) moreheavily than a shortfall in completeness (modeled by IQ n TI).
These constants werefixed before we carried out the experiments whose results are presented below.The explanation for setting a3 to 0 was that trees in the Penn Treebank containmany constituents hat do not correspond to QLF form or term expressions; we hadto avoid penalizing QLF analyses imply because the treebank uses a different kind oflinguistic representation.
For QLF-induced trees, in which the correspondence is oneto one, it is also reasonable to set a3 to 0 because when IT \ Q I is non-zero, I Q A T I tendsto be non-maximal.
Among the 4092 sentences for which skeletal trees were derived,there were only 5 with alternative QLFs for which the training score value was thesame with a3 = 0 but would be different if a3 were non-zero.3.
Computing Scaling FactorsWhen we first implemented a disambiguation mechanism of the kind described above,an initial set of scaling factors was chosen by hand according to knowledge of howthe particular aw preference functions were computed and introspection about the"strength" of the functions as indicators of preference.
These initial scaling factorswere subsequently revised according to their observed behavior in ranking analyses,eventually leading to reasonably well-behaved rankings.However, as suggested earlier, there are a number of disadvantages to manualtuning of scaling factors.
These include the effort spent in maintaining the parame-ters.
This effort is greater for those with less knowledge of how the raw preferencefunctions are computed, since this increases the effort for trial-and-error tuning.
Apoint of diminishing returns is also reached, after which further attempts at improve-ment through hand tuning often turn out to be counterproductive.
Another problemwas that it became difficult to detect preference functions that were ineffective, orsimply wrong, if they were given sufficiently low scaling factors.
Probably a moreserious problem is that the contributions of different preference functions to selectingthe most plausible analyses eem to vary from one sublanguage to another.
Thesedisadvantages point to the need for automatic procedures to determine scaling factorsthat optimize preference function rankings for a particular sublanguage.In our framework, a numerical "preference score" is computed for each of thealternative analyses, and the analyses are ranked according to this score.
As mentionedearlier, the preference score is a weighted sum of a set of preference functions: Eachpreference function f/ takes a complete QLF representation qi as input, returning anumerical score sq, the overall preference score being computed by summing over the639Computational Linguistics Volume 20, Number 4product of function scores with their associated scaling factors cj:ClSil q- .
.
.
-}- CmSim3.1 Collection ProcedureThe training process begins by analyzing the corpus sentences and computing, foreach analysis of each sentence, the training score of the analysis with respect o themanually approved skeletal tree and the (unscaled) values of the preference functionsapplied to that analysis.One source of variation in the data that we want to ignore in order to derive scalingfactors appropriate for selecting QLFs is the fact that preference function values for ananalysis often reflect characteristics shared by all analyses of a sentence, as much asthe differences between alternative analyses.
For example, a function that counts theoccurrences of certain constructs in a QLF will tend to give higher values for QLFsfor longer sentences.
In the limit, one can imagine a function ~b that, for an N-wordsentence, returned a value of N + G for a QLF with training score G with respect to theskeletal tree.
Such a function, if it existed, would be extremely useful, but (if sentencelength were not also considered) would not be a particularly accurate predictor of theQLF training score.To discount irrelevant intersentence variability, both the training score with respectto the skeletal tree and all the preference function scores are therefore relativized bysubtracting from them the corresponding values for the analysis of that sentence whichbest matches the skeletal tree.
If the best match is shared by several analyses, theaverage for those analyses is subtracted.
The relativized training score is the distancefunction with respect o which the first stage of scaling factor calculation takes place.It can be seen that the relativized results of our hypothetical preference function 6 area perfect predictor of the relativized training score.
Consider, for example, a six-wordsentence with three QLFs, two of which, ql and q2, have completely correct skeletaltree structures and the third of which, q3, does not.
Suppose also that the trainingscores and the scores assigned by preference functions, G fl, and fz, are as follows:Training ~ fl f2ql 10 16 8 4q2 10 16 6 10q3 4 10 2 12After relativizing (subtracting the average of the ql and q2 values), we getTraining ~ fl f2ql 0 0 1 -3q2 0 0 --1 3q3 --6 --6 --5 53.2 Least Squares CalculationAn initial set of scaling factors is calculated in a straightforward analytic way byapproximating gi, the relativized training score of qi, by ~j  cjzij, where cj is the scalingfactor for preference function fj and zq is the relativized score assigned to qi by ~.
Wevary the values of cj to minimize the sum, over all QLFs for all training sentences, ofthe squares of the errors in the approximation2640Hiyan Alshawi and David Carter Training and Scaling Preference FunctionsDefining the error function as a sum of squares of differences in this way means that theminimum error is attained when the derivative with respect o each ck, --2 ~ i  Zik(gi --y'~q CjZq), is zero.
These linear simultaneous equations, one for each of cl ... c,,, can besolved by Gaussian elimination.
(For a full explanation of this standard technique, seeMoore and McCabe 1989, pp.
174ff and 680ff.
)This least squares et of scaling factors achieves quite good disambiguation per-formance (see Section 4) but is not truly optimal because of the inherent nonlinearityof the goal, which is to maximize the proportion of sentences for which a correct QLFis selected, rather than to approximate raining scores (even relativized ones).
Supposethat a function M has a tendency to give high scores to correct QLFs when the contri-butions of other functions do not clearly favor any QLF, but that M tends to performmuch less well when other functions come up with a clear choice.
Then increasing thescaling factor on M from the least squares value will tend to improve system perfor-mance ven though the sum of squares of errors is increased; M's tendency to performwell just when it is important to do so should be rewarded.3.3 Iterative Scaling Factor AdjustmentThe least squares caling factors are therefore adjusted iteratively by a hill-climbingprocedure that directly examines the QLF choices they give rise to on the trainingcorpus.
Scaling factors are altered one at a time in an attempt to locally optimize 2 thenumber of correct disambiguation decisions, i.e., the number of training sentences forwhich a QLF with a correct skeletal tree receives the highest score.A step in the iteration involves calculating the effect of an alteration to each factorin turn.
3 If factors Ck, k ~ j, are held constant, it is easy to find a set (possibly empty)of real-valued intervals \[u/j, viii such that a correct choice will be made on sentence iif uij < cj <_ vii.
By collecting these intervals for all the functions and for all the sen-tences in the training corpus, one can determine the effect on the number of correctdisambiguation decisions of any alteration to any single scaling factor.
The alterationselected is the one that gives the biggest increase in the number of sentences for whicha correct choice is made.
When no increase is possible, the procedure terminates.
Wefound that convergence t nds to be fairly rapid, with the number of steps seldom ex-ceeding the number of scaling factors involved (although the process does occasionallychange a scaling factor it has previously altered, when intervening changes make thisappropriate).One of the functions we used shows the limitations of least squares caling factoroptimization, alluded to above, in quite a dramatic way.
The function in questionreturns the number of temporal modifiers in a QLE Its intended purpose is to favorreadings of utterances like "Atlanta to Boston Tuesday," in which "Tuesday" is atemporal modifier of the (elliptical) sentence rather than a compound noun formedwith "Boston."
Linear scaling always gives this function a negative weight, causingtemporal modifications tobe downgraded, and in fact the relativized training score ofa QLF turns out to be negatively correlated with the number of temporal modifiers itcontains.
However, the intuitions that led to the introduction of the function do seem2 Finding a global optimum would of course be desirable.
However, inspection of the results, overvarious conditions, of the iterative scheme presented here did not suggest that the introduction of atechnique such as simulated annealing, which in general can improve the prospect of finding a moreglobal optimum, would have had much effect on performance.3 An algorithm based on gradient descent might appear preferable, on the grounds that it would alter allfactors imultaneously and have a better chance of locating a global optimum.
However, the objectivefunction, the number of correct disambiguation decisions, varies discontinuously with the scalingfactors, so no gradients can be calculated.641Computational Linguistics Volume 20, Number 4Table 2Performance ofscaling factor setsScaling factor set Number Percentagecorrect correct(Random baseline) 1949 47.6Normalized 3549 86.7Hand tuned 3717 90.8Least squares 3841 93.9Hill climbing 3857 94.3to hold for QLFs that are close to being correct, and therefore iterative adjustmentmakes the weight positive.4.
Comparing Scaling Factor SetsThe performance of the factors derived from least squares calculation and adjustmentby hill climbing was compared with that of various other sets of factors.
The factor setsconsidered, roughly in increasing order of their expected quality, were the following:?
"Normalized" factors: the magnitude of each factor is the inverse of thestandard eviation of the preference function in question, making eachfunction contribute qually.
A factor is positive if it correlates positivelywith training scores; otherwise it is negative.?
Factors chosen and tuned by hand for ATIS sentences before the workdescribed in this paper was done, or, for functions developed uring thework described here, without reference to any automatically derivedvalues.?
Factors resulting from least squares calculation, as described inSection 3.2.?
Factors resulting from least squares calculation followed by hill-climbingadjustment (Section 3.3).To provide a baseline, performance was also evaluated for the technique of a randomselection of a single QLF for each sentence.The performance of each set of factors was evaluated as follows.
The set of 4092sentences with skeletal trees was divided into five subsets of roughly equal size.
Eachsubset was "held out" in turn: the functions and scaling factors were trained on theother four subsets, and the system was then evaluated on the held-out subset.
Thesystem was deemed to have correctly processed a sentence if the QLF to which itassigned the highest score agreed exactly with the corresponding skeletal tree.The numbers of correctly processed sentences (i.e., sentences whose selected QLFshad correct constituent structures) are shown in Table 2; because all the sentencesinvolved were within coverage, the theoretical maximum achievable is 4092 (100%).We use a standard statistical method, the sign test (explained in, for example,Dixon and Massey 1968), to assess the significance of the difference between twofactor sets, $1 and $2.
Define Fi(x) to be the function that assigns 1 to a sentence x ifSi makes the correct choice in disambiguating x and 0 if it makes the wrong choice.642Hiyan Alshawi and David Carter Training and Scaling Preference FunctionsTable 3Sign test comparisons of scaling factor setsS1 $2 + - #SDsNormalized Hand tuned 154 322 7.7Normalized Least squares 67 359 14.1Normalized Hill climbing 75 383 14.4Hand tuned Least squares 78 202 7.4Hand tuned Hill climbing 76 216 8.2Least squares Hill climbing 20 36 2.1The null hypothesis is that F1 (x) and F2(x), treated as random variables over x, havethe same distribution, from which we would expect he difference between F1 (x) andF2(x) to be positive as often as it is negative.
Table 3 gives the number of cases inwhich this difference is positive or negative.
As is usual for the sign test, the casesin which the difference is 0 do not need to be taken into account.
The test is appliedto compare six pairs of factor sets.
The "#SDs" column in Table 3 shows the numberof standard eviations represented by the difference between the "+" and "-" figuresunder the null hypothesis; a #SDs value of 1.95 is statistically significant at the 5%level (two tail), and a value of 3.3 is significant at the 0.1% level.Table 3 shows that, in terms of wrong QLF choices, both sets of machine-optimizedfactors perform significantly better than the hand-optimized factors, to which consid-erable skilled effort had been devoted.
It is worth emphasizing that the process ofdetermining the machine-optimized factors does not make use of the knowledge n-coded by hand optimization.
The hill-climbing factor set, in turn, performs ignificantlybetter than the least squares et from which it is derived.A possible objection to this analysis is that, because QLFs are much richer struc-tures than constituent trees, it is possible for a QLF to match a tree perfectly but havesome other characteristic that makes it incorrect.
In general, the principal source ofsuch discrepancies i a wrong choice of word sense, but pure sense ambiguity (i.e.,different predicates for the same syntactic behavior of the same word) turns out tobe extremely rare in the ATIS corpus.
An examination of the selected QLFs for the20 + 36 = 56 sentences making up the ?
and - values for the comparison between theleast squares and hill-climbing factor sets showed that in no case did a QLF have acorrect constituent structure but fail to be acceptable on other criteria.
Thus althoughthe absolute percent correctness figures for a set of scaling factors may be very slightly(perhaps up to 1%) overoptimistic, this has no noticeable ffect on the differences be-tween factor sets.5.
Lexical Semantic CollocationsIn this section we move from the problem of calculating scaling factors to the othermain topic of this paper, showing how our experimental framework can be used di-agnostically to compare the utility of competing suggestions for preference functions.We refer to the variant of collocations we used as lexical semantic ollocations because(i) they are collocations between word senses rather than lexical items, and (ii) the rela-tions used are often deeper than syntactic relations (for example the relations betweena verb and its subject are different for passive and active sentences).The semantic ollocations extracted from QLF expressions take the form of (H1, R,H2) triples, in which H1 and H2 are the head predicates of phrases in a sentence643Computational Linguistics Volume 20, Number 4and R indicates the relation (e.g., a preposition or an argument position) between thetwo phrases in the proposed analysis.
For this purpose, the triple derivation softwareabstracted away from proper names and some noun and verb predicates when they ap-peared as heads of phrases, replacing them by hand-coded class predicates.
For exam-ple, predicates for names of meals are mapped onto the class name cc SpecificMealon the grounds that their distributions in unseen sentences are likely to be very similar.Some of the triples for the high-attachment QLF for "Do I get dinner on thisflight?"
are as follows:(getAcquire, 2, personal)(getAcquire, 3, cc_Specif icMeal)(get_Acquire, on, f light_AirplaneTrip).The first two triples correspond to the agent and theme positions in the predicate forget, whereas the third expresses the vital prepositional phrase attachment.
In the tripleset for the other QLF, this triple is replaced by(cc_SpecificMeal, on, flight_AirplaneTrip).Data collection for the semantic ollocation functions proceeds by deriving a setof triples from each QLF analysis of the sentences in the training set.
This is followedby statistical analysis to produce the following functions of each triple in the observedtriple population.
The first two functions have been used in other work on collocation;some authors use simple pairs rather than triples (i.e., no relation, just two words)when computing collocation strengths, so direct comparisons are a little difficult.
Thethird function is an original variant of the second; the fourth is original; and the fifthis prompted by the arguments of Dunning (1993).?
Mutual information: this relates the probability Pl(a)P2(b)P3(c) of thetriple (a, b~ c) assuming independence between its three fields, whereP~(x) is the probability of observing x in position p, with the probabilityA estimated from actual observations of triples derived from analysesranked highest (or joint highest) in training score.
More specifically, weuse ln{a/\[P1 (a)Pa(b)P3(c)\]}.?
X2: compares the expected frequency E of a triple with the square of thedifference between E and the observed frequency F of the triple.
Herethe observed frequency is in analyses ranked highest (or joint highest) intraining score, and the "expected" frequency assumes independencebetween triple fields.
More specifically we use IF - E\] ?
(F - E)/E.
Thisvariant of X 2, in which the numerator is signed, is used so that thefunction is monotonic, making it more suitable in preference functions.?
X: as X 2, but the quantity used is (F - E)/v"E, as large values of F - Ehave a tendency to swamp the X 2 function.?
Mean distance: the average of the relativized training score for all QLFanalyses (not necessarily the highest ranked ones) that include thesemantic ollocation corresponding to the triple.
In other words, themean distance value for a triple is the mean amount by which a QLFgiving rise to that triple falls short of a perfect score.?
Likelihood ratio: for each triple (HI~ R, H2), the ratio of the maximumlikelihood of the triple, given the distribution of triples in correct644Hiyan Alshawi and David Carter Training and Scaling Preference FunctionsTable 4Performance of collocational nd syntactic rulefunctions aloneFunction Number Percentagecorrect correct(Random baseline) 1949 47.6Mutual info 2817 68.9Syntactic rule cost 2913 71.2Likelihood ratio 3120 76.3X 2 3339 81.6X 3407 83.3Mean distance 3670 89.7analyses of the training data, on the assumption that H1 and H2 areindependent given R, to the maximum likelihood without thatassumption.
(See Dunning 1993, for a fuller explanation of the use oflikelihood ratios.
)Computation of the mutual information and 2 functions for triples involves thesimple smoothing technique, suggested by Ken Church, of adding 0.5 to actual counts.From these five functions on triples, we define five semantic ollocation preferencefunctions applied to QLFs, in each case by averaging over the result of applyingthe function to each triple derived from a QLE We refer to these functions by thesame names as their underlying functions on triples.
The collocation functions arenormalized by multiplying up by the number of words in the sentence to whichthe function is being applied.
This normalization keeps scores for QLFs in the samesentence comparable, while at the same time ensuring that the triple function scorestend to grow with sentence length in the same way that the non-collocation functionstend to do.
Thus the optimality of a set of scaling factors is relatively insensitive tosentence length.Our use of the mean distance function was motivated by the desire to take intoaccount additional information from the training material that is not exploited by theother collocation functions.
Specifically, it takes into account all analyses proposedby the system, as well as the magnitude of the training score.
In contrast, the othercollocation functions make use only of the training score to select the best analysisof a sentence, discarding the rest.
Another way of putting this is that the mean dis-tance function is making use of negative examples and a measure of the degree ofunacceptability of an analysis.6.
Comparing Semantic Collocation FunctionsAn evaluation of each function acting alone on the five held-out sets of test datayielded the numbers of correctly processed sentences hown in Table 4.
The figuresfor the random baseline are repeated from Table 2.
We also show, for comparison,the results for a function that scores a QLF according to the sum of the logs of theestimated probabilities of the syntactic rules used in its construction.
44 We estimate the probability of occurrence ofa syntactic rule R as the number of occurrences of Rleading to QLFs with correct skeletal trees, divided by the number of occurrences of all rules leading tosuch QLFs.645Computational Linguistics Volume 20, Number 4Table 5Performance ofcollocational functions withothersFunction Number Percentagecorrect correctX 2 3741 91.4X 3766 92.0Mean distance 3857 94.3In cases where a function judged N QLFs equally plausible, of which 0 < G < Nwere correct, we assigned a fractional count G/N to that sentence; a random choiceamong the N QLFs would pick a correct one with probability G/N.
For significancetests, which require binary data, we took a function as performing correctly only if allthe QLFs it selected were correct.
Such ties did not occur at all for the other experimentsreported in this paper.A pairwise comparison of the results hows that all the differences between collo-cational functions are statistically highly significant.
The syntactic rule cost function issignificantly worse than all the collocational functions except he mutual informationone, for which the difference is not significant either way.
(There may, of course, existbetter syntactic functions than the one we have tried.)
The mean distance functionis much superior to all the others when acting alone.
Presumably, this function hasan edge over the other functions because it exploits the additional information fromnegative xamples and degree of correctness.The difference in performance b tween our syntactic and semantic preference func-tions is broadly in line with the results presented by Chang, Luo, and Su (1992), whouse probabilities of semantic ategory tuples.
However, this similarity in the resultsshould be taken with some caution, because our syntactic preference function is rathercrude, and because our best semantic function (mean distance) uses the additional in-formation mentioned above.
This information is not normally taken into account bydirect estimates of tuple probabilities.When I collocation function is selected to act together with the 19 non-collocation-based functions from the default set (the set defined in Section 2 and used in theexperiments on scaling factor calculation), the picture changes lightly.
In this context,when scaling factors are calculated in the usual way, by least squares followed by hillclimbing, the results for the best 3 of the above functions are as shown in Table 5.The difference between the mean distance function and the other 2 functions isstill highly significant; herefore this function is chosen to be the only collocational oneto be included in the default set of 20 (hence the "mean distance" condition here is thesame as the "hill-climbing" condition in Section 4).
However, the difference betweenthe X and X 2 functions is no longer quite so clear cut, and the relative advantage ofthe mean distance function compared with the X function is less.
It may be that otherpreference functions make up for some shortfall of the X function that is, at least inpart, taken into account by the mean distance function.7.
Conclus ionWe have presented a relatively simple analytic technique for automatically determininga set of scaling factors for preference functions used in semantic disambiguation.
Theinitial scaling factors produced are optimal with respect o a score provided by a646Hiyan Alshawi and David Carter Training and Scaling Preference Functionstraining procedure and are further improved by comparison with instances of the taskthey are intended to perform.
The experimental results presented indicate that, byusing a fairly crude training score measure (comparing only phrase structure trees)with a few thousand training sentences, the method can yield a set of scaling factorsthat are significantly better than those derived by a labor-intensive hand-tuning effort.We have also confirmed empirically that considerable differences exist between theeffectiveness of differently formulated collocation functions for disambiguation.
Theexperiments provide a basis for selecting among different collocational functions andsuggest hat a collocation function must be evaluated in the context of other functions,rather than on its own, if the correct selection is to be made.It should be possible to extend this work fruitfully in several directions, includingthe following.
Training with a measure defined directly on semantic representations islikely to lead to a further reduction in the disambiguation error rate.
The method forcomputing scaling factors described here has more recently been applied to optimizingpreference selection for the task of choosing between analyses arising from differentword hypotheses in a speech recognition system (Rayner et al 1994) and is applica-ble to other problems, such as choosing between possible target representations in amachine translation system.
Finally, it would be interesting to combine the work onsemantic ollocation functions with that on similarity-based clustering (Pereira, Tishby,and Lee 1993; Dagan, Marcus, and Markovitch 1993), with the aim of overcoming theproblem of sparse training data.
If this is successful, it might make these functionssuitable for disambiguation i domains with larger vocabularies than ATIS.AcknowledgmentsWe would like to thank Manny Rayner formany useful suggestions in carrying outthis work, for making the selectionsnecessary to create the database of skeletaltrees, and for helping with inspection ofexperimental results.
The paper has alsobenefited from discussions with FernandoPereira, Ido Dagan, Michael Collins, StevePulman, and Jaan Kaja and from thecomments of four anonymous ComputationalLinguistics referees.Most of the work reported here wascarried out under a Spoken LanguageTranslation project funded by Telia(formerly Televerket/Swedish Telecom).Other parts were done under the CLAREproject (JFIT project IED4/1/1165), fundedby the UK Department of Trade andIndustry, SRI International, the DefenceResearch Agency, British Telecom, BritishPetroleum, and British Aerospace.ReferencesAgn~s, M-S., et al (1993).
"Spoken languagetranslator: First year report."
SRIInternational Cambridge ComputerScience Research Centre, Report 043.Alshawi, H., editor (1992).
The Core LanguageEngine.
Cambridge: MIT Press.Alshawi, H., and Crouch, R.
(1992).
"Monotonic semantic interpretation."
InProceedings, 30th Annual Meeting of theAssociation for Computational Linguistics,Newark, Delaware, 32-39.Calzolari, N., and Bindi, R.
(1990).
"Acquisition of lexical information from alarge textual Italian corpus."
InProceedings, 13th International Conference onComputational Linguistics, 3:54-59.Chang, J., Luo, Y., and Su, K.
(1992).
"GPSM: A generalized probabilisticsemantic model for ambiguityresolution."
In Proceedings, 30th AnnualMeeting of the Association for ComputationalLinguistics, 177-192.Church, K. W., and Hanks, P. (1990).
"Wordassociation orms, mutual information,and lexicography."
ComputationalLinguistics 16:22-30.Dagan, I.; Marcus, S.; and Markovitch, S.(1993).
"Contextual word similarity andestimation from sparse data."
InProceedings, 31st meeting of the Associationfor Computational Linguistics, ACL, 164-171.Dixon, W. J., and Massey, E J.
(1968).Introduction to Statistical Analysis, ThirdEdition.
New York: McGraw-Hill.Dunning, T. (1993).
"Accurate methods forstatistics of surprise and coincidence.
"Computational Linguistics 19:61-74.Gale, W. A., and Church, K. W.
(1991).
"Identifying word correspondences inparallel texts."
In Proceedings, DARPASpeech and Natural Language Workshop,647Computational Linguistics Volume 20, Number 4152-157.
Los Altos, California: MorganKaufmann.Hindle, D., and Rooth, M. (1993).
"Structuralambiguity and lexical relations.
"Computational Linguistics 19:103-120.Hobbs, J. R., and Bear, J.
(1990).
"Twoprinciples of parse preference."
Volume 3,In Proceedings, 13th International Conferenceon Computational Linguistics, Helsinki,162-167.Marcus, M. P.; Santorini, B.; andMarcinkiewicz, M. A.
(1993).
"Building alarge annotated corpus of English: ThePenn treebank."
Computational Linguistics19:313-330.McCord, M. C. (1989).
"A new version ofslot grammar."
IBM Research Report RC14506, IBM Thomas J. Watson ResearchCenter, Yorktown Heights, New York.McCord, M. C. (1993).
"Heuristics forbroad-coverage natural languageparsing."
In Proceedings, ARPA HumanLanguage Technology Workshop, 127-132.Los Altos, California: Morgan Kaufmann.Moore, D. S., and McCabe, G. P. (1989).Introduction to the Practice of Statistics.
NewYork/Oxford: Freeman.Ostendorf, M.; Kannan, A.; Austin, S.;Kimball, O.; Schwartz, R.; and Rohlicek,J.
R. (1991).
"Integration of diverserecognition methodologies throughreevaluation of N-best sentencehypotheses."
In Proceedings, DARPA Speechand Natural Language Workshop, 83-87.
LosAltos, California: Morgan Kaufmann.Pereira, F., Tishby, N., and Lee, L.
(1993).
"Distributional c ustering of Englishwords."
In Proceedings, 31st meeting of theAssociation for Computational Linguistics,ACL, 183-190.Rayner, M.; Carter, D. M.; Digalakis, V.; andPrice, P. (1994).
"Combining knowledgesources to reorder N-best speechhypothesis lists."
In Proceedings, ARPAHuman Language Technology Workshop, inpress.Resnik, P., and Hearst, M. A.
(1993).
"Structural ambiguity and conceptualrelations."
In Proceedings, Workshop on VeryLarge Corpora, ACL, 58-64.Sekine, S.; Carroll, J. J.; Ananiadou, S.; andTsujii, J.
(1992).
"Automatic learning forsemantic ollocation."
In Proceedings, ThirdConference on Applied Natural LanguageProcessing, ACL, 104-110.648
