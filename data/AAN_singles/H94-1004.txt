Lexicons for Human Language TechnologyMark LibermanLinguistic Data ConsortiumUniversity of PennsylvaniaPhiladelphia, PA 19104-6305myl@unagi, cis.
upenn, eduABSTRACTInformation about words--their pronunciation, syntax andmeaning--is a crucial and costly part of human languagetechnology.
Many questions remain about the best way toexpress and use such lexical information.
Nevertheless, muchof this information is common to all current approaches, andtherefore the effort to collect it can usefully be shared.
TheLinguistic Data Consortium (LDC) has undertaken to pro-vide such common lexical information for the community ofHLT researchers.
The purpose of this paper is to sketch thevarious LDC lexical projects now underway or planned, andto solicit feedback from the community of HLT researchers.1.
IntroductionThis paper will give an overview of current LDC effortsto develop lexical resources and describe some effortsnow in the planning stage.
Readers are invited to joinan on-going discussion of priorities, methods and evenformats for our present and future efforts in this area.1.1.
Intellectual Property RightsSince lexicons, unlike text and speech databases, arelikely to be incorporated (perhaps in derived form) incommercial HLT products, intellectual property rightscome to center stage.
The LDC's charter as a consor-tium requires us to leverage the U.S. government's in-vestment by sharing the cost of resource developmentamong our members.
This forces us to limit usage ofsuch resources to consortium members, or others whohave paid an appropriate fee.
However, we also want toencourage rapid development and broad exploitation ofcommercial HLT technology.
Therefore, we need to pro-tect our members' investment in research based on LDCresources by ensuring their rights to future commercialexploitation without additional license negotiations, roy-alty payments, or other intellectual property issues.This contrasts with the general practice for research useof machine-readable dictionaries, in which all rights toderived works are typically reserved to the publisher.
Forthis reason, our lexicons will not be derived from exist-ing lexicons, except as permitted by normal provisionsof copyright law, or in case we are able to purchase up-propriate rights from the owner of the existing resource.This also contrast with our practice with respect o textdatabases, where we have negotiated agreements o dis-tribute for research purposes many bodies of text whosecopyright remains with the original owner.
The differ-ence here is that the text corpora themselves will nottypically be incorporated in future products, and ourunderstanding of the applicable law (which we openlyexplain to information providers) is that language mod-els trained on such text are free of any IPR taint.We have worked hard, in consultation with our mem-bers, to develop an appropriate license for LDC lexi-cons.
A copy of the draft license agreement for COM-LEX syntax will be furnished on request o the author,or ldc@unagi.cis.upenn.edu.2.
Current English Lexicon EffortsOur primary effort is to provide lexicons for English.
Weare funding a large, high-quality English pronunciationlexicon; an English syntactic lexicon, including detailedinformation about syntactic properties of verbs; and aset of improvements in an existing lexicon of Englishword-sense differences.
All three lexicons will eventuallybe tied to appropriately sampled occurrences in text andspeech corpora.Based on an original proposal from Ralph Grishman andJames Pustejovsky, we have been called our English lex-icon "COMLEX," for COMmon LEXicon.2.1.
Pronunciation: PRONLEXFor the COMLEX English pronouncing dictionary("PRONLEX"),  the LDC has obtained (by purchase ordonation) rights to combine four existing large and high-quality lexicons.
Bill Fisher at NIST has been carryingout a pilot project o design a consensus segment set, andto map the representations in the multiple sources intoit automatically.
Then words where the various sourcesagree will be accepted, while disagreements will be adju-dicated by human judges, and new words will be addedas needed.13The., sources we are starting from will provide coverageof more than 250K word forms.
Appropriate coverage ofthe words found in the various ARPA speech recognitiondatabases will also be guaranteed.
We solicit suggestionsfor :lists of other words to cover, such as proper names,including surnames, place names, and company names.The pronunciation representations u ed in the first re-lease of PRONLEX, being based on those in the lexiconswe are starting from, will be similar to those provided intypical dictionary pronunciation fields and used in mostof today's peech recognition systems.
This level is bestdescribed as "surface phonemic" rather than phonetic--it abstracts away from most dialect variation, context-conditioned variation, and casual-speech reduction.Pat Keating at UCLA has been carrying out a pilotproject to examine systematically the relationship be-tween such normative pronunciations and the actualphonetic segments found when the corresponding wordsare used in conversational speech.
We provided a sam-ple of occurrences of words with high, medium and lowfrequencies of occurrence, drawn from the Switchboarddata base.
We will use the results of this study to planhow to improve the pronunciations in the initial releaseof PRONLEX.
Readers are invited to join an on-goingemail discussion of this topic.2.2.
COMLEX SyntaxA lexicon of syntactic information, known as "COMLEXSyntax," is under development by Ralph Grishman andothers at NYU.
After designing the feature set and rep-resentational conventions, Grishman created a zeroth-order mock-up from existing resources.
This has beencirculated for comments and is available to interestedparties from the LDC, along with the specifications forthe syntactic features and lexical representations to beused.
The project at NYU is now doing the lexicon over?
again by hand, guided by corpus-derived xamples.
Thefirst release will occur later this year.2.3.
COMLEX Semant icsThe existing WordNet lexical database, available fromGeorge Miller's group at Princeton, provides a num-ber of kinds of semantic information, including hypo-/hypernym relations and word sense specification.
Inorder to improve the quality of its coverage of real wordusage, and to provide material for training and testing"semantic taggers," the LDC has funded an effort byMiller's group to tag the Brown corpus using WordNetcategories, modifying WordNet as needed.2.4.
COMLEX CorpusBecause of the Zipfian 1/f distribution of word frequen-cies, a corpus would have to be unreasonably large inorder to offer reasonable sample of an adequate numberof words.
Although it is no longer difficult to amass acorpus of hundreds of millions or even billions of words,complete human annotation of such a corpus is imprac-tical.
Therefore the LDC proposes to create a new kindof sampled corpus, offering a reasonable sample of thewords in a lexicon the size of COMLEX Syntax, so thathuman annotation or verification of (for instance) fourmillion tokens would provide 100 instances of each of40K word types?
This sampled corpus (in reality to besampled according to a more complex scheme) can thenbe "tagged" with both syntactic and semantic ategories.The entire corpus from which the sample is drawn willalso be available, so that arbitrary amounts of contextcan be provided for each citation.
The design of thissampled corpus is still under discussion, and reader par-ticipation is again invited.3.
Other LanguagesThis past year, we cooperated with the CELEX groupin the Netherlands to publish their excellent lexicaldatabases for English, German and Dutch.
In this case,our willingness to pay for CD-ROM production, to han-dle the technical arrangements for publication, and toshoulder some of the burden of distribution was enoughto help bring this resource into general availability.As a first step towards providing new lexical resourcesin languages other than English, the LDC has begun aneffort to provide medium-sized pronouncing dictionariesin a variety of languages.
This effort, which will be co-ordinated with efforts to provide transcribed speech andtext resources in the same languages, is beginning thisyear with Japanese, Mandarin, and Spanish.
It aims atcoverage comparable toan English dictionary with about20K words.As the U.S. speech research community begins to workon languages other than English, it is confronted withnew issues that have reflexes in the design and imple-mentation ofeven such a simple-seeming object as a pro-nouncing dictionary.
Again, we solicit the community'sparticipation i helping us choose a useful approach.
Inthe next section, we would like to highlight one of thequestions that will need to be answered, language bylanguage, in the early stages of such a project.Morphology?
The question is, howshould orthographically-defined units be broken up orcombined in a lexicon?
One answer, which is the eas-iest one to give for an English pronouncing dictionary14for speech recognition applications, is "not at all: list alland only the orthographic units paired with their pro-nunciations."
For other languages, this answer may nolonger apply.Table 1 shows (for 5 databases of journalistic text) how?
many word types are needed to account for various per-centages of word tokens.
In all languages except Chi-nese, the principles for defining "words" in the text werethe same: a contiguous tring of Mphabetic charactersflanked by white space preceded and followed by anynumber of punctuation characters, with case distinctionsignored.
All "words" containing digits or other non-alphabetic haracters were left out of the counts, exceptthat a single internal hyphen was permitted.
In the caseof Chinese, the notion of "word" was replaced by "char-acter" for purposes of calculating this table.As Table 1 shows, languages with a larger number of in-flected forms per word, or with more productive deriva-tional processes not split up in the orthography (such asGerman compounding), tend to require a larger numberof word types to match a given number of word tokens.The counts for Chinese represent the other extreme, inwhich every morpheme (= Chinese character) is writtenseparately, and the orthography does not even indicatehow these morphemes are grouped into words (either inthe phonological sense, or in the sense that any Chinesedictionary lists tens of thousands of 2-, 3-, or 4-charactercombinations whose meaning is not predictable from themeaning of the parts).In English, the orthographic word is a fairly convenientunit both for pronunciation determination and for lan-guage modeling.
Depending on the mix of word types inthe sample, there are only about 2 to 2.5 inflected formsper "lemma" (base form before inflection), and the rulesof regular inflection are fairly easy to write.
Productivederivation of new words from old (e.g.
"sentencize") isnot all that common.
Most compounds are written withwhite space between the members, even if their meaningand stress are not entirely predictable (e.g.
"red her-ring," "chair lift").
For these reasons, a moderate-sizedlist of English orthographic forms can be found that willachieve good coverage in new text or speech.Smoothing is required for good-quality n-gram modelingof English word sequences in text, but morphologicalrelations among words have not been an important di-mension in most approaches.
Language models, like pro-nunciation models, can thus treat English orthographicwords as atoms.
As a result, from the point of viewof speech recognition technology, there has not been astrong need for an English pronouncing dictionary thatencodes morphological structure and features.However, the situation in German may be different.
AsTable 1 suggests, simple reliance on word lists derivedfrom a given amount of German text will produce a sig-nificantly lower coverage than for a corresponding En-glish case, and even very large lexicons will leave asurprisingly large number of words uncovered.
Thusthe Celex German lexicon, which contains 359,611 wordforms corresponding to 50,708 lemmas, failed to coverabout 10% of a sample of German text and transcribedspeech.
Of the missing words, about half were regularcompounds whose pieces were in the lexicon (e.g.
Leben-squalitdt), while by comparison less than 1/6 were propernames.The same sort of relative difficulty in unigram cover-age can be seen in Table 2, where we look at the countof word types for a lexicon derived from one sample inorder to cover a given percentage of word tokens in an-other sample.
German requires a two- or three-timeslarger lexicon than English does to achieve a given levelof coverage, and the factor increases with the coveragelevel.
This is not because of differences in the type oftext--al l  samples are drawn from the same or similarnewswires, covering the same or similar distributions oftopics.
Spanish is in between German and English inthis matter.One simple approach is to make the lexicon into a net-work that generates a large set of words and their pro-nunciations.
Thus German Lebensqualitdt will be derivedas a compound made up of Leben and Qualitdt.
Thepoint of such an exercise is not to shrink the size of thelexicon, or to express its redundancies (although bothare consequences), but rather to predict how the formswe have seen will generalize to the much larger numberof forms we have not seen yet.A similar issue arises for inflectional morphology.
AnItalian verb has at least 53 inflected forms (3 persons by2 numbers by 7 combinations of tense, aspect and mood,plus 4 past participle forms, 5 imperative forms, theinfinitive and the gerund).
Several hundred additional"cliticized" forms (joining the infinitive, the gerund andthree of the imperative forms with various combinationsof the 10 direct object and 10 indirect object pronouns)are also written without internal white space.
In adatabase of 3.2M words of Italian, forms of the com-mon verb "cercare" to look for occur 1818 times, but 8of the 53 regular forms are missing, and a larger num-ber of the possible combinations with object pronouns.Forms of the (also fairly common) verb "congiungere"occur 89 times, and 41 of its 53 forms are missing.
Thisindicates both the difficulty of finding all inflected formsas unigrams by simple observation, and also the greaterproblem for language modeling caused by the distribu-15tion of a lemma's probability mass among its variousforms.It is not obvious what the right approach is to thesecases, so researchers should have convenient access tolexicons that can easily be reconfigured to provide vari-ous types and degrees of subword analysis.Chinese presents exactly the opposite problem.
TheTaiwanese newspaper text used in the counts (done byRichard Sproat of AT&T Bell Labs) employs a total ofabout '7,300 character types in a corpus of more than17M character tokens.
Each character (with a few excep-tions) is pronounced injust one way, as a single syllable.However, a given syllable might be written as quite a fewdifferent possible characters, each one (roughly speaking)a separate morpheme.
There is no inflection in Chinese,but there is a lot of compounding of morphemes intowords with unpredictable meanings.
A typical Chinesedictionary will list tens of thousands of such combina-tions, and new forms are seen all the time, just as inGerman.
However, this compounding is not indicated inthe orthography.A language model based on (at least some) compoundwords will of course be effectively of higher order thanone based only on characters.
Again, there are severalapproaches to this question, ranging from explicit list-ing of the largest possible number of multiple-characterwords on standard lexicographical criteria, to a simplesmoothed N-gram model based on individual charactersas the only unigrams.
This issue has a phonetic sideas well, since multiple-character words in Mandarin of-ten have a fixed or strongly preferred stress pattern, andat least for some dialects, unstressed syllables may bestrongly reduced.Both issues--explicit representation of the internalstructure of certain orthographic words, and groupingof several contiguous orthographic words as a single lex-ical entry--have scattered echoes in speech recognitiontechnology as applied to English.
However, other lan-guages put these (and other) question on the agenda ina much stronger form.4.
New K inds  o f  Lex iconsNew ARPA tasks are likely to require new kinds of re-sources.
For instance, the outcome of the on-going dis-cussion about semantic evaluation will probably moti-vate new sorts of lexicons as well as new kinds of anno-tated corpora.16Table 1Corpus SizeAP English 3.0M 2,421Reuters Spanish 3.0M 2,510AP German 3.0M 4,742Italian 3.2M 5,136Mandarin Chinese 17M i80% 85% 90% 94% 98% 99% 100%3,784 6,406 11,095 26,844 38,990 66,5574,178 7,514 13,496 33,581 49,416 79,8438,258 16,091 31,440 76,704 107,141 137,5788,768 16,209 29,668 70,023 99,880 132,171659 843 1,124 1,509 2,384 2,937 7,337Table 1: Number of word types required to cover variouspercentages of word tokens within a given sample.Table 2Corpus 80% 85% 90% 94% 98%AP English 2,643 4,319 7,997 17,974 *Reuters Spanish 3,091 5,526 11,161 28,320 *AP German 5,558 10,715 27,404 * *Table 2: Number of word types, in frequency order, froma 500K-word sample, needed to cover various percent-ages of word tokens in a non-contiguous sample (abouttwo months away).
Asterisk means coverage at that levelwas not possible from the given sample.17
