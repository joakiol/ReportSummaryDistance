Proceedings of the Fifteenth Conference on Computational Natural Language Learning, page 219,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsBayesian Tools for Natural Language LearningInvited talkYee Whye TehGatsby Computational Neuroscience Unit, UCLywteh@gatsby.ucl.ac.ukIn recent years Bayesian techniques have made good inroads in computational linguistics, due to their pro-tection against overfitting and expressiveness of the Bayesian modeling language.
However most Bayesianmodels proposed so far have used pretty simple prior distributions, chosen more for computational conve-nience than as reflections of real prior knowledge.In this talk I will propose that prior distributions can be powerful ways to put computational linguis-tics knowledge into your models, and give two examples from my own work.
Firstly, hierarchical priorscan allow you to specify relationships among different components of your model so that the informationlearned in one component can be shared with the rest, improving the estimation of parameters for all.
Sec-ondly, newer distributions like Pitman-Yor processes have interesting power-law characteristics that if usedas prior distributions can allow your linguistic models to express Zipf?s Law and Heap?s Law.I will round up the talk with a discussion of the viability of the Bayesian approach, in a future wherewe have too much data, making the natural language learning problem more a computational rather than astatistical one.219
