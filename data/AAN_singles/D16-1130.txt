Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1224?1233,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsRecognizing Implicit Discourse Relations via Repeated Reading:Neural Networks with Multi-Level AttentionYang Liu1,2, Sujian Li11 Key Laboratory of Computational Linguistics, Peking University, MOE, China2 ILCC, School of Informatics, University of Edinburgh, United Kingdom{cs-ly, lisujian}@pku.edu.cnAbstractRecognizing implicit discourse relations is achallenging but important task in the fieldof Natural Language Processing.
For sucha complex text processing task, differentfrom previous studies, we argue that it isnecessary to repeatedly read the argumentsand dynamically exploit the efficient featuresuseful for recognizing discourse relations.To mimic the repeated reading strategy, wepropose the neural networks with multi-levelattention (NNMA), combining the attentionmechanism and external memories to gradu-ally fix the attention on some specific wordshelpful to judging the discourse relations.Experiments on the PDTB dataset show thatour proposed method achieves the state-of-art results.
The visualization of the attentionweights also illustrates the progress that ourmodel observes the arguments on each leveland progressively locates the important words.1 IntroductionDiscourse relations (e.g., contrast and causality)support a set of sentences to form a coherenttext.
Automatically recognizing discourse relationscan help many downstream tasks such as questionanswering and automatic summarization.
Despitegreat progress in classifying explicit discourserelations where the discourse connectives (e.g.,?because?, ?but?)
explicitly exist in the text,implicit discourse relation recognition remains achallenge due to the absence of discourse connec-tives.
Previous research mainly focus on exploringvarious kinds of efficient features and machinelearning models to classify the implicit discourserelations (Soricut and Marcu, 2003; Baldridge andLascarides, 2005; Subba and Di Eugenio, 2009;Hernault et al, 2010; Pitler et al, 2009; Joty etal., 2012).
To some extent, these methods simulatethe single-pass reading process that a person quicklyskim the text through one-pass reading and directlycollect important clues for understanding the text.Although single-pass reading plays a crucial rolewhen we just want the general meaning and donot necessarily need to understand every singlepoint of the text, it is not enough for tacklingtasks that need a deep analysis of the text.
Incontrast with single-pass reading, repeated readinginvolves the process where learners repeatedly readthe text in detail with specific learning aims, andhas the potential to improve readers?
reading fluencyand comprehension of the text (National Instituteof Child Health and Human Development, 2000;LaBerge and Samuels, 1974).
Therefore, for the taskof discourse parsing, repeated reading is necessary,as it is difficult to generalize which words are reallyuseful on the first try and efficient features shouldbe dynamically exploited through several passes ofreading .Now, let us check one real example to elaboratethe necessity of using repeated reading in discourseparsing.Arg-1 : the use of 900 toll numbers has beenexpanding rapidly in recent yearsArg-2 : for a while, high-cost pornography linesand services that tempt children to dial (andredial) movie or music information earned theservice a somewhat sleazy image(Comparison - wsj 2100)To identify the ?Comparison?
relation between1224the two arguments Arg-1 and Arg-2, the most crucialclues mainly lie in some content, like ?expandingrapidly?
in Arg-1 and ?earned the service asomewhat sleazy image?
in Arg-2, since there existsa contrast between the semantic meanings of thesetwo text spans.
However, it is difficult to obtainsufficient information for pinpointing these wordsthrough scanning the argument pair left to right inone pass.
In such case, we follow the repeatedreading strategy, where we obtain the generalmeaning through reading the arguments for the firsttime, re-read them later and gradually pay closeattention to the key content.Recently, some approaches simulating repeatedreading have witnessed their success in differenttasks.
These models mostly combine the attentionmechanism that has been originally designed tosolve the alignment problem in machine trans-lation (Bahdanau et al, 2014) and the externalmemory which can be read and written whenprocessing the objects (Sukhbaatar et al, 2015).For example, Kumar et al (2015) drew attention tospecific facts of the input sequence and processedthe sequence via multiple hops to generate ananswer.
In computation vision, Yang et al (2015)pointed out that repeatedly giving attention todifferent regions of an image could gradually leadto more precise image representations.Inspired by these recent work, for discourseparsing, we propose a model that aims to repeatedlyread an argument pair and gradually focus onmore fine-grained parts after grasping the globalinformation.
Specifically, we design the NeuralNetworks with Multi-Level Attention (NNMA)consisting of one general level and several attentionlevels.
In the general level, we capture thegeneral representations of each argument based ontwo bidirectional long short-term memory (LSTM)models.
For each attention level, NNMA generatesa weight vector over the argument pair to locatethe important parts related to the discourse relation.And an external short-term memory is designed tostore the information exploited in previous levelsand help update the argument representations.
Westack this structure in a recurrent manner, mimickingthe process of reading the arguments multiple times.Finally, we use the representation output from thehighest attention level to identify the discourserelation.
Experiments on the PDTB dataset showthat our proposed model achieves the state-of-artresults.2 Repeated Reading Neural Network withMulti-Level AttentionIn this section, we describe how we use the neuralnetworks with multi-level attention to repeatedlyread the argument pairs and recognize implicitdiscourse relations.First, we get the general understanding of thearguments through skimming them.
To implementthis, we adopt the bidirectional Long-Short TermMemory Neural Network (bi-LSTM) to model eachargument, as bi-LSTM is good at modeling over asequence of words and can represent each word withconsideration of more contextual information.
Then,several attention levels are designed to simulate thesubsequent multiple passes of reading.
On eachattention level, an external short-term memory isused to store what has been learned from previouspasses and guide which words should be focused on.To pinpoint the useful parts of the arguments, theattention mechanism is used to predict a probabilitydistribution over each word, indicating to whatdegree each word should be concerned.
The overallarchitecture of our model is shown in Figure 1.
Forclarity, we only illustrate two attention levels in thefigure.
It is noted that we can easily extend ourmodel to more attention levels.2.1 Representing Arguments with LSTMThe Long-Short Term Memory (LSTM) NeuralNetwork is a variant of the Recurrent NeuralNetwork which is usually used for modeling asequence.
In our model, we adopt two LSTM neuralnetworks to respectively model the two arguments:the left argument Arg-1 and the right argument Arg-2.First of all, we associate each word w in ourvocabulary with a vector representation xw ?
RDe .Here we adopt the pre-trained vectors provided byGloVe (Pennington et al, 2014).
Since an argumentcan be viewed as a sequence of word vectors, let x1i(x2i ) be the i-th word vector in argument Arg-1 (Arg-1225Arg-1 Arg-2AttentionAttentionAttention AttentionGeneralLevelAttentionLevel 1AttentionLevel 2MeanPooling12R11R21R22R1M2Msoftmax11a12a21a22aWeightedPooling10h?20R10R11Lh?11h?1ih?10h?11h?1ih?11Lh?20h?2ih?21h?22Lh?22Lh?2ih?21h?20h?DiscourseRelationFigure 1: Neural Network with Multi-LevelAttention.
(Two attention levels are given here.
)2) and the two arguments can be represented as,Arg-1 : [x11,x12, ?
?
?
,x1L1 ]Arg-2 : [x21,x22, ?
?
?
,x2L2 ]where Arg-1 has L1 words and Arg-2 has L2 words.To model the two arguments, we briefly introducethe working process how the LSTM neural networksmodel a sequence of words.
For the i-th time step,the model reads the i-th word xi as the input andupdates the output vector hi as follows (Zarembaand Sutskever, 2014).ii = sigmoid(Wi[xi,hi?1] + bi) (1)fi = sigmoid(Wf [xi,hi?1] + bf ) (2)oi = sigmoid(Wo[xi,hi?1] + bo) (3)c?i = tanh(Wc[xi,hi?1] + bc) (4)ci = ii ?
c?i + fi ?
ci?1 (5)hi = oi ?
tanh(ci) (6)where [ ] means the concatenation operation ofseveral vectors.
i,f ,o and c denote the inputgate, forget gate, output gate and memory cellrespectively in the LSTM architecture.
The inputgate i determines how much the input xi updates thememory cell.
The output gate o controls how muchthe memory cell influences the output.
The forgetgate f controls how the past memory ci?1 affectsthe current state.
Wi,Wf ,Wo,Wc, bi, bf , bo, bcare the network parameters.Referring to the work of Wang and Nyberg(2015), we implement the bidirectional versionof LSTM neural network to model the argumentsequence.
Besides processing the sequence inthe forward direction, the bidirectional LSTM (bi-LSTM) neural network also processes it in thereverse direction.
As shown in Figure 1, using twobi-LSTM neural networks, we can obtain h1i =[~h1i , ~h1i ] for the i-th word in Arg-1 andh2i = [~h2i , ~h2i ]for the i-th word in Arg-2, where ~h1i , ~h2i ?
Rdand ~h1i , ~h2i ?
Rd are the output vectors from twodirections.Next, to get the general-level representations ofthe arguments, we apply a mean pooling operationover the bi-LSTM outputs, and obtain two vectorsR10 and R20, which can reflect the global informationof the argument pair.R10 =1L1L1?i=0h1i (7)R20 =1L2L2?i=0h2i (8)2.2 Tuning Attention via Repeated ReadingAfter obtaining the general-level representationsby treating each word equally, we simulate therepeated reading and design multiple attentionlevels to gradually pinpoint those words particularlyuseful for discourse relation recognition.
In eachattention level, we adopt the attention mechanismto determine which words should be focused on.An external short-term memory is designed toremember what has seen in the prior levels and guidethe attention tuning process in current level.Specifically, in the first attention level, weconcatenate R10, R20 and R10?R20 and apply anon-linear transformation over the concatenation tocatch the general understanding of the argumentpair.
The use of R10?R20 takes a cue from thedifference between two vector representations which1226has been found explainable and meaningful in manyapplications (Mikolov et al, 2013).
Then, we getthe memory vector M1 ?
Rdm of the first attentionlevel asM1 = tanh(Wm,1[R10,R20,R10?R20]) (9)where Wm,1 ?
Rdm?6d is the weight matrix.With M1 recording the general meaning ofthe argument pair, our model re-calculates theimportance of each word.
We assign each word aweight measuring to what degree our model shouldpay attention to it.
The weights are so-called?attention?
in our paper.
This process is designed tosimulate the process that we re-read the argumentsand pay more attention to some specific words withan overall understanding derived from the first-passreading.
Formally, for Arg-1, we use the memoryvector M1 to update the representation of eachword with a non-linear transformation.
Accordingto the updated word representations o11, we get theattention vector a11.h1 = [h10,h11, ?
?
?
,h1L1 ] (10)o11 = tanh(W 1a,1h1 + W 1b,1(M1 ?
e)) (11)a11 = softmax(W 1s,1o11) (12)where h1 ?
R2d?L1 is the concatenation of allLSTM output vectors of Arg-1.
e ?
RL1 is avector of 1s and the M1 ?
e operation denotes thatwe repeat the vector M1 L1 times and generate adm ?
L1 matrix.
The attention vector a11 ?
RL1is obtained through applying a softmax operationover o11.
Wa,11 ?
R2d?2d,Wb,11 ?
R2d?dm andWs,11 ?
R1?2d are the transformation weights.
It isnoted that the subscripts denote the current attentionlevel and the superscripts denote the correspondingargument.
In the same way, we can get the attentionvector a21 for Arg-2.Then, according to a11 and a21, our model re-readsthe arguments and get the new representations R11and R21 for the first attention level.R11 = h1(a11)T (13)R21 = h2(a21)T (14)Next, we iterate the ?memory-attention-representation?
process and design more attentionlevels, giving NNMA the ability to gradually infermore precise attention vectors.
The processingof the second or above attention levels is slightlydifferent from that of the first level, as we updatethe memory vector in a recurrent way.
To formalize,for the k-th attention level (k ?
2), we use thefollowing formulae for Arg-1.Mk = tanh(Wm,k[R1k?1,R2k?1,R1k?1?R2k?1,Mk?1])(15)o1k = tanh(W 1a,kh1 + W 1b,k(Mk ?
e)) (16)a1k = softmax(W 1s,ko1k) (17)R1k = h1(a1k)T (18)In the same way, we can computer o2k,a2k and R2kfor Arg-2.Finally, we use the newest representation derivedfrom the top attention level to recognize thediscourse relations.
Suppose there are totally Kattention levels and n relation types, the predicteddiscourse relation distribution P ?
Rn is calculatedasP = softmax(Wp[R1K ,R2K ,R1K?R2K ] + bp)(19)where Wp ?
Rn?6d and bp ?
Rn are thetransformation weights.2.3 Model TrainingTo train our model, the training objective is definedas the cross-entropy loss between the outputs ofthe softmax layer and the ground-truth class labels.We use stochastic gradient descent (SGD) withmomentum to train the neural networks.To avoid over-fitting, dropout operation is appliedon the top feature vector before the softmax layer.Also, we use different learning rates ?
and ?eto train the neural network parameters ?
and theword embeddings ?e referring to (Ji and Eisenstein,2015).
?e is set to a small value for preventing over-fitting on this task.
In the experimental part, we willintroduce the setting of the hyper-parameters.3 Experiments3.1 PreparationWe evaluate our model on the Penn DiscourseTreebank (PDTB) (Prasad et al, 2008).
In our work,1227we experiment on the four top-level classes in thiscorpus as in previous work (Rutherford and Xue,2015).
We extract all the implicit relations of PDTB,and follow the setup of (Rutherford and Xue, 2015).We split the data into a training set (Sections 2-20), development set (Sections 0-1), and test set(Section 21-22).
Table 1 summarizes the statistics ofthe four PDTB discourse relations, i.e., Comparison,Contingency, Expansion and Temporal.Relation Train Dev TestComparison 1855 189 145Contingency 3235 281 273Expansion 6673 638 538Temporal 582 48 55Total 12345 1156 1011Table 1: Statistics of Implicit Discourse Relations inPDTB.We first convert the tokens in PDTB to lowercase.The word embeddings used for initializing the wordrepresentations are provided by GloVe (Penningtonet al, 2014), and the dimension of the embeddingsDe is 50.
The hyper-parameters, including themomentum ?, the two learning rates ?
and ?e,the dropout rate q, the dimension of LSTM outputvector d, the dimension of memory vector dm are allset according to the performance on the developmentset Due to space limitation, we do not present thedetails of tuning the hyper-parameters and only givetheir final settings as shown in Table 2.?
?
?e q d dm0.9 0.01 0.002 0.1 50 200Table 2: Hyper-parameters for Neural Network withMulti-Level Attention.To evaluate our model, we adopt two kinds ofexperiment settings.
The first one is the four-way classification task, and the second one is thebinary classification task, where we build a one-vs-other classifier for each class.
For the secondsetting, to solve the problem of unbalanced classesin the training data, we follow the reweightingmethod of (Rutherford and Xue, 2015) to reweighthe training instances according to the size of eachrelation class.
We also use visualization methods toanalyze how multi-level attention helps our model.3.2 ResultsFirst, we design experiments to evaluate the effec-tiveness of attention levels and how many attentionlevels are appropriate.
To this end, we implementa baseline model (LSTM with no attention) whichdirectly applies the mean pooling operation overLSTM output vectors of two arguments withoutany attention mechanism.
Then we considerdifferent attention levels including one-level, two-level and three-level.
The detailed results are shownin Table 3.
For four-way classification, macro-averaged F1 and Accuracy are used as evaluationmetrics.
For binary classification, F1 is adopted toevaluate the performance on each class.System Four-way BinaryF1 Acc.
Comp.
Cont.
Expa.
Temp.LSTM 39.40 54.50 33.72 44.79 68.74 33.14NNMA(one-level) 43.48 55.59 34.72 49.47 68.52 36.70NNMA(two-level) 46.29 57.17 36.70 54.48 70.43 38.84NNMA(three-level) 44.95 57.57 39.86 53.69 69.71 37.61Table 3: Performances of NNMA with DifferentAttention Levels.From Table 3, we can see that the basic LSTMmodel performs the worst.
With attention levelsadded, our NNMA model performs much better.This confirms the observation above that one-passreading is not enough for identifying the discourserelations.
With respect to the four-way F1 measure,using NNMA with one-level attention produces a4% improvement over the baseline system withno attention.
Adding the second attention levelgives another 2.8% improvement.
We performsignificance test for these two improvements, andthey are both significant under one-tailed t-test (p <0.05).
However, when adding the third attentionlevel, the performance does not promote much andalmost reaches its plateau.
We can see that three-level NNMA experiences a decease in F1 and aslight increase in Accuracy compared to two-levelNNMA.
The results imply that with more attentionlevels considered, our model may perform slightlybetter, but it may incur the over-fitting problemdue to adding more parameters.
With respect tothe binary classification F1 measures, we can see1228System Four-way BinaryF1 Acc.
Comp.
Cont.
Expa.
Expa.+EntRel Temp.P&C2012 - - 31.32 49.82 - 79.22 26.57J&E2015 - - 35.93 52.78 - 80.02 27.63Zhang2015 38.80 55.39 32.03 47.08 68.96 80.22 20.29R&X2014 38.40 55.50 39.70 54.40 70.20 80.44 28.70R&X2015 40.50 57.10 41.00 53.80 69.40 - 33.30B&D2015 - - 36.36 55.76 61.76 - 27.30Liu2016 44.98 57.27 37.91 55.88 69.97 - 37.17Ji2016 42.30 59.50 - - - - -NNMA(two-level) 46.29 57.17 36.70 54.48 70.43 80.73 38.84NNMA(three-level) 44.95 57.57 39.86 53.69 69.71 80.86 37.61Table 4: Comparison with the State-of-the-art Approaches.that the ?Comparison?
relation needs more passesof reading compared to the other three relations.The reason may be that the identification of the?Comparison?
depends more on some deep analysissuch as semantic parsing, according to (Zhou et al,2010).Next, we compare our models with six state-of-the-art baseline approaches, as shown in Table 4.The six baselines are introduced as follows.?
P&C2012: Park and Cardie (2012) designeda feature-based method and promoted theperformance through optimizing the featureset.?
J&E2015: Ji and Eisenstein (2015) used tworecursive neural networks on the syntacticparse tree to induce the representation of thearguments and the entity spans.?
Zhang2015: Zhang et al (2015) proposedto use shallow convolutional neural networksto model two arguments respectively.
Wereplicated their model since they used adifferent setting in preprocessing PDTB.?
R&X2014, R&X2015: Rutherford and Xue(2014) selected lexical features, productionrules, and Brown cluster pairs, and fed theminto a maximum entropy classifier.
Rutherfordand Xue (2015) further proposed to gather extraweakly labeled data based on the discourseconnectives for the classifier.?
B&D2015: Braud and Denis (2015) combinedseveral hand-crafted lexical features and wordembeddings to train a max-entropy classifier.?
Liu2016: Liu et al (2016) proposed to betterclassify the discourse relations by learningfrom other discourse-related tasks with a multi-task neural network.?
Ji2016: Ji et al (2016) proposed a neurallanguage model over sequences of words andused the discourse relations as latent variablesto connect the adjacent sequences.It is noted that P&C2012 and J&E2015 mergedthe ?EntRel?
relation into the ?Expansion?
rela-tion1.
For a comprehensive comparison, we alsoexperiment our model by adding a Expa.+EntRel vsOther classification.
Our NNMA model with twoattention levels exhibits obvious advantages over thesix baseline methods on the whole.
It is worthnoting that NNMA is even better than the R&X2015approach which employs extra data.As for the performance on each discourserelation, with respect to the F1 measure, we cansee that our NNMA model can achieve the bestresults on the ?Expansion?, ?Expansion+EntRel?and ?Temporal?
relations and competitive results onthe ?Contingency?
relation .
The performance ofrecognizing the ?Comparison?
relation is only worsethan R&X2014 and R&X2015.
As (Rutherford andXue, 2014) stated, the ?Comparison?
relation isclosely related to the constituent parse feature of thetext, like production rules.
How to represent and1EntRel is the entity-based coherence relation which isindependent of implicit and explicit relations in PDTB.However some research merges it into the implicit Expansionrelation.1229exploit these information in our model will be ournext research focus.3.3 Analysis of Attention LevelsThe multiple attention levels in our model greatlyboost the performance of classifying implicit dis-course relations.
In this subsection, we perform bothqualitative and quantitative analysis on the attentionlevels.First, we take a three-level NNMA model forexample and analyze its attention distributions ondifferent attention levels by calculating the meanKullback-Leibler (KL) Divergence between any twolevels on the training set.
In Figure 3, we useklij to denote the KL Divergence between the ithand the jthattention level and use klui to denotethe KL Divergence between the uniform distributionand the ith attention level.
We can see that eachattention level forms different attention distributionsand the difference increases in the higher levels.It can be inferred that the 2nd and 3rd levels inNNMA gradually neglect some words and pay moreattention to some other words in the arguments.
Onepoint worth mentioning is that Arg-2 tends to havemore non-uniform attention weights, since klu2 andklu3 of Arg-2 are much larger than those of Arg-1.
And also, the changes between attention levelsare more obvious for Arg-2 through observing thevalues of kl12, kl13 and kl23.
The reason may bethat Arg-2 contains more information related withdiscourse relation and some words in it tend torequire focused attention, as Arg-2 is syntacticallybound to the implicit connective.At the same time, we visualize the attention levelsof some example argument pairs which are analyzedby the three-level NNMA.
To illustrate the kthattention level, we get its attention weights a1k anda2k which reflect the contribution of each word andthen depict them by a row of color-shaded grids inFigure 2.We can see that the NNMA model focuseson different words on different attention levels.Interestingly, from Figure 2, we find that the 1st and3rd attention levels focus on some similar words,while the 2nd level is relatively different from them.It seems that NNMA tries to find some clues (e.g.
?moscow could be suspended?
in Arg-2a; ?wonthe business?
in Arg-1b; ?with great aplomb heconsiders not only?
in Arg-2c) for recognizing thediscourse relation on the 1st level, looking closelyat other words (e.g.
?misuse of psychiatry againstdissenters?
in Arg-2a; ?a third party that?
in Arg-1b;?and support of hitler?
in Arg-2c) on the 2nd level,and then reconsider the arguments, focus on somespecific words (e.g.
?moscow could be suspended?in Arg-2a; ?has not only hurt?
in Arg-2b) and makethe final decision on the last level.4 Related Work4.1 Implicit Discourse Relation ClassificationThe Penn Discourse Treebank (PDTB) (Prasad etal., 2008), known as the largest discourse corpus, iscomposed of 2159 Wall Street Journal articles.
Eachdocument is annotated with the predicate-argumentstructure, where the predicate is the discourseconnective (e.g.
while) and the arguments are twotext spans around the connective.
The discourseconnective can be either explicit or implicit.
InPDTB, a hierarchy of relation tags is provided forannotation.
In our study, we use the four top-leveltags, including Temporal, Contingency, Comparisonand Expansion.
These four core relations allow usto be theory-neutral, since they are almost includedin all discourse theories, sometimes with differentnames (Wang et al, 2012).Implicit discourse relation recognition is oftentreated as a classification problem.
The first work totackle this task on PDTB is (Pitler et al, 2009).
Theyselected several surface features to train four binaryclassifiers, each for one of the top-level PDTBrelation classes.
Extending from this work, Lin etal.
(2009) further identified four different featuretypes representing the context, the constituent parsetrees, the dependency parse trees and the raw textrespectively.
Rutherford and Xue (2014) used browncluster to replace the word pair features for solvingthe sparsity problem.
Ji and Eisenstein (2015)adopted two recursive neural networks to exploitthe representation of arguments and entity spans.Very recently, Liu et al (2016) proposed a two-dimensional convolutional neural network (CNN) tomodel the argument pairs and employed a multi-task learning framework to boost the performanceby learning from other discourse-related tasks.
Jiet al (2016) considered discourse relations as1230Arg-1athe world psychiatric association votedat an athens parley toconditionally readmit the soviet union1 2 3Attention Level  Arg-2amoscow could besuspended ifthe misuse ofpsychiatry against dissenters isdiscovered during areview within ayear1 2 3Attention Level(a) Example with Comparison relationArg-1bbut ibm would have won thebusiness anyway asasale toathird party that would have thenleased the equipment tothe customer1 2 3Attention Level  Arg-2bibm has not only hurtits short-term revenue outlook buthas also been losing money onits leases1 2 3Attention Level(b) Example with Contingency relationArg-1cnowshiftinghisscenefromthecountryheleftatfivetotheenglandhehaslivdinfornearly30years,hehasfashioned anovelinthemodeofhenryjamesande.
m.forster1 2 3Attention Level  Arg-2cwithgreataplombheconsiders notonlyfilialdevotionand(utterlyrepressed )sexuallove,butbritishanti-semitism ,thegentry'simpatience withdemocracy andsupportofhitler,andthemoralproblematics ofloyalty1 2 3Attention Level(c) Example with Expansion relationFigure 2: Visualization Examples: Illustrating Attentions Learned by NNMA.
(The blue grid means thethe attention on this word is lower than the value of a uniform distribution and the red red grid means theattention is higher than that.)Arg-1Arg-2kl_120.007490.04177kl_230.0997840.502146kl_130.0853940.35212kl_u10.1234130.136228kl_u20.1566190.254844kl_u30.1623790.46797600.10.20.30.40.50.6Arg-1Arg-2?l?????????????????????
?Figure 3: KL-divergences between attention levelslatent variables connecting two token sequences andtrained a discourse informed language model.4.2 Neural Networks and AttentionMechanismRecently, neural network-based methods havegained prominence in the field of natural languageprocessing (Kim, 2014).
Such methods are primar-ily based on learning a distributed representationfor each word, which is also called a wordembedding (Collobert et al, 2011).Attention mechanism was first introduced intoneural models to solve the alignment problembetween different modalities.
Graves (2013)designed a neural network to generate handwritingbased on a text.
It assigned a window on the inputtext at each step and generate characters based on thecontent within the window.
Bahdanau et al (2014)introduced this idea into machine translation, wheretheir model computed a probabilistic distributionover the input sequence when generating each targetword.
Tan et al (2015) proposed an attention-based neural network to model both questions andsentences for selecting the appropriate non-factoidanswers.In parallel, the idea of equipping the neural modelwith an external memory has gained increasingattention recently.
A memory can remember whatthe model has learned and guide its subsequentactions.
Weston et al (2015) presented a neuralnetwork to read and update the external memory ina recurrent manner with the guidance of a questionembedding.
Kumar et al (2015) proposed a similarmodel where a memory was designed to change thegate of the gated recurrent unit for each iteration.5 ConclusionAs a complex text processing task, implicit dis-course relation recognition needs a deep analysis1231of the arguments.
To this end, we for the firsttime propose to imitate the repeated reading strategyand dynamically exploit efficient features throughseveral passes of reading.
Following this idea,we design neural networks with multiple levels ofattention (NNMA), where the general level and theattention levels represent the first and subsequentpasses of reading.
With the help of externalshort-term memories, NNMA can gradually updatethe arguments representations on each attentionlevel and fix attention on some specific wordswhich provide effective clues to discourse relationrecognition.
We conducted experiments on PDTBand the evaluation results show that our modelcan achieve the state-of-the-art performance onrecognizing the implicit discourse relations.AcknowledgmentsWe thank all the anonymous reviewers for theirinsightful comments on this paper.
This work waspartially supported by National Key Basic ResearchProgram of China (2014CB340504), and NationalNatural Science Foundation of China (61273278 and61572049).
The correspondence author of this paperis Sujian Li.ReferencesDzmitry Bahdanau, Kyunghyun Cho, and YoshuaBengio.
2014.
Neural machine translation byjointly learning to align and translate.
arXiv preprintarXiv:1409.0473.Jason Baldridge and Alex Lascarides.
2005.
Proba-bilistic head-driven parsing for discourse structure.
InProceedings of CoNLL.Chloe?
Braud and Pascal Denis.
2015.
Comparingword representations for implicit discourse relationclassification.
In Proceedings of EMNLP.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Alex Graves.
2013.
Generating sequences with recurrentneural networks.
arXiv preprint arXiv:1308.0850.Hugo Hernault, Helmut Prendinger, David A duVerle,Mitsuru Ishizuka, et al 2010.
Hilda: a discourseparser using support vector machine classification.Dialogue and Discourse, 1(3):1?33.Yangfeng Ji and Jacob Eisenstein.
2015.
Onevector is not enough: Entity-augmented distributedsemantics for discourse relations.
Transactions ofthe Association for Computational Linguistics, 3:329?344.Yangfeng Ji, Gholamreza Haffari, and Jacob Eisenstein.2016.
A latent variable recurrent neural network fordiscourse relation language models.
arXiv preprintarXiv:1603.01913.Shafiq Joty, Giuseppe Carenini, and Raymond T Ng.2012.
A novel discriminative framework for sentence-level discourse analysis.
In Proceedings of EMNLP.Yoon Kim.
2014.
Convolutional neural networks forsentence classification.
In Proceedings of EMNLP.Ankit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury,Robert English, Brian Pierce, Peter Ondruska, IshaanGulrajani, and Richard Socher.
2015.
Ask meanything: Dynamic memory networks for natural lan-guage processing.
arXiv preprint arXiv:1506.07285.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing implicit discourse relations in the penndiscourse treebank.
In Proceedings of EMNLP.Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui.2016.
Implicit discourse relation classification viamulti-task neural network.
In Proceedings of AAAI.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous space wordrepresentations.
In Proceedings of NAACL.Joonsuk Park and Claire Cardie.
2012.
Improvingimplicit discourse relation recognition through featureset optimization.
In Proceedings of SigDial.Jeffrey Pennington, Richard Socher, and Christopher D.Manning.
2014.
Glove: Global vectors for wordrepresentation.
In Proceedings of EMNLP 2014.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.Automatic sense prediction for implicit discourserelations in text.
In Proceedings of ACL.Rashmi Prasad, Nikhil Dinesh, Alan Lee, EleniMiltsakaki, Livio Robaldo, Aravind K. Joshi, andBonnie L. Webber.
2008.
The Penn DiscourseTreeBank 2.0.
In Proceedings of LREC.Attapol Rutherford and Nianwen Xue.
2014.
Dis-covering implicit discourse relations through browncluster pair representation and coreference patterns.
InProceedings of EACL.Attapol T Rutherford and Nianwen Xue.
2015.Improving the inference of implicit discourse relationsvia classifying explicit discourse connectives.
InProceedings of NAACL.Radu Soricut and Daniel Marcu.
2003.
Sentencelevel discourse parsing using syntactic and lexicalinformation.
In Proceedings of NAACL.1232Rajen Subba and Barbara Di Eugenio.
2009.
Aneffective discourse parser that uses rich linguisticinformation.
In Proceedings of NAACL.Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al2015.
End-to-end memory networks.
In Proceedingsof NIPS.Ming Tan, Bing Xiang, and Bowen Zhou.
2015.
Lstm-based deep learning models for non-factoid answerselection.
arXiv preprint arXiv:1511.04108.Di Wang and Eric Nyberg.
2015.
A long short-term memory model for answer sentence selection inquestion answering.
In Proceedings of EMNLP.Xun Wang, Sujian Li, Jiwei Li, and Wenjie Li.
2012.Implicit Discourse Relation Recognition by SelectingTypical Training Examples.
In Proceedings ofCOLING.Jason Weston, Sumit Chopra, and Antoine Bordes.
2015.Memory networks.
Proceedings of ICLR.Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,and Alexander J. Smola.
2015.
Stacked attentionnetworks for image question answering.
arXivpreprint arXiv:1511.02274.Wojciech Zaremba and Ilya Sutskever.
2014.
Learningto execute.
arXiv preprint arXiv:1410.4615.Biao Zhang, Jinsong Su, Deyi Xiong, Yaojie Lu,Hong Duan, and Junfeng Yao.
2015.
Shallowconvolutional neural network for implicit discourserelation recognition.
In Proceedings of EMNLP.Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, JianSu, and Chew Lim Tan.
2010.
Predicting discourseconnectives for implicit discourse relation recognition.In Proceedings of the ICCL, pages 1507?1514.1233
