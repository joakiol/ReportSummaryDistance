Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 43?51,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsOpen-domain Commonsense Reasoning Using Discourse Relations from aCorpus of Weblog StoriesMatt GerberDepartment of Computer ScienceMichigan State Universitygerberm2@msu.eduAndrew S. Gordon and Kenji SagaeInstitute for Creative TechnologiesUniversity of Southern California{gordon,sagae}@ict.usc.eduAbstractWe present a method of extracting open-domain commonsense knowledge by apply-ing discourse parsing to a large corpus of per-sonal stories written by Internet authors.
Wedemonstrate the use of a linear-time, joint syn-tax/discourse dependency parser for this pur-pose, and we show how the extracted dis-course relations can be used to generate open-domain textual inferences.
Our evaluationsof the discourse parser and inference modelsshow some success, but also identify a num-ber of interesting directions for future work.1 IntroductionThe acquisition of open-domain knowledge in sup-port of commonsense reasoning has long been abottleneck within artificial intelligence.
Such rea-soning supports fundamental tasks such as textualentailment (Giampiccolo et al, 2008), automatedquestion answering (Clark et al, 2008), and narra-tive comprehension (Graesser et al, 1994).
Thesetasks, when conducted in open domains, require vastamounts of commonsense knowledge pertaining tostates, events, and their causal and temporal relation-ships.
Manually created resources such as FrameNet(Baker et al, 1998), WordNet (Fellbaum, 1998), andCyc (Lenat, 1995) encode many aspects of com-monsense knowledge; however, coverage of causaland temporal relationships remains low for many do-mains.Gordon and Swanson (2008) argued that thecommonsense tasks of prediction, explanation, andimagination (collectively called envisionment) canbe supported by knowledge mined from a large cor-pus of personal stories written by Internet weblogauthors.1 Gordon and Swanson (2008) identifiedthree primary obstacles to such an approach.
First,stories must be distinguished from other weblogcontent (e.g., lists, recipes, and reviews).
Second,stories must be analyzed in order to extract the im-plicit commonsense knowledge that they contain.Third, inference mechanisms must be developed thatuse the extracted knowledge to perform the core en-visionment tasks listed above.In the current paper, we present an approach toopen-domain commonsense inference that addresseseach of the three obstacles identified by Gordon andSwanson (2008).
We built on the work of Gordonand Swanson (2009), who describe a classification-based approach to the task of story identification.The authors?
system produced a corpus of approx-imately one million personal stories, which we usedas a starting point.
We applied efficient discourseparsing techniques to this corpus as a means of ex-tracting causal and temporal relationships.
Further-more, we developed methods that use the extractedknowledge to generate textual inferences for de-scriptions of states and events.
This work resultedin an end-to-end prototype system capable of gen-erating open-domain, commonsense inferences us-ing a repository of knowledge extracted from un-structured weblog text.
We focused on identifying1We follow Gordon and Swanson (2009) in defining a storyto be a ?textual discourse that describes a specific series ofcausally related events in the past, spanning a period of timeof minutes, hours, or days, where the author or a close associateis among the participants.
?43strengths and weaknesses of the system in an effortto guide future work.We structure our presentation as follows: in Sec-tion 2, we present previous research that has inves-tigated the use of large web corpora for natural lan-guage processing (NLP) tasks.
In Section 3, we de-scribe an efficient method of automatically parsingweblog stories for discourse structure.
In Section 4,we present a set of inference mechanisms that usethe extracted discourse relations to generate open-domain textual inferences.
We conclude, in Section5, with insights into story-based envisionment thatwe hope will guide future work in this area.2 Related workResearchers have made many attempts to use themassive amount of linguistic content created byusers of the World Wide Web.
Progress and chal-lenges in this area have spawned multiple workshops(e.g., those described by Gurevych and Zesch (2009)and Evert et al (2008)) that specifically target theuse of content that is collaboratively created by In-ternet users.
Of particular relevance to the presentwork is the weblog corpus developed by Burton etal.
(2009), which was used for the data challengeportion of the International Conference on Weblogsand Social Media (ICWSM).
The ICWSM weblogcorpus (referred to here as Spinn3r) is freely avail-able and comprises tens of millions of weblog en-tries posted between August 1st, 2008 and October1st, 2008.Gordon et al (2009) describe an approach toknowledge extraction over the Spinn3r corpus usingtechniques described by Schubert and Tong (2003).In this approach, logical propositions (known as fac-toids) are constructed via approximate interpreta-tion of syntactic analyses.
As an example, the sys-tem identified a factoid glossed as ?doors to a roommay be opened?.
Gordon et al (2009) found thatthe extracted factoids cover roughly half of the fac-toids present in the corresponding Wikipedia2 arti-cles.
We used a subset of the Spinn3r corpus inour work, but focused on discourse analyses of en-tire texts instead of syntactic analyses of single sen-tences.
Our goal was to extract general causal andtemporal propositions instead of the fine-grained2http://en.wikipedia.orgproperties expressed by many factoids extracted byGordon et al (2009).Clark and Harrison (2009) pursued large-scaleextraction of knowledge from text using a syntax-based approach that was also inspired by the workof Schubert and Tong (2003).
The authors showedhow the extracted knowledge tuples can be usedto improve syntactic parsing and textual entailmentrecognition.
Bar-Haim et al (2009) present an ef-ficient method of performing inference with suchknowledge.Our work is also related to the work of Persingand Ng (2009), in which the authors developed asemi-supervised method of identifying the causes ofevents described in aviation safety reports.
Simi-larly, our system extracts causal (as well as tem-poral) knowledge; however, it does this in an opendomain and does not place limitations on the typesof causes to be identified.
This greatly increasesthe complexity of the inference task, and our resultsexhibit a corresponding degradation; however, ourevaluations provide important insights into the task.3 Discourse parsing a corpus of storiesGordon and Swanson (2009) developed a super-vised classification-based approach for identifyingpersonal stories within the Spinn3r corpus.
Theirmethod achieved 75% precision on the binary taskof predicting story versus non-story on a held-outsubset of the Spinn3r corpus.
The extracted ?storycorpus?
comprises 960,098 personal stories writtenby weblog users.
Due to its large size and broaddomain coverage, the story corpus offers unique op-portunities to NLP researchers.
For example, Swan-son and Gordon (2008) showed how the corpus canbe used to support open-domain collaborative storywriting.3As described by Gordon and Swanson (2008),story identification is just the first step towards com-monsense reasoning using personal stories.
We ad-dressed the second step - knowledge extraction -by parsing the corpus using a Rhetorical StructureTheory (Carlson and Marcu, 2001) parser based onthe one described by Sagae (2009).
The parserperforms joint syntactic and discourse dependency3The system (called SayAnything) is available athttp://sayanything.ict.usc.edu44parsing using a stack-based, shift-reduce algorithmwith runtime that is linear in the input length.
Thislightweight approach is very efficient; however, itmay not be quite as accurate as more complex, chart-based approaches (e.g., the approach of Charniakand Johnson (2005) for syntactic parsing).We trained the discourse parser over the causaland temporal relations contained in the RST corpus.Examples of these relations are shown below:(1) [cause Packages often get buried in the load][result and are delivered late.
](2) [before Three months after she arrived in L.A.][after she spent $120 she didn?t have.
]The RST corpus defines many fine-grained rela-tions that capture causal and temporal properties.For example, the corpus differentiates between re-sult and reason for causation and temporal-after andtemporal-before for temporal order.
In order to in-crease the amount of available training data, we col-lapsed all causal and temporal relations into twogeneral relations causes and precedes.
This step re-quired normalization of asymmetric relations suchas temporal-before and temporal-after.To evaluate the discourse parser described above,we manually annotated 100 randomly selected we-blog stories from the story corpus produced by Gor-don and Swanson (2009).
For increased efficiency,we limited our annotation to the generalized causesand precedes relations described above.
We at-tempted to keep our definitions of these relationsin line with those used by RST.
Following previousdiscourse annotation efforts, we annotated relationsover clause-level discourse units, permitting rela-tions between adjacent sentences.
In total, we an-notated 770 instances of causes and 1,009 instancesof precedes.We experimented with two versions of the RSTparser, one trained on the fine-grained RST rela-tions and the other trained on the collapsed relations.At testing time, we automatically mapped the fine-grained relations to their corresponding causes orprecedes relation.
We computed the following ac-curacy statistics:Discourse segmentation accuracy For each pre-dicted discourse unit, we located the referencediscourse unit with the highest overlap.
Accu-racy for the predicted discourse unit is equal tothe percentage word overlap between the refer-ence and predicted discourse units.Argument identification accuracy For each dis-course unit of a predicted discourse relation,we located the reference discourse unit with thehighest overlap.
Accuracy is equal to the per-centage of times that a reference discourse rela-tion (of any type) holds between the referencediscourse units that overlap most with the pre-dicted discourse units.Argument classification accuracy For the subsetof instances in which a reference discourse re-lation holds between the units that overlap mostwith the predicted discourse units, accuracy isequal to the percentage of times that the pre-dicted discourse relation matches the referencediscourse relation.Complete accuracy For each predicted discourserelation, accuracy is equal to the percentageword overlap with a reference discourse rela-tion of the same type.Table 1 shows the accuracy results for the fine-grained and collapsed versions of the RST discourseparser.
As shown in Table 1, the collapsed versionof the discourse parser exhibits higher overall ac-curacy.
Both parsers predicted the causes relationmuch more often than the precedes relation, so theoverall scores are biased toward the scores for thecauses relation.
For comparison, Sagae (2009) eval-uated a similar RST parser over the test section ofthe RST corpus, obtaining precision of 42.9% andrecall of 46.2% (F1 = 44.5%).In addition to the automatic evaluation describedabove, we also manually assessed the output of thediscourse parsers.
One of the authors judged thecorrectness of each extracted discourse relation, andwe found that the fine-grained and collapsed ver-sions of the parser performed equally well with aprecision near 33%; however, throughout our exper-iments, we observed more desirable discourse seg-mentation when working with the collapsed versionof the discourse parser.
This fact, combined with theresults of the automatic evaluation presented above,45Fine-grained RST parser Collapsed RST parserAccuracy metric causes precedes overall causes precedes overallSegmentation 36.08 44.20 36.67 44.36 30.13 43.10Argument identification 25.00 33.33 25.86 26.15 23.08 25.87Argument classification 66.15 50.00 64.00 79.41 83.33 79.23Complete 22.20 28.88 22.68 31.26 21.21 30.37Table 1: RST parser evaluation.
All values are percentages.led us to use the collapsed version of the parser inall subsequent experiments.Having developed and evaluated the discourseparser, we conducted a full discourse parse of thestory corpus, which comprises more than 25 millionsentences split into nearly 1 million weblog entries.The discourse parser extracted 2.2 million instancesof the causes relation and 220,000 instances of theprecedes relation.
As a final step, we indexed theextracted discourse relations with the Lucene infor-mation retrieval engine.4 Each discourse unit (twoper discourse relation) is treated as a single docu-ment, allowing us to query the extracted relationsusing information retrieval techniques implementedin the Lucene toolkit.4 Generating textual inferencesAs mentioned previously, Gordon and Swan-son (2008) cite three obstacles to performing com-monsense reasoning using weblog stories.
Gordonand Swanson (2009) addressed the first (story col-lection).
We addressed the second (story analysis)by developing a discourse parser capable of extract-ing causal and temporal relations from weblog text(Section 3).
In this section, we present a prelimi-nary solution to the third problem - reasoning withthe extracted knowledge.4.1 Inference methodIn general, we require an inference method that takesas input the following things:1.
A description of the state or event of interest.This is a free-text description of any length.2.
The type of inference to perform, either causalor temporal.4Available at http://lucene.apache.org3.
The inference direction, either forward or back-ward.
Forward causal inference produces theeffects of the given state or event.
Backwardcausal inference produces causes of the givenstate or event.
Similarly, forward and back-ward temporal inferences produce subsequentand preceding states and events, respectively.As a simple baseline approach, we implemented thefollowing procedure.
First, given a textual input de-scription d, we query the extracted discourse unitsusing Lucene?s modified version of the vector spacemodel over TF-IDF term weights.
This produces aranked list Rd of discourse units matching the inputdescription d. We then filterRd, removing discourseunits that are not linked to other discourse units bythe given relation and in the given direction.
Each el-ement of the filtered Rd is thus linked to a discourseunit that could potentially satisfy the inference re-quest.To demonstrate, we perform forward causal infer-ence using the following input description d:(3) John traveled the world.Below, we list the three top-ranked discourse unitsthat matched d (left-hand side) and their associatedconsequents (right-hand side):1. traveling the world?
to murder2.
traveling from around the world to be there ?even though this crowd was international3.
traveled across the world?
to experience itIn a na?
?ve way, one might simply choose the top-ranked clause in Rd and select its associated clauseas the answer to the inference request; however, inthe example above, this would incorrectly generate?to murder?
as the effect of John?s traveling (this is46more appropriately viewed as the purpose of trav-eling).
The other effect clauses also appear to beincorrect.
This should not come as much of a sur-prise because the ranking was generated soley fromthe match score between the input description andthe causes in Rd, which are quite relevant.One potential problem with the na?
?ve selectionmethod is that it ignores information contained inthe ranked list R?d of clauses that are associated withthe clauses in Rd.
In our experiments, we oftenobserved redundancies in R?d that captured generalproperties of the desired inference.
Intuitively, con-tent that is shared across elements ofR?d could repre-sent the core meaning of the desired inference result.In what follows, we describe various re-rankingsof R?d using this shared content.
For each modeldescribed, the final inference prediction is the top-ranked element of R?d.Centroid similarity To approximate the sharedcontent of discourse units in R?d, we treat eachdiscourse unit as a vector of TF scores.
We thencompute the average vector and re-rank all dis-course units in R?d based on their cosine simi-larity with the average vector.
This favors infer-ence results that ?agree?
with many alternativehypotheses.Description score scaling In this approach, we in-corporate the score from Rd into the centroidsimilarity score, multiplying the two and givingequal weight to each.
This captures the intu-ition that the top-ranked element of R?d shouldrepresent the general content of the list butshould also be linked to an element of Rd thatbears high similarity to the given state or eventdescription d.Log-length scaling When working with the cen-troid similarity score, we often observed top-ranked elements of R?d that were only a fewwords in length.
This was typically the casewhen components from sparse TF vectors inR?d matched well with components from thecentroid vector.
Ideally, we would like morelengthy (but not too long) descriptions.
Toachieve this, we multiplied the centroid simi-larity score by the logarithm of the word lengthof the discourse unit in R?d.Description score/log-length scaling In this ap-proach, we combine the description score scal-ing and log-length scaling, multiplying the cen-troid similarity by both and giving equal weightto all three factors.4.2 Evaluating the generated textual inferencesTo evaluate the inference re-ranking models de-scribed above, we automatically generated for-ward/backward causal and temporal inferences forfive documents (265 sentences) drawn randomlyfrom the story corpus.
For simplicity, we gener-ated an inference for each sentence in each docu-ment.
Each inference re-ranking model is able togenerate four textual inferences (forward/backwardcausal/temporal) for each sentence.
In our experi-ments, we only kept the highest-scoring of the fourinferences generated by a model.
One of the authorsthen manually evaluated the final predictions for cor-rectness.
This was a subjective process, but it wasguided by the following requirements:1.
The generated inference must increase the lo-cal coherence of the document.
As describedby Graesser et al (1994), readers are typicallyrequired to make inferences about the text thatlead to a coherent understanding thereof.
Werequired the generated inferences to aid in thistask.2.
The generated inferences must be globallyvalid.
To demonstrate global validity, considerthe following actual output:(4) I didn?t even need a jacket (until I gotthere).In Example 4, the system-generated forwardtemporal inference is shown in parentheses.The inference makes sense given its local con-text; however, it is clear from the surround-ing discourse (not shown) that a jacket was notneeded at any point in time (it happened to bea warm day).
As a result, this prediction wastagged as incorrect.Table 2 presents the results of the evaluation.
Asshown in the table, the top-performing models arethose that combine centroid similarity with one orboth of the other re-ranking heuristics.47Re-ranking model Inference accuracy (%)None 10.19Centroid similarity 12.83Description score scaling 17.36Log-length scaling 12.83Description score/log-length scaling 16.60Table 2: Inference generation evaluation results.00.050.10.150.20.250.30.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1Confidence-ordered percentage of allinferencesInferenceaccuracy NoneCentroid similarityDescription score scalingLog-length scalingCombined scalingFigure 1: Inference rate versus accuracy.
Values along the x-axis indicate that the top-scoring x% of all inferenceswere evaluated.
Values along the y-axis indicate the prediction accuracy.The analysis above demonstrates the relative per-formance of the models when making inferences forall sentences; however it is probably the case thatmany generated inferences should be rejected due totheir low score.
Because the output scores of a singlemodel can be meaningfully compared across predic-tions, it is possible to impose a threshold on the in-ference generation process such that any predictionscoring at or below the threshold is withheld.
Wevaried the prediction threshold from zero to a valuesufficiently large that it excluded all predictions fora model.
Doing so demonstrates the trade-off be-tween making a large number of textual inferencesand making accurate textual inferences.
Figure 1shows the effects of this variable on the re-rankingmodels.
As shown in Figure 1, the highest infer-ence accuracy is reached by the re-ranker that com-bines description score and log-length scaling withthe centroid similarity measure.
This accuracy is at-tained by keeping the top 25% most confident infer-ences.5 ConclusionsWe have presented an approach to commonsensereasoning that relies on (1) the availability of a largecorpus of personal weblog stories and (2) the abil-ity to analyze and perform inference with these sto-ries.
Our current results, although preliminary, sug-gest novel and important areas of future exploration.We group our observations according to the last twoproblems identified by Gordon and Swanson (2008):story analysis and envisioning with the analysis re-sults.5.1 Story analysisAs in other NLP tasks, we observed significant per-formance degradation when moving from the train-ing genre (newswire) to the testing genre (Internet48weblog stories).
Because our discourse parser reliesheavily on lexical and syntactic features for classi-fication, and because the distribution of the featurevalues varies widely between the two genres, theperformance degradation is to be expected.
Recenttechniques in parser adaptation for the Brown corpus(McClosky et al, 2006) might be usefully applied tothe weblog genre as well.Our supervised classification-based approach todiscourse parsing could also be improved with ad-ditional training data.
Causal and temporal relationsare instantiated a combined 2,840 times in the RSTcorpus, with a large majority of these being causal.In contrast, the Penn Discourse TreeBank (Prasad etal., 2008) contains 7,448 training instances of causalrelations and 2,763 training instances of temporalrelations.
This represents a significant increase inthe amount of training data over the RST corpus.
Itwould be informative to compare our current resultswith those obtained using a discourse parser trainedon the Penn Discourse TreeBank.One might also extract causal and temporal rela-tions using traditional semantic role analysis basedon FrameNet (Baker et al, 1998) or PropBank(Kingsbury and Palmer, 2003).
The former defines anumber of frames related to causation and temporalorder, and roles within the latter could be mapped tostandard thematic roles (e.g., cause) via SemLink.55.2 Envisioning with the analysis resultsWe believe commonsense reasoning based on we-blog stories can also be improved through more so-phisticated uses of the extracted discourse relations.As a first step, it would be beneficial to explore alter-nate input descriptions.
As presented in Section 4.2,we make textual inferences at the sentence level forsimplicity; however, it might be more reasonable tomake inferences at the clause level, since clauses arethe basis for RST and Penn Discourse TreeBank an-notation.
This could result in the generation of sig-nificantly more inferences due to multi-clause sen-tences; thus, more intelligent inference filtering willbe required.Our models use prediction scores for the tasksof rejecting inferences and selecting between mul-tiple candidate inferences (i.e., forward/backward5Available at http://verbs.colorado.edu/semlinkcausal/temporal).
Instead of relying on predictionscores for these tasks, it might be advantageous tofirst identify whether or not envisionment should beperformed for a clause, and, if it should, what typeand direction of envisionment would be best.
Forexample, consider the following sentence:(5) [clause1 John went to the store] [clause2because he was hungry].It would be better - from a local coherence perspec-tive - to infer the cause of the second clause insteadof the cause of the first.
This is due to the fact that acause for the first clause is explicitly stated, whereasa cause for the second clause is not.
Inferences madeabout the first clause (e.g., that John went to the storebecause his dog was hungry), are likely to be unin-formative or in conflict with explicitly stated infor-mation.Example 5 raises the important issue of context,which we believe needs to be investigated further.Here, context refers to the discourse that surroundsthe clause or sentence for which the system is at-tempting to generate a textual inference.
The con-text places a number of constraints on allowable in-ferences.
For example, in addition to content-basedconstraints demonstrated in Example 5, the contextlimits pronoun usage, entity references, and tense.Violations of these constraints will reduce local co-herence.Finally, the story corpus, with its vast size, islikely to contain a significant amount of redundancyfor common events and states.
Our centroid-basedre-ranking heuristics are inspired by this redun-dancy, and we expect that aggregation techniquessuch as clustering might be of some use when ap-plied to the corpus as a whole.
Having identifiedcoherent clusters of causes, it might be easier to finda consequence for a previously unseen cause.In summary, we have presented preliminary re-search into the task of using a large, collaborativelyconstructed corpus as a commonsense knowledgerepository.
Rather than relying on hand-coded on-tologies and event schemas, our approach relies onthe implicit knowledge contained in written natu-ral language.
We have demonstrated the feasibilityof obtaining the discourse structure of such a cor-pus via linear-time parsing models.
Furthermore,49we have introduced inference procedures that are ca-pable of generating open-domain textual inferencesfrom the extracted knowledge.
Our evaluation re-sults suggest many opportunities for future work inthis area.AcknowledgmentsThe authors would like to thank the anonymousreviewers for their helpful comments and sugges-tions.
The project or effort described here hasbeen sponsored by the U.S. Army Research, Devel-opment, and Engineering Command (RDECOM).Statements and opinions expressed do not necessar-ily reflect the position or the policy of the UnitedStates Government, and no official endorsementshould be inferred.ReferencesCollin Baker, Charles Fillmore, and John Lowe.
1998.The Berkeley FrameNet project.
In Christian Boitetand PeteWhitelock, editors, Proceedings of the Thirty-Sixth Annual Meeting of the Association for Computa-tional Linguistics and Seventeenth International Con-ference on Computational Linguistics, pages 86?90,San Francisco, California.
MorganKaufmann Publish-ers.Roy Bar-Haim, Jonathan Berant, and Ido Dagan.
2009.A compact forest for scalable inference over entail-ment and paraphrase rules.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 1056?1065, Singapore, Au-gust.
Association for Computational Linguistics.K.
Burton, A. Java, and I. Soboroff.
2009.
The icwsm2009 spinn3r dataset.
In Proceedings of the Third An-nual Conference on Weblogs and Social Media.Lynn Carlson and Daniel Marcu.
2001.
Discourse tag-ging manual.
Technical Report ISI-TR-545, ISI, July.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics.Peter Clark and Phil Harrison.
2009.
Large-scale extrac-tion and use of knowledge from text.
In K-CAP ?09:Proceedings of the fifth international conference onKnowledge capture, pages 153?160, New York, NY,USA.
ACM.Peter Clark, Christiane Fellbaum, Jerry R. Hobbs, PhilHarrison, William R. Murray, and John Thompson.2008.
Augmenting WordNet for Deep Understandingof Text.
In Johan Bos and Rodolfo Delmonte, editors,Semantics in Text Processing.
STEP 2008 ConferenceProceedings, volume 1 of Research in ComputationalSemantics, pages 45?57.
College Publications.Stefan Evert, Adam Kilgarriff, and Serge Sharoff, edi-tors.
2008.
4th Web as Corpus Workshop Can we beatGoogle?Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database (Language, Speech, and Communi-cation).
The MIT Press, May.Danilo Giampiccolo, Hoa Trang Dang, BernardoMagnini, Ido Dagan, and Bill Dolan.
2008.
Thefourth fascal recognizing textual entailment challenge.In Proceedings of the First Text Analysis Conference.Andrew Gordon and Reid Swanson.
2008.
Envision-ing with weblogs.
In International Conference on NewMedia Technology.Andrew Gordon and Reid Swanson.
2009.
Identifyingpersonal stories in millions of weblog entries.
In ThirdInternational Conference on Weblogs and Social Me-dia.Jonathan Gordon, Benjamin Van Durme, and LenhartSchubert.
2009.
Weblogs as a source for extractinggeneral world knowledge.
In K-CAP ?09: Proceed-ings of the fifth international conference on Knowledgecapture, pages 185?186, New York, NY, USA.
ACM.A.
C. Graesser, M. Singer, and T. Trabasso.
1994.
Con-structing inferences during narrative text comprehen-sion.
Psychological Review, 101:371?395.Iryna Gurevych and Torsten Zesch, editors.
2009.
ThePeoples Web Meets NLP: Collaboratively ConstructedSemantic Resources.Paul Kingsbury and Martha Palmer.
2003.
Propbank: thenext level of treebank.
In Proceedings of Treebanksand Lexical Theories.Douglas B. Lenat.
1995.
Cyc: a large-scale investmentin knowledge infrastructure.
Communications of theACM, 38(11):33?38.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Reranking and self-training for parser adapta-tion.
In ACL-44: Proceedings of the 21st Interna-tional Conference on Computational Linguistics andthe 44th annual meeting of the Association for Compu-tational Linguistics, pages 337?344, Morristown, NJ,USA.
Association for Computational Linguistics.Isaac Persing and Vincent Ng.
2009.
Semi-supervisedcause identification from aviation safety reports.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 843?851, Suntec, Singapore, Au-gust.
Association for Computational Linguistics.Rashmi Prasad, Alan Lee, Nikhil Dinesh, Eleni Milt-sakaki, Geraud Campion, Aravind Joshi, and Bonnie50Webber.
2008.
Penn discourse treebank version 2.0.Linguistic Data Consortium, February.Kenji Sagae.
2009.
Analysis of discourse structure withsyntactic dependencies and data-driven shift-reduceparsing.
In Proceedings of the 11th International Con-ference on Parsing Technologies (IWPT?09), pages81?84, Paris, France, October.
Association for Com-putational Linguistics.Lenhart Schubert and Matthew Tong.
2003.
Extract-ing and evaluating general world knowledge from thebrown corpus.
In Proceedings of the HLT-NAACL2003 workshop on Text meaning, pages 7?13, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Reid Swanson and AndrewGordon.
2008.
Say anything:A massively collaborative open domain story writingcompanion.
In First International Conference on In-teractive Digital Storytelling.51
