Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 306?311,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsApplying Machine Translation Metrics to Student-Written TranslationsLisa N. MichaudComputer Science DepartmentMerrimack CollegeNorth Andover, MA, USAmichaudl@merrimack.eduPatricia Ann McCoyLanguage DepartmentUniversidad de las Americas PueblaPuebla, Mexicopatricia.mccoy@udlap.mxAbstractThis paper discusses preliminary work investi-gating the application of Machine Translation(MT) metrics toward the evaluation of transla-tions written by human novice (student) trans-lators.
We describe a study in which we ap-ply the metric TERp (Translation Edit RatePlus) to a corpus of student-written transla-tions from Spanish to English and compare thejudgments of TERp against assessments pro-vided by a translation instructor.1 IntroductionExtensive work in the field of Computational Lin-guistics has focused on the development of gold-standard metrics to automatically judge the accuracyof machine-generated translations.
We are exploringwhether these metrics, or a modified version thereof,may be applied to the translations generated by hu-man novices.While Machine Translation (MT) metrics havebeen shown to perform poorly when evaluatinghuman-written translations due to their lack of toler-ance for the high level of variation in human-writtenwork, it is our belief that novice student translatorskeep much closer to the source text, and thereforewill be easier to assess using automatic metrics.Initial motivation for this work comes from de-veloping the King Alfred translation environment(Michaud, 2008) supporting students of Anglo-Saxon English translating sentences into ModernEnglish.
Criticisms of the application of compu-tational tools toward language learning have oftenhighlighted the reality that the mainstays of modernlanguage teaching?dialogue and a focus on com-municative goals over syntactic perfectionism?parallel the shortcomings of a computational envi-ronment.
While efforts continue to extend the stateof the art toward making the computer a conver-sational partner, they nevertheless often fall shortof providing the language learner with learning as-sistance in the task of communicative competencethat can make a real difference within or withoutthe classroom.
The modern learner of ancient or?dead?
languages, however, has fundamentally dif-ferent needs; the focus is on translation from sourcetexts into the learner?s L1.
An initial goal, therefore,was to provide the King Alfred system with abilityto automatically judge and respond to student trans-lations given a single instructor-provided reference.The potential applications of this work extend be-yond the learning of dead languages, however; trans-lation skills in modern languages (until the field ofMT reaches its full potential) are still needed for pro-viding readers with access to cross-lingual informa-tion.
The ability to assist translation instruction via atutoring system outside of the classroom, or to assesstranslator skill automatically, is therefore greatly de-sirable.The study described in this paper therefore fo-cuses on a corpus of learner-written translationsfrom a Spanish-English translation course; in Sec-tion 6 we discuss how these results may compareto those using a corpus of translations from Anglo-Saxon, which is one of our future tasks.306Figure 1: Output from the TERp system.2 Evaluating Student-WrittenTranslations Using TERpA primary challenge facing the assessment of trans-lation fitness is the abstract nature of the definitionof fitness with respect to the translating task.
Mostpeople approach this definition with two major foci:fluency (is it well-formed?)
and fidelity (does it con-vey original meaning?)
(Hovy et al 2002).
Thereare also stylistic concerns; translation can be definedas ?rendering the meaning of a text into another lan-guage in the way the author intended the text?
(New-mark, 1988)?and intention is difficult to preciselydefine.
None of these viewpoints dictates that thereexists only one way to write a translation.We were drawn to the TERp (Translation EditRate Plus) translation metric (Snover et al 2009)for our initial study because of its particular ap-proach toward capturing this multiplicity of correcttranslations.
Other metrics have addressed this is-sue; BLEU (Papineni et al 2002), for example, usesmultiple reference translations, in the hopes of cap-turing diversity through using diverse sources.
Thecreators of TERp, however, create an alignment be-tween reference and hypothesis strings in which di-rect matches are not required; they acknowledgesynonymy by leveraging WordNet synsets (Fell-baum, 1998; Princeton University, 2010), in addi-tion to using a stemmer, and a phrase table to handleprobabilistic phrasal substitution.
TERp also allowsfor words or phrases to be shifted into a different po-sition, which nicely accounts for flexibility in termsof prepositional phrase or adverb placement or tohandle modifiers that can take multiple forms.There has been some dismissal of the appropri-ateness of MT metrics for Computer-Aided Lan-guage Learning (CALL) applications (cf.
(Mc-Carthy, 2006)) due to the fact that they often providea holistic score comparing the hypothesis translationto one or more reference translations without identi-fying the source and nature of the differences.
How-ever, the output of TERp also includes more than aholistic score; there is complete documentation ofthe alignments, with tags identifying the ?edits?
re-quired to line up the hypothesis with the reference,as seen in Figure 1.
This is an excellent resourcefrom the perspective of translation pedagogy.
Whilethe METEOR system (Agarwal and Lavie, 2008)also uses WordNet synonymy and a stemmer to sim-ilar purpose, we believe that TERp comes the clos-est to embracing the multiplicity of translation pathswhile at the same time flagging issues of fundamen-tal concern in a pedagogical application of MT met-rics.3 Related WorkOther environments seeking to support studenttranslations have addressed the issue of auto-matically determining translation accuracy.
AEnglish-Chinese translation environment describedby (Wang and Seneff, 2007; Xu and Seneff, 2008)presents students with L1 sentences to translate intoL2 speech.
Because many of its L1 sentences areautomatically generated, there is no possibility ofprestored reference translations, so the system usesspeech recognition to obtain the L2 sentence, andthen parses both the English and Chinese sentencesinto a common interlingual representation in orderto compare for accuracy.
The authors report a highlevel of agreement between the system?s judgmentson translation acceptability compared to that of ahuman expert, but unfortunately, the system can-not give a finer-grained judgment on student perfor-mance than accept or reject.Another English-Chinese system is described by(Shei and Pain, 2002), creators of TMT, the Trans-lation Method Tutor.
In this case, students are trans-lating from their L2 (English) into their L1 (Chi-nese) using source sentences from Jane Austen?sPride and Prejudice, each selected to practice a par-ticular linguistic structure.
Students?
translationsare matched against four possible reference trans-lations: word-to-word (MT generated), literal (MT-307generated and then post-processed to obey word or-der rules), semantic (professional translations), andcommunicative (done by the authors), and the feed-back provided to the student includes which trans-lation she matched most closely and a lesson onhow to deal with the structure at hand.
Comparisonsbetween the student translation and the referenceslook at strict similarity and are heavily influencedby word selection rather than structure.The Translator Choice Program (McCarthy, 2006)focuses on French-English translation for native En-glish speakers.
It presents passages in the L2(French) and asks students to look at five candidateEnglish translations written by students in previousyears.
Students either pick the best translation orrank them, and are scored in how similar their judg-ment is to that of their instructor.
This system doesnot attempt, therefore, to handle novel translationsperformed by the student.4 A Corpus of Student-WrittenTranslationsIn Spring 2012, we solicited participation from stu-dents of a Spanish-English translation course.
In thiscourse, students are asked to translate a sequenceof articles in both Spanish and English, typicallyalternating the source language.
The articles ad-dress varied topics from financial advice to currentnews.
Thirteen students (both native English speak-ers and native Spanish speakers) opted to have theirsemester?s work collected as part of our study.
Ref-erence translations were provided for the entire cor-pus by the instructor of the course.For our initial study, we have focused on onlythe Spanish-to-English translations, as many aspectsof the metric we used focus on comparing an En-glish hypothesis sentence against an English refer-ence sentence.
This yielded a total of 2,982 sen-tences.
They are described in Table 1.Table 1: Our Student-Written Translation Corpus.Number of Subjects 13Native English Speakers 3Native Spanish Speakers 10Number of Articles Translated 11Average Number of Sentences per Article 28Total Translated Sentences 29825 Comparing Human Judgments to TERpBefore analyzing the translations with the MT met-ric, we post-processed the corpus to create an align-ment between student translations, source sentences,and the instructor reference.
One of the challengeswe faced in this step is that these students, unlikean MT system, are actively encouraged to recog-nize the stylistic differences between English andSpanish native writing in terms of sentence brevity.The students therefore sometimes create translationsthat do not always perfectly match sentence bound-aries of the source text; in some cases a singleSpanish sentence has been split into multiple En-glish sentences (following a general principle thatEnglish native speakers typically use more conciseutterances), but sometimes also the opposite occurs,where two source sentences are combined into onetranslated sentence.
While most translations (morethan 99%) did obey source sentence boundaries, foralignment purposes whenever a sentence was splitboth target sentences were concatenated into a singlestring (including the end-of-sentence punctuation,which is ignored by TERp)1 for comparison againstthe reference.
Where the student had merged twosentences, the clauses were separated at an appro-priate boundary and treated as separate utterances.The instructor-provided references obeyed a 1:1 cor-respondence between source and target sentences.Our entire corpus has been graded using theTERp-A variant, with unchanged parameters2.
TheTERp system scores sentences on an interval of[0,100], where a lower score indicates closer agree-ment to the reference translation, and 100 indicatesno agreement; for the ease of our human grader, wenormalized the TERp scores to invert the scale andbetter match a human-intuitive scale of 100 for ex-cellence and 0 for no agreement.Figure 2 illustrates for those subjects submittingmore than three assignments to the study the longitu-dinal progress of the average TERp score (inverted)across the sentences in each assignment given over1The insertion of a connector, such as ?and,?
to form a uni-fied sentence could be penalized by TERp, so it was avoided;the alternative to avoid penalty would be to include whateverconnector the original author used, but this would not be avail-able during automated analysis later.2As will be discussed in Section 6, a future goal is to tunethe parameters for performance on this data.308Figure 2: TERp scores across development.the term.
Although there were clearly a coupleof assignments that were very challenging to all ofthe students, the trend line shown indicates that thescores were rising over the course of the semester.We have also collected instructor-assigned scoreson a portion of our corpus in order to compare themagainst these TERp scores.
An example of the rubricused by the instructor as part of her regular gradingpractices in the course is shown in Table 2.
Eachof these categories receive a score from 0-10 with10 being excellent, 9 good, 8 satisfactory, and 0-7deficient.Table 2: Instructor rubric for assigning sentence grades.Conveys original meaning 55%Written in natural language 20%Uses appropriate vocabulary 10%Written in accurate language 15%Our preliminary study has yielded some interest-ing results.
The Pearson correlation between thetwo sets of scores is r=0.232236, which on a [-1,1]interval indicates weak positive correlation.
But ifTERp does not have significant agreement with thestudents?
instructor, what is the source of the dis-agreement?
One illustration of this disagreement isthe distribution of the grades; Figure 3 shows thatthe instructor?s grades are heavily slanted toward thehigh end of the scale, with 42% of the sentencesscored receiving a grade of 90 or higher; TERp,by contrast, gave very few sentences higher normal-ized accuracy scores.
This is most likely due to theinstructor?s heavy emphasis placed on communica-tive rather than syntactic accuracy, as shown in therubric.
We are in the process of rescoring the corpuswith a revised rubric that places stronger emphasison syntactic accuracy.Figure 3: TERp score distribution compared against thehuman expert.While TERp has already been evaluated in termsof its correlation to human judgment, this has notbeen done before with learner-written sentences3.We also performed an analysis of a randomized sam-ple of individual sentences with a particular focuson the four edits designed to accommodate diver-gence but equivalence (or near equivalence): phraseequivalency, stemming, synonymy, and shifts.
Ourpilot study results indicate that TERp?s identified ed-its have very high precision: 100% for the stemmer,which is to be expected, but also 92% for appropri-ate shifts, 89% for synonymy, and 83% for phraseequivalency.
In recall, the edits performed less well;for example, synonymy achieved a recall of only65%.
This is possibly a limitation of the synset re-source.6 Conclusion and Future WorkWe have seen that TERp?s identification of thesource and nature of divergences between a student3The word learner here refers to the fact that the writer is astudent of translation, not to whether he or she is writing in anL2.309translation and a teacher?s reference translation isreliable; it correctly identifies the nature of the di-vergence from the reference in a high percentageof cases.
This can provide a tutoring environmentwith sufficient information to address the transla-tion?s problems in feedback to the student, and in-dicates that holistic scores will be much more corre-lated with human scores that place equal emphasison syntactic quality.
A future version of the KingAlfred system will use these error identifications todrive its feedback.Once the rescoring of the corpus with an empha-sis on syntactic accuracy is complete, further workwill include tuning the TERp parameters for higherperformance on the student corpus, with the aim ofgreatly improving the correlation of the scores.We are also looking at post-processing TERp?sscores so that certain divergences are not penalized.There is a cost associated with the edits that repre-sent mismatches between the reference and hypoth-esis texts.
While the idea of flexible phrase order,and the equality of synonym choice or phrase choiceis captured by the metric, the application of suchedits worsens the grade of the translation.
We be-lieve that stemming and substitution, deletion, orinsertion should be penalized, but that synonymy,phrase matches, and shifts should be free of charge;those costs will therefore be added back into the fi-nal score.As part of our larger investigation, we will con-tinue to evaluate the applicability of machine trans-lation metrics in general to the learner translationproblem.
The Mult-Eval suite of metrics (Clark etal., 2011) is a short term target, and iBLEU (Mad-nani, 2011) may provide useful data for a pedagogi-cal context.With a recent addition of 14 more subjects, wewould also like to do an investigation of whetherthe performance of an MT metric is affected bywhether the novice translator is translating L1?L2,or L2?L1.
English native speakers are a minorityin our subject pool, but with doubling the size of ourcorpus, we may be able to explore this more reliably.One of our other interests going forward is to ac-commodate the distinct errors made by a very novicehuman translator.
One such error is a tendencyto fall prey to false cognates or faux amis?falsefriends, words that look similar (like Spanish em-barazada and English embarrassed) that have sig-nificantly different meanings (embarazada, for ex-ample, meaning ?pregnant?).
We have a workinghypothesis that student translators are often misledby these similar-looking words.
We are currentlyworking to automatically extract potential faux amisfrom parallel Spanish/English dictionaries with thehope of augmenting TERp?s ability to align parallelelements between the student and reference trans-lation.
We are leveraging the spellcheck algorithmHunspell to identify the similarly-spelled words.Finally, it is our intention to do a comparativestudy between evaluating learner translations frommodern languages and learner translations from an-cient languages such as Anglo-Saxon.
One chal-lenge that may arise is that many ancient languagessuch as Anglo-Saxon are morphologically rich andtherefore not strict word order languages; the sourcetext will be fluid with its own order and this mayintroduce more diversity than in a modern languagetranslation even among novice translators.AcknowledgmentsWe wish to sincerely thank the students who havevolunteered to share their semester?s work with usfor the purpose of this study.
We would also like tothank the reviewers for their helpful comments andadditional references.ReferencesAbhaya Agarwal and Alon Lavie.
2008.
METEOR,M-BLEU and M-TER: Evaluation metrics for high-correlation with human rankings of machine transla-tion output.
In Proceedings of the Third Workshopon Statistical Machine Translation, pages 115?118,Columbus, Ohio, June.
ACL.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statisti-cal machine translation: Controlling for optimizer in-stability.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies (ACL-HLT), pages 176?181.
ACL.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Eduard Hovy, Margaret Kine, and Andrei Popescu-Belis.2002.
Principles of context-based machine translationevaluation.
Machine Translation, 17:43?75.310Hunspell: open source spell checking, stem-ming, morphological analysis and generationunder GPL, LGPL or MPL licenses.
Website.http://hunspell.sourceforge.net/ Accessed February2013.Nitin Madnani.
2011. iBLEU: Interactively debuggingand scoring statistical machine translation systems.In Proceedings of the 2011 IEEE Fifth InternationalConference on Semantic Computing, ICSC ?11, pages213?214, Washington, DC, USA.
IEEE Computer So-ciety.Brian McCarthy.
2006.
Tutoring translation skills: Re-flections on a cmoputer-managed teaching-learning-research triangle.
CALL-EJ Online, 7(2), January.Lisa N. Michaud.
2008.
King Alfred: A translationenvironment for learners of anglo-saxon english.
InProceedings of the 3rd Workshop on Innovative Use ofNLP for Building Educational Applications, an ACL-HLT ?08 Workshop, pages 19?26, Columbus, Ohio,June.
ACL.Peter Newmark.
1988.
A Textbook of Translation.
Pren-tice Hall International, New York.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics, pages 311?318, Philadelphia, PA,July 6-12.
ACL.Princeton University.
2010.
WordNet.
Website.http://wordnet.princeton.edu Accessed July 2011.Chi-Chiang Shei and Helen Pain.
2002.
Computer-assisted teaching of translation methods.
Literary &Linguistic Computing, 17(3):323?343.Matthew Snover, Nitin Madnani, Bonnie J. Dorr, andRichard Schwartz.
2009.
Fluency, adequacy, orHTER?
exploring different judgments with a tunableMT metric.
In Proceedings of the EACL 2009 FourthWorkshop on Statistical Machine Translation, pages259?268, Athens, Greece, March 30-31.
ACL.Chao Wang and Stephanie Seneff.
2007.
Automatic as-sessment of student translations for foreign languagetutoring.
In Proceedings of Human Language Tech-nologies 2007: The Conference of the North AmericanChapter of the Association for Computational Linguis-tics (HLT-NAACL), pages 468?475, Rochester, NY,April 22-27.
ACL.Yushi Xu and Stephanie Seneff.
2008.
Mandarian learn-ing using speech and language technolgies: A trans-lation game in the travel domain.
In Proceedings ofthe 6th International Symposium on Chinese SpokenLanguage Processing (ISCSLP08), Kunming, China,December.311
