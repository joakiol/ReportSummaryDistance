Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 45?50,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsExpectation Vectors: A Semiotics Inspired Approachto Geometric Lexical-Semantic RepresentationJustin WashtellUniversity of LeedsLeeds, UKwashtell  @ comp.leeds.ac.ukAbstractWe  introduce  a  new  family  of  geometricmodels  of  meaning,  inspired  by  principlesfrom  semiotics  and  information  theory,based on what we call Expectation Vectors.We present theoretical arguments in supportof  these  representations  over  traditionalcontext-feature  vectors:  primarily that  theyprovide  a  more  intuitive  representation  ofmeaning,  and  detach  vector  representationfrom  the  specific  context  features  therebyallowing  arbitrarily  sophisticated  languagemodels  to  be  leveraged.
We  present  apreliminary  evaluation  of  an  expectationvector  based  word  sense  disambiguationsystem  using  the  SemEval-2007  task  2dataset,  with  very  encouraging  results,particularly with respect to ambiguous verbs.1 IntroductionIt  is  a  cornerstone  assumption  of  distributionallexical semantics that the distribution of words in acorpus  reflects their  meaning.
Commoninterpretations  of  this  include  the  DistributionalHypothesis  (Harris,  1954)  and  the  ContextualHypotheses (Miller & Charles, 1991), which statethat  there  is  a  relationship  between  a  word'smeaning, and the context(s) in which it appears.
Inrecent  years  this  insight  has  been  borne  out  bycorrelations  between  human  judgements  anddistributional  models  of  word  similarity  (Rapp,2002), and steady advances in tasks such as wordsense  disambiguation  (Sch?tze,  1998)  andinformation  retrieval.
The  workhorse  of  theseapproaches  are  wordspace  models:  vectors  builtfrom context  features  which  serve  as  geometricanalogues  of  meaning.
Despite  many  advances,substantial  problems  exist  with  this  approach  tomodelling  meaning.
Amongst  these  are  theproblems of data sparseness and of how to modelcompositional meaning.In this short paper, we introduce a new familyof  wordspace models,  based on insights  gleanedfrom  semiotics  and  information  theory,  calledExpectation Vectors.
These retain the  convenientvector-based  paradigm  whilst  encouraging  theexploitation  of  advances  in  language  modellingfrom other areas of NLP.
We finish by outliningsome  present  efforts  to  evaluate  expectationvectors in the area of word sense disambiguation.2 Modelling meaning from contextPerhaps  one  of  the  most  prominent  applicationareas to exploit  context-based wordspace modelsis that of word sense induction and disambiguation(WSI/WSD).
The  prevailing  approach  to  thisproblem is based on a fairly literal interpretation ofthe Distributional Hypothesis: that is to cluster orclassify instances of  ambiguous words accordingto  certain  features  of  the  context  in  which  theyappear ?
invariably other words.
It is not difficultto see why this approach is limiting: as Pedersen(2008) observes,  ?the unifying thread that  bindstogether  many  short  context  applications  andmethods is the fact that similarity decisions mustbe made between contexts that share few (if any)words  in  common.?
This  is  a  manifestation  ofwhat  is  commonly  referred  to  at  the  datasparseness problem, and it pervades all of corpus-based  NLP.
This  problem  is  exacerbated  asavailable examples of  a word sense decrease,  orfiner sense granularities are sought.
For supervisedtasks  this  implies  that  a  large  training  set  isrequired,  which  is  often  expensive.
Forunsupervised tasks,  such as WSI,  it  has negativeimplications for cluster quality and rule learning.Consequently,  Leacock  et  al (1996)  observe thatWSD systems which operate directly upon contextare: ?plagued  with  the  same  problem,  excellentprecision but low recall?.
?Backing off?
to more general feature classesthrough  say  lemmatization  or  part-of-speechtagging affords one way of alleviating sparseness(Joshi  &  Penstein-Ros?,  2009),  assuming  thesefeatures are pertinent to the task.
Similar strategiesinclude  the  use  of  dual-context  models  whereimmediate lexical features are backed up by moregeneral  topical  ones  garnered  from  the  widercontext  of  the  ambiguous  word  (Leacock  et  al,1996; Yarowsky, 1993).Others have tackled the problem of sparsenesswithout  recourse  to  generalized  feature  classes,45through  the  exploitation  of  higher-orderdistributional  information.
Sch?tze  (1998)popularised  this  approach  within  the  WSD/WSItask.
Rather than comparing contexts directly, it isthe  distributional  similarity of  those features  (inthe  corpus)  which  are  compared.
Specifically,Sch?tze  composed  context  vectors  by  summingthe  vectors  for  every word  in  a  context,  wherethose  vectors  were  themselves  formed  from thetotal  of  word co-occurrence counts pertaining toevery  instance  of  that  word  in  the  corpus.
Theresultant  context  vectors  are  thereforecomparatively  dense,  and  carry  second-orderinformation  which  makes  otherwise  unlikecontexts  more  amenable  to  comparison.
Onecontention  of  this  model  is  that  it  conflates  co-occurrence information from all occurrences of aword in the corpus, regardless of their sense.
Thedefence is  that  because the  actual  senses  of  theterm instances which appear in the context of theambiguous word will tend to be pertinent to thatword?s  own  specific  sense,  it  is  that  commonaspect of their respective conflated-sense vectors -when summed - which will dominant the resultantcontext  vector.
Purandare  &  Pedersen  (2001)performed a comparative study of disambiguationapproaches  based  on  first-order  context,  and  onsecond order context as per Sch?tze (1998).
Theyfound  that  while  Sch?tze's  approach  providedgains  when data  was  limited,  when  the  trainingcorpus was large enough that sufficient examplesexisted,  clustering  on  first  order  context  wasactually  a  better  approach.
This  suggests  thatwhile alleviating the data-sparseness problem, thepractice of expanding context vectors in this wayintroduces a certain amount of noise, presumablyby inappropriately over-smoothing the data.Another  approach  to  the  sparse  data  problemwhich  was  also  part  of  Sch?tze's  framework  isdimensionality  reduction  by  Singular  ValueDecomposition (SVD).
In SVD the set of contextfeatures are analytically combined and reduced ina  manner  that  exploits  their  latent  similarities,whereafter  traditional  vector  measures  can  beused.
Very similar techniques to both of those usedby Sch?tze have been used for query expansionand  document  representation  in  informationretrieval (Qiu & Frei, 1993; Xu et al 2007).Several variations upon Sch?tze?s approach toWSD have been explored.
Dagan et al(1995) andKarov & Edelman (1996)  both apply what  theycall  ?similarity-based?
methods  which,  whilemarkedly  different  on  the  surface  to  that  ofSch?tze, are similar in spirit and intent.
Karov &Edelman,  for  example,  use  machine-readabledictionary glosses  as  opposed  to  corpus-derivedco-occurrences,  and  apply  an  iterativebootstrapping approach to augment the availabledata, rather than strict second-order information.Typically,  context  vectors  comprise  acomponent  (dimension)  for  each  designatedfeature  in  a word?s  context.
In a simple bag-of-words  model  this  might  equate  to  one  vectorcomponent for each potential word that can appearin the context.
For more sophisticated n-gram ordependency-based models, which attempt to bettercapture the structure inherent in the language, thisnumber of vector components must be increased.The  more  sophisticated  the  language  modelbecomes therefore, the more acute the sparse dataproblem.
Techniques  like  SVD  can  reduce  thissparseness, but other issues remain.
How does oneweight  heterogeneous  features  when  forming  avector?
How does  one interpret  vectors  reducedby SVD?
Looking at the variety of approaches totackling  the  problem,  we  might  be  forgiven  forquestioning  whether  representing  meaning  as  avector  of  context  features  is  in  fact  an  idealstarting point for semantic tasks such as WSD.In the following section we describe a means ofentirely detaching context  feature selection fromvector  representation,  such  that  an  arbitrarilysophisticated  language  model  can  be  used  togenerate  dense,  comparable vectors.
Necessarily,we  also  present  a  prototype  distributionallanguage model that will serve as the basis of ourinvestigations into this approach.3 System & approach3.1 Lexical Expectation VectorsTheoretical  motivation.
The  motivation  behindthe method presented herein comes both from thefields of semiotics and information theory.
It is thenotion that the ?meaning?
of an utterance is not inthe utterance itself, nor in its individual or typicalcontext;  it  is  in  the  disparity between  ourexpectations  based  on  that  context,  and  theutterance (Noth, 1990; Chandler, 2002).
Meaningin this sense can be seen as related to information(Attneave,  1959;  Shannon,  1948):  an  utterancewhich is entirely expected under a regime wherespeaker  and  interpreter  have  identical  frames  ofreference  communicates  nothing;  conversely  anextremely  creative  utterance  is  laden  withinformation, and may have multiple non-obviousinterpretations  (poetry  being  a  case  in  point  -Riffaterre,  1978).
This  idea  is  also  lent  someweight  by  psycholinguistic  experiments  whichhave  revealed  correlations  between  a  word'sdisparity  from  its  preceding  context,  andprocessing  times  in  human  subjects.
Similarinsights have been employed in some very recent46attempts  to  model  compositional  word  meaningErk & Pad?
(2008) and Thater et al(2009).
Thesemodels augment word and context representationswith  additional  vectors  encoding  the  selectionalpreferences  (expectations)  pertaining  to  thespecific  syntactic/semantic  roles  of  theparticipating words.
So far these systems rely uponparsed  corpora  and  have  been  tested  only  withvery limited contexts (e.g.
pairs of words havingspecific dependency relations).Lexical  expectation  vectors  are  based  on  asimilar  and  very  simple  premise:  rather  thanbuilding a vector for  a context by conflating thefeatures which comprise the various context words(as per Schutze, 1998), we instead conflate all thewords which might be expected to appear  withinthe  context (i.e.
in  the  headword  position).Consider  the  following short  context  taken fromthe SemEval-2007 task 2 dataset:Mr. Meador takes responsibilityfor <?> and property management .The  strongest  twenty  elements  of  itsexpectation  vector  (as  generated  by  the  systemdescribed below) are shown in table 1.
The figuresrepresent some measure of confidence that a givenword will be found in the headword position <?>.0.42 education 0.31 chancellor0.38 forms 0.31 routine0.36 housing 0.31 health0.35 counselling 0.31 research0.35 these 0.31 assessment0.35 herself 0.3 detailed0.34 database 0.3 management0.33 injuries 0.3 many0.32 advice 0.3 training0.31 this 0.3 whatTable 1: An example of an expectation vector.We make  the  supposition  that  when  the  vectorsimplied by the respective likelihoods of  all wordsimplied by two contexts are identical, the contextscan be considered semantically equivalent.1 Notethat the actual headword appearing in the contextis not taken into consideration for the purposes ofcalculating expectation.
In this example it occur atrank 62 out of  ~650,000, implying that its use inthis context is not atypical.Formal approach.
For the purpose of our presentresearch,  we  adopt  the  following  formalframework  for  generating  an  expectation  vector.1 Equivalent with respect to the head of the context.This is not the same as saying the passages have the samemeaning, which requires recourse to compositionality.Given  a  context  c,  each  component  of  theexpectation vector  e arising from that  context  isestimated thusly:Where j is a given word type in the lexicon, Ojis  the  set  of  all  observed contexts  of  that  wordtype in some corpus, ojk is the kth observed contextof that word type, and sim(o,c) is some similaritymeasure between two contexts.The process of generating an expectation vectorcan  be  thought  of  as  a  kind  of  transform fromsyntagmatic space, into  paradigmatic space.
Thismapping need not be trivial: items which are closein the syntagmatic space need not be close in theparadigmatic  space  and  vice-versa  (although  inpractice we expect some considerable correlationby virtue  of  the  distributional  hypothesis).
Notethat although our work herein assumes a popularvector representation of context, the nature of thecontexts and the similarity measure which operatesupon them are not constrained in any way by theframework  given  above.
For  example  they  mayequally well be dependency trees.In the following section we outline a distance-based language model comprising a context modeland  a  similarity  metric  which  operates  upon  it.This  choice  of  model  allows  us  to  maintain  apurely  distributional  approach  without  sufferingthe  data-sparseness  associated  with  n-grammodels.3.2 Language modelTheoretical motivation.
The precise relationshipbetween  syntagmatic  and  paradigmatic  spacesimplied  by  the  expectation  transform  dependsupon  the  language  model  employed.
In  a  naivelanguage  model  which  assumes  independencebetween  features,  this  mapping  can  be  fullyrepresented by a square matrix over word types.Although such models are the mainstay of manysystems  in  NLP,  adopting  the  toolset  of  anexpection transform in such a case gains us little.Therefore  the  relevance  of  the  approach  to  thepresent  task  depends  wholly  upon  having  asuitably sophisticated language model.Building on the work of Washtell  (2009) andTerra & Clarke (2004), a distance-based languagemodel  is  used  in  the  present  work.
This  is  incontrast to the bag-of-words, n-gram, or syntacticdependency models more commonly described inthe  NLP literature.
There  are  two  hypothesisedadvantages to this approach.
Firstly, this avoids theissue  of  immediate  context  versus  wider  topical47context.
While  immediate  context  is  generallyaccepted to  play a  dominant  role  in  WSD, bothnear and far context have been shown to be useful- the specific balance being somewhat dependenton  the  ambiguous  word  in  question  (Yarowsky,1993; Gale et al 1992; Leacock  et al  1996).
AsIde & Veronis (1998) astutely observe, ?althougha distinction is made between micro-context andtopical  context  in  current  WSD  work,  it  is  notclear that this distinction is meaningful.
It may bemore  useful  to  regard  the  two as  lying  along acontinuum,  and  to  consider  the  role  andimportance of contextual information as a functionof distance from the target.?
This is precisely theassumption adopted herein.
Secondly,  the  use  ofdistance-based  information  alleviates  datasparseness.
This is simply by virtue of the fact thatall  words  types  in  a  document  form  part  of  atoken's context (barring document boundaries, nocut-off  distance  is  imposed).
Moreover,  as  it  isspecific  distance  information  which  is  beingrecorded,  rather  than  (usually  low)  frequencycounts,  context  vector  components  and  thesimilarity  measurements  which  arise  from  themexhibit  good  precision.
Washtell  (2009)  showedthat these properties of distance-based metrics leadto measurable gains in information extracted froma  corpus.
In  the  context  of  modelling  humannotions  of  association  this  also  led  to  improvedpredictive power (Washtell & Markert, 2009).Formal  approach.
We  do  not  pre-compute  anystatistical  representation  of  the  data  upon whichour  language  model  draws.
With  availableapproaches  this  would  either  require  throwingaway  a  large  number  of  potentially  relevanthigher-order dependencies, or would otherwise beintractable.
Our  intuition  is  that  the  truestrepresentation  of  the  language  encoded  in  thecorpus  is  the  corpus  itself.
We  therefore  use  anindexed corpus directly for all queries.We use the following as a prototype measure ofstructural  similarity  (see  section  3.1),  althoughnote that others are by all means possible.Where  o and  c are  context  vectors  whose  jcomponents each specify the position in the text ofthe nearest occurrence (to the head of the context)of  a  given  word  type.
O and  C are  the  set  ofindices of all non-zero (i.e.
observed) componentsin o and c respectively.
The head of the context isrepresented by an additional component in vectorso and  c, and is always treated as observed.
f is afurther function of the positions of words p and qin both contexts.
It returns a similarity score in theunit  range  designating  how similar  the  distanceop?oq is to that of cp?cq.The  more  consistent  the  relative  positions  ofthe various symbols comprising two contexts, thestronger their similarity.
Note that the measure isadditive:  symbols  which  occur  at  all  in  bothcontexts result in positive score contributions.
Weassume that  a context  is  usually incomplete  (i.e.that that which lies outside it is unknown, ratherthan non-existent).
The minimum operator in thedenominator  (the  normalization  factor)  thereforeensures  that  words  present  only in  the  larger  oftwo contexts do not constitute negative evidence.This  formulation  allows  for  considerableleeway in how word distances are represented andcompared.
In  this  work  we  choose  to  treatdistances  proportionately,  so  small  variations  inword  position  between  distant  (presumablytopically related words)  are  tolerated better  thansimilar  distance variations  between neighbouring(more syntactico-semantically related) words.4 Word Sense DisambiguationA WSD system based on expectation vectors wasineligible in the SemEval-2010 WSI/WSD task byvirtue  of  restrictions  disallowing  the  use  of  acorpus-based  language  model.
Instead,  this  taskimplicitly  encouraged  participants  to  focus  oncontext  feature  selection  and  clusteringapproaches.
It  seems  unlikely  to  us  that  thesestages are where the major bottlenecks for WSD(or WSI) lie;  performing WSD on short contextswithout  any  extra-contextual  information  (i.e.general  linguistic  or  domain  experience)  isarguably not a task which even humans could beexpected to perform well.
For this reason we havechosen  to  focus  initially  on  the  well  exploredSemEval-2007 task 2 dataset.4.1 Preliminary EvaluationAn  expectation  vector  was  produced  for  eachtraining and test instance in the SemEval datasetby matching the headword?s context against that ofeach word position in the British National Corpususing  an  implementation  of  the  distance  basedsimilarity  measure  outlined  in  section  3.2.
Formatters of convenience, independent forwards andbackwards  expectation  vectors  were  producedfrom the context preceding the headword and thatfollowing it,  and  their  elements  were  multipliedtogether  to  produce  the  final  vector.
Nolemmatization  or  part-of-speech  tagging  was48employed.
Neither  was  any  dimensionalityreduction, each vector therefore having  ~650,000elements: one for each word type in the corpus.Each test sample's vector was compared againstall  corresponding  training  sample  vectors  usingboth cosine similarity and Euclidean distance2.
Inthe MAX setups (see Table 2), each test case wasassigned the  sense  of  the  single  nearest  trainingexample according to the metric being used.
In theCosOR  setup,  sense  scores  were  generated  byapplying  a  probabilistic  OR  operation  over  thesquared Cosine similarities of all relevant trainingexamples3.
The  BaseMFS  setup  is  a  popularbaseline in which the most frequent sense in thetraining  set  for  a  given  ambiguous  word  isattributed to every test case.Nouns Verbs AllCosMAX 83.6 ?6.1 ?22.8 70.5?7.6 ?14.4 79.5 ?6.7 ?19.5EucMAX 78.9 67.0 75.1CosOR 83.5 66.1 78.0BaseMFS 78.8 65.5 74.5Table  2:  Recall  on  SemEval  WSD  task,  includingrelative performance gain (?)
and error reduction (?
)over baseline for best setup (preliminary based on first25% of test cases).Nouns Verbs AllBEST 86.8 ?7.3 ?30.9 76.2 ?0.0 ?0.0 81.6 ?3.7 ?13.6BaseMFS 80.9 76.2 78.7Table 3: Recall of best official SemEval WSD systems(Agirre & Soroa, 2007), showing relative performancegain and error reduction over baseline.Table 2 shows the results for each test case interms of recall,  for  all  words and for nouns andverbs separately.
Also shown in table 3 are the bestand baseline  figures for  the  official  entries fromthe Semeval workshop.
Note that figures are notdirectly  comparable  between  tables  because  ourpreliminary results represent only the first 25% ofthe  SemEval  dataset  (hence  the  differentbaselines).
To  aid  some  comparison,  figures  areincluded  in  both  tables  indicating  the  relativeincreases in recall over the baseline, and relative2 Cosine Similarity captures the similarity between therelative  proportions  of  features  present  in  each  of  twovectors.
By  contrast,  Euclidean  Distance  compares  theactual values of corresponding features.3 Although encountered rarely in the literature, squaredCosine Similarity is a pertinent quantity for tasks that gobeyond simple ranking.
As with Pearson's R2, it representsthe degree  or  proportion  of  similarity  (consider  that  thesquare of an angle's cosine and that of its sine total 1).reduction in error.
Note that the system employedhere is not a word sense induction system as weremost of those participating in the official SemEvaltask.
The setup of  the  tasks  however  allows forsystems which perform poorly under the inductionevaluation  to  perform  competitively  asdisambiguation systems, so we are not precludedfrom making meaningful comparisons here.5 Discussion and Future DirectionWe have  presented  a  new type  of  wordspacemodel  based  on  vectors  derived  from  thepredictions  of  a  language  model  applied  to  acontext, rather than directly from the features of acontext  itself.
We  have  conducted  a  preliminaryinvestigation of the semantic modelling power ofsuch vectors in the setting of a popular WSD task.The results  are  very encouraging.
Although it  istoo  early  to  draw  hard  conclusions,  preliminaryresults suggest a performance at least comparablethe present  state of  the art  on this  task.
What  isparticularly noteworthy is that the approach takenhere  seems  to  perform  equally  well  atdiscriminating  verbs  and  nouns.
Verbs  havetraditionally proven very problematic: none of thesix SemEval systems were able to improve uponthe  verb  baseline.
More  recent  studies  havefocused on discriminating nouns (Brody & Lapata,2009; Klapaftis & Manandhar, 2007).Further gains might be expected by employinga  corpus  which  is  more  closely  matched  to  thematerial  being  disambiguated,  such  as  the  WallStreet Journal in the present case.It is also worth noting that the system presentedhere  was  aided  only  by  an  untagged  un-lemmatised  corpus,  without  the  use  of  anystructured  knowledge  sources.
While  we  expectthat judicious use of lemmatization could improvethese results, we believe the key to the quality ofexpectation  vectors  is  in  the  specific  predictivelanguage  model  employed.
We  have  scarcelyexperimented  with  this,  opting  for  a  relativelyuntested  distance-based  model  throughout,  andchoosing  instead  to  experiment  with  theapplication of different vector similarity measures.While  the  nature  of  the  language  model  usedenables  it  to  capture  complex  interdependencies,and long-range dependencies, it is based on directquerying of a corpus and therefore does not scaleat  all  well.
This  makes  its  use  in the  context  ofmost applications or with larger corpora untenable.Exploring  alternative  language  models  (drawingupon the copious research in this field) is thereforea focus for future research; the ability to do thishighlights  one  of  the  major  advantages  of  thisapproach to modelling meaning.49ReferencesEneko Agirre, Airotr Soroa, 2007, SemEval-2007Task  02:  Evaluating  Word  Sense  Induction  andDiscrimination SystemsFred  Attneave,  Applications  of  InformationTheory  to  Psychology:  A  Summary  of  BasicConcepts, Methods, and Results, Holt, New YorkSamuel  Brody,  Mirella  Lapata,  2009,  BayesianWord Sense Induction, Proceedings of EACL 2009,pages 103-111Daniel  Chandler,  2002,  Semiotics:  The  Basics,p88, RoutleadgeIdo  Dagan,  Shaul  Marcus,  Shaul  Markovitch,1995,  Contextual  Word  Similarity  and  Estimationfrom Sparse Data,  Proceedings of 31st ACL, pages164-171Katrin  Erk,  Sebastian  Pad?,  2008,  A StructuredVector Space Model for  Word Meaning in Context,Proceedings on EMNLP 2008William  Gale,  Kenneth  Church,  ,  DavidYarowsky, 1992, A Method for Disambiguating WordSenses  in  a  Large  Corpus,  Computers  and  theHumanities, 26, 415-429Zellig  Harris,  1954,  Distributional  structure.Word, 10(23), pages 146-162Nancy  Ide,  Jean  Veronis,  1998,  Word  SenseDisambiguation: The State of The Art,Mahesh  Joshi,  Carolyn  Penstein-Ros?,  2009,Generalizing  Dependency  Features  for  OpinionMining,  Proceeding  of  the  ACL-IJCNLP  2009Conference Short Papers, pages 313-316Yael  Karov, Shimon  Edelman,  1996,  LearningSimilarity-Based  Word Sense  Disambiguation  fromSparse DataIoannis Klapaftis, Suresh Manandhar, 2008, WordSense  Induction  using  Graphs  of  Collocations,Proceedings of ECAI 2008, pages 298-302Claudia  Leacock,  Geoffrey  Towell,  Ellen  M.Voorhees,  1996,  Towards  Building  ContextualRepresentations  of  Word  Senses  Using  StatisticalModels.
In  B.  Boguraev  and  H.Pustejovsky  (eds)Corpus  Processing  for  Lexical  Acquisition.
MITPress, pages 97-113G.
A.  Miller,  W.  G  Charles,  1991.
Contextualcorrelates  of  semantic  similarity.
Language  andCognitive Processes, 6, 1-28.Winfried  Noth,  1990,  Handbook  of  Semiotics,Indiana University Press,  p 142Reinhard Rapp.
2002.
The computation of wordassociations:  comparing  syntagmatic  andparadigmatic approaches.
In Proceedings of the 19thinternational  Conference  on  ComputationalLinguistics.Hinrich Sch?tze,  1998,  Automatic  Word  SenseDiscrimination,  Computational  Linguistics,  24(1),pages 97-123Ted Pedersen, 2008,  Computational Approachesto  Measuring  the  Similarity  of  Short  Contexts:  AReview of Applications and Methods, to appearAmruta  Purandare,  Ted  Pederson,  2004, Wordsense discrimination by clustering contexts in vectorand similarity spaces.
Proceeding of the Conferenceon Computational Natural Language Learning, pages41-48Yonggang  Qiu,  Hans-Peter  Frei,  1993,Proceedings of  the 16th annual  international  ACMSIGIR conference on Research and development ininformation retrieval table of contents, 160-169Miachael  Riffaterre,  1978,  Semiotics  Of  Poetry,MethuenHae Jong Seo, Peyman Milanfar, 2009, Training-free,  Generic  Object  Detection  using  LocallyAdaptive Regression Kernels, IEEE Transactions onPattern Analysis and Machine Intelligence, 99Claude E Shannon, 1948, A Mathematical Theoryof  Communication,  Bell  System Technical  Journal,vol.
27, pages 379-423, 623-656,Egidio  Terra,  Charles  L.  A.  Clarke,  2004, Fastcomputation of lexical affinity models , Proceedingsof  the  20th  international  conference  onComputational LinguisticsStefan Thater, Georgiana Dinu, Manfred Pinkal,2009, Ranking Paraphrases in Context, Proceedingsof the 2009 Workshop on Applied Textual Inference,pages 44-47Justin  Washtell.
2009.
Co-dispersion:  Awindowless  approach  to  lexical  association.
InProceedings of EACL-2009.Justin Washtell, Katja Markert.
2009.
Comparingwindowless  and  window-based  computationalassociation  measures  as  predictors  of  syntagmatichuman  associations.
In  Proceedings  of  EMNLP-2009, pages 628-637.Xuheng Xu, Xiaodan Zhang, Xiaohua Hu, 2007,Using  Two-Stage  Concept-Based  Singular  ValueDecomposition  Technique  as  a  Query  ExpansionStrategy, AINAW'07David  Yarowsky,  1993,  One  Sense  PerCollocation, Proceedings  on  the  Workshop  onHuman Language Technology, pages 266-27150
