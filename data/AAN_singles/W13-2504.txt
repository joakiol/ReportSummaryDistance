Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 24?33,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsA Comparison of Smoothing Techniques for Bilingual Lexicon Extractionfrom Comparable CorporaAmir Hazem and Emmanuel MorinLaboratore d?Informatique de Nantes-Atlantique (LINA)Universite?
de Nantes, 44322 Nantes Cedex 3, France{Amir.Hazem, Emmanuel.Morin}@univ-nantes.frAbstractSmoothing is a central issue in lan-guage modeling and a prior step in dif-ferent natural language processing (NLP)tasks.
However, less attention has beengiven to it for bilingual lexicon extrac-tion from comparable corpora.
If a firstwork to improve the extraction of lowfrequency words showed significant im-provement while using distance-based av-eraging (Pekar et al 2006), no investi-gation of the many smoothing techniqueshas been carried out so far.
In this pa-per, we present a study of some widely-used smoothing algorithms for languagen-gram modeling (Laplace, Good-Turing,Kneser-Ney...).
Our main contribution isto investigate how the different smoothingtechniques affect the performance of thestandard approach (Fung, 1998) tradition-ally used for bilingual lexicon extraction.We show that using smoothing as a pre-processing step of the standard approachincreases its performance significantly.1 IntroductionCooccurrences play an important role in manycorpus based approaches in the field of natural-language processing (Dagan et al 1993).
Theyrepresent the observable evidence that can bedistilled from a corpus and are employed for avariety of applications such as machine transla-tion (Brown et al 1992), information retrieval(Maarek and Smadja, 1989), word sense disam-biguation (Brown et al 1991), etc.
In bilinguallexicon extraction from comparable corpora,frequency counts for word pairs often serve asa basis for distributional methods, such as thestandard approach (Fung, 1998) which comparesthe cooccurrence profile of a given source word, avector of association scores for its translated cooc-currences (Fano, 1961; Dunning, 1993), with theprofiles of all words of the target language.
Thedistance between two such vectors is interpretedas an indicator of their semantic similarity andtheir translational relation.
If using associationmeasures to extract word translation equivalentshas shown a better performance than using araw cooccurrence model, the latter remains thecore of any statistical generalisation (Evert, 2005).As has been known, words and other type-richlinguistic populations do not contain instancesof all types in the population, even the largestsamples (Zipf, 1949; Evert and Baroni, 2007).Therefore, the number and distribution of typesin the available sample are not reliable estimators(Evert and Baroni, 2007), especially for smallcomparable corpora.
The literature suggests twomajor approaches for solving the data sparsenessproblem: smoothing and class-based methods.Smoothing techniques (Good, 1953) are oftenused to better estimate probabilities when thereis insufficient data to estimate probabilities ac-curately.
They tend to make distributions moreuniform, by adjusting low probabilities such aszero probabilities upward, and high probabilitiesdownward.
Generally, smoothing methods notonly prevent zero probabilities, but they alsoattempt to improve the accuracy of the model as awhole (Chen and Goodman, 1999).
Class-basedmodels (Pereira et al 1993) use classes of similarwords to distinguish between unseen cooccur-rences.
The relationship between given words ismodeled by analogy with other words that arein some sense similar to the given ones.
Hence,class-based models provide an alternative to theindependence assumption on the cooccurrenceof given words w1 and w2: the more frequentw2 is, the higher estimate of P (w2|w1) will be,regardless of w1.24Starting from the observation that smoothing es-timates ignore the expected degree of associationbetween words (assign the same estimate for allunseen cooccurrences) and that class-based mod-els may not structure and generalize word cooc-currence to class cooccurrence patterns withoutlosing too much information, (Dagan et al 1993)proposed an alternative to these latter approachesto estimate the probabilities of unseen cooccur-rences.
They presented a method that makesanalogies between each specific unseen cooccur-rence and other cooccurrences that contain similarwords.
The analogies are based on the assump-tion that similar word cooccurrences have simi-lar values of mutual information.
Their methodhas shown significant improvement for both: wordsense disambiguation in machine translation anddata recovery tasks.
(Pekar et al 2006) em-ployed the nearest neighbor variety of the previ-ous approach to extract translation equivalents forlow frequency words from comparable corpora.They used a distance-based averaging techniquefor smoothing (Dagan et al 1999).
Their methodyielded a significant improvement in relation tolow frequency words.Starting from the assumption that smoothingimproves the accuracy of the model as a whole(Chen and Goodman, 1999), we believe thatsmoothed context vectors should lead to bet-ter performance for bilingual terminology extrac-tion from comparable corpora.
In this work wecarry out an empirical comparison of the mostwidely-used smoothing techniques, including ad-ditive smoothing (Lidstone, 1920), Good-Turingestimate (Good, 1953), Jelinek-Mercer (Mercer,1980), Katz (Katz, 1987) and kneser-Ney smooth-ing (Kneser and Ney, 1995).
Unlike (Pekar et al2006), the present work does not investigate un-seen words.
We only concentrate on observedcooccurrences.
We believe it constitutes the mostsystematic comparison made so far with differ-ent smoothing techniques for aligning translationequivalents from comparable corpora.
We showthat using smoothing as a pre-processing step ofthe standard approach, leads to significant im-provement even without considering unseen cooc-currences.In the remainder of this paper, we present inSection 2, the different smoothing techniques.
Thesteps of the standard approach and our extendedmethod are then described in Section 3.
Section4 describes the experimental setup and our re-sources.
Section 5 presents the experiments andcomments on several results.
We finally discussthe results in Section 6 and conclude in Section 7.2 Smoothing TechniquesSmoothing describes techniques for adjusting themaximum likelihood estimate of probabilities toreduce more accurate probabilities.
The smooth-ing techniques tend to make distributions moreuniform.
In this section we present the mostwidely used techniques.2.1 Additive SmoothingThe Laplace estimator or the additive smoothing(Lidstone, 1920; Johnson, 1932; Jeffreys, 1948)is one of the simplest types of smoothing.
Itsprinciple is to estimate probabilities P assumingthat each unseen word type actually occurred once.Then, if we have N events and V possible wordsinstead of :P(w) = occ(w)N (1)We estimate:Paddone(w) =occ(w) + 1N + V (2)Applying Laplace estimation to word?s cooc-currence suppose that : if two words cooccur to-gether n times in a corpus, they can cooccur to-gether (n + 1) times.
According to the maximumlikelihood estimation (MLE):P(wi+1|wi) =C(wi,wi+1)C(wi)(3)Laplace smoothing:P?
(wi+1|wi) =C(wi,wi+1) + 1C(wi) + V(4)Several disadvantages emanate from thismethod:1.
The probability of frequent n-grams is under-estimated.2.
The probability of rare or unseen n-grams isoverestimated.253.
All the unseen n-grams are smoothed in thesame way.4.
Too much probability mass is shifted towardsunseen n-grams.One improvement is to use smaller added countfollowing the equation below:P?
(wi+1|wi) =?
+ C(wi,wi+1)?|V|+ C(wi)(5)with ?
?
]0, 1].2.2 Good-Turing EstimatorThe Good-Turing estimator (Good, 1953) pro-vides another way to smooth probabilities.
It statesthat for any n-gram that occurs r times, we shouldpretend that it occurs r?
times.
The Good-Turingestimators use the count of things you have seenonce to help estimate the count of things you havenever seen.
In order to compute the frequency ofwords, we need to compute Nc, the number ofevents that occur c times (assuming that all itemsare binomially distributed).
Let Nr be the num-ber of items that occur r times.
Nr can be used toprovide a better estimate of r, given the binomialdistribution.
The adjusted frequency r?
is then:r?
= (r + 1)Nr+1Nr(6)2.3 Jelinek-Mercer SmoothingAs one alternative to missing n-grams, useful in-formation can be provided by the corresponding(n-1)-gram probability estimate.
A simple methodfor combining the information from lower-ordern-gram in estimating higher-order probabilities islinear interpolation (Mercer, 1980).
The equationof linear interpolation is given below:Pint(wi+1|wi) = ?P(wi+1|wi) + (1?
?
)P(wi) (7)?
is the confidence weight for the longer n-gram.
In general, ?
is learned from a held-outcorpus.
It is useful to interpolate higher-order n-gram models with lower-order n-gram models, be-cause when there is insufficient data to estimate aprobability in the higher order model, the lower-order model can often provide useful information.Instead of the cooccurrence counts, we used theGood-Turing estimator in the linear interpolationas follows:c?int(wi+1|wi) = ?c?
(wi+1|wi) + (1?
?
)P(wi) (8)2.4 Katz Smoothing(Katz, 1987) extends the intuitions of Good-Turing estimate by adding the combination ofhigher-order models with lower-order models.
Fora bigram wii?1 with count r = c(wii?1), its cor-rected count is given by the equation:ckatz(wii?1) ={r?
if r > 0?
(wi?1)PML(wi) if r = 0 (9)and:?
(wi?1) =1??wi:c(wii?1)>0Pkatz(wii?1)1?
?wi:c(wii?1)>0PML(wi?1)(10)According to (Katz, 1987), the general dis-counted estimate c?
of Good-Turing is not used forall counts c. Large counts where c > k for somethreshold k are assumed to be reliable.
(Katz,1987) suggests k = 5.
Thus, we define c?
= cfor c > k, and:c?
=(c + 1)Nc+1Nc ?
c(k+1)Nk+1N11?
(k+1)Nk+1N1(11)2.5 Kneser-Ney SmoothingKneser-Ney have introduced an extension of ab-solute discounting (Kneser and Ney, 1995).
Theestimate of the higher-order distribution is createdby subtracting a fixed discount D from each non-zero count.
The difference with the absolute dis-counting smoothing resides in the estimate of thelower-order distribution as shown in the followingequation:r =????
?Max(c(wii?n+1)?D,0)?wic(wii?n+1)if c(wii?n+1) > 0?
(wi?1i?n+1)Pkn(wi|wi?1i?n+2) if c(wii?n+1) = 0(12)where r = Pkn(wi|wi?1i?n+1) and ?
(wi?1i?n+1) ischosen to make the distribution sum to 1 (Chenand Goodman, 1999).3 MethodsIn this section we first introduce the different stepsof the standard approach, then we present our ex-tended approach that makes use of smoothing as anew step in the process of the standard approach.263.1 Standard ApproachThe main idea for identifying translations of termsin comparable corpora relies on the distributionalhypothesis 1 that has been extended to the bilin-gual scenario (Fung, 1998; Rapp, 1999).
If manyvariants of the standard approach have been pro-posed (Chiao and Zweigenbaum, 2002; Herve?De?jean and Gaussier, 2002; Morin et al 2007;Gamallo, 2008)[among others], they mainly differin the way they implement each step and define itsparameters.
The standard approach can be carriedout as follows:Step 1 For a source word to translate wsi , we firstbuild its context vector vwsi .
The vector vwsicontains all the words that cooccur with wsiwithin windows of n words.
Lets denote bycooc(wsi , wsj ) the cooccurrence value of wsiand a given word of its context wsj .
The pro-cess of building context vectors is repeatedfor all the words of the target language.Step 2 An association measure such as the point-wise mutual information (Fano, 1961), thelog-likelihood (Dunning, 1993) or the dis-counted odds-ratio (Laroche and Langlais,2010) is used to score the strength of corre-lation between a word and all the words of itscontext vector.Step 3 The context vector vwsi is projected intothe target language vtwsi .
Each wordwsj of vwsiis translated with the help of a bilingual dic-tionary D. If wsj is not present in D, wsj isdiscarded.
Whenever the bilingual dictionaryprovides several translations for a word, allthe entries are considered but weighted ac-cording to their frequency in the target lan-guage (Morin et al 2007).Step 4 A similarity measure is used to score eachtarget word wti , in the target language withrespect to the translated context vector, vtwsi .Usual measures of vector similarity includethe cosine similarity (Salton and Lesk, 1968)or the weighted Jaccard index (WJ) (Grefen-stette, 1994) for instance.
The candidatetranslations of the word wsi are the targetwords ranked following the similarity score.1words with similar meaning tend to occur in similar con-texts3.2 Extended ApproachWe aim at investigating the impact of differ-ent smoothing techniques for the task of bilin-gual terminology extraction from comparable cor-pora.
Starting from the assumption that wordcooccurrences are not reliable especially for smallcorpora (Zipf, 1949; Evert and Baroni, 2007)and that smoothing is usually used to counter-act this problem, we apply smoothing as a pre-processing step of the standard approach.
Eachcooc(wsi , wsj ) is smoothed according to the tech-niques described in Section 2.
The smoothedcooccurrence cooc?
(wsi , wsj ) is then used for cal-culating the association measure between wsi andwsj and so on (steps 2, 3 and 4 of the standard ap-proach are unchanged).
We chose not to studythe prediction of unseen cooccurrences.
The lat-ter has been carried out successfully by (Pekaret al 2006).
We concentrate on the evaluationof smoothing techniques of known cooccurrencesand their effect according to different associationand similarity measures.4 Experimental SetupIn order to evaluate the smoothing techniques, sev-eral resources and parameters are needed.
Wepresent hereafter the experiment data and the pa-rameters of the standard approach.4.1 Corpus DataThe experiments have been carried out on twoEnglish-French comparable corpora.
A special-ized corpus of 530,000 words from the medicaldomain within the sub-domain of ?breast cancer?and a specialize corpus from the domain of ?wind-energy?
of 300,000 words.
The two bilingual cor-pora have been normalized through the follow-ing linguistic pre-processing steps: tokenization,part-of-speech tagging, and lemmatization.
Thefunction words have been removed and the wordsoccurring once (i.e.
hapax) in the French andthe English parts have been discarded.
For thebreast cancer corpus, we have selected the doc-uments from the Elsevier website2 in order toobtain an English-French specialized comparablecorpora.
We have automatically selected the doc-uments published between 2001 and 2008 wherethe title or the keywords contain the term ?cancerdu sein?
in French and ?breast cancer?
in English.We collected 130 documents in French and 118 in2www.elsevier.com27English.
As summarised in Table 1, The compara-ble corpora comprised about 6631 distinct wordsin French and 8221 in English.
For the wind en-ergy corpus, we used the Babook crawler (Groc,2011) to collect documents in French and Englishfrom the web.
We could only obtain 50 documentsin French and 65 in English.
As the documentswere collected from different websites accordingto some keywords of the domain, this corpus ismore noisy and not well structured compared tothe breast cancer corpus.
The wind-energy corpuscomprised about 5606 distinct words in Frenchand 6081 in English.Breast cancer Wind energyTokensS 527,705 307,996TokensT 531,626 314,551|S| 8,221 6,081|T | 6,631 5,606Table 1: Corpus size4.2 DictionaryIn our experiments we used the French-Englishbilingual dictionary ELRA-M0033 of about200,000 entries3.
It contains, after linguistic pre-processing steps and projection on both corporafewer than 4000 single words.
The details aregiven in Table 2.Breast cancer Wind energy|ELRAS | 3,573 3,459|ELRAT | 3,670 3,326Table 2: Dictionary coverage4.3 Reference ListsIn bilingual terminology extraction from special-ized comparable corpora, the terminology refer-ence list required to evaluate the performanceof the alignment programs is often composed of100 single-word terms (SWTs) (180 SWTs in(Herve?
De?jean and Gaussier, 2002), 95 SWTs in(Chiao and Zweigenbaum, 2002), and 100 SWTsin (Daille and Morin, 2005)).
To build our ref-erence lists, we selected only the French/Englishpair of SWTs which occur more than five times ineach part of the comparable corpus.
As a result3ELRA dictionary has been created by Sciper in the Tech-nolangue/Euradic projectof filtering, 321 French/English SWTs were ex-tracted (from the UMLS4 meta-thesaurus.)
for thebreast cancer corpus, and 100 pairs for the wind-energy corpus.4.4 Evaluation MeasureThree major parameters need to be set to thestandard approach, namely the similarity measure,the association measure defining the entry vec-tors and the size of the window used to build thecontext vectors.
(Laroche and Langlais, 2010)carried out a complete study of the influence ofthese parameters on the quality of bilingual align-ment.
As a similarity measure, we chose to useWeighted Jaccard Index (Grefenstette, 1994) andCosine similarity (Salton and Lesk, 1968).
The en-tries of the context vectors were determined by thelog-likelihood (Dunning, 1993), mutual informa-tion (Fano, 1961) and the discounted Odds-ratio(Laroche and Langlais, 2010).
We also chose a 7-window size.
Other combinations of parameterswere assessed but the previous parameters turnedout to give the best performance.
We note that?Top k?
means that the correct translation of agiven word is present in the k first candidates ofthe list returned by the standard approach.
We usealso the mean average precision MAP (Manninget al 2008) which represents the quality of thesystem.MAP (Q) = 1|Q||Q|?i=11mik?mi=1P (Rik) (13)where |Q| is the number of terms to be trans-lated, mi is the number of reference translationsfor the ith term (always 1 in our case), and P (Rik)is 0 if the reference translation is not found for theith term or 1/r if it is (r is the rank of the referencetranslation in the translation candidates).4.5 BaselineThe baseline in our experiments is the standardapproach (Fung, 1998) without any smoothing ofthe data.
The standard approach is often used forcomparison (Pekar et al 2006; Gamallo, 2008;Prochasson and Morin, 2009), etc.4.6 Training Data SetSome smoothing techniques such as the Good-Turing estimators need a large training corpus to4http://www.nlm.nih.gov/research/umls28estimate the adjusted cooccurrences.
For that pur-pose, we chose a training general corpus of 10 mil-lion words.
We selected the documents publishedin 1994 from the ?Los Angeles Times/Le Monde?newspapers.5 Experiments and ResultsWe conducted a set of three experiments on twospecialized comparable corpora.
We carried out acomparison between the standard approach (SA)and the smoothing techniques presented in Sec-tion 2 namely : additive smoothing (Add1), Good-Turing smoothing (GT), the Jelinek-Mercer tech-nique (JM), the Katz-Backoff (Katz) and kneser-Ney smoothing (Kney).
Experiment 1 shows theresults for the breast cancer corpus.
Experiment 2shows the results for the wind energy corpus andfinally experiment 3 presents a comparison of thebest configurations on both corpora.5.1 Experiment 1Table 3 shows the results of the experiments onthe breast cancer corpus.
The first observationconcerns the standard approach (SA).
The bestresults are obtained using the Log-Jac parame-ters with a MAP = 27.9%.
We can also no-tice that for this configuration, only the Addi-tive smoothing significantly improves the perfor-mance of the standard approach with a MAP =30.6%.
The other smoothing techniques even de-grade the results.
The second observation con-cerns the Odds-Cos parameters where none ofthe smoothing techniques significantly improvedthe performance of the baseline (SA).
AlthoughGood-Turing and Katz-Backoff smoothing giveslightly better results with respectively a MAP =25.2 % and MAP = 25.3 %, these results are notsignificant.
The most notable result concerns thePMI-COS parameters.
We can notice that four ofthe five smoothing techniques improve the perfor-mance of the baseline.
The best smoothing is theJelinek-Mercer technique which reaches a MAP =29.5% and improves the Top1 precision of 6% andthe Top10 precision of 10.3%.5.2 Experiment 2Table 4 shows the results of the experiments onthe wind energy corpus.
Generally the resultsexhibit the same behaviour as the previous ex-periment.
The best results of the standard ap-proach are obtained using the Log-Jac parametersSA Add1 GT JM Katz KneyP1 15.5 17.1 18.7 21.5 18.7 05.3PMI-CosP5 31.1 32.7 32.0 38.3 33.9 13.4P10 34.5 37.0 37.0 44.8 38.0 15.2MAP 22.6 24.8 25.6 29.5 25.9 09.1P1 15.8 16.1 16.8 14.6 17.1 09.0Odds-CosP5 34.8 33.6 34.2 33.0 33.9 19.6P10 40.4 41.7 39.8 38.3 40.1 25.2MAP 24.8 24.4 25.2 23.3 25.3 14.1P1 20.2 22.4 14.6 14.6 14.6 16.2Log-JacP5 35.8 40.5 27.7 26.7 26.7 29.9P10 42.6 44.2 34.2 33.3 33.0 33.9MAP 27.9 30.6 21.4 21.2 21.2 22.9Table 3: Results of the experiments on the ?Breastcancer?
corpus (except the Odds-Cos configura-tion, the improvements indicate a significance atthe 0.05 level using the Student t-test).SA Add1 GT JM Katz KneyP1 07.0 14.0 14.0 21.0 16.0 09.0PMI-CosP5 27.0 32.0 31.0 37.0 30.0 17.0P10 37.0 42.0 43.0 51.0 44.0 28.0MAP 17.8 23.6 22.9 30.1 24.2 14.1P1 12.0 17.0 12.0 12.0 12.0 06.0Odds-CosP5 31.0 35.0 31.0 32.0 28.0 16.0P10 38.0 44.0 36.0 39.0 35.0 21.0MAP 21.8 26.5 19.8 20.8 19.7 11.1P1 17.0 22.0 13.0 13.0 13.0 14.0Log-JacP5 36.0 38.0 27.0 27.0 27.0 29.0P10 42.0 50.0 37.0 38.0 38.0 39.0MAP 25.7 29.7 20.5 21.3 21.3 22.9Table 4: Results of the experiments on the ?WindEnergy?
corpus (except the Odds-Cos configura-tion, the improvements indicate a significance atthe 0.05 level using the Student t-test).with a MAP = 25.7%.
Here also, only the Ad-ditive smoothing significantly improves the per-formance of the standard approach with a MAP= 39.7%.
The other smoothing techniques alsodegrade the results.
About the Odds-Cos param-eters, except the additive smoothing, here againnone of the smoothing techniques significantly im-proved the performance of the baseline.
Finallythe most remarkable result still concerns the PMI-COS parameters where the same four of the fivesmoothing techniques improve the performance ofthe baseline.
The best smoothing is the Jelinek-Mercer technique which reaches a MAP = 30.1%and improves the Top1 and and the Top10 preci-sions by 14.0%.295.3 Experiment 3In this experiment, we would like to investigatewhether the smoothing techniques are more effi-cient for frequent translation equivalents or lessfrequent ones.
For that purpose, we split the breastcancer reference list of 321 entries into two setsof translation pairs.
A set of 133 frequent pairsnamed : High-test set and a set of 188 less fre-quent pairs called Low-test set.
The initial refer-ence list of 321 pairs is the Full-test set.
We con-sider frequent pairs those of a frequency higherthan 100.
We chose to analyse the two configu-rations that provided the best performance that is :Log-Jac and Pmi-Cos parameters according to theFull-test, High-test and Low-test sets.Figure 1 shows the results using the Log-Jac configuration.
We can see that the additivesmoothing always outperforms the standard ap-proach for all the test sets.
The other smoothingtechniques are always under the baseline and be-have approximately the same way.
Figure 2 showsthe results using the PMI-COS configuration.
Wecan see that except the Kneser-Ney smoothing, allthe smoothing techniques outperform the standardapproach for all the test sets.
We can also noticethat the Jelinek-Mercer smoothing improves morenotably the High-test set.6 DiscussionSmoothing techniques are often evaluated on theirability to predict unseen n-grams.
In our exper-iments we only focused on smoothing observedcooccurrences of context vectors.
Hence, the pre-vious evaluations of smoothing techniques maynot always be consistent with our findings.
Thisis for example the case for the additive smooth-ing technique.
The latter which is described asa poor estimator in statistical NLP, turns out toperform well when associated with the Log-Jacparameters.
This is because we did not considerunseen cooccurences which are over estimated bythe Add-one smoothing.
Obviously, we can imag-ine that adding one to all unobserved cooccur-rences would not make sense and would certainlydegrade the results.
Except the add-one smooth-ing, none of the other algorithms reached goodresults when associated to the Log-Jac configu-ration.
This is certainly related to the propertiesof the log-likelihood association measure.
Addi-tive smoothing has been used to address the prob-lem of rare words aligning to too many words(Moore, 2004).
At each iteration of the standardExpectation-Maximization (EM) procedure all thetranslation probability estimates are smoothed byadding virtual counts to uniform probability dis-tribution over all target words.
Here also additivesmoothing has shown interesting results.
Accord-ing to these findings, we can consider the addi-tive smoothing as an appropriate technique for ourtask.Concerning the Odds-Cos parameters, althoughthere have been slight improvements in the add-one algorithm, smoothing techniques have showndisappointing results.
Here again the Odds-ratioassociation measure seems to be incompatiblewith re-estimating small cooccurrences.
More in-vestigations are certainly needed to highlight thereasons for this poor performance.
It seems thatsmoothing techniques based on discounting doesnot fit well with association measures based oncontingency table.
The most noticeable improve-ment concerns the PMI-Cos configurations.
Ex-cept Kneser-Ney smoothing, all the other tech-niques showed better performance than the stan-dard approach.
According to the results, point-wise mutual information performs better withsmoothing techniques especially with the linearinterpolation of Jelinek-Mercer method that com-bines high-order (cooccurrences) and low-order(unigrams) counts of the Good-Turing estima-tions.
Jelinek-Mercer smoothing counteracts thedisadvantage of the point-wise mutual informationwhich consists of over estimating less frequentwords.
This latter weakness is corrected first bythe Good-Turing estimators and then by consider-ing the low order counts.
The best performancewas obtained with ?
= 0.5.Smoothing techniques attempt to improve theaccuracy of the model as a whole.
This particu-larity has been confirmed by the third experimentwhere we noticed the smoothing improvements forboth reference lists, that is the High-test and Low-test sets.
This latter experiment has shown thatsmoothing observed cooccurrences is useful for allfrequency ranges.
The difference of precision be-tween the two test lists can be explained by the factthat less frequent words are harder to translate.In statistical NLP, smoothing techniques for n-gram models have been addressed in a numberof studies (Chen and Goodman, 1999).
The ad-301 5 10 15 20 25 30 35 40 45 5010203040506070TopPrecision(%)SAAdd1GTJMKatzKney(a)1 5 10 15 20 25 30 35 40 45 5020304050607080TopPrecision(%)SAAdd1GTJMKatzKney(b)1 5 10 15 20 25 30 35 40 45 5001020304050TopPrecision(%)SAAdd1GTJMKatzKney(c)Figure 1: A set of three figures on the breast cancer corpus for the Log-Jac configuration : (a) Full-testset ; (b) High-test set; and (c) Low-test set.1 5 10 15 20 25 30 35 40 45 50010203040506070TopPrecision(%)SAAdd1GTJMKatzKney(a)1 5 10 15 20 25 30 35 40 45 500102030405060708090TopPrecision(%)SAAdd1GTJMKatzKney(b)1 5 10 15 20 25 30 35 40 45 5001020304050TopPrecision(%)SAAdd1GTJMKatzKney(c)Figure 2: A set of three figures on the breast cancer corpus for the PMI-COS configuration : (a) Full-testset ; (b) High-test set; and (c) Low-test set.ditive smoothing that performs rather poorly hasshown good results in our evaluation.
The Good-Turing estimate which is not used in isolationforms the basis of later techniques such as Back-off or Jelinek-Mercer smoothing, two techniquesthat generally work well.
The good performanceof Katz and JM on the PMI-Cos configura-tion was expected.
The reason is that these twomethods have used the Good-Turing estimatorswhich also achieved good performances in ourexperiments.
Concerning the Kneser-Ney algo-rithm, surprisingly this performed poorly in ourexperiments while it is known to be one of thebest smoothing techniques.
Discounting a fixedamount in all counts of observed cooccurrencesdegrades the results in our data set.
We also im-plemented the modified Knener-ney method (notpresented in this paper) but this also performedpoorly.
We conclude that discounting is not anappropriate method for observed cooccurrences.Especially for point-wise mutual information thatover-estimates low frequencies, hense, discount-ing low cooccurrences will increase this over-estimation.7 ConclusionIn this paper, we have described and comparedthe most widely-used smoothing techniques forthe task of bilingual lexicon extraction from com-parable corpora.
Regarding the empirical resultsof our proposition, performance of smoothing onour dataset was better than the baseline for theAdd-One smoothing combined with the Log-Jacparameters and all smoothing techniques exceptthe Kneser-ney for the Pmi-Cos parameters.
Ourfindings thus lend support to the hypothesis thata re-estimation process of word cooccurrence in asmall specialized comparable corpora is an appro-priate way to improve the accuracy of the standardapproach.31AcknowledgmentsThe research leading to these results has re-ceived funding from the French National ResearchAgency under grant ANR-12-CORD-0020.ReferencesBrown, P. F., Pietra, S. D., Pietra, V. J. D., and Mercer,R.
L. (1991).
Word-sense disambiguation using sta-tistical methods.
In Proceedings of the 29th AnnualMeeting of the Association for Computational Lin-guistics (ACL?91), pages 264?270, California, USA.Brown, P. F., Pietra, V. J. D., de Souza, P. V., Lai, J. C.,and Mercer, R. L. (1992).
Class-based n-gram mod-els of natural language.
Computational Linguistics,18(4):467?479.Chen, S. F. and Goodman, J.
(1999).
An empiricalstudy of smoothing techniques for language model-ing.
Computer Speech & Language, 13(4):359?393.Chiao, Y.-C. and Zweigenbaum, P. (2002).
Look-ing for candidate translational equivalents in spe-cialized, comparable corpora.
In Proceedings ofthe 19th International Conference on ComputationalLinguistics (COLING?02), pages 1208?1212, Tapei,Taiwan.Dagan, I., Lee, L., and Pereira, F. C. N. (1999).Similarity-based models of word cooccurrenceprobabilities.
Machine Learning, 34(1-3):43?69.Dagan, I., Marcus, S., and Markovitch, S. (1993).
Con-textual word similarity and estimation from sparsedata.
In Proceedings of the 31ST Annual Meet-ing of the Association for Computational Linguistics(ACL?93), pages 164?171, Ohio, USA.Daille, B. and Morin, E. (2005).
French-English Ter-minology Extraction from Comparable Corpora.
InProceedings of the 2nd International Joint Confer-ence on Natural Language Processing (IJCLNP?05),pages 707?718, Jeju Island, Korea.Dunning, T. (1993).
Accurate Methods for the Statis-tics of Surprise and Coincidence.
ComputationalLinguistics, 19(1):61?74.Evert, S. (2005).
The statistics of word cooccurrences :word pairs and collocations.
PhD thesis, Universityof Stuttgart, Holzgartenstr.
16, 70174 Stuttgart.Evert, S. and Baroni, M. (2007).
zipfr: Word frequencymodeling in r. In Proceedings of the 45th AnnualMeeting of the Association for Computational Lin-guistics (ACL?07), Prague, Czech Republic.Fano, R. M. (1961).
Transmission of Information: AStatistical Theory of Communications.
MIT Press,Cambridge, MA, USA.Fung, P. (1998).
A Statistical View on Bilingual Lex-icon Extraction: From Parallel Corpora to Non-parallel Corpora.
In Proceedings of the 3rd Confer-ence of the Association for Machine Translation inthe Americas (AMTA?98), pages 1?16, Langhorne,PA, USA.Gamallo, O.
(2008).
Evaluating two different meth-ods for the task of extracting bilingual lexicons fromcomparable corpora.
In Proceedings of LREC 2008Workshop on Comparable Corpora (LREC?08),pages 19?26, Marrakech, Marroco.Good, I. J.
(1953).
The population frequencies ofspecies and the estimation of population parameters.Biometrika, 40:16?264.Grefenstette, G. (1994).
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Publisher,Boston, MA, USA.Groc, C. D. (2011).
Babouk : Focused Web Crawl-ing for Corpus Compilation and Automatic Termi-nology Extraction.
In Proceedings of The IEEE-WICACM International Conferences on Web Intel-ligence, pages 497?498, Lyon, France.Herve?
De?jean and Gaussier, E?.
(2002).
Une nouvelleapproche a` l?extraction de lexiques bilingues a` partirde corpus comparables.
Lexicometrica, Alignementlexical dans les corpus multilingues, pages 1?22.Jeffreys, H. (1948).
Theory of Probability.
ClarendonPress, Oxford.
2nd edn Section 3.23.Johnson, W. (1932).
Probability: the deductive and in-ductive problems.
Mind, 41(164):409?423.Katz, S. M. (1987).
Estimation of probabilities fromsparse data for the language model component of aspeech recognizer.
IEEE Transactions on Acoustics,Speech and Signal Processing, 35(3):400?401.Kneser, R. and Ney, H. (1995).
Improved backing-off for M-gram language modeling.
In Proceedingsof the 20th International Conference on Acoustics,Speech, and Signal Processing (ICASSP?95), pages181?184, Michigan, USA.Laroche, A. and Langlais, P. (2010).
Revisit-ing Context-based Projection Methods for Term-Translation Spotting in Comparable Corpora.
InProceedings of the 23rd International Conferenceon Computational Linguistics (COLING?10), pages617?625, Beijing, China.Lidstone, G. J.
(1920).
Note on the general case of thebayes-laplace formula for inductive or a posterioriprobabilities.
Transactions of the Faculty of Actuar-ies, 8:182?192.Maarek, Y. S. and Smadja, F. A.
(1989).
Full textindexing based on lexical relations an application:Software libraries.
In SIGIR, pages 198?206, Mas-sachusetts, USA.32Manning, D. C., Raghavan, P., and Schu?tze, H. (2008).Introduction to information retrieval.
CambridgeUniversity Press.Mercer, L. ; Jelinek, F. (1980).
Interpolated estimationof markov source parameters from sparse data.
InWorkshop on pattern recognition in Practice, Ams-terdam, The Netherlands.Moore, R. C. (2004).
Improving ibm word alignmentmodel 1.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics(ACL?04), pages 518?525, Barcelona, Spain.Morin, E., Daille, B., Takeuchi, K., and Kageura, K.(2007).
Bilingual Terminology Mining ?
UsingBrain, not brawn comparable corpora.
In Proceed-ings of the 45th Annual Meeting of the Associationfor Computational Linguistics (ACL?07), pages 664?671, Prague, Czech Republic.Pekar, V., Mitkov, R., Blagoev, D., and Mulloni,A.
(2006).
Finding translations for low-frequencywords in comparable corpora.
Machine Translation,20(4):247?266.Pereira, F. C. N., Tishby, N., and Lee, L. (1993).
Dis-tributional clustering of english words.
In Proceed-ings of the 31ST Annual Meeting of the Associationfor Computational Linguistics (ACL?93), pages 183?190, Ohio, USA.Prochasson, E. and Morin, E. (2009).
Anchor pointsfor bilingual extraction from small specialized com-parable corpora.
TAL, 50(1):283?304.Rapp, R. (1999).
Automatic Identification of WordTranslations from Unrelated English and GermanCorpora.
In Proceedings of the 37th Annual Meet-ing of the Association for Computational Linguistics(ACL?99), pages 519?526, College Park, MD, USA.Salton, G. and Lesk, M. E. (1968).
Computer evalua-tion of indexing and text processing.
Journal of theAssociation for Computational Machinery, 15(1):8?36.Zipf, G. K. (1949).
Human Behaviour and the Princi-ple of Least Effort: an Introduction to Human Ecol-ogy.
Addison-Wesley.33
