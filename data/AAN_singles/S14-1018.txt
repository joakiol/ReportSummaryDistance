Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 141?150,Dublin, Ireland, August 23-24 2014.Cognitive Compositional Semantics using Continuation DependenciesWilliam SchulerDepartment of LinguisticsThe Ohio State Universityschuler@ling.osu.eduAdam WheelerDepartment of LinguisticsThe Ohio State Universitywheeler@ling.osu.eduAbstractThis paper describes a graphical semanticrepresentation based on bottom-up ?con-tinuation?
dependencies which has the im-portant property that its vertices define ausable set of discourse referents in work-ing memory even in contexts involvingconjunction in the scope of quantifiers.
Anevaluation on an existing quantifier scopedisambiguation task shows that non-localcontinuation dependencies can be as reli-ably learned from annotated data as repre-sentations used in a state-of-the-art quanti-fier scope resolver, suggesting that contin-uation dependencies may provide a naturalrepresentation for scope information.1 IntroductionIt is now fairly well established that at least shal-low semantic interpretation informs parsing deci-sions in human sentence processing (Tanenhaus etal., 1995; Brown-Schmidt et al., 2002), and re-cent evidence points to incremental processing ofquantifier implicatures as well (Degen and Tanen-haus, 2011).
This may indicate that inferencesabout the meaning of quantifiers are processed di-rectly in working memory.
Human working mem-ory is widely assumed to store events (includ-ing linguistic events) as re-usable activation-basedstates, connected by a durable but rapidly mutableweight-based memory of cued associations (Marr,1971; Anderson et al., 1977; Murdock, 1982; Mc-Clelland et al., 1995; Howard and Kahana, 2002).Complex dependency structures can therefore bestored in this associative memory as graphs, withstates as vertices and cued associations as directededges (e.g.
Kintsch, 1988).
This kind of represen-tation is necessary to formulate and evaluate algo-rithmic claims (Marr, 1982) about cued associa-tions and working memory use in human sentenceprocessing (e.g.
van Schijndel and Schuler, 2013).But accounting for syntax and semantics in thisway must be done carefully in order to preservelinguistically important distinctions.
For example,positing spurious local dependencies in filler-gapconstructions can lead to missed integrations ofdependency structure in incremental processing,resulting in weaker model fitting (van Schijndel etal., 2013).
Similar care may be necessary in casesof dependencies arising from anaphoric corefer-ence or quantifier scope.Unfortunately, most existing theories of compo-sitional semantics (Montague, 1973; Barwise andCooper, 1981; Bos, 1996; Baldridge and Kruijff,2002; Koller, 2004; Copestake et al., 2005) aredefined at the computational level (Marr, 1982),employing beta reduction over complete or under-specified lambda calculus expressions as a precisedescription of the language processing task to bemodeled, not at the algorithmic level, as a modelof human language processing itself.
The struc-tured expressions these theories generate are notintended to represent re-usable referential statesof the sort that could be modeled in current theo-ries of associative memory.
As such, it should notbe surprising that structural adaptations of lambdacalculus expressions as referential states exhibit anumber of apparent deficiencies:First, representations based on lambda calculusexpressions lack topologically distinguishable ref-erents for sets defined in the context of outscop-ing quantifiers.
For example, a structural adapta-tion of a lambda calculus expression for the sen-tence Every line contains two numbers, shown inFigure 1a (adapted from Koller, 2004), containsreferents for the set of all document lines (sL) andfor the set of all numbers (sN) which can be iden-tified by cued associations to predicate constantslike Number, but it is not clear how a referent forthe set of numbers in document lines can be dis-tinguished from a referent for the set of numbers141a)pLEvery0sL?0dL1eLLine021s?L?0d?L1pNTwo0sN?0dN1eNNumber021s?N?0d?N1eCContain022221112b)pLEvery0sL?0dL1eLLine021s?L?0d?L1pApSA0sS?0dS1eSSpace021s?S?0d?S1eBBeginsWith0221And0pNTwo0sN?0dN1eNNumber021s?N?0d?N1eCContain0222221112 112c)(Every pLsLs?L) ?
(Set sLdLeL) ?
(Line eLdL) ?
(Set s?Ld?LpN) ?
(Two pNsNs?N) ?
(Set sNdNeN) ?
(Number eNdN) ?
(Set s?Nd?NeC) ?
(Contain eCd?Ld?N)Figure 1: Semantic dependency graph in a ?direct?
(top-down) style, adapted from a disambiguated rep-resentation of Koller (2004), excluding quantifiers over eventualities.
The semantic dependency structurefor the sentence Every line contains two numbers (a), with flat logical form (c), is not a subgraph of thesemantic dependency structure for Every line begins with a space and contains two numbers (b), becausethe structure is interrupted by the explicit conjunction predicate ?And?.in each document line (s?N) using local topologicalfeatures of the dependency graph, as would be re-quired to accurately recall assertions about total oraverage quantities of numbers in document lines.1Second, graphs based on traditional lambdacalculus representations do not model conjunctsas subgraphs of conjunctions.
For example, thegraphical representation of the sentence Every line1This graph matching can be implemented in a vectorialmodel of associative memory by comparing the (e.g.
cosine)similarity of superposed vectors resulting from cueing in-coming and outgoing dependencies with all possible labelsin increasingly longer paths from one or more constant vec-tor states (e.g.
vectors for predicate constants).
This graphmatching does not necessarily preclude the introduction ofmonotonicity constraints from matched quantifiers.
For ex-ample, More than two perl scripts work, can entail Morethan two scripts work, using a subgraph in the first argu-ment, but Fewer than two scripts work, can entail Fewer thantwo perl scripts work, using a supergraph in the first argu-ment.
This consideration is similar to those observed in rep-resentations based on natural logic (MacCartney and Man-ning, 2009) which also uses low-level matching to performsome kinds of inference, but representations based on naturallogic typically exclude other forms of inference, whereas thepresent model does not.This matching also assumes properties of nuclear scopevariables are inherited from associated restrictor variables,e.g.
through a set of dependencies from nuclear scope setsto restrictor sets not shown in the figure.
This assumptionwill be revisited in Section 3.begins with a space and contains two numbersshown in Figure 1b does not contain the graphicalrepresentation of the sentence Every line containstwo numbers shown in Figure 1a as a connectedsubgraph.
Although one might expect a queryabout a conjunct to be directly answerable froma knowledge base containing the conjoined repre-sentation, the pattern of dependencies that makeup the conjunct in a graphical representation of alambda calculus expression does not match thosein the larger conjunction.Finally, representations based on lambda calcu-lus expressions contain vertices that do not seemto correspond to viable discourse referents.
Forexample, following the sentence Every line con-tains two numbers, using the lambda expressionshown in Figure 1b, dLmay serve as a referent ofit in but it has only one underscore, sNmay serveas a referent of they in but they are not negative,eCmay serve as a referent of that in but that wasbefore it was edited, and pLmay serve as a ref-erent of that in but the compiler doesn?t enforcethat, but it is not clear what if anything would nat-urally refer to the internal conjunction pA. Predi-cations over such conjunctions (e.g.
Kim believesthat every line begins with a space and contains142two numbers) are usually predicated at the outerproposition pL, and in any case do not have truthvalues that are independent of the same predica-tion at each conjunct.
One of the goals of MinimalRecursion Semantics (Copestake et al., 2005) wasto eliminate similar kinds of superfluous conjunc-tion structure.Fortunately, lambda calculus expressions likethose shown in Figure 1 are not the only way torepresent compositional semantics of sentences.This paper defines a graphical semantic depen-dency representation that can be translated intolambda calculus, but has the important propertythat its vertices define a usable set of discoursereferents in working memory even in contexts in-volving conjunction in the scope of quantifiers.It does this by reversing the direction of de-pendencies from parent-to-child subsumption ina lambda-calculus tree to a representation sim-ilar to the inside-out structure of function def-initions in a continuation-passing style (Barker,2002; Shan and Barker, 2006)2so that sets are de-fined in terms of their context, and explicit ?And?predicates are no longer required, leaving noth-ing to get in the way of an exact pattern match.3The learnability of the non-local continuation de-pendencies involved in this representation is thenevaluated on an existing quantifier scope disam-biguation task using a dependency-based statisti-cal scope resolver, with results comparable to astate-of-the-art unrestricted graph-based quantifierscope resolver (Manshadi et al., 2013).2 Continuation DependenciesThis paper explores the use of a bottom-up depen-dency representation, inspired by the inside-outstructure of function definitions in a continuation-passing style (Barker, 2002; Shan and Barker,2006), which creates discourse referents for setsthat are associated with particular scoping con-texts.
This dependency representation preservesthe propositions, sets, eventualities, and ordinary2This representation also has much in common with gen-eralized Skolem terms of Steedman (2012), which also repre-sent dependencies to outscoped terms, but here continuationdependencies are applied to all quantifiers, including univer-sals.3This also holds for explicit disjunction predicates, whichcan be cast as conjunction through application of de Morgan?slaw and manipulation of the polarity of adjacent quantifiers.For example, Every line begins with at least one space orcontains at least two numbers, is equivalent to No line be-gins with fewer than one space and contains fewer than twonumbers.discourse referents of a ?direct?
representation (thep, s, e, and d nodes in Figure 1), but replaces thedownward dependencies departing set referentswith upward dependencies to context sets (high-lighted in Figure 2).Figures 1c and 2c also show flat logical formscomposed of elementary predications, adaptedfrom Kruijff (2001) and Copestake et al.
(2005),for the sentence Every line contains two numbers,which are formed by identifying the function as-sociated with the predicate constant (e.g.
Contain)that is connected to each proposition or eventual-ity referent (e.g.
eC) by a dependency labeled ?0?,then applying that function to this referent, fol-lowed by the list of arguments connected to thisreferent by functions numbered ?1?
and up: e.g.
(Contain eCd?Ld?N).
These dependencies can alsobe defined by numbered dependency functions fnfrom source instance j to destination instance i,notated (fnj) = i.
This notation will be usedin Section 4 to define constraints in the form ofequations.
For example, the subject (first argu-ment) of a lexical item may be constrained to bethe subject (first argument) of that item?s senten-tial complement (second argument), as in an in-stance of subject control, using the dependencyequation (f1i) = (f1(f2i)).Since continuation dependencies all flow up thetree, any number of conjuncts can impinge upon acommon outscoping continuation, so there is nolonger any need for explicit conjunction nodes.The representation is also attractive in that it lo-cally distinguishes queries about, say, the cardi-nality of the set of numbers in each document line(Set s?Nd?Ns?L) from queries about the cardinal-ity of the set of numbers in general (Set s?Nd?Ns??
)which is crucial for successful inference by patternmatching.
Finally, connected sets of continuationdependencies form natural ?scope graphs?
for usein graph-based disambiguation algorithms (Man-shadi and Allen, 2011; Manshadi et al., 2013),which will be used to evaluate this representationin Section 6.3 Mapping to Lambda CalculusIt is important for this representation not onlyto have attractive graphical subsumption proper-ties, but also to be sufficiently expressive to de-fine corresponding expressions in lambda calcu-lus.
When continuation dependencies are filled in,the resulting dependency structure can be trans-143a)eLLinepLEvery0sL?0dL11s?L?0d?L12eNNumberpNTwo0sN?0dN11s?N?0d?N12pCSome0sC?0eCContain011s?C?0e?C121 11222b)eLLinepLEvery0sL?0dL11s?L?0d?L12eSSpacepSA0sS?0dS11s?S?0d?S12pBSome0sB?0eBBeginWith011s?B?0e?B12eNNumberpNTwo0sN?0dN11s?N?0d?N12pCSome0sC?0eCContain011s?C?0e?C121 1121122222c)(Every pLsLs?L) ?
(Set sLdLs?)
?
(Line eLdL) ?
(Set s?Ld?Ls?)
?
(Two pNsNs?N) ?
(Set sNdNs?)
?
(Number eNdN) ?
(Set s?Nd?Ns?L) ?
(Contain eCd?Ld?N)Figure 2: Semantic dependency graph in a ?continuation-passing?
(bottom-up) style, including quantifiersover eventualities for verbs (in gray).
The semantic dependency structure for the sentence Every linecontains two numbers (a), with flat logical form (c), is now contained by the semantic dependencystructure for Every line begins with a space and contains two numbers (b).lated into a lambda calculus expression by a de-terministic algorithm which traverses sequences ofcontinuation dependencies and constructs accord-ingly nested terms in a manner similar to that de-fined for DRT (Kamp, 1981).
This graphical rep-resentation can be translated into lambda calculusby representing the source graph as a set ?
of ele-mentary predications ( f i0.. iN) and the target asa set ?
of translated lambda calculus expressions,e.g.
(?i(hfi0.. i .. iN)).
The set ?
can then be de-rived from ?
using the following natural deductionrules:4?
Initialize ?
with lambda terms (sets) that haveno outscoped sets in ?
:?, (Set s i ) ; ?
?, (Set s i ) ; (?iTrue),?
(Set s ) < ??
Add constraints to appropriate sets in ?
:4Here, set predications are defined with an additional finalargument position, which is defined to refer in a nuclear scopeset to the restrictor set that is its sibling, and in a restrictor setto refer to s?.
?, ( f i0.. i .. iN) ; (?io),??
; (?io ?
(hfi0.. i .. iN)),?i0?
E?
Add constraints of supersets as constraints onsubsets in ?
:?, (Set s i ), (Set s?i?s?
?s) ;(?io ?
(hfi0.. i .. iN)), (?i?o?),?
?, (Set s i ), (Set s?i?s?
?s) ;(?io ?
(hfi0.. i ..
iN)),(?i?o??
(hfi0..
i?..
iN)),??
Add quantifiers over completely constrainedsets in ?
:?, (Set s i ), ( f p s?s??
),(Set s?i?s ), (Set s??i??s?s?)
;(?io), (?i?o?
), (?i??o??),?
?, (Set s i ) ;(?io ?
(hf(?i?o?)
(?i??o??
))),?p ?
P,( f?..
i?..)
< ?,( f??..
i??..)
< ?.For example, the graph in Figure 2 can be trans-lated into the following lambda calculus expres-sion (including quantifiers over eventualities in thesource graph, to eliminate unbound variables):144(Every (?dLSome (?eLLine eLdL))(?d?LTwo (?dNSome (?eNNumber eNdN))(?d?NSome (?eCContain eCd?Ld?N))))4 Derivation of Syntactic and SemanticDependenciesThe semantic dependency representation definedin this paper assumes semantic dependencies otherthan those representing continuations are derivedcompositionally by a categorial grammar.
In par-ticular, this definition assumes a Generalized Cat-egorial Grammar (GCG) (Bach, 1981; Oehrle,1994), because it can be used to distinguish argu-ment and modifier compositions (from which re-strictor and nuclear scope sets are derived in a tree-structured continuation graph), and because largeGCG-annotated corpora defined with this distinc-tion are readily available (Nguyen et al., 2012).GCG category types c ?
C each consist of a prim-itive category type u ?
U, typically labeled withthe part of speech of the head of a category (e.g.V, N, A, etc., for phrases or clauses headed byverbs, nouns, adjectives, etc.
), followed by one ormore unsatisfied dependencies, each consisting ofan operator o ?
O (-a and -b for adjacent argumentdependencies preceding and succeeding a head, -cand -d for adjacent conjunct dependencies preced-ing and succeeding a head, -g for filler-gap depen-dencies, -r for relative pronoun dependencies, andsome others), each followed by a dependent cate-gory type from C. For example, the category typefor a transitive verb would be V-aN-bN, since it isheaded by a verb, and has unsatisfied dependen-cies to satisfied noun-headed categories preced-ing and succeeding it (for the subject and directobject noun phrase, respectively).
This formula-tion has the advantage for semantic dependencycalculation that it distinguishes modifier and ar-gument attachment.
Since the semantic represen-tation described in this paper makes explicit dis-tinctions between restrictor sets and scope sets(which is necessary for coherent interpretation ofquantifiers) it is necessary to consistently applypredicate-argument constraints to discourse refer-ents in the nuclear scope set of a quantifier andmodifier-modificand constraints to discourse ref-erents in the restrictor set of a quantifier.
For ex-ample, in Sentence 1:(1) Everything is [A-aNopen].the predicate open constrains the nuclear scope setof every, but in Sentence 2:(2) Everything [A-aNopen] is finished.the predicate open constrains the restrictor set.These constraints can be consistently applied inthe argument and modifier attachment rules of aGCG.Like a Combinatory Categorial Grammar(Steedman, 2000), a GCG defines syntactic depen-dencies for compositions that are determined bythe number and kind of unsatisfied dependenciesof the composed category types.
These are similarto dependencies for subject, direct object, prepo-sition complement, etc., of Stanford dependencies(de Marneffe et al., 2006), but are reduced to num-bers based on the order of the associated depen-dencies in the category type of the lexical head.These syntactic dependencies are then associ-ated with semantic dependencies, with the refer-ent of a subject associated with the first argumentof an eventuality, the referent of a direct object as-sociated with the second argument, and so on, forall verb forms other than passive verbs.
In the caseof passive verbs, the referent of a subject is asso-ciated with the second argument of an eventuality,the referent of a direct object associated with thethird argument, and so on.In order to have a consistent treatment of ar-gument and modifier attachment across all cate-gory types, and also in order to model referentsof verbs as eventualities which can be quantifiedby adverbs like never, once, twice, etc.
(Parsons,1990), it is desirable for eventualities associatedwith verbs to also be quantified.
Outgoing seman-tic dependencies to arguments of eventualities arethen applied as constraints to the discourse refer-ent variable of the restrictor sets of these quanti-fiers.
Incoming dependencies to eventualities andother discourse referents used as modificands ofmodifiers are also applied as constraints to dis-course referent variables of restrictor sets, but in-coming dependencies to discourse referents usedas arguments of predicates are applied as con-straints to discourse referent variables of nuclearscope sets.
This assignment to restrictor or nuclearscope sets depends on the context of the relevant(argument or modifier attachment) parser opera-tion, so associations between syntactic and seman-tic dependencies must be left partially undefinedin lexical entries.
Lexical entries are therefore de-fined with separate syntactic and semantic depen-dencies, using even numbers for syntactic depen-dencies from lexical items, and odd numbers for145a)containings?d?1iCpCsCeCContain0111s??d?
?1123 50b)i?p?s?21i32c)i?p?s?11i32Figure 3: Example lexical semantic dependencies for the verb containing (a), and dependency equationsfor argument attachment (b) and modifier attachment (c) in GCG deduction rules.
Lexical dependenciesare shown in gray.
Even numbered edges departing lexical items denote lexical syntactic dependen-cies, and odd numbered edges departing lexical items are lexical semantic dependencies.
Argumentattachments constrain semantic arguments to the nuclear scope sets of syntactic arguments, and modifierattachments constrain semantic arguments to the restrictor sets of syntactic arguments.semantic dependencies from lexical items.
For ex-ample, a lexical mapping for the finite transitiveverb contains might be associated with the pred-icate Contain, and have the discourse referent ofits first lexical semantic argument (f1(f3i)) as-sociated with the first argument of the eventualitydiscourse referent of the restrictor set of its propo-sition (f1(f1(f1(f1i)))), and the discourse referentof its second lexical semantic argument (f1(f5i))associated with the second argument of the even-tuality discourse referent of the restrictor set of itsproposition (f2(f1(f1(f1i)))):contains ?
V-aN-bN : ?i(f0i)=contains?
(f0(f1(f1(f1i))))=Contain?
(f1(f1(f1(f1i))))=(f1(f3i))?
(f2(f1(f1(f1i))))=(f1(f5i))A graphical representation of these dependenciesis shown in Figure 3a.
These lexical semantic con-straints are then associated with syntactic depen-dencies by grammar rules for argument and modi-fier attachment, as described below.4.1 Inference rules for argument attachmentIn GCG, as in other categorial grammars, infer-ence rules for argument attachment apply functorsof category c-ad or c-bd to preceding or succeed-ing arguments of category d:d : g c-ad : h?
c : (fc-adg h) (Aa)c-bd : g d : h?
c : (fc-bdg h) (Ab)where fu?1...?nare composition functions for u?Uand ??
{-a, -b, -c, -d}?C, which connect the lexi-cal item (f2ni) of a preceding child function g asthe 2nthargument of lexical item i of a succeedingchild function h, or vice versa:fu?1..n?1-addef= ?g h i(g (f2ni)) ?
(h i)?
(f2n+1i)=(f2(f1(f2ni))) (1a)fu?1..n?1-bddef= ?g h i(g i) ?
(h (f2ni))?
(f2n+1i)=(f2(f1(f2ni))) (1b)as shown in Figure 3b.
This associates the lex-ical semantic argument of the predicate (f2n+1i)with the nuclear scope of the quantifier propo-sition associated with the syntactic argument(f2(f1(f2ni))).
For example, the following infer-ence attaches a subject to a verb:every lineN : ?i(f0i)=line ..contains two numbersV-aN : ?i(f0i)=contains ..V : ?i(f0(f2i))=line .. ?
(f0i)=contains ..?
(f3i)=(f2(f1(f2i)))Aa4.2 Inference rules for modifier attachmentThis grammar also uses distinguished inferencerules for modifier attachment.
Inference rules formodifier attachment apply preceding or succeed-ing modifiers of category u-ad to modificands ofcategory c, for u ?
U and c, d ?
C:u-ad : g c : h?
c : (fPMg h) (Ma)c : g u-ad : h?
c : (fSMg h) (Mb)146eLLine0linesiLpLSome0sL?0dL11s?L?0d?L121containingiCpCSome0sC?0eCContain011s?C?0e?C121eNNumber0numbersiNpNSome0sN?0dN11s?N?0d?N1210002435221121Figure 4: Compositional analysis of noun phrase lines containing numbers exemplifying both argumentattachment (to numbers) and modifier attachment (to lines).
Lexical dependencies are shown in gray, andcontinuation dependencies (which do not result from syntactic composition) are highlighted.where fPMand fSMare category-independent com-position functions for preceding and succeedingmodifiers, which return the lexical item of the ar-gument ( j) rather than of the predicate (i):fPMdef= ?g h j?i(f2i)= j ?
(g i) ?
(h j)?
(f3i)=(f1(f1(f2i))) (2a)fSMdef= ?g h j?i(f2i)= j ?
(g j) ?
(h i)?
(f3i)=(f1(f1(f2i))) (2b)as shown in Figure 3c.
This allows categoriesfor predicates to be re-used as modifiers.
Unlikeargument attachment, modifier attachment asso-ciates the lexical semantic argument of the mod-ifier (f2n+1i) with the restrictor of the quantifierproposition of the modificand (f1(f1(f2ni))).
Forexample, the following inference attaches an ad-jectival modifier to the quantifier proposition of anoun phrase:every lineN:?i(f0i)=line ..containing two numbersA-aN:?i(f0i)=containing ..N : ?i(f0i)=line .. ?
?j(f0j)=containing ..?
(f2j)=i ?
(f3j)=(f1(f1(f2j)))MbAn example of argument and modifier attachmentis shown in Figure 4.5 Estimation of Scope DependenciesSemantic dependency graphs obtained from GCGderivations as described in Section 4 are scopallyunderspecified.
Scope disambiguations must thenbe obtained by specifying continuation dependen-cies from every set referent to some other set ref-erent (or to a null context, indicating a top-levelset).
In a sentence processing model, these non-local continuation dependencies would be incre-mentally calculated in working memory in a man-ner similar to coreference resolution.5However, inthis paper, in order to obtain a reasonable estimateof the learnability of such a system, continuationdependencies are assigned post-hoc by a statisticalinference algorithm.The disambiguation algorithm first defines apartition of the set of reified set referents intosets {s, s?, s??}
of reified set referents s whose dis-course referent variables (f1s) are connected bysemantic dependencies.
For example, sL, sCands?Nin Figure 4 are part of the same partition, but s?Lis not.Scope dependencies are then constructed fromthese partitions using a greedy algorithm whichstarts with an arbitrary set from this partition in5Like any other dependency, a continuation dependencymay be stored during incremental processing when both itscue (source) and target (destination) referents have been hy-pothesized.
For example, upon processing the word numbersin the sentence Every line contains two numbers, a continu-ation dependency may be stored from the nuclear scope setassociated with this word to the nuclear scope set of the sub-ject every line, forming an in-situ interpretation with someamount of activation (see Figure 4), and with some (proba-bly smaller) amount of activation, a continuation dependencymay be stored from the nuclear scope set of this subject tothe nuclear scope set of this word, forming an inverted inter-pretation.
See Schuler (2014) for a model of how sentenceprocessing in associative memory might incrementally storedependencies like these as cued associations.147the dependency graph, then begins connecting it,selecting the highest-ranked referent of that par-tition that is not yet attached and designating itas the new highest-scoping referent in that parti-tion, attaching it as the context of the previouslyhighest-scoping referent in that partition if one ex-ists.
This proceeds until:1. the algorithm reaches a restrictor or nuclearscope referent with a sibling (superset or sub-set) nuclear scope or restrictor referent thathas not yet served as the highest-scoping ref-erent in its partition, at which point the algo-rithm switches to the partition of that siblingreferent and begins connecting that; or2.
the algorithm reaches a restrictor or nuclearscope referent with a sibling nuclear scope orrestrictor referent that is the highest-scopingreferent in its partition, in which case it con-nects it to its sibling with a continuation de-pendency from the nuclear scope referent tothe restrictor referent and merges the two sib-lings?
partitions.In this manner, all set referents in the dependencygraph are eventually assembled into a single treeof continuation dependencies.6 EvaluationThis paper defines a graphical semantic represen-tation with desirable properties for storing sen-tence meanings as cued associations in associa-tive memory.
In order to determine whether thisrepresentation of continuation dependencies is re-liably learnable, the set of test sentences from theQuanText corpus (Manshadi et al., 2011) was au-tomatically annotated with these continuation de-pendencies and evaluated against the associatedset of gold-standard quantifier scopes.
The sen-tences in this corpus were collected as descrip-tions of text editing tasks using unix tools like sedand awk, collected from online tutorials and fromgraduate students asked to write and describe ex-ample scripts.
Gold-standard scoping relations inthis corpus are specified over bracketed sequencesof words in each sentence.
For example, the sen-tence Print every line that starts with a numbermight be annotated:Print [1every line] that starts with [2a number] .scoping relations: 1 > 2meaning that the quantifier over lines, referencedin constituent 1, outscopes the quantifier overnumbers, referenced in constituent 2.
In order toisolate the learnablility of the continuation depen-dencies described in this paper, both training andtest sentences of this corpus were annotated withhand-corrected GCG derivations which are thenused to obtain semantic dependencies as describedin Section 4.
Continuation dependencies are theninferred from these semantic dependencies us-ing the algorithm described in Section 5.
Gold-standard scoping relations are considered success-fully recalled if a restrictor (f1(f1i)) or nuclearscope (f2(f1i)) referent of any lexical item i withinthe outscoped span is connected by a sequence ofcontinuation dependencies (in the appropriate di-rection) to any restrictor or nuclear scope referentof any lexical item within the outscoping span.First, the algorithm was run without any lexical-ization on the 94 non-duplicate sentences of theQuanText test set.
Results of this evaluation areshown in the third line of Table 1 using the per-sentence complete recall accuracy (?AR?)
definedby Manshadi et al.
(2013).The algorithm was then run using bilexicalweights based on the frequencies?F(h, h?)
withwhich a word h?occurs as a head of a categoryoutscoped by a category headed by word h in the350-sentence training set of the QuanText corpus.For example, since quantifiers over lines are oftenoutscoped by quantifiers over files in the trainingdata, the system learns to rank continuation de-pendencies to referents associated with the wordlines ahead of continuation dependencies to ref-erents associated with the word files in bottom-up inference.
These lexical features may be par-ticularly helpful because continuation dependen-cies are generated only between directly adjacentsets.
Results for scope disambiguation using theserankings are shown in the fourth line of Table 1.This increase is statistically significant (p = 0.001by two-tailed McNemar?s test).
This significancefor local head-word features on continuation de-pendencies shows that these dependencies can bereliably learned from training examples, and sug-gests that continuation dependencies may be a nat-ural representation for scope information.Interestingly, effects of lexical features forquantifiers (the word each, or definite/indefinitedistinctions) were not substantial or statisticallysignificant, despite the relatively high frequencies148System ARManshadi and Allen (2011) baseline 63%Manshadi et al.
(2013) 72%This system, w/o lexicalized model 61%This system, w. lexicalized model 72%Table 1: Per-sentence complete recall accuracy(?AR?)
of tree-based algorithm as compared toManshadi and Allen (2011) and Manshadi et al.
(2013) on explicit NP chunks in the QuanText testset, correcting for use of gold standard trees as de-scribed in footnote 19 of Manshadi et al.
(2013).of the words each and the in the test corpus (oc-curring in 16% and 68% of test sentences, respec-tively), which suggests that these words may oftenbe redundant with syntactic and head-word con-straints.
Results using preferences that rank refer-ents quantified by the word each after other refer-ents achieve a numerical increase in accuracy overa model with no preferences (up 5 points, to 66%),but it is not statistically significant (p = .13).
Re-sults using preferences that rank referents quanti-fied by the word the after other referents achieve anumerical increase in accuracy over a model withno preferences (up 1 point, to 62%), but this iseven less significant (p = 1).
Results are evenweaker in combination with head-word features(up 1 point, to 73%, for each; down two points,to 70%, for the).
This suggests that world knowl-edge (in the form of head-word information) maybe more salient to quantifier scope disambiguationthan many intuitive linguistic preferences.7 ConclusionThis paper has presented a graphical semantic de-pendency representation based on bottom-up con-tinuation dependencies which can be translatedinto lambda calculus, but has the important prop-erty that its vertices define a usable set of discoursereferents in working memory even in contexts in-volving conjunction in the scope of quantifiers.An evaluation on an existing quantifier scope dis-ambiguation task shows that non-local continua-tion dependencies can be as reliably learned fromannotated data as representations used in a state-of-the-art quantifier scope resolver.
This suggeststhat continuation dependencies may be a naturalrepresentation for scope information.Continuation dependencies as defined in thispaper provide a local representation for quantifi-cational context.
This ensures that graphical repre-sentations match only when their quantificationalcontexts match.
When used to guide a statisticalor vectorial representation, it is possible that thislocal context will allow certain types of inferenceto be defined by simple pattern matching, whichcould be implemented in existing working mem-ory models.
Future work will explore the use ofthis graph-based semantic representation as a ba-sis for vectorial semantics in a cognitive model ofinference during sentence processing.8 AcknowledgementsThe authors would like to thank Mehdi Manshadifor assistance in obtaining the QuanText corpus.The authors would also like to thank Erhard Hin-richs, Craige Roberts, the members of the OSULLIC Reading Group, and the three anonymous*SEM reviewers for their helpful comments aboutthis work.ReferencesJames A. Anderson, Jack W. Silverstein, Stephen A.Ritz, and Randall S. Jones.
1977.
Distinctive fea-tures, categorical perception and probability learn-ing: Some applications of a neural model.
Psycho-logical Review, 84:413?451.Emmon Bach.
1981.
Discontinuous constituents ingeneralized categorial grammars.
Proceedings ofthe Annual Meeting of the Northeast Linguistic So-ciety (NELS), 11:1?12.Jason Baldridge and Geert-Jan M. Kruijff.
2002.
Cou-pling CCG and hybrid logic dependency seman-tics.
In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL2002), Philadelphia, Pennsylvania.Chris Barker.
2002.
Continuations and the natureof quantification.
Natural Language Semantics,10:211?242.Jon Barwise and Robin Cooper.
1981.
Generalizedquantifiers and natural language.
Linguistics andPhilosophy, 4.Johan Bos.
1996.
Predicate logic unplugged.
In Pro-ceedings of the 10th Amsterdam Colloquium, pages133?143.Sarah Brown-Schmidt, Ellen Campana, and Michael K.Tanenhaus.
2002.
Reference resolution in the wild:Online circumscription of referential domains in anatural interactive problem-solving task.
In Pro-ceedings of the 24th Annual Meeting of the Cogni-tive Science Society, pages 148?153, Fairfax, VA,August.149Ann Copestake, Dan Flickinger, Carl Pollard, and IvanSag.
2005.
Minimal recursion semantics: An intro-duction.
Research on Language and Computation,pages 281?332.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of LREC 2006.Judith Degen and Michael K. Tanenhaus.
2011.
Mak-ing inferences: The case of scalar implicature pro-cessing.
In Proceedings of the 33rd Annual Confer-ence of the Cognitive Science Society, pages 3299?3304.Marc W. Howard and Michael J. Kahana.
2002.
A dis-tributed representation of temporal context.
Journalof Mathematical Psychology, 45:269?299.Hans Kamp.
1981.
A theory of truth and semanticrepresentation.
In Jeroen A. G. Groenendijk, TheoM.
V. Janssen, and Martin B. J. Stokhof, editors,Formal Methods in the Study of Language: Math-ematical Centre Tracts 135, pages 277?322.
Mathe-matical Center, Amsterdam.Walter Kintsch.
1988.
The role of knowledge in dis-course comprehension: A construction-integrationmodel.
Psychological review, 95(2):163?182.Alexander Koller.
2004.
Constraint-based and graph-based resolution of ambiguities in natural language.Ph.D.
thesis, Universit?at des Saarlandes.Geert-Jan M. Kruijff.
2001.
A Categorial-ModalArchitecture of Informativity: Dependency Gram-mar Logic and Information Structure.
Ph.D. thesis,Charles University.Bill MacCartney and Christopher D. Manning.
2009.An Extended Model of Natural Logic.
In Proceed-ings of the Eighth International Conference on Com-putational Semantics, IWCS-8 ?09, pages 140?156.Association for Computational Linguistics.Mehdi Manshadi and James F. Allen.
2011.
Unre-stricted quantifier scope disambiguation.
In Graph-based Methods for Natural Language Processing,pages 51?59.Mehdi Manshadi, James F. Allen, and Mary Swift.2011.
A corpus of scope-disambiguated englishtext.
In Proceedings of ACL, pages 141?146.Mehdi Manshadi, Daniel Gildea, and James F. Allen.2013.
Plurality, negation, and quantification: To-wards comprehensive quantifier scope disambigua-tion.
In Proceedings of ACL, pages 64?72.David Marr.
1971.
Simple memory: A theoryfor archicortex.
Philosophical Transactions of theRoyal Society (London) B, 262:23?81.David Marr.
1982.
Vision.
A Computational Investiga-tion into the Human Representation and Processingof Visual Information.
W.H.
Freeman and Company.J.
L. McClelland, B. L. McNaughton, and R. C.O?Reilly.
1995.
Why there are complementarylearning systems in the hippocampus and neocortex:Insights from the successes and failures of connec-tionist models of learning and memory.
Psychologi-cal Review, 102:419?457.Richard Montague.
1973.
The proper treatmentof quantification in ordinary English.
In J. Hin-tikka, J.M.E.
Moravcsik, and P. Suppes, editors,Approaches to Natural Langauge, pages 221?242.D.
Riedel, Dordrecht.
Reprinted in R. H. Thoma-son ed., Formal Philosophy, Yale University Press,1994.B.B.
Murdock.
1982.
A theory for the storage andretrieval of item and associative information.
Psy-chological Review, 89:609?626.Luan Nguyen, Marten van Schijndel, and WilliamSchuler.
2012.
Accurate unbounded dependency re-covery using generalized categorial grammars.
InProceedings of the 24th International Conferenceon Computational Linguistics (COLING ?12), pages2125?2140, Mumbai, India.Richard T. Oehrle.
1994.
Term-labeled categorial typesystems.
Linguistics and Philosophy, 17(6):633?678.Terence Parsons.
1990.
Events in the Semantics ofEnglish.
MIT Press.William Schuler.
2014.
Sentence processing in avectorial model of working memory.
In Fifth An-nual Workshop on Cognitive Modeling and Compu-tational Linguistics (CMCL 2014).Chung-chieh Shan and Chris Barker.
2006.
Explainingcrossover and superiority as left-to-right evaluation.Linguistics and Philosophy, 29:91?134.Mark Steedman.
2000.
The syntactic process.
MITPress/Bradford Books, Cambridge, MA.Mark Steedman.
2012.
Taking Scope - The NaturalSemantics of Quantifiers.
MIT Press.Michael K. Tanenhaus, Michael J. Spivey-Knowlton,Kathy M. Eberhard, and Julie E. Sedivy.
1995.
In-tegration of visual and linguistic information in spo-ken language comprehension.
Science, 268:1632?1634.Marten van Schijndel and William Schuler.
2013.
Ananalysis of frequency- and recency-based processingcosts.
In Proceedings of NAACL-HLT 2013.
Associ-ation for Computational Linguistics.Marten van Schijndel, Luan Nguyen, and WilliamSchuler.
2013.
An analysis of memory-based pro-cessing costs using incremental deep syntactic de-pendency parsing.
In Proceedings of CMCL 2013.Association for Computational Linguistics.150
