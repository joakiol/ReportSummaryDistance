American Jo&nsl of Computational Linguistics Microf i che  61T O W A R D  A  " N A T U R A L "  L A N G U A G EQ U E S T I O N - A N S W E R I N G  F A C I L I T YBILL D .
MAXWELL AND FRANCIS D. ~ U G G L EDepartment of Computer ScienceUniversity of KansasLawrence 66045Maxwell is also associated with the Computation Center-Tuggle is also associated with the School of BusinessCopyright  @ 1977Associa  t l o n  f p r  Computational L i n g u i s t i c sThis study describes the structure, implementation and potentialof a simple computer program that understands and answers questions ina humanoid mannet.
An emphasis bas been placed on the creation of anextendible memory structure--one capable of supposting conversationin normal, unrestricted ErIgli6h on a variety of topics.
An attempthas also been made to find procedures that can easily and accuratelydetermine the meaning of input text- A parser using a combination ofsyntax and semantics has been developed for understanding YES-NOquestions, in particular, DO-type (DO, DID, DOES, etc.)
questions.
Athird and major emphasis has been on the development of proceduresto allow the program to converse, easily and "naturally1' with a human.This general gaql has been met by developing procedures that generateanswers to DO-questions in a manner similar to the way a person mightanswer them.TABLE OF CONTENTS1.
INTRODUCTION.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.1.1 MIfiGOALS .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.1.2 SMLEDIALOG .
.
.
.
.
.
m e .
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1.3 REVJE\J1.3.)
llemory Models .
.1.3.2 L i n g u i s t i c  P a r s e r s  .
.
.1.3.3 Outbu tGenera t io r l  .
= .
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4 .
PARSER4 .1  PARSINGSTRATEGY .
.
.
.
.
.
.
.
.
.
.
.
.
.
.4.2 G M R  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.4.2.1 Acceptable  Inpu t  Forms .
.
.4.2.2 Semantics and Syntax .
94.2.3 Pronouns.
Ambiguity arid Undefined W o r k4.3 PARSING ALGORITHM .
.
.
.
.
.
.
.
.
.
.
.
.
.
.4.3.1 Preprocess ing  of  t h e  Inpu t  Text , .
.
.4.3.2 Determining Type of  Inpu t  .
.
.
.
.
.
.4.3.3 P a r s i n g  a DO-Question o r  Statement  .
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5 1EYORY scARCHING.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5.1 OVERVIEII.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5.2 TIIE IlEIIORY MATCH STRUCTURE.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5.2.1 General  S t r u c t u r e. .
.
.
.
.
.
5.2.2 Actor.
Act and Objec t  Comparison R e s u l t s5.2.3 lJord l l o d i f i c a t i o n  R e s u l t s  .
.
.
.
.
.
.
.
.
.
.
.
.
.5.2.4 Example S t r u c t u r e s  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.5.3 PlATCllING MEllORY .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.5.3.1 Bas ic  Algoritlun .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.5.3.2 S e l e c t i n g  S u i t a b l e  Actors  .
.
.
.
.
.
.
.
.
.
.
.
.
.. 6 PRODUCTION OF OUTPUT .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.6.1 OVERVIEW .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
6.2 THE OUTPUT PRODUCTION LIST6.3  PRODUCTION METHOD .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
6.3.1 Responding t o  I n p u t  l o t  Understood.
.
.
.
.
.
.
.
.
.
.
.
.
6.3.2 Determining Mode of Response6.3.3 Producing a Nomal  Answer .
.
.
.
.
.
.
.
.
.
.
.
.
.6.3.4 Flaking Output Grammatical .
.
.
.
.
.
.
.
.
.
.
.
.
.I)ISCUSSIOI~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.7 .
1  RESULTS .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
7.1.1 Objectives Net7.1.2 Does the Program Really Understand?I *  2 LIMITATIONS .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.7 .2 .1  Memory Model .
.
.
.
.
.
.
.
.
.
.7.2.2 Parser .
.
.
.
.
.
.
.
.
.
.
.
.
.7 .2 .3  Output Production .
.
.
.
.
.
.
.
.7 .
3  IGNORED PROBLEMS .
.
.
.
.
.
.
.
.
.
.
.
.7.3.1 Intentions andPlotivations .
.
.
.7.3.2 ilorman's Problems .
.
.
.
.
.
.
.
.1.
INTRODUCTIONThere a r e  a number of e x t a n t  computer programs which i n t e r a c ti n t e l l i g e n t l y  wi th  human i n t e r r o g a t o r s ,  but  a l l  do s o  i n  a way w echa rac t e r i ee  a s  "unnatural".
By "unnatural ,"  we mean t h a t  they con-v e r s e  i n  a way s i g n i f i c a n t l y  un l ike  two normal a d u l t  humans do.
Forexample, Winograd's SHRBLU [19] seems chi ld- l ike ,  and i t  sha res  w i th  t h ewoods' moonrocks system [20] the  problem of being task-const rained.Weizenbaum's ELIZA [16] gives  the  appearance of be ing  vague, evas lve  anddoubting.
Colby's PARRY [s], of course,  is va r ious ly  paranoid.
F i n a l l y ,t h e  C A I  systems, such as ca rbone l l ' s  SCHOLAR [ 4 ]  and Brown and Burton'sSOPHIE [3], seem o f f i c i o u s l y  pedantic.
I n  c o n t r a s t  t o  t hose  approaches,w e  wanted t o  cons t ruc t  a program a b l e  t o  c a r r y  on an i n - l l i g e n t  and"na tura l"  d ia logue  wi th  a human user .As an example of a "natural"  and a n  "unnatural" d ia logue ,  imaginetwo computer programs (CPs) conversing wi th  a human (H) i n  which eachCP a l r eady  knows "THE BOY O~JNS A BALL."
One CP is "natural"  (NCP); t h eo t h e r  is  "unnatural" (uCP) .
(One may f i n d  fragments of responses frome s i s t i n g  programs i n  those  responses we have l a b e l l e d  a s  "UCP.
")H: The b a l l  is red.U C t :  By ' t h e  b a l l , '  I assume you mean t h e  b a l l  t h e  boy owns.NCP: b.K.H: Does t h e  boy own a l a rge ,  round b a l l ?UCP: No.
[or a t  b e s t ]  I don't  know.NCP: H e  owns a red  one.
It probably i s  round and may be la rge .H: The boy l i v e s  on Main S t r e e t .UCP:.
By ' t h e  boy', I assume you mean t h e  one who owns t h e  red  b a l l .NCP: O.K.H: l lhete does t h e  boy p l ay  with h i s  b a l l ?UCP : I don' t krtow.NCP: I don' t  know, but I assume near  h i s  res idence  on Main S t r e e t .H: A house is on Main S t r e e t .UCP: O.K.NCP: Is i t  t h e  house of t h e  boy who owns t h e  red b a l l ?H: It was,expensive t o  cons t ruc t .UCP: By 'it,' do youmean1.
the  house2.
Main S t r e e t ?NCP: 0 .
K .H: The b a l l  broke a window.UCP: By ' t h e  Ball, ' 1 assume you mean the  red  one the  boy owns.
BY 'awindow,' I assume i r  is  a p a r t  of  t h e  house i n  Main S t r ee t .NCP: DPd t h e  boy throw t h e  b a l l ?H: Was t h e  window expensive t o  r e p a i r ?UCP: I don' t  know.NCP: S i ~ c e  t h e  house was expensive t o  cons t ruc t ,  I assume t h a t  i tswindows a r e  expensive t o  repa i r .From t h i s  s h o r t ,  s imple dialogue some of  t he  c h a r a c t e r i s t i c s  whichd is t inguixh  "natural"  d ia logue  can be seen: "natural"  language under-s tanding processes  a r e  a b l e  t o  work with p a r t i a l  and overlappinginformation, a r e  a b l e  t o  r e t a i n  ambiguity u n t i l  disambigation i s  needed,a r e  a b l e  t o  perform "short" chains of deductions,  a r e  a b l e  t o  engage i ncommon s e n s e  reasoning, and a r e  a b l e  t o  pose r ec ip roca l  ques t ions  t othe human t o  confirm expectat ions.Reasons f o r  t r y i n g  t o  i n s t i l l  a c e r t a i n  degree of "humanness1' t ocomputer proBrams should be obvious--to f a c i l i t a t e  t h e i r  acceptance, t oextend t h e i r  use, and t o  make them more p l easan t  t o  dea l  with.
Peoplew i l l  be much more w i l l i n g  r o  work with a computer program i f  i t  g ives  t h eappearance of being humanoid i t s e l f ,  whether t h e  ke rne l  p a r t  of  Cheprogram concerns CAT, MIS, o r  whatever.- 7-We have developed a computer program c a l l e d  J I M M Y 3  which embodiessome of t he  abave s e t  of c h a r a c t e r i s t i c s  of a "natural1'  language processingsystem s o  a s  t o  demonstrate i t s  f e a s i b i l i t y ,  usefu lness ,  and p o t e n t i a lpower.
The implementation has neces sa r i l y  been l a r g e l y  ad hoc, bu t  asLindsay [ 6 ]  notes ,  t h i s  is no t  a l t oge the r  bad.
Mewell [7] proper1.yrecords t h a t  t he re  is a t radeoff  between g e n e r a l i t y  and power.
LikeLindsay, we d e l i b e r a t e l y  eschew t h e  general i n  favor  of t h e  spec i f i c .I n  t h e  i n t e r e s t s  of r e p l i c a b i l i t y  and e x t e n s i b i l i t y ,  we a l s o  provide areasonably complete desc r ip t i on  of t he  inards  of JIMMY3.1.1 MAJOR GOALSThe focus of t h i s  work has been on th ree  problems: a )  t h e  develop-ment ~f a memory s t r u c t u r e  u se fu l  f o r  engaging i n  a dialogue wi th  a person,b) t h e  development of procedures t h a t  can e a s i l y  and accura-tely determinethe  meaning of i npu t  t e x t ,  and c )  t he  development of procedures f o r  t h egenerat ion of output .A s  cont ras ted  with more conventional  models f o r  language understand-i ng  where input  is decoded i n  terms of concepts and then mapped i n t omemory, t h i s  model s t o r c s  su r f ace  s t r u c t u r e s  more o r  l e s s  i n t a c t  withoutany conversion.
This  approach s i m p l i f i e s  both t h e  i npu t  processing andoutput  generat ion phases of t he  system.
However, i t  does n e c e s s i t a t e  t h euse of complex memory matching procedures during t h e  answer-producingphase-of understanding.For parsing,  s eve ra l  ad hoc r u l e s  were developed f o r  applxing amet r ic  t o  measure "meaningfulness."
Using t h i s  procedure, s e v e r a ld i f f e r e n t  i n t e r p r e t a t i o n s  of t h e  i npu t  a r e  examined.
The one having t h eh ighes t  "score," as determined by t h e  met r ic ,  is  s e l e c t e d  a s  t h e  propermeaning.Most memorymodels and undez'standing systems do not  have e labora te.
f a c i l i t i e s  f o r  t h e  c rea t ibn  of output t h a t  is i n t e r e s t i n g  o r  s t imula t ingto conversation.
Output production by t h i s  program approaches so lu t ionst o  t h r e e  re la fed  problems: a )  what information should be used i n  the9esponse t o  a given input ,  b)  how a response can be s t ruc tu red  soc e r t a i n  p a r t s  a r e  emphasized an$ c )  how B Tesponse can be made tpappear n a t u r a l  i n  the  sense of being l i k e  a s i m i l a r  remark a personmight make.1.2 SAMPLE DIALOGUETo g e t  a f e e l i n g  f o r  t h e  types of responses t h e  currentJy Imple-mented program CBn produce, t h e  following s h o r t  dialogue is presented.Note t h a t  with only the  c a p a b i l i t y  for answering W-questionsimplemeqted, a continuing dialogue is  not very easy t o  obta in .Three e x p l i c i t  f a c t s  e x i s t e d  i n  remory a t  the time of t h i s  con-versa t ion.
They were:1 )  BRANDT -JELL OWNS A RED BOOK.2) BRANDT MAXWELL OWNS A BLUE BOOK.3) JIMMY3 KNOWS BRANDT MAXWELL.
( t h i s  computer model has  been dubbed JIMElY3)The dialogue is  given i n  upper case  wi th  commentary i n  lower case.The j: and p: ( iden t i fy ing  JZMMY3 and person) were added liere f o rc l a r i t y .j: HI.
WHO ARE YOU?JIMMY3 must f i n d  o u t  who it is t a l k i n g  with i n  orderrt o  t r a n s l a t e  the  pronoun "I" t o  f a c i l i t a t e  memorysearching.p : BdANDT MAXWELL.OK.BR@DT@MAXWELL i s  r tcognized as a l e g i t i m a t e  name.The @ i n  BRANDT@EWLL is used t o  i n d i c a t e  thecombining o f  two o r  more words t o  form a Separateert t i ty.
I n  the  above example, t h e  names BRANDTand W E L L  a r e  recognized as f i r s t  and l a s t  names,respect ively ,  t h a t ,  when appearing together ,  a r ein te rp re ted  a s  t h e  f u l l  name, BRANDT@MAXWELL.W YOU KNOW M S N E ?YES, YOU.Pars ing is accomplished by matching inpu t  t otemplates c o n s i s t i n g  of (ACTOR ACT OBJECT)descr ibing t h e  d i f f e r e n t  meaning senses  f o r  mainverbs.
Th i s  quest ion is parsed using t h e  (PERSONKNOW PERSON) d e f i n i t i o n  of KNOW.
On i n p u t ,  YOUis t r a n s l a t e d  t o  JIEIMY3 (which has ISA PERSON a sone of i t s  proper t ies) .Memory is matched aga ins t  t h e  (JIMMY3 KNOWPERSON) p a t t e r n  which y i e l d s  the  match (JIEIMY3KNOWS BRANDT@MAxwELL)'.
Output procedures thencdnvert t h e  match i n t o - t h e  given response.DO YOU KNOW BILL MAXWELL?NO.The search using (JIMMY3 KNOW BILLeMAXWELL) y i e l d sno match.
Under o t h e r  circumstances a f a i l u r e  oft h j s  type would produce an answer of "I DON'T KNOW".However, t h e  a c t  KNOW is tagged s o  t h a t  "NO" is t h eproper response i f  a sea rch  of memory f a i l s  whent h e  t r i p l e  has  KNOW a s  i ts act(main verb).DO YOU THINK?r DON'T MOW.The input  was c o r r e c t l y  parsed a s  (PERSON THINK NIL)but  t h e  memory search found no matches.
Thecur ren t  model has no f a c i l i t i e s  f o r  i n t r o s p e c t i o no t h e r  t h a n  the examination of t r i p l e s  i n  memoryt h a t  d e a l  e x p l i c i t l y  wi th  JIMMY3.DO YOU KNOW ANYTHING9I DON'T UNDERSTAND THE USE OF KNOW.A t  t h e  t i m e  t h i s  d ia logue was produced, t h e  only mean-ing  of KNOW contained i n  memory was (PERSON KNOW PERSON).Therefore, t h e  reasonable pa r se  of (PERSON KNOW THING)was not found.p: DO I HAVE A BLUE ANI;EML BOOK?j: A BLUE BOOK BUT NOT AN ANIMAL BOOK:Some, bu t  no t  a l l ,  of t h e  modi f ie rs  on the  inputwere matched by a t r i p l e  i n  memory a s  ind ica ted  byt h e  response.
Since not  a l l  modi f ie rs  matchkd, t h eansQer is not  prefaced wi th  YES.p: DOES N O N E  HAVE A RED THING?j: YES.
YOU OIJN A RED BOOK.liemory Ls searched using t h e  (PERSON 1WVE TIIING)p a t t e r n  which b e s t  matches (BRAEJDT@MAXWELL OWNSBOOK) .
9 comparing modi f ie rs  , (BRANDT@MAXWELLOWNS RED-BOOK) i s  s e l e c t e d  over  (BRANDT@MAXWELLorws BLUE-BOOK) .p: M I OWN A GREEN IjOOK?j : NO.
YOU OWN A RED BOOK.S ince  GREEN arhd RED a r e  mutually exc lus ive  i n s t ancesof COLOR, t h e  answer is  NO.
(BRANDT@MAX\ELL OWNBLUE-BOOK) was a l s o  matched buQ fie t r i p l e  withRED HOOK was used t o  produce the  response s i n c e  itwas found f i r s t  by t h e  memory search  rout ine .p: DO I HAVE AN QLD BOOK?j: YES.
A RED ONE.
I DON'T mow WHAT OLP MEANS.OLD is  ignored s in=  ~t i s  an unaefined word.
Inputis parsed a s  i f  i t  had been "DO I HAVE AN BOOK?
"p: DOES BILL IfAXWELL O \ N  A W O K ?j: L DON'T KNOW.The f a c t  ( B I L L @ W J E L L  Oms BOOK) is  no t  presenti n  memory.
Nei ther  i s  any con t r ad i c t i on  of t h a t  f a c t .p: B Y E -j: GOODBYE .1.3 REVIEWThe r e sea rch  involving memory models can b e  divided i n t o  two bas icapproaches.
F i r s t  are models c rea ted  f o r  t h e  e x p l o r a a o n  o f  t heo r i e s  ofhuman memory and f o r  t h e  t e s t i n g  of l i n g u i s t i c .
theor ies .
In  thesesystems, t h e  o t h e r  f e a t u r e s  of a  complete language understanding systemassume a background p o s i t i o n  s i n c e  t h e  main emphasis is t h e  memory modeli t s e l f .
Models which f a l l  i n t o  t h i s  c l a s s  a r e  t h e  works of Q u i l l i a n  [ll],Anderson and Bower (11, and Norman, Rumelhart and the  LNR research-11 -group [lo].
The second type of memory model is t h a t  developed a s  v a r tof a system which has  splpe component o t h e r  than memory a s  the  majoremphasis.
These models include ~ 5 n o g r a d ' s  blocks world L1-93, ~ c h a n k ' sConceptml Dependency System C13, 143 and ~ o l b y ' s  A r t i f  l c i a l  Paranoiamodel [s].The memory model developed by t h i s  cu r ren t  research does notcorrespond t o  any  xis sting model.
It is  not based n n  case  grammar i na s t r i c t  sense, but  s t o r e s  informat5on i n  a form more c lose ly  r e l a t e dt o  su r face  Qtructures u t i l i z i n g  only th ree  main components: a c t o r ,a c t  and object .
Although t h i s  development was in f luen ted  t o  a l imi tedex ten t  by Anderson and Bower's model [ I ] ,  i t  was shaped I n  a c t u a l  designby t h e  pa r se r  Osed by Wilks (gce below) which a t tempts  t o  f i n d  meaningby searching f o r  t r i p l e s .
Wilks' work was a l s o  i n f l u e n t i a l  i n  thedevelopment oP the  parser.1.3.2 LINGUISTIC PARSERSI n  the  a r e a  of l i n g u i s t i c  parsers ,  t h e r e  a r e  cur ren t ly  fourd i f f e r e n t  methods betng used.
They a re :  a) semantic t r i p l e s ,  b) aug-mented t r a n s i t i o n  networks, c) procedures and d) pa t  t e r n  matching.These can be described most e a s i l y  by examining s p e c i f i c  examplesof each.The parser  developed by Yorick Wilks [17, 181 is based on i d e n t i -fy ing t r i p l e s  composed of an a c t o r ,  an  a c t  and an object .
Input i sparsed by applying t r ia l  templates t o  the  input and i d e n t i f y i n gcandidates t o  f i l l  t h e  t h r e e  s l o t s  i n  a template.
The a s s o c i a t i o n  ofan  input  word with a template s l o t  is made by consideratiom of t h e-12-semantic  r e l a t i o n s h i p  between t h e  i n p u t  word and t h e  requirements  of t h es l o t ,  The c l o s e r  t h e  r e l a t i o n s h i p ,  t h e  b e t t e r  t h e  match, and t h e r e f o r e ,t h e  b e t t e r  t h e  parse .
The cho ice  between s e v e r a l  p o t e n t i a l  pa r s ingsof an  i n p u t  s t r i n g  is detennined by a scheme for computing t h e  qemanticd e n s i t y  of t h e  pa r se  based on t h e  dumber and types  of matches t h a t  a r eobtainkd by f i l l i n g  t h e  s l o t s  i n  t h e  t r i p l e .
Assuming Engl i sh  i s  aredundant language, tbe par se  having t h e  g r e a t e s t  dens i ty  is t h e  onewhich con ta ins  t h e  most  i n t e rconnec t ions  and, t he re fn re ,  is  t h e  one tobe  accepted as the  c o r r e c t  pa r se .An important  f e a t u r e  of Wilks' system is  t h a t  s y n t a c t i c  propert iciso f  words t a k e  a r o l e  secondary t o  t h a t  assumed by t h e  semantic  p rope r t i e s .Thus i t  is  p o s s i b l e  t o  determine t.he meaning of  i n p u t  which i s  ill-formedand grammatically i pco r rec t .Augmented t r a n s i t i o n  networks have been used f o r  &me t i m e  a s  amethod f o r  parsing.
The p r i n c i p a l  system u t i l i z i n g  t h i s  method is t h eNASA l u n a r  rocks  system developed by Woods [20].
The p a r s e r  used by thLNR group [lo] is  of  similar design.
While t h i s  technique is veryflexible i n  a l lowing a l a r g e  grammar t o  be c o n s i s t e n t l y  modelled, mostimplementations of it  have been s t r i c t l y  s y n t a c t i c .
L i t t l e  a t tempt  hasbeen made t o  inco rpora t e  semantic  knowledge i n t o  t h e  pars ing .
Withoutsemantic  knowledge, and us ing  the s t r i c t  grammatical r u l e s  embedded i nt h e  t r a n s i t i o n  ne'tuorks, t h i s  approach is  r e a l l y  very b r i t t l e .
I t  is no tcapable  of handl ing  ungrammatical i npu t  w i t h  much success.- i n -Clqsely r e l a t e d  t o  t h e  augmented t r a n s i t i o n  network approach is  t h euse of procedures t o  desc r ibe  a g r a m a r  a s  exemplif ied hy Winograd's pro-gram [lg] .
This  system is a l s o  predominately s y n t a c t i c  i n  nature.
A l linformation about t h e  grammar is represented  i n  terms of  a c t u a l  t o u t i n e swhioh a r e  invoked during parsing.The input  ana lyze r  used i n  Colby's A r t i f i c i a l  Paranoia modbl.
[ 5 ]is e s s e n t i a l l y  a p a t t e r n  matcher which uses  a few t r i c k s  t o  normalizea l l  i npu t  t o  s h o r t  s t r i n g s  whibb it hopes t o  recognize.
It i s  no t  r e a l l ye i t h e r  s y n t a c t i c a l l y  o r  semanr lca l ly  based bu t  depends mostly on t r ans -formations t o  reduce inpu t  t o  s imple,  empi r i ca l ly  de r ived ,  recognizableforms.
The power of  t h i s  approach l i e s  i n  i t s  a b i l i t y  t g  accept  event h e  most ungrammatical i npu t  and r e l a t e  i t  t c  sodeth ing  which is a l r eadyknown.
Thus, t h e r e  is a given context  i n  which a l l  inpu t  is  in terpre ted--t he  con tex t  of what t h e  model knows and wants t o  cont inue  t a l k i n g  about.1.3.3 OUTPUT GENERATIONPerhaps t h e  work on output  product ion  which has  most inf luenced t h ec u r r e n t  program i s  t h e  model by Colby [ 5 ] .
A s  descr ibed  i n  t h e  previoussec t ion ,  i n p u t  is recognized by reduct ion  t o  s imple  i d e n t i f i a b l e  p a t t e r n swhich can b e  matched t o  p re s to red  s t r i n g s  i n  t h e  program's memory.
Alsoinc luded i n  t h e  memory a r e  corresponding sets of s t r i n g s  which a r e  t o  beused as output .
Fixed responses a r e  s e l e c t e d  based on t h e  c u r r e n t  s t a t eof t h e  program's s e l f  model; t he  s t a t e  of i t s  mgdel of t h e  person and t h eprevious conversat ion.
Th i s  technique g ives  t h e  appearance of a normal,humanlike dialogue.
Thfs approach is success fu l  because conversa t ion  isalways d i - r ec t ed - in  one p a r t i c u l a r ,  very  narrow, d i r e c t i o n  by t h e  way inpu ti s  understood.
The context  is f i x e d  and must no t  d e v i a t e  from a s i n g l e2.
PROGM1 OVERVIEW2.1 PROGRAEi STRUCTUREThe l o g i c a l  s t r u c t u r e  f o r  t h e  program c o n s i s t s  of  c s s e n t i n l l y  fou rmain s e c t i o n s  executed  i n  sequence wi th  each s t e p  producing informnt iour e q u i r e d  by t h e  fo l lowing  s t e p .
This  des ign ,  w i th  minor v a r i a t i o n ,  i sc h a r a c t e r i s t i c  o f  many e x i s t i n g  programs f o r  language understanding.One no t a b l e  except ion  has  been the  SPEECH UNDERSTANDING PIIOJKCT [8]which advocates  t h e  use of p a r a l l e l  nodules  working s i~nu l tnneous ly  onrhe i n p u t ,  p a s s i n g  d a t a  f r e e l y  between r o u t i n e s ,  u n t i l  t h e  d e s i r e d  endr e s u l t  is reached.The fo l lowing  a lgo r i t hm d e s c r i b e s  how JIMEfY3 behaves q$ t h e  m B s ts u p e r f i c i a l  l e v e l .DO u n t i l  person i s  through t a lk ing :(1)  .
Request u s e r  i npu t  and t r a n s l a t e  Engl i sh  words i n t o  i n t e r n a l. codes (?EMORY node numbers).
(2)  .
P a r s e  i n p u t  t o  c r e a t e  t h e  "best"  p a r s e  network(s) .
(3) .
Match each p a r s e  network wi th  s t r u c t u r e s  i n  memory t o  produce.
t h e  "best"  match(es)  ..( 4 )  .
Produce a  response  based on t h e  memory match(es) .END-DOTo make t h e  p roces s ing  of  t e x t  more e f f i c i e h t ,  Engl i sh  words andpunc tua t ion  a r e  t r a n s l a t e d  i n  s t e p  (1)  i n t o  node p o i n t e r s .
Undefinedwords a r e  changed t o  n u l l  p o i n t e r s .The p a r s e r  t hen  c r e a t e s  a p a r s e  network from which a network d e n s i t yis compu~ed.
This d e n s i t y  is a measure of how w e l l  t h e  p a r t i c u l a r  p a r s ec a p t u r e s  t h e  meaning of t h e  i npu t .
The p a r s i n g  procedure is d r iven  byheur is t ics  t o  do a search of the most IikBly parse networks.
In case ofambiguity, several  parse networks can be passed t o  the  !uemurymatch routine.I f  the input was a statement, the  information given by the netpork isstored i n  short  term membry (Sdi) .
If a question is asked, then the parsenetwork is  marched agains t  event nodes i n  the memory s t ruc tu re  t o  f indpotent ia l  answers.
A match score id computed f o r  each pat tern  matchbased on how well  the three  major components (ACTOR, ACT and OBJECT) anda l l  minor components (s ingle  modifiers and preposit ional  phrase modification)match, Only the c loses t  matches a r e  re ta ined fo r  use by the outputg & e r a t ~ r .
Ambiguity i n  the parse, i f  i t  ex i s t s ,  is  resolved here by these lec t ion of the bes t  match regardless of i ts  generating parse network.The response Is generated by procedures operating on the memorymatches.
The value of the match score determines generally what theresponse content should be.
The exact form of the response i s  determinedby.
theway the components of the input matched (or did not hatch) themalory pattern.
After  a form is decided on, the  response i s  madeg ra rnmat ica lmi ihen  printed.
Control is then returned to  the inputroutine and the w N e  sequence is  ready t o  be repeated.2.2 TI= RUNNING PROGRAMThe environment, the fime-sharing system on the  Honeywell 66/60running GCOS a t  the University of Kansas, i n  which t h i s  program wasdeveloped and is  run dic ta ted  its form..
This machine poses severa lproblems besides the  lack of su i t ab le  languages, the most ser ious  beingthe 25K word l i m i t  on the amount of memory an in te rac t ive  programcan obtain.Since the re  a r e  no i n t e r a c t i v e  s t r f n g  ar l is t  processin2 languagesava i l ab le  on the system, only FORTRAN came c l o s e  t o  s a t i s f y i n g  t h erequirements of a language i n  which a natura l  langpage system could ber e a l i s t i c a l l y  implemented.
What was wanted was a high l e v e l  languagewith overlay c a p a b i l i t i e s ,  s m a l l  but powerful I/o packages and thef a c i l i t y  f o r  independently compiled and t e s t e d  subrout ines  with u t i l i t i e sf o r  the  maintenance of subrout ine  l i b r a r i e s .The program now running cons i s t s  of over 130 subroutines w r i t t e n  i na t ~ t a l  of about 13K l i n e s  of FORTRAN code.
It runs i n  approximately20K (36-bit) words of memory when segmented i n t o  5 overlays.
Anunlinked vers ion  i s  approximately 37K words i n  s ize .
The use  of coreby various p a r t s  of t h e  program i s  given i n  Table 1.Responee time f o r  t h e  pragfam is  goad considering t h e  amount ofoverhead required because of memory cons t ra in t s .
Dialogue l i k e  t h a tgiven e a r l i e r  t akes  2-3 seconds between the l a s t  charac te r  of inputu n t i l  t h e  answer s t a r t s  t o  P r j n t .TOTALS1 / 0  PACKAGEDATASCRTCH ar rayMEMORY Paging a r e aWORDLIST a reaMiscellaneousPROGRAMMain l i n k  + support r o u t i n e s  10.2I n i t i a l i z a t i o n  & Setup 1.9Command processor 3.3Parsing 5-3Memory matching 6.2Output production 3.3 -------- 30.2Table 1: Storage a l l o c a t i o n  f o r  unlinked program i nthousands of Honeywell 66/60 words-17-2.3 DATA FILESTwo data f i l e s ,  WORDLIST and MDIORY, a r e  required by the program.The WORDLIST f i l e  contains t he  tat representation fo r  a l l  words*punctuation and system commands along with the keys fo r  t ranslat ing tha ttex t  in to  memory node numbers.
The MEliORY f i l e  is the col lect ion of a l lmemory nodes.To aid i n  the recreation and continual updating of these f i l e s ,  thedata ~ m t a i n e d  i n  them is present i n  a text  f i l e  which is maintained on-l i n e  using the time-sharing tex t  edi tor .
After changes to t h i s  primaryf i le ,  are made, a program, separate from JIMMY3, t rans la tes  the tex t  codein to  the WORDLIST and 1lEMORY f i l e s .
A second program can unload theWORDLIST and MEMORY f i l e s  back to tex t  whennecessary.
Currently thetext  f i l e  contains a vocabulary of 387 en t r ies  (t~oltds, punctuation andcommands), 22 ACT usages and 7 facts .
This information is encoded i napproximately 4000 l i n e s  of symbolic node representation.
It, whenloaded, creates a WORDLIST of 387 seoarate en t r ies  and a MMORY withapproximately 800 memory nodes.3.
MEMORY STRUCTUREThe memory system fo r  JIMMY3 is an aggregation of four components:WORDLIST, MEMORY, temporary s t ructures  and STM.
With the exception of theWORDLIST, each part  i s  a collection of one o r  more substructures consist-ing of nodes connected by relations.-18-3.1 COEPONENTSWORDLIST.
The WORDLIST is an index i n t o  t h e  tEMORY component endcons i s t s  of r epresen ta t ions  f o r  a l l  u n i t s  of input  JIMMY3 is  t o  recognize.Items not  included i n  the  WORIILIST a r e  declared undefined by the  inputdecoding routines.
I n  add i t ion  t o  the  exact  t e x t  representa t ion,  apo in te r  t o  the  corresponding node i n  MEMORY is given.MEMORY.
This is  t h e  model's long term memory--essentially a collec-t i o n  of i n t e r r e l a t e d  nodes.A node is  t h e  smal les t  packet of information i n  MISElORY t h a t  can Bereferenced by a s i n g l e  po in te r  ( e i t h e r  a WORDLIST pointer  o r  a po in te rfrom another node.)
The information contained i n  a node may descr ibe  o rmodify a s i n g l e  word o r  symbol o r  may be a c o l l e c t i o n  of r e l a t i o n sconnecting s e v e r a l  nodes i n t o  a more complex s t ruc tu re .information s to red  i n  nodes describing s i n g l e  words is  var ied andincludes such items a s  p a r t  of speech, i n f l e c t i o n a l  v a r i a t i o n s  and subseto r  superset  names.
Other types  of data ,  f o r  exampld, events and s t a t e -ments of f a c t ,  a r e  formed by nodes t h a t  contain polnter  s t r u c t u r e sr e l a t i n g  o ther  nodes i n  a predetermined fashion.TENPORARY STRUCTURES.
There a r e  th ree  b a s i c  memory s t r u c t u r e s  of at r a n s i e n t  na tu re  t h a t  can e x i s t  during t h e  processing required t o  deter-mine meaning and produce an answer.
They are :  a )  the  pa rse  network,b) the  memory match s t r u c t u r e ,  and c) t h e  output production l is t .A parse  network is a smal l  co l l ec t ion  of nodes of t h e  same format,and connected i n  the  same fashion,  a s  the  nodes i n  MEMORY.
Produced bythe parser ,  i t  is  used t o  represent  the  meaning of input t ex t .
This net--1 9-work canta ins  information about the  major components of the  input ,  i.e.,what wards o r  phrases represent.
the  ACTOR, ACT and OBJECT; i t  a l s o  conta insi n f o v a t i o n  abwY dl modifying words and phrases Found i n  the  input.A memory match s t zuc tu re  is  produced by matching the parse networkwith MEMORY.
It conta ins  comparison da ta  r e l a t i n g  t h e  correspondingcomponents of the  input ( ~ a r s e  hetwork) and a subs t ruc tu re  i n  MEMORY.T ~ Q  output production l i s t  is used during the  examination of t h ememory match s t r u c t u r e s  t o  accumulate the  d i s c r e t e  components of t h eresponse t o  be made, i.e., the  words and punctuation f o r  t h e  answer t othe q u e s t f ~ n .
Each element i n  t h i s  f i s t  contains a po in te r  t o  a t e x trepresenta t ion f o r  the  word (or punctuation), its funct ion and i t sr e l a t i o n  t o  o the r  words i n  the  sentence.
When the  output l ist  has  beenformed, it  i s  passed t o  a p r i n t  rou t ine  which w r i t e s  the  answer t o  theterminal.STM.
STM is t h e  s h o r t  term memory component f o r  the  program.
Itis e s s e n t i a l l y  a co l l ec t ion  a rea  f o r  parse  networks, memory matchs t ruc tu res  and output production lists generated f o r  previous inpu t sand t h e i r  responses.3.2 RELATIONSAmong the a t t r i b u t e s  used t o  r e l a t e  the  d i f f e r e n t  MEMORY and parsenetwork nodes t o  each o the r  a r e  those  f o r  descr ib ing h ie ra rch ies  andchains.HIERARCHY.
Hierarchies a r e  v e r t i c a l  s t r u c t u r e s  formed by r e l a t i n gnodes t o  one another using the  ISA a t t r i b u t e .
Set  (superse t ,  subset)r e l a t i o n s  a r e  implement& as hierarchies .
A t y p i c a l  example would be-20-t h e  path through t h e  nodes: BR,ANDT (ISA) BOY (ISA) PERSON which d i sp layst h e  r e l a t i o n s  among t h e  t h r e e  nodes.I n  a hierarchy,  each node is connected only t o  a s i n g l e  nodeimmediately be fore  and a f t e r  it.
However, f o r  any given hierarchy,  i tis poss ib le  t h a t  t h e  node9 involved a r e  connected t o  o thers  both i n  ando u t  of t h e  hierarchy by connections independent of t h a t  p a r t i c u l a rh ierarchy s t r u c t u r e .T r a n s i t i v i t y  is a property of h i e r a r c h i e s  as implemented by t h eprogram.
Therefore,  i n  t h e  example slbove, t h e  information, BRANDT (ISA)PERSON, is  i m p l i c i t  dn t h e  hierarchy.
It should be pointed our t h a t  i nt h e  forward l i n k  between BRANDT and BOY, t h e  connection is  n o t  one ofset membership but  r a t h e r  one of subse t  as is the  r e l a t i o n  between BOYand PERSON.
This  is  accomplished by the  concept of GENERIC nodes evenf o r  s p e c i f i c  ins tances  o f ,  say, people.
Therefore,  t h e  BRANDT i n  t h ehierarchy is a GENERIC node which w i l l ,  i n  tu rn ,  have under i t  anINSTANCE node of BRANDT which i s  r e l a t e d  by set  membership.
The GENERICBRANDT is a set of one element.CHAIN.
The mechanism f o r  const ruct ing hor izon ta l  s t r u c t u r e s  isthe chain.
A s  contras ted t o  t h e  be fore  l ink-a f te r  l i n k  s t r u c t u r e  oft h e  hierarchy, t h e  chain allows t h e  c rea t ion  of a l i n e a r l y  ordered setof nodes each r e l a t i n g  t o  a common "root" node t o  which i t  i ss  attached.This allows t h e  c r e a t i o n  o f  s e t s  of nodes r e l a t e d  by a common property.For example, cha ins  e x i s t  i n  MEMORY tying toge ther  a l l  i n f l e c t i o n a lv a r i a t i o n s  of a word.
Another example of a chain is t h e  l ist  of co lo rnodes: RED, BLUE and GREEN.
A s ing ly  l inked list exdsts  through thesenodes b u t  i n  addi t ion,  each of t h e  t h r e e  po in t s  back t o  t h e  node f o rCOLOR--the node which po in t s  t o  t h e  f i r s t  c o l o r  i n  t h e  list.The more important l i n k s  cur ren t ly  used t o  form chains a r edescribed next.DEFN.
The DEFN l i n k  i s  used t o  t i e  together a l l  nodes t h a trepresent  d i f f e r e n t  d e f i n i t i o n s  for  a word.
Thus, f o r  every symbol i nthe  WORDLIST, the re  is a chain of nodes i n  MEMORY connected v i a  t h eDEFN l ink..
.'
- [  U]ISA\ +j : -3 To next  nodctcprcscnLing as u b s e t  oE PERSON.S " " " '  ""Ir IIMND'SI \ h............Flgure 1 : H i  cra rcliv C t l r  RRANIIT ( I S A )BOY ( 1 S A )  PERSONINST.
A chain emanates from each d e f i n i t i o n  of each word i n  MEMORY.These chains a r e  created w i t h  t h e  INST l ink  and represent  the set of a l lins tances  of the  word given by t h e  roo t  node of t h e  chain.-22-ISA.
This l i n k  is used t o  generate h ierarchies .
Actually createdis  a chain with the  property t h a t  every node i n  the  chain is re la ted  bythe  ISA l i n k  t o  the root  node.
The root  node can be a member ofanother ISA chain, thus giving a mul t i level  l i~e ra rchy .
Figure 1 showsp a r t  of the hierarchy f o r  B W D T  (IsA) BOY (ISA) PERSON.ACTOR.
This is used t o  connect a s e t  of ACTION nodes i n  which theroot  node is used a s  the  ACTOR of a t r i p l e .
For example, the re  would bea chain through the  t r i p l e s  representing (BRANDT IUVE BOOK), (BRANDT LIKEMILK) and (BRANDT HAVE CAT).ACT.
Used t o  form s e t s  of ACTION nodes which contain the  root  nodeas the  4CT of a t r i p l e .OBJECT.
Similar t o  the  ACTOR and ACT l i n k s  except i t  chains throughACTION nodes which.
have the root  node a s  the  OBJECJ of a t r i p l e .MODIFY.
This i s  used t o  speci fy  a s e t  of nodes which a r e  modifiedby the  root  node.
The type of t h i s  s ing le  word modification i s  p a r t i a l l ydetermined by the  p a r t  of speech of the  root  node and t h a t  of the  modifiednode.
It i s  f u r t h e r  speci f ied  by the  hierarchy of t h e  root  node and thenode being modified, For example, RED may be used t o  modify BOOK.
I nterms of grammatical function, the  p a r t s  of speech specify t h i s  a s  anadject ive  modifying a noun.
However, examination of the h ie ra rch iesd isc loses  t h a t  RED (ISA) COLOR which is  a property tha t  TIIINGk (BOOK (ISA)THING) can have.POS.
This is  the  p a r t  of speech l ink.PREP.
This l i n k  is used i n  a CONTEXT node t o  point  t o  the  preposit ionp a r t  of the  preposi t ional  phrase represented by the  node.-2 3-POBJ.
The CONTEXT node a l so  contains the  reference t o  a preposi t ionalobject.CNTXT, This is the  l i n k  used t o  a t t ach  a CONTEXT node a s  a modifterof another node.
The chain created by a CNTXT l i n k  represents  a l l  nodesmodified by the  same CONTEXT node.INFLEC.
Most words i n  MEMORY belong t o  an INFLEC chain.
This is as e t  of i n f l e c t i o n a l  va r ia t ions  of the  word.
It brings together d i f f e r e n tforms which vary in tense,  number, person, e tc .
The I N n E C  chain through"I" would jo in  nodes f o r  I. m., MY, and MINE.
A similar chain throughOWN would l i n k  nodes f o r  OWN, OWNS, OWED and OWNING.ANTONYM.
A l l  antonyms of a word a r e  chained together  using t h i sl ink.
The words i n  the  chain a r e  not  antonyms of each other.PARTS.
This l i n k  is used i n  nodes t o  express the  sub-part, super-pa r t  re la t ionships .3.3 SUBSTRUCTURESUsing the r e l a t i o n s  f o r  constructing h ie ra rch ies  and chains, var iousother ,  more complex, s t ruc tu res  can be created.
These a r e  s t r u c t u r e sformed by t h e  coincidence of several  h ie ra rch ies  o r  chains passing througha s ing le  node, Of a l l  poss ible  subst ructures ,  t r i p l e s  (ACTION nodes),CONTEXT nodes and SEMANTIC MODIFICATION nodes a r e  of g rea tes t  importaae .TRIPLES.
A t r i p l e  (ACTION node) is a node through which passes th reeseparate chains, one each f o r  ACTOR, ACT, and OBJECT.
These t r i p l e s  a r eused t o  specify events, f a c t s  and a s  semantic d e f i n i t i o n s  f o r  t h e  ACTSii.e., as ranges f o r  acceptable candidates f o r  ACTORS and OBJECTS.)
Thechains f o r  ACTOR, ACT and OBJECT o r i g i n a t e  i n  nodes which represent thosemajor components and continue through t h i s  node t o  where they merge i nd i f f e r e n t  combinYtions with s t i l l  o t h e r  chains  t o  form more t r i p l e s .Figure 2 show$ the  re levant  stsructure of a t r i p l e  represent ing "PERSONHAVE BOOKI~.CONTEXT NODES.
A c l o s e  r e l a t i v e  of t h e  t r i p l e  is  the  context node.It too has chains passing through it determining i ts s t ruc tu re .
However,i t  has only two chains: those  fo r  PREP and POBJ.
These spec i fy  a pre-pos i t ion  and i t s  object .
Nodes of t h i s  s o r t  a r e  used a s  modif iers  ofs i n g l e  nodes, t r i p l e s  and o t h e r  context nodes.PERSON., .
.
.c3:.
.
"-:-r-i ACTOH [:.
... .
.. .
..: IIl o  n r x t  t r i p l e  u d n ql'1'.1?501\1 a s an ACTOI?.I-MVE.
.. :,..' YbI ACT[- lo  n e x t  t r l p l f ?
u s i n 7I-L$Vl: a s  an ACP.
:'.
.
.
.
.
.
.
.
.
.ro n o x t  t r i p l e  u ,'in3f 3 O O K  a .c a n  013J LC;-I .,,y IlOOK.. .
.
.
.
.I .I* .
.... 0 .
...*Figure 2: A t r i p l e  represent ing (PERSONIlAVE BOOK).-25-SEMANTIC MODIFICATION NODES, These nodes a r e  not  s i m i l a r  t o  t r i p l e so r  context  nodes e i t h e r  i n  design o r  i n  function.
However, they a r e  oneof t h e  major subs t ruc tures  appearing i n  MEMORY so  w i l l  b e  considered he reb r i e f ly .
These nodes o r  co l l ec t ions  of nodes a r e  t h e  d a t a  s t r i n g s  whicha r e  used t o  d r ive  the  pa r se r  and i n  t h a t  capaci tyl  w i l l  be described l a t e r .Essen t i a l ly  they contain ordered lists of semantic ca t egor i e s  represent ingp o t e n t i a l  modif icat ion pa t t e rns  f o r  words, These lists a re  appl ied  by theparser  t o  determine semantical ly acceptable  word s t r i n g s  i n  the inpu t  t h a tcan moaify o the r  word s t r ings .
For example, a noun would have a listdescribing the  types of  ad j ec t ives  t h a t  could modify it.
Tor ad jec t ives ,t h e r e  would be a list of p o t e n t i a l  adverb types.
Each d e f i n i t i o n  of everyword i n  MEMORY t h a t  i s  t o  be recognized during pars ing  of input  must haveat tached t o  i t  a semantic rnudification node.
I n  cases  where many wordshave t h e  same node, t h e  s t r u c t u r e  f o r  semantic modif icat ion is  implementedas a chain th rough-a l l  words having t h e  same modif ica t ion  requirements.3.4 NODESNodes i n  MEMORY are represented a s  f ixed-size blocks of contiguous d i s ko r  co re  loca t ions  and are the  smal les t  u n i t s  of MEMORY t h a t  can be refer -enced.
Each node is composed of l i n k s  and v a r i a b l e  length  a t t r i b u t e s  whichmay be da t a  o r  po in t e r s  t o  o the r  nodes.
A l l  t h e  space a l loca t ed  to  a nodedoes n o t  have t o  be used.
I n  f a c t ,  most nodes use only  p a r t  of t h e i ra l l oca t ed  space t o  conta in  a t t r i b u t e s ;  t he  remainder is empty (zero).
Thecurrent  model uses a s i z e  of 8 words f o r  its nodes.
(This s i z e  w a s  no tdetermined ebnpirically a s  t h e  optimum s i z e  but  was, ins tead ,  s e l ec t edbecause of d i sk  hardware cons idera t ions , )  The f i r s t  word of  every node i nMEMORY is used f o r  bookkeeping and, t he re fo re ,  is no t  ava i l ab le  f o r  s t o r i n g-26-a t t r i bu te s .
Contained i n  t h e  f i r s t  word is  the  node number i t s e l f ,  anind ica to r  of &he kind of node p lus  a poin ter  t h a t  gives The next  word ofthe  node ava i l ab le  f o r  use a s  a 3 Lnk o r  va r i ab le  length a t t r i b u t e .Whenever a s i n g l e  node has more a t t r i b u t e s  than i t  can conta-in, addi t ional8-word blocks a r e  a l loca ted  a s  extensions of the o r i g i n a l  node, Theseextensions are  connected i n  a chain t o  the o r i g i n a l  node by t h e  CONT l i n kand a r e  t ransparent  t o  a l l  t he  program except f o r  t he  most bas i c  MEMORYmanipulation routines.There a r e  e igh t  d i f f e r e n t  kinds of nodes i n  memory, each with i t sown function.
The Kinds a r e  TYPE, SIMPLE GENERIC, ACTION GENERIC, CONTUTGENERIC, SIlPLE.INSTANCE, ACTION INSTANCE, CONTEXT INSTANCE and SEMANTICMODIFICATION.
Each ha@ a d i f f e r e n t  purpose i n  t h e  representa t ion  of in-formation i n  MEMORY.
Br ief ly ,  t h e i r  purposes a r e  a s  follows.
The TYPEnode is used a s  t h e  reference poin t  between the  WORDLIST.
and MEMORY.
Theth ree  GENERIC nodes a r e  used t o  spec i fy  s y n t a c t i c  and semantic informationassocia ted  with words and subs t ruc tures  given by t h e  INSTANCE nodes.
TheINSTANCE nodes a r e  used t o  represent  ac tua l  ins tances  of words o r  fac ts .SEMANTIC MODIFICATION nodes a r e  used t o  cdntain information required bythe  parser  t o  he lp  d i r e c t  i ts  s e l e c t i o n  of t he  modification pa t t e rns  duringparsing of t h e  input.
The o the r  d i s t i n c t i o n  made on node types is amongSIMPLE, ACTION and CONTEXT.
SIMPLE nodes reprebent  s ing le  words, ACTIONnodes a r e  used t o  represent  t r i p l e s  and CONTEXT nodes a r e  used f o rpreposf t i o n a l  phrases.TYPE.
The 'TYPE nodes in MEMORY a r e  i n  a one-to-one correspondence withthe  e n t r i e s  i n  t h e  WORDLIST and serve  as t he  reference nodes f o r  the WORDLISTpoin ters  t o  MEMORY.
Some a t t r i b u t e s  t h a t  may appear i n  a TYPE node a r e  DEFN-27-and POS.
F i r s t  to appear i n  a TYPE node is an a t t r i b u t e  giving the  t e x trepresenta t ion f o r  t h e  symbol.
An): rou t ines ,  sucb a s  t h e  output produc-t i o n  programs, can g e t  t h e  t e x t  r epresen ta t ion  f o r  p r i n t i n g  d i r e c t l y  fromthe TYPE nodes.
This t e x t  is repeated here  s ince  the  symbol given ilP theWQRDLIST is i n  6 charac te r  chunks l inked together  -- a form not s u i t a b l ef o r  p r in t ing .
The second a t t r i b u t e  always present  is the  DEFN l i n k  usedt o  chain together a l l  d e f i n i t i o n s  of t h e  symbol.
Only words andpunctuation w i l l  have non-null chains of d e f i n i t i o n s .
For words, t h e r eis a SIMPLE GENERIC node i n  the  DEFN rliilrn f o r  each d i f f e r e n t  wordusage.
For punctuation, t h e r e  is  a s i n g l e  SIMPLE GENERIC node chainedt o  the TYPk node.
System commands and s e t  names have a n u l l  Set  acrusages s i n c e  information of a more d e t a i l e d  na tu re  f o r  them is  notrequired; Figure 3 shows the  r e l a t i o n s h i p  between TYPE nodes and theWORDLIST and between TYPE- nodes and SIMPLE GEIJERIZ nodes.Among tne opt iona l  a t t r i b u t e s  used i n  a TYPE node is t h e  SYSSET l i n kused t o  chain together  a l l  TYPE nodes which name i t e m s  i n  t h e  set, Anexample of the use of SYSSET is f o r  p a r t  of speech.
I n  t h e  TYPE nodef o r  t h e  symbol i s  t h e  root  f o r  t h e  chain througli t h e  TYPE nodes f o rNOUN, PRONOUN, VERB, e t c .The' POS l i n k  is present  i n  the  TYPE nodes f o r  words t h a t  name t h evarious grammatical p roper t i e s  (s ingular ,  nominal, e t c . )
and p a r t s  ofspeech (noun, e tc . )
.-28-MEMORYI - 1I --------III1 BOOK 1- - * TYPE nodef o r  ROOK.S LEPLE GENERICnode f o r  oneusage of  BOOK.SIMPLE GENERICnode f o r  a secondusage of BOOK.Figure 3: A segment o f  MEMORY showing t h e  kJa majurfunc t ions  of the  TYPE node.SIMPLE GENEKIC.
These nodes are used t o  represen t  t h e  d i f f e r e n tusages f o r  words, i.e., 40 s e r v e  mainly a s  a source  f o r  semantic ands y n t a c t i c  information about a word.
A l a r g e  v a r i e t y  of a t t r i b u t e s  canappear i n  a SIMPLE GENERIC node, such as DEFN, IRST, ISA, POS, SYSMOD,INFLEC, SYNONYM, ANTONYM, ACTOR, ACT, OBJECT, MGDIFY, and PARTS.
O fthese ,  only two a r e  required.
The DEFN l i n k  must be p resen t  t o  t i e  t h i susage of t h e  word wi th  i ts TYPE node and t o  cont inue t h e  chain  t o  t h enex t  usage, i f  any.
A l s c ~ r e a u i r e d  is  t h e  part of  peech l ink ,  POS.Of the other  a t t r i b u t e s  tha t  can appear, two of the more important: nnesa r e  the  INST and ISA l inks .
The INST l i n k  is used t o  c rea te  the  chainof spec i f i c  instances of t h i s  word represented by SIMPLE INSTANCE nodesTo create  h ierarchies  within the  s e t  of SIMPLE GENERIC nodes, the ISAl i n k  is  used.
Although a c h  ISA l ink  belongs t o  a chain, t h e  presence oftwo ISA l inks  -- one a roo t  l ink,  t h e  other d $on l i h k  -- r e l a t e s  thecurrent node t o  the  ohe immediately above it and t h e  ones below it.The ACTOR, ACT and OBJECT l inks  i n  a SIMPLE GENERIC node point t cACTION GENERIC nodes that  use the  node a s  an ACTOR, ACT o r  OBJECT, ThePREP and POBJ l inks ,  i f  present, point  t o  CONTEXT GENERIC nodes t h a t  uset h i s  node as  a preposit lop o r  a preposi t ional  object .
The INFLEC, MODIFY,PARTS, ANTONYM and SYNONYM l inks  poidt  to  o ther  SIMPLE GENERIC nodes tha t .a r e  re la ted  t o  the current  node i n  the  specif ied  manner.ACTION GENERIC.
These nodes represent the  t r i p l e s  bsed t o  give themeanings f o r  ACTS.
They ushally contain three  mandatory links- those f o rthe ACTOR, ACT and OBJECT chhins.
However, f o r  some ACTS, t he  OBJECT i se i t h e r  not required o r  is optional.
As an example of an ACTION GENERICnode, consider the  ACT "know1*.
In  t h e  curreht MEMORY, i t  has three  t r i p l e sattacbed t o  it  giving the  verb senses of (PERSON OWN THING), (PERSON OWNANIILZL) and (PERSON OWN SLAVE).Only two other a t t r i b u t e s  a re  allowed i n  an ACTION GENEKIC node.These a r e  the  MODIFY and CNTXT l i n k s  which are used t o  specify s i n g l e  wordand preposi t ional  phrase modification of the  ACTION node.
When these  Gwolinks appear i n  a GENENC node, they r e f e r  t o  the po ten t i a l  types ofm d d f  i ca t ion  t h a t  may occur?-30-CONTEXT GENERIC.
Nodes of t h i s  kind always contain exactly threel inks:  PREP, POW and CmT.
Since t h i s  node is used t o  specify po ten t i a ltypes of modification, the PREP and POBJ l i n k s  a r e  used t o  point  t opa r t i cu la r  SIMPLE GENERIC nodes f o r  the  preposit ion and preposi t ionalobject.
The CNTXT l i n k  is  used t o  t i e  t h i s  CONTEXT node t o  the ACTIOKGENERIC node i t  modifies.SIMPLE INSTANCE.
A SIMPLE INSTANCE node is  present i n  MBIORY f o p  eachd i s t i n c t  instance of each word tha t  has been used anywhere na an ACTOR, ACT&,OBJECT, modifier, etc.,  i n  the.
representation of information by ACTIONINSTANCE and CONTEXT INSTANCE nodes, T b r e  is only one mandatory l ink  inthe  SIMPLE INSTANCE node, the  INST link.
However, there  a r e  usually severalmore se lected from the set of ABTOR, ACT, OBJECT, PREP, POBJ, and MODIFYdepending on t h e  uses to  which t h i s   articular instance has been put.
I nthe  case of a l l  l i n k s  except MODIFY, t h e  l i n k  p o i ~ t s  t o  the  ACTION INSTMCEo r  CONTEXT INSTANCE nodes where the  current node is used.
For MODIFY,however, i t  can show where t h i s  node modifies another o r  is modified byanother depending on whether o r  not t h i s  is the  root  l ink.ACTION INSTANCE.
A l l  f ac tua l  information within the  systm.
isrepresented by ACTION INSTANCE nodes.
Fhese nodes are  t r i p l e s  t h a t  bringtogether the re la t ions  between actual  INSTANCES of ACTORS, ACTS andOBJECTS plus  t h e i r  modiffcatioq.CONTEXT INSTAN&.
The modification of ACTION INSTANCE nodes bypreposit ional  phrases is specif ied  by the  use of PREP, POBJ and CNTXT l inksi n  nodes of t h i s  kind.SEMANTIC MODIFICATION.
The s t r u c t u r a l  information necessary f o r  theparser t o  determine correct  forms of modification is given by var iablelength a t t r i b u t e s  t h a t  can occur i n  t h i s  kind.
of node.4.
PARSER4.1 PARSING STRATEGYThe data s t ruc tu re  used t o  drive the  parser is the  c r ip le  (ACTIONGENERIC node) which spec i f i es  the semantics f o r  the  major components ofthe parse.
By applying the t r i p l e  a s  a template t o  the input, the  ACTOR,ACT and OBJECT can be identif ied.A s  the  input is parsed, its meaning is converted' i n t o  a parse networkand a network "score" is calculated Usually there  a r e  several  parsenetworks constructed from a s ing le  input representing d i f fe ren t  meanings ofthat  input.
The b e s t  parse is tha t  one which has t h e  highest score from i t sparse networkA parse network is created from nodes s imilar  i n  design and furrcbion t othose present i n  MEMORY.
Like the MEMORY s t ructures ,  i t  is composed ofINSTANCE nodes of a l l  kinds: SIMPLE, ACTION and CONTEXT (see Figure 4).These nodes a r e  connected t o  one another by the same kinds of a t t r i b u t e s ,e.g., the  ACTION INSTANCE node has l i n k s  t o  the ACTOR, ACT and OBJECTINSTANCE nodes, the  SIMPLE INSTANCE nodes contain MODIFY and CNTXT l inksto other SIMPLE nodes o r  CONTEXT nodes, respectively,  etc.
Because thenormal access paths to  INSTANCE nodes using DEFN and INST l inks  i n  TYPE andGENERIC nodes do not e x i s t  f o r  temporary nodes, the  nodes i n  a parse  net-work are kept track of by a system of pointers a s  shown along the r i g h t  $nFigure 4.
The e n t i r e  network is referenced by the  pointer  word i n  the  upperr igh t  hand corner.
411 references i n  the  temporary nodes of the  parse nef-Ii TEFlPORARY STORAGEII iIIIIIIII.
....
I +.. \.iIII: SG .
.. ....... , I IIIII : (BRANDTI :(.
, ,%, ,J , , , :...........: I !IIIIII: SG I .........
I !
IIIII: ( W E )  I :+- -- 1 - -............. IiIIII :SG.... .....: II i: I: (BOOK) I....................: 4 -  - - I - -iIIII* SG.. : I IIII - y e  : e :.....: (RED) I - -  - - -  - <+*]INST[ ]Figure 4: Parse network for "BRANDT HAVE RED BOOK.
"work t o  GENERIC nodes a r e  by l inks which point to nodes that  a r e  par t  ofMEMORY.
This relates  the input to specif ic  par ts  of memory as well asprovides the required syntactic and semantic properties of the inputwordg for  reference during other stages of parsing.
When complete, theparse network is i n  a format ident ical  to similar s t ructures  i n  MlMORY.This is very important l a t e r  during the matching of input to MEMORY wherecompatibility between the two is necessary.4.2 GRAMMAR4.2.1 ACCEPTABLE INPUT FORMSThe parser has been developed to  correctly handle restr ic ted formsof W;questions and declarative sentences.<DO> ::= DO o r  DID or  DOES: := (< lef t  modification>) ACTOR(<right modification>): := (<context phrase>) (<adverb>) ACT(<adverb>) (<context phrase>).
.- (< lef t  modif ication>) OBJECT(<right modification>)c l e f t  modification> .
,- .
a s t r i ng  of words, usually adjectives, nouns,adverbs acid determiners whichtgodify the itemt o  the i r  right.
::= one of a s e t  of prepositions specified by ana t t r i bu te::= a noun from a p a r t i c u l a r  semantic c l a s s  a sspeci f ied  by an a t t r i b u t e: := essen t i a l ly  the  same as <r ight  modification,but i e  used t o  modify verbs::- an adverb from a set of pa r t i cu la r  wordsspeci f ied  by an a t t r i b u t e  o r  an adverb plusits c l e f t  modification>.Table 2: Grammar f o r  DO-questions and statements.The th ree  components of <ACTOR>, <ACT> and <QBJECT> a r e  iden t i ca l  f o r  theDO-question and statement.
The <DO> component is  found only i n  DO-questions.The question mark and period a r e  the only terminating punctuation symbolscurrent ly  allowed.
A l l  of these components a r e  expanded i n  Table 2 i n  aBNF-like format.Additional r e s t r i c t i o n s  currently imposed upon the input a r e  thefollowing:1.
No r e l a t i v e  clauses a r e  allowed.2.
No compound u n i t s  (subject ,  verb, etc.)
a r e  allowed.Elements i n  Table 2 enclosed i n  < > a r e  non-terminal elements of thegrammar.
Such elements- enclosed i n  ( ) a r e  optional.I n  the de f in i t ion  of <ACTOR> and <OBJECT>, the l e f t  and r i g h t  modi-Eication is  t o  the  i m e d i a t e  left o r  r i g h t  of the  word being modified.
I nthe case of <ACT>, however, the <adverb> and <context phrase> modificationcan occur anywhere i n  the sentence.
Usually the  s ing le  word modifiers a r eadjacent t o  the  ACT but do not have t o  be.
The <context phrase>s usuallyappear at the  beginning of the  sentence o r  somewhere a f t e r  the  ACT.I n  the  case of t h e  l a s t  f i v e  d e f i n i t i o n s  i n  Table 2,  i.e.,c l e f t  modification> through cadverb>, the re  a r e  r e s t r i c t i o n s  on thenodes which a r e  appl icable  at t h a t  point  i n  the  parsing.
Some examplesof t h e  types of modification allowed are :c l e f t  modification> of a noun a blue animal bookc l e f t  modification> of an  adj  e c t i v e  a very blue sky< r i g h t  modification> of a noun a f r i end  of mlnei n  Junet o  the  houseyesterdaynot4.2.2 SEMANTICS AND SYNTAXSemantics and syntax have been in tegra ted  throughout the  parsingprocedure so tha t  both work together i n  the se lec t ion  of appropr ia tewords and phrases out  of which the  pa r se  network is constructed.When matching an ACTION GENEP.IC node t o  the  input,  syntax is  used f i r s tt o  i d e n t i f y  nouns as p o t e n t i a l  ACTORS and OBJECTS.
Then the  semanticaccep tab i l i ty  of each is v e r i f i e d  by comparing the word with the semanticc l a s s  spec i f i ed  by the  t r t p l e .Syntax is checked by a simple matching of the  p a r t  of speech.Proper word order within a grammatical subunit  IS maintained automat ica l lby the  way the  modification requirements a r e  s e t  up.
A word is  deemedsemantically acceptable i f  it matches the  semantic c l a s s  l i s t e d  as arequirement, o r  i f  a word upward i n  its hierarchy matches the semanticc l ass .
For example, suppose the  candidate is BRANDT and the  requiredsemantic c l a s s  l a  PERSON.
I n  the  hierarchy containing BRANDT w e  haveBRANDT (ISA) BOY (ISA) PERSON.
A t  t h a t  polnt  the re  is a match on PERSON,so  BRANDT would be semantically acceptable.-36-4.2.3 PRONOUNS, AMBIGUITY AND UNDEFINED WORDSThe cur ren t ly  implemented vers ion  of t he  program provides f o r  onlyvery simple treatment of pronouns, On input ,  "I" pronouns (I, Me, MY, etc.
)are t r ans l a t ed  t o  the  name of  the  person talking.
"YOU" pronouns a r et r ans l a t ed  t o  JIMMY3.
Similarly,  on output ,  t he  person's name and JIMMY3are t r ans l a t ed  back t o  "YOU" and "I", respect ive ly .Ambiguity i e  not a problem i n  t h i s  model.
If two parse networkshave i d e n t i c a l  scores ,  ambiguity i s  resolved by the  memory matching scheme.A l l  networks are matched aga ins t  lremory i n  the  attempt t o  l o c a t e  an  answer.I f  s eve ra l  matches come ou t  e q u a l l y  l i k e l y ,  they can a l l  be  reported.This  simple-minded approach works w e l l  by r e l a t i n g  the  input  t o  what t heprogram knows.Throughout t h e  program, undefined words i n  the  input  are ignored.However, t h e i r  t e x t  representa t ions  are saved so they can be p r in t edl a t e r  t o  help explain,  say, why the  program was unable t o  i n t e r p r e t  theinput.
A s  i l l u s t r a t e d  by one l i n e  of t h e  sample conversat ian i ns ec t ion  1, t h e  response t o  "DO I HAVE AN OLD BOOK?"
was "YES.
A RED ONE.I DON'T KNOW WllhT OLD MEANS."
T11e input  was interpreted a s  i f  i t  hadbeen "DO I HAVE AN BOOK?
".4 .3  PARSING ALGORITHMThere a r e  two operat ions performed on the  Input t e x t  before it 1sa c t u a l l y  parsed: a )  preprocessing of  t he  text I n  an  e f f o r t  t o11standardize" it and b) t h e  determination of t he  type of input  recelvedso the  proper pars ing  technique can be selected.-37-4.3.1 PREPROCESSING OF THE: INPUT TEXTThe f i r s t  operation perfonaed on the  input is the  t r ans la t ion  ofeach defined component t o  its TYPE node pointer  t o  MEMORY.
Undefinedinput items a r e  converted t o  n u l l  pointers.
Af ter  the  t r ans la t ion  t oTYPE node pointers,  the  input i s  checked f o r  standard greet ings  orc l iches  tha t  usually e l i c i t  a standard response.
Examples a r e  "IIELLO.HOW ARE YOU?
", "HI", "GOODBYE", e tc .The second type of preprocessing is  the attempted reductioh ofwords and phrases t o  simpler forms.
This approach can be used f o r  thereduction of slang, misspelled words, idioms, names, e t c .
Stoled i n  theTYPE nodes f o r  input items t h a t  can be reduced a r e  context s t r i n g s  i nwhich an item can occur p lus  its replacement form.4.3.2 DETERMINING TYPE OF INPUTThis par t  of the parsing algorithm 2s where the kznd of input, i .e .
,DO-question, IS-question, Wh-question, declara t ive  statement, etc., isdeter mined.Questions can be detected by t h e  presence of the  question mark a tt h e n  end.
Discrimination of questions into classes  of YES-NO, o r  Wh-is  determined almost completely by the  f i r s t  word of the  question.
Theonly questions not co r rec t ly  c l a s s i f l e d  by t h i s  approach a r e  those wi thinverted order, e.g., "IiE ASKED A QUESTION, DIDN'T HE?"
and hypotheticalquestions, e.g.
, "IF I ASK A QUESTION.
WHAT WILL YOUR ANSWER BE?
".Any input  t h a t  ends with a period and does not begln with a verb 18c l a s s i f l e d  a s  a statement,Text t h a t  does not  end with e i t h e r  a question mark o r  a perlod is xe-jected with a request t h a t  the person supply punctuation with his  input.4.3.3 PARSING A DO-QUESTION OR STATEMENTThe DO-question and the  statement a r e  parsed i n  iden t i ca l  fashiona f t e r  the  DO-word which begins the  question is s t r ipped off  and the  f i n a lpunctuation i a  thrown away.An exhaustive approach t o  parsing has been selected fo r  implementationra the r  t h a t  one designed t o  use predict ion coupled w i t h  backup f a c i l i t i e s .This decision was made pa r t ly  on technical  grounds--the inherent  d i f f i c u l t yi n  implementing such a parser i n  FORTRAN.
A more important consideration,though, was the a t t r ac t iveness  of working with the input as a whole usingthe  matching of templates (GENERIC ACTION nodes) r a the r  than parsing i n  as e r i a l  fashion whereby components a r e  recognized i n  some left- to-rightdecoding process.The algorithm i n  Figure 5 describes how the  parser  works.DO f o r  a l l  possible combinations of usages of Lhe input words:.
DO f o r  a l l  combinations of reasonable re fe ren t s  f o r  a l l  pronouns:.
.. .
DO f o r  each verb i n  the  input:.
.
.. .
.
DO f o r  a l l  GENERIC ACTION nodes for t h a t  verb:.
a * .. .
.
.
Find su i t ab le  ac to r  and object  candidates.. .
.
.. .
.
.
DO f o r  a l l  combinat~ons of ac to r s  and objects:.
.
.
.
.. .
.
.
.
Create a parse network skeleton.. .
.
.
.
Add modification to  the  parse network.. .
.
.
.
Retam network i f  b e t t e r  than previous one.
* .
.
.
*.
.
.
.
END-DO.
.
.
.. .
END-W. .
.. .
END-DO.
.. END-W.END-DOFigure 5: The parsing a l g a ~ i t h mEssential ly,  the  algorithm is s e t  up t o  produce a l l  possible parsenetworks using the known meanings of the  words.Steps (7), (8), and 1" a r e  t h e  hear t  of t h e  parsing scheme.
Foreach t r i p l e  provided by s t ep  (6), a skeletson consist ing of an ACTIONINSTANCE node and th ree  SIMPLE INSTANCE nodes is constructed a s  a temporary,data s t ructure .
The other  words i n  t h e  input a r e  then attached t o  i taccording t o  the following modification scheme.1.
Apply l e f t  modification t o  the  ACTOR, i.e., form a noungroup tha t  consiste of the ACTOR plus a l l  i t s  modificationtha t  l i e s  t o  i ts  immediate l e f t .
This includes a l l  adject ivesand determiners.2.
Apply l e f t  modification t o  the  OBJECT.
This process isiden t i ca l  t o  t h a t  used to  get  l e f t  modification of the  ACTOR.3.
Find CONTEXT modification of the  ACT.
Preposit ional  phraseswhich modify the  main verb a r e  located.
A s  prepositional objectsa r e  found, they have t h e i r  l e f t  modification at tached by a processiden t i ca l  t o  t h a t  used i n  s t eps  1 and 2 f o r  t h e  ACTOR and theOBJECT.
Note t h a t  no r i g h t  modification is  attempted f o r  ob jec t sof prepositions.4.
Find r i g h t  modification (preposit ional  phrase) f o r  the  ACTOR.5.
Find r i g h t  modification f o r  the  OBJECT.6.
Locate s ing le  word (adverb) modification of t h e  ACT, Thisprocess works from r i g h t  t o  l e f t  through a l l  remaining unattachedwords of t h e  input.As each modification is iden t i f i ed ,  nodes a r e  at tached t o  the  growing parse network.
Simple modification nodes a r e  at tached using theMODIFY a t t r i b u t e ;  phrase modi f~ca t ion  i s  constructed using a CONTEXTsubstructure and at tached with a CNTXT a t t r ibu te .Upon completing the  parsing, t h e  score f a r  the  newly constructedparse network is colnpared with the score f o r  the  previous bes t  network.The higher scoring one is retained t o  the  next i t e ra t ion .The scheme developed f o r  scoring a parse  network is as follows:1.
4-3 i s  added f o r  both the  a c t o r  and t h e  objec t  when they a r ei den t i f i ed .
A network wi th  a n u l l  ob jec t  would ge t  only +3 f o ri ts  actor.2.
Add $1 f o r  each s i n g l e  word modifier.3.
Add +1 f o r  each prepos i t ional  phrase.
Note tha t  this i s  j u s tt h e  prepos i t ion  plus i ts  objec t .
Modification of the ob jec t  scoresadd i t i ona l  points .4 .
After  t he  network is created,  sub t r ac t  + 1  f o r  each word of t heinput ,  including undefined words, t h a t  was not  used i n  the  crea t ionof t h e  network.Upon termination of the  algorithm, the re  w i l l  be a "best" parse  net-work which represents  t h e  meaning of t he  input .
I n  case seve ra l  networkshad t h e  same "best" score, then disambiguation of meaning is deferred,  t obe resolved according t o  the  memory matching process described i n  sec t lon  5.To de t ec t  and con t ro l  the parsing of "garbage1' Input ,  t h e r e  is athreshold value f o r  the  parse  network score  t h a t  must be exceeded beforet h e  parse w i l l  be accepted.
The current  threshold value is zero.
A parsingt h a t  does not exceed t h i s  value 1s re j ec t ed  and leads t o  t he  response ofTo i l l u s t r a t e  t he  way t h e  parsing algorithm works, consider t hequestxon DOES BRANDT OWN A RED ANIMAL BOOK?Afte r  the  DO-question form of t h e  input  i s  recognized, the DOES and thequest ion mark a r e  discarded leavingBRANDT OWN A RED ANIMAL BOOKt o  be processed.
I n  MEMORY, these  w o r d s  have the  following usages:BRANDT - pos-noun; ISA BOY.OWN - posiverb; GENERIC ACTION nodes a r e  (PERSON OWN THING) and(PERSON OWN ANIMAL).A - pos=artxcle.RED - pos=adlective; I S A  COLOR.ANIMAL - posPneun.BOOK - pos-noun; ISA THING.-41-I n  t h i s  example, each word has only one usage so, i n  terms of t h ealgorithm i n  Figure 3, t h e  top l e v e l  loop (1) w i l l  be i t e r a t e d  once.
A ts t e p  (2).
the re  are no pronaun r e f e r e n t s  t o  resolve.
For the  verb "OWN",t he re  are two t r i p l e e  t h a t  must be matched t o  the  input.U s i y  (PERSON OWN THING), i n  step (5) w e  compile a list containingBRANDT as i ts  s i n g l e  entry  t o  be used a s  an ACTOR candidate (BRANDT ( I S A )PERSON).
Similarly, t h e  s e t  of OBJECT candidates contains BOOK s i n c eBOOK (MA) THING.
Now, i n  s t e p  (6),  t h e  only poss ib le  combination ofACTOR and OBJECT, (BRANDT OWN BOOK) i s  formed and passed t o  s t e p  (7)where t h e  skeleton of t h i s  network is formed.In s t e p  (8) t h e  procedure for adding modification i s  applxed.There is  no l e f t  modification poss ible  f o r  BRAEIOT.
llowever, f o r  BOOK,the re  a r e  th ree  words, ANIMAL, RED and A, t o  ~ t s  l e f t  t h a t  have not beenused i n  t h e  pa rse  so far.
A l l  a r e  found t o  nlodify BOOK.The score  f o r  t h l s  parse  network i s  9, 3 each f o r  a semantfcallyacceptable ACTOR and OBJECT p l u s  one each f o r  A, RED, and ANIMAL.
Therea r e  no unused o r  undefined words m t h e  input  so  nothing i s  subtracted.Now, consider what happens when a second meaning of OWN, (PERSONOWN ANIMAL), is  used i n  s t e p s  (5) through (8).
Again, the  s e t  ofs u i t a b l e  ACTORS w ~ l l  be t h e  s ingle ton,  BRAPIDT.The set of s u ~ t a b l e  o b j e c t s  contains only ANIMAL.
Therefore, t h et r l p l e ,  (PERSON OWN ANIMAL), matches (BRANDT OWN ANIMAL) and the nodef o r  ANIMAL allows A and RED a s  m o d ~ f i e r s .
However, t h e r e  1s no way t oa t t ach  BOOK t o  t h e  p a r s e  network.
T h ~ s  second parsing g e t s  a score  of7 (ACMR = +3, OBJECT = +3, A= + 1, RED = +1, and BOOK = -1) .As a second example, consider  the  s t r i n g :DID BRANJ)T WILL THE PROPERTY TO YOU?withBRAM)T - posPnoun; ISA PERSON.WILL - postnoun; ISA PERSON.WILL - posaverb; GA node is (PERSON WILL THING).
(TO PERSON) i s  opt iona l  context  modif icat ion.TElE - pos=a r t i c l e .PROPERTY - pas=noun; ISA THING.TO - pos=preposi t  ion ,JTMMY3 - pos=noun; ISA PERSON.One poss ib le  s e t  of d e f i n i t i o n s  (usages) of the  words wi th  WILL a s  anoun w i l l  not  inc lude  any verbs.
Therefore, t h a t  combination i s  r e j ec t edimmediately i n  s t e p  (3) of t he  pars ing  algorithm.
The o the r  pos s ib l e  s e tconta ins  WILL a s  a verb.Once t h a t  s e t  of usages is  declded on, t h e  parslng 1s straight-forwardi n  t he  manner s l m i l a r  t o  t ha t  used I n  t h e  p revmus  example.
The malord i f f e r ence  I n  t h l s  input  is the  ex is tence  of Lhe phrase "TO JIMMY3 (astransformed from "TO YOU" i n  s t e p  (2)) .
Context modif icat ion f o r  the  ACTis searched f o r  aefore  r i g h t  modif icat ion of t he  OBJECT s o  "TO JIMMY3" i sproperly a t tached  to  WILL I n  t h e  parse ,  However, the node f o r  PROPERTYd id  not  conta ln  any pa t t e rns  f o r  r i g h t  m o d i f l c a t l ~ n  s o  t h a t  phrase couldnot  have been a t tached  t o  t h e  OBJECT anyway.
The complete parse f o r  t h ~ si npu t  has a s co re  of 8.5.
MEMORY SEARCHING5.1 OVERVIEWFigure 6.
General fonh of a memorymatch s t ruoture .The s t ra tegy used i n  MEMORY searching is s imi lar  i n  one respect t othe  parsing procedure.
Namely, during the  search process a s t ruc tu re  isconstructed and a score is calculated t o  measure the degree of s i m i l a r i t ybetween the  question and the  candidate answer.
Kowever, unlike theexhaustive process used i n  obtaining a parse, the memory searching procedurtis ra ther  Gelective and does not examine a l l ,  o r  even a large p a r t ,  ofMEMORY.-44-5.2 THE MEMORY MATCII STRUCTURE5.2.1 GENERAL STRUCTUREFor each match attempted between a parse  network and an ACTION INSTANCEnode i n  MEMORY, a complete, new memory match s t r u c t u r e  is generated.
Thiss t r u c t u r e  has a s t a t i c  component of 8 r e g i s t e r s  plus anywhere from zero t ofour var iable  length,  linked lists attached t o  it  a t  various places --(see Figure 6).
These l inked lists a r e  used f o r  col lec t ing informationabout word modification.5.2.2 ACTOR, ACT AND OBJECT COllPARISON RESULTSThree of the r e g i s t e r s ,  corresponding t o  the ACTOK, ACT and OBJECTcomparison r e s u l t s ,  form the hea r t  of the  memory match s t ructure .
In  eachr e g i s t e r  is recorded t h e  exact kind of match between input and MEMORY.Three values a r e  contained f o r  the  ACT comparison resu l t s :  a pointert o  the SIMPLE GENERIC node i n  )EMORY f o r  the  ACT aiven i n  the  parse network,a pointer  t o  the  list which has the comparison between the modifiers ofinput and of the  METIORY s t ruc tu re ,  and an ind ica to r  pf the type of match.Five possible types of matches can occur f o r  ACTs.1.
(+O) No match.2.
(+5) Exact match.3.
(+4) The input and MEMORY ACTs a r e  synonyms.4.
(+4)  The input  and MEMORY ACTs a r e  antonyms.5.. (+0) The ACT was missing from e i t h e r  the  input o r  the  MEMORYnode.The r e g i s t e r s  f o r  ACTOR and OBJECT a r e  iden t i ca l .
Like the ACT, theycontain three  items of information: a pointer  t o  the SIMPLE GENERIC nodef o r  the  ACTOR i n  the parse network, a pointer  t o  the  modifier list and t h ematch type indicator .
The match type ind ica to r  f o r  the  ACTOR is dividedi n t o  th ree  subunits.
F i r s t ,  the  number, s ingular  o r  p l u r a l ,  of the  ACTORis given.
Second, the  type of reEerence t0  the  input ACTOR is  xecordedas e i t h e r  a apec i f i c  ins tance (i.e.,  i t  re fe r red  t o  a p a r t i c u l a r  ins tanceof the  ACTOR) o r  a s  an i n d e f i n i t e  reference ( referred t o  a c l a s s  ofACTORS r a t h e r  than a s p e c i f i c  one).
Final ly ,  the re  is  the  r e s u l t  of t h ematch between input and MEMORY.1.
(+O) No match.2.
(+5) Exact match-3.
(+4) The input ACTOR is a member of the  s e t  named by theMEMORY ACTOR, i. e., input  ACTOR (ISA) MEMORY ACTOR.4.
(+4) The input ACTOR is the  name of a set f 6 r  which theMEMORY ACTOR is a member, i. e. , MEMORY ACTOR (ISA) input ACTOR.5.
(+4) Input ACTOR matched a synonym of the  MEMORY ACTOR-6 .
(4-4) Input ACTOR matched an ACTOR of another node i n  MEMORY ofthe  form A ACTOR^ BE ACTORZ" where ACTOR1 matched, t h e  input ACTORand ACTOR2 matched the MEMORY ACTOR, o r  v ice  versa,  i.e., inputACTOR BE x and x BE MEMORY ACTOR.7.
(+2) Did not f i n d  an ins tance of the  input ACTOR i n  W O R Ybut did f ind an ins tance of a member of the  set the  input ACTORwould belong t o  3.f i t  had been i n  memory, i.e., t h e  two r e l a t i o n sof input ACTOR (ISA) x and MEMORY ACTOR (ISA) x both hold f o r  somes u i t a b l e  cakegory x.8.
(-I-0) ACTOR missing from e i t h e r  input  o r  MEMORY.5.2.3 WORD MODIFICATION RESULTSRegisters of the  match s t r u c t u r e  corresponding t o  t h e  ACTION, ACTOR,ACT and OBJECT nodes can a l l  have modifier lists attached.The word modification l ist  is s i n g l y  l inked; each ifem on the  listis given by two r e g i s t e r s  and represents  a s i n g l e  modification, e i t h e rs ing le  word o r  phrase.
A l l  words o r  phrases t h a t  modify, say, the  ACTOR,w i l l  be on the  satne list.
However, i f  any of those words is modified by a-46-word o r  phrase,  then i t  w i l l  have a l ist  a t tached containing a l l  its modi-f i e r s .
Thus, f o r  any component of the input,  word modification is r e a l l y  at r e e  of s u b l t s t s  whose s t r u c t u r e  is  determined by the  input word re la t ions .Six r e s u l t s  of  the  match of modifiers a r e  possible.1.
(+O) Was not compared because previous l e v e l  modlficatlon didnot  match.2.
(+2) Exact match.3.
(+2) Exaqt match i f  i n f l e c t i o n s  are ignored, e .
g .
, s ingu la rmatching p lu ra l .4.
(+I) One of t h e  two modification words is a member of the  s e tnamed by the o ther ,  i.e., t h e r e  is an ISA chain leading from one tothe  other.5.
(+I) The two words a r e  both from a s e t  of mutually exclus iveelements, e.g., matching RED d t h  BLUE.6.
(+a) No match because t h e  modifier appeared i n  only one ofthe two (input and MEMORY) pla tes .5.2.4 EXAMPLE STRUCTURESConsider t h e  question:(1) DOES A PERSON HAVE A RED BOOK?This w i l l  be parsed a s  (PERSON MVE BOOK) with RED modifying BOOK.
Thetwo i n d e f i n i t e  a r t i c l e s  w i l l  a l s o  be p a r t  of t h e  parse network but a r e  notmatched s ince  a r t i c l e s  a r e  not  included i l i  any memory s t ructures .
Themashing of t h i s  parse network with a MEMORY s t r u c t u r e  f o r(2) BRANDT OWNS A RED BOOKwould y i e l d  a memory match s t r u c t u r e  wi th  the following propert iee.~ a t c h  score  = 15; Maximum poss ib le  score  = 1.7MEMORY ACTOR = BRANDT (PERSON).Input mode = singular, indefinite.MQlORY mode * shgular.Match type = XEMORY (ISA) input.MEMORY ACT = OWNS (HAVE).Match type = synonymMEMORY OBJECT - BOOK' (BOOK)Input mode = singular, indefinite.MEMORY mode = singular.Match type = exact match.Modifiers:RED - Location = input and M E M WMatch type = exact match.
(i-4 points)(t4 points)(+5 points)(i-2 points)If (1) above is matched against the NEMORY structure representing(3) BRANDT HAS A BLUE ANIMAL BOOKThe following memory match structure will result.Match score = 15; Maximum possible score a 17MEMORY ACTOR = BRANDT (PERSON)Input mode = singular, indefinite.MEHORY mode = singular.Match rype = MEMORY (ISA) input.MEMORY ACT = HAVE (HAS)Match rype = exact match (in?
lections areignored)(+4 points)(t5 points)QEMORY OBJECT = BOOK (BOOK)Input mode - singular, indefinite.MEMORY node = singular.Match type = exact match.
(+5 points)Modifiers :RED - Location = inpm and MEMORY.
(+1 point)(Note: RED matches BLUE as mutually exclusive elementsfrom the same set.
)ANIMAL - Location = MEMORY only.
(+0 points)This second MEMORY node matches the input as well (+I5 score) asthe first because of the slightly better ACT match even though the OWECTis closer in the first case.
It serves to indicate some of the problemsghat are encountered by the procedure during matching.
(1) DO f o r  a l l  pa r se  networks:(2) .
Compute maximum poss ib l e  match score  f o r  t h i s  network.
(3) .
Don't search  f o r  t h i s  network i f  maximum i s  not  good enough.
(4) .
Get  list o f  synonyms and antonyms f o r  p a r s e  network ACT.
(5) .
DO u n t i l  a reasonable match has  been obtained o r  u n t i l  no more.
.
ACTORs can b e  found:.
.
(6) .
.
Se lec t  an ACTOR t o  match on..
.
(7) .
.
DO f o r  a l l  INSTANCES of t h a t  ACTOR:.-?
(8) .
.
.
DO f o r  a l l  ACTION INSTANCES which have t h a t  ACTOR:Create  t he  ske le ton  f o r  a memory match s t r u c t u r e .Compare modif iers  of the  ACTION INSTANCE nodes.Compare ACTORs and t h e i r  modi f ie rs  .Compare ACTS and t h e i r  modif iers  .Compare OBJECTS and t h e i r  modif iers .I f  no good OBJZCTS, then t r y  a l t e r n a t e s .Accumulate t he  t o t a l  match sco re  f o r  the  s t r u c t u r e .Add s t r u c t u r e  t o  t he  l ist  f o r  t h a t  ACT6R INSTANCE.. * .
.. .
.
END-DO.
.
.. .
END-00(17) .
.
Save b e s t  match s t r u c t u r e s  f o r  t h a t  ACTOR.. .. END-DO(18) .
Fur ther  prune t h e  set of b e s t  match s t r u c t u r e sEND-DOFigure 7: The memory search algorithm.5.3 MATCH1 NG #EMORY5.3.1 I3AS IC ALGORITHMThe algorithm used t o  search memory (see Fieure 7) examines a l imi tedsubset of a l l  s t ruc tu res  i n  MEMORY while t ry ing  t o  match the input.
Thesearch i e  r e s t r i c t e d  t o  ACTION INSTANCE nodes i n  E-RY t h a t  have e i t h e rthe  same o r  a c losely  re la ted  ACTOR.
The object  of the  algorithm is t oobtain a small s e t  of the  bes t  matches of the parse network with anACTION INSTANCE node i n  MEMORY.To handle ambiguous input,  i.e., mul t ip le  parse networks, the matchingprocedure must be repeated f o r  each network passed on by the  parser  (seeseep (1)).
Unpromising networks a r e  eliminated i n  s t e p s  (2) and (3).Synonym and antonym lists a r e  compiled i n  s t ep  (4).The termination c r i t e r i a  f o r  MEMORY searching is the  discovery of asu i t ab le  match o r  the exhaustion of t h e  set of s u i t a b l e  ACTORS used t od i rec t  the  search.
The adequacy of the  match between input and MEMORY isthe  memory match score--the accumulation of many component scores  whichmeasure the  s i lp i l a r i ty  of corresponding p a r t s  of two s t ruc tu res .
Theterm$nation c r i t e r i o n  f o r  a s u i t a b l e  match is based on the value of t h i sscore r e l a t i v e  t o  the  network's maximum poss ible  score  determined i n  s t e p(2).
The threshold foL a "suitable1' match is current ly  s e t  a t  70 percentof the maximum )oss ible  score.
The number 70 is  not pe r fec t  i n  any sensebut was se lec ted i n  a trial-and-error fashion.5.3.2 SELECTING SUITABLE ACTORSThe se lec t ion of a c t o r s  t o  Control the  range of the  search procedurei s  designed t o  provide a reasonable set of nodes c losely  re la ted  t o  theinput ACTOR.
This se lec t ion  procedure is used pnly when no ACTION INSTANCEnodes with the  input ACTOR produce s u f f i c i e n t l y  good matches.-50-There a re  f i v e  a l t e rna te  bethods, described below, f o r  gett ing newACTOR candidates.
Not a l l  of these f ive  are  always used, however.
Theones t o  use and the order i n  which they a r e  t o  be applied is  determined bythe  mode and number of the input ACTOR.1.
Search memory of ACTION INSTANCE nodes of the form"ACTOR1 BE ACTOR2" where e i the r  ACTOR1 or  ACTOR2 matches theinput ACTOR exactly.
Collect the unmatched members of a l lthese nodes f o r  use a s  new ACTOR candidates.
For example,suppose the ACTOR, BRANDT, was not successful a t  generating agood match.
Search fo r  INSTANCES of (X BE BRANDT) and(BRANDT BE X) where X is i n  the same general hierarchy asBRANDT , e .g .
, (BRANDT IS SECOND-GWER) .
Now SECOND-GRADERcan be used as a source fo r  more ACTION IHSTANCE nodes to  search.2.
Use a l l  nodes above the ACTOR i n  its hierarchy.3.
Use a l l  nodes below the ACTOR i n  i t s  hierarchy.4.
Use a l l  synonyms of the input ACTOR.5.
Find a l l  INSTANCE nodes tha t  a r e  i n  t h e  same s e t  a s  theinput ACTOR, i,e.,  search the other nodes i n  the ISA chainrooted i n  the node immediately above the input ACTOR i n  i t shierarchy.
For example, f o r  an input ACTOR, BRANDT, we haveBRANDT (ISA) BOY.
Search the cliain which gives a l l  subdetsof BOY, but excludes BRANDT.For spec i f i c  mode input ACTORs, t he  above procedures a r e  executed i nthe order: 1, 2, 4, 5.
Procedure 3 i s  not used f o r  speci f ic  inputACTORs s ince  i t s  purpoce is  t o  f ind speci f ic  instances fo r  general references.For indef in i te  input ACTORs, the procedures a r e  executed i n  the order: 1, 2,3, 4.
Probedure 5 is not used since i t  would lead t o  too diversif ied a s e tof potent ia l  candidates.
For instance, consider PERSON a s  the  input ACTOR.I f  we had PERSON (ISA) THING, and ANIMAL (ISA) THING, ACTORs could beselected tha t  a r e  only vaguely re la ted  t o  the input.This s e t  of procedures can be executed twice.
The f i r s t  time, the  inputACTOR is used as i t  appeared i n  the  input.
The second time, i t s  number ischanged, i.e., f o r  4 s ingu la r  ACTOR, the second tinre through, a11 comparisonsand searches would be performed f o r  its p l u r a l .I n  order  f o r  t h i s  procedure t o  work, t h e  mode of t h e  input  ACTOR mustbe known.
This  is  deterained as follows:1.
An ACTOR is i n d e f i n i t e  i fa )  i t  is an i n d e f i n i t e  proqoun,b) i t  has modif iers  but  none is a d e f i n i t e  determiner( the ,  t h i s ,  these ,  t h a t ,  tlxose) o rc) i t  is p l u r a l  and has no modif iers .2.
An ACTOR is s p e c i f i c  i fa )  i t  is modified by a def iriite determiner o rb) it is s ingu la r  and has no modifiers .6.
PRODUCTION OF OUTPUT6.1 OVERVIEWProblems t o  be solved before "natural"  sounding output  can be produced)a r e1.
what information should b e  used i n  t h e  tesponse to  a giveninput ,2. how should the  response be s t ruc tu red  so  c e r t a i n  p a r t s  a r e  p toper lyemphasized and3.
how can the response be made t o  appear n a t u r a l  i n  t h e  sense ofbeing l i k e  a similar remark a person might make?Clearly,  t he re  a r e  many f a c t o r s  which play a p a r t  i n  how a persondecides what t o  say a t  any tine during the  course of a conversation.
Suchf a c t o r s  include t h e  p lace  of occurrence, t he  reason f o r  t h e  conversat ion,the  r o l e s  assumed by t h e  pa r t i c ipan t s ,  t he  type of information passed andthe motives of the  p a r t i d p a n t s .
These f a c t o r s  have nbt been s tudied ,indeed, have not  even been exhaust ively i d e n t i f i e d ,  during t h i s  research.What  has been done, however, is the  development of  a few simple procedurest h a t  w i l l  generate reasonable sounding answers t o  DO-questions based on ther e s u l t s  .of memory search.6.2 THE OUTPUT PRODUCTION LISTThe only temporary s t r u c t u r e  c rea ted  duriilg tile ou tput  phase is asimple doubly l i nked  List of elements represent ing  t h e  words andpunctua t ion  of t h e  answer t o  be p r i n t e d  (see Figure 8).> :: [ func t ion]  : : [funct ion]  1->.
- \ I [ func t ion  ] 1Figure 8: S t ruc tu re  of t h e  output  product ion listAs t h e  output  product ion r o u t i n e s  analyze t h e  r e l evan t  memory matchs t r u c t u r e s ,  they produce t h e  output; product ion list an element a t  a  time.Each element of  t h e  list has  t h e  ful lowin& two r e g i s t e r  format:l l eg i s t e r  1 - conta ins  t he  forward and backward l i n k s  t o  t h e  o t h e relements i n  tlhe list.Regis te r  2 - conta ins  a  word func t ion  i n d i c a t o r  (represented as[ func t ion ]  i n  Figure 6 )  and a MEMORY p o i n t e r  t o  t h e  GENERICnode f o r  t h e  word (given a s  GEN [ 1).
The p o i n t e r  can be usedby t h e  f i n a l  p r i n t  rou t ines  t o  t r a c e  back through t h e  word'sGENERIC node t o  its TYPE node t o  r e t r i e v e  i t s  t e x t  r ep re sen ta t i on .The word func t ion  is  an i n d i c a t o r  t o  he lp  out  t h e  rou t ine  t h a t  makesthe output  grammatical.
Curren t ly  10  d i f f e r e n t  func t ion  codes whichcorrespond very loose ly  t o  sentence p a r t s  a r e  used.Subjec t  .Verb.Object.P repos i t i ona l  ob j ec t .Prepos i t ion .Modifier of subjec t .Modif ier  of verb.l l od i f i e r  of ob j ec t .l l b d i f i e r  of p r e p o s i t i o n a l  ob j ec t .Other ( inc ludes  punctuat ion) .6.3 PRODUCTION IBTHOD6.3.1 RESPONDING TO INPUT NOT UNDERSTOODWhen the  input is not understood o r  no answer has been found i n  memory,enough information should be returned s o  t h a t  the  person knows why he d idnot  receive the expected response.
The procedure t h a t  is used to  de tec ttha t  s i t u a t i o n  and produce a response current ly  worm as follows:1.
I f  input was not parsed, respond with "1 DON'T UNDERSTANDTHAT."
Then p r i n t  a l l  undefined words i n  the form: I DON'TKNOW IdliAT word1 (OR trord2 OR w r d 3  .
.
. )
IIISANS . "
For example,the input "DOES BRANDT RIDE A BICYCLE?"
would generate theresponse "I DON'T UUDERSTAND THAT.
I DON'T KNOW WHAT RIDE ORBICYCLE I W J S  ."2.
I f  the  input i s  parsed, but there  was a poor, o r  no, memorymatch, then check the  'GENERIC node f o r  the  ACT to  s e e  i f  werespond with "YES", "EIO", "I DON'T KNOW" o r  some "canned"response.
What is searched f o r  is an a t t r i b u t e  which is some-times present i n  ACT nodes and i s  t h e  answer t o  be given whenno match i s  found with kCMORY.
An example is the.
ACT "KNOW".I f  the  program i s  asked "DO YOU KNOW x?"
and x is not  i nMEtlORY, then the response w i l l  be "NO."
r a the r  than "I DON'TKNOW.I f  the input  is parsed and the re  is a good memory match then thefollowing s t e p s  w i l t  be executed.3.
Check the  OBJECT.
I f  the re  i s n o n e  and one i s ' r e q u i r e dby this  ACT then, i f  the re  a r e  undefined words, p r i n t"I DON'T UNDERSTAND THAT."
plus t h e  undefined words.
I fthe re  a re  no undefined words, p r i n t  "I DON'T UNDERSTAND YOURUSE OF act."
Subs t i tu te  the  current  ACT i n  the  output f o r"act" .4.
Check the  ACTUK.
If i t  is not  d i r e c t l y  r e la ted  t o  the  in-put ACTOR but is a member of the  same s e t  Ci.e., ~ r :  wasse lec ted  during the  memory search for  producing a c t o r s  a sgiven i n  sect ion 5.2.3), p r i n t  "I DON'T KNOW."
Iiowever, i nt h i s  case, do not terminate the output production here.
Passthe  good memory match s t r u c t u r e  involving t h i s  a c t o r  on t othe  next procedure t o  be used t o  produce more output,  As anexample, the  inpu t  "DOES BILL IIAVE A BOOR?"
would generatethe  response "I DON'T KNOW.
BRAlJllT U S  A RED BOOK."5.
Check the  ACT.
I f  i t  does not  match, p r i n t  "I DON'T KNOW.
"p l u s  the undefined words, f f any.6.3.2 DETERMINING MODE OF RESPONSEThe answer t o  a DO-question h a s  an o p t i o n a l  i n t e r j e c t i o n  o f  YES o rNO which precedes t h e  answer and is determined a s  follows:1.
S e t  t h e  mode t o  YES un less  t h e  MEMORY ACTOR i s  a subse tof t h e  inpu t  ACTOR.
I n  t h a t  case ,  t h e  response slrould nothave e i t h e r  YES o r  'NO.
This w i l l  occur when s p e c i f i c  d a t aa r e  being used t o  answer a ques t ion  of a genera l  nature .For example, the  inpu t  "DO PEOPLE OWN TlIIIJGS?"
would beanswered by "RRANDT OlnqS A RED BOOK."
The YES is omitteds i n c e  tilc ques t ion  I ~ a s  not  been answered i n  genera l .
Donot  cont inue examination of  the  match f o r  c o n t r a d i c t i o nwhen t h i s  occurs.2.
Search t h e  memory match s t r u c t u r e  looking f o r  contra-d i c t o r y  d a t a  t h a t  would change t h e  mode from YES t o  NO.
I ft h c  inpu t  ACT matched an antonym, s e t  t h e  mode t o  NO.3.
Check t h e  f i r s t  l e v e l  modi f i e r s  o f  t h e  ACT.
Look f o rc o n t r a d i c t i o n ,  i .e.,  modi f i e r s  from t h e  same s e t  t h a t  a r emutually exclus ive .
I f  any a r e  found, r e v e r s e  t h e  s e t t i n gof t h e  mode.
For example, t h e  inpu t  "DID LIRANDT PLAY OUTSIDE?
"which would match t h e  MEMORY node, {'BRANDT PLAYED INSIDE.
"would have a nega t ive  mode since INSIDE and OUTSIDE a r emutually exclus ive .4.
Check t h e  OBJECT f o r  c o n f l i c t i n g  modif iers .
Also changemode i f  t h e  OBJECT is  no t  d i r e c t l y  r e l a t e d  t o  t h e  inpu tOBJECT bu t  is an element from t h e  same s e t  as t h e  inputOBJECT.
Th i s  w i l l  be t h e  c a s e  when matching o b j e c t s  such asGFEEN I300K wi th  YELLOW BOOK o r  OLD BOOK wi th  NEW BOOK.5.
F i n a l l y  check a l l  s i n g l e  word modi f i e r s  of t h e  ACT t h a twere p resen t  only i n  the  inpu t  or IMblORY looking f o r  nega t ivemodif iers .
These would be  words l i k e  NOT, NEVER, e t c .6.3.3 PRODUCING A NORMAL NiSWERA t  t h e  top l e v e l  i n  t h i s  procedure is  t h e  d e c i s i o n  of how muchof t h e  answer t o  p r i n t .
The r u l e s  used are :  a)  i f  t h e  answer is ane x a c t  match of t h e  i n p u t  wi th  r e s p e c t  t o  t h e  three .
major components,don' t  p r i n t  anything except f o r  t h e  YES o r  NO, b) i f  t h e  answer is  an-exact match except  f o r  t h e  OBJECT, then p r i n t  only  the ~ B J E C T  o r  C) i.ft h e  ACTOR o r  ACT does n o t  match, then p r i n t  the- whole MEMORY node givenby t h e  memory match s t r u c t u r e .The list elements r e p r e s e n t i n g  t h e  output  t o  be p r i n t e d  f o r  t h eACTOR o r  OBJECT a r e  generated by t h e  fol lowing procedure.1.
Add t h e m n e r i c  node f o r  t h e  ACTOR (OBJECT) t o  t h e  endof t h e  ou tpu t  production list.
S e t  i ts  func t ion  type t oSUBJECT (OBJECT).2.
Put  t h e  modif iers  i n t o  tlie l i s t  inunediately i n  f r o n t  o ftile ADTOR (OBJECT).
Use only  modi f i e r s  t h a t  were presen t  i nthe'blEMORY node o r  t h a t  rnatched between MEMORY and inpu t .These modif iers  a r e  conta ined i n  t h e  l i n k e d  l i s t  a t t ached  t othe  memory match structure f o r  t h e  ACTOR (OBJECT).3.
Put an a r t i c l e  b e f o r e  t h e  modi f i e r s ,  i f  required.4 .
Add p r e p o s i t i o n a l  modif icat ion o f  t h e  ACTOR (OBJECT) a tt h e  end of t h e  list.
Thi$ i s  accomplished by adding t h ep repos i t ion  followed by t h e  p r e p o s i t i o n a l  ob jec t .
Then t h emodif icat ion on t h e  p r e p p s i t i o n a l  o b j e c t  i s  added be,tweent h e  two.5.
I f  t h e  ACTOR (OBJECT) is t h e  same f o r  both  i n p u t  andlfE3fORY but  some Input-only modif icat ion e x i s t s ,  then add "BUTNOT" p lus  t h e  inpu t  node and t h e  input-only modif iers .
Anexample of t h i s  is  where "A BLUE BOOK BUT NOT NU ANIMAL BOOK"i s  given i n  response t o  "DO I HAVE A BLUE ANIMAL BOOR?
"6.3.4 EWCIBG OUTPUT GRhEalATICALThe procedure used t o  f i x  up t h e  ou tpu t  examines t h e  elements oft h e  output  l ist  and, using t h e  word f u n c t i o n s  a s  s p e c i f i e d  i n  thoseelements and t h e  p r o p e r t i e s  given i n  t h e  words' GENERIC nodes, a t t emptst o  apply the.
four  r u l e s  beluw:1.
JIbIMY3 and t h e  person 's  name g e t  t r a n s l a t e d  t o  thepronouns "I" ind "YOU".
A t  t h i s  s t a g e ,  t h e  form of t h epronoun may b e  wrong.2.
G e t  person ahd number of t h e  pronouns t o  agree  wi th  t h eACT (main verb).
Change a pronoun t o  its possess ive  form i fi t  is used as a modi f i e r  o f  another  sentence element,3.
S e t  t h e  proper ve rb  tense.4.
Convert the o b j e c t s  i n  t h e  sen tence  (main OBJECT andp r e p o s i t i o n a l  o b j e c t s )  t o  ob j e c r i v e  case.-56-A s t e p  i n  making the  output grammatical t h a t  was g i w n  before wasthe  generation of an a r t i c l e  a s  a modifier.
An i n d e f i n i t e  a r t i c le(A o r  AN) is always used; tlie one t o  be se lec ted  is given by the follow'fngr u l e s  :1. uon't use an a r t i c l e  i f  t he  word modified is a pronoun ora proper name.2.
Otherwise, s e l e c t  A o r  AN according t o  t h e  f i r s t  l e t t e rof the  word i t  w i l l  precgde.Following the  completion of t h i s  operation the  output is pr in ted .7.
DISCUSSION7.1 1USITLTS7.1.1 OUJECTIVES METThe ob jec t ives  of t h i s  research,  a s  r e s ta ted  from sect ion 1, a r ethe  development of th ree  components a b l e  to  carry  on a "natural" dialoguewith humans: an extefidible memory model, procedures f o r  determinine t h emeaning of the input and procedures t o  allow t h e  model t o  converse"naturally" with a human.The current  design of memory, although a s t a r t  i n  the r igl i t  d i rec t ion,is f a r  from complete.
Thexe i s  mucll t h a t  cannot now be represented withthe  s t r u c t u r e s  avai lable .
However, t h e  design of memory is f l ex ib le .Extensions can be added t o  represent more complex surface  s t r u c t u r e s  v i aINSTANCE nodes and a l s o  t o  capture more of the  underlying s t r u c t u r e  of theword meanings themselves i n  terms of GENERIC nodes.Other f ea tu res  t h a t  an extended memory model v i l l  need are thec a p a b i l i t i e s  f o r  supporting more e laborate  question answering, t h eprobessing of imperatives and t h e  in tegra t ion  of new information i n t o  the-5 7-ex i s t ing  s t ruc tu res .
We be l i eve  tha t  the  current  design w i l l  allow suchprocedures t o  he developed.The parser  now implemented does a reasonably good job -on ther e s t r i c t e d  input glven i t .
The bas ic  parsing philosophy, i .e .
,  these lec t ion  of one of severa l  poss ible  networks based on semantic densi ty ,is believed to  be a sound way t o  approach the problem of  determiningmeaning.The development of output generation procedures has j u s t  s t a r t e d .The current ly  implemented procedures a r e  not general o r  powerful enought b  handle more than a few.
specia l ized s i tua t ions .
Work is  required i nt h i s  area  t o  f i r s t  develop a l a r g e  number of s p e c i f i c  r u l e s  from whichmore general  r u l e s  can be deduced.We have i d e n t i f i e d  t en  fea tu res  which charac te r i ze  "natural" languageunderstanding processes.
Speci f ica l ly ,  they are the  a b i l i t y .
to:(1) Work with p a r t i a l  i n ?
ormation ( t o  make p laus ib le  inferencesabout missing information, e.g., defau l t  values from frames),(2)  Work with overlapping and conf l i c t ing  information (not t or e j e c t  i t  out of hand, o r  seek only consis tent  information, o rt o  ass ign a l l  t r u t h  value F, but t o  s i f t  through i t  t o  r e j e c tt h a t  which - based on experience o r  knowledge - is implausibleo r  to  temporarily suspend judgement),(3) Retain ambiguity u n t i l  disambituation is  absolute ly  ca l l ed  f o r ,(4) Perform "short" chains of deduct ions ,(5) Engage i n  common sense reasoning (i .e .
,  t he re  is knowledge ofthe  proper t ies  of commonplace ob jec t s ,  events,  e t c  .
),(6) Pose questions i n  order  t o  confirm expecta t ions  o r  t o  e l i c i tmore information about some subjact  mat ter  of personal  i n t e r e s t ,(71 Construct, modify and e x t r a c t  information from a model of thein ten t ions ,  i n t e r e s t s ,  s k i l l s *  motiyations,  e t c .
,  of t h e  o the r  p a r t y ,-58-(8) Interrogate  and update a s imi lar  model of the  a t t i t u d e s ,  be l i e f s ,a b i l i t i e s ,  goals, etc.,  of i t s e l f ,(9) Be sens i t ive  t o  the p l a u s i b i l i t y  of information received (seeNorman's "Charles' Sickens" problem, discussed l a t e r ) ,(10) Be aware of the  context i n  which the conversation is occurring(~lonnan's "Empire S ta te  Building" problem, discussed below, i d e n t i f i e sone kind of con-text; the r o l e s  of the  p a r t i e s  involved, e.g., parent-chi ld ,  teacher-student, superior-subordinate; bureaucratic o f f i c i a l -c l i e a ,  is a second context; other context types a re  no doubt   resent),With JIEIMY3 we have only begun t o  address t h i s  list of a t t r ibu tes .
In  pa r t i -cular ,  JIElMY3 i l l u s t r a t e s  one approach to  coping with items 12 and #3.Other prograpes pose avenues of a t t a d  f o r  o ther  items of the list (e.g., #I,84, #5), but the  extendabil i ty of tlmse programs t o  other fea tures  l i s t e d  i squestionable - a s  i t  is fo r  JIMMY3.
However, by grappling with what weconsider t o  be the fundamental problem of designing the  system from thes t a r t  t o  be extendable, we believe JIlIMY3 can be grown t o  cope wi th  otherf ea tu res  on t h a t  list, and thus approach being a "natural" languageunderstanding system.7.1.2 DOES THE PROGRAM REALLY UNDERSTAND?The answer t o  t h i s  questidn is  plrbbably no.
In the more res t r i c t edsense of ,  can the program r e l a t e  the input t o  something i t  already knows,the answer is yes.
A l l  input surface s t ruc tu res  are matched during thequestion answering phase to  slrnilar s t ruc tu res  i n  memory.
However, givena s ing le  i i p u t  i n  i so la t ion ,  the program does not understand what i t  means.It has no de f in i t ions  f o r  individual  words.
I t  i s  t rue  athat there  arere la t ionships  between words v ia  t h e  chains and hierarchies  but t h i s  is  notenough.
Knowing tha t  BRANDT (158) PERSON does not help a t  a l l  i n  under-standing, i f  the  model does not what a PERSON is,-59-7.2 LIMITATIONSA majori ty of the  model's general  l i m i t a t i o n s  a re  the  r e s u l t  ofdef ic ienciee  i n  the  memory s t ructure .
For instance,  c e r t a i n  types afinput cannbt be parsed because of inadequate s t r u c t u r a l  building u n i tof membry .7.2.1 MEMORY MODELI f  the  current  philosophy of what i s  t o  beas to red  i n  memory i smainrainmd, i .e .
,  only surface  s t r u c t u r e s  a r e  t o  be represented,  then themost ser ious  l imi ta t ion  is  the  i n a b i l i t y  t o  represent  more kinds of s t r i n g sof English.
Nothing more complex than simple (ACTOR ACT OBJECT) f a c t s  r;aunow be recorded.
I n  genefal ,  thexe is no way of r e l a t i n g  d i f fexen t  f a c t st o  show causation, r e s u l t ,  presupposition, e t c .I f  w e  look a t  memory as more than a place i n  which surface  s t r i n g s  a r estored, then there  a r e  many shortcomings.
Nowhere i n  the current  s t r u c t u r ea r e  words given def tn i t ions .
A s  Por ACTs, the  o r i g i n a l  design ca l l ed  f o rthe building of h ie ra rch ies  of ACTS i n  a manner s i m i l a r  t o  h ie ra rch ies  of"thin&$".
Instead ACTS could be broken down to  some b a s i c  pr imi t ives  ofaction.
Dif ferent  ACT5 can then be compared, not  by looking a t  t h e i r  h le r -archies  , but  by comparing t h e i r  common pr imi t ive  ac t ions .7.2.2 PARSERThe s ing le  major l i m i t a t i o n  of the  current  parser  i s  i t s  i n a b i l i t y  t oparse anything other  than simple DO-questions and statements.
This is nota theore t i ca l  shortcoming; i t  simply a question of implementation e f f o r t .Hodeuef, there  a r e  problems ,with f m p a r s e r  Zrrespective of t h i s  majorl imi ta t ion.
These problems a r e  qu i t e  o f t e n  re la ted  t o  de f i c ienc ies  i n  thememory s t ructure .The p a r s e r  w i l l  c u r r e n t l y  f a i l  i f  i t  does n o t  f i n d  a semant ical lyaccep tab le  parse.
I n  cases  such as t h i s ,  i t  should be allowed t o  f ind ,ins tead~ ,  a s y n t a c t i c a l l y  acceptable  pa rse  t h a t  would be marked as asemantic anomaly.Undefined words a r e  now ignored.
The a b i l i t y  t o  make reasonablep r e d i c t i o n s  about t h e  funct ion and meaning of u n d e f i ~ e d  words would bedes i rab le .7.2.3 OUTPUT PRODUCTIONThe most s e r i o u s  l i m i t a t i o n  i n  t h e  production of output  has  beent h e  l ack  of time t o  develop more extensive  procedures.
Examples of newr u l e s  t h a t  a r e  n o t  y e t  implemented t h a t  could be used t o  produce answersa r e  t h e  following:1.
Use more than one memory match s t r u c t u r e  t o  produce t h eanswer.
Currept ly  only  t h e  b e s t  one is  used.
For example,t h e  quest ion "DOES ANYONE HAVE A RED TOY?"
could be answeredby combining t h e  two memory matches, "BRANDT IUVE RED BOOK"and "JENA HAVE RED BALL," t o  g e t  "YES.
JENA HAS A RED BALL ANDBRANDT HAS A RED  BOOK,^^2.
Several  equal ly  good memory matches could be combined t oproduce a s i n g l e  answer t~ l l i ch  conta ins  a compound a c t o r  o rob jec t .
The quest ion "DOES BIUNDT HAVE A BOOK?"
matches twomemory s t r u c t u r e s  equal ly  wel l .
These two matches, "BRANDTIlAVE, RED BOOK" and "BTLANDT HAVE BLUE BOOK," could be combinedt o  form t h e  response "YES.
HE 1US A RED ONE AND A BLUE ONE.
"Note t h a t  work a l s o  needs t o  be done wi th  respec t  t o  pronouns u b s t i t u t i  ~n ( i .
e., ONE f o r  BOOK) befose  t h e  above responsecould be produced.3.
I f  t h e  memory search f a i l s  and t h e r e  was no a c t o r  i n  memory,t h i s  should be  repor ted e x p l i c i t l y  r a t h e r  than saying "I IXN'TKMW."
For example, consider  t h e  quest ion "DOES RILL MAXWELLHAVE A SON?"
I f  BILL@MN(WI?LL is no t  p resen t  i n  MEMORY, r e p o r t"I DON'T KNOtJ BILL MAXJELL .I1 However, i f  t h e  memory matchprocedures were success fu l  i n  f ind ing  gener ic  information suchas "bEN HAVE SONS" then t h e  phrase  "BUT IIE COULD HAVE A SON.
"could a l s o  be generated.-61-Other l imi ta t ions  a r e  t h e  i n a b i l i t y  t o  handle d e f i n i t e  a r t i c l e s  o rt o  a t tach modifying c lauses  o r  phrases t o  help  dis t inguis l l  p a r t s  ofoutput.
The production procedures allow only s u p e r f i c i a l  treatment ofthe  ACT.Typical of simple questions t h a t  current ly  cannot be answered is"DOES BRANDT 11AV13 TWO BOOKS?''.
There is no mechanism f o r  counting o r  com-paring occurrences of re levant  f a c t s  i n  memory during t h e  search process.nor of using them i n  t h e  output production phase.7.3 IGNORED PROBLEEIS7.3.1 INTENTIONS AND MOTIVATIONSThe most ser ious  problem is t h e  lack of any modelling of in ten t ionsand motivations of t h e  person ta lking wi th  JIIIMY3 (or of J I M 3  i t s e l f ) .In order f o r  a dialog t o  be sustained f o r  any length of time with a senseof continuity,  models of t h e  person and of JIIMY3 a r e  requtred.
Twopsychological models should be maintained by t h e  program t o  ind ica te1) what the  program thinks about i t s e l f  and the  person and 2) what t h eprogram thinks the  person thinks about himself and the  program.
A t  a l ltimes during the  conversation, information i n  the  models w i l l  speci fy  f o rboth the  person and the  pragram what each a) van t s  t o  know, b) wants t ot e l l ,  c) already knows, d)  f e e l s ,  e )  believes, etc., with respect  t o  t h econtext of t h e  conversatian up t o  t h a t  point .
Also required w i l l  beinformation about t h e  motives and physical  and mental a t t r i b u t e s  andc w b i l i t i e s  of each par t i c ipan t  from each po in t  of view.
In tegra t ion  ofthese models i n t o  the  workings of the  parsing,  memory searching and output-62-production phases of the program will be a necessary step towards morecomplete natural language understanding,For example, to continue that hypothetical dialogue of section 1,one would like to be able to see sohe exchanges such as:H: Was the window expensive to repair?NCP: Why do you ask?H: Why do you want to know?NCP: I fear I may be responsible for the debt.H: I thought that might be the casq.NCP: You mean I do owe you money?H: No, it's just that I regret making you feel uncomfortable.NCP: Then why did you ask if the window was expensive to repair?H: To test your power of deduction.NCP: You really don't seem to understand me.Computer analysis of such complex interchanges is dependent upon the existenceof psychological models of both parties.
Successful realization is probablymany years off.7.3.2 NORMlU?
"S PROBLEMSWith respect to answering capabilities in language understandingunderstanding models, Norman [I)] has presented several basic problems thatshould be considered.
These are illustrated as: 1) the telephone numberproblem, 2) the three drugstores p'roblem and 3) the Empire State Buildingproblem.The first problem concerns the appropriate response to the question:"What is Charles Dickens' phone number?"
The normal human response of"That's a stupid question."
or "Phones weren't invented then.
requiresthe action of a plausibility check on the question before an attempt ismade to find an answer.-63-The t h r e e  drugotor* problem considers  ww t h e  co r rec t  r eac t ionshould be t o  t h e  statement: "I went t o  t h r e e  drugstores."
This isr e a l l y  a problem of integraking current  wi th  pas t  knowledge and ofdetermining presuppositions t o  t h e  new da ta .
FOK ins tance ,  t he  programshould note  (or  ask) Why, didn' t  t he  o the r  two drugstores have what youwere looking fo r?The~Empire S t a t e  Building probfem r e f e r s  t o  the  context  and scopeof any p a r t i c u l a r  question.
The quest ion,  "Where is the  Empire S t a t eBuilding?"
requi res  d i f f e r e n t  answers depending on the  context  of t heconversation.
To paraphrase Norman's response: I f  asked t h e  quest ion i nRussia, t he  answer most l i k e l y  would be "In the  United States.
"; i f  askedby an a d u l t  i n  Europe, "In New York City.
"; i f  asked by someone i nNew York City,  then "On 34th Street."
would be appropriate .From these  examples, i t  is obvious t h a t  much more is  involved i ndeveloping a language understanding system t h a t  answers quest ions "natural ly"than j u s t  t he  a b i l i t y  t o  pa r se  input  co r rec t ly ,  look up an answer andreport  it.
We hope w e  have more c lea r ly  i d e n t i f i e d  some of t h e  des i r ab lecha rac t e r i s t i c s ,  and we hope we have i l l u s t r a t e d  some progress toward t h eu l t imate  goal.
If  JIMMY3 allows o the r  problems from our list t o  be addresseda lso ,  we s h a l l  f e e l  for tunate ,  indeed.REFERENCES[l] Anderson, John R. and Gordon I[.
U o ~ ~ e r ,  Iturnan Associat ive Nenlory,Wiley and Sons, 1974.
[2] Bobrow, Daniel G. , and Allan Col l ins  (eds . )
, Represen t a t  ion  - andUnderstanding, k a d e m i c  Press ,  1975.
[3] Brotm, John Seely nnd Burton, Richard R., "Multiple Representationsof Knowledge f o r  T u t o r i a l  Reasoning," i n  123,C41 Carbonell,  Jaime K., "A1 i n  C A I :  An A r t i f i c i a l  Intelligence Approach.
-t o ~ ~ o m ~ u r e r - ~ i d e d  1ns t rue t ion ,"  Transact ions l lan-~aehine Sys tarns,Vol.
MIIS-11, 1970, pp.
190-202.
[5] Colby , Kenneth H., A r t i f i c i a l  Paranoia: Computer Simulation ofParanoid Processes, Pergamon Press ,  1975.
[6] Lindsay, Kobeat ,K., "In Defense of Ad lIoc Systems," i n  1151.
[7] Newell, Allen, "11euristic Programing:  I l l -S t ruc tured  Problems,"i n  J.S.
Aronofsky (ed.
), Progress & Operations Research, Wiley, 1969.
[a] Newell, Allen, e t  a l .
,  Speech Understandin& Systems, AmericanElsevier ,  1973.
[9] Norman, Donald A., "Mernory , Knowledge, and the  Answering of Questions,' 'i n  R.L.
Solso (ed.
),  Contemporary -- I ssues  i n  Cognitive ~ s ~ c h o l o ~ y :  -TheLoyola Symposium, Winston, 1973.
[ lo ]  Norman, Donald A .
,  navid E .
Rumelhart and t h e  LNR Research Group,Explorat ions Cognition, Freeman and Co., 1975.
[ll] Qui l l ian ,  #.
Ross, "Semantic ~ e m o r y  ," i n  Marvin Ifinsky (ed. )
,Semantic Information Processin&, MIT Press,  1968.
[12] Rustin, Randall, Natural Language Processing, Algorithmics Press,1973.
[13] Schank, Roger, "Conceptual Dependency: A Theory of Natural LangaugeUnderstanding," Cognitive P S Y C ~ O ~ O R Y ,  vol.
3, 1972, pp.
552-631.
[14] Schank, Roger, " ~ d e n t i f i c a t i o n  of Conceptualizations U n d e r l y i n ~Natural  Language, " i n  [15].
[153 Schank, Roger and Kenneth Colby (eds. )
, Computer Nodels of Thoughtand Language, Freeman, 1973.-[16] Weizenbaum; Jooeph, "ELIZA: A Computer Program f o r  t h e  Study ofNatural  Language Conununicat3on between Men and Zfachine," Conununf cqt ionsof the  ACM, Vol.
9, 1967, pp.
36-54.---[l7] Milks, Yorick, "An A r t i f i c i a l  I n t e l l i g e n c e  Approach to Machine~ r a n s l a t i u n , "  i n  [15].
[18] Wilks, Yorick, "The Stanford Machine Trans la t ion  ~ r o j e c t , "  i n  [12].
[19] Winograd, Terry, "Understanding Natural  Language ,I1 CognitivePsychology, Vol.
3, 1972, pp.
1-191.
[20] Woods, Will ian~, "An Experimental Pars ing System f o r  Trans i t ionNetwork Grammars," i n  [12].American Journal of Computational Lingllirtics Microfiche 61 : 71FIFTEENTH ANNUAL MEETINGA S S O C I A T I O N  F O R  C O f l P U T A T I O N A LL I N G U I S T I C SThe decision to hold the 1977 meeting of the Associationin March was taken only in October 1976.
The six-monthinterval has not allowed advance distribution of abstractsthrough the Journal.
The abstracts accepted for presenta-tion are published here for the record, and for the useof members who could not attend the meeting.Some or all of these papers may be published in futureissues.The program, on frames 72 and 73, gives frame numbers forall abstracts.ASSOCIATI(M FOR C [ ] M F u r A T I ~  LINGUISTICSFifteenth m u a l  UeetingCEtorgetbwn mivers i  ty,  Washington D.C. 16-17 March 1977Palrns m e ,  Walsh Building; 36th Street between N and Prospectwednesdav.
16 MarchSession i IANcXla MDDEKS AND 'IWHNIQUESSt  OC A Bi-Directional Parser for ATN Gramnars 74G.
B r o w  and W. A.
Woods, 801t Beranek and NevaMn9: 30 The Efficient Integration of Syntactic Processing withCasedriented Semailtic Interpretation 75R.
J. Bobrow and M. Bates, Bolt Beranek and Fhman10:00 Same Properties of Arbitrary Fhrase Structure Ianguagesand Translation 76W.
Buttehann , Ohio Sta te  lsliversity10:30 The Augmented Finite State Machine Approach to  Synthesisby Rule 7 7T.
K a c m e k ,  University of Pennsylvania11:OO A lexicon for a Cdmplter Question-Anmering System 78M.
Evens and R. Smith, I l l i n o i s  Ins t i tu te  of- Technology11:30 Pragress &port on REL 79B,  8.
Ttmnpon and F. B, Thompson, California Ins t i tu teof WchrmlogySession I1 Panel Discussion: SPEECH Ihn;rERSI'ANUfNG AND CCMPWI'ATI~I;EIWXTISTICS - A CRITICAL EWINAW(Xi OF 'ME AREW PKUEC2:00 Chairman: J. Allen, M.I.T.Wrticipants: R. m y ,  Carnegie-Mellon UliversityD.
E. Walker, Stanford Research InstituteW.
A. Waods, Bolt Beranek atwl NePaMn5:30 Business Meeting, Paul Chapin, ACL Resident ,  presiding6:  30 Adjourn for dinner8200 Dinner and Presidential Rddress by Paul ChapinThe Foundry, 1050 30th Street, Washiwton$14.00; mke reservations ear ly  Wednesday morning40 person l imi t ,  because of restaurant c a p c i t yFifteenth Annual MeetingGeorgetown [lniversity, Washington D.C. 16-17 March 1977Palms bunget Walsh Building; 36th Street betwen N and ProspectBuraday, 17 MarchSession I11 RMBfMS OF DISCOURSE9:00 Parsing Free NarrativeN.
Sager and C. Insolio, New York Cltliversity9: 30 Disoourse ConnectivesB.
Phillips, Uliversity of I l l i n o i s10:00 A Frmewrk for Processing DialogueG.
E. B r m ,  M.I.T.10:30 Natural Language Prqremning: Kitchen RecipesL.
A, Miller, 3 3 311:00 A-ting the Spatial MetaphorJ.
Ri Hobbs, C i t y  University of New York11: 30 Inferring an AntecedentB.
Nash-Webber, B o l t  Beranek and N e mm i s t r a t i o n  fees: $10.00 Umber$ 5.00 Student$15.00 OtherNo advanced registration procedures; individual dues for ACL mdershipof $15 ?or the year 1977 m y  b paid at the m&ting.
(Institutionaldues are $30 for 1977.
)ACL Meeting 1977G.
Brown and W .
A. WoodsBi -d i rec t iona l  Parsing With  ATN GrammarsThe S y n t a c t i c  component o f  HWIM, t h e  BBN SpeechUnderstanfing ___ _ -- w, is a  middle-out , b i -d i r  e c t i o n a l  pa r se r  f o rAugmented Trarls i t ion NetworK grammars.
Severa l  o f  t h e  HWIMc o n t r o l  s t r a t e g i e s  r e q u i r e  a  pa r se r  t h a t  can e v a l u a t e  an i s o l a t e dsequence of  words ( c a l l e d  an " i s l a n d n )  somewhere i n  t h e  middle o fan u t t e r a n c e  t o  determine whether it is  a  p o s s i b l e  fragment o f  acomplete sentence .
I f  s o ,  t h e  pa r se r  is required  t o  makep r e d i c t i o n s  f b r  a l l  o f  t h e  p o s s i b l e  words t h a t  could be used t oextend the  fragment i n  each d i r e c t i o n .
In t h i s  t a l k  we w i l ld e s c r i b e  a  pa r s ing  a lgor i thm which e f f i c i e n t l y  achieves  t h e s ec a p a b i l i t i e s .The HWIM parse r  can be viewed as a b i - d i r e c t i o n a lg e n e r a l i z a t i o n  of  Ear ley  s algor i thm extended t o  handlec o n t e x t - s e n s i t i v e  , ATN grammars.
The a lgor i thm s t o r e s  thecomputations i n  "segment" and " i s l a n d "  c o n f i g u r a t i o n s  indexed byend s t a t e s  and boundaries.
( A  segment is a  p a r t i a l  pa r se  t h a t  isconta ined comple t r ry  wi th in  one l e v e l  o f  t h e  t r a n s i t i o n  networkgrammar .)
Organizing t h e  computation i n t o  segment and i s l andc o n f i g u r a t i o n s  el iminate 's  t h e  need f o r  a  s t a c k ,  t h u s  so lv ing ad i f f i c u l t  problem in middle-out pars ing .In the  usual  ATN formalism, the  grammar i s  w r i t t e n  a si f  t o  be processed from l e f t  t o  r i g h t ,  and i n  genera l  some of t h ea r c  a c t i o n s  w i l l  be dependent on o the r  a c t i o n s  t o  t h e i r  l e f t  i nt h e  grammar.
To insure  t h e  c o r r e c t  handling of suchcontext-dependent a r c  a c t i o n s  by t h e  b i - d i r e c t i o n a l  p a r s e r ,  t h egrammar wr i t e r  mu$t s p e c i f y  t h e  "scope" o f  any such a c t i o n .Except for  the  e x p l i c i t  d e c l a r a t i o n  o f  scopes f o rcontext-dependent a c t i o n s ,  a  b i - d i r e c t i o n a l  ATN grammar i se x a c t l y  l i k e  an o rd ina ry  ATN, and l e f t - t o - r i g h t  ATN grammars canbe converted t o  b i - d i r e c t i o m l  opera t ion  simply by adding scopes t a t ements .Although developed i n  the  c o n t e x t  of  a speechunder s tanding a p p l i c a t i o n  we f e e l  t h a t  t h e  b i - d i r e c t i o n a l  ,middle-out pars ing  a lgor i thm a l s o  has a p p l i c a t i o n s  i n  t e x tpa r s ing  f o r  problems such a s  e r r o r  c o r r e c t i o n ,  p a r t i a li n t e r p r e t a t i o n  o f  sentence  fragments,  and management ofcombinator i c s  i n  long sentences .ACL Meeting 1977R.
J. Bobrow and M. BatesTHE EFFICIENT INTEGRATION OF SYNTACTIC PROCESSINGWITH CASE-ORIENTED SEMANTIC INTERPRETATIONIt has lon been the goal  of those wr i t ing  na tu ra l  langua eprocessing sysfems t o  express syn tac t i c  c o n s t r a i n t s  i n  broa%.general  way while usinrz t i ~ h t  scmantlc c o n s t r a i n t s  t o  ulde theparsing and t o  i n t e r p r e t  the  r e s u l t i n g  s t r u c t u r e s .
?he systemdescribed here uses an a ~ ~ m e n t e d  t r a n s i t i o n  network grammart o  e the r  w i t h  a case-oriented dic t ionary  t o  achieve a  c lose  ande f f i c i e n t  in teqra t ion  o r  the  syn tac t i c  processin with the  cases t r u c t u r e s  (which include semantic and p roper t i e s  ofob jec t s )  .The ATN def ines ,  using normal s n t a c t i c  c a t e  o r i e s ,  a verygeneral  surface  s t ruc tu re  of aboue the  capab l l f ty  of the  LUNARand GSP systems.
If case s t r u c t u r e s  and semantic inf'ornation (including in te rp re ta t lon  r u l e s )  a r e  omitted from the d i c t i o n a rr e la ted  "deep s t ructures"  f o r  syn tac t i c  paraphrases.I the  grammar functions a s  a  standard pa r se r ,  producin3 c lose  yThe system provides mechanisms fo r  users  t o  def ine  semanticin te rp re ta t ion  r u l e s  and case  frame checks which a r e  t o  beapplied a t  various points  i n  t h e  pa r s ins  process.
Thusconst i tuents  may be In terpre ted a s  soon a s  they a r e  parsed,  andthe s t r u c t u r e  of the semantic i n t e r p r e t a t i o n s  thus produced maybe checked when, f i l l i n g  the  case  frames f o r  higher s t r u c t u r e s .Since the "most l l k e l y  l o c a l  i n t e r p r e t a t i o n  may not f i t  the  caserequirements of contaming s t r u c t u r e s ,  the  system provides ageneral  coercion mechanism t o  r e i n t e r p r e t  a cons t i tuen t  i n  l i g h tof its context  when necessary.
To f a c i l i t a t e  r e i n t e r p r e t a t i o n ,as  well a s  c e r t a i n  anaphoric references ,  the  o r i g l n a l  s y n t a c t i cs t r u c t u r e  is maintained throughout the parsing together with anysemantic interpretations.The present system i s  being used a s  the na tu ra l  languagefront-end f o r  a sophis t ica ted  message processing and f i l i n gsystem.
Ultimately, we hope t o  have a  general  system which canbe adapted t o  o the r  na tu ra l  language input s stems bynew dic t ionary  e n t r i e s  and aamantic in te rp reea t ion  lunct!K:ifdinqACL Meeting 1977W ButtelmanSOME PROPERTIES OF ARBITRARY PHRASE STRUCTURE LANGUAGES AND TRANSLATIONDERIVED USING A FORELAL MODEL OF PIRASE STRUCTURE SYNTAX AND SEM.ilNTICSAbstractThe notion of a phrase structure linguistic description is introduced --a pair, D = ((1,s) where G is an arbitrary phrase structure grammar and S is aformal semantics (defined in the paper).
S may be either context free or con-text sensitive.
S models the following notion of meaning: the meaningful unitsof language are phrases; the meaning of a phrase is a function of its syntacticstructure and of the meanings of its constituents and of its semantic context.This concept is a generalization of semantic notions due to Tarski, later suggestedby Thompson and by Katz and Fodor, and recently popularized for programming lan-guages in Knuth's synthesized attributes and for natural languages by Montague.The (phrase structure) language of D, L(D), is the set of ordered pairs (w,m)where w i s  a sentence of G and m is a meaning assigned to w by S.We prove the following results: The set of phrase structure languages isjust the set of products of r.e.
sets.
Every phrase structure language has adescription using a regular grammar and a context free semantics.
For everydescription D with an qnrestricted grammar and context sensitive semantics thereis a description D' using a context free grammar and context free semantics suchthat L(D) = L(D').
Furthermore, D and Q are "strongly equivalent" in the sensethat the phrase trees assigned by Dl to each sentence are just the skeleton treesof the phrase structures assigned by D to the sentence.
The notions of "weak"and "strong equivalence" are extended to semantics (if two descriptions arestrongly equivalent in a semantic sense, then the structure of their semanticfunctions is identical -- in a programming sense, the same programs can be usedto compute the meanings of the same ~"entencesj.
In this sense, D and D' are notstrongly equivalent.
However, if D has a context: free semantics, then D and D'are semantically strongly equivalent.
Also, we prove that there is a descriptionDm' for L(D) using a context sensitive semantics which is strongly equivalent toD in both the syntactic and semantic senses,Next we define translation on phrase structure languages and consider a par-ticularly appealing strategy for translation, which we call "syntax-controlled"translation.
(I have avoided the term "syntax-directed" because it has haddiffering uses in t11e li~erature.)
We prove the following results: Eveiy com-putable translation is definable as a syntax-controlled translation.
For twoarbitrary descriptions D and D', it is undecidable whether any syntax-controlledtranslation from L(D) to L(D1) exists.
We give an algorithm which, given twoarbitrary descriptions D and D', will halt and produce the definition (program)of a syntax-controlled translation from L(D) to L(D1)  if and only if such atranslation definable by D and D' exists.Syntax-controlled translation requires no semantic computation at translatetime (for which one pays a dear price in the time required to generate syntax-controlled translators).
For a syntax-controlled translation which produces aACL Meeting 1977single target  sentence having a meaning i n  common with the source sentence, thetime complexity is 0 (ptw) where p is parsing time and w is the weight of thesource phrase structure.
To produce the smallest s e t  of ta rge t  sentences suchthat  each target sentence has a t  l ea s t  one meaning i n  common with the source .andsuch that  a l l  t ranslatable meanings of the source are  represented requires0 ((aCn) (cn! )
f (pi-cn) 3tihe, where c and d are  constants, n is the source sentence length, f is  the timeto  check for  semantic va l id i ty  of a parse, and p is the time to prcduce a l l  parses.Finally, we consider phrase s tructure language descriptions having bothinherited and synthetic meaning.
No new languages can be defined, but the useof inherited meaning lcads i n  a natural  way t o  a notion of semantic-controlledtranslation.T.
KaczmarekThe AuRnented F i n i t e  S t a t e  E-lachine -A !
:ore E f f i c i en t  Approach t o  Synthesis  by RuleThe au~mented t r a n s i t i o n  network ( A T N ) ,  which hasproven usefu l  f o r  n a t u r a l  l a n ~ u a p e  u n d e r s t a n d i n ~ ,  has beenreformulated and r e s t r i c t e d  s l i q h t l y  t o  ye i ld  a mechanismtermed the aucrrnented f i n i t e  s t a t e  machine (AFSbl).
The AFSlli s  b e i n ~  used to  do speech syn thes i s  by r u l e ,  a process bvwhich phonetic t r a n s c r i p t i o n s  of speech a r e  converted i n t osynthesizer  parane tersb  The r u l e s  f o r  syn thes i s  i n  t h i sapproach take the form of procedures which a r e  cond i t i ona l lyexecuted dependin8 on con tex tb  )lost previous syn thes i s  byrule systems began w i t h  a t ransformational  component andadded procedural s t a t e n e n t s .
I n  t h i s  system proceduralstatements have been added t o  a much simpler mechanism, thef i n i t e  s t a t e  nachine, The advantame of t h e  AFSIf approach ist h a t  the phonetic s t r i n g  may be processed in  a  s i n g l e  l i n e a rpassACL Meeting 1977M.
Evens and R. SmithA LEXICON FOR A COMPUTER QUESTION-ANSWERING SYSTEMComputer question-answering systems and o the r  models of na tu ra llanguage processing need lexicons t h a t  a r e  much l a r g e r  than thoseava i l ab l e  tioday (c f .
Simmons.
1970 and Becker, 1975).
But the  modelscu r r en t ly  i n  operat ion (e.g.
Winograd, 1971) a l ready  consume a l l  ava i l -ab l e  high-speed memory i n  l a rge  computer systems.
Lexica l  r e l a t i o n sas developed by Raphael (1968), Apresyan e t  a l .
(1971), and Simmons(1973) provide a method of s t o r i n g  l e x i c a l  information i n  more compactform.
While Schank (1973) and Wilks (1975) both claim t h a t  t he re  i'sa f ixed un ive r sa l  s e t  of semantic primes, we argue i n  opposi t ion,follbwing the  Russians and Mi l le r  and Johnson-Laird (1976), t h a t  t heset of l e x i c a l  r e l a t i o n s  is open-ended; our  system i s  designed t oadd new r e l a t i o n s  whenever a l e x i c a l  r e g u l a r i t y  is noticed.Our lexicon i s  being developed a s  an  i n t e g r a l  p a r t  o f  a computerquestion-answering system which answers mult iple-choice quest ionsAbout simple ch i ld ren ' s  s t o r i e s .
It serves  a s  a global  data-base f o rt h i s  system - a combination lexicon-encyclopedia - and must make in-formation r ead i ly  ava i l ab l e  f o r  t he  parsing process, f o r  bui lding ani n t e r n a l  model of t he  s t o r y  being read,  and f o r  making inferences.One of our  test paragraphs, which comes from a t e s t  desigped f o r  f i r s tand second graders ,  says "Ted has a puppy.
His name is Happy.
Ted andHappy l i k e  t o  play."
I n  order  t o  answer the  f i r s t  quest ion,  "The pe tis a: dog-boy-toy?
", we need t o  know what pet means.
The l e x i c a len t ry  f o r  pet contains a simple d e f i n i t i o n ,  t h a t  a pe t  is an animalt h a t  is owned by a human, i n  a f i r s t - o r d e r  pred ica te  ca lcu lus  form:NCOM(PET,Z1) = ( Z2)NCOM(ANIMAL,Z1) .NCOM(HllMAN,Z2) .R(OlJN,Z2,Z1).
I norder  t o  answer t h l s  ques t ion  we a l s o  need t o  know t h a t  a puppy i s  ayoung dog.
This information : NCOMf PUPPY ,Zl) "- NCOM(DOG,Z1).
PROP(AGE,Z1,YOUNG) could be p a r t  of the  l e x i c a l  e n t r y  f o r  puppy.
We would,of course, need axioms of t he  same form a s  wel l  f o r  t he  e n t r i e s  f o rk i t t e n ,  lamb, e t c .
Instead w e  express t h i s  informarion by using al e x i c a l  r e l a t i o n ,  CHILD.
The l e x i c a l  e n t r y  f o r  puppy therefore  con-t a i n s  CHILD dog; the l e x i c a l  en t ry  f o r  k i t t e n  contains CHILD &; andt h e  l e x i c a l  e n t r y  fo r  CHILD contains t h e  aximn scheme from which t h er e l evan t  axioms a r e  formed when needed.For verbs ,  corresponding t o  each case  r e l a t i o n  t h e r e  is a l e x i c a lr e l a t i o n  which points  t o  t yp i ca l  f i l l e r s  of t h a t  case s l o t .
The l e x i -cal e n t r y  f o r  includes TAGENT baker and TLOC kitchen4 It a l s o  includesT where T is b e  well-known taxonomy r e l a t i o n ,  s o  t h a t  i f  thes t o r y  says t h a t  Mother baked a cake w e  can i n f e r  t h a t  she  one andCAUSE bakP.
s o  t ha t  we can deduce t h a t  the cake has baked.
The se lec-eional  r e s t r i c t i o n s  t h a t  h e l p  us t e l l  ins tances  of Wl and !x&apa r t  can a l s o  be expressed compactly us ing  the  T r e l a n o n .
We a f s oneed t o  make deductions from main verbs  i n  predica te  complement: con-s t r u c t i o n s ;  deduction6 such a s  , the speaker 's  view of t h e  t r u t h  of t h eACL Meeting 1977proposition stated i n  the complement as derived from the fac t iv i tyof the verb ( in  these s#  :ies the reader must infer that  everythingthat  mother says i s  true!).
Lexical entr ies  for main verbs tha ttake predicate complements contain pointers to the implication class.These relations can then be expanded t o  give the proper axiom schemes.The use of lexical relations allows us to,express both syntacticand semantic information i n  a form that  i s  compact, easy to retr ieve,and that provides effective .input to both parsing and deductiveprocesses.B.
H. Thompson and F. B. ThompsonA Progress Report on RELThe REL (Rapidly Extensible Language) System.
is now i n  operationalprototype form.
An experimental version of the system wasdemonstrated i n  1973 and s ince has undergone very thorough revisionand clean up.
The REL English grammar, which includes an extensivearithmetical compclnent, has been improved and extended.
Thesystem can be demonstrated and made available for  user tes t ing  onIBM 360/370 computers using most operating systems, e.g.
TSO, CMS.A user's Reference Manual is now i n  preparation and will beavailable a t  the time of the conference.The basic system philosophy has remained the  same,, namely t oprovide the user with a t oo l  for  natural  man-machine communicationtha t  can eas i ly  be sui ted t o  h is  individual needs.
Thus the systemprovides the user with the capabili ty t o  modify m d  extend h i s  datab w e  and language package.
Such modification can be carr ied outby statements about the data base items; for  example:John was not a student a f t e r  June 1, 1976.will remove John from the student c lass  a s  of t ha t  date.
Extensionscan be carried out by adding new primitive individuals,  c lasses ,  andrelat ions,  as well as  through def in i t iona l  capabi l i t i es  whichallow for  defining new concepts i n  terms of exis t ing ones.
A s  apar t  of t h i s  capablli ty , verbs can be introduced by paraphrases,for example :def:ships "carry" coa1:the cargo of ships is  coaland then used i n  a question such as:What s t r a t eg i c  materials were carr ied by USSR ships  i n  19631ACL Meeting 1971N.
Sager and C .
InsolioPar sing Free NarrativeThe r e su l t s  of an experiment i n  parsing narrat ive t ex t sa re  presented.
The t e x t s  were discharge s m a r i e s  obtainedfrom a hospital  ' s computerized f i l e s  of pat ient  records.Each document s ta tes  the  background of the case, the  r e su l t sof the  physical examination and laboratory t e s t s ,  t he  timecourse.
of the i l l nes s  i n  the hospital ,  diagnosis, s t a tu s  ondischarge, eto.
These t ex t s  a r e  par t icu lar ly  in te res t ingbecause they are unedited--cryptic phrases are mixed withf u l l  sentences, punctuation i s  not consistent, and spel l ingerrors  and abbr,eviations abound.
In short, t he  material  isf ree  narrat ive as  one would find it i n  a technical enviromentwhsre repbrts  are dictated and where there would be motivationfor  processing the data  i n  t h e i r  natural  language form.
The paperw l l l  a s c r i b e  how the aboye d i f f i c u l t i e s  were t rea ted  and w i l lpresent s t a t i s t i c a l  r e su l t s  of t he  experiment, such as  t he  numberof sentences correctly parsed vs. the t o t a l  number of sentencesand t he  average parsing times fo r  different  types of sentences.I n  addition, the special problems due t o  commas, conjunctions,quantifiers,  and run on sentences w i l l  be discussed.ACL Meeting 1977B.
P h i l l i p 8  DISCOURSE CONNECTIVESI n  essence current  systems of discourse ana lys i s  w p  sur face  s t ruc tu re si n to  underlying causal chains of propositions.
As the  sur face  form is e l l i p t i c ,i t  is necessary t o  include a knowledge base by means of which omitted l ink ingprbpositions of the discourse may be inferred,  rendeting them e x p l i c i t  i n  t heunderlying representation.Cause is not t he  only l i n k  between propositions, however; a l s o  used a r es y l l o g i s t i c  and analogic mechanisms, statements of r e l a t i v e  b e l i e f ,  and processesof decomposition and abs t rac t ion ,  the  l a s t  being t he  expl ica t ion  of abs t r ac tconcepts.M d i t i v e  discourse connectives - 'because', ' so ' ,  e t c .
,  a r e  reaLizat ions ofthe  l i nks  between propositions i n  these modes of discourse construct ion.
Therea r e  a l s o  adversative connectives, such a s  'however ' , ' bu t ' ,  e t c .
, tha t  cannotbe so  explained.
They must be in te rpre ted  a s  s i gna l s  t o  tu rn  off  inferencemechanisms.To understand the need f o r  adversat ive connectives, we first need t orecognize two kinds of p ropos i t io t~s ,  episodic  and systemic.
The former encodespec i f i c  a c t s  and s t a t e s ,  e.g., 'Thomason won the  e lec t ion  f o r  t he  governorshipof I l l i n o i s ' ,  whereas the l a t t e r  a r e  generalized ca tegor ica l  statements,  e.g.,'b i rds  have wings'.
The content of discourse is usual ly episodic.
Theknowledge base contains both kinds of propositions, there  a r e  episodic andsystemic memories.There is a predict ive component i n  t he  process of understanding discourse.A s t a u d  s i t ua t i on  s e t s  up expectancies which may e i t h e r  become the  unstatedl ink ing  propositions, or  may be e x p l i c i t l y  s t a t ed ,  and hence confirmed, a ta l a t e r  point.
The predict ions a r e  s e t  up by systemic memory'.
An episodicproposition has a counterpart i n  systemic memory, e -g .
,(1) John a t e  cheese.
(Episodic)(2) Person eat  food.
(Systemic)The predict ions a r e  associated with systemic memory, e.g.,(3) Person e a t  food CAUSE person not hungry.Thus given (11, a l a t e r  expectancy of (4) would be s e t  up by (3)(4) John was not hungry.But systemic knowledge contains general izat ions,  not  inv io lab le  t r u th s ,  and t heinference may not be va l id .
This can be marked by the  use  of an adversat iveconnective:( 5 )  John a t e  t he  cheese, but he was still  hungry.ACL Meeting 1977G .
P. BrownA FRAMEWORK FOR PROCESSING DIALOGUEThts report describes a framework for handing dxed-initiative Englishdialogue in a conale session environment, with emphasis on recognition.
W ithin thisframework, both Iinguistk and non-linguistic actlvlties are madelled by structures calledmalods, which are a declarative form of procedural knowledge.
Our  design focusses onunlts of Ilnguistlc.actlvity larger than the speech act, so that the pragmatic and semanticcontext of an utterance can be used to guide Its interpretation.
Also important is thetreatment of Indirect illocutions, e.g., the different ways to ask a question, give a command,etcOur basic approach has been to combine careful structural distinctionswith a mixed recognition strategy.
The central distinction is in the way that utterances canbe related to the methods in the dialogue model.
First, an utterance (called an initiator) mayIntroduce a method that corresponds to one of the standard activities in an environment (forexample, asking a question at an information desk or requesting help from a consultant).Second, an  utterance may correspond to a step in a standard path In a method alreadyunderway; here, a standard path is a normally expected succession of activity steps.
Third,m utterance may be part of recovery discussion, which Is generated when when someviolation of standard expectations occurs, necessitating clarification, correction, etc.
Flnally,an utterance may belong to mdadiscussion, a relatively constrained class whose function is tolay out the context for other utterances so that these may be identified w~th the appropriatemethod step.Given the static model of dialogue embodied in the methods, the problem isto find the correct method step that relates to a particular input.
We handle this problem bydeflning a set of special structures to aid in matchag, by using the methods to generateexpectations dynamtcally, and by differentiating overall matching strategies according to thefour utterance classes described.The  ideas presented here have been implemented In a prototype system calledSusie Software, whtch is embedded in OWL-I The OWL system is currently underdevelopment in the Knowledge-Bases Systems Group at the M.I.T.
Laboratory for ComputerScience.
This research was supported by the Advanced Research P r o F u  Agency of theDepartment of Defense and was monitored by the Office of Naval Research under ContractNumber N00014-75CMj61.ACL Meeting 1977 83Abstract submitted for presentation at The Association for Computational Linguistic8March 16-17, 1977, Georgetown University, Washington, D, CL.
A. MillerNatural Language Programming :Kitchen RecipesLaboratory studies of computer programming by naive programmersindicated that, for formal programming languages, most behavioral e r r o r sare associated with specification of the transfer-of-control characteristicsSubsequent studies revealed that it is this feature which most discriminatesbetween formal computer programming and "natural language" programming:the former embeds the data-manipulation actions within a complex controlstructure whereas the latter emphasizes f irst  the action, followed by suhsequentqualifications.
This ACTION-QUALIFICA T I 0  style i s  su strikipgly differentfrom the CONTROL-ACTION style of programming computers that a study ofnatural language programming by professionals was initiated.
The objectiveof the investigation is  to determine the mechanisms whereby process inforniatianis communicated and to assess the oft-asserted (but empirically untesied)I 1  imprecision" and "ambiguity" of natural language usage in procedural donlainsPotentially, such an investigation could result in an alternative to formalprogramming languages for the linguistic man-machine interface --  e,  g., NaturalLanguage Procedure Specification.We report on our progress tp date in the analysis of a corpus of recipes from'I'he JOY of Cooking.
Our present understanding of the communication process inrecipes is  that the imperative verb is  a call to some procedure which returns acase-frame into which are mapped the remaining object- group and verb-qualifierelements of the surface text.
We present statistics concerning case frequencies,syntactic structures, and word usage, and we detail our approach for the automaticcomprehension and symbolic modelling of the activities invblved in recipeexecution (we are using Heidornts NLP LISP system).ACL MeePlng 1977 84J .
p, Hobbe A~XO~MODATXNG TliE SIJATI ,\L bU:T1\1'1 l(1H~ i n ~ u i s t s  and psycho log i s t s  have f requcnt lp  noted t h a t  i n~ , , g l i s l ~  and o t h e r  languages one o f t e n  appeals t o  s p a t i a l  metaphorswhen speaking of a b s t r a c t  ideas  (9,1,3).
For example, we speak of**lli & p  h 110 es1I, pr ices t l ,  "deep thought", "being p o l i t i c s u ,"a book pn sociology", l l s e t t l n g  t h e  idea", e t c .
Heretofore,  t h i shas been only an obscrvation.
Evcn Schank's work, w i t h  i t sdccompositions i n t o  YTTUWS, ATRANS, and MTKANS, is onlv suegcs t iveof an underlying un i ty ,  and Jackendoff I s  c l a s s i f i c a t i o n  of wordsenses i n t o  p o s i t i o n a l ,  possess iona l ,  idenf i f i c a t i a n a l ,  andc i rcumstan t i a l  modes remains only a c l a s s i  f i ca t inn .
This pal-erdesc r ibes  an approach which a t i l i ~ e s  tire s p t i a l  metaphor i ncons t ruc t ing  econornical d e f i n i t i o n s  of H a l l - p r ~ r p o s e l  words t h d t  havepreviously de f i ed  p r e c i s e  specif  j ca t ion ,  and a method f o ri n t e r p r e t i n g  these  words i n  context  which t r e a t s  metdphor not  asan anomoly but a s  t h e  na tu ra l  s t a t e  of a f f a i r s .The basic idea is  t o  clef rne words i n  t c r n s  of very genera l  s p a t i a lp red ica tes  and then,  i n  t h e  a n a l y s i s  of a give2 t e x t ,  t o  ~ c e k  a mores p e c i f i c ,  context-dependent i n  t e r p r e t a t i n n ,  nr kinding,  j u s t  asa  compiler o r  i n t c a p r e t e r  seeks bindings f o r  t h e  v a r i a b l e s  andprocedrire names mentioned i n  a  program.I n t e r p r e t a t i o n  a s  Binding: I n  yrograrn~r~infr l a n g u a ~ c s ,  there isnormally a f ixed  means of ctetermihing bindinqs.
e i t h e r  by fa l lowinga chain of access modulcn (2: or by consu l t ing  an a - l i s t  o rPUNARG-frozen environment.Van E~nden & Kowalski (8) have presented unothcr outlook.
I n  amechanical theorem proving sys te~u ,  they show h o w  Horn c l d r ~ s e snay be viewed as procedure d e c l a r a t i o n s  i n  which t h e  p o s i t i v el i t e r a l  i s  a procedure name, the  negat ive  l i t e r a l s  the  procedureACL Meeting 1977 85body, and each n e g a t i v e  l i  t e r a l  a  c a l l  t o  another  procedure.
t i  setef Horn c l a u s e s  is a n o n - d e t e r s i n i s t i c  program, non-de te rmin i s t i cbecause s e v e r a l  tforn c l a u s e s  may have t h e  same p o s i t i v e  l i t e r a l .That is, the  procedure name in  a  procedure c a l l  may be bound t o  oneof s e v e r a l  d i f f e r m t  p r o c e d u r ~  bodies.
Resolut ion is  an a t t empt  t obind a procedure name i n  f i  way t h a t  l e a d s  t o  t h e  d e s i r e d  r e f u t a t i o n .Put i n  another  way, we may view the  i n f e r c n c e  ";\>Bt1 a s  s p e c i f y i n gA a s  a p o s s i b l e  b inding f o r  H.Montague (6,4) developed a  v a r i e t y  of i n t e n s i o n a l  l o g i cas a  represen ta t ion  f o r  n a t u r a l  language.
I n  h i s  formalism,i n d i v i d u a l  words can be d e f i n e d  a s  f u n c r i ~ n s  expressed i n  terms ofi n t e n s i o n s ,  i.e.
v a r i a b l e s  and procedure names.
S y n t s c t i c  r e l d t i o n si n  English a r e  t r a n q l a t e d  i n t o  f u n c t i o n  a p p l i c a t i o n s  i ni n t e n s i o n a l  logic.
These func t ion  a p p l i c a t i o n s  bind t h e  i n t e n s i o n st o  s p e c i f i c  i n t e r p r e t a t i o n s .
In this way the  meanings ,of indivPdua1words a r e  composed i n t o  the  meaning of t h e  sentence.
However, t h ebinding mechanism is q u i t e  f i x t d ,  making the  f o r n ~ a l i s m  insuf  f  i c i e p  t l yf l e x i b l e  f o r  t h e  wliole range of n a t u r a l  language.Our approach combines Montagpets wi th  t h a t  of Van Emden &Kowalski.
A s  i n  Mon taguc 's  approach, i n d i v i d u a l  words a r e  d e f i n e di n  terms of genecal pre r l i ca tes  t h a t  may be  viewed as unboundp r e d i c a t e  namea, and.
t h e i r  b ind ings  i n  a gjvcn t ex t  are determinedFrom s y n t a c t i c a l l y  r e l a t e &  words.
Ilowever, t h e  b ind ing  mechanismis  not f i x e d ,  but as  with Van Bmdcn & Kowalski, i t  is a s e a r c h  f o r  achain  of in fe rence  which culnl inates  in  an express ion  invo lv ing  t h egehera l  p red ica te .
An example is given belod.
I n  a d d i t i o n ,  adynamic o r d e r i n g  determined by con tex t  is imposed on the  axioms i nt h e  d a t a  base of l e x i c a l  and world k ~ i i i w l c d g e ,  d e f i n i n g  an o r d e r i n gon c h a i n s  of inference.
The, bind ing  is chosen which is given by t h edWL Meeting 1977 86chain of inference Irighest in  t h i s  ordering.The Spa t i a l  Metaphor: A t  tile base of the ~ c x i c o n ,  o r  s e t  ofaxioms, are a m d l l  number of pr imit ive not ions w i t h  a highlys p a t i a l  o r  v i sua l  flavor.
Among these a re  * * ~ c a l e ~ ~  o r  a  p a r t i a lordering defined by possible changes of s t a t e ,  the r e l a t i on  "onwwhich places poin ts  on the sca l e ,  and* %itt1 which among o ther  th ingsr e l a t e s  an e n t i t y  t o  a  point  on a  scale .
Moreover, "at1' is re la tedt o  predic-ation: f o r  an e n t i t y  t o  be at a predicat ion is f o r  thee n t i t y  t o  be one of its arguments, a s  i l l u s t r a t e d  by the equivalenceJohn is  hard a t  work 3 John is working hard.Concepts a t  higher l eve l s  of the Lexicon are  defined i l l  terms ofthese basic s p a t i a l  conccpts.
For example, "to think of"' o r  "tohave i n  mindt8 i s  defined as a va r i e ty  of "att1 Time i s  a sca le ,  andan event may be 5 a point  on t h a t  scale .
A s e t  may a l so  be though tof as a scale and i t s  elements a s  being points on the scale .
Kotetha t  t h i s  takes ser ious ly  tlre visual image one h a s  of a  s e t  as theelements spread out  before one.Final ly ,  " a l l - p ~ r p o s e ' ~  words such a s  t h e  common adverbs andpreposi t ions arc defined in  terms of the bas ic  concepts l i k e  "scalett ,"ann, and glatw.
In t h e  ana lys i s  of a t ex t ,  we f i nd  in t e rp re t a t ionsf o r  these basic concepts by f inding chain8 of inference fromproper t ies  of the arguments of the "411-purpose" war-ds to g,ropos i t i o n sinvolving the basic  concepts.Simplified Exanlple: Consider llJolin i s  i n  pol i t ics" .
S~r>poseatin" means to  be at a poikt on a ?talc.
We nrust f ind  bindings fo r  t h eunderlined words.
P o l i t i c s  i s  a s e t  of a c t i v i t i e s  d i rec ted  towardthe goal  of obtaining dnd using power i n  an ort;anization.
r\ seta  isq q  a scale .
The typ ica l  a c t i v i t y  is on the Scale.
For John to  bea t  such an a c t i v i t y  is f o r  him t o  Ire one of the p a r t i t i p a n t s  i n  it.
-MX Meeting 1977 87I'I~US, f o r  John t o  be i n  p o l i t i c s  i s  f o r  h i m  t o  engage i n  thea c t i v i t i e s  t h a t  c l ~ a r a c t e r i z e  p o l i t i c s .Otliez examples i l l u s t r a t i r ~ g  the d iskinct ic-n  be.tween "in" andI1onft and tlie meaning of that elusive adverb "even" will be presented.Signif icance:  T h i s  work represen t s .
an arlvance i n  ourunderstanding of how meanings of worcls am, composed i n t o  the  meaningsof l a r g e r  s t r e t c h e s  of t e x t ,  and of the e f f e c t  of con tex t  oni n t e r p r e t a t i o n .
Moreover, i t  is the  r e s u l t  of a happy blend ofcomputational o r  l o g i c a l  tcchnioue with l i n g u i s t i c  and psychologicalins igh t s .Bibliography1.
Aschi S.X.
'The Metaphor: A Psychological  Inqu i ryn  i n  dl Rcnley,cd.
Docun~ents of Ciestal t Psychology, 1961.2.
Bobrow, D. & B.
We@reit.
"A Model and Stack Implementation ofM i l l  t iple  .Environmentsf1 CACM, Octohe r 1973, p. 591.3.
Clark, Herbert  11. l1Space, Time, Semantics, and t h e  Child'' i nCogni t ive  Develnpment and the Acqr~ i s i  t i o n  of Langilage.
2973.4.
Hobbs, J.
B S. Rosenschein.
Making Computational Sense ofMontague 's I n  tenssonal  Logic.
Report No.
NSO-11, CourantI n s t i t u t e  of Mathematical Sciences.
December 1976.5.
Jackendoff, Ray.
"Toward an Expl  ana to ry  Semantic Representa t ionvL i n g u i s t i c  Inquiry ,  Winter 1976. p. 89.6.
Mon tague,  Richard.
"The Proper Treatment of Quan t i f i ca t ion  i nOrdinary-English11 i n  Approaches to Natural  Canguage, 1973.7.
Schank, R., N. Goldhun, d. Rieger, & C. Riesbeck.
IfInferenceand Paraphrase by Computer1' JACM, July 1975.
P. 309.8.
Van Emden, M.H.
& R.A. Kowalski, 'The Semantics of P r e d i c a t eLogic a s  a Programming Language" JAW, October 1976. p. 733.9.
Whorf, Ben jamin.
'The Rela t ion of Habi tual  Thought and Heliav i o rt o  Languagef1 i n  Language, Thought, k Reqlity, 2956.ACL Meetipg 1977I n f e r t i n q  a n  A n t e c e d a n tC o m v u t a t i o n a l  r e s e a r c h  o n  p r o n o m i n a l  a n a p h c r a  h a s  c e n t e r e da r o u n d  t h e  p r o b l e m  o f  ' ' r e f e r e n c e  c e s o l u t i o n " ,  i.e.
the p r o b l e m  ofc h o o s i n q  t h e  c o r r e c t  a n t e c e d a n t  f o r  a n  a n a p h o r  jc e x p r e s s i o n  f roma se t  of  s e v e r a l  p o s s i b l e  c a n d i d a  tes.
B u t  r e f e r e n c e  r e s o l u t i o n ,t h o u q h  a complex process r a q n i r  i h q  t h e  i n  t a r a c t  i o n  of manys o u r c e s  of k n u w l e d q e ,  is really o n l y  h a l f  t h e  p r o b l e m .
The o t h e rh a l f  i n v o l v e s  a c t u a l 1  y  f i n d i n g  t h e  c a n d i d a t e s .
Tn c u r r e n t  n a t u r a llanquaqe s y s t e m s ,  t h i s  h a l f  o f  t h e  ~ r o b l e m  h a s  teen handled i n  ar a t h e r  24 f a s h i o n  o r  h a s  been i q n 0 r e .
l  e n t i r e l y .
I n  t h e s es y s t e m s ,  the set off p o s s i b l e  a n t e c e d a n t s  f o r  a Fronoun is u s u a l l yculled o f f  a h i s t o r y  l ist o f  o b j e c t s  i n t r o d u c e d  e a r l i e r  i n  t h ed i s c o u r s e .
V a r i o u s  h e u r i s t i c s  inc1 u d i n g  r e c e n c y ,  s t r u c t u r a lc o n s t r a i n t s ,  s e m a n t i c  se lec t  i o n a l  r e s t r i c t i o n s ,  # n o v nh i g h e r -  l e v e l  t a s k  o r  d i s c o u r s e  o r q a n i z a t i o n ,  a n d  c a s e  a n d  n u m b e ra q r e e m e n t  a r e  t h e n  a p p l i e d ,  i n  o r d e r  t o  c h o o s e  t h e  b e s t - f i t t  ingc a n d i d a t e .On t h e  one h s n d ,  it h a s  l o n g  been r e c o a n i z e d  t h a t  i n f e r e n c emay be n e e d e d  t o  f i n d  p o s s i b l e  a n t e c e d a n t s  f o r  a d e f i n i t e  n o u nphrase, F o r  e x a m p l e ,  i nA l e e r i n q  face a p p e a r e d  at.
Mary's window.Slie c a l l e r 1  t h e  p o l i c e  t o  =rest t h ~  Ean.at  l e a s t  o n e  i n f e r e n c e  r u l e  r e l a t i n q  man a n d  face i s  needed  t of i q u r e  o u t  a possible a n t e c e d a n t  f o r  "the man".The p o i n t  T s h a l l  be a a k i n q  i n  t h i s  F a p e r  is t h a t  i n f e r e n c emay be r e q u i r e d  , t o  c o n j u r e  u p  p o s s i b l e  a n t e c e  E a n t s  f o r  p r o n o u n sas J e l l .
S x a m c l e s  l i k e  t h e  f o l l o w i n q  w i l l  b e  u s e d  t o  i l l u s t r a t etbis p o i n t .I s a w  a m a r r i e d  c o u p l e  w a l k i n g  i n  t h e  p a r k .Be had on a w f u l  p l a i d  s h o r t s ,  a n d  s h e  h a d  on a d a s h i k i .r he = t h e  h u s b a n d ,  s h e  = t h e  w i f e 1J o h n  b l r n d e r l  some f l o u r  a n d  wa'ter a n d  used 4. t o  sealt h e  I i a  onf o t 1 1 9  p o t .r i t  = the f l o u r - u a t e r  m i x t u r e ' ]Mary q a v e  each q i r l  a T - s h i r t .T h e y  t -hanked h e r  f o r  them.r t h e m  = t h e  set of T - s h i r t s ,  each of r h i c h  Naryqave t o  some g i r l  1T s h a l l  show t h a t  a n y  p r o n o u n  r ~ s o l u t i o n  p r c c e d u r e ,  e v e n  o n et h a t  uses h i q h l v  s o p h i s t i c a t e d  s y n t a c t i c ,  soman t i c  a n d  p r a q m a t i cchccks, c a n n a t  cpstrict.
i t se l f  t o  c o n s i i c r i n q  o n l y  o h  j ec ts  a n devents  q i v e n  ~ x p l i c i t l y  i n  the t 9 x t .
I n  a d d i t i o r  I s h a l l  s h o v  howt h e  needed a n t s c e d a n t g  c a n  be  i n f e r r e d ,  c s i n g  a f o r m a lE e p r e s a n t s t i  or.
l a n s u a s e  f o r  E n q l i s h  a e s c r i b c - d  e lsewharc.
