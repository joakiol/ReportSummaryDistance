Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 201?204,Sydney, July 2006. c?2006 Association for Computational LinguisticsChinese Word Segmentation based on an Approach of Maximum EntropyModelingYan Song1 Jiaqing Guo1 Dongfeng Cai2Natural Language Processing LabShenyang Institute of Aeronautical EngineeringShenyang, 110034, China1.
{mattsure,guojiaqing}@gmail.com2.cdf@ge-soft.comAbstractIn this paper, we described our Chineseword segmentation system for the 3rdSIGHAN Chinese Language ProcessingBakeoff Word Segmentation Task.
Oursystem deal with the Chinese character se-quence by using the Maximum Entropymodel, which is fully automatically gen-erated from the training data by analyz-ing the character sequences from the train-ing corpus.
We analyze its performanceon both closed and open tracks on Mi-crosoft Research (MSRA) and Universityof Pennsylvania and University of Col-orado (UPUC) corpus.
It is shown that wecan get the results just acceptable withoutusing dictionary.
The conclusion is alsopresented.1 IntroductionIn the 3rd SIGHAN Chinese Language Process-ing Bakeoff Word Segmentation Task, we partici-pated in both closed and open tracks on MicrosoftResearch corpus (MSRA for short) and Universityof Pennsylvania and University of Colorado cor-pus (UPUC for short).
The following sections de-scribed how our system works and presented theresults and analysis.
Finally, the conclusion is pre-sented with discussions of the system.2 System OverviewUsing Maximum Entropy approach for ChineseWord Segmentation is not a fresh idea, some pre-vious works (Xue and Shen, 2003; Low, Ng andGuo, 2005) have got good performance in thisfield.
But what we consider in the process ofSegmentation is another way.
We treat the inputtext which need to be segmented as a sequence ofthe Chinese characters, The segment process is, infact, to find where we should split the character se-quence.
The point is to get the segment probabilitybetween 2 Chinese characters, which is differentfrom dealing with the character itself.In this section, training and segmentation pro-cess of the system is described to show how oursystem works.2.1 Pre-Process of TrainingFor the first step we find the Minimal SegmentUnit (MSU for short) of a text fragment in thetraining corpus.
A MSU is a character or a stringwhich is the minimal unit in a text fragment thatcannot be segmented any more.
According to thecorpus, all of the MSUs can be divided into 5type classes: ?C?
- Chinese Character (such as/\0 and /?0), ?AB?
- alphabetic string(such as ?SIGHAN?
), ?EN?
- digit string (suchas ?1234567?
), ?CN?
- Chinese number string(such as /?z?0) and ?P?
- punctua-tion (/?0,/"0,/?0, etc).
Besides theclasses above, we define a tag ?NL?
as a specialMSU, which refers to the beginning or ending of atext fragment.
So, any MSU u can be describedas: u?C?AB?EN?CN?P?{NL}.
In orderto check the capability of the pure Maximum En-tropy model, in closed tracks, we didn?t have anytype of classes, the MSU here is every characterof the text fragment, u?C ??{NL}.
For instance,/??
?\SIGHAN2006?c?m"0is segmented into these MSUs: /?/?/?/\//S/I/G/H/A/N/2/0/0/6/?/c/?/m/"0.Once we get al the MSUs of a text fragment,we can get the value of the Nexus Coefficient (NCfor short) of any 2 adjacent MSUs according tothe training corpus.
The set of NC value can be201described as: NC ?
{0, 1}, where 0 means those2 MSUs are segmented and 1 means they are notsegmented (Roughly, we appoint r = 0 if eitherone of the 2 adjacent MSUs is NL).
For example,the NC value of these 2 MSUs/\0 and/?0in the text fragment /\?0 is 0 since these 2characters is segmented according to the trainingcorpus.2.2 TrainingSince the segmentation is to obtain NC value ofany 2 adjacent MSUs (here we call the interspaceof the 2 adjacent MSUs a check point, illustratedbelow),.
.
.U?3 U?2 U?1 U+1 U+2 U+3 .
.
.6Check Point of U?1 and U+1we built a tool to extract the feature as follows:(?)
U?3, U?2, U?1, U+1, U+2, U+3(?)
U?1U+1(?)
r?2r?1(?)
U?3r?2, U?2r?1() r?2U?2, r?1U?1In these features above, U+n (U?n) refers tothe following (previous) n MSU of the checkpoint with the information of relative position(Intuitively, We consider the same MSU hasdifferent effect on the NC value of the check pointwhen its relative position is different to checkpoint).
And U?1U+1 is the 2 adjacent MSUs ofthe check point.
r?2r?1 is the NC value of theprevious 2 check points.
Similarly, the (?)
and ()features represent the MSUs with their adjacentr.
For instance, in the sentence????
?I<,we can extract these features for the check pointbetween the MSU?
and?:(?)
NL?3,NL?2,??1,?+1,?+2,?+3,(?)??1?+1(?)
00 (because ?
is the boundary of the sen-tence)(?)
NL?30,NL?20() 0NL?2,0?
?1and also these features for the check point be-tween the MSU?
and?:(?)??3,??2,??1,?+1,I+2,<+3(?)?
?1?+1Figure 1: MSRA training curveFigure 2: UPUC training curve(?)
01 (for UPUC corpus, here the value is 00since ??
is segmented into 2 characters, but inMSRA corpus,??
is treated as a word)(?)??30,?
?21() 0??2,1?
?1After the extraction of the features, we use theZhangLe?s Maximum Entropy Toolkit1 to train themodel with a feature cutoff of 1.
In order to getthe best number of iteration, 9/10 of the trainingdata is used to train the model, and the other 1/10portion of the training data is used to evaluate themodel.
Figure 1 and 2 show the results of the eval-uation on MSRA and UPUC corpus.From the figures we can see the best iterationnumber range from 555 to 575 for MSRA corpus,and 360 to 375 for UPUC corpus.
So we decidethe iteration for 560 rounds for MSRA tracks and365 rounds for UPUC tracks, respectively.2.3 SegmentationAs we mentioned in the beginning of this section,the segmentation is the process to obtain the value1Download from http://maxent.sourceforge.net202of every NC in a text fragment.
This process issimilar to the training process.
Firstly, We scanthe text fragment from start to end to get al ofthe MSUs.
Then we can extract all of the featuresfrom the text fragment and decide which checkpoint we should tag as r = 0 by this equation:p(r|c) =1ZK?j=1?fj(r|c)j (1)where K is the number of features, Z is the nor-malization constant used to ensure that a probabil-ity distribution results, and c represents the con-text of the check point.
?j is the weight for fea-ture fj , here {?1?2 .
.
.
?K} is generated by thetraining data.
We then compute P (r = 0|c) andP (r = 1|c) by the equation (1).After one check point is treated with value ofr, the system shifts backward to the next checkpoint until all of the check point in the whole textfragment are treated.
And by calculating:P =n?1?i=1p(ri|ci) =n?1?i=11ZK?j=1?fk(ri|ci)j (2)to get an r sequence which can maximize P .
Fromthis process we can see that the sequence is, in fact,a second-order Markov Model.
Thus it is easily tothink about more tags prior to the check point (asan nth-order Markov Model) to get more accuracy,but in this paper we only use the previous 2 tagsfrom the check point.2.4 Identification of New wordsWe perform the new word(s) identification as apost-process by check the word formation power(WFP) of characters.
The WFP of a character isdefined as: WFP (c) = Nwc/Nc, where Nwc isthe number of times that the character c appearsin a word of at least 2 characters in the trainingcorpus, Nc is the number of times the character coccurs in the training corpus.
After a text fragmentis segmented by our system, we extract all consec-utive single characters.
If at least 2 consecutivecharacters have the WFP larger than our thresholdof 0.88, we polymerize them together as a word.For example,/??
?0 is a new word which issegmented as /?/?/?0 by our system, WFPof these 3 characters is 0.9517,0.9818 and 1.0 re-spectively, then they are polymerized as one word.Besides the WFP, during the experiments, wefind that the Maximum Entropy model can poly-merize someMSUs as a newword (We call it poly-merization characteristic of the model), such as???
in the training corpus, we can extract ?
?as the previous context feature of the checkpoint after, in another stringS,?, we canextract the backward context?
of the check pointafterwith r = 1.
Then in the test, a new word???
is recognized by the model since ?
?and ?
are polymerized if?
appears to-gether a large number of times in the training cor-pus.3 Performance analysisHere Table 1 illustrates the results of all 4 trackswe participate.
The first column is the track name,and the 2nd column presents the Recall (R), the3rd column the Precision (P), the 4th column isF-measure (F).
The Roov refers to the recall of theout-of-vocabulary words and the Riv refers to therecall of the words in training corpus.Track R P F Roov RivMSRA Closed 0.923 0.929 0.926 0.554 0.936MSRA Open 0.938 0.946 0.942 0.706 0.946UPUC Closed 0.902 0.887 0.895 0.568 0.934UPUC Open 0.926 0.906 0.917 0.660 0.954Table 1: Results of our system in 4 tracks.3.1 Closed tracksFor all of the closed tracks, we perform the seg-mentation as we mentioned in the section above,without any class defined.
Every MSU we extractfrom the training data is a character, which may bea Chinese character, an English letter or a singledigit.
We extract the features based on this kind ofMSUs to generate the models.
The results showthese models are not precise.For the UPUC closed track, the official releasedtraining data is rather small.
Then the capabilityof the model is limited, this is the most reasonablenegative effect on our F-measure 0.895.3.2 Open tracksThe primary change between open tracks andclosed tracks is that we have classified 5 classes(?C?,?AB?,?EN?,?CN?
and ?P?)
to MSUs in or-der to improve the accuracy of the model.
Theclassification really works and affects the perfor-mance of the system in a great deal.
As this textfragment 1998c can be recognized as (EN)(C),which can also presents 1644c, thus 1644c can203be easily recognized though there is no 1664c inthe training data.The training corpus we used in UPUC opentrack is the same as in UPUC closed track.
Withthose 5 classes, it is easily seen that the F-measureincreased by 2.2% in the open tracks.For the MSRA open track, we adjust the class?P?
by removing the punctuation ?!?
from theclass, because in the MSRA corpus, ?!?
can bea part of a organization name, such as ?!?
in/?l?!???u???0.
Besides,we add the Microsoft Research training data ofSIGHAN bakeoff 2005 as extended training cor-pus.
The larger training data cooperate with theclassification method, the F-measure of the opentrack increased to 0.942 as comparison with 0.926of closed track.3.3 Discussion of the tracksThrough the tracks, we tested the performance byusing the pure Maximum Entropy model in closedtracks and run with the improved model with clas-sified MSUs in open tracks.
It is shown that thepure model without any additional methods canhardly make us satisfied, for the open tracks, themodel with classes are just acceptable in segmen-tation.In both closed and open tracks, we use thesame new word identification process, and withthe polymerization characteristic of the model, wefind the Roov is better than we expected.On the other hand, in our system, there is no dic-tionary used as we described in the sections above,the Riv of each track shows that affects the systemperformance.Another factor affects our system in the UPUCtracks is the wrongly written characters.
Considerthat our system is based on the sequence of char-acters, this kind of mistake is fatal.
For example,in the sentence ???#u??Off<ff{?,where{?
is written as{?.
The model cannotrecognize it since {?
didn?t occur in the train-ing corpus.
In the step of new word identification,the WFPs of the 2 characters {??
are 0.8917and 0.8310, thus they are wrongly segmented into2 single characters while they are treated as a wordin the gold standard corpus.
Therefore, we believethe results can increase if there are no such mis-takes in the test data.4 ConclusionWe propose an approach to Chinese word seg-mentation by using Maximum Entropy model,which focuses on the nexus relationship of any2 adjacent MSUs in a text fragment.
We testedour system with pure Maximum Entropy modelsand models with simplex classification method.Compare with the pure models, the models withclassified MSUs show us better performances.However, the Maximum Entropy models of oursystem still need improvement if we want toachieve higher performance.
In future works,we will consider using more training data andadd some hybrid methods with pre- and post-processes to improve the system.AcknowledgementsWe would like to thank all the colleagues of ourLab.
Without their encouragement and help, thiswork cannot be accomplished in time.This is our first time to participate such an in-ternational bakeoff.
There are a lot of things wehaven?t experienced ever before, but with the en-thusiastic help from the organizers, we can comethrough the task.
Especially, We wish to thankGina-Anne Levow for her patience and immediatereply for any of our questions, and we also thankOlivia Kwong for the advice of paper submission.ReferencesNianwen Xue and Libin Shen.
2003.
Chinese WordSegmentation as LMR tagging.
In Proceedings ofthe Second SIGHAN Workshop on Chinese Lan-guage Processing, p176-179.Maosong Sun, Ming Xiao, B K Tsou.
2004.
ChineseWord Segmentation without Using Dictionary Basedon Unsupervised Learning Strategy.
Chinese Jour-nal of Computers, Vol.27, #6, p736-742.Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo.2005.
A Maximum Entropy Approach to ChineseWord Segmentation.
In Proceedings of the FourthSIGHAN Workshop on Chinese Language Process-ing, p161-164.204
