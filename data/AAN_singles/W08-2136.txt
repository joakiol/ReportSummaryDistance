CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 248?252Manchester, August 2008Mixing and Blending Syntactic and Semantic DependenciesYvonne SamuelssonDept.
of LinguisticsStockholm Universityyvonne.samuelsson@ling.su.seJohan EklundSSLISUniversity College of Bor?asjohan.eklund@hb.seOscar T?ackstr?omDept.
of Linguistics and PhilologySICS / Uppsala Universityoscar@sics.seMark Fi?selDept.
of Computer ScienceUniversity of Tartufishel@ut.eeSumithra VelupillaiDept.
of Computer and Systems SciencesStockholm University / KTHsumithra@dsv.su.seMarkus SaersDept.
of Linguistics and PhilologyUppsala Universitymarkus.saers@lingfil.uu.seAbstractOur system for the CoNLL 2008 sharedtask uses a set of individual parsers, a set ofstand-alone semantic role labellers, and ajoint system for parsing and semantic rolelabelling, all blended together.
The systemachieved a macro averaged labelled F1-score of 79.79 (WSJ 80.92, Brown 70.49)for the overall task.
The labelled attach-ment score for syntactic dependencies was86.63 (WSJ 87.36, Brown 80.77) and thelabelled F1-score for semantic dependen-cies was 72.94 (WSJ 74.47, Brown 60.18).1 IntroductionThis paper presents a system for the CoNLL 2008shared task on joint learning of syntactic and se-mantic dependencies (Surdeanu et al, 2008), com-bining a two-step pipelined approach with a jointapproach.In the pipelined system, eight different syntac-tic parses were blended, yielding the input for twovariants of a semantic role labelling (SRL) system.Furthermore, one of the syntactic parses was usedwith an early version of the SRL system, to pro-vide predicate predictions for a joint syntactic andsemantic parser.
For the final submission, all ninesyntactic parses and all three semantic parses wereblended.The system is outlined in Figure 1; the dashedarrow indicates the potential for using the predi-c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.ProcessParse + SRLProcess8 MaltParsersParser BlenderProcess2 Pipelined SRLsSRL BlenderJoint Parser/SRLPossibleIterationFigure 1: Overview of the submitted system.cate prediction to improve the joint syntactic andsemantic system.2 Dependency ParsingThe initial parsing system was created using Malt-Parser (Nivre et al, 2007) by blending eight dif-ferent parsers.
To further advance the syntactic ac-curacy, we added the syntactic structure predictedby a joint system for syntactic and semantic depen-dencies (see Section 3.4) in the blending process.2.1 ParsersThe MaltParser is a dependency parser genera-tor, with three parsing algorithms: Nivre?s arcstandard, Nivre?s arc eager (see Nivre (2004)for a comparison between the two Nivre algo-rithms), and Covington?s (Covington, 2001).
Bothof Nivre?s algorithms assume projectivity, butthe MaltParser supports pseudo-projective parsing(Nilsson et al, 2007), for projectivization and de-projectivization.248WSJ BrownBest single parse 85.22% 78.37%LAS weights 87.00% 80.60%Learned weights 87.36% 80.77%Table 1: Labelled attachment score on the two testsets of the best single parse, blended with weightsset to PoS labelled attachment score (LAS) andblended with learned weights.Four parsing algorithms (the two Nivre al-gorithms, and Covington?s projective and non-projective version) were used, creating eightparsers by varying the parsing direction, left-to-right and right-to-left.
The latter was achieved byreversing the word order in a pre-processing stepand then restoring it in post-processing.
For the fi-nal system, feature models and training parameterswere adapted from Hall et al (2007).2.2 BlenderThe single parses were blended following the pro-cedure of Hall et al (2007).
The parses of eachsentence were combined into a weighted directedgraph.
The Chu-Liu-Edmonds algorithm (Chu andLiu, 1965; Edmonds, 1967) was then used to findthe maximum spanning tree (MST) of the graph,which was considered the final parse of the sen-tence.
The weight of each graph edge was calcu-lated as the sum of the weights of the correspond-ing edges in each single parse tree.We used a simple iterative weight updating algo-rithm to learn the individual weights of each singleparser output and part-of-speech (PoS) using thedevelopment set.
To construct an initial MST, thelabelled attachment score was used.
Each singleweight, corresponding to an edge of the hypoth-esis tree, was then iteratively updated by slightlyincreasing or decreasing the weight, depending onwhether it belonged to a correct or incorrect edgeas compared to the reference tree.2.3 ResultsThe results are summarized in Table 1; the parsewith LAS weights and the best single parse(Nivre?s arc eager algorithm with left-to-right pars-ing direction) are also included for comparison.3 Semantic Role LabellingThe SRL system is a pipeline with three chainedstages: predicate identification, argument identifi-cation, and argument classification.
Predicate andargument identification are treated as binary clas-sification problems.
In a simple post-processingpredicate classification step, a predicted predicateis assigned the most frequent sense from the train-ing data.
Argument classification is treated as amulti-class learning problem, where the classescorrespond to the argument types.3.1 Learning and Parameter OptimizationFor learning and prediction we used the freelyavailable support vector machine (SVM) imple-mentation LIBSVM (version 2.86) (Chang andLin, 2001).
The choice of cost and kernel parame-ter values will often significantly influence the per-formance of the SVM classifier.
We therefore im-plemented a parameter optimizer based on the DI-RECT optimization algorithm (Gablonsky, 2001).It iteratively divides the search space into smallerhyperrectangles, sampling the objective functionin the centroid of each hyperrectangle, and select-ing those hyperrectangles that are potentially opti-mal for further processing.
The search space con-sisted of the SVM parameters to optimize and theobjective function was the cross-validation accu-racy reported by LIBSVM.Tests performed during training for predicateidentification showed that the use of runtime opti-mization of the SVM parameters for nonlinear ker-nels yielded a higher average F1-score effective-ness.
Surprisingly, the best nonlinear kernels werealways outperformed by the linear kernel with de-fault settings, which indicates that the data is ap-proximately linearly separable.3.2 Filtering and Data Set SplittingTo decrease the number of instances during train-ing, all predicate and argument candidates withPoS-tags that occur very infrequently in thetraining set were filtered out.
Some PoS-tagswere filtered out for all three stages, e.g.
non-alphanumerics, HYPH, SYM, and LS.
This ap-proach was effective, e.g.
removing more than halfof the total number of instances for predicate pre-diction.To speed up the SVM training and allow forparallelization, each data set was split into severalbins.
However, there is a trade-off between speedand accuracy.
Performance consistently deterio-rated when splitting into smaller bins.
The finalsystem contained two variants, one with more binsbased on a combination of PoS-tags and lemmafrequency information, and one with fewer bins249based only on PoS-tag information.
The threelearning tasks used different splits.
In general, theargument identification step was the most difficultand therefore required a larger number of bins.3.3 FeaturesWe implemented a large number of features (over50)1for the SRL system.
Many of them can befound in the literature, starting from Gildea andJurafsky (2002) and onward.
All features, exceptbag-of-words, take nominal values, which are bi-narized for the vectors used as input to the SVMclassifier.
Low-frequency feature values (exceptfor Voice, Initial Letter, Number of Words, Rela-tive Position, and the Distance features), below athreshold of 20 occurrences, were given a defaultvalue.We distinguish between single node and nodepair features.
The following single node featureswere used for all three learning tasks and for boththe predicate and argument node:2?
Lemma, PoS, and Dependency relation (DepRel) forthe node itself, the parent, and the left and right sibling?
Initial Letter (upper-case/lower-case), Number ofWords, and Voice (based on simple heuristics, only forthe predicate node during argument classification)?
PoS Sequence and PoS bag-of-words (BoW) for thenode itself with children and for the parent with chil-dren?
Lemma and PoS for the first and last child of the node?
Sequence and BoW of Lemma and PoS for contentwords?
Sequence and BoW of PoS for the immediate children?scontent words?
Sequence and BoW of PoS for the parent?s contentwords and for the parent?s immediate children?
Sequence and BoW of DepRels for the node itself, forthe immediate children, and for the parent?s immediatechildrenAll extractors of node pair features, where the pairconsists of the predicate and the argument node,can be used both for argument identification andargument classification.
We used the followingnode pair features:?
Relative Position (the argument is before/after the pred-icate), Distance in Words, Middle Distance in DepRels?
PoS Full Path, PoS Middle Path, PoS Short Path1Some features were discarded for the final system basedon Information Gain, calculated using Weka (Witten andFrank, 2005).2For all features using lemma or PoS the (predicted) splitvalue is used.The full path feature contains the PoS-tag of the ar-gument node, all dependency relations between theargument node and the predicate node and finallythe PoS-tag of the predicate node.
The middle pathgoes to the lowest common ancestor for argumentand predicate (this is also the distance calculatedby Middle Distance in DepRels) and the short pathonly contains the dependency relation of the argu-ment and predicate nodes.3.4 Joint Syntactic and Semantic ParsingWhen considering one predicate at a time, SRL be-comes a regular labelling problem.
Given a pre-dicted predicate, joint learning of syntactic and se-mantic dependencies can be carried out by simulta-neously assigning an argument label and a depen-dency relation.
This is possible because we knowa priori where to attach the argument, since thereis only one predicate candidate3.
The MaltParsersystem for English described in Hall et al (2007)was used as a baseline, and then optimized for thisnew task, focusing on feature selection.A large feature model was constructed, andbackward selection was carried out until no fur-ther gain could be observed.
The feature model ofMaltParser consists of a number of feature types,each describing a starting point, a path through thestructure so far, and a column of the node arrivedat.
The number of feature types was reduced from37 to 35 based on the labelled F1-score.As parsing is done at the same time as argu-ment labelling, different syntactic structures riskbeing assigned to the same sentence, dependingon which predicate is currently processed.
Thismeans that several, possibly different, parses haveto be combined into one.
In this experiment, thehead and the dependency label were concatenated,and the most frequent one was used.
In case ofa tie, the first one to appear was used.
The like-lihood of the chosen labelling was also used as aconfidence measure for the syntactic blender.3.5 Blending and Post-ProcessingCombining the output from several different sys-tems has been shown to be beneficial (Koomenet al, 2005).
For the final submission, we com-bined the output of two variants of the pipelinedSRL system, each using different data splits, with3The version of the joint system used in the submissionwas based on an early predicate prediction.
More accuratepredicates would give a major improvement for the results.250Test set Pred PoS Labelled F1Unlabelled F1WSJ All 82.90 90.90NN* 81.12 86.39VB* 85.52 96.49Brown All 67.48 85.49NN* 58.34 75.35VB* 73.24 91.97Table 2: Semantic predicate results on the test sets.the SRL output of the joint system.
A simple uni-form weight majority vote heuristic was used, withno combinatorial constraints on the selected argu-ments.
For each sentence, all predicates that wereidentified by a majority of the systems were se-lected.
Then, for each selected predicate, its ar-guments were picked by majority vote (ignoringthe systems not voting for the predicate).
The bestsingle SRL system achieved a labelled F1-scoreof 71.34 on the WSJ test set and 57.73 on theBrown test set, compared to 74.47 and 60.18 forthe blended system.As a final step, we filtered out all verbal andnominal predicates not in PropBank or NomBank,respectively, based on the predicted PoS-tag andlemma.
Each lexicon was expanded with lemmasfrom the training set, due to predicted lemma er-rors in the training data.
This turned out to be asuccessful strategy for the individual systems, butslightly detrimental for the blended system.3.6 ResultsSemantic predicate results for WSJ and Brown canbe found in Table 2.
Table 4 shows the results foridentification and classification of arguments.4 Analysis and ConclusionsIn general, the mixed and blended system performswell on all tasks, rendering a sixth place in theCoNLL 2008 shared task.
The overall scores forthe submitted system can be seen in Table 3.4.1 ParsingFor the blended parsing system, the labelled at-tachment score drops from 87.36 for the WSJ testset to 80.77 for the Brown test set, while the unla-belled attachment score only drops from 89.88 to86.28.
This shows that the system is robust withregards to the overall syntactic structure, even ifpicking the correct label is more difficult for theout-of-domain text.The parser has difficulties finding the right headfor punctuation and symbols.
Apart from errors re-WSJ + Brown WSJ BrownSyn + Sem 79.79 80.92 70.49Syn 86.63 87.36 80.77Sem 72.94 74.47 60.18Table 3: Syntactic and semantic scores on the testsets for the submitted system.
The scores, from topto bottom, are labelled macro F1, labelled attach-ment score and labelled F1.garding punctuation, most errors occur for IN andTO.
A majority of these problems are related to as-signing the correct dependency.
This is not surpris-ing, since these are categories that focus on formrather than function.There is no significant difference in score for leftand right dependencies, presumably because of thebi-directional parsing.
However, the system over-predicts dependencies to the root.
This is mainlydue to the way MaltParser handles tokens not be-ing attached anywhere during parsing.
These to-kens are by default assigned to the root.4.2 SRLSimilarly to the parsing results, the blended SRLsystem is less robust with respect to labelled F1-score, dropping from 74.47 on the WSJ test set to60.18 on the Brown test set.
The correspondingdrop in unlabelled F1-score is from 82.90 to 75.49.The simple method of picking the most com-mon sense from the training data works quite well,but the difference in domain makes it more diffi-cult to find the correct sense for the Brown corpus.In the future, a predicate classification module isneeded.
For the WSJ corpus, assigning the mostcommon predicate sense works better with nomi-nal than with verbal predicates, while verbal pred-icates are handled better for the Brown corpus.In general, verbal predicate-argument structuresare handled better than nominal ones, for bothtest sets.
This is not surprising, since nominalpredicate-argument structures tend to vary more intheir composition.Since we do not use global constraints for theargument labelling (looking at the whole argumentstructure for each predicate), the system can out-put the same argument label for a predicate severaltimes.
For the WSJ test set, for instance, the ra-tio of repeated argument labels is 5.4% in the sys-tem output, compared to 0.3% in the gold standard.However, since there are no confidence scores forpredictions it is difficult to handle this in the cur-rent system.251PPOSS(pred) + ARG WSJ F1Brown F1NN* + A0 61.42 38.99NN* + A1 67.07 53.10NN* + A2 57.02 26.19NN* + A3 63.08 (16.67)NN* + AM-ADV 4.65 (-)NN* + AM-EXT 44.78 (40.00)NN* + AM-LOC 49.45 (-)NN* + AM-MNR 53.51 21.82NN* + AM-NEG 79.37 (46.15)NN* + AM-TMP 67.23 (25.00)VB* + A0 81.72 73.58VB* + A1 81.77 67.99VB* + A2 60.91 50.67VB* + A3 61.49 (14.28)VB* + A4 77.84 (40.00)VB* + AM-ADV 47.49 30.33VB* + AM-CAU 55.12 (35.29)VB* + AM-DIR 41.86 37.14VB* + AM-DIS 71.91 37.04VB* + AM-EXT 60.38 (-)VB* + AM-LOC 55.69 37.50VB* + AM-MNR 49.54 36.25VB* + AM-MOD 94.85 82.42VB* + AM-NEG 93.45 77.08VB* + AM-PNC 50.00 (62.50)VB* + AM-TMP 69.59 49.07VB* + C-A1 70.76 55.32VB* + R-A0 83.68 70.83VB* + R-A1 68.87 51.43VB* + R-AM-LOC 38.46 (25.00)VB* + R-AM-TMP 56.82 (58.82)Table 4: Semantic argument results on the twotest sets, showing arguments with more than 20instances in the gold test set (fewer instances forBrown are given in parentheses).AcknowledgementsThis project was carried out within the course Ma-chine Learning 2, organized by GSLT (SwedishNational Graduate School of Language Tech-nology), with additional support from NGSLT(Nordic Graduate School of Language Technol-ogy).
We thank our supervisors Joakim Nivre,Bj?orn Gamb?ack and Pierre Nugues for advice andsupport.
Computations were performed on theBalticGrid and UPPMAX (projects p2005008 andp2005028) resources.
We thank Tore Sundqvist atUPPMAX for technical assistance.ReferencesChang, Chih-Chung and Chih-Jen Lin, 2001.
LIBSVM:A library for support vector machines.Chu, Y. J. and T. H. Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14:1396?1400.Covington, Michael A.
2001.
A fundamental algo-rithm for dependency parsing.
In Proceedings of the39th Annual Association for Computing MachinerySoutheast Conference, Athens, Georgia.Edmonds, Jack.
1967.
Optimum branchings.
Jour-nal of Research of the National Bureau of Standards,71(B):233?240.Gablonsky, J?org M. 2001.
Modifications of the DI-RECT algorithm.
Ph.D. thesis, North Carolina StateUniversity, Raleigh, North Carolina.Gildea, Daniel and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Hall, Johan, Jens Nilsson, Joakim Nivre, G?uls?enEryi?git, Be?ata Megyesi, Mattias Nilsson, andMarkus Saers.
2007.
Single malt or blended?A study in multilingual parser optimization.
InProceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL 2007, Prague, Czech Republic.Koomen, Peter, Vasin Punyakanok, Dan Roth, andWen-tau Yih.
2005.
Generalized inference withmultiple semantic role labeling systems.
In Proceed-ings of the Ninth Conference on Computational Nat-ural Language Learning (CoNLL-2005), Ann Arbor,Michigan.Nilsson, Jens, Joakim Nivre, and Johan Hall.
2007.Generalizing tree transformations for inductive de-pendency parsing.
In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics, Prague, Czech Republic.Nivre, Joakim, Johan Hall, Jens Nilsson, AtanasChanev, G?uls?en Eryi?git, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,2(13):95?135.Nivre, Joakim.
2004.
Incrementality in deterministicdependency parsing.
In Proceedings of the ACL?04Workshop on Incremental Parsing: Bringing Engi-neering and Cognition Together, Barcelona, Spain.Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syntac-tic and semantic dependencies.
In Proceedings ofthe 12th Conference on Computational Natural Lan-guage Learning (CoNLL-2008), Manchester, GreatBritain.Witten, Ian H. and Eibe Frank.
2005.
Data mining:Practical machine learning tools and techniques.Morgan Kaufmann, Amsterdam, 2nd edition.252
