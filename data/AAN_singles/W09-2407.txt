Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 46?51,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsMaking Semantic Topicality Robust Through Term Abstraction?Paul M. HeiderDepartment of LinguisticsUniversity at BuffaloThe State University of New YorkBuffalo, NY 14260, USApmheider@buffalo.eduRohini K. SrihariJanya, Inc.1408 Sweet Home RoadSuite 1Amherst, NY 14228rohini@cedar.buffalo.eduAbstractDespite early intuitions, semantic similarityhas not proven to be robust for splitting multi-party interactions into separate conversations.We discuss some initial successes with usingthesaural headwords to abstract the seman-tics of an utterance.
This simple profilingtechnique showed improvements over base-line conversation threading models.1 IntroductionTopic segmentation is the problem of dividing adocument into smaller coherent units.
The seg-ments can be hierarchical or linear; the topics canbe localized or distributed; the documents can benewswire or chat logs.
Of course, each of thesevariables is best analyzed as continuous rather thandiscrete.
Newswire, for instance, is a more formal,monologue-style genre while a chat log tends to-wards the informal register with different conversa-tions interwoven.We present a topic segmenter which uses seman-tics to define coherent conversations within a larger,multi-party document.
Using a word?s thesaurus en-try as a proxy for its underlying semantics providesa domain-neutral metric for distinguishing conver-sations.
Also, our classifier does not rely on met-alinguistic properties that may not be robust acrossgenres.?
The first author was partially funded through a fel-lowship from the SUNY at Buffalo Department of Linguis-tics and partially through a research assistantship at Janya,Inc.
(http://www.janyainc.com, Air Force Grant No.sFA8750-07-C-0077 and FA8750-07-D-0019, Task Order 0004)2 BackgroundMost work on lexical cohesion extends from Halli-day and Hasan (1976).
They formalize a text as anysemantic unit realized through sentences.
Linguisticfeatures found to justify binding sentences togetherinto Halliday and Hasan?s notion of a text includepronouns (Hobbs, 1979; Kehler, 2000), lexical over-lap (Hearst, 1997; Kozima, 1993; Morris and Hirst,1991), cue phrases (Manning, 1998), and discoursemarkers (Power et al, 2003; Reynar, 1999; Beefer-man et al, 1999), among others.
Of course, mostof this earlier work assumes the sentences constitut-ing any text are contiguous.
Thus, a document iscomprised of a series of semantic units that progressfrom one to the next with no returns to old topics.Multi-party interactions1 abide by a different setof assumptions.
Namely, a multi-party interactioncan include multiple floors (Aoki et al, 2006).
Muchlike at a cocktail party, we can expect more than asingle conversation at every given time.
These dif-ferent conversational floors are the major semanticunits a topic segmentation algorithm must recog-nize.
Spoken chat models (Aoki et al, 2006; Aokiet al, 2003) can make a simplifying assumption thatspeakers tend to only participate in one conversationat a time.
However, in text chat models, Elsner andCharniak (2008) show that speakers seem to partici-pate in more conversations roughly as a function ofhow talkative they are (cf.
Camtepe et al, 2005).In both modalities, speaker tendency to stay on thesame topics is a robust cue for conversational coher-1See O?Neill and Martin (2003) for an analysis of differ-ences between two- and multi-party interactions.46ence (Elsner and Charniak, 2008; Acar et al, 2005).Despite the initial intuitions of Halliday andHasan (1976), semantic similarity has not provento be a robust cue for multi-party topic segmen-tation.
For instance, Acar et al (2005) and Gal-ley et al (2003) used word repetition in their defi-nition of coherence but found that words commonto too many conversations hurt modeling perfor-mance.
Elsner and Charniak (2008) used frequencybinning based on the entire document to reduce thenoise introduced by high-frequency words.
Un-fortunately, binning requires a priori knowledge ofthe relative frequencies of words.2 Additionally,those authors used an on-topic/off-topic word listto bifurcate technical and non-technical utterances.Again, this technique assumes prior knowledge ofthe strongest on-topic cue words.Since semantic repetition is clearly useful butsimple word repetition is not a reliable measure, weinvestigated other measures of semantic relatedness.Elsner and Charniak (2008) conceded that context-based measures like LSA (Deerwester et al, 1990)require a clear notion of document boundary to func-tion well.
Dictionary-based models (Kozima andFurugori, 1993) are a step in the right direction be-cause they leverage word co-occurrence within defi-nitions to measure relatedness.
The richer set of con-nections available in WordNet models should pro-vide an even better measure of relatedness (Sussna,1993; Resnik, 1995).
Unfortunately, these mea-sures have unequal distribution by part-of-speechand uneven density of lemmas by semantic domain.3Thesaurus-based models (Morris and Hirst, 1991)provide many of the same advantages as dictionary-and WordNet-based models.4 In addition to the hier-archical relations encoded by the thesaurus, we cantreat each thesaural category as one dimension ofa topicality domain similar to the way Elsner andCharniak leveraged their list of technical terms.
Insum, our model focuses on the abstraction of lem-mas that is inherent to a thesaurus while limitingthe domain-specific and a priori knowledge required2One could use frequencies from a general corpus but thatshould only perform as well as a graded stop-word list.3As one reviewer noted, some parts-of-speech may con-tribute more to a topic profile than others.
Unfortunately, thisempirical question must wait to be tested.4Budanitsky and Hirst (2006) review the advantages.by a classifier to divide multi-party interactions intoseparate conversational floors.3 ModelAt a high level, our chat topic segmenter works likemost other classifiers: each input line is tokenized5,passed to a feature analyzer, and clustered with re-lated lines.
Unlike traditional topic segmentationmodels, each input represents a new utterance in thechat log.
These utterances can range from singlewords to multiple sentences.
Another aspect of ourmodel (although not unique to it) is the on-line clas-sification of text.
We aim to model topic segmenta-tion as if our classifier were sitting in a chat room,and not as a post-process.While our feature analyzer focuses on semanticmarkers, interlocutor names are also recorded.
Twointuitions were implemented with respect to individ-uals?
names: continued affiliation with old conver-sations and naming interlocutors to focus attention.All else being equal, one would assume a speakerwill continue in the conversations she has alreadyparticipated in.
Moreover, she will most likely con-tinue with the last conversation she was part of.
Asthe total number of conversations increases, the like-lihood of sticking to the last conversation will de-crease.The second intuition derives from the observa-tion in O?Neill and Martin (2003) that speakers ac-commodate for cocktail-style conversations by usingdirect mentions of interlocutors?
names.
We onlymodel backward referencing names.
That is, if aspeaker uses the name of another user, we assumethat the speaker is overtly affiliating with a conver-sation of the other user.
Forward referencing is dis-cussed under future work (see Section 6).Following Budanitsky and Hirst (2006), we baseour notion of semantic topicality on thesaural rela-tions.
Broadly speaking, two utterances are highlyrelated if their tokenized words (hereafter, lemmas)co-occur in more of the same thesaural categoriesthan not.
We will defer further explanation of thesefeatures until we have explained our reference the-sauri in Subsection 3.1.
Unfortunately, many desir-able and robust features are missing from our classi-fier.
See Section 6 for a discussion of future work.5We used SemantexTM (Srihari et al, 2008).47In the final stage of our processing pipeline, weuse a panel of experts to generate a simple weightedclassification.
Each feature described above con-tributes a roughly equal vote towards the final sort-ing decision.
Barring a single strong preference or acohort of weak preferences for one conversation, themodel assumes the incoming utterance introduces anew conversational floor.3.1 ThesauriWe chose two machine-readable and public-domainthesauri for our model: Roget?s Thesaurus (1911)and Moby Thesaurus II (2002).
Both are avail-able from Project Gutenberg (gutenberg.org).
In thecompilation notes for Roget?s Thesaurus, the editormentions a supplement of 1,000+ words to the orig-inal work.
A rough count shows 1,000 headwords(the basic grouping level) and 55,000 synonyms (anyword listed under a headword).
The second edi-tion of Moby Thesaurus contains some 30,000 head-words and 2.5 million synonyms.
Moby Thesaurusincludes many newer terms than Roget?s Thesaurus.Structurally, Roget?s Thesaurus has a distinct advan-tage over Moby Thesaurus.
The former includes asix-tiered category structure with cross-indexing be-tween headwords.
The latter is only organized intoheadword lists.3.2 MetricsAs we mentioned above, our model uses three pri-mary metrics in classifying a new utterance: con-versation affiliations of the current speaker, conver-sation affiliations of any explicitly mentioned inter-locutors, and semantic similarity.
In the end, all theconversation affiliation votes are summed with theone conversation preferred by each of the three the-saural measures.6 The input line is then merged withthe conversation that received the most votes.
De-tails for deriving the votes follow.Every conversation a speaker has participated inreceives a vote.
Moreover, his last conversation getsadditional votes as a function of his total numberof conversations (see Equation 1).
Likewise, everyconversation a named interlocutor has participatedin receives a vote with extra votes given to her lastconversation as a function of how gregarious she is.6In the long run, a list of conversations ranked by similarityscore would be better than a winner-takes-all return value.Headword Type Weight ChangeDirect Match 1Co-hyponymous Headword 0.25Cross-indexed Headword 0.75Table 1: Spreading Activation Weights.V ote = 3ln(|Conversationsspeaker|) + 1 (1)Each utterance is then profiled in terms of the-saural headwords.
Every lemma in an utterancematching to some headword increments the activa-tion of that headword by one.7 A conversation?s se-mantic profile is a summation of the profiles of itsconstituent sentences.
In order to simulate the driftof topic in a conversation, the conversation?s seman-tic profile decays with every utterance.
Thus, morerecent headwords will be more activated than head-words activated near the beginning of a conversa-tion.
Decay is modeled by halving the activation ofa headword in every cycle that it was not topical.Moreover, a third profile was kept to simulatespreading activation within the thesaurus.
For thisprofile, each topical headword is activated.
Everycross-indexed headword listed within this categoryis also augmented by a fixed degree.
Finally, everyheadword that occupies the same thesaurus sectionis augmented.
An overview of the weight changesis listed in Table 1.
The specific weights fit the au-thors?
intuitions as good baselines.
These weightscan easily be trained to generate a better model.The similarity between a new line (the test) anda conversation (the base) is computed as the sum ofmatch bonuses and mismatch penalties in Table 2.8Table 3 scores an input line (TEST) against two con-versations (BASE1 and BASE2) with respect to fourheadwords (A, B, C, and D).
In order to control fortext size, we also computed the average headword7Most other models include an explicit stop-word list to re-duce the effect of function words.
Our model implicitly relieson the thesaurus look-up to filter out function words.
One ad-vantage to our approach is the ability to preferentially weightdifferent headwords or lemma to headword relations.8Like with Table 1, these numbers reflect the authors?
intu-itions and can be improved through standard machine learningmethods.48Headword TestPresent Yes NoAvg?
Above BelowBase YesAbove +1 +0.5 -0.1Below +0.5 +1 -0.05No -1 -0.5 +0.0001Table 2: Similarity Score Calculations.A B C D ScoreTEST high high low 0 ?BASE1 high low low high+1 +.5 +1 -.1 2.4BASE2 0 high 0 0-1 +1 -.5 +.0001 -.4999Table 3: A Example Similarity Scoring for Two Conver-sations.
?High?
and ?low?
refers to headword activation.activation in a conversation.
Intuitively, we considerit best when a headword is activated for both thebase and test condition.
Moreover, a headword withequally above average or equally below average acti-vation is better than a headword with above averageactivation in the base but below average activationin the test.
In the second best case, neither conditionshows any activation in a headword.
The penulti-mately bad condition occurs when the base containsa headword that the test does not.
We do not wantto penalize the test (which is usually smaller) for notcontaining everything that the base does.
Finally, ifthe test condition contains a headword but the basedoes not, we want to penalize the conversation most.4 DatasetOur primary dataset was distributed by Elsner andCharniak (2008).
They collected conversations fromthe IRC (Internet Relay Chat) channel ##LINUX,a very popular room on freenode.net with widelyranging topics.
University students then annotatedthese chat logs into conversations.
We take the col-lection of these annotations to be our gold standardfor topic segmentation with respect to the chat logs.4.1 MetricsElsner and Charniak (2008) use three major mea-sures to compare annotations: a 1-to-1 comparison,E&C Annotators Our ModelMean Max MinConversations 81.33 128 50 153Avg.
Length 10.6 16.0 6.2 5.2Table 4: General statistics for our model as comparedwith Elsner and Charniak?s human annotators.
Somenumbers are taken from Table 1 (Elsner and Charniak,2008).Mean Max MinInter-annotator 86.70 94.13 75.50Our Model 65.17 74.50 53.38Table 5: Comparative many-to-1 measures for evaluatingdifferences in annotation granularity.
Some numbers aretaken from Table 1 (Elsner and Charniak, 2008).a loc3 comparison, and a many-to-1 comparison.The 1-to-1 metric tries to maximize the global con-versation overlap in two annotations.
The loc3 scaleis better at measuring local agreement.
This scorecalculates accuracy between two annotations foreach window of three utterances.
Slight differencesin a conversation?s start and end are minimized.
Fi-nally, the many-to-1 score measures the entropy dif-ference between annotations.
In other words, sim-plifying a fine-grained analysis to a coarse-grainedanalysis will yield good results because of sharedmajor boundaries.
Disagreeing about the major con-versation boundaries will yield a low score.5 AnalysisCompared with the gold standard, our model has astrong preference to split conversations into smallerunits.
As is evident from Table 4, our model hasmore conversations than the maximally splitting hu-man annotator.
These results are unsurprising giventhat our classifier posits a new conversation in theabsence of contrary evidence.
Despite a low 1-to-1score, our many-to-1 score is relatively high (see Ta-ble 5).
We can interpret these results to mean thatour model is splitting gold standard conversationsinto smaller sets rather than creating conversationsacross gold standard boundaries.A similar interaction of annotation granularityshows up in Table 6.
Our 1-to-1 measures are justbarely above the baseline, on average.
On the other49As % of .
.
.
Misclassified AllError Type Mean Max Min MeanMismatch 25.84 23.78 28.13 11.93Split Error 62.96 63.11 63.89 29.08Lump Error 11.20 13.11 7.99 5.17Table 7: Source of misclassified utterances as a percent-age of misclassified utterances and all utterances.hand, our loc3 measure jumps much closer to thehuman annotators.
In other words, the maximum an-notation overlap of our model and any given humanis poor9 while the local coherence of our annotationwith respect to any human annotation is high.
Thispattern is symptomatic of over-splitting, which is ex-cessively penalized by the 1-to-1 metric.10We also analyzed the types of errors our modelmade while holding the conversation history con-stant.
We simulated a consistent conversation his-tory by treating the gold standard?s choice as an un-beatable vote and tabulating the number of times ourmodel voted with and against the winning conversa-tion.
There were five numbers tabulated: matchingnew conversation votes, matching old conversationvotes, mismatching old conversation votes, incorrectsplit vote, and incorrect lump vote.
The mismatch-ing old conversation votes occurred when our modelvoted for an old conversation but guessed the wrongconversation.
The incorrect split vote occurred whenour model wanted to create a new conversation butthe gold standard voted with an old conversation.Finally, the incorrect lump vote occurred when ourmodel matched the utterance with a old conversationwhen the gold standard created a new conversation.Across all six gold standard annotations, nearlytwo-thirds of the errors arose from incorrect splittingvotes (see Table 7).
In fact, nearly one-third of allutterances fell into this category.6 Future WorkThe high granularity for what our model considers aconversation had a huge impact on our performance9Elsner and Charniak (2008) found their annotators alsotended to disagree on the exact point when a new conversationbegins.10Aoki et al (2006) present a thorough analysis of conver-sational features associated with schisming, the splitting off ofnew conversations from old conversations.scores.
The high many-to-1 scores imply that morehuman-like chunks will improve performance.
Thegranularity may be very task dependent and so wewill need to be careful not to overfit our model to thisdata set and these annotators.
New features shouldbe tested with several chat corpora to better under-stand the cue trading effects of genre.At present, our model uses only a minimal set offeatures.
Discourse cues and temporal cues are twosimple measures that can be added.
Our current fea-tures can also use refinement.
For instance, even par-tially disambiguating the particular sense of the lem-mas should reduce the noise in our similarity mea-sures.
Ranking the semantic similarity, in contrastwith the current winner-takes-all approach, shouldimprove our results.
Accounting for forward refer-encing, when a speaker invokes another?s name todraw them into a conversation, is also important.Finally, understanding the different voting pat-terns of each feature system will help us to betterunderstand the reliability of the different cues.
To-wards this end, we need to monitor and act upon thestrength and type of disagreement among voters.AcknowledgmentsHarish Srinivasan was great help in tokenizing thedata.
The NLP Group at Janya, Inc., Jordana Heller,Jean-Pierre Koenig, Michael Prentice, and threeanonymous reviewers provided useful feedback.ReferencesEvrim Acar, Seyit Ahmet Camtepe, Mukkai S. Kr-ishnamoorthy, and Blent Yener.
2005.
Model-ing and multiway analysis of chatroom tensors.
InPaul B. Kantor, Gheorghe Muresan, Fred Roberts,Daniel Dajun Zeng, Fei-Yue Wang, Hsinchun Chen,and Ralph C. Merkle, editors, ISI, volume 3495 ofLecture Notes in Computer Science, pages 256?268.Springer.Paul M. Aoki, Matthew Romaine, Margaret H. Szyman-ski, James D. Thornton, Daniel Wilson, and AllisonWoodruff.
2003.
The Mad Hatter?s cocktail party: Asocial mobile audio space supporting multiple simul-taneous conversations.
In CHI 03: Proceedings of theSIGCHI Conference on Human Factors in ComputingSystems, pages 425?432, New York, NY, USA.
ACMPress.Paul M. Aoki, Margaret H. Szymanski, Luke Plurkowski,James D. Thornton, Allison Woodruff, and Weilie Yi.50Other Annotators E&C Model Our Model E&C Best BaselineMean 1-to-1 52.98 40.62 35.77 34.73Max 1-to-1 63.50 51.12 49.88 56.00Min 1-to-1 35.63 33.63 28.25 28.62Mean loc3 81.09 72.75 68.73 62.16Max loc3 86.53 75.16 72.77 69.05Min loc3 74.75 70.47 64.45 54.37Table 6: Metric values for our model as compared with Elsner and Charniak?s human annotators and classifier.
Somenumbers are taken from Table 3 (Elsner and Charniak, 2008).2006.
Where?s the ?party?
in ?multi-party??
Ana-lyzing the structure of small-group sociable talk.
InACM Conference on Computer Supported Coopera-tive Work, pages 393?402, Banff, Alberta, Canada,November.
ACM Press.Doug Beeferman, Adam Berger, and John Lafferty.1999.
Statistical models for text segmentation.
In Ma-chine Learning, pages 177?210.Alexander Budanitsky and Graeme Hirst.
2006.
Evalu-ating WordNet-based measures of lexical semantic re-latedness.
Computational Linguistics, 32(1):13?47.Seyit Ahmet Camtepe, Mark K. Goldberg, MalikMagdon-Ismail, and Mukkai Krishnamoorty.
2005.Detecting conversing groups of chatters: A model, al-gorithms, and tests.
In IADIS AC, pages 89?96.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-dauer, and R. Harshman.
1990.
Indexing by LatentSemantic Analysis.
Journal of the American Societyof Information Science, 41:391?407.Micha Elsner and Eugene Charniak.
2008.
You talk-ing to me?
A corpus and algorithm for conversa-tion disentanglement.
In The Association for Compu-tational Linguistics: Human Language Technologies(ACL-HLT 2008), Columbus, Ohio.Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,and Hongyan Jing.
2003.
Discourse segmentation ofmulti-party conversation.
In Proceedings of the ACL,pages 562?569.Michael Halliday and Ruqaiya Hasan.
1976.
Cohesionin English.
Longman Group, New York.Marti A. Hearst.
1997.
TextTiling: Segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1):33?64.J.
R. Hobbs.
1979.
Coherence and coreference.
Cogni-tive Science, 3:67?90.A.
Kehler.
2000.
Coherence, Reference, and the Theoryof Grammar.
CSLI Publications.Hideki Kozima and Teiji Furugori.
1993.
Similaritybetween words computed by spreading activation onan English dictionary.
In Proceedings of 6th Con-ference of the European Chapter of the Associationfor Computational Linguistics (EACL?93), pages 232?239, Utrecht.Hideki Kozima.
1993.
Text segmentation based on sim-ilarity between words.
In Proceedings of ACL?93,pages 286?288, Ohio.C.
D. Manning.
1998.
Rethinking text segmentationmodels: An information extraction case study.
Tech-nical Report SULTRY?98?07?01, University of Syd-ney.Jane Morris and Graeme Hirst.
1991.
Lexical cohe-sion computed by thesaural relations as an indicatorof the structure of text.
Computational Linguistics,17(1):21?48.Jacki O?Neill and David Martin.
2003.
Text chat in ac-tion.
In GROUP ?03: Proceedings of the 2003 Inter-national ACM SIGGROUP Conference on SupportingGroup Work, pages 40?49, New York, NY, USA.
ACMPress.R.
Power, D. Scott, and N. Bouayad-Agha.
2003.Document structure.
Computational Linguistics,29(2):211?260.Philip Resnik.
1995.
Using information content to evalu-ate semantic similarity in a taxonomy.
In Proceedingsof the 14th International Joint Conference on ArtificialIntelligence, pages 448?453.Jeffrey C. Reynar.
1999.
Statistical models for topic seg-mentation.
In Proceedings of the 37th Annual Meetingof the ACL, pages 357?364, Maryland, USA, June.Peter Mark Roget, editor.
1911.
Roget?s Thesaurus.Project Gutenberg.R.
K. Srihari, W. Li, C. Niu, and T. Cornell.
2008.
Infox-tract: A customizable intermediate level informationextraction engine.
Journal of Natural Language Engi-neering, 14(1):33?69.Michael Sussna.
1993.
Word sense disambiguationfor free-text indexing using a massive semantic net-work.
In Proceedings of the Second InternationalConference on Information and Knowledge Manage-ment (CIKMA?93), pages 67?74, Arlington, VA.Grady Ward, editor.
2002.
Moby Thesaurus List.
ProjectGutenberg.51
