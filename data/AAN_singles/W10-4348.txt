Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 257?260,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsHow to Drink from a Fire Hose:One Person Can Annoscribe 693 Thousand Utterances in One MonthDavid Suendermann, Jackson Liscombe, Roberto PieracciniSpeechCycle LabsNew York, USA{david, jackson, roberto}@speechcycle.comAbstract.Transcription and semantic annotation(annoscription) of utterances is crucialpart of speech performance analysis andtuning of spoken dialog systems and othernatural language processing disciplines.However, the fact that these are manualtasks makes them expensive and slow.
Inthis paper, we will discuss how anno-scription can be partially automated.
Wewill show that annoscription can reach athroughput of 693 thousand utterances perperson month under certain assumptions.1 IntroductionEver since spoken dialog systems entered the com-mercial market in the mid 1990s, the caller?sspeech input is subject to collection, transcription,and often also semantic annotation.
Utterancetranscriptions and annotations (annoscriptions) areused to measure speech recognition and spokenlanguage understanding performance of the appli-cation.
Furthermore, they are used to improvespeech recognition and application functionalityby tuning grammars, introducing new transitionsin the call flow to cover more of the callers?
de-mands, or changing prompt wording or applica-tion logic to influence the speech input.
Anno-scriptions are also crucial for training statisticallanguage models and utterance classifiers for callrouting or other unconstrained speech input con-texts (Gorin et al, 1997).
Since very recently, sta-tistical methods are used to replace conventionalrule-based grammars in every recognition contextof commercial spoken dialog systems (Suender-mann et al, 2009b).
This replacement is onlypossible by collecting massive amounts of anno-scribed data from all contexts of an application.To give the reader an idea of what massive meansin this case, in (Suendermann et al, 2009b), weused 2,184,203 utterances to build a complex callrouting system.
In (Suendermann et al, 2009a),4,293,898 utterances were used to localize an En-glish Internet troubleshooting application to Span-ish.Considering that professional service providersmay charge as much as 50 US cents for annoscrib-ing a single utterance, the usage of these amountsof data seems prohibitive since costs for such aproject could potentially add up to several millionUS dollars.
Furthermore, one has to consider theaverage speed of annoscription which rarely ex-ceeds 1000 utterances per hour and person.
Thismeans that the turn-around of a project as men-tioned above would be several years unless teamsof many people work simultaneously.
However,the integration of the work of a large team be-comes the more tricky the more people are in-volved.
This is especially true for the annotationportion since it requires a thorough understand-ing of the spoken dialog system?s domain and de-sign and very often can only be conducted underclose supervision by the interaction designer incharge of the project.
Furthermore, there are cru-cial issues related to intra- and inter-labeler incon-sistency becoming more critical the more peoplework on the same or similar recognition contextsof a given project.This paper is to show how it is possible to au-tomate large portions of both transcription and an-notation while meeting human performance1 stan-dards.
As an example case, we show how the pro-posed automation techniques can increase anno-scription speed to nearly 693 thousand utterancesper person and month.2 Automatic Transcription2.1 Two FundamentalsAutomatic transcription of spoken utterances maynot sound as something new to the reader.
Infact, the entire field of automatic speech recogni-tion is about machine transcription.
So, why is itworth dedicating a full section to something well-covered in research and industry for half a cen-tury?
The reason is the demand for achieving hu-man performance as formulated in the introduc-tion which, as is also well-known, cannot be satis-fied by any of the large-vocabulary speech recog-nizers ever developed.
In order to demonstrate thatthere is indeed a way to achieve human transcrip-tion performance using automatic speech recogni-tion, we would like to refer to two fundamentalobservations on the performance of speech recog-1In this paper, performance stands for quality or accuracyof transcription or annotation.
It does not refer to speed orthroughput.257nition:(1) Speech recognition performance can be veryhigh for contexts of constrained vocabulary.
Anexample is the recognition of isolated letters inthe scope of a name spelling task as discussedin (Waibel and Lee, 1990) that achieved a worderror rate of only 1.1%.
In contrast, the word errorrate of large-vocabulary continuous speech recog-nition can be as high as 40 to 65% on telephonespeech (Yuk and Flanagan, 1999).
(2) The positive dependence between speechrecognition performance and amount of data usedto train acoustic and language models, so far, didnot reach a saturation point even considering bil-lions of training tokens (Och, 2006).Both of these fundamentals can be applied to thetranscription task for utterances collected on spo-ken dialog production systems as follows:(1) The vocabulary of spoken dialog systems canbe rather complex.
E.g., the caller utterances usedfor the localization project mentioned in Section 1distinguish more than 13,000 types.
However,the nature of commercial spoken dialog applica-tions being mostly system-driven strongly con-strains the vocabulary in many recognition con-texts.
E.g., when the prompt readsYou can say: recording problems, newinstallation, frozen screen, or won?t turnoncallers mostly respond things matching the pro-posed phrases, occasionally altering the wording,and only seldomly using completely unexpectedutterances.
(2) The continuous data feed available on high-traffic spoken dialog systems in production pro-cessing millions of calls per month can providelarge numbers of utterances for every possiblerecognition context.
Even if the context appears tobe of a simple nature, as for a yes/no question, thecontinuous collection of more data will still havean impact on the performance of a language modelbuilt using this data.2.2 How to Achieve Human PerformanceEven though we have suggested that the recog-nition performance in many contexts of spokendialog systems may be very high, we have stillnot shown how our observations can be utilized toachieve human performance as demanded in Sec-tion 1.
How would a context-dependent speechrecognizer respond when the caller says some-thing completely unexpected such as let?s wreck anice beach when asked for the cell phone number?While a human transcriber may still be able to cor-rectly transcribe this sentence, automatic speechrecognition will certainly fail even with the largestpossible training set.
The answer to this questionis that the speech recognizer should not respond atall in this case but admit that it had trouble rec-ognizing this utterance.
Rejection of hypothesesbased on confidence scores is common practice inmany speech and language processing tasks andis heavily used in spoken dialog systems to avoidmis-interpretation of user inputs.So, we now know that we can limit automatictranscriptions to hypotheses of a minimum relia-bility.
However, how do we prove that this limitedset resembles human performance?
What is actu-ally human performance?
Does the human makeerrors transcribing?
And, if so, how do we mea-sure human error?
What do we compare it against?To err is human.
Accordingly, there is an errorassociated with manual transcription which canonly be estimated by comparing somebody?s tran-scription with somebody else?s due to a lack ofground truth.
Preferably, one should have a goodnumber of people transcribe the same speech ut-terances and than compute the average word errorrate comparing every transcription batch with ev-ery other producing a reliable estimate of the man-ual error inherent to the transcription task of spo-ken dialog system utterances.
In order to do so,we compared transcriptions of 258,843 utterancescollected from a variety of applications and recog-nition contexts partially shared by up to six tran-scribers and found that they averaged at an inter-transcriber word error rate of WER0 = 1.3%.Now, for every recognition context a languagemodel had been trained, we performed automaticspeech recognition on held-out test sets of N =1000 utterances producing N hypotheses and theirassociated confidence scores P = {p1, .
.
.
pN}.Now, we determined that minimum confidencethreshold p0 for which the word error rate betweenthe set of hypotheses and manual reference tran-scriptions was not statistically significantly greaterthan WER0:p0 = arg minp?PWER(V (p)) ?6> WER0; (1)V (p) = {?1, .
.
.
, ?K} : ?k ?
{1, .
.
.
, N}, p?k ?
p.Statistical significance was achieved when thedelta resulted in a p value greater than 0.05 usingthe ?2 calculus.
For the number of test utterances,1000, this point is reached when the word erroron the test set falls below WER1 = 2.2%.
Thismeans that Equation 2.2?s ?not statistically signifi-cantly greater than?
sign can be replaced by a reg-ular smaller-than sign asWER ?6> WER0 ?
WER < WER1.
(2)This essentially means that there is a chance thatthe error produced by automatic transcription isgreater than that of manual transcription, however,on the test set it could not be found to be of signifi-cance.
Requesting to lower the p value or even de-manding that the test set performance falls belowthe reported manual error can drastically lower theautomation rate and, in the latter case, is not evenreasonable?how can a machine possibly commit258trainingutterances.
transcriptionautomationrate.
training dateFigure 1: Dependency between amount of trainingdata and transcription automation rateless errors than a human being as it is trained onhuman transcriptions?As a proof of concept, we ran automatic tran-scription against the same set of utterances usedto determine the manual transcription error, andwe found that the average word error rate betweenmanual and automatic annotation was as low as1.1% for all utterances whose confidence score ex-ceeded the context-dependent threshold trained asdescribed above.
In this initial experiment, a totalof 60,608 utterances, i.e., 23.4%, had been auto-mated.2.3 On Automation RateFormally, transcription automation rate is the ra-tio of utterances whose confidence exeeded p0 inEquation 2.2:transcription automation rate = |V (p0)|N(3)where |V | refers to the cardinality of the set V ,i.e., the number of V ?s members.The above example?s transcription automationrate of 23.4% does not yet sound tremendouslyhigh, so we should look at what can be done toincrease the automation rate as much as possible.It is predictable that the two fundamentals formu-lated in Section 2.1 have a large impact on recog-nition performance and, hence, the transcriptionautomation rate:(1) In large-scale experiments, we were able toshow a significant (negative) correlation betweenthe annotation automation rate and task complex-ity.
Since this study does not fit the present paper?sscope, we will refrain from reporting on details atthis point.
(2) As an example which influence the amount oftraining data can have on the transcription automa-tion rate, Figure 1 shows statistics drawn fromtwenty runs of language model training carried outover the course of seven months while collectingmore and more data.3 Automatic AnnotationSemantic annotation of utterances into one of a fi-nal set of classes is a task which may require pro-found understanding of the application and recog-nition context the specific utterances were col-lected in.
Examples include simple contexts suchas yes/no questions which may be easily manage-able also by annotators unfamiliar with the ap-plication, high-resolution open prompt contextswith hundreds of technical and highly application-specific classes, or number collection contexts al-lowing for billions of classes.
All these contextscan benefit from two rules which help to signifi-cantly reduce an annotator?s workload:(A) Never do anything twice.
This simple state-ment means that there should be functionality builtinto the annotation software or the underlyingdatabase that?
lets the annotator process multiple utteranceswith identical transcription in a single step and?
makes sure that whenever a new utterance showsup with a transcription identical to a formerly an-notated one, the new utterance gets assigned thesame class automatically.Figure 2 demonstrates the impact of Rule (A) withtwo typical examples.
The first is a yes/no contextallowing for the additional global commands help,hold, agent, repeat, and i don?t know.
The other isan open prompt context distinguishing 79 classes.When using the token/type distinction, the im-pact of Rule (A) is that annotation effort becomeslinear with the number of types to work on.
Whilethe ratio between types and tokens in a given cor-pus can be very small (i.e., the automation rate isvery high, e.g., 95% in the above yes/no example),this ratio reaches saturation at some point.
In theyes/no example, there is only a gradual differencebetween the automation rates for 10 thousand and1 million utterances.
Hence, at a certain point, theeffort becomes virtually linear with the number oftokens to be processed.
(B) Predict as much as possible.
Most of therecognition contexts for which utterances are tran-scribed and annotated use grammars to implementspeech recognition functionality.
Many of theseannotationautomationrate.
training utterancesFigure 2: Dependency between number of col-lected utterances and annotation automation ratebased on Rule (A) for two different contexts259Table 1: Annotation automation rates for three dif-ferent recognition contexts based on Rule (B).grammar #symptoms ann.
auto.
ratemodem type 43 70.3%blue/black/snow 10 77.0%yes/no 10 88.6%grammars will be rule-based grammars.
Even ifthe grammars are statistical, most often, earlierin time, rule-based grammars had been used inthe same recognition context.
Hence, we can as-sume that we are given rule-based grammars formany recognition contexts of the dialog systemin question.
Per definition, rule-based grammarsshall contain canonical rules expressing the rela-tionship between expected utterances in a givencontext and the semantic classes these utterancesare to be associated with.
Consequently, when-ever for an utterance recorded in the context un-der consideration there is a rule in the grammar,it provides the correct class for this utterance, andit can be excluded from annotation.
These rulescan be strongly extended to allow for complex pre-fix and suffix rules, repetitions, sub-grammars &c.making sure that the majority of utterances willbe covered by the rule-based grammars therebyminimizing the annotation effort.
Table 1 showsthree example grammars of different complex-ity: One that collects the type of the caller?s mo-dem, one for the identification of a TV set?s pic-ture color (blue/black/snow), and a yes/no con-text with global commands.
Annotation automa-tion rates for these grammars that were not specif-ically tuned for maximizing automation but di-rectly taken from the production dialog systemsvaried between 70.3% and 88.6%.To never ever touch a formerly annotated utter-ance type again and to blindly rely on (mayby out-dated or erroneous) rule-based grammars to pro-vide baseline annotations may result in annota-tion mistakes, possibly major ones when frequentutterances are concerned.
So, how do we makesure that high annotation performance standardsare met?To answer this question, the authors have de-veloped a set of techniques called C7 taking careof completeness, consistency, congruence, corre-lation, confusion, coverage, and corpus size of anannotation set (Suendermann et al, 2008).
Thementioned techniques are also useful in the fre-quent event of changes to the number or scope ofannotation classes.
This can happen e.g.
due tofunctional changes to the application, changes toprompts, user behavior, or to contexts preceedingthe current annotation context.
Another frequentreason is the introduction of additional classes toenlarge the scope of the current context2.2In a specific context, callers may be asked whether theywant A, B, or C, but they may respond D. The introduc-tion of a new class D which the application is able to handle4 693 Thousand UtterancesFinally, we want to return to the initial statementof this paper claiming that one person is able toannoscribe 693 thousand utterances within onemonth.
An approximated automation rate of 80%for transcription and 90% for annotation is possi-ble when there is already a massive database ofannoscriptions available to be exploited for au-tomation.
These rates result in about 139 thou-sand transcriptions and 69 thousand annotationsoutstanding.
At a pace of 1000 transcribed or 2000annotated utterances per hour, the required timewould be 139 hours transcription and 35 hours an-notation which averages at 40 hours per week3.5 ConclusionThis paper has demonstrated how automatedannoscription of utterances collected in theproduction scope of spoken dialog systems caneffectively accelerate this conventionally entirelymanual effort.
When allowing for some overtime,we have shown that a single person is able toproduce 693 thousand annoscriptions within onemonth.ReferencesA.
Gorin, G. Riccardi, and J. Wright.
1997.
How MayI Help You?
Speech Communication, 23(1/2).F.
Och.
2006.
Challenges in Machine Translation.
InProc.
of the TC-Star Workshop, Barcelona, Spain.D.
Suendermann, J. Liscombe, K. Evanini,K.
Dayanidhi, and R. Pieraccini.
2008.
C5.In Proc.
of the SLT, Goa, India.D.
Suendermann, J. Liscombe, K. Dayanidhi, andR.
Pieraccini.
2009a.
Localization of SpeechRecognition in Spoken Dialog Systems: How Ma-chine Translation Can Make Our Lives Easier.
InProc.
of the Interspeech, Brighton, UK.D.
Suendermann, J. Liscombe, K. Evanini,K.
Dayanidhi, and R. Pieraccini.
2009b.
FromRule-Based to Statistical Grammars: Continu-ous Improvement of Large-Scale Spoken DialogSystems.
In Proc.
of the ICASSP, Taipei, Taiwan.A.
Waibel and K.-F. Lee.
1990.
Readings in SpeechRecognition.
Morgan Kaufmann, San Francisco,USA.D.
Yuk and J. Flanagan.
1999.
Telephone SpeechRecognition Using Neural Networks and HiddenMarkov Models.
In Proc.
of the ICASSP, Phoenix,USA.requires the re-annotation of all utterances falling into D?sscope.3The original title of this paper claimed that one personcould annoscribe even one million utterances in a month.However, after receiving multiple complaints about the un-lawfulness of a 58-hour workweek, we had to change the titleaccordingly to avoid disputes with the Department of Labor.Furthermore, as discussed earlier, at the starting point of anannoscription project, automation rates are much lower thanlater.260
