Proceedings of the Fourth International Natural Language Generation Conference, pages 20?22,Sydney, July 2006. c?2006 Association for Computational LinguisticsOvergeneration and ranking for spoken dialogue systemsSebastian VargesCenter for the Study of Language and InformationStanford UniversityStanford, CA 94305, USAvarges@stanford.eduAbstractWe describe an implemented generatorfor a spoken dialogue system that fol-lows the ?overgeneration and ranking?
ap-proach.
We find that overgeneration basedon bottom-up chart generation is well-suited to a) model phenomena such asalignment and variation in dialogue, and b)address robustness issues in the face of im-perfect generation input.
We report evalu-ation results of a first user study involving20 subjects.1 IntroductionOvergeneration and ranking approaches have be-come increasingly popular in recent years (Langk-ilde, 2002; Varges, 2002).
However, most work ongeneration for practical dialogue systems makesuse of generation components that work towarda single output, often using simple templates.
Inthe following, we first describe our dialogue sys-tem and then turn to the generator which is basedon the overgeneration and ranking paradigm.
Weoutline the results of a user study, followed by adiscussion section.The dialogue system: Dialogue processingstarts with the output of a speech recognizer(Nuance) which is analyzed by both a statisticaldependency parser and a topic classifier.
Parsetrees and topic labels are matched by the ?di-alogue move scripts?
of the dialogue manager(DM) (Mirkovic and Cavedon, 2005).
Thedialogue system is fully implemented and hasbeen used in restaurant selection and MP3 playertasks (Weng et al, 2004).
There are 41 task-independent, generic dialogue rules, 52 restaurantselection rules and 89 MP3 player rules.
Queryconstraints are built by dialogue move scripts ifthe parse tree matches input patterns specifiedin the scripts.
For example, a request ?I wantto find an inexpensive Japanese restaurant thattakes reservations?
results in constraints such asrestaurant:Cuisine = restaurant:japaneseand restaurant:PriceLevel = 0-10.
If thedatabase query constructed from these constraintsreturns no results, various constraint modificationstrategies such as constraint relaxation or removalcan be employed.
For example, ?Japanese food?can be relaxed to ?Asian food?
since cuisine typesare hierarchically organized.2 Overgeneration for spoken dialogueTable 1 shows some example outputs of the sys-tem.
The wording of the realizations is informedby a wizard-of-oz data collection.
The task ofthe generator is to produce these verbalizationsgiven dialogue strategy, constraints and furtherdiscourse context, i.e.
the input to the generatoris non-linguistic.
We perform mild overgenera-tion of candidate moves, followed by ranking.
Thehighest-ranked candidate is selected for output.2.1 Chart generationWe follow a bottom-up chart generation approach(Kay, 1996) for production systems similar to(Varges, 2005).
The rule-based core of the gen-erator is a set of productions written in a produc-tion system.
Productions map individual databaseconstraints to phrases such as ?open for lunch?,?within 3 miles?, ?a formal dress code?, and re-cursively combine them into NPs.
This includesthe use of coordination to produce ?restaurantswith a 5-star rating and a formal dress code?,for example.
The NPs are integrated into sen-tence templates, several of which can be combined20|result| mod example realization fexps1 0 no I?m sorry but I found no restaurants on Mayfield Road that serve Mediterranean food .
0s2 small: no There are 2 cheap Thai restaurants in Lincoln in my database : Thai Mee Choke and 61> 0, < t1 Noodle House .s3 medium: no I found 9 restaurants with a two star rating and a formal dress code that are open 212>= t1, < t2 for dinner and serve French food .
Here are the first ones :s4 large: no I found 258 restaurants on Page Mill Road, for example Maya Restaurant , 300>= t2 Green Frog and Pho Hoa Restaurant .
Would you like to try searching by cuisine ?s5 large yes I found no restaurants that ...
However, there are NUM restaurants that ... Would you like to ...?
16s6 (any) yes/no I found 18 items .
2Table 1: Some system responses (?|result|?
: size of database result set, ?mod?
: performed modifications).Last column: frequency in user study (180 tasks, 596 constraint inputs to generator)to form an output candidate turn.
For example,a constraint realizing template ?I found no [NP-original] but there are [NUM] [NP-optimized] inmy database?
can be combined with a follow-upsentence template such as ?You could try to lookfor [NP-constraint-suggestion]?.
?NP-original?
re-alizes constraints directly constructed from theuser utterance; ?NP-optimized?
realizes potentiallymodified constraints used to obtain the actualquery result.
To avoid generating separate sets ofNPs independently for these two ?
often largelyoverlapping ?
constraint sets, we assign unique in-dices to the input constraints, overgenerate NPsand check their indices.The generator maintains state across dialogueturns, allowing it to track its previous decisions(see ?variation?
below).
Both input constraints andchart edges are indexed by turn numbers to avoidconfusing edges of different turns.We currently use 102 productions overall in therestaurant and MP3 domains, 38 of them to gener-ate NPs that realize 19 input constraints.2.2 Ranking: alignment & variationAlignment Alignment is a key to successful nat-ural language dialogue (Brockmann et al, 2005).We perform alignment of system utterances withuser utterances by computing an ngram-basedoverlap score.
For example, a user utterance ?Iwant to find a Chinese restaurant?
is presented bythe bag-of-words {?I?, ?want?, ?to?, ?find?, ...} andthe bag-of-bigrams {?I want?, ?want to?, ?to find?,...}.
We compute the overlap with candidate sys-tem utterances represented in the same way andcombine the unigram and bigram match scores.Words are lemmatized and proper nouns of exam-ple items removed from the utterances.Alignment allows us to prefer ?restaurants thatserve Chinese food?
over ?Chinese restaurants?if the user used a wording more similar to thefirst.
The Gricean Maxim of Brevity, applied toNLG in (Dale and Reiter, 1995), suggests a prefer-ence for the second, shorter realization.
However,if the user thought it necessary to use ?serves?,maybe to correct an earlier mislabeling by theclassifier/parse-matching patterns, then the systemshould make it clear that it understood the usercorrectly by using those same words.
On the otherhand, a general preference for brevity is desirablein spoken dialogue systems: users are generallynot willing to listen to lengthy synthesized speech.Variation We use a variation score to ?cycle?over sentence-level paraphrases.
Alternative can-didates for realizing a certain input move aregiven a unique alternation (?alt?)
number in in-creasing order.
For example, for the simple movecontinuation query we may assign the follow-ing alt values: ?Do you want more??
(alt=1) and?Do you want me to continue??
(alt=2).
The sys-tem cycles over these alternatives in turn.
Oncewe reach alt=2, it starts over from alt=1.
The ac-tual alt ?score?
is inversely related to recency andnormalized to [0...1].Score combination The final candidate score isa linear combination of alignment and variationscores:scorefinal = ?1 ?
alignuni,bi +(1 ?
?1) ?
variation (1)alignuni,bi = ?2 ?
alignuni +(1 ?
?2) ?
alignbi (2)where ?1, ?2 ?
{0...1}.
A high value of ?1places more emphasis on alignment, a low valueyields candidates that are more different from pre-viously chosen ones.
In our experience, align-ment should be given a higher weight than vari-ation, and, within alignment, bigrams should be21weighted higher than unigrams, i.e.
?1 > 0.5 and?2 < 0.5.
Deriving weights empirically from cor-pus data is an avenue for future research.3 User studyEach of 20 subjects in a restaurant selection taskwas given 9 scenario descriptions involving 3 con-straints.
We use a back-end database of 2500restaurants containing the 13 attributes/constraintsfor each restaurant.On average, the generator produced 16 outputcandidates for inputs of two constraints, 160 can-didates for typical inputs of 3 constraints and 320candidates for 4 constraints.
For larger constraintsets, we currently reduce the level of overgenera-tion but in the future intend to interleave overgen-eration with ranking similar to (Varges, 2002).Task completion in the experiments was high:the subjects met al target constraints in 170 out of180 tasks, i.e.
completion rate was 94.44%.
Tothe question ?The responses of the system wereappropriate, helpful, and clear.?
(on a scale where1 = ?strongly agree?, 5 = ?strongly disagree?
), thesubjects gave the following ratings: 1: 7, 2: 9, 3:2, 4: 2 and 5: 0, i.e.
the mean user rating is 1.95.4 Discussion & ConclusionsWhere NLG affects the dialogue system: Dis-course entities introduced by NLG add items to thesystem?s salience list as an equal partner to NLU.Robustness: due to imperfect ASR and NLU,we relax completeness requirements when doingovergeneration, and reason about the generationinput by adding defaults for missing constraints,checking ranges of attribute values etc.
Moreover,we use a template generator as a fall-back if NLGfails to at least give some feedback to the user (s6in table 1).What-to-say vs how-to-say-it: the classic sep-aration of NLG into separate modules also holdsin our dialogue system, albeit with some mod-ifications: ?content determination?
is ultimatelyperformed by the user and the constraint opti-mizer.
The presentation dialogue moves do micro-planning, for example by deciding to present re-trieved database items either as examples (s4 intable 1) or as part of a larger answer list of items.The chart generator performs realization.In sum, flexible and expressive NLG is cru-cial for the robustness of the entire speech-baseddialogue system by verbalizing what the systemunderstood and what actions it performed as aconsequence of this understanding.
We find thatovergeneration and ranking techniques allow us tomodel alignment and variation even in situationswhere no corpus data is available by using the dis-course history as a ?corpus?.Acknowledgments This work is supported by theUS government?s NIST Advanced Technology Program.Collaborating partners are CSLI, Robert Bosch Corporation,VW America, and SRI International.
We thank the manypeople involved in this project, in particular Fuliang Wengand Heather Pon-Barry for developing the content optimiza-tion module; Annie Lien, Badri Raghunathan, Brian Lathrop,Fuliang Weng, Heather Pon-Barry, Jeff Russell, and TobiasScheideck for performing the evaluations and compiling theresults; Matthew Purver and Florin Ratiu for work on theCSLI dialogue manager.
The content optimizer, knowledgemanager, and the NLU module have been developed by theBosch Research and Technology Center.ReferencesCarsten Brockmann, Amy Isard, Jon Oberlander, andMichael White.
2005.
Modelling alignment for af-fective dialogue.
In Proc.
of the UM?05 Workshopon Adapting the Interaction Style to Affective Fac-tors.Robert Dale and Ehud Reiter.
1995.
ComputationalInterpretations of the Gricean Maxims in the Gener-ation of Referring Expressions.
Cognitive Science,19:233?263.Martin Kay.
1996.
Chart Generation.
In Proceedingsof ACL-96, pages 200?204.Irene Langkilde.
2002.
An Empirical Verificationof Coverage and Correctness for a General-PurposeSentence Generator.
In Proc.
of INLG-02.Danilo Mirkovic and Lawrence Cavedon.
2005.
Prac-tical Plug-and-Play Dialogue Management.
In Pro-ceedings of the 6th Meeting of the Pacific Associa-tion for Computational Linguistics (PACLING).Sebastian Varges.
2002.
Fluency and Completenessin Instance-based Natural Language Generation.
InProc.
of COLING-02.Sebastian Varges.
2005.
Chart generation using pro-duction systems (short paper).
In Proc.
of 10th Eu-ropean Workshop On Natural Language Generation.Fuliang Weng, L. Cavedon, B. Raghunathan,D.
Mirkovic, H. Cheng, H. Schmidt, H. Bratt,R.
Mishra, S. Peters, L. Zhao, S. Upson, E. Shriberg,and C. Bergmann.
2004.
Developing a conversa-tional dialogue system for cognitively overloadedusers.
In Proceedings of the International Congresson Intelligent Transportation Systems (ICSLP).22
