Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 961?970,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsJoint Case Argument Identification forJapanese Predicate Argument Structure AnalysisHiroki Ouchi Hiroyuki Shindo Kevin Duh Yuji MatsumotoGraduate School of Information and ScienceNara Institute of Science and Technology8916-5, Takayama, Ikoma, Nara, 630-0192, Japan{ ouchi.hiroki.nt6, shindo, kevinduh, matsu }@is.naist.jpAbstractExisting methods for Japanese predicateargument structure (PAS) analysis identifycase arguments of each predicate withoutconsidering interactions between the tar-get PAS and others in a sentence.
How-ever, the argument structures of the pred-icates in a sentence are semantically re-lated to each other.
This paper proposesnew methods for Japanese PAS analysisto jointly identify case arguments of allpredicates in a sentence by (1) modelingmultiple PAS interactions with a bipar-tite graph and (2) approximately search-ing optimal PAS combinations.
Perform-ing experiments on the NAIST Text Cor-pus, we demonstrate that our joint analysismethods substantially outperform a strongbaseline and are comparable to previouswork.1 IntroductionPredicate argument structure (PAS) analysis is ashallow semantic parsing task that identifies ba-sic semantic units of a sentence, such as who doeswhat to whom, which is similar to semantic rolelabeling (SRL)1.In Japanese PAS analysis, one of the most prob-lematic issues is that arguments are often omittedin the surface form, resulting in so-called zero-pronouns.
Consider the sentence of Figure 1.1We use ?PAS analysis?
in this paper following previouswork on Japanese PAS analysis.Figure 1: An example of Japanese PAS.
The En-glish translation is ?Because ?icaught a cold, Iiskipped school.?.
The upper edges are dependencyrelations, and the under edges are case arguments.?NOM?
and ?ACC?
represents the nominative andaccusative arguments, respectively.
??i?
is a zero-pronoun, referring to the antecedent ?watashii?.The case role label ?NOM?
and ?ACC?
respec-tively represents the nominative and accusativeroles, and ?irepresents a zero-pronoun.
Thereare two predicates ?hiita (caught)?
and ?yasunda(skipped)?.
For the predicate ?yasunda (skipped)?,?watashii-wa (Ii)?
is the ?skipper?, and ?gakko-wo(school)?
is the ?entity skipped?.
It is easy to iden-tify these arguments, since syntactic dependencybetween an argument and its predicate is a strongclue.
On the other hand, the nominative argumentof the predicate ?hiita (caught)?
is ?watashii-wa(Ii)?, and this identification is more difficult be-cause of the lack of the direct syntactic depen-dency with ?hiita (caught)?.
The original nomina-tive argument appears as a zero-pronoun, so thatwe have to explore the antecedent, an element re-ferred to by a zero-pronoun, as the argument.
Asthe example sentence shows, we cannot use ef-fective syntactic information for identifying sucharguments.
This type of arguments is known asimplicit arguments, a very problematic language961phenomenon for PAS analysis (Gerber and Chai,2010; Laparra and Rigau, 2013).Previous work on Japanese PAS analysis at-tempted to solve this problem by identifying argu-ments per predicate without considering interac-tions between multiple predicates and arguments(Taira et al, 2008; Imamura et al, 2009).
How-ever, implicit arguments are likely to be sharedby semantically-related predicates.
In the aboveexample (Figure 1), the implicit argument of thepredicate ?hiita (caught)?
is shared by the otherpredicate ?yasunda (skipped)?
as its nominativeargument ?watashii(Ii)?.Based on this intuition, we propose methods tojointly identify optimal case arguments of all pred-icates in a sentence taking their interactions intoaccount.
We represent the interactions as a bipar-tite graph that covers all predicates and candidatearguments in a sentence, and factorize the wholerelation into the second-order relations.
This in-teraction modeling results in a hard combinatorialproblem because it is required to select the optimalPAS combination from all possible PAS combina-tions in a sentence.
To solve this issue, we extendthe randomized hill-climbing algorithm (Zhang etal., 2014) to search all possible PAS in the spaceof bipartite graphs.We perform experiments on the NAIST TextCorpus (Iida et al, 2007), a standard bench-mark for Japanese PAS analysis.
Experimentalresults show that compared with a strong base-line, our methods achieve an improvement of1.0-1.2 points in F-measure for total case argu-ment identification, and especially improve per-formance for implicit argument identification by2.0-2.5 points.
In addition, although we exploit noexternal resources, we get comparable results toprevious work exploiting large-scale external re-sources (Taira et al, 2008; Imamura et al, 2009;Sasano and Kurohashi, 2011).
These results sug-gest that there is potential for more improvementby adding external resources.The main contributions of this work are: (1) Wepresent new methods to jointly identify case ar-guments of all predicates in a sentence.
(2) Wepropose global feature templates that capture inter-actions over multiple PAS.
(3) Performing experi-ments on the NAIST Text Corpus, we demonstrateour methods are superior to a strong baseline andcomparable to the methods of representative pre-vious work.2 Japanese Predicate ArgumentStructure Analysis2.1 Task OverviewIn Japanese PAS analysis, we identify argumentstaking part in the three major case roles, nomina-tive (NOM), accusative (ACC) and dative (DAT)cases, for each predicate.
Case arguments can bedivided into three categories according to the posi-tions relative to their predicates (Hayashibe et al,2011):Dep: The arguments that have direct syntactic de-pendency with the predicate.Zero: The implicit arguments whose antecedentsappear in the same sentence and have no di-rect syntactic dependency with the predicate.Inter-Zero: The implicit arguments whose an-tecedents do not appear in the same sentence.For example, in Figure 1, the accusative argu-ment ?gakko-wo (school)?
of the predicate ?ya-sunda (skipped)?
is regarded as Dep, and thenominative argument ?watashii-wa (I)?
(the an-tecedent of zero-pronoun ??i?)
of the predicate?hiita (caught)?
is Zero.In this paper, we focus on the analysis for intra-sentential arguments (Dep and Zero).
In order toidentify inter-sentential arguments (Inter-Zero), itis required to search a much broader space, suchas the whole document, resulting in a much harderanalysis than intra-sentential arguments.2There-fore, we believe that quite different approaches arenecessary to realize an inter-sentential PAS analy-sis with high accuracy, and leave it for future work.2.2 Related WorkFor Japanese PAS analysis research, the NAISTText Corpus has been used as a standard bench-mark (Iida et al, 2007).
One of the representa-tive researches using the NAIST Text Corpus isImamura et al (2009).
They built three distinctmodels corresponding to the three case roles byextracting features defined on each pair of a predi-cate and a candidate argument.
Using each model,they select the best candidate argument for eachcase per predicate.
Their models are based onmaximum entropy model and can easily incorpo-rate various features, resulting in high accuracy.2Around 10-20% in F measure has been achieved in pre-vious work (Taira et al, 2008; Imamura et al, 2009; Sasanoand Kurohashi, 2011).962Figure 2: Intuitive image of a predicate-argumentgraph.
This graph is factorized into the local andglobal features.
The different line color/style indi-cate different cases.While in Imamura et al (2009) one case ar-gument is identified at a time per predicate, themethod proposed by Sasano and Kurohashi (2011)simultaneously determines all the three case argu-ments per predicate by exploiting large-scale caseframes obtained from large raw texts.
They fo-cus on identification of implicit arguments (Zeroand Inter-Zero), and achieves comparable resultsto Imamura et al (2009).In these approaches, case arguments were iden-tified per predicate without considering interac-tions between multiple predicates and candidatearguments in a sentence.
In the semantic role la-beling (SRL) task, Yang and Zong (2014) pointedout that information of different predicates andtheir candidate arguments could help each otherfor identifying arguments taking part in semanticroles.
They exploited a reranking method to cap-ture the interactions between multiple predicatesand candidate arguments, and jointly determine ar-gument structures of all predicates in a sentence(Yang and Zong, 2014).
In this paper, we proposenew joint analysis methods for identifying case ar-guments of all predicates in a sentence capturinginteractions between multiple predicates and can-didate arguments.3 Graph-Based Joint Models3.1 A Predicate-Argument GraphWe define predicate argument relations by exploit-ing a bipartite graph, illustrated in Figure 2.
Thenodes of the graph consist of two disjoint sets: theleft one is a set of candidate arguments and theright one is a set of predicates.
In this paper, wecall it a predicate-argument (PA) graph.Each predicate node has three distinct edgescorresponding to nominative (NOM), accusative(ACC), and dative (DAT) cases.
Each edge witha case role label joins a candidate argument nodewith a predicate node, which represents a case ar-gument of a predicate.
For instance, in Figure 2a1is the nominative argument of p1, and a3is theaccusative argument of p2.Formally, a PA graph is a bipartite graph?A,P,E?, where A is the node set consisting ofcandidate arguments, P the node set consisting ofpredicates, and E the set of edges subject to thatthere is exactly one edge e with a case role label coutgoing from each of the predicate nodes p to acandidate argument node a.
A PA graph is definedas follows:A = {a1, ..., an, an+1= NULL}P = {p1, ..., pm}E = {?a, p, c?
| deg(p, c) = 1,?a ?
A, ?p ?
P, ?c ?
C }where deg(p, c) is the number of edges with a caserole c outgoing from p, and C is the case role labelset.
We add a dummy node an+1, which is definedfor the cases where the predicate requires no caseargument or the required case argument does notappear in the sentence.
An edge e ?
E is repre-sented by a tuple ?a, p, c?, indicating the edge witha case role c joining a candidate argument node aand a predicate node p. An admissible PA graphsatisfies the constraint deg(p, c) = 1, representingthat each predicate node p has only one edge witha case role c.To identify the whole PAS for a sentence x, wepredict the PA graph with an edge set correspond-ing to the correct PAS from the admissible PAgraph set G(x) based on a score associated witha PA graph y as follows:y?
= argmaxy?G(x)Score(x, y)A scoring function Score(x, y) receives a sen-tence x and a candidate graph y as its input, andreturns a scalar value.In this paper, we propose two scoring functionsas analysis models based on different assumptions:(1) Per-Case Joint Model assumes the interac-tion between multiple predicates (predicate inter-action) and the independence between case roles,and (2) All-Cases Joint Model assumes the in-teraction between case roles (case interaction) aswell as the predicate interaction.9633.2 Per-Case Joint ModelPer-Case Joint Model assumes that different caseroles are independent from each other.
However,for each case, interactions between multiple pred-icates are considered jointly.We define the score of a PA graph y to be thesum of the scores for each case role c of the set ofthe case roles C:Scoreper(x, y) =?c?CScorec(x, y) (1)The scores for each case role are defined as the dotproducts between a weight vector ?c and a featurevector ?c(x,E(y, c)):Scorec(x, y) = ?c ?
?c(x,E(y, c)) (2)where E(y, c) is the edge set associated with acase role c in the candidate graph y, and the featurevector is defined on the edge set.The edge set E(y, c) in the equation (2) is uti-lized for the two types of features, the local fea-tures and global features, inspired by (Huang,2008), defined as follows:?c ?
?c(x,E(y, c)) =?e?E(y,c)?c ?l(x, e) + ?c ?g(x,E(y, c)) (3)where ?l(x, e) denotes the local feature vector,and ?g(x,E(y, c)) the global feature vector.
Thelocal feature vector ?l(x, e) is defined on eachedge e in the edge set E(y, c) and a sentence x,which captures a predicate-argument pair.
Con-sider the example of Figure 2.
For Per-Case JointModel, we use edges, ea1p1, ea1p2, and ea2p3, aslocal features to compute the score of the edge setwith the nominative case.In addition, the global feature vector?g(x,E(y, c)) is defined on the edge setE(y, c), and enables the model to utilize lin-guistically richer information over multiplepredicate-argument pairs.
In this paper, weexploit second-order relations, similar to thesecond-order edge factorization of dependencytrees (McDonald and Pereira, 2006).
We make aset of edge pairs Epairby combining two edgesei, ejin the edge set E(y, c), as follows:Epair= { {ei, ej} | ?ei, ej?
E(y, c), ei?= ej}For instance, in the PA graph in Figure 2, to com-pute the score of the nominative arguments, wemake three edge pairs:{{ea1p1, ea1p2}, {ea1p1, ea2p3}, {ea1p2, ea2p3}}Then, features are extracted from these edge pairsand utilized for the score computation.
For theaccusative and dative cases, their scores are com-puted in the same manner.
Then, we obtain theresulting score of the PA graph by summing upthe scores of the local and global features.
If wedo not consider the global features, the model re-duces to a per-case local model similar to previouswork (Imamura et al, 2009).3.3 All-Cases Joint ModelWhile Per-Case Joint Model assumes the predi-cate interaction with the independence betweencase roles, All-Cases Joint Model assumes thecase interaction together with the predicate inter-action.
Our graph-based formulation is very flex-ible and easily enables the extension of Per-CaseJoint Model to All-Cases Joint Model.
Therefore,we extend Per-Case Joint Model to All-Cases JointModel to capture the interactions between predi-cates and all case arguments in a sentence.We define the score of a PA graph y based onthe local and global features as follows:Scoreall(x, y) =?e?E(y)?
?
?l(x, e) + ?
?
?g(x,E(y)) (4)where E(y) is the edge set associated with all thecase roles on the candidate graph y, ?l(x, e) is thelocal feature vector defined on each edge e in theedge set E(y), and ?g(x,E(y)) is the global fea-ture vector defined on the edge set E(y).Consider the PA graph in Figure 2.
The localfeatures are extracted from each edge:Nominative : ea1p1, ea1p2, ea2p3Accusative : ea2p1, ea3p2, ea3p3Dative : ea3p1, ea4p2, ea4p3For the global features, we make a set of edgepairs Epairby combining two edges ei, ejin theedge set E(y), like Per-Case Joint Model.
How-ever, in the All-Cases Joint Model, the global fea-tures may involve different cases (i.e.
mixingedges with different case roles).
For the PA graphin Figure 2, we make the edge pairs {ea1p1, ea2p1},{ea3p1, ea1p2}, {ea3p2, ea4p3}, and so on.
Fromthese edge pairs, we extract information as globalfeatures to compute a graph score.964Structure Name DescriptionDiff-Arg PAIR ?
pi.rf ?
pj.rf ?
pi.vo ?
pj.vo ?,?
ai.ax ?
ai.rp ?
pi.ax ?
pi.vo ?, ?
aj.ax ?
aj.rp ?
pj.ax ?
pj.vo ?TRIANGLE ?
ai.ax ?
ai.ax ?
ai.rp ?
aj.rp ?
pi.ax ?
pi.vo ?,?
ai.ax ?
aj.ax ?
ai.rp ?
aj.rp ?
pj.ax ?
pj.vo ?,QUAD ?
ai.ax ?
aj.ax ?
ai.rp ?
aj.rp ?
pi.vo ?
pj.vo ??
ai.ax ?
aj.ax ?
pi.ax ?
pj.ax ?
ai.rp ?
aj.rp ?
pi.vo ?
pj.vo ??
ai.ax ?
aj.ax ?
pi.rf ?
pj.rf ?
ai.rp ?
ai.rp ?
pi.vo ?
pi.vo ?Co-Arg BI-PREDS ?
ai.rp ?
pi.rf ?
pj.rf ?,?
ai.ax ?
ai.rp ?
pi.rf ?
pj.rf ?DEP-REL ?
ai.ax ?
ai.rp ?
pi.ax ?
pj.ax ?
pi.vo ?
pj.vo ?
(x, y).dep ?if x depends on y for x,y in (pi,pj), (ai,pi), (ai,pj), (pi,ai), (pj,ai)Table 1: Global feature templates.
pi, pjis a predicate, aiis the argument connected with pi, andajis the argument connected with pj.
Feature conjunction is indicated by ?
; ax=auxiliary, rp=relativeposition, vo=voice, rf=regular form, dep=dependency.
All the features are conjoined with the relativeposition and the case role labels of the two predicates.4 Global FeaturesFeatures are extracted based on feature tem-plates, which are functions that draw informationfrom the given entity.
For instance, one featuretemplate ?100= a.ax ?
p.vo is a conjunction oftwo atomic features a.ax and p.vo, representing anauxiliary word attached to a candidate argument(a.ax) and the voice of a predicate (p.vo).
Wedesign several feature templates for characterizingeach specific PA graph.
Consider the PA graphconstructed from the sentence in Figure 1, and acandidate argument ?kaze-wo (a cold)?
and a pred-icate ?hiita (caught)?
are connected with an edge.To characterize the graph, we draw some linguis-tic information associated with the edge.
Since theauxiliary word attached to the candidate argumentis ?wo?
and the voice of the predicate is ?active?,the above feature template ?100will generate afeature instance as follows.
(a.ax = wo) ?
(p.vo = active)Such features are utilized for the local and globalfeatures in the joint models.We propose the global feature templates thatcapture multiple PAS interactions based on theDiff-Arg and Co-Arg structures, depicted in theright part of Figure 1.
The Diff-Arg structure rep-resents that the two predicates have different can-didate arguments, and the Co-Arg structure repre-sents that the two predicates share the same can-didate argument.
Based on these structures, wedefine the global feature templates that receive apair of edges in a PA graph as input and return afeature vector, shown in Table 1.4.1 Diff-Arg FeaturesThe feature templates based on the Diff-Arg struc-ture are three types: PAIR (a pair of predicate-argument relation), TRIANGLE (a predicate andits two arguments relation), and QUAD (twopredicate-argument relations).PAIR These feature templates denote where thetarget argument is located relative to another argu-ment and the two predicates in the Diff-Arg struc-ture.
We combine the relative position information(rp) with the auxiliary words (ax) and the voice ofthe two predicates (vo).TRIANGLE This type of feature templates cap-tures the interactions between three elements: twocandidate arguments and a predicate.
Like thePAIR feature templates, we encode the relative po-sition information of two candidate arguments anda predicate with the auxiliary words and voice.QUAD When we judge if a candidate argu-ment takes part in a case role of a predicate, itwould be beneficial to grasp information of an-other predicate-argument pair.
The QUAD fea-ture templates capture the mutual relation betweenfour elements: two candidate arguments and pred-icates.
We encode the relative position informa-tion, the auxiliary words, and the voice.4.2 Co-Arg FeaturesTo identify predicates that take implicit (Zero) ar-guments, we set two feature types, BI-PREDS andDEP-REL, based on the Co-Arg structure.BI-PREDS For identifying an implicit argu-965Input: the set of cases to be analyzed C,parameter ?c, sentence xOutput: a locally optimal PA graph y?1: Sample a PA graph y(0)from G(x)2: t?
03: for each case c ?
C do4: repeat5: Yc?
NeighborG(y(t), c) ?
y(t)6: y(t+1)?
argmaxy?Yc?c ?
?c(x,E(y, c))7: t?
t + 18: until y(t)= y(t+1)9: end for10: return y?
?
y(t)Figure 3: Hill-Climbing for Per-Case Joint ModelInput: the set of cases to be analyzed C,parameter ?, sentence xOutput: a locally optimal PA graph y?1: Sample a PA graph y(0)from G(x)2: t?
03: repeat4: Y ?
NeighborG(y(t)) ?
y(t)5: y(t+1)?
argmaxy?Y?
?
?
(x,E(y))6: t?
t + 17: until y(t)= y(t+1)8: return y?
?
y(t)Figure 4: Hill-Climbing for All-Cases Joint Modelment of a predicate, information of anothersemantically-related predicate in the sentencecould be effective.
We utilize bi-grams of the reg-ular forms (rf) of the two predicates in the Co-Argstructure to capture the predicates that are likely toshare the same argument in the sentence.DEP-REL We set five distinct feature templatesto capture dependency relations (dep) between theshared argument and the two predicates.
If twoelements have a direct dependency relation, weencode its dependency relation with the auxiliarywords and the voice.5 Inference and Training5.1 Inference for the Joint ModelsGlobal features make the inference of finding themaximum scoring PA graph more difficult.
Forsearching the graph with the highest score, we pro-pose two greedy search algorithms by extendingthe randomized hill-climbing algorithm proposedin (Zhang et al, 2014), which has been shown toachieve the state-of-the-art performance in depen-dency parsing.Figure 3 describes the pseudo code of our pro-posed algorithm for Per-Case Joint Model.
Firstly,we set an initial PA graph y(0)sampled uniformlyfrom the set of admissible PA graphs G(x) (line 1in Figure 3).
Then, the union Ycis constructedfrom the set of neighboring graphs with a caseNeighborG(y(t), c), which is a set of admissiblegraphs obtained by changing one edge with thecase c in y(t), and the current graph y(t)(line 5).The current graph y(t)is updated to a higher scor-ing graph y(t+1)selected from the union Yc(line6).
The algorithm continues until no more scoreimprovement is possible by changing an edge withthe case c in y(t)(line 8).
This repetition is exe-cuted for other case roles in the same manner.
Asa result, we can get a locally optimal graph y?.Figure 4 describes the pseudo code of the algo-rithm for All-Cases Joint Model.
The large part ofthe algorithm is the same as that for Per-Case JointModel.
The difference is that the union Y consistsof the current graph y(t)and the neighboring graphset obtained by changing one edge in y(t)regard-less of case roles (line 4 in Figure 4), and that theiteration process for each case role (line 3 in Fig-ure 3) is removed.
The algorithm also continuesuntil no more score improvement is possible bychanging an edge in y(t), resulting in a locally op-timal graph y?.Following Zhang et al (2014), for a given sen-tence x, we repeatedly run these algorithms withK consecutive restarts.
Each run starts with initialgraphs randomly sampled from the set of admis-sible PA graphs G(x), so that we obtain K localoptimal graphs by K restarts.
Then the highestscoring one of K graphs is selected for the sen-tence x as the result.
Each run of the algorithms isindependent from each other, so that multiple runsare easily executable in parallel.5.2 TrainingGiven a training data set D = {(x?, y?
)}Ni, theweight vectors ?
(?c) in the scoring functions ofthe joint models are estimated by using machinelearning techniques.
We adopt averaged percep-tron (Collins, 2002) with a max-margin technique:966?i ?
{1, ..., N}, y ?
G(xi),Score(x?i, y?i) ?
Score(x?i, y) + ?y?i?
y?1?
?iwhere ?i?
0 is the slack variable and ?y?i?
y?1isthe Hamming distance between the gold PA graphy?iand a candidate PA graph y of the admissiblePA graphs G(xi).
Following Zhang et al (2014),we select the highest scoring graph y?
as follows:TRAIN : y?
= argmaxy?G(x?i){Score(x?i, y)+?y?i?y?1}TEST : y?
= argmaxy?G(x){Score(x, y)}Using the weight vector tuned by the training, weperform analysis on a sentence x in the test set.6 Experiment6.1 Experimental SettingsData Set We evaluate our proposed methods onthe NAIST Text Corpus 1.5, which consists of40,000 sentences of Japanese newspaper text (Iidaet al, 2007).
While previous work has adoptedthe version 1.4 beta, we adopt the latest version.The major difference between version 1.4 beta and1.5 is revision of dative case (corresponding toJapanese case particle ?ni?).
In 1.4 beta, most ofadjunct usages of ?ni?
are mixed up with the argu-ment usages of ?ni?, making the identification ofdative cases seemingly easy.
Therefore, our resultsare not directly comparable with previous work.We adopt standard train/dev/test split (Taira etal., 2008) as follows:Train Articles: Jan 1-11, Editorials: Jan-AugDev Articles: Jan 12-13, Editorials: SeptTest Articles: Jan 14-17, Editorials: Oct-DecWe exclude inter-sentential arguments (Inter-Zero) in our experiments.
Our features make useof the annotated POS tags, phrase boundaries, anddependency relations annotated in the NAIST TextCorpus.
We do not use any external resources.Baseline We adopt the pointwise method (usingonly local features) proposed by Imamura et al(2009) as the baseline.
They built three distinctmodels corresponding to the three case roles.
Byusing each model, they estimate the likelihood thateach candidate argument plays a case role of thetarget predicate as a score, and independently se-lect the highest scoring one per predicate.feature Dep Zero TotalPC Joint local 84.59 42.55 77.89+ global 85.51 44.54 78.85AC Joint local 84.17 41.33 77.43+ global 85.92 44.45 79.17Table 2: Global vs Local features on the develop-ment sets in F-measures.
?PC Joint?
denotes thePer-Case Joint Model, and ?AC Joint?
denotes theAll-Cases Joint Model.Features The baseline utilizes the Baseline Fea-tures used in Imamura et al (2009) and Grammat-ical features used in Hayashibe et al (2009), asthe ?Local Features?.
In addition, the joint modelsutilize the ?Global Features?
in Table 1.Implementation Details For our joint modelswith hill-climbing, we report the average per-formance across ten independent runs with 10restarts, which almost reaches convergence3.
Wetrain the baseline and our joint models for 20 iter-ations with averaged perceptron.6.2 ResultsLocal Features vs Global FeaturesTable 2 shows the effectiveness of the global fea-tures on the development sets.
We incrementallyadd the global features to the both models that uti-lize only the local features.
The results show thatthe global features improve the performance byabout 1.0 point in F-measures in total.
For andare particularly beneficial to the implicit (Zero)argument identification (an improvement of 1.99points in Per-Case Joint Model and 3.12 points inAll-Cases Joint Model).Pointwise Methods vs Joint MethodsTable 3 presents the F-measures of the baselineand our joint methods on the test set of the NAISTText Corpus.
We used the bootstrap resamplingmethod as the significance test.
In most of the met-rics, our proposed joint methods outperform thebaseline pointwise method.
Note that since Per-Case Joint Model yields better results comparedwith the baseline, capturing the predicate inter-action is beneficial to Japanese PAS analysis.
Inaddition, the joint methods achieve a considerableimprovement of 2.0-2.5 points in F-measure for3Performance did not change when increasing the numberof restarts967Case Type # of Args.
Baseline PC Joint AC JointNOM Dep 14055 86.50 87.54 ?
88.13 ?
?Zero 4935 45.56 47.62 48.11Total 18990 77.31 78.39 ?
79.03 ?
?ACC Dep 9473 92.84 ?
93.09 ?
?
92.74Zero 833 21.38 22.73 24.43Total 10306 88.86 ?
89.00 ?
?
88.47DAT Dep 2518 30.97 34.29 ?
38.39 ?
?Zero 239 0.83 0.83 4.80Total 2757 29.02 32.20 ?
36.35 ?
?ALL Dep 26046 85.06 85.79 ?
86.07 ?
?Zero 6007 41.65 43.60 44.09Total 32053 78.15 78.91 ?
79.23 ?
?Table 3: F-measures of the three methods in the test sets.
The bold values denote the highest F-measuresamong all the three methods.
Statistical significance with p < 0.05 is marked with ?
compared withBaseline, ?
compared with PC Joint, and ?
compared with AC Joint.Dep ZeroNOM ACC DAT NOM ACC DATTA08 75.53 88.20 89.51 30.15 11.41 3.66IM09 87.0 93.9 80.8 50.0 30.8 0.0S&K11 - - - 39.5 17.5 8.9PC Joint 87.54 93.09 34.19 47.62 22.73 0.83AC Joint 88.13 92.74 38.39 48.11 24.44 4.80Table 4: Comparison with previous work using the NAIST Text Corpus in F-measure.
TA08 is Taira etal.
(2008), IM09 is Imamura et al (2009), and S&K11 is Sasano & Kurohashi (2011).
Their results arenot directly comparable to ours since they use external resources and the NAIST Text Corpus 1.4 beta.the implicit arguments (Zero), one of the problem-atic issues in Japanese PAS analysis.Comparing the joint methods, each of our twojoint methods is effective for a different case role.Per-Case Joint Model is better at the ACC case,and All-Cases Joint Model is better at the NOMandDAT cases.
One of the possible explanations isthat the distribution of ACC cases is different fromNOM cases.
While the ratio ofDep and Zero argu-ments for ACC cases is 90:10, the ratio for NOMcases is 75:25.
This might have some negativeeffects on the ACC case identification with All-Cases Joint Model.
However, in total, All-CasesJoint Model achieves significantly better results.This suggests that capturing case interactions im-proves performance of Japanese PAS analysis.Existing Methods vs Joint MethodsTo compare our proposed methods with previouswork, we pick the three pieces of representativeprevious work exploiting the NAIST Text Cor-pus: Taira et al (2008) (TA08), Imamura et al(2009) (IM09), and Sasano and Kurohashi (2011)(S&K11).
Sasano and Kurohashi (2011) focus onthe analysis for the Zero and Inter-Zero arguments,and do not report the results on theDep arguments.With respect to the Dep arguments, the All-CasesJoint Model achieves the best result for the NOMcases, Imamura et al (2009) the best for the ACCcases, and Taira et al (2008) the best for the DATcases.
In terms of the Zero arguments, Imamuraet al (2009) is the best for the NOM and ACCcases, and Sasano and Kurohashi (2011) the bestfor the DAT cases.
Our joint methods achieve highperformance comparable to Imamura et al (2009).However, because they used additional exter-nal resources and a different version of the NAISTText Corpus, the results of previous work are notdirectly comparable to ours.
Our research direc-tion and contributions are orthogonal to theirs, andadding their external resources could potentiallyleads to much better results.9687 ConclusionWe have presented joint methods for Japanese PASanalysis, which model interactions between mul-tiple predicates and arguments using a bipartitegraph and greedily search the optimal PAS combi-nation in a sentence.
Experimental results showsthat capturing the predicate interaction and caseinteraction is effective for Japanese PAS analy-sis.
In particular, implicit (Zero) argument identi-fication, one of the problematic issues in JapanesePAS analysis, is improved by taking such interac-tions into account.
Since this framework is appli-cable to the argument classification in SRL, apply-ing our methods to that task is an interesting lineof the future research.
In addition, the final resultsof our joint methods are comparable to represen-tative existing methods despite using no externalresources.
For future work, we plan to incorporateexternal resources for our joint methods.AcknowledgmentsWe are grateful to the anonymous reviewers.
Thiswork is partially supported by a JSPS KAKENHIGrant Number 26730121 and 15K16053.ReferencesMichael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedingsof the 2002 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 1?8, Philadelphia, July.
Association for ComputationalLinguistics.Matthew Gerber and Joyce Chai.
2010.
Beyond nom-bank: A study of implicit arguments for nominalpredicates.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, pages 1583?1592, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.Yuta Hayashibe, Mamoru Komachi, and Yuji Mat-sumoto.
2011.
Japanese predicate argument struc-ture analysis exploiting argument position and type.In Proceedings of 5th International Joint Conferenceon Natural Language Processing, pages 201?209,Chiang Mai, Thailand, November.
Asian Federationof Natural Language Processing.Liang Huang.
2008.
Forest reranking: Discrimina-tive parsing with non-local features.
In Proceedingsof 46th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 586?594, Columbus, Ohio, June.
Asso-ciation for Computational Linguistics.Ryu Iida, Mamoru Komachi, Kentaro Inui, and YujiMatsumoto.
2007.
Annotating a japanese text cor-pus with predicate-argument and coreference rela-tions.
In Proceedings of the Linguistic AnnotationWorkshop, pages 132?139, Prague, Czech Republic,June.
Association for Computational Linguistics.Kenji Imamura, Kuniko Saito, and Tomoko Izumi.2009.
Discriminative approach to predicate-argument structure analysis with zero-anaphora res-olution.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the Association for Com-putational Linguistics and 4th International JointConference on Natural Language Processing, pages85?88, Suntec, Singapore, August.
Association forComputational Linguistics.Egoitz Laparra and German Rigau.
2013.
Impar: Adeterministic algorithm for implicit semantic role la-belling.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguis-tics, pages 1180?1189, Sofia, Bulgaria, August.
As-sociation for Computational Linguistics.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proceedings of the 11th conference onEuropean Chapter of the Association for Compu-tational Linguistics (EACL), pages 81?88, Trento,Italy, April.
Association for Computational Linguis-tics.969Ryohei Sasano and Sadao Kurohashi.
2011.
A dis-criminative approach to japanese zero anaphora res-olution with large-scale lexicalized case frames.
InProceedings of 5th International Joint Conferenceon Natural Language Processing, pages 758?766,Chiang Mai, Thailand, November.
Asian Federationof Natural Language Processing.Hirotoshi Taira, Sanae Fujita, and Masaaki Nagata.2008.
A japanese predicate argument structure anal-ysis using decision lists.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 523?532, Hon-olulu, Hawaii, October.
Association for Computa-tional Linguistics.Haitong Yang and Chengqing Zong.
2014.
Multi-predicate semantic role labeling.
In Proceedings ofthe 2014 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 363?373, Doha, Qatar, October.
Association for Compu-tational Linguistics.Yuan Zhang, Tao Lei, Regina Barzilay, and TommiJaakkola.
2014.
Greed is good if randomized: Newinference for dependency parsing.
In Proceedings ofthe 2014 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 1013?1024, Doha, Qatar, October.
Association for Com-putational Linguistics.970
