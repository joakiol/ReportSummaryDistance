Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 131?133,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsA Demonstration of Incremental Speech Understanding and ConfidenceEstimation in a Virtual Human Dialogue SystemDavid DeVault and David TraumInstitute for Creative TechnologiesUniversity of Southern California12015 Waterfront Drive, Playa Vista, CA 90094{devault,traum}@ict.usc.edu1 OverviewThis demonstration highlights some emerging ca-pabilities for incremental speech understanding andprocessing in virtual human dialogue systems.
Thiswork is part of an ongoing effort that aims to en-able realistic spoken dialogue with virtual humans inmulti-party negotiation scenarios (Plu?ss et al, 2011;Traum et al, 2008).
In these negotiation scenarios,ideally the virtual humans should demonstrate fluidturn-taking, complex reasoning, and appropriate re-sponses based on factors like trust and emotions.
Animportant component in achieving this naturalisticbehavior is for the virtual humans to begin to un-derstand and in some cases respond in real time tousers?
speech, as the users are speaking (DeVaultet al, 2011b).
These responses could include rel-atively straightforward turn management behaviors,like having a virtual human recognize when it is be-ing addressed and turn to look at the user.
Theycould also include more complex responses such asemotional reactions to what users are saying.Our demonstration is set in an implemented ne-gotiation domain (Plu?ss et al, 2011) in which twovirtual humans, Utah and Harmony (pictured in Fig-ure 1), talk with two human negotiation trainees,who play the roles of Ranger and Deputy.
The di-alogue takes place inside a saloon in an Americantown in the Old West.
In this scenario, the goal of thetwo human role players is to convince Utah and Har-mony that Utah, who is currently the local bartender,should take on the job of town sheriff.
We presenteda substantially similar demonstration of this scenarioin (DeVault and Traum, 2012).Figure 1: SASO negotiation in the saloon: Utah (left)looking at Harmony (right).To support more natural behavior in such negotia-tion scenarios, we have developed an approach to in-cremental speech understanding.
The understandingmodels are trained using a corpus of in-domain spo-ken utterances, including both paraphrases selectedand spoken by system developers, as well as spo-ken utterances from user testing sessions (DeVaultet al, 2011b).
Every utterance in the corpus is an-notated with an utterance meaning, which is repre-sented using a frame.
Each frame is an attribute-value matrix (AVM), where the attributes and val-ues represent semantic information that is linked toa domain-specific ontology and task model (Traum,2003; Hartholt et al, 2008; Plu?ss et al, 2011).
TheAVMs are linearized, using a path-value notation, asseen at the lower left in Figure 2.
Our frameworkuses this corpus to train two data-driven models, onefor incremental natural language understanding, anda second for incremental confidence modeling.
Webriefly summarize these two models here; for addi-tional details and motivation for this framework, anddiscussion of alternative approaches, see (DeVault etal., 2011b; DeVault et al, 2011a).The first step is to train a predictive incrementalunderstanding model.
This model is based on maxi-131mum entropy classification, and treats entire individ-ual frames as output classes, with input features ex-tracted from partial ASR results, calculated in incre-ments of 200 milliseconds (DeVault et al, 2011b).Each partial ASR result serves as an incremental in-put to NLU, which is specially trained for partialinput as discussed in (Sagae et al, 2009).
NLU ispredictive in the sense that, for each partial ASR re-sult, the NLU module tries to output the completeframe that a human annotator would associate withthe user?s complete utterance, even if that utterancehas not yet been fully processed by the ASR.The second step in our framework is to train a setof incremental confidence models (DeVault et al,2011a), which allow the agents to assess in real time,while a user is speaking, how well the understand-ing process is proceeding.
The incremental confi-dence models build on the notion of NLU F-score,which we use to quantify the quality of a predictedNLU frame in relation to the hand-annotated correctframe.
The NLU F-score is the harmonic mean ofthe precision and recall of the attribute-value pairs(or frame elements) that compose the predicted andcorrect frames for each partial ASR result.Each of our incremental confidence modelsmakes a binary prediction for each partial NLU re-sult as an utterance proceeds.
At each time t dur-ing an utterance, we consider the current NLU F-Score Ft as well as the final NLU F-Score Ffinalthat will be achieved at the conclusion of the utter-ance.
In (DeVault et al, 2009) and (DeVault et al,2011a), we explored the use of data-driven decisiontree classifiers to make predictions about these val-ues, for example whether Ft ?
12 (current level ofunderstanding is ?high?
), Ft ?
Ffinal (current levelof understanding will not improve), or Ffinal ?
12(final level of understanding will be ?high?).
Inthis demonstration, we focus on the first and thirdof these incremental confidence metrics, which wesummarize as ?Now Understanding?
and ?Will Un-derstand?, respectively.The incremental ASR, NLU, and confidence out-puts are passed to the dialogue managers for each ofthe agents, Harmony and Utah.
These agents thenrelate these inputs to their own models of dialoguecontext, plans, and emotions, to calculate pragmaticinterpretations, including speech acts, reference res-olution, participant status, and how they feel aboutwhat is being discussed.
A subset of this informa-tion is passed to the non-verbal behavior generationmodule to produce incremental non-verbal listeningbehaviors (Wang et al, 2011).2 Demo scriptThe demonstration begins with the demo operatorproviding a brief overview of the system design, ne-gotiation scenario, and incremental processing capa-bilities.
The virtual humans Utah and Harmony (seeFigure 1) are running and ready to begin a dialoguewith the user, who will play the role of the Ranger.The demonstration includes a real-time visualizationof incremental speech processing results, which willallow attendees to track the virtual humans?
under-standing as an utterance progresses.
An example ofthis visualization is shown in Figure 2.As the user speaks to Utah or Harmony, attendeescan observe the real time visualization of incremen-tal speech processing.
Further, the visualization in-terface enables the demo operator to ?rewind?
an ut-terance and step through the incremental processingresults that arrived each 200 milliseconds.For example, Figure 2 shows the incrementalspeech processing state at a moment 4.8 seconds intoa user?s 7.4 second long utterance, i?ve come heretoday to talk to you about whether you?d like to be-come the sheriff of this town.
At this point in time,the visualization shows (at top left) that the virtualhumans are confident that they are Now Understand-ing and also Will Understand this utterance.
Next,the graph (in white) shows the history of the agents?expected NLU F-Score for this utterance (rangingfrom 0 to 1).
Beneath the graph, the partial ASR re-sult (HAVE COME HERE TODAY TO TALK TOYOU ABOUT...) is displayed (in white), alongwith the currently predicted NLU frame (in blue).For ease of comprehension, an English gloss (utahdo you want to be the sheriff?)
for the NLU frame isalso shown (in blue) above the frame.To the right, in pink, we show some of Utah andHarmony?s agent state that is based on the current in-cremental NLU results.
The display shows that bothof the virtual humans believe that Utah is being ad-dressed by this utterance, that utah has a positive at-titude toward the content of the utterance while har-mony does not, and that both have comprehension132Figure 2: Visualization of Incremental Speech Processing.and participation goals.
Further, Harmony believesshe is a side participant at this moment.AcknowledgmentsWe thank the entire ICT Virtual Humans team.
Theproject or effort described here has been sponsoredby the U.S. Army Research, Development, and En-gineering Command (RDECOM).
Statements andopinions expressed do not necessarily reflect the po-sition or the policy of the United States Government,and no official endorsement should be inferred.ReferencesDavid DeVault and David R. Traum.
2012.
Incremen-tal speech understanding in a multi-party virtual hu-man dialogue system.
In Demonstration Proceedingsof NAACL-HLT.David DeVault, Kenji Sagae, and David Traum.
2009.Can I finish?
Learning when to respond to incrementalinterpretation results in interactive dialogue.
In Pro-ceedings of SIGDIAL.David DeVault, Kenji Sagae, and David Traum.
2011a.Detecting the status of a predictive incremental speechunderstanding model for real-time decision-making ina spoken dialogue system.
In Proceedings of Inter-Speech.David DeVault, Kenji Sagae, and David Traum.
2011b.Incremental interpretation and prediction of utterancemeaning for interactive dialogue.
Dialogue & Dis-course, 2(1).Arno Hartholt, Thomas Russ, David Traum, EduardHovy, and Susan Robinson.
2008.
A common groundfor virtual humans: Using an ontology in a naturallanguage oriented virtual human architecture.
In Pro-ceedings of LREC, Marrakech, Morocco, may.Brian Plu?ss, David DeVault, and David Traum.
2011.Toward rapid development of multi-party virtual hu-man negotiation scenarios.
In Proceedings of Sem-Dial.Kenji Sagae, Gwen Christian, David DeVault, andDavid R. Traum.
2009.
Towards natural language un-derstanding of partial speech recognition results in dia-logue systems.
In Short Paper Proceedings of NAACLHLT.David Traum, Stacy Marsella, Jonathan Gratch, JinaLee, and Arno Hartholt.
2008.
Multi-party, multi-issue, multi-strategy negotiation for multi-modal vir-tual agents.
In Proceedings of IVA.David Traum.
2003.
Semantics and pragmatics of ques-tions and answers for dialogue agents.
In Proc.
of theInternational Workshop on Computational Semantics.Zhiyang Wang, Jina Lee, and Stacy Marsella.
2011.Towards more comprehensive listening behavior: Be-yond the bobble head.
In Proceedings of IVA.133
