Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 49?60,Paris, October 2009. c?2009 Association for Computational LinguisticsParsing Formal Languages using Natural Language Parsing TechniquesJens Nilsson?
Welf Lo?we?
Johan Hall??
Joakim Nivre???Va?xjo?
University, School of Mathematics and Systems Engineering, Sweden?Uppsala University, Department of Linguistics and Philology, Sweden{jens.nilsson|welf.lowe|johan.hall|joakim.nivre}@vxu.seAbstractProgram analysis tools used in softwaremaintenance must be robust and ought tobe accurate.
Many data-driven parsing ap-proaches developed for natural languagesare robust and have quite high accuracywhen applied to parsing of software.
Weshow this for the programming languagesJava, C/C++, and Python.
Further studiesindicate that post-processing can almostcompletely remove the remaining errors.Finally, the training data for instantiatingthe generic data-driven parser can be gen-erated automatically for formal languages,as opposed to the manually developmentof treebanks for natural languages.
Hence,our approach could improve the robust-ness of software maintenance tools, proba-bly without showing a significant negativeeffect on their accuracy.1 IntroductionSoftware engineering, especially software mainte-nance, is supported by numerous program anal-ysis tools.
Maintenance tasks include programcomprehension (understanding unknown code forfixing bugs or further development), quality as-sessment (judging code, e.g., in code reviews),and reverse-engineering (reifying the design doc-uments for given source code).
To extract infor-mation from the programs, the tools first parse theprogram code and produce an abstract syntax tree(AST) for further analysis and abstraction (Streinet al, 2007).
As long as the program conformsto the syntax of a programming language, clas-sical parsing techniques known from the field ofcompiler construction may be applied.
This, how-ever, cannot be assumed in general, as the pro-grams to analyze can be incomplete, erroneous, orconform to a (yet unknown) dialect or version ofthe language.
Despite error stabilization, classi-cal parsers then lose a lot of information or simplybreak down.
This is unsatisfactory for tools sup-porting maintenance.
Therefore, quite some efforthas gone into the development of robust parsers ofprograms for these tools (cf.
our related work sec-tion 5).
This effort, however, has to be repeatedfor every programming language.The development of robust parsers is of specialinterest for languages like C/C++ due to their nu-merous dialects in use (Anderson, 2008).
Also,tools for languages frequently coming in new ver-sions, like Java, benefit from robust parsing.
Fi-nally, there are languages like HTML where exist-ing browsers are forgiving if documents do not ad-here to the formal standard with the consequencethat there exist many formally erroneous docu-ments.
In such cases, robust parsing is even a pre-requisite for tool-supported maintenance.The accuracy of parsing is a secondary goalin the context of software maintenance.
Taskslike program comprehension, quality assessment,and reverse-engineering are fuzzy by their nature.There is no well-defined notion of correctness?rather an empirical answer to the question: Didit help the software engineers in fulfilling theirtasks?
Moreover, the information provided to theengineers abstracts anyway from the concrete pro-gram syntax and semantics, i.e., inaccuracies inthe input may disappear in the output.
Finally, pro-gram analyses are often heuristics themselves, ap-proximating computationally hard problems likepattern matching and optimal clustering.The natural language processing (NLP) com-munity has for many years developed parsing tech-nology that is both completely robust and highlyaccurate.
The present approach applies this tech-nology to programming languages.
It is robust inthe sense that, for each program, the parser alwaysgives a meaningful model even for slightly incor-rect and incomplete programs.
The approach is,49however, not accurate to 100%, i.e., even correctprograms may lead to slightly incorrect models.As we will show, it is quite accurate when appliedto programming languages.The data-driven dependency parsing approachapplied here only needs correct examples of thesource and the expected analysis model.
Then itautomatically trains and adapts a generic parser.As we will show, training data for adapting to anew programming language can even be gener-ated automatically.
Hence, the effort for creatinga parser for a new programming language is quitesmall.The basic idea ?
applying natural language pars-ing to programming languages ?
has been pre-sented to the program maintenance community be-fore (Nilsson et al, 2009).
This paper contributeswith experimental results on1.
data-driven dependency parsing of the pro-gramming languages C/C++, Java, andPython,2.
transformations between dependency struc-ture and phrase structure adapted to program-ming languages,3.
generic parser model selection and its effecton parsing accuracy.Section 2 gives an introduction to the parsing tech-nology applied here.
In section 3, the preparationof the training examples necessary is described,while section 4 presents the experimental results.Section 5 discusses related work in informationextraction for software maintenance.
We end withconclusions and future work in section 6.2 NLP BackgroundDependency structure is one way of representingthe syntax of natural languages.
Dependency treesform labeled, directed and rooted trees, as shownin figure 1.
One essential difference compared tocontext-free grammar is the absence of nontermi-nals.
Another difference is that the syntactic struc-ture is composed of lexical tokens (also called ter-minals or words) linked by binary and directed re-lations called dependencies.
Each token in the fig-ure is labeled with a part-of-speech, shown at thebottom of the figure.
Each dependency relation isalso labeled.The parsing algorithm used in the experimentsof section 4, known as the Nivre?s arc-eager al-Figure 1: Sentence with a dependency tree.gorithm (Nivre, 2003), can produce such depen-dency trees.
It bears a resemblance to the shift-reduce parser for context-free grammars, with themost apparent difference being that terminals (notnonterminals) are pushed onto the stack.
Parserconfigurations are represented by a stack, a listof (remaining) input tokens, and the (current) setof arcs for the dependency tree.
Similar to theshift-reduce parser, the construction of syntacticstructure is created by a sequence of transitions.The parser starts with an empty stack and termi-nates when the input queue is empty, parsing in-put from left to right.
It has four transitions (Left-Arc, Right-Arc, Reduce and Shift), manipulatingthese data structures.
The algorithm has a lineartime complexity as it is guaranteed to terminateafter at most 2n transitions, given that the lengthof the input sentence is n.In contrast to a parser guided by a grammar(e.g., ordinary shift-reduce parsing for context-free grammars), this parser is guided by a clas-sifier induced from empirical data using machinelearning (Nivre et al, 2004).
Hence, the parser re-quires training data containing dependency trees.In other words, the parser has a training phasewhere the training data is used by the trainingmodule in order to learn the correct sequence oftransitions.
The training data can contain depen-dency trees for sentences of any language irrespec-tively of whether the language is a natural or for-mal one.The training module produces the correct tran-sition sequences using the dependency trees ofthe training data.
These correct parser configura-tions and transition sequences are then provided astraining data to a classifier, which predicts the cor-rect transitions (including a dependency label forLeft-Arc, Right-Arc) given parser configurations.A parser configuration contains a vast amount ofinformation located in the data-structures.
It istherefore necessary to abstract it into a set of fea-tures.
Possible features are word forms and parts-50of-speech of tokens on the stack and in the listof input tokens, and dependency labels of depen-dency arcs created so far.The parser produces exactly one syntactic anal-ysis for every input, even if the input does not con-form to a grammar.
The price we have to pay forthis robustness is that any classifier is bound tocommit errors even if the input is acceptable ac-cording to a grammar.3 General ApproachIn section 2, we presented a parsing algorithm forproducing dependency trees for natural languages.Here we will show how it can be used to producesyntactic structures for programming languages.Since the framework requires training data form-ing correct dependency trees, we need an approachfor converting source code to dependency trees.The general approach can be divided into twophases, training and production.
In order to beable to perform both these phases in this study, weneed to adapt natural language parsing to the needsof information extraction from programming lan-guage code, i.e., we need to automatically producetraining data.
Therefore, we apply:(a) Source Code ?
Syntax Tree: the classicalapproach for generating syntax trees for cor-rect and complete source code of a program-ming language.
(b) Syntax Tree ?
Dependency Tree: an ap-proach for encoding the syntax trees as de-pendency trees adapted to programming lan-guages.
(c) Dependency Tree ?
Syntax Tree: an ap-proach to convert the dependency trees backto syntax trees.These approaches have been accomplished as pre-sented below.
In the training phase, we need totrain and adapt the generic parsing approach to aspecific programming language.
Therefore:(1) Generate training data automatically byproducing syntax trees and then dependencytrees for correct programs using approaches(a) and (b).
(2) Train the generic parser with the trainingdata.This automated training phase needs to be donefor every new programming language we adapt to.Finally, in the production phase, we extract the in-formation from (not necessarily correct and com-plete) programs:(3) Parse the new source code into dependencytrees.
(4) Convert the dependency trees into syntaxtrees using approach (c).This automated production phase needs to be exe-cuted for every project we analyze.Steps (2) and (3) have already been discussed insection 2 for parsing natural languages.
They canbe generalized to parsing programming languagesas described in section 3.1.
Both the training phaseand the production phase are complete, once thesteps (a)?
(c) have been accomplished.
We presentthem in sections 3.2, 3.3, and 3.4, respectively.3.1 Adapting the InputAs mentioned, the parsing algorithm describedin section 2 has been developed for natural lan-guages, which makes it necessary to resolve anumber of issues that arise when the parser isadapted for source code as input.
First, the parsingalgorithm takes a sequence of words as input, andfor simplicity, we map the tokens in a program-ming language to words.One slightly more problematic issue is how todefine a ?sentence?
in source code.
A naturallanguage text syntactically decomposes into a se-quence of sentences in a relatively natural way.But is there also a natural way of splitting sourcecode into sentences?
The most apparent approachmay be to define a sentence as a compilation unit,that is, a file of source code.
This can however re-sult in practical problems since a sentence in a nat-ural language text is usually on average between15?25 words long, partially depending on the au-thor and the type of text.
The sequence of tokensin a source file may on the other hand be muchlonger.
Time complexity is usually in practice ofless importance when the average sentence lengthis as low as in natural languages, but that is hardlythe case when there can be several thousands to-kens in a sentence to parse.Other approaches could for instance be to letone method be a sentence.
However, then we needto deal with other types of source code construc-tions explicitly.
We have in this study for sim-plicity let one compilation unit be one sentence.This is possible in practice due to the linear time51complexity of the parsing algorithm of section 2,a quite unusual property compared to other NLPparsers guided by machine learning with state-of-the-art accuracy.3.2 Source Code?
Syntax TreeIn order to produce training data for the parserfor a programming language, an analyzer thatconstructs syntax trees for correct and completesource code of the programming language isneeded.
We are in this study focusing on Java,Python and C/C++, and consequently need onesuch analyzer for each language.
For example, fig-ure 2 shows the concrete syntax tree of the follow-ing fragments of Java:Example (1):public String getName() {return name;}Example (2):while (count > 0) {stack[--count]=null;}We also map the output of the lexical ana-lyzer to the parts-of-speech for the words (e.g.,Identifier for String and getName).
Allsource code comments and indentation informa-tion (except for Python where the indentation con-veys hierarchical information) have been excludedfrom the syntax trees.
All string and characterliterals have also been mapped to ?string?
and?char?, respectively.
This does not entail that theapproach is lossy, since all this information canbe retained in a post-processing step, if neces-sary.
As pointed out by, for instance, Collard etal.
(2003), comments and indentation may amongother things be of interest when trying to under-stand source code.3.3 Syntax Tree?
Dependency TreeHere we will discuss the conversion of syntax treesinto dependency trees.
We use a method that hasbeen successfully applied for natural languagesfor converting syntax trees into a convertible de-pendency tree that makes it possible to performthe inverse conversion, meaning that informationabout the syntax tree is saved in complex arc la-bels (Hall and Nivre, 2008).
We also present re-sults in section 4 using the dependency trees thatcannot be used for the inverse conversion, whichwe call non-convertible dependency trees.The conversion is performed in a two-step ap-proach.
First, the algorithm traverses the syntaxtree from the root and identifies the head-child andthe terminal head for all nonterminals in a recur-sive depth-first search.
To identify the head-childfor each nonterminal, the algorithm uses heuristicscalled head-finding rules, inspired by, for instance,Magerman (1995).
Three head-finding strategieshave been investigated.
For each nonterminal:1.
FREQ: Let the element with the most fre-quently occurring name be the head, but ex-clude the token ?;?
as a potential head.
If twotokens have the same frequency, let the left-most occurring element be the head.2.
LEFT: let the leftmost terminal in the entiresubtree of the nonterminal be the head of allother elements.3.
RIGHT: let the rightmost terminal in the en-tire subtree of the nonterminal be the head ofall other elements.The dependency trees in figures 3 and 4 use LEFTand FREQ.
LEFT and RIGHT induce that all arcsare pointing to the right and left, respectively.
Thehead-finding rules for FREQ are automatically cre-ated by counting the children?s names for eachdistinct non-terminal name in the syntax trees ofthe training data.
The priority list is then com-piled by ordering the elements by descending fre-quency for each distinct non-terminal name.
Forinstance, given that the syntax trees are grammati-cally correct, every non-terminal While will con-tain the tokens (, ) and while.
These tokenshave thus the highest priority, and while there-fore becomes the head in the lower dependencytree of figure 4.
This is the same as choosing theleft-most mandatory element for each left-handside in the grammar.
An interesting observationis that binary operators and the copy assignmentoperator become the heads of their operands forFREQ, which is the case for < and = in figure 4.Note also that the element names of terminals actas part-of-speech tags, e.g., the part-of-speech forString is Identifier.In the second step, a dependency tree is createdaccording to the identified terminal heads.
Thearcs in the convertible dependency tree are labeledwith complex arc labels, where each complex arclabel consists of two sublabels:52Figure 2: Syntax trees for examples (1) and (2).Figure 3: Non-convertible dependency trees for example (1) using LEFT (upper) and FREQ (lower).1.
Encode the dependent spine, i.e., the se-quence of nonterminal labels from the de-pendent terminal to the highest nonterminalwhere the dependent terminal is the terminalhead; ?|?
separates the nonterminal labels,2.
Encode the attachment point in the headspine, a non-negative integer value a, whichmeans that the dependent spine is attached asteps up in the head spine.By encoding the arc labels with these two subla-bels, it is possible to perform the inverse conver-sion, (see subsection 3.4).The non-convertible dependency labels allow usto reduce the complexity of the arc labels, makingthe learning problem simpler due to fewer distinctarc labels.
This may result in a higher accuracyduring parsing and can be used as input for fur-ther processing directly without taking the detourback to syntax trees.
This can be motivated bythe fact that all information in the syntax trees isusually not needed anyway in many reverse engi-neering tasks, but labels indicating method callsand declarations ?
the most important informationfor most program comprehension tasks ?
are pre-served.
This is exemplified by the fact that bothdependency structures in figure 3 contain the la-bel MethodsDecl.. We thus believe that all thenecessary information is also captured in this lessinformative dependency tree.
Each dependency la-bel is the highest nonterminal name of the spine,that is, the single nonterminal name that is closestto its head.
The non-convertible dependency labelalso excludes the attachment point value, makingthe learning problem even simpler.
Figures 3 and4 show the non-convertible dependency labels ofthe syntax trees (or phrase structure trees) in thesame figures, where each label contains just a sin-gle nonterminal name of the original syntax trees.3.4 Dependency Tree?
Syntax TreeThe inverse conversion is a bottom-up and top-down process on the convertible dependency tree53Figure 4: Non-convertible dependency trees for example (2) using LEFT (upper) and FREQ (lower).
(must contain complex arc labels).
First, the algo-rithm visits every terminal in the convertible de-pendency tree and restores the spines of nontermi-nals with labels for each terminal using the infor-mation in the first sublabel of the incoming arc.Thus, the bottom-up process results in a spine ofzero or more arcs from each terminal to the highestnonterminal of which the terminal is the terminalhead.
Secondly, the spines are weaved together ac-cording to the arcs of the dependency tree.
This isachieved by traversing the dependency tree recur-sively from the root using a pre-order depth-firstsearch, where the dependent spine is attached toits head spine or to the root of the syntax tree.
Theattachment point a, given by the second sublabel,specifies the number of nonterminals between theterminal head and the attachment nonterminal.4 ExperimentsWe will in this section present parsing experimentsand evaluate the accuracy of the syntax trees pro-duced by the parser.
As mentioned in section 2,the parsing algorithm is robust in the sense that italways produces a syntactic analysis no matter theinput, but it can commit errors even for correct in-put.
This section investigates the accuracy for cor-rect input, when varying feature set, head-findingrules and language.
We begin with the experimen-tal setup.4.1 Experimental SetupThe open-source software MaltParser (malt-parser.org) (Nivre et al, 2006) is used in the ex-periments.
It contains an implementation of theparsing algorithm, as well as an implementationof the conversion strategy from syntax trees todependency trees and back, presented in subsec-tions 3.3 and 3.4.
It comes with the machinelearner LIBSVM (Chang and Lin, 2001), pro-ducing the most accurate results for parsing nat-ural languages compared to other evaluated ma-chine learners (Hall et al, 2006).
LIBSVM re-quires training data.
The source files of the follow-ing projects have been converted into dependencytrees:?
For Java: Recoder 0.83 (Gutzmann et al,2007), using all source files in the directory?src?
(having 400 source files with 92k LOCand 335k tokens).?
For C/C++: Elsa 2005.08.22b (McPeak,2005), where 1389 source files were used,including the 978 C/C++ benchmark files inthe distribution (thus comprising 1389 sourcefiles with 265k LOC and 691k tokens).?
For Python: Natural Language Toolkit0.9.5 (Bird et al, 2008), where all source filesin the directory ?nltk?
were used (having 160source files with 65k LOC and 280k tokens).To construct the syntax tree for the source codefile of Recoder, we have used Recoder.
It cre-ates an abstract syntax tree for a source file, butwe are currently interested in the concrete syntaxtree with all the original tokens.
In this first con-version step, the tokens of the syntax trees are thusretained.
For example, the syntax trees in figure 2are generated by Recoder.54The same strategy was adopted for Elsa with thedifference that CDT 4.0.3, a plug-in to the EclipseIDE to produce syntax trees for source code ofC/C++, was used for producing the abstract syntaxtrees.1 It produces abstract syntax trees just likeRecoder, so the concrete syntax trees have alsobeen created by retaining the tokens.The Python 2.5 interpreter is actually shippedwith an analyzer that produces concrete syn-tax trees (using the Python imports fromast import PyCF ONLY AST and importparser), which we have utilized for the Pythonproject above.
Hence, no additional processing isneeded in order prepare the concrete syntax treesas training data.For the experiments, the source files have beendivided into a training set T and a developmenttest set D, where the former comprises 80% of thedependency trees and the latter 10%.
The remain-ing 10% (E) has been left untouched for later use.The source files have been ordered alphabeticallyby the file names including the path.
The depen-dency trees have then been distributed into the datasets in a pseudo-randomized way.
Every tenth de-pendency tree starting at index 9 (i.e.
dependencytrees 9, 19, 29, .
.
. )
will belong to D, and everytenth dependency trees starting at index 0 to E.The remaining trees constitute the training set T .4.2 MetricsThe standard evaluation metric for parse trees fornatural languages based on context-free grammaris F-score, the harmonic mean of precision andrecall.
F-score compares constituents ?
definedby triples ?i, j,XP ?
spanning between terminalsi and j ?
derived from the test data with thosederived from the parser.
A constituent in theparser output matches a constituent in the test datawhen they span over the same terminals in theinput string.
Recall is the ratio of matched con-stituents over all constituents in the test data.
Pre-cision is the ratio of matched constituents overall constituents found by the parser.
F-scorecomes in two versions, one unlabeled (FU ) andone labeled (FL), where each correct constituentin the latter also must have the correct nontermi-nal name (i.e., XP ).
The metric is implementedin Evalb (Collins and Sekine, 2008).1It is worth noting that CDT failed to produce syntax treesfor 2.2% of these source files, which were consequently ex-cluded from the experiments.
This again indicates the diffi-cult of parsing C/C++ due to its different dialects.FL FUFR LE RI FR LE RIUL 82.1 93.5 74.6 92.3 97.9 90.6L 89.7 97.7 80.8 95.8 99.3 92.1Table 1: F-score for various parser models andhead-finding rules for Java, where FR = FREQ, LE= LEFT and RI = RIGHT.The standard evaluation metric measuring accu-racy for dependency parsing for natural languageis, on the other hand, labeled (ASL) and unlabeled(ASU ) attachment score.
ASU is the ratio of to-kens attached to its correct head.
ASL is the sameas ASU with the additional requirement that thedependency label should be correct as well.4.3 ResultsThis section presents the parsing results.
The firstexperiment was conducted for Java, using the in-verse transformation back to syntax trees.
Twofeature models are evaluated, one unlexicalizedfeature sets (UL) containing 13 parts-of-speechand 4 dependency label features, and one lexical-ized feature sets (L) containing all these 17 fea-tures and 13 additional word form features, de-veloped by manual feature optimization.
Table 1compares these two feature sets, as well as the dif-ferent head-finding rules discussed previously.The figures give a clear answer to the questionwhether lexical information is beneficial or not.Every figure in the row L is higher than its cor-responding figure in the row UL.
This means thatnames of variables, methods, classes, etc., actu-ally contain valuable information for the classifier.This is in contrast to ordinary syntactic parsing us-ing a grammar of programming languages whereall names are mapped to the same value (e.g.
Iden-tifier), and, e.g., integer constants to IntLiteral, be-fore the parse.
One potential contributing factorof the difference is the naming conventions thatprogrammers normally follow.
For example, nam-ing classes, class attributes and local variables, etc.using typical methods names, such as equals inJava, is usually avoided by programmers.It is just as clear that the choice of head-findingstrategy is very important.
For both FL and FU ,the best choice is with a wide margin LEFT, fol-lowed by FREQ.
RIGHT is consequently the leastaccurate one.
A higher amount of arcs pointing tothe right seems to be beneficial for the strategy of55ASL ASUFR LE RI FR LE RICO 87.6 96.6 86.6 90.9 98.2 90.7NC 91.0 99.1 89.5 92.1 99.7 90.7Table 2: Attachment score for Java and the lexicalfeature set, where CO = convertible and NC = non-convertible dependency trees.Python C/C++FL FU FL FUUL 91.5 92.1 95.6 96.4L 99.1 99.2 96.5 96.9Table 3: F-score for various parser models andhead-finding rules LEFT for Python and C/C++.parsing from left to right.Table 1 can be compared to the accuracy onthe parser output before conversion from depen-dency trees to syntax trees.
This is shown in thefirst row (CO) of table 2, where all informationin the complex dependency label is concatenatedand placed in the dependency label.
The relation-ships between the head-finding strategies remainthe same, but it is worth noting that the accuraciesfor FREQ and RIGHT are closer to each other, en-tailing a more difficult conversion to syntax treesfor the latter.
The first row can also be comparedto the second row (NC) in the same table, show-ing the accuracies when training and parsing withnon-convertible dependency trees.
One observa-tion is that each figure in NC is higher than itscorresponding figure in CO (even ASU for RIGHTwith more decimals), probably attributed to thelower burden on the parser.
Both ASU and ASLare above 99% for the non-convertible dependencytrees using LEFT.We can see that choosing an appropriate repre-sentation of syntactic structure to be used duringparsing is just as important for programming lan-guages as for natural languages, when using data-driven natural language parsers (Bikel, 2004).The parser output in table 1 can more eas-ily be used as input to existing program com-prehension tools, normally requiring abstract syn-tax trees.
However, the highly accurate outputfor LEFT using non-convertible dependency treescould be worth using instead, but it requires someadditional processing.In order to investigate the language indepen-dence of our approach, table 3 contains the cor-responding figures as in table 1 for Python andC/C++, restricted to LEFT, which is the besthead-finding strategy for these languages as well.Again, each lexicalized feature set (L) outper-forms its corresponding unlexicalized feature set(UL).
Python has higher FL and virtually the sameFU as Java, whereas C/C++ has the lowest accu-racies for L. However, the UL figures are not farbehind the L figures for C/C++, and C/C++ hasin fact higher FL for UL compared to Java andPython.
These results can maybe be explained bythe fact that C/C++ has less verbose syntax thanboth Java and Python, making the lexical featuresless informative.The FL figures for Java, Python and C/C++ us-ing LEFT can also be compared to the correspond-ing figures in Nilsson et al (2009).
They use thesame data sets but a slightly different head-findingstrategy.
Instead of selecting the leftmost element(terminal or non-terminal) as in LEFT, they alwaysselect the leftmost terminal, resulting in FL=99.5for Java, FL=98.3 for Python and FL=96.5 forC/C++.
That is, our results are slightly lower forJava, higher for Python, and slightly higher forC/C++.
The same holds for FU as well.
Thatis, having only arcs pointing to the right results inhigh accuracy for all languages (which is the casefor Left described in section 3), but small devia-tions from this head-finding strategy can in fact bebeneficial for some languages.We are not aware of any similar studies forprogramming languages2 so we compare the re-sults to natural language parsing.
First, the fig-ures in table 2 for dependency structure are betterthan figures reported for natural languages.
Somenatural languages are easier to parse than others,and the parsing results of the CoNLL shared task2007 (Nivre et al, 2007) for dependency structureindicate that English and Catalan are relativelyeasy, with ASL around 88-89% and ASU around90-94% for the best dependency parsers.Secondly, compared to parsing German withphrase structure with the same approach as here,with FU = 81.4 and FL = 78.7%, and Swedish,with FU = 76.8 and FL = 74.0 (Hall and Nivre,2A comparative experiment using another data-drivenNLP parser for context-free grammar could be of theoreti-cal interest.
However, fast parsing time is important in pro-gram comprehension tasks, and data-driven NLP parsers forcontext-free grammar have worse than a linear time complex-ity.
As, e.g., the Java project has 838 tokens per source file,linear time complexity is a prerequisite in practice.56Correct Label Parsing Label66 FieldReference VariableReference25 VariableReference FieldReference12 MethodDeclaration LocalVariableDeclaration9 Conditional FieldReference5 NotEquals MethodReference4 Plus MethodReference4 Positive *4 LessThan FieldReference4 GreaterOrEquals FieldReference4 Divide FieldReference4 Modulo FieldReference4 LessOrEquals FieldReference3 Equals NotEquals3 LessOrEquals Equals3 NotEquals EqualsTable 4: Confusion matrix for Java using non-convertible dependency trees with LEFT, orderedby descending frequency.2008), the figures reported in tables 1 and 3 arealso much better.
It is however worth noting thatnatural languages are more complex and less reg-ular compared to programming languages.
Al-though it remains to be shown, we conjecture thatthese figures are sufficiently high for a large num-ber of program comprehension tasks.4.4 Error AnalysisThis subsection will study the result for Java withnon-convertible dependency trees (NC) and LEFT,in order to get a deeper insight into the types oferrors that the parser commits.
Specifically, thelabeling mistakes caused by the parser are investi-gated here.
This is done by producing a confusionmatrix based on the dependency labels.
That is,how often does a parser confuse label X with la-bel Y .
This is shown in table 4 for the 15 mostcommon errors.The two most frequent errors show that theparser confuses FieldReference and VariableRef-erence.
A FieldReference refers to a class attributewhereas a VariableReference could refer to eitheran attribute or a local variable.
The parser mixes areference to a class attribute with a reference thatcould also be a local variable or vice versa.
Theerror is understandable, since the parser obviouslyhas no knowledge about where the variables aredeclared.
This is an error that type and name anal-ysis can easily resolve.
On the use-occurrence of aname (reference), analysis looks up for both pos-sible define-occurrences of the name (declaration),first a LocalVariableDeclaration and then a Field-Declaration.
It uses the one that is found first.Another type of confusion involves declara-tions, where a MethodDeclaration is misinter-preted as a LocalVariableDeclaration.
This typeof error can be resolved by a simple post-processing step: a LocalVariableDeclaration fol-lowed by opening parenthesis (always recognizedcorrectly) is a MethodDeclaration.Errors that involve binary operators, e.g., Con-ditional, NotEqual, Plus, are at rank 4 and belowin the list of the most frequent errors.
They aremost likely a result of the incremental left-to-rightparsing strategy.
The whole expression should belabeled in accordance with its binary operator (seecount > 0 in figure 4 for LEFT), but is incor-rectly labeled as either MethodReference, Field-Reference or some other operator instead.
The ref-erences actually occur in the left-hand side sub-expression of the binary operators.
This meansthat subexpressions and bracketing were recog-nized correctly, but the type of the top expressionnode was mixed up.
Extending the lookahead inthe list of remaining input tokens, making it pos-sible for the classifier in the parser to look at evenmore yet unparsed tokens, might be one possiblesolution.
However, these errors are by and largerelatively harmless anyway.
Hence, no correctionis in practice needed.Figure 5 displays some typical mistakes for theexample program fragmentreturn (fw.unitIndex == unitIndex &&fw.unitIndex.equals(unitList));The parser mixes up a ParenthesizedExpressionwith a Conditional, a boolean ParenthesizedEx-pression only occurring in conditional statementsand expressions.
Then it incorrectly assigns thelabel Equals to the arc between the first left paren-thesis and the first fw instead of the correct la-bel LogicalAnd.
It mixes up the type of the wholeexpression, an Equals- (i.e., ==) is taken for anLogicalAnd-expression (i.e., &&).
Finally, the twoFieldReferences are taken as more general Vari-ableReferences, which is corrigible as discussed.In addition to a possible error correction in apost-processing step, the parsing errors could dis-appear due to the abstraction of subsequent anal-yses as commonly used in software maintenancetools.
For instance, without any error correction,the type reference graphs of our test program, thecorrect one and the one constructed using the notquite correct parsing results, are identical.57Correct:Parsed:Figure 5: Typical errors for LEFT using by non-convertible dependency trees.5 Related WorkClassical parsers for formal languages have beenknown for many years.
They (conventionally) ac-cept a context-free language defined by a context-free grammar.
For each program, the parsersproduce a phrase structure referred to as an ab-stract syntax tree (AST) which is also defined by acontext-free language.
Parsers including error sta-bilization and AST-constructors can be generatedfrom context-free grammars for parsers (Kastenset al, 2007).
A parser for a new language stillrequires the development of a complex specifica-tion.
Moreover, error stabilization often throwsaway large parts of the source ?
it is robust butdoes not care about maximizing accuracy.Breadth-First Parsing (Ophel, 1997) was de-signed to provide better error stabilization than tra-ditional parsers and parser generators.
It uses atwo phase approach: the first phase identifies high-level entities ?
the second phase parses the struc-ture with these entities as root nonterminals (ax-ioms).Fuzzy Parsing (Koppler, 1997) was designedto efficiently develop parsers by performing theanalysis on selected parts of the source insteadof the whole input.
It is specified by a set of(sub)grammars each with their own axioms.
Theactual approach is then similar to Breadth-FirstParsing: it scans for instances of the axioms andthen parses according to the grammar.
It makesparsing more robust in the sense that it ignoressource fragments ?
including missing parts, errorsand deviations therein ?
that subsequent analysesabstract from anyway.
A prominent tool usingthe fuzzy parsing approach for information extrac-tion in reverse-engineering tools is Sniff (Bischof-berger, 1992) for analyzing C++ code.Island grammars (Moonen, 2001) generalize onFuzzy Parsing.
Parsing is controlled by two gram-mar levels (island and sea) where the sea-level isused when no island-level production applies.
Theisland-level corresponds to the sub-grammars offuzzy parsing.
Island grammars have been appliedin reverse-engineering, specifically, to bank soft-ware (Moonen, 2002).Syntactic approximation based on lexical anal-ysis was developed with the same motivation asour work: when maintenance tools need syntac-tic information but the documents could not beparsed for some reason, hierarchies of regular ex-pression analyses could be used to approximatethe information with high accuracy (Murphy andNotkin, 1995; Cox and Clarke, 2003).
Their in-formation extraction approach is characterized as?lightweight?
in the sense that it requires littlespecification effort.A similar robust and light-weight approach forinformation extraction constructs XML formats(JavaML and srcML) from C/C++/Java programsfirst, before further processing with XML toolslike Xpath (Badros, 2000; Collard et al, 2003).
Itcombines lexical and context free analyses.
Lex-ical pattern matching is also used in combinationwith context free parsing in order to extract factsfrom semi-structured specific comments and con-58figuration specifications in frameworks (Knodeland Pinzger, 2003).TXL is a rule-based language defining informa-tion extraction and transformation rules for formallanguages (Cordy et al, 1991).
It makes it possibleto incrementally extend the rule base and to adaptto language dialects and extensions.
As the rulesare context-sensitive, TXL goes beyond the lexicaland context-free approaches discussed before.The fundamental difference of our approachcompared to lexical, context-free, and context-sensitive approaches (and combinations thereof) isthat we use automated machine learning instead ofmanual specification for defining and adapting theinformation extraction.General NLP techniques have been applied forextracting facts from general source code com-ments to support software maintenance (Etzkornet al, 1999).
Comments are extracted from sourcecode using classical lexical analysis; additional in-formation is extracted (and then added) with clas-sical compiler front-end technology.NLP has also been applied to other informa-tion extraction tasks in software maintenance toanalyze unstructured or very large informationsources, e.g., for analyzing requirement speci-fications (Sawyer et al, 2002), in clone detec-tion (Marcus and Maletic, 2001; Grant and Cordy,2009), and to connect program documentation tosource code (Marcus and Maletic, 2003).6 Conclusions and Future WorkIn this paper, we applied natural language parsingtechniques to programming languages.
One ad-vantage is that it offers robustness, since it alwaysproduces some output even if the input is incorrector incomplete.
Completely correct analysis can,however, not be guaranteed even for correct input.However, the experiments showed that accuracy isin fact close to 100%.In contrast to robust information extractors usedso far for formal languages, the approach pre-sented here is rapidly adaptable to new languages.We automatically generate the language specificinformation extractor using machine learning andtraining of a generic parsing, instead of explicitlyspecifying the information extractor using gram-mar and transformation rules.
Also the trainingdata can be generated automatically.
This couldincrease the development efficiency of parsers,since no language specification has to be provided,only examples.Regarding accuracy, the experiments showedthat selecting the syntactic base representationused by the parser internally has a major impact.Incorporating, for instance, class, method andvariable names in the set of features of the parserimproves the accuracy more than expected.
Thedetailed error analysis showed that many errorscommitted by the parser are forgivable, as theyare anyway abstracted in later processing phases.Other errors are easily corrigible.
We can alsosee that the best results presented here are muchhigher than the best parsing results for natural lan-guages.Besides efficient information extractor develop-ment, efficient parsing itself is important.
Appliedto programs which can easily contain several mil-lion lines of code, a parser with more than lineartime complexity is not acceptable.
The data-drivenparser utilized here has linear parsing time.These results are only the first (promising) steptowards natural language parsing leveraging infor-mation extraction for software maintenance.
How-ever, the only way to really evaluate the usefulnessof the approach is to use its output as input to clientanalyses, e.g., software measurement and archi-tecture recovery, which we plan to do in the fu-ture.
Another direction for future work is to applythe approach to more dialects of C/C++, such asanalyzing correct, incomplete, and erroneous pro-grams for both standard C and its dialects.ReferencesPaul Anderson.
2008.
90 % Perspiration: EngineeringStatic Analysis Techniques for Industrial Applica-tions.
In Proceedings of the 8th IEEE InternationalWorking Conference on Source Code Analysis andManipulation, pages 3?12.Greg J. Badros.
2000.
JavaML: a Markup Languagefor Java Source Code.
In Proceedings of the 9thInternational World Wide Web conference on Com-puter networks : the international journal of com-puter and telecommunications networking, pages159?177.Daniel M. Bikel.
2004.
Intricacies of Collins?
ParsingModel.
Computational Linguistics, 30(4):479?511.Steven Bird, Edward Loper, and Ewan Klein.2008.
Natural Language Toolkit (NLTK) 0.9.5.http://nltk.org/.Walter R. Bischofberger.
1992.
Sniff: A PragmaticApproach to a C++ Programming Environment.
InUSENIX C++ Conference, pages 67?82.59Chih-Chung Chang and Chih-Jen Lin.
2001.
LIB-SVM: A Library for Support Vector Machines.Michael L. Collard, Huzefa H. Kagdi, and Jonathan I.Maletic.
2003.
An XML-Based Lightweight C++Fact Extractor.
In 11th IEEE International Work-shop on Program Comprehension, pages 134?143.Michael Collins and Satoshi Sekine.
2008.
Evalb.http://nlp.cs.nyu.edu/evalb/.James R. Cordy, Charles D. Halpern-Hamu, and EricPromislow.
1991.
TXL: a Rapid Prototyping Sys-tem for Programming Language Dialects.
ComputerLanguages, 16(1):97?107.Anthony Cox and Charles L. A. Clarke.
2003.
Syntac-tic Approximation Using Iterative Lexical Analysis.In Proceedings of the 11th IEEE International Work-shop on Program Comprehension, pages 154?163.Letha H. Etzkorn, Lisa L. Bowen, and Carl G. Davis.1999.
An Approach to Program Understanding byNatural Language Understanding.
Natural Lan-guage Engineering, 5(3):219?236.Scott Grant and James R. Cordy.
2009.
Vector SpaceAnalysis of Software Clones.
In Proceedings ofthe IEEE 17th International Conference on ProgramComprehension, pages 233?237.Tobias Gutzmann, Dirk Heuzeroth, and Mircea Trifu.2007.
Recoder 0.83. http://recoder.sourceforge.net/.Johan Hall and Joakim Nivre.
2008.
Parsing Discon-tinuous Phrase Structure with Grammatical Func-tions.
In Proceedings of GoTAL, pages 169?180.Johan Hall, Joakim Nivre, and Jens Nilsson.
2006.Discriminative Classifiers for Deterministic Depen-dency Parsing.
In Proceedings of COLING-ACL,pages 316?323.Uwe Kastens, Anthony M. Sloane, and William M.Waite.
2007.
Generating Software from Specifica-tions.
Jones and Bartlett Publishers.Jens Knodel and Martin Pinzger.
2003.
ImprovingFact Extraction of Framework-Based Software Sys-tems.
In Proceedings of 10th Working Conferenceon Reverse Engineering, pages 186?195.Rainer Koppler.
1997.
A Systematic Approach toFuzzy Parsing.
Software - Practice and Experience,27(6):637?649.David M. Magerman.
1995.
Statistical Decision-treeModels for Parsing.
In Proceedings of ACL, pages276?283.Andrian Marcus and Jonathan I. Maletic.
2001.
Iden-tification of High-Level Concept Clones in SourceCode.
In Proceedings of the 16th IEEE interna-tional conference on Automated software engineer-ing, page 107.Andrian Marcus and Jonathan I. Maletic.
2003.
Re-covering Documentation-to-Source-Code Traceabil-ity Links using Latent Semantic Indexing.
In Pro-ceedings of the 25th International Conference onSoftware Engineering, pages 125?135.Scott McPeak.
2005.
Elsa: TheElkhound-based C/C++ Parser.http://www.cs.berkeley.edu/?smcpeak.Leon Moonen.
2001.
Generating Robust Parsers usingIsland Grammars.
In Proceedings of the 8th Work-ing Conference on Reverse Engineering, pages 13?22.Leon Moonen.
2002.
Lightweight Impact Analysis us-ing Island Grammars.
In Proceedings of the 10th In-ternational Workshop on Program Comprehension,pages 219?228.Gail C. Murphy and David Notkin.
1995.
LightweightSource Model Extraction.
SIGSOFT Software Engi-neering Notes, 20(4):116?127.Jens Nilsson, Welf Lo?we, Johan Hall, and JoakimNivre.
2009.
Natural Language Parsing for Fact Ex-traction from Source Code.
In Proceedings of 17thIEEE International Conference on Program Com-prehension, pages 223?227.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-based Dependency Parsing.
In Proceed-ings of CoNLL, pages 49?56.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.MaltParser: A Data-Driven Parser-Generator forDependency Parsing.
In Proceedings of LREC,pages 2216?2219.Joakim Nivre, Johan Hall, Sanda Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 Shared Task on De-pendency Parsing.
In Proceedings of CoNLL/ACL,pages 915?932.Joakim Nivre.
2003.
An Efficient Algorithm forProjective Dependency Parsing.
In Proceedings ofIWPT, pages 149?160.John Ophel.
1997.
Breadth-FirstParsing.
citeseerx.ist.psu.edu/view-doc/summary?doi=10.1.1.50.3035.Pete Sawyer, Paul Rayson, and Roger Garside.
2002.REVERE: Support for Requirements Synthesisfrom Documents.
Information Systems Frontiers,4(11):343?353.Dennis Strein, Ru?diger Lincke, Jonas Lundberg, andWelf Lo?we.
2007.
An Extensible Meta-Model forProgram Analysis.
IEEE Transactions on SoftwareEngineering, 33(9):592?607.60
