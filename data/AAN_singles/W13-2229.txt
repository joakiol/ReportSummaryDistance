Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 225?231,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsTunable Distortion Limits and Corpus Cleaning for SMTSara Stymne Christian Hardmeier Jo?rg Tiedemann Joakim NivreUppsala UniversityDepartment of Linguistics and Philologyfirstname.lastname@lingfil.uu.seAbstractWe describe the Uppsala University sys-tem for WMT13, for English-to-Germantranslation.
We use the Docent decoder,a local search decoder that translates atthe document level.
We add tunable dis-tortion limits, that is, soft constraints onthe maximum distortion allowed, to Do-cent.
We also investigate cleaning of thenoisy Common Crawl corpus.
We showthat we can use alignment-based filteringfor cleaning with good results.
Finally weinvestigate effects of corpus selection forrecasing.1 IntroductionIn this paper we present the Uppsala Universitysubmission to WMT 2013.
We have submitted onesystem, for translation from English to German.In our submission we use the document-level de-coder Docent (Hardmeier et al 2012; Hardmeieret al 2013).
In the current setup, we take advan-tage of Docent in that we introduce tunable dis-tortion limits, that is, modeling distortion limits assoft constraints instead of as hard constraints.
Inaddition we perform experiments on corpus clean-ing.
We investigate how the noisy Common Crawlcorpus can be cleaned, and suggest an alignment-based cleaning method, which works well.
Wealso investigate corpus selection for recasing.In Section 2 we introduce our decoder, Docent,followed by a general system description in Sec-tion 3.
In Section 4 we describe our experimentswith corpus cleaning, and in Section 5 we describeexperiments with tunable distortion limits.
In Sec-tion 6 we investigate corpus selection for recasing.In Section 7 we compare our results with Docentto results using Moses (Koehn et al 2007).
Weconclude in Section 8.2 The Docent DecoderDocent (Hardmeier et al 2013) is a decoder forphrase-based SMT (Koehn et al 2003).
It differsfrom other publicly available decoders by its useof a different search algorithm that imposes fewerrestrictions on the feature models that can be im-plemented.The most popular decoding algorithm forphrase-based SMT is the one described by Koehnet al(2003), which has become known as stackdecoding.
It constructs output sentences bit bybit by appending phrase translations to an initiallyempty hypothesis.
Complexity is kept in check,on the one hand, by a beam search approach thatonly expands the most promising hypotheses.
Onthe other hand, a dynamic programming techniquecalled hypothesis recombination exploits the lo-cality of the standard feature models, in particu-lar the n-gram language model, to achieve a loss-free reduction of the search space.
While this de-coding approach delivers excellent search perfor-mance at a very reasonable speed, it limits theinformation available to the feature models to ann-gram window similar to a language model his-tory.
In stack decoding, it is difficult to implementmodels with sentence-internal long-range depen-dencies and cross-sentence dependencies, wherethe model score of a given sentence depends onthe translations generated for another sentence.In contrast to this very popular stack decod-ing approach, our decoder Docent implements asearch procedure based on local search (Hard-meier et al 2012).
At any stage of the search pro-cess, its search state consists of a complete docu-ment translation, making it easy for feature mod-els to access the complete document with its cur-rent translation at any point in time.
The searchalgorithm is a stochastic variant of standard hillclimbing.
At each step, it generates a successorof the current search state by randomly applying225one of a set of state changing operations to a ran-dom location in the document.
If the new statehas a better score than the previous one, it is ac-cepted, else search continues from the previousstate.
The operations are designed in such a waythat every state in the search space can be reachedfrom every other state through a sequence of stateoperations.
In the standard setup we use three op-erations: change-phrase-translation replaces thetranslation of a single phrase with another optionfrom the phrase table, resegment alters the phrasesegmentation of a sequence of phrases, and swap-phrases alters the output word order by exchang-ing two phrases.In contrast to stack decoding, the search algo-rithm in Docent leaves model developers muchgreater freedom in the design of their feature func-tions because it gives them access to the transla-tion of the complete document.
On the downside,there is an increased risk of search errors becausethe document-level hill-climbing decoder cannotmake as strong assumptions about the problemstructure as the stack decoder does.
In prac-tice, this drawback can be mitigated by initializingthe hill-climber with the output of a stack decod-ing pass using the baseline set of models withoutdocument-level features (Hardmeier et al 2012).Since its inception, Docent has been used to ex-periment with document-level semantic languagemodels (Hardmeier et al 2012) and models toenhance text readability (Stymne et al 2013b).Work on other discourse phenomena is ongoing.In the present paper, we focus on sentence-internalreordering by exploiting the fact that Docent im-plements distortion limits as soft constraints ratherthan strictly enforced limitations.
We do not in-clude any of our document-level feature functions.3 System SetupIn this section we will describe our basic systemsetup.
We used all corpora made available forEnglish?German by the WMT13 workshop.
Wealways concatenated the two bilingual corpora Eu-roparl and News Commentary, which we will callEP-NC.
We pre-processed all corpora by usingthe tools provided for tokenization and we alsolower-cased all corpora.
For the bilingual corporawe also filtered sentence pairs with a length ra-tio larger than three, or where either sentence waslonger than 60 tokens.
Recasing was performed asa post-processing step, trained using the resourcesin the Moses toolkit (Koehn et al 2007).For the language model we trained two sepa-rate models, one on the German side of EP-NC,and one on the monolingual News corpus.
Inboth cases we trained 5-gram models.
For thelarge News corpus we used entropy-based prun-ing, with 10?8 as a threshold (Stolcke, 1998).
Thelanguage models were trained using the SRILMtoolkit (Stolcke, 2002) and during decoding weused the KenLM toolkit (Heafield, 2011).For the translation model we also trained twomodels, one with EP-NC, and one with CommonCrawl.
These two models were interpolated andused as a single model at decoding time, based onperplexity minimization interpolation (Sennrich,2012), see details in Section 4.
The transla-tion models were trained using the Moses toolkit(Koehn et al 2007), with standard settings with5 features, phrase probabilities and lexical weight-ing in both directions and a phrase penalty.
We ap-plied significance-based filtering (Johnson et al2007) to the resulting phrase tables.
For decod-ing we used the Docent decoder with random ini-tialization and standard parameter settings (Hard-meier et al 2012; Hardmeier et al 2013), whichbeside translation and language model features in-clude a word penalty and a distortion penalty.Parameter optimization was performed usingMERT (Och, 2003) at the document-level (Stymneet al 2013a).
In this setup we calculate bothmodel and metric scores on the document-levelinstead of on the sentence-level.
We produce k-best lists by sampling from the decoder.
In eachoptimization run we run 40,000 hill-climbing it-erations of the decoder, and sample translationswith interval 100, from iteration 10,000.
Thisprocedure has been shown to give competitive re-sults to standard tuning with Moses (Koehn etal., 2007) with relatively stable results (Stymneet al 2013a).
For tuning data we concate-nated the tuning sets news-test 2008?2010 andnewssyscomb2009, to get a higher number of doc-uments.
In this set there are 319 documents and7434 sentences.To evaluate our system we use newstest2012,which has 99 documents and 3003 sentences.
Inthis article we give lower-case Bleu scores (Pap-ineni et al 2002), except in Section 6 where weinvestigate the effect of different recasing models.226Cleaning Sentences ReductionNone 2,399,123Basic 2,271,912 5.3%Langid 2,072,294 8.8%Alignment-based 1,512,401 27.0%Table 1: Size of Common Crawl after the differentcleaning steps and reduction in size compared tothe previous step4 Cleaning of Common CrawlThe Common Crawl (CC) corpus was collectedfrom web sources, and was made available for theWMT13 workshop.
It is noisy, with many sen-tences with the wrong language and also manynon-corresponding sentence pairs.
To make betteruse of this resource we investigated two methodsfor cleaning it, by making use of language identi-fication and alignment-based filtering.
Before anyother cleaning we performed basic filtering wherewe only kept pairs where both sentences had atmost 60 words, and with a length ratio of maxi-mum 3.
This led to a 5.3% reduction of sentences,as shown in Table 1.Language Identification For language identifi-cation we used the off-the-shelf tool langid.py (Luiand Baldwin, 2012).
It is a python library, cover-ing 97 languages, including English and German,trained on data drawn from five different domains.It uses a naive Bayes classifier with a multino-mial event model, over a mixture of byte n-grams.As for many language identification packages itworks best for longer texts, but Lui and Bald-win (2012) also showed that it has good perfor-mance for short microblog texts, with an accuracyof 0.89?0.94.We applied langid.py for each sentence in theCC corpus, and kept only those sentence pairswhere the correct language was identified for bothsentences with a confidence of at least 0.999.
Thetotal number of sentences was reduced by a further8.8% based on the langid filtering.We performed an analysis on a set of 1000 sen-tence pairs.
Among the 907 sentences that werekept in this set we did not find any cases withthe wrong language.
Table 2 shows an analysisof the 93 sentences that were removed from thistest set.
The overall accuracy of langid.py is muchhigher than indicated in the table, however, sinceit does not include the correctly identified Englishand German sentences.
We grouped the removedsentences into four categories, cases where bothlanguages were correctly identified, but under theconfidence threshold of 0.999, cases where bothlanguages were incorrectly identified, and caseswhere one language was incorrectly identified.Overall the language identification was accurateon 54 of the 93 removed sentences.
In 18 of thecases where it was wrong, the sentences were nottranslation correspondents, which means that weonly wrongly removed 21 out of 1000 sentences.It was also often the case when the language waswrongly identified, that large parts of the sentenceconsisted of place names, such as ?Forums aboutConil de la Frontera - Ca?diz.?
?
?Foren u?ber Conilde la Frontera - Ca?diz.
?, which were identified ases/ht instead of en/de.
Even though such sentencepairs do correspond, they do not contain much use-ful translation material.Alignment-Based Cleaning For the alignment-based cleaning, we aligned the data from the pre-vious step using GIZA++ (Och and Ney, 2003)in both directions, and used the intersection ofthe alignments.
The intersection of alignments ismore sparse than the standard SMT symmetriza-tion heuristics, like grow-diag-final-and (Koehn etal., 2005).
Our hypothesis was that sentence pairswith very few alignment points in the intersectionwould likely not be corresponding sentences.We used two types of filtering thresholds basedon alignment points.
The first threshold is for theratio of the number of alignment points and themaximum sentence length.
The second thresholdis the absolute number of alignment points in asentence pair.
In addition we used a third thresh-old based on the length ratio of the sentences.To find good values for the filtering thresholds,we created a small gold standard where we man-ually annotated 100 sentence pairs as being cor-responding or not.
In this set the sentence pairsdid not match in 33 cases.
Table 3 show results forsome different values for the threshold parameters.Overall we are able to get a very high precisionon the task of removing non-corresponding sen-tences, which means that most sentences that areremoved based on this cleaning are actually non-corresponding sentences.
The recall is a bit lower,indicating that there are still non-correspondingsentences left in our data.
In our translation sys-tem we used the bold values in Table 3, since itgave high precision with reasonable recall for theremoval of non-corresponding sentences, meaning227Identification Total Wrong lang.
Non-corr Corr Languages identifiedEnglish and German < 0.999 15 0 7 8Both English and German wrong 6 2 2 2 2:na/es, 2:et/et, 1: es/an, 1:es/htEnglish wrong 13 1 6 6 5: es 4: fr 1: br, it, de, eoGerman wrong 59 51 3 5 51: en 3: es 2:nl 1: af, la, lbTotal 93 54 18 21Table 2: Reasons and correctness for removing sentences based on language ID for 93 sentences out ofa 1000 sentence subset, divided into wrong lang(uage), non-corr(esponding) pairs, and corr(esponding)pairs.Ratio align Min align Ratio length Prec.
Recall F Kept0.1 4 2 0.70 0.77 0.73 70%0.28 4 2 0.94 0.72 0.82 57%0.42 4 2 1.00 0.56 0.72 41%0.28 2 2 0.91 0.73 0.81 59%0.28 6 2 0.94 0.63 0.76 51%0.28 4 1.5 0.94 0.65 0.77 52%0.28 4 3 0.91 0.75 0.82 60%Table 3: Results of alignment-based cleaning for different values of the filtering parameters, with pre-cision, recall and F-score for the identification of erroneous sentence pairs and the percentage of keptsentence pairsthat we kept most correctly aligned sentence pairs.This cleaning method is more aggressive thanthe other cleaning methods we described.
For thegold standard only 57% of sentences were kept,but in the full training set it was a bit higher, 73%,as shown in Table 1.Phrase Table Interpolation To use the CC cor-pus in our system we first trained a separate phrasetable which we then interpolated with the phrasetable trained on EP-NC.
In this way we could al-ways run the system with a single phrase table.
Forinterpolation, we used the perplexity minimizationfor weighted counts method by Sennrich (2012).Each of the four weights in the phrase table, back-ward and forward phrase translation probabilitiesand lexical weights, are optimized separately.
Thismethod minimizes the cross-entropy based on aheld-out corpus, for which we used the concate-nation of all available News development sets.The cross-entropy and the contribution of CCrelative to EP-NC, are shown for phrase transla-tion probabilities in both directions in Table 4.
Thenumbers for lexical weights show similar trends.For each cleaning step the cross-entropy is re-duced and the contribution of CC is increased.
Thedifference between the basic cleaning and langid isvery small, however.
The alignment-based clean-ing shows a much larger effect.
After that cleaningstep the CC corpus has a similar contribution toEP-NC.
This is an indicator that the final cleanedCC corpus fits the development set well.p(S|T ) p(T |S)Cleaning CE IP CE IPBasic 3.18 0.12 3.31 0.06Langid 3.17 0.13 3.29 0.07Alignment-based 3.02 0.47 3.17 0.61Table 4: Cross-entropy (CE) and relative interpo-lation weights (IP) compared to EP-NC for theCommon Crawl corpus, with different cleaningResults In Table 5 we show the translation re-sults with the different types of cleaning of CC,and without it.
We show results of different corpuscombinations both during tuning and testing.
Wesee that we get the overall best result by both tun-ing and testing with the alignment-based cleaningof CC, but it is not as useful to do the extra clean-ing if we do not tune with it as well.
Overall weget the best results when tuning is performed in-cluding a cleaned version of CC.
This setup givesa large improvement compared to not using CC atall, or to use it with only basic cleaning.
There islittle difference in Bleu scores when testing witheither basic cleaning, or cleaning based on lan-guage ID, with a given tuning, which is not sur-prising given their small and similar interpolationweights.
Tuning was, however, not successfulwhen using CC with basic cleaning.Overall we think that alignment-based corpuscleaning worked well.
It reduced the size of thecorpus by over 25%, improved the cross-entropyfor interpolation with the EP-NC phrase-table, and228TestingTuning not used basic langid alignmentnot used 14.0 13.9 13.9 14.0basic 14.2 14.5 14.3 14.3langid 15.2 15.3 15.3 15.3alignment 12.7 15.3 15.3 15.7Table 5: Bleu scores with different types of clean-ing and without Common Crawlgave an improvement on the translation task.
Westill think that there is potential for further improv-ing this filtering and to annotate larger test sets toinvestigate the effects in more detail.5 Tunable Distortion LimitsThe Docent decoder uses a hill-climbing searchand can perform operations anywhere in the sen-tence.
Thus, it does not need to enforce a strictdistortion limit.
In the Docent implementation, thedistortion limit is actually implemented as a fea-ture, which is normally given a very large weight,which effectively means that it works as a hardconstraint.
This could easily be relaxed, however,and in this work we investigate the effects of usingsoft distortion limits, which can be optimized dur-ing tuning, like other features.
In this way long-distance movements can be allowed when they areuseful, instead of prohibiting them completely.
Adrawback of using no or soft distortion limits isthat it increases the search space.In this work we mostly experiment with variantsof one or two standard distortion limits, but with atunable weight.
We also tried to use separate softdistortion limits for left- and right-movement.
Ta-ble 6 show the results with different types of dis-tortion limits.
The system with a standard fixeddistortion limits of 6 has a somewhat lower scorethan most of the systems with no or soft distortionlimits.
In most cases the scores are similar, andwe see no clear affects of allowing tunable lim-its over allowing unlimited distortion.
The systemthat uses two mono-directional limits of 6 and 10has slightly higher scores than the other systems,and is used in our final submission.One possible reason for the lack of effect of al-lowing more distortion could be that it rarely hap-pens that an operator is chosen that performs suchdistortion, when we use the standard Docent set-tings.
To investigate this, we varied the settings ofthe parameters that guide the swap-phrases opera-tor, and used the move-phrases operator instead ofswap-phrases.
None of these changes led to anyDL type Limit BleuNo DL ?
15.5Hard DL 6 15.0One soft DL 6 15.58 14.210 15.5Two soft DLs 4,8 15.56,10 15.7Bidirectional soft DLs 6,10 15.5Table 6: Bleu scores for different distortion limit(DL) settingsimprovements, however.While we saw no clear effects when using tun-able distortion limits, we plan to extend this workin the future to model movement differently basedon parts of speech.
For the English?German lan-guage pair, for instance, it would be reasonable toallow long distance moves of verb groups with noor little cost, but use a hard limit or a high cost forother parts of speech.6 Corpus Selection for RecasingIn this section we investigate the effect of usingdifferent corpus combinations for recasing.
Welower-cased our training corpus, which means thatwe need a full recasing step as post-processing.This is performed by training a SMT system onlower-cased and true-cased target language.
Weused the Moses toolkit to train the recasing systemand to decode during recasing.
We investigate theeffect of using different combinations of the avail-able training corpora to train the recasing model.Table 7 show case sensitive Bleu scores, whichcan be compared to the previous case-insensitivescores of 15.7.
We see that there is a larger effectof including more data in the language model thanin the translation model.
There is a performancejump both when adding CC data and when addingNews data to the language model.
The resultsare best when we include the News data, whichis not included in the English?German translationmodel, but which is much larger than the other cor-pora.
There is no further gain by using News incombination with other corpora compared to usingonly News.
When adding more data to the trans-lation model there is only a minor effect, with thedifference between only using EP-NC and usingall available corpora is at most 0.2 Bleu points.In our submitted system we use the monolingualNews corpus both in the LM and the TM.There are other options for how to treat recas-229Language modelTM EP-NC EP-NC-CC News EP-NC-News EP-NC-CC-NewsEP-NC 13.8 14.4 14.8 14.8 14.8EP-NC-CC 13.9 14.5 14.9 14.8 14.8News 13.9 14.5 14.9 14.9 14.9EP-NC-News 13.9 14.5 14.9 14.9 14.9EP-NC-CC-News 13.9 14.5 14.9 14.9 15.0Table 7: Case-sensitive Bleu scores with different corpus combinations for the language model andtranslation model (TM) for recasinging.
It is common to train the system on true-cased data instead of lower-cased data, which hasbeen shown to lead to small gains for the English?German language pair (Koehn et al 2008).
In thisframework there is still a need to find the correctcase for the first word of each sentence, for whicha similar corpus study might be useful.7 Comparison to MosesSo far we have only shown results using the Do-cent decoder on its own, with a random initializa-tion, since we wanted to submit a Docent-only sys-tem for the shared task.
In this section we alsoshow contrastive results with Moses, and for Do-cent initialized with stack decoding, using Moses,and for different type of tuning.Previous research have shown mixed results forthe effect of initializing Docent with and with-out stack decoding, when using the same featuresets.
In Hardmeier et al(2012) there was a dropof about 1 Bleu point for English?French trans-lation based on WMT11 data when random ini-tialization was used.
In Stymne et al(2013a),on the other hand, Docent gave very similar re-sults with both types of initialization for German?English WMT13 data.
The latter setup is similarto ours, except that no Common Crawl data wasused.The results with our setup are shown in Ta-ble 8.
In this case we lose around a Bleu pointwhen using Docent on its own, without Moses ini-tialization.
We also see that the results are lowerwhen using Moses with the Docent tuning method,or when combining Moses and Docent with Do-cent tuning.
This indicates that the document-level tuning has not given satisfactory results inthis scenario, contrary to the results in Stymne etal.
(2013a), which we plan to explore further infuture work.
Overall we think it is important todevelop stronger context-sensitive models for Do-cent, which can take advantage of the documentcontext.Test system Tuning system BleuDocent (random) Docent 15.7Docent (stack) Docent 15.9Moses Docent 15.9Docent (random) Moses 15.9Docent (stack) Moses 16.8Moses Moses 16.8Table 8: Bleu scores for Docent initialized ran-domly or with stack decoding compared to Moses.Tuning is performed with either Moses or Docent.For the top line we used tunable distortion lim-its 6,10 with Docent, in the other cases a standardhard distortion limit of 6, since Moses does not al-low soft distortion limits.8 ConclusionWe have presented the Uppsala University systemfor WMT 2013.
Our submitted system uses Do-cent with random initialization and two tunabledistortion limits of 6 and 10.
It is trained with theCommon Crawl corpus, cleaned using languageidentification and alignment-based filtering.
Forrecasing we used the monolingual News corpora.For corpus-cleaning, we present a novel methodfor cleaning noisy corpora based on the numberand ratio of word alignment links for sentencepairs, which leads to a large reduction of corpussize, and to small improvements on the transla-tion task.
We also experiment with tunable dis-tortion limits, which do not lead to any consistentimprovements at this stage.In the current setup the search algorithm ofDocent is not strong enough to compete withthe effective search in standard decoders likeMoses.
We are, however, working on developingdiscourse-aware models that can take advantage ofthe document-level context, which is available inDocent.
We also need to further investigate tuningmethods for Docent.230ReferencesChristian Hardmeier, Joakim Nivre, and Jo?rg Tiede-mann.
2012.
Document-wide decoding for phrase-based statistical machine translation.
In Proceed-ings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1179?1190, Jeju Island, Korea.Christian Hardmeier, Sara Stymne, Jo?rg Tiedemann,and Joakim Nivre.
2013.
Docent: A document-leveldecoder for phrase-based statistical machine transla-tion.
In Proceedings of the 51st Annual Meeting ofthe ACL, Demonstration session, Sofia, Bulgaria.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages187?197, Edinburgh, Scotland.Howard Johnson, Joel Martin, George Foster, andRoland Kuhn.
2007.
Improving translation qual-ity by discarding most of the phrasetable.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 967?975, Prague, Czech Republic.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Human Language TechnologyConference of the NAACL, pages 48?54, Edmonton,Alberta, Canada.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descrip-tion for the 2005 IWSLT speech translation evalu-ation.
In Proceedings of the International Workshopon Spoken Language Translation, Pittsburgh, Penn-sylvania, USA.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACL,Demo and Poster Sessions, pages 177?180, Prague,Czech Republic.Philipp Koehn, Abhishek Arun, and Hieu Hoang.2008.
Towards better machine translation quality forthe German-English language pairs.
In Proceedingsof the Third Workshop on Statistical Machine Trans-lation, pages 139?142, Columbus, Ohio, USA.Marco Lui and Timothy Baldwin.
2012. langid.py: Anoff-the-shelf language identification tool.
In Pro-ceedings of the 50th Annual Meeting of the ACL,System Demonstrations, pages 25?30, Jeju Island,Korea.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 42nd Annual Meeting of the ACL, pages 160?167, Sapporo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting of the ACL, pages 311?318, Philadelphia, Pennsylvania, USA.Rico Sennrich.
2012.
Perplexity minimization fortranslation model domain adaptation in statisticalmachine translation.
In Proceedings of the 16thAnnual Conference of the European Associationfor Machine Translation, pages 539?549, Avignon,France.Andreas Stolcke.
1998.
Entropy-based pruning ofbackoff language models.
In Proceedings of theDARPA Broadcast News Transcription and Under-standing Workshop, pages 270?274, Landsdowne,Virginia, USA.Andreas Stolcke.
2002.
SRILM ?
an extensiblelanguage modeling toolkit.
In Proceedings of theSeventh International Conference on Spoken Lan-guage Processing, pages 901?904, Denver, Col-orado, USA.Sara Stymne, Christian Hardmeier, Jo?rg Tiedemann,and Joakim Nivre.
2013a.
Feature weight opti-mization for discourse-level SMT.
In Proceedingsof the ACL 2013 Workshop on Discourse in MachineTranslation (DiscoMT 2013), Sofia, Bulgaria.Sara Stymne, Jo?rg Tiedemann, Christian Hardmeier,and Joakim Nivre.
2013b.
Statistical machine trans-lation with readability constraints.
In Proceedingsof the 19th Nordic Conference on ComputationalLinguistics (NODALIDA?13), pages 375?386, Oslo,Norway.231
