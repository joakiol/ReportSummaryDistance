Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 136?145,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsDistributional Semantics in TechnicolorElia BruniUniversity of Trentoelia.bruni@unitn.itGemma BoledaUniversity of Texas at Austingemma.boleda@utcompling.comMarco BaroniNam-Khanh TranUniversity of Trentoname.surname@unitn.itAbstractOur research aims at building computationalmodels of word meaning that are perceptuallygrounded.
Using computer vision techniques,we build visual and multimodal distributionalmodels and compare them to standard textualmodels.
Our results show that, while visualmodels with state-of-the-art computer visiontechniques perform worse than textual modelsin general tasks (accounting for semantic re-latedness), they are as good or better modelsof the meaning of words with visual correlatessuch as color terms, even in a nontrivial taskthat involves nonliteral uses of such words.Moreover, we show that visual and textual in-formation are tapping on different aspects ofmeaning, and indeed combining them in mul-timodal models often improves performance.1 IntroductionTraditional semantic space models represent mean-ing on the basis of word co-occurrence statistics inlarge text corpora (Turney and Pantel, 2010).
Thesemodels (as well as virtually all work in computa-tional lexical semantics) rely on verbal informationonly, while human semantic knowledge also relieson non-verbal experience and representation (Louw-erse, 2011), crucially on the information gatheredthrough perception.
Recent developments in com-puter vision make it possible to computationallymodel one vital human perceptual channel: vision(Mooney, 2008).
A few studies have begun to usevisual information extracted from images as part ofdistributional semantic models (Bergsma and VanDurme, 2011; Bergsma and Goebel, 2011; Bruni etal., 2011; Feng and Lapata, 2010; Leong and Mihal-cea, 2011).
These preliminary studies all focus onhow vision may help text-based models in generalterms, by evaluating performance on, for instance,word similarity datasets such as WordSim353.This paper contributes to connecting language andperception, focusing on how to exploit visual infor-mation to build better models of word meaning, inthree ways: (1) We carry out a systematic compari-son of models using textual, visual, and both types ofinformation.
(2) We evaluate the models on generalsemantic relatedness tasks and on two specific taskswhere visual information is highly relevant, as theyfocus on color terms.
(3) Unlike previous work, westudy the impact of using different kinds of visualinformation for these semantic tasks.Our results show that, while visual models withstate-of-the-art computer vision techniques performworse than textual models in general semantic tasks,they are as good or better models of the mean-ing of words with visual correlates such as colorterms, even in a nontrivial task that involves nonlit-eral uses of such words.
Moreover, we show that vi-sual and textual information are tapping on differentaspects of meaning, such that they are complemen-tary sources of information, and indeed combiningthem in multimodal models often improves perfor-mance.
We also show that ?hybrid?
models exploit-ing the patterns of co-occurrence of words as tagsof the same images can be a powerful surrogate ofvisual information under certain circumstances.The rest of the paper is structured as follows.
Sec-tion 2 introduces the textual, visual, multimodal,136and hybrid models we use for our experiments.
Wepresent our experiments in sections 3 to 5.
Section6 reviews related work, and section 7 finishes withconclusions and future work.2 Distributional semantic models2.1 Textual modelsFor the current project, we constructed a set oftextual distributional models that implement vari-ous standard ways to extract them from a corpus,chosen to be representative of the state of the art.In all cases, occurrence and co-occurrence statis-tics are extracted from the freely available ukWaCand Wackypedia corpora combined (size: 1.9B and820M tokens, respectively).1 Moreover, in all mod-els the raw co-occurrence counts are transformedinto nonnegative Local Mutual Information (LMI)scores.2 Finally, in all models we harvest vector rep-resentations for the same words (lemmas), namelythe top 20K most frequent nouns, 5K most frequentadjectives and 5K most frequent verbs in the com-bined corpora (for coherence with the vision-basedmodels, that cannot exploit contextual informationto distinguish nouns and adjectives, we merge nom-inal and adjectival usages of the color adjectives inthe text-based models as well).
The same 30K tar-get nouns, verbs and adjectives are also employed ascontextual elements.The Window2 and Window20 models are basedon counting co-occurrences with collocates withina window of fixed width, in the tradition of HAL(Lund and Burgess, 1996).
Window2 recordssentence-internal co-occurrence with the nearest 2content words to the left and right of each target con-cept, a narrow context definition expected to capturetaxonomic relations.
Window20 considers a largerwindow of 20 words to the left and right of the target,and should capture broader topical relations.
TheDocument model corresponds to a ?topic-based?approach in which words are represented as distri-butions over documents.
It is based on a word-by-document matrix, recording the distribution of the1http://wacky.sslmit.unibo.it/2LMI is obtained by multiplying raw counts by PointwiseMutual Information, and it is a close approximation to the Log-Likelihood Ratio (Evert, 2005).
It counteracts the tendency ofPMI to favour extremely rare events.30K target words across the 30K documents in theconcatenated corpus that have the largest cumulativeLMI mass.
This model is thus akin to traditionalLatent Semantic Analysis (Landauer and Dumais,1997), without dimensionality reduction.We add to the models we constructed the freelyavailable Distributional Memory (DM) model,3 thathas been shown to reach state-of-the-art perfor-mance in many semantic tasks (Baroni and Lenci,2010).
DM is an example of a more complex text-based model that exploits lexico-syntactic and de-pendency relations between words (see Baroni andLenci?s article for details), and we use it as an in-stance of a grammar-based model.
DM is basedon the same corpora we used plus the 100M-wordBritish National Corpus,4 and it also uses LMIscores.2.2 Visual modelsThe visual models use information extracted fromimages instead of textual corpora.
We use imagedata where each image is associated with one ormore words or tags (we use ?tag?
for each word as-sociated to the image, and ?label?
for the set of tagsof an image).
We use the ESP-Game dataset,5 con-taining 100K images labeled through a game with apurpose in which two people partnered online mustindependently and rapidly agree on an appropriateword to label randomly selected images.
Once aword is entered by both partners in a certain num-ber of game matches, that word is added to the labelfor that image, and it becomes a taboo word for thefollowing rounds of the game (von Ahn and Dab-bish, 2004).
There are 20,515 distinct tags in thedataset, with an average of 4 tags per image.
Webuild one vector with visual features for each tag inthe dataset.The visual features are extracted with the use ofa standard bag-of-visual-words (BoVW) represen-tation of images, inspired by NLP (Sivic and Zisser-man, 2003; Csurka et al, 2004; Nister and Stewe-nius, 2006; Bosch et al, 2007; Yang et al, 2007).This approach relies on the notion of a common vo-cabulary of ?visual words?
that can serve as discreterepresentations for all images.
Contrary to what hap-3http://clic.cimec.unitn.it/dm4http://www.natcorp.ox.ac.uk/5http://www.espgame.org137pens in NLP, where words are (mostly) discrete andeasy to identify, in vision the visual words need tobe first defined.
The process is completely induc-tive.
In a nutshell, BoVW works as follows.
Fromevery image in a dataset, relevant areas are identifiedand a low-level feature vector (called a ?descriptor?
)is built to represent each area.
These vectors, livingin what is sometimes called a descriptor space, arethen grouped into a number of clusters.
Each clusteris treated as a discrete visual word, and the clusterswill be the vocabulary of visual words used to rep-resent all the images in the collection.
Now, givena new image, the nearest visual word is identifiedfor each descriptor extracted from it, such that theimage can be represented as a BoVW feature vec-tor, by counting the instances of each visual wordin the image (note that an occurrence of a low-leveldescriptor vector in an image, after mapping to thenearest cluster, will increment the count of a singledimension of the higher-level BoVW vector).
In ourwork, the representation of each word (tag) is a alsoa BoVW vector.
The values of each dimension areobtained by summing the occurrences of the relevantvisual word in all the images tagged with the word.Again, raw counts are transformed into Local Mu-tual Information scores.
The process to extract vi-sual words and use them to create image-based vec-tors to represent (real) words is illustrated in Figure1, for a hypothetical example in which there is onlyone image in the collection labeled with the wordhorse.!
!!"#$%&'()*&!$(+%#!!
!!!
"#$%,*&$# - .
/ .!!!!0#%)*&!&#(&#$#1)+)'*1!!!!!!!
!2345!.6.Figure 1: Procedure to build a visual representation for aword, exemplified with SIFT features.We extract descriptor features of two types.6First, the standard Scale-Invariant Feature Trans-form (SIFT) feature vectors (Lowe, 1999; Lowe,2004), good at characterizing parts of objects.
Sec-ond, LAB features (Fairchild, 2005), which encodeonly color information.
We also experimented withother visual features, such as those focusing onedges (Canny, 1986), texture (Zhu et al, 2002), andshapes (Oliva and Torralba, 2001), but they werenot useful for the color tasks.
Moreover, we ex-perimented also with different color scales, such asLUV, HSV and RGB, obtaining significantly worseperformance compared to LAB.
Further details onfeature extraction follow.SIFT features are designed to be invariant to im-age scale and rotation, and have been shown to pro-vide a robust matching across affine distortion, noiseand change in illumination.
The version of SIFT fea-tures that we use is sensitive to color (RGB scale;LUV, LAB and OPPONENT gave worse results).We automatically identified keypoints for each im-age and extracted SIFT features on a regular grid de-fined around the keypoint with five pixels spacing,at four multiple scales (10, 15, 20, 25 pixel radii),zeroing the low contrast ones.
To obtain the visualword vocabulary, we cluster the SIFT feature vec-tors with the standardly used k-means clustering al-gorithm.
We varied the number k of visual wordsbetween 500 and 2,500 in steps of 500.For the SIFT-based representation of images, weused spatial histograms to introduce weak geometry(Grauman and Darrell, 2005; Lazebnik et al, 2006),dividing the image into several (spatial) regions, rep-resenting each region in terms of BoVW, and thenconcatenating the vectors.
In our experiments, thespatial regions were obtained by dividing the imagein 4?
4, for a total of 16 regions (other values and aglobal representation did not perform as well).
Notethat, following standard practice, descriptor cluster-ing was performed ignoring the region partition, butthe resulting visual words correspond to different di-mensions in the concatenated BoVW vectors, de-pending on the region in which they occur.
Con-sequently, a vocabulary of k visual words results inBoVW vectors with k ?
16 dimensions.6We use VLFeat (http://www.vlfeat.org/) for fea-ture extraction (Vedaldi and Fulkerson, 2008).138The LAB color space plots image data in 3 di-mensions along 3 independent (orthogonal) axes,one for brightness (luminance) and two for color(chrominance).
Luminance corresponds closely tobrightness as recorded by the brain-eye system;the chrominance (red-green and yellow-blue) axesmimic the oppositional color sensations the retinareports to the brain (Szeliski, 2010).
LAB featuresare densely sampled for each pixel.
Also here we usethe k-means algorithm to build the descriptor space.We varied the number of k visual words between128 and 1,024 in steps of 128.2.3 Multimodal modelsTo assemble the textual and visual representations inmultimodal semantic spaces, we concatenate the twovectors after normalizing them.
We use the linearweighted combination function proposed by Bruniet al (2011): Given a word that is present both inthe textual model and in the visual model, we sepa-rately normalize the two vectors Ft and Fv and wecombine them as follows:F = ??
Ft ?
(1?
?)?
Fvwhere ?
is the vector concatenate operator.
Theweighting parameter ?
(0 ?
?
?
1) is tuned on theMEN development data (2,000 word pairs; detailson the MEN dataset in the next section).
We find theoptimal value to be close to ?
= 0.5 for most modelcombinations, suggesting that textual and visual in-formation should have similar weight.
Our imple-mentation of the proposed method is open sourceand publicly available.72.4 Hybrid modelsWe further introduce hybrid models that exploit thepatterns of co-occurrence of words as tags of thesame images.
Like textual models, these mod-els are based on word co-occurrence; like visualmodels, they consider co-occurrence in images (im-age labels).
In one model (ESP-Win, analogousto window-based models), words tagging an im-age were represented in terms of co-occurrence withthe other tags in the image label (Baroni and Lenci(2008) are a precedent for the use of ESP-Win).The other (ESP-Doc, analogous to document-based7https://github.com/s2m/FUSEmodels) represented words in terms of their co-occurrence with images, using each image as a dif-ferent dimension.
This information is very easy toextract, as it does not require the sophisticated tech-niques used in computer vision.
We expected thesemodels to perform very bad; however, as we willshow, they perform relatively well in all but one ofthe tasks tested.3 Textual and visual models as generalsemantic modelsWe test the models just presented in two differentways: First, as general models of word meaning,testing their correlation to human judgements onword similarity and relatedness (this section).
Sec-ond, as models of the meaning of color terms (sec-tions 4 and 5).We use one standard dataset (WordSim353) andone new dataset (MEN).
WordSim353 (Finkelsteinet al, 2002) is a widely used benchmark constructedby asking 16 subjects to rate a set of 353 word pairson a 10-point similarity scale and averaging the rat-ings (dollar/buck receives a high 9.22 average rat-ing, professor/cucumber a low 0.31).
MEN is anew evaluation benchmark with a better coverage ofour multimodal semantic models.8 It contains 3,000pairs of randomly selected words that occur as ESPtags (pairs sampled to ensure a balanced range of re-latedness levels according to a text-based semanticscore).
Each pair is scored on a [0, 1]-normalizedsemantic relatedness scale via ratings obtained bycrowdsourcing on the Amazon Mechanical Turk (re-fer to the online MEN documentation for more de-tails).
For example, cold/frost has a high 0.9 MENscore, eat/hair a low 0.1.
We evaluate the modelsin terms of their Spearman correlation to the humanratings.
Our models have a perfect MEN coverageand a coverage of 252 WordSim pairs.We used the development set of MEN to testthe effect of varying the number k of visual wordsin SIFT and LAB.
We restrict the discussion toSIFT with the optimal k (2.5K words) and to LABwith the optimal (256), lowest (128), and highestk (1024).
We report the results of the multimodal8An updated version of MEN is available from http://clic.cimec.unitn.it/?elia.bruni/MEN.html.The version used here contained 10 judgements per word pair.139models built with these visual models and the besttextual models (Window2 and Window20).Columns WS and MEN in Table 1 report corre-lations with the WordSim and MEN ratings, respec-tively.
As expected, because they are more matureand capture a broader range of semantic informa-tion, textual models perform much better than purelyvisual models.
Also as expected, SIFT features out-perform the simpler LAB features for this task.A first indication that visual information helps isthe fact that, for MEN, multimodal models performbest.
Note that all models that are sensitive to vi-sual information perform better for MEN than forWordSim, and the reverse is true for textual models.Because of its design, word pairs in MEN can beexpected to be more imageable than those in Word-Sim, so the visual information is more relevant forthis dataset.
Also recall that we did some parametertuning on held-out MEN data.Surprisingly, hybrid models perform quite well:They are around 10 points worse than textual andmultimodal models for WordSim, and only slightlyworse than multimodal models for MEN.4 Experiment 1: Discovering the color ofconcrete objectsIn Experiment 1, we test the hypothesis that the re-lation between words denoting concrete things andwords denoting their typical color is reflected by thedistance of the corresponding vectors better whenthe models are sensitive to visual information.4.1 MethodTwo authors labeled by consensus a list of concretenouns (extracted from the BLESS dataset9 and thenouns in the BNC occurring with color terms morethan 100 times) with one of the 11 colors fromthe basic set proposed by Berlin and Kay (1969):black, blue, brown, green, grey, orange, pink, pur-ple, red, white, yellow.
Objects that do not havean obvious characteristic color (computer) and thosewith more than one characteristic color (zebra, bear)were eliminated.
Moreover, only nouns covered byall the models were preserved.
The final list con-9http://sites.google.com/site/geometricalmodels/shared-evaluationModel WS MEN E1 E2DM .44 .42 3 (09) .14Document .63 .62 3 (07) .06Window2 .70 .66 5 (13) .49***Window20 .70 .62 3 (11) .53***LAB128 .21 .41 1 (27) .25*LAB256 .21 .41 2 (24) .24*LAB1024 .19 .41 2 (24) .28**SIFT2.5K .33 .44 3 (15) .57***W2-LAB128 .40 .59 1 (27) .40***W2-LAB256 .41 .60 2 (23) .40***W2-LAB1024 .39 .61 2 (24) .44***W20-LAB128 .40 .60 1 (27) .36***W20-LAB256 .41 .60 2 (23) .36***W20-LAB1024 .39 .62 2 (24) .40***W2-SIFT2.5K .64 .69 2.5 (19) .68***W20-SIFT2.5K .64 .68 2 (17) .73***ESP-Doc .52 .66 1 (37) .29*ESP-Win .55 .68 4 (15) .16Table 1: Results of the textual, visual, multimodal, andhybrid models on the general semantic tasks (first twocolumns, section 3; Pearson ?)
and Experiments 1 (E1,section 4) and 2 (E2, section 5).
E1 reports the medianrank of the correct color and the number of top matches(in parentheses), and E2 the average difference in nor-malized cosines between literal and nonliteral adjective-noun phrases, with the significance of a t-test (*** forp< 0.001, ** < 0.01, * < 0.05).tains 52 nouns.10 Some random examples are fog?grey, crow?black, wood?brown, parsley?green, andgrass?green.For evaluation, we measured the cosine of eachnoun with the 11 basic color words in the space pro-duced by each model, and recorded the rank of thecorrect color in the resulting ordered list.4.2 ResultsColumn E1 in Table 1 reports the median rank foreach model (the smaller the rank, the better themodel), as well as the number of exact matches (thatis, number of nouns for which the model ranks thecorrect color first).Discovering knowledge such that grass is greenis arguably a simple task but Experiment 1 shows10Dataset available from the second author?s webpage, underresources.140that textual models fail this simple task, with medianranks around 3.11 This is consistent with the findingsin Baroni and Lenci (2008) that standard distribu-tional models do not capture the association betweenconcrete concepts and their typical attributes.
Visualmodels, as expected, are better at capturing the as-sociation between concepts and visual attributes.
Infact, all models that are sensitive to visual informa-tion achieve median rank 1.Multimodal models do not increase performancewith respect to visual models: For instance, bothW2-LAB128 and W20-LAB128 have the same me-dian rank and number of exact matches as LAB128alone.
Textual information in this case is not com-plementary to visual information, but simply poorer.Also note that LAB features do better than SIFTfeatures.
This is probably due to the fact that Exper-iment 1 is basically about identifying a large patchof color.
The SIFT features we are using are alsosensitive to color, but they seem to be misguided bythe other cues that they extract from images.
Forexample, pigs are pink in LAB space but brown inSIFT space, perhaps because SIFT focused on thecolor of the typical environment of a pig.
We canthus confirm that, by limiting multimodal spaces toSIFT features, as has been done until now in the lit-erature, we are missing important semantic informa-tion, such as the color information that we can minewith LAB.Again we find that hybrid models do very well,in fact in this case they have the top performance,as they perform better than LAB128 (the differ-ence, which can be noticed in the number of exactmatches, is highly significant according to a pairedMann-Whitney test, with p<0.001).5 Experiment 2Experiment 2 requires more sophisticated informa-tion than Experiment 1, as it involves distinguishingbetween literal and nonliteral uses of color terms.11We also experimented with a model based on direct co-occurrence of adjectives and nouns, obtaining promising resultsin a preliminary version of Exp.
1.
We abandoned this approachbecause such a model inherently lacks scalability, as it will notgeneralize behind cases where the training data contain directexamples of co-occurrences of the target pairs.5.1 MethodWe test the performance of the different modelswith a dataset consisting of color adjective-nounphrases, randomly drawn from the most frequent 8Knouns and 4K adjectives in the concatenated ukWaC,Wackypedia, and BNC corpora (four color terms arenot among these, so the dataset includes phrases forblack, blue, brown, green, red, white, and yellowonly).
These were tagged by consensus by two hu-man judges as literal (white towel, black feather)or nonliteral (white wine, white musician, green fu-ture).
Some phrases had both literal and nonliteraluses, such as blue book in ?book that is blue?
vs.?automobile price guide?.
In these cases, only themost common sense (according to the judges) wastaken into account for the present experiment.
Thedataset consists of 370 phrases, of which our modelscover 342, 227 literal and 115 nonliteral.12The prediction is that, in good semantic models,literal uses will in general result in a higher simi-larity between the noun and color term vectors: Awhite towel is white, while wine or musicians arenot white in the same manner.
We test this predictionby comparing the average cosine between the colorterm and the nouns across the literal and nonliteralpairs (similar results were obtained in an evaluationin terms of prediction accuracy of a simple classi-fier).5.2 ResultsColumn E2 in Table 1 summarizes the results ofthe experiment, reporting the mean difference be-tween the normalized cosines (that is, how largethe difference is between the literal and nonliteraluses of color terms), as well as the significance ofthe differences according to a t-test.
Window-basedmodels perform best among textual models, partic-ularly Window20, while the rest can?t discriminatebetween the two uses.
This is particularly strikingfor the Document model, which performs quite wellin general semantic tasks but bad in visual tasks.Visual models are all able to discriminate betweenthe two uses, suggesting that indeed visual infor-mation can capture nonliteral aspects of meaning.However, in this case SIFT features perform muchbetter than LAB features, as Experiment 2 involves12Dataset available upon request to the second author.141tackling much more sophisticated information thanExperiment 1.
This is consistent with the fact that,for LAB, a lower k (lower granularity of the in-formation) performs better for Experiment 1 and ahigher k (higher granularity) for Experiment 2.One crucial question to ask, given the goals ofour research, is whether textual and visual modelsare doing essentially the same job, only using dif-ferent types of information.
Note that, in this case,multimodal models increase performance over theindividual modalities, and are the best models forthis task.
This suggests that the information used inthe individual models is complementary, and indeedthere is no correlation between the cosines obtainedwith the best textual and visual models (Pearson?s?
= .09, p = .11).Figure 2 depicts the results broken down bycolor.13 Both modalities can capture the differ-ences for black and green, probably because nonlit-eral uses of these color terms have also clear textualcorrelates (more concretely, topical correlates, asthey are related to race and ecology, respectively).14Significantly, however, vision can capture nonliteraluses of blue and red, while text can?t.
Note thatthese uses (blue note, shark, shield, red meat, dis-trict, face) do not have a clear topical correlate, andthus it makes sense that vision does a better job.Finally, note that for this more sophisticated task,hybrid models perform quite bad, which shows theirlimitations as models of word meaning.15 Overall,13Yellow and brown are excluded because the dataset containsonly one and two instances of nonliteral cases for these terms,respectively.
The significance of the differences as explained inthe text has been tested via t-tests.14It?s not entirely clear why neither modality can capturethe differences for white; for text, it may be because the non-literal cases are not so tied to race as is the cases for black,but they also contain many other types of nonliteral uses, suchas type-referring (white wine/rice/cell) or metonymical ones(white smile).15The hybrid model that performs best in the color tasks isESP-Doc.
This model can only detect a relation between an ad-jective and a noun if they directly co-occur in the label of at leastone image (a ?document?
in this setting).
The more direct co-occurrences there are, the more related the words will be for themodel.
This works for Exp.
1: Since the ESP labels are lists ofwhat subjects saw in a picture, and the adjectives of Exp.
1 aretypical colors of objects, there is a high co-occurrence, as all butone adjective-noun pairs co-occur in at least one ESP label.
Forthe model to perform well in Exp.
2 too, literal phrases shouldoccur in the same labels and non-literal pairs should not.
Weour results suggest that co-occurrence in an imagelabel can be used as a surrogate of true visual infor-mation to some extent, but the behavior of hybridmodels depends on ad-hoc aspects of the labeleddataset, and, from an empirical perspective, they aremore limited than truly multimodal models, becausethey require large amounts of rich verbal picture de-scriptions to reach good coverage.6 Related workThere is an increasing amount of work in com-puter vision that exploits text-derived informationfor image retrieval and annotation tasks (Farhadiet al, 2010; Kulkarni et al, 2011).
One particu-lar techinque inspired by NLP that has acted as avery effective proxy from CV to NLP is preciselythe BoVW.
Recently, NLPers have begun exploit-ing BoVW to enrich distributional models that rep-resent word meaning with visual features automati-cally extracted from images (Feng and Lapata, 2010;Bruni et al, 2011; Leong and Mihalcea, 2011).
Pre-vious work in this area relied on SIFT features only,whereas we have enriched the visual representationof words with other kinds of features from computervision, namely, color-related features (LAB).
More-over, earlier evaluation of multimodal models hasfocused only on standard word similarity tasks (us-ing mainly WordSim353), whereas we have testedthem on both general semantic tasks and specifictasks that tap directly into aspects of semantics (suchas color) where we expect visual information to becrucial.The most closely related work to ours is that re-cently presented by O?zbal et al (2011).
Like us,O?zbal and colleagues use both a textual model and avisual model (as well as Google adjective-noun co-occurrence counts) to find the typical color of an ob-ject.
However, their visual model works by analyz-ing pictures associated with an object, and determin-ing the color of the object directly by image analysis.We attempt the more ambitious goal of separatelyassociating a vector to nouns and adjectives, and de-find no such difference (89% of adjective-noun pairs co-occurin at least one image in the literal set, 86% in the nonliteral set),because many of the relevant pairs describe concrete conceptsthat, while not necessarily of the ?right?
literal colour, are per-fectly fit to be depicted in images (?blue shark?, ?black boy?,?white wine?
).142L N0.050.100.150.200.250.30Vision: blacklllllllL N0.00.10.20.30.40.5Text: blackL N0.100.150.200.250.300.35Vision: bluellL N0.00.10.20.3Text: bluellL N0.050.150.25Vision: greenlllL N0.000.040.080.12Text: greenL N0.050.100.150.200.250.30Vision: redlllL N0.000.100.200.30Text: redlL N0.050.100.150.200.250.30Vision: whitelllllllL N0.000.050.100.15Text: whiteFigure 2: Discrimination of literal (L) vs. nonliteral (N) uses by the best visual and textual models.termining the color of an object by the nearness ofthe noun denoting the object to the color term.
Inother words, we are trying to model the meaning ofcolor terms and how they relate to other words, andnot to directly extract the color of an object from pic-tures depicting them.
Our second experiment is con-nected to the literature on the automated detection offigurative language (Shutova, 2010).
There is in par-ticular some similarity with the tasks studied by Tur-ney et al (2011).
Turney and colleagues try, amongother things, to distinguish literal and metaphoricalusages of adjectives when combined with nouns, in-cluding the highly visual adjective dark (dark hairvs.
dark humour).
Their method, based on automat-ically quantifying the degree of abstractness of thenoun, is complementary to ours.
Future work couldcombine our approach and theirs.7 ConclusionWe have presented evidence that distributional se-mantic models based on text, while providing agood general semantic representation of word mean-ing, can be outperformed by models using visualinformation for semantic aspects of words wherevision is relevant.
More generally, this suggeststhat computer vision is mature enough to signifi-cantly contribute to perceptually grounded compu-tational models of language.
We have also shownthat different types of visual features (LAB, SIFT)are appropriate for different tasks.
Future researchshould investigate automated methods to discoverwhich (if any) kind of visual information should behighlighted in which task, more sophisticated mul-timodal models, visual properties other than color,and larger color datasets, such as the one recentlyintroduced by Mohammad (2011).AcknowledgmentsE.B.
and M.B.
are partially supported by a GoogleResearch Award.
G.B.
is partially supportedby the Spanish Ministry of Science and Innova-tion (FFI2010-15006, TIN2009-14715-C04-04), theEU PASCAL2 Network of Excellence (FP7-ICT-216886) and the AGAUR (2010 BP-A 00070).
TheE2 evaluation set was created by G.B.
with LouiseMcNally and Eva Maria Vecchi.
Fig.
1 was adaptedfrom a figure by Jasper Uijlings.
G. B. thanks Mar-garita Torrent for taking care of her children whileshe worked hard to meet the Sunday deadline.ReferencesMarco Baroni and Alessandro Lenci.
2008.
Conceptsand properties in word spaces.
Italian Journal of Lin-guistics, 20(1):55?88.Marco Baroni and Alessandro Lenci.
2010.
Dis-tributional Memory: A general framework for143corpus-based semantics.
Computational Linguistics,36(4):673?721.Shane Bergsma and Randy Goebel.
2011.
Using visualinformation to predict lexical preference.
In Proceed-ings of Recent Advances in Natural Language Process-ing, pages 399?405, Hissar.Shane Bergsma and Benjamin Van Durme.
2011.
Learn-ing bilingual lexicons using the visual similarity of la-beled web images.
In Proc.
IJCAI, pages 1764?1769,Barcelona, Spain, July.Brent Berlin and Paul Key.
1969.
Basic Color Terms:Their Universality and Evolution.
University of Cali-fornia Press, Berkeley, CA.Anna Bosch, Andrew Zisserman, and Xavier Munoz.2007.
Image Classification using Random Forests andFerns.
In Computer Vision, 2007.
ICCV 2007.
IEEE11th International Conference on, pages 1?8.Elia Bruni, Giang Binh Tran, and Marco Baroni.
2011.Distributional semantics from text and images.
In Pro-ceedings of the EMNLP GEMS Workshop, pages 22?32, Edinburgh.John Canny.
1986.
A computational approach to edgedetection.
IEEE Trans.
Pattern Anal.
Mach.
Intell,36(4):679?698.Gabriella Csurka, Christopher Dance, Lixin Fan, JuttaWillamowski, and Ce?dric Bray.
2004.
Visual cate-gorization with bags of keypoints.
In In Workshop onStatistical Learning in Computer Vision, ECCV, pages1?22.Stefan Evert.
2005.
The Statistics of Word Cooccur-rences.
Dissertation, Stuttgart University.Mark D. Fairchild.
2005.
Status of cie color appearancemodels.A.
Farhadi, M. Hejrati, M. Sadeghi, P. Young,C.
Rashtchian, J. Hockenmaier, and D. Forsyth.
2010.Every picture tells a story: Generating sentences fromimages.
In Proceedings of ECCV.Yansong Feng and Mirella Lapata.
2010.
Visual infor-mation in semantic representation.
In Proceedings ofHLT-NAACL, pages 91?99, Los Angeles, CA.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2002.
Placing search in context: The conceptrevisited.
ACM Transactions on Information Systems,20(1):116?131.Kristen Grauman and Trevor Darrell.
2005.
The pyramidmatch kernel: Discriminative classification with setsof image features.
In In ICCV, pages 1458?1465.G.
Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. Berg,and T. Berg.
2011.
Baby talk: Understanding andgenerating simple image descriptions.
In Proceedingsof CVPR.Thomas Landauer and Susan Dumais.
1997.
A solu-tion to Plato?s problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychological Review, 104(2):211?240.Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.2006.
Beyond bags of features: Spatial pyramidmatching for recognizing natural scene categories.
InProceedings of the 2006 IEEE Computer Society Con-ference on Computer Vision and Pattern Recognition- Volume 2, CVPR 2006, pages 2169?2178, Washing-ton, DC, USA.
IEEE Computer Society.Chee Wee Leong and Rada Mihalcea.
2011.
Goingbeyond text: A hybrid image-text approach for mea-suring word relatedness.
In Proceedings of IJCNLP,pages 1403?1407, Chiang Mai, Thailand.Max Louwerse.
2011.
Symbol interdependency in sym-bolic and embodied cognition.
Topics in CognitiveScience, 3:273?302.David Lowe.
1999.
Object Recognition from LocalScale-Invariant Features.
Computer Vision, IEEE In-ternational Conference on, 2:1150?1157 vol.2, Au-gust.David Lowe.
2004.
Distinctive image features fromscale-invariant keypoints.
International Journal ofComputer Vision, 60(2), November.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, 28:203?208.Saif Mohammad.
2011.
Colourful language: Measuringword-colour associations.
In Proceedings of the 2ndWorkshop on Cognitive Modeling and ComputationalLinguistics, pages 97?106, Portland, Oregon.Raymond J. Mooney.
2008.
Learning to connect lan-guage and perception.David Nister and Henrik Stewenius.
2006.
Scalablerecognition with a vocabulary tree.
In Proceedingsof the 2006 IEEE Computer Society Conference onComputer Vision and Pattern Recognition - Volume 2,CVPR ?06, pages 2161?2168.Aude Oliva and Antonio Torralba.
2001.
Modeling theshape of the scene: A holistic representation of thespatial envelope.
Int.
J. Comput.
Vision, 42:145?175.Go?zde O?zbal, Carlo Strapparava, Rada Mihalcea, andDaniele Pighin.
2011.
A comparison of unsupervisedmethods to associate colors with words.
In Proceed-ings of ACII, pages 42?51, Memphis, TN.Ekaterina Shutova.
2010.
Models of metaphor in NLP.In Proceedings of ACL, pages 688?697, Uppsala, Swe-den.Josef Sivic and Andrew Zisserman.
2003.
Video Google:A text retrieval approach to object matching in videos.In Proceedings of the International Conference onComputer Vision, volume 2, pages 1470?1477, Octo-ber.144Richard Szeliski.
2010.
Computer Vision : Algorithmsand Applications.
Springer-Verlag New York Inc.Peter Turney and Patrick Pantel.
2010.
From frequencyto meaning: Vector space models of semantics.
Jour-nal of Artificial Intelligence Research, 37:141?188.Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-hen.
2011.
Literal and metaphorical sense identifi-cation through concrete and abstract context.
In Pro-ceedings of EMNLP, pages 680?690, Edinburgh, UK.Andrea Vedaldi and Brian Fulkerson.
2008.
VLFeat:An open and portable library of computer vision algo-rithms.
http://www.vlfeat.org/.Luis von Ahn and Laura Dabbish.
2004.
Labeling im-ages with a computer game.
In Proceedings of theSIGCHI Conference on Human Factors in ComputingSystems, pages 319?326, Vienna, Austria.Jun Yang, Yu-Gang Jiang, Alexander G. Hauptmann, andChong-Wah Ngo.
2007.
Evaluating bag-of-visual-words representations in scene classification.
In Mul-timedia Information Retrieval, pages 197?206.Song Chun Zhu, Cheng en Guo, Ying Nian Wu, andYizhou Wang.
2002.
What are textons?
In ComputerVision - ECCV 2002, 7th European Conference onComputer Vision, Copenhagen, Denmark, May 28-31,2002, Proceedings, Part IV, pages 793?807.
Springer.145
