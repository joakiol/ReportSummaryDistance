Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 202?210,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPVoted NER System using Appropriate Unlabeled DataAsif EkbalDept.
of Computer Science &Engg.,Jadavpur University, Kolkata-700032,Indiaasif.ekbal@gmail.comSivaji BandyopadhyayDept.
of Computer Science &Engg.,Jadavpur University, Kolkata-700032,Indiasivaji_cse_ju@yahoo.comAbstractThis paper reports a voted Named Entity Rec-ognition (NER) system with the use of appro-priate unlabeled data.
The proposed method isbased on the classifiers such as Maximum En-tropy (ME), Conditional Random Field (CRF)and Support Vector Machine (SVM) and hasbeen tested for Bengali.
The system makes useof the language independent features in theform of different contextual and orthographicword level features along with the languagedependent features extracted from the Part ofSpeech (POS) tagger and gazetteers.
Contextpatterns generated from the unlabeled data us-ing an active learning method have been usedas the features in each of the classifiers.
Asemi-supervised method has been used to de-scribe the measures to automatically select ef-fective documents and sentences from unla-beled data.
Finally, the models have beencombined together into a final system byweighted voting technique.
Experimental re-sults show the effectiveness of the proposedapproach with the overall Recall, Precision,and F-Score values of 93.81%, 92.18% and92.98%, respectively.
We have shown how thelanguage dependent features can improve thesystem performance.1 IntroductionNamed Entity Recognition (NER) is an impor-tant tool in almost all Natural Language Process-ing (NLP) application areas.
Machine learning(ML) approaches are more popularly used inNER because these are easily trainable, adopt-able to different domains and languages as wellas their maintenance are also less expensive.Some of the very effective ML approaches usedin NER are ME (Borthwick, 1999), CRF(Lafferty et al, 2001) and SVM (Yamada et al,2002).
In the earlier work (Florian et al, 2003), ithas been shown that combination of several MLmodels yields better performance than any singleML model.
One drawback of the ML techniquesto NLP tasks is the requirement of a largeamount of annotated data to achieve a reasonableperformance.Indian languages are resource-constrained andthe manual preparation of NE annotated data isboth time consuming and cost intensive.
It is im-portant to decide how the system should effec-tively select unlabeled data and how the size andrelevance of data impact the performance.
Indiais a multilingual country with great cultural di-versities.
Named Entity (NE) identification inIndian languages in general and Bengali in par-ticular is difficult and challenging as:1.
Unlike English and most of the Europeanlanguages, Bengali lacks capitalization infor-mation, which plays a very important role inidentifying NEs.2.
Indian person names are generally found inthe dictionary as common nouns with somespecific meanings.
For example, kabitA[Kabita] is a person name and can also befound in the dictionary as a common noun withthe meaning ?poem?.3.
Bengali is an inflectional language provid-ing one of the richest and most challenging setsof linguistic and statistical features resulting inlong and complex wordforms.
For example, theperson name sachin [root] can appear as sa-chiner [inflection:-er], sachInke [inflection:-ke], sachInbAbu [inflection: -bAbu], sachIndA[ inflection:-dA] etc.
The location name kol-kAtA [root] can appear in different wordformslike kolkAtAr  [inflection:-r], kolkAtAte [inflec-tion:-te], kolkAtAi  [inflection:-i] etc.4.
Bengali is a relatively free phrase order lan-guage.
Thus, NEs can appear in any position ofthe sentence making the NER task more diffi-cult.5.
Bengali, like other Indian languages, is a re-source-constrained language.
The annotatedcorpus, name dictionaries, good morphological202analyzers, POS taggers etc.
are not yet avail-able in the required measure.6.
Although Indian languages have a very oldand rich literary history, technological devel-opments are of recent origin.7.
Web sources for name lists are available inEnglish, but such lists are not available in Ben-gali.
This necessitates the use of transliterationfor creating such lists.A HMM based NER system for Bengali hasbeen reported in Ekbal et al (2007b), where ad-ditional contextual information has been consid-ered during emission probabilities and NE suf-fixes are used for handling the unknown words.More recently, the works in the area of BengaliNER can be found in Ekbal et al (2008a), andEkbal and Bandyopadhyay (2008b) with the CRF,and SVM approach, respectively.
Other thanBengali, the works on Hindi can be found in Liand McCallum (2004) with CRF and Saha et al(2008) with a hybrid feature set based ME ap-proach.
Various works of NER involving Indianlanguages are reported in IJCNLP-08 NERShared Task on South and South East AsianLanguages (NERSSEAL) 1  using various tech-niques.2 Named Entity Recognition in BengaliWe have used a Bengali news corpus (Ekbal andBandyopadhyay, 2008c), developed from theweb-archive of a widely read Bengali newspaperfor NER.
A portion of this corpus containing200K wordforms has been manually annotatedwith the four NE tags namely, Person, Location,Organization and Miscellaneous.
We have alsoused the NE annotated data of 122K wordforms,collected from the NERSSEAL shared task.
Theshared task data was originally annotated with afine-grained NE tagset of twelve tags.
We con-sider only those tags that represent person, loca-tion, organization, and miscellaneous names(NEN [number], NEM [Measurement] and NETI[Time]).
Other tags have been mapped to theNNE tags that represent the ?other-than-NE?category.
In order to properly denote the bounda-ries of NEs, four NE tags are further divided intothe following forms:B-XXX: Beginning of a multiword NE, I-XXX: Internal of a multiword NE consisting ofmore than two words, E-XXX: End of a multi-word NE, XXX?PER/LOC/ORG/MISC.
Forexample, the name sachin ramesh tendulkar is1 http://ltrc.iiit.ac.in/ner-ssea-08/proc/index.htmltagged as sachin/B-PER ramesh/I-PER tendul-kar/E-PER.
The single word NE is tagged as,PER: Person name, LOC: Location name, ORG:Organization name and MISC: Miscellaneousname.
In the output, sixteen NE tags are replacedwith the four NE tags.2.1 Our ApproachesInitially, we started with the development of aNER system using an active learning method.This is used as the baseline model.
Four super-vised NER systems based on ME, CRF and SVMhave been developed.
Two different systems withthe SVM model, one using forward parsing(SVM-F) that parses from left to right and otherusing backward parsing (SVM-B) that parsesfrom right to left, have been developed.
TheSVM system has been developed based on(Valdimir, 1995), which perform classificationby constructing an N-dimensional hyperplanethat optimally separates data into two categories.We have used YamCha toolkit (http://chasen-org/~taku/software/yamcha), an SVM based toolfor detecting classes in documents and formulat-ing the NER task as a sequential labeling prob-lem.
Here, the pairwise multi-class decisionmethod and polynomial kernel function havebeen used.
The TinySVM-0.02 classifier has beenused for classification.
The C++ based CRF++package (http://crfpp.sourceforge.net) and theC++ based ME package 3 have been used for NER.Performance of the supervised NER models islimited in part by the amount of labeled trainingdata available.
A part of the available unlabeledcorpus (Ekbal and Bandyopadhyay, 2008c) hasbeen used to address this problem.
Based on theoriginal training on the labeled corpus, there willbe some tags in the unlabeled corpus that thetaggers will be very sure about.
We have pro-posed a semi-supervised learning technique thatselects appropriate data from the available largeunlabeled corpora and adds to the initial trainingset in order to improve the performance of thetaggers.
The models are retrained with this newtraining set and this process is repeated in a boot-strapped manner.2.2 Named Entity FeaturesThe main features for the NER task have beenidentified based on the different possible combi-nations of available word and tag contexts.
In2http://cl.aist-nara.ac.jp/~taku ku/software/TinySVM3http://homepages.inf.ed.ac.uk/s0450736/software/maxent/maxent-20061005.tar.bz2203addition to these, various gazetteer lists havebeen developed for use in the NER tasks.The set of features ?F?
contains language inde-pendent as well as language dependent features.The set of language independent features in-cludes the context words, fixed length prefixesand suffixes of all the words, dynamic NE infor-mation of the previous word(s), first word, lengthof the word, digit and infrequent word informa-tion.
Language dependent features include the setof known suffixes that may appear with the vari-ous NEs, clue words that help in predicting thelocation and organization names, words that helpto recognize measurement expressions, designa-tion words that help to identify person names,various gazetteer lists that include the firstnames, middle names, last names, locationnames, organization names, function words,weekdays and month names.
We have also usedthe part of speech (POS) information of the cur-rent and/or the surrounding word(s) as the fea-tures.Language independent NE features can be ap-plied for NER in any language without any priorknowledge of that language.
The lists or gazet-teers are basically language dependent at thelexical level and not at the morphology or syntaxlevel.
Also, we include the POS information inthe set of language dependent features as thePOS information depends on some language spe-cific phenomenon such as person, number, tense,gender etc.
Also, the particular POS tagger, usedin this work, makes use of the several languagespecific resources such as lexicon, inflection listsand a NER system to improve its performance.Evaluation results have demonstrated that the useof language specific features is helpful to im-prove the performance of the NER system.
In theresource-constrained Indian language environ-ment, the non-availability of language specificresources acts as a stimulant for the developmentof such resources for use in NER systems.
Thisleads to the necessity of apriori knowledge of thelanguage.
The features are described below verybriefly.
?Context words: Such words include the pre-ceding and succeeding words of the currentword.
This is based on the observation that thesurrounding words carry effective informationfor the identification of NEs.
?Word suffix and prefix: Fixed length wordsuffixes and prefixes are helpful to identify NEs.In addition, variable length word suffixes arealso used.
Word suffixes and prefixes are the ef-fective features and work well for the inflectiveIndian languages like Bengali.
?Named Entity Information: This is the onlydynamic feature in the experiment.
The previousword NE tag is very informative in deciding thecurrent word NE tag.
?First word (binary valued): This featurechecks whether the current token is the first wordof the sentence or not.
Though Bengali is a rela-tively free phrase order language, the first wordof the sentence is most likely a NE as it appearsmost of the time in the subject position.
?Length of the word (binary valued): This fea-ture checks whether the length of the token isless than three or not.
We have observed thatvery short words are most probably not the NEs.
?Infrequent word (binary valued): A cut offfrequency has been chosen in order to considerthe infrequent words in the training corpus.
Thisis based on the observation that the infrequentwords are rarely NEs.
?Digit features: Several digit features havebeen considered depending upon the presenceand/or the number of digit(s) in a token.
Thesebinary valued features are helpful in recognizingmiscellaneous NEs such as time, monetary anddate expressions, percentages, numerical num-bers etc.
?Position of the word (binary valued):  Posi-tion of the word (whether last word or not) in asentence is a good indicator of NEs.
?Part of Speech (POS) Information: We haveused an SVM-based POS tagger (Ekbal andBandyopadhyay, 2008d) that was originally de-veloped with 26 POS tags, defined for the Indianlanguages.
For SVM models, we have used thisPOS tagger.
However, for the ME and CRFmodels, we have considered a coarse-grainedPOS tagger that has the following tags: Nominal,PREP (Postpositions) and Other.
?Gazetteer Lists: Gazetteer lists, developedmanually as well as semi-automatically from thenews corpus (Ekbal and Bandyopadhyay, 2008c),have been used as the features in each of theclassifiers.
The set of gazetteers along with thenumber of entries are as follows:(1).
Organization clue word (e.g., ko.m [Co.],limited [Limited] etc): 94, Person prefix words(e.g., shrimAn [Mr.], shrImati [Mrs.] etc.
): 145,Middle names: 2,491, Surnames: 5,288, NE suf-fixes (e.g., -bAbu [-babu], -dA [-da], -di [-di] forperson and  -lyAnd [-land] -pur[-pur],  -liyA [-lia]etc for location):115, Common location (e.g.,sarani [Sarani], roDa [Road] etc.
): 147, Action204verb (e.g., balen [says], ballen [told] etc.
):141,Function words:743, Designation words (e.g.,netA[leader], sA.msad [MP] etc.
): 139, Firstnames:72,206, Location names:7,870, Organiza-tion names:2,225, Month name (English andBengali calendars):24, Weekdays (English andBengali calendars):14(2).
Common word (521 entries): Most of theIndian language NEs appears in the dictionarywith some meanings.
For example, the word ka-mol may be the name of a person but also ap-pears in the dictionary with another meaning lo-tus, the name of a flower; the word dhar may bea verb and also can be the part of a person name.We have manually created a list, containing thewords that can be NEs as well as valid dictionarywords.3  Active Learning Method for BaselineNER SystemWe have used a portion, containing 35,143 newsdocuments and approximately 10 million word-forms, of the Bengali news corpus (Ekbal andBandyopadhyay, 2008c) for developing the base-line NER system.The frequently occurring words have been col-lected from the reporter, location and agencytags of the Bengali news corpus.
The unlabeledcorpus is tagged with the elements from the seedlists.
In addition, various gazetteers have beenused that include surname, middle name, personprefix words, NE suffixes, common location anddesignations for further tagging of the NEs in thetraining corpus.
The following linguistic ruleshave been used to tag the training corpus:(i).
If there are two or more words in a se-quence that represent the characters of Bengali orEnglish alphabet, then such words are part ofNEs.
For example, bi e (B A), ci em di e (C M DA), bi je pi (B J P) are all NEs.(ii).
If at the end of a word, there are strings like- era(-er),  -eraa (-eraa),  -ra (-ra), -rA (-raa), -ke(-ke), -dera (-der) then the word is likely to be aperson name.(iii).
If a clue word like saranI (sarani), ro.Da(road), lena (lane) etc.
is found after an unknownword then the unknown word along with the clueword may be a location name.(iv).
A few names or words in Bengali consistof the characters chandrabindu or khanda ta.
So,if a particular word W is not identified as NE byany of the above rules but includes any of thesetwo characters, then W may be a NE.
Forexample o.NrI (onry) is a person name.(v).
The set of action verbs like balen (says),ballen (told), ballo (told), shunla (heared),ha.Nslo (haslo) etc.
often determines thepresence of person names.
If an unknown wordW appears in the sentence followed by the actionverbs, then W is most likely a person name.Otherwise, W is not likely to be a NE.(vi).
If there is reduplication of a word W in asentence then W is not likely to be a NE.
This isso because rarely name words are reduplicated.In fact, reduplicated name words may signifysomething else.
For example, rAm rAm (ramram)  is used to greet a person.(vii).
If at the end of any word W there aresuffixes like -gulo (-gulo), -guli (guli), -khAnA (-khana) etc., then W is not a NE.For each tag T inserted in the training corpus,the algorithm generates a lexical pattern p usinga context window of maximum width 6 (exclud-ing the tagged NE) around the left and the righttags, e.g.,p = [l-3l-2 l-1  <T> ...</T> l+1 l+2 l+3],where, l?i   are the context of p. All these pat-terns, derived from the different tags of the la-beled and unlabeled training corpora, are storedin a Pattern Table (or, set P), which has four dif-ferent fields namely, pattern id (identifies anyparticular pattern), pattern example (pattern), pat-tern type (Person/Location/Organization) andrelative frequency (indicates the number of timesany pattern of a particular type appears in theentire training corpus relative to the total numberof patterns generated of that type).
This table has20,967 distinct entries.Every pattern p in the set P is matched againstthe same unlabeled corpus.
In a place, where thecontext of p matches, p predicts the occurrenceof the left or right boundary of name.
POS in-formation of the words as well as some linguisticrules and/or length of the entity have been usedin detecting the other boundary.
The extractedentity may fall in one of the following categories:?
positive example: The extracted entity isof the same NE type as that of the pattern.?
negative example: The extracted entity isof the different NE type as that of the pattern.?
error example: The extracted entity isnot at all a NE.The type of the extracted entity is determinedby checking whether it appears in any of the seedlists; otherwise, its type is determined manually.The positive and negative examples are thenadded to the appropriate seed lists.
The accuracyof the pattern is calculated as follows:205accuracy(p)= |positive (p)|/[| positive (p)| +|negative (p)| + |error(p)|]A threshold value of accuracy has been cho-sen in order to discard the patterns below thisthreshold.
A pattern is also discarded if its totalpositive count is less than a predeterminedthreshold value.
The remaining patterns areranked by their relative frequency values.
The ntop high frequent patterns are retained in the pat-tern set P and this set is denoted as Accept Pat-tern.All the positive and negative examples ex-tracted by a pattern p can be used to generatefurther patterns from the same training corpus.Each new positive or negative instance (not ap-pearing in the seed lists) is used to further tag thetraining corpus.
We repeat the previous steps foreach new NE until no new patterns can be gener-ated.
A newly generated pattern may be identicalto a pattern that is already in the set P. In such acase, the type and relative frequency fields in theset P are updated accordingly.
Otherwise, thenewly generated pattern is added to the set withthe type and relative frequency fields set prop-erly.
The algorithm terminates after 13 iterationsand there are 20,176 distinct entries in the set P.4 Semi-supervised Approach for Unla-beled Document and Sentence Selec-tionA method for automatically selecting the appro-priate unlabeled data from a large collection ofunlabeled documents for NER has been de-scribed in Ekbal and Bandyopadhyay (2008e).This work reported the selection of unlabeleddocuments based on the overall F-Score value ofthe individual system.
In this work, the unlabeleddocuments have been selected based on the Re-call, Precision as well as the F-Score values ofthe participating systems.
Also, we have consid-ered only the SVM-F model trained with the lan-guage independent, language dependent and con-text features for selecting the appropriate sen-tences to be included into the initial training data.The use of single model makes the training fastercompared to Ekbal and Bandyopadhyay (2008e).The SVM-F model has been considered as itproduced the best results for the development setas well as during the 10-fold cross validation test.The unlabeled 35,143 news documents have beendivided based on news sources/types in order tocreate segments of manageable size, separatelyevaluate the contribution of each segment using agold standard development test set and rejectthose that are not helpful and to apply the latestupdated best model to each subsequent segment.It has been observed that incorporation of unla-beled data can only be effective if it is related tothe target problem, i.e., the test set.
Once the ap-propriate documents are selected, it is necessaryto select the tagged sentences that are useful toimprove both the Recall and Precision values ofthe system.
Appropriate sentences are selectedusing the SVM-F model depending upon thestructure and/or contents of the sentences.4.1 Unlabeled Document SelectionThe unlabeled data supports the acquisition ofnew names and contexts to provide new evi-dences to be incorporated in the models.
Unla-beled data can degrade rather than improve theclassifier?s performance on the test set if it is ir-relevant to the test document.
So, it is necessaryto measure the relevance of the unlabeled data toour target test set.
We construct a set of keywords from the test set T to check whether anunlabeled document d is useful or not.?
We do not use all words in the test set T asthe key words since we are only concernedabout the distribution of name candidates.So, each document is tested with the CRFmodel using the language independent fea-tures, language dependent features and thecontext features.?
We take all the name candidates in the top Nbest hypotheses (N=10) for each sentence ofthe test set T to construct a query set Q. Us-ing this query set, we find all the relevantdocuments that include three (heuristicallyset) names belonging to the set Q.
In addi-tion, the documents are not considered ifthey contain fewer than seven (heuristic)names.4.2 Sentence SelectionAll the tagged sentences of a relevant documentare not added to training corpus as incorrectlytagged or irrelevant sentences can lead to thedegradation in model performance.
Our mainconcern is on how much new information is ex-tracted from each sentence of the unlabeled datacompared to the training corpus that already wehave in our hand.The SVM-F model has been used to select therelevant sentences.
All the relevant documentsare tagged with the SVM-F model developedwith the language independent, language de-206pendent and context features along with the classdecomposition technique.
If both Recall and Pre-cision values of the SVM-F model increase thenthat sentence is selected to be added to the initialtraining corpus.
A close investigation reveals thefact that this criterion often selects a number ofsentences which are too short or do not includeany name.
These words may make the modelworse if added to the training data.
For example,the distribution of non-names may increase sig-nificantly that may lead to degradation of modelperformance.
In this experiment, we have notincluded the sentences that include fewer thanfive words or do not include any names.
Thebootstrapping procedure is given as follows:1.
Select a relevant document RelatedDfrom a large corpus of unlabeled datawith respect to the test set T using thedocument selection method described inSection 4.1.2.
Split RelatedD into n subsets and markthem C1, C2, ?., Cn.3.
Call the development set DevT.4.
For I=1 to n4.1.
Run SVM-F model, developed with thelanguage independent features, languagedependent feature and context featuresalong with the class decomposition tech-nique, on Ci.4.2.
If the length of each tagged sentence S isless than five or it does not contain anyname then discard S.4.3.
Add Ci to the training data and retrainSVM-F model.
This produces the up-dated model.4.4.
Run the updated model on DevT; if theRecall and Precision values reduce thendon?t use Ci and use the old model.5.
Repeat steps 1-4 until Recall and Precisionvalues of the SVM-F model either become equalor differ by some threshold values (set to 0.01) inconsecutive two iterations.5 Evaluation Results and DiscussionsOut of 200K wordforms, 150K wordforms alongwith the IJCNLP-08 shared task data has beenused for training the models.
Out of 200K word-forms, 50K wordforms have been used as thedevelopment data.
The system has been testedwith a gold standard test set of 35K wordforms.Each of the models has been evaluated in twodifferent ways, being guided by language inde-pendent features (language independent systemdenoted as LI) and being guided by languageindependent as well as language dependent fea-tures (language dependent system denoted asLD).5.1 Language Independent EvaluationA number of experiments have been carried outin order to identify the best-suited set of lan-guage independent features for NER in each ofmodels.
Evaluation results of the developmentset for the NER models are presented in Table 1in terms of percentages of Recall (R), Precision(P) and F-Score (FS).
The ME based system hasdemonstrated the F-Score value of 74.67% forthe context word window of size three, i.e., pre-vious one word, current word and the next word,prefixes and suffixes of length up to three char-acters of only the current word, dynamic NE tagof the previous word, first word, infrequent word,length and the various digit features.
The CRFbased system yielded the highest F-Score valueof 76.97% for context window of size five, i.e.,two preceding, current and two succeeding wordsalong with the other set of features as in the MEmodel.
Both the SVM based systems have dem-onstrated the best performance for the contextwindow of size seven, i.e., three preceding, cur-rent and two succeeding words, dynamic NE in-formation of the previous two words along withthe other set of features as in the ME and CRFbased systems.
In SVM models, we have con-ducted experiments with the different polynomialkernel functions and observed the highest F-Score value with degree 2.
It has been also ob-served that pairwise multiclass decision methodperforms better than the one vs rest method.
Forall the models, context words and prefixes and/orsuffixes have been found to be the most effectivefeatures.Model R  P  FSME 76.82 72.64 74.67CRF 78.17 75.81 76.97SVM-F 79.14 77.26 78.19SVM-B 79.09 77.15 78.11Table 1.
Results on the development set forthe language independent supervised models5.2 Language Dependent EvaluationEvaluation results of the systems that include thePOS information and other language dependentfeatures are presented in the Table 2.
During theexperiments, it has been observed that all thelanguage dependent features are not equally im-portant.
POS information is the most effective207followed by NE suffixes, person prefix words,designations, organization clue words and loca-tion clue words.
Table 1 and Table 2 show thatthe language dependent features can improve theoverall performance of the systems significantly.Model R  P  FSME 87.02 80.77 83.78CRF 87.63 84.03 85.79SVM-F 87.74 85.89 86.81SVM-B 87.69 85.17 86.72Table 2.
Results on the development set for thelanguage dependent supervised models5.3 Use of Context Features as FeaturesNow, the high ranked patterns of the Accept Pat-tern set (Section 3) can be used as the features ofthe individual classifier.
A feature ?ContextInf?
isdefined by observing the three preceding andsucceeding words of the current word.
Evalua-tion results are presented in Table 3.
Clearly, it isevident from the results of Table 2 and Table 3that context features are very effective to im-prove the Precision values in each of the models.Model R  P  FSME 88.22 83.71 85.91CRF 89.51 85.94 87.69SVM-F 89.67 86.49 88.05SVM-B 89.61 86.47 88.01Table 3.
Results on the development set by in-cluding context features5.4 Results on the Test SetA gold standard test set of 35K wordforms hasbeen used to report the evaluation results.
Themodels have been trained with the language in-dependent, language dependent and the contextfeatures.
Results have been presented in Table 4for the test set.
In the baseline model, each pat-tern of the Accept Pattern set is matched againstthe test set.
Results show that SVM-F model per-forms best for the test set.Error analyses have been conducted with thehelp of confusion matrix.
In order to improve theperformance of the classifiers, we have usedsome post-processing techniques.Output of the ME based system has been post-processed with a set of heuristics (Ekbal andBandyopadhyay, 2009) to improve the perform-ance further.
The post-processing as described inEkbal and Bandyopadhyay (2008e) tries to as-sign the correct tag according to the n-best re-sults for every sentence of the test set in the CRFframework.
In order to remove the unbalancedclass distribution between names and non-namesin the training set, we have considered the classdecomposition technique (Ekbal and Bandyop-adhyay, 2008e) for SVM.
Evaluation results ofthe post-processed systems are presented in Ta-ble 5.Model R  P  FSBaseline 68.11 71.37 69.32ME 86.04 84.98 85.51CRF 87.94 87.12 87.53SVM-F 89.91 85.97 87.89SVM-B 89.82 85.93 87.83Table 4.
Results on the test setModel R  P  FSME 87.29 86.81 87.05CRF 89.19 88.85 89.02SVM-F 90.23 88.62 89.41SVM-B 90.05 88.61 89.09Table 5.
Results of the post-processed modelson the test setEach of the models has been also evaluated forthe 10-fold cross validation tests.
Initially all themodels have been developed with the languageindependent features along with the context fea-tures.
Then, language dependent features havebeen included into the models.
In each run of the10 tests, the outputs have been post-processedwith the several post-processing techniques asdescribed earlier.
Results are shown in Table 6.Model R  P  FSME  81.34 79.01 80.16CRF 82.66 80.75 81.69SVM-F 83.87 81.83 82.83LISVM-B 83.87 81.77 82.62ME  87.54 87.97 87.11CRF 89.5 88.73 89.19SVM-F 89.97 88.61 89.29LDSVM-B 89.76 88.51 89.13Table 6.
Results of the 10-fold cross validationtestsStatistical ANOVA tests (Anderson andScolve, 1978) demonstrated that the performanceimprovement in each of the language dependentmodel is statistically significant over the lan-guage independent model.
We have also carriedout the statistical tests to show that performanceimprovement in CRF over ME and SVM-F overCRF are statistically significant.2085.5 Impact of Unlabeled Data SelectionIn order to investigate the contribution ofdocument selection in bootstrapping, the post-processed models are run on 35,143 newsdocuments.
This yields the gradually improvingperformance for the SVM-F model as shown inTable 7.
After selection of the appropriateunlabeled data, all the models have beenretrained by including the unlabeled documents.Results have been presented in Table 8.Itera-tionSentencesaddedR  P FS0 0 89.97 88.61 89.291 129 90.19 88.97 89.582 223 90.62 89.14 89.873 332 90.89 89.73 90.314 416 91.24 90.11 90.675 482 91.69 90.65 91.166 543 91.88 90.97 91.427 633 92.07 91.05 91.568 682 92.33 91.31 91.829 712 92.52 91.39 91.9510 723 92.55 91.44 91.9911 729 92.57 91.45 92.0112 734 92.58 91.45 92.01Table 7.
Incremental improvement of perform-anceModel R  P  FSME 90.7 89.78 90.24CRF 92.02 91.66 91.84SVM-B 92.34 91.42 91.88SVM-F 92.58 91.45 92.01Table 8.
Results after unlabeled data selection5.6 Voting TechniquesIn order to obtain higher performance, we haveapplied weighted voting to the four models.
Wehave used the following weighting methods:(1).
Uniform weights (Majority voting): Allthe models are assigned the same voting weight.The combined system selects the classifications,which are proposed by the majority of the mod-els.
In case of a tie, the output of the SVM-Fmodel is selected.
The output of the SVM-Fmodel has been selected due to its highest per-formance among all the models.(2).
Cross validation Precision values: Twodifferent types of weights have been defined de-pending on the 10-fold cross validation Precisionon the training data as follows:(a).
Total Precision: In this method, theoverall average Precision of any classifier is as-signed as the weight for it.(b).
Tag Precision: In this method, the aver-age Precision value of the individual tag is as-signed as the weight for the corresponding model.Experimental results of the voted system arepresented in Table 9.
Evaluation results showthat the system achieves the highest performancefor the voting scheme ?Tag Precision?.
Votingshows (Tables 8-9) an overall improvement of2.74% over the least performing ME based sys-tem and 0.97% over the best performing SVM-Fsystem.
This also shows an improvement of23.66% F-Score over the baseline model.Voting  R  P  FSMajority 92.59 91.47 92.03Total Precision 93.08 91.79 92.43Tag Precision 93.81 92.18 92.98Table 9.
Results of the voted system6 ConclusionIn this paper, we have reported a voted systemwith the use of appropriate unlabeled data.
Wehave also demonstrated how language dependentfeatures can improve the system performance.
Ithas been experimentally verified that effectivemeasures to select relevant documents and usefullabeled sentences are important.
The system hasdemonstrated the overall Recall, Precision, andF-Score values of 93.81%, 92.18%, and 92.98%,respectively.Future works include the development of NERsystem using other machine learning techniquessuch as decision tree, AdaBoost etc.
We wouldlike to apply the proposed voted technique forthe development of NER systems in other Indianlanguages.
Future direction of the work will be toinvestigate an appropriate clustering techniquethat can be very effective for the development ofNER systems in the resource-constrained Indianlanguage environment.
Instead of the words, thecluster of words can be used as the features ofthe classifiers.
It may reduce the cost of trainingas well as may be helpful to improve the per-formance.
We would like to explore other votingtechniques.209ReferencesAnderson, T. W. and Scolve, S. Introduction to theStatistical Analysis of Data.
Houghton Mifflin,1978.Bikel, Daniel M., R. Schwartz, Ralph M. Weischedel.1999.
An Algorithm that Learns What?s in Name.Machine Learning (Special Issue on NLP), 1-20.Bothwick, Andrew.
1999.
A Maximum Entropy Ap-proach to Named Entity Recognition.
Ph.D. Thesis,NYU.Ekbal, Asif, Naskar, Sudip and S. Bandyopadhyay.2007b.
Named Entity Recognition and Translitera-tion in Bengali.
Named Entities: Recognition,Classification and Use, Special Issue of Lingvisti-cae Investigationes Journal, 30:1 (2007), 95-114.Ekbal, Asif, Haque, R and S. Bandyopadhyay.
2008a.Named Entity Recognition in Bengali: A Condi-tional Random Field Approach.
In Proceedings of3rd International Joint Conference on Natural Lan-guage Processing (IJCNLP-08), 589-594.Ekbal, Asif, and S. Bandyopadhyay.
2008b.
BengaliNamed Entity Recognition using Support VectorMachine.
In Proceedings of the Workshop onNamed Entity Recognition on South and South EastAsian Languages (NERSSEAL), IJCNLP-08, 51-58.Ekbal, Asif, and S. Bandyopadhyay.
2008c.
A Web-based Bengali News Corpus for Named EntityRecognition.
Language Resources and EvaluationJournal, Volume (40), 173-182.Ekbal, Asif and S. Bandyopadhyay.
2008d.
Web-based Bengali News Corpus for Lexicon Develop-ment and POS Tagging.
In POLIBITS, an Interna-tional Journal, Volume (37), 20-29, ISSN: 1870-9044.Ekbal, Asif and S. Bandyopadhyay.
2008e.
Appropri-ate Unlabeled Data, Post-processing and VotingCan Improve the Performance of NER System.
InProceedings of the 6th International Conference onNatural Language Processing (ICON-08), 234-239, India.Ekbal, Asif and S. Bandyopadhyay.
2009.
Improvingthe Performance of a NER System by Post-processing, Context Patterns and Voting.
In W. Liand D. Molla-Aliod (Eds): ICCPOL 2009, LectureNotes in Artificial Intelligence (LNAI), SpringerBerlin/Heidelberg, Volume (5459), 45-56.Florian, Radu, Ittycheriah, A., Jing, H. and Zhang, T.2003.
Named Entity Recognition through ClassifierCombination.
In Proceedings of CoNLL-2003.Lafferty, J., McCallum, A., and Pereira, F. 2001.Conditional Random Fields: Probabilistic Modelsfor Segmenting and Labeling Sequence Data.
InProceedings  of 18th International Conference onMachine Learning (ICML), 282-289.Li, Wei and Andrew McCallum.
2003.
Rapid Devel-opment of Hindi Named Entity Recognition UsingConditional Random Fields and Feature Induc-tions.
ACM TALIP, 2(3), (2003), 290-294.Saha, Sujan, Sarkar, S and Mitra, P. 2008.
A HybridFeature Set based Maximum Entropy Hindi NamedEntity Recognition.
In Proceedings of the 3rd Inter-national Joint Conference on Natural LanguageProcessing (IJCNLP-08), 343-349.Valdimir N., Vapnik 1995.
The Nature of StatisticalLearning Theory.
Springer.Yamada, Hiroyasu, Taku Kudo and Yuji Matsumoto.2002.
Japanese Named Entity Extraction usingSupport Vector Machine.
In Transactions of IPSJ,Vol.
43 No.
1, 44-53.210
