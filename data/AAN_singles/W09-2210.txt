Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 75?83,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsOn Semi-Supervised Learning of Gaussian Mixture Modelsfor Phonetic Classification?Jui-Ting Huang and Mark Hasegawa-JohnsonDepartment of Electrical and Computer EngineeringUniversity of Illinois at Urbana-ChampaignIllinois, IL 61801, USA{jhuang29,jhasegaw}@illinois.eduAbstractThis paper investigates semi-supervised learn-ing of Gaussian mixture models using an uni-fied objective function taking both labeled andunlabeled data into account.
Two methodsare compared in this work ?
the hybrid dis-criminative/generative method and the purelygenerative method.
They differ in the crite-rion type on labeled data; the hybrid methoduses the class posterior probabilities and thepurely generative method uses the data like-lihood.
We conducted experiments on theTIMIT database and a standard synthetic dataset from UCI Machine Learning repository.The results show that the two methods be-have similarly in various conditions.
For bothmethods, unlabeled data improve training onmodels of higher complexity in which the su-pervised method performs poorly.
In addition,there is a trend that more unlabeled data re-sults in more improvement in classification ac-curacy over the supervised model.
We alsoprovided experimental observations on the rel-ative weights of labeled and unlabeled partsof the training objective and suggested a criti-cal value which could be useful for selecting agood weighing factor.1 IntroductionSpeech recognition acoustic models can be trainedusing untranscribed speech data (Wessel and Ney,2005; Lamel et al, 2002; L. Wang and Woodland,2007).
Most such experiments begin by boostraping?This research is funded by NSF grants 0534106 and0703624.an initial acoustic model using a limited amount ofmanually transcribed data (normally in a scale from30 minutes to several hours), and then the initialmodel is used to transcribe a relatively large amountof untranscribed data.
Only the transcriptions withhigh confidence measures (Wessel and Ney, 2005;L. Wang and Woodland, 2007) or high agreementwith closed captions (Lamel et al, 2002) are se-lected to augment the manually transcribed data, andnew acoustic models are trained on the augmenteddata set.The general procedure described above exactlylies in the context of semi-supervised learning prob-lems and can be categorized as a self-training algo-rithm.
Self-training is probably the simplest semi-supervised learning method, but it is also flexibleto be applied to complex classifiers such as speechrecognition systems.
This may be the reason whylittle work has been done on exploiting other semi-supervised learning methods in speech recognition.Though not incorporated to speech recognizers yet,there has been some work on semi-supervised learn-ing of Hidden Markov Models (HMM) for sequen-tial classification.
Inoue and Ueda (2003) treated theunknown class labels of the unlabeled data as hiddenvariables and used the expectation-maximization(EM) algorithm to optimize the joint likelihood oflabeled and unlabeled data.
Recently Ji et al (2009)applied a homotopy method to select the optimalweight to balance between the log likelihood of la-beled and unlabeled data when training HMMs.Besides generative training of acoustic models,discriminative training is another popular paradigmin the area of speech recognition, but only when75the transcriptions are available.
Wang and Wood-land (2007) used the self-training method to aug-ment the training set for discriminative training.Huang and Hasegawa-Johnson (2008) investigatedanother use of discriminative information from la-beled data by replacing the likelihood of labeled datawith the class posterior probability of labeled data inthe semi-supervised training objective for GaussianMixture Models (GMM), resulting in a hybrid dis-criminative/generative objective function.
Their ex-perimental results in binary phonetic classificationshowed significant improvement in classification ac-curacy when labeled data are scarce.
A similar strat-egy called ?
?multi-conditional learning??
was pre-sented in (Druck et al, 2007) applied to MarkovRandom Field models for text classification tasks,with the difference that the likelihood of labeled datais also included in the objective.
The hybrid dis-criminative/generative objective function can be in-terpreted as having an extra regularization term, thelikelihood of unlabeled data, in the discriminativetraining criterion for labeled data.
However, bothmethods in (Huang and Hasegawa-Johnson, 2008)and (Druck et al, 2007) encountered the same issueabout determining the weights for labeled and un-labeled part in the objective function and chose touse a development set to select the optimal weight.This paper provides an experimental analysis on theeffect of the weight.With the ultimate goal of applying semi-supervised learning in speech recognition, this pa-per investigates the learning capability of algorithmswithin Gaussian Mixture Models because GMM isthe basic model inside a HMM, therefore 1) the up-date equations derived for the parameters of GMMcan be conveniently extended to HMM for speechrecognition.
2) GMM can serve as an initial pointto help us understand more details about the semi-supervised learning process of spectral features.This paper makes the following contribution:?
it provides an experimental comparison of hy-brid and purely generative training objectives.?
it studies the impact of model complexity onlearning capability of algorithms.?
it studies the impact of the amount of unlabeleddata on learning capability of algorithms.?
it analyzes the role of the relative weights oflabeled and unlabeled parts of the training ob-jective.2 AlgorithmSuppose a labeled set XL = (x1, .
.
.
, xn, .
.
.
, xNL)has NL data points and xn ?
Rd.
YL =(y1, .
.
.
, yn, .
.
.
, yNL) are the corresponding classlabels, where yn ?
{1, 2, .
.
.
, Y } and Y is the num-ber of classes.
In addition, we also have an unla-beled set XU = (x1, .
.
.
, xn, .
.
.
, xNU ) without cor-responding class labels.
Each class is assigned aGaussian Mixture model, and all models are trainedgiven XL and XU .
This section first presents thehybrid discriminative/generative objective functionfor training and then the purely generative objectivefunction.
The parameter update equations are alsoderived here.2.1 Hybrid Objective FunctionThe hybrid discriminative/generative objective func-tion combines the discriminative criterion for la-beled data and the generative criterion for unlabeleddata:F (?)
= logP (YL|XL;?)
+ ?
logP (XU ;?
), (1)and we chose the parameters so that (1) is maxi-mized:??
= argmax?F (?)
.
(2)The first component considers the log posteriorclass probability of the labeled set whereas the sec-ond component considers the log likelihood of theunlabeled set weighted by ?.
In ASR community,model training based the first component is usuallyreferred to as Maximum Mutual Information Esti-mation (MMIE) and the second component Maxi-mum Likelihood Estimation (MLE), therefore in thispaper we use a brief notation for (1) just for conve-nience:F (?)
= F (DL)MMI (?)
+ ?F (DU )ML (?)
.
(3)The two components are different in scale.
First,the size of the labeled set is usually smaller thanthe size of the unlabeled set in the scenario of semi-supervised learning, so the sums over the data setsinvolve different numbers of terms; Second, the76scales of the posterior probability and the likeli-hood are essentially different, so are their gradients.While the weight ?
balances the impacts of twocomponents on the training process, it may also im-plicitly normalize the scales of the two components.In section (3.2) we will discuss and provide a furtherexperimental analysis.In this paper, the models to be trained are Gaus-sian mixture models of continuous spectral featurevectors for phonetic classes, which can be furtherextended to Hidden Markov Models with extra pa-rameters such as transition probabilities.The maximization of (1) follows the techniquesin (Povey, 2003), which uses auxiliary functions forobjective maximization; In each iteration, a strongor weak sense auxiliary function is maximized, suchthat if the auxiliary function converges after itera-tions, the objective function will be at a local maxi-mum as well.The objective function (1) can be rewritten asF (?)
= logP (XL|YL;?)?
logP (XL;?
)+ ?
logP (XU ;?
), (4)where the term logP (YL;?)
is removed because itis independent of acoustic model parameters.The auxiliary function at the current parameter?old for (4) isG(?, ?
(old)) =Gnum(?, ?(old))?
Gden(?, ?
(old))+?Gden(?, ?
(old);DU ) + Gsm(?, ?
(old)),(5)where the first three terms are strong-sense auxiliaryfunctions for the conditional likelihood (referred toas the numerator(num) model because it appears inthe numerator when computing the class posteriorprobability) logP (XL|YL;?)
and the marginal like-lihoods (referred to as the denominator(den) modellikewise) logP (XL;?)
and ?
log P (XU ;?)
respec-tively.
The last term is a smoothing function thatdoesn?t affect the local differential but ensures thatthe sum of the first three term is at least a convexweak-sense auxiliary function for good convergencein optimization.Maximization of (5) leads to the update equationsfor the class j and mixture m given as follows:?
?jm = 1?jm(xnumjm, ?
xdenjm + ?xdenjm(DU ) + Djm?jm)(6)?
?2jm =1?jm(snumjm ?
sdenjm + ?sdenjm(DU )+Djm(?2jm + ?2jm))?
?
?2jm,(7)where for clarity the following substitution is used:?jm = ?numjm ?
?denjm + ?
?denjm(DU ) + Djm (8)and ?jm is the sum of the posterior probabilities ofoccupation of mixture component m of class j overthe dataset:?numjm (X) =?xi?X,yi=jp (m|xi, yi = j)?denjm(X) =?xi?Xp (m|xi)(9)and xjm and sjm are respectively the weightedsum of xi and x2i over the whole dataset with theweight p (m|xi, yi = j) or p (m|xi), depending onwhether the superscript is the numerator or denomi-nator model.
Djm is a constant set to be the greaterof twice the smallest value that guarantees positivevariances or ?denjm (Povey, 2003).
The re-estimationformula for mixture weights is also derived from theExtended Baum-Welch algorithm:c?jm =cjm{?F?cjm + C}?m?
cjm?
{?F?cjm + C} , (10)where the derivative was approximated (Merialdo,1988) in the following form for practical robustnessfor small-valued parameters :?FMMI?cjm??numjm?m?
?numjm???denjm?m?
?denjm?.
(11)Under our hybrid framework, there is an extra term?denjm(DU )/?m?
?denjm?
(DU ) that should exist in (11),but in practice we found that adding this term to theapproximation is not better than the original form.Therefore, we keep using MMI-only update for mix-ture weights.
The constant C is chosen such that allparameter derivatives are positive.772.2 Purely Generative ObjectiveIn this paper we compare the hybrid objective withthe purely generative one:F (?)
= logP (XL|YL;?)
+ ?
logP (XU ;?
),(12)where the two components are total log likelihood oflabeled and unlabeled data respectively.
(12) doesn?tsuffer from the problem of combining two heteroge-neous probabilistic items, and the weight ?
beingequal to one means that the objective is a joint datalikelihood of labeled and unlabeled set with the as-sumption that the two sets are independent.
How-ever, DL or DU might just be a sampled set of thepopulation and might not reflect the true proportion,so we keep ?
to allow a flexible combination of twocriteria.
On top of that, we need to adjust the relativeweights of the two components in practical experi-ments.The parameter update equation is a reduced formof the equations in Section (2.1):?
?jm =xnumjm, + ?xdenjm(DU )?numjm + ?
?denjm(DU )(13)?
?2jm =snumjm + ?sdenjm(DU )?numjm + ?
?denjm(DU )?
?
?2jm (14)3 Results and DiscussionThe purpose of designing the learning algorithmsis for classification/recognition of speech sounds,so we conducted phonetic classification experimentsusing the TIMIT database (Garofolo et al, 1993).We would like to investigate the relation of learningcapability of semi-supervised algorithms to otherfactors and generalize our observations to other datasets.
Therefore, we used another synthetic datasetWaveform for the evaluation of semi-supervisedlearning algorithms for Gaussian Mixture model.TIMIT: We used the same 48 phone classes andfurther grouped into 39 classes according to (Leeand Hon, 1989) as our final set of phone classes tomodel.
We extracted 50 speakers out of the NISTcomplete test set to form the development set.
Allof our experimental analyses were on the develop-ment set.
We used segmental features (Halberstadt,1998) in the phonetic classification task.
For eachphone occurrence, a fixed-length vector was calcu-lated from the frame-based spectral features (12 PLPcoefficients plus energy) with a 5 ms frame rate anda 25 ms Hamming window.
More specifically, wedivided the frames for each phone into three regionswith 3-4-3 proportion and calculated the PLP av-erage over each region.
Three averages plus thelog duration of that phone gave a 40-dimensional(13?
3 + 1) measurement vector.Waveform: We used the second versions ofthe Waveform dataset available at the UCI reposi-tory (Asuncion and Newman, 2007).
There are threeclasses of data.
Each token is described by 40 realattributes, and the class distribution is even.Forwaveform, because the class labels are equallydistributed, we simply assigned equal number ofmixtures for each class.
For TIMIT, the phoneclasses are unevenly distributed, so we assignedvariable number of Gaussian mixtures for each classby controlling the averaged data counts per mixture.For all experiments, the initial model is an MLEmodel trained with labeled data only.To construct a mixed labeled/unlabeled data set,the original training set were randomly divided intothe labeled and unlabeled sets with desired ratio, andthe class labels in the unlabeled set are assumed to beunknown.
To avoid that the classifier performancemay vary with particular portions of data, we ran fivefolds for every experiment, each fold correspondingto different division of training data into labeled andunlabeled set, and took the averaged performance.3.1 Model ComplexityThis section analyzes the learning capability ofsemi-supervised learning algorithms for differentmodel complexities, that is, the number of mix-tures for Gaussian mixture model.
In this experi-ment, the sizes of labeled and unlabeled set are fixed(|DL| : |DU | = 1 : 10 and the averaged tokencounts per class is around 140 for both data sets),as we varied the total number of mixtures and eval-uated the updated model by its classification accu-racy.
For waveform, number of mixtures was setfrom 2 to 7; for TIMIT, because the number of mix-tures per class is determined by the averaged datacounts per mixture c, we set c to 25, 20 and 15 asthe higher c gives less number of mixtures in total.Figure 3.1 plots the averaged classification accura-78Figure 1: Mean classification accuracies vs. ?
for different model complexity.
The accuracies for the initial MLEmodels are indicated in the parentheses.
(a) waveform: training with the hybrid objective.
(b) waveform: purelygenerative objective.
(c) TIMIT: training with the hybrid objective.
(d) TIMIT: purely generative objective.!
!"
#!
#!"
$!%&''#'$'(')'"!"!
""#$%"&'()#$#%#&# '()*&+"#$#%#(# '&)*'+"#$#%#,# '-)./+"#$#%#.# '*)-'+"#$#%#/# 01)/-+"2"!
! "
!#  !#"  !$"(")"""*"%  3"#!
""#$%"&'()#3#%#&.# ..)(,+"#3#%#&*# ..)(/+"#3##%#-.# .,)0&+"!
!"
#!
#!"
$!%&''#'$'(')'"4"#!
""#$%"&'()##$#%#&# '()*&+"#$#%#(# '&)*'+"#$#%#,# '-)./+"#$#%#.# '*)-'+"#$#%#/# 01)/-+"!
! "
!#  !#"  !$"(")"""*"%##!
""#$%"&'()#3#%#&.# ..)(,+"#3#%#&*# ..)(/+"#3#%#-.# .,)0&+"cies of the updated model versus the value of ?
withdifferent model complexities.
The ranges of ?
aredifferent for waveform and TIMIT because the valueof ?
for each dataset has different scales.First of all, the hybrid method and purely gen-erative method have very similar behaviors in bothwaveform and TIMIT; the differences between thetwo methods are insignificant regardless of ?.
Thehybrid method with ?
= 0 means supervised MMI-training with labeled data only, and the purely gener-ative method with ?
= 0means extra several roundsof supervised MLE-training if the convergence cri-terion is not achieved.
With the small amount of la-beled data, most of hybrid curves start slightly lowerthan the purely generative ones at ?
= 0, but in-crease to as high as the purely generative ones as ?increases.For waveform, the accuracies increase with ?
in-creases for all cases except for the 2-mixture model.Table 1 summarizes the numbers from Figure 3.1.Except for the 2-mixture case, the improvement overthe supervised model (?
= 0) is positively corre-lated to the model complexity, as the largest im-provements occur at the 5-mixture and 6-mixturemodel for the hybrid and purely generative methodrespectively.
However, the highest complexity doesnot necessarily gives the best classification accu-racy; the 3-mixture model achieves the best accu-racy among all models after semi-supervised learn-ing whereas the 2-mixture model is the best modelfor supervised learning using labeled data only.Experiments on TIMIT show a similar behavior1 ;as shown in both Figure 3.1 and Table 2, the im-provement over the supervised model (?
= 0) isalso positively correlated to the model complexity,1Note that our baseline performance (the initial MLEmodel)is much worse than benchmark because only 10% of the train-ing data were used.
We justified our baseline model by usingthe whole training data and a similar accuracy ( 74%) to otherwork (e.g.
(Sha and Saul, 2007)) was obtained.79Table 1: The accuracies(%) of the initial MLEmodel, the supervised model (?
= 0), the best accuracies with unlabeleddata and the absolute improvements (?)
over ?
= 0 for different model complexities for waveform.
The boldednumber is the highest value along the same column.Hybrid Purely generative#.
mix init.
acc.
?
= 0 best acc.
?
?
= 0 best acc.
?2 83.02 81.73 83.74 2.01 82.96 83.14 0.183 82.08 81.66 84.69 3.03 82.18 84.58 2.404 81.56 80.53 83.93 3.40 81.34 84.13 2.795 80.18 80.14 83.82 3.68 80.16 83.84 3.686 79.61 79.40 83.19 3.79 79.71 83.31 3.60Table 2: The accuracies(%) of the initial MLEmodel, the supervised model (?
= 0), the best accuracies with unlabeleddata and the absolute improvements (?)
over ?
= 0 for different model complexities for TIMIT.
The bolded numberis the highest value along the same column.Hybrid Purely generativec init.
acc.
?
= 0 best acc.
?
?
= 0 best acc.
?25 55.34 55.47 56.58 1.11 55.32 56.7 1.3820 55.36 55.67 56.72 1.05 55.2 56.25 1.0515 54.72 53.71 55.39 1.68 53.7 56.09 2.39as the most improvements occur at c = 25 for bothhybrid and purely generative methods.
The semi-supervised model consistently improves over the su-pervised model.
To summarize, unlabeled data im-prove training on models of higher complexity, andsometimes it helps achieve the best performancewith a more complex model.3.2 Size of Unlabeled DataIn Figure 2, we fixed the size of the labeled set (4%of the training set) and plotted the averaged classi-fication accuracies for learning with different sizesof unlabeled data.
First of all, the hybrid methodand purely generative method still behave similarlyin both waveform and TIMIT.
For both datasets, thefigures clearly illustrate that more unlabeled datacontributes more improvement over the supervisedmodel regardless of the value of ?.
Generally, a datadistribution can be expected more precisely with alarger sample size from the data pool, therefore weexpect the more unlabeled data the more precise in-formation about the population, which improves thelearning capability.3.3 Discussion of ?During training, the weighted sum ofFMMI andFMLin equation (15) increases with iterations, howeverFMMI and FML are not guaranteed to increase indi-vidually.
Figure 3 illustrates how ?
affects the re-spective change of the two components for a partic-ular setting for waveform.
When ?
= 0, the ob-jective function does not take unlabeled data intoaccount, so FMMI increases while FML decreases.FML starts to increase for nonzero ?
; ?
= 0.01corresponds to the case where both objectives in-creases.
As ?
keeps growing, FMMI starts to de-crease whereas FML keeps rising.
In this partic-ular example, ?
= 0.05 is the critical value atwhich FMMI changes from increasing to decreas-ing.
According to our observation, the value of ?depends on the dataset and the relative size of la-beled/unlabeled data.
Table 3 shows the critical val-ues for waveform and TIMIT for different sizes oflabeled data (5, 10, 15, 20% of the training set) witha fixed set of unlabeled data (80%.)
The numbers arevery different across the datasets, but there is a con-sistent pattern within the dataset?the critical valueincreases as the size of labeled set increases.
Onepossible explanation is that ?
contains an normal-80Figure 2: Mean classification accuracies vs. ?
for different amounts of unlabeled data (the percentage in the trainingset).
The averaged accuracy for the initial MLE model is 81.66% for waveform and 59.41% for TIMIT.
(a) waveform:training with the hybrid objective.
(b) waveform: purely generative objective.
(c) TIMIT: training with the hybridobjective.
(d) TIMIT: purely generative objective.!
!"
#!
#!"
$!%#%$%&%'!""#$%"&'()! "
#$%! "
&$%! "
'$%! "
($%!
! "
!#"(!'"(!
)"(!%) !)
!$!""#$%"&'()! "
)$%! "
*$%! "
($%!
!"
#!
#!"
$!%$%&%'!""#$%"&'()! "
#$%! "
&$%! "
'$%! "
($%!
! "
!#"(!'"(!
)"(!%) !!""#$%"&'()! "
)$%! "
*$%! "
($%+,-+.-+/-+0-ization factor with respect to the relative size of la-beled/unlabeled set.
The objective function in (15)can be rewritten in terms of the normalized objectivewith respect to the data size:F (?)
= |DL|F (DL)MMI (?
)+?|DU |F (DU )ML (?)
.
(15)where F (X) means the averaged value over the dataset X.
When the labeled set size increases, ?
mayhave to scale up accordingly such that the relativechange of the two averaged component remains inthe same scale.Although ?
controls the dominance of the crite-rion on labeled data or on unlabeled data, the factthat which dominates the objective or the criticalvalue does not necessary indicate the best ?.
How-ever, we observed that the best ?
is usually close toor larger than the critical value, but the exact valuevaries with different data.
At this point, it might stillbe easier to find the best weight using a small de-velopment set.
But this observation also provides aguide about the reasonable range to search the best?
?
searching starting from the critical value and itshould reach the optimal value soon according to theplots in Figure 3.1.Table 3: The critical values for waveform and TIMITfor different sizes of labeled data (percentage of trainingdata) with a fixed set of unlabeled data (80 %.
)Size of labeled data waveform TIMIT5% 0.09-0.11 0.03-0.0410% 0.12-0.14 0.07-0.0815% 0.5-0.6 0.08-0.0920% 1-1.5 0.11-0.1281Figure 3: Accuracy (left), FMMI (center), and FML (right) at different values of alpha.! "
# $ % & '(''(')('*()()!()"()#()$!""#$%"&'()!
"#$!%&'()*+,"#( (-(.
( (-(./.0( (-(./.1( (-(./1! "
# $ % & '+#+"+!*+,,-!
"#$!%&'()*+,"#( (-(.
( (-(./.0( (-(./.1( (-(./1! "
# $ % & '+!*)+!*&+!*$+!*"+!*+!))*+,./0111!
"#$!%&'()*+,"#( (-(.
( (-(./.0( (-(./.1( (-(./13.4 Hybrid Criterion vs. Purely GenerativeCriterionFrom the previous experiments, we found that thehybrid criterion and purely generative criterion al-most match each other in performance and are ableto learn models of the same complexity.
This impliesthat the criterion on labeled data has less impact onthe overall training direction than unlabeled data.
InSection 3.2, we mentioned that the best ?
is usuallylarger than or close to the critical value around whichthe unlabeled data likelihood tends to dominate thetraining objective.
This again suggests that labeleddata contribute less to the training objective functioncompared to unlabeled data, and the criterion on la-beled data doesn?t matter as much as the criterion onunlabeled data.
It is possible that most of the con-tributions from labeled data have already been usedfor training an initial MLE model, therefore little in-formation could be extracted in the further trainingprocess.4 ConclusionRegardless of the dataset and the training objectivetype on labeled data, there are some general prop-erties about the semi-supervised learning algorithmsstudied in this work.
First, while limited amount oflabeled data can at most train models of lower com-plexity well, the addition of unlabeled data makesthe updated models of higher complexity much im-proved and sometimes perform better than less com-plex models.
Second, the amount of unlabeled datain our semi-supervised framework generally follows?the-more-the-better?
principle; there is a trend thatmore unlabeled data results in more improvement inclassification accuracy over the supervised model.We also found that the objective type on labeleddata has little impact on the updated model, in thesense that hybrid and purely generative objectivesbehave similarly in learning capability.
The obser-vation that the best ?
occurs after the MMI criterionbegins to decrease supports the fact that the criterionon labeled data contributes less than the criterion onunlabeled data.
This observation is also helpful indetermining the search range for the best ?
on thedevelopment set by locating the critical value of theobjective as a start point to perform search.The unified training objective method has a niceconvergence property which self-training methodscan not guarantee.
The next step is to extend thesimilar framework to speech recognition task whereHMMs are trained and phone boundaries are seg-mented.
It would be interesting to compare it withself-training methods in different aspects (e.g.
per-formance, reliability, stability and computational ef-ficiency).82ReferencesA.
Asuncion and D.J.
Newman.
2007.
UCI machinelearning repository.Gregory Druck, Chris Pal, AndrewMcCallum, and Xiao-jin Zhu.
2007.
Semi-supervised classification with hy-brid generative/discriminative methods.
In KDD ?07:Proceedings of the 13th ACM SIGKDD internationalconference on Knowledge discovery and data mining,pages 280?289, New York, NY, USA.
ACM.J.
S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus,D.
S. Pallett, and N. L. Dahlgren.
1993.
Darpa timitacoustic phonetic continuous speech corpus.Andrew K. Halberstadt.
1998.
Heterogeneous Acous-tic Measurements and Multiple Classifiers for SpeechRecognition.
Ph.D. thesis, Massachusetts Institute ofTechnology.J.-T. Huang and Mark Hasegawa-Johnson.
2008.
Max-imum mutual information estimation with unlabeleddata for phonetic classification.
In Interspeech.Masashi Inoue and Naonori Ueda.
2003.
Exploitation ofunlabeled sequences in hidden markov models.
IEEETrans.
On Pattern Analysis and Machine Intelligence,25:1570?1581.Shihao Ji, Layne T. Watson, and Lawrence Carin.
2009.Semisupervised learning of hidden markov models viaa homotopymethod.
IEEE Trans.
Pattern Anal.
Mach.Intell., 31(2):275?287.M.J.F.
Gales L. Wang and P.C.
Woodland.
2007.
Un-supervised training for mandarin broadcast news andconversation transcription.
In Proc.
IEEE Confer-ence on Acoustics, Speech, and Signal Processing(ICASSP), volume 4, pages 353?356.Lori Lamel, Jean-Luc Gauvain, and Gilles Adda.
2002.Lightly supervised and unsupervised acoustic modeltraining.
16:115?129.K.-F. Lee and H.-W. Hon.
1989.
Speaker-independentphone recognition using hidden markov models.IEEE Transactions on Speech and Audio Processing,37(11):1641?1648.B.
Merialdo.
1988.
Phonetic recognition using hid-den markov models and maximum mutualinformationtraining.
In Proc.
IEEE Conference on Acoustics,Speech, and Signal Processing (ICASSP), volume 1,pages 111?114.Daniel Povey.
2003.
Discriminative Training for LargeVocabulary Speech Recognition.
Ph.D. thesis, Cam-bridge University.Fei Sha and Lawrence K. Saul.
2007.
Large margin hid-den markov models for automatic speech recognition.In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-vances in Neural Information Processing Systems 19,pages 1249?1256.
MIT Press, Cambridge, MA.Frank Wessel and Hermann Ney.
2005.
Unsupervisedtraining of acoustic models for large vocabulary con-tinuous speech recognition.
IEEE Transactions onSpeech and Audio Processing, 13(1):23?31, January.83
