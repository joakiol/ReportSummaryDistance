Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 19?27,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsProducts of Random Latent Variable GrammarsSlav PetrovGoogle ResearchNew York, NY, 10011slav@google.comAbstractWe show that the automatically induced latentvariable grammars of Petrov et al (2006) varywidely in their underlying representations, de-pending on their EM initialization point.
Weuse this to our advantage, combining multipleautomatically learned grammars into an un-weighted product model, which gives signif-icantly improved performance over state-of-the-art individual grammars.
In our model,the probability of a constituent is estimated asa product of posteriors obtained from multi-ple grammars that differ only in the randomseed used for initialization, without any learn-ing or tuning of combination weights.
Despiteits simplicity, a product of eight automaticallylearned grammars improves parsing accuracyfrom 90.2% to 91.8% on English, and from80.3% to 84.5% on German.1 IntroductionLearning a context-free grammar for parsing re-quires the estimation of a more highly articulatedmodel than the one embodied by the observed tree-bank.
This is because the naive treebank grammar(Charniak, 1996) is too permissive, making unreal-istic context-freedom assumptions.
For example, itpostulates that there is only one type of noun phrase(NP), which can appear in all positions (subject, ob-ject, etc.
), regardless of case, number or gender.
Asa result, the grammar can generate millions of (in-correct) parse trees for a given sentence, and has aflat posterior distribution.
High accuracy grammarstherefore add soft constraints on the way categoriescan be combined, and enrich the label set with addi-tional information.
These constraints can be lexical-ized (Collins, 1999; Charniak, 2000), unlexicalized(Johnson, 1998; Klein and Manning, 2003b) or au-tomatically learned (Matsuzaki et al, 2005; Petrovet al, 2006).
The constraints serve the purpose ofweakening the independence assumptions, and re-duce the number of possible (but incorrect) parses.Here, we focus on the latent variable approach ofPetrov et al (2006), where an Expectation Maxi-mization (EM) algorithm is used to induce a hier-archy of increasingly more refined grammars.
Eachround of refinement introduces new constraints onhow constituents can be combined, which in turnleads to a higher parsing accuracy.
However, EM is alocal method, and there are no guarantees that it willfind the same grammars when initialized from dif-ferent starting points.
In fact, it turns out that eventhough the final performance of these grammars isconsistently high, there are significant variations inthe learned refinements.We use these variations to our advantage, andtreat grammars learned from different random seedsas independent and equipotent experts.
We use aproduct distribution for joint prediction, which givesmore peaked posteriors than a sum, and enforces allconstraints of the individual grammars, without theneed to tune mixing weights.
It should be noted herethat our focus is on improving parsing performanceusing a single underlying grammar class, which issomewhat orthogonal to the issue of parser combina-tion, that has been studied elsewhere in the literature(Sagae and Lavie, 2006; Fossum and Knight, 2009;Zhang et al, 2009).
In contrast to that line of work,we also do not restrict ourselves to working with k-best output, but work directly with a packed forestrepresentation of the posteriors, much in the spiritof Huang (2008), except that we work with severalforests rather than rescoring a single one.19In our experimental section we give empirical an-swers to some of the remaining theoretical ques-tions.
We address the question of averaging versusmultiplying classifier predictions, we investigate dif-ferent ways of introducing more diversity into theunderlying grammars, and also compare combiningpartial (constituent-level) and complete (tree-level)predictions.
Quite serendipitously, the simplest ap-proaches work best in our experiments.
A productof eight latent variable grammars, learned on thesame data, and only differing in the seed used inthe random number generator that initialized EM,improves parsing accuracy from 90.2% to 91.8%on English, and from 80.3% to 84.5% on German.These parsing results are even better than those ob-tained by discriminative systems which have accessto additional non-local features (Charniak and John-son, 2005; Huang, 2008).2 Latent Variable GrammarsBefore giving the details of our model, we brieflyreview the basic properties of latent variable gram-mars.
Learning latent variable grammars consists oftwo tasks: (1) determining the data representation(the set of context-free productions to be used in thegrammar), and (2) estimating the parameters of themodel (the production probabilities).
We focus onthe randomness introduced by the EM algorithm andrefer the reader to Matsuzaki et al (2005) and Petrovet al (2006) for a more general introduction.2.1 Split & Merge LearningLatent variable grammars split the coarse (but ob-served) grammar categories of a treebank into morefine-grained (but hidden) subcategories, which arebetter suited for modeling the syntax of naturallanguages (e.g.
NP becomes NP1 through NPk).Accordingly, each grammar production A?BCover observed categories A,B,C is split into a setof productions Ax?ByCz over hidden categoriesAx,By,Cz.
Computing the joint likelihood of the ob-served parse trees T and sentences w requires sum-ming over all derivations t over split subcategories:?iP(wi, Ti) =?i?t:TiP(wi, t) (1)Matsuzaki et al (2005) derive an EM algorithmfor maximizing the joint likelihood, and Petrov etal.
(2006) extend this algorithm to use a split&mergeprocedure to adaptively determine the optimal num-ber of subcategories for each observed category.Starting from a completely markovized X-Bar gram-mar, each category is split in two, generating eightnew productions for each original binary production.To break symmetries, the production probabilitiesare perturbed by 1% of random noise.
EM is theninitialized with this starting point and used to climbthe highly non-convex objective function given inEq.
1.
Each splitting step is followed by a mergingstep, which uses a likelihood ratio test to reverse theleast useful half of the splits.
Learning proceeds byiterating between those two steps for six rounds.
Toprevent overfitting, the production probabilities arelinearly smoothed by shrinking them towards theircommon base category.2.2 EM induced RandomnessWhile the split&merge procedure described aboveis shown in Petrov et al (2006) to reduce the vari-ance in final performance, we found after closerexamination that there are substantial differencesin the patterns learned by the grammars.
Sincethe initialization is not systematically biased in anyway, one can obtain different grammars by simplychanging the seed of the random number genera-tor.
We trained 16 different grammars by initial-izing the random number generator with seed val-ues 1 through 16, but without biasing the initial-ization in any other way.
Figure 1 shows that thenumber of subcategories allocated to each observedcategory varies significantly between the differentinitialization points, especially for the phrasal cate-gories.
Figure 2 shows posteriors over the most fre-quent subcategories given their base category for thefirst four grammars.
Clearly, EM is allocating the la-tent variables in very different ways in each case.As a more quantitative measure of difference,1 weevaluated all 16 grammars on sections 22 and 24 ofthe Penn Treebank.
Figure 3 shows the performanceon those two sets, and reveals that there is no singlegrammar that achieves the best score on both.
Whilethe parsing accuracies are consistently high,2 there1While cherry-picking similarities is fairly straight-forward,it is less obvious how to quantify differences.2Note that despite their variance, the performance is alwayshigher than the one of the lexicalized parser of Charniak (2000).20102030405060NPVP PPADVPADJP SSBAR QPNNP JJNNSNN RBVBNVBG VB IN CDVBD VBZ DTVBPAutomatically determined number of subcategoriesFigure 1: There is large variance in the number of subcat-egories (error bars correspond to one standard deviation).is only a weak correlation between the accuracieson the two evaluation sets (Pearson coefficient 0.34).This suggests that no single grammar should be pre-ferred over the others.
In previous work (Petrov etal., 2006; Petrov and Klein, 2007) the final grammarwas chosen based on its performance on a held-outset (section 22), and corresponds to the second bestgrammar in Figure 3 (because only 8 different gram-mars were trained).A more detailed error analysis is given in Fig-ure 4, where we show a breakdown of F1 scores forselected phrasal categories in addition to the overallF1 score and exact match (on the WSJ developmentset).
While grammar G2 has the highest overall F1score, its exact match is not particularly high, andit turns out to be the weakest at predicting quanti-fier phrases (QP).
Similarly, the performance of theother grammars varies between the different errormeasures, indicating again that no single grammardominates the others.3 A Simple Product ModelIt should be clear by now that simply varying therandom seed used for initialization causes EM todiscover very different latent variable grammars.While this behavior is worrisome in general, it turnsout that we can use it to our advantage in this partic-ular case.
Recall that we are using EM to learn both,the data representation, as well as the parameters ofthe model.
Our analysis showed that changing theinitialization point results in learning grammars thatvary quite significantly in the errors they make, buthave comparable overall accuracies.
This suggeststhat the different local maxima found by EM corre-spond to different data representations rather than to4%7%10%1 2 3 4 5 6 7 8NP0%15%25%1 2 3 4 5 6 7 8PP0%15%30%1 2 3 4 5 6 7 8IN0%30%60%1 2 3 4 5 6 7 8DTFigure 2: Posterior probabilities of the eight most fre-quent hidden subcategories given their observed base cat-egories.
The four grammars (indicated by shading) arepopulating the subcategories in very different ways.suboptimal parameter estimates.To leverage the strengths of the individual gram-mars, we combine them in a product model.
Productmodels have the nice property that their Kullback-Liebler divergence from the true distribution willalways be smaller than the average of the KL di-vergences of the individual distributions (Hinton,2001).
Therefore, as long as no individual gram-mar Gi is significantly worse than the others, we canonly benefit from combining multiple latent variablegrammars and searching for the tree that maximizesP(T |w) ?
?iP(T |w, Gi) (2)Here, we are making the assumption that the individ-ual grammars are conditionally independent, whichis of course not true in theory, but holds surprisinglywell in practice.
To avoid this assumption, we coulduse a sum model, but we will show in Section 4.1that the product formulation performs significantlybetter.
Intuitively speaking, products have the ad-vantage that the final prediction has a high poste-rior under all models, giving each model veto power.This is exactly the behavior that we need in the caseof parsing, where each grammar has learned differ-ent constraints for ruling out improbable parses.3.1 LearningJoint training of our product model would couple theparameters of the individual grammars, necessitat-ing the computation of an intractable global parti-tion function (Brown and Hinton, 2001).
Instead,we use EM to train each grammar independently,2189.589.689.789.889.99090.190.290.6  90.7  90.8  90.9  91  91.1  91.2  91.3  91.4F1ScoreonSection24F1 Score on Section 22Figure 3: Parsing accuracies for grammars learned fromdifferent random seeds.
The large variance and weak cor-relation suggest that no single grammar is to be preferred.but from a different, randomly chosen starting point.To emphasize, we do not introduce any systematicbias (but see Section 4.3 for some experiments), orattempt to train the models to be maximally dif-ferent (Hinton, 2002) ?
we simply train a randomcollection of grammars by varying the random seedused for initialization.
We found in our experimentsthat the randomness provided by EM is sufficientto achieve diversity among the individual grammars,and gives results that are as good as more involvedtraining procedures.
Xu and Jelinek (2004) madea similar observation when learning random forestsfor language modeling.Our model is reminiscent of Logarithmic OpinionPools (Bordley, 1982) and Products of Experts (Hin-ton, 2001).3 However, because we believe that noneof the underlying grammars should be favored, wedeliberately do not use any combination weights.3.2 InferenceComputing the most likely parse tree is intractablefor latent variable grammars (Sima?an, 2002), andtherefore also for our product model.
This is becausethere are exponentially many derivations over splitsubcategories that correspond to a single parse treeover unsplit categories, and there is no dynamic pro-gram to efficiently marginalize out the latent vari-ables.
Previous work on parse risk minimization hasaddressed this problem in two different ways: bychanging the objective function, or by constraining3As a matter of fact, Hinton (2001) mentions syntactic pars-ing as one of the motivating examples for Products of Experts.G1G2G3G4P90% 91.5% 93%F1 ScoreG1G2G3G4P40% 45% 50%Exact MatchG1G2G3G4P91% 93% 95%NPG1G2G3G4P90% 92% 94%VPG1G2G3G4P85% 88% 91%PPG1G2G3G4P90% 92.5% 95%QPFigure 4: Breakdown of different accuracy measures forfour randomly selected grammars (G1-G4), as well as aproduct model (P) that uses those four grammars.
Notethat no single grammar does well on all measures, whilethe product model does significantly better on all.the search space (Goodman, 1996; Titov and Hen-derson, 2006; Petrov and Klein, 2007).The simplest approach is to stick to likelihood asthe objective function, but to limit the search spaceto a set of high quality candidates T :T ?
= argmaxT?TP(T |w) (3)Because the likelihood of a given parse tree can becomputed exactly for our product model (Eq.
2), thequality of this approximation is only limited by thequality of the candidate list.
To generate the candi-date list, we produce k-best lists of Viterbi deriva-tions with the efficient algorithm of Huang and Chi-ang (2005), and erase the subcategory informationto obtain parse trees over unsplit categories.
We re-fer to this approximation as TREE-LEVEL inference,because it considers a list of complete trees fromthe underlying grammars, and selects the tree thathas the highest likelihood under the product model.While the k-best lists are of very high quality, this isa fairly crude and unsatisfactory way of approximat-ing the posterior distribution of the product model,as it does not allow the synthesis of new trees basedon tree fragments from different grammars.An alternative is to use a tractable objective func-tion that allows the efficient exploration of the entire22SINV?
SNPSuchagency?self-help?borrowingVPis ADJPADJPunauthorizedandexpensive, ADJPADJPfar moreexpensiveADJPADVPfar morePPthandirectTreasuryborrowing,?
VPsaidNPNPRep.FortneyStarkPRN(D.Calif.)NPNPRep.FortneyStarkPRN(D.Calif.
), NPthebill?schiefsponsor.expensive?
?G1-11.7 -12.4G2-12.9 -11.5z}|{,|{z}?
?G1 G2-68.8 -65.9 -66.7 -67.4z}|{,|{z}Legend: log G1-score log G2-scoreFigure 5: Grammar G1 has a preference for flat structures, while grammar G2 prefers deeper hierarchical structures.Both grammars therefore make one mistake each on their own.
However, the correct parse tree (which uses a flatADJP in the first slot and a hierarchical NP in the second) scores highest under the product model.search space.
Petrov and Klein (2007) present suchan objective function, which maximizes the productof expected correct productions r:T ?
= argmaxT?r?TE(r|w) (4)These expectations can be easily computed from theinside/outside scores, similarly as in the maximumbracket recall algorithm of Goodman (1996), or inthe variational approximation of Matsuzaki et al(2005).
We extend the algorithm to work over poste-rior distributions from multiple grammars, by aggre-gating their expectations into a product.
In practice,we use a packed forest representation to approxi-mate the posterior distribution, as in Huang (2008).We refer to this approximation as CONSTITUENT-LEVEL, because it allows us to form new parse treesfrom individual constituents.Figure 5 illustrates a real case where the prod-uct model was able to construct a completely correctparse tree from two partially correct ones.
In the ex-ample, one of the underlying grammars (G1) had animperfect recall score, because of its preference forflat structures (it missed an NP node in the secondpart of the sentence).
In contrast, the other gram-mar (G2) favors deeper structures, and therefore in-troduced a superfluous ADVP node.
The productmodel gives each underlying grammar veto power,and picks the least controversial tree (which is thecorrect one in this case).
Note that a sum model al-lows the most confident model to dominate the de-cision, and would chose the incorrect hierarchicalADJP construction here (as one can verify using theprovided model scores).To make inference efficient, we can use thesame coarse-to-fine pruning techniques as Petrovand Klein (2007).
We generate a hierarchy of pro-jected grammars for each individual grammar andparse with each one in sequence.
Because only thevery last pass requires scores from the different un-derlying grammars, this computation can be triviallyparallelized across multiple CPUs.
Additionally, thefirst (X-Bar) pruning pass needs to be computedonly once because it is shared among all grammars.Since the X-Bar pass is the bottleneck of the multi-pass scheme (using nearly 50% of the total process-ing time), the overhead of using a product model isquite manageable.
It would have also been possi-ble to use A*-search for factored models (Klein andManning, 2003a; Sun and Tsujii, 2009), but we didnot attempt this in the present work.4 ExperimentsIn our experiments, we follow the standard setupsdescribed in Table 1, and use the EVALB tool forcomputing parsing figures.
Unless noted other-wise, we use CONSTITUENT-LEVEL inference.
Allour experiments are based on the publicly availableBerkeleyParser.44http://code.google.com/p/berkeleyparser23Training Set Dev.
Set Test SetENGLISH-WSJ Sections Section 22 Section 23(Marcus et al, 1993) 2-21ENGLISH-BROWN see 10% of 10% of the(Francis et al 1979) ENGLISH-WSJ the data5 the data5GERMAN Sentences Sentences Sentences(Skut et al, 1997) 1-18,602 18,603-19,602 19,603-20,602Table 1: Corpora and standard experimental setups.4.1 (Weighted) Product vs. (Weighted) SumA great deal has been written on the topic of prod-ucts versus sums of probability distributions for jointprediction (Genest and Zidek, 1986; Tax et al,2000).
However, those theoretical results do notapply directly here, because we are using multi-ple randomly permuted models from the same class,rather models from different classes.
To shed somelight on this issue, we addressed the question em-pirically, and combined two grammars into an un-weighted product model, and also an unweightedsum model.
The individual grammars had parsingaccuracies (F1) of 91.2 and 90.7 respectively, andtheir product (91.7) clearly outperformed their sum(91.3).
When more grammars are added, the gapwidens even further, and the trends persist indepen-dently of whether the models use TREE-LEVEL orCONSTITUENT-LEVEL inference.
At least for thecase of unweighted combinations, the product dis-tribution seems to be superior.In related work, Zhang et al (2009) achieve ex-cellent results with a weighted sum model.
Usingweights learned on a held-out set and rescoring 50-best lists from Charniak (2000) and Petrov et al(2006), they obtain an F1 score of 91.0 (which theyfurther improve to 91.4 using a voting scheme).
Wereplicated their experiment, but used an unweightedproduct of the two model scores.
Using TREE-LEVEL inference, we obtained an F1 score of 91.6,suggesting that weighting is not so important in theproduct case, as long as the classifiers are of compa-rable quality.6 This is in line with previous work onproduct models, where weighting has been impor-tant when combining heterogenous classifiers (Hes-kes, 1998), and less important when the classifiersare of similar accuracy (Smith et al, 2005).5See Gildea (2001) for the exact setup.6The unweighted sum model, however, underperforms theindividual models with an F1 score of only 90.3.90.59191.59292.51 2 4 8 16Number of grammars in product modelParsing accuracy on the WSJ development setConstituent-Level InferenceTree-Level InferenceFigure 6: Adding more grammars to the product modelimproves parsing accuracy, while CONSTITUENT-LEVELinference gives consistently better results.4.2 Tree-Level vs. Constituent-Level InferenceFigure 6 shows that accuracy increases when moregrammars are added to the product model, but levelsoff after eight grammars.
The plot also comparesour two inference approximations, and shows thatCONSTITUENT-LEVEL inference results in a small(0.2), but consistent improvement in F1 score.A first thought might be that the improvement isdue to the limited scope of the k-best lists.
How-ever, this is not the case, as the results hold evenwhen the candidate set for CONSTITUENT-LEVELinference is constrained to trees from the k-best lists.While the packed forrest representation can very ef-ficiently encode an exponential set of parse trees, inour case the k-best lists appear to be already very di-verse because they are generated by multiple gram-mars.
Starting at 96.1 for a single latent variablegrammar, merging two 50-best lists from differentgrammars gives an oracle score of 97.4, and addingmore k-best lists further improves the oracle score to98.6 for 16 grammars.
This compares favorably tothe results of Huang (2008), where the oracle scoreover a pruned forest is shown to be 97.8 (comparedto 96.7 for a 50-best list).The accuracy improvement can instead be ex-plained by the change in the objective function.
Re-call from section Section 3.2, that CONSTITUENT-LEVEL inference maximizes the expected numberof correct productions, while TREE-LEVEL infer-ence maximizes tree-likelihood.
It is therefore nottoo surprising that the two objective functions se-lect the same tree only 41% of the time, even whenlimited to the same candidate set.
Maximizing the24expected number of correct productions is superiorfor F1 score (see the one grammar case in Figure 6).However, as to be expected, likelihood is better forexact match, giving a score of 47.6% vs. 46.8%.4.3 Systematic BiasDiversity among the underlying models is whatgives combined models their strength.
One way ofincreasing diversity is by modifying the feature setsof the individual models (Baldridge and Osborne,2008; Smith and Osborne, 2007).
This approachhas the disadvantage that it reduces the performanceof the individual models, and is not directly appli-cable for latent variable grammars because the fea-tures are automatically learned.
Alternatively, onecan introduce diversity by changing the training dis-tribution.
Bagging (Breiman, 1996) and Boosting(Freund and Shapire, 1996) fall into this category,but have had limited success for parsing (Hender-son and Brill, 2000).
Furthermore boosting is im-practical here, because it requires training dozens ofgrammars in sequence.Since training a single grammar takes roughly oneday, we opted for a different, parallelizable way ofchanging the training distribution.
In a first exper-iment, we divided the training set into two disjointsets, and trained separate grammars on each half.These truly disjoint grammars had low F1 scoresof 89.4 and 89.6 respectively (because they weretrained on less data).
Their combination unfortu-nately also achieves only an accuracy of 90.9, whichis lower than what we get when training a singlegrammar on the entire training set.
In another exper-iment, we used a cross-validation setup where indi-vidual sections of the treebank were held out.
Theresulting grammars had parsing accuracies of about90.5, and the product model was again not able toovercome the lower starting point, despite the poten-tially larger diversity among the underlying gram-mars.
It appears that any systematic bias that lowersthe accuracy of the individual grammars also hurtsthe final performance of the product model.4.4 Product Distribution as SmoothingSmith et al (2005) interpret Logarithmic OpinionPools (LOPs) as a smoothing technique.
Theycompare regularizing Conditional Random Fields(CRFs) with Gaussian priors (Lafferty et al, 2001),to training a set of unregularized CRFs over differ-ent feature sets and combining them in an LOP.
Intheir experiments, both approaches work compara-bly well, but their combination, an LOP of regular-ized CRFs works best.Not too surprisingly, we find this to be the casehere as well.
The parameters of each latent vari-able grammar are typically smoothed in a linearfashion to prevent excessive overfitting (Petrov etal., 2006).
While all the experiments so far usedsmoothed grammars, we reran the experiments alsowith a set of unsmoothed grammars.
The individ-ual unsmoothed grammars have on average an 1.2%lower accuracy.
Even though our product modelis able to increase accuracy by combining multiplegrammars, the gap to the smoothed models remainsconsistent.
This suggests that the product model isdoing more than just smoothing.
In fact, because theproduct distribution is more peaked, it seems to bedoing the opposite of smoothing.4.5 Final ResultsOur final model uses an unweighted product of eightgrammars trained by initializing the random numbergenerator with seeds 1 through 8.
Table 2 showsour test set results (obtained with CONSTITUENT-LEVEL inference), and compares them to relatedwork.
There is a large body of work that has re-ported parsing accuracies for English, and we havegrouped the different methods into categories forbetter overview.Our results on the English in-domain test set arehigher than those obtained by any single componentparser (SINGLE).
The other methods quoted in Ta-ble 2 operate over the output of one or more singlecomponent parsers and are therefore largely orthog-onal to our line of work.
It is nonetheless excitingto see that our product model is competitive withthe discriminative rescoring methods (RE) of Char-niak and Johnson (2005) and Huang (2008), achiev-ing higher F1 scores but lower exact match.
Thesetwo methods work on top of the Charniak (2000)parser, and it would be possible to exchange thatparser with our product model.
We did not attemptthis experiment, but we expect that those methodswould stack well with our model, because they useprimarily non-local features that are not available ina context-free grammar.25Techniques like self-training (SELF) and systemcombinations (COMBO) can further improve pars-ing accuracies, but are also orthogonal to our work.In particular the COMBO methods seem related toour work, but are very different in their nature.While we use multiple grammars in our work, allgrammars are from the same model class for us.
Incontrast, those methods rely on a diverse set of in-dividual parsers, each of which requires a signifi-cant effort to build.
Furthermore, those techniqueshave largely relied on different voting schemes in thepast (Henderson and Brill, 1999; Sagae and Lavie,2006), and only more recently have started using ac-tual posteriors from the underlying models (Fossumand Knight, 2009; Zhang et al, 2009).
Even then,those methods operate only over k-best lists, and weare the first to work directly with parse forests frommultiple grammars.It is also interesting to note that the best resultsin Zhang et al (2009) are achieved by combining k-best lists from a latent variable grammar of Petrovet al (2006) with the self-trained reranking parser ofMcClosky et al (2006).
Clearly, replacing the sin-gle latent variable grammar with a product of latentvariable grammars ought to improve performance.The results on the other two corpora are similar.A product of latent variable grammars very signifi-cantly outperforms a single latent variable grammarand sets new standards for the state-of-the-art.We also analyzed the errors of the product mod-els.
In addition to the illustrative example in Fig-ure 5, we computed detailed error metrics for differ-ent phrasal categories.
Figure 4 shows that a productof four random grammars is always better than eventhe best underlying grammar.
The individual gram-mars seem to learn different sets of constraints, andthe product model is able to model them all at once,giving consistent accuracy improvements across allmetrics.5 ConclusionsWe presented a simple product model that signifi-cantly improves parsing accuracies on different do-mains and languages.
Our model leverages multi-ple automatically learned latent variable grammars,which differ only in the seed of the random num-ber generator used to initialize the EM learning al-Type all sentencesParser LP LR EXENGLISH-WSJThis Paper 92.0 91.7 41.9SINGLE Charniak (2000) 89.9 89.5 37.2Petrov and Klein (2007) 90.2 90.1 36.7Carreras et al (2008) 91.4 90.7 -RE Charniak et al (2005) 91.8 91.2 44.8Huang (2008) 92.2 91.2 43.5SELF Huang and Harper (2009) 91.37 91.57 39.37McClosky et al (2006) 92.5 92.1 45.3COMBO Sagae and Lavie (2006) 93.2 91.0 -Fossum and Knight (2009) 93.2 91.7 -Zhang et al (2009) 93.3 92.0 -ENGLISH-BROWNThis Paper 86.5 86.3 35.8SING Charniak (2000) 82.9 82.9 31.7Petrov and Klein (2007) 83.9 83.8 29.6RE Charniak et al (2005) 86.1 85.2 36.8GERMANThis Paper 84.5 84.0 51.2SING Petrov and Klein (2007) 80.0 80.2 42.4Petrov and Klein (2008) 80.6 80.8 43.9Table 2: Final test set accuracies for English and German.gorithm.
As our analysis showed, the grammars varywidely, making very different errors.
This is in partdue to the fact that EM is used not only for estimat-ing the parameters of the grammar, but also to deter-mine the set of context-free productions that under-lie it.
Because the resulting data representations arelargely independent, they can be easily combined inan unweighted product model.
The product modeldoes not require any additional training and is ca-pable of significantly improving the state-of-the-artin parsing accuracy.
It remains to be seen if a sim-ilar approach can be used in other cases where EMconverges to widely varying local maxima.AcknowledgementsI would like to thank Ryan McDonald for numerousdiscussions on this topic and his feedback on earlierversions of this paper.
This work also benefited fromconversations with Gideon Mann, Fernando Pereira,Dan Klein and Mehryar Mohri.7Note that these results are on a modified version of the tree-bank where unary productions are removed.26ReferencesJ.
Baldridge and M. Osborne.
2008.
Active learning andlogarithmic opinion pools for HPSG parse selection.Natural Language Engineering.R.
F. Bordley.
1982.
A multiplicative formula for aggre-gating probability assessments.
Management Science.L.
Breiman.
1996.
Bagging predictors.
Machine Learn-ing.A.
Brown and G. Hinton.
2001.
Products of hiddenMarkov models.
In AISTATS ?01.X.
Carreras, M. Collins, and T. Koo.
2008.
TAG, dy-namic programming, and the perceptron for efficient,feature-rich parsing.
In CoNLL ?08.E.
Charniak and M. Johnson.
2005.
Coarse-to-Fine N-Best Parsing and MaxEnt Discriminative Reranking.In ACL?05.E.
Charniak.
1996.
Tree-bank grammars.
In AAAI ?96.E.
Charniak.
2000.
A maximum?entropy?inspiredparser.
In NAACL ?00.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, UPenn.V.
Fossum and K. Knight.
2009.
Combining constituentparsers.
In NAACL ?09.W.
N. Francis and H. Kucera.
1979.
Manual of infor-mation to accompany a standard corpus of present-dayedited American English.
Technical report, BrownUniversity.Y.
Freund and R. E. Shapire.
1996.
Experiments with anew boosting algorithm.
In ICML ?96.C.
Genest and J. V. Zidek.
1986.
Combining probabilitydistributions: A critique and an annotated bibliogra-phy.
Statistical Science.D.
Gildea.
2001.
Corpus variation and parser perfor-mance.
EMNLP ?01.J.
Goodman.
1996.
Parsing algorithms and metrics.
ACL?96.J.
Henderson and E. Brill.
1999.
Exploiting diversityin natural language processing: combining parsers.
InEMNLP ?99.J.
Henderson and E. Brill.
2000.
Bagging and boosting atreebank parser.
In NAACL ?00.T.
Heskes.
1998.
Selecting weighting factors in logarith-mic opinion pools.
In NIPS ?98.G.
Hinton.
2001.
Products of experts.
In ICANN ?01.G.
Hinton.
2002.
Training products of experts by mini-mizing contrastive divergence.
Neural Computation.L.
Huang and D. Chiang.
2005.
Better k-best parsing.
InIWPT ?05.Z.
Huang and M. Harper.
2009.
Self-training PCFGgrammars with latent annotations across languages.
InEMNLP ?09.L.
Huang.
2008.
Forest reranking: Discriminative pars-ing with non-local features.
In ACL ?08.M.
Johnson.
1998.
PCFG models of linguistic tree rep-resentations.
Computational Linguistics, 24.D.
Klein and C. Manning.
2003a.
A* parsing: fast exactviterbi parse selection.
In NAACL ?03.D.
Klein and C. Manning.
2003b.
Accurate unlexicalizedparsing.
In ACL ?03.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional Random Fields: Probabilistic models for seg-menting and labeling sequence data.
In ICML ?01.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: ThePenn Treebank.
In Computational Linguistics.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic CFG with latent annotations.
In ACL ?05.D.
McClosky, E. Charniak, and M. Johnson.
2006.
Ef-fective self-training for parsing.
In NAACL ?06.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In NAACL ?07.S.
Petrov and D. Klein.
2008.
Sparse multi-scale gram-mars for discriminative latent variable parsing.
InEMNLP ?08.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In ACL ?06.K.
Sagae and A. Lavie.
2006.
Parser combination byreparsing.
In NAACL ?06.K.
Sima?an.
2002.
Computatoinal complexity of proba-bilistic disambiguation.
Grammars.W.
Skut, B. Krenn, T. Brants, and H. Uszkoreit.
1997.An annotation scheme for free word order languages.In ANLP ?97.A.
Smith and M. Osborne.
2007.
Diversity in logarith-mic opinion pools.
Lingvisticae Investigationes.A.
Smith, T. Cohn, and M. Osborne.
2005.
Logarithmicopinion pools for conditional random fields.
In ACL?05.X.
Sun and J. Tsujii.
2009.
Sequential labeling with la-tent variables: An exact inference algorithm and itsefficient approximation.
In EACL ?09.D.
Tax, M. Van Breukelen, R. Duin, and J. Kittler.
2000.Combining multiple classifiers by averaging or bymultiplying?
Pattern Recognition.I.
Titov and J. Henderson.
2006.
Loss minimization inparse reranking.
In EMNLP ?06.P.
Xu and F. Jelinek.
2004.
Random forests in languagemodeling.
In EMNLP ?04.H.
Zhang, M. Zhang, C. L. Tan, and H. Li.
2009.
K-bestcombination of syntactic parsers.
In EMNLP ?09.27
