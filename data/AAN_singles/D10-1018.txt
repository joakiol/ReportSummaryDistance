Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 177?186,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsBetter Punctuation Prediction with Dynamic Conditional Random FieldsWei Lu and Hwee Tou NgDepartment of Computer ScienceNational University of Singapore13 Computing Drive, Singapore 117417{luwei,nght}@comp.nus.edu.sgAbstractThis paper focuses on the task of insert-ing punctuation symbols into transcribed con-versational speech texts, without relying onprosodic cues.
We investigate limitations as-sociated with previous methods, and propose anovel approach based on dynamic conditionalrandom fields.
Different from previous work,our proposed approach is designed to jointlyperform both sentence boundary and sentencetype prediction, and punctuation prediction onspeech utterances.We performed evaluations on a transcribedconversational speech domain consisting ofboth English and Chinese texts.
Empirical re-sults show that our method outperforms an ap-proach based on linear-chain conditional ran-dom fields and other previous approaches.1 IntroductionOutputs of standard automatic speech recognition(ASR) systems typically consist of utterances whereimportant linguistic and structural information (e.g.,true case, sentence boundaries, punctuation sym-bols, etc) is not available.
Such information is cru-cial in improving the readability of the transcribedspeech texts, and plays an important role when fur-ther processing is required, such as in part-of-speech(POS) tagging, parsing, information extraction, andmachine translation.We focus on the punctuation prediction task inthis work.
Most previous punctuation predictiontechniques, developed mostly by the speech process-ing community, exploit both lexical and prosodiccues.
However, in order to fully exploit prosodic fea-tures such as pitch and pause duration, it is necessaryto have access to the original raw speech waveforms.In some scenarios where further natural languageprocessing (NLP) tasks on the transcribed speechtexts become the main concern, speech prosody in-formation may not be readily available.
For exam-ple, in the recent evaluation campaign of the Inter-national Workshop on Spoken Language Translation(IWSLT) (Paul, 2009), only manually transcribed orautomatically recognized speech texts are providedbut the original raw speech waveforms are not avail-able.In this paper, we tackle the task of predictingpunctuation symbols from a standard text processingperspective, where only the speech texts are avail-able, without relying on additional prosodic fea-tures such as pitch and pause duration.
Specifi-cally, we perform the punctuation prediction taskon transcribed conversational speech texts, using theIWSLT corpus (Paul, 2009) as the evaluation data.Different from many other corpora such as broad-cast news corpora, a conversational speech corpusconsists of dialogs where informal and short sen-tences frequently appear.
In addition, due to thenature of conversation, it also contains more ques-tion sentences compared to other corpora.
An ex-ample English utterance randomly selected from theIWSLT corpus, along with its punctuated and casedversion, are shown below:you are quite welcome and by the way we may getother reservations so could you please call usas soon as you fix the dateYou are quite welcome .
And by the way , we mayget other reservations , so could you please callus as soon as you fix the date ?177The rest of this paper is organized as follows.We start with surveying related work in Section 2.One class of widely-used previous techniques is thenstudied in detail in Section 3.
Next, we investigatemethods for improving existing methods in Section4 and 5.
Empirical evaluation results are presentedand discussed in Section 6.
We finally conclude inSection 7.2 Related WorkPunctuation prediction has been extensively studiedin the speech processing field.
It is also sometimesstudied together with a closely related task ?
sen-tence boundary detection.Much previous work assumes that both lexicaland prosodic cues are available for the task.
Kimand Woodland (2001) performed punctuation inser-tion during speech recognition.
Prosodic features to-gether with language model probabilities were usedwithin a decision tree framework.
Christensen etal.
(2001) focused on the broadcast news domainand investigated both finite state and multi-layer per-ceptron methods for the task, where prosodic andlexical information was incorporated.
Huang andZweig (2002) presented a maximum entropy-basedtagging approach to punctuation insertion in spon-taneous English conversational speech, where bothlexical and prosodic features were exploited.
Liuet al (2005) focused on the sentence boundary de-tection task, by making use of conditional randomfields (CRF) (Lafferty et al, 2001).
Their methodwas shown to improve over a previous method basedon hidden Markov model (HMM).There is relatively less work that exploited lexicalfeatures only.
Beeferman et al (1998) focused oncomma prediction with a trigram language model.
Ajoint language model was learned from punctuatedtexts, and commas were inserted so as to maximizethe joint probability score.
Recent work by Gravanoet al (2009) presented a purely n-gram based ap-proach that jointly predicted punctuation and caseinformation of English.Stolcke et al (1998) presented a ?hidden eventlanguage model?
that treated boundary detectionand punctuation insertion as an interword hiddenevent detection task.
Their proposed method wasimplemented in the handy utility hidden-ngram aspart of the SRILM toolkit (Stolcke, 2002).
It waswidely used in many recent spoken language trans-lation tasks as either a preprocessing (Wang et al,2008) or postprocessing (Kirchhoff and Yang, 2007)step.
More details about this model will be given inthe next section.Recently, there are also several research effortsthat try to optimize some downstream applicationafter punctuation prediction, rather than the predic-tion task itself.
Examples of such downstream ap-plications include punctuation prediction for part-of-speech (POS) tagging and name tagging (Hillard etal., 2006), statistical machine translation (Matusovet al, 2006), and information extraction (Favre etal., 2008).3 Hidden Event Language ModelMany previous research efforts consider the bound-ary detection and punctuation insertion task as a hid-den event detection task.
One such well-known ap-proach was introduced by Stolcke et al (1998).They adopted a HMM to describe a joint distribu-tion over words and interword events, where the ob-servations are the words, and the word/event pairsare encoded as hidden states.
Specifically, in thistask word boundaries and punctuation symbols areencoded as interword events.
The training phaseinvolves training an n-gram language model overall observed words and events with smoothing tech-niques.
The learned n-gram probability scores arethen used as the HMM state-transition scores.
Dur-ing testing, the posterior probability of an eventat each word is computed with dynamic program-ming using the forward-backward algorithm.
Thesequence of most probable states thus forms the out-put which gives the punctuated sentence.Such a HMM-based approach has several draw-backs.
First, the n-gram language model is onlyable to capture surrounding contextual information.However, we argue that in many cases, modeling oflonger range dependencies is required for punctua-tion insertion.
For example, the method is unableto effectively capture the long range dependency be-tween the initial phrase ?would you?
which stronglyindicates a question sentence, and an ending ques-tion mark.
This hurts the punctuation prediction per-formance for our task since we are particularly inter-178ested in conversational speech texts where questionsentences appear frequently.Thus, in practice, special techniques are usuallyrequired on top of using a hidden event languagemodel in order to overcome long range dependen-cies.
Examples include relocating or duplicatingpunctuation symbols to different positions of a sen-tence such that they appear closer to the indicativewords (e.g., ?how much?
indicates a question sen-tence).
One such technique was introduced by theorganizers of the IWSLT evaluation campaign, whosuggested duplicating the ending punctuation sym-bol to the beginning of each sentence before trainingthe language model1.
Empirically, the technique hasdemonstrated its effectiveness in predicting questionmarks in English, since most of the indicative wordsfor English question sentences appear at the begin-ning of a question.
However, such a technique isspecially designed and may not be widely applica-ble in general or to languages other than English.Furthermore, a direct application of such a methodmay fail in the event of multiple sentences per utter-ance without clearly annotated sentence boundarieswithin an utterance.Another drawback associated with such an ap-proach is that the method encodes strong depen-dency assumptions between the punctuation symbolto be inserted and its surrounding words.
Thus, itlacks the robustness to handle cases where noisy orout-of-vocabulary (OOV) words frequently appear,such as in texts automatically recognized by ASRsystems.
In this paper, we devise techniques basedon conditional random fields to tackle the difficultiesdue to long range dependencies.4 Linear-Chain Conditional RandomFieldsOne natural approach to relax the strong depen-dency assumptions encoded by the hidden event lan-guage model is to adopt an undirected graphicalmodel, where arbitrary overlapping features can beexploited.Conditional random fields (CRF) (Lafferty et al,2001) have been widely used in various sequencelabeling and segmentation tasks (Sha and Pereira,1http://mastarpj.nict.go.jp/IWSLT2008/downloads/case+punc tool using SRILM.instructions.txt2003; Tseng et al, 2005).
Unlike a HMM whichmodels the joint distribution of both the label se-quence and the observation, a CRF is a discrimi-native model of the conditional distribution of thecomplete label sequence given the observation.Specifically, a first-order linear-chain CRF whichassumes first-order Markov property is defined bythe following equation:p?
(y|x) =1Z(x)exp(?t?k?kfk(x, yt?1, yt, t))(1)where x is the observation and y is the label se-quence.
Feature functions fk with time step t aredefined over the entire observation x and two adja-cent hidden labels.
Z(x) is a normalization factor toensure a well-formed probability distribution.
Fig-ure 1 gives a simplified graphical representation ofthe model, where only the dependencies between la-bel and observation in the same time step are shown.y1x1y2x2y3x3.
.
.
ynxnFigure 1: A simplified graphical representation for linear-chain CRF (observations are shaded)proposed tagsNONE COMMA (,) PERIOD (.
)QMARK (?)
EMARK (!
)Table 1: The set of all possible tags for linear-chain CRFWe can model the punctuation prediction task asthe process of assigning a tag to each word, wherethe set of possible tags is given in Table 1.
Thatis, we assume each word can be associated withan event, which tells us which punctuation sym-bol (possibly NONE) should be inserted after theword.
The training data consists of a set of utter-ances where punctuation symbols are encoded astags that are assigned to the individual words.
Thetag NONE means no punctuation symbol is insertedafter the current word.
Any other tag refers to insert-ing the corresponding punctuation symbol.
In thetesting phase, the most probable sequence of tags is179Sentence: no , please do not .
would you save your questions for the end of my talk , when i ask for them ?no please do not would you .
.
.
my talk when .
.
.
themCOMMA NONE NONE PERIOD NONE NONE .
.
.
NONE COMMA NONE .
.
.
QMARKFigure 2: An example tagging of a training sentence for the linear-chain CRFpredicted and the punctuated text can then be con-structed from such an output.
An example taggingof an utterance is illustrated in Figure 2.Following (Sutton et al, 2007), we factorize afeature of conditional random fields as a productof a binary function on assignment of the set ofcliques at the current time step (in this case an edge),and a feature function solely defined on the ob-servation sequence.
n-gram occurrences surround-ing the current word, together with position infor-mation, are used as binary feature functions, forn = 1, 2, 3.
All words that appear within 5 wordsfrom the current word are considered when build-ing the features.
Special start and end symbols areused beyond the utterance boundaries.
For example,for the word do shown in Figure 2, example fea-tures include unigram features do@0, please@-1,bigram feature would+you@[2,3], and trigram fea-ture no+please+do@[-2,0].Such a linear-chain CRF model is capable of mod-eling dependencies between words and punctuationsymbols with arbitrary overlapping features, thusavoiding the strong dependency assumptions in thehidden event language model.
However, the linear-chain CRF model still exhibits several problems forthe punctuation task.
In particular, the dependencybetween the punctuation symbols and the indicativewords cannot be captured adequately, if they appeartoo far away from each other.
For example, in thesample utterance shown in Figure 2, the long rangedependency between the ending question mark andthe indicative words would you which appear veryfar away cannot be directly captured.
The problemarises because a linear-chain CRF only learns a se-quence of tags at the individual word level but is notfully aware of sentence level information, such asthe start and end of a complete sentence.Hence, it would be more reasonable to hypothe-size that the punctuation symbols are annotated atthe sentence level, rather than relying on a limitedwindow of surrounding words.
A model that canjointly perform sentence segmentation and sentencetype prediction, together with word level punctu-ation prediction would be more beneficial for ourtask.
This motivates us to build a joint model forperforming such a task, to be presented in the nextsection.5 Factorial Conditional Random FieldsExtensions to the linear-chain CRF model have beenproposed in previous research efforts to encode longrange dependencies.
One such well-known exten-sion is the semi-Markov CRF (semi-CRF) (Sarawagiand Cohen, 2005).
Motivated by the hidden semi-Markov model, the semi-CRF is particularly helpfulin text chunking tasks as it allows a state to persistfor a certain interval of time steps.
This in practiceoften leads to better modeling capability of chunks,since state transitions within a chunk need not pre-cisely follow the Markov property as in the case oflinear-chain CRF.
However, it is not clear how sucha model can benefit our task, which requires word-level labeling in addition to sentence boundary de-tection and sentence type prediction.The skip-chain CRF (Sutton and McCallum,2004), another variant of linear-chain CRF, attachesadditional edges on top of a linear-chain CRF forbetter modeling of long range dependencies betweenstates with similar observations.
However, such amodel usually requires known long range dependen-cies in advance and may not be readily applicable toour task where such clues are not explicit.As we have discussed above, since we wouldlike to jointly model both the word-level labelingtask and the sentence-level annotation task (sentenceboundary detection and sentence type prediction),introducing an additional layer of tags to performboth tasks together would be desirable.
In this sec-tion, we propose the use of factorial CRF (F-CRF)(Sutton et al, 2007), which has previously beenshown to be effective for joint labeling of multiplesequences (McCallum et al, 2003).180The F-CRF as a specific case of dynamic condi-tional random fields was originally motivated fromdynamic Bayesian networks, where an identicalstructure repeats over different time steps.
Analo-gous to the linear-chain CRF, one can think of the F-CRF as a framework that provides the capability ofsimultaneously labeling multiple layers of tags for agiven sequence.
It learns a joint conditional distri-bution of the tags given the observation.
Formally,dynamic conditional random fields define the con-ditional probability of a sequence of label vectors ygiven the observation x as:p?
(y|x) =1Z(x)exp(?t?c?C?k?kfk(x, y(c,t), t))(2)where cliques are indexed at each time step, C is a setof clique indices, and y(c,t) is the set of variables inthe unrolled version of a clique with index c at timet (Sutton et al, 2007).
Figure 3 gives a graphicalrepresentation of a two-layer factorial CRF, wherethe cliques include the two within-chain edges (e.g.,z2 ?
z3 and y2 ?
y3) and one between-chain edge(e.g., z3 ?
y3) at each time step.z1y1x1z2y2x2z3y3x3.
.
.. .
.znynxnFigure 3: A two-layer factorial CRFlayer proposed tagsword NONE,COMMA,PERIOD,QMARK,EMARKsentence DEBEG,DEIN,QNBEG,QNIN,EXBEG,EXINTable 2: The set of all possible tags proposed for eachlayerWe build two layers of labels for this task, aslisted in Table 2.
The word layer tags are respon-sible for inserting a punctuation symbol (includingNONE) after each word, while the sentence layertags are used for annotating sentence boundaries andidentifying the sentence type (declarative, question,or exclamatory).
Tags from the word layer are thesame as those of the linear-chain CRF.
The sentencelayer tags are designed for three types of sentences.DEBEG and DEIN indicate the start and the innerpart of a declarative sentence respectively, likewisefor QNBEG and QNIN (question sentences), as wellas EXBEG and EXIN (exclamatory sentences).
Thesame example utterance we looked at in the previoussection is now tagged with these two layers of tags,as shown in Figure 4.
Analogous feature factoriza-tion and the same n-gram feature functions used inlinear-chain CRF are used in F-CRF.When learning the sentence layer tags togetherwith the word layer tags, the F-CRF model is capa-ble of leveraging useful clues learned from the sen-tence layer about sentence type (e.g., a question sen-tence, annotated with QNBEG, QNIN, QNIN, .
.
.,or a declarative sentence, annotated with DEBEG,DEIN, DEIN, .
.
.
), which can be used to guide theprediction of the punctuation symbol at each word,hence improving the performance at the word layer.For example, consider jointly labeling the utteranceshown in Figure 4.
Intuitively, when evidencesshow that the utterance consists of two sentences ?a declarative sentence followed by a question sen-tence, the model tends to annotate the second half ofthe utterance with the sequence QNBEG QNIN .
.
..This in turn helps to predict the word level tag atthe end of the utterance as QMARK, given the de-pendencies between the two layers existing at eachtime step.
In practice, during the learning process,the two layers of tags are jointly learned, thus pro-viding evidences that influence each other?s taggingprocess.In this work, we use the GRMM package (Sutton,2006) for building both the linear-chain CRF (L-CRF) and factorial CRF (F-CRF).
The tree-basedreparameterization (TRP) schedule for belief propa-gation (Wainwright et al, 2001) is used for approxi-mate inference.6 ExperimentsWe perform experiments on part of the corpus of theIWSLT09 evaluation campaign (Paul, 2009), whereboth Chinese and English conversational speech181Sentence: no , please do not .
would you save your questions for the end of my talk , when i ask for them ?no please do not would you .
.
.
my talk when .
.
.
themCOMMA NONE NONE PERIOD NONE NONE .
.
.
NONE COMMA NONE .
.
.
QMARKDEBEG DEIN DEIN DEIN QNBEG QNIN .
.
.
QNIN QNIN QNIN .
.
.
QNINFigure 4: An example tagging of a training sentence for the factorial CRFtexts are used.
Two multilingual datasets are consid-ered, the BTEC (Basic Travel Expression Corpus)dataset and the CT (Challenge Task) dataset.
Theformer consists of tourism-related sentences, and thelatter consists of human-mediated cross-lingual di-alogs in travel domain.
The official IWSLT09 BTECtraining set consists of 19,972 Chinese-English ut-terance pairs, and the CT training set consists of10,061 such pairs.
We randomly split each of thetwo datasets into two portions, where 90% of the ut-terances are used for training the punctuation predic-tion models, and the remaining 10% for evaluatingthe prediction performance.
For all the experiments,we use the default segmentation of Chinese as pro-vided, and English texts are preprocessed with thePenn Treebank tokenizer2.
We list the statistics ofthe two datasets after processing in Table 3.
Theproportions of sentence types in the two datasets arelisted.
The majority of the sentences are declarativesentences.
However, question sentences are morefrequent in the BTEC dataset compared to the CTdataset.
Exclamatory sentences contribute less than1% for all datasets and are not listed.
We also counthow often each utterance consists of multiple sen-tences.
The utterances from the CT dataset are muchlonger (with more words per utterance), and there-fore more CT utterances actually consist of multiplesentences.BTEC CTCN EN CN ENdeclarative sent.
64% 65% 77% 81%question sent.
36% 35% 22% 19%multi.sent./uttr.
14% 17% 29% 39%avg.words./uttr.
8.59 9.46 10.18 14.33Table 3: Statistics of the BTEC and CT datasetsFor the methods based on the hidden event lan-guage model, we design extensive experiments due2http://www.cis.upenn.edu/?treebank/tokenization.htmlto many possible setups.
Specifically, these exper-iments can be divided into two categories: with orwithout duplicating the ending punctuation symbolto the start of a sentence before training.
This set-ting can be used to assess the impact of the proxim-ity between the punctuation symbol and the indica-tive words for the prediction task.
Under each cat-egory, two possible approaches are tried.
The sin-gle pass approach performs prediction in one sin-gle step, where all the punctuation symbols are pre-dicted sequentially from left to right.
In the cas-caded approach, we format the training sentencesby replacing all sentence-ending punctuation sym-bols with special sentence boundary symbols first.A model for sentence boundary prediction is learnedbased on such training data.
This step is then fol-lowed by predicting the actual punctuation symbols.Both trigram and 5-gram language models are triedfor all combinations of the above settings.
This givesus a total of 8 possible combinations based on thehidden event language model.
When training all thelanguage models, modified Kneser-Ney smoothing(Chen and Goodman, 1996) for n-grams is used.To assess the performance of the punctuation pre-diction task, we compute precision (prec.
), recall(rec.
), and F1-measure (F1), as defined by the fol-lowing equations:prec.
=# Correctly predicted punctuation symbols# predicted punctuation symbolsrec.
=# Correctly predicted punctuation symbols# expected punctuation symbolsF1 =21/prec.+ 1/rec.6.1 Performance on Correctly RecognizedTextsThe performance of punctuation prediction on bothChinese (CN) and English (EN) texts in the correctlyrecognized output of the BTEC and CT datasets arepresented in Table 4 and Table 5 respectively.
The182BTECNO DUPLICATION USE DUPLICATIONSINGLE PASS CASCADED SINGLE PASS CASCADED L-CRF F-CRFLM ORDER 3 5 3 5 3 5 3 5CNPrec.
87.40 86.44 87.72 87.13 76.74 77.58 77.89 78.50 94.82 94.83Rec.
83.01 83.58 82.04 83.76 72.62 73.72 73.02 75.53 87.06 87.94F1 85.15 84.99 84.79 85.41 74.63 75.60 75.37 76.99 90.78 91.25ENPrec.
64.72 62.70 62.39 58.10 85.33 85.74 84.44 81.37 88.37 92.76Rec.
60.76 59.49 58.57 55.28 80.42 80.98 79.43 77.52 80.28 84.73F1 62.68 61.06 60.42 56.66 82.80 83.29 81.86 79.40 84.13 88.56Table 4: Punctuation prediction performance on Chinese (CN) and English (EN) texts in the correctly recognizedoutput of the BTEC dataset.
Percentage scores of precision (Prec.
), recall (Rec.
), and F1 measure (F1) are reported.CTNO DUPLICATION USE DUPLICATIONSINGLE PASS CASCADED SINGLE PASS CASCADED L-CRF F-CRFLM ORDER 3 5 3 5 3 5 3 5CNPrec.
89.14 87.83 90.97 88.04 74.63 75.42 75.37 76.87 93.14 92.77Rec.
84.71 84.16 77.78 84.08 70.69 70.84 64.62 73.60 83.45 86.92F1 86.87 85.96 83.86 86.01 72.60 73.06 69.58 75.20 88.03 89.75ENPrec.
73.86 73.42 67.02 65.15 75.87 77.78 74.75 74.44 83.07 86.69Rec.
68.94 68.79 62.13 61.23 70.33 72.56 69.28 69.93 76.09 79.62F1 71.31 71.03 64.48 63.13 72.99 75.08 71.91 72.12 79.43 83.01Table 5: Punctuation prediction performance on Chinese (CN) and English (EN) texts in the correctly recognizedoutput of the CT dataset.
Percentage scores of precision (Prec.
), recall (Rec.
), and F1 measure (F1) are reported.performance of the hidden event language modelheavily depends on whether the duplication methodis used and on the actual language under considera-tion.
Specifically, for English, duplicating the end-ing punctuation symbol to the start of a sentencebefore training is shown to be very helpful in im-proving the overall prediction performance.
In con-trast, applying the same technique to Chinese hurtsthe performance.This observed difference is reasonable and ex-pected.
An English question sentence usually startswith indicative words such as do you or where thatdistinguish it from a declarative sentence.
Thus, du-plicating the ending punctuation symbol to the startof a sentence so that it is near these indicative wordshelps to improve the prediction accuracy.
However,Chinese presents quite different syntactic structuresfor question sentences.
First, we found that in manycases, Chinese tends to use semantically vague aux-iliary words at the end of a sentence to indicate aquestion.
Such auxiliary words include ?
and ?.Thus, retaining the position of the ending punctu-ation symbol before training yields better perfor-mance.
Another interesting finding is that, differ-ent from English, other words that indicate a ques-tion sentence in Chinese can appear at almost anyposition in a Chinese sentence.
Examples include???.
.
.
(where .
.
.
), .
.
.???
(what .
.
.
), or.
.
.??.
.
.
(how many/much .
.
.
).
These pose diffi-culties for the simple hidden event language model,which only encodes simple dependencies over sur-rounding words by means of n-gram language mod-eling.By adopting a discriminative model which ex-ploits non-independent, overlapping features, the L-CRF model generally outperforms the hidden eventlanguage model.
By introducing an additional layerof tags for performing sentence segmentation andsentence type prediction, the F-CRF model furtherboosts the performance over the L-CRF model.
Weperform statistical significance tests using bootstrapresampling (Efron et al, 1993).
The improvementsof F-CRF over L-CRF are statistically significant(p < 0.01) on Chinese and English texts in the CT183BTECNO DUPLICATION USE DUPLICATIONSINGLE PASS CASCADED SINGLE PASS CASCADED L-CRF F-CRFLM ORDER 3 5 3 5 3 5 3 5CNPrec.
85.96 84.80 86.48 85.12 66.86 68.76 68.00 68.75 92.81 93.82Rec.
81.87 82.78 83.15 82.78 63.92 66.12 65.38 66.48 85.16 89.01F1 83.86 83.78 84.78 83.94 65.36 67.41 66.67 67.60 88.83 91.35ENPrec.
62.38 59.29 56.86 54.22 85.23 87.29 84.49 81.32 90.67 93.72Rec.
64.17 60.99 58.76 56.21 88.22 89.65 87.58 84.55 88.22 92.68F1 63.27 60.13 57.79 55.20 86.70 88.45 86.00 82.90 89.43 93.19Table 6: Punctuation prediction performance on Chinese (CN) and English (EN) texts in the ASR output of IWSLT08BTEC evaluation dataset.
Percentage scores of precision (Prec.
), recall (Rec.
), and F1 measure (F1) are reported.dataset, and on English texts in the BTEC dataset.The improvements of F-CRF over L-CRF on Chi-nese texts are smaller, probably because L-CRFis already performing quite well on Chinese.
F1measures on the CT dataset are lower than thoseon BTEC, mainly because the CT dataset consistsof longer utterances and fewer question sentences.Overall, our proposed F-CRF model is robust andconsistently works well regardless of the languageand dataset it is tested on.
This indicates that theapproach is general and relies on minimal linguisticassumptions, and thus can be readily used on otherlanguages and datasets.6.2 Performance on Automatically RecognizedTextsSo far we only evaluated punctuation prediction per-formance on transcribed texts consisting of correctlyrecognized words.
We now present the evaluationresults on texts produced by ASR systems.For evaluation, we use the 1-best ASR outputs ofspontaneous speech of the official IWSLT08 BTECevaluation dataset, which is released as part of theIWSLT09 corpus.
The dataset consists of 504 utter-ances in Chinese, and 498 in English.
Unlike thecorrectly recognized texts described in Section 6.1,the ASR outputs contain substantial recognition er-rors (recognition accuracy is 86% for Chinese, and80% for English (Paul, 2008)).
In the dataset re-leased by the IWSLT organizers, the correct punctu-ation symbols are not annotated in the ASR outputs.To conduct our experimental evaluation, we manu-ally annotated the correct punctuation symbols onthe ASR outputs.We used all the learned models in Section 6.1, andapplied them to this dataset.
The evaluation resultsare shown in Table 6.
The results show that F-CRFstill gives higher performance than L-CRF and thehidden event language model, and the improvementsare statistically significant (p < 0.01).6.3 Performance in TranslationThe evaluation process as described in Section 6.2requires substantial manual efforts to annotate thecorrect punctuation symbols.
In this section, we in-stead adopt an indirect approach to automaticallyevaluate the performance of punctuation predictionon ASR output texts by feeding the punctuated ASRtexts to a state-of-the-art machine translation sys-tem, and evaluate the resulting translation perfor-mance.
The translation performance is in turn mea-sured by an automatic evaluation metric which cor-relates well with human judgments.
We believethat such a task-oriented approach for evaluating thequality of punctuation prediction for ASR outputtexts is useful, since it tells us how well the punc-tuated ASR output texts from each punctuation pre-diction system can be used for further processing,such as in statistical machine translation.In this paper, we use Moses (Koehn et al, 2007),a state-of-the-art phrase-based statistical machinetranslation toolkit, as our translation engine.
Weuse the entire IWSLT09 BTEC training set for train-ing the translation system.
The state-of-the-art un-supervised Berkeley aligner3 (Liang et al, 2006) isused for aligning the training bitext.
We use allthe default settings of Moses, except with the lexi-calized reordering model enabled.
This is because3http://code.google.com/p/berkeleyaligner/184NO DUPLICATION USE DUPLICATIONSINGLE PASS CASCADED SINGLE PASS CASCADED L-CRF F-CRFLM ORDER 3 5 3 5 3 5 3 5CN?
EN 30.77 30.71 30.98 30.64 30.16 30.26 30.33 30.42 31.27 31.30EN?
CN 21.21 21.00 21.16 20.76 23.03 24.04 23.61 23.34 23.44 24.18Table 7: Translation performance on punctuated ASR outputs using Moses (Averaged percentage scores of BLEU)lexicalized reordering gives better performance thansimple distance-based reordering (Koehn et al,2005).
Specifically, the default lexicalized reorder-ing model (msd-bidirectional-fe) is used.For tuning the parameters of Moses, we use theofficial IWSLT05 evaluation set where the correctpunctuation symbols are present.
Evaluations areperformed on the ASR outputs of the IWSLT08BTEC evaluation dataset, with punctuation symbolsinserted by each punctuation prediction method.The tuning set and evaluation set include 7 referencetranslations.
Following a common practice in statis-tical machine translation, we report BLEU-4 scores(Papineni et al, 2002), which were shown to havegood correlation with human judgments, with theclosest reference length as the effective referencelength.
The minimum error rate training (MERT)(Och, 2003) procedure is used for tuning the modelparameters of the translation system.
Due to the un-stable nature of MERT, we perform 10 runs for eachtranslation task, with a different random initializa-tion of parameters in each run, and report the BLEU-4 scores averaged over 10 runs.The results are reported in Table 7.
The besttranslation performances for both translation direc-tions are achieved by applying F-CRF as the punc-tuation prediction model to the ASR texts.
Such im-provements are observed to be consistent over dif-ferent runs.
The improvement of F-CRF over L-CRF in translation quality is statistically significant(p < 0.05) when translating from English to Chi-nese.
In addition, we also assess the translationperformance when the manually annotated punctu-ation symbols as mentioned in Section 6.2 are usedfor translation.
The averaged BLEU scores for thetwo translation tasks are 31.58 (Chinese to English)and 24.16 (English to Chinese) respectively, whichshow that our punctuation prediction method givescompetitive performance for spoken language trans-lation.It is important to note that in this work, we onlyfocus on optimizing the punctuation prediction per-formance in the form of F1-measure, without regardto the subsequent NLP tasks.
How to perform punc-tuation prediction so as to optimize translation per-formance is an important research topic that is be-yond the scope of this paper and needs further in-vestigation in future work.7 ConclusionIn this paper, we have proposed a novel approachfor predicting punctuation symbols for transcribedconversational speech texts.
Our proposed approachis built on top of a dynamic conditional randomfields framework, which jointly performs punctua-tion prediction together with sentence boundary andsentence type prediction on speech utterances.
Un-like most previous work, it tackles the task from apurely text processing perspective and does not relyon prosodic cues.Experimental results have shown that our pro-posed approach outperforms the widely used ap-proach based on the hidden event language model,and also outperforms a method based on linear-chainconditional random fields.
Our proposed approachhas been shown to be general, working well on bothChinese and English, and on both correctly recog-nized and automatically recognized texts.
Our pro-posed approach also results in better translation ac-curacy when the punctuated automatically recog-nized texts are used in subsequent translation.AcknowledgmentsThis research was done for CSIDM Project No.CSIDM-200804 partially funded by a grant fromthe National Research Foundation (NRF) adminis-tered by the Media Development Authority (MDA)of Singapore.185ReferencesD.
Beeferman, A. Berger, and J. Lafferty.
1998.CYBERPUNC: A lightweight punctuation annotationsystem for speech.
In Proc.
of ICASSP?98.S.F.
Chen and J. Goodman.
1996.
An empirical study ofsmoothing techniques for language modeling.
In Proc.of ACL?06.H.
Christensen, Y. Gotoh, and S. Renals.
2001.
Punctu-ation annotation using statistical prosody models.
InProc.
of ISCA Workshop on Prosody in Speech Recog-nition and Understanding.B.
Efron, R. Tibshirani, and R.J. Tibshirani.
1993.
Anintroduction to the bootstrap.
Chapman & Hall/CRC.B.
Favre, R. Grishman, D. Hillard, H. Ji, D. Hakkani-Tur, and M. Ostendorf.
2008.
Punctuating speech forinformation extraction.
In Proc.
of ICASSP?08.A.
Gravano, M. Jansche, and M. Bacchiani.
2009.Restoring punctuation and capitalization in transcribedspeech.
In Proc.
of ICASSP?09.D.
Hillard, Z. Huang, H. Ji, R. Grishman, D. Hakkani-Tur, M. Harper, M. Ostendorf, and W. Wang.
2006.Impact of automatic comma prediction on POS/nametagging of speech.
In Proc.
of SLT?06.J.
Huang and G. Zweig.
2002.
Maximum entropy modelfor punctuation annotation from speech.
In Proc.
ofICSLP?02.J.H.
Kim and P.C.
Woodland.
2001.
The use of prosodyin a combined system for punctuation generation andspeech recognition.
In Proc.
of EuroSpeech?01.K.
Kirchhoff and M. Yang.
2007.
The University ofWashington machine translation system for the IWSLT2007 competition.
In Proc.
of IWSLT?07.P.
Koehn, A. Axelrod, A.B.
Mayne, C. Callison-Burch,M.
Osborne, and D. Talbot.
2005.
Edinburgh sys-tem description for the 2005 IWSLT speech translationevaluation.
In Proc.
of IWSLT?05.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proc.
ofACL?07 (Demo Session).J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
ofICML?01.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In Proc.
of HLT/NAACL?06.Y.
Liu, A. Stolcke, E. Shriberg, and M. Harper.
2005.Using conditional random fields for sentence boundarydetection in speech.
In Proc.
of ACL?05.E.
Matusov, A. Mauser, and H. Ney.
2006.
Automaticsentence segmentation and punctuation prediction forspoken language translation.
In Proc.
of IWSLT?06.A.
McCallum, K. Rohanimanesh, and C. Sutton.
2003.Dynamic conditional random fields for jointly labelingmultiple sequences.
In Proc.
of NIPS?03 Workshop onSyntax, Semantics and Statistics.F.J.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
of ACL?03.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL?02.M.
Paul.
2008.
Overview of the IWSLT 2008 evaluationcampaign.
In Proc.
of IWSLT?08.M.
Paul.
2009.
Overview of the IWSLT 2009 evaluationcampaign.
In Proc.
of IWSLT?09.S.
Sarawagi and W.W. Cohen.
2005.
Semi-Markov con-ditional random fields for information extraction.
InProc.
of NIPS?05.F.
Sha and F. Pereira.
2003.
Shallow parsing with condi-tional random fields.
In Proc.
of HLT-NAACL?03.A.
Stolcke, E. Shriberg, R. Bates, M. Ostendorf,D.
Hakkani, M. Plauche, G. Tur, and Y. Lu.
1998.Automatic detection of sentence boundaries and dis-fluencies based on recognized words.
In Proc.
of IC-SLP?98.A.
Stolcke.
2002.
SRILM?an extensible language mod-eling toolkit.
In Proc.
of ICSLP?02.C.
Sutton and A. McCallum.
2004.
Collective segmenta-tion and labeling of distant entities in information ex-traction.
In Proc.
of ICML?04 workshop on StatisticalRelational Learning.C.
Sutton, A. McCallum, and K. Rohanimanesh.
2007.Dynamic conditional random fields: Factorized prob-abilistic models for labeling and segmenting sequencedata.
Journal of Machine Learning Research, 8.C.
Sutton.
2006.
GRMM: GRaphical Models in Mallet.http://mallet.cs.umass.edu/grmm/.H.
Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-ning.
2005.
A conditional random field word seg-menter for sighan bakeoff 2005.
In Proc.
of the FourthSIGHAN Workshop on Chinese Language Processing.M.
Wainwright, T. Jaakkola, and A. Willsky.
2001.
Tree-based reparameterization for approximate inference onloopy graphs.
In Proc.
of NIPS?01.H.
Wang, H. Wu, X. Hu, Z. Liu, J. Li, D. Ren, andZ.
Niu.
2008.
The TCH machine translation systemfor IWSLT 2008.
In Proc.
of IWSLT?08.186
