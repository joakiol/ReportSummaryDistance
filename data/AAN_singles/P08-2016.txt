Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 61?64,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCombined One Sense Disambiguation of AbbreviationsYaakov HaCohen-KernerDepartment of ComputerScience, Jerusalem College ofTechnology (Machon Lev)21 Havaad Haleumi St., P.O.B.16031, 91160 Jerusalem, Israelkerner@jct.ac.ilAriel KassDepartment of ComputerScience, Jerusalem College ofTechnology (Machon Lev)21 Havaad Haleumi St., P.O.B.16031, 91160 Jerusalem, Israelariel.kass@gmail.comAriel PeretzDepartment of ComputerScience, Jerusalem College ofTechnology (Machon Lev)21 Havaad Haleumi St., P.O.B.16031, 91160 Jerusalem, Israelrelperetz@gmail.comAbstractA process that attempts to solve abbreviationambiguity is presented.
Various context-related features and statistical features havebeen explored.
Almost all features are domainindependent and language independent.
Theapplication domain is Jewish Law documentswritten in Hebrew.
Such documents areknown to be rich in ambiguous abbreviations.Various implementations of the one sense perdiscourse hypothesis are used, improving thefeatures with new variants.
An accuracy of96.09% has been achieved by SVM.1 IntroductionAn abbreviation is a letter or sequence of letters,which is a shortened form of a word or a sequenceof words, which is called the sense of theabbreviation.
Abbreviation disambiguation meansto choose the correct sense for a specific context.Jewish Law documents written in Hebrew areknown to be rich in ambiguous abbreviations(HaCohen-Kerner et al, 2004).
They can,therefore, serve as an excellent test-bed for thedevelopment of models for abbreviationdisambiguation.As opposed to the documents investigated inprevious systems, Jewish Law documents usuallydo not contain the sense of the abbreviations in thesame discourse.
Therefore, the abbreviations areregarded as more difficult to disambiguate.This research defines features, as well asexperiments with various variants of the one senseper discourse hypothesis.
The developed processconsiders other languages and does not define pre-execution assumptions.
The only limitation to thisprocess is the input itself: the languages of thedifferent text documents and the man-madesolution database inputted during the learningprocess limit the datasets of documents that may besolved by the resulting disambiguation system.The proposed system, preserves its portabilitybetween languages and domains because it doesnot use any natural language processing (NLP)sub-system (e.g.
: tokenizer and tagger).
In thismatter, the system is not limited to any specificlanguage or dataset.
The system is only limited bythe different inputs used during the system?slearning stage and the set of abbreviations defined.This paper is organized as follows: Section 2presents previous systems dealing withdisambiguation of abbreviations.
Section 3describes the features for disambiguation ofHebrew abbreviations.
Section 4 presents theimplementation of the one sense per discoursehypothesis.
Section 5 describes the experimentsthat have been carried out.
Section 6 concludes andproposes future directions for research.2 Abbreviation DisambiguationThe one sense per collocation hypothesis wasintroduced by Yarowsky (1993).
This hypothesisstates that natural languages tend to use consistentspoken and written styles.
Based on thishypothesis, many terms repeat themselves with thesame meaning in all their occurrences.
Within thecontext of determining the sense of anabbreviation, it may be assumed that authors tendto use the same words in the vicinity of a specificlong form of an abbreviation.
The words may bereused as indicators of the proper solution of anadditional unknown abbreviation with the samewords in its vicinity.
This is the basis for allcontextual features defined in this research.The one sense per discourse hypothesis (OS)was introduced by Gale et al (1992).
This61hypothesis assumes that in natural languages, thereis a tendency for an author to be consistent in thesame discourse or article.
That is, if in a specificdiscourse, an ambiguous phrase or term has aspecific meaning, any other subsequent instance ofthis phrase or term will have the same specificmeaning.
Within the context of determining thesense of an abbreviation, it may be assumed thatauthors tend to use a specific abbreviation in aspecific sense throughout the discourse or article.Research has been done within this domain,mainly for English medical documents.
Systemsdeveloped by Pakhomov (2002; 2005), Yu et al(2003) and Gaudan et al (2005) achieved 84% to98% accuracy.
These systems used variousmachine learning (ML) methods, e.g.
: MaximumEntropy, SVM and C5.0.In our previous research (HaCohen-Kerner etal., 2004), we developed a prototype abbreviationdisambiguation system for Jewish Law documentswritten in Hebrew, without using any ML method.The system integrated six basic features: commonwords, prefixes, suffixes, two statistical featuresand a Hebrew specific feature.
It achieved about60% accuracy while solving 50 abbreviations withan average of 2.3 different senses in the dataset.3 Abbreviation Disambiguation FeaturesEighteen different features of any abbreviationinstance were defined.
They are divided into threedistinct groups, as follows:Statistical attributes: Writer/DatasetCommon Rule (WC/DS).
The most commonsolution used for the specific abbreviation by thediscussed writer/ in the entire dataset.Hebrew specific attribute: Gimatria Rule(GM).
The numerical sum of the numerical valuesattributed to the Hebrew letters forming theabbreviation (HaCohen-Kerner et al, 2004).Contextual relationship attributes:1.
Prefix Counted Rule (PRC): The selectedsense is the most commonly appended sense by thespecific prefix.2.
Before/After K (1,2,3,4) Words CountedRule (BKWC/AKWC): The selected sense is themost commonly preceded/succeeded sense by theK specific words in the sentence of the specificabbreviation instance.3.
Before/After Sentence Counted Rule(BSC/ASC): The selected sense is the mostcommonly preceded/succeeded sense by all thespecific words in the sentence of the specificabbreviation instance.4.
All Sentence/Article Counted Rule(AllSC/AllAC): The selected sense is the mostcommonly surrounded sense by all the specificwords in the sentence/article of the specificabbreviation instance.5.
Before/After Article Counted Rule(BAC/AAC): The selected sense is the mostcommonly preceded/succeeded sense by all thespecific words in the article of the specificabbreviation instance.4 Implementing the OS HypothesisAs mentioned above, the basic assumption of theOS hypothesis is that there exists at least onesolvable abbreviation in the discourse and that thesense of that abbreviation is the same for all theinstances of this abbreviation in the discourse.
Thecorrectness of all the features was investigatedbased on this hypothesis for several variants of"one sense" based on the discussed discourse: none(No OS), a sentence (osS), an article (osA) or allthe articles of the writer (osW).The OS hypothesis was implemented in twoforms.
The ?pure?
form (with the suffix S/A/Wwithout C) uses the sense found by the majorityvoting method for an abbreviation in the discourseand applies it ?blindly?
to all other instances.The ?combined?
form (with the suffix C) tries tofind the sense of the abbreviation using thediscussed feature only.
If the feature isunsuccessful, then we use the relevant one sensevariant using the majority voting method.
Thisform is derived from the possibility that more thanone sense may be used within a single discourseand only instances with an unknown senseconform to the hypothesis.The use of the OS hypothesis, in both forms, isonly relevant for context based features, since thesolutions by other features are static and identicalfrom one instance to another.Therefore, for each of the 15 context basedfeatures, 6 variants of the hypothesis wereimplemented.
This produces 90 variants, whichtogether with the 18 features in their normal form,results in a total of 108 variants.
In addition, theML methods were experimented together with theOS hypothesis.
Of the 108 possible variants, forthe 18 features, the best variant for each feature62was chosen.
In each step, the next best variant isadded, starting from the 2 best variants.5 ExperimentsThe examined dataset includes Jewish LawDocuments written by two Jewish scholars: RabbiY.
M. HaCohen (1995) and Rabbi O. Yosef (1977;1986).
This dataset includes 564,554 words where114,814 of them are abbreviations instances, and42,687 of them are ambiguous.
That is, about 7.5%of the words are ambiguous abbreviations.
Theseambiguous abbreviations are instances of a set of135 different abbreviations.
Each one of theabbreviations has between 2 to 8 relevant possiblesenses.
The average number of senses for anabbreviation in the dataset is 3.27.To determine the accuracy of the system, all theinstances of the ambiguous abbreviations weresolved beforehand.
Some of them were based onpublished solutions (HaCohen, 1995) and some ofthem were solved by experienced readers.5.1 Results of the variants of OS HypothesisThe results of the OS hypothesis variants, for allthe features, are presented in Table 1.
These resultsare obtained without using any ML methods.Accuracy Percentage % Use of OS /Feature  No OS osS osSC osA osAC osW osWCPRC 33.67 34.41 34.52 52.77 54.54 66.66 71.04B1WC 56.05 56.41 56.61 67.74 71.84 72.93 82.51B2WC 55.72 56.23 56.35 69 72.34 74.85 82.84B3WC 60.54 60.89 61.01 72.67 75.48 75.44 82.86B4WC 64.49 64.72 64.85 74.29 76.5 75.52 82.2BSC 75.21 75.18 75.24 76.85 78.15 74.92 78.52BAC 76 76 76 76.01 76 75.39 76A1WC 78.79 79.01 79.21 78.72 83.81 76.32 87.75A2WC 77.57 78.07 78.26 79.15 83.43 78.54 87.62A3WC 78.64 79.11 79.28 79.61 83 78.19 85.8A4WC 75.44 79.28 79.5 79.41 82.42 78.01 84.99ASC 78.59 78.61 78.62 78.25 78.94 77.37 79.04AAC 75.44 75.44 75.44 75.34 75.44 77.28 75.44AllSC 77.97 77.97 77.97 77.9 78.02 77.22 78.04AllAC 74.12 74.12 74.12 74.12 74.12 76.93 74.12GM 46.82 46.82 46.82 46.82 46.82 46.82 46.82WC 82.84 82.84 82.84 82.84 82.84 82.84 82.84DC 78.34 78.34 78.34 78.34 78.34 78.34 78.34Table 1.
Results of the OS Variants for all the Features.The two best pure features were WC and A1WCwith 82.84% and 78.79% of accuracy, respectively.The first finding shows that about 83% of theabbreviations have the same sense in the wholedataset.
The second finding shows that about 79%of the abbreviations can be solved by the first wordthat comes after the abbreviation.Generally, contextual features based on thecontext that comes after the abbreviation, achieveconsiderably better results than all other contextualfeatures.
Specifically, the A1WC_osWC featurevariant achieves the best result with 87.75%accuracy.
These results suggest that eachindividual abbreviation has stronger relationship tothe words after a specific instance, especially to thefirst word.Almost every feature has at least one variant thatachieves a substantial improvement in resultscompared the results achieved by the feature in itsnormal form.
The average relative improvement isabout 18%.For all features, except BAC, the best variantuses the OS implementation with the discoursedefined as the entire dataset.
This may beattributed to the similarity of the different articlesin the dataset.
This is supported by the fact that thebest feature, in its normal form, is the WC feature.In addition, for all but three features (BAC,AAC, AllAC), the best variant used the combinedform of the OS implementation.
This is intuitivelyunderstandable, since ?blindly?
overwritingprobably erases many successes of the feature in itsnormal form.5.2   The Results of the Supervised ML MethodsSeveral well-known supervised ML methods havebeen selected: artificial neural networks (ANN),Na?ve Bayes (NB), Support Vector Machines(SVM) and J48 (Witten and Frank, 1999) animproved variant of the C4.5 decision treeinduction.
These methods have been applied withdefault values and no feature normalization usingWeka (Witten and Frank, 1999).
Tuning is left forfuture research.
To test the accuracy of the models,10-fold cross-validation was used.Table 2 presents the results of these supervisedML methods, by incrementally combining the bestvariant for each feature (according to Table 1).Table 2 shows that SVM achieved the best resultwith 96.09% accuracy.
The best improvement is63about 13%, from 82.84% accuracy for the bestvariant of any feature to 96.02% accuracy.
Thistable also reveals that incremental combining ofmost of the variants leads to better results for mostof the ML methods.# ofVari-antsVariants /ML Method ANN NB SVM J482 A1WC_osWC+A2WC_osWC 91.56 91.40 94.29 91.943 + A3WC_osWC 91.72 91.42 94.43 92.204 + A4WC_osWC 91.75 91.51 94.43 92.345 + B3WC_osWC 92.68 92.11 95.33 93.336 + WC 92.95 92.16 95.71 93.547 + B2WC_osWC 92.81 91.79 95.67 93.598 + B1WC_osWC 92.91 91.06 95.68 93.569 + B4WC_osWC 92.83 91.15 95.62 93.5510 + ASC_osWC 92.83 91.10 95.60 93.5211 + BSC_osWC 92.95 91.17 95.65 93.5812 + DC 92.98 91.17 95.63 93.5813 + AllSC_osWC 92.82 91.50 95.63 93.5814 + AAC_osW 92.84 91.42 95.59 93.5815 + AllAC_osW 93.10 91.43 95.77 93.5816 + BAC_osA 93.09 91.28 95.79 93.7017 + PRC_osWC 93.25 91.50 96.09 93.7118 + GM 93.28 91.52 96.02 93.93Table 2.
The Results of the ML Methods.The comparison of the SVM results to theresults of previous (Section 2) shows that oursystem achieves relatively high accuracy.However, most previous systems researchedambiguous abbreviations in the English language,as well as different abbreviations and texts.6   Conclusions, Summary and Future WorkThis is the first ML system for disambiguation ofabbreviations in Hebrew.
High accuracypercentages were achieved, with improvementascribed to the use of OS hypothesis combinedwith ML methods.
These results were achievedwithout the use of any NLP features.
Therefore, thedeveloped system is adjustable to any specific typeof texts, simply by changing the database of textsand abbreviations.This system is the first that applies manyversions of the one sense per discourse hypothesis.In addition, we performed a comparison betweenthe achievements of four different standard MLmethods, to the goal of achieving the best results,as opposed to the other systems that mainlyfocused on one ML method, each.Future research directions are: comparison toabbreviation disambiguation using the standardbag-of-words or collocation featurerepresentations, definition and implementation ofother NLP-based features and use of these featuresinterlaced with the already defined features,applying additional ML methods, and augmentingthe databases with articles from additional datasetsin the Hebrew language and other languages.ReferencesY.
M. Hacohen (Kagan).
1995.
Mishnah Berurah (inHebrew), Hotzaat Leshem, Jerusalem.Y.
HaCohen-Kerner, A. Kass and A. Peretz.
2004.Baseline Methods for Automatic Disambiguation ofAbbreviations in Jewish Law Documents.Proceedings of the 4th International Conference onAdvances in Natural Language LNAI, SpringerBerlin/Heidelberg, 3230: 58-69.W.
Gale, K. Church and D. Yarowsky.
1992.
One SensePer Discourse.
Proceedings of the 4th DARPAspeech in Natural Language Workshop, 233-237.S.
Gaudan, H. Kirsch and D. Rebholz-Schuhmann.2005.
Resolving abbreviations to their senses inMedline.
Bioinformatics, 21 (18): 3658-3664.S.
Pakhomov.
2002.
Semi-Supervised MaximumEntropy Based Approach to Acronym andAbbreviation Normalization in Medical Texts.Association for Computational Linguistics (ACL),160-167.S.
Pakhomov, T. Pedersen and C. G. Chute.
2005.Abbreviation and Acronym Disambiguation inClinical Discourse.
American Medical InformaticsAssociation Annual Symposium, 589-593.D.
Yarowsky.
1993.
One sense per collocation.Proceedings of the workshop on Human LanguageTechnology, 266-271.O.
Yosef.
1977.
Yechave Daat (in Hebrew), Publisher:Chazon Ovadia, Jerusalem.O.
Yosef.
1986.
Yabia Omer (in Hebrew), Publisher:Chazon Ovadia, Jerusalem.Z.
Yu, Y. Tsuruoka and J. Tsujii.
2003.
AutomaticResolution of Ambiguous Abbreviations inBiomedical Texts using SVM and One Sense PerDiscourse Hypothesis.
SIGIR?03 Workshop on TextAnalysis and Search for Bioinformatics.H.
Witten and E. Frank.
2007.
Weka 3.4.12: MachineLearning Software in Java:http://www.cs.waikato.ac.nz/~ml/weka.64
