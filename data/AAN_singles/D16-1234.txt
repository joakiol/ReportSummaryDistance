Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2163?2172,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsRelations such as Hypernymy: Identifying and Exploiting Hearst Patterns inDistributional Vectors for Lexical EntailmentStephen RollerDepartment of Computer ScienceThe University of Texas at Austinroller@cs.utexas.eduKatrin ErkDepartment of LinguisticsThe University of Texas at Austinkatrin.erk@mail.utexas.eduAbstractWe consider the task of predicting lexicalentailment using distributional vectors.
Weperform a novel qualitative analysis of oneexisting model which was previously shownto only measure the prototypicality of wordpairs.
We find that the model strongly learnsto identify hypernyms using Hearst patterns,which are well known to be predictive of lexi-cal relations.
We present a novel model whichexploits this behavior as a method of fea-ture extraction in an iterative procedure sim-ilar to Principal Component Analysis.
Ourmodel combines the extracted features withthe strengths of other proposed models in theliterature, and matches or outperforms priorwork on multiple data sets.1 IntroductionAs the field of Natural Language Processing has de-veloped, more ambitious semantic tasks are startingto be addressed, such as Question Answering (QA)and Recognizing Textual Entailment (RTE).
Thesesystems often depend on the use of lexical resourceslike WordNet in order to infer entailments for indi-vidual words, but these resources are expensive todevelop, and always have limited coverage.To address these issues, many works have con-sidered on how lexical entailments can be derivedautomatically using distributional semantics.
Somefocus mostly on the use of unsupervised techniques,and study measures which emphasize particularword relations (Baroni and Lenci, 2011).
Many arebased on the Distributional Inclusion Hypothesis,which states that the contexts in which a hypernymappears are a superset of its hyponyms?
contexts(Zhitomirsky-Geffet and Dagan, 2005; Kotlerman etal., 2010).
More recently, a great deal of work haspushed toward using supervised methods (Baroni etal., 2012; Roller et al, 2014; Weeds et al, 2014;Levy et al, 2015; Kruszewski et al, 2015), varyingby their experimental setup or proposed model.Yet the literature disagrees about which modelsare strongest (Weeds et al, 2014; Roller et al,2014), or even if they work at all (Levy et al, 2015).Indeed, Levy et al (2015) showed that two exist-ing lexical entailment models fail to account forsimilarity between the antecedent and consequent,and conclude that such models are only learningto predict prototypicality: that is, they predict thatcat entails animal because animal is usually en-tailed, and therefore will also predict that sofa en-tails animal.
Yet it remains unclear why such modelsmake for such strong baselines (Weeds et al, 2014;Kruszewski et al, 2015; Levy et al, 2015).We present a novel qualitative analysis of one pro-totypicality classifier, giving new insight into whyprototypicality classifiers perform strongly in the lit-erature.
We find the model overwhelmingly learnsto identify hypernyms using Hearst patterns avail-able in the distributional space, like ?animals suchas cats?
and ?animals including cats.?
These pat-terns have long been used to identify lexical rela-tions (Hearst, 1992; Snow et al, 2004).We propose a novel model which exploits this be-havior as a method of feature extraction, which wecall H-feature detectors.
Using an iterative proce-dure similar to Principal Component Analysis, our2163model is able to extract and learn using multiple H-feature detectors.
Our model also integrates overallword similarity and Distributional Inclusion, bring-ing together strengths of several models in the litera-ture.
Our model matches or outperforms prior workon multiple data sets.
The code, data sets, and modelpredictions are made available for future research.12 BackgroundResearch on lexical entailment using distributionalsemantics has now spanned more than a decade,and has been approached using both unsupervised(Weeds et al, 2004; Kotlerman et al, 2010; Lenciand Benotto, 2012; Santus, 2013) and supervisedtechniques (Baroni et al, 2012; Fu et al, 2014;Roller et al, 2014; Weeds et al, 2014; Kruszewskiet al, 2015; Levy et al, 2015; Turney and Mo-hammad, 2015; Santus et al, 2016).
Most of thework in unsupervised methods is based on the Dis-tributional Inclusion Hypothesis (Weeds et al, 2004;Zhitomirsky-Geffet and Dagan, 2005), which statesthat the contexts in which a hypernym appear shouldbe a superset over its hyponyms?
contexts.This work focuses primarily on the supervisedworks in the literature.
Formally, we consider meth-ods which treat lexical entailment as a supervisedclassification problem, which take as input the dis-tributional vectors for a pair of words, (H, w), andpredict on whether the antecedent w entails the con-sequent H .2One of the earliest supervised approaches wasConcat (Baroni et al, 2012).
In this work, the con-catenation of the pair ?H, w?was used as input to anoff-the-shelf SVM classifier.
At the time, it was verysuccessful, but later works noted that it had majorproblems with lexical memorization (Roller et al,2014; Weeds et al, 2014; Levy et al, 2015).
That is,when the training and test sets were carefully con-structed to ensure they were completely disjoint, itperformed extremely poorly.
Nonetheless, Concat iscontinually used as a strong baseline in more recentwork (Kruszewski et al, 2015).1http://github.com/stephenroller/emnlp20162We use the notation w and H for word and hypernym.These variables refer to either the lexical items, or their dis-tributional vectors, depending on context.In response to these issues of lexical memoriza-tion, alternative models were proposed.
Of particu-lar note are the Diff (Fu et al, 2014; Weeds et al,2014) and Asym classifiers (Roller et al, 2014).
TheDiff model takes the vector difference H ?
w asinput, while the Asym model uses both the vectordifference and the squared vector difference as in-put.
Weeds et al (2014) found that Concat moder-ately outperformed Diff, while Roller et al (2014)found that Asym outperformed Concat.
Both Diffand Asym can also be seen as a form of supervisedDistributional Inclusion Hypothesis, with the vectordifference being analogous to the set-inclusion mea-sures of some unsupervised techniques (Roller et al,2014).
All of these works focused exclusively on hy-pernymy detection, rather than the more general taskof lexical entailment.Recently, other works have begun to analyze Con-cat and Diff for their ability to go beyond just hyper-nymy detection.
Vylomova et al (2016) take an ex-tensive look at Diff?s ability to model a wide varietyof lexical relations and conclude it is generally ro-bust, and Kruszewski et al (2015) have success witha neural network model based on the DistributionalInclusion Hypothesis.On the other hand, Levy et al (2015) analyze bothConcat and Diff in their ability to detect general lex-ical entailment on five data sets: two consisting ofonly hypernymy, and three covering a wide varietyof other entailing word relations.
They find that bothConcat and Diff fail, and analytically show that theyare learning to predict the prototypicality of the con-sequent H , rather than the relationship between theantecedent and the consequent, and consider this aform of lexical memorization.
They propose a newmodel, Ksim, which addresses their concerns, butlacks any notion of Distributional Inclusion.
In par-ticular, they argue for directly including the cosinesimilarity of w and H as a term in a custom SVMkernel, in order to determine whether w and H arerelated all.
Ultimately, Levy et al (2015) concludethat distributional vectors may simply be the wrongtool for the job.3 Data and ResourcesPrior work on lexical entailment relied on a varietyof data sets, each constructed in a different manner.2164We focus on four different data sets, each of whichhas been used for evaluation in prior work.
Two datasets contain only hypernymy relations, and two con-sider general lexical entailment.Our first data set is LEDS, the Lexical Entail-ment Data Set, originally created by Baroni et al(2012).
The data set contains 1385 hyponym-hypernym pairs extracted directly from WordNet,forming a set of positive examples.
Negative exam-ples were generated by randomly shuffling the orig-inal set of 1385 pairs.
As such, LEDS only containsexamples of hypernymy and random relations.Another major data set has been BLESS, theBaroni and Lenci (2011) Evaluation of SemanticSpaces.
The data set contains annotations of wordrelations for 200 unambiguous, concrete nouns from17 broad categories.
Each noun is annotated with itsco-hyponyms, meronyms, hypernym and some ran-dom words.
In this work, we treat hypernymy aspositive, and other relations as negative.These two data sets form our hypernymy datasets, but we cannot overstate their important differ-ences: LEDS is balanced, while BLESS containsmostly negative examples; negatives in BLESS in-clude both random pairs and pairs exhibiting otherstrong semantic relations, while LEDS only containsrandom pairs.
Furthermore, all of the negative ex-amples in LEDS are the same lexical items as thepositive items, which has strong implications on theprototypicality argument of Levy et al (2015).The next data set we consider is Medical (Levy etal., 2014).
This data set contains high quality anno-tations of subject-verb-object entailments extractedfrom medical texts, and transformed into noun-nounentailments by argument alignments.
The data con-tains 12,600 annotations, but only 945 positive ex-amples encompassing various relations like hyper-nymy, meronomy, synonymy and contextonymy.3This makes it one of the most difficult data sets: it isboth domain specific and highly unbalanced.The final data set we consider is TM14, a varia-tion on the SemEval 2012 Shared Task of identifyingthe degree to which word pairs exhibit various rela-tions.
These relationships include a small amountof hypernymy, but also many more uncommon rela-3A term for entailments that occur in some contexts, but donot cleanly fit in other categories; e.g.
hospital entails doctor.tions (agent-object, cause-effect, time-activity, etc).Relationships were binarized into (non-)entailingpairs by Turney and Mohammad (2015).
The dataset covers 2188 pairs, 1084 of which are entailing.These two entailment data sets also contain im-portant differences, especially in contrast to the hy-pernymy data sets.
Neither contains any randomnegative pairs, meaning general semantic similaritymeasures should be less useful; And both exhibit avariety of non-hypernymy relations, which are lessstrictly defined and more difficult to model.3.1 Distributional VectorsIn all experiments, we use a standard, count-based,syntactic distributional vector space.
We use a cor-pus composed of the concatenation of Gigaword,Wikipedia, BNC and ukWaC.
We preprocess thecorpus using Stanford CoreNLP 3.5.2 (Chen andManning, 2014) for tokenization, lemmatization,POS-tagging and universal dependency parses.
Wecompute a syntactic distributional space for the 250kmost frequent lemmas by counting their dependencyneighbors across the corpus.
We use only the top 1Mmost frequent dependency attachments as contexts.We use CoreNLP?s ?collapsed dependencies?, inwhich prepositional dependencies are collapsed e.g.
?go to the store?
emits the tuples (go, prep:to+store)and (store, prep:to?1+go).
After collecting counts,vectors are transformed using PPMI, SVD reducedto 300 dimensions, and normalized to unit length.The use of collapsed dependencies is very important,as we will see in Section 4, but other parameters arereasonably robust.4 Motivating AnalysisAs discussed in Section 2, the Concat classifier is aclassifier trained on the concatenation of the wordvectors, ?H, w?.
As additional background, wefirst review the findings of Levy et al (2015), whoshowed that Concat trained using a linear classifier isonly able to capture notions of prototypicality; thatis, Concat guesses that (animal, sofa) is a positiveexample because animal looks like a hypernym.Formally, a linear classifier like Logistic Regres-sion or Linear SVM learns a decision hyperplanerepresented by a vector p?.
Data points are comparedto this plane with the inner product: those above2165the plane (positive inner product) are classified asentailing, and those below as non-entailing.
Cru-cially, since the input features are the concatenationof the pair vectors ?H, w?, the hyperplane p?
vec-tor can be decomposed into separate H and w com-ponents.
Namely, if we rewrite the decision planep?
= ?H?, w?
?, we find that each pair ?H, w?
is classi-fied using:p?>?H, w?= ?H?, w?
?>?H, w?= H?>H + w?>w.
(1)This analysis shows that, when the hyperplane p?
isevaluated on a novel pair, it lacks any form of directinteraction between H and w like the inner prod-uct H>w.
Without any interaction terms, the Con-cat classifier has no way of estimating the relation-ship between the two words, and instead only makespredictions based on two independent terms, H?
andw?, the prototypicality vectors.
Furthermore, the Diffclassifier can be analyzed in the same fashion andtherefore has the same fatal property.We agree with this prototypicality interpretation,although we believe it is incomplete: while it placesa fundamental ceiling on the performance of theseclassifiers, it does not explain why others have foundthem to persist as strong baselines (Weeds et al,2014; Roller et al, 2014; Kruszewski et al, 2015;Vylomova et al, 2016).
To approach this ques-tion, we consider a baseline Concat classifier trainedusing a linear model.
This classifier should moststrongly exhibit the prototypicality behavior accord-ing to Equation 1, making it the best choice for anal-ysis.
We first consider the most pessimistic hypothe-sis: is it only learning to memorize which words arehypernyms at all?We train the baseline Concat classifier using Lo-gistic Regression on each of the four data sets, andextract the vocabulary words which are most simi-lar to the H?
half of the learned hyperplane p?.
If theclassifier is only learning to memorize the trainingdata, we would expect items from the data to dom-inate this list of closest vocabulary terms.
Table 1gives the five most similar words to the learned hy-perplane, with bold words appearing directly in thedata set.Interestingly, we notice there are very few boldwords at all in the list.
In LEDS, we actually seeLEDS BLESS Medical TM14material goods item sensitivenessstructure lifeform unlockable tactilityobject item succor palateprocess equipment team-up stiffnessactivity herbivore non-essential contentTable 1: Most similar words to the prototype H?
learned by theConcat model.
Bold items appear in the data set.some hypernyms of data set items that do not evenappear in the data set, and the Medical and TM14words do not even appear related to the content ofthe data sets.
Similar results were also found forDiff and Asym, and both when using Linear SVMand Logistic Regression.
These lists cannot explainthe success of the prototypicality classifiers in priorwork.
Instead, we propose an alternative interpreta-tion of the hyperplane: that of a feature detector forhypernyms, or an H-feature detector.4.1 H-Feature DetectorsRecall that distributional vectors are derived froma matrix M containing counts of how often wordsco-occur with the different syntactic contexts.
Thisco-occurrence matrix is factorized using SingularValue Decomposition, producing both W , the ubiq-uitous word-embedding matrix, and C, the context-embedding matrix (Levy and Goldberg, 2014):M ?WC>Since the word and context embeddings implicitlylive in the same vector space (Melamud et al, 2015),we can also compare Concat?s hyperplane with thecontext matrix C. Under this interpretation, theConcat model does not learn what words are hy-pernyms, but rather what contexts or features are in-dicative of hypernymy.
Table 2 shows the syntacticcontexts with the highest cosine similarity to the H?prototype for each of the different data sets.This view of Concat as an H-feature detectorproduces a radically different perspective on theclassifier?s hyperplane.
Nearly all of the featureslearned take the form of Hearst patterns (Hearst,1992; Snow et al, 2004).
The most recognizableand common pattern learned is the ?such as?
pat-tern, as in ?animals such as cats?.
These patternshave been well known to be indicative of hyper-nymy for over two decades.
Other interesting pat-2166LEDS BLESSnmod:such as+animal nmod:such as+submarineacl:relcl+identifiable nmod:such as+shipnmod:of?1+determine nmod:such as+sealnmod:of?1+categorisation nmod:such as+planecompound+many nmod:such as+racknmod:such as+pot nmod:such as+ropeMedical TM14nmod:such as+patch amod+desirenmod:such as+skin amod+heightennmod:including+skin nsubj?1+disparatenmod:such as+tooth nmod:such as+honeynmod:such as+feather nmod:with?1+bodynmod:including+finger nsubj?1+unconstrainedTable 2: Most similar contexts to the prototype H?
learned bythe Concat model.terns are the ?including?
pattern (?animals includ-ing cats?)
and ?many?
pattern (?many animals?
).Although we list only the six most similar contextitems for the data sets, we find similar contexts con-tinue to dominate the list for the next 30-50 items.Taken together, it is remarkable that the model iden-tified these patterns using only distributional vectorsand only the positive/negative example pairs.
How-ever, the reader should note these are not true Hearstpatterns: Hearst patterns explicitly relate a hyper-nym and hyponym using an exact pattern match ofa single co-occurrence.
On the other hand, theseH-features are aggregate indicators of hypernymyacross a large corpus.These learned features are much more inter-pretable than those found in the analysis of priorwork like Roller et al (2014) and Levy et al (2015).Roller et al (2014) found no signals of H-featuresin their analysis of one classifier, but their modelwas focused on bag-of-words distributional vectors,which perform significantly worse on the task.
Levyet al (2015) also performed an analysis of lexicalentailment classifiers, and found weak signals like?such?
and ?of?
appearing as prominent contexts intheir classifier, giving an early hint of H-feature de-tectors, but not to such an overwhelming degree aswe see in this work.
Critically, their analysis fo-cused on a classifier trained on high-dimensional,sparse vectors, rather than focusing on context em-beddings as we do.
By using these sparse vectors,their model was unable to generalize across simi-lar contexts.
Additionally, their model did not makeuse of collapsed dependencies, making features like?such?
much weaker signals of entailment and there-fore less dominant during analysis.Among these remarkable lists, the LEDS andTM14 data sets stand out for having much fewer?such as?
patterns compared to BLESS and Medi-cal.
The reason for this is explained by the construc-tion of the data sets: since LEDS contains the samewords used as both positive and negative examples,the classifier has a hard time picking out clear sig-nal.
The TM14 data set, however, does not containany such negative examples.We hypothesize the TM14 data set contains toomany diverse and mutually exclusive forms of lex-ical entailment, like instrument-goal (e.g.
?honey??
?sweetness?).
To test this, we retrained the modelwith only hypernymy as positive examples, and allother relations as negative.
We find that ?such as?type patterns become top features, but also someinteresting data specific features, like ?retailer of[clothes]?.
Examining the data shows it containsmany consumer goods, like ?beverage?
or ?clothes?,which explains these features.5 Proposed ModelAs we saw in the previous section, Concat only actsas a sort of H-feature detector for whether H is aprototypical hypernym, but does not actually inferthe relationship between H and w. Nonetheless, thisis powerful behavior which should still be used incombination with the insights of other models likeKsim and Asym.
To this end, we propose a novelmodel which exploits Concat?s H-feature detectorbehavior, extends its modeling power, and adds twoother types of evidence proposed in the literature:overall similarity, and distributional inclusion.Our model works through an iterative proceduresimilar to Principal Component Analysis (PCA).Each iteration repeatedly trains a Concat classifierunder the assumption that it acts as an H-feature de-tector, and then explicitly discards this informationfrom the distributional vectors.
By training a newH-feature detector on these modified distributionalvectors, we can find additional features indicative ofentailment which were missed by the first classifier.The entire procedure is iteratively repeated similar2167Figure 1: A vector p?
is used to break x into two orthogonalcomponents, its projection and the rejection over p?.to how in Principal Component Analysis, the secondprincipal component is computed after the first prin-cipal component has been removed from the data.The main insight is that after training some H-feature detector using Concat, we can remove thisprototype from the distributional vectors through theuse of vector projection.
Formally, the vector pro-jection of x onto a vector p?, projp?
(x) finds the com-ponent of x which is in the direction of p?,projp?
(x) =(x>p??p??
)p?.Figure 1 gives a geometric illustration of the vectorprojection.
If x forms the hypotenuse of a right tri-angle, projp?
(x) forms a leg of the triangle.
This alsogives rise to the vector rejection, which is the vec-tor forming the third leg of the triangle.
The vectorrejection is orthogonal to the projection, and intu-itively, is the original vector after the projection hasbeen removed:rejp?
(x) = x?
projp?
(x).Using the vector rejection, we take a learned H-feature detector p?, and discard these features fromeach of the word vectors.
That is, for every datapoint ?H,w?, we replace it by its vector rejectionand rescale it to unit magnitude:Hi+1 = rejp?(H)/?rejp?
(H)?wi+1 = rejp?(w)/?rejp?
(w)?A new classifier trained on the ?Hi+1, wi+1?
datamust now learn a different decision plane than p?, asp?
is no longer present in any data points.
This repeti-tion of the procedure is roughly analogous to learn-ing the second principal component of the data; wewish to classify the pairs without using any informa-tion learned from the previous iteration.This second classifier must perform strictly worsethan the original, otherwise the first classifier wouldhave learned this second hyperplane.
Nonetheless,it will be able to learn new H-feature detectorswhich the original classifier was unable to capture.By repeating this process, we can find several H-feature detectors, p?1, .
.
.
, p?n.
Although the first, p?1is the best possible single H-feature detector, eachadditional H-feature detector increases the model?srepresentational power (albeit with diminishing re-turns).This procedure alone does not address the mainconcern of Levy et al (2015): that these linear clas-sifiers never actually model any connection betweenH and w. To address this, we explicitly compareH and w by extracting additional information abouthow H and w interact with respect to each of theH-feature detectors.
This additional information isthen used to train one final classifier which makesthe final prediction.Concretely, in each iteration i of the procedure,we generate a four-valued feature vector Fi, basedon the H-feature detector p?i.
Each feature vectorcontains (1) the similarity of Hi and wi (before pro-jection); (2) the feature p?i applied to Hi; (3) the H-feature detector p?i applied to wi; and (4) the differ-ence of 2 and 3.Fi(?Hi, wi?, p?i)= ?H>i wi, H>i p?i, w>i p?i, (Hi ?
wi)>p?i?These four ?meta?-features capture all the bene-fits of the H-feature detector (slots 2 and 3), whilestill addressing Concat?s issues with similarity argu-ments (slot 1) and distributional inclusion (slot 4).The final feature?s relation to the DIH comes fromthe observation of Roller et al (2014) that the vec-tor difference intuitively captures whether the hyper-nym includes the hyponym.The union of all the feature vectors F1, .
.
.
, Fnfrom repeated iteration form a 4n-dimensional fea-ture vector which we use as input to one final classi-fier which makes the ultimate decision.
This classi-fier is trained on the same training data as each of theindividual H-feature detectors, so our iterative pro-cedure acts only as a method of feature extraction.2168For our final classifier, we use an SVM with anRBF-kernel, though decision trees and other non-linear classifiers also perform reasonably well.
Thenonlinear final classifier can be understood as do-ing a form of logical reasoning about the four slots:?animal?
is a hypernym of ?cat?
because (1) they aresimilar words where (2) animal looks like a hyper-nym, but (3) cat does not, and (4) some ?animal?contexts are not good ?cat?
contexts.6 Experimental Setup and EvaluationIn our experiments, we use a variation of 20-foldcross validation which accounts for lexical overlap.To simplify explanation, we first explain how wegenerate splits for training/testing, and then after-wards introduce validation methodology.We first pool all the words from the antecedent(LHS) side of the data into a set, and split these lex-ical items into 20 distinct cross-validation folds.
Foreach fold Fi, we then use all pairs (w,H) wherew ?
Fi as the test set pairs.
That is, if ?car?
is inthe test set fold, then ?car ?
vehicle?
and ?car 9truck?
will appear as test set pairs.
The training setwill then be every pair which does not contain anyoverlap with the test set; e.g.
the training set will beall pairs which do not contain ?car?, ?truck?
or ?ve-hicle?
as either the antecedent or consequent.
Thisensures that both (1) there is zero lexical overlap be-tween training and testing and (2) every pair is usedas an item in a test fold exactly once.
One quirk ofthis setup is that all test sets are approximately thesame size, but training sizes vary dramatically.This setup differs from those of previous workslike Kruszewski et al (2015) and Levy et al (2015),who both use single, fixed train/test/val sets withoutlexical overlap.
We find our setup has several advan-tages over fixed sets.
First, we find there can be con-siderable variance if the train/test set is regeneratedwith a different random seed, indicating that multi-ple trials are necessary.
Second, fixed setups con-sistently discard roughly half the data as ineligiblefor either training or test, as lexical items appear inmany pairs.
Our CV-like setup allows us to evaluateperformance over every item in the data set exactlyonce, making a much more efficient and representa-tive use of the original data set.Our performance metric is F1 score.
This is moreModel LEDS BLESS Medical TM14Linear ModelsCosine .787 .208 .168 .676Concat .794 .612 .218 .693Diff .805 .440 .195 .665Asym .865 .510 .210 .671Concat+Diff .801 .604 .224 .703Concat+Asym .843 .631 .240 .701Nonlinear ModelsRBF .779 .574 .215 .705Ksim .893 .488 .224 .707Our model .901 .631 .260 .697Table 3: Mean F1 scores for each model and data set.representative than accuracy, as most of the data setsare heavily unbalanced.
We report the mean F1scores across all cross validation folds.6.1 Hyperparameter OptimizationIn order to handle hyperparameter selection, we ac-tually generate the test set using fold i, and usefold i ?
1 as a validation set (removing pairs whichwould overlap with test), and the remaining 18folds as training (removing pairs which would over-lap with test or validation).
We select hyperpa-rameters using grid search.
For all models, weoptimize over the regularization parameter C ?
{10?4, 10?3, .
.
.
, 104}, and for our proposed model,the number of iterations n ?
{1, .
.
.
, 6}.
All otherhyperparameters are left as defaults provided byScikit-Learn (Pedregosa et al, 2011), except for us-ing balanced class weights.
Without balanced classweights, several of the baseline models learn degen-erate functions (e.g.
always guess non-entailing).7 ResultsWe compare our proposed model to several ex-isting and alternative baselines from the literature.Namely, we include a baseline Cosine classifier,which only learns a threshold which maximizes F1score on the training set; three linear models of priorwork, Concat, Diff and Asym; and the RBF andKsim models found to be successful in Kruszewskiet al (2015) and Levy et al (2015).
We also in-clude two additional novel baselines, Concat+Diffand Concat+Asym, which add a notion of Distri-butional Inclusion into the Concat baseline, but arestill linear models.
We cannot include baselines like2169Model LEDS BLESS Medical TM14No Similarity .099 .061 .034 .003No Detectors -.008 .136 .018 .028No Inclusion .010 .031 .014 .001Table 4: Absolute decrease in mean F1 on the developmentsets with the different feature types ablated.
Higher numbersindicate greater feature importance.Ksim+Asym, because Ksim is based on a customSVM kernel which is not amenable to combinations.Table 3 the results across all four data sets for allof the listed models.
Our proposed model improvessignificantly4 over Concat in the LEDS, BLESS andMedical data sets, indicating the benefits of combin-ing these aspects of similarity and distributional in-clusion with the H-feature detectors of Concat.
TheConcat+Asym classifier also improves over the Con-cat baseline, further emphasizing these benefits.
Ourmodel performs approximately the same as Ksimon the LEDS and TM14 data sets (no significantdifference), while significantly outperforming it onBLESS and Medical data sets.7.1 Ablation ExperimentsIn order to evaluate how important each of the vari-ous F features are to the model, we also performedan ablation experiment where the classifier is notgiven the similarity (slot 1), prototype H-feature de-tectors (slots 2 and 3) or the inclusion features (slot4).
To evaluate the importance of these features,we fix the regularization parameter at C = 1, andtrain all ablated classifiers on each training fold withnumber of iterations n = 1, .
.
.
, 6.
Table 4 showsthe decrease (absolute difference) in performancebetween the full and ablated models on the develop-ment sets, so higher numbers indicate greater featureimportance.We find the similarity feature is extremely impor-tant in the LEDS, BLESS and Medical data sets,therefore reinforcing the findings of Levy et al(2015).
The similarity feature is especially impor-tant in the LEDS and BLESS data sets, where neg-ative examples include many random pairs.
Thedetector features are moderately important for theMedical and TM14 data sets, and critically impor-tant on BLESS, where we found the strongest evi-4Bootstrap test, p < .01.dence of Hearst patterns in the H-feature detectors.Surprisingly, the detector features are moderatelydetrimental on the LEDS data set, though this canalso be understood in the data set?s construction:since the negative examples are randomly shuffledpositive examples, the same detector signal will ap-pear in both positive and negative examples.
Finally,we find the model performs somewhat robustly with-out the inclusion feature, but still is moderately im-pactful on three of the four data sets, lending furtherevidence to the Distributional Inclusion Hypothesis.In general, we find all three components are valu-able sources of information for identifying hyper-nymy and lexical entailment.7.2 Analysis by Number of IterationsIn order to evaluate how the iterative feature extrac-tion affects model performance, we fix the regular-ization parameter at C = 1, and train our modelfixing the number of iterations to n = {1, .
.
.
, 6}.We then measure the mean F1 score across the de-velopment folds and compare to a baseline whichuses only one iteration.
Figure 2 shows these resultsacross all four data sets, with the 0 line set at per-formance of the n = 1 baseline.
Models above 0benefit from the additional iterations, while modelsbelow do not.In the figure, we see that the iterative pro-cedure moderately improves performance LEDS,while greatly improving the scores of BLESS andTM14, but on the medical data set, additional it-erations actually hurt performance.
The differingcurves indicate that the optimal number of itera-tions is very data set specific, and provides differingamounts of improvement, and therefore should betuned carefully.
The LEDS and BLESS curves indi-cate a sort of ?sweet spot?
behavior, where furtheriterations degrade performance.To gain some additional insight into what is cap-tured by the various iterations of the feature extrac-tion procedure, we repeat the procedure from Sec-tion 4: we train our model on the entire BLESSdata set using a fixed four iterations and regular-ization parameter.
For each iteration, we compareits learned H-feature detector to the context embed-dings, and report the most similar contexts for eachiteration in Table 5.The first iteration is identical to the one in Ta-2170LEDS BLESS Medical TM14?0.02?0.010.000.010.020.030.040.050.061 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6Number of iterationsRelative F1Figure 2: Performance of model on development folds by number of iterations.
Plots show the improvement (absolute difference)in mean F1 over the model fixed at one iteration.Iteration 1 Iteration 2 Iteration 3 Iteration 4nmod:such as+submarine nmod:including+animal amod+free-swimming advcl+crownnmod:such as+ship nmod:including+snail nmod:including?1+thing advcl+victoriousnmod:such as+seal nmod:including+insect nsubj?1+scarcer nsubj+eatersnmod:such as+plane nmod:such as+crustacean nsubj?1+pupate nsubj+kainenmod:such as+rack nmod:such as+mollusc nmod:such as+mollusc nmod:at+finalenmod:such as+rope nmod:such as+insect nmod:of?1+value nsubj+gowennmod:such as+box nmod:such as+animal nmod:as?1+exhibit nsubj+pillmanTable 5: Most similar contexts to the H-feature detector for each iteration of the PCA-like procedure.
This model was trained on alldata of BLESS.
The first and second iterations contain clear Hearst patterns, while the third and fourth contain some data-specificand non-obvious signals.ble 2, as expected.
The second iteration includesmany H-features not picked up by the first itera-tion, mostly those of the form ?X including Y?.
Thethird iteration picks up some data set specific signal,like ?free-swimming [animal]?
and ?value of [com-puter]?, and so on.
By the fourth iteration, the fea-tures no longer exhibit any obvious Hearst patterns,perhaps exceeding the sweet spot we observed inFigure 2.
Nonetheless, we see how multiple iter-ations of the procedure allows our model to capturemany more useful features than a single Concat clas-sifier on its own.8 ConclusionWe considered the task of detecting lexical entail-ment using distributional vectors of word meaning.Motivated by the fact that the Concat classifier actsas a strong baseline in the literature, we proposed anovel interpretation of the model?s hyperplane.
Wefound the Concat classifier overwhelmingly actedas a feature detector which automatically identifiesHearst Patterns in the distributional vectors.We proposed a novel model that embraces theseH-feature detectors fully, and extends their model-ing power through an iterative procedure similar toPrincipal Component Analysis.
In each iteration ofthe procedure, an H-feature detector is learned, andthen removed from the data, allowing us to iden-tify several different kinds of Hearst Patterns in thedata.
Our final model combines these H-feature de-tectors with measurements of general similarity andDistributional Inclusion, in order to integrate thestrengths of different models in prior work.
Ourmodel matches or exceeds the performance of priorwork, both on hypernymy detection and general lex-ical entailment.AcknowledgmentsThe authors would like to thank I. Beltagy, VeredShwartz, Subhashini Venugopalan, and the review-ers for their helpful comments and suggestions.This research was supported by the NSF grant IIS1523637.
We acknowledge the Texas AdvancedComputing Center for providing grid resources thatcontributed to these results.2171ReferencesMarco Baroni and Alessandro Lenci.
2011.
How weBLESSed distributional semantic evaluation.
In Pro-ceedings of the GEMS 2011Workshop on GEometricalModels of Natural Language Semantics, pages 1?10,Edinburgh, UK.Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, andChung-chieh Shan.
2012.
Entailment above the wordlevel in distributional semantics.
In Proceedings of the2012 Conference of the European Chapter of the As-sociation for Computational Linguists, pages 23?32,Avignon, France.Danqi Chen and Christopher Manning.
2014.
A fast andaccurate dependency parser using neural networks.In Proceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing, pages 740?750, Doha, Qatar.Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, HaifengWang, and Ting Liu.
2014.
Learning semantic hi-erarchies via word embeddings.
In Proceedings ofthe 2014 Annual Meeting of the Association for Com-putational Linguistics, pages 1199?1209, Baltimore,Maryland.Marti A Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings of the1992 Conference on Computational Linguistics, pages539?545, Nantes, France.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distributionalsimilarity for lexical inference.
Natural Language En-gineering, 16:359?389, 10.Germa?n Kruszewski, Denis Paperno, and Marco Baroni.2015.
Deriving Boolean structures from distributionalvectors.
Transactions of the Association for Computa-tional Linguistics, 3:375?388.Alessandro Lenci and Giulia Benotto.
2012.
Identifyinghypernyms in distributional semantic spaces.
In TheFirst Joint Conference on Lexical and ComputationalSemantics, pages 75?79, Montre?al, Canada.Omer Levy and Yoav Goldberg.
2014.
Neural wordembedding as implicit matrix factorization.
In Ad-vances in Neural Information Processing Systems,pages 2177?2185.Omer Levy, Ido Dagan, and Jacob Goldberger.
2014.
Fo-cused entailment graphs for Open IE propositions.
InProceedings of the 2014 Conference on ComputationalNatural Language Learning, pages 87?97, Ann Arbor,Michigan.Omer Levy, Steffen Remus, Chris Biemann, and Ido Da-gan.
2015.
Do supervised distributional methods re-ally learn lexical inference relations?
In Proceedingsof the 2015 North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 970?976, Denver, Colorado.Oren Melamud, Omer Levy, and Ido Dagan.
2015.
Asimple word embedding model for lexical substitution.In Proceedings of the First Workshop on Vector SpaceModeling for Natural Language Processing, pages 1?7, Denver, Colorado.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.Stephen Roller, Katrin Erk, and Gemma Boleda.
2014.Inclusive yet selective: Supervised distributional hy-pernymy detection.
In Proceedings of the 2014 In-ternational Conference on Computational Linguistics,pages 1025?1036, Dublin, Ireland.Enrico Santus, Alessandro Lenci, Tin-Shing Chiu, QinLu, and Chu-Ren Huang.
2016.
Nine features in arandom forest to learn taxonomical semantic relations.In Proceedings of the Tenth International Conferenceon Language Resources and Evaluation, Paris, France.Enrico Santus.
2013.
SLQS: An entropy measure.
Mas-ter?s thesis, University of Pisa.Rion Snow, Daniel Jurafsky, and Andrew Y Ng.
2004.Learning syntactic patterns for automatic hypernymdiscovery.
In Advances in Neural Information Pro-cessing Systems, pages 1297?1304.Peter D Turney and Saif M Mohammad.
2015.
Ex-periments with three approaches to recognizing lex-ical entailment.
Natural Language Engineering,21(03):437?476.Ekaterina Vylomova, Laura Rimell, Trevor Cohn, andTimothy Baldwin.
2016.
Take and took, gaggle andgoose, book and read: Evaluating the utility of vectordifferences for lexical relation learning.
In Proceed-ings of the 54th Annual Meeting of the Association forComputational Linguistics, pages 1671?1682, Berlin,Germany, August.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributional simi-larity.
In Proceedings of the 2004 International Con-ference on Computational Linguistics, pages 1015?1021, Geneva, Switzerland.Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir,and Bill Keller.
2014.
Learning to distinguish hyper-nyms and co-hyponyms.
In Proceedings of the 2014International Conference on Computational Linguis-tics, pages 2249?2259, Dublin, Ireland.Maayan Zhitomirsky-Geffet and Ido Dagan.
2005.
Thedistributional inclusion hypotheses and lexical entail-ment.
In Proceedings of the 2005 Annual Meeting ofthe Association for Computational Linguistics, pages107?114, Ann Arbor, Michigan.2172
