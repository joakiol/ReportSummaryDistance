Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 379?383,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsDealing with Spurious Ambiguity in Learning ITG-based Word AlignmentShujian HuangState Key Laboratory forNovel Software TechnologyNanjing Universityhuangsj@nlp.nju.edu.cnStephan VogelLanguage Technologies InstituteCarnegie Mellon Universityvogel@cs.cmu.eduJiajun ChenState Key Laboratory forNovel Software TechnologyNanjing Universitychenjj@nlp.nju.edu.cnAbstractWord alignment has an exponentially largesearch space, which often makes exact infer-ence infeasible.
Recent studies have shownthat inversion transduction grammars are rea-sonable constraints for word alignment, andthat the constrained space could be efficientlysearched using synchronous parsing algo-rithms.
However, spurious ambiguity may oc-cur in synchronous parsing and cause prob-lems in both search efficiency and accuracy.
Inthis paper, we conduct a detailed study of thecauses of spurious ambiguity and how it ef-fects parsing and discriminative learning.
Wealso propose a variant of the grammar whicheliminates those ambiguities.
Our grammarshows advantages over previous grammars inboth synthetic and real-world experiments.1 IntroductionIn statistical machine translation, word alignment at-tempts to find word correspondences in parallel sen-tence pairs.
The search space of word alignmentwill grow exponentially with the length of sourceand target sentences, which makes the inference forcomplex models infeasible (Brown et al, 1993).
Re-cently, inversion transduction grammars (Wu, 1997),namely ITG, have been used to constrain the searchspace for word alignment (Zhang and Gildea, 2005;Cherry and Lin, 2007; Haghighi et al, 2009; Liu etal., 2010).
ITG is a family of grammars in which theright hand side of the rule is either two nonterminalsor a terminal sequence.
The most general case of theITG family is the bracketing transduction grammarA?
[AA] | ?AA?
| e/f | /f | e/Figure 1: BTG rules.
[AA] denotes a monotone concate-nation and ?AA?
denotes an inverted concatenation.
(BTG, Figure 1), which has only one nonterminalsymbol.Synchronous parsing of ITG may generate a largenumber of different derivations for the same under-lying word alignment.
This is often referred to asthe spurious ambiguity problem.
Calculating andsaving those derivations will slow down the parsingspeed significantly.
Furthermore, spurious deriva-tions may fill up the n-best list and supersede po-tentially good results, making it harder to find thebest alignment.
Besides, over-counting those spu-rious derivations will also affect the likelihood es-timation.
In order to reduce spurious derivations,Wu (1997), Haghighi et al (2009), Liu et al (2010)propose different variations of the grammar.
Thesegrammars have different behaviors in parsing effi-ciency and accuracy, but so far no detailed compari-son between them has been done.In this paper, we formally analyze alignments un-der ITG constraints and the different causes of spu-rious ambiguity for those alignments.
We do an em-pirical study of the influence of spurious ambiguityon parsing and discriminative learning by compar-ing different grammars in both synthetic and real-data experiments.
To our knowledge, this is the firstin-depth analysis on this specific issue.
A new vari-ant of the grammar is proposed, which efficiently re-moves all spurious ambiguities.
Our grammar showsadvantages over previous ones in both experiments.379AAA AAe1 e2 e3f1 f2 f3AA AA Ae1 e2 e3f1 f2 f3AAA AAe1 e2 e3f1 f2 f3AA AA Ae1 e2 e3f1 f2 f3Figure 2: Possible monotone/inverted t-splits (dashedlines) under BTG, causing branching ambiguities.2 ITG Alignment FamilyBy lexical rules like A ?
e/f , each ITG derivationactually represents a unique alignment between thetwo sequences.
Thus the family of ITG derivationsrepresents a family of word alignment.Definition 1.
The ITG alignment family is a set ofword alignments that has at least one BTG deriva-tion.ITG alignment family is only a subset of wordalignments because there are cases, known as inside-outside alignments (Wu, 1997), that could not berepresented by any ITG derivation.
On the otherhand, an ITG alignment may have multiple deriva-tions.Definition 2.
For a given grammar G, spurious am-biguity in word alignment is the case where two ormore derivations d1, d2, ... dk of G have the sameunderlying word alignmentA.
A grammarG is non-spurious if for any given word alignment, there existat most one derivation under G.In any given derivation, an ITG rule applies by ei-ther generating a bilingual word pair (lexical rules)or splitting the current alignment into two parts,which will recursively generate two sub-derivations(transition rules).Definition 3.
Applying a monotone (or inverted)concatenation transition rule forms a monotone t-split (or inverted t-split) of the original alignment(Figure 2).3 Causes of Spurious Ambiguity3.1 Branching AmbiguityAs shown in Figure 2, left-branching and right-branching will produce different derivations underA?
[AB] | [BB] | [CB] | [AC] | [BC] | [CC]B ?
?AA?
| ?BA?
| ?CA?
| ?AC?
| ?BC?
| ?CC?C ?
e/f | /f | e/Figure 3: A Left heavy Grammar (LG).BTG, but yield the same word alignment.
Branchingambiguity was identified and solved in Wu (1997),using the grammar in Figure 3, denoted as LG.
LGuses two separate non-terminals for monotone andinverted concatenation, respectively.
It only allowsleft branching of such non-terminals, by excludingrules like A?
[BA].Theorem 1.
For each ITG alignment A, in whichall the words are aligned, LG will produce a uniquederivation.Proof: Induction on n, the length of A.
Case n=1is trivial.
Induction hypothesis: the theorem holdsfor any A with length less than n.For A of length n, let s be the right most t-splitwhich splits A into S1 and S2.
s exists because A isan ITG alignment.
Assume that there exists anothert-split s?, splitting A into S11 and (S12S2).
BecauseA is fixed and fully aligned, it is easy to see that ifs is a monotone t-split, s?
could only be monotone,and S12 and S2 in the right sub-derivation of t-split s?could only be combined by monotone concatenationas well.
So s?
will have a right branching of mono-tone concatenation, which contradicts with the def-inition of LG because right branching of monotoneconcatenations is prohibited.
A similar contradic-tion occurs if s is an inverted t-split.
Thus s shouldbe the unique t-split forA.
By I.H., S1 and S2 have aunique derivation, because their lengths are less thann.
Thus the derivation for A will be unique.3.2 Null-word Attachment AmbiguityDefinition 4.
For any given sentence pair (e, f) andits alignment A, let (e?, f ?)
be the sentence pairswith all null-aligned words removed from (e, f).The alignment skeletonAS is the alignment between(e?, f ?)
that preserves all links in A.From Theorem 1 we know that every ITG align-ment has a unique LG derivation for its alignmentskeleton (Figure 4 (c)).However, because of the lexical or syntactic dif-ferences between languages, some words may have380AC BAC CCe1/ e2 e3 e4f1 f2 f3(a)BAAC CCCe1/ e2 e3 e4f1 f2 f3(b)BACC01Ct C01CCe1/ e2 e3 e4f1 f2 f3(c)Figure 4: Null-word attachment for the same alignment.
((a) and (b) are spurious derivations under LG causedby null-aligned words attachment.
(c) shows the uniquederivation under LGFN.
The dotted lines have omittedsome unary rules for simplicity.
The dashed box marksthe alignment skeleton.)A?
[AB] | [BB] | [CB] | [AC] | [BC] | [CC]B ?
?AA?
| ?BA?
| ?CA?
| ?AC?
| ?BC?
| ?CC?C ?
C01 | [Cs C]C01 ?
C00 | [Ct C01]C00 ?
e/f, Ct ?
e/, Cs ?
/fFigure 5: A Left heavy Grammar with Fixed Null-wordattachment (LGFN).no explicit correspondence in the other language andtend to stay unaligned.
These null-aligned words,also called singletons, should be attached to someother nodes in the derivation.
It will produce dif-ferent derivations if those null-aligned words are at-tached by different rules, or to different nodes.Haghighi et al (2009) give some restrictions onnull-aligned word attachment.
However, they fail torestrict the node to which the null-aligned word isattached, e.g.
the cases (a) and (b) in Figure 4.3.3 LGFN GrammarWe propose here a new variant of ITG, denoted asLGFN (Figure 5).
Our grammar takes similar tran-sition rules as LG and efficiently constrains the at-tachment of null-aligned words.
We will empiricallycompare those different grammars in the next sec-tion.Lemma 1.
LGFN has a unique mapping from thederivation of any given ITG alignment A to thederivation of its alignment skeleton AS .Proof: LGFN maps the null-aligned source wordsequence, Cs1 , Cs2 , ..., Csk , the null-aligned targetword sequence, Ct1 , Ct2 , ..., Ctk?
, together with thealigned word-pair C00 that directly follows, to thenodeC exactly in the way of Equation 1.
The brack-ets indicate monotone concatenations.C ?
[Cs1 ...[Csk [Ct1 ...[Ctk?C00]...]]...] (1)The mapping exists when every null-aligned se-quence has an aligned word-pair after it.
Thus itrequires an artificial word at the end of the sentence.Note that our grammar attaches null-alignedwords in a right-branching manner, which means itbuilds the span only when there is an aligned word-pair.
After initialization, any newly-built span willcontain at least one aligned word-pair.
Compara-tively, the grammar in Liu et al (2010) uses a left-branching manner.
It may generate more spans thatonly contain null-aligned words, which makes it lessefficient than ours.Theorem 2.
LGFN has a unique derivation for eachITG alignment, i.e.
LGFN is non-spurious.Proof: Derived directly from Definition 4, Theo-rem 1 and Lemma 1.4 Experiments4.1 Synthetic ExperimentsWe automatically generated 1000 fully aligned ITGalignments of length 20 by generating random per-mutations first and checking ITG constraints using alinear time algorithm (Zhang et al, 2006).
Sparseralignments were generated by random removal ofalignment links according to a given null-alignedword ratio.
Four grammars were used to parse thesealignments, namely LG (Wu, 1997), HaG (Haghighiet al, 2009), LiuG (Liu et al, 2010) and LGFN (Sec-tion 3.3).Table 1 shows the average number of derivationsper alignment generated under LG and HaG.
Thenumber of derivations produced by LG increaseddramatically because LG has no restrictions on null-aligned word attachment.
HaG also produced a largenumber of spurious derivations as the number ofnull-aligned words increased.
Both LiuG and LGFNproduced a unique derivation for each alignment, asexpected.
One interpretation is that in order to get381% 0 5 10 15 20 25LG 1 42.2 1920.8 9914.1+ 10000+ 10000+HaG 1 3.5 10.9 34.1 89.2 219.9Table 1: Average #derivations per alignment for LG andHaG v.s.
Percentage of unaligned words.
(+ markedparses have reached the beam size limit of 10000.
)600s)HaGLiuG200300400500sing time (HaGLiuGLFGLG01000510152025ParPtfllliddPercentage of null-alignedwordsFigure 6: Total parsing time (in seconds) v.s.
Percentageof un-aligned words.the 10-best alignments for sentence pairs that have10% of words unaligned, the top 109 HaG deriva-tions should be generated, while the top 10 LiuG orLGFN derivations are already enough.Figure 6 shows the total parsing time using eachgrammar.
LG and HaG showed better performanceswhen most of the words were aligned because theirgrammars are simpler and less constrained.
How-ever, when the number of null-aligned words in-creased, the parsing times for LG and HaG becamemuch longer, caused by the calculation of the largenumber of spurious derivations.
Parsings using LGfor 10 and 15 percent of null-aligned words tookaround 15 and 80 minutes, respectively, which can-not be plotted in the same scale with other gram-mars.
The parsing times of LGFN and LiuG alsoslowly increased, but parsing LGFN consistentlytook less time than LiuG.It should be noticed that the above results camefrom parsing according to some given alignment.When searching without knowing the correct align-ment, it is possible for every word to stay unaligned,which makes spurious ambiguity a much more seri-ous issue.4.2 Discriminative Learning ExperimentsTo further study how spurious ambiguity affects thediscriminative learning, we implemented a frame-work following Haghighi et al (2009).
We useda log-linear model, with features like IBM model1020.21 0170.180.190.2AE R0.150.160.17161116AHaG-20bestLFG-1bestLFG-20bestNumber of iterationsFigure 7: Test set AER after each iteration.probabilities (collected from FBIS data), relativedistances, matchings of high frequency words,matchings of pos-tags, etc.
Online training wasperformed using the margin infused relaxed algo-rithm (Crammer et al, 2006), MIRA.
For eachsentence pair (e, f), we optimized with alignmentresults generated from the nbest parsing results.Alignment error rate (Och and Ney, 2003), AER,was used as the loss function.
We ran MIRA train-ing for 20 iterations and evaluated the alignments ofthe best-scored derivations on the test set using theaverage weights.We used the manually aligned Chinese-Englishcorpus in NIST MT02 evaluation.
The first 200 sen-tence pairs were used for training, and the last 150for testing.
There are, on average, 10.3% words staynull-aligned in each sentence, but if restricted to surelinks the average ratio increases to 22.6%.We compared training using LGFN with 1-best,20-best and HaG with 20-best (Figure 7).
Train-ing with HaG only obtained similar results with 1-best trained LGFN, which demonstrated that spu-rious ambiguity highly affected the nbest list here,resulting in a less accurate training.
Actually, the20-best parsing using HaG only generated 4.53 dif-ferent alignments on average.
20-best training us-ing LGFN converged quickly after the first few it-erations and obtained an AER score (17.23) betterthan other systems, which is also lower than the re-fined IBM Model 4 result (19.07).We also trained a similar discriminative model butextended the lexical rule of LGFN to accept at max-imum 3 consecutive words.
The model was usedto align FBIS data for machine translation exper-iments.
Without initializing by phrases extractedfrom existing alignments (Cherry and Lin, 2007) orusing complicated block features (Haghighi et al,3822009), we further reduced AER on the test set to12.25.
An average improvement of 0.52 BLEU (Pa-pineni et al, 2002) score and 2.05 TER (Snoveret al, 2006) score over 5 test sets for a typicalphrase-based translation system, Moses (Koehn etal., 2003), validated the effectiveness of our experi-ments.5 ConclusionGreat efforts have been made in reducing spuriousambiguities in parsing combinatory categorial gram-mar (Karttunen, 1986; Eisner, 1996).
However, toour knowledge, we give the first detailed analysis onspurious ambiguity of word alignment.
Empiricalcomparisons between different grammars also vali-dates our analysis.This paper makes its own contribution in demon-strating that spurious ambiguity has a negative im-pact on discriminative learning.
We will continueworking on this line of research and improve ourdiscriminative learning model in the future, for ex-ample, by adding more phrase level features.It is worth noting that the definition of spuri-ous ambiguity actually varies for different tasks.
Insome cases, e.g.
bilingual chunking, keeping differ-ent null-aligned word attachments could be useful.It will also be interesting to explore spurious ambi-guity and its effects in those different tasks.AcknowledgmentsThe authors would like to thank Alon Lavie, QinGao and the anonymous reviewers for their valu-able comments.
This work is supported by the Na-tional Natural Science Foundation of China (No.61003112), the National Fundamental ResearchProgram of China (2010CB327903) and by NSF un-der the CluE program, award IIS 084450.ReferencesPeter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Colin Cherry and Dekang Lin.
2007.
Inversion transduc-tion grammar for joint phrasal translation modeling.In Proceedings of the NAACL-HLT 2007/AMTA Work-shop on Syntax and Structure in Statistical Transla-tion, SSST ?07, pages 17?24, Stroudsburg, PA, USA.Association for Computational Linguistics.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
J. Mach.
Learn.
Res., 7:551?585, December.Jason Eisner.
1996.
Efficient normal-form parsing forcombinatory categorial grammar.
In Proceedings ofthe 34th annual meeting on Association for Compu-tational Linguistics, ACL ?96, pages 79?86, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Aria Haghighi, John Blitzer, and Dan Klein.
2009.
Bet-ter word alignments with supervised itg models.
InAssociation for Computational Linguistics, Singapore.Lauri Karttunen.
1986.
Radical lexicalism.
TechnicalReport CSLI-86-68, Stanford University.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In HLT-NAACL.Shujie Liu, Chi-Ho Li, and Ming Zhou.
2010.
Dis-criminative pruning for discriminative itg alignment.In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics, ACL ?10,pages 316?324, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Comput.
Linguist., 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In ACL ?02: Proceedingsof the 40th Annual Meeting on Association for Compu-tational Linguistics, pages 311?318, Morristown, NJ,USA.
Association for Computational Linguistics.Matthew Snover, Bonnie J. Dorr, and Richard Schwartz.2006.
A study of translation edit rate with targetedhuman annotation.
In Proceedings of AMTA.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Comput.
Linguist., 23:377?403, September.Hao Zhang and Daniel Gildea.
2005.
Stochastic lexi-calized inversion transduction grammar for alignment.In Proceedings of the 43rd Annual Meeting on As-sociation for Computational Linguistics, ACL ?05,pages 475?482, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proceedings of the main conferenceon Human Language Technology Conference of theNorth American Chapter of the Association of Compu-tational Linguistics, pages 256?263, Morristown, NJ,USA.
Association for Computational Linguistics.383
