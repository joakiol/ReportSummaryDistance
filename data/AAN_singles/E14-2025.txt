Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 97?100,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsThe GATE Crowdsourcing Plugin: Crowdsourcing Annotated CorporaMade EasyKalina Bontcheva, Ian Roberts, Leon Derczynski, Dominic RoutUniversity of Sheffield{kalina,ian,leon,d.rout}@dcs.shef.ac.ukAbstractCrowdsourcing is an increasingly popu-lar, collaborative approach for acquiringannotated corpora.
Despite this, reuseof corpus conversion tools and user in-terfaces between projects is still problem-atic, since these are not generally madeavailable.
This demonstration will intro-duce the new, open-source GATE Crowd-sourcing plugin, which offers infrastruc-tural support for mapping documents tocrowdsourcing units and back, as well asautomatically generating reusable crowd-sourcing interfaces for NLP classificationand selection tasks.
The entire work-flow will be demonstrated on: annotatingnamed entities; disambiguating words andnamed entities with respect to DBpediaURIs; annotation of opinion holders andtargets; and sentiment.1 IntroductionAnnotation science (Hovy, 2010; Stede andHuang, 2012) and general purpose corpus anno-tation tools (e.g.
Bontcheva et al.
(2013)) haveevolved in response to the need for creating high-quality NLP corpora.
Crowdsourcing is a popu-lar collaborative approach that has been appliedto acquiring annotated corpora and a wide rangeof other linguistic resources (Callison-Burch andDredze, 2010; Fort et al., 2011; Wang et al., 2012).Although the use of this approach is intensifying,especially paid-for crowdsourcing, the reuse of an-notation guidelines, task designs, and user inter-faces between projects is still problematic, sincethese are generally not made available, despitetheir important role in result quality (Khanna etal., 2010).A big outstanding challenge for crowdsourc-ing projects is that the cost to define a singleannotation task remains quite substantial.
Thisdemonstration will introduce the new, open-sourceGATE Crowdsourcing plugin, which offers in-frastructural support for mapping documents tocrowdsourcing units, as well as automatically gen-erated, reusable user interfaces1for NLP classi-fication and selection tasks.
Their use will bedemonstrated on annotating named entities (selec-tion task), disambiguating words and named enti-ties with respect to DBpedia URIs (classificationtask), annotation of opinion holders and targets(selection task), as well as sentiment (classifica-tion task).2 Crowdsourcing Stages and the Role ofInfrastructural SupportConceptually, the process of crowdsourcing anno-tated corpora can be broken down into four mainstages, within which there are a number of largelyinfrastructural steps.
In particular, data prepara-tion and transformation into CrowdFlower units,creation of the annotation UI, creation and uploadof gold units for quality control, and finally map-ping judgements back into documents and aggre-gating all judgements into a finished corpus.The rest of this section discusses in more de-tail where reusable components and infrastructuralsupport for automatic data mapping and user inter-face generation are necessary, in order to reducethe overhead of crowdsourcing NLP corpora.2.1 Project DefinitionAn important part of project definition is the map-ping of the NLP problem into one or more crowd-sourcing tasks, which are sufficiently simple to becarried out by non-experts and with a good qual-ity.
What are helpful here are reusable patternsfor how best to crowdsource different kinds ofNLP corpora.
The GATE Crowdsourcing plugin1Currently for CrowdFlower, which unlike Amazon Me-chanical Turk is available globally.97currently provides such patterns for selection andclassification tasks.This stage also focuses on setup of the task pa-rameters (e.g.
number of crowd workers per task,payment per task) and piloting the project, in orderto tune in its design.
With respect to task param-eters, infrastructural support is helpful, in orderto enable automatic splitting of longer documentsacross crowdsourcing tasks.2.2 Data PreparationThis stage, in particular, can benefit significantlyfrom infrastructural support and reusable compo-nents, in order to collect the data (e.g.
crawlthe web, download samples from Twitter), pre-process it with linguistic tools (e.g.
tokenisation,POS tagging, entity recognition), and then mapautomatically from documents and sentences tocrowdsourcing micro-tasks.2.3 Running the Crowdsourcing ProjectThis is the main phase of each crowdsourcingproject.
It consists of three kinds of tasks: taskworkflow and management, contributor manage-ment (including profiling and retention), and qual-ity control.
Paid-for marketplaces like AmazonMechanical Turk and CrowdFlower already pro-vide this support.
As with conventional corpus an-notation, quality control is particularly challeng-ing, and additional NLP-specific infrastructuralsupport can help.2.4 Data Evaluation and AggregationIn this phase, additional NLP-specific, infrastruc-tural support is needed for evaluating and aggre-gating the multiple contributor inputs into a com-plete linguistic resource, and in assessing the re-sulting overall quality.Next we demonstrate how these challenges havebeen addressed in our work.3 The GATE Crowdsourcing PluginTo address these NLP-specific requirements,we implemented a generic, open-source GATECrowdsourcing plugin, which makes it very easyto set up and conduct crowdsourcing-based corpusannotation from within GATE?s visual interface.3.1 Physical representation for documentsand annotationsDocuments and their annotations are encoded inthe GATE stand-off XML format (CunninghamFigure 1: Classification UI Configurationet al., 2002), which was chosen for its supportfor overlapping annotations and the wide range ofautomatic pre-processing tools available.
GATEalso has support for the XCES standard (Ide et al.,2000) and others (e.g.
CoNLL) if preferred.
An-notations are grouped in separate annotation sets:one for the automatically pre-annotated annota-tions, one for the crowdsourced judgements, anda consensus set, which can be considered as the fi-nal resulting corpus annotation layer.
In this way,provenance is fully tracked, which makes it possi-ble to experiment with methods that consider morethan one answer as potentially correct.3.2 Automatic data mapping toCrowdFlowerThe plugin expects documents to be pre-segmented into paragraphs, sentences and wordtokens, using a tokeniser, POS tagger, and sen-tence splitter ?
e.g.
those built in to GATE (Cun-ningham et al., 2002).
The GATE Crowdsourcingplugin allows choice between these of which touse as the crowdsourcing task unit; e.g., to showone sentence per unit or one paragraph.
In thedemonstration we will show both automatic map-ping at sentence level (for named entity annota-tion) and at paragraph level (for named entity dis-ambiguation).3.3 Automatic user interface generationThe User Interfaces (UIs) applicable to varioustask types tend to fall into a set of categories, themost commonly used being categorisation, selec-tion, and text input.
The GATE Crowdsourcingplugin provides generalised and re-usable, auto-matically generated interfaces for categorisation98Figure 2: Classification Interface: Sense Disambiguation ExampleFigure 3: Sequential Selection Interface: Named Entity Recognition Exampleand selection.In the first step, task name, instructions, andclassification choices are provided, in a UI config-uration dialog (see Figure 1).
In this example, theinstructions are for disambiguating named entities.We have configured three fixed choices, which ap-ply to each entity classification task.For some categorisation NLP annotation tasks(e.g.
classifying sentiment in tweets into posi-tive, negative, and neutral), fixed categories aresufficient.
In others, where the available categorychoices depend on the text that is being classi-fied (e.g.
the possible disambiguations of Parisare different from those of London), choices aredefined through annotations on each of the clas-sification targets.
In this case case, the UI gen-erator then takes these annotations as a parame-ter and automatically creates the different categorychoices, specific to each crowdsourcing unit.
Fig-ure 2 shows an example for sense disambiguation,which combines two unit-specific classes with thethree fixed classification categories shown before.Figure 3 shows the CrowdFlower-based user in-terface for word-constrained sequential selection,which in this case is parameterised for named en-tity annotation.
In sequential selection, sub-unitsare defined in the UI configuration ?
tokens, forthis example.
The annotators are instructed toclick on all words that constitute the desired se-quence (the annotation guidelines are given as aparameter during the automatic user interface gen-eration).Since the text may not contain a sequence to beannotated, we also generate an explicit confirma-tion checkbox.
This forces annotators to declarethat they have made the selection or there is noth-ing to be selected in this text.
CrowdFlower canthen use gold units and test the correctness of theselections, even in cases where no sequences areselected in the text.
In addition, requiring at leastsome worker interaction and decision-making inevery task improves overall result quality.3.4 Quality controlThe key mechanism for spam prevention and qual-ity control in CrowdFlower is test data, whichwe also refer to as gold units.
These are com-pleted examples which are mixed in with the un-processed data shown to workers, and used toevaluate worker performance.
The GATE Crowd-sourcing plugin supports automatic creation ofgold units from GATE annotations having a fea-ture correct.
The value of that feature is thentaken to be the answer expected from the humanannotator.
Gold units need to be 10%?30% of theunits to be annotated.
The minimum performancethreshold for workers can be set in the job config-uration.3.5 Automatic data import fromCrowdFlower and adjudicationOn completion, the plugin automatically importscollected multiple judgements back into GATE99Figure 4: CrowdFlower Judgements in GATEand the original documents are enriched with thecrowdsourced information, modelled as multipleannotations (one per contributor).
Figure 4 showsjudgements that have been imported from Crowd-Flower and stored as annotations on the originaldocument.
One useful feature is the trust metric,assigned by CrowdFlower for this judgement.GATE?s existing tools for calculating inter-annotator agreement and for corpus analysis areused to gain further insights into the quality of thecollected information.
If manual adjudication isrequired, GATE?s existing annotations stack edi-tor is used to show in parallel the annotations im-ported from CrowdFlower, so that differences injudgement can easily be seen and resolved.
Alter-natively, automatic adjudication via majority voteor other more sophisticated strategies can be im-plemented in GATE as components.4 ConclusionThis paper described the GATE Crowdsourcingplugin2and the reusable components that it pro-vides for automatic mapping of corpora to micro-tasks and vice versa, as well as the generic se-quence selection and classification user interfaces.These are easily configurable for a wide rangeof NLP corpus annotation tasks and, as part ofthis demonstration, several example crowdsourc-ing projects will be shown.Future work will focus on expanding the num-ber of reusable components, the implementationof reusable automatic adjudication algorithms,and providing support for crowdsourcing throughgames-with-a-purpose (GWAPs).Acknowledgments This was part of the uCompproject (www.ucomp.eu).
uComp receives thefunding support of EPSRC EP/K017896/1, FWF1097-N23, and ANR-12-CHRI-0003-03, in theframework of the CHIST-ERA ERA-NET.2It is available to download from http://gate.ac.uk/ .ReferencesKalina Bontcheva, Hamish Cunningham, Ian Roberts,Angus.
Roberts, Valentin.
Tablan, Niraj Aswani, andGenevieve Gorrell.
2013.
GATE Teamware: AWeb-based, Collaborative Text Annotation Frame-work.
Language Resources and Evaluation,47:1007?1029.Chris Callison-Burch and Mark Dredze.
2010.
Cre-ating speech and language data with Amazon?s Me-chanical Turk.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk, pages 1?12.Hamish Cunningham, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
GATE:an Architecture for Development of Robust HLTApplications.
In Proceedings of the 40th An-nual Meeting on Association for ComputationalLinguistics, 7?12 July 2002, ACL ?02, pages168?175, Stroudsburg, PA, USA.
Association forComputational Linguistics.Karen Fort, Gilles Adda, and K. Bretonnel Cohen.2011.
Amazon mechanical turk: Gold mine or coalmine?
Computational Linguistics, 37(2):413 ?420.Eduard Hovy.
2010.
Annotation.
In Tutorial Abstractsof ACL.N.
Ide, P. Bonhomme, and L. Romary.
2000.
XCES:An XML-based Standard for Linguistic Corpora.In Proceedings of the second International Confer-ence on Language Resources and Evaluation (LREC2000), 30 May ?
2 Jun 2000, pages 825?830,Athens, Greece.Shashank Khanna, Aishwarya Ratan, James Davis, andWilliam Thies.
2010.
Evaluating and improving theusability of Mechanical Turk for low-income work-ers in India.
In Proceedings of the first ACM sympo-sium on computing for development.
ACM.Manfred Stede and Chu-Ren Huang.
2012.
Inter-operability and reusability: the science of annota-tion.
Language Resources and Evaluation, 46:91?94.
10.1007/s10579-011-9164-x.A.
Wang, C.D.V.
Hoang, and M. Y. Kan. 2012.
Per-spectives on Crowdsourcing Annotations for Natu-ral Language Processing.
Language Resources andEvaluation, Mar:1?23.100
