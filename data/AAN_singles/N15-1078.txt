Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 767?777,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsSentiment after Translation:A Case-Study on Arabic Social Media PostsMohammad SalamehUniversity of Albertamsalameh@ualberta.caSaif M. Mohammad and Svetlana KiritchenkoNational Research Council Canada{saif.mohammad,svetlana.kiritchenko}@nrc-cnrc.gc.caAbstractWhen text is translated from one languageinto another, sentiment is preserved to varyingdegrees.
In this paper, we use Arabic socialmedia posts as stand-in for source languagetext, and determine loss in sentiment pre-dictability when they are translated into En-glish, manually and automatically.
As bench-marks, we use manually and automatically de-termined sentiment labels of the Arabic texts.We show that sentiment analysis of Englishtranslations of Arabic texts produces compet-itive results, w.r.t.
Arabic sentiment analy-sis.
We discover that even though translationsignificantly reduces the human ability to re-cover sentiment, automatic sentiment systemsare still able to capture sentiment informationfrom the translations.1 IntroductionAutomatic sentiment analysis of text, especially so-cial media posts, has a number of applications incommerce, public health, and public policy devel-opment.
However, a vast majority of prior researchon automatic sentiment analysis has been on En-glish texts.
Furthermore, many sentiment resourcesessential to automatic sentiment analysis (e.g., sen-timent lexicons) exist only in English.
Thus thereis a growing need for effective methods for analyz-ing text from other languages such as Arabic andChinese, especially posts on social media.
Therehas also been marked progress in automatic trans-lation of texts, especially from other languages intoEnglish.
Thus, instead of building source-languagespecific sentiment analysis systems, one can trans-late the texts into English and use an English sen-timent analysis system.
However, it is widely be-lieved that aspects of sentiment may be lost in trans-lation, especially in automatic translation.
Though,the extent of this loss, in terms of drop in accuracy ofautomatic sentiment systems remains undetermined.This paper analyzes several methods available inannotating non-English texts for sentiment:?
Use a source-language sentiment analysis sys-tem.?
Run an English sentiment analysis system onmanually created English translations of sourcelanguage text.?
Run an English sentiment analysis system onautomatically generated English translations ofsource language text.In our experiments, we use Arabic social mediaposts as a specific instance of the source languagetext.
We use state-of-the-art Arabic and English sen-timent analysis systems as well as a state-of-the-art Arabic-to-English translation system.
We out-line the advantages and disadvantages of each of themethods listed above, and more importantly con-duct experiments to determine accuracy of sentimentlabels obtained using each of these methods.
Asbenchmarks we use manually and automatically de-termined sentiment labels of the Arabic tweets.These results will help users determine methodsbest suited for their particular needs.
Along the way,we answer several research questions such as:1.
What sentiment prediction accuracy is ex-pected when Arabic blog posts and tweets are767translated into English (using the current state-of-art techniques), and then run through a state-of-the-art English sentiment analysis system?2.
How does this performance compare with thatof a current state-of-the-art Arabic sentimentsystem?3.
What is the loss in sentiment predictabilitywhen translating Arabic text into English au-tomatically vs. manually?4.
How difficult is it for humans to determine sen-timent of automatically translated text?5.
When dealing with translated text, which ismore accurate at determining the sentiment ofArabic text: (1) automatic sentiment analysisof the translated text, or (2) human annotationof the translated text for sentiment?The inferences drawn from these experiments donot necessarily apply to language pairs other thanArabic?English.
Languages can differ significantlyin terms of characteristics that impact accuracy of anautomatic sentiment analysis system.
Our goal herespecifically is to understand sentiment predictabilityof Arabic dialectal text on translation.
However, asimilar set of experiments can be used for other lan-guage pairs as well to determine the impact of trans-lation on sentiment.Through our experiments on two differentdatasets, we show that sentiment analysis of Englishtranslations of Arabic texts produces competitive re-sults, w.r.t.
Arabic sentiment analysis.
We also showthat translation (both manual and automatic) intro-duces marked changes in sentiment carried by thetext; positive and negative texts can often be trans-lated into texts that are neutral.
We also find thatcertain attributes of automatically translated text thatmislead humans with regards to the true sentiment ofthe source text, do not seem to affect the automaticsentiment analysis system.In the process of developing these experiments tostudy how translation alters sentiment, we created astate-of-the-art Arabic sentiment analysis system byporting NRC-Canada?s competition winning system(Kiritchenko et al, 2014) to Arabic.
We also cre-ated a substantial amount of sentiment labeled datapertaining to Arabic social media texts and their En-glish translations which is made freely available.11http://www.purl.com/net/ArabicSentimentThis is the first such resource where text in onelanguage and its translations into another language(both manually and automatically produced) areeach manually labeled for sentiment.2 Related Work2.1 Sentiment Analysis of English Social MediaSentiment analysis systems have been applied tomany different kinds of texts including customerreviews, newspaper headlines (Bellegarda, 2010),novels (Boucouvalas, 2002; Mohammad and Yang,2011), emails (Liu et al, 2003; Mohammad andYang, 2011), blogs (Neviarouskaya et al, 2011),and tweets (Mohammad, 2012).
Often these sys-tems have to cater to the specific needs of the textsuch as formality versus informality, length of utter-ances, etc.
Sentiment analysis systems developedspecifically for tweets include those by Go et al(2009), Pak and Paroubek (2010), Agarwal et al(2011), and Thelwall et al (2011).
A survey byMart?
?nez-C?amara et al (2012) provides an overviewof the research on sentiment analysis of tweets.
Inthe last two years, several shared tasks on sentimentanalysis were organized by the Conference on Se-mantic Evaluation Exercises (SemEval), which al-lowed for comparison of different approaches oncommon datasets from different domains (Wilsonet al, 2013; Rosenthal et al, 2014; Pontiki et al,2014).
The NRC-Canada system (Kiritchenko et al,2014) ranked first in these competitions, and we useit in our experiments.
Details of the system are de-scribed in Section 6.2.2 Sentiment Analysis of Arabic Social MediaSentiment analysis of Arabic social media texts hasseveral challenges.
The text is often in a regionalArabic dialect rather than Modern Standard Arabic(MSA).
Unlike MSA which is a standardized formof Arabic, dialectal Arabic is the spoken form ofArabic and lacks strict writing standards.
The textoften includes words from languages other than Ara-bic and multiple scripts may be used to express Ara-bic and foreign words.
In addition, Arabic is a mor-phologically complex language, thus having a lex-icon of word-sentiment associations that covers alldifferent surface forms becomes a cumbersome task.Negation in MSA is expressed through negation par-768ticles, but in some dialects (Egyptian) it is expressedusing suffixes at the end of the word.
We refer thereader to Mourad and Darwish (2013) for more de-tails on these issues.There have been a few studies tackling senti-ment analysis of Arabic texts (Ahmad et al, 2006;Badaro et al, 2014).
The ones most closely relatedto our work are the studies of sentiment analysisof Arabic social media (Al-Kabi et al, 2013; El-Beltagy and Ali, 2013; Mourad and Darwish, 2013;Abdul-Mageed et al, 2014).
Here we review exist-ing Arabic sentiment analysis systems that were de-signed specifically for Arabic social media datasets.Abdul-Mageed et al (2014) trained an SVM clas-sifier on a manually labeled dataset and applied atwo-stage classification that first separates subjec-tive from objective sentences and then classifies thesubjective into positive or negative instances.
Theauthors have compiled several datasets from mul-tiple social media resources that include chatroommessages, tweets, forum posts, and Wikipedia Talkpages.
However, these resources have not beenmade publicly available yet.Mourad and Darwish (2013) trained SVM andNaive Bayes classifiers on Arabic tweets annotatedby two native Arabic speakers.
We compare our sys-tem?s performance to theirs in Section 7.Refaee and Rieser (2014b) manually annotatedtweets for sentiment by two native Arabic speak-ers.
They used an SVM to classify tweets in a two-stage approach, polar vs neutral, then positive vs.negative.
The authors shared their data with us andwe test our system on their dataset.
However, thedataset they provided us is a larger superset thanthe one they had originally used (Refaee and Rieser,2014a).
Thus, the results of sentiment systems onthe two sets are not directly comparable.2.3 Multilingual Sentiment AnalysisWork on multilingual sentiment analysis has mainlyaddressed mapping sentiment resources from En-glish into morphologically complex languages.
Mi-halcea et al (2007) used English resources to au-tomatically generate a Romanian subjectivity lex-icon using an English?Romanian dictionary.
Thegenerated lexicon is then used to classify Roma-nian text.
Wan (2008) translated Chinese cus-tomer reviews to English using a machine trans-lation system.
The translated reviews are thenclassified with a rule-based system that relies onEnglish lexicons.
A higher accuracy is achievedby using ensemble methods and combining knowl-edge from Chinese and English resources.
Bal-ahur and Turchi (2014) conducted a study to as-sess the performance of statistical sentiment analy-sis techniques on machine-translated texts.
Opinion-bearing phrases from the New York Times text cor-pus (2002?2005) were automatically translated us-ing publicly available machine-translation engines(Google, Bing, and Moses).
Then, the accuracyof a sentiment analysis system trained on originalEnglish texts was compared to the accuracy of thesystem trained on automatic translations to German,Spanish, and French.
The authors concluded thatthe quality of machine translation is sufficient forsentiment analysis to be performed on automaticallytranslated texts without a substantial loss in accu-racy.
Contrary to that work, our study uses bothmanual and automatic translations as well as bothmanual and automatic sentiment assignments to sys-tematically examine the effect of translation on sen-timent.
Additionally, we deal with noisy social me-dia texts as opposed to more polished news mediatexts.
There exists research on using sentiment anal-ysis to improve machine translation (Chen and Zhu,2014), but that is beyond the scope of this paper.3 Method for Determining SentimentPredictability on TranslationIn order to systematically study the impact of trans-lation on sentiment analysis, we propose the follow-ing experimental setup:?
Identify or compile an Arabic social mediadataset.
We will refer to it as Ar.
(Ar comesfrom the first two letter of Arabic.)?
Manually translate Ar into English.
Wewill refer to these English translations asEn(Manl.Trans.)
[Manl.
is for manual, andTrans.
is for translations.]?
Automatically translate Ar into English.
Wewill refer to these English translations asEn(Auto.Trans.)
[Auto.
is for automatic.]?
Manually annotate Ar.
for sentiment.
Wewill refer to the sentiment-labeled dataset asAr(Manl.Sent.
)769Figure 1: Experimental setup to determine the impact of translation on sentiment.
We compare sentiment labels be-tween Ar(Manl.Sent.)
(shown in a shaded box) and other datasets shown on the right side of the figure.
Ar(Manl.Sent.
)is the original Arabic text manually annotated for sentiment.?
Manually annotate all English datasets[En(Manl.Trans.)
and En(Auto.Trans.
)]for sentiment, creating En(Manl.Trans.,Manl.Sent.)
and En(Auto.Trans., Manl.Sent.),respectively.?
Run a state-of-the-art Arabic sentiment analy-sis system on Ar, creating Ar(Auto.Sent.)?
Run a state-of-the-art English sentimentanalysis system on all the English datasets[En(Manl.Trans.)
and En(Auto.Trans.
)],creating En(Manl.Trans., Auto.Sent.)
andEn(Auto.Trans., Auto.Sent.
), respectively.Figure 1 depicts this setup.
Once the varioussentiment-labeled datasets are created, we can com-pare pairs of datasets to draw inferences.
For ex-ample, comparing the labels for Ar(Manl.Sent.)
andEn(Manl.Trans., Manl.Sent.)
will show how differ-ent the sentiment labels tend to be when text is trans-lated from Arabic to English.
The comparison willalso show, for example, whether positive tweets tendto often be translated into neutral tweets, and to whatextent.
The results will also show how feasible it isto first translate Arabic text into English and then useautomatic sentiment analysis (Ar(Manl.Sent.)
vs.En(Auto.Trans., Auto.Sent.)).
In Section 8, we pro-vide an analysis of several such comparisons for twodifferent Arabic social media datasets.DATA: Since manual translation of text from Ara-bic to English is a costly exercise, we chose, for ourexperiments, an existing Arabic social media datasetthat has already been translated ?
the BBN Arabic-Dialect/English Parallel Text (Zbib et al, 2012).2Itcontains about 3.5 million tokens of Arabic dialectsentences and their English translations.
We use arandomly chosen subset of 1200 Levantine dialectalsentences, which we will refer to as the BBN posts orBBN dataset, in our experiments.
Additionally, wealso conduct experiments on a dataset of 2000 tweetsoriginating from Syria (a country where Levantinedialectal Arabic is commonly spoken).
These tweetswere collected in May 2014 by polling the TwitterAPI.
We will refer to this dataset as the Syrian tweetsor Syrian dataset.
Note, however, that manual trans-lations of the Syrian dataset are not available.The experimental setup described above involvesseveral component tasks: generating translationsmanually and automatically (Section 4), manuallyannotating Arabic and English texts for sentiment(Section 5), automatic sentiment analysis of Englishtexts (Section 6), and automatic sentiment analysisof Arabic texts (Section 7).4 Generating English TranslationsThe BBN dialectal Arabic dataset comes with man-ual translations into English.
We generate automatictranslations of the Arabic BBN posts and the Syr-ian tweets, by training a multi-stack phrase-basedmachine translation system to translate from Arabicto English.
Our in-house system is quite similar toCherry and Foster (2012).
This statistical machinetranslation (SMT) system is trained on data fromOpenMT 2012.
We preprocess the training data by2https://catalog.ldc.upenn.edu/LDC2012T09770segmenting the Arabic source side of the trainingdata with MADA 3.2 (Habash et al, 2009), usingPenn Arabic Treebank (PATB) segmentation schemeas recommended by El Kholy and Habash (2012).The Arabic script is further normalized by convert-ing different forms of Alif @@@ @ and Ya ?
?to bareAlif @ and dotless Ya ?.
The different forms areused interchangeably, and normalization decreasesthe sparcity of Arabic tokens and improves transla-tion.
The English side of the training data is lower-cased and tokenized by stripping punctuation marks.We set the decoder?s stack size to 10000 and dis-tortion limit to 7.
We replace the out-of-vocabularywords in the translated text with UNKNOWN token(which is shown to the annotators).
The decoder?slog-linear model is tuned with MIRA (Chiang et al,2008; Cherry and Foster, 2012).
A KN-smoothed 5-gram language model is trained on the English Gi-gaword and the target side of the parallel data.5 Creating sentiment labeled data inArabic and EnglishManual sentiment annotations were performed onthe crowdsourcing platform CrowdFlower3for threeBBN datasets and two Syrian datasets:1.
Original Arabic posts (BBN and Syriadatasets), annotated by Arabic speakers.2.
Manual English translations of Arabic posts,annotated by English speakers (only for BBNdataset).3.
Automatic English translations of Arabic posts(BBN and Syria datasets), annotated by En-glish speakers.Each post was annotated by at least ten annotatorsand the majority sentiment label was chosen.
Ta-ble 1 shows the class distribution of sentiment la-bels in various datasets.
Observe from rows a andd that neutral tweets constitute only about 10% ofthe data in both BBN and Syria datasets.
The Syriantweets have a much higher percentage of negativeposts, whereas in the BBN data, the percentages ofpositive and negative posts are comparable.
(Arabictweets in general tend to be much more skewed tothe negative class than Arabic blog post sentences.
)Rows b, c, and e show that translated texts tend to3http://www.crowdflower.comlose some of the sentiment information and there isa relatively higher percentage of neutral instances inthe translated text than in the original text.For each post, we determine the count of the mostfrequent annotation divided by the total number ofannotations.
This score is averaged for all posts todetermine the inter-annotator agreement shown inthe last column of Table 1.
We use this agreementscore as benchmark to compare performance of au-tomatic sentiment systems (described below).6 English Sentiment AnalysisWe use the English-language sentiment analysis sys-tem developed by NRC-Canada (Kiritchenko et al,2014) in our experiments.
This system obtainedhighest scores in two recent international compe-titions on sentiment analysis of tweets ?SemEval-2013 Task 2 and SemEval-2014 Task 9 (Wilson etal., 2013; Rosenthal et al, 2014).
We briefly de-scribe the system below; for more details, we referthe reader to Kiritchenko et al (2014).A linear-kernel Support Vector Machine (Changand Lin, 2011) classifier is trained on the avail-able training data.
The classifier leverages a vari-ety of surface-form, semantic, and sentiment lexi-con features described below.
The sentiment lex-icon features are derived from existing, general-purpose, manual lexicons, namely NRC EmotionLexicon (Mohammad and Turney, 2010; Moham-mad and Turney, 2013), Bing Liu?s Lexicon (Huand Liu, 2004), and MPQA Subjectivity Lexicon(Wilson et al, 2005), as well as automatically gen-erated, tweet-specific lexicons, Hashtag SentimentLexicon and Sentiment140 Lexicon (Kiritchenko etal., 2014).46.1 Generating English Sentiment LexiconAblation experiments in Mohammad et al (2013)showed that their sentiment system benefited mostfrom the use of the Hashtag Sentiment Lexicon.
Thelexicon was created as follows.
A list of 77 seedwords, which are synonyms of positive and negative,was compiled from the Roget?s Thesaurus.
Then,the Twitter API was polled to collect tweets that hadthese words as hashtags.
A tweet is considered pos-itive if it has a positive hashtag and negative if it4http://www.purl.com/net/lexicons771positive negative neutral agreementBBN dataa.
Ar(Manl.Sent) 41.50 47.92 10.58 73.80b.
En(Manl.Trans., Manl.Sent) 35.00 43.25 21.75 68.00c.
En(Auto.Trans., Manl.Sent) 36.17 36.50 27.34 65.70Syria datad.
Ar(Manl.Sent) 22.40 67.50 10.10 79.00e.
En(Auto.Trans., Manl.Sent) 14.25 66.15 19.60 76.10Table 1: Class distribution (in percentage) of the sentiment annotated datasets.has a negative hashtag.
For each term in the tweetset, a sentiment score is computed by measuringthe PMI (pointwise mutual information) between theterm and the positive and negative categories:SenScore (w) = PMI(w, pos)?
PMI(w, neg) (1)where w is a term in the lexicon.
PMI(w, pos) isthe PMI score between w and the positive class, andPMI(w, neg) is the PMI score between w and thenegative class.
A positive SenScore (w) suggests thatthe word is associated with positive sentiment and anegative score suggests that the word is associatedwith negative sentiment.
The magnitude indicatesthe strength of the association.6.2 Pre-processing and Feature GenerationThe following pre-processing steps are performed.URLs and user mentions are normalized tohttp://someurl and @someuser, respectively.
Tweetsare tokenized and part-of-speech tagged with theCMU Twitter NLP tool (Gimpel et al, 2011).
Then,each tweet is represented as a feature vector.The features:- Word and character ngrams;- POS: # occurrences of each part-of-speech tag;- Negation: # negated contexts.
Negation alsoaffects the ngram features: a word w becomesw NEG in a negated context;- Automatic sentiment lexicons: For each token woccurring in a tweet, its sentiment score score(w) isused to compute: # tokens with score(w) 6= 0; thetotal score =?w?tweetscore(w); the maximal score= maxw?tweetscore(w); the score of the last tokenin the tweet.- Manually created sentiment lexicons: For each ofthe three manual sentiment lexicons, the followingfeatures are computed: the sum of positive and thesum of negative scores for tweet tokens in affirma-tive contexts and in negated contexts, separately.7 Arabic Sentiment Analysis7.1 Building an Arabic Sentiment SystemWe built an Arabic sentiment analysis system byreconstructing the NRC-Canada English system todeal with Arabic text.
It extracts the same featureset as described in Section 6.2.
We also generateda word-sentiment association lexicon as describedin Section 6.1, but for Arabic words from Arabictweets (more details in sub-section below).
We pre-process Arabic text by tokenizing with CMU TwitterNLP tool to deal with specific tokens such as URLs,usernames, and emoticons.
Then we use MADA togenerate lemmas.
Finally, we normalize differentforms of Alif and Ya to bare Alif and dotless Ya todecrease token sparcity in Arabic datasets.7.1.1 Generating Arabic Sentiment LexiconWe translated 77 positive and negative seed wordsused to generate the English NRC Hashtag Senti-ment Lexicon into Arabic using Google Translate.Among the several translations provided by it, wechose words that were less ambiguous and tendedto have strong sentiment in Arabic texts.
To increasethe coverage of our seed list, we manually added dif-ferent inflections for these translations.We polled the Twitter API for the period of Juneto August 2014 and collected tweets with #(key-word).
After filtering out duplicate tweets andretweets, we ended up with 163,944 positive uniquetweets and 37,848 negative unique tweets.
Then foreach word w, SenScore (w) was calculated just asdescribed in Section 6.1.772Arabic Sentiment Labeled Dataset MD RR BBN Syriasentiment classes pos, neg pos,neg pos, neg, neu pos, neg, neunumber of instances 1111 2681 1199 2000Most frequent class baseline 66.06 68.92 47.95 67.50Human agreement benchmark - - 73.82 79.05Mourad and Darwish Arabic SA system 72.50 - - -Our Arabic SA system 74.62 85.23 63.89 78.65Table 2: Accuracy (in percentage) of sentiment analysis (SA) systems on various Arabic social media datasets.pos neg neuBBN dataa.
Ar(Auto.Sent) 39.78 60.05 0.17b.
En(Manl.Trans., Auto.Sent) 43.12 55.63 1.25c.
En(Auto.Trans., Auto.Sent) 42.87 56.05 1.08Syria datad.
Ar(Auto.Sent) 20.60 75.30 4.10e.
En(Auto.Trans., Auto.Sent) 24.75 69.75 5.50Table 3: Class distribution (in percentage) resulting fromautomatic sentiment analysis.7.2 EvaluationWe tested the Arabic sentiment system on two ex-isting Arabic datasets (Mourad and Darwish (2013)(MD) and Refaee and Rieser (2014a) (RR)) and twonewly sentiment-annotated Arabic datasets (BBNand Syria).
Table 2 shows results of ten-fold cross-validation experiments on each of the datasets.
ForMD and RR, the presented results are for the two-class problem (positive vs. negative) to allow forcomparison with prior published results.
For BBNand Syria, the results are shown for the case wherethe system has to identify one of three classes: pos-itive, negative, or neutral.
Human agreement scoresare shown where available.Note that the accuracy of our system is higherthan previously published results on the MD dataset.The only previously published results on the RRdataset are on a small subset (about 1000 instances)for which Refaee and Rieser (2014a) obtained an ac-curacy of 87%.
The results in Table 2 are for a largerdataset and so not directly comparable.8 Sentiment After TranslationUsing the methods and systems described in Sec-tions 4, 5, 6, and 7, we generated all the manu-ally and automatically labeled datasets mentioned inSection 3?s Experimental Setup.
Table 3 shows thedistribution of positive, negative, and neutral classesin datasets that have been automatically labeled withsentiment.
These percentages can be compared withthose in Table 1 (rows a and d) which show thetrue sentiment distribution in the BBN and Syriadatasets.
Observe that the automatic system hasdifficulty in assigning neutral class to posts.
Thisis probably because of the small percentage (about10%) of neutral tweets in the training data.
Also no-tice that the system predominantly guesses negative,which is also a reflection of the distribution in thetraining data.
The strong bias to negatives is less-ened in the English translations.Main Result: Tables 4 and 5 show how simi-lar the sentiment labels are across various pairs ofdatasets for the BBN posts and the Syrian posts, re-spectively.
For example, row a. in Table 4 shows thecomparison between Arabic tweets that were man-ually annotated for sentiment and those that wereautomatically labeled for sentiment by our Arabicsentiment analysis system.
Column 2 shows thepercentage of instances where the sentiment labelsmatch across the two datasets being compared.
Forrow a. the match percentage of 63.89% representsthe accuracy of the automatic sentiment analysissystem on the Arabic BBN posts.Row b. shows the difference in labels when textis manually translated from Arabic to English, eventhough sentiment labeling in both Arabic and En-glish is done manually.
Observe that the two labelsmatch only 71.31% of the time.
However, the agree-ment among human sentiment annotators on originalArabic texts was only 73.8%.
So, the English trans-lation does affect sentiment, but not dramatically.Row c. shows results for when the manually trans-lated text is run through an English sentiment anal-ysis system and the labels are compared againstAr(Manl.Sent.)
Observe that the match for this pairis 68.65%, which is not too much lower than 71.31%obtained by manual sentiment labeling.
This shows773Data Pair Match %a.
Ar(Manl.Sent) - Ar(Auto.Sent) 63.89b.
Ar(Manl.Sent) - En(Manl.Trans., Manl.Sent) 71.31c.
Ar(Manl.Sent) - En(Manl.Trans., Auto.Sent) 68.65d.
Ar(Manl.Sent) - En(Auto.Trans., Manl.Sent) 57.21e.
Ar(Manl.Sent) - En(Auto.Trans., Auto.Sent) 62.49f.
En(Manl.Trans., Manl.Sent) - En(Auto.Trans., Manl.Sent) 60.08g.
En(Manl.Trans., Manl.Sent) - En(Manl.Trans., Auto.Sent) 66.51h.
En(Auto.Trans., Manl.Sent) - En(Auto.Trans., Auto.Sent) 69.58Table 4: Match percentage between pairs of sentiment labelled BBN datasets.Data Pair Match %a.
Ar(Manl.Sent) - Ar(Auto.Sent) 78.65b.
Ar(Manl.Sent) - En(Auto.Trans., Manl.Sent) 71.05c.
Ar(Manl.Sent) - En(Auto.Trans.-Auto.Sent) 78.11d.
En(Auto.Trans, Manl.Sent) - En(Auto.Trans., Auto.Sent) 78.80Table 5: Match percentage between pairs of sentiment labelled Syria datasets.that the English sentiment system is performingrather well.
(One would not expect it to get a matchgreater than 71.31%.)
More importantly, the En-glish sentiment system shows a competitive resultof 62.49% when run on the automatically trans-lated text (row e.), which makes this choice a viableoption for sentiment analysis of non-English texts.This result is inline with previous findings in Infor-mation Retrieval (Nie et al, 1999) and Text Classi-fication (Amini and Goutte, 2010).Rows d. and e. compare Ar(Manl.Sent.)
withmanual and automatic sentiment labeling of auto-matic translations.
Since automatic translation fromArabic to English is fairly difficult, we expect thesematch percentages to be lower than those in rows b.and c., and that is exactly what we observe.
How-ever, it is unexpected to find the number for row e.to be higher than that of row d. We find the same pat-tern for corresponding data pairs in the Syrian tweetsas well (rows b. and c. in Table 6).
This suggeststhat certain attributes of automatically translated textmislead humans with regards to the true sentiment ofthe source text.
However, these same attributes donot seem to affect the automatic sentiment analysissystem as much.
Since the NRC sentiment analy-sis system is largely reliant on word-sentiment as-sociations and does not use syntax-based features,it is possible that syntactic abnormalities introducedby automatic translation impact human perceptionof sentiment.
However, this supposition needs to bevalidated by future work.Row f. shows that manual and automatic transla-tion lead to only about 60% match in manually an-notated sentiment labels with each other.
Row g.shows accuracy of the English automatic sentimentanalysis system on the manually translated text (as-suming the English sentiment labels as gold).
Theresult of 66.51% is very close to human agreementon manually translated data (68%), which demon-strates the high quality of the English sentimentanalysis system.
Row h. shows accuracy of the En-glish automatic sentiment analysis system on theautomatically translated text (assuming the Englishsentiment labels as gold).
In this case, the sys-tem?s accuracy of 69.58% is higher than the humanagreement on automatically translated text (65.7%),which again shows that automatic translation greatlyimpacts sentiment perceived by humans.We manually examined several tweets from theBBN dataset to understand why humans incorrectlyannotate a tweet?s automatic translation.
Most ofthe cases were due to bad translation where sen-timent words either disappeared or were replacedwith words of opposite sentiment.
In some cases,the translation was affected by typos on the Arabicside.
Table 6 shows some examples.
Often the mis-translations occurred due to word sense ambiguity.For example, H.PA??
has two meanings: scorpionsand clock arms.
In example 1 (metaphorically stat-ing that relatives can hurt like scorpion bites), theword is mistranslated, leading to neutral (instead ofnegative) sentiment.7741.
Bad auto.
translation: mistranslation of ambiguous wordsPost H.PA??
H.PA?B@Q?
@?
@ ??J???
AJKY?
@ negativeAuto.Trans.
the minimum taught me that more relatives clock neutralManl.Trans.
Life has taught me that most of the relatives are scorpions negative2.
Bad auto.
translation: mistranslation of ambiguous wordsPost h.??J?
@ ?J?
?
??JK B?A??
????
@ ??JJ?
positiveAuto.Trans.
i wish i live in a place not cut off by snow negativeManl.Trans.
I wish I live in a place where snow never stops falling positive3.
Bad auto.
translation: sarcasm is hard to translatePost?A?P???X?k.??
???
@ H.Q???@Y??Qm?
'@ ???
negativeAuto.Trans.
you?re still good in front of the leakage of water existed from time positiveManl.Trans.
Expect more good to come, water has been leaking since a long time negativeTable 6: Examples where the automatic translation was annotated a sentiment different from the sentiment of theoriginal Arabic tweet, but whose original sentiment was correctly predicted by the English sentiment system.
Themanual translations are also listed for reference.One reason why the automatic sentiment analy-sis system correctly annotates several automaticallytranslated instances (where manual annotations ofthe translation may fail), is that the system canlearn an appropriate model even from mistranslatedtext ?
especially when automatic translation makesconsistent errors.
For example, Q??
@ ????
@ (Oh Godgrant victory to) has been consistently translated toGod forsake.
All tweets having this phrase are cor-rectly annotated as positive by our system, but weremarked negative by the human annotators.Caveats: The automatic systems employed inthese experiments, i.e., Arabic sentiment analysis,English sentiment analysis, and SMT systems, ex-hibit state-of-the-art performance; nevertheless, fur-ther improvements are possible.
The Arabic senti-ment system will benefit from extended sentimentlexicons and features derived specifically for theArabic language.
The English sentiment analysissystem can be further adapted to the peculiarities ofmachine-translated texts, which are notably differentfrom regular English.
The current translation systemhas been trained on non-tweet data that results in ahigh percentage of out-of-vocabulary words on ourdatasets.
In our experiments, we assumed that alltexts are written in Levantine dialect of the Arabiclanguage.
However, tweets can have a mixture ofdialects or even a mixture of languages (e.g., Ara-bic and English).
Addressing these factors will giveeven more insight on how sentiment is altered ontranslation, in specific contexts.9 ConclusionsWe presented a set of experiments to systemati-cally study the impact of English translation (man-ual and automatic) on sentiment analysis of Arabicsocial media posts.
Our experiments show that au-tomatic sentiment analysis of English translations(even of automatic translations) can lead to com-petitive results?results that are similar to that ob-tained by current state-of-the-art Arabic sentimentanalysis systems.
Our results also show that auto-matic sentiment analysis of automatic translationsoutperforms the manual sentiment annotations ofthe automatically translated text.
This suggests thatSMT errors impact human perception of sentimentmarkedly more than automatic sentiment systems.This is an interesting avenue for future exploration.We also show that translated texts tend to lose someof the sentiment information and there is a relativelyhigher percentage of neutral instances in the trans-lated text than in the original dataset.
The resourcescreated as part of this project (Arabic sentiment lex-icons, Arabic sentiment annotations of social me-dia posts, and English sentiment annotations of theirtranslations) are made freely available.5AcknowledgmentsThanks to Kareem Darwish and Eshrag Refaee forsharing their data.
We thank Colin Cherry, SamuelLarkin, and Marine Carpuat for helpful discussions.5http://www.purl.com/net/ArabicSentiment775ReferencesMuhammad Abdul-Mageed, Mona Diab, and SandraK?ubler.
2014.
SAMAR: Subjectivity and sentimentanalysis for Arabic social media.
Computer Speech &Language, 28(1):20 ?
37.Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,and Rebecca Passonneau.
2011.
Sentiment analysisof Twitter data.
In Proceedings of the Workshop onLanguages in Social Media, LSM ?11, pages 30?38,Portland, Oregon.Khurshid Ahmad, David Cheng, and Yousif Almas.2006.
Multi-lingual sentiment analysis of financialnews streams.
In Proceedings of the 1st InternationalConference on Grid in Finance.Mohammed Al-Kabi, Amal Gigieh, Izzat Alsmadi, Hei-der Wahsheh, and Mohamad Haidar.
2013.
An opin-ion analysis tool for colloquial and standard Arabic.In Proceedings of the 4th International Conference onInformation and Communication Systems, ICICS ?13.Massih-Reza Amini and Cyril Goutte.
2010.
A co-classification approach to learning from multilingualcorpora.
Machine learning, 79(1-2):105?121.Gilbert Badaro, Ramy Baly, Hazem Hajj, Nizar Habash,and Wassim El-Hajj.
2014.
A large scale Arabic sen-timent lexicon for Arabic opinion mining.
In Proceed-ings of the EMNLP Workshop on Arabic Natural Lan-guage Processing (ANLP), pages 165?173.
Associa-tion for Computational Linguistics.Alexandra Balahur and Marco Turchi.
2014.
Compar-ative experiments using supervised learning and ma-chine translation for multilingual sentiment analysis.Computer Speech & Language, 28(1):56?75.Jerome Bellegarda.
2010.
Emotion analysis using la-tent affective folding and embedding.
In Proceedingsof the NAACL-HLT Workshop on Computational Ap-proaches to Analysis and Generation of Emotion inText, pages 1?9, Los Angeles, California.Anthony C. Boucouvalas.
2002.
Real time text-to-emotion engine for expressive Internet communica-tion.
Emerging Communication: Studies on NewTechnologies and Practices in Communication, 5:305?318.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2(3):27:1?27:27.Boxing Chen and Xiaodan Zhu.
2014.
Bilingual sen-timent consistency for statistical machine translation.In Proceedings of the 14th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 607?615, Gothenburg, Sweden, April.
As-sociation for Computational Linguistics.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies, pages 427?436.Association for Computational Linguistics.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?08, pages 224?233.
Associationfor Computational Linguistics.Samhaa R El-Beltagy and Ahmed Ali.
2013.
Open is-sues in the sentiment analysis of Arabic social media:A case study.
In Proceedings of the 9th InternationalConference on Innovations in Information Technology,pages 215?220.
IEEE.Ahmed El Kholy and Nizar Habash.
2012.
Ortho-graphic and morphological processing for English?Arabic statistical machine translation.
Machine Trans-lation, 26(1-2):25?45, March.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging forTwitter: Annotation, features, and experiments.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics, ACL ?11, pages 42?47.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.Technical report, Stanford University.Nizar Habash, Owen Rambow, and Ryan Roth.
2009.MADA+TOKAN: A toolkit for Arabic tokenization,diacritization, morphological disambiguation, POStagging, stemming and lemmatization.
In Proceed-ings of the 2nd International Conference on ArabicLanguage Resources and Tools, pages 102?109, Cairo,Egypt, April.
The MEDAR Consortium.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the 10thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?04, pages168?177, New York, NY, USA.
ACM.Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-hammad.
2014.
Sentiment analysis of short infor-mal texts.
Journal of Artificial Intelligence Research,50:723?762.Hugo Liu, Henry Lieberman, and Ted Selker.
2003.A model of textual affect sensing using real-worldknowledge.
In Proceedings of the 8th InternationalConference on Intelligent User Interfaces, IUI ?03,pages 125?132, New York, NY.
ACM.Eugenio Mart?
?nez-C?amara, M Teresa Mart?
?n-Valdivia,L Alfonso Ure?nal?opez, and A Rturo Montejor?aez.7762012.
Sentiment analysis in Twitter.
Natural Lan-guage Engineering, pages 1?28.Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007.Learning multilingual subjective language via cross-lingual projections.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Linguis-tics, page 976.Saif M. Mohammad and Peter D. Turney.
2010.
Emo-tions evoked by common words and phrases: UsingMechanical Turk to create an emotion lexicon.
In Pro-ceedings of the NAACL-HLT Workshop on Computa-tional Approaches to Analysis and Generation of Emo-tion in Text, pages 26?34, LA, California.Saif M. Mohammad and Peter D. Turney.
2013.
Crowd-sourcing a word-emotion association lexicon.
Compu-tational Intelligence, 29(3):436?465.Saif M. Mohammad and Tony (Wenda) Yang.
2011.Tracking sentiment in mail: How genders differ onemotional axes.
In Proceedings of the ACL Workshopon Computational Approaches to Subjectivity and Sen-timent Analysis, WASSA ?11, pages 70?79, Portland,OR, USA.Saif M. Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.
In Proceedings ofthe 7th International Workshop on Semantic Evalua-tion Exercises, SemEval ?13, Atlanta, Georgia, USA,June.Saif M. Mohammad.
2012.
#Emotional tweets.
In Pro-ceedings of the First Joint Conference on Lexical andComputational Semantics, *SEM ?12, pages 246?255,Montr?eal, Canada.Ahmed Mourad and Kareem Darwish.
2013.
Subjec-tivity and sentiment analysis of modern standard Ara-bic and Arabic microblogs.
In Proceedings of the 4thWorkshop on Computational Approaches to Subjec-tivity, Sentiment and Social Media Analysis, WASSA?13, pages 55?64.Alena Neviarouskaya, Helmut Prendinger, and MitsuruIshizuka.
2011.
Affect analysis model: novel rule-based approach to affect sensing from text.
NaturalLanguage Engineering, 17:95?135, 1.Jian-Yun Nie, Michel Simard, Pierre Isabelle, andRichard Durand.
1999.
Cross-language informationretrieval based on parallel texts and automatic miningof parallel texts from the Web.
In Proceedings of the22nd Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 74?81.
ACM.Alexander Pak and Patrick Paroubek.
2010.
Twitteras a corpus for sentiment analysis and opinion min-ing.
In Proceedings of the 7th Conference on Inter-national Language Resources and Evaluation, LREC?10, pages 1320?1326, Valletta, Malta, May.Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, Har-ris Papageorgiou, Ion Androutsopoulos, and SureshManandhar.
2014.
SemEval-2014 Task 4: Aspectbased sentiment analysis.
In Proceedings of the Inter-national Workshop on Semantic Evaluation, SemEval?14, pages 27?35, Dublin, Ireland, August.Eshrag Refaee and Verena Rieser.
2014a.
An ArabicTwitter corpus for subjectivity and sentiment analy-sis.
In Proceedings of the 9th International Confer-ence on Language Resources and Evaluation, LREC?14, Reykjavik, Iceland, May.
European Language Re-sources Association.Eshrag Refaee and Verena Rieser.
2014b.
Subjectivityand sentiment analysis of Arabic Twitter feeds withlimited resources.
In Proceedings of the Workshop onFree/Open-Source Arabic Corpora and Corpora Pro-cessing Tools, page 16.Sara Rosenthal, Alan Ritter, Preslav Nakov, and VeselinStoyanov.
2014.
SemEval-2014 Task 9: Sentimentanalysis in Twitter.
In Proceedings of the 8th Inter-national Workshop on Semantic Evaluation, SemEval?14, pages 73?80, Dublin, Ireland, August.Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.2011.
Sentiment in Twitter events.
Journal of theAmerican Society for Information Science and Tech-nology, 62(2):406?418.Xiaojun Wan.
2008.
Using bilingual knowledge andensemble techniques for unsupervised Chinese senti-ment analysis.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,EMNLP ?08, pages 553?561, Stroudsburg, PA, USA.Association for Computational Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the Conferenceon Human Language Technology and Empirical Meth-ods in Natural Language Processing, HLT ?05, pages347?354, Stroudsburg, PA, USA.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, SaraRosenthal, Veselin Stoyanov, and Alan Ritter.
2013.SemEval-2013 Task 2: Sentiment analysis in Twit-ter.
In Proceedings of the International Workshop onSemantic Evaluation, SemEval ?13, Atlanta, Georgia,USA, June.Rabih Zbib, Erika Malchiodi, Jacob Devlin, DavidStallard, Spyros Matsoukas, Richard Schwartz, JohnMakhoul, Omar F Zaidan, and Chris Callison-Burch.2012.
Machine translation of Arabic dialects.
InProceedings of the Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies, pages 49?59.Association for Computational Linguistics.777
