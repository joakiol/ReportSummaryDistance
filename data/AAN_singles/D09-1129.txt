Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1241?1249,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPReal-Word Spelling Correction using Google Web 1T 3-gramsAminul IslamDepartment of Computer ScienceUniversity of OttawaOttawa, ON, K1N 6N5, Canadamdislam@site.uottawa.caDiana InkpenDepartment of Computer ScienceUniversity of OttawaOttawa, ON, K1N 6N5, Canadadiana@site.uottawa.caAbstractWe present a method for detecting andcorrecting multiple real-word spelling er-rors using the Google Web 1T 3-gram dataset and a normalized and modified ver-sion of the Longest Common Subsequence(LCS) string matching algorithm.
Ourmethod is focused mainly on how to im-prove the detection recall (the fraction oferrors correctly detected) and the correc-tion recall (the fraction of errors correctlyamended), while keeping the respectiveprecisions (the fraction of detections oramendments that are correct) as high aspossible.
Evaluation results on a standarddata set show that our method outperformstwo other methods on the same task.1 IntroductionReal-word spelling errors are words in a text thatoccur when a user mistakenly types a correctlyspelled word when another was intended.
Errorsof this type may be caused by the writer?s igno-rance of the correct spelling of the intended wordor by typing mistakes.
Such errors generally gounnoticed by most spellcheckers as they deal withwords in isolation, accepting them as correct ifthey are found in the dictionary, and flagging themas errors if they are not.
This approach wouldbe sufficient to detect the non-word error myss in?It doesn?t know what the myss is all about.?
butnot the real-word error muss in ?It doesn?t knowwhat the muss is all about.?
To detect the latter,the spell-checker needs to make use of the sur-rounding context such as, in this case, to recog-nise that fuss is more likely to occur than muss inthe context of all about.
Ironically, errors of thistype may even be caused by spelling checkers inthe correction of non-word spelling errors whenthe auto-correct feature in some word-processingsoftware sometimes silently change a non-word tothe wrong real word (Hirst and Budanitsky, 2005),and sometimes when correcting a flagged error,the user accidentally make a wrong selection fromthe choices offered (Wilcox-O?Hearn et al, 2008).An extensive review of real-word spelling cor-rection is given in (Pedler, 2007; Hirst and Budan-itsky, 2005) and the problem of spelling correctionmore generally is reviewed in (Kukich, 1992).The Google Web 1T data set (Brants and Franz,2006), contributed by Google Inc., contains En-glish word n-grams (from unigrams to 5-grams)and their observed frequency counts calculatedover 1 trillion words from web page text col-lected by Google in January 2006.
The text wastokenised following the Penn Treebank tokenisa-tion, except that hyphenated words, dates, emailaddresses and URLs are kept as single tokens.The sentence boundaries are marked with two spe-cial tokens <S> and </S>.
Words that occurredfewer than 200 times were replaced with the spe-cial token <UNK>.
Table 1 shows the data sizesof the Web 1T corpus.
The n-grams themselvesTable 1: Google Web 1T Data SizesNumber of Number Size on disk(in KB)Tokens 1,024,908,267,229 N/ASentences 95,119,665,584 N/AUnigrams 13,588,391 185,569Bigrams 314,843,401 5,213,440Trigrams 977,069,902 19,978,5404-grams 1,313,818,354 32,040,8845-grams 1,176,470,663 33,678,504must appear at least 40 times to be included in theWeb 1T corpus1.
It is expected that this data willbe useful for statistical language modeling, e.g.,1Details of the Google Web 1T data set can be found atwww.ldc.upenn.edu/Catalog/docs/LDC2006T13/readme.txt1241for machine translation or speech recognition, aswell as for other uses.In this paper, we present a method for detectingand correcting multiple real-word spelling errorsusing the Google Web 1T 3-gram data set, and anormalized and modified version of the LongestCommon Subsequence (LCS) string matching al-gorithm (details are in section 3.1).
By multiple er-rors, we mean that if we have n words in the inputsentence, then we try to detect and correct at mostn-1 errors.
We do not try to detect and correct anerror, if any, in the first word as it is not compu-tationally feasible to search in the Google Web 1T3-grams while keeping the first word in the 3-gramas a variable.
Our intention is to focus on how toimprove the detection recall (the fraction of errorscorrectly detected) or correction recall (the frac-tion of errors correctly amended) while maintain-ing the respective precisions (the fraction of de-tections or amendments that are correct) as high aspossible.
The reason behind this intention is that ifthe recall for any method is around 0.5, this meansthat the method fails to detect or correct around 50percent of the errors.
As a result, we can not com-pletely rely on these type of methods, for that weneed some type of human interventions or sugges-tions to detect or correct the rest of the undetectedor uncorrected errors.
Thus, if we have a methodthat can detect or correct almost 80 percent of theerrors, even generating some extra candidates thatare incorrect is more helpful to the human.This paper is organized as follow: Section 2presents a brief overview of the related work.
Ourproposed method is described in Section 3.
Eval-uation and experimental results are discussed inSection 4.
We conclude in Section 5.2 Related WorkWork on real-word spelling correction can roughlybe classified into two basic categories: methodsbased on semantic information or human-madelexical resources, and methods based on machinelearning or probability information.
Our proposedmethod falls into the latter category.2.1 Methods Based on Semantic InformationThe ?semantic information?
approach first pro-posed by Hirst and St-Onge (1998) and later devel-oped by Hirst and Budanitsky (2005) detected se-mantic anomalies, but was not restricted to check-ing words from predefined confusion sets.
Thisapproach was based on the observation that thewords that a writer intends are generally seman-tically related to their surrounding words, whereassome types of real-word spelling errors are not,such as (using Hirst and Budanitsky?s example),?It is my sincere hole (hope) that you will recoverswiftly.?
Such ?malapropisms?
cause ?a pertur-bation of the cohesion (and coherence) of a text.
?Hirst and Budanitsky (2005) use semantic distancemeasures in WordNet (Miller et al, 1993) to de-tect words that are potentially anomalous in con-text - that is, semantically distant from nearbywords; if a variation in spelling results in a wordthat was semantically closer to the context, it ishypothesized that the original word is an error (a?malapropism?)
and the closer word is its correc-tion.2.2 Methods Based on Machine LearningMachine learning methods are regarded as lexicaldisambiguation tasks and confusion sets are usedto model the ambiguity between words.
Normally,the machine learning and statistical approachesrely on pre-defined confusion sets, which are sets(usually pairs) of commonly confounded words,such as {their, there, they?re} and {principle, prin-cipal}.
The methods learn the characteristics oftypical context for each member of the set and de-tect situations in which one member occurs in con-text that is more typical of another.
Such meth-ods, therefore, are inherently limited to a set ofcommon, predefined errors, but such errors can in-clude both content and function words.
Given anoccurrence of one of its confusion set members,the spellchecker?s job is to predict which mem-ber of that confusion set is the most appropriate inthe context.
Golding and Roth (1999), an exam-ple of a machine-learning method, combined theWinnow algorithm with weighted-majority voting,using nearby and adjacent words as features.
An-other example of a machine-learning method isthat of Carlson et al (2001).2.3 Methods Based on ProbabilityInformationMays et al (1991) proposed a statistical methodusing word-trigram probabilities for detecting andcorrecting real-word errors without requiring pre-defined confusion sets.
In this method, if thetrigram-derived probability of an observed sen-tence is lower than that of a sentence obtained byreplacing one of the words with a spelling varia-1242tion, then we hypothesize that the original is anerror and the variation is what the user intended.Wilcox-O?Hearn et al (2008) analyze the ad-vantages and limitations of Mays et al (1991)?smethod, and present a new evaluation of the al-gorithm, designed so that the results can be com-pared with those of other methods, and then con-struct and evaluate some variations of the algo-rithm that use fixed-length windows.
They con-sider a variation of the method that optimizes overrelatively short, fixed-length windows instead ofover a whole sentence (except in the special casewhen the sentence is smaller than the window),while respecting sentence boundaries as naturalbreakpoints.
To check the spelling of a span ofd words requires a window of length d+4 to ac-commodate all the trigrams that overlap with thewords in the span.
The smallest possible windowis therefore 5 words long, which uses 3 trigramsto optimize only its middle word.
They assumethat the sentence is bracketed by twoBoS and twoEoS markers (to accommodate trigrams involvingthe first two and last two words of the sentence).The window starts with its left-hand edge at thefirst BoS marker, and the Mays et al (1991)?smethod is run on the words covered by the tri-grams that it contains; the window then moves dwords to the right and the process repeats until allthe words in the sentence have been checked.
AsMays et al (1991)?s algorithm is run separately ineach window, potentially changing a word in each,Wilcox-O?Hearn et al (2008)?s method as a side-effect also permits multiple corrections in a singlesentence.Wilcox-O?Hearn et al (2008) show thatthe trigram-based real-word spelling-correctionmethod of Mays et al (1991) is superior in per-formance to the WordNet-based method of Hirstand Budanitsky (2005), even on content words(?malapropisms?
), especially when supplied witha realistically large trigram model.
Wilcox-O?Hearn et al (2008) state that their attempts toimprove the method with smaller windows andwith multiple corrections per sentence were notsuccessful, because of excessive false positives.Verberne (2002) proposed a trigram-basedmethod for real-word errors without explicitly us-ing probabilities or even localizing the possible er-ror to a specific word.
This method simply as-sumes that any word trigram in the text that isattested in the British National Corpus (Burnard,2000) is correct, and any unattested trigram is alikely error.
When an unattested trigram is ob-served, the method then tries the spelling varia-tions of all words in the trigram to find attestedtrigrams to present to the user as possible correc-tions.
The evaluation of this method was carriedout on only 7100 words of the Wall Street Journalcorpus, with 31 errors introduced (i.e., one errorin every approximately 200 words) obtaining a re-call of 0.33 for correction, a precision of 0.05 anda F-measure of 0.086.3 Proposed MethodThe proposed method first tries to determine someprobable candidates and then finds the best oneamong the candidates or sorts them based on someweights.
We consider a string similarity functionand a normalized frequency value function in ourmethod.
The following sections present a detaileddescription of each of these functions followed bythe procedure to determine some probable candi-dates along with the procedure to sort the candi-dates.3.1 Similarity between Two StringsWe use the longest common subsequence (LCS)(Allison and Dix, 1986) measure with some nor-malization and small modifications for our stringsimilarity measure.
We use the same three differ-ent modified versions of LCS that we (Islam andInkpen, 2008) used, along with another modifiedversion of LCS, and then take a weighted sum ofthese2.
Kondrak (2005) showed that edit distanceand the length of the longest common subsequenceare special cases of n-gram distance and similarity,respectively.
Melamed (1999) normalized LCS bydividing the length of the longest common subse-quence by the length of the longer string and calledit longest common subsequence ratio (LCSR).
ButLCSR does not take into account the length of theshorter string which sometimes has a significantimpact on the similarity score.Islam and Inkpen (2008) normalized the longestcommon subsequence so that it takes into accountthe length of both the shorter and the longer stringand called it normalized longest common subse-2We (Islam and Inkpen, 2008) use modified versions be-cause in our experiments we obtained better results (precisionand recall) for schema matching on a sample of data thanwhen using the original LCS, or other string similarity mea-sures.1243quence (NLCS) which is:v1= NLCS(si, sj) =len(LCS(si, sj))2len(si)?
len(sj)(1)While in classical LCS, the common subse-quence needs not be consecutive, in spelling cor-rection, a consecutive common subsequence is im-portant for a high degree of matching.
We (Is-lam and Inkpen, 2008) used maximal consecutivelongest common subsequence starting at charac-ter 1, MCLCS1and maximal consecutive longestcommon subsequence starting at any character n,MCLCSn.
MCLCS1takes two strings as inputand returns the shorter string or maximal consec-utive portions of the shorter string that consecu-tively match with the longer string, where match-ing must be from first character (character 1) forboth strings.
MCLCSntakes two strings as in-put and returns the shorter string or maximal con-secutive portions of the shorter string that con-secutively match with the longer string, wherematching may start from any character (char-acter n) for both of the strings.
We normal-ized MCLCS1and MCLCSnand called it nor-malized MCLCS1(NMCLCS1) and normalizedMCLCSn(NMCLCSn), respectively.v2=NMCLCS1(si, sj) =len(MCLCS1(si, sj))2len(si)?
len(sj)(2)v3=NMCLCSn(si, sj) =len(MCLCSn(si, sj))2len(si)?
len(sj)(3)Islam and Inkpen (2008) did not consider consecu-tive common subsequences ending at the last char-acter, though MCLCSnsometimes covers this,but not always.
We argue that the consecutivecommon subsequence ending at the last characteris as significant as the consecutive common sub-sequence starting at the first character.
So, weintroduce the maximal consecutive longest com-mon subsequence ending at the last character,MCLCSz(Algorithm 1).
Algorithm 1, takes twostrings as input and returns the shorter string or themaximal consecutive portions of the shorter stringthat consecutively matches with the longer string,where matching must end at the last character forboth strings.
We normalize MCLCSzand call itnormalized MCLCSz(NMCLCSz).v4=NMCLCSz(si, sj) =len(MCLCSz(si, sj))2len(si)?
len(sj)(4)We take the weighted sum of these individualvalues v1, v2, v3, and v4to determine string simi-larity score, where ?1, ?2, ?3, ?4are weights and?1+?2+?3+?4= 1.
Therefore, the similarityof the two strings, S ?
[0, 1] is:S(si, sj) = ?1v1+ ?2v2+ ?3v3+ ?4v4(5)We heuristically set equal weights for our ex-periments3.
Theoretically, v3?
v2and v3?
v4.To give an example, consider si= albastru andsj= alabasteru, thenLCS(si, sj) = albastruMCLCS1(si, sj) = alMCLCSn(si, sj) = bastMCLCSz(si, sj) = ruNLCS(si, sj) = 82/(8?
10) = 0.8NMCLCS1(si, sj) = 22/(8?
10) = 0.05NMCLCSn(si, sj) = 42/(8?
10) = 0.2NMCLCSz(si, sj) = 22/(8?
10) = 0.05The string similarity, S = ?1v1+?2v2+?3v3+?4v4= 0.25?
0.8 + 0.25?
0.05 + 0.25?
0.2 +0.25?
0.05 = 0.2753.2 Normalized Frequency ValueWe determine the normalized frequency value ofeach candidate word for a single position with re-spect to all other candidates for the same position.If we find n replacements of a word wiwhich are{wi1, wi2, ?
?
?
, wij, ?
?
?
, win}, and their frequen-cies {fi1, fi2, ?
?
?
, fij, ?
?
?
, fin}, where fijis thefrequency of a 3-gram (where any candidate wordwijis a member of the 3-gram), then we determinethe normalized frequency value of any candidateword wij, represented as F (wij) ?
(0, 1], as thefrequency of the 3-gram havingwijover the maxi-mum frequency among all the candidate words forthat position:F (wij) =fijmax(fi1, fi2, ?
?
?
, fij, ?
?
?
, fin)(6)3.3 Determining Candidate WordsOur task is to correct real-word spelling errorfrom an input text using Google Web 1T 3-gramdata set.
Let us consider an input text W which3We use equal weights in several places in this paper inorder to keep the system unsupervised.
If development datawould be available, we could adjust the weights.1244Algorithm 1: MCLCSz( Maximal Consec-utive LCS ending at the last character)input : si, sj/*siand sjare inputstrings where |si| ?
|sj|*/output: str /*str is the MaximalConsecutive LCS ending atthe last character*/str ?NULL1c?
12while |si| ?
c do3x?
SubStr(si,?c, 1) /*returns4cth character of sifrom theend*/y ?
SubStr(sj,?c, 1) /*returns5cth character of sjfrom theend*/if x = y then6str ?
SubStr(si,?c, c)7else8return str9end10increment c11end12after tokenization4has m words, i.e., W ={w1, w2, .
.
.
, wm}.
Our method aims to correctm-1 spelling errors, for all m-1 word positions,except for the first word position, as we do not tryto correct the first word.
We use a slight differ-ent way to correct the first word (i.e., w2) and thelast word (i.e., wm) among those m-1 words, thanfor the rest of the words.
First, we discuss howwe find the candidates for a word (say wi, where2<i<m) which is not either w2or wm.
Then, wediscuss the procedure to find the candidates for ei-ther w2or wm.
Our method could have workedfor the first word too.
We did not do it here due4We need to tokenize the input sentence to make the 3-grams formed using the tokens returned after the tokeniza-tion consistent with the Google 3-grams.
The input sentenceis tokenized in a manner similar to the tokenization of theWall Street Journal portion of the Penn Treebank.
Notableexceptions include the following:- Hyphenated word are usually separated, and hyphen-ated numbers usually form one token.- Sequences of numbers separated by slashes (e.g., indates) form one token.- Sequences that look like urls or email addresses formone token.to efficiency reasons.
Google 3-grams are sortedbased on the first word, then the second word, andso on.
Based on this sorting, all Google 3-gramsare stored in 97 different files.
All the 97 Google3-gram files could have been needed to access asingle word, instead of accessing just one 3-gramfile as we do for any other words.
This is becausewhen the first word needs to be corrected, it mightbe in any file among those 97 Google 3-gram files.No error appears in the first position among 1402inserted malapropisms.
The errors start appearingfrom the second position till the last position.3.3.1 Determining Candidate Words for wi(2 < i < m)We use the following steps:1.
We define the term cut off frequency for wordwior word wi+1as the frequency of the 3-gram wi?1wiwi+1in the Google Web 1T 3-grams, if the said 3-gram exists.
Otherwise,we set the cut off frequency of wias 0.
Theintuition behind using the cut off frequencyis the fact that, if the word is misspelled,then the correct one should have a higher fre-quency than the misspelled one.
Thus, usingthe cut off frequency, we isolate a large num-ber of candidates that we do not need to pro-cess.2.
We find all the 3-grams (where only wiis changed while wi?1and wi+1are un-changed) having frequency greater than thecut off frequency of wi(determined instep 1).
Let us consider that we findn replacements of wiwhich are R1={wi1, wi2, ?
?
?
, win} and their frequenciesF1= {fi1, fi2, ?
?
?
, fin} where fijis the fre-quency of the 3-gram wi?1wijwi+1.3.
We determine the cut off frequency for wordwi?1or word wias the frequency of the 3-gram wi?2wi?1wiin the Google Web 1T 3-grams, if the said 3-gram exists.
Otherwise,we set the cut off frequency of wias 0.4.
We find all the 3-grams (where only wiis changed while wi?2and wi?1are un-changed) having frequency greater than thecut off frequency of wi(determined instep 3).
Let us consider that we findn replacements of wiwhich are R2={wi1, wi2, ?
?
?
, win} and their frequencies1245F2= {fi1, fi2, ?
?
?
, fin} where fijis the fre-quency of the 3-gram wi?2wi?1wij.5.
For each wij?
R1, we calculate the stringsimilarity between wijand wiusing equation(5) and then assign a weight using the follow-ing equation (7) only to the words that returnthe string similarity value greater than 0.5.weight = ?S(wi, wij)+(1??
)F (wij) (7)Equation (7) is used to ensure a balancedweight between the string similarity functionand the normalized frequency value functionwhere ?
refers to how much importance wegive to the string similarity function with re-spect to the normalized frequency value func-tion5.6.
For each wij?
R2, we calculate the stringsimilarity between wijand wiusing equa-tion (5), and then assign a weight using theequation (7) only to the words that return thestring similarity value greater than 0.5.7.
We sort the words found in step 5 and in step6 that were given weights, if any, in descend-ing order by the assigned weights and keeponly one word as candidate word6.3.3.2 Determining Candidate Words for w2We use the following steps:1.
We determine the cut off frequency for wordw2as the frequency of the 3-gram w1w2w3in the Google Web 1T 3-grams, if the said3-gram exists.
Otherwise, we set the cut offfrequency of w2as 0.2.
We find all the 3-grams (where only w2ischanged while w1and w3are unchanged)having frequency greater than the cut off fre-quency of w2(determined in step 1).
Let usconsider that we find n replacements of w2which are R1= {w21, w22, ?
?
?
, w2n}, andtheir frequencies F1= {f21, f22, ?
?
?
, f2n},5We give more importance to string similarity functionwith respect to frequency value function throughout the sec-tion of ?determining candidate words?
to have more candidatewords so that the chance of including the target word into theset of candidate words gets higher.
For this reason, we heuris-tically set ?=0.85 in equation (7) instead of setting ?=0.5.6Sometimes the top candidate word might be either a plu-ral form or a past participle form of the original word.
Oreven it might be a high frequency function word (e.g., the).We omit these type of words from the candidacy.where f2jis the frequency of the 3-gram w1w2jw3.3.
For each w2j?
R1, we calculate the stringsimilarity between w2jand w2using equa-tion (5), and then assign a weight using thefollowing equation only to the words that re-turn the string similarity value greater than0.5.weight = ?S(w2, w2j) + (1?
?
)F (w2j)4.
We sort the words found in step 3 that weregiven weights, if any, in descending order bythe assigned weights and keep only one wordas candidate word.3.3.3 Determining Candidate Words for wmWe use the following steps:1.
We determine the cut off frequency for wordwmas the frequency of the 3-gram wm?2wm?1wmin the Google Web 1T 3-grams,if the said 3-gram exists.
Otherwise, we setthe cut off frequency of wmas 0.2.
We find all the 3-grams (where only wmis changed while wm?2and wm?1are un-changed) having frequency greater than thecut off frequency of wm(determined instep 1).
Let us consider that we findn replacements of wmwhich are R2={wm1, wm2, ?
?
?
, wmn} and their frequenciesF2= {fm1, fm2, ?
?
?
, fmn}, where fmjisthe frequency of the 3-gram wm?2wm?1wmj.3.
For each wmj?
R2, we calculate the stringsimilarity between wmjand wmusing equa-tion (5) and then assign a weight using thefollowing equation only to the words that re-turn the string similarity value greater than0.5.weight = ?S(wm, wmj) + (1?
?
)F (wmj)4.
We sort the words found in step 3 that weregiven weights, if any, in descending order bythe assigned weights and keep only one wordas the candidate word.12464 Evaluation and Experimental ResultsWe used as test data the same data that Wilcox-O?Hearn et al (2008) used in their evaluation ofMays et al (1991) method, which in turn was areplication of the data used by Hirst and St-Onge(1998) and Hirst and Budanitsky (2005) to evalu-ate their methods.The data consisted of 500 articles (approxi-mately 300,000 words) from the 1987?89 WallStreet Journal corpus, with all headings, identi-fiers, and so on removed; that is, just a long streamof text.
It is assumed that this data contains no er-rors; that is, the Wall Street Journal contains nomalapropisms or other typos.
In fact, a few typos(both non-word and real-word) were noticed dur-ing the evaluation, but they were small in numbercompared to the size of the text.Malapropisms were randomly induced into thistext at a frequency of approximately one word in200.
Specifically, any word whose base form waslisted as a noun in WordNet (but regardless ofwhether it was used as a noun in the text; there wasno syntactic analysis) was potentially replaced byany spelling variation found in the lexicon of theispell spelling checker7.
A spelling variation wasdefined as any word with an edit distance of 1 fromthe original word; that is, any single-character in-sertion, deletion, or substitution, or the transposi-tion of two characters, that results in another realword.
Thus, none of the induced malapropismswere derived from closed-class words, and nonewere formed by the insertion or deletion of anapostrophe or by splitting a word.
The data con-tained 1402 inserted malapropisms.Because it had earlier been used for evaluat-ing Mays et al (1991)?s trigram method, whichoperates at the sentence level, the data set hadbeen divided into three parts, without regardfor article boundaries or text coherence: sen-tences into which no malapropism had been in-duced; the original versions of the sentencesthat received malapropisms; and the malapropizedsentences.
In addition, all instances of num-bers of various kinds had been replaced by tagssuch as <INTEGER>, <DOLLAR VALUE>,7Ispell is a fast screen-oriented spelling checker thatshows you your errors in the context of the original file, andsuggests possible corrections when it can figure them out.The original was written in PDP-10 assembly in 1971, byR.
E. Gorin.
The C version was written by Pace Willissonof MIT.
Geoff Kuenning added the international support andcreated the current release.and <PERCENTAGE VALUE>.
Actual (ran-dom) numbers or values were restored for thesetags.
Some spacing anomalies around punctuationmarks were corrected.
A detailed description ofthis data can be found in (Hirst, 2008; Wilcox-O?Hearn et al, 2008).SUCCESSFUL CORRECTION:The Iran revelations were particularly disturbingto the Europeans because they came on the heelsof the Reykjavik summit between President Rea-gan and Soviet reader ?
leader [leader] MikhailGorbachev.Even the now sainted Abraham Lincoln was of-ten reviled while in officer?
office [office], some-times painted by cartoonists and editorial writersas that baboon in the White House.FALSE POSITIVE:?
?
?
by such public displays of interest in Latinos?
Latin [Latinos], many undocumented ?
?
?The southeast Asian nation was one reportedcontributor ?
contribution [contributor] to theNicaraguans.FALSE NEGATIVE:Kevin Mack, Geldermann president and chiefexecutive officer, didn?t return calls for commenton the Clayton purchaser [purchase].U.S.
manufactures [manufacturers], in short,again are confronting a ball game in which theywill be able to play.TRUE POSITIVE DETECTION, FALSE POSI-TIVE CORRECTION:Hawkeye also is known to rear ?
reader [fear]that a bankruptcy-law filing by the parent com-pany, which theoretically shouldn?t affect the op-erations of its member banks, would spark runs onthe banks that could drag down the whole entity.The London Daily News has quoted sourcessaying as many as 23 British mercenaries were en-listed by KMS to lid?
slide [aid] the Contras.Table 2: Examples of successful and unsuccessfulcorrections.
Italics indicate observed word, arrowindicates correction, square brackets indicate in-tended word.Some examples of successful and unsuccessfulcorrections using our proposed method are shownin Table 2.Table 3 shows our method?s results on the de-scribed data set compared with the results for thetrigram method of Wilcox-O?Hearn et al (2008)1247Detection correctionR P F1R P F1Lexical cohesion(Hirst and Budanitsky, 2005)0.306 0.225 0.260 0.281 0.207 0.238Trigrams(Wilcox-O?Hearn et al, 2008)0.544 0.528 0.536 0.491 0.503 0.497Multiple 3-grams0.890 0.445 0.593 0.763 0.381 0.508Table 3: A comparison of recall, precision, and F1score for three methods of malapropism detectionand correction on the same data set.and the lexical cohesion method of Hirst and Bu-danitsky (2005).
The data shown here for tri-gram method are not from (Wilcox-O?Hearn et al,2008), but rather are later results following somecorrections reported in (Hirst, 2008).
We have nottried optimizing our adjustable parameters: ?
and?s, because the whole data set was used as test-ing set by the other methods we compare with.
Tokeep the comparison consistent, we did not useany portion of the data set for training purpose.Having optimized parameters could lead to a bet-ter result.
The performance is measured using Re-call (R), Precision (P ) and F1:R =true positivestrue positives + false negativesP =true positivestrue positives + false positivesF1=2PRP +RThe fraction of errors correctly detected is the de-tection recall and the fraction of detections thatare correct is the detection precision.
Again, thefraction of errors correctly amended is the correc-tion recall and the fraction of amendments thatare correct is the correction precision.
To givean example, consider a sentence from the data set:?The Philippine president, in her commencementaddress at the academy, complained that the U.S.was living?
giving [giving] advice instead of theaid ?
said [aid] it pledged.
?, where italics indi-cate the observed word, arrow indicates the correc-tion and the square brackets indicate the intendedword.
The detection recall of this sentence is 1.0and the precision is 0.5.
The correction recall ofthis sentence is 1.0 and the precision is 0.5.
Forboth cases, the F1score is 0.667.We loose some precision because our methodtries to detect and correct errors for all the words(except the first word) in the input sentence, and,as a result, it generates more false positives thanthe other methods.
Even so, we get better F1scores than the other competing methods.
Ac-cepting 8.3 percents extra incorrect detections, weget 34.6 percents extra correct detections of errors,and similarly, accepting 12.2 percents extra incor-rect amendments, we get 27.2 percents extra cor-rect amendments of errors compared with the tri-grams method (Wilcox-O?Hearn et al, 2008)8.5 ConclusionThe Google 3-grams proved to be very useful indetecting real-word errors, and finding the correc-tions.
We did not use the 4-grams and 5-gramsbecause of data sparsity.
When we tried with 5-grams the results were lower than the ones pre-sented in Section 4.
Having sacrificed a bit theprecision score, our proposed method achieves avery good detection recall (0.89) and correctionrecall (0.76).
Our attempts to improve the detec-tion recall or correction recall, while maintainingthe respective precisions as high as possible arehelpful to the human correctors who post-edit theoutput of the real-word spell checker.
If there isno postediting, at least more errors get correctedautomatically.
Our method could also detect andcorrect misspelled words, not only malapropisms,without any modification.
In future work, we planto extend our method to allow for deleted or in-serted words, and to find the corrected strings inthe Google Web 1T n-grams.
In this way wewill be able to correct grammar errors too.
Wealso plan more experiments using the 5-grams, butbacking off to 4-grams and 3-grams when needed.AcknowledgmentsThis work is funded by the Natural Sciences andEngineering Research Council of Canada.
Wewant to thank Professor Graeme Hirst from theDepartment of Computer Science, University ofToronto, for providing the evaluation data set.8We can run our algorithm on subsets of data to check forvariance in the results.
We cannot test statistical significancecompared to the related work (t-test), because we do not havethe system from related work to run it on subsets of the data.1248ReferencesL.
Allison and T.I.
Dix.
1986.
A bit-string longest-common-subsequence algorithm.
Information Pro-cessing Letters, 23:305?310.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram corpus version 1.1.
Technical report, GoogleResearch.Lou Burnard, 2000.
Reference Guide for theBritish National Corpus (World Edition), October.www.natcorp.ox.ac.uk/docs/userManual/urg.pdf.Andrew J. Carlson, Jeffrey Rosen, and Dan Roth.2001.
Scaling up context-sensitive text correction.In Proceedings of the Thirteenth Conference on In-novative Applications of Artificial Intelligence Con-ference, pages 45?50.
AAAI Press.Andrew R. Golding and Dan Roth.
1999.
A winnow-based approach to context-sensitive spelling correc-tion.
Machine Learning, 34(1-3):107?130.Graeme Hirst and Alexander Budanitsky.
2005.
Cor-recting real-word spelling errors by restoring lex-ical cohesion.
Natural Language Engineering,11(1):87?111, March.Graeme Hirst and David St-Onge, 1998.
WordNet: Anelectronic lexical database, chapter Lexical chainsas representations of context for the detection andcorrection of malapropisms, pages 305?332.
TheMIT Press, Cambridge, MA.Graeme Hirst.
2008.
An evaluation of the contextualspelling checker of microsoft office word 2007, Jan-uary.
http://ftp.cs.toronto.edu/pub/gh/Hirst-2008-Word.pdf.Aminul Islam and Diana Inkpen.
2008.
Semantic textsimilarity using corpus-based word similarity andstring similarity.
ACM Transactions on KnowledgeDiscovery from Data, 2(2):1?25.G.
Kondrak.
2005.
N-gram similarity and distance.
InProceedings of the 12h International Conference onString Processing and Information Retrieval, pages115?126, Buenos Aires, Argentina.Karen Kukich.
1992.
Technique for automaticallycorrecting words in text.
ACM Comput.
Surv.,24(4):377?439.Eric Mays, Fred J. Damerau, and Robert L. Mercer.1991.
Context based spelling correction.
Informa-tion Processing and Management, 27(5):517?522.I.
D. Melamed.
1999.
Bitext maps and alignmentvia pattern recognition.
Computational Linguistics,25(1):107?130.G.A.
Miller, R. Beckwith, C. Fellbaum, D. Gross,and K.J.
Miller.
1993.
Introduction to wordnet:An on-line lexical database.
Technical Report 43,Cognitive Science Laboratory, Princeton University,Princeton, NJ.Jennifer Pedler.
2007.
Computer Correction of Real-word Spelling Errors in Dyslexic Text.
Ph.D. thesis,Birkbeck, London University.Suzan Verberne.
2002.
Context-sensitive spell check-ing based on word trigram probabilities.
Master?sthesis, University of Nijmegen, February-August.L.
Amber Wilcox-O?Hearn, Graeme Hirst, and Alexan-der Budanitsky.
2008.
Real-word spelling correc-tion with trigrams: A reconsideration of the mays,damerau, and mercer model.
In Alexander Gel-bukh, editor, Proceedings, 9th International Con-ference on Intelligent Text Processing and Compu-tational Linguistics (CICLing-2008) (Lecture Notesin Computer Science 4919, Springer-Verlag), pages605?616, Haifa, February.1249
