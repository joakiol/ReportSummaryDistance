Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 482?491,Sydney, July 2006. c?2006 Association for Computational LinguisticsShort Text Authorship Attribution via Sequence Kernels,Markov Chains and Author Unmasking: An InvestigationConrad Sanderson and Simon GuenterAustralian National University, Canberra, ACT 0200, AustraliaNational ICT Australia, Locked Bag 8001, ACT 2601, Australiaconrad.sanderson.
@.anu.edu.au, simon.guenter.
@.rsise.anu.edu.auAbstractWe present an investigation of recentlyproposed character and word sequencekernels for the task of authorship attribu-tion based on relatively short texts.
Per-formance is compared with two corre-sponding probabilistic approaches basedon Markov chains.
Several configurationsof the sequence kernels are studied on arelatively large dataset (50 authors), whereeach author covered several topics.
Utilis-ing Moffat smoothing, the two probabilis-tic approaches obtain similar performance,which in turn is comparable to that of char-acter sequence kernels and is better thanthat of word sequence kernels.
The resultsfurther suggest that when using a realisticsetup that takes into account the case oftexts which are not written by any hypoth-esised authors, the amount of training ma-terial has more influence on discriminationperformance than the amount of test mate-rial.
Moreover, we show that the recentlyproposed author unmasking approach isless useful when dealing with short texts.1 IntroductionApplications of authorship attribution include pla-giarism detection (e.g.
college essays), deducingthe writer of inappropriate communications thatwere sent anonymously or under a pseudonym(e.g.
threatening or harassing e-mails), as wellas resolving historical questions of unclear ordisputed authorship.
Specific examples are theFederalist papers (Hanus and Hagenauer, 2005;Mosteller, 1984) and the forensic analysis of theUnabomber manifesto (Foster, 2001).Within the area of automatic author at-tribution, recently it has been shown thatencouraging performance can be achievedvia the use of probabilistic models based onn-grams (Clement and Sharp, 2003) and Markovchains of characters and words (Peng et al, 2004).Diederich et al (2003) showed that Support VectorMachines (SVMs), using the bag-of-words kernel,can obtain promising performance, while in an-other study, SVMs with kernels based on charactercollocations obtained mixed performance (Cor-ney, 2003).
Gamon (2004) utilised SVMs withsyntactic and semantic features to obtain relativelyminor accuracy improvements over the use offunction word frequencies and part-of-speechtrigrams.
Koppel & Schler (2004) proposed aword-level heuristic, resembling recursive featureelimination used for cancer classification (Guyonet al, 2002; Huang and Kecman, 2005), to obtainauthor unmasking curves.
The curves wereprocessed to obtain feature vectors that were inturn classified in a traditional SVM setting.The studies listed above have several limita-tions.
In (Clement and Sharp, 2003), a rudimen-tary probability smoothing technique was used tohandle n-grams which were unseen during thetraining phase.
In the dataset used by (Peng et al,2004) each author tended to stick to one or twotopics, raising the possibility that the discrimina-tion was based on topic rather than by author style.In (Corney, 2003; Gamon, 2004; Peng et al,2004; Koppel and Schler, 2004) the datasets wererather small in terms of the number of authors,indicating the results may not be generalisable.Specifically, in (Corney, 2003) the largest datasetcontains texts from five authors, in (Gamon, 2004)from three, while in (Peng et al, 2004) and (Kop-pel and Schler, 2004) from ten.In (Clement and Sharp, 2003; Gamon, 2004;Peng et al, 2004), the attribution of a given doc-ument was forced to one of the authors from aset of possible authors (i.e.
a closed set identifi-cation setup), thus not taking into account the re-alistic case of having a document which was not482written by any of the authors.
In (Koppel andSchler, 2004), the unmasking method was evalu-ated exclusively on books, raising the question asto whether the method is applicable to consider-ably shorter texts.Lastly, all of the studies used different datasetsand experiment setups, thus making a quantita-tive performance comparison of the different ap-proaches infeasible.Recently, various practical character and wordsequence kernels have been proposed (Canceddaet al, 2003; Leslie et al, 2004; Vishwanathanand Smola, 2003) for the purposes of text andbiological sequence analysis.
This allows kernelbased techniques (such as SVMs) to be used inlieu of traditional probabilistic approaches basedon Markov chains.
In comparison to the latter,SVMs have the advantage of directly optimisingthe discrimination criterion.This paper has four main aims: (i) to evalu-ate the usefulness of sequence kernel based ap-proaches for the task of authorship attribution;(ii) to compare their performance with two prob-abilistic approaches based on Markov chains ofcharacters and words; (iii) to appraise the applica-bility of the author unmasking approach for deal-ing with short texts; and (iv) to address some ofthe limitations of the previous studies.Several configurations of the sequence kernelsare studied.
The evaluations are done on a rela-tively large dataset (50 authors) where each authorcovers several topics.
Rather than using long texts(such as books), in almost all of the experimentsthe amount of training and test material per authoris varied from approx.
300 to 5000 words for bothcases.
Moreover, rather than using a closed setidentification setup, the evaluations are done usinga verification setup.
Here, a given text material isclassified as either having been written by a hy-pothesised author or as not written by that author(i.e.
a two class discrimination task).The paper is structured as follows.
Section 2describes author attribution systems based onMarkov chains of characters and words, followedby a description of the corresponding sequencekernel based approaches in Section 3.
Section 4provides an empirical performance comparison ofthe abovementioned approaches, while in Sec-tion 5 the author unmasking method is appraised.Section 6 concludes the paper by presenting themain findings and suggesting future directions.2 Markov Chain Based ApproachesThe opinion on how likely a given text X was writ-ten by author A, rather than any other author, canbe found by a log likelihood ratio:OA,G (X) = |ez(X)|?1 log [ pA (ez(X)) / pG (ez(X)) ]where z ?
{words, chars}, ez(X) extracts an orderedset of items from X (where the items are eitherwords or characters, indicated by z), |ez(X)|?1 isused as a normalisation for varying number ofitems, while pA(ez(X)) and pG(ez(X)) estimate thelikelihood of the text having been written by au-thor A and a generic author1, G, respectively.Given a threshold t, text X is classified as hav-ing been written by author A when OA,G (X) > t,or as written by someone else when OA,G (X) ?
t.The |ez(X)|?1 normalisation term allows for theuse of a common threshold (i.e.
shared by all au-thors), which facilitates the interpretation of per-formance (e.g.
via the use of the Equal Error Rate(EER) point on a Receiver Operating Characteris-tic (ROC) curve (Ortega-Garcia et al, 2004)).Appropriating a technique originally used inlanguage modelling (Chen and Goodman, 1999),the likelihood of author A having written a partic-ular sequence of items, X =`i1, i2, ?
?
?
, i|X|?, canbe approximated using the joint probability of allpresent m-th order Markov chains:pA (X) ?Y|X|j=(m+1)pA?ij |ij?1j?m?
(1)where ij?1j?m is a shorthand for ij?m ?
?
?
ij?1 and mindicates the length of the history.
Given train-ing material for author A, denoted as XA, the max-imum likelihood (ML) probability estimate for aparticular m-th order Markov chain is:pmlA?ij |ij?1j?m?= C?ijj?m|XA?/ C?ij?1j?m|XA?
(2)where C`ijj?m|XA?is the number of times the se-quence ijj?m occurs in XA.
For chains that havenot been seen during training, elaborate smooth-ing techniques (Chen and Goodman, 1999) areutilised to avoid zero probabilities in Eqn.
(1).The probabilities for the generic author are es-timated from a dataset comprised of texts frommany authors.In this work we utilise interpolated Moffatsmoothing2, where the probability of an m-th or-1A generic author is a composite of a number of authors.2Moffat smoothing is often mistakenly referred to asWitten-Bell smoothing.
Witten & Bell (1991) referred to thistechnique as Method C and cited Moffat (1988).483der chain is a linear interpolation of its ML esti-mate and the smoothed probability estimate of thecorresponding (m-1)-th order chain:pmofA?ij |ij-1j-m?= ?ij-1j-mpmlA?ij |ij-1j-m?+?ij-1j-mpmofA?ij |ij-1j-(m-1)?where ?ij-1j-m = 1 ?
?ij-1j-m , and?ij?1j?m=?
?ij : C`ij?1j?mij |XA?> 0???
?ij : C`ij?1j?mij |XA?> 0??+PijC`ijj?m|XA?Here,?
?ij : C`ij?1j?mij |XA?> 0??
is the number ofunique (m+1)-grams that have the same ij?1j?m his-tory items.
Further elucidation of this method isgiven in (Chen and Goodman, 1999; Witten andBell, 1991).The (m-1)-th order probability will typicallycorrelate with the m-th order probability and hasthe advantage of being estimated from a largernumber of examples (Chen and Goodman, 1999).The 0-th order probability is interpolated with theuniform distribution, given by: punifA = 1/ |VA|,where |VA| is the vocabulary size (Chen and Good-man, 1999).When an m-th order chain has a history (i.e.
theitems ij?1j?m) which hasn?t been observed duringtraining, a back-off to the corresponding reducedorder chain is done3:if C?ij?1j?m|XA?= 0, pmofA?ij |ij?1j?m?= pmofA?ij |ij?1j?
(m?1)?Note that if the 0-th order chain also hasn?t beenobserved during training, we are effectively back-ing off to the uniform distribution.A caveat: the training dataset for an author canbe much smaller (and hence have a smaller vo-cabulary) than the combined training dataset forthe generic author, resulting in punifA > punifG .
Thuswhen a previously unseen chain is encounteredthere is a dangerous bias towards author A, i.e.,pmofA`ij |ij?1j?m?> pmofG`ij |ij?1j?m?.
To avoid this, punifAmust be set equal to punifG .3 Sequence Kernel Based ApproachesKernel based techniques, such as SVMs, allow thecomparison of, and discrimination between, vec-torial as well as non-vectorial objects.
In a binarySVM, the opinion on whether object X belongs toclass -1 or +1 is given by:O+1,?1(X) =X|S|j=1?j yj k(sj , X) + b (3)where k(XA, XB) is a symmetric kernel functionwhich reflects the degree of similarity between3Personal correspondence with the authors of (Chen andGoodman, 1999).objects XA and XB, while S = (sj)|S|j=1 is a setof support objects with corresponding class labels(yj ?
{?1,+1} )|S|j=1 and weights ?
= (?j)|S|j=1.
Thekernel function, b as well as sets S and ?
define ahyperplane which separates the +1 and -1 classes.Given a training dataset, quadratic programmingbased optimisation is used to maximise the separa-tion margin4 (Scho?lkopf and Smola, 2002; Shawe-Taylor and Cristianini, 2004).Recently, kernels for measuring the similarity oftexts based on sequences of characters and wordshave been proposed (Cancedda et al, 2003; Leslieet al, 2004; Vishwanathan and Smola, 2003).
Onekernel belonging to this family is:k(XA, XB) =Xq?Q?wq C(q|XA) C(q|XB) (4)where Q?
represents all possible sequences,in XA and XB, of the symbols in Q.
In turn,Q is a set of possible symbols, which can becharacters, e.g.
Q = { ?a?, ?b?, ?c?, ?
?
?
}, or words,e.g.
Q = {?kangaroo?, ?koala?, ?platypus?, ?
?
?
}.Furthermore, C(q|X) is the number of occurrencesof sequence q in X, and wq is the weight forsequence q.
If the sequences are restricted to haveonly one item, Eqn.
(4) for the case of words isin effect a bag-of-words kernel (Cancedda et al,2003; Shawe-Taylor and Cristianini, 2004).In this work we have utilised weights that weredependent only on the length of each sequence,i.e.
wq = w|q|.
By default w|q| = 0, modified by oneof the following functions:specific length: w|q| = 1, if |q| = ?bounded range: w|q| = 1, if |q| ?
[1, ?
]bounded linear decay: w|q| = 1 + 1?|q|?
, if |q| ?
[1, ?
]bounded linear growth: w|q| = |q| / ?
, if |q| ?
[1, ?
]where ?
indicates a user defined maximum se-quence length.To allow comparison of texts with differentlengths, a normalised version (Scho?lkopf andSmola, 2002; Shawe-Taylor and Cristianini, 2004)of the kernel can be used:bk(XA, XB) = k(XA, XB) /pk(XA, XA) k(XB , XB)with constraints |XA| ?
1 and |XB | ?
1.It has been suggested that SVM discrimina-tion based on character sequence kernels in effectutilises a noisy version of stemming (Canceddaet al, 2003).
As such, word sequence kernelscould be more effective than character sequence4Based on preliminary experiments, the regularisationconstant C, used in SVM training, was set to 100.484kernels, since proper word stems, instead of fullwords, can be explicitly used.
However, it mustbe noted that Eqn.
(4) implicitly maps texts to afeature space which has one dimension for eachof the possible sequences comprised of the sym-bols from Q (Cancedda et al, 2003).
When us-ing words, the number of unique symbols (i.e.
|Q|)can be much greater than when using characters(e.g.
10,000 vs 100); furthermore, for a giventext the number of words is always smaller thanthe number of characters.
For a given sequencelength, these observations indicate that for wordsequence kernels the implicit feature space repre-sentation can have considerably higher dimension-ality and be sparser than for character sequencekernels, which could lead to poorer generalisationof the resulting classifier.4 Evaluation4.1 ?Columnists?
DatasetWe have compiled a dataset that is comprised oftexts from 50 newspaper journalists, with a min-imum of 10,000 words per journalist.
Journalistswere selected based on their coverage of severaltopics; any journalist who covered only one spe-cific area (e.g.
sports or economics) was not in-cluded in the dataset.
Apart from removing alladvertising material and standardising the repre-sentation by converting any unicode characters totheir closest ASCII counterparts, no further edit-ing was performed.
The dataset is available foruse by other researchers by contacting the authors.4.2 SetupThe experiments followed a verification setup,where a given text material was classified as ei-ther having been written by a hypothesised authoror as not written by that author (i.e.
a two classdiscrimination task).
This is distinct from a closedset identification setup, where a text is assigned asbelonging to one author out of a pool of authors.The presentation of an impostor text (a text knownTable 1: Approximate correspondence betweenthe number of characters and number of words.For comparison purposes, this paper has about5900 words.No.
characters 1750 3500 7000 14000 28000No.
words 312 625 1250 2500 5000not to be written by the hypothesised author) willbe referred to as an impostor claim, while the pre-sentation of a true text (a text known to be writtenby the hypothesised author) will be referred to asa true claim.For a given text, one of the following two classi-fication errors can occur: (i) a false positive, wherean impostor text is incorrectly classified as a truetext; (ii) a false negative, where a true text is in-correctly classified as an impostor text.
The er-rors are measured in terms of the false positive rate(FPR) and the false negative rate (FNR).
Follow-ing the approach often used within the biometricsfield, the decision threshold was then adjusted sothat the FPR is equal to the FNR, giving Equal Er-ror Rate (EER) performance (Ortega-Garcia et al,2004; Sanderson et al, 2006).The authors in the database were randomly as-signed into two disjoint sections: (i) 10 back-ground authors; (ii) 40 evaluation authors.
Forthe case of Markov chain approaches, texts fromthe background authors were used to construct thegeneric author model, while for kernel based ap-proaches they were used to represent the negativeclass.
In both cases, text materials each comprisedof approx.
28,000 characters were used, via ran-domly choosing a sufficient number of sentencesfrom the pooled texts.
Table 1 shows a corre-spondence between the number of characters andwords, using the average word length of 5.6 char-acters including a trailing whitespace (found onthe whole dataset).For each author in the evaluation section, theirmaterial was randomly split5 into two continuousparts: training and testing.
The split occurredwithout breaking sentences.
The training materialwas used to construct the author model, while thetest material was used to simulate a true claim aswell as impostor claims against all other authors?models.
Note that if material from the evaluationsection was used for constructing the generic au-thor model, the system would have prior knowl-edge about the writing style of the authors usedfor the impostor claims.For each configuration of an approach (where,for example, the configuration is the order of theMarkov chains), the above procedure was repeatedten times, with the randomised assignments andsplitting being done each time.
The final results5By ?randomly split?
we mean that the location of thetraining and testing parts within the text material is random.485were then obtained in terms of the mean and thecorresponding standard deviation of the ten EERs(the standard deviations are shown as error barsin the result figures).
Based on preliminary ex-periments, stemming was used for word based ap-proaches (Manning and Schu?tze, 1999).4.3 Experiments and DiscussionIn the first experiment we studied the effects ofvarying the order for character and word Markovchain approaches, while the amount of trainingmaterial was fixed at approx.
28,000 charactersand the test material (for evaluation authors) wasdecreased from approx.
28,000 to 1,750 charac-ters.
Results are presented in Fig.
1.The results show that 2nd order chains ofcharacters generally obtain the best performance.However, the difference in performance between1st order and 2nd order chains could be consideredas statistically insignificant due to the large over-lap of the error bars.
The best performing wordchain approach had an order of zero, with higherorders (not shown) having virtually the same per-formance as the 0th order.
Its performance islargely similar to the 2nd order character chain ap-proach, with the latter obtaining a somewhat lowererror rate at 28,000 characters.The second experiment was similar to the first,with the difference being that the amount of train-ing material and test material was decreased fromapprox.
28,000 to 1,750 characters.
The mainchange between the results of this experiment(shown in Fig.
2) and the previous experiment?sresults is the faster degradation in performance asthe number of characters is decreased.
We com-ment on this effect later.In the third experiment we utilised SVMs withcharacter sequence kernels and studied the effectsof chunk size.
As SVMs employ support ob-jects in the definition of the discriminant function(see Section 3), the training material was split intovarying size chunks, ranging from approximately62 to 4000 characters.
Each of the chunks can be-come a support chunk.
Naturally, the smaller thechunk size, the larger the number of chunks.
Asthe split was done without breaking sentences, theeffective chunk size tended to be somewhat larger.If there is less words available than a given chunksize, then all of the remaining words are used forforming a chunk.
Based on preliminary experi-ments, the bounded range weight function with1750 3500 7000 14000 2800051015202530354045NUMBER OF CHARACTERSEERchar: 0 orderchar: 1 orderchar: 2 orderword: 0 orderFigure 1: Performance of character and wordMarkov chain approaches using fixed size trainingmaterial (approx.
28,000 characters) and varyingsize test material.1750 3500 7000 14000 2800051015202530354045NUMBER OF CHARACTERSEERchar: 0 orderchar: 1 orderchar: 2 orderword: 0 orderFigure 2: Performance of character and wordMarkov chain approaches for varying size of train-ing and test material.
At each point the size of thetraining and test materials is equal.
?=3 was used.
The amount of training and testmaterial was equal and three cases were evaluated:28,000, 14,000 and 7,000 characters.
Results, pre-sented in Fig.
3, indicate that the optimum chunksize is approximately 500 characters for the threecases.
Furthermore, the optimum chunk size ap-pears to be independent of the number of availablechunks for training.In the fourth experiment we studied the ef-fects of various weight functions and sequencelengths for the character sequence kernel.
Theamount of training and test material was fixed atapprox.
28,000 characters.
Based on the resultsfrom the previous experiment, chunk size was setat 500.
Results for specific length (Fig.
4) suggestthat most of the reliable discriminatory informa-tion is contained in sequences of length 2.
Theerror rates for the bounded range and bounded lin-48662 125 250 500 1000 2000 40005101520253035APPROXIMATE CHUNK SIZEEERN=28000N=14000N=7000Figure 3: Performance of the character sequencekernel approach for varying chunk sizes.
Boundedrange weight function with ?=3 was used.1 2 3 4 5 6810121416182022SEQUENCE LENGTHEERspecific lengthbounded rangebounded linear decaybounded linear growthFigure 4: Performance of the character sequencekernel approach for various weight functions.
Thesize of training and test materials was fixed at ap-prox.
28,000 characters.
Chunk size of 500 char-acters was used.
Error bars were omitted for clar-ity.ear decay functions are quite similar, with bothreaching minima for sequences of length 4; mostof the improvement occurs when the sequencesreach a length of 3.
This indicates that while se-quences with a specific length of 3 and 4 are lessreliable than sequences with a specific length of2, they contain (partly) complementary informa-tion which is useful when combined with infor-mation from shorter lengths.
Emphasising longerlengths of 5 and 6 (via the bounded linear growthfunction) achieves a minor, but noticeable, perfor-mance degradation.
We conjecture that the degra-dation is caused by the sparsity of relatively longsequences, which affects the generalisation of theclassifier.The fifth experiment was devoted to an evalua-tion of the effects of chunk size for the word se-62 125 250 500 1000 2000 4000 80005101520253035APPROXIMATE CHUNK SIZEEERN=28000N=14000N=7000Figure 5: Performance of the word sequence ker-nel approach for varying chunk sizes.
Specificlength weight function with ?=1 was used.quence approach.
To keep the results comparablewith the character sequence approach (third exper-iment), the training material was split into vary-ing size chunks, ranging from approximately 62to 8000 characters.
Based on the results from thefirst experiment, the specific length weight func-tion with ?=1 was used6 (resulting in a bag-of-words kernel).The amount of training and test material wasequal and three cases were evaluated: 28,000,14,000 and 7,000 characters.
Results, shown inFig.
5, suggest that the optimum chunk size is ap-proximately 4000 characters for the three cases.As mentioned in Section 3, for the word basedapproach the implicit feature space representationcan have considerably higher dimensionality andbe sparser than for the character based approach.Consequently, longer texts would be required toadequately populate the feature space.
This is re-flected by the optimum chunk size for the wordbased approach, which is roughly an order of mag-nitude larger than the optimum chunk size for thecharacter based approach.In the sixth experiment we compared the per-formance of character sequence kernels (usingthe bounded range function with ?=4) and sev-eral configurations of the word sequence kernels.The amount of training material was fixed at ap-prox.
28,000 characters and the test material wasdecreased from approx.
28,000 to 1,750 charac-ters.
Based on the results of previous experi-ments, chunk size was set to 500 for the charac-ter based approach and to 4000 for the word based6Note that for ?=1, all of the weight functions presentedin Section 3 are equivalent.4871750 3500 7000 14000 2800051015202530354045NUMBER OF CHARACTERSEERchar: len=4 (bounded range)word: len=1 (specific length)word: len=2 (specific length)word: len=2 (bounded linear decay)Figure 6: Performance of character and word se-quence kernel approaches using fixed size trainingmaterial (approx.
28,000 characters) and varyingsize test material.approach.
Fig.
6 shows that word sequences witha specific length of 2 lead to considerably worseperformance than sequences of length 1 (i.e.
indi-vidual words).
Furthermore, the best performingcombination of lengths (i.e.
via the bounded lineardecay function7) does not provide better perfor-mance than using individual words.
The charac-ter sequence kernels consistently achieve a lowererror rate than the best performing word sequencekernel.
This suggests that the sparse feature spacerepresentation, described in Section 3, is becom-ing an issue.The final experiment was similar to the sixth,with the difference being that the amount of train-ing material and test material was decreased fromapprox.
28,000 to 1,750 characters.
As observedfor the Markov chain approaches, the main changebetween the results of this experiment (shown inFig.
7) and the previous experiment?s results is thefaster degradation in performance as the numberof characters is decreased.
Along with the resultsfrom experiments 1 and 2, this indicates that theamount of training material has considerably moreinfluence on discrimination performance than theamount of test material.In Fig.
8 it can be observed that the best per-forming Markov chain based approach (charac-ters, 2nd order) obtains comparable performanceto the character sequence kernel based approach(using the bounded range function with ?=4).7Other combinations of lengths were also evaluated,though the results are not shown here.1750 3500 7000 14000 2800051015202530354045NUMBER OF CHARACTERSEERchar: len=4 (bounded range)word: len=1 (specific length)word: len=2 (specific length)word: len=2 (bounded linear decay)Figure 7: Performance of character and word se-quence kernel approaches for varying size of train-ing and test material.
At each point the size of thetraining and test materials is equal.1750 3500 7000 14000 2800051015202530354045NUMBER OF CHARACTERSEER(A) Seq.
kernel (char, len=4, bounded range)(A) Markov chain (char, order=2)(B) Seq.
kernel (char, len=4, bounded range)(B) Markov chain (char, order=2)Figure 8: Comparison between the best sequencekernel approach with the best Markov chain ap-proach for two cases: (A) varying size of trainingand test material, (B) fixed size training material(approx.
28,000 characters) and varying size testmaterial.5 Author Unmasking On Short TextsKoppel & Schler (2004) proposed an alternativemethod for author verification.
Rather than treat-ing the verification problem directly as a two-classdiscrimination task (as done in Section 4), an ?au-thor unmasking?
curve is first built.
A vector rep-resenting the ?essential features?
of the curve isthen classified in a traditional SVM setting.
Theunmasking procedure is reminiscent of the recur-sive feature elimination procedure first proposedin the context of gene selection for cancer classifi-cation (Guyon et al, 2002).Instead of having an author specific model (asin the Markov chain approach) or an author spe-cific SVM, a reference text is used.
The text to be4880 1 2 3 4 5 6 7 8 96065707580859095100ITERATIONACCURACYWILDE vs WILDEWILDE vs COOPERWILDE vs HAWTHORNEWILDE vs MELVILLEWILDE vs SHAWFigure 9: Unmasking of Wilde?s An Ideal Hus-band using Wilde?s Woman of No Importance aswell as the works of other authors as referencetexts.classified as well as the reference text are dividedinto chunks; the features representing each chunkare the counts of pre-selected words.
Each point inthe author unmasking curve is the cross-validationaccuracy of discriminating between the two setsof chunks (using a linear SVM).
At each iteration,several of the most discriminative features are re-moved from further consideration.The underlying hypothesis is that if the twogiven texts have been written by the same author,the differences between them will be reflected ina relatively small number of features.
Koppel &Schler (2004) observed that for texts authored bythe same person, the extent of the cross-validationaccuracy degradation is much larger than for textswritten by different authors.
Encouraging classifi-cation results were obtained for long texts (booksavailable from Project Gutenberg8).In this section we first confirm the unmaskingeffect for long texts and then show that for shortertexts (i.e.
approx.
5000 words), the effect is con-siderably less distinctive.For the first experiment we followed the setupin (Koppel and Schler, 2004), i.e.
the same books,chunks with a size of approximately 500 words,10 fold cross-validation, removing 6 features ateach iteration, and using 250 words with the high-est average frequency in both texts as the set ofpre-selected words.
Fig.
9 shows curves for un-masking Oscar Wilde?s An Ideal Husband usingWilde?s Woman of No Importance (same-authorcurve) as well as the works of other authors asreference texts (different-author curves).
As can8http://www.gutenberg.org0 1 2 3 4 5 6 7 8 9405060708090100ITERATIONACCURACYA vs AA vs BA vs CA vs DA vs EFigure 10: Unmasking of a text from author Afrom the Columnists dataset, using A?s as well asother authors?
reference texts.Table 2: Performance of author unmasking, char-acter sequence kernel approach (?
= 4, boundedrange) and character Markov chain approach (2ndorder).Approach mean EER std.
dev.Author unmasking 30.88 4.32Character sequence kernel 8.08 2.08Character Markov chain 8.14 1.79be observed, the unmasking effect is most pro-nounced for Wilde?s text.
Furthermore, this figurehas a close resemblance to Fig.
2 in (Koppel andSchler, 2004).In the second experiment we used text mate-rials from the Columnists dataset.
Each author?stext material was divided into two sections of ap-proximately 5000 words, with the one of the sec-tions randomly selected to be the reference mate-rial, leaving the other as the test material.
Basedon preliminary experiments, the number of pre-selected words was set to 100 (with the highestaverage frequency in both texts) and the size ofthe chunks was set to 200 words.
The remainderof the unmasking procedure setup was the same asfor the first experiment.
The setup for verificationtrials was similar to the setup in Section 4.2, withthe difference being that the background authorswere used to generate same-author and different-author curves for training the secondary SVM.
Inall cases features from each curve were extracted,as done in (Koppel and Schler, 2004), prior to fur-ther processing.Table 2 provides a comparison between the per-formance of the unmasking approach with thatof the character sequence kernel and character489Markov chain based approaches, as evaluated inSection 4.
Fig.
10 shows representative curves re-sulting from unmasking of the test material fromauthor A, using A?s as well as other authors?
refer-ence materials.
Generally, the unmasking effectfor the same-author curves is considerably lesspronounced and in some cases it is non-existent.More dangerously, different-author curves oftenhave close similarities to same-author curves.
Theresults and the above observations hence suggestthat the unmasking method is less useful whendealing with relatively short texts.6 Main Findings and Future DirectionsIn this paper we investigated the use of charac-ter and word sequence kernels for the task ofauthorship attribution and compared their perfor-mance with two probabilistic approaches based onMarkov chains of characters and words.
The eval-uations were done on a relatively large dataset (50authors), where each author covered several top-ics.
Rather than using the restrictive closed setidentification setup, a verification setup was usedwhich takes into account the realistic case of textswhich are not written by any hypothesised authors.We also appraised the applicability of the recentlyproposed author unmasking approach for dealingwith relatively short texts.In the framework of Support Vector Machines,several configurations of the sequence kernelswere studied, showing that word sequence ker-nels do not achieve better performance than a bag-of-words kernel.
Character sequence kernels (us-ing sequences with a length of 4) generally havebetter performance than the bag-of-words kerneland also have comparable performance to the twoprobabilistic approaches.A possible advantage of character sequence ker-nels over word-based kernels is their inherent abil-ity to do partial matching of words.
Let us con-sider two examples.
(i) Given the words ?negotia-tion?
and ?negotiate?, the character sequence ker-nel can match ?negotiat?, while a standard word-based kernel requires explicit word stemming be-forehand in order to match the two related words(as done in our experiments).
(ii) Given thewords ?negotiation?
and ?desalination?, a charac-ter sequence kernel can match the common ending?ation?.
Particular word endings may be indica-tive of a particular author?s style; such informationwould not be picked up by a standard word-basedkernel.Interestingly, the bag-of-words kernel based ap-proach obtains worse performance than the cor-responding word based Markov chain approach.Apart from the issue of sparse feature space rep-resentation, factors such as the chunk size and thesetting of the C parameter in SVM training canalso affect the generalisation performance.The results also show that the amount of train-ing material has more influence on discrimina-tion performance than the amount of test material;about 5000 training words are required to obtainrelatively good performance when using between1250 and 5000 test words.Further experiments suggest that the author un-masking approach is less useful when dealing withrelatively short texts, due to the unmasking effectbeing considerably less pronounced than for longtexts and also due to different-author unmaskingcurves having close similarities to the same-authorcurves.In future work it would be useful to appraisecomposite kernels (Joachims et al, 2001) in or-der to combine character and word sequence ker-nels.
If the two kernel types use (partly) com-plementary information, better performance couldbe achieved.
Furthermore, more sophisticatedcharacter sequence kernels can be evaluated, suchas mismatch string kernels used in bioinformat-ics, where mutations in the sequences are al-lowed (Leslie et al, 2004).AcknowledgementsThe authors thank the anonymous reviewersas well as Simon Burton, Ari Chanen, ArvinDehghani, Etienne Grossmann, Adam Kowalczykand Silvia Richter for useful suggestions and dis-cussions.National ICT Australia (NICTA) is funded bythe Australian Government?s Backing Australia?sAbility initiative, in part through the AustralianResearch Council.ReferencesN.
Cancedda, E. Gaussier, C. Goutte, and J.-M. Ren-ders.
2003.
Word-sequence kernels.
J. MachineLearning Research, 3:1059?1082.S.F.
Chen and J. Goodman.
1999.
An empiricalstudy of smoothing techniques for language model-ing.
Computer Speech and Language, 13:359?394.490R.
Clement and D. Sharp.
2003.
Ngram and Bayesianclassification of documents for topic and authorship.Literary and Linguistic Computing, 18(4):423?447.M.
W. Corney.
2003.
Analysing e-mail text authorshipfor forensic purposes.
Master?s thesis, QueenslandUniversity of Technology, Australia.J.
Diederich, J. Kindermann, E. Leopold, and G. Paass.2003.
Authorship attribution with support vectormachines.
Applied Intelligence, 19(1-2):109?123.D.
W. Foster.
2001.
Author Unknown: On the Trail ofAnonymous.
Henry Holt & Company, 2nd ed.M.
Gamon.
2004.
Linguistic correlates of style: au-thorship classification with deep linguistic analysisfeatures.
In Proc.
20th Int.
Conf.
ComputationalLinguistics (COLING), pages 611?617, Geneva.I.
Guyon, J. Weston, S. Barnhill, and V. Vapnik.
2002.Gene selection for cancer classification using sup-port vector machines.
Machine Learning, 46:389?422.P.
Hanus and J. Hagenauer.
2005.
Information theoryhelps historians.
IEEE Information Theory SocietyNewsletter, 55(September):8.T.-M. Huang and V. Kecman.
2005.
Gene extractionfor cancer diagnosis by support vector machines ?
animprovement and comparison with nearest shrunkencentroid method.
In Proc.
15th Int.
Conf.
ArtificialNeural Networks (ICANN), pages 617?624, Warsaw.T.
Joachims, N. Cristianini, and J. Shawe-Taylor.
2001.Composite kernels for hypertext categorisation.
InProc.
18th Int.
Conf.
Machine Learning (ICML),pages 250?257, Massachusetts.M.
Koppel and J. Schler.
2004.
Authorship verifica-tion as a one-class classification problem.
In Proc.21st Int.
Conf.
Machine Learning (ICML), Banff,Canada.C.
Leslie, E. Eskin, A. Cohen, J. Weston, andW.
Noble.2004.
Mismatch string kernels for discriminativeprotein classification.
Bioinformatics, 20(4):467?476.C.D Manning and H. Schu?tze.
1999.
Foundationsof Statistical Natural Language Processing.
MITPress.A.
Moffat.
1988.
A note on the PPM data compressionalgorithm, Res.
Report.
88/7, Dept.
Comput.
Sci.,University of Melbourne, Australia.F.
Mosteller.
1984.
Applied Bayesian and Classi-cal Inference: The Case of the Federalist Papers.Springer, 2nd edition.J.
Ortega-Garcia, J. Bigun, D. Reynolds, andJ.
Gonzalez-Rodriguez.
2004.
Authentication getspersonal with biometrics.
IEEE Signal ProcessingMagazine, 21(2):50?62.F.
Peng, D. Schuurmans, and S. Wang.
2004.
Aug-menting naive Bayes classifiers with statistical lan-guage models.
Information Retrieval, 7:317?345.C.
Sanderson, S. Bengio, and Y. Gao.
2006.
On trans-forming statistical models for non-frontal face veri-fication.
Pattern Recognition, 39(2):288?302.B.
Scho?lkopf and A. Smola.
2002.
Learning with Ker-nels: Support Vector Machines, Regularization, Op-timization and Beyond.
The MIT Press, USA.J.
Shawe-Taylor and N. Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge Univer-sity Press, UK.S.V.N.
Vishwanathan and A. Smola.
2003.
Fast ker-nels for string and tree matching.
In Advances inNeural Information Processing Systems (NIPS) 15,pages 569?576, Cambridge.
MIT Press.I.H.
Witten and T.C.
Bell.
1991.
The zero-frequencyproblem: Estimating the probabilities of novelevents in adaptive text compression.
IEEE Trans.Information Theory, 37(4):1085?1094.491
