Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 73?78,Baltimore, Maryland, USA.
June 27, 2014.c?2014 Association for Computational LinguisticsLinguistically Informed Tweet Categorization for Online ReputationManagementGerard Lynch and P?adraig CunninghamCentre for Applied Data Analytics Research(CeADAR)University College DublinBelfield Office ParkDublin 4, Irelandfirstname.lastname@ucd.ieAbstractDetermining relevant content automati-cally is a challenging task for any ag-gregation system.
In the business intel-ligence domain, particularly in the appli-cation area of Online Reputation Manage-ment, it may be desirable to label tweetsas either customer comments which de-serve rapid attention or tweets from in-dustry experts or sources regarding thehigher-level operations of a particular en-tity.
We present an approach using a com-bination of linguistic and Twitter-specificfeatures to represent tweets and examinethe efficacy of these in distinguishing be-tween tweets which have been labelledusing Amazon?s Mechanical Turk crowd-sourcing platform.
Features such as part-of-speech tags and function words provehighly effective at discriminating betweenthe two categories of tweet related to sev-eral distinct entity types, with Twitter-related metrics such as the presence ofhashtags, retweets and user mentions alsoadding to classification accuracy.
Accu-racy of 86% is reported using an SVMclassifier and a mixed set of the aforemen-tioned features on a corpus of tweets re-lated to seven business entities.1 MotivationOnline Reputation Management (ORM) is a grow-ing field of interest in the domain of business in-telligence.
Companies and individuals alike arehighly interested in monitoring the opinions ofothers across social and traditional media and thisinformation can have considerable business valuefor corporate entities in particular.1.1 ChallengesThere are a number of challenges in creating anend-to-end software solution for such purposes,and several shared tasks have already been estab-lished to tackle these issues1.
The most recentRepLab evaluation was concerned with four tasksrelated to ORM, filtering, polarity for reputation,topic detection and priority assignment.
Basedon these evaluations, it is clear that although thestate of the art of topic-based filtering of tweets isrelatively accomplished (Perez-Tellez et al., 2011;Yerva et al., 2011; Spina et al., 2013), other as-pects of the task such as sentiment analysis andprioritisation of tweets based on content are lesstrivial and require further analysis.Whether Twitter mentions of entities are ac-tual customer comments or in fact represent theviews of traditional media or industry experts andsources is an important distinction for ORM sys-tems.
With this study we investigate the degree towhich this task can be automated using supervisedlearning methods.2 Related Work2.1 Studies on Twitter dataWhile the majority of research in the computa-tional sciences on Twitter data has focused on is-sues such as topic detection (Cataldi et al., 2010),event detection, (Weng and Lee, 2011; Sakakiet al., 2010), sentiment analysis, (Kouloumpis etal., 2011), and other tasks based primarily on thetopical and/or semantic content of tweets, thereis a growing body of work which investigatesmore subtle forms of information represented intweets, such as reputation and trustworthiness,(O?Donovan et al., 2012), authorship attribution(Layton et al., 2010; Bhargava et al., 2013) andTwitter spam detection, (Benevenuto et al., 2010).1See (Amig?o et al., 2012) and (Amig?o et al., 2013) fordetails of the RepLab series73These studies combine Twitter-specific and textualfeatures such as retweet counts, tweet lengths andhashtag frequency, together with sentence-length,character n-grams and punctuation counts.2.2 Studies on non-Twitter dataThe textual features used in our work suchas n-grams of words and parts-of-speech havebeen used for gender-based language classifica-tion (Koppel et al., 2002), social profiling and per-sonality type detection (Mairesse et al., 2007), na-tive language detection from L2 text, (Brooke andHirst, 2012) translation source language detection,(van Halteren, 2008; Lynch and Vogel, 2012) andtranslation quality detection, (Vogel et al., 2013).3 Experimental setup and corpusTweets were gathered between June 2013 and Jan-uary 2014 using the twitter4j Java library.
A lan-guage detector was used to filter only English-language tweets.2The criteria for inclusion werethat the entity name was present in the tweet.
Theentities focused on in this study had relatively un-ambigious business names, so no complex filteringwas necessary.3.1 Pilot studyA smaller pilot study was carried out before themain study in order to examine response qualityand accuracy of instruction.
Two hundred sam-ple tweets concerning two airlines3were anno-tated using Amazon?s Mechanical Turk system byfourteen Master annotators.
After annotation, weselected the subset (72%) of tweets for which bothannotators agreed on the category to train the clas-sifier.
During the pilot study, the tweets werepre-processed4to remove @ and # symbols andpunctuation to treat account names and hashtagsas words.
Hyperlinks representations were main-tained within the tweets.
The Twitter-specific met-rics were not employed in the pilot study.3.2 Full studyIn the full study, 2454 tweets concerning sevenbusiness entities5were tagged by forty annota-tors as to whether they corresponded to one of the2A small amount of non-English tweets were found in thedataset, these were assigned to the Other category.3Aer Lingus and Ryanair4This was not done in the full study, these symbols werecounted and used as features.5Aer Lingus, Ryanair, Bank of Ireland, C & C Group,Permanent TSB, Glanbia, Greencorethree categories described in Section 1.1.
For 57%of the tweets, annotators agreed on the categorieswith disagreement in the remaining 43%.
The dis-puted tweets were annotated again by two anno-tators.
From this batch, a similar proportion wereagreed on.
For the non-agreed tweets in the sec-ond round, a majority category vote was reachedby combining the four annotations over the firstand second rounds.
After this process, roughlytwo hundred tweets remained as ambiguous (eachhaving two annotations for one of two particularcategories) and these were removed from the cor-pus used in the experiments.3.3 Category breakdownTable 5 displays the number of tweets for whichno majority category agreement was reached.
Themajority disagreement class across all entities aretexts which have been labelled as both businessoperations and other.
For the airline entities, alarge proportion of tweets were annotated as bothcustomer comment and other, this appeared to bea categorical issue which may have required clar-ification in the instructions.
The smallest cate-gory for tied agreement is customer comment andbusiness operations, it appears that the distinc-tion between these categories was clearer basedon the data provided to annotators.
2078 tweetswere used in the final experiments.
The classeswere somewhat imbalanced for the final corpus,the business operations category was the largest,with 1184 examples, customer comments con-tained 585 examples and the other category con-tained 309 examples.3.4 Feature typesThe features used for classification purposes canbe divided into the following two categories:1.
Twitter-specific:?
Tweet is a retweet or not?
Tweet contains a mention?
Tweet contains a hashtag or a link?
Weight measure (See Fig 3)?
Retweet account for a tweet.2.
Linguistic: The linguistic features are basedon the textual content of the tweet repre-sented as word unigrams, word bigrams andpart-of-speech bigrams.74We used TagHelperTools, (Ros?e et al., 2008) fortextual feature creation which utilises the StanfordNLP toolkit for NLP annotation and returns for-matted representations of textual features whichcan be employed in the Weka toolkit which imple-ments various machine learning algorithms.
Alllinguistic feature frequencies were binarised in ourrepresentations6.4 Results4.1 Pilot studyUsing the Naive Bayes classifier in the Wekatoolkit and a feature set consisting of 130 wordtokens, 80% classification accuracy was obtainedusing ten-fold cross validation on the full set oftweets .
Table 1 shows the top word features whenranked using 10-fold cross validation and the in-formation gain metric for classification power overthe three classes.
Using the top 50 ranked POS-bigram features alone, 74% classification accuracywas obtained using the Naive Bayes classifier.
Ta-ble 2 shows the top twenty features, again rankedby information gain.Combining the fifty POS-bigrams and the 130word features, we obtained 84% classification ac-curacy using the Naive Bayes classifier.
Accuracywas improved by removing all noun features fromthe dataset and using the top seventy five featuresfrom the remaining set ranked with informationgain, resulting in 86.6% accuracy using the SVMclassifier with a linear kernel.
Table 3 displays thetop twenty combined features.Rank Feature Rank Feature1 http 11 investors2 flight 12 would3 talks 13 by4 for 14 says5 strike 15 profit6 an 16 cabin7 you 17 crew8 I 18 via9 that 19 at10 action 20 sinceTable 1: Top 20 ranked word features for pilotstudy61 if feature is present in a tweet, otherwise 0.Rank Feature Rank Feature1 NNP EOL 11 VB PRP2 VBD JJ 12 NN NNS3 NNP VBD 13 IN PRP$4 NNP NN 14 BOL CD5 BOL PRP 15 BOL JJS6 VBD NNP 16 IN VBN7 NNP CC 17 PRP$ JJ8 TO NNP 18 PRP MD9 NN RB 19 PRP$ VBG10 RB JJ 20 CC VBPTable 2: Top 20 ranked POS bigram features forpilot studyRank Feature Rank Feature1 http 11 TO NNP2 NNP EOL 12 RB JJ3 NNP VBD 13 that4 VBD JJ 14 tells5 NNP NN 15 way6 BOL PRP 16 I7 VBD NNP 17 would8 NNP CC 18 you9 for 19 NN RB10 an 20 BOL JJSTable 3: Top 20 ranked combined features for pilotstudy4.2 Full study4.2.1 ResultsUsing the SMO classifier, Weka?s support vec-tor machine implementation using a linear kernel,a hybrid feature set containing linguistic, customand Twitter-specific features obtained 72% clas-sification accuracy for the three categories.
F-measures were highest for the business operationsclass, and lowest for the other class, which con-tained the most diversity.
Examining Figure 2, itis clear that f-measures for the other class are al-most zero.
This indicates that tweets given thiscategory may not be homogeneous enough to cat-egorise using the features defined in Table 7.4.3 Two classesAfter the removal of the other class from theexperiment, the same feature set obtained 86%classification accuracy between the two remain-ing classes.
The distinguishing features consistedpredominantly of pronouns (I, me, my), part-of-75Entity BO CC OtherAer Lingus 174 138 44Ryanair 58 212 52AIB 69 29 43BOI 208 85 40C&C 45 14 15Glanbia 276 39 46Greencore 37 4 13Kerry Group 158 10 36Permanent TSB 160 54 20Table 4: Tweets per entity by category: MajorityagreementEntity CC+BO O-CC O-BOAer Lingus 4 24 15Ryanair 7 30 8AIB 4 5 11BOI 9 5 16C&C 0 1 3Glanbia 7 4 19Greencore 0 0 2Kerry Group 5 2 12Permanent TSB 3 6 10Table 5: Tweets per entity by category: Tiedagreementspeech bigrams including pairs of plural nouns,lines beginning with prepositions and functionwords (so, just, new, it).
Business operationstweets were more likely to mention a user accountor be a retweet, personal pronouns were morecommonplace in customer comments and as ob-served in the pilot study, customer comments weremore likely to begin with a preposition and busi-ness operations tweets were more likely to containnoun-noun compounds and pairs of coordinatingconjunctions and nouns.4.4 FeaturesHashtags were slightly more common in businessoperations tweets, however the number of hash-tags was not counted, simply whether at least onewas present.
Hashtags as a proportion of wordsmight be a useful feature for further studies.
Func-tion words and POS tags were highly discrimina-tory, indicating that this classifier may be applica-ble to different topic areas.
Weight (See Figure 3)was a distinguishing feature, with business opera-tions tweets having higher weight scores, reflect-Figure 1: F-scores by category for pilot studyFigure 2: F-scores by category for full studying the tendency for these tweets to originate fromTwitter accounts linked to news sources or influ-ential industry experts.5 Results per sub-categoryTo investigate whether the entity domain had abearing on the results, we separated the data intothree subsets, airlines, banks and food industryconcerns.
We performed the same feature selec-tion as in previous experiments, calculating eachfeature type separately, removing proper nouns,hashtags and account names from the word n-grams, then combining and ranking the featuresusing ten-fold cross validation and informationgain.
The SVM classifier reported similar resultsto the main study on the three class problem foreach sub-domain, and for the two class problemresults ranged between 86-87% accuracy, similarNumber of followersNumber following(retweets)Figure 3: Twitter weight metric76to the results on the mixed set7.
Thus, we be-lieve that the individual subdomains do not war-rant different classifiers for the problem, indeedexamining the top 20-ranked features for each sub-domain, there is a large degree of overlap, as seenin bold and italics in Table 6.Banks Airlines Food@ @ @my NNP NNP PRP VBPi i ime BOL IN BOL INPRP VBP PRP VBP VB PRPaccount DT NN BOL PRPNNP VBZ IN PRP HASHASHVB PRP the youIN PRP new meyou PRP VBD knowBOL RB NNP VBZ myRB JJ IN DT i knowNNP NNP you PRP CCPRP VBD BOL PRP usedmy bank ISRT BOL CCDT NN it NNP CDNN PRP me NN NNPVBD PRP my CC PRPBOL IN RB RB ISRTi?m so CC NNPTable 6: Top twenty ranked features by Informa-tion Gain for three domains6 Conclusions and future directions6.1 Classification resultsWe found that accurate categorization of our pre-defined tweet types was possible using shallowlinguistic features.
This was aided by Twitter spe-cific metrics but these did not add significantly tothe classification accuracy8.
The lower score (72-73%) in the three class categorization problem isdue to the linguistic diversity of the other tweetcategory.6.2 Annotation and Mechanical TurkWe found the definition of categorization criteriato be an important and challenging step when us-ing Mechanical Turk for annotation.
The high de-gree of annotator disagreement reflected this, how-ever it is important to note that in many cases,tweets fit equally into two or more of our definedcategories.
The use of extra annotations9allowedfor agreement to be reached in the majority of7The food subset was highly imbalanced however, con-taining only 43 customer comments and 313 business opera-tions tweets, the other two subsets were relatively balanced.8ca.
2% decrease in accuracy on removal.9over the initial two annotatorscases, however employing more evaluations couldhave also resulted in deadlock.
Examples of am-biguous tweets included: Cheap marketing tactics.Well, if it ain?t broke, why fix it!
RT @Ryanair?ssummer ?14 schedule is now on sale!
where aTwitter user has retweeted an official announce-ment and added their own comment.Another possible pitfall is that as MechanicalTurk is a US-based service and requires workers tohave a US bank account in order to perform work,Turkers tend to be US-based, and therefore an an-notation task concerning non-US business entitiesis perhaps more difficult without sufficient back-ground awareness of the entities in question.Future experiments will apply the methodologydeveloped here to a larger dataset of tweets, onecandidate would be the dataset used in the RepLab2013 evaluation series which contains 2,200 an-notated tweets for 61 business entities in four do-mains.AcknowledgmentsThe authors are grateful to Enterprise Ireland andthe IDA for funding this research and CeADARthrough their Technology Centre Programme.Rank Feature Rank Feature1 @ 26 NNP PRP2 i 27 NN PRP3 PRP VBP 28 VBP PRP4 my 29 when5 BOL IN 30 if6 me 31 don?t7 you 32 PRP MD8 NNP NNP 33 they9 IN PRP 34 like10 VB PRP 35 PRP VB11 PRP VBD 36 got12 WEIGHT 37 CC NNP13 so 38 but14 NNP VBZ 39 RB IN15 BOL PRP 40 RT16 RB JJ 41 with17 DT NN 42 PRP IN18 BOL RB 43 a19 it 44 NNS RB20 PRP RB 45 CC PRP21 RB RB 46 VBD PRP22 IN DT 47 VBD DT23 i?m 48 no24 just 49 the25 get 50 PRP$ NNTable 7: Top 50 ranked mixed features for mainstudy77ReferencesEnrique Amig?o, Adolfo Corujo, Julio Gonzalo, EdgarMeij, and Maarten de Rijke.
2012.
Overviewof replab 2012: Evaluating online reputation man-agement systems.
In CLEF (Online WorkingNotes/Labs/Workshop).Enrique Amig?o, Jorge Carrillo de Albornoz, IrinaChugur, Adolfo Corujo, Julio Gonzalo, TamaraMart?
?n, Edgar Meij, Maarten de Rijke, and Dami-ano Spina.
2013.
Overview of replab 2013:Evaluating online reputation monitoring systems.In Information Access Evaluation.
Multilinguality,Multimodality, and Visualization, pages 333?352.Springer.Fabr?cio Benevenuto, Gabriel Magno, Tiago Ro-drigues, and Virg?lio Almeida.
2010.
Detect-ing spammers on twitter.
In Collaboration, elec-tronic messaging, anti-abuse and spam conference(CEAS), volume 6.Mudit Bhargava, Pulkit Mehndiratta, and KrishnaAsawa.
2013.
Stylometric analysis for authorshipattribution on twitter.
In Big Data Analytics, pages37?47.
Springer International Publishing.Julian Brooke and Graeme Hirst.
2012.
Measuringinterlanguage: Native language identification withl1-influence metrics.
In LREC, pages 779?784.Mario Cataldi, Luigi Di Caro, and Claudio Schifanella.2010.
Emerging topic detection on twitter based ontemporal and social terms evaluation.
In Proceed-ings of the Tenth International Workshop on Multi-media Data Mining, page 4.
ACM.Moshe Koppel, Shlomo Argamon, and Anat RachelShimoni.
2002.
Automatically categorizing writ-ten texts by author gender.
Literary and LinguisticComputing, 17(4):401?412.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: The goodthe bad and the omg!
In ICWSM.Robert Layton, Paul Watters, and Richard Dazeley.2010.
Authorship attribution for twitter in 140 char-acters or less.
In Cybercrime and Trustworthy Com-puting Workshop (CTC), 2010 Second, pages 1?8.IEEE.Gerard Lynch and Carl Vogel.
2012.
Towards the au-tomatic detection of the source language of a literarytranslation.
In COLING (Posters), pages 775?784.Franc?ois Mairesse, Marilyn A Walker, Matthias RMehl, and Roger K Moore.
2007.
Using linguis-tic cues for the automatic recognition of personalityin conversation and text.
J. Artif.
Intell.
Res.
(JAIR),30:457?500.John O?Donovan, Byungkyu Kang, Greg Meyer, To-bias Hollerer, and Sibel Adalii.
2012.
Credibility incontext: An analysis of feature distributions in twit-ter.
In Privacy, Security, Risk and Trust (PASSAT),2012 International Conference on and 2012 Inter-national Confernece on Social Computing (Social-Com), pages 293?301.
IEEE.Fernando Perez-Tellez, David Pinto, John Cardiff, andPaolo Rosso.
2011.
On the difficulty of cluster-ing microblog texts for online reputation manage-ment.
In Proceedings of the 2nd Workshop on Com-putational Approaches to Subjectivity and SentimentAnalysis, pages 146?152.
Association for Computa-tional Linguistics.Carolyn Ros?e, Yi-Chia Wang, Yue Cui, Jaime Ar-guello, Karsten Stegmann, Armin Weinberger, andFrank Fischer.
2008.
Analyzing collaborativelearning processes automatically: Exploiting the ad-vances of computational linguistics in computer-supported collaborative learning.
Internationaljournal of computer-supported collaborative learn-ing, 3(3):237?271.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.2010.
Earthquake shakes twitter users: real-timeevent detection by social sensors.
In Proceedingsof the 19th international conference on World wideweb, pages 851?860.
ACM.Damiano Spina, Julio Gonzalo, and Enrique Amig?o.2013.
Discovering filter keywords for companyname disambiguation in twitter.
Expert Systems withApplications.Hans van Halteren.
2008.
Source language mark-ers in europarl translations.
In Proceedings of the22nd International Conference on ComputationalLinguistics-Volume 1, pages 937?944.
Associationfor Computational Linguistics.Carl Vogel, Ger Lynch, Erwan Moreau, Liliana Ma-mani Sanchez, and Phil Ritchie.
2013.
Found intranslation: Computational discovery of translationeffects.
Translation Spaces, 2(1):81?104.Jianshu Weng and Bu-Sung Lee.
2011.
Event detec-tion in twitter.
In ICWSM.Surender Reddy Yerva, Zolt?an Mikl?os, and KarlAberer.
2011.
What have fruits to do with technol-ogy?
: the case of orange, blackberry and apple.
InProceedings of the International Conference on WebIntelligence, Mining and Semantics, page 48.
ACM.78
