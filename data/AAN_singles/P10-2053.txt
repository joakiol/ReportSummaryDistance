Proceedings of the ACL 2010 Conference Short Papers, pages 286?290,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsExtracting Sequences from the WebAnthony Fader, Stephen Soderland, and Oren EtzioniUniversity of Washington, Seattle{afader,soderlan,etzioni}@cs.washington.eduAbstractClassical Information Extraction (IE) sys-tems fill slots in domain-specific frames.This paper reports on SEQ, a novelopen IE system that leverages a domain-independent frame to extract ordered se-quences such as presidents of the UnitedStates or the most common causes of deathin the U.S. SEQ leverages regularitiesabout sequences to extract a coherent setof sequences from Web text.
SEQ nearlydoubles the area under the precision-recallcurve compared to an extractor that doesnot exploit these regularities.1 IntroductionClassical IE systems fill slots in domain-specificframes such as the time and location slots in sem-inar announcements (Freitag, 2000) or the terror-ist organization slot in news stories (Chieu et al,2003).
In contrast, open IE systems are domain-independent, but extract ?flat?
sets of assertionsthat are not organized into frames and slots(Sekine, 2006; Banko et al, 2007).
This paperreports on SEQ?an open IE system that leveragesa domain-independent frame to extract ordered se-quences of objects from Web text.
We show thatthe novel, domain-independent sequence frame inSEQ substantially boosts the precision and recallof the system and yields coherent sequences fil-tered from low-precision extractions (Table 1).Sequence extraction is distinct from set expan-sion (Etzioni et al, 2004; Wang and Cohen, 2007)because sequences are ordered and because the ex-traction process does not require seeds or HTMLlists as input.The domain-independent sequence frame con-sists of a sequence name s (e.g., presidents of theUnited States), and a set of ordered pairs (x, k)where x is a string naming a member of the se-quence with name s, and k is an integer indicatingMost common cause of death in the United States:1. heart disease, 2. cancer, 3. stroke, 4.
COPD,5.
pneumonia, 6. cirrhosis, 7.
AIDS, 8. chronic liverdisease, 9. sepsis, 10. suicide, 11. septic shock.Largest tobacco company in the world:1.
Philip Morris, 2.
BAT, 3.
Japan Tobacco,4.
Imperial Tobacco, 5.
Altadis.Largest rodent in the world:1.
Capybara, 2.
Beaver, 3.
Patagonian Cavies.
4.
Maras.Sign of the zodiac:1.
Aries, 2.
Taurus, 3.
Gemini, 4.
Cancer, 5.
Leo,6.
Virgo, 7.
Libra, 8.
Scorpio, 9.
Sagittarius,10.
Capricorn, 11.
Aquarius, 12.
Pisces, 13.
Ophiuchus.Table 1: Examples of sequences extracted by SEQfrom unstructured Web text.its position (e.g., (Washington, 1) and (JFK, 35)).The task of sequence extraction is to automaticallyinstantiate sequence frames given a corpus of un-structured text.By definition, sequences have two propertiesthat we can leverage in creating a sequence ex-tractor: functionality and density.
Functionalitymeans position k in a sequence is occupied by asingle real-world entity x. Density means that ifa value has been observed at position k then theremust exist values for all i < k, and possibly moreafter it.2 The SEQ SystemSequence extraction has two parts: identify-ing possible extractions (x, k, s) from text, andthen classifying those extractions as either cor-rect or incorrect.
In the following section, wedescribe a way to identify candidate extractionsfrom text using a set of lexico-syntactic patterns.We then show that classifying extractions basedon sentence-level features and redundancy aloneyields low precision, which is improved by lever-aging the functionality and density properties ofsequences as done in our SEQ system.286Pattern Examplethe ORD the fifththe RB ORD the very firstthe JJS the bestthe RB JJS the very bestthe ORD JJS the third biggestthe RBS JJ the most popularthe ORD RBS JJ the second least likelyTable 2: The patterns used by SEQ to detect ordi-nal phrases are noun phrases that begin with oneof the part-of-speech patterns listed above.2.1 Generating Sequence ExtractionsTo obtain candidate sequence extractions (x, k, s)from text, the SEQ system finds sentences in itsinput corpus that contain an ordinal phrase (OP).Table 2 lists the lexico-syntactic patterns SEQ usesto detect ordinal phrases.
The value of k is set tothe integer corresponding to the ordinal number inthe OP.1Next, SEQ takes each sentence that contains anordinal phrase o, and finds candidate items of theform (x, k) for the sequence with name s. SEQconstrains x to be an NP that is disjoint from o, ands to be an NP (which may have post-modifyingPPs or clauses) following the ordinal number in o.For example, given the sentence ?With helpfrom his father, JFK was elected as the 35th Pres-ident of the United States in 1960?, SEQ findsthe candidate sequences with names ?President?,?President of the United States?, and ?President ofthe United States in 1960?, each of which has can-didate extractions (JFK, 35), (his father, 35), and(help, 35).
We use heuristics to filter out many ofthe candidate values (e.g., no value should cross asentence-like boundary, and x should be at mostsome distance from the OP).This process of generating candidate ex-tractions has high coverage, but low preci-sion.
The first step in identifying correct ex-tractions is to compute a confidence measurelocalConf(x, k, s|sentence), which measureshow likely (x, k, s) is given the sentence it camefrom.
We do this using domain-independent syn-tactic features based on POS tags and the pattern-based features ?x {is,are,was,were} the kth s?
and?the kth s {is,are,was,were} x?.
The features arethen combined using a Naive Bayes classifier.In addition to the local, sentence-based features,1Sequences often use a superlative for the first item (k =1) such as ?the deepest lake in Africa?, ?the second deepestlake in Africa?
(or ?the 2nd deepest ...?
), etc.we define the measure totalConf that takes intoaccount redundancy in an input corpus C. AsDowney et al observed (2005), extractions thatoccur more frequently in multiple distinct sen-tences are more likely to be correct.totalConf(x, k, s|C) =?sentence?ClocalConf(x, k, s|sentence) (1)2.2 ChallengesThe scores localConf and totalConf are not suffi-cient to identify valid sequence extractions.
Theytend to give high scores to extractions where thesequence scope is too general or too specific.
Inour running example, the sequence name ?Presi-dent?
is too general ?
many countries and orga-nizations have a president.
The sequence name?President of the United States in 1960?
is too spe-cific ?
there were not multiple U.S. presidents in1960.These errors can be explained as violations offunctionality and density.
The sequence withname ?President?
will have many distinct candi-date extractions in its positions, which is a vio-lation of functionality.
The sequence with name?President of the United States in 1960?
will notsatisfy density, since it will have extractions foronly one position.In the next section, we present the details of howSEQ incorporates functionality and density into itsassessment of a candidate extraction.Given an extraction (x, k, s), SEQ must clas-sify it as either correct or incorrect.
SEQ breaksthis problem down into two parts: (1) determiningwhether s is a correct sequence name, and (2) de-termining whether (x, k) is an item in s, assumings is correct.A joint probabilistic model of these two deci-sions would require a significant amount of la-beled data.
To get around this problem, we repre-sent each (x, k, s) as a vector of features and traintwo Naive Bayes classifiers: one for classifying sand one for classifying (x, k).
We then rank ex-tractions by taking the product of the two classi-fiers?
confidence scores.We now describe the features used in the twoclassifiers and how the classifiers are trained.Classifying Sequences To classify a sequencename s, SEQ uses features to measure the func-tionality and density of s. Functionality means287that a correct sequence with name s has one cor-rect value x at each position k, possibly with ad-ditional noise due to extraction errors and synony-mous values of x.
For a fixed sequence name sand position k, we can weight each of the candi-date x values in that position by their normalizedtotal confidence:w(x|k, s, C) =totalConf(x, k, s|C)?x?
totalConf(x?, k, s|C)For overly general sequences, the distribution ofweights for a position will tend to be more flat,since there are many equally-likely candidate xvalues.
To measure this property, we use a func-tion analogous to information entropy:H(k, s|C) = ?
?xw(x|k, s, C) log2w(x|k, s, C)Sequences s that are too general will tend to havehigh values of H(k, s|C) for many values of k.We found that a good measure of the overall non-functionality of s is the average value of H(k, s|C)for k = 1, 2, 3, 4.For a sequence name s that is too specific, wewould expect that there are only a few filled-in po-sitions.
We model the density of s with two met-rics.
The first is numFilledPos(s|C), the num-ber of distinct values of k such that there is someextraction (x, k) for s in the corpus.
The secondis totalSeqConf(s|C), which is the sum of thescores of most confident x in each position:totalSeqConf(s|C) =?kmaxxtotalConf(x, k, s|C) (2)The functionality and density features are com-bined using a Naive Bayes classifier.
To train theclassifier, we use a set of sequence names s labeledas either correct or incorrect, which we describe inSection 3.Classifying Sequence Items To classify (x, k)given s, SEQ uses two features: the total con-fidence totalConf(x, k, s|C) and the same totalconfidence normalized to sum to 1 over all x, hold-ing k and s constant.
To train the classifier, we usea set of extractions (x, k, s) where s is known tobe a correct sequence name.3 Experimental ResultsThis section reports on two experiments.
First, wemeasured how the density and functionality fea-tures improve performance on the sequence name0.0 0.2 0.4 0.6 0.8 1.0Recall0.00.20.40.60.81.0PrecisionBoth Feature SetsOnly DensityOnly FunctionalityMax localConfFigure 1: Using density or functionality featuresalone is effective in identifying correct sequencenames.
Combining both types of features outper-forms either by a statistically significant margin(paired t-test, p < 0.05).classification sub-task (Figure 1).
Second, wereport on SEQ?s performance on the sequence-extraction task (Figure 2).To create a test set, we selected all sentencescontaining ordinal phrases from Banko?s 500MWeb page corpus (2008).
To enrich this set O,we obtained additional sentences from Bing.comas follows.
For each sequence name s satis-fying localConf(x, k, s|sentence) ?
0.5 forsome sentence in O, we queried Bing.com for?the kth s?
for k = 1, 2, .
.
.
until no more hitswere returned.2 For each query, we downloadedthe search snippets and added them to our cor-pus.
This procedure resulted in making 95, 611search engine queries.
The final corpus contained3, 716, 745 distinct sentences containing an OP.Generating candidate extractions using themethod from Section 2.1 resulted in a set of over40 million distinct extractions, the vast majorityof which are incorrect.
To get a sample witha significant number of correct extractions, wefiltered this set to include only extractions withtotalConf(x, k, s|C) ?
0.8 for some sentence,resulting in a set of 2, 409, 211 extractions.We then randomly sampled and manually la-beled 2, 000 of these extractions for evaluation.We did a Web search to verify the correctness ofthe sequence name s and that x is the kth item inthe sequence.
In some cases, the ordering rela-tion of the sequence name was ambiguous (e.g.,2We queried for both the numeric form of the ordinal andthe number spelled out (e.g ?the 2nd ...?
and ?the second ...?
).We took up to 100 results per query.2880.0 0.2 0.4 0.6 0.8 1.0Recall0.00.20.40.60.81.0PrecisionSEQREDUNDLOCALFigure 2: SEQ outperforms the baseline systems,increasing the area under the curve by 247% rela-tive to LOCAL and by 90% relative to REDUND.
?largest state in the US?
could refer to land area orpopulation), which could lead to merging two dis-tinct sequences.
In practice, we found that mostordering relations were used in a consistent way(e.g., ?largest city in?
always means largest bypopulation) and only about 5% of the sequencenames in our sample have an ambiguous orderingrelation.We compute precision-recall curves relative tothis random sample by changing a confidencethreshold.
Precision is the percentage of correctextractions above a threshold, while recall is thepercentage correct above a threshold divided bythe total number of correct extractions.
BecauseSEQ requires training data, we used 15-fold crossvalidation on the labeled sample.The functionality and density features boostSEQ?s ability to correctly identify sequencenames.
Figure 1 shows how well SEQ can iden-tify correct sequence names using only functional-ity, only density, and using functionality and den-sity in concert.
The baseline used is the maximumvalue of localConf(x, k, s) over all (x, k).
Boththe density features and the functionality featuresare effective at this task, but using both types offeatures resulted in a statistically significant im-provement over using either type of feature in-dividually (paired t-test of area under the curve,p < 0.05).We measure SEQ?s efficacy on the completesequence-extraction task by contrasting it with twobaseline systems.
The first is LOCAL, whichranks extractions by localConf .3 The second is3If an extraction arises from multiple sentences, we useREDUND, which ranks extractions by totalConf .Figure 2 shows the precision-recall curves for eachsystem on the test data.
The area under the curvesfor SEQ, REDUND, and LOCAL are 0.59, 0.31,and 0.17, respectively.
The low precision and flatcurve for LOCAL suggests that localConf is notinformative for classifying extractions on its own.REDUND outperformed LOCAL, especially atthe high-precision part of the curve.
On the subsetof extractions with correct s, REDUND can iden-tify x as the kth item with precision of 0.85 at re-call 0.80.
This is consistent with previous work onredundancy-based extractors on the Web.
How-ever, REDUND still suffered from the problemsof over-specification and over-generalization de-scribed in Section 2.
SEQ reduces the negative ef-fects of these problems by decreasing the scoresof sequence names that appear too general or toospecific.4 Related WorkThere has been extensive work in extracting listsor sets of entities from the Web.
These extrac-tors rely on either (1) HTML features (Cohenet al, 2002; Wang and Cohen, 2007) to extractfrom structured text or (2) lexico-syntactic pat-terns (Hearst, 1992; Etzioni et al, 2005) to ex-tract from unstructured text.
SEQ is most similarto this second type of extractor, but additionallyleverages the sequence regularities of functionalityand density.
These regularities allow the system toovercome the poor performance of the purely syn-tactic extractor LOCAL and the redundancy-basedextractor REDUND.5 ConclusionsWe have demonstrated that an extractor leveragingsequence regularities can greatly outperform ex-tractors without this knowledge.
Identifying likelysequence names and then filling in sequence itemsproved to be an effective approach to sequence ex-traction.One line of future research is to investigateother types of domain-independent frames that ex-hibit useful regularities.
Other examples includeevents (with regularities about actor, location, andtime) and a generic organization-role frame (withregularities about person, organization, and roleplayed).the maximal localConf .2896 AcknowledgementsThis research was supported in part by NSFgrant IIS-0803481, ONR grant N00014-08-1-0431, DARPA contract FA8750-09-C-0179, andan NSF Graduate Research Fellowship, and wascarried out at the University of Washington?s Tur-ing Center.ReferencesMichele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
InProceedings of ACL-08: HLT, pages 28?36.Michele Banko, Michael J. Cafarella, Stephen Soder-land, Matthew Broadhead, and Oren Etzioni.
2007.Open information extraction from the web.
In IJ-CAI, pages 2670?2676.H.
Chieu, H. Ng, and Y. Lee.
2003.
Closing thegap: Learning-based information extraction rival-ing knowledge-engineering methods.
In ACL, pages216?223.William W. Cohen, Matthew Hurst, and Lee S. Jensen.2002.
A flexible learning system for wrapping ta-bles and lists in html documents.
In In InternationalWorld Wide Web Conference, pages 232?241.Doug Downey, Oren Etzioni, and Stephen Soderland.2005.
A probabilistic model of redundancy in infor-mation extraction.
In IJCAI, pages 1034?1041.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.2004.
Methods for domain-independent informa-tion extraction from the Web: An experimental com-parison.
In Proceedings of the Nineteenth NationalConference on Artificial Intelligence (AAAI-2004),pages 391?398.Oren Etzioni, Michael Cafarella, Doug Downey,Ana maria Popescu, Tal Shaked, Stephen Soderl,Daniel S. Weld, and Er Yates.
2005.
Unsupervisednamed-entity extraction from the web: An experi-mental study.
Artificial Intelligence, 165:91?134.D.
Freitag.
2000.
Machine learning for informationextraction in informal domains.
Machine Learning,39(2-3):169?202.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In COLING, pages539?545.Satoshi Sekine.
2006.
On-demand information extrac-tion.
In Proceedings of the COLING/ACL on Mainconference poster sessions, pages 731?738, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Richard C. Wang and William W. Cohen.
2007.Language-independent set expansion of named enti-ties using the web.
In ICDM, pages 342?350.
IEEEComputer Society.290
