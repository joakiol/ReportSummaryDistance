Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 11?19,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsBootstrapping Biomedical Ontologies for Scientific Text using NELLDana Movshovitz-AttiasCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213 USAdma@cs.cmu.eduWilliam W. CohenCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213 USAwcohen@cs.cmu.eduAbstractWe describe an open information extractionsystem for biomedical text based on NELL(the Never-Ending Language Learner) (Carl-son et al, 2010), a system designed for ex-traction from Web text.
NELL uses a cou-pled semi-supervised bootstrapping approachto learn new facts from text, given an initialontology and a small number of ?seeds?
foreach ontology category.
In contrast to previ-ous applications of NELL, in our task the ini-tial ontology and seeds are automatically de-rived from existing resources.
We show thatNELL?s bootstrapping algorithm is suscepti-ble to ambiguous seeds, which are frequent inthe biomedical domain.
Using NELL to ex-tract facts from biomedical text quickly leadsto semantic drift.
To address this problem, weintroduce a method for assessing seed qual-ity, based on a larger corpus of data derivedfrom the Web.
In our method, seed qualityis assessed at each iteration of the bootstrap-ping process.
Experimental results show sig-nificant improvements over NELL?s originalbootstrapping algorithm on two types of tasks:learning terms from biomedical categories,and named-entity recognition for biomedicalentities using a learned lexicon.1 IntroductionNELL (the Never-Ending Language Learner) is asemi-supervised learning system, designed for ex-traction of information from the Web.
The systemuses a coupled semi-supervised bootstrapping app-roach to learn new facts from text, given an initialontology and a small number of ?seeds?, i.e., labeledexamples for each ontology category.
The new factsare stored in a growing structured knowledge base.One of the concerns about gathering data from theWeb is that it comes from various un-authoritativesources, and may not be reliable.
This is especiallytrue when gathering scientific information.
In con-trast to Web data, scientific text is potentially morereliable, as it is guided by the peer-review process.Open access scientific archives make this informa-tion available for all.
In fact, the production rate ofpublicly available scientific data far exceeds the abil-ity of researchers to ?manually?
process it, and thereis a growing need for the automation of this process.The biomedical field presents a great potential fortext mining applications.
An integral part of life sci-ence research involves production and publication oflarge collections of data by curators, and as part ofcollaborative community effort.
Prominent exam-ples include: publication of genomic sequence data,e.g., by the Human Genome Project; online col-lections of three-dimensional coordinates of proteinstructures; and databases holding data on genes.
Animportant resource, initiated as a means of enforc-ing data standardization, are ontologies describingbiological, chemical and medical terms.
These areheavily used by the research community.
With thiswealth of available data the biomedical field holdsmany information extraction opportunities.We describe an open information extraction sys-tem adapting NELL to the biomedical domain.
Wepresent an implementation of our approach, namedBioNELL, which uses three main sources of infor-mation: (1) a public corpus of biomedical scientifictext, (2) commonly used biomedical ontologies, and11High PMI Seeds Random SeedsSoxN achaete cycA cac section 33 28Pax-6 Drosomycin Zfh-1 crybaby hv BobBX-C Ultrabithorax GATAe ael LRS dipD-Fos sine oculis FMRFa chm sht 3520Abd-A dCtBP Antp M-2 AGI touPKAc huckebein abd-A shanti disp zenHmgcr Goosecoid knirps Buffy Gap Scmfkh decapentaplegic Sxl lac Mercurio REPOabdA naked cuticle BR-C subcosta mef Ferritinzfh-1 Kruppel hmgcr Slam dad dTCFtkv gypsy insulator Dichaete Cbs Helicase magoCrebA alpha-Adaptin Abd-B Sufu ora PtenD-raf doublesex gusA pelo vu sbMtnA FasII AbdA sombre domain II TrpRSDcr-2 GAGA factor dTCF TAS CCK ripcordfushitarazukanamycinresistanceEcdysonereceptorGABAAreceptordiazepambindinginhibitoryolkproteinTkv dCBP Debcl armTable 1: Two samples of fruit-fly genes, taken from thecomplete fly gene dictionary.
High PMI Seeds are the top50 terms selected using PMI ranking, and Random Seedsare a random draw of 50 terms from the dictionary.
Theseare used as seeds for the Fly Gene category (Section 4.2).Notice that the random set contains many terms that areoften not used as genes including arm, 28, and dad.
Us-ing these as seeds can lead to semantic drift.
In contrast,high PMI seeds exhibit much less ambiguity.
(3) a corpus of Web documents.NELL?s ontology, including categories and seeds,has been manually designed during the system de-velopment.
Ontology design involves assembling aset of interesting categories, organized in a meaning-ful hierarchical structure, and providing represen-tative seeds for each category.
Redesigning a newontology for a technical domain is difficult withoutnon-trivial knowledge of the domain.
We describe aprocess of merging source ontologies into one struc-ture of categories with seed examples.However, as we will show, using NELL?s boot-strapping algorithm to extract facts from a biomed-ical corpus is susceptible to noisy and ambiguousterms.
Such ambiguities are common in biomedi-cal terminology (see examples in Table 1), and someambiguous terms are heavily used in the literature.For example, in the sentence ?We have cloned aninduced white mutation and characterized the in-sertion sequence responsible for the mutant pheno-type?, white is an ambiguous term referring to thename of a gene.
In NELL, ambiguity is limited us-ing coupled semi-supervised learning (Carlson et al,2009): if two categories in the ontology are declaredmutually exclusive, instances of one category areused as negative examples for the other, and the twocategories cannot share any instances.
To resolvethe ambiguity of white with mutual exclusion, wewould have to include a Color category in the ontol-ogy, and declare it mutually exclusive with the Genecategory.
Then, instances of Color will not be ableto refer to genes in the KB.
It is hard to estimate whatadditional categories should be added, and buildinga ?complete?
ontology tree is practically infeasible.NELL also includes a polysemy resolution com-ponent that acknowledges that one term, for exam-ple white, may refer to two distinct concepts, saya color and a gene, that map to different ontologycategories, such as Color and Fly Gene (Krishna-murthy and Mitchell, 2011).
By including a Colorcategory, this component can identify that white isboth a color and a gene.
The polysemy resolver per-forms word sense induction and synonym resolutionbased on relations defined between categories in theontology, and labeled synonym examples.
However,at present, BioNELL?s ontology does not contain re-lation definitions (it is based only on categories),so we cannot include this component in our exper-iments.
Additionally, it is unclear how to avoid theuse of polysemous terms as category seeds, and nomethod has been suggested for selecting seeds thatare representative of a single specific category.To address the problem of ambiguity, we intro-duce a method for assessing the desirability of nounphrases to be used as seeds for a specific target cat-egory.
We propose ranking seeds using a Point-wise Mutual Information (PMI) -based collocationmeasure for a seed and a category name.
Colloca-tion is measured based on a large corpus of domain-independent data derived from the Web, accountingfor uses of the seed in many different contexts.NELL?s bootstrapping algorithm uses the mor-phological and semantic features of seeds to pro-pose new facts, which are added to the KB and usedas seeds in the next bootstrapping iteration to learnmore facts.
This means that ambiguous terms maybe added at any learning iteration.
Since white reallyis a name of a gene, it is sometimes used in the samesemantic context as other genes, and may be addedto the KB despite not being used as an initial seed.12To resolve this problem, we propose measuring seedquality in a Rank-and-Learn bootstrapping method-ology: after every iteration, we rank all the instancesthat have been added to the KB by their qualityas potential category seeds.
Only high-ranking in-stances are used as seeds in the next iteration.
Low-ranking instances are stored in the KB and ?remem-bered?
as true facts, but are not used for learningnew information.
This is in contrast to NELL?s ap-proach (and most other bootstrapping systems), inwhich there is no distinction between acquired facts,and facts that are used for learning.2 Related WorkBiomedical Information Extraction systems havetraditionally targeted recognition of few distinct bi-ological entities, focusing mainly on genes (e.g.,(Chang et al, 2004)).
Few systems have been devel-oped for fact-extraction of many biomedical predi-cates, and these are relatively small scale (Wattaru-jeekrit et al, 2004), or they account for limited sub-domains (Dolbey et al, 2006).
We suggest a moregeneral approach, using bootstrapping to extend ex-isting biomedical ontologies, including a wide rangeof sub-domains and many categories.
The currentimplementation of BioNELL includes an ontologywith over 100 categories.
To the best of our knowl-edge, such large-scale biomedical bootstrapping hasnot been done before.Bootstrap Learning and Semantic Drift.
Carl-son et al (2010) use coupled semi-supervised boot-strap learning in NELL to learn a large set of cate-gory classifiers with high precision.
One drawbackof using iterative bootstrapping is the sensitivity ofthis method to the set of initial seeds (Pantel et al,2009).
An ambiguous set of seeds can lead to se-mantic drift, i.e., accumulation of erroneous termsand contexts when learning a semantic class.
Strictbootstrapping environments reduce this problem byadding boundaries or limiting the learning process,including learning mutual terms and contexts (Riloffand Jones, 1999) and using mutual exclusion andnegative class examples (Curran et al, 2007).McIntosh and Curran (2009) propose a metricfor measuring the semantic drift introduced by alearned term, favoring terms different than the recentm learned terms and similar to the first n, (shownfor n=20 and n=100), following the assumption thatsemantic drift develops in late bootstrapping itera-tions.
As we will show, for biomedical categories,semantic drift in NELL occurs within a handful ofiterations (< 5), however according to the authors,using low values for n produces inadequate results.In fact, selecting effective n and m parameters maynot only be a function of the data being used, butalso of the specific category, and it is unclear how toautomatically tune them.Seed Set Refinement.
Vyas et al (2009) suggesta method for reducing ambiguity in seeds providedby human experts, by selecting the tightest seedclusters based on context similarity.
The method isdescribed for an order of 10 seeds, however, in anontology containing hundreds of seeds per class, it isunclear how to estimate the correct number of clus-ters to choose from.
Another approach, suggestedby Kozareva et al (2010), is using only constrainedcontexts where both seed and class are present in asentence.
Extending this idea, we consider a moregeneral collocation metric, looking at entire docu-ments including both the seed and its category.3 Implementation3.1 NELL?s Bootstrapping SystemWe have implemented BioNELL based on the sys-tem design of NELL.
NELL?s bootstrapping algo-rithm is initiated with an input ontology structure ofcategories and seeds.
Three sub-components oper-ate to introduce new facts based on the semantic andmorphological attributes of known facts.
At everyiteration, each component proposes candidate facts,specifying the supporting evidence for each candi-date, and the candidates with the most strongly sup-ported evidence are added to the KB.
The processand sub-components are described in detail by Carl-son et al (2010) and Wang and Cohen (2009).3.2 Text CorporaPubMed Corpus: We used a corpus of 200K full-text biomedical articles taken from the PubMedCentral Open Access Subset (extracted in October2010)1, which were processed using the OpenNLPpackage2.
This is the main BioNELL corpus and it1http://www.ncbi.nlm.nih.gov/pmc/2http://opennlp.sourceforge.net13is used to extract category instances in all the exper-iments presented in this paper.Web Corpus: BioNELL?s seed-quality colloca-tion measure (Section 3.4) is based on a domain-independent Web corpus, the English portion of theClueWeb09 data set (Callan and Hoy, 2009), whichincludes 500 million web documents.3.3 OntologyBioNELL?s ontology is composed of six base on-tologies, covering a wide range of biomedical sub-domains: the Gene Ontology (GO) (Ashburner etal., 2000), describing gene attributes; the NCBI Tax-onomy for model organisms (Sayers et al, 2009);Chemical Entities of Biological Interest (ChEBI)(Degtyarenko et al, 2008), a dictionary focused onsmall chemical compounds; the Sequence Ontol-ogy (Eilbeck et al, 2005), describing biological se-quences; the Cell Type Ontology (Bard et al, 2005);and the Human Disease Ontology (Osborne et al,2009).
Each ontology provides a hierarchy of termsbut does not distinguish concepts from instances.We used an automatic process for merging baseontologies into one ontology tree.
First, we groupthe ontologies under one hierarchical structure, pro-ducing a tree of over 1 million entities, including856K terms and 154K synonyms.
We then separatethese into potential categories and potential seeds.Categories are nodes that are unambiguous (have asingle parent in the ontology tree), with at least 100descendants.
These descendants are the category?sPotential seeds.
This results in 4188 category nodes.In the experiments of this paper we selected onlythe top (most general) 20 categories in the tree ofeach base ontology.
We are left with 109 final cate-gories, as some base ontologies had less than 20 cat-egories under these restrictions.
Leaf categories aregiven seeds from their descendants in the full tree ofall terms and synonyms, giving a total of around 1million potential seeds.
Seed set refinement is de-scribed below.
The seeds of leaf categories are laterextended by the bootstrapping process.3.4 BioNELL?s Bootstrapping System3.4.1 PMI Collocation with the Category NameWe define a seed quality metric based on a largecorpus of Web data.
Let s and c be a seed and a tar-get category, respectively.
For example, we can takes = ?white?, the name of a gene of the fruit-fly, and c= ?fly gene?.
Now, let D be a document corpus (Sec-tion 3.2 describes the Web corpus used for ranking),and let Dc be a subset of the documents contain-ing a mention of the category name.
We measurethe collocation of the seed and the category by thenumber of times s appears in Dc, |Occur(s,Dc)|.The overall occurrence of s in the corpus is givenby |Occur(s,D)|.
Following the formulation ofChurch and Hanks (1990), we compute the PMI-rank of s and c asPMI(s, c) =|Occur(s,Dc)||Occur(s,D)|(1)Since this measure is used to compare seeds of thesame category, we omit the log from the original for-mulation.
In our example, as white is a highly am-biguous gene name, we find that it appears in manydocuments that do not discuss the fruit fly, resultingin a PMI rank close to 0.The proposed ranking is sensitive to the descrip-tive name given to categories.
For a more robustranking, we use a combination of rankings of theseed with several of its ancestors in the ontology hi-erarchy.
In (Movshovitz-Attias and Cohen, 2012)we describe this hierarchical ranking in more detailand additionally explore the use of the binomial log-likelihood ratio test (BLRT) as an alternative collo-cation measure for ranking.We further note that some specialized biomedicalterms follow strict nomenclature rules making themeasily identifiable as category specific.
These termsmay not be frequent in general Web context, lead-ing to a low PMI rank under the proposed method.Given such a set of high confidence seeds from areliable source, one can enforce their inclusion inthe learning process, and specialized seeds can addi-tionally be identified by high-confidence patterns, ifsuch exist.
However, the scope of this work involvesselecting seeds from an ambiguous source, biomed-ical ontologies, thus we do not include an analysisfor these specialized cases.3.4.2 Rank-and-Learn BootstrappingWe incorporate PMI ranking into BioNELL usinga Rank-and-Learn bootstrapping methodology.
Af-ter every iteration, we rank all the instances that havebeen added to the KB.
Only high-ranking instances14Learning System BootstrappingAlgorithmInitialSeedsCorpusBioNELL Rank-and-Learnwith PMIPMItop 50PubMedNELL NELL?salgorithmRandom50PubMedBioNELL+Random Rank-and-Learnwith PMIRandom50PubMedTable 2: Learning systems used in our evaluation, all us-ing the PubMed biomedical corpus and the biomedicalontology described in Sections 3.2 and 3.3.are added to the collection of seeds that are used inthe next learning iteration.
Instances with low PMIrank are stored in the KB and are not used for learn-ing new information.
We consider a high-rankinginstance to be one with PMI rank higher than 0.25.4 Experimental Evaluation4.1 Experimental Settings4.1.1 Configurations of the AlgorithmIn our experiments, we ran BioNELL and NELLwith the following system configurations, all usingthe biomedical corpus and the ontology described inSections 3.2 and 3.3, and all running 50 iterations,in order to evaluate the long term effects of ranking.Section 4.2 includes a discussion on the learning rateof the tested systems which motivates the reason forevaluating performance at the 50th iteration.To expand a category we used the following sys-tems, also summarized in Table 2: (1) the BioNELLsystem, using Rank-and-Learn bootstrapping (Sec-tion 3.4.2) initialized with the top 50 seeds usingPMI ranking, (2) the NELL system, using NELL?soriginal bootstrapping algorithm (Section 3.1) ini-tialized with 50 random seeds from the category?spotential seeds (NELL does not provide a seed se-lection method), and (3) in order to distinguishthe contribution of Rank-and-Learn bootstrappingover ranking the initial seeds, we tested a thirdsystem, BioNELL+Random, using Rank-and-Learnbootstrapping initialized with 50 random seeds.4.1.2 Evaluation MethodologyUsing BioNELL we can learn lexicons, collec-tions of category terms accumulated after runningthe system.
One evaluation approach is to selecta set of learned instances and assess their correct-ness (Carlson et al, 2010).
This is relatively easyfor data extracted for general categories like City orSports Team.
For example, it is easy to evaluate thestatement ?London is a City?.
This task becomesmore difficult when assessing domain-specific factssuch as ?Beryllium is an S-block molecular entity?
(in fact, it is).
We cannot, for example, use the helpof Mechanical Turk for this task.
A possible alter-native evaluation approach is asking an expert.
Ontop of being a costly and slow approach, the rangeof topics covered by BioNELL is large and a singleexpert is not likely be able to assess all of them.We evaluated lexicons learned by BioNELL bycomparing them to available resources.
Lexicons ofgene names for certain species are available, and theFreebase database (Google, 2011), an open repos-itory holding data for millions of entities, includessome biomedical concepts.
For most biomedicalcategories, however, complete lexicons are scarce.4.1.3 Data SetsWe compared learned lexicons to category dictio-naries, lists of concept terms taken from the follow-ing sources, which we consider as a Gold Standard.We used three lexicons of biomedical categoriestaken from Freebase: Disease (9420 terms), Chemi-cal Compound (9225 terms), and Drug (3896 terms).To evaluate gene names we used data from theBioCreative Challenge (Hirschman et al, 2005),an evaluation competition focused on annotationsof genes and gene products.
The data includesa dictionary of genes of the fruit-fly, DrosophilaMelanogaster, which specifies a set of gene iden-tifiers and possible alternative forms of the genename, for a total of 7151 terms, which we considerto be the complete fly gene dictionary.We used additional BioCreative data for a named-entity recognition task.
This includes 108 scientificabstracts, manually annotated by BioCreative withgene IDs of fly genes discussed in the text.
The ab-stracts contain either the gene ID or any gene name.4.2 Extending Lexicons of BiomedicalCategories4.2.1 Recovering a Closed Category LexiconWe used BioNELL to learn the lexicon of aclosed category, representing genes of the fruit-fly,1510 20 30 40 5000.20.40.60.81IterationPrecisionBioNELLNELLBioNELL+Random(a) Precision10 20 30 40 50050100150200250IterationCumulative correct lexiconitemsBioNELLNELLBioNELL+Random(b) Cumulative correct items10 20 30 40 500100200300400500IterationCumulative incorrect lexicon itemsBioNELLNELLBioNELL+Random(c) Cumulative incorrect itemsFigure 1: Performance per learning iteration for gene lexicons learned using BioNELL and NELL.Learning System Precision Correct TotalBioNELL .83 109 132NELL .29 186 651BioNELL+Random .73 248 338NELL by size 132 .72 93 130Table 3: Precision, total number of instances (Total),and correct instances (Correct) of gene lexicons learnedwith BioNELL and NELL.
BioNELL significantly im-proves the precision of the learned lexicon compared withNELL.
When examining only the first 132 learned items,BioNELL has both higher precision and more correct in-stances than NELL (last row, NELL by size 132).D.
Melanogaster, a model organism used to studygenetics and developmental biology.
Two samplesof genes from the full fly gene dictionary are shownin Table 1: High PMI Seeds are the top 50 dictio-nary terms selected using PMI ranking, and RandomSeeds are a random draw of 50 terms.
Notice that therandom set contains many seeds that are not distinctgene names including arm, 28, and dad.
In con-trast, high PMI seeds exhibit much less ambiguity.We learned gene lexicons using the test systems de-scribed in Section 4.1.1 with the high-PMI and ran-dom seed sets shown in Table 1.
We measured theprecision, total number of instances, and correct in-stances of the learned lexicons against the full dic-tionary of genes.
Table 3 summarizes the results.BioNELL, initialized with PMI-ranked seeds, sig-nificantly improved the precision of the learnedlexicon over NELL (29% for NELL to 83% forBioNELL).
In fact, the two learning systems us-ing Rank-and-Learn bootstrapping resulted in higherprecision lexicons (83%, 73%), suggesting that con-strained bootstrapping using iterative seed rankingsuccessfully eliminates noisy and ambiguous seeds.BioNELL?s bootstrapping methodology is highlyrestrictive and it affects the size of the learned lexi-con as well as its precision.
Notice, however, thatwhile NELL?s final lexicon is 5 times larger thanBioNELL?s, the number of correctly learned items init are less than twice that of BioNELL.
Additionally,BioNELL+Random has learned a smaller dictionarythan NELL (338 and 651 terms, respectively) with agreater number of correct instances (248 and 186).We examined the performance of NELL after the7th iteration, when it has learned a lexicon of 130items, similar in size to BioNELL?s final lexicon (Ta-ble 3, last row).
After learning 130 items, BioNELLachieved both higher precision (83% versus 72%)and higher recall (109 versus 93 correct lexiconinstances) than NELL, indicating that BioNELL?slearning method is overall more accurate.After running for 50 iterations, all systems re-cover only a small portion of the complete gene dic-tionary (109-248 instances out of 7151), suggestingeither that, (1) more learning iterations are required,(2) the biomedical corpus we use is too small anddoes not contain (frequent) mentions of some genenames from the dictionary, or (3) some other limita-tions exist that prevent the learning algorithm fromfinding additional class examples.Lexicons learned using BioNELL show persis-tently high precision throughout the 50 iterations,even when initiated with random seeds (Figure 1A).By the final iteration, all systems stop accumulatingfurther significant amounts of correct gene instances(Figure 1B).
Systems that use PMI-based Rank-and-Learn bootstrapping also stop learning incorrect16Learning System Precision Correct TotalCC Drug Disease CC Drug Disease CC Drug DiseaseBioNELL .66 .52 .43 63 508 276 96 972 624NELL .15 .40 .37 74 522 288 449 1300 782NELL by size .58 .47 .37 58 455 232 100 968 623Table 4: Precision, total number of instances (Total), and correct instances (Correct) of learned lexicons of ChemicalCompound (CC), Drug, and Disease.
BioNELL?s lexicons have higher precision on all categories compared withNELL, while learning a similar number of correct instances.
When restricting NELL to a total lexicon size similar toBioNELL?s, BioNELL has both higher precision and more correct instances (last row, NELL by size).instances (BioNELL and BioNELL+Random; Fig-ure 1C).
This is in contrast to NELL which continueslearning incorrect examples.Interestingly, the highest number of correct geneinstances was learned using Rank-and-Learn boot-strapping with random initial seeds (248 items;BioNELL+Random).
This may happen when therandom set includes genes that are infrequent inthe general Web corpus, despite being otherwisecategory-specific in the biomedical context.
Assuch, these would result in low PMI rank (see notein Section 3.4.1).
However, random seed selectiondoes not offer any guarantees on the quality of theseeds used, and therefore will result in unstable per-formance.
Note that BioNELL+Random was initi-ated with the same random seeds as NELL, but dueto the more constrained Rank-and-Learn bootstrap-ping it achieves both higher recall (248 versus 186correct instances) and precision (73% versus 29%).4.2.2 Extending Lexicons of Open CategoriesWe evaluated learned lexicons for three open cat-egories, Chemical Compound (CC), Drug, and Dis-ease, using dictionaries from Freebase.
Since theseare open categories ?
new drugs are being devel-oped every year, new diseases are discovered, andvaried chemical compounds can be created ?
theFreebase dictionaries are not likely to contain com-plete information on these categories.
For our evalu-ation, however, we considered them to be complete.We used BioNELL and NELL to learn these cat-egories, and for all of them BioNELL?s lexiconsachieved higher precision than NELL (Table 4).
Thenumber of correct learned instances was similar inboth systems (63 and 74 for CC, 508 and 522 forDrug, and 276 and 288 for Disease), however inBioNELL, the additional bootstrapping restrictionsassist in rejecting incorrect instances, resulting in asmaller, more accurate lexicon.We examined NELL?s lexicons when they reacheda size similar to BioNELL?s final lexicons (at the 8th,42nd and 39th iterations for CC, Drug, and Disease,respectively).
BioNELL?s lexicons have both higherprecision and higher recall (more correct learned in-stances) than the comparable NELL lexicons (Ta-ble 4, NELL by size, last row).4.3 Named-Entity Recognition using aLearned LexiconWe examined the use of gene lexicons learned withBioNELL and NELL for the task of recognizingconcepts in free text, using a simple strategy ofmatching words in the text with terms from the lex-icon.
We use data from the BioCreative challenge(Section 4.1.3), which includes text abstracts and theIDs of genes that appear in each abstract.
We showthat BioNELL?s lexicon achieves both higher preci-sion and recall in this task than NELL?s.We implemented an annotator for predicting whatgenes are discussed in text, which uses a gene lexi-con as input.
Given sample text, if any of the termsin the lexicon appear in the text, the correspondinggene is predicted to be discussed in the text.
Follow-ing BioCreative?s annotation format, the annotatoremits as output the set of gene IDs of the genes pre-dicted for the sample text.We evaluated annotators that were given as in-put: the complete fly-genes dictionary, a filteredversion of that dictionary, or lexicons learned us-ing BioNELL and NELL.
Using these annotators wepredicted gene mentions for all text abstracts in thedata.
We report the average precision (over 108 text17Lexicon Precision Correct TotalBioNELL .90 18 20NELL .02 5 268BioNELL+Random .03 3 82Complete Dictionary .09 153 1616Filtered Dictionary .18 138 675Table 5: Precision, total number of predicted genes (To-tal), and correct predictions (Correct), in a named-entityrecognition task using a complete lexicon, a filtered lex-icon, and lexicons learned with BioNELL and NELL.BioNELL?s lexicon achieves the highest precision, andmakes more correct predictions than NELL.abstracts) and number of total and correct predic-tions of gene mentions, compared with the labeledannotations for each text (Table 5).Many gene names are shared among multiplevariants.
For example, the name Antennapedia mayrefer to several gene variations, e.g., Dgua\Antp orDmed\Antp.
Thus, in our precision measurements,we consider a prediction of a gene ID as ?true?
if itis labeled as such by BioCreative, or if it shares asynonym name with another true labeled gene ID.First, we used the complete fly gene dictionaryfor the recognition task.
Any dictionary gene thatis mentioned in the text was recovered, resultingin high recall.
However, the full dictionary con-tains ambiguous gene names that contribute manyfalse predictions to the complete dictionary annota-tor, leading to a low precision of 9%.Some ambiguous terms can be detected usingsimple rules, e.g., short abbreviations and numbers.For example, section 9 is a gene named after thefunctional unit to which it belongs, and abbreviatedby the symbol 9.
Clearly, removing 9 from the fulllexicon should improve precision without great costto recall.
We similarly filtered the full dictionary, re-moving one- and two-letter abbreviations and termscomposed only of non-alphabetical characters, leav-ing 6253 terms.
Using the filtered dictionary, pre-cision has doubled (18%) with minor compromiseto recall.
Using complete or manually refined genedictionaries for named-entity recognition has beenshown before to produce similar high-recall andlow-precision results (Bunescu et al, 2005).We evaluated annotators on gene lexicons learnedwith BioNELL and NELL.
BioNELL?s lexiconachieved significantly higher precision (90%) thanother lexicons (2%-18%).
It is evident that this lexi-con contains few ambiguous terms as it leads to only2 false predictions.
Note also, that BioNELL?s lexi-con has both higher precision and recall than NELL.5 ConclusionsWe have proposed a methodology for an open infor-mation extraction system for biomedical scientifictext, using an automatically derived ontology of cat-egories and seeds.
Our implementation is based onconstrained bootstrapping in which seeds are rankedat every iteration.The benefits of iterative seed ranking have beendemonstrated, showing that our method leads to sig-nificantly less ambiguous lexicons for all the eval-uated biomedical concepts.
BioNELL shows 51%increase over NELL in the precision of a learnedlexicon of chemical compounds, and 45% increasefor a category of gene names.
Importantly, whenBioNELL and NELL learn lexicons of similar size,BioNELL?s lexicons have both higher precision andrecall.
We have demonstrated the use of BioNELL?slearned gene lexicon as a high precision annotatorin an entity recognition task (with 90% precision).The results are promising, though it is currently dif-ficult to provide a similar quantitative evaluation fora wider range of concepts.Many interesting improvements could be madein the current system, mainly discovery of relationsbetween existing ontology categories.
In addition,we believe that Rank-and-Learn bootstrapping anditerative seed ranking can be beneficial in general,domain-independent settings, and we would like toexplore further use of this method.AcknowledgmentsThis work was funded by grant 1R101GM081293from NIH, IIS-0811562 from NSF and by a gift fromGoogle.
The opinions expressed in this paper aresolely those of the authors.ReferencesM.
Ashburner, C.A.
Ball, J.A.
Blake, D. Botstein, H. But-ler, J.M.
Cherry, A.P.
Davis, K. Dolinski, S.S. Dwight,J.T.
Eppig, et al 2000.
Gene ontology: tool for theunification of biology.
Nature genetics, 25(1):25.18J.
Bard, S.Y.
Rhee, and M. Ashburner.
2005.
An ontol-ogy for cell types.
Genome Biology, 6(2):R21.R.
Bunescu, R. Ge, R.J. Kate, E.M. Marcotte, and R.J.Mooney.
2005.
Comparative experiments on learninginformation extractors for proteins and their interac-tions.
Artificial Intelligence in Medicine, 33(2).J.
Callan and M. Hoy.
2009.
Clueweb09 data set.http://boston.lti.cs.cmu.edu/Data/clueweb09/.A.
Carlson, J. Betteridge, E.R.
Hruschka Jr, T.M.Mitchell, and SP Sao Carlos.
2009.
Coupling semi-supervised learning of categories and relations.
Semi-supervised Learning for Natural Language Process-ing, page 1.A.
Carlson, J. Betteridge, B. Kisiel, B.
Settles, E.R.
Hr-uschka Jr, and T.M.
Mitchell.
2010.
Toward an ar-chitecture for never-ending language learning.
In Pro-ceedings of the Twenty-Fourth Conference on ArtificialIntelligence (AAAI 2010), volume 2, pages 3?3.J.T.
Chang, H. Schu?tze, and R.B.
Altman.
2004.
Gap-score: finding gene and protein names one word at atime.
Bioinformatics, 20(2):216.K.W.
Church and P. Hanks.
1990.
Word associationnorms, mutual information, and lexicography.
Com-putational linguistics, 16(1):22?29.J.R.
Curran, T. Murphy, and B. Scholz.
2007.
Minimis-ing semantic drift with mutual exclusion bootstrap-ping.
In Proceedings of the 10th Conference of thePacific Association for Computational Linguistics.K.
Degtyarenko, P. De Matos, M. Ennis, J. Hastings,M.
Zbinden, A. McNaught, R. Alca?ntara, M. Darsow,M.
Guedj, and M. Ashburner.
2008.
Chebi: a databaseand ontology for chemical entities of biological inter-est.
Nucleic acids research, 36(suppl 1):D344.A.
Dolbey, M. Ellsworth, and J. Scheffczyk.
2006.Bioframenet: A domain-specific framenet extensionwith links to biomedical ontologies.
In Proceedingsof KR-MED, pages 87?94.
Citeseer.K.
Eilbeck, S.E.
Lewis, C.J.
Mungall, M. Yandell,L.
Stein, R. Durbin, and M. Ashburner.
2005.
The se-quence ontology: a tool for the unification of genomeannotations.
Genome biology, 6(5):R44.Google.
2011.
Freebase data dumps.http://download.freebase.com/datadumps/.L.
Hirschman, A. Yeh, C. Blaschke, and A. Valencia.2005.
Overview of biocreative: critical assessment ofinformation extraction for biology.
BMC bioinformat-ics, 6(Suppl 1):S1.Z.
Kozareva and E. Hovy.
2010.
Not all seeds are equal:measuring the quality of text mining seeds.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 618?626.
Associa-tion for Computational Linguistics.J.
Krishnamurthy and T.M.
Mitchell.
2011.
Which nounphrases denote which concepts?
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies.Association for Computational Linguistics.T.
McIntosh and J.R. Curran.
2009.
Reducing seman-tic drift with bagging and distributional similarity.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP: Volume 1-Volume 1, pages 396?404.
As-sociation for Computational Linguistics.D.
Movshovitz-Attias and W.W. Cohen.
2012.
Boot-strapping biomedical ontologies for scientific text us-ing nell.
Technical report, Carnegie Mellon Univer-sity, CMU-ML-12-101.J.
Osborne, J. Flatow, M. Holko, S. Lin, W. Kibbe,L.
Zhu, M. Danila, G. Feng, and R. Chisholm.
2009.Annotating the human genome with disease ontology.BMC genomics, 10(Suppl 1):S6.P.
Pantel, E. Crestan, A. Borkovsky, A.M. Popescu, andV.
Vyas.
2009.
Web-scale distributional similarity andentity set expansion.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 2-Volume 2, pages 938?947.
As-sociation for Computational Linguistics.E.
Riloff and R. Jones.
1999.
Learning dictionaries forinformation extraction by multi-level bootstrapping.In Proceedings of the National Conference on Artifi-cial Intelligence (AAAI-99), pages 474?479.E.
W. Sayers, T. Barrett, D. A. Benson, S. H. Bryant,K.
Canese, V. Chetvernin, D. M. Church, M. DiCuc-cio, R. Edgar, S. Federhen, M. Feolo, L. Y. Geer,W.
Helmberg, Y. Kapustin, D. Landsman, D. J.Lipman, T. L. Madden, D. R. Maglott, V. Miller,I.
Mizrachi, J. Ostell, K. D. Pruitt, G. D. Schuler,E.
Sequeira, S. T. Sherry, M. Shumway, K. Sirotkin,A.
Souvorov, G. Starchenko, T. A. Tatusova, L. Wag-ner, E. Yaschenko, and J. Ye.
2009.
Database re-sources of the National Center for Biotechnology In-formation.
Nucleic Acids Res., 37:5?15, Jan.V.
Vyas, P. Pantel, and E. Crestan.
2009.
Helping edi-tors choose better seed sets for entity set expansion.
InProceeding of the 18th ACM conference on Informa-tion and knowledge management.
ACM.R.C.
Wang and W.W. Cohen.
2009.
Character-level anal-ysis of semi-structured documents for set expansion.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume 3-Volume 3, pages 1503?1512.
Association for Compu-tational Linguistics.T.
Wattarujeekrit, P. Shah, and N. Collier.
2004.
Pasbio:predicate-argument structures for event extraction inmolecular biology.
BMC bioinformatics, 5(1):155.19
