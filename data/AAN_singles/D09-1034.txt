Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324?333,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPDeriving lexical and syntactic expectation-based measuresfor psycholinguistic modeling via incremental top-down parsingBrian Roark?Asaf Bachrach?Carlos Cardenas?and Christophe Pallier?
?Center for Spoken Language Understanding, Oregon Health & Science University?INSERM-CEA Cognitive Neuroimaging Unit, Gif sur Yvette, France?MITroark@cslu.ogi.edu asafbac@gmail.com cardenas@mit.edu christophe@pallier.orgAbstractA number of recent publications havemade use of the incremental output ofstochastic parsers to derive measures ofhigh utility for psycholinguistic modeling,following the work of Hale (2001; 2003;2006).
In this paper, we present novelmethods for calculating separate lexicaland syntactic surprisal measures from asingle incremental parser using a lexical-ized PCFG.
We also present an approx-imation to entropy measures that wouldotherwise be intractable to calculate for agrammar of that size.
Empirical resultsdemonstrate the utility of our methods inpredicting human reading times.1 IntroductionAssessment of linguistic complexity has playedan important role in psycholinguistics and neu-rolinguistics for a long time, from the use ofmean length of utterance and related scores inchild language development (Klee and Fitzgerald,1985), to complexity scores related to reading dif-ficulty in human sentence processing studies (Yn-gve, 1960; Frazier, 1985; Gibson, 1998).
Opera-tionally, such linguistic complexity scores are de-rived via deterministic manual (human) annotationand scoring algorithms of language samples.
Nat-ural language processing has been employed toautomate the extraction of such measures (Sagaeet al, 2005; Roark et al, 2007), which can havehigh utility in terms of reduction of time requiredto annotate and score samples.
More interest-ingly, however, novel data driven methods are be-ing increasingly employed in this sphere, yield-ing language sample characterizations that requireNLP in their derivation.
For example, scoresderived from variously estimated language mod-els have been used to evaluate and classify lan-guage samples associated with neurodevelopmen-tal or neurodegenerative disorders (Roark et al,2007; Solorio and Liu, 2008; Gabani et al, 2009),as well as within general studies of human sen-tence processing (Hale, 2001; 2003; 2006).
Thesescores cannot feasibly be derived by hand, butrather rely on large-scale statistical models andstructured inference algorithms to be derived.
Thisis quickly becoming an important application ofNLP, making possible new methods in the studyof human language processing in both typical andimpaired populations.The use of broad-coverage parsing for psy-cholinguistic modeling has become very popularrecently.
Hale (2001) suggested a measure (sur-prisal) derived from an Earley (1970) parser us-ing a probabilistic context-free grammar (PCFG)for psycholinguistic modeling; and in later work(Hale, 2003; 2006) he suggested an alternateparser-derived measure (entropy reduction) thatmay also account for some human sentence pro-cessing performance.
Recent work continues toadvocate surprisal in particular as a very use-ful measure for predicting processing difficulty(Boston et al, 2008a; Boston et al, 2008b; Dem-berg and Keller, 2008; Levy, 2008), and the mea-sure has been derived using a variety of incre-mental (left-to-right) parsing strategies, includ-ing an Earley parser (Boston et al, 2008a), theRoark (2001) incremental top-down parser (Dem-berg and Keller, 2008), and an n-best version ofthe Nivre et al (2007) incremental dependencyparser (Boston et al, 2008a; 2008b).
Derivingsuch measures by hand, even for a relatively lim-ited set of stimuli, is not feasible, hence parsingplays a critical role in this developing psycholin-guistic enterprise.There is no single measure that can account forall of the factors influencing human sentence pro-cessing performance, and some of the most recentwork on using parser-derived measures for psy-cholinguistic modeling has looked to try to de-rive multiple, complementary measures.
One of324the key distinctions being looked at is syntacticversus lexical expectations (Gibson, 2006).
Forexample, in Demberg and Keller (2008), trialswere run deriving surprisal from the Roark (2001)parser under two different conditions: fully lex-icalized parsing, and fully unlexicalized parsing(to pre-terminal part-of-speech tags).
Boston etal.
(2008a) capture a similar distinction by mak-ing use of an unlexicalized PCFG within an Ear-ley parser and a fully lexicalized unlabeled depen-dency parser (Nivre et al, 2007).
As Demberg andKeller (2008) point out, fully unlexicalized gram-mars ignore important lexico-syntactic informa-tion when deriving the ?syntactic?
expectations,such as subcategorization preferences of particularverbs, which are generally accepted to impact syn-tactic expectations in human sentence processing(Garnsey et al, 1997).
Demberg and Keller argue,based on their results, for unlexicalized surprisalinstead of lexicalized surprisal.
Here we present anovel method for deriving separate syntactic andlexical surprisal measures from a fully lexicalizedincremental parser, to allow for rich probabilisticgrammars to be used to derive either measure, anddemonstrate the utility of this method versus thatof Demberg and Keller in empirical trials.The use of large-scale lexicalized grammarspresents a problem for using an Earley parser toderive surprisal or for the calculation of entropy asHale (2003; 2006) defines it, because both meth-ods require matrix inversion of a matrix with di-mensionality the size of the non-terminal set.
Withvery large lexicalized PCFGs, the size of the non-terminal set is too large for tractable matrix in-version.
The use of an incremental, beam-searchparser provides a tractable approximation to bothmeasures.
Incremental top-down and left-cornerparsers have been shown to effectively (and effi-ciently) make use of non-local features from theleft-context to yield very high accuracy syntacticparses (Roark, 2001; Henderson, 2003; Collinsand Roark, 2004), and we will use such rich mod-els to derive our scores.In addition to teasing apart syntactic and lexicalsurprisal (defined explicitly in ?3), we present anapproximation to the full entropy that Hale (2003;2006) used to define the entropy reduction hypoth-esis.
Such an entropy measure is derived via a pre-dictive step, advancing the parses independentlyof the input, as described in ?3.3.
We also presentsyntactic and lexical alternatives for this measure,and demonstrate the utility of making such a dis-tinction for entropy as well as surprisal.The purpose of this paper is threefold.
First,to present a careful and well-motivated decompo-sition of lexical and syntactic expectation-basedmeasures from a given lexicalized PCFG.
Sec-ond, to explicitly document methods for calculat-ing these and other measures from a specific in-cremental parser.
And finally, to present some em-pirical validation of the novel measures from realreading time trials.
We modified the Roark (2001)parser to calculate the discussed measures1, andthe empirical results in ?4 show several things,including: 1) using a fully lexicalized parser tocalculate syntactic surprisal and entropy provideshigher predictive utility for reading times thanthese measures calculated via unlexicalized pars-ing (as in Demberg and Keller); and 2) syntacticentropy is a useful predictor of reading time.2 Notation and preliminariesA probabilistic context-free grammar (PCFG)G = (V, T, S?, P, ?)
consists of a set of non-terminal variables V ; a set of terminal items(words) T ; a special start non-terminal S??
V ;a set of rule productions P of the form A ?
?for A ?
V , ?
?
(V ?
T )?
; and a function ?that assigns probabilities to each rule in P suchthat for any given non-terminal symbol X ?
V ,???
(X ?
?)
= 1.For a given rule A ?
?
?
P , let the func-tion RHS return the right-hand side of the rule, i.e.,RHS(A ?
?)
= ?.
Without loss of generality, wewill assume that for every rule A ?
?
?
P , oneof two cases holds: either RHS(A ?
?)
?
T orRHS(A ?
?)
?
V?.
That is, the right-hand sidesequences consist of either (1) exactly one termi-nal item, or (2) zero or more non-terminals.Let W ?
Tnbe a terminal string of length n,i.e., W = W1.
.
.Wnand |W | = n. Let W [i, j]denote the substring beginning at word Wiandending at word Wjof the string.
Then W|W |is thelast word in the string, and W [1, |W |] is the stringas a whole.
Adjacent strings represent concate-nation, i.e., W [1, i]W [i+1, j] = W [1, j].
ThusW [1, i]w represents the string where Wi+1= w.We can define a ?derives?
relation (denoted?Gfor a given PCFG G) as follows: ?A?
?G??
?if and only if A ?
?
?
P .
A string W ?
T?is in the language of a grammar G if and onlyif S?+?GW , i.e., a sequence of one or morederivation steps yields the string from the start1The parser version will be made publicly available.325non-terminal.
A leftmost derivation begins withS?and each derivation step replaces the leftmostnon-terminal A in the yield with some ?
such thatA ?
?
?
P .
For a leftmost derivation S??
?G?,where ?
?
(V ?
T )?, the sequence of deriva-tion steps that yield ?
can be represented as atree, with the start symbol S?at the root, and the?yield?
sequence ?
at the leaves of the tree.
Acomplete tree has only terminal items in the yield,i.e., ?
?
T?
; a partial tree has some non-terminalitems in the yield.
With a leftmost derivation, theyield ?
= ??
partitions into an initial sequenceof terminals ?
?
T?followed by a sequence ofnon-terminals ?
?
V?.
For a complete derivation,?
= ; for a partial derivation ?
?
V+, i.e., one ormore non-terminals.
Let T (G,W [1, i]) be the setof complete trees with W [1, i] as the yield of thetree, given PCFG G.A leftmost derivation D consists of a sequenceof |D| steps.
Let Direpresent the ithstep inthe derivation D, and D[i, j] represent the subse-quence of steps in D beginning with Diand end-ing with Dj.
Note that D|D|is the last step inthe derivation, and D[1, |D|] is the derivation asa whole.
Each step Diin the derivation is a rulein G, i.e., Di?
P for all i.
The probability of thederivation and the corresponding tree is:?
(D) =m?i=1?
(Di) (1)Let D(G,W [1, i]) be the set of all possible left-most derivations D (with respect to G) such thatRHS(D|D|) = Wi.
These are the set of partial left-most derivations whose last step used a productionwith terminal Wion the right-hand side.
The pre-fix probability of W [1, i] with respect to G isPrefixProbG(W [1, i]) =?D?D(G,W [1,i])?
(D) (2)From this prefix probability, we can calculate theconditional probability of each word w ?
T in theterminal vocabulary, given the preceding sequenceW [1, i] as follows:PG(w | W [1, i]) =PrefixProbG(W [1, i]w)Pw?
?TPrefixProbG(W [1, i]w?
)=PrefixProbG(W [1, i]w)PrefixProbG(W [1, i])(3)This, in fact, is precisely the conditional proba-bility that is used for language modeling for suchapplications as speech recognition and machinetranslation, which was the motivation for varioussyntactic language modeling approaches (Jelinekand Lafferty, 1991; Stolcke, 1995; Chelba and Je-linek, 1998; Roark, 2001).As with language modeling, it is important tomodel the end of the string as well, usually withan explicit end symbol, e.g., </s>.
For a stringW [1, i], we can calculate its prefix probability asshown above.
To calculate its complete probabil-ity, we must sum the probabilities over the set ofcomplete trees T (G,W [1, i]).
In such a way, wecan calculate the conditional probability of endingthe string with </s> given W [1, i] as follows:PG(</s> | W [1, i]) =?D?T (G,W [1,i])?
(D)PrefixProbG(W [1, i])(4)2.1 Incremental top-down parsingIn this section, we review relevant details ofthe Roark (2001) incremental top-down parser,as configured for use here.
As presented inRoark (2004), the probabilities in the PCFG aresmoothed so that the parser is guaranteed not tofail due to garden pathing, despite following abeam search strategy.
Hence there is always a non-zero prefix probability as defined in Eq.
2.The parser follows a top-down leftmost deriva-tion strategy.
The grammar is factored so that ev-ery production has either a single terminal item onthe right-hand side or is of the form A ?
B A-B,where A,B ?
V and the factored A-B categorycan expand to any sequence of children categoriesof A that can follow B.
This factorization of n-ary productions continues to nullary factored pro-ductions, i.e., the end of the original productionA ?
B1.
.
.
Bnis signaled with an empty produc-tion A-B1-.
.
.
-Bn?
.The parser maintains a set of possible connectedderivations, weighted via the PCFG.
It uses a beamsearch, whereby the highest scoring derivationsare worked on first, and derivations that fall out-side of the beam are discarded.
The reader is re-ferred to Roark (2001; 2004) for specifics aboutthe beam search.The model conditions the probability of eachproduction on features extracted from the par-tial tree, including non-local node labels such asparents, grandparents and siblings from the left-context, as well as c-commanding lexical items.Hence this is a lexicalized grammar, though theincremental nature precludes a general head-firststrategy, rather one that looks to the left-contextfor c-commanding lexical items.To avoid some of the early prediction of struc-ture, the version of the Roark parser that we used326performs an additional grammar transformationbeyond the simple factorization already described?
a selective left-corner transform of left-recursiveproductions (Johnson and Roark, 2000).
In thetransformed structure, slash categories are used toavoid predicting left-recursive structure until someexplicit indication of modification is present, e.g.,a preposition.The final step in parsing, following the last wordin the string, is to ?complete?
all non-terminalsin the yield of the tree.
All of these open non-terminals are composite factored categories, suchas S-NP-VP, which are ?completed?
by rewritingto .
The probability of these  productions is whatallows for the calculation of the conditional prob-ability of ending the string, shown in Eq.
4.One final note about the size of the non-terminalset and the intractability of exact inference forsuch a scenario.
The non-terminal set not onlyincludes the original atomic non-terminals of thegrammar, but also any categories created by gram-mar factorization (S-NP) or the left-corner trans-form (NP/NP).
Additionally, however, to remaincontext-free, the non-terminal set must includecategories that incorporate non-local features usedby the statistical model into their label, includ-ing parents, grandparents and sibling categories inthe left-context, as well as c-commanding lexicalheads.
These non-local features must be made lo-cal by encoding them in the non-terminal labels,leading to a very large non-terminal set and in-tractable exact inference.
Heavy smoothing is re-quired when estimating the resulting PCFG.
Thebenefit of such a non-terminal set is a rich model,which enables a more peaked statistical distribu-tion around high quality syntactic structures andthus more effective pruning of the search space.The fully connected left-context produced by top-down derivation strategies provides very rich fea-tures for the stochastic parsing models.
See Roark(2001; 2004) for discussion of these issues.We now turn to measures that can be derivedfrom the parser which may be of use for psycholin-guistic modeling.3 Parser and grammar derived measures3.1 SurprisalThe surprisal at word Wiis the negative log prob-ability of Wigiven the preceding words.
Usingprefix probabilities, this can be calculated as:SG(Wi) = ?
logPrefixProbG(W [1, i])PrefixProbG(W [1, i?
1])(5)Substituting equation 2 into this, we getSG(Wi) = ?
log?D?D(G,W [1,i])?
(D)?D?D(G,W [1,i?1])?
(D)(6)If we are using a beam-search parser, some of thederivations are pruned away.
Let B(G,W [1, i]) ?D(G,W [1, i]) be the set of derivations in thebeam.
Then the surprisal can be approximated asSG(Wi) ?
?
log?D?B(G,W [1,i])?
(D)?D?B(G,W [1,i?1])?
(D)(7)Any pruning in the beam search will result in a de-ficient probability distribution, i.e., a distributionthat sums to less than 1.
Roark?s thesis (2001)showed that the amount of probability mass lostfor this particular approach is very low, hence thisprovides a very tight bound on the actual surprisalgiven the model.3.2 Lexical and Syntactic surprisalHigh surprisal scores result when the prefix proba-bility at word Wiis low relative to the prefix prob-ability at word Wi?1.
Sometimes this is due to theidentity of Wi, i.e., it is a surprising word giventhe context.
Other times, it may not be the lexicalidentity of the word so much as the syntactic struc-ture that must be created to integrate the word intothe derivations.
One would like to tease surprisalapart into ?syntactic surprisal?
versus ?lexical sur-prisal?, which would capture this intuition of thelexical versus syntactic dimensions to the score.Our solution to this has the beneficial property ofproducing two scores whose sum equals the origi-nal surprisal score.The original surprisal score is calculated viasets of partial derivations at the point when eachword Wiis integrated into the syntactic structure,D(G,W [1, i]).
We then calculate the ratio frompoint to point in sequence.
To tease apart the lexi-cal and syntactic surprisal, we will consider sets ofpartial derivations immediately before each wordWiis integrated into the syntactic structure, i.e.,D[1, |D|?1] for D ?
D(G,W [1, i]).
Recall thatthe last derivation move for every derivation in theset is from the POS-tag to the lexical item.
Hencethe sequence of derivation moves that excludes thelast one includes all structure except the word Wi.Then the syntactic surprisal is calculated as:SynSG(Wi) = ?
logPD?D(G,W [1,i])?
(D[1, |D|?1])PD?D(G,W [1,i?1])?
(D)(8)327and the lexical surprisal is calculated as:LexSG(Wi) = ?
logPD?D(G,W [1,i])?
(D)PD?D(G,W [1,i])?
(D[1, |D|?1])(9)Note that the numerator of SynSG(Wi) is the de-nominator of LexSG(Wi), hence they sum to formtotal surprisal SG(Wi).
As with total surprisal,these measures can be defined either for the fullset D(G,W [1, i]) or for a pruned beam of deriva-tions B(G,W [1, i]) ?
D(G,W [1, i]).Finally, we replicated the Demberg and Keller(2008) ?unlexicalized?
surprisal by replacing ev-ery lexical item in the training corpus with itsPOS-tag, and then parsing the POS-tags of the lan-guage samples rather than the words.
This differsfrom our syntactic surprisal by having no lexicalconditioning events for rule probabilities, and byhaving no ambiguity about the POS-tag of the lex-ical items in the string.
We will refer to the result-ing surprisal measure as ?POS surprisal?
to distin-guish it from our syntactic surprisal measure.3.3 EntropyEntropy scores of the sort advocated by Hale(2003; 2006) involve calculation over the set ofcomplete derivations consistent with the set of par-tial derivations.
Hale performs this calculationefficiently via matrix inversion, which explainsthe use of relatively small-scale grammars withtractably sized non-terminal sets.
Such methodsare not tractable for the kinds of richly condi-tioned, large-scale PCFGs that we advocate usinghere.
At each word in the string, the Roark (2001)top-down parser provides access to the weightedset of partial analyses in the beam; the set of com-plete derivations consistent with these is not im-mediately accessible, hence additional work is re-quired to calculate such measures.Let H(D) be the entropy over a set of deriva-tions D, calculated as follows:H(D) = ?XD?D?(D)PD??D?(D?)log?(D)PD??D?(D?
)(10)If the set of derivations D = D(G,W [1, i])is a set of partial derivations for string W [1, i],then H(D) is a measure of uncertainty over thepartial derivations, i.e., the uncertainty regardingthe correct analysis of what has already been pro-cessed.
This can be calculated directly from theexisting parser operations.
If the set of derivationsare the complete derivations consistent with the setof partial derivations ?
complete derivations thatcould occur over the set of possible continuationsof the string ?
then this is a measure of the un-certainty about what is yet to come.
We wouldlike measures that can capture this distinction be-tween (a) uncertainty of what has already beenprocessed (?current ambiguity?)
versus (b) uncer-tainty of what is yet to be processed (?predictiveentropy?).
In addition, as with surprisal, we wouldlike to tease apart the syntactic uncertainty versuslexical uncertainty.To calculate the predictive entropy after wordsequence W [1, i], we modify the parser as fol-lows: the parser extends the set of partial deriva-tions to include all possible next words (the entirevocabulary plus </s>), and calculates the entropyover that set.
This measure is calculated from justone additional word beyond the current word, andhence is an approximation to Hale?s conditionalentropy of grammatical continuations, which isover complete derivations.
We will denote this asH1G(W [1, i]) and calculate it as follows:H1G(W [1, i]) = H(?w?T?
{</s>}D(G,W [1, i]w)) (11)This is performing a predictive step that the base-line parser does not perform, extending the parsesto all possible next words.Unlike surprisal, entropy does not decomposestraightforwardly into syntactic and lexical com-ponents that sum to the original composite mea-sure.
To tease apart entropy due to syntactic un-certainty versus that due to lexical uncertainty, wecan define the set of derivations up to the pre-terminal (POS-tag) non-terminals as follows.
LetS(D) = {D[1, |D|?1] : D ?
D}, i.e., the set ofderivations achieved by removing the last step ofall derivations inD.
Then we can calculate a ?syn-tactic?
H1Gas follows:SynH1G(W [1, i]) = H([w?T?
{</s>}S(D(G,W [1, i]w))) (12)Finally, ?lexical?
H1Gis defined in terms of theconditional probabilities derived from prefix prob-abilities as defined in Eq.
3.LexH1G(W [1, i]) =?Xw?T?
{</s>}PG(w | W [1, i]) logPG(w | W [1, i]) (13)As a practical matter, these values are calculatedwithin the Roark parser as follows.
A ?dummy?word is created that can be assigned every POS-tag, and the parser extends from the current state tothis dummy word.
(The beam threshold is greatly328expanded to allow for many possible extensions.
)Then every word in the vocabulary is substitutedfor the word, and the appropriate probabilities cal-culated over the beam.
Finally, the actual nextword is substituted, the beam threshold is reducedto the actual working threshold, and the requisitenumber of analyses are advanced to continue pars-ing the string.
This represents a significant amountof additional work for the parser ?
particularly forvocabulary sizes that we currently use, on the or-der of tens of thousands of words.As with surprisal, we can calculate an ?unlex-icalized?
version of the measure by training andparsing just to POS-tags.
We will refer to this sortof entropy as ?POS entropy?.4 Empirical validation4.1 Subjects and stimuliIn order to test the psycholinguistic relevance ofthe different measures produced by the parser, weconducted a word by word reading experiment.23 native speakers of English read 4 short texts(mean length: 883.5 words, 49.25 sentences).
Thetexts were the written versions of narratives usedin a parallel fMRI experiment making use of thesame parser derived measures and whose resultswill be published in a different paper (Bachrach etal., 2009).
The narratives contained a high densityof syntactically complex structures (in the form ofsentential embeddings, relative clauses and othernon-local dependencies) but were constructed soas to appear highly natural.
The modified versionof the Roark parser, trained on the Brown Cor-pus section of the Penn Treebank (Marcus et al,1993), was used to parse the different narrativesand produce the word by word measures.4.2 ProcedureEach narrative was presented line by line (cer-tain sentences required more than one line) on acomputer screen (Dell Optiplex 755 running Win-dows XP Professional) using Linger 2.882.
Eachline contained 11.5 words on average.
Each wordwould appear in its relative position on the screen.The subject would then be required to push a key-board button to advance to the next word.
Theoriginal word would then disappear and the fol-lowing word appear in the subsequent position onthe screen.
After certain sentences a comprehen-sion question would appear on the screen (10 pernarrative).
This was done in order to encourage2http://tedlab.mit.edu/?dr/Linger/readme.htmlsubjects to pay attention and to provide data for apost-hoc evaluation of comprehension.
After eachnarrative, subjects were instructed to take a shortbreak (2 minutes on average).4.3 Data analysisThe log (base 10) of the reaction times were ana-lyzed using a linear mixed effects regression anal-ysis implemented in the language R (Bates et al,2008).
Reaction times longer than 1500 ms andshorter than 150 ms (raw) were excluded from theanalysis (4.8% of total data).
Since button press la-tencies inferior to 150 ms must have been plannedprior to the presentation of the word, we consid-ered that they could not reflect stimulus driven ef-fects.
Data from the first and last words on eachline were discarded.The combined data from the 4 narratives wasfirst modeled using a model which included or-der of word in the narrative3, word length, parser-derived lexical surprisal, unigram frequency, bi-gram probability, syntactic surprisal, lexical en-tropy, syntactic entropy and mean number ofparser derivation steps as numeric regressors.
Wealso included the unlexicalized POS variants ofsyntactic surprisal and entropy, along the lines ofDemberg and Keller (2008), as detailed in ?
3.Table 1 presents the correlations between thesemean-centered measures.In addition, we modeled word class(open/closed) as a categorical factor in orderto assess interaction between class and the vari-ables of interest, since such an interaction hasbeen observed in the case of frequency (Bradley,1983).
Finally, the random effect part of themodel included intercepts for subjects, words andsentences.
We report significant effects at thethreshold p < .05.Given the presence of significant interactionsbetween lexical class (open/closed) and a numberof the variables of interests, we decided to splitthe data set into open and closed class words andmodel these separately (linear mixed effects withthe same numeric variables as in the full model).In order to evaluate the usefulness of splittingtotal surprisal into lexical and syntactic compo-nents we compared, using a likelihood ratio test,a model where lexical and syntactic surprisal aremodeled as distinct regressors to a model where asingle regressor equal to their sum (total surprisal)3This is a regressor to control for the trend of subjects toread faster later in the narrative.329Predictor SynH LexH SynS LexS Freq Bgrm PosS PosH Step WLenSyntactic Entropy (SynH) 1.00 -0.26 0.00 0.24 -0.24 0.20 0.02 0.55 -0.05 0.18Lexical Entropy (LexH) -0.26 1.00 0.01 -0.40 0.43 -0.38 -0.03 0.02 0.11 -0.29Syntactic Surprisal (SynS) 0.00 0.01 1.00 -0.12 0.08 0.18 0.77 0.21 0.38 -0.03Lexical Surprisal (LexS) 0.24 -0.40 -0.12 1.00 -0.81 0.87 -0.10 -0.20 -0.35 0.64Unigram Frequency (Freq) -0.24 0.43 0.08 -0.81 1.00 -0.69 0.02 0.18 0.31 -0.72Bigram Probability (Bgrm) 0.20 -0.38 0.18 0.87 -0.69 1.00 0.11 -0.11 -0.16 0.56POS Surprisal (PosS) 0.02 -0.03 0.77 -0.10 0.02 0.11 1.00 0.22 0.32 0.02POS Entropy (PosH) 0.55 0.02 0.21 -0.20 0.18 -0.11 0.22 1.00 0.16 -0.11Derivation steps (Step) -0.05 0.11 0.38 -0.35 0.31 -0.16 0.32 0.16 1.00 -0.24Word Length (WLen) 0.18 -0.29 -0.03 0.64 -0.72 0.56 0.02 -0.11 -0.24 1.00Table 1: Correlations between (mean-centered) predictors.
Note that unigram frequencies were represented as logs, otherscores as negative logs, hence the sign of the correlations.was included.
If the larger model provides a sig-nificantly better fit than the smaller model, thisprovides evidence that distinguishing between lex-ical and syntactic contributions to surprisal is rel-evant.
Since total entropy is not a sum of syntacticand lexical entropy, an analogous test would notbe valid in that case.4.4 ResultsAll subjects successfully answered the com-prehension questions (92.8% correct responses,S.D.=5.1).
In the full model, we observed signifi-cant main effects of word class as well as of lexicalsurprisal, bigram probability, unigram frequency,syntactic entropy, POS entropy and of order in thenarrative.
Syntactic surprisal, lexical entropy andnumber of steps had no significant effect.
Wordlength also had no significant main effect but inter-acted significantly with word class (open/closed).Word class also interacted significantly with lexi-cal surprisal, unigram frequency and syntactic sur-prisal.The presence of these interactions led us toconstruct models restricted to open and closedclass items respectively.
The estimated parame-ters are reported in Table 2.
Reading time for openclass words showed significant effects of unigramfrequency, syntactic surprisal, syntactic entropy,POS entropy and order within the narrative.
Thepositive effect of length approached significance.Reading time for closed class words exhibited sig-nificant effects of lexical surprisal, bigram prob-ability, syntactic entropy and order in the narra-tive.
Length had a non-significant negative effect,thus explaining the interaction observed in the fullmodel.The models with separate lexical and syntac-tic surprisal performed better than models includ-ing combined surprisal.
For open class words, theAkaike?s information criterion (AIC) was -54810for the combined model and -54819 for the inde-pendent model (likelihood ratio test comparing theEstimate Std.
Error t-valueOpen-class(Intercept) 2.40?10+002.39?10?02100.4*Lexical Surprisal -1.99?10?047.28?10?04-0.3Word Length 8.97?10?044.62?10?041.9Bigram 4.18?10?045.27?10?040.8Unigram Freq -2.43?10?031.20?10?03-2.0*Derivation Steps -1.17?10?039.02?10?04-1.3Syntactic Entropy 2.55?10?036.19?10?044.1*Lexical Entropy 3.96?10?046.68?10?040.6Syntactic Surprisal 3.28?10?039.71?10?043.4*Order in narrative -1.43?10?054.34?10?06-3.3*POS Surprisal -6.84?10?048.11?10?04-0.8POS Entropy 1.47?10?036.05?10?042.4*Closed-class(Intercept) 2.42?10+002.32?10?02104.3*Lexical Surprisal 2.02?10?037.84?10?042.6*Word Length -1.87?10?031.13?10?03-1.7Bigram 1.19?10?034.94?10?042.4*Unigram Freq 1.69?10?032.67?10?030.6Derivation Steps 3.01?10?045.09?10?040.6Syntactic Entropy 3.15?10?035.05?10?046.2*Lexical Entropy 1.83?10?048.63?10?040.2Syntactic Surprisal 3.00?10?048.35?10?040.4Order in narrative -1.33?10?053.99?10?06-3.3*POS Surprisal -6.46?10?046.81?10?04-0.9POS Entropy 6.63?10?045.04?10?041.3Table 2: Estimated effects from mixed effects models onopen and closed items (stars denote significance at p<.05)two, nested, models: ?2(1)=10.7, p<.001).
Forclosed class items, combined model?s AIC was -61467 and full model?s AIC was -61469 (likeli-hood ratio test: ?2(1)=3.54, p=0.06).4.5 DiscussionOur results demonstrate the relevance of model-ing psycholinguistic processes using an incremen-tal probabilistic parser, and the utility of the novelmeasures presented here.
Of particular interestare: the significant effects of our syntactic en-tropy measure; the independent contributions oflexical surprisal, bigram probability and unigramfrequency; and the differences between the pre-dictions of the lexicalized parsing model and theunlexicalized (POS) parsing model.The effect of entropy, or uncertainty regarding330the upcoming input independent of the surpriseof that input, has been observed in non-linguistictasks (Hyman, 1953; Bestmann et al, 2008) butto our knowledge has not been quantified beforein the context of sentence processing.
The use-fulness of computational modeling is particularlyevident in the case of entropy given the absence ofany subjective procedure for its evaluation4.
Theresults argue in favor of a predictive parsing archi-tecture (Van Berkum et al, 2005).
The approachto entropy here differs from the one described inHale (2006) in a couple of ways.
First, as dis-cussed above, the calculation procedure is differ-ent ?
we focus on extending the derivations withjust one word, rather than to all possible completederivations.
Second, and most importantly, Haleemphasizes entropy reduction (or the gain in in-formation, given an input, regarding the rest of thesentence) as the correlate of cognitive cost whilehere we are interested in the amount of entropy it-self (and not the size of change).Interestingly, we observed only an effect of syn-tactic entropy, not lexical entropy.
Recent ERPwork has demonstrated that subjects do form spe-cific lexical predictions in the context of sentenceprocessing (Van Berkum et al, 2005; DeLong etal., 2005) and so we suspect that the absence oflexical entropy effect might be partly due to sparsedata.
Lexical surprisal and entropy were calcu-lated using the internal state of a parser trainedon the relatively small Brown corpus.
Lexical en-tropy showed no significant effect while lexicalsurprisal affected only closed class words.
Thispattern of results might be due to the sparsenessof the relevant information in such a small corpus(e.g., verb/object preferences) and the relevance ofextra-textual dimensions (world knowledge, con-textual information) to lexical-specific prediction.Closed class words are both more frequent (andhence better sampled) and are less sensitive toworld knowledge, yet are often determined by thegrammatical context.Demberg and Keller (2008) made use of thesame parsing architecture used here to compute asyntactic surprisal measure, but used an unlexical-ized parser (down to POS-tags rather than words)for this score.
Their ?lexicalized?
surprisal isequivalent to our total surprisal (lexical surprisal+ syntactic surprisal), while their POS surprisal is4The Cloze procedure (Taylor, 1953) is one way to deriveprobabilities that could be used to calculate entropy, thoughthis procedure is usually conducted with lexical elicitation,which would make syntactic entropy calculations difficult.derived from a completely different model.
In con-trast, our approach achieves lexical and syntacticmeasures from the same model.
In order to eval-uate the difference between the two approacheswe added unlexicalized POS surprisal calculatedalong the lines of that paper to our model, alongwith an unlexicalized POS entropy from the samemodel.
We found no effect of unlexicalized POSsurprisal5and a significant (but relatively small)effect of unlexicalized POS entropy.
While syn-tactic surprisal was correlated with POS surprisal(see Table 1) and syntactic entropy correlated withPOS entropy, the fact that our syntactic measuresstill had a significant effect suggests that lexicalinformation contributes towards the formation ofsyntactic expectations.While the effect of surprisal calculated by anincremental top down parser has been alreadydemonstrated (Demberg and Keller, 2008), our re-sults argue for a distinction between the effectof lexical surprisal and that of syntactic surprisalwithout requiring unlexicalized parsing of the sortthat Demberg and Keller advocate.
It is importantto keep in mind that this distinction between typesof prediction (and as a consequence, prediction er-ror) is not equivalent to the one drawn in the tradi-tional cognitive science modularity debate, whichhas focused on the source of these predictions.
Wefound a positive effect of syntactic surprisal in thecase of open class words.
The absence of an effectfor closed class words remains to be explained.We quantified word specific surprisal using 3sources: the parser?s internal state (lexical sur-prisal); probability given the preceding word (neg-ative log bigram probability); and the unigram fre-quency of the word in a large corpus6.
As canbe observed in Table 1, these three measures arehighly correlated7.
This is the consequence of thesmoothing in the estimation procedure but also re-lates to a more general fact about language use:overall, more frequent words are also words moreexpected to appear in a specific context (Andersonand Schooler, 1991).
Despite these strong corre-lations, the three measures produced independent5We also ran the model including unlexicalized POS sur-prisal without our syntactic surprisal or syntactic entropy, andin this condition the unlexicalized POS surprisal measure hada nearly significant effect (t = 1.85), which is consistent withthe results in Boston et al (2008a) and Demberg and Keller(2008).6The unigram frequencies came from the HAL corpus(Lund and Burgess, 1996).
All other statistical models wereestimated from the Brown Corpus.7Unigram frequencies were represented as logs, the othersas negative logs, hence the sign of the correlations.331effects.
Unigram frequency had a significant effectfor open class words while bigram probability andlexical surprisal each had an effect on reading timeof closed class items.
Bigram probability has beenoften found to affect reading time using eye move-ment measures.
This is the first study to demon-strate an additional effect of contextual surprisalgiven the preceding sentential context (lexical sur-prisal).
Demberg and Keller found no effect forsurprisal once bigram and unigram probabilitieswere included in the model but, importantly, theydid not distinguish lexical and syntactic surprisal,rather ?lexicalized?
and ?unlexicalized?
surprisal.5 SummaryWe have presented novel methods for teasing apartsyntactic and lexical surprisal from a fully lexi-calized parser, as well as for extending the oper-ation of a predictive parser to capture novel en-tropy measures that are also shown to be rele-vant to psycholinguistic modeling.
Such auto-matic methods provide psycholinguistically rele-vant measures that are intractable to calculate byhand.
The empirical validation presented heredemonstrated that the new measures ?
particularlysyntactic entropy and syntactic surprisal ?
havehigh utility for modeling human reading time data.Our approach to calculating syntactic surprisal,based on fully lexicalized parsing, provided sig-nificant effects, while the POS-tag based (unlexi-calized) surprisal ?
of the sort used in Boston etal.
(2008a) and Demberg and Keller (2008) ?
didnot provide a significant effect in our trials.
Fur-ther, we showed an effect of lexical surprisal forclosed class words even when combined with uni-gram and bigram probabilities in the same model.This work contributes to the important, develop-ing enterprise of leveraging data-driven NLP ap-proaches to derive new measures of high utility forpsycholinguistic and neuropsychological studies.AcknowledgmentsThanks to Michael Collins, John Hale and ShravanVasishth for valuable discussions about this work.This research was supported in part by NSF Grant#BCS-0826654.
Any opinions, findings, conclu-sions or recommendations expressed in this publi-cation are those of the authors and do not neces-sarily reflect the views of the NSF.ReferencesJ.R.
Anderson and L.J.
Schooler.
1991.
Reflections ofthe environment in memory.
Psychological Science,2(6):396?408.A.
Bachrach, B. Roark, A. Marantz, S. Whitfield-Gabrieli, C. Cardenas, and J.D.E.
Gabrieli.
2009.Incremental prediction in naturalistic language pro-cessing: An fMRI study.
In preparation.D.
Bates, M. Maechler, and B. Dai, 2008. lme4: Linearmixed-effects models using S4 classes.
R packageversion 0.999375-20.S.
Bestmann, L.M.
Harrison, F. Blankenburg, R.B.Mars, P. Haggard, and K.J.
Friston.
2008.
Influenceof uncertainty and surprise on human corticospinalexcitability during preparation for action.
CurrentBiology, 18:775?780.M.
Ferrara Boston, J.T.
Hale, R. Kliegl, U. Patil, andS.
Vasishth.
2008a.
Parsing costs as predictorsof reading difficulty: An evaluation using the Pots-dam sentence corpus.
Journal of Eye Movement Re-search, 2(1):1?12.M.
Ferrara Boston, J.T.
Hale, R. Kliegl, and S. Va-sishth.
2008b.
Surprising parser actions and read-ing difficulty.
In Proceedings of ACL-08:HLT, ShortPapers, pages 5?8.D.C.
Bradley.
1983.
Computational Distinctions ofVocabulary Type.
Indiana University LinguisticsClub, Bloomington.C.
Chelba and F. Jelinek.
1998.
Exploiting syntacticstructure for language modeling.
In Proceedings ofACL-COLING, pages 225?231.M.J.
Collins and B. Roark.
2004.
Incremental parsingwith the perceptron algorithm.
In Proceedings ofACL, pages 111?118.K.A.
DeLong, T.P.
Urbach, and M. Kutas.
2005.
Prob-abilistic word pre-activation during language com-prehension inferred from electrical brain activity.Nature Neuroscience, 8(8):1117?1121.V.
Demberg and F. Keller.
2008.
Data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity.
Cognition, 109(2):193?210.J.
Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 6(8):451?455.L.
Frazier.
1985.
Syntactic complexity.
In D.R.Dowty, L. Karttunen, and A.M. Zwicky, editors,Natural Language Parsing.
Cambridge UniversityPress, Cambridge, UK.K.
Gabani, M. Sherman, T. Solorio, and Y. Liu.2009.
A corpus-based approach for the predictionof language impairment in monolingual English andSpanish-English bilingual children.
In Proceedingsof NAACL-HLT.S.M.
Garnsey, N.J. Pearlmutter, E. Myers, and M.A.Lotocky.
1997.
The contributions of verb bias andplausibility to the comprehension of temporarily am-biguous sentences.
Journal of Memory and Lan-guage, 37(1):58?93.332E.
Gibson.
1998.
Linguistic complexity: locality ofsyntactic dependencies.
Cognition, 68(1):1?76.E.
Gibson.
2006.
The interaction of top-down andbottom-up statistics in the resolution of syntacticcategory ambiguity.
Journal of Memory and Lan-guage, 54(3):363?388.J.T.
Hale.
2001.
A probabilistic Earley parser as apsycholinguistic model.
In Proceedings of the 2ndmeeting of NAACL.J.T.
Hale.
2003.
The information conveyed by wordsin sentences.
Journal of Psycholinguistic Research,32(2):101?123.J.T.
Hale.
2006.
Uncertainty about the rest of the sen-tence.
Cognitive Science, 30(4):643?672.J.
Henderson.
2003.
Inducing history representationsfor broad coverage statistical parsing.
In Proceed-ings of HLT-NAACL, pages 24?31.R.
Hyman.
1953.
Stimulus information as a determi-nant of reaction time.
Journal of Experimental Psy-chology: General, 45(3):188?96.F.
Jelinek and J. Lafferty.
1991.
Computation ofthe probability of initial substring generation bystochastic context-free grammars.
ComputationalLinguistics, 17(3):315?323.M.
Johnson and B. Roark.
2000.
Compact non-left-recursive grammars using the selective left-cornertransform and factoring.
In Proceedings of COL-ING, pages 355?361.T.
Klee and M.D.
Fitzgerald.
1985.
The relation be-tween grammatical development and mean lengthof utterance in morphemes.
Journal of Child Lan-guage, 12:251?269.R.
Levy.
2008.
Expectation-based syntactic compre-hension.
Cognition, 106(3):1126?1177.K.
Lund and C. Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-ments, & Computers, 28:203?208.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
K?ubler, S. Marinov, and E. Marsi.
2007.
Malt-parser: A language-independent system for data-driven dependency parsing.
Natural Language En-gineering, 13(2):95?135.B.
Roark, M. Mitchell, and K. Hollingshead.
2007.Syntactic complexity measures for detecting mildcognitive impairment.
In Proceedings of BioNLPWorkshop at ACL, pages 1?8.B.
Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.B.
Roark.
2004.
Robust garden path parsing.
NaturalLanguage Engineering, 10(1):1?24.K.
Sagae, A. Lavie, and B. MacWhinney.
2005.
Au-tomatic measurement of syntactic development inchild langugage.
In Proceedings of ACL, pages 197?204.T.
Solorio and Y. Liu.
2008.
Using language modelsto identify language impairment in Spanish-Englishbilingual children.
In Proceedings of BioNLP Work-shop at ACL, pages 116?117.A.
Stolcke.
1995.
An efficient probabilistic context-free parsing algorithm that computes prefix proba-bilities.
Computational Linguistics, 21(2):165?202.W.L.
Taylor.
1953.
Cloze procedure: A new toolfor measuring readability.
Journalism Quarterly,30:415?433.J.J.A.
Van Berkum, C.M.
Brown, P. Zwitserlood,V.Kooijman, and P. Hagoort.
2005.
Anticipat-ing upcoming words in discourse: Evidence fromERPs and reading times.
Learning and Memory,31(3):443?467.V.H.
Yngve.
1960.
A model and an hypothesis for lan-guage structure.
Proceedings of the American Philo-sophical Society, 104:444?466.333
