Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1851?1857,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsDetecting Promotional Content in WikipediaShruti Bhosale Heath Vinicombe Raymond J. MooneyDepartment of Computer ScienceThe University of Texas at Austin{shruti,vini,mooney}@cs.utexas.eduAbstractThis paper presents an approach for detectingpromotional content in Wikipedia.
By incor-porating stylometric features, including fea-tures based on n-gram and PCFG languagemodels, we demonstrate improved accuracyat identifying promotional articles, comparedto using only lexical information and meta-features.1 IntroductionWikipedia is a free, collaboratively edited encyclo-pedia.
Since normally anyone can create and editpages, some articles are written in a promotionaltone, violating Wikipedia?s policy requiring a neu-tral viewpoint.
Currently, such articles are identifiedmanually and tagged with an appropriate Cleanupmessage1 by Wikipedia editors.
Given the scale andrate of growth of Wikipedia, it is infeasible to man-ually identify all such articles.
Hence, we presentan approach to automatically detect promotional ar-ticles.Related work in quality flaw detection inWikipedia (Anderka et al 2012) has relied onmeta-features based on edit history, Wikipedia links,structural features and counts of words, sentencesand paragraphs.
However, we hypothesize that thereare subtle differences in the linguistic style that dis-tinguish promotional tone, which we attempt to cap-ture using stylometric features, particularly deepersyntactic features.
We model the style of promo-tional and normal articles using language models1http://en.wikipedia.org/wiki/Wikipedia:Template_messages/Cleanupbased on both n-grams and Probabilistic ContextFree Grammars (PCFGs).
We show that using suchstylometric features improves over using only shal-low lexical and meta-features.2 Related WorkAnderka et al(2012) developed a general model fordetecting ten of Wikipedia?s most frequent qualityflaws.
One of these flaw types, ?Advert?2, refers toarticles written like advertisements.
Their classifierswere trained using a set of lexical, structural, net-work and edit-history related features of Wikipediaarticles.
However, they used no features capturingsyntactic structure, at a level deeper than Part-Of-Speech (POS) tags.A related area is that of vandalism detection inWikipedia.
Several systems have been developedto detect vandalizing edits in Wikipedia.
These fallinto two major categories: those analyzing author in-formation and edit metadata (Wilkinson and Huber-man, 2007; Stein and Hess, 2007); and those usingNLP techniques such as n-gram language modelsand PCFGs (Wang and McKeown, 2010; Harpalaniet al 2011).
We combine relevant features fromboth these categories to train a classifier that distin-guishes promotional content from normal Wikipediaarticles.3 Dataset CollectionWe extracted a set of about 13,000 articles fromEnglish Wikipedia?s category, ?Category:All arti-2?Advert?
is the flaw-type of majority of the articles in theCategory ?Articles with a promotional tone?.1851Content FeaturesNumber of charactersNumber of wordsNumber of sentencesAverage Word LengthAverage, Minimum, Maximum Sentence Lengths,Ratio of Maximum to minimum sentence lengthsRatio of long sentences (>48 words) to Short Sen-tences (<33 words)Percentage of Sentences in the passive voiceRelative Frequencies of POS tags for pronouns, con-junctions, prepositions, auxiliary verbs, modal verbs,adjectives and adverbsPercentage of sentences beginning with a pronoun,article, conjunction, preposition, adjective, adverbPercentage of special phrases3 such as peacockterms (?legendary?, ?acclaimed?, ?world-class?
),weasel terms (?many scholars state?, ?it is be-lieved/regarded?, ?many are of the opinion?, ?mostfeel?, ?experts declare?, ?it is often reported?)
, edi-torializing terms (?without a doubt?, ?of course?, ?es-sentially?
)Percentage of easy words, difficult words (Dale-Chall List), long words and stop wordsOverall Sentiment Score based on SentiWordNet4Table 1: Content Features of a Wikipedia Articlecles with a promotional tone?
as a set of positiveexamples.
We extracted a set of 26,000 untaggedarticles to form a noisy set of negative examples,which may contain some promotional articles thathave not yet been tagged by Wikipedia editors.
Tocounter this noise, we repeated the experiment us-ing Wikipedia?s Featured Articles and Good Articles(approx.
11,000) as a set of clean negative exam-ples.
We used 70% of the articles in each categoryto train language models for each of the three cate-gories (promotional articles, featured/good articles,untagged articles), and used the remaining 30% toevaluate classifier performance using 10-fold cross-validation.4 Features4.1 Content and Meta Features of an ArticleWe used the content and meta features proposed byAnderka et al(2012) as given in Tables 1-4.
We also3http://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Words_to_watch4This feature is not included in Anderka et al(2012)Structural FeaturesNumber of SectionsNumber of ImagesNumber of CategoriesNumber of Wikipedia Templates usedNumber of References, Number of References persentence and Number of references per sectionTable 2: Structural Features of a Wikipedia ArticleWikipedia Network FeaturesNumber of Internal Wikilinks (to other Wikipediapages)Number of External Links (to other websites)Number of Backlinks (i.e.
Number of wikilinks fromother Wikipedia articles to an article)Number of Language Links (i.e.
Number of links tothe same article in other languages)Table 3: Network Features of a Wikipedia Articleadded a new feature, ?Overall Sentiment Score?
foran article.
This feature is the average of the senti-ment scores assigned by SentiWordnet (Baccianellaet al 2010) to all positive and negative sentimentbearing words in an article.
In total, this results in58 basic document features.4.2 N-Gram Language ModelsLanguage models are commonly used to measurestylistic differences in language usage between au-thors.
For this work, we employed them to modelthe difference in style of neutral vs. promotionalWikipedia articles.
We trained trigram word lan-guage models and trigram character language mod-els5 with Witten-Bell smoothing to produce proba-bilistic models of both classes.4.3 PCFG Language ModelsProbabilistic Context Free Grammars (PCFG) cap-ture the syntactic structure of language by mod-eling sentence generation using probabilistic CFGproductions.
We hypothesize that sentences in pro-motional articles and those in neutral articles tendto have different kinds of syntactic structures andtherefore, we explored the utility of PCFG modelsfor detecting this difference.
Since we do not haveground-truth parse trees for sentences in our dataset,5Modeling longer character sequences did not help.1852Features based on PCFG models and n-gram Language modelsDifference in the probabilities assigned to an article by the positive and the negative class character trigramlanguage models (LM char trigram)Difference in the probabilities assigned to an article by the positive and the negative class word trigram languagemodels (LM word trigram)Difference in the mean values of the probabilities assigned to sentences of an article by the positive and negativeclass PCFG models (PCFG mean)Difference in the maximum values of the probabilities assigned to sentences of an article by the positive andnegative class PCFG models (PCFG max)Difference in the minimum values of the probabilities assigned to sentences of an article by the positive andnegative class PCFG models (PCFG min)Difference in the standard deviation values of the probabilities of sentences of an article by the positive andnegative class PCFG models (PCFG std deviation)Table 5: Features of a Wikipedia Article based on PCFG models and n-gram Language modelsEdit History FeaturesAge of the articleDays since last revision of the articleNumber of edits to the articleNumber of unique editorsNumber of edits made by registered users and byanonymous IP addressesNumber of edits per editorPercentage of edits by top 5% of the top contributorsto the articleTable 4: Edit-History Features of a Wikipedia Articlewe followed the method of (Raghavan et al 2010;Harpalani et al 2011), which uses the output ofthe Stanford parser to train PCFG models for stylis-tic analysis.
We used the PCFG implementation ofKlein and Manning (2003) to learn a PCFG modelfor each category.4.4 ClassificationThe n-gram and PCFG language models were usedto create a set of additional document features.
Weused the probability assigned by the language mod-els to each sentence in a test document to calculatedocument-wide statistics such as the mean, maxi-mum, and minimum probability and standard devia-tion in probabilities of the set of sentences in an arti-cle.
The language-modeling features used are shownin Table 5.Since we have a wide variety of features, weexperimented with various ensemble learning tech-niques and found that LogitBoost performed bestempirically.
We used the Weka implementation ofLogitBoost (Friedman et al 2000) to train a classi-fier using various combinations of features.
We usedDecision Stumps as a base classifier and ran boost-ing for 500 iterations.5 Experimental Evaluation5.1 MethodologyWe used 10-fold cross-validation to test the perfor-mance of our classifier using various combinationsof features.
We ran the classifier on the portion(30%) of the dataset not used for language model-ing.6 We measured overall classification accuracyas well as precision, recall, F-measure, and area un-der the ROC curve for all experiments.
We testedperformance in two settings (Anderka et al 2012):?
Pessimistic Setting: The negative class consistsof articles from the Untagged set.
Since someof these could be manually undetected promo-tional articles, the accuracy measured in thissetting is probably an under-estimate.?
Optimistic Setting: The negative class consistsof articles from the Featured/Good set.
Thesearticles are at one end of the quality spectrum,making it relatively easier to distinguish themfrom promotional articles.The true performance of the classifier is likely some-where between that achieved in these two settings.6We maintain an equal number of positive and negative testcases in both the settings.1853FeaturesPessimistic Setting Optimistic SettingP R F1 AUC P R F1 AUCBag-of-words Baseline 0.823 0.820 0.821 0.893 0.931 0.931 0.931 0.979PCFG 0.881 0.870 0.865 0.903 0.910 0.910 0.910 0.961Character trigrams 0.889 0.887 0.888 0.952 0.858 0.843 0.841 0.877Word trigrams 0.863 0.863 0.863 0.931 0.887 0.883 0.882 0.931Character trigrams + Word trigrams 0.89 0.888 0.889 0.952 0.908 0.907 0.907 0.962PCFG+Char.
trigrams+Word trigrams 0.914 0.915 0.914 0.974 0.950 0.950 0.950 0.98358 Content and Meta Features 0.866 0.867 0.867 0.938 0.986 0.986 0.986 0.996All Features 0.940 0.940 0.940 0.986 0.989 0.989 0.989 0.997Table 6: Performance (Precision(P), Recall(R), F1 score, AUC) of the classifier in the two settings5.2 Results for Pessimistic SettingFrom Table 6, we see that all features performbetter than the bag-of-words baseline.
We alsosee that character trigrams, one of the simplestfeatures, gives the best individual performance.However, deeper syntactic features using PCFGsalso performs quite well.
Combining all of thelanguage-modeling features (PCFG + character tri-grams + Word trigrams) further improves perfor-mance.
Compared to the 58 content and meta fea-tures utilized by Anderka et al (2012) describedin Section 4.1, the PCFG and character trigram fea-tures give much better performance, both individu-ally and when combined.
It is interesting to notethat adding Anderka et als features to the language-modeling ones gives a fairly small improvement inperformance.
This validates our hypothesis that pro-motional articles tend to have a distinct linguisticstyle that is captured well using language models.5.3 Results for Optimistic SettingIn the Optimistic Setting, as shown in Table 6,the content and meta features give the best perfor-mance, which improves only slightly when com-bined with language-modeling features.
The bag-of-words baseline performs better than all the languagemodeling features.
This performance could be be-cause there is a much clearer distinction betweenpromotional articles and featured/good articles thatcan be captured by simple features alone.
For exam-ple, featured/good articles are generally longer thanusual Wikipedia articles and have more references.5.4 Top Ranked Features and theirPerformanceTo analyze the performance of different features, wedetermined the top ranked features using Informa-tion Gain.
In the Pessimistic Setting, the top sixfeatures are all language-modeling features (charac-ter trigram model feature works best), followed bybasic meta-features such as character count, wordcount, category count and sentence count.
The newfeature we introduced, ?Overall Sentiment Score?
isthe 18th most informative feature in the pessimisticsetting, indicating that the cumulative sentiment of abag of words is not as discriminative as we would in-tuitively assume.
Using the 10 top-ranked features,we get an F1 of 0.93, which is only slightly worsethan that achieved using all features (F1 = 0.94).In the Optimistic Setting, the top-ranked featuresare the number of references and the number ofreferences per section.
This is consistent with theobservation that featured/good articles have verylong and comprehensive lists of references, sinceWikipedia?s fundamental policy is to maintain ver-ifiability by citing relevant sources.
Features basedon the n-gram and PCFG models also appear in thelist of ten best features.
Using only the top 10 fea-tures, gives an F1 of 0.988, which is almost as goodas using all features (F1 = 0.989).5.5 Optimistic and Pessimistic SettingsIn the optimistic setting, there is a clear distinc-tion between the positive (promotional) and negative(featured/good) classes.
But there are only subtledifferences between the positive and negative (un-tagged articles) classes in the pessimistic setting.1854Best Features in Pessimistic Setting Best Features in Optimistic SettingLM char trigram Number of ReferencesLM word trigram Number of References per SectionPCFG min LM word trigramPCFG max Number of WordsPCFG mean PCFG meanPCFG std deviation Number of SentencesNumber of Characters LM char trigramNumber of Words Number of WordsNumber of Categories Number of CharactersNumber of Sentences Number of BacklinksTable 7: Top 10 Features (listed in order) in both Settings ranked using Information GainThese two classes are superficially similar, in termsof length, reference count, section count etc.
Stylo-metric features based on the trained language mod-els are successful at detecting the subtle linguisticdifferences in the two types of articles.
This is use-ful because the pessimistic setting is closer to thereal-world setting of articles in Wikipedia.5.6 Error AnalysisSince the pessimistic setting is close to the real set-ting of Wikipedia articles, it is useful to do an erroranalysis of the classifier?s performance in this set-ting.
There is an approximately equal proportion offalse positives and false negatives.A significant number of false positives seem tobe cases of manually undetected promotional arti-cles.
This demonstrates the practical utility of ourclassifier.
But there are also many false positivesthat seem to be truly unbiased.
These articles ap-pear to have been poorly written, without followingWikipedia?s editing policies.
Examples include useof very long lists of nouns, use of ambiguous termslike ?many believe?
and excessive use of superla-tives.
Other common characteristics of most of thefalse positives are presence of a considerable num-ber of complex sentences with multiple subordinateclauses.
These stylistic cues seem to be misleadingthe classifier.A common thread underlying most of the falsenegatives is the fact that they are written in a nar-rative style or they have excessive details in terms ofthe content.
Examples include narrating a detailedstory of a fictional character in an unbiased manneror writing a minutely detailed account of the historyof an organization.
Another source of false negativescomes from biographical Wikipedia pages which arewritten in a resume style, listing all their qualifi-cations and achievements.
These cues could helpone manually detect that the article, though not pro-motional in style, is probably written with the viewof promoting the entity.
As possible future work,we could incorporate features derived from languagemodels for narrative style trained using an appropri-ate external corpus of narrative text.
This might en-able the classifier to detect some cases of unbiasedpromotional articles.6 ConclusionOur experiments and analysis show that stylomet-ric features based on n-gram language models anddeeper syntactic PCFG models work very well fordetecting promotional articles in Wikipedia.
Af-ter analyzing the errors that are made during clas-sification, we realize that though promotional con-tent is non-neutral in majority of the cases, there doexist promotional articles that are neutral in style.Adding additional features based on language mod-els of narrative style could lead to a better model ofWikipedia?s promotional content.7 AcknowledgementsThis research was supported in part by the DARPADEFT program under AFRL grant FA8750-13-2-0026 and by MURI ARO grant W911NF-08-1-0242.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the author and do not necessarily reflect theview of DARPA, AFRL, ARO, or the US govern-ment.1855ReferencesMaik Anderka, Benno Stein, and Nedim Lipka.
2012.Predicting quality flaws in user-generated content: thecase of Wikipedia.
In Proceedings of the 35th Inter-national ACM SIGIR Conference on Research and de-velopment in Information Retrieval, SIGIR ?12, pages981?990, New York, NY, USA.
ACM.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In Proceedings of the 7th Conference on InternationalLanguage Resources and Evaluation (LREC?10), Val-letta, Malta, May.Joachim Diederich, Jo?rg Kindermann, Edda Leopold, andGerhard Paass.
2003.
Authorship attribution withsupport vector machines.
Applied Intelligence, 19(1-2):109?123.Hugo J Escalante, Thamar Solorio, and M Montes-yGo?mez.
2011.
Local histograms of character n-grams for authorship attribution.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,volume 1, pages 288?298.Rudolf Flesch.
1948.
A new readability yardstick.
TheJournal of Applied Psychology, 32(3):221.Jerome Friedman, Trevor Hastie, and Robert Tibshirani.2000.
Additive logistic regression: a statistical viewof boosting (with discussion and a rejoinder by the au-thors).
The Annals of Statistics, 28(2):337?407.Michael Gamon.
2004.
Linguistic correlates of style:Authorship classification with deep linguistic analy-sis features.
In Proceedings of the 20th InternationalConference on Computational Linguistics, page 611.Association for Computational Linguistics.Manoj Harpalani, Michael Hart, Sandesh Singh, RobJohnson, and Yejin Choi.
2011.
Language of van-dalism: Improving Wikipedia vandalism detection viastylometric analysis.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies: ShortPapers, volume 2, pages 83?88.Daniel Hasan Dalip, Marcos Andre?
Gonc?alves, MarcoCristo, and Pa?vel Calado.
2009.
Automatic qual-ity assessment of content created collaboratively byweb communities: a case study of Wikipedia.
In Pro-ceedings of the 9th ACM/IEEE-CS Joint Conferenceon Digital libraries, JCDL ?09, pages 295?304, NewYork, NY, USA.
ACM.Michael Heilman, Kevyn Collins-Thompson, and Max-ine Eskenazi.
2008.
An analysis of statistical modelsand features for reading difficulty prediction.
In Pro-ceedings of the Third Workshop on Innovative Use ofNLP for Building Educational Applications, pages 71?79.
Association for Computational Linguistics.Vlado Kes?elj, Fuchun Peng, Nick Cercone, and CalvinThomas.
2003.
N-gram-based author profiles for au-thorship attribution.
In Proceedings of the ConferencePacific Association for Computational Linguistics, PA-CLING, volume 3, pages 255?264.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Associ-ation for Computational Linguistics.Emily Pitler, Annie Louis, and Ani Nenkova.
2010.Automatic evaluation of linguistic quality in multi-document summarization.
In Proceedings of the 48thannual meeting of the Association for ComputationalLinguistics, pages 544?554.
Association for Computa-tional Linguistics.Sindhu Raghavan, Adriana Kovashka, and RaymondMooney.
2010.
Authorship attribution using proba-bilistic context-free grammars.
In Proceedings of theACL 2010 Conference Short Papers, ACLShort ?10,pages 38?42, Stroudsburg, PA, USA.
Association forComputational Linguistics.Congzhou He Ramyaa and Khaled Rasheed.
2004.
Us-ing machine learning techniques for stylometry.
InProceedings of International Conference on MachineLearning.Paul Rayson, Andrew Wilson, and Geoffrey Leech.2001.
Grammatical word class variation within thebritish national corpus sampler.
Language and Com-puters, 36(1):295?306.Klaus Stein and Claudia Hess.
2007.
Does it matter whocontributes: a study on featured articles in the GermanWikipedia.
In Proceedings of the Eighteenth Confer-ence on Hypertext and Hypermedia, pages 171?174.ACM.Kristina Toutanova and Christopher D Manning.
2000.Enriching the knowledge sources used in a maximumentropy part-of-speech tagger.
In Proceedings of the2000 Joint SIGDAT Conference on Empirical Methodsin Natural Language Processing and Very Large Cor-pora: held in conjunction with the 38th Annual Meet-ing of the Association for Computational Linguistics-Volume 13, pages 63?70.
Association for Computa-tional Linguistics.Kristina Toutanova, Dan Klein, Christopher D Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology-Volume 1,pages 173?180.
Association for Computational Lin-guistics.1856William Yang Wang and Kathleen R. McKeown.
2010.?Got you!?
: Automatic vandalism detection inWikipedia with web-based shallow syntactic-semanticmodeling.
In Proceedings of the 23rd InternationalConference on Computational Linguistics, COLING?10, pages 1146?1154, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Dennis M Wilkinson and Bernardo A Huberman.
2007.Assessing the value of coooperation in Wikipedia.arXiv preprint cs/0702140.1857
