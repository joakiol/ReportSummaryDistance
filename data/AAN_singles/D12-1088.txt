Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 962?971, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsEntropy-based Pruning for Phrase-based Machine TranslationWang Ling, Joa?o Grac?a, Isabel Trancoso, Alan BlackL2F Spoken Systems Lab, INESC-ID, Lisboa, PortugalLanguage Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA{wang.ling,joao.graca,isabel.trancoso}@inesc-id.ptawb@cs.cmu.eduAbstractPhrase-based machine translation modelshave shown to yield better translations thanWord-based models, since phrase pairs en-code the contextual information that is neededfor a more accurate translation.
However,many phrase pairs do not encode any rele-vant context, which means that the transla-tion event encoded in that phrase pair is ledby smaller translation events that are indepen-dent from each other, and can be found onsmaller phrase pairs, with little or no loss intranslation accuracy.
In this work, we pro-pose a relative entropy model for translationmodels, that measures how likely a phrase pairencodes a translation event that is derivableusing smaller translation events with similarprobabilities.
This model is then applied tophrase table pruning.
Tests show that con-siderable amounts of phrase pairs can be ex-cluded, without much impact on the transla-tion quality.
In fact, we show that better trans-lations can be obtained using our pruned mod-els, due to the compression of the search spaceduring decoding.1 IntroductionPhrase-based Machine Translation Models (Koehnet al 2003) model n-to-m translations of n sourcewords to m target words, which are encoded inphrase pairs and stored in the translation model.This approach has an advantage over Word-basedTranslation Models (Brown et al 1993), since trans-lating multiple source words allows the context foreach source word to be considered during trans-lation.
For instance, the translation of the En-glish word ?in?
by itself to Portuguese is not ob-vious, since we do not have any context for theword.
This word can be translated in the con-text of ?in (the box)?
to ?dentro?, or in the con-text of ?in (China)?
as ?na?.
In fact, the lexicalentry for ?in?
has more than 10 good translationsin Portuguese.
Consequently, the lexical translationentry for Word-based models splits the probabilis-tic mass between different translations, leaving thechoice based on context to the language model.
Onthe other hand, in Phrase-based Models, we wouldhave a phrase pair p(in the box, dentro da caixa)and p(in china, na china), where the words ?in thebox?
and ?in China?
can be translated together to?dentro da caixa?
and ?na China?, which substan-tially reduces the ambiguity.
In this case, both thetranslation and language models contribute to findthe best translation based on the local context, whichgenerally leads to better translations.However, not all words add the same amount ofcontextual information.
Using the same example for?in?, if we add the context ?
(hid the key) in?, it isstill not possible to accurately identify the best trans-lation for the word ?in?.
The phrase extraction algo-rithm (Ling et al 2010) does not discriminate whichphrases pairs encode contextual information, and ex-tracts all phrase pairs with consistent alignments.Hence, phrases that add no contextual information,such as, p(hid the key in, escondeu a chave na)and p(hid the key in, escondeu a chave dentro)are extracted.
This is undesirable because we arepopulating translation models with redundant phrasepairs, whose translations can be obtained using com-962binations of other phrases with the same probabil-ities, namely p(hid the key, escondeu a chave),p(in, dentro) and p(in, na).
This is a problemthat is also found in language modeling, wherelarge amounts of redundant higher-order n-gramscan make the model needlessly large.
For backofflanguage models, multiple pruning strategies basedon relative entropy have been proposed (Seymoreand Rosenfeld, 1996) (Stolcke, 1998), where the ob-jective is to prune n-grams in a way to minimize therelative entropy between the model before and afterpruning.While the concept of using relative entropy forpruning is not new and frequently used in backofflanguage models, there are no such models for ma-chine translation.
Thus, the main contribution ofour work is to propose a relative entropy pruningmodel for translation models used in Phrase-basedMachine Translation.
It is shown that our pruningalgorithm can eliminate phrase pairs with little orno impact in the predictions made in our translationmodel.
In fact, by reducing the search space, lesssearch errors are made during decoding, which leadsto improvements in translation quality.This paper is organized as follows.
We describeand contrast the state of the art pruning algorithmsin section 2.
In section 3, we describe our relative-entropy model for machine translation.
Afterwards,in section 4, we apply our model for pruning inPhrase-based Machine Translation systems.
We per-form experiments with our pruning algorithm basedon phrase pair independence and analyse the resultsin section 5.
Finally, we conclude in section 6.2 Phrase Table PruningPhrase table pruning algorithms are important intranslation, since they efficiently reduce the size ofthe translation model, without having a large nega-tive impact in the translation quality.
This is espe-cially relevant in environments where memory con-straints are imposed, such as translation systems forsmall devices like cellphones, and also when timeconstraints for the translation are defined, such asonline Speech-to-Speech systems.2.1 Significance PruningA relevant reference in phrase table pruning is thework of (Johnson and Martin, 2007), where it isshown that a significant portion of the phrase ta-ble can be discarded without a considerable negativeimpact on translation quality, or even positive one.This work computes the probability, named p-value,that the joint occurrence event of the source phrases and target phrase t occurring in same sentence pairhappens by chance, and are actually statistically in-dependent.
Phrase pairs that have a high p-value,are more likely to be spurious and more prone tobe pruned.
This work is followed in (Tomeh et al2009), where phrase pairs are treated discriminatelybased on their complexity.
Significance-based prun-ing has also been successfully applied in languagemodeling in (Moore and Quirk, 2009).Our work has a similar objective, but insteadof trying to predict the independence between thesource and target phrases in each phrase pair, we at-tempt to predict the independence between a phrasepair and other phrase pairs in the model.2.2 Relevance PruningAnother proposed approach (Matthias Eck andWaibel, 2007) consists at collecting usage statisticsfor phrase pairs.
This algorithm decodes the train-ing corpora and extracts the number of times eachphrase pair is used in the 1-best translation hypoth-esis.
Thus, phrase pairs that are rarely used duringdecoding are excluded first during pruning.This method considers the relationship betweenphrase pairs in the model, since it tests whetherthe decoder is more prone to use some phrase pairsthan others.
However, it leads to some undesirablepruning choices.
Let us consider a source phrase?the box in China?
and 2 translation hypotheses,where the first hypothesis uses the phrase transla-tion p(the key in China, a chave na China) withprobability 70%, and the second hypothesis usestwo phrase translations p(the key, a chave) andp(in China, na China) with probability 65%.
Thisapproach will lean towards pruning the phrase pairsin the second hypothesis, since the decoder will usethe first hypothesis.
This is generally not desired,since the 2 smaller phrase pairs can be used to trans-late the same source sentence with a small probabil-963ity loss (5%), even if the longer phrase is pruned.On the other hand, if the smaller phrases are pruned,the longer phrase can not be used to translate smallerchunks, such as ?the key in Portugal?.
This matter isaggravated due to the fact that the training corpora isused to decode, so longer phrase pairs will be usedmore frequently than when translating unseen sen-tences, which will make the model more biased intopruning shorter phrase pairs.3 Relative Entropy Model ForPhrase-based Translation ModelsIn this section, we shall define our entropy modelfor phrase pairs.
We start by introducing some no-tation to distinguish different types of phrase pairsand show why some phrase pairs are more redun-dant than others.
Afterwards, we illustrate our no-tion of relative entropy between phrase pairs.
Then,we describe our entropy model, its computation andits application to phrase table pruning.3.1 Atomic and Composite Phrase PairsWe discriminate between 2 types of phrase pairs:atomic phrase pairs and composite phrase pairs.Atomic phrase pairs define the smallest transla-tion units, such that given an atomic phrase pair thattranslates from s to t, the same translation cannotbe obtained using any combination of other phrasepairs.
Removing these phrase pairs reduces therange of translations that our model is capable oftranslating and also the possible translations.Composite phrase pairs define translations of agiven sequence of words that can also be obtainedusing atomic or other smaller composite phrasepairs.
Each combination is called a derivation ortranslation hypothesis.
Removing these phrase pairsdoes not change the amount of sentences that themodel can translate, since all translations encodedin these phrases can still be translated using otherphrases, but these will lead to different translationprobabilities.Considering table 1, we can see that atomicphrases encode one elementary translation event,while composite phrases encode joint events that areencoded in atomic phrase pairs.
If we look at thesource phrase ?in?, there is a multitude of possibletranslations for this word in most target languages.Taking Portuguese as the target language, the proba-bility that ?in?
is translated to ?em?
is relatively low,since it can also be translated to ?no?, ?na?, ?den-tro?, ?dentro de?
and many others.However, if we add another word such as ?Por-tugal?
forming ?in Portugal?, it is more likely that?in?
is translated to ?em?.
Thus, we define thejoint event of ?in?
translating to ?em?
(A1) and?Portugal?
to ?Portugal?
(B1), denoted as A1 ?
B1,in the phrase pair p(in Portugal, em Portugal).Without this phrase pair it is assumed that theseare independent events with probability given byP (A1)P (B1)1, which would be 10%, leading to a60% reduction.
In this case, it would be more likely,that in Portugal is translated to no Portugal orna Portugal, which would be incorrect.Some words, such as ?John?, forming ?John in?,do not influence the translations for the word ?in?,since it can still be translated to ?em?, ?no?, ?na?,?dentro?
or ?dentro de?
depending on the word thatfollows.
By definition, if the presence of phrasep(John, John) does not influence the translation ofp(in, em) and viceversa, we can say that probabilityof the joint event P (A1?C1) is equal to the productof the probabilities of the events P (A1)P (C1).If we were given a choice of pruning either thecomposite phrase pairs p(John in, John em) orp(in Portugal, em Portugal), the obvious choicewould be the former, since the probability of theevent encoded in that phrase pair is composed by 2independent events, in which case the decoder willinherently consider the hypothesis that ?John in?
istranslated to ?John em?
with the same probability.
Inanother words, the model?s predictions even, with-out this phrase pair will remain the same.The example above shows an extreme case,where the event encoded in the phrase pairp(John in, John em) is decomposed into indepen-dent events, and can be removed without chang-ing the model?s prediction.
However, finding andpruning phrase pairs that are independent, based onsmaller events is impractical, since most translationevents are not strictly independent.
However, manyphrase pairs can be replaced with derivations usingsmaller phrases with a small loss in the model?s pre-1For simplicity, we assume at this stage that no reorderingmodel is used964Phrase Pair Prob EventAtomic Phrase Pairsin?
em 10% A1in?
na 20% A2in?
no 20% A3in?
dentro 5% A4in?
dentro de 5% A5Portugal?
Portugal 100% B1John?
John 100% C1Composite Phrase Pairsin Portugal?
em Portugal 70% A1 ?B1John in?
John em 10% C1 ?A1John in?
John na 20% C1 ?A2John in?
John no 20% C1 ?A3John in?
John dentro 5% C1 ?A4John in?
John dentro de 5% C1 ?A5Table 1: Phrase Translation Table with associated eventsdictions.Hence, we would like to define a metric for phrasepairs that allows us evaluate how discarding eachphrase pair will affect the pruned model?s predic-tions.
By removing phrase pairs that can be derivedusing smaller phrase pairs with similar probability,it is possible to discard a significant portion of thetranslation model, while minimizing the impact onthe model?s predictions.3.2 Relative Entropy Model for MachineTranslationFor each phrase pair pa, we define the supportingset SP (pa(s, t)) = S1, ..., Sk, where each elementSi = pi, ..., pj is a distinct derivation of pa(s, t) thattranslates s to t, with probability P (Si) = P (pi) ?...
?P (pj).
A phrase pair can have multiple elementsin its supporting set.
For instance, the phrase pairp(John in Portugal, John em Portugal), has 3elements in the support set:?
S1 = {p(John, John), p(in, em), p(Portugal, Portugal)}?
S2 = {p(John, John), p(in Portugal, em Portugal)}?
S3 = {p(John in, John em), p(Portugal, Portugal)}S1, S2 and S3 encode 3 different assumptionsabout the event of translating ?John in Portugal?to ?John em Portugal?.
S1 assumes that the eventis composed by 3 independent events A1, B1 andC1, S2 assumes that A1 and B1 are dependent, andgroups them into a single composite event A1 ?B1,which is independent from C1, and S3 groups A1and C1 independently from B1.
As expected, theevent encoded in the phrase pair p itself isA1?B1?C1, which assumes thatA1,B1 andC1 are all depen-dent.
We can see that if any of the events S1, S2 orS3 has a ?similar probability?
as the event coded inthe phrase pair, we can remove this phrase pair witha minimal impact in the phrase prediction.To formalize our notion of ?similar probabil-ity?, we apply the relative entropy or the Kullback-Leibler divergence, and define the divergence be-tween a pruned translation model Pp(s, t) and theunpruned model P (s, t) as:D(Pp||P ) = ?
?s,tP (s, t)logPp(t|s)P (t|s)(1)Where Pp(t|s)P (t|s) , measures the deviation from theprobability emission from the pruned model and theoriginal probability from the unpruned model, foreach source-target pair s, t. This is weighted bythe frequency that the pair s, t is observed, given byP (s, t).Our objective is to minimize D(Pp||P ), whichcan be done locally by removing phrase pairs p(s, t)with the lowest values for ?P (s, t)logPp(t|s)P (t|s) .
Ide-ally, we would want to minimize the relative entropyfor all possible source and target sentences, ratherthan all phrases in our model.
However, minimiz-ing such an objective function would be intractabledue to reordering, since the probability assigned to aphrase pair in a sentence pair by each model woulddepend on the positioning of all other phrase pairsused in the sentence.
Because of these dependen-cies, we would not be able to reduce this problem toa local minimization problem.
Thus, we assume thatall phrase pairs have the same probability regardlessof their context in a sentence.Thus, our pruning algorithm takes a threshold ?and prunes all phrase pairs that fail to meet the fol-lowing criteria:?P (s, t)logPp(t|s)P (t|s)> ?
(2)The main components of this function is the ratiobetween the emission from the pruned model and965unpruned models given by Pp(t|s)P (t|s) , and the weightgiven to each s, t pair given by P (s, t).
In the re-mainder of this section, we will focus on how tomodel each of these components in equation 2.3.3 Computing P (s, t)The term P (s, t) can be seen as a weighting functionfor each s, t pair.
There is no obvious optimal dis-tribution to model P (s, t).
In this work, we apply 2different distributions for P (s, t).
First, an uniformdistribution, where all phrases are weighted equally.Secondly, a multinomial function defined as:P (s, t) =N(s, t)N(3)whereN is the number of sentence pairs in the paral-lel data, and N(s, t) is the number of sentence pairswhere s was observed in the source sentence and twas observed in the target sentence.
Using this dis-tribution, the model is more biased in pruning phrasepairs with s, t pairs that do not occur frequently.3.4 Computing Pp(t|s)P (t|s)The computation of Pp(t|s)P (t|s) depends on how the de-coder adapts when a phrase pair is pruned from themodel.
In the case of back-off language models,this can be solved by calculating the difference ofthe logs between the n-gram estimate and the back-off estimate.
However, a translation decoder gen-erally functions differently.
In our work, we willassume that the decoding will be performed usinga Viterbi decoder, such as MOSES (Koehn et al2007), where the translation with the highest scoreis chosen.In the example above, where s=?John in Portu-gal?
and t=?John em Portugal?, the decoder wouldchoose the derivation with the highest probabilityfrom s to t. Using the unpruned model, the possi-ble derivations are either using phrase p(s, t) or oneelement of its support set S1, S2 or S3.
On the otherhand, on the pruned model where p(s, t) does notexist, only S1, S2 and S3 can be used.
Thus, givena s, t pair one of three situations may occur.
First, ifthe probability of the phrase pair p(s, t) is lower thanthe highest probability element in SP (p(s, t)), thenboth the models will choose that element, in whichcase, Pp(t|s)P (t|s) = 1.
This can happen, if we definefeatures that penalize longer phrase pairs, such aslexical weighting, or if we apply smoothing (Fosteret al 2006).
Secondly, if the probability of p(s, t)is equal to the most likely element in SP (p(s, t)),regardless of whether the unpruned model choses touse p(s, t) or that element, the probability emissionsof the pruned and unpruned model will be identi-cal.
Thus, for this case Pp(t|s)P (t|s) = 1.
Finally, if theprobability of p(s, t) is higher than other possiblederivations, the unpruned model will choose to emitthe probability of p(s, t), while the pruned modelwill emit the most likely element in SP (p(s, t)).Hence, the probability loss between the 2 models,will be the ratio between the probability of p(s, t)and the probability of the most likely element inSP (p(s, t)).From the example above, we can generalize thefunction for Pp(t|s)P (t|s) as:?p?
?argmax(SP (p(s,t))) P (p?
)P (p(s, t))(4)Where P (p(s, t)) denotes the probability ofp(s, t) and?p?
?argmax(SP (p(s,t))) P (p?)
the mostlikely sequence of phrasal translations that translatess to t, with the probability equal to the product of allphrase translation probabilities in that sequence.Replacing in equation 2, our final condition thatmust be satisfied for keeping a phrase pair is:?P (s, t)log?p?
?argmax(SP (p(s,t))) P (p?
)P (p(s, t))> ?
(5)4 Application for Phrase-based MachineTranslationWe will now show how we apply our entropy prun-ing model in the state-of-the-art phrase-based trans-lation system MOSES and describe the problemsthat need to be addressed during the implementationof this model.4.1 Translation ModelThe translation model in Moses is composed bya phrase translation model and a phrase reorder-ing model.
The first one models, for each phrasepair p(s, t), the probability of translating the s tot by combining multiple features ?i, weighted by966wTi , as PT (p) =?ni=1 ?i(p)wTi .
The reorderingmodel is similar, but models the local reordering be-tween p, given the previous and next phrase accord-ing to the target side, pP and pN , or more formally,PR(p|pP , pN ) =?mi=1 ?i(p|pP , pP )wRi4.2 Building the Support SetEssentially, implementing our model is equiva-lent to calculating the components described inequation 5.
These are P (s, t), P (p(s|t)) andargmax(SP (p(s, t))).
Calculating the uniform dis-tribution and multinomial distributions for P (s, t)is simple, the uniform distribution just assumes thesame value for all s and t, and the multinomial dis-tribution can be modeled by extracting counts fromthe parallel corpora.Calculating P (s|t) is also trivial, since it only en-volves calculating PT (p(s, t)), which can be doneby retrieving the translation features of p and apply-ing the weights for each feature.The most challenging task is to calculateargmax(SP (p(s, t))), which is similar to the de-coding task in machine translation, where we need tofind the best translation t?
for a sentence s, that is, t?
=argmaxtP (s|t)P (t).
In practice, we are not search-ing in the space of possible translations, but in thespace of possible derivations, which are sequencesof phrase translations p1(s1, t1), ..., pn(sn, tn) thatcan be applied to s to generate an output t with thescore given by P (t)?ni=1 P (si, ti).Our algorithm to determine SP (p(s, t)) can bedescribed as an adaptation to the decoding algorithmin Moses, where we restrict the search space to thesubspace SP (p(s, t)), that is, our search space isonly composed by derivations that output t, with-out using p itself.
This can be done using the forceddecoding algorithm proposed in (Schwartz, 2008).Secondly, the score of a given translation hypothesisdoes not depend on the language model probabilityP (t), since all derivations in this search space havethe same t, thus we discard this probability fromthe score function.
Finally, rather than using beamsearch, we exhaustively search all the search space,to reduce the hypothesis of incurring a search errorat this stage.
This is possible, since phrase pairs aregenerally smaller than text (less than 8 words), andbecause we are constraining the search space to t,which is an order of magnitude smaller than the reg-ular search space with all possible translations.4.3 Pruning AlgorithmThe algorithm to generate a pruned translationmodel is shown in 1.
We iterate over all phrase pairsp1(s1, t1), ..., pn(sn, tn), decode using our forceddecoding algorithm from si to ti, to obtain the bestpath S. If no path is found then it means that the piis atomic.
Then, we prune pi based on condition 5.Algorithm 1 Independence PruningRequire: pruning threshold ?,unpruned model {p1(s1, t1), ..., pn(sn, tn)}for pi(si, ti) ?
{p1(s1, t1), ..., pn(sn, tn)} doS := argmax(SP (pi)) \ piscore :=?if S 6= {} thenscore := ?P (s, t)log?p?(s?,t?
)?S P (s?|t?
)P (s|t)end ifif score ?
?
thenprune(pi)end ifend forreturn pruned modelThe main bottle neck in this algorithm is find-ing argmax(SP (pi)).
While this appears relativelysimple and similar to a document decoding task, thesize of our task is on a different order of magni-tude, since we need to decode every phrase pair inthe translation model, which might not be tractablefor large models with millions of phrase pairs.
Weaddress this problem in section 5.3.Another problem with this algorithm is that thedecision to prune each phrase pair is made assumingthat all other phrase pairs will remain in the model.Thus, there is a chance a phrase pair p1 is prunedbecause of a derivation using p2 and p3 that leads tothe same translation.
However, if p3 also happens tobe pruned, such a derivation will no longer be pos-sible.
One possible solution to address this problemis to perform pruning iteratively, from the smallestphrase pairs (number of words) and increase the sizeat each iteration.
However, we find this undesirable,since the model will be biased into removing smallerphrase pairs, which are generally more useful, sincethey can be used in multiple derivation to replacelarger phrase pairs.
In the example above, the model967would eliminate p3 and keep p1, yet the best deci-sion could be to keep p3 and remove p1, if p3 is alsofrequently used in derivations of other phrase pairs.Thus, we leave the problem of finding the best set ofphrases to prune as future work.5 ExperimentsWe tested the performance of our system under twodifferent environments.
The first is the small scaleDIALOG translation task for IWSLT 2010 evalua-tion (Paul et al 2010) using a small corpora forthe Chinese-English language pair (henceforth re-ferred to as ?IWSLT?).
The second one is a largescale test using the complete EUROPARL (Koehn,2005) corpora for the Portuguese-English languagepair, which we will denote by ?EUROPARL?.5.1 CorpusThe IWSLT model was trained with 30K trainingsentences.
The development corpus and test corpuswere taken from the evaluation dataset in IWSLT2006 (489 tuning and 500 test sentences with 7 ref-erences).
The EUROPARL model was trained usingthe EUROPARL corpora with approximately 1.3Msentence pairs, leaving out 1K sentences for tuningand another 1K sentences for tests.5.2 SetupIn the IWSLT experiment, word alignments weregenerated using an HMM model (Vogel et al 1996),with symmetric posterior constraints (V. Grac?a etal., 2010), using the Geppetto toolkit2.
This setupwas used in the official evaluation in (Ling et al2010).
For the EUROPARL experiment the wordalignments were generated using IBM model 4.
Inboth experiments, the translation model was builtusing the phrase extraction algorithm (Paul et al2010), with commonly used features in Moses (Ex:probability, lexical weighting, lexicalized reorderingmodel).
The optimization of the translation modelweights was done using MERT tuning (Och, 2003)and the results were evaluated using BLEU-4.5.3 Pruning SetupOur pruning algorithm is applied after the translationmodel weight optimization with MERT.
We gener-2http://code.google.com/p/geppetto/ate multiple translation models by setting differentvalues for ?, so that translation models of differentsizes are generated at intervals of 5%.
We also runthe significance pruning (Johnson and Martin, 2007)algorithm in these conditions.While the IWSLT translation model has only88,424 phrase pairs, for the EUROPARL exper-iment, the translation model was composed by48,762,372 phrase pairs, which had to be decoded.The average time to decode each phrase pair us-ing the full translation model is 4 seconds per sen-tence, since the table must be read from disk due toits size.
This would make translating 48M phrasepairs unfeasible.
To address this problem, we di-vide the phrase pairs in the translation model intoblocks of K phrase pairs, that are processed sepa-rately.
For each block, we resort to the approachused in MERT tuning, where the model is filtered toonly include the phrase pairs that are used for trans-lating tuning sentences.
We filter each block withphrase pairs fromK to 2K with the source sentencessK , ..., s2K .
Furthermore, since we are force de-coding using the target sentences, we also filter theremaining translation models using the target sen-tences tK , ..., t2K .
We used blocks of 10,000 phrasepairs and each filtered table was reduced to less than1% of the translation table on average, reducing theaverage decoding time to 0.03 seconds per sentence.Furthermore, each block can be processed in parallelallowing multiple processes to be used for the task,depending on the resources that are available.5.4 ResultsFigure 1 shows the BLEU results for different sizesof the translation model for the IWSLT experimentusing the uniform and multinomial distributions forP (s, t).
We observe that there is a range of valuesfrom 65% to 95% where we actually observe im-provements caused by our pruning algorithm, withthe peak at 85% for the uniform distribution, wherewe improve from 15.68 to 15.82 (0.9% improve-ment).
Between 26% and 65%, the BLEU score islower than the baseline at 100%, with the minimumat 26% with 15.54, where only atomic phrase pairsremain and both the multinomial and uniform distri-bution have the same performance, obviously.
Thisis a considerable reduction in phrase table size bysacrificing 0.14 BLEU points.
Regarding the com-96815.5?15.55?15.6?15.65?15.7?15.75?15.8?15.85?25%?30%?35%?40%?45%?50%?55%?60%?65%?70%?75%?80%?85%?90%?95%?100%?IWSLT?Results?Uniform?Mul?nomial?Figure 1: Results for the IWSLT experiment.
The x-axis shows the percentage of the phrase table used.
TheBLEU scores are shown in the y-axis.
Two distributionsfor P (s, t) were tested Uniform and Multinomial.parison between the uniform and multinomial distri-bution, we can see that both distributions yield sim-ilar results, specially when a low number of phrasepairs is pruned.
In theory, the multinomial distri-bution should yield better results, since the pruningmodel will prefer to prune phrase pairs that are morelikely to be observed.
However, longer phrase pairs,which tend compete with other long phrase pairs onwhich get pruned first.
These phrase pairs gener-ally occur only once or twice, so the multinomialmodel will act similarly to the uniform model re-garding longer phrase pairs.
On the other hand, asthe model size reduces, we can see that using multi-nomial distribution seems to start to improve overthe uniform distribution.The comparison between our pruning model andpruning based on significance is shown in table 2.These models are hard to compare, since not allphrase table sizes can be obtained using both met-rics.
For instance, the significance metric can ei-ther keep or remove all phrase pairs that only appearonce, leaving a large gap of phrase table sizes thatcannot be attained.
In the EUROPARL experimentthe sizes of the table suddenly drops from 60% to8%.
The same happens with our metric that cannotdistinguish atomic phrase pairs.
In the EUROPARLexperiment, we cannot generate phrase tables withsizes smaller than 15%.
Thus, we only show re-sults at points where both algorithms can producea phrase table.Significant improvements are observed in theTable size Significance Entropy (u) Entropy (m)Pruning Pruning PruningIWSLT57K (65%) 14.82 15.77 15.7871K (80%) 15.14 15.76 15.7780K (90%) 15.31 15.73 15.7288K (100%) 15.68 15.68 15.68EUROPARL29M (60%) 28.64 28.82 28.9134M (70%) 28.84 28.94 28.9939M (80%) 28.86 28.99 28.9944M (90%) 28.91 29.00 29.0249M (100%) 29.18 29.18 29.18Table 2: Comparison between Significance Pruning (Sig-nificance Pruning) and Entropy-based pruning using theuniform (Entropy (u) Pruning) and multinomial distribu-tions (Entropy (m) Pruning).IWSLT experiment, where significance pruningdoes not perform as well.
On the other hand, on theEUROPARL experiment, our model only achievesslightly higher results.
We believe that this is re-lated by the fact the EUROPARL corpora is gener-ated from automatically aligning documents, whichmeans that there are misaligned sentence pairs.Thus, many spurious phrase pairs are extracted.
Sig-nificance pruning performs well under these condi-tions, since the measure is designed for this purpose.In our metric, we do not have any means for detect-ing spurious phrase pairs, in fact, spurious phrasepairs are probably kept in the phrase table, sinceeach distinct spurious phrase pair is only extractedonce, and thus, they have very few derivations inits support set.
This suggests, that the significancescore can be integrated in our model to improve ourmodel, which we leave as future work.John married PortugalmarriedininPortugalmarriedmarriedinJohninPortugalPortugala)b)Figure 2: Translation order in for different reorderingstarting from left to right.We believe that in language pairs such as Chinese-969English with large distance reorderings betweenphrases are more prone to search errors and benefitmore from our pruning algorithm.
To illustrate this,let us consider the source sentence ?John marriedin Portugal?, and translating either using the blocks?John?, ?married?
and ?in Portugal?
or the blocks?John?, ?married in?, ?Portugal?, the first hypoth-esis would be much more viable, since the word?Portugal?
is more relevant as the context for theword ?in?.
Thus, the key choice for the decoder isto decide whether to translate using ?married?
withor without ?in?, and it is only able to predict thatit is better to translate ?married?
by itself until itfinds that ?in?
is better translated with ?Portugal?.Thus, a search error occurs if the hypothesis where?married?
is translated by itself is removed.
In fig-ure 2, we can see the order that blocks are consid-ered for different reorderings, starting from left toright.
In a), we illustrate the case for a monotonoustranslation.
We observe that the correct decision be-tween translating ?married in?
or just ?married?
isfound immediately, since the blocks ?Portugal?
and?in Portugal?
are considered right afterwards.
In thiscase, it is unlikely that the hypothesis using ?mar-ried?
is removed.
However, if we consider that dueto reordering, ?John?
is translated after ?married?and before ?Portugal?, which is shown in b).
Then,the correct decision can only be found after consid-ering ?John?.
In this case, ?John?
does not havemany translations, so the likelihood of eliminatingthe correct hypothesis.
However, if there were manytranslations for John, it is highly likely that the cor-rect partial hypothesis is eliminated.
Furthermore,the more words exist between ?married?
and ?Portu-gal?, the more likely will the correct hypothesis notexist when we reach ?Portugal?.
By pruning the hy-pothesis ?married in?
a priori, we contribute in pre-venting such search errors.We observe that some categories of phrase pairsthat are systematically pruned, but these cannotbe generalized in rules, since there are many ex-ceptions.
The most obvious type of phrase pairsare phrases with punctuations, such as ???.?
to?thanks .?
and ?.
???
to ?thanks .
?, since ?.
?is translated independently from most contextualwords.
However, this rule should not be general-ized, since in some cases ?.?
is a relevant contextualmarker.
For instance, the word ?please?
is translatedto ???
in the sentence ?open the door, please.?
andtranslated to ?????
in ?please my advisors?.
An-other example are sequences of numbers, which aregenerally translated literally.
For instance, ??(8)?(3)?(8)?
is translated to ?eight three eight?
(Ex:?room eight three eight?).
Thus, phrase pairs fornumber sequences can be removed, since those num-bers can be translated one by one.
However, for se-quences such as ??(1)?
(8)?, we need a phrase pairto represent this specifically.
This is because ??
(1)?can be translated to ?one?, but also to ?a?, ?an?, ?sin-gle?.
Other exceptions include ??(1)?
(1)?, whichtends to be translated as ?eleven?, and which tends tobe translated to ?o?, rather than ?zero?
in sequences(?room eleven o five?
).6 ConclusionsWe present a pruning algorithm for Machine Trans-lation based on relative entropy, where we assesswhether the translation event encoded in a phrasepair can be decomposed into combinations of eventsencoded in other phrase pairs.
We show that suchphrase pairs can be removed from the translationmodel with little negative impact or even a positiveone in the overall translation quality.
Tests show thatour method yields comparable or better results withstate of the art pruning algorithms.As future work, we would like to combine ourapproach with significance pruning, since both ap-proaches are orthogonal and address different issues.We also plan to improve the pruning step of our algo-rithm to find the optimal set of phrase pairs to prunegiven the pruning threshold.The code used in this work will be made available.7 AcknowledgementsThis work was partially supported by FCT (INESC-ID multiannual funding) through the PIDDAC Pro-gram funds, and also through projects CMU-PT/HuMach/0039/2008 and CMU-PT/0005/2007.The PhD thesis of Wang Ling is supported by FCTgrant SFRH/BD/51157/2010.
The authors also wishto thank the anonymous reviewers for many helpfulcomments.970ReferencesPeter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Comput.
Linguist., 19:263?311, June.George Foster, Roland Kuhn, and Howard Johnson.2006.
Phrasetable smoothing for statistical machinetranslation.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing,EMNLP ?06, pages 53?61, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.J Howard Johnson and Joel Martin.
2007.
Improv-ing translation quality by discarding most of thephrasetable.
In In Proceedings of EMNLP-CoNLL?07,pages 967?975.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1,NAACL ?03, pages 48?54, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-burch, Richard Zens, Rwth Aachen, Alexan-dra Constantin, Marcello Federico, Nicola Bertoldi,Chris Dyer, Brooke Cowan, Wade Shen, ChristineMoran, and Ondrej Bojar.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics Companion Volume Pro-ceedings of the Demo and Poster Sessions, pages 177?180, Prague, Czech Republic, June.
Association forComputational Linguistics.Philipp Koehn.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In ConferenceProceedings: the tenth Machine Translation Summit,pages 79?86, Phuket, Thailand.
AAMT, AAMT.Wang Ling, Tiago Lu?
?s, Joa?o Grac?a, Lu?
?sa Coheur, andIsabel Trancoso.
2010.
Towards a general and ex-tensible phrase-extraction algorithm.
In IWSLT ?10:International Workshop on Spoken Language Transla-tion, pages 313?320, Paris, France.Stephen Vogal Matthias Eck and Alex Waibel.
2007.
Es-timating phrase pair relevance for translation modelpruning.
MTSummit XI.Robert C. Moore and Chris Quirk.
2009.
Less is more:significance-based n-gram selection for smaller, bet-ter language models.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing: Volume 2 - Volume 2, EMNLP ?09,pages 746?755, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics - Volume 1, ACL ?03, pages 160?167, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Michael Paul, Marcello Federico, and Sebastian Stu?ker.2010.
Overview of the iwslt 2010 evaluation cam-paign.
In IWSLT ?10: International Workshop on Spo-ken Language Translation, pages 3?27.Lane Schwartz.
2008.
Multi-source translation methods.In Proceedings of AMTA, pages 279?288.Kristie Seymore and Ronald Rosenfeld.
1996.
Scalablebackoff language models.
In In Proceedings of ICSLP,pages 232?235.Andreas Stolcke.
1998.
Entropy-based pruning of back-off language models.
In In Proc.
DARPA Broad-cast News Transcription and Understanding Work-shop, pages 270?274.Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.2009.
Complexity-based phrase-table filtering for sta-tistical machine translation.
MTSummit XII, Aug.Joa?o V. Grac?a, Kuzman Ganchev, and Ben Taskar.
2010.Learning Tractable Word Alignment Models withComplex Constraints.
Comput.
Linguist., 36:481?504.S.
Vogel, H. Ney, and C. Tillmann.
1996.
Hmm-based word alignment in statistical translation.
InProceedings of the 16th conference on Computationallinguistics-Volume 2, pages 836?841.
Association forComputational Linguistics.971
