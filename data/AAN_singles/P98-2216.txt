The Computational Lexical Semantics of Syntagmatic RelationsEve lyne  V iegas ,  S tephen Bea le  and  Serge i  N i renburgNew Mexico State  Univers i tyComput ing  Research Lab,Las Cruces, NM 88003,USAviegas, sb, sergei?crl, nmsu.
eduAbst ractIn this paper, we address the issue of syntagmaticexpressions from a computational lexical semanticperspective.
From a representational viewpoint, weargue for a hybrid approach combining linguistic andconceptual paradigms, in order to account for thecontinuum we find in natural languages from freecombining words to frozen expressions.
In particu-lar, we focus on the place of lexical and semanticrestricted co-occurrences.
From a processing view-point, we show how to generate/analyze syntag-matic expressions by using an efficient constraint-based processor, well fitted for a knowledge-drivenapproach.1 In t roduct ionYou can take advantage o\] the chambermaid 1 isnot acollocation one would like to generate in the contextof a hotel to mean "use the services of."
This is whycollocations should constitute an important part inthe design of Machine Translation or MultilingualGeneration systems.In this paper, we address the issue of syntagmaticexpressions from a computational lexical semanticperspective.
From a representational viewpoint, weargue for a hybrid approach combining linguistic andconceptual paradigms, in order to account for thecontinuum we find in natural languages from freecombining words to frozen expressions (such as inidioms kick the (proverbial) bucket).
In particular,we focus on the representation f restricted seman-tic and lexical co-occurrences, such as heavy smokerand pro#ssor ... students respectively, that we de-fine later.
From a processing viewpoint, we showhow to generate/analyze syntagmatic expressions byusing an efficient constraint-based processor, well fit-ted for a knowledge-driven approach.
In the follow-ing, we first compare different approaches to collo-cations.
Second, we present our approach in termsof representation a d processing.
Finally, we showhow to facilitate the acquisition of co-occurrences byusing 1) the formalism of lexical rules (LRs), 2) an1Lederer, R. 1990.
Anguished English A Laurel Book, DellPublishing.inheritance hierarchy of Lexical Semantic Functions(LSFs).2 Approaches  to  Syntagmat icRe la t ionsSyntagmatic relations, also known as collocations,are used differently by lexicographers, linguists andstatisticians denoting almost similar but not identi-cal classes of expressions.The traditional approach to collocations has beenlexicographic.
Here dictionaries provide infor-mation about what is unpredictable or idiosyn-cratic.
Benson (1989) synthesizes Hausmann's stud-ies on collocations, calling expressions such as com-mit murder, compile a dictionary, inflict a wound,etc.
"fixed combinations, recurrent combinations"or "collocations".
In Hausmann's terms (1979) acollocation iscomposed of two elements, a base ("Ba-sis") and a collocate ("Kollokator"); the base is se-mantically autonomous whereas the collocate cannotbe semantically interpreted in isolation.
In otherwords, the set of lexical collocates which can com-bine with a given basis is not predictable and there-fore collocations must be listed in dictionaries.It is hard to say that there has been a real focuson collocations from a l inguistic perspective.
Thelexicon has been broadly sacrificed by both English-speaking schools and continental European schools.The scientific agenda of the former has been largelydominated by syntactic issues until recently, whereasthe latter was more concerned with pragmatic as-pects of natural anguages.
The focus has been ongrammatical collocations uch as adapt to, aim at,look \]or.
Lakoff (1970) distinguishes a class of ex-pressions which cannot undergo certain operations,such as nominalization, causativization: the problemis hard; *the hardness of the problem; *the problemhardened.
The restriction on the application of cer-tain syntactic operations can help define collocationssuch as hard problem, for example.
Mel'~uk's treat-ment of collocations will be detailed below.In recent years, there has been a resurgence ofstatist ical  approaches applied to the study of nat-ural languages.
Sinclair (1991) states that '% word1328which occurs in close proximity to a word under in-vestigation is called a collocate of it .
.
.
.
Collocationis the occurrence of two or more words within ashort space of each other in a text".
The prob-lem is that with such a definition of collocations,even when improved, z one identifies not only collo-cations but free-combining pairs frequently appear-ing together such as lawyer-client; doctor-hospital.However, nowadays, researchers seem to agree thatcombining statistic with symbolic approaches leadto quantifiable improvements (Klavans and Resnik,1996).The  Mean ing  Text  Theory  Approach  TheMeaning Text Theory (MTT) is a generator-orientedlexical grammatical formalism.
Lexical knowledge isencoded in an entry of the Explanatory Combina-torial Dictionary (ECD), each entry being dividedinto three zones: the semantic zone (a semantic net-work representing the meaning of the entry in termsof more primitive words), the syntactic zone (thegrammatical properties of the entry) and the lexi-cal combinatorics zone (containing the values of theLexical  Funct ions  (LFs) 3).
LFs are central to thestudy of collocations:A lexical function F is a correspondencewhich associates a lexical item L, called thekey word of F, with a set of lexical itemsF(L)-the value of F. (Mel'6uk, 1988) 4We focus here on syntagmatic LFs describing co-occurrence r lations uch as pay attention, legitimatecomplaint; from a distance.
5Heylen et al (1993) have worked out some caseswhich help license a starting point for assigning LFs.They distinguish four types of syntagmatic LFs:?
evaluative qualifierMagn(bleed) = profusely?
distributional qualifierMult(sheep) = flock?
co -occur renceLoc-in(distance)= at a distance?
verbal operatorOper l (at tent ion)  = payThe MTT approach is very interesting as it pro-vides a model of production well suited for genera-tion with its different strata and also a lot of lexical-semantic information.
It seems nevertheless that all2Church and Hanks (1989), Smadja (1993) use statisticsin their algorithms to extract collocations from texts.3See (Iordanskaja et al, 1991) and (Ramos et al, 1994)for their use of LFs in MTT and NLG respectively.4(Held, 1989) contrasts Hausman's base and collate toMel'tuk's keyword and LF values.5There are about 60 LFs listed said to be universal; thelexicographic approach of Mel'tuk and Zolkovsky has beenapplied among other languages to Russian, French, Germanand English.the collocational information is listed in a static way.We believe that one of the main drawbacks of the ap-proach is the lack of any predictable calculi on thepossible expressions which can collocate with eachother semant ica l ly .3 The  Computat iona l  Lex ica lSemant ic  ApproachIn order to account for the continuum we find in nat-ural languages, we argue for a continuum perspec-tive, spanning the range from free-combining wordsto idioms, with semantic ollocations and idiosyn-crasies in between as defined in (Viegas and Bouil-lon, 1994):?
f ree -combin ing  words (the girl ate candies)* semant ic  co l locat ions  (fast car; long book) 6?
id iosyncras ies  (large coke; green jealousy)?
id ioms (to kick the (proverbial) bucket)Formally, we go from a purely compositionalapproach in "free-combining words" to a non-compositional approach in idioms.
In between, a(semi-)compositional approach is still possible.
(Vie-gas and Bouillon, 1994) showed that we can reducethe set of what are conventionally considered as id-iosyncrasies by differentiating "true" idiosyncrasies(difficult to derive or calculate) from expressionswhich have well-defined calculi, being compositionalin nature, and that have been called semantic ollo-cations.
In this paper, we further distinguish theiridiosyncrasies into:?
res t r i c ted  semant ic  co -occur rence ,  wherethe meaning of the co-occurrence is semi-compositional between the base and the collo-cate (strong coffee, pay attention, heavy smoker,...)?
res t r i c ted  lexical co -occur rence ,  where themeaning of the collocate is compositional buthas a lexical idiosyncratic behavior (lecture ...student; rancid butter; sour milk).We provide below examples of restricted seman-tic co-occurrences in (1), and restricted lexical co-occurrences in (2).Rest r i c ted  semant ic  co -occur rence  The se-mantics of the combination of the entries is semi-compositional.
In other words, there is an entry in "the lexicon for the base, (the semantic ollocate isencoded inside the base), whereas we cannot directlyrefer to the sense of the semantic collocate in thelexicon, as it is not part of its senses.
We assignthe co-occurrence a new semi-compositional sense,6See (Pustejovsky, 1995) for his account of such expres-sions using a coercion operator.1329where the sense of the base is composed with a newsense for the collocate.
(la) #O=\[key:rel:(lb) #0= \[key:rel:"smoker",\[syntagmatic: LSFIntensity\[base: #0, collocate:\[key: "heavy",gram: \[subCat: Attributive,freq: \[value: 8\]\]\]\]\] ...\]"attention",\[syntagmatic: LSFOper\[base: #0, collocate:\[key: "pay",gram: \[subCat: SupportVerb,freq: \[value: 5\]\]\]\]\] ...\]In examples (1), the LSFs (LSFIntensity, LS-FOper, ...) are equivalent (and some identical) tothe LFs provided in the ECD.
The notion of LSFis the same as that of LFs.
However, LSFs andLFs are different in two ways: i) conceptually, LSFsare organized into an inheritance hierarchy; ii) for-mally, they are rules, and produce a new entry com-posed of two entries, the base with the collocate.As such, the new composed entry is ready for pro-cessing.
These LSFs signal a compositional syntaxand a semi-compositional semantics.
For instance,in (la), a heavy smoker is somebody who smokes alot, and not a "fat" person.
It has been shown thatone cannot code in the lexicon all uses of heavy forheavy smoker, heavy drinker, ....
Therefore, we donot have in our lexicon for heavy a sense for "a lot",or a sense for "strong" to be composed with wine,etc...
It is well known that such co-occurrences arelexically marked; if we allowed in our lexicons a pro-liferation of senses, multiplying ambiguities in anal-ysis and choices in generation, then there would beno limit to what could be combined and we couldend up generating *heavy coffee with the sense of"strong" for heavy, in our lexicon.The left hand-side of the rule LSFIntensity spec-ifies an "Intensity-Attribute" applied to an eventwhich accepts aspectual features of duration.
In(la), the event is smoke.
The LSFIntensity alsoprovides the syntax-semantic interface, allowing foran Adj-Noun construction to be either predicative(the car is red) or attributive (the red car).
Weneed therefore to restrict the co-occurrence to theAttributive use only, as the predicative use is notallowed: (the smoker is heavy) has a literal meaningor figurative, but not collocational.In ( lb) again, there is no sense in the dictionaryfor pay which would mean concentrate.
The rule LS-FOper makes the verb a verbal operator.
No furtherrestriction is required.Rest r i c ted  lexical  co -occur rence  The seman-tics of the combination of the entries is composi-tional.
In other words, there are entries in the lex-icon for the base and the collocate, with the samesenses as in the co-occurrence.
Therefore, we can di-rectly refer to the senses of the co-occurring words.What we are capturing here is a lexical idiosyncrasyor in other words, we specify that we should preferthis particular combination of words.
This is usefulfor analysis, where it can help disambiguate a sense,and is most relevant for generation; it can be viewedas a preference among the paradigmatic family ofthe co-occurrence.
(2a) #O=\[key:tel:"truth",\[syntagmatic: LSFSyn\[base: #0, collocate:\[key: "plain", sense: adj2,Ir: \[comp:no, superl:no\]\]\]\] ...\](2b) #0=\[key:rel:"pupil",\[syntagmatic: LSFSyn\[base: #0, collocate:\[key: "teacher", sense: n2,freq: \[value: 5\]\]\]\]...\](2c) #O=\[key:tel:"conference" ,\[syntagmatic: LSFSyn\[base: #0, collocate:\[key: "student", sense: nl,freq: \[value: 9\]\]\]\] ...\]In examples (2), the LSFSyn produces a new en-try composed of two or more entries.
As such, thenew entry is ready for processing.
LSFSyn signalsa compositional syntax and a compositional seman-tics, and restricts the use of lexemes to be used inthe composition.
We can directly refer to the senseof the collocate, as it is part of the lexicon.In (2a) the entry for truth specifies one co-occurrence (plain truth), where the sense of plainhere is adj2 (obvious), and not say adj3 (flat).
Thesyntagmatic expression inherits all the zones of theentry for "plain", sense adj2, we only code here theirregularities.
For instance, "plain" can be usedas "plainer .
.
.
.
plainest" in its "plain" sense in itsadj2 entry, but not as such within the lexical co-occurrence "*plainer truth", "*plainest truth", wetherefore must block it in the collocate, as expressedin (comp: no, superh no).
In other words, we willnot generate "plainer/plainest truth".
Examples(2b) and (2c) illustrate complex entries as there isno direct grammatical dependency between the baseand the collocate.
In (2b) for instance, we preferto associate teacher in the context of a pupil ratherthan any other element belonging to the paradig-matic family of teacher such as professor, instructor.Formally, there is no difference between the twotypes of co-occurrences.
In both cases, we specifythe base (which is the word described in the en-1330try itself), the collocate, the frequency of the co-occurrence in some corpus, and the LSF which linksthe base with the collocate.
Using the formalismof typed feature structures, both cases are of typeCo-occurrence as defined below:Co-occurrence = \[base: Entry,collocate: Entry,freq: Frequency\] ;3.1 Processing of Syntagrnatic RelationsWe utilize an efficient constraint-based control mech-anism called Hunter-Gatherer (HG)  (Beale, 1997).HG allows us to mark certain compositions as be-ing dependent on each other and then forget about h +them.
Thus, once we have two lexicon entries bitterthat we know go together, HG will ensure that heavythey do.
HG also gives preference to co-occurring bigcompositions.
In analysis, meaning representationsconstructed using co-occurrences are preferred over v +those that are not, and, in generation, realizations opposeinvolving co-occurrences are preferred over equally obligecorrect, but non-cooccurring realizations, rThe real work in processing is making sure that wehave the correct two entries to put together.
In re-striated semantic o-occurrences, the co-occurrencedoes not have the correct sense in the lexicon.
Forexample, when the phrase heavy smoker is encoun-tered, the lexicon entry for heavy would not containthe correct sense.
( la) could be used to create thecorrect entry.
In (la), the entry for smoker containsthe key, or trigger, heavy.
This signals the analyzerto produce another sense for heavy smoker.
Thissense will contain the same syntactic informationpresent in the "old" heavy, except for any modifi-cations listed in the "gram" section (see (la)).
Thesemantics of the new sense comes directly from theLSF.
Generation works the same, except the trig-ger is different.
The input to generation will be aSMOKE event along with an Intensity-Attribute.
(la), which would be used to realize the SMOKEevent, would trigger LSFIntensify which has theIntensity-Attribute in the left hand-side, thus con-firming the production of heavy.Restricted lexical co-occurrences are easier in the v + Nsense that the correct entry already exists in the lexi-con.
The analyzer/generator simply needs to detectthe co-occurrence and add the constraint hat the N + Ncorresponding senses be used together.
In exampleslike (2b), there is no direct grammatical or semanticrelationship between the words that co-occur.
Thus,the entire clause, sentence or even text may have tobe searched for the co-occurrence.
In practice, welimit such searches to the sentence level.7The selection of co-occurrences i  part  of the lexical pro-cess, in other words, if there are reasons not to choose a co-occurrence because of the presence of modifiers or becauseof styl ist ics reasons, the generator will not generate the co-occurrence.3.2 Acquisition of Syntagmatic RelationsThe acquisition of syntagmatic relations is knowl-edge intensive as it requires human intervention.
Inorder to minimize this cost we rely on conceptualtools such as lexical rules, on the LSF inheritancehierarchy.Lexica l  Ru les  in Acquisition The acquisition ofrestricted semantic o-occurrences can be minimizedby detecting rules between different classes of co-occurrences (modulo presence of derived forms in thelexicon with same or subsumed semantics).
Lookingat the following example,N <=> V + Advresentment resent bitterlysmoker smoke heavilyeater eat *biglyhdv <=> Adv + Adj-edstrongly strongly opposedmorally morally obligedwe see that after having acquired with human in-tervention co-occurrences belonging to the A + Nclass, we can use lexical rules to derive the V + Advclass and also Adv + Adj-ed class.Lexical rules are a useful conceptual tool to extenda dictionary.
(Viegas et al, 1996) used derivationallexical rules to extend a Spanish lexicon.
We ap-ply their approach to the production of restrictedsemantic o-occurrences.
Note that eat bigly will beproduced but then rejected, as the form bigly doesnot exist in a dictionary.
The rules overgenerate co-occurrences.
This is a minor problem for analysisthan for generation.
To use these derived restrictedco-occurrences in generation, the output of the lexi-cal rule processor must be checked.
This can be donein different ways: dictionary check, corpus check andultimately human check.Other classes, such as the ones below can beextracted using lexico-statistical tools, such as in(Smadja, 1993), and then checked by a human.pay attention, meet an obligation,commit an offence, ...dance marathon, marriage ceremonyobject of derision .
.
.
.LSFs  and Inheritance We take advantage of 1)the semantics encoded in the lexemes, and 2) an in-heritance hierarchy of LSFs.
We illustrate brieflythis notion of LSF inheritance hierarchy.
For in-stance, the left hand-side of LSFChangeState spec-ifies that it applies to foods (solid or liquid) whichare human processed, and produces the collocatesrancid, rancio (Spanish).
Therefore it could applyto milk, butter, or wine.
The rule would end up1331producing rancid milk, rancid butter, or vino rancio(rancid wine) which is fine in Spanish.
We thereforeneed to further distinguish LSFChangeState intoLSFChangeStateSolid and LSFChangeStateLiquid.This restricts the application of the rule to producerancid butter, by going down the hierarchy.
Thisenables us to factor out information common to sev-eral entries, and can be applied to both types ofco-occurrences.
We only have to code in the co-occurrence information relevant to the combination,the rest is inherited from its entry in the dictionary.4 Conc lus ionIn this paper, we built on a continuum perspec-tive, knowledge-based, spanning the range from free-combining words to  idioms.
We further distin-guished the notion of idiosyncrasies as defined in(Viegas and Bouillon, 1994), into restricted semanticco--occurrences and restricted lexical co-occurrences.We showed that they were formally equivalent, husfacilitating the processing of strictly compositionaland semi-compositional expressions.
Moreover, byconsidering the information in the lexicon as con-straints, the linguistic difference between composi-tionality and semi-compositionality becomes a vir-tual difference for Hunter-Gatherer.
We showedways of minimizing the acquisition costs, by 1) usinglexical rules as a way of expanding co-occurrences, 2)taking advantage of the LSF inheritance hierarchy.The main advantage ofour approach over the ECDapproach is to use the semantics coded in the lex-emes along with the language independent LSF in-heritance hierarchy to propagate r stricted semanticco-occurrences.
The work presented here is completeconcerning representational aspects and processingaspects (analysis and generation): it has been testedon the translations ofon-line unrestricted texts.
Thelarge-scale acquisition of restricted co-occurrences isin progress.5 AcknowledgementsThis work has been supported in part by DoD undercontract number MDA-904-92-C-5189.
We wouldlike to thank Pierrette Bouillon, L~o Wanner andR~mi Zajac for helpful discussions and the anony-mous reviewers for their useful comments.ReferencesS.
Beale.
1997.
HUNTER-GATHERER: ApplyingConstraint Satisfaction, Branch-and-Bound andSolution Synthesis to Computational Semantics.Ph.D.
Diss., Carnegie Mellon University.M.
Benson.
1989.
The Structure of the Colloca-tional Dictionary.
In International Journal of Lex-icography.K.W.
Church and P. Hanks.
1989.
Word Associa-tion Norms, Mutual Information and Lexicogra-phy.
In Proceedings of the 27th Annual Meetingof the Association for Computational Linguistics.F.J.
Hausmann.
1979.
Un dictionnaire des colloca-tions est-il possible ?
In Travaux de Linguistiqueet de Littdrature XVII, 1.U.
Heid.
1979.
D~crire les collocations : deux ap-proches lexicographiques et leur application dansun outil informatisd.
Internal Report, StuttgartUniversity.D.
Heylen.
1993.
Collocations and the Lexicalisa-tion of Semantic Information.
In Collocations, TRET-10/75, Taaltechnologie, Utrecht.L.
Iordanskaja, R. Kittredge and A. Polgu~re.
1991.Lexical Selection and Paraphrase in a Meaning-text Generation Model.
In C. L. Paris, W.Swartout and W. Mann (eds), NLG in AI  andCL.
Kluwer Academic Publishers.J.
Klavans and P. Resnik.
1996.
The Balancing Act,Combining Symbolic and Statistical Approaches toLanguage.
MIT Press, Cambridge Mass., LondonEngland.G.
Lakoff.
1970.
Irregularities in Syntax.
New York:Holt, Rinehart and Winston, Inc.I.
Mel'~uk.
1988.
Paraphrase t lexique dans lath~orie Sens-Texte.
In Bes & Fuchs (ed) Lexique6.S.
Nirenburg and I. Nirenburg.
1988.
A Frameworkfor Lexical Selection in NLG.
In Proceedings ofCOLING 88.J.
Pustejovsky.
1995.
The Generative Lexicon.
MITPress.M.
Ramos, A. Tutin and G. Lapalme.
1994.
LexicalFunctions of Explanatory Combinatorial Dictio-nary for Lexicalization i Text Generation.
In P.St-Dizier & E. Viegas (Ed) Computational LexicalSemantics: CUP.J.
Sinclair.
1991.
Corpus, Concordance, Colloca-tions.
Oxford University Press.F.
Smadja.
1993.
Retrieving Collocations fromTexts: Xtract.
Computational Linguistics, 19(1).E.
Viegas and P. Bouillon.
1994.
Semantic Lexi-cons: the Cornerstone for Lexical Choice in Nat-ural Language Generation.
In Proceedings of the7th INLG, Kennebunkport.E.
Viegas, B. Onyshkevych, V. Raskin and S. Niren-burg.
1996.
From Submit o Submitted via Sub-mission: on Lexical Rules in Large-scale Lexi-con Acquisition.
In Proceedings of the 34th An-nual Meeting of the Association for Computa-tional Linguists.L.
Wanner.
1996.
Lexical Functions in Lexicographyand Natural Language Processing.
John BenjaminPublishing Company.1332
