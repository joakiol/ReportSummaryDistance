Summarizing Information Graphics TextuallySeniz Demir?TUBITAK-BILGEMSandra Carberry?
?University of DelawareKathleen F. McCoy?University of DelawareInformation graphics (such as bar charts and line graphs) play a vital role in manymultimodal documents.
The majority of information graphics that appear in popular mediaare intended to convey a message and the graphic designer uses deliberate communicativesignals, such as highlighting certain aspects of the graphic, in order to bring that messageout.
The graphic, whose communicative goal (intended message) is often not captured by thedocument?s accompanying text, contributes to the overall purpose of the document and cannot beignored.
This article presents our approach to providing the high-level content of a non-scientificinformation graphic via a brief textual summary which includes the intended message and thesalient features of the graphic.
This work brings together insights obtained from empirical studiesin order to determine what should be contained in the summaries of this form of non-linguisticinput data, and how the information required for realizing the selected content can be extractedfrom the visual image and the textual components of the graphic.
This work also presents anovel bottom?up generation approach to simultaneously construct the discourse and sentencestructures of textual summaries by leveraging different discourse related considerations such asthe syntactic complexity of realized sentences and clause embeddings.
The effectiveness of ourwork was validated by different evaluation studies.1.
IntroductionGraphical representations are widely used to depict quantitative data and the relationsamong them (Friendly 2008).
Although some graphics are constructed from raw dataonly for visualization purposes, the majority of information graphics (such as bar chartsand line graphs) found in popular media (such as magazines and newspapers) are?
The Scientific and Technological Research Council of Turkey, Center of Research for AdvancedTechnologies of Informatics and Information Security, Gebze, Kocaeli, TURKEY, 41470.E-mail: senizd@uekae.tubitak.gov.tr.
(This work was done while the author was a graduate student atthe Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716.)??
Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716.E-mail: carberry@cis.udel.edu.?
Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716.E-mail: mccoy@cis.udel.edu.Submission received: 20 April 2010; revised submission received: 8 July 2011; accepted for publication:6 September 2011.?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 3Figure 1Graphic conveying a maximum bar.constructed to convey a message.
For example, the graphic in Figure 1 ostensibly isintended to convey that ?The United States has the highest number of hacker attacksamong the countries listed.?
The graphic designer made deliberate choices in order tobring that message out.
For example, the bar representing the United States is high-lighted with a different color from the other bars and the bars are sorted with respect totheir values instead of their labels so that the bar with the highest value can be easilyrecognized.
Such choices, we argue, are examples of communicative signals that graphicdesigners use.
Under Clark?s definition (1996), language is not just text and utterances,but instead includes any deliberate signal (such as gestures and facial expressions) thatis intended to convey a message; thus an information graphic is a form of language.In popular media, information graphics often appear as part of a multimodaldocument.
Carberry, Elzer, and Demir (2006) conducted a corpus study of informationgraphics from popular media, where the extent to which the message of a graphic isalso captured by the text of the accompanying document was analyzed.
One hundredrandomly selected graphics of different kinds (e.g., bar charts and line graphs) werecollected from newspapers and magazines along with their articles.
It was observedthat in 26% of the instances, the text conveyed only a small portion of the graphic?smessage and in 35% of the instances, the text didn?t capture the graphic?s messageat all.
Thus graphics, together with the textual segments, contribute to the overallpurpose of a document (Grosz and Sidner 1986) and cannot be ignored.
We argue thatinformation graphics are an important knowledge resource that should be exploited,and understanding the intention of a graphic is the first step towards exploiting it.This article presents our novel approach to identifying and textually conveyingthe high-level content of an information graphic (the message and knowledge that onewould gain from viewing a graphic) from popular media.
Our system summarizes thisform of non-linguistic input data by utilizing the inferred intention of the graphic de-signer and the communicative signals present in the visual representation.
Our overallgoal is to generate a succinct coherent summary of a graphic that captures the intendedmessage of the graphic and its visually salient features, which we hypothesize as beingrelated to the intended message.
Input to our system is the intention of the graphicinferred by the Bayesian Inference System (Elzer, Carberry, and Zukerman 2011), andan XML representation of the visual graphic (Chester and Elzer 2005) that specifies thecomponents of the graphic such as the number of bars and the heights of each bar.
Ourwork focuses on the generation issues inherent in generating a textual summary of agraphic given this information.
The current implementation of the system is applicableto only one kind of information graphic, simple bar charts, but we hypothesize that theoverall summarization approach could be extended to other kinds of graphics.528Demir, Carberry, and McCoy Summarizing Information Graphics TextuallyIn this article, we investigate answers to the following questions: (1) Among allpossible information that could be conveyed about a bar chart, what should be includedin its summary?
(2) How should the content of a summary be organized into a coherenttext?
(3) How should the text structure be best realized in natural language?
Given theintended message and the XML representation of a graphic, our system first determinesthe content of the graphic?s summary (a list of propositions) by applying the contentidentification rules constructed for that intended message category.
Our system thenproduces a coherent organization of the selected content by applying a bottom?upapproach which leverages a variety of considerations (such as the syntactic complexityof the realized sentences and clause embeddings) in choosing how to aggregate informa-tion into sentence-sized units.
The system finally orders and realizes the sentence-sizedunits in natural language and generates referring expressions for graphical elementsthat are required in realization.The rest of this article is structured as follows.
Section 2 discusses related workon summarization of non-linguistic input data and describes some natural languageapplications which could benefit from summaries generated by our work.
Section 3outlines our summarization framework.
Section 4 is concerned with identifying thepropositional content of a summary and presents our content-identification rules thatspecify what should be included in the summary of a graphic.
Section 5 describesour bottom?up approach, which applies operators to relate propositions selected forinclusion, explores aggregating them into sentence-sized units, and selects the best orga-nization via an evaluation metric.
Section 6 presents our sentence-ordering mechanism,which incorporates centering theory to specify the order in which the sentence-sizedunits should be presented.
Section 7 describes how our system realizes the selectedcontent in natural language.
Particular attention is devoted to our methodology forgenerating referring expressions for certain graphical elements such as a descriptorof what is being measured in the graphic.
Section 8 presents a user study that wasconducted to evaluate the effectiveness of the generated summaries for the purposesof this research by measuring readers?
comprehension.
Section 9 concludes the articleand outlines our future work.2.
Background2.1 Related WorkThere has been a growing interest in language systems that generate textual summariesof non-linguistic input data (Reiter 2007).
The overall goal of these systems, generallyreferred to as data-to-text systems, is to enable efficient processing of large volumesof numeric data by supporting traditional visualisation modalities and to reducethe effort spent by human experts on analyzing the data.
Various examples of data-to-text systems in the literature include systems that summarize weather forecastdata (Goldberg, Driedger, and Kittredge 1994; Coch 1998), stock market data (Kukich1983), and georeferenced data (Turner, Sripada, and Reiter 2009).One of the most successful data-to-text generation research efforts is the SumTimeproject, which uses pattern recognition techniques to generate textual summaries ofautomatically generated time-series data in order to convey the significant and inter-esting events (such as spikes and oscillations) that a domain expert would recognizeby analyzing the data.
The SumTime-Mousam (Somayajulu, Reiter, and Davy 2003)and SumTime-Turbine (Yu et al 2007) systems were designed to summarize weatherforecast data and the data from gas turbine engines, respectively.
More recently, the529Computational Linguistics Volume 38, Number 3project was extended to the medical domain.
The BabyTalk (Gatt et al 2009) projectproduces textual summaries of clinical data collected for babies in a neonatal intensivecare unit, where the summaries are intended to present key information to medical stafffor decision support.
The implemented prototype (BT-45) (Portet et al 2009) generatesmulti-paragraph summaries from large quantities of heterogeneous data (e.g., timeseries sensor data and the records of actions taken by the medical staff).
The overallgoal of these systems (identifying and presenting significant events) is similar to ourgoal of generating a summary that conveys what a person would get by viewing aninformation graphic, and these systems contend with each of the generation issues wemust face with our system.
Our generation methodology, however, is different from theapproaches deployed in these systems in various respects.
For example, BT-45 producesmulti-paragraph summaries where each paragraph presents first a key event (of highestimportance), then events related to the key event (e.g., an event that causes the keyevent), and finally other co-temporal events.
Our system, on the other hand, producessingle-paragraph summaries where the selected propositions are grouped and orderedwith respect to the kind of information they convey.
In addition, BT-45 performs alimited amount of aggregation at the conceptual level, where the aggregation is usedto express the relations between events with the use of temporal adverbials and cuephrases (such as as a result).
Contrarily, our system syntactically aggregates the selectedpropositions with respect to the entities they share.There is also a growing literature on summarizing numeric data visualized viagraphical representations.
One of the recent studies, the iGRAPH-Lite (Ferres et al2007) system, provides visually impaired users access to the information in a graphic viakeyboard commands.
The system is specifically designed for the graphics that appearin ?The Daily?
(Statistics Canada?s main dissemination venue) and presents the userwith a template-based textual summary of the graphic.
Although this system is veryuseful for in depth analysis of statistical graphs and interpreting numeric data, it isnot appropriate for graphics from popular media where the intended message of thegraphic is important.
In the iGRAPH-Lite system, the summary generated for a graphicconveys the same information (such as the title of the graphic, and the maximum andminimum values) no matter what the visual features of the graphic are.
The content ofthe summaries that our system generates, however, is dependent on the intention andthe visual features of the graphic.
Moreover, that system does not consider many of thegeneration issues that we address in our work.Choosing an appropriate presentation for a large amount of quantitative data isa difficult and time-consuming task (Foster 1999).
A variety of systems were built toautomatically generate presentations of statistical data?such as the PostGraphe sys-tem (Corio and Lapalme 1999; Fasciano and Lapalme 2000), which generates graphicsand complementary text based on the information explicitly given by the user suchas the intention to be conveyed in the graphic and the data of special interest to theuser.
The content of the accompanying text is determined according to the intentionof the graphic and the features of the data.
Moreover, the generated texts are intendedto reinforce some important facts that are visually present in the graphic.
In this re-spect, the generation in PostGraphe is similar to our work, although the output textshave a limited range and are heavily dependent on the information explicitly givenby the user.2.2 Role of Graphical Summaries in Natural Language Applications2.2.1 Accessibility.
Electronic documents that contain information graphics pose chal-lenging problems for visually impaired individuals.
The information residing in the530Demir, Carberry, and McCoy Summarizing Information Graphics Textuallytext can be delivered via screen reader programs but visually impaired individuals aregenerally stymied when they come across graphics.
These individuals can only receivethe ALT text (human-generated text that conveys the content of a graphic) associatedwith the graphic.
Many electronic documents do not provide ALT texts and even in thecases where ALT text is present, it is often very general or inadequate for conveying theintended message of the graphic (Lazar, Kleinman, and Malarkey 2007).Researchers have explored different techniques for providing access to the in-formational content of graphics for visually impaired users, such as sound (Meijer1992; Alty and Rigas 1998), touch (Ina 1996; Jayant et al 2007), or a combinationof the two (Kennel 1996; Ramloll et al 2000).
Unfortunately, these approaches haveserious limitations such as requiring the use of special equipment (e.g., printers andtouch panels) or preparation work done by sighted individuals.
Research has alsoinvestigated language-based accessibility systems to provide access to graphics (Kurze1995; Ferres et al 2007).
As mentioned in Section 2.1, these language-based systemsare not appropriate for graphics in articles from popular media where the intendedmessage of the graphic is important.
We hypothesize that providing alternative accessto what the graphic looks like is not enough and that the user should be providedwith the message and knowledge that one would gain from viewing the graphic.
Weargue that the textual summaries generated by our approach could be associated withgraphics as ALT texts so that individuals with sight impairments would be providedwith the high-level content of graphics while reading electronic documents via screenreaders.2.2.2 Document Summarization.
Research has extensively investigated various techniquesfor single (Hovy and Lin 1996; Baldwin andMorton 1998) and multi-document summa-rization (Goldstein et al 2000; Schiffman, Nenkova, andMcKeown 2002).
The summaryshould provide the topic and an overview of the summarized documents by identifyingthe important and interesting aspects of these documents.
Document summarizersgenerally evaluate and extract items of information from documents according to theirrelevance to a particular request (such as a request for a person or an event) and addressdiscourse related issues such as removing redundancies (Radev et al 2004) and orderingsentences (Barzilay, Elhadad, and McKeown 2002) in order to make the summary morecoherent.It is widely accepted that to produce a good summary of a document, one mustunderstand the document and recognize the communicative intentions of the author.Summarization work primarily focuses on the text of a document but, as mentionedearlier, information graphics are an important part of many multimodal documentsthat appear in popular media and these graphics contribute to the overall commu-nicative intention of the document.
We argue that document summarization shouldcapture the high-level content of graphics that are included in the document, becauseinformation graphics often convey information that is not repeated elsewhere in thedocument.
We believe that the summary of a graphic generated by our system, whichprovides the intended message of the graphic and the information that would beperceived with a casual look at the graphic, might help in summarizing multi-modaldocuments.11 Our colleagues are currently investigating how the findings from this work can be used incommunicating the content of multimodal documents.531Computational Linguistics Volume 38, Number 33.
System OverviewFigure 2 provides an overview of the overall system architecture.
The inputs to oursystem are an XML representation of a bar chart and the intended message of the chart;the former is the responsibility of a Visual Extraction System (Chester and Elzer 2005)and the latter is the responsibility of a Bayesian Inference System (Elzer, Carberry, andZukerman 2011).
Given these inputs, the Content Identification Module (CIM) firstidentifies the salient and important features of a graphic that are used to augment itsinferred message in the summary.
The propositions conveying the selected features andthe inferred message of the graphic are then passed to the Text Structuring and Aggre-gation Module (TSAM).
This module produces a partial ordering of the propositionsaccording to the kind of information they convey, and aggregates them into sentence-sized units.
The Sentence OrderingModule (SOM) then determines the final ordering ofthe sentence-sized units.
Finally, the Sentence Generation Module (SGM) realizes theseunits in natural language, giving particular attention to generating referring expressionsfor graphical elements when appropriate.
In the rest of this section, we briefly presentthe systems that provide input to our work and describe the corpus of bar chartsused for developing and testing our system.
The following sections then describe themodules implemented within our system in greater detail, starting from the ContentIdentification Module.Figure 2System architecture.532Demir, Carberry, and McCoy Summarizing Information Graphics Textually3.1 Visual Extraction SystemThe Visual Extraction System (Chester and Elzer 2005) analyzes a graphic image (visualimage of a bar chart) and creates an XML representation specifying the componentsof the graphic, such as the height and color of each bar, any annotations on a bar, thecaption of the graphic, and so forth.
The current implementation handles vertical andhorizontal bar charts that are clearly drawn with specific fonts and no overlappingcharacters.
The charts can have a variety of textual components such as axis labels,caption, further descriptive text, text inside the graphic, and text below the graphic.The current system cannot handle 3D charts, charts where the bars are represented byicons, or charts containing texts at multiple angles, however.3.2 Bayesian Inference System for Intention RecognitionThe Bayesian Inference System (Elzer, Carberry, and Zukerman 2011) treats an informa-tion graphic as a form of language with a communicative intention, and reasons aboutthe communicative signals present in the graphic to recognize its intendedmessage.
Thesystem is currently limited to simple bar charts and takes as input the XML representa-tion of the chart produced by the Visual Extraction System described previously.Three kinds of communicative signals that appear in bar charts are extracted from agraphic and utilized by the system.
The first kind of signal is the relative effort requiredfor various perceptual and cognitive tasks.
The system adopts the AutoBrief (Kerpedjievand Roth 2000) hypothesis that the graphic designer chooses the best design to facilitatethe perceptual and cognitive tasks that a viewer will need to perform on the graphic.Thus, the relative effort for different perceptual tasks serves as a communicative signalabout what message the graphic designer intended to convey (Elzer et al 2006).
Thesecond and third types of communicative signals used in the system are salience andthe presence of certain verbs and adjectives in the caption that suggest a particularmessage category.
The presence of any of these three kinds of communicative signalsare entered into a Bayesian network as evidence.
The top level of the network capturesone of the 12 message categories that have been identified as the kinds of messages thatcan be conveyed by a bar chart, such as conveying a change in trend (Changing Trend)or conveying the bar with the highest value (Maximum Bar).
The system produces asoutput the hypothesized intended message of a bar chart as one of these 12 messagecategories, along with the instantiated parameters of the message category, in the formof a logical representation such as Maximum Bar(first bar) for the graphic in Figure 1and Increasing Trend(first bar, last bar) for the graphic in Figure 3a.3.3 Corpus of GraphicsWe collected 82 groups of graphics along with their articles from 11 different magazines(such as Newsweek and Business Week) and newspapers.
These groups of graphicsvaried in their structural organization: 60% consisted solely of a simple bar chart (e.g.,the graphic in Figure 1 on Page 2) and 40% were composite graphics (e.g., the graphicin Figure 8a in Section 7.1.1) consisting of at least one simple bar chart along withother bar charts or other kinds of graphics (e.g., stacked bar charts or line graphs).
Weselected at least one simple bar chart from each group and our corpus contained a totalof 107 bar charts.
The Bayesian Inference System had an overall success rate of 79.1% inrecognizing the correct intended message for the bar charts in our corpus using leave-one-out cross-validation (Elzer, Carberry, and Zukerman 2011).533Computational Linguistics Volume 38, Number 3Figure 3(a) Graphic conveying an increasing trend.
(b) Graphic conveying the ranking of all bars.In the work described in this article, we only used the bar charts whose intendedmessage was correctly recognized by the Bayesian Inference System and associated eachchart with the inferred message category.
Here, our intent is to describe a generationapproach that works through a novel problem from beginning to end by handling amultitude of generation issues.
Thus, using bar charts with the perfect intention isreasonably appropriate within the scope of the present work.
For each bar chart, wealso used the XML representation that was utilized by the Bayesian Inference System.Slightly less than half of the selected bar charts were kept for testing the system per-formance (which we refer to as the test corpus), and the remaining graphs were usedfor developing the system (which we refer to as the development corpus).
Because thenumber of graphics in the development corpus was quite limited, we constructed anumber of bar charts2 in order to examine the effects of individual salient featuresobserved in the graphics from the development corpus.
These graphs, most of whichwere obtained by modifying original graphics, enabled us to increase the number ofgraphics in the development corpus and to explore the system behavior in various newcases.4.
Content Identification Module (CIM)Our ultimate goal is to generate a brief and coherent summary of a graphic.
Identifyingand realizing the high-level informational content of a graphic is not an easy task,however.
First, a graphic depicts a large amount of information and therefore it wouldbe impractical to attempt to provide all of this information textually to a user.
Second, agraphic is chosen as the communication medium because a reader can get informationfrom it at many different levels.
A casual look at the graphic is likely to convey theintended message of the graphic and its salient features.
At the same time, a readercould spend much more time examining the graphic to further investigate somethingof interest or something they noticed during their casual glance.In order to address the task of identifying the content of a summary, we extendto simple bar charts the insights obtained from an informal experiment where humanparticipants were asked to write a brief summary of a series of line graphs with thesame high-level intention (McCoy et al 2001).
The most important insight gained from2 The graphics that we constructed were not used in any of the evaluation experiments with humanparticipants described throughout this article.534Demir, Carberry, and McCoy Summarizing Information Graphics Textuallythis study is that the intended message of a graphic was conveyed in all summaries nomatter what the visual features of the graphic were.
It was observed that the participantsaugmented the intended message with salient features of the graphic (e.g., if a linegraph is displaying an increasing trend and the variance in that trend is large, thenthe variance is salient) and that what was found salient depended on the graphic?sintended message.
Because the participants generated similar summaries for a par-ticular graphic, we hypothesize that they perceived the same salient features for thatgraphic.
Although the set of features that might be salient is the same for differentgraphics sharing the same underlying intention, the differences observed between thesummaries generated for different graphics with the same intention can be explainedby whether or not the features are salient in those graphics.
The fact that the summariesdid not include all information that could be extracted from the graphic (such asthe value of every point in a line graph) but only visually salient features, correlateswith Grice?s Maxim of Quantity (1975) which states that one?s discourse contributionshould be as informative as necessary for the purposes of the current exchange but notmore so.To extend these observations to constructing brief summaries of bar charts, wehypothesize that (1) the intended message of the bar chart should form the core ofits textual summary and (2) the most significant and salient features of the bar chart,which are related to its intended message, should be identified and included in thatsummary.
The inferred intended message of a bar chart serves as a starting point forour content identification approach.
In the rest of this section, we first describe a seriesof experiments that we conducted to identify what constitutes the salient features ofa given bar chart and in which circumstances these features should be included in itstextual summary.
We then present the content identification rules that were constructedto automatically select appropriate content for the summary of a bar chart.4.1 ExperimentsWe conducted a set of formal experiments to find patterns between the intended mes-sage of a graphic, salient visual features of the displayed data, and the propositionsselected for inclusion in a brief summary.
We identified the set of all propositions(PROPALL) that capture information that we envisioned someone might determineby looking at a bar chart.
This set included a wide variety of pieces of informationpresent in a bar chart and contained propositions common to all bar charts as wellas propositions which were applicable only to some of the message categories.
Thefollowing is a subset of the identified propositions.
In this example, Propositions 1?4are common to all bar charts; in contrast, Propositions 5?8 are only present when thebar chart is intended to convey a trend: The labels of all bars (Proposition 1) The value of a bar (Proposition 2) The percentage difference between the values of two bars (Proposition 3) The average of all bar values (Proposition 4) The range of the bar values in the trend (Proposition 5) The overall percentage change in the trend (Proposition 6)535Computational Linguistics Volume 38, Number 3 The change observed at a time period (Proposition 7) The difference between the largest and the smallest changes observed inthe trend (Proposition 8)Some propositions, which we refer to as open propositions, require instantiation(such as Propositions 2, 3, and 7 given here) and the information that they convey variesaccording to their instantiations.3 In addition, the instantiation of an open propositionmay duplicate another proposition.
For example, if the Proposition 3 is instantiatedwiththe first and the last bars of the trend, then the information conveyed by that propositionis exactly the same as Proposition 6.To keep the size of the experiment reasonable, we selected 8 message categoriesfrom among the 12 categories that could be recognized by the Bayesian Inference Sys-tem; these categories were the onesmost frequently observed in our corpus and could beused as a model for the remaining message categories.
These categories were IncreasingTrend, Decreasing Trend, Changing Trend, Contrast Point with Trend, Maximum Bar,Rank Bar, Rank All, and Relative Difference.
In the experiments, we did not use thecategoriesMinimumBar (which can bemodeled viaMaximumBar), Relative Differencewith Degree (which can be modeled via Relative Difference), Stable Trend (which wasnot observed in the corpus), and Present Data (which is the default category selectedwhen the system cannot infer an intended message for the graphic).For each message category, we selected two to three original graphics from thedevelopment corpus, where the graphics with the same intended message presenteddifferent visual features.
For example, we selected two graphics conveying that a par-ticular bar has the highest value among the bars listed, but only in one of these graphicswas the value of the maximum bar significantly larger than the values of the other bars(such as the graphic in Figure 1).
In total, 21 graphics were used in the experiments andthese graphics covered all selected intended message categories.
Because the number ofpropositions applicable to each message category was quite large, 10?12 propositionswere presented for each graphic.
Each graphic was presented to at least four partici-pants.
Overall, the experiments covered all selected intended message categories andall identified propositions.Twenty participants, who were unaware of our system, participated in the experi-ments.
The participants were graduate students or recent Ph.D. graduates from a varietyof departments at the University of Delaware.
Each experiment started with a briefdescription of the task, where the participants were told to assume that in each casethe graphic was part of an article that the user is reading and that the most importantinformation depicted in the graphic should be conveyed in its summary.
They were alsotold that they would be given an information graphic along with a sentence conveyingthe intended message of the graphic and a set of propositions, and would be asked toclassify these additional propositions into one of three classes according to how impor-tant they felt it was to include that proposition in the textual summary:4 (1) Essential:This proposition should be included in the brief textual summary, (2) Possible: Thisproposition could be included in the brief textual summary but it?s not essential, and (3)Not Important: This proposition should not be included in the brief textual summary.3 We used open propositions in order to keep PROPALL within a manageable size.4 The participants were also asked to instantiate the open propositions that they classified as Essential orPossible.536Demir, Carberry, and McCoy Summarizing Information Graphics Textually4.2 AnalysisTo analyze the experiment?s results, we first assigned a numeric score to eachclass indicating the level of importance assigned by the participants: Essential = 3,Possible = 1, Not-important = 0.
We then calculated an ?importance level?
(IL) for eachproposition with respect to a particular graphic, where the importance level estimateshow important it is for that proposition to be included in the graphic?s summary.The importance level of a proposition was computed by summing the numericscores associated with the classes assigned by the participants.
For example, if threeparticipants classified a proposition as Essential and two participants as Possible, theimportance level of that proposition in the graphic was (3?
3) + (2?
1) = 11.
In caseswhere a proposition (Prop A) and an instantiated open proposition which conveyedthe same information were classified by a participant into different classes for the samegraphic, the classification of the proposition that came earlier in the presentation wasused in computing the importance level of Prop A.Given these computed scores, we needed to identify which propositions to con-sider further for inclusion in a summary.
Because there was a divergence betweenthe sets of propositions that were classified as essential by different participants, wedecided to capture the general tendency of the participants.
For this purpose, wedefined majority importance level as a ranking criteria, which is the importance levelthat would be obtained if half of the participants classify a proposition as essential.For example, the majority importance level would be (6?
3)/2 = 9 if there were sixparticipants.
We classified a proposition as a highly rated proposition if its importancelevel was equal to or above the majority importance level.5 The propositions that wereclassified as highly rated for the graphics with a message category formed the set ofhighly rated propositions that should be considered for inclusion for that messagecategory.We had to ensure that the propositions presented to the participants (PROPALL)actually covered all information that is important enough to include in the summary ofa bar chart.
Thus, for each graphic, we also asked participants if there was anything elsethey felt should be included in the brief summary of the graphic.
We received only afew isolated suggestions such as a proposition conveying what type of a curve could fitthe trend.
Moreover, these suggestions were not common among the participants, andnothing was mentioned by more than one participant (indeed most did not make anysuggestions).
Thus, we concluded that these suggestions were not appropriate for thetextual summary of a bar chart.4.3 Content Identification Rules for Message CategoriesUsing the importance level scores, we needed to identify the subset of the highly ratedpropositions that should be included in the textual summary in addition to the graphic?sintended message.
For each message category, we examined the similarities and thedifferences between the sets of highly rated propositions identified for the graphics5 The reason behind assigning particular scores (3,1,0) to the classes is to guarantee that a proposition willnot be selected as a highly rated proposition if none of the participants thought that it was essential.Assume k participants classified a proposition (Prop A).
The majority importance level of this proposition(MIL(Prop A)) is (3?
k)/2.
A proposition is classified as highly rated if its importance level (IL(Prop A))is equal to or greater than the majority importance level (IL(Prop A) ?MIL(Prop A)).
If all of theparticipants classified the proposition as Possible, the IL(Prop A) is 1?
k, which is less than MIL(Prop A).537Computational Linguistics Volume 38, Number 3associated with that message category, related these differences to the visual featurespresent in these graphics, and constructed a set of content identification rules foridentifying propositions to be included in the summary of a graphic from that messagecategory.
If a proposition was marked as highly rated for all graphics in a particularmessage category, then its selection was not dependent on particular visual featurespresent in these graphics.
In such cases, our content identification rule simply states thatthe proposition should be included in the textual summary for every graphic whoseinferred message falls into that message category.
For the other propositions that arehighly rated for only a subset of the graphics in a message category, we identified a fea-ture that was present in the graphics where the proposition was marked as highly ratedand was absent when it was not marked as highly rated, and our content identificationrules use the presence of this feature in the graphic as a condition for the propositionto be included in the textual summary.
In addition, we observed that a highly ratedproposition for a message category might require inclusion of another proposition forrealization purposes.
For example, in the Rank All message category, the propositionindicating the rank of each bar was identified as highly rated and thus could be includedin the textual summary.
Because the rank of a bar cannot be conveyed without its label,we added the proposition indicating the label of each bar to the content identificationrule containing the rank proposition, although this extra proposition was not explicitlyselected by the participants for inclusion.
Notice that these steps?identifying featuresthat distinguish one subset of graphs from the other and identifying propositions thatneed to be included for realizing other propositions?make it difficult to use machinelearning for this task.
In our case the number of possible features that can be extractedfrom a graphic is very large and it is difficult to know which features from amongthose may be important/defining in advance.
In addition, the number of graphics inour development corpus is too small to expect machine learning to be effective.The following are glosses of two partial sets of representative content identificationrules.
The first set is applicable to a graphic conveying an increasing trend and thesecond set is applicable to a graphic conveying the rankings of all bars present in thegraph: Increasing Trend message category:61.
If (message category equals ?increasing trend?)
theninclude(proposition conveying the rate of increase of the trend):Include the proposition conveying the rate of increase of the trend2.
If (message category equals ?increasing trend?)
andnotsteady7(trend) then include(proposition conveying theperiod(s) with a decrease):8If the trend is not steady and has variability, then include the propositionindicating where the trend varies6 The ?notsteady?
function returns true if its argument is not a steady trend; the ?value?
function returnsthe values of all members of its argument; the ?greaterthan?
function returns true if the left argument isgreater than the right argument; the ?withinrange?
function returns true if all members of its leftargument are within the range given by its right argument; the ?average?
function returns the average ofthe values of all members of its argument.7 A trend is unsteady if there is at least one period with a decrease in contrast with the increasing trend.8 The inclusion of propositions whose absence might lead the user to draw false conclusions is consistentwith Joshi, Webber, and Weischedel?s (1984) maxim, which states that a system should not only producecorrect information but should also prevent the user from drawing false inferences.538Demir, Carberry, and McCoy Summarizing Information Graphics Textually3.
If (message category equals ?increasing trend?)
and (value(last bar)greaterthan (3*value(first bar))) then include(propositionconveying the overall percentage increase in the trend):If the overall percentage increase in the trend is significantly large, theninclude the proposition conveying the percentage increase in the trend Rank All message category:1.
If (message category equals ?rank all?)
then include(propositionsconveying the label and the value of the highest bar):Include the propositions conveying the label and the value of thehighest bar2.
If (message category equals ?rank all?)
and (value(all bars)withinrange ((0.7*average(all bars)),(1.3*average(all bars)))) theninclude(proposition indicating that the bar values vary slightly):If the values of bars are close to each other, then include the propositionindicating that the bar values vary slightly3.
If (message category equals ?rank all?)
and (not(value(all bars)withinrange ((0.7*average(all bars)),(1.3*average(all bars))))) theninclude(propositions conveying the label and the value of thelowest bar):If the values of bars are not close to each other, then include thepropositions conveying the label and the value of the lowest barWe defined the conditions of all content identification rules as a conjunction of oneor more expressions where some expressions required us to determine threshold valuesto be used for comparison purposes.
For example, we observed that the propositionconveying the overall percentage change in the trend was marked as highly ratedonly for graphics which depicted a significant change in the trend.
We handled thissituation for graphics with an increasing trend by defining the third content identi-fication rule (shown earlier) where we needed to set the lowest threshold at whichan overall increase observed in a trend can be accepted as significantly large.
Forsetting such threshold values, we examined all graphs in the development corpusto which the corresponding content identification rule is applicable (i.e., the graphsassociated with the message category for which the rule is defined) and used ourintuitions about whether the proposition captured by the rule should be selected forinclusion in the summaries of these graphs.
We set the threshold values using theresults obtained from group discussions such that the final setting classified all ofthe original graphics the way the participants did in the experiments described inSection 4.1.When the content identification rules constructed for the Increasing Trend messagecategory are applied to the bar chart in Figure 3a, the following pieces of informationare selected for inclusion in addition to the intended message of the graphic: The rate of increase of the trend, which is slight The small drop observed in the year 1999 The overall percentage increase in the trend, which is 225%539Computational Linguistics Volume 38, Number 3When the content identification rules constructed for the RankAll message categoryare applied to the bar chart in Figure 3b, the following pieces of information are selectedfor inclusion in addition to the intended message of the graphic: The label and the value of the highest bar, which is Army with 233,030 The label and the value of the lowest bar, which is Other defense agencieswith 100,678 The label and the ranking of each bar:9 Army is the highest, Navy is thesecond highest, Air Force is the third highest, and Other defense agenciesis the lowest4.4 Evaluation of the Content Identification ModuleWe conducted a user study to assess the effectiveness of our content identificationmodule in identifying the most important information that should be conveyed abouta bar chart.
More specifically, the study had three goals: (1) to determine whether theset of highly rated propositions that we identified for each message category containsall propositions that should be considered for inclusion in the summaries of graphicswith that message category; (2) to determine how successful our content identificationrules are in selecting highly rated propositions for inclusion in the summary; and (3)to determine whether the information conveyed by the highly rated propositions ismisleading or not.Nineteen students majoring in different disciplines (such as Computer Science andMaterials Science and Engineering) at the University of Delaware were participantsin the study.
These students neither participated in the earlier study described in Sec-tion 4.1 nor were aware of our system.
Twelve graphics from the test corpus (describedin Section 3.3) whose intended message was correctly identified by the Bayesian Infer-ence System were used in the experiments.
Once the intended message was recognized,the corresponding content identification rules were executed in order to determine thecontent of the graphic?s summary.
Prior to the experiment, all participants were toldthat they would be given a summary and that it should include the most importantinformation that they thought should be conveyed about the graphic.
Each participantwas presented with three graphics from among the selected graphics such that eachgraphic was viewed by at least four participants.
For each graphic, the participantswere first given the summary of the graphic generated by our approach and then shownthe graphic.
The participants were then asked to specify if there was anything omittedthat they thought was important and therefore should be included in the summary.
Inaddition, the participants were asked to specify whether or not they were surprisedor felt that the summary was misleading (i.e., whether the bar chart was similar towhat they expected to see after reading its summary).
Note that our summaries withrelatively few propositions are quite short.
Thus our evaluation focused on determin-ing whether anything of importance was missing from the summary or whether thesummary was misleading.
In the experiments, we did not ask the participants to rate9 This piece of information is selected by a rule defined for the Rank All message category not shown in thebulleted list on the previous page.540Demir, Carberry, and McCoy Summarizing Information Graphics Textuallythe content of summaries on a numeric scale in order to restrict them to evaluating onlythe selected content as opposed to its presentation (i.e., the organization and realizationof the summary).Feedback that we received from the participants was very promising.
In most ofthe cases (43 out of 57 cases), the participants were satisfied with the content that ourapproach selected for the presented bar charts.
There were a number of suggestions forwhat should be added to the summaries in addition towhat had already been conveyed,and in a couple of these cases, we observed that a highly rated proposition which wasnot selected by the corresponding content identification rule was contrarily suggestedby the participants.
There was no consensus in these suggestions, however, as nonewas made by more than two participants.
Some of the participants (3 out of 19) evencommented that we provided more information than they could easily get from justlooking at the graphic.
In addition, a few participants (2 out of 19) commented that,in some graphics, they didn?t agree with the degree (e.g., moderate or steep) assessedby our approach for differences between bar values (e.g., the rate of change of thetrend), and therefore they thought the summary was misleading.
Because there wasn?tany common consensus among the participants, we didn?t address this very subjectiveissue.
Overall, we conclude that the sets of highly rated propositions that we identifiedcontain the most important information that should be considered for inclusion in thesummaries of bar charts and that our system effectively selects highly rated propositionsfor inclusion when appropriate.5.
Text Structuring and Aggregation Module (TSAM)A coherent text has an underlying structure where the informational content is pre-sented in some particular order.
Good text structure and information ordering haveproven to enhance the text?s quality by improving user comprehension.
For example,Barzilay, Elhadad, and McKeown (2002) showed that the ordering has a significantimpact on the overall quality of the summaries generated in theMULTIGEN system.
Al-though previous research highlights a variety of structuring techniques, there are threeprominent approaches that we looked to for guidance: top?down planning, applicationof schemata, and bottom?up planning.In top?down planning (Hovy 1988, 1993; Moore and Paris 1993), the assumption isthat a discourse is coherent if the hearer can recognize the communicative role of eachof its segments and the relation between these segments (generally mapped from theset of relations defined in rhetorical structure theory (RST; Mann and Thompson 1987).The discourse is usually represented as a tree-like structure and the planner constructsa text plan by applying plan operators starting from the initial goal.In the TEXT system (McKeown 1985), a collection of naturally occurring textswere analyzed to identify certain discourse patterns for different discourse goals, andthese patterns were represented as schemas which are defined in terms of rhetoricalpredicates.
The schemas both specify what should be included in the generated textsand how they should be ordered given a discourse goal.
Lester and Porter (1997)used explanation design packages, schema-like structures with procedural constructs(for example, the inclusion of a proposition can be constrained by a condition), inthe KNIGHT system, which is designed to generate explanations from a large-scalebiology knowledge base.
Paris (1988) applied the idea of schemata in the TAILORsystem to tailor object descriptions according to the user?s level of knowledge aboutthe domain.541Computational Linguistics Volume 38, Number 3Marcu (1998) argued that text coherence can be achieved by satisfying local con-straints on ordering and clustering of semantic units to be realized.
He developed aconstraint satisfaction based approach to select the best plan that can be constructedfrom a given set of textual units and RST relations between them, and showed thatsuch bottom?up planning overcomes the major weakness of top?down approachesby guaranteeing that all semantic units are subsumed by the resulting text plan.
TheILEX system (O?Donnell et al 2001), which generates descriptions for exhibits in amuseum gallery, utilizes a similar bottom?up planning approach (Mellish et al 1998)where the best rhetorical structure tree over the semantic units is used as the textstructure.Because our content identification rules identify a set of propositions to be con-veyed, it appears that a bottom?up approach that ensures that all propositions will beincluded is in order.
At the same time, it is important that our generated text adheres toan overall discourse organization such as is provided by the top?down approaches.Because of the nature of the propositions (the kinds of rhetorical relations that canexist between propositions in a descriptive domain are arguably limited [O?Donnellet al 2001]), however, a structure such as RST is not helpful here.
Thus, the top?downplanning approach does not appear to fit.
Although something akin to a schema mightwork, it is not clear that our individual propositions fit into the kind of patterns usedin the schema-based approach.
Instead we use what can be considered a combinationof a schema and a bottom?up approach to structure the discourse.
In particular, weuse the notion of global focus (Grosz and Sidner 1986) and group together proposi-tions according to the kind of information they convey about the graphic.
We definethree proposition classes (message-related, specific, and computational) to classify thepropositions selected for inclusion in a textual summary.
The message-related classcontains propositions that convey the intended message of the graphic.
The specificclass contains the propositions that focus on specific pieces of information in thegraphic, such as the proposition conveying the period with an exceptional drop in agraphic with an increasing trend or the proposition conveying the period with a changewhich is significantly larger than the changes observed in other periods in a graphicwith a trend.
Lastly, propositions in the computational class capture computations orabstractions over the whole graphic, such as the proposition conveying the rate ofincrease in a graphic with an increasing trend or the proposition conveying the overallpercentage change in the trend.
In our system, all propositions within a class willbe delivered as a block.
But we must decide how to order these blocks with respectto each other.
In order to emphasize the intended message of the graphic (the mostimportant piece of the summary), we hypothesize that the message-related propositionsshould be presented first.
We also hypothesize that it is appropriate to close the textualsummary by bringing the whole graphic back into the user?s focus of attention (Groszand Sidner 1986) (via the propositions in the computational class).
Thus, we define anordering of the proposition classes (creating a partial ordering over the propositions)and present first the message-related propositions, then the specific propositions, andfinally the computational propositions.
Section 6 will address the issue of ordering thepropositions within these three classes.5.1 Representing Summary ContentFirst we needed to have a representation of content that would provide us with themost flexibility in structuring and realizing content.
For this we used a set of basicpropositions.
These were minimal information units that could be combined to form542Demir, Carberry, and McCoy Summarizing Information Graphics Textuallythe intended message and all of the propositions identified in our content identificationrules.
This representation scheme increases the number of aggregation and realizationpossibilities that could be explored by the system, which is described in the nextsubsection.
We defined two kinds of knowledge-base predicates to represent the basicpropositions:(1) Relative Knowledge Base: These predicates are used to represent the basicpropositions which introduce graphical elements or express relationsbetween the graphical elements.
(2) Attributive Knowledge Base: These predicates are used to represent thebasic propositions which present an attribute or a characteristic of agraphical element.Each predicate contains at least two arguments and we refer to the first argumentas the main entity and the others as the secondary entities.
The main entity of eachpredicate is a graphical element and the secondary entities are either a string constantor a graphical element.
Some of the graphical elements that we used in this work are asfollows: graphic: ?the graphic itself? trend: ?the trend observed in the graphic? descriptor: ?a referring expression that represents what is being measuredin the graphic?10 bar(x): ?a particular bar in the graphic?1 <= x <= n where n = number of bars in the graph all bars: ?all bars depicted in the graphic?
bset = {bar(x) | 1 <= x <= n} period(x,y): ?a period depicted in the graphic?
1 ?
x < n and 1 < y ?
n change(x,y): ?
the change between the values of any two bars?1 ?
x < n and 1 < y ?
n all changes: ?changes between all pairs of bars of the graphic?cset = {change(x, y) | 1 ?
x < n, 1 < y ?
n} trend period: ?the period over which the trend is observed? graph period: ?the period which is depicted by the graphic? trend change: ?the overall change observed in the trend?Table 1 presents sample instantiations of a subset of the predicates that we definedfor this work along with a possible realization for each instantiation.
Although thenumber of arguments in Relative Knowledge Base predicates (predicates 1 to 15) varies,10 How that referring expression is extracted from the text associated with the graphic is described indetail in Section 7.1.
For example, the descriptor identified by our system for the graphic in Figure 4is the dollar value of net profit.543Computational Linguistics Volume 38, Number 3Table 1Sample instantiations and possible realizations of a subset of our predicates.1 shows(graphic,trend)The graphic shows a trend2 focuses(graphic,bar(3))The graphic is about the third bar3 covers(graphic, graph period)The graphic covers the graph period4 exists(trend,descriptor)The trend is in the descriptor5 has(trend,trend period)11The trend is over the trend period6 starts(trend period,?2001?
)The trend period starts at the year 20017 ends(trend period,?2010?
)The trend period ends at the year 20108 ranges(descriptor,?from?,?20 percent?,trend period)The descriptor ranges from 20 percent over the trend period9 hasextreme(descriptor,?largest?,change(3,4),period(3,4))The descriptor shows the largest change between the third and the fourth bars10 averages(descriptor,all bars,?55.4 billion dollars?
)The descriptor for all bars averages to 55.4 billion dollars11 comprises(descriptor,trend change,trend period)The descriptor comprises a trend change over the trend period12 occurs(change(2,3),period(2,3))A change occurs between the second and the third bars13 hasdifference(change(1,5),bar(1),bar(5),descriptor)A difference is observed between the descriptor of the first bar and that of the fifth bar14 observed(all changes,?every?,interval,trend period)Changes are observed every interval over the trend period15 presents(descriptor,bar(3),?12 percent?
)The descriptor for the third bar is 12 percent16 hasattr(trend change,?type?,?increase?
)The trend change is an increase17 hasattr(change(2,3),?degree?,?moderate?
)The change is of degree moderate18 hasattr(change(2,3),?amount?,?70 dollars?
)The change amount is 70 dollars19 hasattr(trend change,?percentage amount?,?22 percent?
)The trend change percentage amount is 22 percent20 hasattr(all changes,?rate?,?slight?
)Changes are slight changesall Attributive Knowledge Base predicates (encoded as hasattr) consist of three argu-ments, where the first argument is the graphical element being described, the secondis an attribute of the graphical element, and the third is the value of that attribute(predicates 16 to 20).1211 Notice that the graphical element trend period in 5 is the main entity in 6 and 7.
These all might becombined using the And operator to produce the realization The trend starts at the year 2001 and endsat the year 2010.12 Because all Attributive Knowledge Base predicates have the same form, the amount and unit of a changeare represented as a single string which is derived from the textual components of the graphic (such as70 dollars in Predicate 18).544Demir, Carberry, and McCoy Summarizing Information Graphics TextuallyFigure 4Graphic conveying a decreasing trend.For example, consider how the propositions given in Section 4.1 can be representedwith the predicates shown in Table 1.
Some of those propositions require a singlepredicate.
For example, the proposition conveying the value of a bar (Proposition 2)can be represented via the predicate ?presents?
(Predicate 15) and the propositionconveying the average of all bar values (Proposition 4) via the predicate ?aver-ages?
(Predicate 10).
On the other hand, some propositions require more than onepredicate.
For example, the proposition conveying the overall percentage changein the trend shown in Figure 4 (Proposition 6) can be represented via the predi-cates ?comprises(descriptor,trend change,trend period)?
(Predicate 11), ?starts(trendperiod,?1998?)?
(Predicate 6), ?ends(trend period,?2006?)?
(Predicate 7), ?hasattr(trendchange,?type?,?decrease?)?
(Predicate 16), and ?hasattr(trend change,?percentageamount?,?65 percent?)?
(Predicate 19).
The same set of predicates can be used to rep-resent the overall amount of change in the trend by replacing the constant ?percentageamount?
with the string ?amount?
in Predicate 19.As is shown by the possible realizations included in Table 1, each basic propositioncan be realized as a single sentence.
Although we determined a couple of differentways (i.e., simple sentences) of realizing each basic proposition, our current imple-mentation always chooses a single realization (which we refer to as ?the realizationassociated with the proposition?)
and the main entity is always realized in subjectposition.135.2 Aggregating Summary ContentThe straightforward way of presenting the informational content of a summary is toconvey each proposition as a single sentence while preserving the partial ordering ofthe proposition classes.
The resultant text would not be very natural and coherent,however.
Aggregation is the process of removing redundancies during the generationof a more concise and fluent text (Shaw 1998; Dalianis 1999).
Aggregation (typicallysyntactic aggregation [Reiter and Dale 2000]) has received considerable attentionfrom the NLG community (McKeown et al 1997; O?Donnell et al 2001; Barzilay andLapata 2006), and has been applied in various existing generation systems such as theintelligent tutoring application developed by Di Eugenio et al (2005).
Our aggregationmechanism works to combine propositions into more complex structures.
It takes13 We leave it as a future work to explore how different realizations for a proposition, including oneswhere the main entity is not in subject position, can be utilized by our approach.545Computational Linguistics Volume 38, Number 3advantage of the two types of predicates (Relative Knowledge Base and AttributiveKnowledge Base predicates) and the shared entities between predicates.
In order torelate propositions and explore syntactically aggregating them, our mechanism treatseach proposition as a single node tree which can be realized as a sentence and attemptsto form more complex trees by combining individual trees via four kinds of operatorsin such a way so that the more complex tree (containing multiple propositions) can stillbe realized as a single sentence.
The first operator (Attribute Operator) works only onpropositions with an Attributive Knowledge Base predicate and essentially identifiesopportunities to realize such a proposition as an adjective attached to a noun objectin the realization of another proposition.
The remaining three operators, which donot work on propositions with an Attributive Knowledge Base predicate, introducenew nodes corresponding to operational predicates (And, Same, and Which) with asingle entity into the tree structures.
Two of these operators (And Operator and WhichOperator) work on trees rooted by a proposition with a Relative Knowledge Base or anAnd predicate.
These operators look for opportunities for VP conjunction and relativeclauses, respectively.
The third operator (Same Operator) works on trees rooted by aproposition with a Relative Knowledge Base predicate and identifies opportunities forNP conjunction.
Although each predicate is associated with a unique realization in thecurrent implementation, none of these operators depend on how the correspondingpredicates or the entities in those predicates are realized.Having defined the operators we next had to turn to the problem of determininghow these operators should be applied (e.g., which combinations are preferred).
Theoperators we defined are similar to the clause-combining operations used by the SPoTsentence planner (Walker, Rambow, and Rogati 2002; Stent, Prasad, and Walker 2004;Walker et al 2007) in the travel planner system AMELIA.
In AMELIA, for each ofthe 100 different input text plans, a set of possible sentence plans (up to 20 plans)were generated by randomly selecting which operations to apply according to assumedpreferences for operations.
These possible sentence plans were then rated by two judgesand the collected ratings were used to train the SPoT planner.
Although we greatlydrew from the work on SPoT as we developed our aggregation method, we chosenot to follow their learning methodology.
In the SPoT system, some of the featureswere domain- and task-dependent and thus porting to a new domain would requireretraining.
In addition, the judgments of the two raters were collected in isolation andit is unclear how these would translate to the task situation the texts were intendedfor.
Although this methodology was innovative and necessary for SPoT because ofthe large number of possible text plans, we chose to select the best text plan on thebasis of theoretically informed complexity features balancing sentence complexity andnumber of sentences.
Because our text plans are significantly more constrained, it ispossible to enumerate each of them and choose the one that best fits our rating cri-teria.14 This has the added benefit of better understanding the complexity features byevaluating the resulting text.
In addition, our method would be open to both upgrad-ing the selection criteria and adding further aggregation operators without requiringretraining.1514 Although in our implementation we do enumerate all plans before the rating criteria are applied to selectthe best one, it is in principle possible to generate the text plans in an order that would allow maximizingthe scoring functions without first enumerating all possibilities.
This is left for future work.15 Such modifications and additions need to be empirically evaluated with empirical data, however.546Demir, Carberry, and McCoy Summarizing Information Graphics TextuallyOperator: Attribute OperatorGloss: This operator attaches a single node tree that consists solely of a proposition withan Attributive Knowledge Base predicate, as a direct subchild of a node N with a RelativeKnowledge Base predicate in another tree, if the main entity of the Attributive Knowledge Basepredicate is an entity (main or secondary) for the proposition at node N.Input: T1 and T2Constraints:1.
(pred(T1-root)==?hasattr?)2.
((pred(T2-node)!=?hasattr?)
?
(pred(T2-node)!=?And?)
?(pred(T2-node)!=?Which?))3.
((main ent(T1-root)==main ent(T2-node)) ?
(main ent(T1-root)==secondary ent(T2-node)))Output: A modified T2 such that1.
left child(T2-node)?T1Glossary:1.
Tx-root: the root node of tree Tx2.
Tx-node: any node in tree Tx (including Tx-root)3. pred(Tx-node): the predicate at Tx-node4.
left/right child(Tx-node): the leftmost/rightmost child of Tx-node5.
main/secondary ent(Tx-node): the main/secondary entity of the proposition at Tx-node6.
!=: not equal, ==: equal, !
: not,?
: assignOperator: And OperatorGloss: This operator combines two trees if the propositions at their root share the same mainentity.
A proposition containing an And predicate with the same main entity forms the root ofthe new tree and the trees that are combined form the immediate descendents of this root.Input: T1 and T2Constraints:1.
((pred(T1-root)!=?hasattr?)
?
(pred(T2-root)!=?hasattr?))2.
!((pred(T1-root)==?And?)
?
(pred(T2-root)==?And?))3.
(main ent(T1-root)==main ent(T2-root))Output: a new tree T3 where the root node has two immediate children such that1.
pred(T3-root)??And?
?main ent(T3-root)?main ent(T1-root)2. left child(T3-root)?T1 ?
right child(T3-root)?T2Operator:Which OperatorGloss: This operator attaches a tree (Tree A) as a descendent of a node N in another tree (Tree B)via a Which predicate, if the main entity of the proposition at the root of Tree A is a secondaryentity for the proposition at node N of the other tree (Tree B).
That particular entity forms themain entity of the Which predicate.
Thus, Tree A will be an immediate child of the node withthe Which predicate and the node with the Which predicate will be an immediate child of nodeN in Tree B.Input: T1 and T2Constraints:1.
((pred(T1-root)!=?hasattr?)?(pred(T2-node)!=?hasattr?))2.
(main ent(T1-root)==secondary ent(T2-node))Output: A modified T2 via the addition of a new node (Node x) with a single immediate childsuch that1.
pred(Node x)?
?Which?
?main ent(Node x)?main ent(T1-root)2. right child(T2-node)?Node x ?
left child(Node x)?T1547Computational Linguistics Volume 38, Number 3Operator: Same OperatorGloss: This operator combines two trees if the propositions at their root contain the samepredicate but the main entities of these predicates are different.
A proposition with a Samepredicate forms the root of the new tree, and the trees that are combined form the descendentsof this root.
Since the descendents of the new tree have different main entities, the main entity ofthe Same predicate is some unique element not occurring elsewhere in the tree.
For instance, inour implementation this element is obtained by appending a unique number, which isn?t usedin another Same predicate in the current forest, to the term random (such as random0).Input: T1 and T2Constraints:1.
((pred(T1-root)!=?hasattr?)?(pred(T2-root)!=?hasattr?)
?(pred(T1-root)!=?And?)
?
(pred(T2-root)!=?And?)
?(pred(T1-root)!=?Same?)
?
(pred(T2-root)!=?Same?))2.
(pred(T1-root)==pred(T2-root))3.
(main ent(T1-root)!=main ent(T2-root))Output: a new tree T3 where the root node has two immediate children such that1.
pred(T3-root)??Same?
?main ent(T3-root)?
a unique element2.
left child(T3-root)?T1 ?
right child(T3-root)?T2In our work, we thus developed a method that would choose a text plan on prin-cipled reasoning concerning the resulting text.
In particular, we looked to balance sen-tence complexity and the number of sentences in the generated text.
Moreover, whereassuch a method was not applicable in the case of SPoT (because of the significantly largerset of operators with few constraints resulting in potential text plans too numerous toevaluate), our work differs in several aspects that make it reasonable to generate alltext plans and apply an evaluation metric.
First, our system has a small number ofaggregation operators and all operators cannot be applied to all kinds of predicates (e.g.,the Attribute Operator cannot be applied to the Relative Knowledge Base predicates).Second, the number of possible sets of basic propositions that our system needs toorganize is significantly lower than the number of possible text plans that the SPoTplanner needs to consider.
Finally, although it is not practical in SPoT to list all possiblesentence plans that might be generated for a particular text plan (since the possibilitiesare too great), generating all possible combinations of propositions in a proposition class(such as message related class) is practical in our work.
This is due to the fact that thenumber of basic propositions in each class is fairly small (e.g., usually between 5 to15 propositions) and that the nature of the operators and constraints that we put ontheir application enable us to prune the space of possible combinations.
Some of theseconstraints are: (1) The And Operator produces only one complex tree from a pair oftrees and it cannot combine two trees if both trees have a proposition with an Andpredicate at their roots (thus we limit the number of conjuncts in a conjoined sentenceto three at most16), and (2) the Attribute Operator produces only one complex tree incases where a single node tree (Tree A) can be attached as a direct subchild of more thanone node in another tree (Tree B); the parent of Tree A is the first such node found bypreorder traversal of Tree B.Our implementation first generates all possible text plans for the propositionswithin each class (message-related, specific, and computational).
Each text plan isrepresented as a forest where each tree in the forest represents a sentence.
Initially,each proposition class is treated as a forest consisting of all single node trees in thatclass (initial candidate forest), and the operators are applied to that forest in orderto produce new candidate forests for the proposition class.
Anytime two trees in a16 We set this limit in order to avoid sentences that are too complex to comprehend.548Demir, Carberry, and McCoy Summarizing Information Graphics TextuallyFigure 5A candidate forest for each proposition class.candidate forest are combined via an operator, a new candidate forest is produced;the new candidate forest is added to the existing set of candidate forests, therebyincreasing the number of candidate forests.
Within each class, our approach first appliesthe And Operator to all possible pairs of trees in the initial candidate forest, whichproduces new candidate forests.
The Same Operator is then applied to all possiblepairs of trees in each candidate forest.
Similarly, the Which Operator and the At-tribute Operator are applied to trees in the candidate forests produced earlier.
Theresult of this aggregation is a number of candidate forests with one or more trees(each using different aggregation) for each of the proposition classes.
For example,Figure 5 shows one candidate forest that can be constructed for each proposition classby applying these operators to the propositions selected for the graphic in Figure 4,where each forest resulting from the aggregation consists of a single tree.17 In thisexample, the Attributive Knowledge Base predicates (*) are attached to their parentsby the Attribute Operator, the nodes containing And predicates (**) are producedby the And Operator, and the Which predicates (***) are produced by the WhichOperator.17 The nodes represented with black circles correspond to the individual predicates.
These individualpredicates within each class form the single node trees upon which the operators work.549Computational Linguistics Volume 38, Number 35.3 Evaluating a Text StructureDifferent combinations of operators produce different candidate forests in each propo-sition class and consequently lead to different realized text with a different complexityof sentences.
The set of candidate forests for each proposition class must be evaluatedto determine which one is best.
Our objective is to find a forest that would producetext which stands at a midpoint between two extremes: a text where each propositionis realized as a single sentence and a text where groups of propositions are realizedwith sentences that are too complex.
Our evaluation metric to identify the best forestleverages different considerations to balance these extremes.
The first two criteria areconcerned with the number and syntactic complexity of sentences that will be used torealize a forest.
The third criteria takes into consideration how hard it is to comprehendthe relative clauses embedded in these sentences.
The insights that we use in selectingthe best forest (e.g., balancing semantic importance, overall text structure, aggregation,and readibility due to sentence complexity) represent our novel contributions to thetext structuring and aggregation literature.
The theory that underlies our evaluationmetric (i.e., what it is we are balancing in the generation) is widely applicable to otherdata-to-text generation domains because it uses general principles from the literatureand has the potential to be improved.5.3.1 Sentence Complexity.
Each tree (single node or complex) in a forest represents aset of propositions that can be realized as a single sentence.
Our aggregation rulesenable us to combine these simple sentences into more complex syntactic structures.In the literature, different measures to assess syntactic complexity of written text andspoken language samples have been proposed, with different considerations such asthe right branching nature of English (Yngve 1960) and dependency distance betweenlexemes (Lin 1996).
We apply the revised D-level sentence complexity scale (Covingtonet al 2006) as the basis of our syntactic complexity measure.
The D-level scale measuresthe complexity of a sentence according to its syntactic structure and the sequence inwhich children acquire the ability to use different types of syntactic structures.
Thesentence types with the lowest score are those that children acquire first and there-fore are the simplest types.
Eight levels are defined in the study, some of which aresimple sentences, coordinated structures (conjoined phrases or sentences), non-finiteclause in adjunct positions, and sentences with more than one level of relative clauseembedding.Among the eight levels defined in that study, the levels of interest in our workare simple sentences, conjoined sentences, sentences with a relative clause modifyingthe object of the main verb, non-finite clauses in adjunct positions, and sentenceswith more than one level of embedding.
However, the definition of sentence types ateach level is too general.
For example, the sentences There is a trend and There is atrend in the dollar value of net profit over the period from the year 1998 to the year 2006are both classified as simple sentences with the lowest complexity score under theD-level classification.
We argue that although these sentences have a lower complexitythan the sentences with higher D-level scores, their complexities are not the same.
Wemake a finer distinction between sentence types defined in the D-level classificationand use the complexity levels shown in Table 2.
For example, according to our clas-sification, a simple sentence with more than one adjunct or preposition has a highercomplexity than a simple sentence without an adjunct.
We preserve the ordering ofthe complexity levels in the D-level classification.
For example, in our classification,550Demir, Carberry, and McCoy Summarizing Information Graphics TextuallyTable 2Our syntactic complexity levels.Complexity Syntactic FormLevel 0 Simple sentence with up to one prepositional phrase or adjunctLevel 1 Simple sentence with more than one prepositional phrase or adjunctLevel 2 Conjoined sentence (two simple sentences?Level 0 or 1)Level 3 Conjoined sentence (more than two simple sentences?Level 0 or 1)Level 4 Sentence with one level of embedding(relative clause that is modifying object of main verb)Level 5 Non-finite clause in adjunct positionsLevel 6 Sentence with more than one level of embeddingLevels 0 and 1 correspond to the class of simple sentences in the D-level classificationand have a lower complexity than Levels 2 and 3, which correspond to the class ofcoordinated structures with a higher complexity than simple sentences in the D-levelclassification.Each basic proposition in our system can be realized as a simple sentence containingat most one prepositional phrase or adjunct.18 Each single node tree with a Relative orAttributive Knowledge Base predicate at its root has the lowest syntactic complexity(Level 0) in this classification.Themost straightforwardway of realizing amore complex tree would be conjoiningthe realizations of subtrees rooted by a proposition with an And or a Same predicate,embedding the realization of a subtree rooted by a proposition with a Which predicateas a relative clause, and realizing a subtree that consists solely of a proposition withan Attributive Knowledge Base predicate as an adjective or a prepositional phrase.For example, the tree rooted by shows(graphic,trend) in Figure 5 can be realized asThe graphic shows a decreasing trend, which is in the dollar value of net profit and is overthe period, which starts at the year 1998 and ends at the year 2006.
The resultant textis fairly complicated, however, and a more sophisticated realization would likelylead to a lower syntactic complexity score.
We defined a number of And predicateand Which predicate complexity estimators to look for realization opportunities ina complex tree structure so that a syntactic complexity score which is lower thanwhat the most straightforward realization would produce can be assigned to that tree.These estimators compute the syntactic complexity of a complex tree by examiningthe associated realizations of all aggregated propositions in that tree in a bottom?upfashion.
Because the complex trees that are rooted by a proposition with a Samepredicate would always be realized as a conjoined sentence (Level 2), we did not definecomplexity estimators for this kind of predicate.The And predicate complexity estimators check whether or not the realizationsof two subtrees rooted by a proposition with an And predicate can be combined intoa simple sentence (Level 1), or a conjoined sentence which consists of two independentsentences (Level 2) if one of the subtrees is rooted by a proposition with an Andpredicate.
For example, the And predicate estimators can successfully identify the18 In the current implementation, there is a single realization associated with each basic proposition withthe main entity in subject position.551Computational Linguistics Volume 38, Number 3following realization opportunities (based on the representations of the sentences aspropositions): The period starts at the year 1998.
AND The period ends at the year 2006.can be combined into:The period is from the year 1998 to the year 2006.
(Level 1) The trend is in the dollar value of net profit.
AND The trend is over the periodfrom the year 1998 to the year 2006. can be combined into:The trend is in the dollar value of net profit over the period from the year 1998to the year 2006.
(Level 1) The dollar value of net profit ranges from 2.77 billion dollars over the period.AND The dollar value of net profit ranges to 0.96 billion dollars over theperiod.
AND The dollar value of net profit shows the largest drop of about0.56 billion dollars between the year 2000 and the year 2001. can becombined into:19The dollar value of net profit ranges from 2.77 to 0.96 billion dollars over theperiod and shows the largest drop of about 0.56 billion dollars between theyear 2000 and the year 2001.
(Level 2)The Which predicate complexity estimators check whether a tree rooted by aproposition with a Which predicate can be realized as a simple adjunct or a prepo-sitional phrase attached to the modified entity rather than a more complex relativeclause (which could increase the complexity level).
For example, the Which predicateestimators can successfully identify the following realization opportunities (based onthe representations of the sentences as propositions): The trend is over the period.WHICH The period is from the year 1998 to theyear 2006. can be realized as:The trend is over the period from the year 1998 to the year 2006.
(Level 1) The graphic shows a trend.WHICH The trend is in the dollar value of net profitover the period from the year 1998 to the year 2006. can be realized as:The graphic shows a trend in the dollar value of net profit over the period fromthe year 1998 to the year 2006.
(Level 1)In our generation approach, multiple realizations for each proposition can be incor-porated by defining new complexity estimators in addition to the estimators that areused in the current implementation.
Defining such estimators, which will not changethe task complexity or the underlying methodology, would add to the generalizabilityof our approach.19 In this case, the single node trees that correspond to the propositions conveying the range of the trendform the immediate descendents of a tree rooted by a proposition with an And predicate, and that treewith the And predicate at its root and the tree corresponding to the proposition conveying the largestdrop constitute the immediate descendents of a tree rooted by another proposition with an Andpredicate.552Demir, Carberry, and McCoy Summarizing Information Graphics Textually5.3.2 Relative Clause Embedding.
In cases where a tree rooted by a proposition with aWhich predicate cannot be realized as a simple adjunct or a prepositional phrase, it willbe realized by a relative clause.
In the D-level classification, the complexity of a sentencewith an embedded clause is determined according to the grammatical role (subjector object) of the entity that is modified by that clause, not the syntactic complexityor position (center-embedded or right-branching) of the clause in the sentence.
Forinstance, a sentence with a complex center-embedded relative clause modifying anobject receives the same syntactic complexity score as a sentence with a simple right-branching relative clause modifying an object.
As argued in the literature, however,center-embedded relative clauses are more difficult to comprehend than correspondingright-branching clauses (Johnson 1998; Kidd and Bavin 2002).
To capture this, ourevaluation metric for identifying the best structure penalizes Which predicates thatwill be realized as a relative clause based on the clause?s syntactic complexity andposition in the sentence (which we refer to as ?comprehension complexity of a relativeclause?).
For example, the following sentences receive different scores by our evaluationmetric with respect to clause embedding; the first one with a right-branching clause(simpler) has a lower score than the second sentence with a center-embedded clause(more complex): The graphic shows a decreasing trend over the period from the year 1998to the year 2006 in the dollar value of net profit, which is 2.7 billion dollarsin the year 1999. The graphic shows a decreasing trend in the dollar value of net profit, which is2.7 billion dollars in the year 1999, over the period from the year 1998 to theyear 2006.The embedded clause (Level 0) in the first of the following sentences has a lowersyntactic score than the clause (Level 2) embedded in the second sentence.
Becauseour evaluation metric takes into consideration both the syntactic complexity of anembedded clause and its position in the sentence, the first sentence receives a lowerscore than the second sentence. The graphic shows a decreasing trend in the dollar value of net profit, which is2.7 billion dollars in the year 1999, over the period from the year 1998 to theyear 2006. The graphic shows a decreasing trend in the dollar value of net profit, which is2.7 billion dollars in the year 1999 and is 2.58 billion dollars in the year 2000,over the period from the year 1998 to the year 2006.5.3.3 Evaluation Metric.
Our evaluation metric takes three criteria into account: thenumber of sentences that will be used to realize a forest, the syntactic complexities ofthese sentences, and the comprehension complexities of the embedded relative clauses.Our metric evaluates the overall score of a candidate forest by summing the normalizedscores that the forest receives with respect to each criteria.
The score of a forest (e.g.,Forest A) is calculated as follows:score(A) = nm1(sentence(A))+ nm2(complexity(A))+ nm3(clause(A)) (1)553Computational Linguistics Volume 38, Number 3where:20sentence(A): stands for the number of sentences that will be used to realize forest Aand equals the number of trees in that forest.complexity(A): stands for the overall syntactic complexity of forest A and equals thesum of the complexities of sentences that will be used to realize that forest.clause(A): stands for the overall comprehension complexity of all relative clausesin sentences used for realizing forest A and equals the sum of the comprehensioncomplexities of all clauses.
The comprehension complexity of a relative clause equalsthe product of its syntactic complexity and its position in the sentence, which is equalto 2 if it is a center-embedded clause and is equal to 1 if it is a right-branching clause.Consider, for example, the forest shown in Figure 6.
Because the forest contains asingle tree, it receives a score of 1 for the sentence criteria.
The syntactic complexity scoreof the sentence that will be used to realize that tree is computed in a bottom?up fashionas follows.
The lowest syntactic complexity score (Level 0) is assigned to all leaf nodes,and all inner nodes that only have single node trees with anAttributive Knowledge Basepredicate as descendents (as shown in Figure 6).
Each of the remaining inner nodes isthen assessed with a syntactic complexity score once the complexity scores for all ofits descendents are computed (i.e., once the best realization possibility with the lowestsyntactic complexity for each descendent tree is determined).
If an inner node containsa proposition with an And predicate, its syntactic complexity score is computed viathe And predicate complexity estimators.
Similarly, the Which predicate complexityestimators are used to compute the syntactic complexity scores for all inner nodes withaWhich predicate.
The syntactic complexity score for the parent node of a tree rooted bya proposition with aWhich predicate is computed based on whether or not that tree willbe realized as a relative clause (as indicated by the complexity score of the root node ofthat tree).
The forest shown in Figure 6 receives a score of 4 for the complexity criteria,which is equal to the syntactic complexity score assigned to the parent node of the tree.In Figure 6, only the tree rooted by Node 4 will be realized as a relative clause.
Becausethat relative clause, which receives a syntactic complexity score of 2, will be realizedas a center-embedded clause, the forest shown in Figure 6 receives a score of 4 for theclause criteria.In the current implementation, once the scores with respect to a criteria are com-puted for each candidate forest, these scores (e.g., sentence(A)) are normalized withrespect to the maximum score (e.g., max(sentence(all forests))) by dividing each scoreby the maximum of the computed scores.
For instance, nm1(sentence(A)) is the nor-malized score that the forest A receives with respect to the sentence criteria and isequal to sentence(A)/max(sentence(all forests)).
Thus, the normalized score that a forestreceives for each criteria is always between 0 and 1 and therefore each criteria hasan equal impact on the overall score of a forest.21 The normalized scores obtainedfor a candidate forest are then summed to obtain the overall score for that forest.
Forexample, assume that three candidate forests, the first of which is shown in Figure 6,20 The terms nm1, nm2, and nm3 stand for the normalized score of a given criteria.21 The simplifying assumption of assigning equal weights to each criteria would be better optimized withmachine learning, as discussed in detail in Section 9.554Demir, Carberry, and McCoy Summarizing Information Graphics TextuallyFigure 6A forest containing a single tree.555Computational Linguistics Volume 38, Number 3Table 3Overall evaluation scores.Sentence Complexity Clause Overall ScoreFirst Forest 1(0.5) 4(1) 4(1) 2.5Second Forest 1(0.5) 4(1) 2(0.5) 2Third Forest 2(1) 3(0.75) 0(0) 1.75are constructed from a set of propositions.
One possible way of realizing the forest inFigure 6 would be The graphic shows a decreasing trend in the dollar value of net profit,which shows the largest drop of about 0.56 billion dollars between the year 2000 and theyear 2001, and shows the smallest drop of nearly 0.07 billion dollars between the year 1998and the year 1999, over the period from the year 1998 to the year 2006.
Suppose that thesecond forest is similar to Figure 6 except that the children (Node 2 and Node 3) ofthe And(trend) node are swapped.22 Suppose also that the third forest is similar toFigure 6 except that the tree is decomposed into two trees, which are rooted by Node 1and Node 5, respectively.
The first tree rooted by Node 1 consists of the nodes markedwith (*) and the second tree rooted by Node 5 consists of the nodes marked with (**).Table 3 shows the actual and the normalized scores (shown in parentheses) for eachforest with respect to each criteria, and the overall score assigned by our evaluationmetric.The number of sentences (1) and the overall sentence complexity (Level 4) are thesame for the first and second forests.
The third forest has more sentences (2) but loweroverall sentence complexity (Level 3) than the other two forests.
The first forest has acenter-embedded relative clause and receives a score of 4 for the clause criteria: theproduct of the complexity of the relative clause (2) and its position (2).
On the otherhand, the second forest has a right-branching relative clause and receives a score of 2for the same criteria: the product of the complexity of the relative clause (2) and itsposition (1).
The third forest doesn?t have an embedded clause and receives a score of 0for the clause criteria.5.4 Identifying the Best Text StructureOur approach selects the forest which receives the lowest evaluation score as the bestforest that can be obtained from a set of input propositions.
For example, accordingto the scores shown in Table 3, the third forest, which could be realized as The graphicshows a decreasing trend in the dollar value of net profit over the period from the year 1998to the year 2006.
The dollar value of net profit shows the largest drop of about 0.56 billiondollars between the year 2000 and the year 2001 and shows the smallest drop of nearly 0.07billion dollars between the year 1998 and the year 1999., would be selected as the bestamong the three forests.
The initial overall text structure of a brief summary con-sists of the best forests identified for the message related, specific, and computationalclasses.As a final step, we check whether we can improve the evaluation of the overallstructure of the summary by moving trees (i.e., trees rooted by a Level-0 node such as22 This swapping would cause the relative clause rooted by Node 4 to be a right-branching clause.556Demir, Carberry, and McCoy Summarizing Information Graphics TextuallyAnd(descriptor) in Figure 5, Specific) or subtrees (i.e., trees rooted by a Level-1 nodewith an And or a Relative Knowledge Base predicate such as hasextreme (descrip-tor,?largest?,change(3,4),period(3,4)) in Figure 5, Specific) between the best forests forthe three proposition classes.
For example, the best forest for the specific class mightcontain a tree that conveys information about an entity introduced by a propositionin the message related class.
Moving this tree to the message related class and using anoperator to combine it with the tree introducing the entity might improve the evaluationof the overall structure of the summary.
For example, for the graphic in Figure 4, thismovement would allow our system to evaluate a structure where the tree rooted by thespecific proposition And(descriptor) (shown in Figure 5, Specific) is attached as a de-scendent of the tree rooted by the message related proposition exists(trend,descriptor)(shown in Figure 5,Message Related) via aWhich Operator.We explore all such possiblemovements between best forests for the proposition classes (if any) and determinethe best overall text structure of the summary.
To be consistent with the motivationbehind the initial groupings of the propositions, we do not allow movements outof the message related class or any movement that will empty the computationalclass.5.5 Evaluation of the Text Structuring and Aggregation ModuleOur text structuring and aggregation approach consists of several different components,all of which contribute to the quality of the generated text.
Our study focused onwhether or not our decisions with respect to these components contributed to theperceived quality of the resultant summary: the organization and ordering (O) ofthe content (partial ordering of the propositions within classes and classification ofthe propositions), the aggregation (A) of the information into more complex tree struc-tures (candidate forests constructed via operators), and the metric (E) used to evaluatecandidate forests that represent different possible aggregations of the informationalcontent.We conducted an experiment with 15 participants (university students and gradu-ate students) who were presented with six different summaries of twelve graphics fromthe test corpus (described in Section 3.3).
The participants neither participated in earlierstudies (described in Sections 4.1 and 4.4) nor were involved in this work.
All presentedsummaries were automatically produced by our generation approach.
The participantswere not told how the presented summaries were produced (i.e., human-generatedor computer-generated), however.
We focused on graphics with an increasing or adecreasing trend, since these message categories exhibit the greatest variety of possiblesummaries.
For each of the graphics, the participants were given a set of summaries inrandom order and asked to rank them in terms of their quality in conveying the content.The summaries varied according to the test parameters as follows:23 S O+A+E+: A summary that uses the ordering rules, the aggregationrules, and receives the lowest (best) overall score by the evaluationmetric.
This is the summary selected as best by the TSAMModule.23 Although eight different summaries are logically possible with three different variables, we limited thenumber to four (the second and the third in which exactly one of the components was turned off and thefourth where all components were turned off) in order to keep the experiment within a manageable size.557Computational Linguistics Volume 38, Number 3Table 4Ranking of summary types.Summary Type Best 2nd 3rd 4thS O+A+E+ 65.6% 26.6% 6.7% 1.1%S O+A+E- 16.7% 32.2% 33.3% 17.8%S O-A+E+ 16.7% 30% 40% 13.3%S O-A-E- 1% 11.2% 20% 67.8% S O+A+E-: A summary that uses the ordering and aggregation rules,but does not receive the lowest overall score by the evaluation metric.This is the summary that received the second lowest score. S O-A+E+: A summary where the propositions are randomly ordered,but aggregation takes place, and it receives the lowest (best) overallscore by the evaluation metric. S O-A-E-: A summary consisting of single sentences that are randomlyordered.Table 4 presents the results of the experiment.
It is particularly noteworthy that thesummary selected as the best by the Text Structuring andAggregationModulewasmostoften (65.6% of the time) rated as the best summary and overwhelmingly (92.2% of thetime) rated as one of the top two summaries.
The table shows that omitting the eval-uation metric (S O+A+E-) or omitting ordering of propositions (S O-A+E+) results insummaries that are substantially less preferred by the participants.
Overall, the resultsshown in Table 4 validate our ordering, aggregation, and evaluation methodology.6.
Sentence Ordering Module (SOM)With the use of different kinds of operators and an evaluation metric, our systemdetermines the partial ordering and the structure of sentences that will be used to realizethe selected content but doesn?t impose ordering constraints (final ordering) on the sen-tences within each proposition class.
To decide in which sequence the sentences shouldbe conveyed, we take advantage of the fact that each proposition has a defined mainentity, which will be realized in the subject position of the sentence that will be used torealize the proposition.
Identifying the subject of the realized sentences in advance al-lows us to use centering theory (Grosz, Weinstein, and Joshi 1995) to generate a text thatis most coherent according to this theory.24 The theory outlines the principles of localtext coherence in terms of the way the discourse entities are introduced and discussed,and the transitions between successive utterances in terms of the entities in the hearer?scenter of attention.
Although some fundamental concepts of the theory, such as theranking of entities in an utterance, aren?t explicitly specified, various researchers haveapplied centering theory to language generation (Kibble and Power 2004; Karamaniset al 2009) with different interpretations.
In our work, each sentence is regarded as an24 If this assumption is relaxed, then centering theory would not be appropriate for an ordering component.In that case, a focusing theory such as McCoy and Cheng (1991), or Suri and McCoy (1994) could be usedto order the sentences to be realized.558Demir, Carberry, and McCoy Summarizing Information Graphics Textuallyutterance.
Following Brennan, Friedman, and Pollard (1987) and Grosz, Weinstein, andJoshi (1995), we rank the entities with respect to their grammatical functions where theentity in subject position is the most salient entity.
When ordering sentences, we takeinto account the preference order for centering transitions: continue is preferred overretain, which is preferred over smooth shift, which is in turn preferred over rough shift.For all message categories, the number of sentences in a proposition class wouldbe limited (less than five) even if all of the highly rated propositions identified forthat message category are selected for inclusion.
Thus, a straightforward ?generate andtest?
strategy is appropriate for ordering sentences in our case.
For each propositionclass, all possible orderings of the sentences within that class are generated.
We assigndifferent numeric scores to each centering transition, where continue receives a scoreof 3, and retain and smooth shift receive scores of 2 and 1, respectively.
The rough shifttransitions are assessed a score of 0.
For each candidate ordering, we sum the scoresfor the kinds of transitions observed between consecutive sentences.
The ordering thatreceives the highest score is selected as the best ordering for that proposition class.
First,the best ordering of the sentences in the message related proposition class is selected.The subject of the last sentence in that ordering is used as the backward-looking centerof the previous utterance when determining the best ordering of the sentences in thespecific proposition class.
Similarly, the subject of the last sentence in the best orderingfor the specific class is used as the backward-looking center when identifying the bestordering for the computational proposition class.For graphics that depict a time period, we also utilize the time periods mentioned ineach conjunct of a conjoined sentence in order to specify in which order these conjunctswill be conveyed in the realized text.
If the time periods mentioned in each conjunct ofa conjoined sentence are different, these conjuncts are ordered such that the time periodin focus in the first conjunct subsumes or precedes the time period in focus in the secondconjunct.
Consider how individual sentences in the following compound sentences areordered by our approach: The dollar value of net profit ranges from 2.77 to 0.96 billion dollars over theperiod from the year 1998 to the year 2006 and shows the largest drop of about0.56 billion dollars between the year 2000 and the year 2001. The dollar value of net profit shows the smallest drop of nearly 0.07 billion dollarsbetween the year 1998 and the year 1999 and shows the largest drop of about0.56 billion dollars between the year 2000 and the year 2001.Note that in the first conjoined sentence, the time period mentioned in the first con-junct (from 1998 to 2006) subsumes the time period mentioned in the second conjunct(between 2000 and 2001).
On the other hand, in the second conjoined sentence, the timeperiod mentioned in the first conjunct (between 1998 to 1999) precedes the time periodmentioned in the second conjunct (between 2000 and 2001).7.
Sentence Generation Module (SGM)To realize the summaries in natural language, we use the FUF/SURGE surface real-izer (Elhadad and Robin 1999), which offers the richest knowledge of English syntax andwidest coverage among the publicly available realizers such as REALPRO (Lavoie andRambow 1997).
The realization of the sentence-sized units requires referring expressions559Computational Linguistics Volume 38, Number 3for certain graphical elements, however.
Our system handles three different issues withrespect to referring expression generation: Generating a referring expression for the dependent axis.
Informationgraphics often do not label the dependent axis with a full descriptor ofwhat is being measured (which we call themeasurement axis descriptor).In such a situation, a referring expression for this element must beextracted from the text of the graphic.
For example, to realize its summary,a measurement axis descriptor (e.g., the dollar value of Chicago Federal HomeLoan Bank?s mortgage program assets) must be generated for the graphic inFigure 7a, whose dependent axis is not explicitly labeled with thedescriptor. Generating a referring expression in order to refer to the bars on theindependent axis (e.g., the countries for the graphic in Figure 1).
Such anexpression must be inferred from the bar labels or extracted from the textof the graphic.
This referring expression is often used in the summaries ofgraphics in some message categories (e.g., Maximum Bar) that requirecomparing a bar with others (e.g., distinguishing the bar with themaximum value from all other bars). It was shown that people prefer less informative descriptions forsubsequent mentions of an entity (Krahmer and Theune 2002).
In order togenerate more natural summaries, the syntactic forms of the subsequentmentions of discourse entities should be constructed in a way that helpstext coherence.7.1 Measurement Axis DescriptorGeneration of referring expressions (noun phrases) is one of the key problems exploredwithin the natural language generation literature.
There is a growing body of researchin this area that, given a knowledge base of entities and their properties, deals withFigure 7(a) Graphic from Business Week.
(b) Graphic from Business Week.560Demir, Carberry, and McCoy Summarizing Information Graphics Textuallydetermining the set of properties that would single out the target entity (Dale and Reiter1995; Krahmer, Van Erk, and Verleg 2003).
More recently, the generation of referringexpressions has been proposed as a postprocessing technique to deal with the lack oftext coherence in extractive multidocument summarization (Belz et al 2008).
Nenkovaand McKeown (2003) developed a method to improve the coherence of a multidoc-ument summary of newswire texts by regenerating referring expressions for the firstand subsequent mentions of people?s names where the expressions are extracted fromthe text of the input documents according to a set of rewrite rules.
The task that weface is similar to this recent body of research in that contextually appropriate referringexpressions for certain graphical elements should be extracted from the text of thegraphic.
At the same time, our task is more complex in some respects.
First, it is often thecase that the required referring expression isn?t explicitly given as a single unit and thusmust be constructed by extracting and combining pieces of information from the text ofthe graphic.
Second, in some cases where the dependent axis is explicitly labelled with adescriptor, it still needs to be augmented.
We undertook a corpus study in order to iden-tify how a measurement axis descriptor could be generated from the text of a graphic;the results of the analysis form the basis for the heuristics and augmentation ruleswe developed for generating the measurement axis descriptor for a graphic.
In Demir,Carberry, and Elzer (2009), we outlined this problem as generating a graphical elementrequired for realizing the intended message of a graphic and thoroughly described thetechnical details of our approach.
Here, however, we treat this particular aspect as anovel text-to-text generation methodology which is combined with other data-to-textapproaches in a complete NLG system.
Thus, our focus in this section is to highlighta new way of using the text associated with images which has been earlier exploitedby various NLP tasks such as indexing and retrieval of images (Pastra, Saggion, andWilks 2003).7.1.1 Corpus Analysis.
Graphic designers generally use text within and around a graphicto present information related to the graphic.
We started our analysis by examining howtexts are distributed around each group of graphics.
We observed that graphics (indi-vidual or composite) contain a set of component texts that are visually distinguishedfrom one another by blank lines, by different fonts/styles, or by different directionsand positions in the graphic.
Although the number of component texts present in agraphic may vary, our analysis recognized an alignment or leveling of text contained ina graphic, which we refer to as ?text levels.
?We observed seven text levels which we refer to as Overall Caption, OverallDescription, Caption, Description, Dependent Axis Label, Text In Graphic, and TextUnder Graphic.
Not every level appears in every graphic.
Overall Caption and Over-all Description apply to composite graphics that contain more than one individualgraphic (the graphics might be of different kinds) and refer to the entire collectionof graphics in the composite.
In composite graphics, Overall Caption is the text thatappears at the top of the overall group and serves as a caption for the whole set (suchas Tallying Up the Hits in Figure 8a).
In composite graphics, there is often anothertext component placed under the Overall Caption but distinguished from it by a linebreak or a change in font.
This text component, if present, is also pertinent to allindividual graphics in the composite graphic and elaborates on them.
We refer to suchtext as the Overall Description (such as Yahoo once relied entirely on banner ads.
Now it?sbroadened its business mix in Figure 8a).
Caption and Description serve the same rolesfor an individual graphic.
For example, the Caption for the bar chart in Figure 8a isActive Users and the Description is Registered users in millions.
The Caption of Figure 8b561Computational Linguistics Volume 38, Number 3Figure 8(a) A composite graph from Newsweek.25 (b) Graphic from Business Week.Table 5Text levels in bar charts.Text level Frequency of occurrenceOverall Caption 31.8% (?34/107)Overall Description 17.8% (?19/107)Caption 99.0% (?106/107)Description 54.2% (?58/107)Text In Graphic 39.3% (?42/107)Dependent Axis Label 18.7% (?20/107)Text Under Graphic 7.5% (?8/107)is A Growing Biotech Market but this graphic does not have a Description.
There may bea label on the dependent axis itself and we refer to it as Dependent Axis Label (suchas Revenues (in billions) in Figure 8b).
In addition to the text levels described so farwhich appear outside the borders of a graphic, we have observed that there is oftena text component residing within the borders of a graphic which we refer to as TextIn Graphic (such as U.S. Biotech Revenues, 1992?2001 in Figure 8b).
Finally, Text UnderGraphic is the text under a graphic which usually starts with a marker symbol (suchas *) and is essentially a footnote (such as U.S. only, one available seat flown one mile, yearending June 2002 in Figure 7b).
Each Text Under Graphic has a referrer elsewhere thatends with the same marker and that referrer could be at any text level of the graphic.
Agraphic might have more than one Text Under Graphic but each is differentiated witha different marker.
For each of the 107 graphics in our corpus (described in Section 3.3),the Visual Extraction System extracts these text levels from the graphical image of thebar chart and inserts them into the graph?s XML representation.
Table 5 lists the varioustext levels, along with how often they appeared in the graphics in our corpus.25 This figure displays two of the five individual graphs constituting the composite graphic that appeared inNewsweek.562Demir, Carberry, and McCoy Summarizing Information Graphics TextuallyTwo annotators first analyzed each graphic in our corpus and determined a mea-surement axis descriptor for the graphic; the annotators used both the informationresiding within the text components of the graphic and the article, and commonsenseknowledge.
All ideal descriptors were noun phrases or wh-phrases26 (such asWhat?s themost important issue affecting voters?
vote?
on a graphic depicting survey results).
After thedescriptors were identified, we analyzed the graphics to see how the descriptors couldbe generated from the text components of the graphic.
We observed that an acceptablemeasurement axis descriptor often cannot be extracted as a whole from a single textlevel and instead must be put together by extracting pieces from one or several differenttext levels; the pieces, though coming from a single level, might not be contiguous in thatlevel and still need to be melded into a coherent whole.
In some cases, the informationis also retrieved from other graphics in the same composite or from the article?s text.Our analysis has also led us to hypothesize that the ideal measurement axis descriptorcan be viewed as consisting of a core?a basic noun or wh-phrase from one text levelthat is often augmented with text from another level (or in some cases, from text in theaccompanying article or other graphs in the same composite) to be more descriptiveand complete.
For example, for the bar chart in Figure 8a, registered users is the coreof the ideal measurement axis descriptor which is Yahoo?s registered users.
The core isfound in the Description and the augmentation to the core is found in the OverallDescription.
When more than one text level is used, the text levels that contain pieces ofthemeasurement axis descriptor also vary among the graphics.We observed thatmostlytext levels particular to a graphic (such as Text In Graphic and Description) contain thepieces of the descriptor as opposed to the levels containing shared information (suchas Overall Description), and with the exception of Text Under Graphic, the ordering oftext levels in Table 5 forms a hierarchy of textual components, with Overall Captionand Dependent Axis Label respectively at the top and bottom of the hierarchy, such thatthe core generally appears in the lowest text level present in the graphic.
During thecorpus analysis, we observed three ways in which a core extracted from one text levelwas augmented with text from another text level: Expansion of the noun phrase: The nouns in the core of the descriptorwere replaced with a noun phrase at another text level which has the samenoun as its head.
The replaced noun phrase appeared in a text level higherin the precedence order than the text level at which the core appears.Consider, for example, Figure 8b.
The core of the descriptor is Revenues(appearing in the Dependent Axis Label), which is reasonable enoughto be the core, but the noun Revenues should be replaced with U.S. BiotechRevenues in order to be complete. Specialization of the noun phrase: The core was augmented with aproper noun which specialized the descriptor to a specific entity.Consider, for example, Figure 8a, which shows a composite graph whereindividual graphics present different attributes of the same particularentity (Yahoo).
The ideal measurement axis descriptor of the bar chart(Yahoo?s registered users) consists of the core registered users (appearing inthe Description) augmented with the proper noun Yahoo that appears inthe Overall Description.26 Generally seen in graphics presenting the results of a survey.563Computational Linguistics Volume 38, Number 3 Addition of detail: Text Under Graphic typically serves as a footnote togive specialized detail about the graphic which is not as important as theinformation given in other text levels.
If the Text Under Graphic beganwith a footnote marker, such as an asterisk, and the core was followed bythe same marker, then Text Under Graphic added detail to the core.Consider, for example, Figure 7b, where unit costs is the core but the idealmeasurement axis descriptor (Unit costs, U.S. only, one available seat flownone mile, year ending June 2002) also contains the information from theText Under Graphic.7.1.2Methodology.
First, preprocessing deletes the scale and unit indicators (phrases usedto give the unit and/or a scale of the values presented in the graphic), and the ontolog-ical category of the bar labels (if explicitly marked by the preposition by) from the textlevels.
Next, heuristics are used to identify the core of the measurement axis descriptorby extracting a noun phrase or a wh-phrase from a text level of the graphic.
Three kindsof augmentation rules, corresponding to the three kinds of augmentation observed inour corpus, are then applied to the core to produce the measurement axis descriptor.If none of the augmentation rules are applicable, then the core of the descriptor formsthe measurement axis descriptor.
Finally, if the measurement axis descriptor does notalready contain the unit of measurement (such as percent), the phrase indicating the unitof measurement is appended to the front of the measurement axis descriptor.For identifying the core of the measurement axis descriptor, we developed nineheuristics that are dependent on the parses of the text levels.
Two of these heuristics arerestricted to Dependent Axis Label and Text In Graphic, and the remaining heuristicsare applicable to all other text levels.
The application of the heuristics gives preferenceto text levels that are lower in the hierarchy and if a core is not identified at one textlevel, the applicable heuristics are applied, in order, to the next higher text level inthe hierarchy.
For example, suppose that the graphic contains only a Description anda Caption and thus the first two heuristics are not applicable.
The next seven heuristicsare first applied to the Description and then to the Caption.
The following presents threerepresentative heuristics where the first two heuristics are applicable only to DependentAxis Label and Text In Graphic:27 Heuristic-1: If the Dependent Axis Label consists of a single noun phrasethat is not a scale or unit indicator, that noun phrase is the core of themeasurement axis descriptor. Heuristic-2: If Text In Graphic consists solely of a noun phrase, then thatnoun phrase is the core; otherwise, if Text In Graphic is a sentence, thenoun phrase that is the subject of the sentence is the core. Heuristic-6: If a fragment at the text level consists solely of a noun phrase,and the noun phrase is not a proper noun, that noun phrase is the core.Once the core is identified, augmentation rules are applied to fill out the descriptor.For example, consider the graphic in Figure 8b where Heuristic-1 identifies Revenuesin Dependent Axis Label as the core.
Because the core and the Text In Graphic, U.S.Biotech Revenues, have the same head noun, the augmentation rule for expansion27 All of the heuristics and augmentation rules can be found in Demir, Carberry, and Elzer (2009).564Demir, Carberry, and McCoy Summarizing Information Graphics Textuallyproduces U.S. Biotech Revenues as the augmented core.
After adding a phrase forthe unit of measurement, the referring expression for the dependent axis becomesThe dollar value of U.S. Biotech Revenues.
As another example, consider the graphic inFigure 7b.
Our work uses Heuristic-2 and the augmentation rule for adding detail.After adding a phrase representing the unit of measurement, the referring expressionfor the dependent axis becomes The cent value of unit costs (U.S. only, one way available seatflown one mile, year ending June 2002).
Finally, consider the graphic in Figure 8a, whereHeuristic-6 identifies the noun phrase registered users as the core.28 The augmentationrule for specialization finds that Yahoo is the only proper noun in the text levels anddoes not match a bar label, and forms Yahoo?s registered users.
After adding a phraserepresenting the unit of measurement, the referring expression for the dependent axisbecomes The number of Yahoo?s registered users.In order to evaluate our approach, we constructed a distinct test corpus consistingof 205 randomly selected bar charts from 21 different newspapers and magazines;only six of these sources were also used to gather the corpus described in Section 3.3.For each graphic, we used our approach to generate the referring expression for thedependent axis.
Finally, the resultant output and three baselines were evaluated bytwo evaluators (Demir, Carberry, and Elzer 2009).
The evaluation results showed thatour approach performs much better than any of the baselines for the 205 graphics inthe corpus.
The detailed analysis of the results also showed that our methodology isapplicable to a wider range of sources in popular media.7.2 Generating an Expression for Referring to All BarsFor some message categories (for example, Maximum Bar), the identification of theontological category for the bar labels results in better natural language than merelyusing a generic expression; for example, compare the phrase among the countries listedwith the phrase among the entities listed in producing natural language text for themessage conveyed by the graphic in Figure 1.
There are a number of different pub-licly available ontologies such as WordNet Fellbaum (1998) and OpenCyc (2011).
Inour work, we need a knowledge base that offers both the semantic relations betweenwords and general commonsense knowledge.
For example, WordNet could not iden-tify Jacques Chirac, a former president of France, whereas OpenCyc ontology containsthis information.
Therefore, we use OpenCyc ontology version v0.7.8b to identify theontological categories of bar labels in our work.
Our implemented system currentlyfinds the most specific category that is a common category for at least two of the barlabels and identifies it as the ontological category.Grice?s Maxim of Quantity (1975) states that one?s discourse contribution should beas informative as necessary for the purposes of the exchange but not more so.
If oursystem were to enumerate all entities involved in a comparison message, the realizationof the inferred message might be lengthy and the enumeration of little utility to theuser.
Thus we set a cut-off C, such that if the number of entities involved in a MaximumBar or Rank Bar message exceeds C, they are not enumerated but rather we use thegenerated referring expression for all bars.
The cut-off value is currently set to 5 in ourimplemented system.28 The preprocessing of this text level would remove In millions because it is a scale indicator.565Computational Linguistics Volume 38, Number 37.3 Subsequent Mentions of Discourse EntitiesIn the current implementation of the system, the syntactic form of a subsequent mentionof an entity is determined based on its salience status in the context.
In particular, thebackward-looking center of an utterance29 is replaced with a less informative definitenoun phrase after a continue or a retain transition because the backward-looking cen-ter remains the same in the latter utterance.
In such cases, the definite noun phraseis constructed by adding the demonstrative this or these to the front of the headnoun of the backward-looking center, such as these revenues for the phrase U.S. biotechrevenues.7.4 Example SummariesFor the graphic in Figure 1, our system generates the following textual summary: Thegraphic shows that United States at 32,434 has the highest number of hacker attacks among thecountries Brazil, Britain, Germany, Italy, and United States.
United States has 5.93 times moreattacks than the average of the other countries.For the graphic in Figure 3a, the following textual summary is generated: The graphicshows an increasing trend in the dollar value of Lands?
End annual revenue over the period fromthe year 1992 to the year 2001.
The dollar value of Lands?
End annual revenue shows an increaseof nearly 225 percent.
Except for a small drop in the year 1999, slight increases are observedalmost every year.For the graphic in Figure 3b, our system generates the following summary: Thegraphic compares the defense agencies army, navy, air force, and other defense agencies, whichare sorted in descending order with respect to the number of civilian employees.
The numberof civilian employees is highest for army at 233,030 and is lowest for other defense agencies at100,678.30For the graphic in Figure 4, the following textual summary is generated: The graphicshows a decreasing trend in the dollar value of net profit over the period from the year 1998 to theyear 2006.
The dollar value of net profit ranges from 2.77 to 0.96 billion dollars over the periodfrom the year 1998 to the year 2006 and shows the largest drop of about 0.56 billion dollarsbetween the year 2000 and the year 2001.
Slight decreases are observed almost every year.For the graphic in Figure 8b, our system generates the following summary: Thegraphic shows an increasing trend in the dollar value of U.S. Biotech Revenues over the periodfrom the year 1992 to the year 2001.
Increasing slightly every year, the dollar value of U.S.Biotech Revenues shows an increase of nearly 265 percent and ranges from 7.87 to 28.52 billiondollars.For the graphic in Figure 7a, the following textual summary is generated: In the year2003, the graphic shows a much higher rise in the dollar value of Chicago Federal Home LoanBank?s mortgage program assets in contrast with the moderate increases over the period from theyear 1998 to the year 2002.
The dollar value of Chicago Federal Home Loan Bank?s mortgageprogram assets reaches to 94.23 billion dollars in the year 2003.
The dollar value of these assetsin the year 2003 is nearly 49.1 times higher than that in the year 1998.For the graphic in Figure 9, the following textual summary is generated: This graphicis about American Express.
The graphic shows that American Express at 255 billion dollars is29 The backward-looking center of a current utterance is the most highly ranked entity of the previousutterance that is realized in the current utterance.30 The reason for saying that the defense agencies are sorted in decreasing order is not to enable the readerto visualize the graphic but rather that it subsumes giving the ranking of each bar.566Demir, Carberry, and McCoy Summarizing Information Graphics TextuallyFigure 9Graphic conveying the rank of a bar.the third highest with respect to the dollar value of total credit-card purchases per year amongthe entities Visa, Mastercard, Discover, Diner?s Club, and American Express.8.
Evaluation of the Effectiveness of the Textual SummariesThe earlier user studies (Sections 4.4 and 5.5) demonstrated the effectiveness of our gen-eration methodology in identifying and presenting the high-level content of bar charts.The success of a generation system depends not only on the quality of the produced text,however, but also on whether the text achieves the impact that it is intended to make onreaders (such as enabling readers to perform a task or changing their opinions in somecontext).
We conducted an evaluation study to measure how adequate and effectivethe summaries generated by our system are for our purpose of providing the messageand high-level knowledge that one would gain from viewing a graphic.
Specifically, wewere interested in (1) what amount of information is retained by a reader from readingthe summary generated by our system, (2) whether someone reading the summarygarners the most important knowledge conveyed by the graphic, and (3) whether theknowledge gleaned from the summary is consistent with the actual graphic.In this study, we used four graphics from the test corpus (described in Section 3.3)with different intended messages.
These graphics conveyed an increasing trend (i.e.,Figure 3a), a decreasing trend, the rank of the maximum bar (i.e., Figure 1), and therank of a bar (i.e., Figure 9) among all bars.
In the first part of the study, each of the18 participants (graduate students) was first presented with the summaries generatedby our system for these graphics; the participants neither saw the original graphics(the graphical images of the bar charts) nor were aware of our system and how thesummaries were generated.
For each summary, the participants were asked to drawthe bar chart being described in that summary.
Although enabling a reader to redrawthe graphic is not a goal of our work, comparing a reader?s mental representationof the graphic with the actual graphic allows us to identify whether there are anyinconsistencies between knowledge gleaned from the summary and the content of theactual graphic.In the second part of the study, we asked three evaluators not involved in this re-search to evaluate the drawings that we collected from the participants.
The evaluatorswere Ph.D. students from the University of Delaware (none of them were the authorsof this work) and had an overall knowledge about our summarization approach (i.e.,what is intended to be conveyed in the summaries of graphics).
The evaluators were567Computational Linguistics Volume 38, Number 3first told that a set of participants were presented with brief summaries of bar chartsand asked to draw the corresponding bar charts based on the information presentedin those summaries.
They were also told that each summary only conveyed what isidentified by our system as the most important information that should be conveyedabout the bar chart being described.
The evaluators were presented with the graphicalimages of the four bar charts used in the study (none saw the summaries presented inthe first part of the study) and the drawings collected from the participants, and thenasked to rate each drawing using the following evaluation scores: 5: The drawing is essentially the same as the original graphic 4: The drawing captures all important information from the originalgraphic but requires some minor modifications 3: The drawing captures most of the important information from theoriginal graphic but is missing one significant piece of information 2: The drawing reflects some information from the original graphic butrequires major modifications 1: The drawing fails to reflect the original graphicThe average score that a drawing received from the evaluators ranged from 3 to 5.The evaluators viewed the drawings drawn for the graphics with a trend more favor-ably and assigned a score of 4 or more in most cases.
For each graphic, we computed theaverage score given by the evaluators to all drawings constructed from the summaryof that graphic (i.e., the average of the three scores given to each of the 18 drawingsdrawn from the same summary).
The graphics conveying an increasing (Figure 3a) anda decreasing trend received a score of 4.22 and 4.63, respectively.
The evaluators gave anaverage score of 3.53 and 4.07 to the graphics which conveyed the rank of the maximumbar (Figure 1) and the rank of a bar among all bars (Figure 9).
Because we do not presentall features of a bar chart in its summary (such as all bar values), obtaining an averagescore of less than 5 for all bar charts is not surprising.We also asked the evaluators to specify a reason (i.e., what is missing or should bechanged) for the cases where they assigned a score of less than 4.
Once we analyzedtheir feedback for the drawings with a trend, we observed that missing values onthe dependent axis (i.e., tick mark labels) and missing measurement axis descriptors(although given in the summaries) were the main reasons.
We argue that presentingtick mark labels is more appropriate for summaries that describe scientific graphics(such as the summaries generated by the iGRAPH-Lite system [Ferres et al 2007]) incontrast to the summaries that we generate for conveying the high-level content ofa graphic.
The evaluators indicated incorrect rankings of some bars as the reason forgiving lower scores to the drawings that present the rank of the maximum bar or therank of a bar among all bars; this is due to the fact that our summaries did not conveythe rankings and the values of all bars in those cases.
Because the intention of thecorresponding graphics is to convey the rank of a single bar (not all bars), we argue thatour summaries facilitate the readers to get the main purpose of these graphics.
Overall,this study demonstrated that our summarization approach is effective in conveyingthe high-level content of bar charts so as to enable readers to correctly understand themain point of the graphic.
We are planning to conduct future studies to explore theeffectiveness of our approach further, however.
One possible evaluation could be asking568Demir, Carberry, and McCoy Summarizing Information Graphics Textuallya different set of participants to draw the bar charts that we used in this study by readingtheir summaries produced by an appropriate baseline approach, and comparing thescores that those new set of drawings received from the same three evaluators with ourcurrent results.
This evaluation will also allow us to determine whether the evaluatorsjudged our summarization approach favorably because they were aware of the overallapproach (i.e., brief summaries are generated by our system and these summaries donot contain everything that can be conveyed by the corresponding bar charts).Favorable results were also achieved when people with visual impairments werepresented with the brief summaries generated by our work in an interactive systemwhich also enabled the user to ask follow-up questions to learn more about about thegraphic.
More on that system and study can be found in Demir et al (2010).9.
Conclusion and Future WorkThe majority of information graphics from popular media are intended to convey amessage that is often not captured by the text of the document.
Thus graphics, alongwith the textual segments, contribute to the overall purpose of a multimodal documentand cannot be ignored.
The work presented in this article is the first to apply natural lan-guage generation technology to provide themessage and high-level knowledge that onewould gain by viewing graphics from popular media via brief textual summaries.
Oursummarization approach treats a graphic as a form of language and utilizes the inferredintention of the graphic designer, the communicative signals present in the graphic, andthe significant visual features of the underlying data in determining what to conveyabout that graph.
Our approach uses a set of content identification rules constructed foreach intended message category of a bar chart in determining the content of the sum-maries.
The propositions selected for inclusion by these rules are organized into a textstructure by applying a novel bottom?up approach which leverages different discourserelated considerations such as the number and syntactic complexity of sentences andclause embeddings that will be used for realization.
Following the generation of refer-ring expressions for certain graphical elements, the structure of a summary is realizedin natural language via a publicly available realizer.
Three different evaluation studiesvalidated the effectiveness of our approach in selecting and coherently organizing themost important information that should be conveyed about a bar chart, and enablingreaders to correctly understand the high-level knowledge of the graphic.In addition to the application area, this article makes contributions to two broadareas of research: data-to-text generation systems and text-to-text generation of referringexpressions.
Here we have viewed the generation of a summary of an informationgraphic as a data-to-text generation problem.
Any data-to-text generation system mustsolve several important problems: (1) out of all of the information in the data, extractout that information that is important enough to be included in the text, (2) structurethe information so it can be realized as a coherent text, (3) aggregate propositions tobe conveyed in the text into more complex yet understandable sentence structures, (4)order the resulting sentence structures so as to maintain text fluency, and (5) realize theinformation as English sentences and generate appropriate referring expressions.
Ourwork has addressed each of these issues in a systematic fashion maintaining modularityof system components and following a development methodology that includes humaninput for making system decisions, and a thorough evaluation of each module as wellas final system evaluation.Although the specific implementations that we developed are geared toward gen-erating summaries of bar charts, the groundwork described in this article is currently569Computational Linguistics Volume 38, Number 3being used by our own group to investigate summarizing other types of graphics (suchas line graphs and grouped bar charts) from popular media.
A Bayesian system forrecognizing the intended message of line graphs has already been developed (Wu et al2010) and the work on constructing the content identification rules for line graphs isunder way (Greenbacker, Carberry, and McCoy 2011).The module that generates referring expressions represents a sophisticated study oftext-to-text referring expression generation.
Referring expression generation is a vibrantfield.
Although the particular rules used for extracting an appropriate referring expres-sion are unique to referring expressions for graphical elements inside an informationgraphic (note: not just bar charts), the work has uncovered some rather interestingproperties in terms of extracting expressions from text which may generalize to otherdomains as well.
Our work is unique in that a full referring expression is pieced togetherfrom text not directly referring to the item in question.
In order to do this, we identifiedtext levels and rules for extracting the core expression along with potential importantmodifiers.
One can imagine this message carrying over to other types of data-to-textactivities as well as to more standard text-to-text generation problems.In future work, we intend to build on the work reported here in several ways.Corpus studies where human subjects tasked with writing brief summaries of graph-ics (the kind of summaries that our system intends to generate) would be of greatpotential in informing generation decisions that our system makes at different levels.Moreover, experts with extensive knowledge of the domain or the targeted end userswere shown to be of greater supply to the development of manyNLG systems.
Learningfrom summaries written by subjects (especially expert writers) would be an excitingarea of future research.
We also believe that our approach would benefit from thesecorpus studies towards exploring how the fluency of the summaries can be furtherimproved particularly by reducing the occasional verbosity in order to achieve tex-tual economy (Stone and Webber 1998).
For example, these efforts might help us todetermine how measurement axis descriptors can be more appropriately phrased indifferent situations.
Improving the current evaluation metric for choosing a text plan isalso in our research agenda.
We utilize three criteria in our evaluation metric for deter-mining the best structure that can be obtained by applying the operators to the selectedpropositions.
In addition, each criteria (the number of sentences, the overall syntacticcomplexity of sentences, and the overall comprehension complexity of all embeddedclauses) has the same impact on the selection process.
We are considering conductingmore user studies in order to identify what other criteria should be taken into accountand how important each criteria should be in relation to all the others.
Moreover,exploring the broader applicability of the novel aspects of our work in other settingsis an interesting topic for future work.
Finally, evaluating the utility of our theoreticallyinformed aggregation mechanism in comparison to or in conjunction with the moresurface-oriented mechanism of the SPoT system would be a promising area for furtherresearch.To our best knowledge, what kinds of summaries best serve the needs of visuallyimpaired individuals has not been throughly studied before.
As mentioned in Sec-tion 2.2.1, we believe that our summaries, once associated with graphics as ALT texts,might be of help to these individuals while reading electronic documents via screenreaders.
One fruitful research direction would be to present such individuals with oursummaries in real-time scenarios and to mine their informational and presentationalneeds.
Such a study would probably provide insights with regard to this questionand potentially lead to guidelines that human summarizers could follow in generatingsummaries for people with visual impairments.570Demir, Carberry, and McCoy Summarizing Information Graphics TextuallyAcknowledgmentsThe authors would like to thank the studyvolunteers for their time and willingnessto participate.
This material is based uponwork supported by the National Institute onDisability and Rehabilitation Research undergrant no.
H133G080047.
For all graphics thatwere used in our evaluations and are givenas examples in this article, inputs (i.e., theinferred intended message and the XMLrepresentation of the graphic) and outputs(i.e., the textual summary of the graphic) ofour system can be found at the followingWeb page: www.cis.udel.edu/?carberry/barcharts-corpus/.ReferencesAlty, James L. and Dimitrios I. Rigas.
1998.Communicating graphical informationto blind users using music: the role ofcontext.
In Proceedings of the SIGCHIConference on Human Factors in ComputingSystems, pages 574?581, Los Angeles, CA.Baldwin, Breck and Thomas Morton.1998.
Dynamic coreference-basedsummarization.
In Proceedings of the3rd Conference on Empirical Methods inNatural Language Processing, pages 1?6,Granada.Barzilay, Regina, Noemie Elhadad, andKathleen McKeown.
2002.
Inferringstrategies for sentence ordering inmultidocument news summarization.Journal of Artificial Intelligence Research,17:35?55.Barzilay, Regina and Mirella Lapata.2006.
Aggregation via set partitioningfor natural language generation.In Proceedings of the Human LanguageTechnology Conference of the NorthAmerican Chapter of the Association ofComputational Linguistics, pages 359?366,New York, NY.Belz, Anja, Eric Kow, Jette Viethen, andAlbert Gatt.
2008.
The Grec challenge2008: Overview and evaluation results.In Proceedings of the 5th InternationalNatural Language Generation Conference,pages 183?191, Salt Fork, OH.Brennan, Susan E., Marilyn W. Friedman,and Carl J. Pollard.
1987.
A centeringapproach to pronouns.
In Proceedingsof the Annual Meeting of the Association ofComputational Linguistics, pages 155?162,Stanford, CA.Carberry, Sandra, Stephanie Elzer, and SenizDemir.
2006.
Information graphics: Anuntapped resource for digital libraries.In Proceedings of the ACM Special InterestGroup on Information Retrieval Conference,pages 581?588, Seattle, WA.Chester, Daniel and Stephanie Elzer.
2005.Getting computers to see informationgraphics so users do not have to.
InProceedings of the 15th InternationalSymposium on Methodologies for IntelligentSystems, pages 660?668, SaratogaSprings, NY.Clark, Herbert.
1996.
Using Language.Cambridge University Press, Cambridge.Coch, Jose.
1998.
Interactive generation andknowledge administration in multimeteo.In Proceedings of 9th International Workshopon Natural Language Generation,pages 300?303, Niagara-on-the-Lake.Corio, Marc and Guy Lapalme.
1999.Generation of texts for informationgraphics.
In Proceedings of the 7th EuropeanWorkshop on Natural Language Generation,pages 49?58, Toulouse.Covington, M., C. He, C. Brown, L. Naci,and J.
Brown.
2006.
How complex is thatsentence?
A proposed revision of therosenberg and abbeduto D-level scale.Research Report, Artificial IntelligenceCenter, University of Georgia.Cycorp.
Open Cyc.
2011.http://www.cyc.com.Dale, Robert and Ehud Reiter.
1995.Computational interpretations of thegricean maxims in the generation ofreferring expressions.
Cognitive Science,19(2):233?263.Dalianis, Hercules.
1999.
Aggregationin natural language generation.Computational Intelligence, 15(4):384?414.Demir, Seniz, Sandra Carberry, andStephanie Elzer.
2009.
Issues in Realizingthe Overall Message of a Bar Chart,John Benjamins, 5th edition.Amsterdam, pages 311?320.Demir, Seniz, David Oliver, EdwardSchwartz, Stephanie Elzer, SandraCarberry, Kathleen F. McCoy, and DanielChester.
2010.
Interactive sight: Textualaccess to simple bar charts.
The NewReview of Hypermedia and Multimedia,16(3):245?279.Di Eugenio, Barbara, Davide Fossati,Dan Yu, Susan Haller, and Michael Glass.2005.
Aggregation improves learning:Experiments in natural languagegeneration for intelligent tutoringsystems.
In Proceedings of the 43rdAnnual Meeting of the Association forComputational Linguistics, pages 50?57,Ann Arbor, MI.571Computational Linguistics Volume 38, Number 3Elhadad, M. and J. Robin.
1999.
SURGE:A comprehensive plug-in syntacticrealization component for text generation.Technical Report, Department ofComputer Science, Ben GurionUniversity.
Beersheba, Israel.Elzer, Stephanie, Sandra Carberry, andIngrid Zukerman.
2011.
The automatedunderstanding of simple bar charts.Artificial Intelligence, 175(2):526?555.Elzer, Stephanie, Nancy Green, SandraCarberry, and James Hoffman.
2006.
Amodel of perceptual task effort for barcharts and its role in recognizing intention.International Journal on User Modeling andUser-Adapted Interaction, 16(1):1?30.Fasciano, Massimo and Guy Lapalme.
2000.Intentions in the coordinated generationof graphics and text from tabular data.Knowledge and Information Systems,2(3):310?339.Fellbaum, Christiane.
1998.WordNet:An Electronic Lexical Database.The MIT Press, Cambridge, MA.Ferres, Leo, Petro Verkhogliad, GitteLindgaard, Louis Boucher, AntoineChretien, and Martin Lachance.
2007.Improving accessibility to statisticalgraphs: the igraph-lite system.In Proceedings of the 9th InternationalACM SIGACCESS Conference onComputers and Accessibility, pages 67?74,Tempe, AZ.Foster, Mary Ellen.
1999.
Automaticallygenerating text to accompanyinformation graphics.
Master?s Thesis,University of Toronto.Friendly, Michael.
2008.
A brief history ofdata visualization.
In C. Chen, W. Ha?rdle,and A. Unwin, editors, Handbook ofComputational Statistics: Data Visualization,volume III.
Springer-Verlag, Heidelberg,pages 1?34.Gatt, Albert, Francois Portet, Ehud Reiter,Jim Hunter, Saad Mahamood, WendyMoncur, and Somayajulu Sripada.
2009.From data to text in the neonatal intensivecare unit: Using NLG technology fordecision support and informationmanagement.
AI Communications,22(3):153?186.Goldberg, Eli, Norbert Driedger, andRichard I. Kittredge.
1994.
Usingnatural-language processing to produceweather forecasts.
IEEE Expert: IntelligentSystems and Their Applications, 9(2):45?53.Goldstein, Jade, Vibhu Mittal, JaimeCarbonell, and Mark Kantrowitz.
2000.Multi-document summarization bysentence extraction.
In Proceedingsof the NAACL-ANLP Workshop onAutomatic Summarization, pages 40?48,Seattle, WA.Greenbacker, Charlie, Sandra Carberry, andKathleen F. McCoy.
2011.
A corpus ofhuman-written summaries of line graphs.In Proceedings of the EMNLP 2011 Workshopon Language Generation and Evaluation(UCNLG+Eval), pages 23?27, Edinburgh.Grice, H. Paul.
1975.
Logic and conversation.Speech Acts, 3:41?58.Grosz, Barbara and Candace Sidner.
1986.Attention, intentions, and the structureof discourse.
Computational Linguistics,12(3):175?204.Grosz, Barbara J., Scott Weinstein, andAravind K. Joshi.
1995.
Centering:A framework for modeling the localcoherence of discourse.
ComputationalLinguistics, 21(2):203?225.Hovy, Eduard H. 1988.
Planning coherentmultisentential text.
In Proceedings ofthe 26th Annual Meeting of the Associationfor Computational Linguistics,pages 163?169, Buffalo, NY.Hovy, Eduard H. 1993.
Automated discoursegeneration using discourse structurerelations.
Artificial Intelligence,63(1-2):341?385.Hovy, Eduard and Chin-Yew Lin.
1996.Automated text summarization and thesummarist system.
In Proceedings of theWorkshop on TIPSTER Text Program,pages 197?214, Vienna, VA.Ina, Satoshi.
1996.
Computer graphics forthe blind.
SIGCAPH Computers and thePhysically Handicapped, 55:16?23.Jayant, Chandrika, Matt Renzelmann,Dana Wen, Satria Krisnandi, RichardLadner, and Dan Comden.
2007.Automated tactile graphics translation: inthe field.
In Proceedings of the 9thInternational ACM SIGACCESS Conferenceon Computers and Accessibility, pages 75?82,Tempe, AZ.Johnson, Mark.
1998.
Proof nets and thecomplexity of processing center embeddedconstructions.
Journal of Logic, Languageand Information, 7(4):433?447.Joshi, Aravind, Bonnie Webber, andRalph Weischedel.
1984.
Living up toexpectations: Computing expert responses.In Proceedings of the National Conference onArtificial Intelligence, pages 169?175,Austin, TX.Karamanis, Nikiforos, Chris Mellish,Massimo Poesio, and Jon Oberlander.2009.
Evaluating centering for information572Demir, Carberry, and McCoy Summarizing Information Graphics Textuallyordering using corpora.
ComputationalLinguistics, 35(1):29?46.Kennel, A.
1996.
Audiograf: Adiagram-reader for the blind.
InProceedings of the 2nd Annual ACMConference on Assistive Technologies,pages 51?56, Vancover, BC, Canada.Kerpedjiev, Stephan and Steven Roth.2000.
Mapping communicative goalsinto conceptual tasks to generategraphics in discourse.
In Proceedingsof the International Conference onIntelligent User Interfaces, pages 60?67,New Orleans, LA.Kibble, Rodger and Richard Power.
2004.Optimizing referential coherence in textgeneration.
Computational Linguistics,30(4):401?416.Kidd, Evan and Edith Bavin.
2002.English-speaking children?scomprehension of relative clauses:Evidence for general-cognitive andlanguage-specific constraints ondevelopment.
Journal of PsycholinguisticResearch, 31(6):599?617.Krahmer, E. and M. Theune.
2002.
Efficientcontext-sensitive generation of referringexpressions.
In K. van Deemter andR.
Kibble, editors, Information Sharing:Reference and Presupposition in LanguageGeneration and Interpretation, Center for theStudy of Language and Information-LectureNotes, volume 143 of CSLI Lecture Notes.CSLI Publications, Stanford, CA,pages 233?264.Krahmer, Emiel, Sebastiaan Van Erk,and Andre?
Verleg.
2003.
Graph-basedgeneration of referring expressions.Computational Linguistics, 29(1):53?72.Kukich, Karen.
1983.
Design of aknowledge-based report generator.In Proceedings of the 21st Annual Meeting ofthe Association for ComputationalLinguistics, pages 145?150,Cambridge, MA.Kurze, Martin.
1995.
Giving blind peopleaccess to graphics (example: businessgraphics).
In Proceedings of theSoftware-Ergonomie Workshop, Dannstadt,Bremen.Lavoie, Benoit and Owen Rambow.
1997.A fast and portable realizer for textgeneration systems.
In Proceedings ofthe 5th Conference on Applied NaturalLanguage Processing, pages 265?268,Washington, DC.Lazar, J., A. Allen, J. Kleinman, andC.
Malarkey.
2007.
What frustrates screenreader users on the web: A study of100 blind users.
International Journal ofHuman-Computer Interaction, 22(3):247?269.Lester, James C. and Bruce W. Porter.
1997.Developing and empirically evaluatingrobust explanation generators: TheKNIGHT experiments.
ComputationalLinguistics, 23(1):65?101.Lin, Dekang.
1996.
On the structuralcomplexity of natural languagesentences.
In Proceedings of the InternationalConference on Computational Linguistics,pages 729?733, Copenhagen, Denmark.Mann, William C. and Sandra A. Thompson.1987.
Rhetorical structure theory: A theoryof text organization.
In Livia Polanyi,editor, The Structure of Discourse.
AblexPublishing Corporation, Norwood, NJ.Marcu, Daniel.
1998.
The Rhetorical Parsing,Summarization, and Generation of NaturalLanguage Texts.
Ph.D. thesis, Department ofComputer Science, University of Toronto.McCoy, Kathleen F., Sandra Carberry, TomRoper, and Nancy Green.
2001.
Towardsgenerating textual summaries of graphs.In Proceedings of the 1st InternationalConference on Universal Access in Human-Computer Interaction, pages 695?699,New Orleans, LA.McCoy, Kathleen F. and Jeannette Cheng.1991.
Focus of attention: Constrainingwhat can be said next.
In Cecile Paris,William Swartout, and William Mann,editors, Natural Language Generation inArtificial Intelligence and ComputationalLinguistics.
Kluwer Academic Publishers,Berlin, pages 103?124.McKeown, Kathleen R. 1985.
Discoursestrategies for generating natural-languagetext.
Artificial Intelligence, 27(1):1?41.McKeown, Kathleen R., Shimei Pan, JamesShaw, Desmond A. Jordan, and Barry A.Allen.
1997.
Language generation formultimedia healthcare briefings.
InProceedings of the 5th Conference on AppliedNatural Language Processing, pages277?282, Washington, DC.Meijer, Peter B.
1992.
An experimentalsystem for auditory image representations.IEEE Transactions on Biomedical Engineering,39(2):112?121.Mellish, Chris, Alisdair Knott, JonOberlander, and Mick O?Donnell.
1998.Experiments using stochastic searchfor text planning.
In Proceedings of the9th International Workshop on NaturalLanguage Generation, pages 98?107,Niagara-on-the-Lake.Moore, Johanna D. and Cecile Paris.1993.
Planning text for advisory573Computational Linguistics Volume 38, Number 3dialogues: Capturing intentional andrhetorical information.
ComputationalLinguistics, 19(4):651?694.Nenkova, Ani and Kathleen McKeown.
2003.References to named entities: a corpusstudy.
In Proceedings of the Conference of theNorth American Chapter of the Association forComputational Linguistics on HumanLanguage Technology, pages 70?72,Edmonton.O?Donnell, M., C. Mellish, J. Oberlander, andA.
Knott.
2001.
Ilex: an architecture for adynamic hypertext generation system.Natural Language Engineering, 7(3):225?250.Paris, Cecile.
1988.
Tailoring objectdescriptions to a user?s level of expertise.Computational Linguistics, 14(3):64?78.Pastra, Katerina, Horacio Saggion, andYorick Wilks.
2003.
Extracting relationalfacts for indexing and retrieval ofcrime-scene photographs.
Knowledge-BasedSystems, 16(5-6):313?320.Portet, Francois, Ehud Reiter, Albert Gatt,Jim Hunter, Somayajulu Sripada, YvonneFreer, and Cindy Sykes.
2009.
Automaticgeneration of textual summaries fromneonatal intensive care data.
ArtificialIntelligence, 173(7-8):789?816.Radev, Dragomir R., Hongyan Jing,Malgorzata Stys, and Daniel Tam.
2004.Centroid-based summarization of multipledocuments.
Information Processing andManagement: An International Journal,40(6):919?938.Ramloll, Rameshsharma, Wai Yu, StephenBrewster, Beate Riedel, Mike Burton, andGisela Dimigen.
2000.
Constructingsonified haptic line graphs for the blindstudent: First steps.
In Proceedings of the4th International ACM Conference onAssistive Technologies, pages 17?25,Arlington, VA.Reiter, Ehud.
2007.
An architecture fordata-to-text systems.
In Proceedings ofthe 11th European Workshop on NaturalLanguage Generation, pages 97?104,Schloss Dagstuhl.Reiter, Ehud and Robert Dale.
2000.
BuildingNatural-language Generation Systems.Cambridge University Press, Cambridge.Schiffman, Barry, Ani Nenkova, andKathleen McKeown.
2002.
Experimentsin multidocument summarization.In Proceedings of the 2nd InternationalConference on Human Language TechnologyResearch, pages 52?58, San Diego, CA.Shaw, James.
1998.
Clause aggregation usinglinguistics knowledge.
In Proceedings ofthe 9th International Workshop on NaturalLanguage Generation, pages 138?147,Niagara-on-the-Lake.Somayajulu, Sripada, Ehud Reiter, and IanDavy.
2003.
Sumtime-mousam:Configurable marine weather forecastgenerator.
Expert Update, 6(3):4?10.Stent, A., Rashmi Prasad, and MarilynWalker.
2004.
Trainable sentenceplanning for complex informationpresentation in spoken dialog systems.In Proceedings of the 42nd Annual Meeting ofthe Association for Computational Linguistics,pages 79?86, Barcelona.Stone, Matthew and Bonnie Webber.
1998.Textual economy through closely coupledsyntax and semantics.
In Proceedingsof the International Natural LanguageGeneration Conference, pages 178?187,Niagara-on-the-Lake.Suri, Linda Z. and Kathleen F. McCoy.
1994.Raft/rapr and centering: A comparisonand discussion of problems related toprocessing complex sentences.Computational Linguistics, 20(2):301?317.Turner, Ross, Yaji Sripada, and Ehud Reiter.2009.
Generating approximate geographicdescriptions.
In Proceedings of the 12thEuropean Workshop on Natural LanguageGeneration, pages 42?49, Athens.Walker, M., O. Rambow, and M. Rogati.
2002.Training a sentence planner for spokendialogue using boosting.
Computer Speechand Language: Special Issue on SpokenLanguage Generation, 16(3):409?434.Walker, M., A. Stent, F. Mairesse, andR.
Prasad.
2007.
Individual and domainadaptation in sentence planning fordialogue.
Journal of Artificial IntelligenceResearch, 30(1):413?456.Wu, Peng, Sandra Carberry, Stephanie Elzer,and Daniel Chester.
2010.
Recognizingthe intended message of line graphs.In Proceedings of the International Conferenceon the Theory and Application of Diagrams,pages 220?234, Portland, OR.Yngve, Victor H. 1960.
A model and anhypothesis for language structure.American Philosophical Society, 104:444?466.Yu, Jin, Ehud Reiter, Jim Hunter, andChris Mellish.
2007.
Choosing thecontent of textual summaries of largetime-series data sets.
Natural Engineering,13(1):25?49.574
