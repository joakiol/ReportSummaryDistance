Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 16?23,Sydney, July 2006. c?2006 Association for Computational LinguisticsQuestion Pre-Processing in a QA System on InternetDiscussion GroupsChuan-Jie Lin and Chun-Hung ChoDepartment of Computer Science and EngineeringNational Taiwan Ocean UniversityNo 2, Rd Pei-Ning, Keelung 202, Taiwan, R.O.Ccjlin@mail.ntou.edu.tw; futurehero@seed.net.twAbstractThis paper proposes methods topre-process questions in the postingsbefore a QA system can find answers in adiscussion group in the Internet.Pre-processing includes garbage textremoval and question segmentation.Garbage keywords are collected anddifferent length thresholds are assigned tothem for garbage text identification.Interrogative forms and question typesare used to segment questions.
The bestperformance on the test set achieves92.57% accuracy in garbage text removaland 85.87% accuracy in questionsegmentation, respectively.1 IntroductionQuestion answering has been a hot research topicin recent years.
Large scale QA evaluationprojects (e.g.
TREC QA-Track1, QA@CLEF2,and NTCIR 3  QAC and CLQA Tracks) arehelpful to the developments of questionanswering.However, real automatic QA services are notready in the Internet.
One popular way forInternet users to ask questions and get answers isto visit discussion groups, such as Usenetnewsgroups 4  or Yahoo!
Answers 5 .
Eachdiscussion group focuses on one topic so thatusers can easily find one to post their questions.There are two ways a user can try to find1 http://trec.nist.gov/data/qa.html2 http://clef-qa.itc.it/3 http://research.nii.ac.jp/ntcir/index-en.html4 Now they can be accessed via Google Groups:http://groups.google.com/5 http://answers.yahoo.com/answers.
You can post your question in arelated discussion group and wait for other usersto provide answers.
Some discussion groupsprovide search toolbars so that you can searchyour question first to see if there are similarpostings asking the same question.
In Yahoo!Answers, you can also judge answers offered byother users and mark the best one.Postings in discussion groups are goodmaterials to develop a FAQ-style QA system inthe Internet.
By finding questions in thediscussion groups similar to a new posting,responses to these questions can provide answersor relevant information.But without pre-processing, measuringsimilarity with original texts will arise someproblems:1.
Some phrases such as ?many thanks?
or?help me please?
are not part of aquestion.
These kinds of phrases willintroduce noise and harm matchingperformance.2.
Quite often there is more than onequestion in one posting.
If the questionwhich is most similar to the user'squestion appears in an existed postingtogether with other different questions, itwill get a lower similarity score than theone it is supposed to have because ofother questions.Therefore, inappropriate phrases should beremoved and different questions in one postingshould be separated before question comparison.There is no research focusing on this topic.FAQ finders (Lai et al, 2002; Lytinen andTomuro, 2002; Burke, 1997) are closely relatedto this topic.
However, there are differencesbetween them.
First of all, questions in a FAQset are often written in perfect grammar without16garbage text.
Second, questions are oftenpaired with answers separately.
I.e.
there isoften one question in one QA pair.There were some research groups whodivided questions into segments.
Soricut andBrill (2004) chunked questions and used them asqueries to search engines.
Saquete et al (2004)focused on decomposition of a complex questioninto several sub-questions.
In this paper,question segmentation is to identify differentquestions posed in one posting.2 Garbage Text Removal2.1 Garbage TextsArticles in discussion groups are colloquial.Users often write articles as if they are talking toother users.
For this reason, phrases expressingappreciation, begging, or emotions of writers areoften seen in the postings.
For example:??
powerpoint ??
??????
1 ????
access ??????????????
2(About Powerpoint, I?d like to ask1, how toput the whole window seen in Access onto aslide?
Thank you2!
)The phrases ????????
(?I?d like to ask?
)and ????
(?Thank you?)
are unimportant to thequestion itself.These phrases often contain content words,not stop words, and thus are hard to bedistinguished with the real questions.
If thesephrases are not removed, it can happen that twoquestions are judged ?similar?
because one ofthese phrases appears in both questions.A phrase which contributes no informationabout a question is called garbage text in thispaper and should be removed beforehand inorder to reduce noise.
The term theme text isused to refer to the remaining text.After examining real querying postings,some characteristics of garbage texts areobserved:1.
Some words strongly suggest themselvesbeing in a garbage text, such as ?thank?in ?thank you so much?, or ?help?
in?who can help me?.2.
Some words appear in both theme textsand garbage texts, hence ambiguityarises.
For example:??????
(Any expert please help)??????
(Flash Expert)The first phrase is a garbage text, whilethe second phrase is a product name.The word ?expert?
suggests an existenceof a garbage text but not in all cases.Because punctuation marks are not reliable inChinese, we use sentence fragment as the unit tobe processed.
A sentence fragment is definedto be a fragment of text segmented by commas,periods, question marks, exclamation marks, orspace marks.
A space mark can be a boundaryof a sentence fragment only when bothcharacters preceding and following the spacemark are not the English letters, digits, orpunctuation marks.2.2 Strategies to Remove Garbage TextsFrequent terms seen in garbage texts arecollected as garbage keywords and grouped intoclasses according to their meanings and usages.Table 1 gives some examples of classes ofgarbage keywords collected from the training set.Class Garbage KeywordsPlease ???
?, ??,????
?Thanks ??,??,??,??
?Help ??,??,????,??
?Urgent ??,??,??,?
?Table 1.
Some Classes of Garbage KeywordsTo handle ambiguity, this paper proposes alength information strategy to determine garbagetexts as follows:If a sentence fragment contains a garbagekeyword and the length of the fragment afterremoving the garbage keyword is less than athreshold, the whole fragment will be judged as agarbage text.
Otherwise, only the garbagekeyword itself is judged as garbage text if it isnever in an ambiguous case.Different length thresholds are assigned todifferent classes of garbage keywords.
If morethan one garbage keyword occurring in afragment, discard all the keywords first, and thencompare the length of the remaining fragmentwith the maximal threshold among the onescorresponding to these garbage keywords.In order to increase the coverage of garbagekeywords, other linguistic resources are used toexpand the list of garbage keywords.Synonyms in Tongyici Cilin (?????
), a17thesaurus of Chinese words, are added into thelist.
More garbage keywords are added bycommon knowledge.3 Question SegmentationWhen a user posts an article in a discussiongroup, he may pose more than one question atone time.
For example, in the followingposting:Office 2003 ?
XP?????????
??
?
?
?
?
?
?
?
?
?
?
?????????????
(Office 2003 and XP ?
What are thedifferences between them?
Whichversion is newer?
What is the latestversion???????????
)there are 3 questions submitted at a time.
If anew user wants to know the latest version ofOffice, responses to the previous posting willgive answers.Table 2 lists the statistics of number ofquestions in the training set.
The first column isthe number of questions in one posting.
Thesecond and the third columns are the number andthe percentage of postings which contain suchnumber of questions, respectively.Q# Post# Perc (%)1 494 56.982 259 29.873 82 9.464 22 2.545 4 0.46?
6 6 0.69?
2 373 43.02Total 867 100.00Table 2.
Statistics of Number of Questionsin PostingsAs we can see in Table 2, nearly half (43.02%) ofthe postings contain two or more questions.That is why question segmentation is necessary.3.1 Characteristics of Questions in a PostingSeveral characteristics of question texts inpostings were found in real discussion groups:1.
Some people use ???
(question mark) atthe end of a question while some peopledo not.
In Chinese, some people evenseparate sentences only by spacesinstead of punctuation marks.
(Notethat there is no space mark betweenwords in Chinese text.)2.
Questions are usually in interrogativeform.
Either interrogatives or questionmarks appear in the questions.3.
One question may occur repeatedly inthe same posting.
It is often the casethat a question appears both in the titleand in the content.
Sometimes a userrepeats a sentence several times to showhis anxiety.4.
One question may be expressed indifferent ways in the same posting.
Thesentences may be similar.
For example:A: Office2000????????12???
?B: Office2000????????12????
(Can the clipboard of Office2000 onlykeep 12 items?)????
and ????
are synonyms in themeaning of ?keep?.Dissimilar sentences may also referto the same question.
For example,(1) How to use automatic textwrapping in Excel?
(2) If I want to put two or more lines inone cell, what can I do?
(3) How to use it?These three sentences ask the samequestion: ?How to use automatic textwrapping in Excel??
The secondsentence makes a detailed description ofwhat he wants to do.
Topic of the thirdsentence is the same as the first sentencehence is omitted.
Topic ellipsis is quiteoften seen in Chinese.5.
Some users will give examples toexplain the questions.
These sentencesoften start with phrases like ?forexample?
or ?such as?.3.2 Strategies to Separate QuestionsAccording to the observations in Section 3.1,several strategies are proposed to separatequestions:18(1) Separating by Question Mark (???
)It is the simplest method.
We use it as abaseline strategy.
(2) Identifying Questions by InterrogativeFormsQuestions are usually in interrogative formsincluding subject inversion (?is he?
?, ?doesit??
), using interrogatives (?who is??
), or adeclarative sentence attached with a questionmark (?Office2000 is better??).
Only thethird form requires a question mark.
Thefirst two forms can specify themselves asquestions by text only.
Moreover, there areparticles in Chinese indicating a question aswell, such as ???
or ??
?.If a sentence fragment is in interrogativeform, it will be judged as a question andseparated from the others.
A fragment notin interrogative form is merged with thenearest question fragment preceding it (orfollowing it if no preceding one).
Note thatgarbage texts have been removed beforequestion separation.
(3) Merging or Removing Similar SentencesIf two sentence fragments are exactly thesame, one of them will be removed.
If twosentence fragments are similar, they aremerged into one question fragment.Similarity is measured by the Dicecoefficient (Dice, 1945) using weights ofcommon words in the two sentencefragments.
The similarity of two sentencefragments X and Y is defined as follows:( ) ( )( ) ( )??????
?+?=YtXwYXktWtwWtkWtYXSim2,  (1)where Wt(w) is the weight of a word w.  InEquation 1, k is one of the words appearingin both X and Y.  Fragments with similarityhigher than a threshold are merged together.The weight of a word is designed as theweight of its part-of-speech as listed in Table3.
Nouns and verbs have higher weights,while adverbs and particles have lowerweights.
Note that foreign words areassigned a rather high weight, because namesof software products such as ?Office?
or?Oracle?
are often written in English, whichare foreign words with respect to Chinese.POS WeightVt (Transitive Verb),FW (Foreign Word) 100N (Noun) 90Vi (Intransitive Verb) 80A (Adjective) 40ADV (Adverb), ASP (Tense),C (Connective), DET (Determiner),P (Preposition), T (Particle)0Table 3.
Weights of Part-of-SpeechesBefore computing similarity, wordsegmentation is performed to identify wordsin Chinese text.
After that, a part-of-speechtagger is used to obtain POS information ofeach word.
(4) Merging Questions with the Same TypeThe information of question type has beenwidely adopted in QA systems (Zhang andLee, 2003; Hovy et al, 2002; Harabagiu etal., 2001).
Question type often refers to thepossible type of its answer, such as a personname, a location name, or a temporalexpression.
The question types used in thispaper are PERSON, LOCATION, REASON,QUANTITY, TEMPORAL, COMPARISON,DEFINITION, METHOD, SELECTION,YESNO, and OTHER.
Rules to determinequestion types are created manually.This strategy tries to merge two questionfragments of the same question type.
Thispaper proposes two features to determine thethreshold to merge two question fragments:length and sum of term weights of a fragment.Length is measured in characters and termweights are designed as in Table 3.Merging algorithm is as follows: if thefeature value of a question fragment issmaller than a threshold, it will be mergedinto the preceding question fragment (or thefollowing fragment if no preceding one).This strategy applies recursively until noquestion fragment has a feature value lowerthan the threshold.
(5) Merging Example FragmentsIf a fragment starts with a phrase such as ?forexample?
or ?such as?, it will be merged intoits preceding question fragment.194 Experiments4.1 Experimental DataAll the experimental data were collected fromYahoo!
Knowledge+ (Yahoo!????
+) 6 ,discussion groups similar to Yahoo!
Answers butusing Chinese instead of English.Three discussion groups, ?BusinessApplication?
(????
), ?Website Building?(????
), and ?Image Processing?
(????
),were selected to collect querying postings.
Thereason that we chose these three discussiongroups was their moderate growing rates.
Wecould collect enough amount of queryingpostings published in the same period of time.The following kinds of postings were notselected as our experimental data:1.
No questions inside2.
Full of algorithms or program codes3.
Full of emoticons or Martian texts (??
?, a funny term used in Chinese to referto a writing style that uses words withsimilar pronunciation to replace theoriginal text)4.
Redundant postingsTotally 598 querying postings were collected asthe training set and 269 postings as the test set.The real numbers of postings collected from eachgroup are listed in Table 4, where ?BA?, ?WB?,and ?IP?
stand for ?Business Application?,?Website Building?, and ?Image Processing?,respectively.Group BA WB IPTraining Set 198 207 193Test Set 101 69 99Table 4.
Numbers of Postings in the Data SetTwo persons were asked to mark garbage textsand separate questions in the whole data set.
Ifa conflicting case occurred, a third person (whowas one of the authors of this paper) would solvethe inconsistency.4.2 Garbage Texts RemovalThe first factor examined in garbage textremoval is the length threshold.
Table 5 liststhe experimental results on the training set and6 http://tw.knowledge.yahoo.com/Table 6 on the test set.
All garbage keywordsare collected from the training set.Eight experiments were conducted to usedifferent values as length thresholds.
Thestrategy Lenk sets the length threshold to be kcharacters (no matter in Chinese or English).Hence, Len0 is one baseline strategy whichremoves only the garbage keyword itself.
LenSis the other baseline strategy which removes thewhole sentence fragment where a garbagekeyword appears.The strategy Heu uses different lengththresholds for different classes of garbagekeywords.
The thresholds are heuristic valuesafter observing many examples in the trainingset.Accuracy is defined as the percentage ofsuccessful removal.
In one posting, if all realgarbage texts are correctly removed and no othertext is wrongly deleted, it counts one successfulremoval.Strategy Accuracy (%)Len0 64.21LenS 27.59Len1 73.91Len2 78.43Len3 80.60Len4 78.26Len5 71.91Heu 99.67HeuExp 99.67Table 5.
Accuracy of Garbage Text Removalwith Different Length Thresholds (Training)Strategy Accuracy (%)Len0 62.08LenS 24.54Len1 69.52Len2 75.09Len3 75.46Len4 71.75Len5 65.80Heu 87.73HeuExp 92.57Table 6.
Accuracy of Garbage Text Removalwith Different Length Thresholds (Test Set)As we can see in both tables, the two baselinestrategies are poorer than any other strategy.
Itmeans that length threshold is useful to decidegarbage existence.Heu is the best strategy (99.67% on thetraining set and 87.73% on the test set).
Len3 is20the best strategy (80.60% on the training set and75.49% on the test set) among Lenk, but it is farworse than Heu.
We can conclude that thelength threshold should be assigned individuallyfor each class of garbage words.
If it isassigned carefully, the performance of garbageremoval will be good.The second factor is the expansion ofgarbage keywords.
The strategy HeuExp is thesame as Heu except that the list of garbagekeywords was expanded as described in Section2.2.Comparing the last two rows in Table 6,HeuExp strategy improves the performance from87.73% to 92.57%.
It shows that a smallamount of postings can provide good coverage ofgarbage keywords after keyword expansion byusing available linguistic resources.The results of HeuExp and Heu on thetraining set are the same.
It makes sensebecause the expanded list suggests garbageexistence in the training set no more than theoriginal list does.4.3 Question SegmentationOverall StrategiesSix experiments were conducted to see theperformance of different strategies for questionsegmentation.
The strategies used in eachexperiment are:Baseline: using only ???
(question mark) toseparate questionsSameS: removing repeated sentencefragments then separating by ??
?Interrg: after removing repeated sentencefragments, separating questions whichare in interrogative formsSimlrS: following the strategy Interrg,removing or merging similar sentencefragments of the same question typeForInst: following the strategy SimlrS,merging a sentence fragment beginningwith ?for instance?
and alike with itspreceding question fragmentSameQT: following the strategy ForInst,merging question fragments of the samequestion type without consideringsimilarityTable 7 and Table 8 depict the results of the sixexperiments on the training set and the test set,respectively.
The second column in each tablelists the accuracy which is defined as thepercentage of postings which are separated intothe same number of questions as manuallytagged.
The third column gives the number ofpostings which are correctly separated.
Thefourth and the fifth columns contain the numbersof postings which are separated into more andfewer questions, respectively.Strategy Acc (%) Same More FewerBaseline 50.67 303 213 82SameS 59.03 353 156 89Interrg 64.88 388 204 6SimlrS 75.08 449 141 8ForInst 75.75 453 137 8SameQT 88.29 528 13 57Table 7.
Accuracy of Question Segmentationby Different Strategies (Training Set)Strategy Acc (%) Same More FewerBaseline 54.28 146 84 39SameS 65.43 176 54 39Interrg 65.43 176 93 0SimlrS 74.35 200 68 1ForInst 74.35 200 68 1SameQT 85.87 231 16 22Table 8.
Accuracy of Question Segmentationby Different Strategies (Test Set)As we can see in Table 7, performance isimproved gradually after adding new strategies.SameQT achieves the best performance with88.29% accuracy.
Same conclusion could alsobe made by the results on the test set.
SameQTis the best one with 85.87% accuracy.In Table 7, Baseline achieves only 50.67%accuracy.
That matches our observations: (1)one question is often stated many times bysentences ended with question marks in oneposting (as 213 postings were separated intomore questions); (2) some users do not use ???
inwriting (as 82 postings were separated into fewerquestions).SameS greatly reduces the cases (57 postings)of separation into more questions by removingrepeated sentences.On the other hand, Interrg greatly reduces thecases (76 postings) of separation into fewerquestions.
Many question sentences withoutquestion marks were successfully captured bydetecting the interrogative forms.SimlrS also improves a lot (successfullyreducing number of questions separated in 63postings).
But ForInst only improves a little.It is more common to express one questionseveral times in different way than giving21examples.SameQT achieves the best performance,which means that question type is a good strategy.Different ways to express a question are usuallyin the same question type.
Comparing withSimlrS which also considers sentence fragmentsin the same question type, more improvementcomes from the successful merging of fragmentswith topic ellipses, co-references, or paraphrases.However, there may be other questions in thesame question type which are wrongly mergedtogether (as 49 failures in the training set).Considering the results on the test set, Interrgdoes not improve the overall performancecomparing to SameS because the improvementequals the drop.
ForInst does not improveeither.
It seems that giving examples is notcommon in the discussion groups.Thresholds in SameQTIn the strategy SameQT, two features, length andsum of term weights, are used to determinethresholds to merge question fragments asmentioned in Section 3.2.
In order to decidewhich feature is better and which threshold valueshould be set, two experiments were conducted.LenThr Acc (%) LenThr Acc (%)0 75.75 9 85.623 76.25 10 86.624 78.60 15 88.295 81.94 20 88.136 84.95 30 88.637 85.79 40 88.298 86.29 ?
88.29Table 9.
Accuracy of Question Segmentationwith Different Length Thresholds70758085900 3 4 5 6 7 8 9 10 15 20 30 40 ?Length Threshold(%)Figure 1.
Accuracy of Question Segmentationwith Different Length ThresholdsTable 9 depicts the experimental results of usinglength of sentence fragments as mergingthreshold.
The column ?LenThr?
lists differentsettings of length threshold and the column?Acc?
gives the accuracy.The performance is gradually improved asthe value of length threshold increases.
Thebest one is LenThr=30 with 88.63% accuracy.However, ?Always Merging?
(LenThr=?
)achieves 88.29% accuracy, which is alsoacceptable comparing to the best performance.Fig 1 shows the curve of accuracy against lengththreshold.Table 10 presents the experimental results ofusing sum of term weights as merging thresold.The column ?WgtThr?
lists different settings oflength threshold and the column ?Acc?
gives theaccuracy.The performance is also gradually improvedas the value of weight threshold increases.When WgtThr is set to be 500, 700, or 900, theperformance is the best, with 88.46% accuracy.But the same as the threshold settings of lengthfeature, the best one does not outperform?Always Merging?
strategy (WgtThr=?, 88.29%accuracy) too much.
Fig 2 shows the curve ofaccuracy against similarity threshold.WgtThr Acc (%) WgtThr Acc (%)0 75.75 350 87.2950 77.93 400 88.13100 83.11 450 88.29150 85.28 500 88.46200 86.29 700 88.46250 86.79 900 88.46300 87.46 ?
88.29Table 10.
Accuracy of Question Segmentationwith Different Weight Thresholds70758085900 50 100 150 200 250 300 350 400 450 500 700 900 ?Weight Threshold(%)Figure 2.
Accuracy of Question Segmentationwith Different Weight ThresholdsFrom the results of above experiments, we cansee that although using length feature with a22threshold LenThr=30 achieves the bestperformance, ?Always Merging?
is morewelcome for a online system because no featureextraction or computation is needed with only alittle sacrifice of performance.
Hence wechoose ?Always Merging?
as merging strategy inSameQT.5 Conclusion and Future WorkThis paper proposes question pre-processingmethods for a FQA-style QA system ondiscussion groups in the Internet.
For a postingalready existing or being submitted to adiscussion group, garbage texts in it are removedfirst, and then different questions in it areidentified so that they can be compared withother questions individually.An expanded list of garbage keywords isused to detect garbage texts.
If there is agarbage keyword appearing in a sentencefragment and the fragment has a length shorterthan a threshold corresponding to the class of thegarbage keyword, the fragment will be judged asa garbage text.
This method achieves 92.57%accuracy on the test set.
It means that a smallset is sufficient to collect all classes of garbagekeywords.In question segmentation, sentence fragmentsin interrogative forms are considered as questionfragments.
Besides, repeated fragments areremoved and fragments of the same questiontypes are merged into one fragment.
Theoverall accuracy is 85.87% on the test set.In the future, performance of a QA systemwith or without question pre-processing will beevaluated to verify its value.New methods to create the list of garbagekeywords more robotically should be studied, aswell as the automatic assignments of the lengththresholds of classes of garbage keywords.New feature should be discovered in thefuture in order to segment questions moreaccurately.Although the strategies and the thresholds aredeveloped according to experimental data inChinese, we can see that many of them arelanguage-independent or can be adapted with nottoo much effort.ReferenceBurke, Robin, Kristian Hammond, VladimirKulyukin, Steven Lytinen, Noriko Tomuro,and Scott Schoenberg (1997) ?Naturallanguage processing in the FAQFindersystem: Results and prospects,?
Proceedingsof the 1997 AAAI Spring Symposium onNatural Language Processing for the WorldWide Web, pp.
17-26.Dice, Lee R. (1945) ?Measures of the amount ofecologic association between species,?Journal of Ecology, Vol.
26, pp.
297-302.Harabagiu, Sanda, Dan Moldovan, Marius Pa?ca,Rada Mihalcea, Mihai Surdeanu, R?zvanBunescu, Roxana G?rju, Vasile Rus, and PaulMor?rescu (2001) ?The Role ofLexico-Semantic Feedback in Open-DomainTextual Question-Answering,?
Proceedingsof ACL-EACL 2001, pp.
274-281.Hovy, Eduard, Ulf Hermjakob, and Chin-YewLin (2002) ?The Use of External Knowledgein Factoid QA,?
Proceedings of TREC-10, pp.644-652.Lai, Yu-Sheng, Kuao-Ann Fung, andChung-Hsien Wu (2002) ?FAQ Mining viaList Detection,?
Proceedings of the COLINGWorkshop on Multilingual Summarizationand Question Answering.Lytinen, Steven and Noriko Tomuro (2002) ?Theuse of question types to match questions inFAQFinder,?
Proceedings of the 2002 AAAISpring Symposium on Mining Answers fromTexts and Knowledge Bases, pp.
46-53.Saquete, Estela, Patricio Martinez-Barco, RafaelMunoz, and Jose Luis Vicedo Gonzalez(2004) ?Splitting Complex TemporalQuestions for Question Answering Systems,?Proceedings of ACL 2004, pp.
566-573.Soricut, Radu and Eric Brill (2004) ?AutomaticQuestion Answering: Beyond the Factoid,?Proceedings of HLT-NAACL 2004, pp.
57-64.Zhang, Dell and Wee Sun Lee (2003) ?QuestionClassification using Support VectorMachines,?
Proceedings of SIGIR 2003, pp.26-32.23
