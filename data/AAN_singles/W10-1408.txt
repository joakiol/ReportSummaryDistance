Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 67?75,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsHandling Unknown Words in Statistical Latent-Variable Parsing Models forArabic, English and FrenchMohammed Attia, Jennifer Foster, Deirdre Hogan, Joseph Le Roux, Lamia Tounsi,Josef van Genabith?National Centre for Language TechnologySchool of Computing, Dublin City University{mattia,jfoster,dhogan,jleroux,ltounsi,josef}@computing.dcu.ieAbstractThis paper presents a study of the impactof using simple and complex morphologicalclues to improve the classification of rare andunknown words for parsing.
We comparethis approach to a language-independent tech-nique often used in parsers which is basedsolely on word frequencies.
This study is ap-plied to three languages that exhibit differentlevels of morphological expressiveness: Ara-bic, French and English.
We integrate infor-mation about Arabic affixes and morphotac-tics into a PCFG-LA parser and obtain state-of-the-art accuracy.
We also show that thesemorphological clues can be learnt automati-cally from an annotated corpus.1 IntroductionFor a parser to do a reasonable job of analysing freetext, it must have a strategy for assigning part-of-speech tags to words which are not in its lexicon.This problem, also known as the problem of un-known words, has received relatively little attentionin the vast literature on Wall-Street-Journal (WSJ)statistical parsing.
This is likely due to the fact thatthe proportion of unknown words in the standardEnglish test set, Section 23 of the WSJ section ofPenn Treebank, is quite small.
The problem mani-fests itself when the text to be analysed comes froma different domain to the text upon which the parserhas been trained, when the treebank upon which theparser has been trained is limited in size and when?Author names are listed in alphabetical order.
For furthercorrespondence, contact L. Tounsi, D. Hogan or J. Foster.the language to be parsed is heavily inflected.
Weconcentrate on the latter case, and examine the prob-lem of unknown words for two languages which lieon opposite ends of the spectrum of morphologi-cal expressiveness and for one language which liessomewhere in between: Arabic, English and French.In our experiments we use a Berkeley-style latent-variable PCFG parser and we contrast two tech-niques for handling unknown words within the gen-erative parsing model: one in which no language-specific information is employed and one in whichmorphological clues (or signatures) are exploited.We find that the improvement accrued from look-ing at a word?s morphology is greater for Arabicand French than for English.
The morphologicalclues we use for English are taken directly from theBerkeley parser (Petrov et al, 2006) and those forFrench from recent work on French statistical pars-ing with the Berkeley parser (Crabbe?
and Candito,2008; Candito et al, 2009).
For Arabic, we presentour own set of heuristics to extract these signaturesand demonstrate a statistically significant improve-ment of 3.25% over the baseline model which doesnot employ morphological information.We next try to establish to what extent these cluescan be learnt automatically by extracting affixesfrom the words in the training data and ranking theseusing information gain.
We show that this automaticmethod performs quite well for all three languages.The paper is organised as follows: In Section 2we describe latent variable PCFG parsing models.This is followed in Section 3 by a description of ourthree datasets, including statistics on the extent ofthe unknown word problem in each.
In Section 4, we67present results on applying a version of the parserwhich uses a simple, language-agnostic, unknown-word handling technique to our three languages.
InSection 5, we show how this technique is extendedto include morphological information and presentparsing results for English and French.
In Section 6,we describe the Arabic morphological system andexplain how we used heuristic rules to cluster wordsinto word-classes or signatures.
We present parsingresults for the version of the parser which uses thisinformation.
In Section 7, we describe our attemptsto automatically determine the signatures for a lan-guage and present parsing results for the three lan-guages.
Finally, in Section 8, we discuss how thiswork might be fruitfully extended.2 Latent Variable PCFG ParsingJohnson (1998) showed that refining treebank cate-gories with parent information leads to more accu-rate grammars.
This was followed by a collection oflinguistically motivated propositions for manual orsemi-automatic modifications of categories in tree-banks (Klein and Manning, 2003).
In PCFG-LAs,first introduced by Matsuzaki et al (2005), the re-fined categories are learnt from the treebank us-ing unsupervised techniques.
Each base category?
and this includes part-of-speech tags ?
is aug-mented with an annotation that refines its distribu-tional properties.Following Petrov et al (2006) latent annotationsand probabilities for the associated rules are learntincrementally following an iterative process consist-ing of the repetition of three steps.1.
Split each annotation of each symbol into n(usually 2) new annotations and create ruleswith the new annotated symbols.
Estimate1 theprobabilities of the newly created rules.2.
Evaluate the impact of the newly created anno-tations and discard the least useful ones.
Re-estimate probabilities with the new set of anno-tations.3.
Smooth the probabilities to prevent overfitting.We use our own parser which trains a PCFG-LA us-ing the above procedure and parses using the max-1Estimation of the parameters is performed by running Ex-pectation/Maximisation on the training corpus.rule parsing algorithm (Petrov et al, 2006; Petrovand Klein, 2007).
PCFG-LA parsing is relativelylanguage-independent but has been shown to be veryeffective on several languages (Petrov, 2009).
Forour experiments, we set the number of iterations tobe 5 and we test on sentences less than or equal to40 words in length.
All our experiments, apart fromthe final one, are carried out on the development setsof our three languages.3 The DatasetsArabic We use the the Penn Arabic Treebank(ATB) (Bies and Maamouri, 2003; Maamouri andBies., 2004).
The ATB describes written ModernStandard Arabic newswire and follows the style andguidelines of the English Penn-II treebank.
We usethe part-of-speech tagset defined by Bikel and Bies(Bikel, 2004).
We employ the usual treebank split(80% training, 10% development and 10% test).English We use the Wall Street Journal section ofthe Penn-II Treebank (Marcus et al, 1994).
We trainour parser on sections 2-21 and use section 22 con-catenated with section 24 as our development set.Final testing is carried out on Section 23.French We use the French Treebank (Abeille?
etal., 2003) and divide it into 80% for training, 10%for development and 10% for final results.
We fol-low the methodology defined by Crabbe?
and Can-dito (2008): compound words are merged and thetagset consists of base categories augmented withmorphological information in some cases2.Table 1 gives basic unknown word statistics forour three datasets.
We calculate the proportion ofwords in our development sets which are unknownor rare (specified by the cutoff value) in the corre-sponding training set.
To control for training setsize, we also provide statistics when the Englishtraining set is reduced to the size of the Arabic andFrench training sets and when the Arabic training setis reduced to the size of the French training set.
In anideal world where training set sizes are the same forall languages, the problem of unknown words willbe greatest for Arabic and smallest for English.
It is2This is called the CC tagset: base categories with verbalmoods and extraction features68language cutoff #train #dev #unk %unk language #train #dev #unk %unkArabic 0 594,683 70,188 3794 5.40 Reduced English 597,999 72,970 2627 3.60- 1 - - 6023 8.58 (Arabic Size) - - 3849 5.27- 5 - - 11,347 16.17 - - - 6700 9.18- 10 - - 15,035 21.42 - - - 9083 12.45English 0 950,028 72,970 2062 2.83 Reduced Arabic 266,132 70,188 7027 10.01- 1 - - 2983 4.09 (French Size) - - 10,208 14.54- 5 - - 5306 7.27 - - - 16,977 24.19- 10 - - 7230 9.91 - - - 21,434 30.54French 0 268,842 35,374 2116 5.98 Reduced English 265,464 72,970 4188 5.74- 1 - - 3136 8.89 (French Size) - - 5894 8.08- 5 - - 5697 16.11 - - - 10,105 13.85- 10 - - 7584 21.44 - - - 13,053 17.89Table 1: Basic Unknown Word Statistics for Arabic, French and Englishreasonable to assume that the levels of inflectionalrichness have a role to play in these differences.4 A Simple Lexical Probability ModelThe simplest method for handling unknown wordswithin a generative probabilistic parsing/taggingmodel is to reserve a proportion of the lexical ruleprobability mass for such cases.
This is done bymapping rare words in the training data to a spe-cial UNKNOWN terminal symbol and estimating ruleprobabilities in the usual way.
We illustrate the pro-cess with the toy unannotated PCFG in Figures 1and 2.
The lexical rules in Fig.
1 are the originalrules and the ones in Fig.
2 are the result of apply-ing the rare-word-to-unknown-symbol transforma-tion.
Given the input sentence The shares recovered,the word recovered is mapped to the UNKNOWN to-ken and the three edges corresponding to the rulesNNS ?
UNKNOWN, V BD ?
UNKNOWN andJJ ?
UNKNOWN are added to the chart at this posi-tion.
The disadvantage of this simple approach is ob-vious: all unknown words are treated equally and thetag whose probability distribution is most dominatedby rare words in the training will be deemed themost likely (JJ for this example), regardless of thecharacteristics of the individual word.
Apart fromits ease of implementation, its main advantage is itslanguage-independence - it can be used off-the-shelffor any language for which a PCFG is available.3One parameter along which the simple lexical3Our simple lexical model is equivalent to the Berkeley sim-pleLexicon option.probability model can vary is the threshold used todecide whether a word in the training data is rare or?unknown?.
When the threshold is set to n, a wordin the training data is considered to be unknown if itoccurs n or fewer times.
We experiment with threethresholds: 1, 5 and 10.
The result of this experi-ment for our three languages is shown in Table 2.The general trend we see in Table 2 is that thenumber of training set words considered to be un-known should be minimized.
For all three lan-guages, the worst performing grammar is the oneobtained when the threshold is increased to 10.
Thisresult is not unexpected.
With this simple lexicalprobability model, there is a trade-off between ob-taining good guesses for words which do not occurin the training data and obtaining reliable statisticsfor words which do.
The greater the proportion ofthe probability mass that we reserve for the unknownword section of the grammar, the more performancesuffers on the known yet rare words since these arethe words which are mapped to the UNKNOWN sym-bol.
For example, assume the word restructuring oc-curs 10 times in the training data, always tagged asa VBG.
If the unknown threshold is less than ten andif the word occurs in the sentence to be parsed, aVBG edge will be added to the chart at this word?sposition with the probability 10/#VBG.
If, however,the threshold is set to 10, the word (in the training setand the input sentence) will be mapped to UNKNOWNand more possibilities will be explored (an edge foreach TAG ?
UNKNOWN rule in the grammar).
Wecan see from Table 1 that at threshold 10, one fifth69VBD -> fell 50/153VBD -> reoriented 2/153VBD -> went 100/153VBD -> latched 1/153NNS -> photofinishers 1/201NNS -> shares 200/201JJ -> financial 20/24JJ -> centrist 4/24DT -> the 170/170Figure 1: The original toy PCFGVBD -> fell 50/153VBD -> UNKNOWN 3/153VBD -> went 100/153NNS -> UNKNOWN 1/201NNS -> shares 200/201JJ -> financial 20/24JJ -> UNKNOWN 4/24DT -> the 170/170Figure 2: Rare ?
UNKNOWNVBD -> fell 50/153VBD -> UNK-ed 3/153VBD -> went 100/153NNS -> UNK-s 1/201NNS -> shares 200/201JJ -> financial 20/24JJ -> UNK-ist 4/24DT -> the 170/170Figure 3: Rare ?
UN-KNOWN+SIGNATUREUnknown Threshold Recall Precision F-Score Tagging AccuracyArabic1 78.60 80.49 79.53 94.035 77.17 79.81 78.47 91.1610 75.32 78.69 76.97 89.06English1 89.20 89.73 89.47 95.605 88.91 89.74 89.33 94.6610 88.00 88.97 88.48 93.61French1 83.60 84.17 83.88 94.905 82.31 83.10 82.70 92.9910 80.87 82.05 81.45 91.56Table 2: Varying the Unknown Threshold with the Simple Lexical Probability Modelof the words in the Arabic and French developmentsets are unknown, and this is reflected in the drop inparsing performance at these thresholds.5 Making use of MorphologyUnknown words are not all the same.
We exploit thisfact by examining the effect on parsing accuracy ofclustering rare training set words using cues fromthe word?s morphological structure.
Affixes havebeen shown to be useful in part-of-speech tagging(Schmid, 1994; Tseng et al, 2005) and have beenused in the Charniak (Charniak, 2000), Stanford(Klein and Manning, 2003) and Berkeley (Petrov etal., 2006) parsers.
In this section, we contrast theeffect on parsing accuracy of making use of such in-formation for our three languages of interest.Returning to our toy English example in Figures 1and 2, and given the input sentence The shares re-covered, we would like to use the fact that the un-known word recovered ends with the past tensesuffix -ed to boost the probability of the lexicalrule V BD ?
UNKNOWN.
If we specialise theUNKNOWN terminal using information from Englishmorphology, we can do just that, resulting in thegrammar in Figure 3.
Now the word recovered ismapped to the symbol UNK-ed and the only edgewhich is added to the chart at this position is the onecorresponding to the rule V BD ?
UNK-ed.For our English experiments we use the unknownword classes (or signatures) which are used in theBerkeley parser.
A signature indicates whether awords contains a digit or a hyphen, if a word startswith a capital letter or ends with one of the followingEnglish suffixes (both derivational and inflectional):-s, -ed, -ing, -ion, -er, -est, -ly, -ity, -y and -al.For our French experiments we employ the samesignature list as Crabbe?
and Candito (2008), whichitself was adapted from Arun and Keller (2005).This list consists of (a) conjugation suffixes of regu-70lar verbs for common tenses (eg.
-ons, -ez, -ent.
.
.
)and (b) derivational suffixes for nouns, adverbs andadjectives (eg.
-tion, -ment, -able.
.
.
).The result of employing signature informationfor French and English is shown in Table 3.
Be-side each f-score the absolute improvement over theUNKNOWN baseline (Table 2) is given.
For bothlanguages there is an improvement at all unknownthresholds.
The improvement for English is statis-tically significant at unknown thresholds 1 and 10.4The improvement is more marked for French and isstatistically significant at all levels.In the next section, we experiment with signaturelists for Arabic.56 Arabic SignaturesIn order to use morphological clues for Arabic wego further than just looking at suffixes.
We exploitall the richness of the morphology of this languagewhich can be expressed through morphotactics.6.1 Handling Arabic MorphotacticsMorphotactics refers to the way morphemes com-bine together to form words (Beesley, 1998; Beesleyand Karttunen, 2003).
Generally speaking, morpho-tactics can be concatenative, with morphemes eitherprefixed or suffixed to stems, or non-concatenative,with stems undergoing internal alternations to con-vey morphosyntactic information.
Arabic is consid-ered a typical example of a language that employsnon-concatenative morphotactics.Arabic words are traditionally classified into threetypes: verbs, nouns and particles.
Adjectives takealmost all the morphological forms of, and share thesame templatic structures with, nouns.
Adjectives,for example, can be definite, and are inflected forcase, number and gender.There are a number of indicators that tell uswhether the word is a verb or a noun.
Among4Statistical significance was determined using the strati-fied shuffling method.
The software used to perform the testwas downloaded from http://www.cis.upenn.edu/?dbikel/software.html.5An inspection of the Berkeley Arabic grammar (availableat http://code.google.com/p/berkeleyparser/downloads/list) shows that no Arabic-specific signatureswere employed.
The Stanford parser uses 9 signatures for Ara-bic, designed for use with unvocalised text.
An immediate fu-ture goal is to test this signature list with our parser.these indicators are prefixes, suffixes and word tem-plates.
A template (Beesley and Karttunen, 2003) isa kind of vocalization mould in which a word fits.
Inderivational morphology Arabic words are formedthrough the amalgamation of two tiers, namely, rootand template.
A root is a sequence of three (rarelytwo or four) consonants which are called radicals,and the template is a pattern of vowels, or a com-bination of consonants and vowels, with slots intowhich the radicals of the root are inserted.For the purpose of detection we use the reverseof this information.
Given that we have a word, wetry to extract the stem, by removing prefixes and suf-fixes, and match the word against a number of verbaland nominal templates.
We found that most Ara-bic templatic structures are in complementary dis-tribution, i.e.
they are either restricted to nominalor verbal usage, and with simple regular expressionmatching we can decide whether a word form is anoun or a verb.6.2 Noun IndicatorsIn order to detect that a word form is a noun (or ad-jective), we employ heuristic rules related to Arabicprefixes/suffixes and if none of these rules apply weattempt to match the word against templatic struc-tures.
Using this methodology, we are able to detect95% of ATB nouns.6We define a list of 42 noun templates which areused to indicate active/passive participle nouns, ver-bal nouns, nouns of instrument and broken pluralnouns (see Table 4 for some examples).
Note thattemplates ending with taa marboutah ?ap?
or start-ing with meem madmoumah ?mu?
are not consid-ered since they are covered by our suffix/prefix rules,which are as follows:1- The definite article prefix ?
 or in Buckwaltertransliteration ?Al?.2- The tanween suffix, , or ?N?, ?F?, ?K?, ?AF?.3- The feminine plural suffix HA, or ?+At?.4- The taa marboutah ending ?
or ?ap?
whether as a6The heuristics we developed are designed to work on dia-critized texts.
Although diacritics are generally ignored in mod-ern writing, the issue of restoring diacritics has been satisfac-torily addressed by different researchers.
For example, Nelkenand Shieber (2005) presented an algorithm for restoring diacrit-ics to undiacritized MSA texts with an accuracy of over 90%and Habasah et al (2009) reported on a freely-available toolkit(MADA-TOKAN) an accuracy of over 96%.71Unknown Threshold Recall Precision F-Score Tagging AccuracyArabic1 80.67 82.19 *81.42 (+ 1.89) 96.325 80.66 82.81 *81.72 (+ 3.25) 95.1510 79.86 82.49 *81.15 (+ 4.18) 94.38English1 ***89.64 89.95 89.79 (+ 0.32) 96.445 89.16 89.80 89.48 (+ 0.15) 96.3210 89.14 89.78 **89.46 (+ 0.98) 96.21French1 85.15 85.77 *85.46 (+ 1.58) 96.135 84.08 84.80 *84.44 (+ 1.74) 95.5410 84.21 84.78 *84.49 (+ 3.04) 94.68Table 3: Baseline Signatures for Arabic, French and Englishstatistically significant with *:p < 10?4, **: p < 10?3, ***: p < 0.004,Template Name Regular SpecificationArabic Buckwalter Expression?A?
?K {inofiEAl {ino.i.A.
verbal noun (masdar)?A??
?mifoEAl mi.o.A.
noun instrument???J??
musotafoEil musota.o.i.
noun participle?J?A??
mafAEiyl ma.A.iy.
noun plural??
?J?{isotafoEal {isota.o.a.
verb????
fuwEil .uw.i.
verb passiveTable 4: Sample Arabic Templatic Structures for Nouns and Verbsfeminine marker suffix or part of the word.5- The genitive case marking kasrah , or ?+i?.6- Words of length of at least five characters endingwith doubled yaa ?or ?y?
?.7- Words of length of at least six characters endingwith alif mamdoudah and hamzah Z  or ?A?
?.8- Words of length of at least seven characters start-ing with meem madmoumah ?
or ?mu?.6.3 Verb IndicatorsIn the same way, we define a list of 16 templates andwe combine them with heuristic rules related to Ara-bic prefixes/suffixes to detect whether a word formis exclusively a verb.
The prefix/suffix heuristics areas follows:9-The plural marker suffix  ?
or ?uwA?
indicates averb.10- The prefixes H , ?,?,,?
or ?sa?, ?>a?,?>u?, ?na?, ?nu?, ?ya?, ?yu?, ?ta?, ?tu?
indicate im-prefective verb.The verbal templates are less in number than thenoun templates yet they are no less effective in de-tecting the word class (see Table 4 for examples).Using these heuristics we are able to detect 85% ofATB verbs.6.4 Arabic SignaturesWe map the 72 noun/verb classes that are identi-fied using our hand-crafted heuristics into sets ofsignatures of varying sizes: 4, 6, 14, 21, 25, 28and 72.
The very coarse-grained set considers just4 signatures UNK-noun, UNK-verb, UNK-num,and UNK and the most fine-grained set of 72 signa-tures associates one signature per heuristic.
In ad-dition, we have evaluated the effect of reorderingrules and templates and also the effect of collatingall signatures satisfying an unknown word.
The re-sults of using these various signatures sets in parsing72UNKNUM NOUN VERBdigits (see section 6.2) (see section 6.3)Al definiteness tashkil At suffix ap suffix imperfectrule 1 rules 2 and 5 rule 3 rule 4 rule 10y?
suffix A?
suffix mu prefix verbal noun templates suffixesrule 6 rule 7 rule 8 3 groupings dual/plural suffixesplural templates participle active templates participle passive templates instrument templates passive templates4 groupingsother templates verbal templates5 groupingsTable 6: Arabic signaturesCutoff 1 5 104 80.78 80.71 80.096 81.14 81.16 81.0614 80.88 81.45 81.1914 reorder 81.39 81.01 80.8121 81.38 81.55 81.3521 reorder 81.20 81.13 80.5821 collect 80.94 80.56 79.6325 81.18 81.25 81.2628 81.42 81.72 (+ 3.25) 81.1572 79.64 78.87 77.58Table 5: Baseline Signatures for Arabicour Arabic development set are presented in Table 5.We achieve our best labeled bracketing f-score using28 signatures with an unknown threshold of five.
Infact we get an improvement of 3.25% over using nosignatures at all (see Table 2).
Table 3 describes inmore detail the scores obtained using the 28 signa-tures present in Table 6.
Apart from the set contain-ing 72 signatures, all of the baseline signature sets inTable 5 yield a statistically significant improvementover the generic UNKNOWN results (p < 10?4).7 Using Information Gain to DetermineSignaturesIt is clear that dividing the UNKNOWN terminal intomore fine-grained categories based on morpholog-ical information helps parsing for our three lan-guages.
In this section we explore whether usefulmorphological clues can be learnt automatically.
Ifthey can, it means that a latent-variable PCFG parsercan be adapted to any language without knowledgeof the language in question since the only language-specific component in such a parser is the unknown-signature specification.In a nutshell, we extract affix features from train-ing set words7 and then use information gain to rankthese features in terms of their predictive power in aPOS-tagging task.
The features deemed most dis-criminative are then used as signatures, replacingour baseline signatures described in Sections 5 and6.
We are not going as far as actual POS-tagging,but rather seeing whether the affixes that make goodfeatures for a part-of-speech tagger also make goodunknown word signatures.We experiment with English and French suffixesof length 1-3 and Arabic prefixes and suffixes of var-ious lengths as well as stem prefixes and suffixes oflength 2, 4 and 6.
For each of our languages weexperiment with several information gain thresholdson our development sets and we fix on an Englishsignature list containing 24 suffixes, a French listcontaining 48 suffixes and an Arabic list containing38 prefixes and suffixes.Our development set results are presented in Ta-ble 7.
For all three languages, the information gainsignatures perform at a comparable level to the base-line hand-crafted signatures (Table 3).
For eachof the three unknown-word handling techniques, nosignature (UNKNOWN), hand-crafted signatures andinformation gain signatures, we select the best un-known threshold for each language?s developmentset and apply these grammars to our test sets.
Thef-scores are presented in Table 8, along with the up-per bounds obtained by parsing with these grammarsin gold-tag mode.
For French, the effect of taggingaccuracy on overall parse accuracy is striking.
Theimprovements that we get from using morphologicalsignatures are greatest for Arabic8 and smallest for7We omit all function words and high frequency words be-cause we are interested in the behaviour of words which arelikely to be similar to rare words.8Bikel?s parser trained on the same Arabic data and testedon the same input achieves an f-score of 76.50%.
We traineda 5-split-merge-iteration Berkeley grammar and parsed with the73Unknown Threshold Recall Precision F-Score Tagging AccuracyArabic IG1 80.10 82.15 *81.11 (+ 1.58) 96.535 80.03 82.49 *81.32 (+ 2.85) 95.3010 80.17 82.40 *81.27 (+ 4.3) 94.66English IG1 89.38 89.87 89.63 (+ 0.16) 96.455 89.54 90.22 ***89.88 (+ 0.55) 96.4110 89.22 90.05 *89.63 (+ 1.15) 96.19French IG1 84.78 85.36 *85.07 (+ 1.19) 96.175 84.63 85.24 **84.93 (+ 2.23) 95.3010 84.18 84.80 *84.49 (+ 3.09) 94.68Table 7: Information Gain Signature Resultsstatistically significant with *:p < 10?4, **: p < 2 ?
10?4, ***: p < 0.005Language No Sig Baseline Sig IG SigArabic 78.34 *81.59 *81.33Arabic Gold Tag 81.46 82.43 81.90English 89.48 89.65 89.77English Gold Tag 89.94 90.10 90.23French 83.74 *85.77 **85.55French Gold Tag 88.82 88.41 88.86statistically significant with *: p < 10?4, **: p < 10?3Table 8: F-Scores on Test SetsEnglish.
The results for the information gain signa-tures are promising and warrant further exploration.8 ConclusionWe experiment with two unknown-word-handlingtechniques in a statistical generative parsing model,applying them to Arabic, French and English.
Onetechnique is language-agnostic and the other makesuse of some morphological information (signatures)in assigning part-of-speech tags to unknown words.The performance differences from the two tech-niques are smallest for English, the language withthe sparsest morphology of the three and the small-est proportion of unknown words in its developmentset.
As a result of carrying out these experiments,we have developed a list of Arabic signatures whichcan be used with any statistical parser which doesBerkeley parser, achieving an f-score of 75.28%.
We trained theBerkeley parser with the -treebank SINGLEFILE option so thatEnglish signatures were not employed.its own tagging.
We also present results which showthat signatures can be learnt automatically.Our experiments have been carried out using goldtokens.
Tokenisation is an issue particularly for Ara-bic, but also for French (since the treebank containsmerged compounds) and to a much lesser extent forEnglish (unedited text with missing apostrophes).
Itis important that the experiments in this paper are re-peated on untokenised text using automatic tokeni-sation methods (e.g.
MADA-TOKAN).The performance improvements that we demon-strate for Arabic unknown-word handling are obvi-ously just the tip of the iceberg in terms of what canbe done to improve performance on a morpholog-ically rich language.
The simple generative lexicalprobability model we use can be improved by adopt-ing a more sophisticated approach in which knownand unknown word counts are combined when esti-mating lexical rule probabilities for rare words (seeHuang and Harper (2009) and the Berkeley sophis-ticatedLexicon training option).
Further work willalso include making use of a lexical resource exter-nal to the treebank (Goldberg et al, 2009; Habash,2008) and investigating clustering techniques to re-duce data sparseness (Candito and Crabbe?, 2009).AcknowledgementsThis research is funded by Enterprise Ireland(CFTD/07/229 and PC/09/037) and the Irish Re-search Council for Science Engineering and Tech-nology (IRCSET).
We thank Marie Candito and ourthree reviewers for their very helpful suggestions.74ReferencesAnne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel,2003.
Treebanks: Building and Using ParsedCorpora, chapter Building a Treebank for French.Kluwer, Dordrecht.Abhishek Arun and Frank Keller.
2005.
Lexicalizationin crosslinguistic probabilistic parsing: The case ofFrench.
In ACL.
The Association for Computer Lin-guistics.Kenneth R. Beesley and Lauri Karttunen.
2003.
FiniteState Morphology.
CSLI studies in computational lin-guistics.Kenneth R. Beesley.
1998.
Arabic morphology usingonly finite-state operations.
In The Workshop on Com-putational Approaches to Semitic Languages.Ann Bies and Mohammed Maamouri.
2003.
Penn Ara-bic Treebank guidelines.
Technical Report TB-1-28-03.Dan Bikel.
2004.
On the Parameter Space of GenerativeLexicalized Parsing Models.
Ph.D. thesis, Universityof Pennslyvania.Marie Candito and Benoit Crabbe?.
2009.
Improving gen-erative statistical parsing with semi-supervised wordclustering.
In Proceedings of IWPT?09.Marie Candito, Beno?
?t Crabbe?, and Djame?
Seddah.
2009.On statistical parsing of French with supervised andsemi-supervised strategies.
In Proceedings of theEACL 2009 Workshop on Computational Linguis-tic Aspects of Grammatical Inference, pages 49?57,Athens, Greece, March.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the Annual Meeting of theNorth American Association for Computational Lin-guistics (NAACL-00), pages 132?139, Seattle, Wash-ington.Beno?
?t Crabbe?
and Marie Candito.
2008.
Expe?riencesd?analyse syntaxique statistique du franc?ais.
In Actesde TALN.Yoav Goldberg, Reut Tsarfaty, Meni Adler, and MichaelElhadad.
2009.
Enhancing unlexicalized parsing per-formance using a wide coverage lexicon, fuzzy tag-setmapping, and EM-HMM-based lexical probabilities.In EACL, pages 327?335.
The Association for Com-puter Linguistics.Nizar Habash, Owen Rambow, and Ryan Roth.
2009.Mada+tokan: A toolkit for Arabic tokenization, di-acritization, morphological disambiguation, pos tag-ging, stemming and lemmatization.
In Proceedings ofthe 2nd International Conference on Arabic LanguageResources and Tools (MEDAR).Nizar Habash.
2008.
Four techniques for online handlingof out-of-vocabulary words in arabic-english statisticalmachine translation.
In Proceedings of Association forComputational Linguistics, pages 57?60.Zhongqiang Huang and Mary Harper.
2009.
Self-training pcfg grammars with latent annotations acrosslanguages.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,Singapore, August.Mark Johnson.
1998.
PCFG models of linguis-tic tree representations.
Computational Linguistics,24(4):613?632.Dan Klein and Chris Manning.
2003.
Accurate unlex-icalised parsing.
In Proceedings of the 41st AnnualMeeting of the ACL.Mohammed Maamouri and Ann Bies.
2004.
Developingan Arabic Treebank: Methods, guidelines, procedures,and tools.
In Workshop on Computational Approachesto Arabic Script-based Languages, COLING.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,Robert MacIntyre, Ann Bies, Mark Ferguson, KarenKatz, and Britta Schasberger.
1994.
The Penn Tree-bank: Annotating predicate argument structure.
InProceedings of the 1994 ARPA Speech and NaturalLanguage Workshop, pages 114?119, Princeton, NewJersey.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProceedings of the 43rd Annual Meeting of the ACL,pages 75?82, Ann Arbor, June.Rani Nelken and Stuart M. Shieber.
2005.
Arabic dia-critization using weighted finite-state transducers.
InACL-05 Workshop on Computational Approaches toSemitic Languages.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of HLT-NAACL, Rochester, NY, April.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact and inter-pretable tree annotation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th Annual Meeting of the ACL, Sydney,Australia, July.Slav Petrov.
2009.
Coarse-to-Fine Natural LanguageProcessing.
Ph.D. thesis, University of California atBerkeley, Berkeley, CA, USA.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of the In-ternational Conference on New Methods in LanguageProcessing (NeMLaP-1), pages 44?49.Huihsin Tseng, Daniel Jurafsky, and Christopher Man-ning.
2005.
Morphological features help POS taggingof unknown words across language varieties.
In Pro-ceedings of the Fourth SIGHAN Workshop on ChineseLanguage Processing.75
