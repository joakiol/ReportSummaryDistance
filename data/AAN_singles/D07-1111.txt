Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp.
1044?1050,Prague, June 2007. c?2007 Association for Computational LinguisticsDependency Parsing and Domain Adaptation with LR Models andParser EnsemblesKenji Sagae1 and Jun?ichi Tsujii1,2,31Department of Computer ScienceUniversity of TokyoHongo 7-3-1, Bunkyo-ku, Tokyo, Japan2School of Computer Science, University of Manchester3National Center for Text Mining{sagae,tsujii}@is.s.u-tokyo.ac.jpAbstractWe present a data-driven variant of the LRalgorithm for dependency parsing, and ex-tend it with a best-first search for probabil-istic generalized LR dependency parsing.Parser actions are determined by a classifi-er, based on features that represent the cur-rent state of the parser.
We apply this pars-ing framework to both tracks of the CoNLL2007 shared task, in each case taking ad-vantage of multiple models trained withdifferent learners.
In the multilingual track,we train three LR models for each of theten languages, and combine the analysesobtained with each individual model with amaximum spanning tree voting scheme.
Inthe domain adaptation track, we use twomodels to parse unlabeled data in the targetdomain to supplement the labeled out-of-domain training set, in a scheme similar toone iteration of co-training.1 IntroductionThere are now several approaches for multilingualdependency parsing, as demonstrated in theCoNLL 2006 shared task (Buchholz and Marsi,2006).
The dependency parsing approach pre-sented here extends the existing body of workmainly in four ways:1.
Although stepwise 1  dependency parsing hascommonly been performed using parsing algo-1Stepwise parsing considers each step in a parsing algo-rithm separately, while all-pairs parsing considers entirerithms designed specifically for this task, suchas those described by Nivre (2003) and Yamadaand Matsumoto (2003), we show that this canalso be done using the well known LR parsingalgorithm (Knuth, 1965), providing a connec-tion between current research on shift-reducedependency parsing and previous parsing workusing LR and GLR models;2.
We generalize the standard deterministic step-wise framework to probabilistic parsing, withthe use of a best-first search strategy similar tothe one employed in constituent parsing by Rat-naparkhi (1997) and later by Sagae and Lavie(2006);3.
We provide additional evidence that the parserensemble approach proposed by Sagae and La-vie (2006a) can be used to improve parsing ac-curacy, even when only a single parsing algo-rithm is used, as long as variation can be ob-tained, for example, by using different learningtechniques or changing parsing direction fromforward to backward (of course, even greatergains may be achieved when different algo-rithms are used, although this is not pursuedhere); and, finally,4.
We present a straightforward way to performparser domain adaptation using unlabeled datain the target domain.We entered a system based on the approach de-scribed in this paper in the CoNLL 2007 sharedtrees.
For a more complete definition, see the CoNLL-X shared task description paper (Buchholz and Marsi,2006).1044task (Nivre et al, 2007), which differed from the2006 edition by featuring two separate tracks, onein multilingual parsing, and a new track on domainadaptation for dependency parsers.
In the multi-lingual parsing track, participants train dependencyparsers using treebanks provided for ten languages:Arabic (Hajic et al, 2004), Basque (Aduriz et al2003), Catalan (Mart?
et al, 2007), Chinese (Chenet al, 2003), Czech (B?hmova et al, 2003), Eng-lish (Marcus et al, 1993; Johansson and Nugues,2007), Greek (Prokopidis et al, 2005), Hungarian(Czendes et al, 2005), Italian (Montemagni et al,2003), and Turkish (Oflazer et al,  2003).
In thedomain adaptation track, participants were pro-vided with English training data from the WallStreet Journal portion of the Penn Treebank (Mar-cus et al, 1993) converted to dependencies (Jo-hansson and Nugues, 2007) to train parsers to beevaluated on material in the biological (develop-ment set) and chemical (test set) domains (Kulicket al, 2004), and optionally on text from theCHILDES database (MacWhinney, 2000; Brown,1973).Our system?s accuracy was the highest in thedomain adaptation track (with labeled attachmentscore of 81.06%), and only 0.43% below the topscoring system in the multilingual parsing track(our average labeled attachment score over the tenlanguages was 79.89%).
We first describe our ap-proach to multilingual dependency parsing, fol-lowed by our approach for domain adaptation.
Wethen provide an analysis of the results obtainedwith our system, and discuss possible improve-ments.2 A Probabilistic LR Approach for De-pendency ParsingOur overall parsing approach uses a best-firstprobabilistic shift-reduce algorithm based on theLR algorithm (Knuth, 1965).
As such, it follows abottom-up strategy, or bottom-up-trees, as definedin Buchholz and Marsi (2006), in contrast to theshift-reduce dependency parsing algorithm de-scribed by Nivre (2003), which is a bottom-up/top-down hybrid, or bottom-up-spans.
It is unclearwhether the use of a bottom-up-trees algorithm hasany advantage over the use of a bottom-up-spansalgorithm (or vice-versa) in practice, but the avail-ability of different algorithms that perform thesame parsing task could be advantageous in parserensembles.
The main difference between our pars-er and a traditional LR parser is that we do not usean LR table derived from an explicit grammar todetermine shift/reduce actions.
Instead, we use aclassifier with features derived from much of thesame information contained in an LR table: the topfew items on the stack, and the next few items oflookahead in the remaining input string.
Addition-ally, following Sagae and Lavie (2006), we extendthe basic deterministic LR algorithm with a best-first search, which results in a parsing strategy sim-ilar to generalized LR parsing (Tomita, 1987;1990), except that we do not perform Tomita?sstack-merging operations.The resulting algorithm is projective, and non-projectivity is handled by pseudo-projective trans-formations as described in (Nivre and Nilsson,2005).
We use Nivre and Nilsson?s PATHscheme2.For clarity, we first describe the basic variant ofthe LR algorithm for dependency parsing, which isa deterministic stepwise algorithm.
We then showhow we extend the deterministic parser into a best-first probabilistic parser.2.1 Dependency Parsing with a Data-DrivenVariant of the LR AlgorithmThe two main data structures in the algorithm are astack S and a queue Q.
S holds subtrees of the fi-nal dependency tree for an input sentence, and Qholds the words in an input sentence.
S is initia-lized to be empty, and Q is initialized to hold everyword in the input in order, so that the first word inthe input is in the front of the queue.3The parser performs two main types of actions:shift and reduce.
When a shift action is taken, aword is shifted from the front of Q, and placed onthe top of S (as a tree containing only one node, theword itself).
When a reduce action is taken, the2The PATH scheme was chosen (even though Nivre andNilsson report slightly better results with the HEADscheme) because it does not result in a potentially qua-dratic increase in the number of dependency label types,as observed with the HEAD and HEAD+PATHschemes.
Unfortunately, experiments comparing theuse of the different pseudo-projectivity schemes werenot performed due to time constraints.3We append a ?virtual root?
word to the beginning ofevery sentence, which is used as the head of every wordin the dependency structure that does not have a head inthe sentence.1045two top items in S (s1 and s2) are popped, and anew item is pushed onto S.  This new item is a treeformed by making the root s1 of a dependent of theroot of s2, or the root of s2 a dependent of the rootof s1.
Depending on which of these two cases oc-cur, we call the action reduce-left or reduce-right,according to whether the head of the new tree is tothe left or to the right its new dependent.
In addi-tion to deciding the direction of a reduce action,the label of the newly formed dependency arc mustalso be decided.Parsing terminates successfully when Q is emp-ty (all words in the input have been processed) andS contains only a single tree (the final dependencytree for the input sentence).
If Q is empty, S con-tains two or more items, and no further reduce ac-tions can be taken, parsing terminates and the inputis rejected.
In such cases, the remaining items in Scontain partial analyses for contiguous segments ofthe input.2.2 A Probabilistic LR Model for Dependen-cy ParsingIn the traditional LR algorithm, parser states areplaced onto the stack, and an LR table is consultedto determine the next parser action.
In our case,the parser state is encoded as a set of features de-rived from the contents of the stack S and queue Q,and the next parser action is determined accordingto that set of features.
In the deterministic casedescribed above, the procedure used for determin-ing parser actions (a classifier, in our case) returnsa single action.
If, instead, this procedure returns alist of several possible actions with correspondingprobabilities, we can then parse with a model simi-lar to the probabilistic LR models described byBriscoe and Carroll (1993), where the probabilityof a parse tree is the product of the probabilities ofeach of the actions taken in its derivation.To find the most probable parse tree accordingto the probabilistic LR model, we use a best-firststrategy.
This involves an extension of the deter-ministic shift-reduce into a best-first shift-reducealgorithm.
To describe this extension, we first in-troduce a new data structure Ti that represents aparser state, which includes a stack Si, a queue Qi,and a probability Pi.
The deterministic algorithmis a special case of the probabilistic algorithmwhere we have a single parser state T0 that containsS0 and Q0, and the probability of the parser state is1.
The best-first algorithm, on the other hand,keeps a heap H containing multiple parser statesT0... Tm.
These states are ordered in the heap ac-cording to their probabilities, which are determinedby multiplying the probabilities of each of theparser actions that resulted in that parser state.
Theheap H is initialized to contain a single parser stateT0, which contains a stack S0, a queue Q0 and prob-ability P0 = 1.0.
S0 and Q0 are initialized in thesame way as S and Q in the deterministic algo-rithm.
The best-first algorithm then loops while His non-empty.
At each iteration, first a state Tcurrentis popped from the top of H.  If Tcurrent correspondsto a final state (Qcurrent is empty and Scurrent containsa single item), we return the single item in Scurrentas the dependency structure corresponding to theinput sentence.
Otherwise, we get a list of parseractions act0...actn (with associated probabilitiesPact0...Pactn) corresponding to state Tcurrent.
Foreach of these parser actions actj, we create a newparser state Tnew by applying actj to Tcurrent, and setthe probability Tnew to be Pnew = Pcurrnet * Pactj.Then, Tnew is inserted into the heap H.  Once newstates have been inserted onto H for each of the nparser actions, we move on to the next iteration ofthe algorithm.3 Multilingual Parsing ExperimentsFor each of the ten languages for which trainingdata was provided in the multilingual track of theCoNLL 2007 shared task, we trained three LRmodels as follows.
The first LR model for eachlanguage uses maximum entropy classification(Berger et al, 1996) to determine possible parseractions and their probabilities4.
To control overfit-ting in the MaxEnt models, we used box-type in-equality constraints (Kazama and Tsujii, 2003).The second LR model for each language also usesMaxEnt classification, but parsing is performedbackwards, which is accomplished simply by re-versing the input string before parsing starts.
Sa-gae and Lavie (2006a) and Zeman and ?abokrtsk?
(2005) have observed that reversing the directionof stepwise parsers can be beneficial in parsercombinations.
The third model uses support vectormachines 5  (Vapnik, 1995) using the polynomial4Implementation by Yoshimasa Tsuruoka, available athttp://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/maxent/5Implementation by Taku Kudo, available athttp://chasen.org/~taku/software/TinySVM/ and all vs.all was used for multi-class classification.1046kernel with degree 2.
Probabilities were estimatedfor SVM outputs using the method described in(Platt, 1999), but accuracy improvements were notobserved during development when these esti-mated probabilities were used instead of simply thesingle best action given by the classifier (withprobability 1.0), so in practice the SVM parsingmodels we used were deterministic.At test time, each input sentence is parsed usingeach of the three LR models, and the three result-ing dependency structures are combined accordingto the maximum-spanning-tree parser combinationscheme6 (Sagae and Lavie, 2006a) where each de-pendency proposed by each of the models has thesame weight (it is possible that one of the moresophisticated weighting schemes proposed by Sa-gae and Lavie may be more effective, but thesewere not attempted).
The combined dependencytree is the final analysis for the input sentence.Although it is clear that fine-tuning could pro-vide accuracy improvements for each of the mod-els in each language, the same set of meta-parameters and features were used for all of the tenlanguages, due to time constraints during systemdevelopment.
The features used were7:?
For the subtrees in S(1) and S(2)?
the number of children of the root word ofthe subtrees;?
the number of children of the root word ofthe subtree to the right of the root word;?
the number of children of the root word ofthe subtree to the left of the root word;?
the POS tag and DEPREL of the rightmostand leftmost children;?
The POS tag of the word immediately to theright of the root word of S(2);?
The POS tag of the word immediately to theleft of S(1);6Each dependency tree is deprojectivized before thecombination occurs.7S(n) denotes the nth item from the top of the stack(where S(1) is the item on top of the stack), and Q(n)denotes the nth item in the queue.
For a description ofthe features names in capital letters, see the shared taskdescription (Nivre et al, 2007).?
The previous parser action;?
The features listed for the root words of thesubtrees in table 1.In addition, the MaxEnt models also used selectedcombinations of these features.
The classes usedto represent parser actions were designed to encodeall aspects of an action (shift vs. reduce, right vs.left, and dependency label) simultaneously.Results for each of the ten languages are shownin table 2 as labeled and unlabeled attachmentscores, along with the average labeled attachmentscore and highest labeled attachment score for allparticipants in the shared task.
Our results shownin boldface were among the top three scores forthose particular languages (five out of the ten lan-guages).S(1) S(2) S(3) Q(0) Q(1) Q(3)WORD x x x x xLEMMA x x  xPOS x x x x x xCPOS x x  xFEATS x x  xTable 1: Additional features.Language LAS UAS AvgLASTopLASArabic 74.71 84.04 68.34 76.52Basque 74.64 81.19 68.06 76.94Catalan 88.16 93.34 79.85 88.70Chinese 84.69 88.94 76.59 84.69Czech 74.83 81.27 70.12 80.19English 89.01 89.87 80.95 89.61Greek 73.58 80.37 70.22 76.31Hungarian 79.53 83.51 71.49 80.27Italian 83.91 87.68 78.06 84.40Turkish 75.91 82.72 70.06 79.81ALL 79.90 85.29 65.50 80.32Table 2: Multilingual results.4 Domain Adaptation ExperimentsIn a similar way as we used multiple LR models inthe multilingual track, in the domain adaptationtrack we first trained two LR models on the out-of-1047domain labeled training data.
The first was a for-ward MaxEnt model, and the second was a back-ward SVM model.
We used these two models toperform a procedure similar to a single iteration ofco-training, except that selection of the newly (au-tomatically) produced training instances was doneby selecting sentences for which the two modelsproduced identical analyses.
On the developmentdata we verified that sentences for which there wasperfect agreement between the two models hadlabeled attachment score just above 90 on average,even though each of the models had accuracy be-tween 78 and 79 over the entire development set.Our approach was as follows:1.
We trained the forward MaxEnt and backwardSVM models using the out-of-domain labeledtraining data;2.
We then used each of the models to parse thefirst two of the three sets of domain-specificunlabeled data that were provided (we did notuse the larger third set)3.
We compared the output for the two models,and selected only identical analyses that wereproduced by each of the two separate models;4.
We added those analyses (about 200k words inthe test domain) to the original (out-of-domain) labeled training set;5.
We retrained the forward MaxEnt model withthe new larger training set; and finally6.
We used this model to parse the test data.Following this procedure we obtained a labeledattachment score of 81.06, and unlabeled attach-ment score of 83.42, both the highest scores forthis track.
This was done without the use of anyadditional resources (closed track), but these re-sults are also higher than the top score for the opentrack, where the use of certain additional resourceswas allowed.
See (Nivre et al, 2007).5 Analysis and DiscussionOne of the main assumptions in our use of differ-ent models based on the same algorithm is thatwhile the output generated by those models mayoften differ, agreement between the models is anindication of correctness.
In our domain adapta-tion approach, this was clearly true.
In fact, theapproach would not have worked if this assump-tion was false.
Experiments on the developmentset were encouraging.
As stated before, when theparsers agreed, labeled attachment score was over90, even though the score of each model alone waslower than 79.
The domain-adapted parser had ascore of 82.1, a significant improvement.
Interes-tingly, the ensemble used in the multilingual trackalso produced good results on the development setfor the domain adaptation data, without the use ofthe unlabeled data at all, with a score of 81.9 (al-though the ensemble is more expensive to run).The different models used in each track weredistinct in a few ways: (1) direction (forward orbackward); (2) learner (MaxEnt or SVM); and (3)search strategy (best-first or deterministic).
Ofthose differences, the first one is particularly inter-esting in single-stack shift-reduce models, as ours.In these models, the context to each side of a (po-tential) dependency differs in a fundamental way.To one side, we have tokens that have already beenprocessed and are already in subtrees, and to theother side we simply have a look-ahead of the re-maining input sentence.
This way, the context ofthe same dependency in a forward parser may dif-fer significantly from the context of the same de-pendency in a backward parser.
Interestingly, theaccuracy scores of the MaxEnt backward modelswere found to be generally just below the accuracyof their corresponding forward models when testedon development data, with two exceptions: Hunga-rian and Turkish.
In Hungarian, the accuracyscores produced by the forward and backwardMaxEnt LR models were not significantly differ-ent, with both labeled attachment scores at about77.3 (the SVM model score was 76.1, and the finalcombination score on development data was 79.3).In Turkish, however, the backward score was sig-nificantly higher than the forward score, 75.0 and72.3, respectively.
The forward SVM score was73.1, and the combined score was 75.8.
In expe-riments performed after the official submission ofresults, we evaluated a backward SVM model(which was trained after submission) on the samedevelopment set, and found it to be significantlymore accurate than the forward model, with a scoreof 75.7.
Adding that score to the combinationraised the combination score to 77.9 (a large im-provement from 75.8).
The likely reason for thisdifference is that over 80% of the dependencies inthe Turkish data set have the head to the right of1048the dependent, while only less than 4% have thehead to the left.
This means that the backwardmodel builds much more partial structure in thestack as it consumes input tokens, while the for-ward model must consume most tokens before itstarts making attachments.
In other words, contextin general in the backward model has more struc-ture, and attachments are made while there are stilllook-ahead tokens, while the opposite is generallytrue in the forward model.6 ConclusionOur results demonstrate the effectiveness of evensmall ensembles of parsers that are relativelysimilar (using the same features and the samealgorithm).
There are several possible extensionsand improvements to the approach we havedescribed.
For example, in section 3 we mentionthe use of different weighting schemes independency voting.
We list additional ideas thatwere not attempted due to time constraints, but thatare likely to produce improved results.One of the simplest improvements to our ap-proach is simply to train more models with no oth-er changes to our set-up.
As mentioned in section5, the addition of a backward SVM model did im-prove accuracy on the Turkish set significantly,and it is likely that improvements would also beobtained in other languages.
In addition, otherlearning approaches, such as memory-based lan-guage processing (Daelemans and Van den Bosch,2005), could be used.
A drawback of adding moremodels that became obvious in our experimentswas the increased cost of both training (for exam-ple, the SVM parsers we used required significant-ly longer to train than the MaxEnt parsers) andrun-time (parsing with MBL models can be severaltimes slower than with MaxEnt, or even SVM).
Asimilar idea that may be more effective, but re-quires more effort, is to add parsers based on dif-ferent approaches.
For example, using MSTParser(McDonald and Pereira, 2005), a large-margin all-pairs parser, in our domain adaptation procedureresults in significantly improved accuracy (83.2LAS).
Of course, the use of different approachesused by different groups in the CoNLL 2006 and2007 shared tasks represents great opportunity forparser ensembles.AcknowledgementsWe thank the shared task organizers and treebankproviders.
We also thank the reviewers for theircomments and suggestions, and Yusuke Miyao forinsightful discussions.
This work was supported inpart by Grant-in-Aid for Specially Promoted Re-search 18002007.ReferencesA.
Abeill?, editor.
2003.
Treebanks: Building and UsingParsed Corpora.
Kluwer.A.
Berger, S. A. Della Pietra, and V. J. Della Pietra.1996.
A maximum entropy approach tonaturallanguage processing.
ComputationalLinguistics, 22(1):39?71.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A.Diaz de Ilarraza, A. Garmendia and M. Oronoz.2003.
Construction of a Basque Dependency Tree-bank.
In Proc.
of the 2nd Workshop on Treebanksand Linguistic Theories (TLT), pages 201?204.A.
B?hmov?, J. Hajic, E. Hajicov?
and B. Hladk?.
2003.The PDT: a 3-level annotation scenario.
In Abeill?
(2003), chapter 7, 103?127.E.
Briscoe and J. Carroll.
1993.
Generalized Probabilis-tic LR Parsing of Natural Language (Corpora) withUnification-Based Grammars.
In Computational Lin-guistics, 19(1), pages 25-59.R.
Brown.
1973.
A First Language: The Early Stages.Harvard University Press.S.
Buchholz and E. Marsi.
2006.
CoNLL-X Shared Taskon Multilingual Dependency Parsing.
In Proc.
of theTenth Conference on Computational NaturalLanguage Learning (CoNLL-X).
New York, NY.K.
Chen, C. Luo, M. Chang, F. Chen, C. Chen, C.Huang and Z. Gao.
2003.
Sinica Treebank: DesignCriteria, Representational Issues and Implementation.In Abeill?
(2003), chapter 13, pages 231?248.D.
Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor.2005.
The Szeged Treebank.
Springer.W.
Daelemans and A.
Van den Bosch.
2005.
Memory-based language processing.
Cambridge UniversityPress.J.
Hajic, O. Smrz, P. Zem?nek, J. Snaidauf and E.Beska.
2004.
Prague Arabic Dependency Treebank:Development in Data and Tools.
In Proc.
of theNEMLAR Intern.
Conf.
on Arabic Language Re-sources and Tools, pages 110?117.1049R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProc.
of the 16th Nordic Conference onComputational Linguistics (NODALIDA).J.
Kazama, and J. Tsujii.
2003.
Evaluation andextension of maximum entropy models with ine-quality constraints.
In Proceedings of EMNLP 2003.D.
Knuth.
1965.
On the translation of languages fromleft to right, Information and Control 8, 607-639.S.
Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-Donald, M. Palmer, A. Schein, and L. Ungar.
2004.Integrated annotation for biomedical information ex-traction.
In Proc.
of the Human LanguageTechnology Conference and the Annual Meeting ofthe North American Chapter of the Association forComputational Linguistics (HLT/NAACL).B.
MacWhinney.
2000.
The CHILDES Project: Toolsfor Analyzing Talk.
Lawrence Erlbaum.R.
McDonald, K.Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProc.
of the 43rd Annual Meeting of the Associationfor Computational Linguistics, 2005M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: thePenn Treebank.
Computational Linguistics,19(2):313?330.M.
A.
Mart?, M.
Taul?, L. M?rquez and M. Bertran.2007.
CESS-ECE: A Multilingual and MultilevelAnnotated Corpus.
Available for download from:http://www.lsi.upc.edu/~mbertran/cess-ece/.S.
Montemagni, F. Barsotti, M. Battista, N. Calzolari,O.
Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M.Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D.Saracino, F. Zanzotto, N. Nana, F. Pianesi, and R.Delmonte.
2003.
Building the Italian Syntactic-Semantic Treebank.
In Abeill?
(2003), chapter 11,pages 189?210.J.
Nivre.
2003.
An efficient algorithm for dependencyparsing.
In Proc.
of the Eighth InternationalWorkshop on Parsing Technologies (IWPT?03).Nancy, France.J.
Nivre, and J. Nilsson.
2005.
Pseudo-ProjectiveDependency Parsing.
In Proceedings of the 43rdAnnual Meeting of the Association for ComputationalLinguistics (ACL), 99-106.
Ann Arbor, MI.J.
Nivre, J.
Hall, S. K?bler, R. McDonald, J. Nilsson, S.Riedel, and D. Yuret.
2007.
The CoNLL 2007 sharedtask on dependency parsing.
In Proc.
of the CoNLL2007 Shared Task.
Joint Conf.
on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL).K.
Oflazer, B.
Say, D. Zeynep Hakkani-T?r, and G. T?r.2003.
Building a Turkish treebank.
In Abeill?
(2003),chapter 15, pages 261?277.J.
Platt.
1999.
Probabilistic Outputs for Support VectorMachines and Comparisons to RegularizedLikelihood Methods.
In Advances in Large MarginClassiers, MIT Press.P.
Prokopidis, E. Desypri, M. Koutsombogera, H.Papageorgiou, and S. Piperidis.
2005.
Theoreticaland practical issues in the construction of a Greekdepen- dency treebank.
In Proc.
of the 4th Workshopon Treebanks and Linguistic Theories (TLT), pages149?160.A.
Ratnaparkhi.
1997.
A linear observed time statisticalparser based on maximum entropy models.
InProceedings of the Second Conference on EmpiricalMethods in Natural Language Processing.
Prov-idence, RIK.
Sagae, and A. Lavie.
2006.
A best-first probabilisticshift-reduce parser.
Proceedings of the 43rd Meetingof the Association for Computational Linguistics -posters (ACL'06).
Sydney, Australia.K.
Sagae, and A. Lavie.
2006a.
Parser combination byreparsing.
Proceedings of the 2006 Human LanguageTechnology Conference of the North AmericanChapter of the Association for ComputationalLinguistics - short papers (HLT-NAACL'06).
NewYork, NY.M.
Tomita.
1987.
An efficient augmented context-freeparsing algorithm.
Computational Linguistics, 13:31?46.M.
Tomita.
1990.
The generalized LR parser/compiler -version 8.4.
In Proceedings of the InternationalConference on Computational Linguistics(COLING?90), pages 59?63.
Helsinki, Finland.V.
N. Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer-Verlag.H.
Yamada, and Y. Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.In Proceedings of the Eighth International Workshopon Parsing Technologies (IWPT?03).
Nancy, France.D.
Zeman, Z.
?abokrtsk?.
2005.
Improving Parsing Ac-curacy by Combining Diverse Dependency Parsers.In Proceedings of the International Workshop onParsing Technologies (IWPT 2005).
Vancouver, Brit-ish Columbia.1050
