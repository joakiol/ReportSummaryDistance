Proceedings of the Second Workshop on Statistical Machine Translation, pages 248?255,Prague, June 2007. c?2007 Association for Computational LinguisticsLocalization of Difficult-to-Translate PhrasesBehrang Mohit1 and Rebecca Hwa1,2Intelligent Systems Program1Department of Computer Science2University of PittsburghPittsburgh, PA 15260 U.S.A.{behrang, hwa}@cs.pitt.eduAbstractThis paper studies the impact that difficult-to-translate source-language phrases might haveon the machine translation process.
We formu-late the notion of difficulty as a measurablequantity; we show that a classifier can betrained to predict whether a phrase might bedifficult to translate; and we develop a frame-work that makes use of the classifier and ex-ternal resources (such as human translators) toimprove the overall translation quality.Through experimental work, we verify that byisolating difficult-to-translate phrases andprocessing them as special cases, their nega-tive impact on the translation of the rest of thesentences can be reduced.1 IntroductionFor translators, not all source sentences are createdequal.
Some are straight-forward enough to beautomatically translated by a machine, while othersmay stump even professional human translators.Similarly, within a single sentence there may besome phrases that are more difficult to translatethan others.
The focus of this paper is on identify-ing Difficult-to-Translate Phrases (DTPs) within asource sentence and determining their impact onthe translation process.
We investigate three ques-tions: (1) how should we formalize the notion ofdifficulty as a measurable quantity over an appro-priately defined phrasal unit?
(2) To what level ofaccuracy can we automatically identify DTPs?
(3)To what extent do DTPs affect an MT system'sperformance on other (not-as-difficult) parts of thesentence?
Conversely, would knowing the correcttranslation for the DTPs improve the system?stranslation for the rest of the sentence?In this work, we model difficulty as a meas-urement with respect to a particular MT system.We further assume that the degree of difficulty of aphrase is directly correlated with the quality of thetranslation produced by the MT system, which canbe approximated using an automatic evaluationmetric, such as BLEU (Papineni et al, 2002).
Us-ing this formulation of difficulty, we build aframework that augments an off-the-shelf phrase-based MT system with a DTP classifier that wedeveloped.
We explore the three questions in a setof experiments, using the framework as a testbed.In the first experiment, we verify that our pro-posed difficulty measurement is sensible.
The sec-ond experiment evaluates the classifier's accuracyin predicting whether a source phrase is a DTP.For that, we train a binary SVM classifier via aseries of lexical and system dependent features.The third is an oracle study in which the DTPs areperfectly identified and human translations are ob-tained.
These human-translated phrases are thenused to constrain the MT system as it translates therest of the sentence.
We evaluate the translationquality of the entire sentence and also the parts thatare not translated by humans.
Finally, the frame-work is evaluated as a whole.
Results from ourexperiments suggest that improved handling ofDTPs will have a positive impact the overall MToutput quality.
Moreover, we find the SVM-trained DTP classifier to have a promising rate ofaccuracy, and that the incorporation of DTP infor-mation can improve the outputs of the underlyingMT system.
Specifically, we achieve an improve-ment of translation quality for non-difficult seg-248ments of a sentence when the DTPs are translatedby humans.2 MotivationThere are several reasons for investigating ways toidentify DTPs.
For instance, it can help to findbetter training examples in an active learningframework; it can be used to coordinate outputs ofmultiple translation systems; or it can be used asmeans of error analysis for MT systemdevelopment.
It can also be used as a pre-processing step, an alternative to post-editing.
Formany languages, MT output requires post-translation editing that can be cumbersome task forlow quality outputs, long sentences, complicatedstructures and idioms.
Pre-translation might beviewed as a kind of preventive medicine; that is, asystem might produce an overall better output if itwere not thwarted by some small portion of theinput.
By identifying DTPs and passing those casesoff to an expensive translation resource (e.g.humans) first, we might avoid problems furtherdown the MT pipeline.
Moreover, pre-translationmight not always have to be performed by humans.What is considered difficult for one system mightnot be difficult for another system; thus, pre-translation might also be conducted using multipleMT systems.3 Our ApproachFigure 1 presents the overall dataflow of oursystem.
The input is a source sentence (a1 ... an),from which DTP candidates are proposed.
Becausethe DTPs will have to be translated by humans asindependent units, we limit the set of possiblephrases to be syntactically meaningful units.Therefore, the framework requires a source-language syntactic parser or chunker.
In this paper,we parse the source sentence with an off-the-shelfsyntactic parser (Bikel, 2002).
From the parse treeproduced for the source sentence, every constituentwhose string span is between 25% and 75% of thefull sentence length is considered a DTP candidate.Additionally we have a tree node depth constraintthat requires the constituent to be at least twolevels above the tree?s yield and two levels belowthe root.
These two constraints ensure that theextracted phrases have balanced lengths.We apply the classifier on each candidate andselect the one labeled as difficult with the highestclassification score.
Depending on the underlyingclassifier, the score can be in various formats suchas class probablity, confidence measure, etc.
Inour SVM based classifier, the score is the distancefrom the margin.Figure 1: An overview of our translation frame-work.The chosen phrase (aj ... ak) is translated by ahuman (ei ... em).
We constrain the underlyingphrase-based MT system (Koehn, 2003) so that itsdecoding of the source sentence must contain thehuman translation for the DTP.
In the followingsubsections, we describe how we develop the DTPclassifier with machine learning techniques andhow we constrain the underlying MT system withhuman translated DTPs.3.1 Training the DTP ClassifierGiven a phrase in the source language, the DTPclassifier extracts a set of features from it and pre-dicts whether it is difficult or not based on its fea-ture values.
We use an SVM classifier in this work.We train the SVM-Light implementation of the249algorithm (Joachims 1999).
To train the classifier,we need to tackle two challenges.
First, we need todevelop some appropriate training data becausethere is no corpus with annotated DTPs.
Second,we need to determine a set of predictive featuresfor the classifier.Development of the Gold StandardUnlike the typical SVM training scenario, labeledtraining examples of DTPs do not exist.
Manualcreation of such data requires deep understandingof the linguistics differences of source and targetlanguages and also deep knowledge about the MTsystem and its training data.
Such resources arenot accessible to us.
Instead, we construct the goldstandard automatically.
We make the strong as-sumption that difficulty is directly correlated totranslation quality and that translation quality canbe approximately measured by automatic metricssuch as BLEU.
We have two resource require-ments ?
a sentence-aligned parallel corpus (differ-ent from the data used to train the underlying MTsystem), and a syntactic parser for the source lan-guage.
The procedure for creating the gold stan-dard data is as follows:1.
Each source sentence is parsed.2.
Phrase translations are extracted from the par-allel corpus.
Specifically, we generate word-alignments using GIZA++ (Och 2001) in bothdirections and combine them using the refinedmethodology (Och and Ney 2003), and thenwe applied Koehn?s toolkit (2004) to extractparallel phrases.
We have relaxed the lengthconstraints of the toolkit to ensure the extrac-tion of long phrases (as long as 16 words).3.
Parallel phrases whose source parts are notwell-formed constituents are filtered out.4.
The source phrases are translated by the under-lying MT system, and a baseline BLEU scoreis computed over this set of MT outputs.5.
To label each source phrase, we remove thatphrase and its translation from the MT outputand calculate the set?s new BLEU score.
Ifnew-score is greater than the baseline score bysome threshold value (a tunable parameter), welabel the phrase as difficult, otherwise we labelit as not difficult.Rather than directly calculating the BLEU scorefor each phrase, we performed the round-robinprocedure described in steps 4 and 5 becauseBLEU is not reliable for short phrases.
BLEU iscalculated as a geometric mean over n-grammatches with references, assigning a score of zeroto an entire phrase if no higher-ordered n-grammatches were found against the references.
How-ever, some phrases with a score of 0 might havemore matches in the lower-ordered n-grams thanother phrases (and thus ought to be considered?easier?).
A comparison of the relative changes inBLEU scores while holding out a phrase from thecorpus gives us a more sensitive measurement thandirectly computing BLEU for each phrase.FeaturesBy analyzing the training corpus, we have found18 features that are indicative of DTPs.
Somephrase-level feature values are computed as an av-erage of the feature values of the individual words.The following first four features use some prob-abilities that are collected from a parallel data andword alignments.
Such a resource does not exist atthe time of testing.
Instead we use the history ofthe source words (estimated from the large parallelcorpus) to predict the feature value.
(I) Average probability of word alignmentcrossings: word alignment crossings are indicativeof word order differences and generally structuraldifference across two languages.
We collect wordalignment crossing statistics from the training cor-pus to estimate the crossing probability for eachword in a new source phrase.
For example theArabic word rhl has 67% probability of alignmentcrossing (word movement across English).
Theseprobabilities are then averaged into one value forthe entire phrase.
(II) Average probability of translation ambi-guity: words that have multiple equally-likelytranslations contribute to translation ambiguity.For example a word that has 4 different transla-tions with similar frequencies tends to be moreambiguous than a word that has one dominanttranslation.
We collect statistics about the lexicaltranslational ambiguities from the training corpusand lexical translation tables and use them to pre-dict the ambiguity of each word in a new sourcephrase.
The score for the phrase is the average ofthe scores for the individual words.
(III) Average probability of POS tag changes:Change of a word?s POS tagging is an indicationof deep structural differences between the sourcephrase and the target phrase.
Using the POS tag-ging information for both sides of the training cor-pus, we learn the probability that each sourceword?s POS gets changed after the translation.
To250overcome data sparseness, we only look at the col-lapsed version of POS tags on both sides of thecorpus.
The phrase?s score is the average the indi-vidual word probabilities.
(IV) Average probability of null alignments:In many cases null alignments of the source wordsare indicative of the weakness of information aboutthe word.
This feature is similar to average ambi-guity probability.
The difference is that we use theprobability of null alignments instead of lexicalprobabilities.
(V-IX) Normalized number of unknownwords, content words, numbers, punctuations:For each of these features we normalize the count(e.g.
: unknown words) with the length of thephrase.
The normalization of the features helps theclassifier to not have length preference for thephrases.
(X) Number of proper nouns: Named entitiestend to create translation difficulty, due to theirdiversity of spellings and also domain differences.We use the number of proper nouns to estimate theoccurrence of the named entities in the phrase.
(XI Depth of the subtree: The feature is used asa measure of syntactic complexity of the phrase.For example continuous right branching of theparse tree which adds to the depth of the subtreecan be indicative of a complex or ambiguous struc-ture that might be difficult to translate.
(XII) Constituency type of the phrase:  Weobserve that the different types of constituentshave varied effects on the translations of thephrase.
For example prepositional phrases tend tobelong to difficult phrases.
(XIII) Constituency type of the parent phrase(XIV) Constituency types of the childrennodes of the phrase: We form a set from the chil-dren nodes of the phrase (on the parse tree).
(XV) Length of the phrase: The feature isbased on the number of the words in the phrase.
(XVI) Proportional length of the phrase: Theproportion of the length of the phrase to the lengthof the sentence.
As this proportion gets larger, thecontextual effect on the translation of the phrasebecomes less.
(XVII) Distance from the start of the sentenceand: Phrases that are further away from the start ofthe sentence tend to not be translated as well due tocompounding translational errors.
(XVIII) Distance from a learned translationphrase: The feature measure the number of wordsbefore reaching a learned phrase.
In other words its an indication of the level of error that is intro-duced in the early parts of the phrase translation.3.2 Constraining the MT SystemOnce human translations have been obtained forthe DTPs, we want the MT system to only consideroutput candidates that contain the human transla-tions.
The additional knowledge can be used by thephrase-based system without any code modifica-tion.
Figure 2 shows the data-flow for this process.First, we append the pre-trained phrase-translationtable with the DTPs and their human translationswith a probability of 1.0.
We also include the hu-man translations for the DTPs as training data forthe language model to ensure that the phrase vo-cabulary is familiar to the decoder and relax thephrase distortion parameter that the decoder caninclude all phrase translations with any length inthe decoding.
Thus, candidates that contain thehuman translations for the DTPs will score higherand be chosen by the decoder.Figure 2: Human translations for the DTPs can beincorporated into the MT system?s phrase table andlanguage model.4 ExperimentsThe goal of these four experiments is to gain a bet-ter understanding of the DTPs and their impact onthe translation process.
All our studies are con-ducted for Arabic-to-English MT.
We formed aone-million word parallel text out of two corporareleased by the Linguistic Data Consortium: Ara-251bic News Translation Text Part 1 and Arabic Eng-lish Parallel News Part 1.
The majority of the datawas used to train the underlying phrase-based MTsystem.
We reserve 2000 sentences for develop-ment and experimentation.
Half of these are usedfor the training and evaluation of the DTP classi-fier (Sections 4.1 and 4.2); the other half is usedfor translation experiments on the rest of theframework (Sections 4.3 and 4.4).In both cases, translation phrases are extractedfrom the sentences and assigned ?gold standard?labels according to the procedure described in Sec-tion 3.1.
It is necessary to keep two separate data-sets because the later experiments make use of thetrained DTP classifier.For the two translation experiments, we also facea practical obstacle: we do not have an army ofhuman translators at our disposal to translate theidentified phrases.
To make the studies possible,we rely on a pre-translated parallel corpus to simu-late the process of asking a human to translate aphrase.
That is, we use the phrase extraction toolkitto find translation phrases corresponding to eachDTP candidate (note that the data used for this ex-periment is separate from the main parallel corpusused to train the MT system, so the system has noknowledge about these translations).4.1 Automatic Labeling of DTPIn this first experiment, we verify whether ourmethod for creating positive and negative labeledexamples of DTPs (as described in Section 3.1) issound.
Out of 2013 extracted phrases, we found949 positive instances (DTPs) and 1064 negativeinstances.
The difficult phrases have an averagelength of 8.8 words while the other phrases have anaverage length of 7.8 words1.
We measured theBLEU scores for the MT outputs for both groupsof phrases (Table 1).Experiment BLEU ScoreDTPs 14.34Non-DTPs 61.22Table 1: Isolated Translation of the selected trainingphrasesThe large gap between the translation qualitiesof the two phrase groups suggests that the DTPsare indeed much more ?difficult?
than the otherphrases.1Arabic words are tokenized and lemmatized by Diab?s Ara-bic Toolset (Diab 2004).4.2 Evaluation of the DTP ClassifierWe now perform a local evaluation of the trainedDTP classifier for its classification accuracy.
Theclassifier is trained as an SVM using a linear ker-nel.
The ?gold standard?
phrases from the section4.1 are split into three groups: 2013 instances areused as training data for the classifier; 100 in-stances are used for development (e.g., parametertuning and feature engineering); and 200 instancesare used as test instances.
The test set has an equalnumber of difficult and non-difficult phrases (50%baseline accuracy).In order to optimize the accuracy of classifica-tion, we used a development set for feature engi-neering and trying various SVM kernels and asso-ciated parameters.
For the feature engineeringpart, we used the all-but-one heuristic to test thecontribution of each individual feature.
Table 2presents the most and least contributing four fea-tures that we used in our classification.
Amongvarious features, we observed that the syntacticfeatures are the most contributing sources of in-formation for our classification.Least Useful Features Most Useful FeaturesFt1: Align Crossing Ft 2: Lexical AmbiguityFt 8: Count of Nums Ft 11: Depth of subtreeFt:9: Count of Puncs Ft 12: Const type of PhrFt 10: Count of NNPs Ft 13: Const type of ParTable 2: The most and least useful featuresThe DTP classifier achieves an average accu-racy of 71.5%, using 10 fold cross validation onthe test set.4.3 Study on the effect of DTPsThis experiment concentrates on the second half ofthe framework: that of constraining the MT systemto use human-translations for the DTPs.
Our objec-tive is to assess to what degree do the DTPs nega-tively impact the MT process.
We compare the MToutputs of two groups of sentences.
Group I ismade up of 242 sentences that contain the mostdifficult to translate phrases in the 1000 sentenceswe reserved for this study.
Group II is a controlgroup made up of 242 sentences with the least dif-ficult to translate phrases.
The DTPs make upabout 9% of word counts in the above 484 sen-tences.
We follow the procedure described in Sec-tion 3.1 to identify and score all the phrases; thus,252this experiment can be considered an oracle study.We compare four scenarios:1.
Adding phrase translations for Group I: MTsystem is constrained using the method de-scribed in Section 3.2 to incorporate humantranslations of the pre-identified DTPs inGroup I.22.
Adding phrase translations for Group II:MT system is constrained to use human trans-lations for the identified (non-difficult) phrasesin Group II.3.
Adding translations for random phrases:randomly replace 242 phrases from eitherGroup I or Group II.4.
Adding translations for classifier labeledDTPs: human translations for phrases that ourtrained classifier has identified as DTPs fromboth Group I and Group II.All of the above scenarios are evaluated on acombined set of 484 sentences (group 1 + group 2).This set up normalizes the relative difficulty ofeach grouping.If the DTPs negatively impact the MT process,we would expect to see a greater improvementwhen Group I phrases are translated by humansthan when Group II phrases are translated byhumans.The baseline for the comparisons is to evaluatethe outputs of the MT system without using anyhuman translations.
This results in a BLEU scoreof 24.0.
When human translations are used, theBLEU score of the dataset increases, as shown inTable 3.Experiment BLEUBaseline (no human trans) 24.0w/ translated DTPs (Group I) 39.6w/ translated non-DTPs (Group II) 33.7w/ translated phrases (random) 35.1w/ translated phrases (classifier) 37.0Table 3: A comparison of BLEU scores for the entire setof sentences under the constraints of using human trans-lations for different types of phrases.While it is unsurprising that the inclusion ofhuman translations increases the overall BLEUscore, this comparison shows that the boost issharper when more DTPs are translated.
This is2In this study, because the sentences are from the trainingparallel corpus, we can extract human translations directlyfrom the corpus.consistent with our conjecture that pre-translatingdifficult phrases may be helpful.A more interesting question is whether the hu-man translations still provide any benefit once wefactor out their direct contributions to the increasein BLEU scores.
To answer this question, we com-pute the BLEU scores for the outputs again, thistime filtering out all 484 identified phrases fromthe evaluation.
In other words in this experimentwe focus on the part of the sentence that is not la-beled and does include any human translations.Table 4 presents the results.Experiment BLEUBaseline (no human trans) 23.0w/ translated DTPs (Group I) 25.4w/ translated non-DTPs (Group II) 23.9w/ translated phrases (random) 24.5w/ translated phrases (classifier) 25.1Table 4: BLEU scores for the translation outputs ex-cluding the 484 (DTP and non-DTP) phrases.The largest gain (2.4 BLEU increment frombaseline) occurs when all and only the DTPs weretranslated.
In contrast, replacing phrases fromGroup II did not improve the BLEU score verymuch.
These results suggest that better handling ofDTPs will have a positive effect on the overall MTprocess.
We also note that using our SVM-trainedclassifier to identify the DTPs, the constrained MTsystem?s outputs obtained a BLEU score that isnearly as high as if a perfect classifier was used.4.4 Full evaluation of the frameworkThis final experiment evaluates the completeframework as described in Section 3.
The setup ofthis study is similar to that of the previous section.The main difference is that now, we rely on theclassifier to predict which phrase would be themost difficult to translate and use human transla-tions for those phrases.Out of 1000 sentences, 356 have been identifiedto contain DTPs (that are in the phrase extractionlist).
In other words, only 356 sentences hold DTPsthat we can find their human translations throughphrase projection.
For the remaining sentences, wedo not use any human translation.253Table 5 presents the increase in BLEU scoreswhen human translations for the 356 DTPs areused.
As expected the BLEU score increases, butthe improvement is less dramatic than in the previ-ous experiment because most sentences are un-changed.Experiment BLEUBaseline (no human trans) 24.9w/ human translations  29.0Table 5: Entire Corpus level evaluation (1000 sen-tences) when replacing DTPs in the hit listTable 6 summarizes the experimental results onthe subset of the 356 sentences.
The first two rowscompare the translation quality at the sentencelevel (similar to Table 3); the next two rows com-pare the translation quality of the non-DTP parts(similar to Table 4).
Rows 1 and 3 are conditionswhen we do not use human translation; and rows 2and 4 are conditions when we replace DTPs withtheir associated human translations.
The im-provements of the BLEU score for the hit list aresimilar to the results we have previously seen.Experiment on 356 sentences BLEUBaseline: full sent.
25.1w/ human translation: full sent.
37.6Baseline: discount DTPs 26.0w/ human translation: discountDTPs27.8Table 6: Evaluation of the subset of 356 sentences: bothfor the full sentence and for non-DTP parts, with andwithout human translation replacement of DTPs.5 Related WorkOur work is related to the problem of confidenceestimation for MT (Blatz et.
al.
2004; Zen and Ney2006).
The confidence measure is a score for n-grams generated by a decoder3.
The measure isbased on the features like lexical probabilities(word posterior), phrase translation probabilities,N-best translation hypothesis, etc.
Our DTP classi-fication differs from the confidence measuring inseveral aspects: one of the main purposes of ourclassification of DTPs is to optimize the usage ofoutside resources.
To do so, we focus on classifi-cation of phrases which are syntactically meaning-ful, because those syntactic constituent units have3Most of the confidence estimation measures are for unigrams(word level measures).less dependency to the whole sentence structureand can be translated independently.
Our classifi-cation relies on syntactic features that are impor-tant source of information about the MT difficultyand also are useful for further error tracking (rea-sons behind the difficulty).
Our classification isperformed as a pre-translation step, so it does notrely on the output of the MT system for a test sen-tence; instead, it uses a parallel training corpus andthe characteristics of the underlying MT system(e.g.
: phrase translations, lexical probabilities).Confidence measures have been used for errorcorrection and interactive MT systems.
Ueffingand Ney (2005) employed confidence measureswithin a trans-type-style interactive MT system.
Intheir system, the MT system iteratively generatesthe translation and the human translator accepts apart of the proposed translation by typing one ormore prefix characters.
The system regenerates anew translation based on the human prefix inputand word level confidence measures.
In contrast,our proposed usage of human knowledge is fortranslation at the phrase level.
We use syntacticrestrictions to make the extracted phrases meaning-ful and easy to translate in isolation.
In otherwords, by the usage of our framework trans-typesystems can use human knowledge at the phraselevel for the most difficult segments of a sentence.Additionally by the usage of our framework, theMT system performs the decoding task only once.The idea of isolated phrase translation has beenexplored successfully in MT community.
Koehnand Knight (2003) used isolated translation of NPand PP phrases and merge them with the phrasebased MT system to translate the complete sen-tence.
In our work, instead of focusing on specifictype of phrases (NP or PP), we focus on isolatedtranslation of difficult phrases with an aim to im-prove the translation quality of non-difficult seg-ments too.6 Conclusion and Future WorkWe have presented an MT framework that makesuse of additional information about difficult-to-translate source phrases.
Our framework includesan SVM-based phrase classifier that finds the seg-ment of a sentence that is most difficult to trans-late.
Our classifier achieves a promising 71.5%accuracy.
By asking external sources (such as hu-man translators) to pre-translate these DTPs andusing them to constrain the MT process, we im-254prove the system outputs for the other parts of thesentences.We plan to extend this work in several direc-tions.
First, our framework can be augmented toinclude multiple MT systems.
We expect differentsystems will have difficulties with different con-structs, and thus they may support each other, andthus reducing the need to ask human translators forhelp with the difficult phrases.
Second, our currentmetric for phrasal difficulty depends on BLEU.Considering the recent debates about the shortcom-ings of the BLEU score (Callison-Burch et.
al.2006), we are interested in applying alternativemetrics such a Meteor (Banerjee and Lavie 2005).Third, we believe that there is more room for im-provement and extension of our classification fea-tures.
Specifically, we believe that our syntacticanalysis of source sentences can be improved byincluding richer parsing features.
Finally, theframework can also be used to diagnose recurringproblems in the MT system.
We are currently de-veloping methods for improving the translation ofthe difficult phrases for the phrase-based MT sys-tem used in our experiments.AcknowledgementsThis work is supported by NSF Grant IIS-0612791.We would like to thank Alon Lavie, Mihai Rotaruand the NLP group at Pitt as well as the anony-mous reviewers for their valuable comments.ReferencesSatanjeev Banerjee, Alon Lavie.
2005.
METEOR: Anautomatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Evalua-tion Measures for MT and/or Summarization, pages65?72.Daniel M. Bikel.
2002.
Design of a multi-lingual, paral-lel-processing statistical parsing engine.
In Proceed-ings of ARPA Workshop on Human Language Tech-nologyJohn Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2003.
Confidence estimation formachine translation.
Technical report, Center forLanguage and Speech Processing, Johns HopkinsUniversity, Baltimore.
Summer Workshop Final Re-port.Chris Callison-Burch, Miles Osborne, and PhilipKoehn.
2006.
Re-evaluating the Role of Bleu in Ma-chine Translation Research.
In Proc.
of the EuropeanChapter of the Association for Computational Lin-guistics (EACL), Trento, Italy.Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004.Automatic tagging of Arabic text: From raw text tobase phrase chunks.
In Proceeding of NAACL-HLT2004.
Boston, MA.Thorsten Joachims, Making large-Scale SVM LearningPractical.
Advances in Kernel Methods - SupportVector Learning, B. Sch?lkopf and C. Burges and A.Smola (ed.
), MIT-Press, 1999.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In Proceedings of the Sixth Conference of the As-sociation for Machine Translation in the Americas,pages 115?124Philipp Koehn and Kevin Knight.
2003.
Feature-richstatistical translation of noun phrases.
In Proceedingsof 41st  the Annual Meeting on Association for Com-putational Linguistics (ACL-2003), pages 311?318.Franz Och, 2001, ?Giza++: Training of statistical trans-lation model?
:  http://www.fjoch.com/GIZA++.htmlFranz.
Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Kishore Papineni and Salim Roukos and Todd Wardand Wei-Jing Zhu.
2002.
BLEU: a method for auto-matic evaluation of machine translation.
In Proceed-ings of the 40th Annual Meeting on Association forComputational Linguistics (ACL-2002), Pages 311-318, Philadelphia, PANicola Ueffing and Hermann Ney.
2005.
Application ofword-level confidence measures in translation.
InProceedings of the conference of the European Asso-ciation of Machine Translation (EAMT 2005) , pages262?270, Budapest, HungaryRichard Zens and Hermann Ney, 2006.
N -Gram Poste-rior Probabilities for Statistical Machine Translation.In Proceedings of ACL Workshop on Statistical Ma-chine Translation.
2006255
