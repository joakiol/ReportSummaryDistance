Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 698?707,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPTree Kernel-based SVM with Structured Syntactic Know-ledge for BTG-based Phrase ReorderingMin Zhang             Haizhou LiInstitute for Infocomm Research1 Fusionopolis Way,#21-01 Connexis (South Tower)Singapore 138632{mzhang,hli}@i2r.a-star.edu.sgAbstractStructured syntactic knowledge is importantfor phrase reordering.
This paper proposes us-ing convolution tree kernel over source parsetree to model structured syntactic knowledgefor BTG-based phrase reordering in the con-text of statistical machine translation.
Ourstudy reveals that the structured syntactic fea-tures over the source phrases are very effectivefor BTG constraint-based phrase reorderingand those features can be well captured by thetree kernel.
We further combine the structuredfeatures and other commonly-used linear fea-tures into a composite kernel.
Experimental re-sults on the NIST MT-2005 Chinese-Englishtranslation tasks show that our proposedphrase reordering model statistically signifi-cantly outperforms the baseline methods.1 IntroductionPhrase-based method (Koehn et al, 2003; Ochand Ney, 2004; Koehn et al, 2007) and syntax-based method (Wu, 1997; Yamada and Knight,2001; Eisner, 2003; Chiang, 2005; Cowan et al,2006; Marcu et al, 2006; Liu et al, 2007; Zhanget al, 2007c, 2008a, 2008b; Shen et al, 2008; Miand Huang, 2008) represent the state-of-the-arttechnologies in statistical machine translation(SMT).
As the two technologies are complemen-tary in many ways, an interesting research topicis how to combine the strengths of the two me-thods.
Many research efforts have been made toaddress this issue, which can be summarized intotwo ideas.
One is to add syntax into phrase-basedmodel while another one is to enhance syntax-based model to handle non-syntactic phrases.
Inthis paper, we bring forward the first idea bystudying the issue of how to utilize structuredsyntactic features for phrase reordering in aphrase-based SMT system with BTG (BracketingTransduction Grammar) constraints (Wu, 1997).Word and phrase reordering is a crucial com-ponent in a SMT system.
In syntax-based method,word reordering is implicitly addressed by trans-lation rules, thus the performance is subject toparsing errors to a large extent (zhang et al,2007a) and the impact of syntax on reordering isdifficult to single out (Li et al, 2007).
In phrase-based method, local word reordering1 can be ef-fectively captured by phrase pairs directly whilelocal phrase reordering is explicitly modeled byphrase reordering model and distortion model.Recently, many phrase reordering methods havebeen proposed, ranging from simple distance-based distortion model (Koehn  et al, 2003; Ochand Ney, 2004), flat reordering model (Wu, 1997;Zens et al, 2004), lexicalized reordering model(Tillmann, 2004; Kumar and Byrne, 2005), tohierarchical phrase-based model (Chiang, 2005;Setiawan et al, 2007) and classifier-based reor-dering model with linear features (Zens and Ney,2006; Xiong et al, 2006; Zhang et al, 2007a;Xiong et al, 2008).
However, one of the majorlimitations of these advances is the structuredsyntactic knowledge, which is important to glob-al reordering (Li et al, 2007; Elming, 2008), hasnot been well exploited.
This makes the phrase-based method particularly weak in handlingglobal phrase reordering.
From machine learningviewpoint (Vapnik, 1995), it is computationallyinfeasible to explicitly generate features involv-ing structured information in many NLP applica-1 This paper follows the term convention of global reorder-ing and local reordering of Li et al (2007), between whichthe distinction is solely defined by reordering distance(whether beyond four source words) (Li et al, 2007).698tions.
For example, one cannot enumerate effi-ciently all the sub-tree features for a full parsetree.
This would be the reason why structuredfeatures are not fully utilized in previous statis-tical feature-based phrase reordering model.Thanks to the nice property of kernel-basedmachine learning method that can implicitly ex-plore (structured) features in a high dimensionalfeature space (Vapnik, 1995), in this paper wepropose using convolution tree kernel (Haussler,1999; Collins and Duffy, 2001) to explore thestructured syntactic knowledge for phrase reor-dering and further combine the tree kernel withother diverse linear features into a compositekernel to strengthen the model?s predictive abili-ty.
Indeed, using tree kernel methods to minestructured knowledge has shown success in someNLP applications like parsing (Collins and Duffy,2001), semantic role labeling (Moschitti, 2004;Zhang et al, 2007b), relation extraction (Zhanget al, 2006), pronoun resolution (Yang et al,2006) and question classification (Zhang andLee, 2003).
However, to our knowledge, suchtechnique still remains unexplored for phrasereordering.In this paper, we look into the phrase reorder-ing problem in two aspects: 1) how to model andoptimize structured features, and 2) how to com-bine the structured features with other linear fea-tures and further integrate them into the log-linear model-based translation framework.
Ourstudy shows that: 1) the structured syntactic fea-tures are very useful and 2) our kernel-basedmodel can well explore diverse knowledge, in-cluding previously-used linear features and thestructured syntactic features, for phrase reorder-ing.
Our model displays one advantage over theprevious work that it is able to utilize the struc-tured syntactic features without the need for ex-tensive feature engineering in decoding a parsetree into a set of linear syntactic features.To have a more insightful evaluation, we de-sign three experiments with three different eval-uation metrics.
Experimental results on the NISTMT-2005 Chinese-English translation tasks showthat our method statistically significantly outper-forms the baseline methods in term of the threedifferent evaluation metrics.The rest of the paper is organized as follows.Section 2 introduces the baseline method ofBTG-based phrase translation method while sec-tion 3 discusses the proposed method in detail.The experimental results are reported and dis-cussed in section 4.
Finally, we conclude the pa-per in section 5.2 Baseline System and MethodWe use the MaxEnt-based BTG translation sys-tem (Xiong et al, 2006) as our baseline.
It is aphrase-based SMT system with BTG reorderingconstraint.
The system uses the BTG lexicaltranslation rules ( ?
?
?/? )
to translate thesource phrase ?
into target phrase ?
, and theBTG merging rules (?
?
?
?, ?
?| ?
?, ?
? )
tocombine two neighboring phrases with a straightor inverted order.
In the translation model, theBTG lexical rules are weighted with several fea-tures, such as phrase translation, word penaltyand language models, in a log-linear form.
Withthe BTG constraint, the reordering model ?
isdefined on the two neighboring phrases ??
and??
and their order ?
?
????????
?, ?????????
asfollows:?
?
f(?, ?
?, ??)
(1)In the baseline system, a MaxEnt-based clas-sifier with boundary words of the two neighbor-ing phrases as features is used to model themerging/reordering order.
The baseline MaxEnt-based reordering model is formulized as follows:?
?
??(?|?
?, ??)
?
???(?
????(?,??,??))??
???(?
????(?,??,??))??
(2)where the functions  ??
(?, ?
?, ??)
?
?0,1?
aremodel feature functions using the boundarywords of the two neighboring phrases as features,and ??
are feature weights that are trained basedon the MaxEnt-based criteria.3 Tree Kernel-based Phrase ReorderingModel3.1 Kernel-based Classifier Solution toPhrase ReorderingIn this paper, phrase reordering is recast as aclassification issue as done in previous work(Xiong et al, 2006 & 2008; Zhang et al, 2007a).In training, we use a machine learning algorithmtraining on the annotated phrase reordering in-stances that are automatically extracted fromword-aligned, source sentence parsed trainingcorpus, to learn a classifier.
In testing (decoding),the learned classifier is applied to two adjacentsource phrases to decide whether they should bemerged (straight) or reordered (inverted) andwhat their probabilities are, and then these prob-abilities are used as one feature in the log-linearmodel in a phrase-based decoder.In addition to the previously-used linear fea-tures, we are more interested in the value ofstructured syntax in phrase reordering and howto capture it using kernel methods.
However, not699all classifiers are able to work with kernel me-thods.
Only those dot-product-based classifierscan work with kernels by replacing the dot prod-uct with a kernel function, where the kernel func-tion is able to directly calculate the similaritybetween two (structured) objects without enume-rating them into linear feature vectors.
In thispaper, we select SVM as our classifier.
In thissection, we first define the structured syntacticfeatures and introduce the commonly used linearfeatures, and then discuss how to utilize thesefeatures by kernel methods together SVM forphrase reordering3.2 Structured Syntactic FeaturesA reordering instance ?
?
??
?, ???
(see Eq.1) inthis paper refers to two adjacent source phrases??
and ??
to be translated.
The structured syn-tactic feature spaces of a reordering instance aredefined as the portion of a parse tree of thesource sentence that at least covers the span ofthe reordering instance (i.e.
the two neighboringphrases).
The syntactic features are defined as allT1) Minimum Sub-Tree (MST)T2) Minimum Sub-Structure (MSS)                T4) Chunking Tree (CT)T3) Context-sensitive Minimum Sub-Structure (CMSS)Figure 1.
Different representations of structured syntactic features of a reordering instance in the examplesentence excerpted from our training corpus ???
?/build  ??/scale?
?/mighty ?/of ?
?/varioustypes ?
?/qualified personnel  ?
?/contingent ?
?/above all  ?
?/urgently  ?
?/necessary ?
?/central authorities  ?
?/overall  ??/planning?
(To build a mighty contingent of qualified personnel ofvarious types, it is necessary, above all, for the central authorities to make overall planning.)
?, where ??
?/various types ?
?/qualified personnel  ?
?/contingent (contingent of qualified personnel of varioustypes)?
is the 1st/left phrase and ??
?/above all  ?
?/urgent  ?
?/necessary (it is necessary, above all,?)?
is the 2nd/right phrase.
Note that different function tags are attached to the grammar tag of each inter-nal node.700the possible subtrees in the structured featurespaces.
We can see that the structured featurespaces and their features are encapsulated by afull parse tree of source sentences.
Thus, it iscritical to understand which portion of a parsetree (i.e.
structured feature space) is the most ef-fective to represent a reordering instance.
Moti-vated by the work of (Zhang et al, 2006), wehere examine four cases that contain differentsub-structures as shown in Fig.
1.
(1) Minimum Sub-Tree (MST): the sub-treerooted by the nearest common ancestor of thetwo phrases.
This feature records the minimumsub-structure covering the two phrases and itsleft and right contexts as shown in Fig 1.T1.
(2) Minimum Sub-Structure (MSS): the smal-lest common sub-structure covering the twophrases.
It is enclosed by the shortest path link-ing the two phrases.
Thus, its leaf nodes exactlyconsist of all the phrasal words.
(3) Context-sensitive Minimum Sub-Structure(CMSS): the MSS extending with the 1st leftsibling node of the left phrase and the 1st rightsibling node of the right phrase and their descen-dants.
If sibling is unavailable, then we move tothe parent of current node and repeat the sameprocess until the sibling is available or the root ofthe MST is reached.
(4) Chunking Tree (CT): the base phrase listextracted from the MSS.
We prune out all theinternal structures of the MSS and only keep theroot node and the base phrase list for generatingthe chunking tree.Fig.
1 illustrates the different representationsof an example reordering instance.
T1 is the MSTfor the example instance, where the sub-structurecircled by a dotted line is the MSS, which is alsoshown in T2 for clarity.
We can see that the MSSis a subset of the MST.
By T2 we would like toevaluate whether the structured information iseffective for phrase reordering while by compar-ing the system performance when using T1 andT2, we would like to evaluate whether the struc-tured context information embedded in the MSTis useful to phrase reordering.
T3 is the CMSS,where the two sub-structures circled by dottedlines are included as the context to T2 and makeT3 limited context-sensitive.
This is to evaluatewhether the limited context information in theCMSS is helpful.
By comparing the performanceof T1 and T3, we would like to see whether thelarger context in T1 is a noisy feature.
T4 is theCT, where only the basic structured informationis kept.
By comparing the performance of T2 andT4, we would like to study whether the high-levelstructured syntactic features in T2 are useful tophrase reordering.After defining the four structured featurespaces, we further partition each feature spaceinto five parts according to their functionalities.Because it only makes sense to evaluate two par-titions of the same functionality between tworeordering instances, the feature space partitionleads to a more precise similarity calculation.
Asshown in Fig 1, all the internal nodes in each par-tition are labeled with a unique function tag inthe following way:?
Left Context (-lc): nodes in this partitiondo not cover any phrase word and they areall in the left of the left phrase.?
Right Context (-rc): nodes in this partitiondo not cover any phrase word and they areall in the right of the right phrase.?
Left Phrase (-lp): nodes in this partitiononly cover the first phrase and/or its leftcontext.?
Right Phrase (-rp): nodes in this partitiononly cover the second phrase and/or its rightcontext.?
Shared Part (-sp): nodes in this partition atleast cover both of the two phrases partially.No lexical word is tagged since it is not a partof the structured features, and therefore not par-ticipating in the tree kernel computing.3.3 Linear FeaturesIn our study, we define the following lexicalizedlinear features which are easily to be extractedand integrated to our composite kernel:?
Leftmost and rightmost boundary words ofthe left and right source phrases?
Leftmost and rightmost boundary words ofthe left and right target phrases?
Internal words of the four phrases (exclud-ing boundary words)?
Target language model (LM) score differ-ence  (monotone-inverted)In total, we arrive at 13 features, including 8boundary word features, 4 (kinds of) internalword features and 1 LM feature.
The first 12 fea-tures have been proven useful (Xiong et al,2006; Zhang et al, 2007a) to phrase reordering.LM score is certainly a strong evidence for mod-eling word orders and lexical selection.
Althoughit is already used as a standalone feature in thelog-linear model, we also would like to explicitlyre-optimize it together with other reordering fea-tures in our reordering model.7013.4 Tree Kernel, Composite Kernel and In-tegrating into our Reordering ModelAs discussed before, we use convolution treekernel to capture the structured syntactic featureimplicitly by directly computing similarity be-tween the parse-tree representations of two reor-dering instances with explicitly enumerating allthe features one by one.
In convolution tree ker-nel (Collins and Duffy, 2001), a parse tree T  isimplicitly represented by a vector of integercounts of each sub-tree type (regardless of itsancestors):( )T?
= (# subtree1(T), ?, # subtreen(T))where # subtreei(T) is the occurrence number ofthe ith sub-tree type (subtreei) in T. Since thenumber of different sub-trees is exponential withthe parse tree size, it is computationally infeasi-ble to directly use the feature vector ( )T?
.
Tosolve this computational issue, Collins and Duffy(2001) proposed the following parse tree kernelto calculate the dot product between the abovehigh dimensional vectors implicitly.1 1 2 21 1 2 21 2 1 21 21 2( , ) ( ), ( )( ) ( )( , )(( ) ( ))i isubtree subtreei n N n Nn N n NK T T T TI n I nn n?
??
??
?=< >== ???
?
??
?where N1 and N2 are the sets of nodes in trees T1and T2, respectively, and ( )isubtreeI n  is a functionthat is 1 iff the subtreei occurs with root at node nand zero otherwise, and 1 2( , )n n?
is the number ofthe common subtrees rooted at n1 and n2, i.e.,1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n?
= ?
?1 2( , )n n?
can be further computed efficiently bythe following recursive rules:Rule 1: if the productions (CFG rules) at 1n  and2n  are different, 1 2( , ) 0n n?
= ;Rule 2: else if both 1n  and 2n  are pre-terminals(POS tags), 1 2( , ) 1n n ??
= ?
;Rule 3: else,1( )1 2 1 21( , ) (1 ( ( , ), ( , )))nc njn n ch n j ch n j?=?
= + ??
,where 1( )nc n is the child number of 1n , ch(n,j) isthe jth child of node n  and ?
(0< ?
<1) is the de-cay factor in order to make the kernel value lessvariable with respect to the subtree sizes.
In ad-dition, the recursive Rule 3 holds because giventwo nodes with the same children, one can con-struct common sub-trees using these children andcommon sub-trees of further offspring.
The timecomplexity for computing this kernel is1 2(| | | |)O N N?
and in practice in near to linearcomputational time without the need of enume-rating all subtree features.In our study, the linear feature-based similarityis simply calculated using dot-product.
We thendefine the following composite kernel to com-bine the structured features-based and the linearfeatures-based similarities:??(?
?, ??)
?
?
?
??(?
?, ??)
?
(1 ?
?)
?
??(?
?, ??)
(3)where Kt is the tree kernel over the structuredfeatures and Kl is the linear kernel (dot-product)over the linear features.
The composite kernel Kcis a linear combination of the two individual ker-nels, where the coefficient ?
is set to its defaultvalue 0.3 as that in Moschitti (2004)?s implemen-tation.
The kernels return the similarities be-tween two reordering instances based on theirfeatures used.
Our basic assumption is, the moresimilar the two reordering instances of x1 and x2are, the more chance they share the same order.Now let us see how to integrate the kernelfunctions into SVM.
The linear classifier learnedby SVM is formulized as:( ) sgn( )i i iif x y a x x b= ?
+?
(4)where ia is the weight of a support vector ix (i.e.,a support reordering instance ??
?
??
?, ??
?in ourstudy), iy  is its class label (1:  ????????
or -1: ????????
in our study) and b is the interceptof the hyperplane.
An input reordering instance xis classified as positive (negative) if ( )f x >0 (( )f x <0).Based on the linear classifier, a kernelizedSVM can be easily implemented by simply re-placing the dot product ix x?
in Eq (4) with akernel function ( , )iK x x .
Thus, the kernelizedSVM classifier is formulated as:( ) sgn( ( , ) )i i iif x y a K x x b= +?
(5)where ( , )iK x x is either ( , )c iK x x , ( , )t iK x x or( , )l iK x x in our study.
Following Eq (1), ourreordering model (implemented by the kerne-lized SVM) can be formulized as follows:?
?
f(?, ?
?, ??)
?
????(?|?
?
??
?, ???)?
???(?
(?????
)?, ??)
?
?)? )
(6)A reordering instance x is classified as straight(or inverted) if ????(?|?)
?
0 (or ????(?|?)
?0).
Eq (6) and Eq (2) show the difference be-tween our kernalized SVM-based reordering702model and the MaxEnt-based reordering model.The main difference between them lies in thatour model is able to utilize structured syntacticfeatures by kernalized SVM while the previouswork can only use lexicalized word features byMaxEnt-based classifier.Finally, because the return value of????(?|?)
is a distance function rather than aprobability, we use a sigmoid function to convert????(?|?)
to a posterior probability as shownusing the following to functions and apply it asone feature to the log-linear model in the decod-ing.
( | )1( | )1 svmp o xP straight xe?=+and( | )1( | )1 svmp o xP inverted xe=+where straight represents a positive instance andinverted represents a negative instance.4 Experiments and Discussion4.1 Experimental SettingsBasic Settings: we evaluate our method on Chi-nese-English translation task.
We use the FBIScorpus as training set, the NIST MT-2002 test setas development (dev) set and the NIST MT-2005test set as test set.
The Stanford parser (Klein andManning, 2003) is used to parse Chinese sen-tences on the training, dev and test sets.
GIZA++(Och and Ney, 2004) and the heuristics ?grow-diag-final-and?
are used to generate m-to-n wordalignments.
The translation model is trained onthe FBIS corpus and a 4-gram language model istrained on the Xinhua portion of the English Gi-gaword corpus using the SRILM Toolkits(Stolcke, 2002) with modified Kneser-Neysmoothing (Kenser and Ney, 1995).
For theMER training (Och, 2003), we modify Koehn?sMER trainer (Koehn, 2004) to train our system.For significance test, we use Zhang et als im-plementation (Zhang et al 2004).Baseline Systems: we set three baseline sys-tems: B1) Moses (Koehn et al, 2007) that useslexicalized unigram reordering model to predictthree orientations: monotone, swap and discon-tinuous; B2) MaxEnt-based reordering modelwith lexical boundary word features only (Xionget al, 2006); B3) Linguistically annotated reor-dering model for BTG-based (LABTG) SMT(Xiong et al, 2008).
For Moses, we used the de-fault settings.
We build a CKY-style decoder andintegrate the corresponding reordering modellingmethods into the decoder to implement the 2ndand the 3rd baseline systems and our system.
Ex-cept reordering models, all the four systems usethe same features in translation model, languagemodel and distortion model as Moses in the log-linear framework.
We tune the four systems us-ing the strategies as discussed previously in thissection.Reordering Model Training: we extract allreordering instances from the m-to-n word-aligned training corpus.
The reordering instancesinclude the two source phrases, two target phras-es, order label and its corresponding parse tree.We generate the boundary word features fromthe extracted reordering instances in the sameway as discussed in Xiong et al (2006) and useZhang?s MaxEnt Tools 2  to train a reorderingmodel for the 2nd baseline system.
Similarly, weuse the algorithm 1 in Xiong et al (2008) to ex-tract features and use the same MaxEnt Tools totrain a reordering model for the 3rd baseline sys-tem.
Based on the extracted reordering instances,we generate the four structured features and thelinear features, and then use the Tree KernelTools (Moschitti, 2004) to train our kernel-basedreordering model (linear, tree and composite).Experimental Design and Evaluation Met-rics: we design three experiments and evaluatethem using three metrics.Classification-based: in the first experiment,we extract all reordering instances and their fea-tures from the dev and test sets, and then use thereordering models trained on the training set toclassify (label) those instances extracted from thedev and test sets.
In this way, we can isolate thereordering problem from the influence of others,such as translation model, pruning and decodingstrategies, to better examine the reordering mod-els?
ability and to give analytical insights into thefeatures.
Classification Accuracy (CAcc), thepercentage of the correctly labeled instances overall trials, is used as the evaluation metric.Forced decoding3-based and normal decoding-based: the two experiments evaluate the reorder-ing models through a real SMT system.
Thereordering model and the language model are thesame in the two experiments.
However, in forceddecoding, we train two translation models, oneusing training data only while another using both2 http://homepages.inf.ed.ac.uk/s0450736/maxent.html3 A normal SMT decoder filters a translation model accord-ing to the source sentences, whereas in forced decoding, atranslation model is filtered based on both source sentenceand target references.
In other words, in forced decoding,the decoder is forced to use those phrases whose translationsare already in the references.703training, dev and test data.
By forced decoding,we aim to isolate the reordering problem fromthose of OOV and lexical selections resultingfrom imperfect translation model in the contextof a real SMT task.
Besides the the case-sensitiveBLEU-4 (Papineni et al, 2002) used in the twoexperiments, we design another evaluation me-trics Reordering Accuracy (RAcc) for forced de-coding evaluation.
RAcc is the percentage of theadjacent word pairs with correct word order 4over all words in one-best translation results.Similar to BLEU score, we also use the similarBrevity Penalty BP (Papineni et al, 2002) to pe-nalize the short translations in computing RAcc.Finally, please note for the three evaluation me-trics, the higher values represent better perfor-mance.Feature Spaces CAcc (%)Dev TestMinimum Sub-Tree (MST) 89.87 89.92Minimum Sub-Structure (MSS) 87.95 87.88Context-Sensitive MSS (CMSS) 89.11 89.01Chunking Tree (CT) 86.17 86.21Linear Features (Kl) 90.79 90.46Kl w/o using LM feature (Kl-LM) 84.24 84.06Composite Kernel (Kc: MST+Kl) 92.98 92.67MST w/o the 5 function tags 86.94 87.03All are straight (monotonic) 78.92 78.67Table 1: Performance of our methods on thedev and test sets with different feature combi-nations4.2 Experimental ResultsClassification of Instances: Table 1 reports theperformance of our defined four structured fea-tures, linear feature and the composite kernel.The results are summarized as follows.The last row reports the performance withoutusing any reordering features.
We just supposethat all the translations are monotonic, no reor-dering happens.
The CAccs of 78.92% and 78.67%serve as the bottom line in our study.
Comparedwith the bottom line, the tree kernels over the 4structured features are very effective for phrase4 An adjacent word pair wiwi+1 in a translation have correctword order if and only if wi appears before wi+1 in transla-tion references.
Note than the two words may not be adja-cent in the references even if they have correct word order.reordering since only structured information isused in the tree kernel5.The CTs performs the worst among the 4structured features.
This suggests that the middleand high-level structures beyond base phrases arevery useful for phrase reordering.
The MSSsshow lower performance than the CMSSs andthe MSTs achieve the best performance.
Thisclearly indicates that the structured context in-formation is useful for phrase reordering.
For thisreason, the subsequent discussions are focusedon the MSTs, unless otherwise specified.
TheMSSs without using the 5 function tags performmuch worse than the original ones.
This suggeststhat the partitions of the structured feature spacesare very helpful, which can effectively avoid theundesired matching between partitions of differ-ent functionalities.
Comparison of Kl and Kl-LMshows the LM plays an important role in phrasereordering.
The composite kernel (Kc) performsmuch better than the two individual kernels.
Thissuggests that the structured and linear featuresare complementary and the composite kernel canwell integrate them for phrase reordering.Methods CAcc (%)Dev TestMinimum Sub-Tree (MST) 89.87 89.92Linear Features (Kl) 90.79 90.46Composite Kernel (Kc: MST+Kl) 92.98 92.67MaxEnt+boundary word (B2) 88.33 86.97MaxEnt+linguistic features (B3_1) 84.83 83.92MaxEnt+LABTG (B3: B2+ B3_1) 88.82 88.18Table 2: Performance comparison of different me-thodsTable 2 compares the performance of the base-line methods with ours.
Comparison betweenB3_1 and MST clearly demonstrates that thestructured syntactic features are much more ef-fective than the linear syntactic features that aremanually extracted via heuristics.
It also suggeststhat the tree kernel can well capture the struc-tured features implicitly.
Kl outperforms B2.
Thisis mainly due to the contribution of LM features.B2 (MaxEnt-based) significantly outperforms Kl-LM in Table 1 (SVM-based).
This suggests thatphrase reordering may not be a good linearly bi-nary-separable task if only boundary word fea-tures are used.
Our composite kernel (Kc) signifi-cantly outperforms LABTG (B3).
This mainly5 The tree kernel algorithm only compares internal struc-tures.
It does not concern any lexical leaf nodes.704attributes to the contributions of structured syn-tactic features, LM and the tree kernel.Forced Decoding: Table 3 compares the per-formance of our composite kernel with that ofthe LABTG (Baseline 3) in forced decoding.
Asdiscussed before, here we try two translationmodels.The composite kernel outperforms theLABTG in all test cases.
This further validatesthe effectiveness of the kernel methods in phrasereordering.
There are still around 30% wordsreordered incorrectly even if we use the transla-tion model trained on both training, dev and testsets.
This reveals the limitations of current SMTmodeling methods and suggests interesting fu-ture work in this area.
The source languageOOV6 rate in forced decoding (13.6%) is muchhigher that in normal decoding (6.22%, see table4).
This is mainly due to the fact that the phrasetable in forced decoding is filtered out based onboth source and target languages while in normaldecoding it is based on source language only.
Asa result, more phrases are filtered out in theforced decoding.
There is 1.4% OOV even if thetranslation model is trained on the test set.
This isdue to the incorrect word alignment, large-spanword alignment and different English tokeniza-tion strategies used in BLEU-scoring tool andours.Methods Test Set (%)RAcc OOV BLEUComposite Kernel (Kc)+translation model onTraining, dev and test51.0372.6713.61.4138.5662.87MaxEnt+LABTG (B3)+translation model ontraining, dev and test48.9671.4513.61.4137.3262.14Table 3: Performance comparison of forced de-codingMethods Test SetBLEU(%) OOV(%)Composite Kernel (Kc) 27.65 6.26Moses (B1) 25.71 6.17MaxEnt+boundary word(B2) 25.99 6.22MaxEnt+LABTG (B3) 26.63 6.22Table 4: Performance comparison6 OOV means a source words has no any English translationaccording to the translation model.
OOV rate is the percent-age of the number of OOV words over all the source words.Normal Decoding/Translation: Table 4 reportsthe translation performance of our system andthe three baseline systems.Moses (B1) and the MaxEnt-based boundaryword model (B2) achieve comparable perfor-mance.
This means the lexicalized orientation-based reordering model in Moses performs simi-larly to the boundary word-based reorderingmodel since the two models are both lexicalword-based.
However, theoretically, the Max-Ent-based model may suffer less from datasparseness issue since it does not depends oninternal phrasal words and uses MaxEnt to op-timize feature weights while the orientation-based model uses relative frequency of the entirephrases to compute the posterior probabilities.
s.The MaxEnt-based LABTG model significantlyoutperforms (p<0.05) the MaxEnt-based boun-dary word model and the lexicalized orientation-based reordering model.
This indicates that thelinearly linguistically syntactic information is auseful feature to phrase reordering.Our composite kernel-based model signifi-cantly outperforms (p<0.01) the three baselinemethods.
This again proves that the structuredsyntactic features are much more effective thanthe linear syntactic features for phrase reorderingand the tree kernel method can well capture theinformative structured features.
The four me-thods show very slight difference in OOV rates.This is mainly due to the difference in implemen-tation detail, such as different OOV penalties andother pruning thresholds.5 Conclusion and Future WorkStructured syntactic knowledge is very useful tophrase reordering.
This paper provides insightsinto how the structured feature can be used forphrase reordering.
In previous work, the struc-tured features are selected manually by heuristicsand represented by a linear feature vector.
Thismay largely compromise the contribution of thestructured features to phrase reordering.
Thanksto the nice properties of kernel-based learningmethod and SVM classifier, we propose leverag-ing on the kernelized SVM learning algorithm toaddress the problem.
Specifically, we proposeusing convolution tree kernel to capture thestructured features and design a composite kernelto combine the structured features and other li-near features for phrase reordering.
The tree ker-nel is able to directly take the structured reorder-ing instances as inputs and compute their similar-ities without enumerating them into a set of liner705features.
In addition, we also study how to findthe optimal structured feature space and how topartition the structured feature spaces accordingto their functionalities.
Finally, we evaluate ourmethod on the NIST MT-2005 Chinese-Englishtranslation tasks.
To provide insights into themodel, we design three kinds of experiments to-gether with three different evaluation metrics.Experimental results show that the structuredfeatures are very effective and our compositekernel can well capture both the structured andthe linear features without the need for extensivefeature engineering.
It also shows that our me-thod significantly outperforms the baseline me-thods.The tree kernel-based phrase reordering me-thod is not only applicable to adjacent phrases.
Itis able to work with any long phrase pairs withgap of any length in-between.
We will study thiscase in the near future.
We would also like to useone individual tree kernel for one partition in astructured feature space.
In doing so, we are ableto give different weights to different partitionsaccording to their functionalities and contribu-tions.
Note that these weights can be automati-cally tuned and optimized easily against a devset.ReferencesDavid Chiang.
2005.
A hierarchical phrase-basedmodel for SMT.
ACL-05.
263-270.Michael Collins and N. Duffy.
2001.
ConvolutionKernels for Natural Language.
NIPS-2001.M.
R. Costa-juss?
and J.A.R.
Fonollosa.
2006.
Statis-tical Machine Reordering.
EMNLP-06.
70-76.Brooke Cowan, Ivona Kucerova and Michael Collins.2006.
A discriminative model for tree-to-tree trans-lation.
EMNLP-06.
232-241.Jason Eisner.
2003.
Learning non-isomorphic treemappings for MT.
ACL-03 (companion volume).Jakob Elming.
2008.
Syntactic Reordering Integratedwith Phrase-Based SMT.
COLING-08.
209-216.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase Reorder-ing Model.
EMNLP-08.
848-856.David Haussler.
1999.
Convolution Kernels on Dis-crete Structures.
TR UCS-CRL-99-10.T.
Joachims.
1998.
Text Categorization with SVM:learning with many relevant features.
ECML-98.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
ACL-03.
423-430.Reinhard Kenser and Hermann Ney.
1995.
Improvedbacking-off for M-gram language modeling.ICASSP-95, 181-184Philipp Koehn, F. Och and D. Marcu.
2003.
Statisticalphrase-based translation.
HLT-NAACL-03.Philipp Koehn, H. Hoang, A. Birch, C. C.-Burch, M.Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-ran, R. Zens, C. Dyer, O. Bojar, A. Constantin andE.
Herbst.
2007.
Moses: Open Source Toolkit forSMT.
ACL-07 (poster).
77-180.Shankar Kumar and William Byrne.
2005.
LocalPhrase Reordering Models for Statistical MachineTranslation.
HLT-EMNLP-2005.
161-168.Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,Minghui Li and Yi Guan.
2007.
A ProbabilisticApproach to Syntax-based Reordering for Statis-tical Machine Translation.
ACL-07.
720-727.Yang Liu, Yun Huang, Qun Liu and Shouxun Lin.2007.
Forest-to-String Statistical TranslationRules.
ACL-07.
704-711.Daniel Marcu, W. Wang, A. Echihabi and K. Knight.2006.
SPMT: SMT with Syntactified Target Lan-guage Phrases.
EMNLP-06.
44-52.Haitao Mi and Liang Huang.
2008.
Forest-basedTranslation Rule Extraction.
EMNLP-08.
206-214.Alessandro Moschitti.
2004.
A Study on ConvolutionKernels for Shallow Semantic Parsing.
ACL-04.Masaaki Nagata, Kuniko Saito, Kazuhide Yamamotoand Kazuteru Ohashi.
2006.
A Clustered GlobalPhrase Reordering Model for Statistical MachineTranslation.
COLING-ACL-06.
713-720.Franz J. Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statis-tical machine translation.
ACL-02.
295-302.Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
ACL-03.
160-167.Franz J. Och and H. Ney.
2003.
A Systematic Com-parison of Various Statistical Alignment Methods.Computational Linguistics, 29(1):20-51.Franz J. Och and H. Ney.
2004.
The alignment tem-plate approach to statistical machine translation.Computational Linguistics, 30(4):417-449.Kishore Papineni, S. Roukos, T. and W. Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
ACL-02.
311-318.Hendra Setiawan, Min-Yen Kan and Haizhou Li.2007.
Ordering Phrases with Function Words.ACL-07.
712-719.Libin Shen, Jinxi Xu and Ralph Weischedel.
2008.
ANew String-to-Dependency Machine TranslationAlgorithm with a Target Dependency LanguageModel.
ACL-HLT-08.
577-585.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
ICSLP-02.
901-904.Christoph Tillmann.
2004.
A Unigram OrientationModel for Statistical Machine Translation.
HLT-NAACL-04 (short paper).Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer.706Chao Wang, M. Collins and P. Koehn.
2007.
ChineseSyntactic Reordering for Statistical MachineTranslation.
EMNLP-CONLL-07.
734-745.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpo-ra.
Computational Linguistics, 23(3):377-403.Fei Xia and Michael McCord.
2004.
Improving a Sta-tistical MT System with Automatically LearnedRewrite Patterns.
COLING-04.Deyi Xiong, Qun Liu and Shouxun Lin.
2006.
Maxi-mum Entropy Based Phrase Reordering Model forSMT.
COLING-ACL-06.
521?528.Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li.2008.
A Linguistically Annotated Reordering Mod-el for BTG-based Statistical Machine Translation.ACL-HLT-08 (short paper).
149-152.Kenji Yamada and K. Knight.
2001.
A syntax-basedstatistical translation model.
ACL-01.
523-530.Xiaofeng Yang, Jian Su and Chew Lim Tan.
2006.Kernel-Based Pronoun Resolution with StructuredSyntactic Knowledge.
COLING-ACL-06.
41-48.Richard Zens, H. Ney, T. Watanabe and E. Sumita.2004.
Reordering Constraints for Phrase-BasedStatistical Machine Translation.
COLING-04.Richard Zens and Hermann Ney.
2006.
Discrimina-tive Reordering Models for Statistical MachineTranslation.
WSMT-2006.Dell Zhang and W. Lee.
2003.
Question classificationusing support vector machines.
SIGIR-03.Min Zhang, Jie Zhang, Jian Su and GuoDong Zhou.2006.
A Composite Kernel to Extract Relations be-tween Entities with Both Flat and Structured Fea-tures.
COLING-ACL-06.
825-832.Dongdong Zhang, M. Li, C.H.
Li and M. Zhou.2007a.
Phrase Reordering Model Integrating Syn-tactic Knowledge for SMT.
EMNLP-CONLL-07.533-540.Min Zhang, W. Che, A. Aw, C. Tan, G. Zhou, T. Liuand S. Li.
2007b.
A Grammar-driven ConvolutionTree Kernel for Semantic Role Classification.ACL-07.
200-207.Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, ShengLi and Chew Lim Tan.
2007c.
A Tree-to-TreeAlignment-based Model for Statistical MachineTranslation.MT-Summit-07.
535-542Min Zhang, Hongfei Jiang, Ai Ti Aw, Haizhou Li,Chew Lim Tan and Chew Lim Tan and Sheng Li.2008a.
A Tree Sequence Alignment-based Tree-to-Tree Translation Model.
ACL-HLT-08.
559-567.Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw,Sheng Li.
2008b.
Grammar Comparison Study forTranslational Equivalence Modeling and Statistic-al Machine Translation.
COLING-08.
1097-1104Ying Zhang, Stephan Vogel and Alex Waibel.
2004.Interpreting BLEU/NIST scores: How much im-provement do we need to have a better system?LREC-04.
2051-2054.707
