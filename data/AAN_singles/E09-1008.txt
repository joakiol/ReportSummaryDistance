Proceedings of the 12th Conference of the European Chapter of the ACL, pages 60?68,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsCorrecting Automatic Translations through Collaborations between MTand Monolingual Target-Language UsersJoshua S. Albrecht and Rebecca Hwa and G. Elisabeta MaraiDepartment of Computer ScienceUniversity of Pittsburgh{jsa8,hwa,marai}@cs.pitt.eduAbstractMachine translation (MT) systems haveimproved significantly; however, their out-puts often contain too many errors to com-municate the intended meaning to theirusers.
This paper describes a collabora-tive approach for mediating between anMT system and users who do not under-stand the source language and thus cannoteasily detect translation mistakes on theirown.
Through a visualization of multi-ple linguistic resources, this approach en-ables the users to correct difficult transla-tion errors and understand translated pas-sages that were otherwise baffling.1 IntroductionRecent advances in machine translation (MT) havegiven us some very good translation systems.They can automatically translate between manylanguages for a variety of texts; and they arewidely accessible to the public via the web.
Thequality of the MT outputs, however, is not reliablyhigh.
People who do not understand the sourcelanguage may be especially baffled by the MT out-puts because they have little means to recover fromtranslation mistakes.The goal of this work is to help monolingualtarget-language users to obtain better translationsby enabling them to identify and overcome er-rors produced by the MT system.
We argue for ahuman-computer collaborative approach becauseboth the users and the MT system have gaps intheir abilities that the other could compensate.
Tofacilitate this collaboration, we propose an inter-face that mediates between the user and the MTsystem.
It manages additional NLP tools for thesource language and translation resources so thatthe user can explore this extra information to gainenough understanding of the source text to correctMT errors.
The interactions between the users andthe MT system may, in turn, offer researchers in-sights into the translation process and inspirationsfor better translation models.We have conducted an experiment in which weasked non-Chinese speakers to correct the outputsof a Chinese-English MT system for several shortpassages of different genres.
They performed thecorrection task both with the help of the visual-ization interface and without.
Our experiment ad-dresses the following questions:?
To what extent can the visual interface helpthe user to understand the source text??
In what way do factors such as the user?sbackgrounds, the properties of source text,and the quality of the MT system and otherNLP resources impact that understanding??
What resources or strategies are more help-ful to the users?
What research directionsdo these observations suggest in terms of im-proving the translation models?Through qualitative and quantitative analysis ofthe user actions and timing statistics, we havefound that users of the interface achieved a moreaccurate understanding of the source texts andcorrected more difficult translation mistakes thanthose who were given the MT outputs alone.
Fur-thermore, we observed that some users made bet-ter use of the interface for certain genres, suchas sports news, suggesting that the translationmodel may be improved by a better integration ofdocument-level contexts.602 Collaborative TranslationThe idea of leveraging human-computer collab-orations to improve MT is not new; computer-aided translation, for instance, was proposed byKay (1980).
The focus of these efforts has been onimproving the performance of professional trans-lators.
In contrast, our intended users cannot readthe source text.These users do, however, have the world knowl-edge and the language model to put together co-herent sentences in the target-language.
From theMT research perspective, this raises an interestingquestion: given that they are missing a transla-tion model, what would it take to make these usersinto effective ?decoders??
While some transla-tion mistakes are recoverable from a strong lan-guage model alone, and some might become read-ily apparent if one can choose from some possi-ble phrasal translations; the most difficult mistakesmay require greater contextual knowledge aboutthe source.
Consider the range of translation re-sources available to an MT decoder?which onesmight the users find informative, handicapped asthey are for not knowing the source language?Studying the users?
interactions with these re-sources may provide insights into how we mightbuild a better translation model and a better de-coder.In exploring the collaborative approach, the de-sign considerations for facilitating human com-puter interaction are crucial.
We chose to makeavailable relatively few resources to prevent theusers from becoming overwhelmed by the options.We also need to determine how to present the in-formation from the resources so that the users caneasily interpret them.
This is a challenge becausethe Chinese processing tools and the translationresources are imperfect themselves.
The informa-tion should be displayed in such a way that con-flicting analyses between different resources arehighlighted.3 Prototype DesignWe present an overview of our prototype for a col-laborative translation interface, named The Chi-nese Room1.
A screen-shot is shown in Figure 1.
It1The inspiration for the name of our system came fromSearle?s thought experiment(Searle, 1980).
We realize thatthere are major differences between our system and Searle?sdescription.
Importantly, our users get to insert their knowl-edge rather than purely operate based on instructions.
We feltFigure 1: A screen-shot of the visual interface.
Itconsists of two main regions.
The left pane is aworkspace for users to explore the sentence; theright pane provides multiple tabs that offer addi-tional functionalities.is a graphical environment that supports five mainsources of information and functionalities.
Thespace separates into two regions.
On the left paneis a large workspace for the user to explore thesource text one sentence at a time.
On the rightpane are tabbed panels that provide the users withaccess to a document view of the MT outputs aswell as additional functionalities for interpretingthe source.
In our prototype, the MT output is ob-tained by querying Google?s Translation API2.
Inthe interest of exploiting user interactions as a di-agnostic tool for improving MT, we chose infor-mation sources that are commonly used by mod-ern MT systems.First, we display the word alignments betweenMT output and segmented Chinese3.
Even with-out knowing the Chinese characters, the userscan visually detect potential misalignments andpoor word reordering.
For instance, the automatictranslation shown in Figure 1 begins: Two yearsago this month...
It is fluent but incorrect.
Thecrossed alignments offer users a clue that ?two?and ?months?
should not have been split up.
Userscan also explore alternative orderings by draggingthe English tokens around.Second, we make available the glosses forwords and characters from a bilingual dictionary4.the name was nonetheless evocative in that the user requiresadditional resources to process the input ?squiggles.
?2http://code.google.com/apis/translate/research3The Chinese segmentation is obtained as a by-product ofGoogle?s translation process.4We used the Chinese-English Translation Lexi-61The placement of the word gloss presents a chal-lenge because there are often alternative Chi-nese segmentations.
We place glosses for multi-character words in the column closer to the source.When the user mouses over each definition, thecorresponding characters are highlighted, helpingthe user to notice potential mis-segmentation inthe Chinese.Third, the Chinese sentence is annotated withits parse structure5.
Constituents are displayedas brackets around the source sentence.
Theyhave been color-coded into four major types (nounphrase, verb phrases, prepositional phrases, andother).
Users can collapse and expand the brack-ets to keep the workspace uncluttered as they workthrough the Chinese sentence.
This also indicatesto us which fragments held the user?s focus.Fourth, based on previous studies reportingthat automatic translations may improve whengiven decomposed source inputs (Mellebeek et al,2005), we allow the users to select a substringfrom the source text for the MT system to trans-late.
We display the N -best alternatives in theTranslation Tab.
The list is kept short; its purposeis less for reranking but more to give the users asense of the kinds of hypotheses that the MT sys-tem is considering.Fifth, users can select a substring from thesource text and search for source sentences froma bilingual corpus and a monolingual corpus thatcontain phrases similar to the query6.
The re-trieved sentences are displayed in the ExampleTab.
For sentences from the bilingual corpus, hu-man translations for the queried phrase are high-lighted.
For sentences retrieved from the monolin-gual corpus, their automatic translations are pro-vided.
If the users wished to examine any of theretrieved translation pairs in detail, they can pushit onto the sentence workspace.4 Experimental MethodologyWe asked eight non-Chinese speakers to correctthe machine translations of four short Chinese pas-con released by the LDC; for a handful of char-acters that serve as function words, we added thefunctional definitions using an online dictionaryhttp://www.mandarintools.com/worddict.html.5It is automatically generated by the Stanford Parser forChinese (Klein and Manning, 2003).6We used Lemur (2006) for the information retrievalback-end; the parallel corpus is from the Federal BroadcastInformation Service corpus; the monolingual corpus is fromthe Chinese Gigaword corpus.Figure 2: The interface for users who are correct-ing translations without help; they have access tothe document view, but they do not have access toany of the other resources.sages, with an average length of 11.5 sentences.Two passages are news articles and two are ex-cerpts of a fictional work.
Each participant wasinstructed to correct the translations for one newsarticle and one fictional passage using all the re-sources made available by The Chinese Room andthe other two passages without.
To keep the ex-perimental conditions as similar as possible, weprovided them with a restricted version of the in-terface (see Figure 2 for a screen-shot) in which alladditional functionalities except for the DocumentView Tab are disabled.
We assigned each personto alternate between working with the full and therestricted versions of the system; half began with-out, and the others began with.
Thus, every pas-sage received four sets of corrections made collab-oratively with the system and four sets of correc-tions made based solely on the participants?
inter-nal language models.
All together, there are 184participant corrected sentences (11.5 sentences ?4 passages ?
4 participants) for each condition.The participants were asked to complete eachpassage in one sitting.
Within a passage, theycould work on the sentences in any arbitrary order.They could also elect to ?pass?
any part of a sen-tence if they found it too difficult to correct.
Tim-ing statistics were automatically collected whilethey made their corrections.
We interviewed eachparticipant for qualitative feedbacks after all fourpassages were corrected.Next, we asked two bilingual speakers to eval-uate all the corrected translations.
The outcomesbetween different groups of users are compared,62and the significance of the difference is deter-mined using the two-sample t-test assuming un-equal variances.
We require 90% confidence (al-pha=0.1) as the cut-off for a difference to be con-sidered statistically significant; when the differ-ence can be established with higher confidence,we report that value.
In the following subsections,we describe the conditions of this study in moredetails.Participants?
Background For this study, westrove to maintain a relatively heterogeneous pop-ulation; participants were selected to be varied intheir exposures to NLP, experiences with foreignlanguages, as well as their age and gender.
A sum-mary of their backgrounds is shown in Table 1.Prior to the start of the study, the participantsreceived a 20 minute long presentational tutorialabout the basic functionalities supported by oursystem, but they did not have an opportunity to ex-plore the system on their own.
This helps us to de-termine whether our interface is intuitive enoughfor new users to pick up quickly.Data The four passages used for this study werechosen to span a range of difficulties and genretypes.
The easiest of the four is a news arti-cle about a new Tamagotchi-like product fromBandai.
It was taken from a webpage that offersbilingual news to help Chinese students to learnEnglish.
A harder news article is taken from apast NIST Chinese-English MT Evaluation; it isabout Michael Jordan?s knee injury.
For a dif-ferent genre, we considered two fictional excerptsfrom the first chapter of Martin Eden, a novel byJack London that has been professionally trans-lated into Chinese7.
One excerpt featured a shortdialog, while the other one was purely descriptive.Evaluation of Translations Bilingual humanjudges are presented with the source text as well asthe parallel English text for reference.
Each judgeis then shown a set of candidate translations (theoriginal MT output, an alternative translation bya bilingual speaker, and corrected translations bythe participants) in a randomized order.
Since thehuman corrected translations are likely to be flu-ent, we have instructed the judges to concentratemore on the adequacy of the meaning conveyed.They are asked to rate each sentence on an abso-7We chose an American story so as to not rely on auser?s knowledge about Chinese culture.
The participantsconfirmed that they were not familiar with the chosen story.Table 2: The guideline used by bilingual judgesfor evaluating the translation quality of the MToutputs and the participants?
corrections.9-10 The meaning of the Chinese sentenceis fully conveyed in the translation.7-8 Most of the meaning is conveyed.5-6 Misunderstands the sentence in amajor way; or has many small mistakes.3-4 Very little meaning is conveyed.1-2 The translation makes no sense at all.lute scale of 1-10 using the guideline in Table 2.To reduce the biases in the rating scales of differ-ent judges, we normalized the judges?
scores, fol-lowing standard practices in MT evaluation (Blatzet al, 2003).
Post normalization, the correlationcoefficient between the judges is 0.64.
The finalassessment score for each translated sentence isthe average of judges?
scores, on a scale of 0-1.5 ResultsThe results of human evaluations for the user ex-periment are summarized in Table 3, and the corre-sponding timing statistics (average minutes spentediting a sentence) is shown in Table 4.
We ob-served that typical MT outputs contain a range oferrors.
Some are primarily problems in fluencysuch that the participants who used the restrictedinterface, which provided no additional resourcesother than the Document View Tab, were still ableto improve the MT quality from 0.35 to 0.42.
Onthe other hand, there are also a number of moreserious errors that require the participants to gainsome level of understanding of the source in orderto correct them.
The participants who had accessto the full collaborative interface were able to im-prove the quality from 0.35 to 0.53, closing thegap between the MT and the bilingual translationsby 36.9%.
These differences are all statisticallysignificant (with >98% confidence).The higher quality of corrections does requirethe participants to put in more time.
Overall, theparticipants took 2.5 times as long when they havethe interface than when they do not.
This may bepartly because the participants have more sourcesof information to explore and partly because theparticipants tended to ?pass?
on fewer sentences.The average Levenshtein edit distance (with wordsas the atomic unit, and with the score normalizedto the interval [0,1]) between the original MT out-63Table 1: A summary of participants?
background.
?User5 recognizes some simple Kanji characters, butdoes not have enough knowledge to gain any additional information beyond what the MT system and thedictionary already provided.User1 User2 User3 User4 User5?
User6 User7 User8NLP background intro grad none none intro grad intro noneNative English yes no yes yes yes yes yes yesOther Languages French multiple none none Japanese none none Greek(beginner) (fluent) (beginner) (beginner)Gender M F F M M M F MEducation Ugrad PhD PhD Ugrad Ugrad PhD Ugrad Ugradputs and the corrected sentences made by partic-ipants using The Chinese Room is 0.59; in con-trast, the edit distance is shorter, at 0.40, when par-ticipants correct MT outputs directly.
The timingstatistics are informative, but they reflect the inter-actions of many factors (e.g., the difficulty of thesource text, the quality of the machine translation,the background and motivation of the user).
Thus,in the next few subsections, we examine how thesefactors correlate with the quality of the participantcorrections.5.1 Impact of Document VariationSince the quality of MT varies depending on thedifficulty and genre of the source text, we inves-tigate how these factors impact our participants?performances.
Columns 3-6 of Table 3 (and Ta-ble 4) compare the corrected translations on a per-document basis.Of the four documents, the baseline MT sys-tem performed the best on the product announce-ment.
Because the article is straight-forward, par-ticipants found it relatively easy to guess the in-tended translation.
The major obstacle is in de-tecting and translating Chinese transliteration ofJapanese names, which stumped everyone.
Thequality difference between the two groups of par-ticipants on this document was not statistically sig-nificant.
Relatedly, the difference in the amount oftime spent is the smallest for this document; par-ticipants using The Chinese Room took about 1.5times longer.The other news article was much more difficult.The baseline MT made many mistakes, and bothgroups of participants spent longer on sentencesfrom this article than the others.
Although sportsnews is fairly formulaic, participants who onlyread MT outputs were baffled, whereas those whohad access to additional resources were able to re-cover from MT errors and produced good qualitytranslations.Finally, as expected, the two fictional excerptswere the most challenging.
Since the participantswere not given any information about the story,they also have little context to go on.
In both cases,participants who collaborated with The ChineseRoom made higher quality corrections than thosewho did not.
The difference is statistically signif-icant at 97% confidence for the first excerpt, and93% confidence for the second.
The differences intime spent between the two groups are greater forthese passages because the participants who hadto make corrections without help tended to giveup more often.5.2 Impact of Participants?
BackgroundWe further analyze the results by separating theparticipants into two groups according to fourfactors: whether they were familiar with NLP,whether they studied another language, their gen-der, and their education level.Exposure to NLP One of our design objectivesfor The Chinese Room is accessibility by a diversepopulation of end-users, many of whom may notbe familiar with human language technologies.
Todetermine how prior knowledge of NLP may im-pact a user?s experience, we analyze the exper-imental results with respect to the participants?background.
In columns 2 and 3 of Table 5, wecompare the quality of the corrections made bythe two groups.
When making corrections on theirown, participants who had been exposed to NLPheld a significant edge (0.35 vs. 0.47).
When bothgroups of participants used The Chinese Room, thedifference is reduced (0.51 vs. 0.54) and is not sta-tistically significant.
Because all the participantswere given the same short tutorial prior to the startof the study, we are optimistic that the interface isintuitive for many users.None of the other factors distinguished one64Table 3: Averaged human judgments of the translation quality of the four different approaches: automaticMT, corrections by participants without help, corrections by participants using The Chinese Room, andtranslation produced by a bilingual speaker.
The second column reports score for all documents; columns3-6 show the per-document scores.Overall News (product) News (sports) Story1 Story2Machine translation 0.35 0.45 0.30 0.25 0.26Corrections without The Chinese Room 0.42 0.56 0.35 0.33 0.41Corrections with The Chinese Room 0.53 0.55 0.62 0.42 0.49Bilingual translation 0.83 0.83 0.73 0.92 0.88Table 4: The average amount of time (minutes) participants spent on correcting a sentence.Overall News (product) News (sports) Story1 Story2Corrections without The Chinese Room 2.5 1.9 3.2 2.9 2.3Corrections with The Chinese Room 6.3 2.9 8.7 6.5 8.5Table 6: The quality of the corrections producedby four participants using The Chinese Room forthe sports news article.User1 0.57User2 0.46User5 0.70User6 0.73bilingual translator 0.73group of participants from the others.
The resultsare summarized in columns 4-9 of Table 5.
In eachcase, the two groups had similar levels of perfor-mance, and the differences between their correc-tions were not statistically significant.
This trendholds for both when they were collaborating withthe system and when editing on their own.Prior Knowledge Another factor that may im-pact the success of the outcome is the user?sknowledge about the domain of the source text.An example from our study is the sports news ar-ticle.
Table 6 lists the scores that the four partic-ipants who used The Chinese Room received fortheir corrected translations for that passage (aver-aged over sentences).
User5 and User6 were morefamiliar with the basketball domain; with the helpof the system, they produced translations that werecomparable to those from the bilingual translator(the differences are not statistically significant).5.3 Impact of Available ResourcesPost-experiment, we asked the participants to de-scribe the strategies they developed for collaborat-ing with the system.
Their responses fall into threemain categories:Figure 3: This graph shows the average counts ofaccess per sentence for different resources.Divide and Conquer Some users found the syn-tactic trees helpful in identifying phrasal units forN -best re-translations or example searches.
Forlonger sentences, they used the constituent col-lapse feature to help them reduce clutter and focuson a portion of the sentence.Example Retrieval Using the search interface,users examined the highlighted query terms to de-termine whether the MT system made any seg-mentation errors.
Sometimes, they used the exam-ples to arbitrate whether they should trust any ofthe dictionary glosses or the MT?s lexical choices.Typically, though, they did not attempt to inspectthe example translations in detail.Document Coherence and Word GlossesUsers often referred to the document view todetermine the context for the sentence they areediting.
Together with the word glosses and other65Table 5: A comparison of translation quality, grouped by four characteristics of participant backgrounds:their level of exposure to NLP, exposure to another language, their gender, and education level.No NLP NLP No 2nd Lang.
2nd Lang.
Female Male Ugrad PhDwithout The Chinese Room 0.35 0.47 0.41 0.43 0.41 0.43 0.41 0.45with The Chinese Room 0.51 0.54 0.56 0.51 0.50 0.55 0.52 0.54resources, the discourse level clues helped toguide users to make better lexical choices thanwhen they made corrections without the fullsystem, relying on sentence coherence alone.Figure 3 compares the average access counts(per sentence) of different resources (aggregatedover all participants and documents).
The optionof inspect retrieved examples in detail (i.e., bringthem up on the sentence workspace) was rarelyused.
The inspiration for this feature was fromwork on translation memory (Macklovitch et al,2000); however, it was not as informative for ourparticipants because they experienced a greater de-gree of uncertainty than professional translators.6 DiscussionThe results suggest that collaborative translationis a promising approach.
Participant experienceswere generally positive.
Because they felt likethey understood the translations better, they didnot mind putting in the time to collaborate withthe system.
Table 7 shows some of the partici-pants?
outputs.
Although there are some transla-tion errors that cannot be overcome with our cur-rent system (e.g., transliterated names), the partic-ipants taken as a collective performed surprisinglywell.
For many mistakes, even when the users can-not correct them, they recognized a problem; andoften, one or two managed to intuit the intendedmeaning with the help of the available resources.As an upper-bound for the effectiveness of the sys-tem, we construct a combined ?oracle?
user out ofall 4 users that used the interface for each sentence.The oracle user?s average score is 0.70; in contrast,an oracle of users who did not use the system is0.54 (cf.
the MT?s overall of 0.35 and the bilin-gual translator?s overall of 0.83).
This suggestsThe Chinese Room affords a potential for human-human collaboration as well.The experiment also made clear some limita-tions of the current resources.
One is domain de-pendency.
Because NLP technologies are typi-cally trained on news corpora, their bias towardthe news domain may mislead our users.
For ex-ample, there is a Chinese character (pronouncedmei3) that could mean either ?beautiful?
or ?theUnited States.?
In one of the passages, the in-tended translation should have been: He was re-sponsive to beauty... but the corresponding MToutput was He was sensitive to the United States...Although many participants suspected that it waswrong, they were unable to recover from this mis-take because the resources (the searchable exam-ples, the part-of-speech tags, and the MT system)did not offer a viable alternative.
This suggeststhat collaborative translation may serve as a usefuldiagnostic tool to help MT researchers verify ideasabout what types of models and data are useful intranslation.
It may also provide a means of datacollection for MT training.
To be sure, there areimportant challenges to be addressed, such as par-ticipation incentive and quality assurance, but sim-ilar types of collaborative efforts have been shownfruitful in other domains (Cosley et al, 2007).
Fi-nally, the statistics of user actions may be usefulfor translation evaluation.
They may be informa-tive features for developing automatic metrics forsentence-level evaluations (Kulesza and Shieber,2004).7 Related WorkWhile there have been many successful computer-aided translation systems both for research and ascommercial products (Bowker, 2002; Langlais etal., 2000), collaborative translation has not beenas widely explored.
Previous efforts such asDerivTool (DeNeefe et al, 2005) and Linear B(Callison-Burch, 2005) placed stronger emphasison improving MT.
They elicited more in-depth in-teractions between the users and the MT system?sphrase tables.
These approaches may be more ap-propriate for users who are MT researchers them-selves.
In contrast, our approach focuses on pro-viding intuitive visualization of a variety of in-formation sources for users who may not be MT-savvy.
By tracking the types of information theyconsulted, the portions of translations they se-lected to modify, and the portions of the source66Table 7: Some examples of translations corrected by the participants and their scores.Score TranslationMT 0.34 He is being discovered almost hit an arm in the pile of books on the desktop, justlike frightened horse as a Lieju Wangbangbian almost Pengfan the piano stool.without The Chinese Room 0.26 Startled, he almost knocked over a pile of book on his desk, just like a frightenedhorse as a Lieju Wangbangbian almost Pengfan the piano stool.with The Chinese Room 0.78 He was nervous, and when one of his arms nearly hit a stack of books on thedesktop, he startled like a horse, falling back and almost knocking over the pianostool.Bilingual Translator 0.93 Feeling nervous, he discovered that one of his arms almost hit the pile of bookson the table.
Like a frightened horse, he stumbled aside, almost turning over apiano stool.MT 0.50 Bandai Group, a spokeswoman for the U.S. to be SIN-West said: ?We want tobring women of all ages that ?the flavor of life?.
?without The Chinese Room 0.67 SIN-West, a spokeswoman for the U.S. Bandai Group declared: ?We want tobring to women of all ages that ?flavor of life?.
?with The Chinese Room 0.68 West, a spokeswoman for the U.S. Toy Manufacturing Group, and soon to beVice President-said: ?We want to bring women of all ages that ?flavor of life?.
?Bilingual Translator 0.75 ?We wanted to let women of all ages taste the ?flavor of life?,?
said Bandai?sspokeswoman Kasumi Nakanishi.text they attempted to understand, we may alterthe design of our translation model.
Our objectiveis also related to that of cross-language informa-tion retrieval (Resnik et al, 2001).
This work canbe seen as providing the next step in helping usersto gain some understanding of the information inthe documents once they are retrieved.By facilitating better collaborations betweenMT and target-language readers, we can naturallyincrease human annotated data for exploring al-ternative MT models.
This form of symbiosis isakin to the paradigm proposed by von Ahn andDabbish (2004).
They designed interactive gamesin which the player generated data could be usedto improve image tagging and other classificationtasks (von Ahn, 2006).
While our interface doesnot have the entertainment value of a game, itsapplication serves a purpose.
Because users aremotivated to understand the documents, they maywillingly spend time to collaborate and make de-tailed corrections to MT outputs.8 ConclusionWe have presented a collaborative approach formediating between an MT system and monolin-gual target-language users.
The approach encour-ages users to combine evidences from comple-mentary information sources to infer alternativehypotheses based on their world knowledge.
Ex-perimental evidences suggest that the collabora-tive effort results in better translations than ei-ther the original MT or uninformed human ed-its.
Moreover, users who are knowledgeable in thedocument domain were enabled to correct transla-tions with a quality approaching that of a bilin-gual speaker.
From the participants?
feedbacks,we learned that the factors that contributed to theirunderstanding include: document coherence, syn-tactic constraints, and re-translation at the phrasallevel.
We believe that the collaborative translationapproach can provide insights about the transla-tion process and help to gather training examplesfor future MT development.AcknowledgmentsThis work has been supported by NSF Grants IIS-0710695 and IIS-0745914.
We would like to thankJarrett Billingsley, Ric Crabbe, Joanna Drum-mund, Nick Farnan, Matt Kaniaris Brian Mad-den, Karen Thickman, Julia Hockenmaier, PaulineHwa, and Dorothea Wei for their help with the ex-periment.
We are also grateful to Chris Callison-Burch for discussions about collaborative trans-lations and to Adam Lopez and the anonymousreviewers for their comments and suggestions onthis paper.67ReferencesJohn Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2003.
Confidence es-timation for machine translation.
Technical ReportNatural Language Engineering Workshop Final Re-port, Johns Hopkins University.Lynne Bowker.
2002.
Computer-Aided TranslationTechnology.
University of Ottawa Press, Ottawa,Canada.Chris Callison-Burch.
2005.
Linear B System descrip-tion for the 2005 NIST MT Evaluation.
In The Pro-ceedings of Machine Translation Evaluation Work-shop.Dan Cosley, Dan Frankowski, Loren Terveen, and JohnRiedl.
2007.
Suggestbot: using intelligent task rout-ing to help people find work in wikipedia.
In IUI?07: Proceedings of the 12th international confer-ence on Intelligent user interfaces, pages 32?41.Steve DeNeefe, Kevin Knight, and Hayward H. Chan.2005.
Interactively exploring a machine transla-tion model.
In Proceedings of the ACL InteractivePoster and Demonstration Sessions, pages 97?100,Ann Arbor, Michigan, June.Martin Kay.
1980.
The proper place of men andmachines in language translation.
Technical Re-port CSL-80-11, Xerox.
Later reprinted in MachineTranslation, vol.
12 no.
(1-2), 1997.Dan Klein and Christopher D. Manning.
2003.
Fastexact inference with a factored model for naturallanguage parsing.
Advances in Neural InformationProcessing Systems, 15.Alex Kulesza and Stuart M. Shieber.
2004.
A learn-ing approach to improving sentence-level MT evalu-ation.
In Proceedings of the 10th International Con-ference on Theoretical and Methodological Issues inMachine Translation (TMI), Baltimore, MD, Octo-ber.Philippe Langlais, George Foster, and Guy Lapalme.2000.
Transtype: a computer-aided translation typ-ing system.
In Workshop on Embedded MachineTranslation Systems, pages 46?51, May.Lemur.
2006.
Lemur toolkit for language modelingand information retrieval.
The Lemur Project is acollaborative project between CMU and UMASS.Elliott Macklovitch, Michel Simard, and PhilippeLanglais.
2000.
Transsearch: A free translationmemory on the world wide web.
In Proceedings ofthe Second International Conference on LanguageResources & Evaluation (LREC).Bart Mellebeek, Anna Khasin, Josef van Genabith, andAndy Way.
2005.
Transbooster: Boosting the per-formance of wide-coverage machine translation sys-tems.
In Proceedings of the 10th Annual Conferenceof the European Association for Machine Transla-tion (EAMT), pages 189?197.Philip S. Resnik, Douglas W. Oard, and Gina-AnneLevow.
2001.
Improved cross-language retrieval us-ing backoff translation.
In Human Language Tech-nology Conference (HLT-2001), San Diego, CA,March.John R. Searle.
1980.
Minds, brains, and programs.Behavioral and Brain Sciences, 3:417?457.Luis von Ahn and Laura Dabbish.
2004.
Labeling im-ages with a computer game.
In CHI ?04: Proceed-ings of the SIGCHI conference on Human factors incomputing systems, pages 319?326, New York, NY,USA.
ACM.Luis von Ahn.
2006.
Games with a purpose.
Com-puter, 39(6):92?94.68
