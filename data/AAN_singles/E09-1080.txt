Proceedings of the 12th Conference of the European Chapter of the ACL, pages 701?709,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsUnsupervised Methods for Head AssignmentsFederico Sangati, Willem ZuidemaInstitute for Logic, Language and ComputationUniversity of Amsterdam, the Netherlands{f.sangati,zuidema}@uva.nlAbstractWe present several algorithms for assign-ing heads in phrase structure trees, basedon different linguistic intuitions on the roleof heads in natural language syntax.
Start-ing point of our approach is the obser-vation that a head-annotated treebank de-fines a unique lexicalized tree substitutiongrammar.
This allows us to go back andforth between the two representations, anddefine objective functions for the unsu-pervised learning of head assignments interms of features of the implicit lexical-ized tree grammars.
We evaluate algo-rithms based on the match with gold stan-dard head-annotations, and the compar-ative parsing accuracy of the lexicalizedgrammars they give rise to.
On the firsttask, we approach the accuracy of hand-designed heuristics for English and inter-annotation-standard agreement for Ger-man.
On the second task, the implied lex-icalized grammars score 4% points higheron parsing accuracy than lexicalized gram-mars derived by commonly used heuris-tics.1 IntroductionThe head of a phrasal constituent is a centralconcept in most current grammatical theories andmany syntax-based NLP techniques.
The term isused to mark, for any nonterminal node in a syn-tactic tree, the specific daughter node that fulfillsa special role; however, theories and applicationsdiffer widely in what that special role is supposedto be.
In descriptive grammatical theories, therole of the head can range from the determinant ofagreement or the locus of inflections, to the gover-nor that selects the morphological form of its sis-ter nodes or the constituent that is distributionallyequivalent to its parent (Corbett et al, 2006).In computational linguistics, heads mainlyserve to select the lexical content on which theprobability of a production should depend (Char-niak, 1997; Collins, 1999).
With the increasedpopularity of dependency parsing, head annota-tions have also become a crucial level of syntac-tic information for transforming constituency tree-banks to dependency structures (Nivre et al, 2007)or richer syntactic representations (e.g., Hocken-maier and Steedman, 2007).For the WSJ-section of the Penn Treebank, a setof heuristic rules for assigning heads has emergedfrom the work of (Magerman, 1995) and (Collins,1999) that has been employed in a wide variety ofstudies and proven extremely useful, even in ratherdifferent applications from what the rules wereoriginally intended for.
However, the rules arespecific to English and the treebank?s syntactic an-notation, and do not offer much insights into howheadedness can be learned in principle or in prac-tice.
Moreover, the rules are heuristic and mightstill leave room for improvement with respect torecovering linguistic head assignment even on thePenn WSJ corpus; in fact, we find that the head-assignments according to the Magerman-Collinsrules correspond only in 85% of the cases to de-pendencies such as annotated in PARC 700 De-pendency Bank (see section 5).Automatic methods for identifying heads aretherefore of interest, both for practical and morefundamental linguistic reasons.
In this paper weinvestigate possible ways of finding heads basedon lexicalized tree structures that can be extractedfrom an available treebank.
The starting pointof our approach is the observation that a head-annotated treebank (obeying the constraint that ev-ery nonterminal node has exactly one daughtermarked as head) defines a unique lexicalized treesubstitution grammar (obeying the constraint thatevery elementary tree has exactly one lexical an-chor).
This allows us to go back and forth between701the two representations, and define objective func-tions for the unsupervised learning of head assign-ments in terms of features of the implicit Lexical-ized Tree Substitution Grammars.Using this grammar formalism (LTSGs) we willinvestigate which objective functions we shouldoptimize for recovering heads.
Should we try toreduce uncertainty about the grammatical framesthat can be associated with a particular lexicalitem?
Or should we assume that linguistic headassignments are based on the occurrence frequen-cies of the productive units they imply?We present two new algorithms for unsuper-vised recovering of heads ?
entropy minimizationand a greedy technique we call ?familiarity max-imization?
?
that can be seen as ways to opera-tionalize these last two linguistic intuitions.
Bothalgorithms are unsupervised, in the sense that theyare trained on data without head annotations, butboth take labeled phrase-structure trees as input.Our work fits well with several recent ap-proaches aimed at completely unsupervised learn-ing of the key aspects of syntactic structure: lex-ical categories (Schu?tze, 1993), phrase-structure(Klein and Manning, 2002; Seginer, 2007),phrasal categories (Borensztajn and Zuidema,2007; Reichart and Rappoport, 2008) and depen-dencies (Klein and Manning, 2004).For the specific task addressed in this paper ?assigning heads in treebanks ?
we only know ofone earlier paper: Chiang and Bikel (2002).
Theseauthors investigated a technique for identifyingheads in constituency trees based on maximiz-ing likelihood, using EM, under a Tree InsertionGrammar (TIG)model1.
In this approach, headed-ness in some sense becomes a state-split, allowingfor grammars that more closely match empiricaldistributions over trees.
The authors report some-what disappointing results, however: the automat-ically induced head-annotations do not lead to sig-nificantly more accurate parsers than simple left-most or rightmost head assignment schemes2.In section 2 we define the grammar model wewill use.
In section 3 we describe the head-assignment algorithms.
In section 4, 5 and 6 we1The space over the possible head assignments that theseauthors consider ?
essentially regular expressions over CFGrules ?
is more restricted than in the current work where weconsider a larger ?domain of locality?.2However, the authors?
approach of using EM for induc-ing latent information in treebanks has led to extremely ac-curate constituency parsers, that neither make use of nor pro-duce headedness information; see (Petrov et al, 2006)then describe our evaluations of these algorithms.2 Lexicalized Tree GrammarsIn this section we define Lexicalised Tree Substi-tution Grammars (LTSGs) and show how they canbe read off unambiguously from a head-annotatedtreebank.
LTSGs are best defined as a restrictionof the more general Probabilistic Tree SubstitutionGrammars, which we describe first.2.1 Tree Substitution GrammarsA tree substitution grammar (TSG) is a 4-tuple?Vn, Vt, S, T ?
where Vn is the set of nonterminals;Vt is the set of of terminals; S ?
Vn is the startsymbol; and T is the set of elementary trees, hav-ing root and internal nodes in Vn and leaf nodes inVn?Vt.
Two elementary trees ?
and ?
can be com-bined by means of the substitution operation ?
?
?to produce a new tree, only if the root of ?
has thesame label of the leftmost nonterminal leaf of ?.The combined tree corresponds to ?
with the left-most nonterminal leaf replaced with ?.
When thetree resulting from a series of substitution opera-tions is a complete parse tree, i.e.
the root is thestart symbol and all leaf nodes are terminals, wedefine the sequence of the elementary trees usedas a complete derivation.A probabilistic TSG defines a probabilisticspace over the set of elementary trees: for every?
?
T , P (?)
?
[0, 1] and??
?:r(?
?)=r(?)
P (??)
=1, where r(?)
returns the root node of ?
.
Assum-ing subsequent substitutions are stochastically in-dependent, we define the probability of a deriva-tion as the product of the probability of its elemen-tary trees.
If a derivation d consists of n elemen-tary trees ?1 ?
?2 ?
.
.
.
?
?n, we have:P (d) =n?i=1P (?i) (1)Depending on the set T of elementary trees, wemight have different derivations producing thesame parse tree.
For any given parse tree t, wedefine ?
(t) as the set of its derivations licensed bythe grammar.
Since any derivation d ?
?
(t) is apossible way to construct the parse tree, we willcompute the probability of a parse tree as the sumof the probabilities of its derivations:P (t) =?d??(t)??
?dP (?)
(2)702Lexicalized Tree Substitution Grammars are de-fined as TSGs with the following contraint on theset of elementary trees T : every ?
in T must haveat least one terminal (the lexical anchor) amongits leaf nodes.
In this paper, we are only con-cerned with single-anchored LTSGs, in which allelementary trees have exactly one lexical anchor.Like TSGs, LTSGs have a weak generative ca-pacity that is context-free; but whereas PTSGs areboth probabilistically and in terms of strong gen-erative capacity richer than PCFGs (Bod, 1998),LTSG are more restricted (Joshi and Schabes,1991).
This limits the usefulness of LTSGs formodeling the full complexity of natural languagesyntax; however, computationally, LTSGs havemany advantages over richer formalisms and forthe current purposes represent a useful compro-mise between linguistic adequacy and computa-tional complexity.2.2 Extracting LTSGs from a head-annotatedcorpusIn this section we will describe a method for as-signing to each word token that occurs in the cor-pus a unique elementary tree.
This method de-pends on the annotation of heads in the treebank,such as for instance provided for the Penn Tree-bank by the Magerman-Collins head-percolationrules.
We adopt the same constraint as used in thisscheme, that each nonterminal node in every parsetree must have exactly one of its children anno-tated as head.
Our method is similar to (Chiang,2000), but is even simpler in ignoring the distinc-tion between arguments and adjuncts (and thus thesister-adjunction operation).
Figure 1 shows anexample parse tree enriched with head-annotation:the suffix -H indicates that the specific node is thehead of the production above it.SNPNNPMs.NNP-HHaagVP-HV-HplaysNPNNP-HEliantiFigure 1: Parse tree of the sentence ?Ms.
Haagplays Elianti?
annotated with head markers.Once a parse tree is annotated with head mark-ers in such a manner, we will be able to extractfor every leaf its spine.
Starting from each lexicalproduction we need to move upwards towards theroot on a path of head-marked nodes until we findthe first internal node which is not marked as heador until we reach the root of the tree.
In the ex-ample above, the verb of the sentence ?plays?
isconnected through head-marked nodes to the rootof the tree.
In this way we can extract the 4 spinesfrom the parse tree in figure 1, as shown in fig-ure 2.NNPMs.NPNNP-HHaagS-HVP-HV-HplaysNPNNP-HEliantiFigure 2: The lexical spines of the tree in fig.
1.It is easy to show that this procedure yields aunique spine for each of its leaves, when appliedto a parse tree where all nonterminals have a singlehead-daughter and all terminals are generated by aunary production.
Having identified the spines, weconvert them to elementary trees, by completingevery internal node with the other daughter nodesnot on the spine.
In this way we have defined away to obtain a derivation of any parse tree com-posed of lexical elementary trees.
The 4 elemen-tary trees completed from the previous paths are infigure 3 with the substitution sites marked with ?.NNPMs.NPNNP?
NNP-HHaagS-HNP?
VP-HV-HplaysNP?NPNNP-HEliantiFigure 3: The extracted elementary trees.3 Head Assignment AlgorithmsWe investigate two novel approaches to automat-ically assign head dependencies to a training cor-pus where the heads are not annotated: entropyminimization and familiarity maximization.
Thebaselines for our experiments will be given by theMagerman and Collins scheme together with therandom, the leftmost daughter, and the rightmostdaughter-based assignments.7033.1 BaselinesThe Magerman-Collins scheme, and very similarversions, are well-known and described in detailelsewhere (Magerman, 1995; Collins, 1999; Ya-mada and Matsumoto, 2003); here we just men-tion that it is based on a number of heuristic rulesthat only use the labels of nonterminal nodes andthe ordering of daughter nodes.
For instance if theroot label of a parse tree is S, the head-percolationscheme will choose to assign the head marker tothe first daughter from the left, labeled with TO.If no such label is present, it will look for the firstIN.
If no IN is found, it will look for the first VP,and so on.
We used the freely available software?Treep?
(Chiang and Bikel, 2002) to annotate thePenn WSJ treebank with heads.We consider three other baselines, that are ap-plicable to other treebanks and other languages aswell: RANDOM, where, for every node in the tree-bank, we choose a random daughter to be markedas head; LEFT, where the leftmost daughter ismarked; and RIGHT, where the rightmost daughteris marked.3.2 Minimizing EntropyIn this section we will describe an entropy basedalgorithm, which aims at learning the simplestgrammar fitting the data.
Specifically, we take a?supertagging?
perspective (Bangalore and Joshi,1999) and aim at reducing the uncertainty aboutwhich elementary tree (supertag) to assign to agiven lexical item.
We achieve this by minimizingan objective function based on the general defini-tion of entropy in information theory.The entropy measure that we are going to de-scribe is calculated from the bag of lexicalized el-ementary trees T extracted from a given trainingcorpus of head annotated parse trees.
We defineTl as a discrete stochastic variable, taking as val-ues the elements from the set of all the elementarytrees having l as lexical anchor {?l1 , ?l2 , .
.
.
, ?ln}.Tl thus takes n possible values with specific prob-abilities; its entropy is then defined as:H(Tl) = ?n?i=1p(?li) log2 p(?li) (3)The most intuitive way to assign probabilities toeach elementary tree is considering its relative fre-quency in T .
If f(?)
is the frequency of the frag-ment ?
and f(l) is the total frequency of fragmentswith l as anchor we will have:p(?lj ) =f(?lj )f(lex(?lj ))=f(?lj )n?i=1f(?li))(4)We will then calculate the entropy H(T ) of ourbag of elementary trees by summing the entropy ofeach single discrete stochastic variable Tl for eachchoice of l:H(T ) =|L |?l=1H(Tl) (5)In order to minimize the entropy, we apply ahill-climbing strategy.
The algorithm starts froman already annotated tree-bank (for instance usingthe RANDOM annotator) and iteratively tries outa random change in the annotation of each parsetree.
Only if the change reduces the entropy of theentire grammar it is kept.
These steps are repeateduntil no further modification which could reducethe entropy is possible.
Since the entropy measureis defined as the sum of the function p(?)
log2 p(?
)of each fragment ?
, we do not need to re-calculatethe entropy of the entire grammar, when modify-ing the annotation of a single parse tree.
In fact:H(T ) = ?|L |?l=1n?i=1p(?li) log2 p(?li)= ?|T |?j=1p(?j) log2 p(?j)(6)For each input parse tree under consideration,the algorithm selects a non-terminal node and triesto change the head annotation from its currenthead-daughter to a different one.
As an example,considering the parse tree of figure 1 and the inter-nal node NP (the leftmost one), we try to annotateits leftmost daughter as the new head.
When con-sidering the changes that this modification bringson the set of the elementary trees T , we understandthat there are only 4 elementary trees affected, asshown in figure 4.After making the change in the head annotation,we just need to decrease the frequencies of the oldtrees by one unit, and increase the ones of the newtrees by one unit.
The change in the entropy of ourgrammar can therefore be computed by calculat-ing the change in the partial entropy of these four704NPNNP NNPHaagNNPMs.NPNNPMs.NNPNNPHaag?h ?d ?
?h ?
?dFigure 4: Lexical trees considered in the EN-TROPY algorithm when changing the head ass-ingnment from the second NNP to the first NNPof the leftmost NP node of figure 1.
?h is the oldhead tree; ?d the old dependent tree; ?
?d the newdependent tree; ?
?h the new head tree.elementary trees before and after the change.
Ifsuch change results in a lower entropy of the gram-mar, the new annotation is kept, otherwise we goback to the previous annotation.
Although there isno guarantee our algorithm finds the global min-imum, it is very efficient and succeeds in drasti-cally minimize the entropy from a random anno-tated corpus.3.3 Maximizing FamiliarityThe main intuition behind our second method isthat we like to assign heads to a tree t in sucha way that the elementary trees that we can ex-tract from t are frequently observed in other treesas well.
That is, we like to use elementary treeswhich are general enough to occur in many possi-ble constructions.We start with building the bag of all one-anchorlexicalized elementary trees from the training cor-pus, consistent with any annotation of the heads.This operation is reminiscent of the extraction ofall subtrees in Data-Oriented Parsing (Bod, 1998).Fortunately, and unlike DOP, the number of possi-ble lexicalised elementary trees is not exponentialin sentence length n, but polynomial: it is alwayssmaller than n2 if the tree is binary branching.Next, for each node in the treebank, we needto select a specific lexical anchor, among the onesit dominates, and annotate the nodes in the spinewith head annotations.
Our algorithm selects thelexical anchor which maximizes the frequency ofthe implied elementary tree in the bag of elemen-tary trees.
In figure 5, algorithm 1 (right) gives thepseudo-code for the algorithm, and the tree (left)shows an example of its usage.3.4 Spine and POS-tag reductionsThe two algorithms described in the previous twosections are also evaluated when performing twopossible generalization operations on the elemen-tary trees, which can be applied both alone or incombination:?
in the spine reduction, lexicalized trees aretransformed to their respective spines.
Thisallows to merge elementary trees that areslightly differing in argument structures.?
in the POStag reduction, every lexical itemof every elementary tree is replaced by itsPOStag category.
This allows for merging el-ementary trees with the same internal struc-ture but differing in lexical production.4 Implementation details4.1 Using CFGs for TSG parsingWhen evaluating parsing accuracy of a givenLTSG, we use a CKY PCFG parser.
We willbriefly describe how to set up an LTSG parser us-ing the CFG formalism.
Every elementary treein the LTSG should be treated by our parser asa unique block which cannot be further decom-posed.
But to feed it to a CFG-parser, we needto break it down into trees of depth 1.
In order tokeep the integrity of every elementary tree we willassign to its internal node a unique label.
We willachieve this by adding ?@i?
to each i-th internalnode encountered in T .Finally, we read off a PCFG from the elemen-tary trees, assigning to each PCFG rule a weightproportional to the weight of the elementary tree itis extracted from.
In this way the PCFG is equiv-alent to the original LTSG: it will produce exactlythe same derivation trees with the same probabil-ities, although we would have to sum over (expo-nentially) many derivations to obtain the correctprobabilities of a parse tree (derived tree).
We ap-proximate parse probability by computing the n-best derivations and summing over the ones thatyield the same parse tree (by removing the ?@i?-labels).
We then take the parse tree with highestprobability as best parse of the input sentence.4.2 Unknown words and smoothingWe use a simple strategy to deal with unknownwords occurring in the test set.
We replace all thewords in the training corpus occurring once, andall the unknown words in the test set, with a spe-cial *UNKNOWN* tag.
Moreover we replace allthe numbers in the training and test set with a spe-cial *NUMBER* tag.705Algorithm 1: MaximizeFamiliarity(N)Input: a non-terminal node N of a parsetree.beginL = null;MAX = ?1;foreach leaf l underN do?Nl = lex.
tree rooted in N and anchored in l;F = frequency of ?Nl ;if F > MAX thenL = l;MAX = F ;Mark all nodes in the path fromN to L with heads;foreach substitution siteNi of ?NL doMaximizeFamiliarity(Ni);endFigure 5: Left: example of a parse tree in an instantiation of the ?Familiarity?
algorithm.
Each arrow,connecting a word to an internal node, represents the elementary tree anchored in that word and rootedin that internal node.
Numbers in parentheses give the frequencies of these trees in the bag of subtreescollected from WSJ20.
The number below each leaf gives the total frequency of the elementary treesanchored in that lexical item.
Right: pseudo-code of the ?Familiarity?
algorithm.Even with unknown words treated in this way,the lexicalized elementary trees that are extractedfrom the training data are often too specific toparse all sentences in the test set.
A simple strat-egy to ensure full coverage is to smooth with thetreebank PCFG.
Specifically, we add to our gram-mars all CFG rules that can be extracted from thetraining corpus and give them a small weight pro-portional to their frequency3.
This in general willensure coverage, i.e.
that all the sentences in thetest set can be successfully parsed, but still priori-tizing lexicalized trees over CFG rules4.4.3 CorporaThe evaluations of the different models were car-ried out on the Penn Wall Street Journal corpus(Marcus et al, 1993) for English, and the Tigertreebank (Brants et al, 2002) for German.
As goldstandard head annotations corpora, we used theParc 700 Dependency Bank (King et al, 2003) andthe Tiger Dependency Bank (Forst et al, 2004),which contain independent reannotations of ex-tracts of the WSJ and Tiger treebanks.5 ResultsWe evaluate the head annotations our algorithmsfind in two ways.
First, we compare the headannotations to gold standard manual annotations3In our implementation, each CFG rule frequency is di-vided by a factor 100.4In this paper, we prefer these simple heuristics over moreelaborate techniques, as our goal is to compare the merits ofthe different head-assignment algorithms.of heads.
Second, we evaluate constituency pars-ing performance using an LTSG parser (trainedon the various LTSGs), and a state-of-the-artparser (Bikel, 2004).5.1 Gold standard head annotationsTable 1 reports the performance of different al-gorithms against gold standard head annotationsof the WSJ and the Tiger treebank.
These an-notations were obtained by converting the depen-dency structures of the PARC corpus (700 sen-tences from section 23) and the Tiger DependencyBank (2000 sentences), into head annotations5.Since the algorithm doesn?t guarantee that the re-covered head annotations always follow the one-head-per-node constraint, when evaluating the ac-curacy of head annotations of different algorithms,we exclude the cases in which in the gold cor-pus no head or multiple heads are assigned to thedaughters of an internal node6, as well as cases inwhich an internal node has a single daughter.In the evaluation against gold standard de-pendencies for the PARC and Tiger dependencybanks, we find that the FAMILIARITY algorithmwhen run with POStags and Spine conversion ob-tains around 74% recall for English and 69% forGerman.
The different scores of the RANDOM as-signment for the two languages can be explained5This procedure is not reported here for reasons of space,but it is available for other researchers (together with the ex-tracted head assignments) at http://staff.science.uva.nl/?fsangati.6After the conversion, the percentage of incorrect headsin PARC 700 is around 9%; in Tiger DB it is around 43%.706by their different branching factors: trees in theGerman treebank are typically more flat than thosein the English WSJ corpus.
However, note thatother settings of our two annotation algorithms donot always obtain better results than random.When focusing on the Tiger results, we ob-serve that the RIGHT head assignment recall ismuch better than the LEFT one.
This result is inline with a classification of German as a predomi-nantly head-final language (in contrast to English).More surprisingly, we find a relatively low recallof the head annotation in the Tiger treebank, whencompared to a gold standard of dependencies forthe same sentences as given by the Tiger depen-dency bank.
Detailed analysis of the differencesin head assigments between the two approachesis left for future work; for now, we note that ourbest performing algorithm approaches the inter-annotation-scheme agreement within only 10 per-centage points7.5.2 Constituency Parsing resultsTable 2 reports the parsing performances of ourLTSG parser on different LTSGs extracted fromthe WSJ treebank, using our two heuristics to-gether with the 4 baseline strategies (plus the re-sult of a standard treebank PCFG).
The parsing re-sults are computed on WSJ20 (WSJ sentences upto length 20), using sections 02-21 for training andsection 22 for testing.We find that all but one of the head-assignmentalgorithms lead to LTSGs that without any fine-tuning perform better than the treebank PCFG.
Onthis metric, our best performing algorithm scores4 percentage points higher than the Magerman-Collins annotation scheme (a 19% error reduc-tion).
The poor results with the RIGHT assign-ment, in contrast with the good results with theLEFT baseline (performing even better than theMagerman-Collins assignments), are in line withthe linguistic tradition of listing English as a pre-dominantly head-initial language.
A surprisingresult is that the RANDOM-assignment gives the7We have also used the various head-assignments to con-vert the treebank trees to dependency structures, and usedthese in turn to train a dependency parser (Nivre et al, 2005).Results from these experiments confirm the ordering of thevarious unsupervised head-assignment algorithms.
Our bestresults, with the FAMILIARITY algorithm, give us an Unla-beled Attachment Score (UAS) of slightly over 50% againsta gold standard obtained by applying the Collins-Magermanrules to the test set.
This is much higher than the three base-lines, but still considerably worse than results based on su-pervised head-assignments.best performing LTSG among the baselines.
Note,however, that this strategy leads to much wield-ier grammars; with many more elementary treesthan for instance the left-head assignment, theRANDOM strategy is apparently better equippedto parse novel sentences.
Both the FAMILIAR-ITY and the ENTROPY strategy are at the level ofthe random-head assignment, but do in fact lead tomuch more compact grammars.We have also used the same head-enriched tree-bank as input to a state-of-the-art constituencyparser8 (Bikel, 2004), using the same training andtest set.
Results, shown in table 3, confirm thatthe differences in parsing success due to differ-ent head-assignments are relatively minor, and thateven RANDOM performs well.
Surprisingly, ourbest FAMILIARITY algorithm performs as well asthe Collins-Magerman scheme.LFS UFS |T|PCFG 78.23 82.12 -RANDOM 82.70 85.54 64kLEFT 80.05 83.21 46kMagerman-Collins 79.01 82.67 54kRIGHT 73.04 77.90 49kFAMILIARITY 84.44 87.22 42kENTROPY-POStags 82.81 85.80 64kFAMILIARITY-Spine 82.67 85.35 47kENTROPY-POStags-Spine 82.64 85.55 64kTable 2: Parsing accuracy on WSJ20 of the LTSGsextracted from various head assignments, whencomputing the most probable derivations for ev-ery sentence in the test set.
The Labeled F-Score(LFS) and unlabeled F-Score (UFS) results are re-ported.
The final column gives the total number ofextracted elementary trees (in thousands).LFS UFSMagerman-Collins 86.20 88.35RANDOM 84.58 86.97RIGHT 81.62 84.41LEFT 81.13 83.95FAMILIARITY-POStags 86.27 88.32FAMILIARITY-POStags-Spine 85.45 87.71FAMILIARITY-Spine 84.41 86.83FAMILIARITY 84.28 86.53Table 3: Evaluation on WSJ20 of various head as-signments on Bikel?s parser.8Although we had to change a small part of the code,since the parser was not able to extract heads from an en-riched treebank, but it was only compatible with rule-basedassignments.
For this reason, results are reported only as abase of comparison.707Gold = PARC 700 % correctMagerman-Collins 84.51LEFT 47.63RANDOM 43.96RIGHT 40.70FAMILIARITY-POStags-Spine 74.05FAMILIARITY-POStags 51.10ENTROPY-POStags-Spine 43.23FAMILIARITY-Spine 39.68FAMILIARITY 37.40Gold = Tiger DB % correctTiger TB Head Assignment?
77.39RIGHT 52.59RANDOM 38.66LEFT 18.64FAMILIARITY-POStags-Spine 68.88FAMILIARITY-POStags 41.74ENTROPY-POStags-Spine 37.99FAMILIARITY 26.08FAMILIARITY-Spine 22.21Table 1: Percentage of correct head assignments against gold standard in Penn WSJ and Tiger.?
The Tiger treebank already comes with built-in head labels, but not for all categories.
In this case thescore is computed only for the internal nodes that conform to the one head per node constraint.6 ConclusionsIn this paper we have described an empirical inves-tigation into possible ways of enriching corporawith head information, based on different linguis-tic intuitions about the role of heads in natural lan-guage syntax.
We have described two novel algo-rithms, based on entropy minimization and famil-iarity maximization, and several variants of thesealgorithms including POS-tag and spine reduction.Evaluation of head assignments is difficult, asno widely agreed upon gold standard annotationsexist.
This is illustrated by the disparities betweenthe (widely used) Magerman-Collins scheme andthe Tiger-corpus head annotations on the onehand, and the ?gold standard?
dependencies ac-cording to the corresponding Dependency Bankson the other.
We have therefore not only evalu-ated our algorithms against such gold standards,but also tested the parsing accuracies of the im-plicit lexicalized grammars (using three differentparsers).
Although the ordering of the algorithmson performance on these various evaluations is dif-ferent, we find that the best performing strategiesin all cases and for two different languages arewith variants of the ?familiarity?
algorithm.Interestingly, we find that the parsing results areconsistently better for the algorithms that keep thefull lexicalized elementary trees, whereas the bestmatches with gold standard annotations are ob-tained by versions that apply the POStag and spinereductions.
Given the uncertainty about the goldstandards, the possibility remains that this reflectsbiases towards the most general headedness-rulesin the annotation practice rather than a linguisti-cally real phenomenon.Unsupervised head assignment algorithms canbe used for the many applications in NLP whereinformation on headedness is needed to convertconstituency trees into dependency trees, or toextract head-lexicalised grammars from a con-stituency treebank.
Of course, it remains to beseen which algorithm performs best in any of thesespecific applications.
Nevertheless, we concludethat among currently available approaches, i.e.,our two algorithms and the EM-based approach of(Chiang and Bikel, 2002), ?familiarity maximiza-tion?
is the most promising approach for automaticassignments of heads in treebanks.From a linguistic point of view, our work canbe seen as investigating ways in which distribu-tional information can be used to determine head-edness in phrase-structure trees.
We have shownthat lexicalized tree grammars provide a promis-ing methodology for linking alternative head as-signments to alternative dependency structures(needed for deeper grammatical structure, includ-ing e.g., argument structure), as well as to alterna-tive derivations of the same sentences (i.e.
the setof lexicalized elementary trees need to derive thegiven parse tree).
In future work, we aim to extendthese results by moving to more expressive gram-matical formalisms (e.g., tree adjoining grammar)and by distinguishing adjuncts from arguments.Acknowledgments We gratefully acknowledgefunding by the Netherlands Organization for Sci-entific Research (NWO): FS is funded througha Vici-grant ?Integrating Cognition?
(277.70.006)to Rens Bod and WZ through a Veni-grant ?Dis-covering Grammar?
(639.021.612).
We thankRens Bod, Yoav Seginer, Reut Tsarfaty andthree anonymous reviewers for helpful comments,Thomas By for providing us with his dependencybank and Joakim Nivre and Dan Bikel for help inadapting their parsers to work with our data.708ReferencesS.
Bangalore and A.K.
Joshi.
1999.
Supertagging: Anapproach to almost parsing.
Computational Linguis-tics, 25(2):237?265.D.M.
Bikel.
2004.
Intricacies of Collins?
ParsingModel.
Computational Linguistics, 30(4):479?511.R.
Bod.
1998.
Beyond Grammar: An experience-based theory of language.
CSLI, Stanford, CA.G.
Borensztajn, and W. Zuidema.
2007.
BayesianModel Merging for Unsupervised Constituent La-beling and Grammar Induction.
Technical Report,ILLC.S.
Brants, S. Dipper, S. Hansen, W. Lezius, andG.
Smith.
2002.
The TIGER treebank.
In Proceed-ings of the Workshop on Treebanks and LinguisticTheories, Sozopol.T.
By.
2007.
Some notes on the PARC 700 dependencybank.
Natural Language Engineering, 13(3):261?282.E.
Charniak.
1997.
Statistical parsing with a context-free grammar and word statistics.
In Proceedings ofthe fourteenth national conference on artificial intel-ligence, Menlo Park.
AAAI Press/MIT Press.D.
Chiang and D.M.
Bikel.
2002.
Recoveringlatent information in treebanks.
Proceedings ofthe 19th international conference on Computationallinguistics-Volume 1, pages 1?7.D.
Chiang.
2000.
Statistical parsing with anautomatically-extracted tree adjoining grammar.
InProceedings of the 38th Annual Meeting of the ACL.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.G.
Corbett, N. Fraser, and S. McGlashan, editors.2006.
Heads in Grammatical Theory.
CambridgeUniversity Press.M.
Forst, N. Bertomeu, B. Crysmann, F. Fouvry,S.
Hansen-Schirra, and V. Kordoni.
2004.
To-wards a dependency-based gold standard for Ger-man parsers.J.
Hockenmaier and M. Steedman.
2007.
CCGbank:A corpus of ccg derivations and dependency struc-tures extracted from the penn treebank.
Comput.Linguist., 33(3):355?396.A.K.
Joshi and Y. Schabes.
1991.
Tree-adjoininggrammars and lexicalized grammars.
Technical re-port, Department of Computer & Information Sci-ence, University of Pennsylvania.T.
King, R. Crouch, S. Riezler, M. Dalrymple, andR.
Kaplan.
2003.
The PARC 700 dependency bank.D.
Klein and C.D.
Manning.
2002.
A generativeconstituent-context model for improved grammar in-duction.
In Proceedings of the 40th Annual Meetingof the ACL.D.
Klein and C.D.
Manning.
2004.
Corpus-basedinduction of syntactic structure: models of depen-dency and constituency.
In Proceedings of the 42ndAnnual Meeting of the ACL.D.M.
Magerman.
1995.
Statistical decision-tree mod-els for parsing.
In Proceedings of the 33rd AnnualMeeting of the ACL.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2).J.
Nivre and J.
Hall.
2005.
MaltParser: A Language-Independent System for Data-Driven DependencyParsing.
In Proceedings of the Fourth Workshopon Treebanks and Linguistic Theories (TLT2005),pages 137?148.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nils-son,S.
Riedel, and D. Yuret.
2007.
The conll 2007shared task on dependency parsing.
In Proc.
of theCoNLL 2007 Shared Task., June.J.
Nivre.
2007.
Inductive Dependency Parsing.
Com-putational Linguistics, 33(2).S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.2006.
Learning accurate, compact, and interpretabletree annotation.
In Proceedings ACL-COLING?06,pages 443?440.R.
Reichart and A. Rappoport.
2008.
UnsupervisedInduction of Labeled Parse Trees by Clustering withSyntactic Features.
In Proceedings Coling.H.
Schu?tze.
1993.
Part-of-speech induction fromscratch.
In Proceedings of the 31st annual meetingof the ACL.Y.
Seginer 2007.
Learning Syntactic Structure.
Ph.D.thesis, University of Amsterdam.H.
Yamada, and Y. Matsumoto.
2003.
Statistical De-pendency Analysis with Support Vector Machines.In Proceedings of the Eighth International Work-shop on Parsing Technologies.
Nancy, France.709
