Proceedings of the ACL Student Research Workshop, pages 85?90,Ann Arbor, Michigan, June 2005. c?2005 Association for Computational LinguisticsLearning Strategies for Open-Domain Natural Language QuestionAnsweringEugene GroisDepartment of Computer ScienceUniversity of Illinois, Urbana-ChampaignUrbana, Illinoise-grois@uiuc.eduAbstractThis work presents a model for learninginference procedures for storycomprehension through inductivegeneralization and reinforcementlearning, based on classified examples.The learned inference procedures (orstrategies) are represented as of sequencesof transformation rules.
The approach iscompared to three prior systems, andexperimental results are presenteddemonstrating the efficacy of the model.1  IntroductionThis paper presents an approach to automaticallylearning strategies for natural language questionanswering from examples composed of textualsources, questions, and answers.
Our approach isfocused on one specific type of text-based questionanswering known as story comprehension.
MostTREC-style QA systems are designed to extract ananswer from a document contained in a fairly largegeneral collection (Voorhees, 2003).
They tend tofollow a generic architecture, such as the onesuggested by (Hirschman and Gaizauskas, 2001),that includes components for document pre-processing and analysis, candidate passageselection, answer extraction, and responsegeneration.
Story comprehension requires asimilar approach, but involves answering questionsfrom a single narrative document.
An importantchallenge in text-based question answering ingeneral is posed by the syntactic and semanticvariability of question and answer forms, whichmakes it difficult to establish a match between thequestion and answer candidate.
This problem isparticularly acute in the case of storycomprehension due to the rarity of informationrestatement in the single document.Several recent systems have specificallyaddressed the task of story comprehension.
TheDeep Read reading comprehension system(Hirschman et al, 1999) uses a statistical bag-of-words approach, matching the question with thelexically most similar sentence in the story.
Quarc(Riloff and Thelen, 2000) utilizes manuallygenerated rules that selects a sentence deemed tocontain the answer based on a combination ofsyntactic similarity and semantic correspondence(i.e., semantic categories of nouns).
The BrownUniversity statistical language processing classproject systems (Charniak et al, 2000) combinethe use of manually generated rules with statisticaltechniques such as bag-of-words and bag-of-verbmatching, as well as deeper semantic analysis ofnouns.
As a rule, these three systems are effectiveat identifying the sentence containing the correctanswer as long as the answer is explicit andcontained entirely in that sentence.
They find itdifficult, however, to deal with semanticalternations of even moderate complexity.
Theyalso do not address situations where answers aresplit across multiple sentences, or those requiringcomplex inference.Our framework, called QABLe (Question-Answering Behavior Learner), draws on priorwork in learning action and problem-solvingstrategies (Tadepalli and Natarajan, 1996;Khardon, 1999).
We represent textual sources assets of features in a sparse domain, and treat theQA task as behavior in a stochastic, partiallyobservable world.
QA strategies are learned assequences of transformation rules capable ofderiving certain types of answers from particulartext-question combinations.
The transformationrules are generated by instantiating primitivedomain operators in specific feature contexts.
Aprocess of reinforcement learning (Kaebling et al,1996) is used to select and promote effectivetransformation rules.
We rely on recent work inattribute-efficient relational learning (Khardon etal., 1999; Cumby and Roth, 2000; Even-Zohar andRoth, 2000) to acquire natural representations ofthe underlying domain features.
These85representations are learned in the course ofinteracting with the domain, and encode thefeatures at the levels of abstraction that are foundto be conducive to successful behavior.
Thisselection effect is achieved through a combinationof inductive generalization and reinforcementlearning elements.The rest of this paper is organized as follows.Section 2 presents the details of the QABLeframework.
In section 3 we describe preliminaryexperimental results which indicate promise forour approach.
In section 4 we summarize anddraw conclusions.2  QABLe ?
Learning to Answer Questions2.1  OverviewFigure 1 shows a diagram of the QABLeframework.
The bottom-most layer is the naturallanguage textual domain.
It represents raw textualsources, questions, and answers.
The intermediatelayer consists of processing modules that translatebetween the raw textual domain and the top-mostlayer, an abstract representation used to reason andlearn.This framework is used both for learning toanswer questions and for the actual QA task.While learning, the system is provided with a set oftraining instances, each consisting of a textualnarrative, a question, and a corresponding answer.During the performance phase, only the narrativeand question are given.At the lexical level, an answer to a question isgenerated by applying a series of transformationrules to the text of the narrative.
Thesetransformation rules augment the original text withone or more additional sentences, such that one ofthese explicitly contains the answer, and matchesthe form of the question.On the abstract level, this is essentially aprocess of searching for a path through problemspace that transforms the world state, as describedby the textual source and question, into a worldstate containing an appropriate answer.
Thisprocess is made efficient by learning answer-generation strategies.
These strategies storeprocedural knowledge regarding the way in whichanswers are derived from text, and suggestappropriate transformation rules at each step in theanswer-generation process.
Strategies (and theprocedural knowledge stored therein) are acquiredby explaining (or deducing) correct answers fromtraining examples.
The framework?s ability toanswer questions is tested only with respect to thekinds of documents it has seen during training, thekinds of questions it has practiced answering, andits interface to the world (domain sensors andoperators).In the next two sections we discuss lexical pre-processing, and the representation of features andrelations over them in the QABLe framework.
Insection 2.4 we look at the structure oftransformation rules and describe how they areinstantiated.
In section 2.5, we build on thisinformation and describe details of how strategiesare learned and utilized to generate answers.
Insection 2.6 we explain how candidate answers arematched to the question, and extracted.2.2  Lexical Pre-ProcessingSeveral levels of syntactic and semantic processingare required in order to generate structures thatfacilitate higher order analysis.
We currently useMontyTagger 1.2, an off-the-shelf POS taggerbased on (Brill, 1995) for POS tagging.
At thenext tier, we utilize a Named Entity (NE) taggerfor proper nouns a semantic category classifier fornouns and noun phrases, and a co-referenceresolver (that is limited to pronominal anaphora).Our taxonomy of semantic categories is derivedfrom the list of unique beginners for WordNetnouns (Fellbaum, 1998).
We also have a parallelstage that identifies phrase types.
Table 1 gives alist of phrase types currently in use, together withthe categories of questions each phrase type cananswer.
In the near future, we plan to utilize a linkparser to boost phrase-type tagging accuracy.
Forquestions, we have a classifier that identifies thelexically pre-process raw textextract currentstate features &compare to goalgoal statereached?moreprocessingtime?lookup existingapplicable rulevalid ruleexists?moreprimitiveops?instantiatenew rulegeneralize againstrule baseexecute rule indomainyesnoyes yesnonomodify raw textmatch candidatesentenceextract answeryesapplyreinforcement torule basenoreturn FAILraw text,   question,  (answer)lexicalized answeracting byinferenceacting bysearchRAWTEXTUALDOMAINABSTRACTLEARNING/REASONINGFRAMEWORKINTERMEDIATEPROCESSINGLAYERSTARTFigure 1.
The QABLe architecture for questionanswering.86semantic category of information requested by thequestion.
Currently, this taxonomy is identical tothat of semantic categories.
However, in thefuture, it may be expanded to accommodate awider range of queries.
A separate modulereformulates questions into statement form for latermatching with answer-containing phrases.2.3  Representing the QA DomainIn this section we explain how features areextracted from raw textual input and tags which aregenerated by pre-processing modules.A sentence is represented as a sequence ofwords  ?w1, w2,?, wn?, where word(wi, word) bindsa particular word to its position in the sentence.The kth sentence in a passage is given a uniquedesignation sk.
Several simple functions capturethe syntax of the sentence.
The sentence Main(e.g., main verb) is the controlling element of thesentence, and is recognized by main(wm, sk).
Partsof speech are recognized by the function pos, as inpos(wi, NN) and pos(wi, VBD).
The relativesyntactic ordering of words is captured by thefunction wj=before(wi).
It can be appliedrecursively, as wk = before(wj) = before(before(wi))to generate the entire sentence starting with anarbitrary word, usually the sentence Main.before() may also be applied as a predicate, such asbefore(wi, wj).
Thus for each word wi  in thesentence, inSentence(wi, si) ?
main(wm, sk) ?
(before(wi, wm) ?
before(wm, wi)).
A consecutivesequence of words is a phrase entity or simplyentity.
It is given the designation ex  and declaredby a binding function, such as entity(ex, NE) for anamed entity, and entity(ex, NP) for a syntacticgroup of type noun phrase.
Each phrase entity isidentified by its head, as head(wh, ex), and we saythat the phrase head controls the entity.
A phraseentity is defined as head(wh, ex) ?
inPhrase(wi, ex)?
?
?
inPhrase(wj, ex).We also wish to represent higher-order relationssuch as functional roles and semantic categories.Functional dependency between pairs of words isencoded as, for example, subj(wi, wj) and aux(wj,wk).
Functional groups are represented just likephrase entities.
Each is assigned a designation rx,declared for example, as func_role(rx, SUBJ), anddefined in terms of its head and members (whichmay be individual words or composite entities).Semantic categories are similarly defined over theset of words and syntactic phrase entities ?
forexample, sem_cat(cx, PERSON) ?
head(wh, cx) ?pos(wi, NNP) ?
word(wh, ?John?
).Semantically, sentences are treated as eventsdefined by their verbs.
A multi-sentential passageis represented by tying the member sentencestogether with relations over their verbs.
Wedeclare two such relations ?
seq and cause.
Theseq relation between two sentences, seq(si, sj) ?prior(main(si), main(sj)), is defined as thesequential ordering in time of the correspondingevents.
The cause relation cause(si, sj) ?cdep(main(si), main(sj)) is defined such that thesecond event is causally dependent on the first.2.4  Primitive Operators and TransformationRulesThe system, in general, starts out with noprocedural knowledge of the domain (i.e., notransformation rules).
However, it is equippedwith 9 primitive operators that define basic actionsin the domain.
Primitive operators are existentiallyquantified.
They have no activation condition, butonly an existence condition ?
the minimal bindingcondition for the operator to be applicable in agiven state.
A primitive operator has the formAC E ??
, where EC  is the existence condition andA?
is an action implemented in the domain.
Anexample primitive operator isprimitive-op-1 :     ?
wx, wy ?
add-word-after-word(wy, wx)Other primitive operators delete words ormanipulate entire phrases.
Note that primitiveoperators act directly on the syntax of the domain.In particular, they manipulate words and phrases.A primitive operator bound to a state in the domainconstitutes a transformation rule.
The procedurePhrase Type CommentsSUBJ  ?Who?
and nominal ?What?
questionsVERB event ?What?
questionsDIR-OBJ  ?Who?
and nominal ?What?
questionsINDIR-OBJ  ?Who?
and nominal ?What?
questionsELAB-SUBJ descriptive ?What?
questions (eg.
what kind)ELAB-VERB-TIMEELAB-VERB-PLACEELAB-VERB-MANNERELAB-VERB-CAUSE  ?Why?
questionELAB-VERB-INTENTION   ?Why?
as well as ?What for?
questionELAB-VERB-OTHERsmooth handling ofundefined verb phrasetypesELAB-DIR-OBJ descriptive ?What?
questions (eg.
what kind)ELAB-INDIR-OBJ descriptive ?What?
questions (eg.
what kind)VERB-COMPLWHERE/WHEN/HOWquestions concerning stateor statusTable 1.
Phrase types used by QABLe framework.87for instantiating transformation rules usingprimitive operators is given in Figure 2.
The resultof this procedure is a universally quantified rulehaving the form AGC R ??
.
A  may representeither the name of an action in the world or aninternal predicate.
C represents the necessarycondition for rule activation in the form of aconjunction over the relevant attributes of theworld state.
RG  represents the expected effect ofthe action.
For example, turn_on_x2???
221 gxxindicates that when 1x  is on and 2x  is off, thisoperator is expected to turn 2x  on.An instantiated rule is assigned a rankcomposed of:?
priority rating?
level of experience with rule?
confidence in current parameter bindingsThe first component, priority rating, is aninductively acquired measure of the rule?sperformance on previous instances.
The secondcomponent modulates the priority rating withrespects to a frequency of use measure.
The thirdcomponent captures any uncertainty inherent in theunderlying features serving as parameters to therule.Each time a new rule is added to the rule base,an attempt is made to combine it with similarexisting rules to produce more general rules havinga wider relevance and applicability.Given a rule1AggccRyRxba ????
covering a setof example instances 1E  and another rule2AggccRzRycb ????
covering a set of examples2E , we add a more general rule 3Agc Ryb ??
to thestrategy.
The new rule 3A  is consistent with 1E and2E .
In addition it will bind to any state where theliteral bc  is active.
Therefore the hypothesisrepresented by the triggering condition is likely anovergeneralization of the target concept.
Thismeans that rule 3A  may bind in some stateserroneously.
However, since all rules that can bindin a state compete to fire in that state, if there is abetter rule, then 3A  will be preempted and will notfire.2.5  Generating AnswersReturning to Figure 1, we note that at the abstractlevel the process of answer generation begins withthe extraction of features active in the current state.These features represent low-level textualattributes and the relations over them described insection 2.3.Immediately upon reading the current state, thesystem checks to see if this is a goal state.
A goalstate is a state who?s corresponding textual domainrepresentation contains an explicit answer in theright form to match the questions.
In the abstractrepresentation, we say that in this state all of thegoal constraints are satisfied.If the current state is indeed a goal state, nofurther inference is required.
The inferenceprocess terminates and the actual answer isidentified by the matching technique described insection 2.6 and extracted.If the current state is not a goal state and moreprocessing time is available, QABLe passes thestate to the Inference Engine (IE).
This modulestores strategies in the form of decision lists ofrules.
For a given state, each strategy mayrecommend at most one rule to execute.
For eachstrategy this is the first rule in its decision list tofire.
The IE selects the rule among these with thehighest relative rank, and recommends it as thenext transformation rule to be applied to thecurrent state.If a valid rule exists it is executed in thedomain.
This modifies the concrete textual layer.At this point, the pre-processing and featureextraction stages are invoked, a new current state isproduced, and the inference cycle begins anew.If a valid rule cannot be recommend by the IE,QABLe passes the current state to the SearchEngine (SE).
The SE uses the current state and itsset of primitive operators to instantiate a new rule,as described in section 2.4.
This rule is thenexecuted in the domain, and another iteration ofthe process begins.If no more primitive operators remain to beapplied to the current state, the SE cannotinstantiate a new rule.
At this point, search for thegoal state cannot proceed, processing terminates,and QABLe returns failure.Instantiate RuleGiven:?
set of primitive operators?
current state specification?
goal specification1.
select primitive operator to instantiate2.
bind active state variables & goal spec to existentiallyquantified condition variables3.
execute action in domain4.
update expected effect of new rule according to changein state variable valuesFigure 2.
Procedure for instantiating transformationrules using primitive operators.88When the system is in the training phase andthe SE instantiates a new rule, that rule isgeneralized against the existing rule base.
Thisprocedure attempts to create more general rulesthat can be applied to unseen example instances.Once the inference/search process terminates(successfully or not), a reinforcement learningalgorithm is applied to the entire rule search-inference tree.
Specifically, rules on the solutionpath receive positive reward, and rules that fired,but are not on the solution path receive negativereinforcement.2.6  Candidate Answer Matching andExtractionAs discussed in the previous section, when a goalstate is generated in the abstract representation, thiscorresponds to a textual domain representation thatcontains an explicit answer in the right form tomatch the questions.
Such a candidate answer maybe present in the original text, or may be generatedby the inference/search process.
In either case, theanswer-containing sentence must be found, and theactual answer extracted.
This is accomplished bythe Answer Matching and Extraction procedure.The first step in this procedure is to reformulatethe question into a statement form.
This results ina sentence containing an empty slot for theinformation being queried.
Recall further thatQABLe?s pre-processing stage analyzes text withrespect to various syntactic and semantic types.
Inaddition to supporting abstract feature generation,these tags can be used to analyze text on a lexicallevel.
The goal now is to find a sentence whosesyntactic and semantic analysis matches that of thereformulated question?s as closely as possible.3  Experimental Evaluation3.1  Experimental SetupWe evaluate our approach to open-domain naturallanguage question answering on the Remediacorpus.
This is a collection of 115 children?sstories provided by Remedia Publications forreading comprehension.
The comprehension ofeach story is tested by answering five who, what,where, and why questions.The Remedia Corpus was initially used toevaluate the Deep Read reading comprehensionsystem, and later also other systems, includingQuarc and the Brown University statisticallanguage processing class project.The corpus includes two answer keys.
The firstanswer key contains annotations indicating thestory sentence that is lexically closest to the answerfound in the published answer key (AutSent).
Thesecond answer key contains sentences that ahuman judged to best answer each question(HumSent).
Examination of the two keys showsthe latter to be more reliable.
We trained andtested using the HumSent answers.
We alsocompare our results to the HumSent results of priorsystems.
In the Remedia corpus, approximately10% of the questions lack an answer.
Followingprior work, only questions with annotated answerswere considered.We divided the Remedia corpus into a set of 55tests used for development, and 60 tests used toevaluate our model, employing the same partitionscheme as followed by the prior work mentionedabove.
With five questions being supplied witheach test, this breakdown provided 275 exampleinstances for training, and 300 example instancesto test with.
However, due to the heavy reliance ofour model on learning, many more trainingexamples were necessary.
We widened thetraining set by adding story-question-answer setsobtained from several online sources.
With theextended corpus, QABLe was trained on 262stories with 3-5 questions each, corresponding to1000 example instances.System who what when where why OverallDeep Read 48% 38% 37% 39% 21% 36%Quarc 41% 28% 55% 47% 28% 40%Brown 57% 32% 32% 50% 22% 41%QABLe-N/L 48% 35% 52% 43% 28% 41%QABLe-L 56% 41% 56% 45% 35% 47%QABLe-L+ 59% 43% 56% 46% 36% 48%Table 2.
Comparison of QA accuracy by question type.System # rules learned # rules on solution path average # rules per correct answerQABLe-L 3,463 426 3.02QABLe-L+ 16,681 411 2.85Table 3.
Analysis of transformation rule learning and use.893.2  Discussion of ResultsTable 2 compares the performance of differentversions of QABLe with those reported by thethree systems described above.
We wish to discernthe particular contribution of transformation rulelearning in the QABLe model, as well as the valueof expanding the training set.
Thus, the QABLe-N/L results indicate the accuracy of answersreturned by the QA matching and extractionalgorithm described in section 2.6 only.
Thisalgorithm is similar to prior answer extractiontechniques, and provides a baseline for ourexperiments.
The QABLe-L results includeanswers returned by the full QABLe framework,including the utilization of learned transformationrules, but trained only on the limited trainingportion of the Remedia corpus.
The QABLe-L+results are for the version trained on the expandedtraining set.As expected, the accuracy of QABLe-N/L iscomparable to those of the earlier systems.
TheRemedia-only training set version, QABLe-L,shows an improvement over both the baselineQABLe, and most of the prior system results.
Thisis due to its expanded ability to deal with semanticalternations in the narrative by finding and learningtransformation rules that reformulate thealternations into a lexical form matching that of thequestion.The results of QABLe-L+, trained on theexpanded training set, are for the most partnoticeably better than those of QABLe-L.
This isbecause training on more example instances leadsto wider domain coverage through the acquisitionof more transformation rules.
Table 3 gives abreak-down of rule learning and use for the twolearning versions of QABLe.
The first column isthe total number of rules learned by each systemversion.
The second column is the number of rulesthat ended up being successfully used in generatingan answer.
The third column gives the averagenumber of rules each system needed to answer ananswer (where a correct answer was generated).Note that QABLe-L+ used fewer rules on averageto generate more correct answers than QABLe-L.This is because QABLe-L+ had more opportunitiesto refine its policy controlling rule firing throughreinforcement and generalization.Note that the learning versions of QABLe dosignificantly better than the QABLe-N/L and allthe prior systems on why-type questions.
This isbecause many of these questions require aninference step, or the combination of informationspanning multiple sentences.
QABLe-L andQABLe-L+ are able to successfully learntransformation rules to deal with a subset of thesecases.4  ConclusionThis paper present an approach to automaticallylearn strategies for natural language questionsanswering from examples composed of textualsources, questions, and corresponding answers.The strategies thus acquired are composed ofranked lists transformation rules that when appliedto an initial state consisting of an unseen text andquestion, can derive the required answer.
Themodel was shown to outperform three priorsystems on a standard story comprehension corpus.ReferencesE.
Brill.
Transformation-based error driven learningand natural language processing: A case study inpart of speech tagging.
In ComputationalLinguistics, 21(4):543-565, 1995.Charniak, Y. Altun, R. de Salvo Braz, B. Garrett, M.Kosmala, T. Moscovich, L. Pang, C. Pyo, Y. Sun,W.
Wy, Z. Yang, S. Zeller, and L. Zorn.
Readingcomprehension programs in a statistical-language-processing class.
ANLP/NAACL-00, 2000.C.
Cumby and D. Roth.
Relational representations thatfacilitate learning.
KR-00, pp.
425-434, 2000.Y.
Even-Zohar and D. Roth.
A classification approachto word prediction.
NAACL-00, pp.
124-131, 2000.C.
Fellbaum (ed.)
WordNet: An Electronic LexicalDatabase.
The MIT Press,  1998.L.
Hirschman and R. Gaizauskas.
Natural languagequestion answering: The view from here.
NaturalLanguage Engineering, 7(4):275-300, 2001.L.
Hirschman, M. Light, and J. Burger.
Deep Read: Areading comprehension system.
ACL-99, 1999.L.
P. Kaebling, M. L. Littman, and A. W. Moore.Reinforcement learning: A survey.
J. Artif.
Intel.Research, 4:237-285, 1996.R.
Khardon, D. Roth, and L. G. Valiant.
Relationallearning for nlp using linear threshold elements,IJCAI-99, 1999.R.
Khardon.
Learning to take action.
MachineLearning 35(1),  1999.E.
Riloff and M. Thelen.
A rule-based questionanswering system for reading comprehension tests.ANLP/NAACL-2000, 2000.P.
Tadepalli and B. Natarajan.
A formal framework forspeedup learning from problems and solutions.
J.Artif.
Intel.
Research, 4:445-475, 1996.E.
M. Voorhees  Overview of the TREC 2003 questionanswering track.
TREC-12, 2003.90
