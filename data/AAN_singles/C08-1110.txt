Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 873?880Manchester, August 2008A Framework for Identifying Textual RedundancyKapil Thadani and Kathleen McKeownDepartment of Computer Science,Columbia University,New York, NY USA{kapil,kathy}@cs.columbia.eduAbstractThe task of identifying redundant infor-mation in documents that are generatedfrom multiple sources provides a signifi-cant challenge for summarization and QAsystems.
Traditional clustering techniquesdetect redundancy at the sentential leveland do not guarantee the preservation ofall information within the document.
Wediscuss an algorithm that generates a novelgraph-based representation for a documentand then utilizes a set cover approximationalgorithm to remove redundant text from it.Our experiments show that this approachoffers a significant performance advantageover clustering when evaluated over an an-notated dataset.1 IntroductionThis paper approaches the problem of identifyingand reducing redundant information in documentsthat are generated from multiple sources.
This taskis closely related to many well-studied problemsin the field of natural language processing such assummarization and paraphrase recognition.
Sys-tems that utilize data from multiple sources, suchas question-answering and extractive summariza-tion systems that operate on news data, usually in-clude a component to remove redundant informa-tion from appearing in their generated output.However, practical attempts at reducing redun-dancy in the output of these types of systems usu-ally involve clustering the sentences of the gener-ated output, picking a representative sentence fromeach cluster and discarding the rest.
Althoughthis strategy would remove some redundant in-formation, clustering approaches tuned for coarsec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.matches could also remove non-redundant infor-mation whereas clustering approaches tuned fornear-exact matches could end up removing verylittle repeated information.
This is simply a con-sequence of the fact that information can, and usu-ally does, exist at the sub-sentential level and thatclusters of sentences don?t necessarily correspondto clusters of information.In this paper, we discuss a framework for build-ing a novel graph-based representation to detect re-dundancy within documents.
We identify redun-dancy at the sub-sentential level through pairwisealignment between the sentences of a documentand use this to build a bipartite graph which en-ables us to keep track of redundant informationacross all sentences.
Common information be-tween pairs of sentences, detected with the align-ment algorithm, can be extrapolated to document-wide units of information using the graph struc-ture.
Individual sentences that are encompassedby the information in the rest of the document canthen be identified and removed efficiently by us-ing a well-known greedy algorithm adapted for thisrepresentation.2 Related WorkThe challenge of minimizing redundant informa-tion is commonly faced by IR engines and extrac-tive summarization systems when generating theirresponses.
A well-known diversity-based rerank-ing technique for these types of systems is MMR(Carbonell and Goldstein, 1998), which attemptsto reduce redundancy by preferring sentences thatdiffer from the sentences already selected for thesummary.
However, this approach does not at-tempt to identify sub-sentential redundancy.Alternative approaches to identifying redun-dancy use clustering at the sentence level (Lin andHovy, 2001) to remove sentences that are largelyrepetitive; however, as noted earlier, this is notwell-suited to the redundancy task.
The use of sen-873tence simplification in conjunction with clustering(Siddharthan et al, 2004) could help alleviate thisproblem by effectively clustering smaller units, butthis issue cannot be avoided unless sentences aresimplified to atomic elements of information.Other research has introduced the notion ofidentifying concepts in the input text (Filatova andHatzivassiloglou, 2004), using a set cover algo-rithm to attempt to include as many concepts aspossible.
However, this approach uses tf-idf toapproximate concepts and thus doesn?t explicitlyidentify redundant text.
Our work draws on thisapproach but extends it to identify all detectableredundancies within a document set.Another approach does identify small sub-sentential units of information within text called?Basic Elements?
and uses these for evaluatingsummarizations (Hovy et al, 2006).
Our approach,in contrast, does not make assumptions about thesize or structure of redundant information sincethis is uncovered through alignments.We thus require the use of an alignment algo-rithm to extract the common information betweentwo pieces of text.
This is related to the well-studied problem of identifying paraphrases (Barzi-lay and Lee, 2003; Pang et al, 2003) and the moregeneral variant of recognizing textual entailment,which explores whether information expressed ina hypothesis can be inferred from a given premise.Entailment problems have also been approachedwith a wide variety of techniques, one of whichis dependency tree alignment (Marsi et al, 2006),which we utilize as well to align segments of textwhile respecting syntax.
However, our definitionof redundancy does not extend to include unidi-rectional entailment, and the alignment process issimply required to identify equivalent information.3 Levels of InformationIn describing the redundancy task, we deal withmultiple levels of semantic abstraction from thebasic lexical form.
This section describes the ter-minology used in this paper and the graph-basedrepresentation that is central to our approach.3.1 TerminologyThe following terms are used throughout this paperto refer to different aspects of a document.Snippet: This is any span of text in the doc-ument and is a lexical realization of information.While a snippet generally refers to a single sen-tence within a document, it can apply to multiplesentences or phrases within sentences.
Since re-dundancy will be reduced by removing whole snip-pets, a snippet can be defined as the smallest unitof text that can be dropped from a document forthe purpose of reducing redundancy.To illustrate the levels of information that weconsider, consider the following set of short sen-tences as snippets.
Although this is a synthe-sized example to simplify presentation, sentenceswith this type of overlapping information occurfrequently in the question-answering scenario overnews in which our approach has been used.x1: Whittington is an attorney.x2: Cheney shot Whittington, a lawyer.x3: Whittington, an attorney, was shot in Texas.x4: Whittington was shot by Cheney while hunting quail.x5: This happened during a quail hunt in Texas.We can see that all the information in x1is con-tained in both x2and x3.
While no other snip-pet is completely subsumed by any single snippet,they can be made redundant given combinations ofother snippets; for example, x4is redundant givenx2, x3and x5.
In order to identify these combina-tions, we need to identify the elements of informa-tion within each snippet.Concept: This refers to a basic unit of informa-tion within a document.
Concepts may be facts,opinions or details.
These are not necessarily se-memes, which are atomic units of meaning, butsimply units of information that are seen as atomicwithin the document.
We further restrict the defi-nition of a concept to a unit of information seen inmore than one snippet, since we are only interestedin concepts which help in identifying redundancy.Formally, a document can be defined as a set ofS snippets X = {x1, .
.
.
,xS}, which is a literalrepresentation of the document.
However, it canalso be defined in terms of its information content.We use Z = {z1, .
.
.
, zC} to represent the set of Cconcepts that cover all information appearing morethan once in the document.
In the example above,we can identify five non-overlapping concepts:zA: Whittington was shotzB: Whittington is an attorneyzC: The shooting occurred in TexaszD: It happened during a hunt for quailzE: Cheney was the shooterWe use subscripts for snippet indices and super-scripts for concept indices throughout this paper.874Nugget: This is a textual representation of aconcept in a snippet and therefore expresses someinformation which is also expressed elsewhere inthe document.
Different nuggets for a given con-cept may have unique lexico-syntactic realizations,as long as they all embody the same semantics.With regard to the notation used above, nuggetscan be represented by an S ?
C matrix Y whereeach ycsdenotes the fragment of text (if any) fromthe snippet xsthat represents concept zc.Since a concept itself has no unique textual re-alization, it can be simply represented by the com-bination of all its nuggets.
For instance, in the ex-ample shown above, concept zDis seen in both x4and x5in the form of two nuggets yD4(?...
whilehunting quail?)
and yD5(?...
during a quail hunt?
),which are paraphrases.
The degree to which wecan consider this and other types of lexical or syn-tactic differences between nuggets that have thesame semantic identity depends on the alignmentalgorithm used.Intersection: This is the common informationbetween two snippets that can be obtained throughtheir alignment.
For example, the intersectionfrom the alignment between x2and x4consistsof two fragments of text that express that Cheneyshot Whittington (an active-voiced fragment fromx2and a passive-voiced fragment from x4).In general, aligning xiand xjproduces an in-tersection vi,jwhich is simply a pair of alignedtext fragments covering the set of concepts that xiand xjhave in common.
However, these undi-vided segments of text may actually contain mul-tiple nuggets from a document-wide perspective.We assume that intersections can be decomposedinto smaller intersections through further align-ments with snippets or other intersections; this pro-cess is explained in Subsection 4.3.3.2 Concept graph representationFigure 1 illustrates the example introduced in Sub-section 3.1 as a network with intersections repre-sented as edges between snippets.
This is the typeof graph that would be built using pairwise align-ments between all snippets.
Note that althoughsome intersections such as v1,2(between x1andx2) and v3,5express concepts directly, other inter-sections such as v2,3and v2,4are undivided com-binations of concepts.
Since we cannot directlyidentify concepts and their nuggets, this graph isnot immediately useful for reducing redundancy.x1Bx2ABEx3ABCx4ADEx5CDBBABAEACDFigure 1: Graph representing pairwise alignmentsbetween the example snippets from Section 3.
Forclarity, alphabetic labels like A represent conceptszA.
Node labels show concepts within snippets andedge labels indicate concepts seen in intersections.x1Bx2ABEx3ABCx4ADEx5CDzAzBzCzDzEyB1yA2yB2yE2yA3yB3yC3yA4yD4yE4yC5yD5Figure 2: Structure of the equivalent concept graphfor the example document illustrated in Figure 1.Circular nodes xsrepresent snippets, large squareszcrepresent concepts and small squares ycsdepictnuggets for each concept within a snippet.Now, since the matrix Y describes the interac-tion of concepts with snippets, it can be viewed asan incidence matrix that defines a bipartite graphbetween snippets and concepts with nuggets rep-resenting the edges.
In this concept graph repre-sentation, each snippet can connect to any numberof other snippets via a shared concept.
Since con-cepts serve to connect multiple snippets together,the concept graph can also be seen as a hypergraph,which is a generalization of a graph in which eachedge may connect together multiple vertices.875Figure 2 illustrates the structure of the equiva-lent concept graph for the previous example.
Thisis simply the bipartite graph with the two types ofnodes, namely snippets and concepts, representedusing different symbols.
For clarity, nuggets arealso depicted as nodes in the graph, thereby reduc-ing edges to simple links indicating membership.This representation identifies the redundancy be-tween snippets in terms of non-overlapping con-cepts and is therefore more useful than the graphfrom Figure 1 for reducing redundancy.4 Constructing the Concept GraphWe now describe how a concept graph can be con-structed from a document by using dependencytree alignment and leveraging the existing struc-ture of the graph during construction.4.1 Alignment of snippetsIn order to obtain the concept graph representation,the common information between each pair ofsnippets in the document must first be discoveredby aligning all pairs of snippets with each other.We make use of dependency parsing and alignmentof dependency parse trees to obtain intersectionsbetween each pair of snippets, where each inter-section may be a discontiguous span of text corre-sponding to an aligned subtree within each snip-pet.
In our experiments, dependency parsing isaccomplished with Minipar (Lin, 1998) and align-ment is done using a bottom-up tree alignment al-gorithm (Barzilay and McKeown, 2005) modifiedto account for the shallow semantic role labels pro-duced by the parser.
The alignment implementa-tion is not the focus of this work, however, and theframework described here could by applied usingany alignment technique between segments of textin potentially any language.As seen in Figure 1, the intersections that can beextracted solely by pairwise comparisons are notunique and may contain multiple concepts.
A trulyinformation-preserving approach requires the ex-plicit identification of concepts as in the conceptgraph from Figure 2, but efficiently converting theformer into the latter poses a non-trivial challenge.4.2 Extraction of irreducible conceptsOur approach attempts to obtain a set of irre-ducible concepts such that each concept in this setcannot wholly or partially contain any other con-cept in the set (thereby conforming to the defini-tion of a concept in Subsection 3.1).We attempt to build the concept graph and main-tain irreducible concepts alongside each of theS(S ?
1)/2 pairwise alignment steps.
Every in-tersection found by aligning a pair of snippets isassumed to represent some concept that these snip-pets share; it is then compared with existing con-cepts and is decomposed into smaller intersectionsif it overlaps partially with any one of them.
Thisimplies a worst-case of C comparisons at eachpairwise alignment step (2C if both fragments ofan intersection are compared separately).
How-ever, this can be made more efficient by exploitingthe structure of the graph.
A new intersection onlyhas to be compared with concepts which might beaffected by it and only affects the other snippetscontaining these concepts.
We can show that thisleads to an algorithm that requires fewer than Ccomparisons, and additionally, that these compar-isons can be performed efficiently.Consider the definition of alignment along thelines of a mathematical relation.
We require snip-pet algnment to be an equivalence relation and ittherefore must have the following properties.Symmetry: If an intersection vi,jcontains aconcept z?, then vj,iwill also contain z?.
Thisproperty allows only S(S ?
1)/2 alignments tosuffice instead of the full S(S ?
1).
Therefore,without loss of generality, we can specify that allalignments between xiand xjshould have i < j.Transitivity: If intersections vi,jand vj,kbothcontain some concept z?, then vi,kwill also con-tain z?.
This property leads to an interesting con-sequence.
Assuming we perform alignments inorder (initially aligning x1and x2and iteratingfor j within each i), we observe that xihas beenaligned with snippets {x1, .
.
.
,xj?1} and, for anyi > 1, snippets {x1, .
.
.
,xi?1} were aligned withall snippets {x1, .
.
.
,xS}.
Since i < j, this im-plies that xiwas directly aligned with snippets{x1, .
.
.
,xi?1} which in turn were each alignedwith all S snippets.
Therefore, due to the prop-erty of transitivity, all concepts contained in anew intersection vi,jthat also exist in the partly-constructed graph would already be directly asso-ciated with xi.
Note that this does not hold forxjas well, since xjhas not been aligned with{xi+1, .
.
.
,xj?1}; therefore, it may not have en-countered all relevant concepts.This implies that for any i and j, all conceptsthat might be affected by a new intersection vi,j876have already been uncovered in xiand thus vi,jonly needs to be compared to these concepts.4.3 Comparisons after alignmentFor every new intersection vi,jproduced by analignment between xiand xj, the algorithm com-pares it (specifically, the fragment from xi) witheach existing nugget ykifor each concept zkal-ready seen in xi.
Checking for the following casesensures that the graph structure contains only irre-ducible concepts for all the alignments seen:1.
If vi,jdoesn?t overlap with any currentnugget from xi, it becomes a new concept thatlinks to xiand xj.
In our example, the firstintersection v1,2contains ?Whittington ... anattorney?
from x1and ?...
Whittington, alawyer?
from x2; this becomes a new conceptzBsince x1has no other nuggets.2.
If vi,joverlaps completely with a nugget yki,then xjmust also be linked to concept zk.
Forexample, x1?s fragment in the second inter-section v1,3is also ?Whittington ... an attor-ney?, so x3must also link to zB.3.
If vi,jsubsumes yki, it is split up and the non-overlapping portion is rechecked against ex-isting nuggets recursively.
For example, x2?sfragment in the intersection v2,3is ?...
shotWhittington, a lawyer?, part of which over-laps with yB2, so this intersection is dividedup and the part representing ?...
shot Whit-tington ...?
becomes a new concept zA.4.
If, on the other hand, ykisubsumes vi,j, theconcept zkis itself split up along with allnuggets that it links to, utilizing the presentstructure of the graph.When comparing intersections, we can restrict thedecomposition of nuggets to prevent the creationof overly-granular concepts.
For instance, wecan filter out intersections containing only isolatednamed-entities or syntactic artifacts like determin-ers since they contain no information by them-selves.
We can also prevent verbs and their ar-guments from being split apart using informationfrom a snippet?s dependency parse, if available.4.4 Efficiency of the algorithmInstead of C additional comparisons in the worstcase after each pairwise snippet algnment, weneed no more comparisons in the worst case thanthe maximum number of concepts that can exist ina single snippet.
Since this value grows no fasterthan C as S increases, this is a significant improve-ment.
Other factors, such as the overhead requiredto split up concepts, remain unchanged.Furthermore, since all the additional compar-isons are carried out between nuggets of the samesnippet, we don?t need to perform any furtheralignment among nuggets or concepts.
Alignmentsare expensive; each is O(n1n2) where n1and n2are the number of words in the two segments oftext being aligned (if dependency tree alignment isused) along with an overhead for checking wordsimilarity.
However, since we now only need tocompare text from the same snippet, the com-parison can be performed in linear time by sim-ply comparing spans of word indices, thereby alsoeliminating the overhead for comparing words.5 Decreasing redundancyThe concept graph can now be applied to the taskof reducing redundancy in the document by drop-ping snippets which contain no information that isnot already present in the rest of the document.5.1 Reduction to set coverEvery snippet xsin a document can be representedas a set of concepts {zc: ycs?
Y}.
Since conceptsare defined as information that is seen in more thanone snippet as per the definition in Subsection 3.1,representing snippets as sets of concepts will over-look any unique information present in a snippet.Without loss of generality, we can add any suchunique information in the form of an artificial con-cept for each snippet to Z so that snippets can becompletely represented as sets of concepts from Z.Note that the union of snippets?Ss=1xsequals Z.Reducing redundancy in the document whilepreserving all information requires us to identifythe most snippets whose entire informational con-tent is covered by the rest of the snippets in thedocument, thereby targeting them for removal.Since we express informational content in con-cepts, this problem reduces to the task of findingthe smallest group of snippets that together coverall the concepts in the document, i.e.
we need tofind the smallest subset X??
X such that, if X?contains R snippets x?r, the union of these snippets?Rr=1x?ralso equals Z.
Therefore, every conceptin a snippet from X?X?also exists in at least onesnippet from X?and no concept from Z is lost.This formulation of the problem is the classic877set cover problem, which seeks to find the smallestpossible group of subsets of a universe that cov-ers all the other subsets.
A more general variantof this problem is weighted set cover in which thesubsets have weights to be maximized or costs tobe minimized.
While this problem is known tobe NP-hard, there exists a straightforward localmaximization approach (Hochbaum, 1997) whichruns in polynomial time and is proven to give so-lutions within a known bound of the optimal solu-tion.
This greedy approximation algorithm can beadapted to our representation.5.2 Selecting non-redundant snippetsThe algorithm selects a snippet xrto the subset X?such that information content of X ?
xris max-imized.
In general, this implies that the snippetwith the highest degree over uncovered conceptsmust be selected at each iteration.
Other measuressuch as snippet length, fluency, or rank in an or-dered list can be included in a weight measure inorder to break ties and introduce a preference forshorter, more fluent, or higher-ranked snippets.Consider the example from Section 3.
The can-didates for selection are x2, x3and x4since theycontain the most uncovered concepts.
If x2is se-lected, its concepts zA, zBand zEare covered.At this stage, x5contains two uncovered conceptswhile x3and x4contain just one each.
Thus, x5is selected next and its concepts zCand zDarecovered.
Since no uncovered concepts remain, allsnippets which haven?t been selected are redun-dant.
This solution, which is shown in Figure 3,selects the following text to cover all the snippets:x2: Cheney shot Whittington, a lawyer.x5: This happened during a quail hunt in Texas.Other solutions are also possible depending onthe factors involved in choosing the snippet to beselected at each iteration.
For example, the algo-rithm might choose to select x3first instead of x2,thereby yielding the following solution:x3: Whittington, an attorney, was shot in Texas.x4: Whittington was shot by Cheney while hunting quail.6 ExperimentsTo evaluate the effectiveness of this frameworkempirically, we ran experiments over documentscontaining annotations corresponding to conceptswithin the document.
We also defined a metricx1Bx2ABEx3ABCx4ADEx5CDzAzBzCzDzEyB1yA2yB2yE2yA3yB3yC3yA4yD4yE4yC5yD5Figure 3: Pruned version of the concept graph ex-ample shown in Figure 2, illustrating the outcomeof removing redundant snippets.for comparing any concept graph over a documentto a gold-standard concept graph.
This was usedto compare the concept graphs created by our ap-proach to perturbed versions of the gold-standardgraphs and graphs created by clustering.6.1 DatasetDue to the scarcity of available annotated datasetssuitable for evaluating redundancy, we utilized thepyramid dataset from DUC 2005 (Nenkova et al,2007) which was created from 20 articles for thepurpose of summarization evaluation.
Each pyra-mid document is a hierarchical representation of 7summaries of the orginal news article.
These sum-maries have been annotated to identify the indi-vidual semantic content units or SCUs where eachSCU represents a certain fact, observation or pieceof information in the summary.
A sentence frag-ment representing an occurrence of an SCU in asummary is a contributor to the SCU.The pyramid construction for a group of sum-maries of the same article mirrors the conceptgraph representation described in Subsection 3.2.SCUs with more than two contributors are simi-lar in definition to concepts while their contribu-tors fill the role of nuggets.
Using this analogy,each dataset consists of a combination of the sevensummaries in a single pyramid document; the 20pyramid documents therefore yield 20 datasets.6.2 Evaluation metricsThe evaluation task requires us to compare the con-cept graph generated by our algorithm to the ideal878x1x2x3x4x1x2x3x4ConceptsSCUs(merge)(split)LalgLpyrFigure 4: The bipartite graph on the left showssnippets xslinked to concepts produced automati-cally; the one on the right shows the same snippetslinked to SCUs from annotated data.
Dashed linesindicate mappings between concepts and SCUs.concept graph extracted from the pyramid docu-ment annotations.
Standard metrics do not ap-ply easily to the problem of comparing bipartitegraphs, so we define a novel metric modeled onthe well-known IR measures of precision, recalland F-measure.
Figure 4 illustrates the elementsinvolved in the evaluation task.We define the metrics of precision, recall and F-measure over the links between snippets and con-cepts.
Assuming we have a mapping between gen-erated concepts and gold-standard SCUs, we canjudge whether each link is correct.
Let each singlelink between a snippet and a concept have an asso-ciated weight of 1 by default and let L indicate aset of such links.
We use Lalgand Lpyrto distin-guish between the sets of links generated by the al-gorithm and retrieved from the annotations respec-tively.
Precision and recall are defined as followswhile F-measure retains its traditional definition astheir harmonic mean.Precision =Sum of weights in Lalg?
LpyrSum of weights in LalgRecall =Sum of weights in Lalg?
LpyrSum of weights in LpyrTo determine a mapping between concepts andSCUs, we identify every concept and SCU pair,say zcand zs, which has one or more snippets incommon and, for each snippet xithat they havein common, we find the longest common subse-quence between their nuggets yciand ysito obtainthe following score which ranges from 0 to 1.LCS score =length(LCS)min (length(yci), length(ysi))Measure Random Clustering ConceptsPrecision 0.0510 0.2961 0.4496Recall 0.0515 0.1162 0.3266F1score 0.0512 0.1669 0.3783Table 1: Summary of the evaluation metrics aver-aged over all 20 pyramid documents when m=0.5This score is compared with a user-defined map-ping threshold m to determine if the concept andSCU are sufficiently similar.
In order to avoid bi-asing the metric by permitting multiple mappingsper concept, we adjust for merges or 1 : N map-pings by cloning the concept and creating N 1 : 1mappings in its place.
We then adjust for splits orN : 1 mappings by dividing the weight of each ofthe links connected to a participating concept byN .
Due to this normalization, the metrics are ob-served to be stable over variations in m.6.3 BaselinesWe compare the performance of the algorithmagainst two baselines.
The first approach involvesa random concept assignment scheme to build arti-ficial concept graphs using the distributional prop-erties of the gold-standard concept graphs.
Thenumber of concepts C and the number of snippetsthat each concept links to is determined by sam-pling from distributions over these properties de-rived from the statistics of the actual SCU graphfor that document.
For evaluation, these artificialconcepts are randomly mapped to SCUs using mto control the likelihood of mapping.
The bestscores from 100 evaluation runs were considered.The second baseline used for comparison is aclustering algorithm, since clustering is the mostcommon approach to dealing with redundancy.
Forthis purpose, we use a recursive spectral partition-ing algorithm, a variant of spectral clustering (Shiand Malik, 2000) which obtains an average V-measure (Rosenberg and Hirschberg, 2007) of 0.93when clustering just pyramid contributors labeledby their SCUs.
The algorithm requires a parame-ter that controls the homogeneity of each cluster;we run it over the entire range of settings of thisparameter.
We consider the clustering that maxi-mizes F-measure in order to avoid any uncertaintyregarding optimal parameter selection and to im-plicitly compare our algorithm against an entire hi-erarchy of possible clusterings.8796.4 ResultsTable 1 shows the F1scores over evaluation runsusing the random concept assignment, clusteringand concept graph techniques.
These results areobtained at a mapping threshold of m = 0.5,which implies that we consider a mapping betweena concept and an SCU if their nuggets over com-mon sentences share more than 50% of their wordson average.
The results do not vary significantly atdifferent settings of m.We observe that the concepts extracted by ourgraph-based approach perform significantly betterthan the best-performing clustering configuration.Despite a fairly limited alignment approach thatdoesn?t use synonyms or semantic analysis, theconcept graph outperforms the baselines by nearlyan order of magnitude on each document.
Thisvalidates our initial hypothesis that clustering ap-proaches are not suitable for tackling the redun-dancy problem at the sub-sentential level.7 Conclusions and Future WorkWe have described a graph-based algorithm foridentifying redundancy at the sub-snippet level andshown that it outperforms clustering methods thatare traditionally applied to the redundancy task.Though the algorithm identifies redundancy atthe sub-snippet level, redundancy can be decreasedby dropping only entirely redundant snippets.
Wehope to be able to overcome this limitation byextending this information-preserving approach tothe synthesis of new non-redundant snippets whichminimize redundant content in the document.In addition, this work currently assumes that re-dundancy is bidirectional; however, we intend toalso address the case of unidirectional redundancyby considering entailment recognition approaches.AcknowledgementsWe are grateful to Andrew Rosenberg, David El-son, Mayank Lahiri and the anonymous review-ers for their useful feedback.
This material isbased upon work supported by the Defense Ad-vanced Research Projects Agency under ContractNo.
HR0011-06-C-0023.ReferencesBarzilay, Regina and Lillian Lee.
2003.
Learn-ing to paraphrase: an unsupervised approach us-ing multiple-sequence alignment.
In Proceedings ofHLT-NAACL, pages 16?23.Barzilay, Regina and Kathleen R. McKeown.
2005.Sentence fusion for multidocument news summa-rization.
Computational Linguistics, 31(3):297?328.Carbonell, Jaime G. and Jade Goldstein.
1998.
Theuse of MMR, diversity-based reranking for reorder-ing documents and producing summaries.
In Pro-ceedings of ACM-SIGIR, pages 335?336.Filatova, Elena and Vasileios Hatzivassiloglou.
2004.A formal model for information selection in multi-sentence text extraction.
In Proceedings of COL-ING, page 397.Hochbaum, Dorit S. 1997.
Approximating coveringand packing problems: set cover, vertex cover, in-dependent set, and related problems.
In Approxi-mation algorithms for NP-hard problems, pages 94?143.
PWS Publishing Co., Boston, MA, USA.Hovy, Eduard, Chin-Yew Lin, Liang Zhou, and JunichiFukumoto.
2006.
Automated summarization evalu-ation with basic elements.
In Proceedings of LREC.Lin, Chin-Yew and Eduard Hovy.
2001.
From singleto multi-document summarization: a prototype sys-tem and its evaluation.
In Proceedings of ACL, pages457?464.Lin, Dekang.
1998.
Dependency-based evaluation ofMINIPAR.
In Proceedings of the Workshop on theEvaluation of Parsing Systems, LREC.Marsi, Erwin, Emiel Krahmer, Wauter Bosma, and Ma-riet Theune.
2006.
Normalized alignment of depen-dency trees for detecting textual entailment.
In Sec-ond PASCAL Recognising Textual Entailment Chal-lenge, pages 56?61.Nenkova, Ani, Rebecca Passonneau, and KathleenMcKeown.
2007.
The pyramid method: Incorporat-ing human content selection variation in summariza-tion evaluation.
ACM Transactions on Speech andLanguage Processing, 4(2):4.Pang, Bo, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations: ex-tracting paraphrases and generating new sentences.In Proceedings of HLT-NAACL, pages 102?109.Rosenberg, Andrew and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clus-ter evaluation measure.
In Proceedings of EMNLP,pages 410?420.Shi, Jianbo and Jitendra Malik.
2000.
Normalized cutsand image segmentation.
IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 22(8):888?905.Siddharthan, Advaith, Ani Nenkova, and KathleenMcKeown.
2004.
Syntactic simplification for im-proving content selection in multi-document summa-rization.
In Proceedings of COLING, page 896.880
