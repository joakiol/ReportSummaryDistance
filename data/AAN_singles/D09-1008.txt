Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 72?80,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPEffective Use of Linguistic and Contextual Informationfor Statistical Machine TranslationLibin Shen and Jinxi Xu and Bing Zhang andSpyros Matsoukas and Ralph WeischedelBBN TechnologiesCambridge, MA 02138, USA{lshen,jxu,bzhang,smatsouk,weisched}@bbn.comAbstractCurrent methods of using lexical featuresin machine translation have difficulty inscaling up to realistic MT tasks due toa prohibitively large number of parame-ters involved.
In this paper, we proposemethods of using new linguistic and con-textual features that do not suffer fromthis problem and apply them in a state-of-the-art hierarchical MT system.
The fea-tures used in this work are non-terminallabels, non-terminal length distribution,source string context and source depen-dency LM scores.
The effectiveness ofour techniques is demonstrated by signif-icant improvements over a strong base-line.
On Arabic-to-English translation,improvements in lower-cased BLEU are2.0 on NIST MT06 and 1.7 on MT08newswire data on decoding output.
OnChinese-to-English translation, the im-provements are 1.0 on MT06 and 0.8 onMT08 newswire data.1 IntroductionLinguistic and context features, especially sparselexical features, have been widely used in re-cent machine translation (MT) research.
Unfor-tunately, existing methods of using such featuresare not ideal for large-scale, practical translationtasks.In this paper, we will propose several prob-abilistic models to effectively exploit linguisticand contextual information for MT decoding, andthese new features do not suffer from the scalabil-ity problem.
Our new models are tested on NISTMT06 and MT08 data, and they provide signifi-cant improvement over a strong baseline system.1.1 Previous WorkThe ideas of using labels, length preference andsource side context in MT decoding were exploredpreviously.
Broadly speaking, two approacheswere commonly used in existing work.One is to use a stochastic gradient descent(SGD) or Perceptron like online learning algo-rithm to optimize the weights of these featuresdirectly for MT (Shen et al, 2004; Liang et al,2006; Tillmann and Zhang, 2006).
This method isvery attractive, since it opens the door to rich lex-ical features.
However, in order to robustly opti-mize the feature weights, one has to use a substan-tially large development set, which results in sig-nificantly slower tuning.
Alternatively, one needsto carefully select a development set that simulatesthe test set to reduce the risk of over-fitting, whichhowever is not always realistic for practical use.A remedy is to aggressively limit the featurespace, e.g.
to syntactic labels or a small fractionof the bi-lingual features available, as in (Chianget al, 2008; Chiang et al, 2009), but that reducesthe benefit of lexical features.
A possible genericsolution is to cluster the lexical features in someway.
However, how to make it work on such alarge space of bi-lingual features is still an openquestion.The other approach is to estimate a single scoreor likelihood of a translation with rich features,for example, with the maximum entropy (Max-Ent) method as in (Carpuat and Wu, 2007; Itty-cheriah and Roukos, 2007; He et al, 2008).
Thismethod avoids the over-fitting problem, at the ex-pense of losing the benefit of discriminative train-ing of rich features directly for MT.
However, thefeature space problem still exists in these pub-lished models.He et al (2008) extended the WSD-like ap-proached proposed in (Carpuat and Wu, 2007) tohierarchical decoders.
In (He et al, 2008), lexical72features were limited on each single side due to thefeature space problem.
In order to further reducethe complexity of MaxEnt training, they ?traineda MaxEnt model for each ambiguous hierarchicalLHS?
(left-hand side or source side) of translationrules.
Different target sides were treated as possi-ble labels.
Therefore, the sample sets of each indi-vidual MaxEnt model were very small, while thenumber of features could easily exceed the numberof samples.
Furthermore, optimizing individualMaxEnt models in this way does not lead to globalmaximum.
In addition, MaxEnt models trained onsmall sets are unstable.The MaxEnt model in (Ittycheriah and Roukos,2007) was optimized globally, so that it could bet-ter employ the distribution of the training data.However, one has to filter the training data ac-cording to the test data to get competitive perfor-mance with this model 1.
In addition, the filteringmethod causes some practical issues.
First, suchmethods are not suitable for real MT tasks, espe-cially for applications with streamed input, sincethe model has to be retrained with each new inputsentence or document and training is slow.
Fur-thermore, the model is ill-posed.
The translationof a source sentence depends on other source sen-tences in the same batch with which the MaxEntmodel is trained.
If we add one more sentence tothe batch, translations of other sentences may be-come different due to the change of the MaxEntmodel.To sum up, the existing models of employingrich bi-lingual lexical information in MT are im-perfect.
Many of them are not ideal for practicaltranslation tasks.1.2 Our ApproachAs for our approach, we mainly use simple proba-bilistic models, i.e.
Gaussian and n-gram models,which are more robust and suitable for large-scaletraining of real data, as manifested in state-of-the-art systems of speech recognition.
The uniquecontribution of our work is to design effective andefficient statistical models to capture useful lin-guistic and context information for MT decoding.Feature functions defined in this way are robustand ideal for practical translation tasks.1According to footnote 2 of (Ittycheriah and Roukos,2007), test set adaptation by test set sampling of the train-ing corpus showed an advantage of more than 2 BLEU pointsover a general system trained on all data.1.2.1 FeaturesIn this paper, we will introduce four new linguisticand contextual feature functions.
Here, we firstprovide a high-level description of these features.Details of the features are discussed in Section 2.The first feature is based on non-terminal labels,i.e.
POS tags of the head words of target non-terminals in transfer rules.
This feature reducesthe ambiguity of translation rules.
The other bene-fit is that POS tags help to weed out bad target sidetree structures, as an enhancement to the target de-pendency language model.The second feature is based on the length dis-tribution of non-terminals.
In English as well asin other languages, the same deep structure canbe represented in different syntactic structures de-pending on the complexity of its constituents.
Wemodel such preferences by associating each non-terminal of a transfer rule with a probability distri-bution over its length.
Similar ideas were exploredin (He et al, 2008).
However their length featuresonly provided insignificant improvement of 0.1BLEU point.
A crucial difference of our approachis how the length preference is modeled.
We ap-proximate the length distribution of non-terminalswith a smoothed Gaussian, which is more robustand gives rise to much larger improvement consis-tently.The third feature utilizes source side context in-formation, i.e.
the neighboring words of an inputspan, to influence the selection of the target trans-lation for a span.
While the use of context infor-mation has been explored in MT, e.g.
(Carpuatand Wu, 2007) and (He et al, 2008), the specifictechnique we used by means of a context languagemodel is rather different.
Our model is trained onthe whole training data, and it is not limited by theconstraint of MaxEnt training.The fourth feature exploits structural informa-tion on the source side.
Specifically, the decodersimultaneously generates both the source and tar-get side dependency trees, and employs two de-pendency LMs, one for the source and the otherfor the target, for scoring translation hypotheses.Our intuition is that the likelihood of source struc-tures provides another piece of evidence about theplausibility of a translation hypothesis and as suchwould help weed out bad ones.731.2.2 Baseline System and ExperimentalSetupWe take BBN?s HierDec, a string-to-dependencydecoder as described in (Shen et al, 2008), as ourbaseline for the following two reasons:?
It provides a strong baseline, which ensuresthe validity of the improvement we would ob-tain.
The baseline model used in this papershowed state-of-the-art performance at NIST2008 MT evaluation.?
The baseline algorithm can be easily ex-tended to incorporate the features proposedin this paper.
The use of source dependencystructures is a natural extension of the string-to-tree model to a tree-to-tree model.To ensure the generality of our results, we testedthe features on two rather different language pairs,Arabic-to-English and Chinese-to-English, usingtwo metrics, IBM BLEU (Papineni et al, 2001)and TER (Snover et al, 2006).
Our experimentsshow that each of the first three features: non-terminal labels, length distribution and source sidecontext, improves MT performance.
Surprisingly,the source dependency feature does not producean improvement.2 Linguistic and Context Features2.1 Non-terminal LabelsIn the original string-to-dependency model (Shenet al, 2008), a translation rule is composed of astring of words and non-terminals on the sourceside and a well-formed dependency structure onthe target side.
A well-formed dependency struc-ture could be either a single-rooted dependencytree or a set of sibling trees.
As in the Hiero system(Chiang, 2007), there is only one non-terminal Xin the string-to-dependency model.
Any sub de-pendency structure can be used to replace a non-terminal in a rule.For example, we have a source sentence in Chi-nese as follows.?
jiantao zhuyao baohan liang fangmianThe literal translation for individual words is?
?review?
?mainly?
?to consist of?
?two?
?part?The reference translation is?
the review mainly consists of two partsA single source word can be translated intomany English words.
For example, jiantao canbe translated into a review, the review, reviews,the reviews, reviewing, reviewed, etc.
Supposewe have source-string-to-target-dependency trans-lation rules as shown in Figure 1.
Since there isno constraint on substitution, any translation forjiantao could replace the X-1 slot.One way to alleviate this problem is to limit thesearch space by using a label system.
We couldassign a label to each non-terminal on the targetside of the rules.
Furthermore, we could assign alabel to the whole target dependency structure, asshown in Figure 2.
In decoding, each target de-pendency sub-structure would be associated witha label.
Whenever substitution happens, we wouldcheck whether the label of the sub-structure andthe label of the slot are the same.
Substitutionswith unmatched labels would be prohibited.In practice, we use a soft constraint by penaliz-ing substitutions with unmatched labels.
We intro-duce a new feature: the number of times substitu-tions with unmatched labels appear in the deriva-tion of a translation hypothesis.Obviously, to implement this feature we need toassociate a label with each non-terminal in the tar-get side of a translation rule.
The labels are gen-erated during rule extraction.
When we create arule from a training example, we replace a sub-tree or dependency structure with a non-terminaland associate it with the POS tag of the head wordif the non-terminal corresponds to a single-rootedtree on the target side.
Otherwise, it is assignedthe generic label X.
(In decoding, all substitutionsof X are considered unmatched ones and incur apenalty.
)2.2 Length DistributionIn English, the length of a phrase may determinethe syntactic structure of a sentence.
For example,possessive relations can be represented either as?A?s B?
or ?B of A?.
The former is preferred if Ais a short phrase (e.g.
?the boy?s mother?)
whilethe latter is preferred if A is a complex structure(e.g.
?the mother of the boy who is sick?
).Our solution is to build a model of length prefer-ence for each non-terminal in each translation rule.To address data sparseness, we assume the lengthdistribution of each non-terminal in a transfer ruleis a Gaussian, whose mean and variance can beestimated from the training data.
In rule extrac-74theXXjiantaoreviewstheXXjiantaoreviewX X?1 baohanXX?1consistsX?2X?2ofmainlyzhuyaoFigure 1: Translation rules with one label XtheX jiantaoreviewstheX jiantaoreviewX X?1 baohanconsistsNNS VBZNNNN?1 NNS?2X?2ofmainlyzhuyaoFigure 2: Translation rules with multiple labelstion, each time a translation rule is generated froma training example, we can record the length of thesource span corresponding to a non-terminal.
Inthe end, we have a frequency histogram for eachnon-terminal in each translation rule.
From thehistogram, a Gaussian distribution can be easilycomputed.In practice, we do not need to collect the fre-quency histogram.
Since all we need to know arethe mean and the variance, it is sufficient to col-lect the sum of the length and the sum of squaredlength.Let r be a translation rule that occurs Nrtimesin training.
Let x be a specific non-terminal in thatrule.
Let l(r, x, i) denote the length of the sourcespan corresponding to non-terminal x in the i-thoccurrence of rule r in training.
Then, we cancompute the following quantities.mr,x=1NrNr?i=1l(r, x, i) (1)sr,x=1NrNr?i=1l(r, x, i)2, (2)which can be subsequently used to estimate themean ?r,xand variance ?2r,xof x?s length distri-bution in rule r as follows.
?r,x= mr,x(3)?2r,x= sr,x?m2r,x(4)Since many of the translation rules have few oc-currences in training, smoothing of the above esti-mates is necessary.
A common smoothing methodis based on maximum a posteriori (MAP) estima-tion as in (Gauvain and Lee, 1994).m?r,x=NrNr+ ?mr,x+?Nr+ ?m?r,xs?r,x=NrNr+ ?sr,x+?Nr+ ?s?r,x,where ?
stands for an MAP distribution and ?
rep-resents a prior distribution.
m?r,xand s?r,xcanbe obtained from a prior Gaussian distributionN (?
?r,x, ?
?r,x) via equations (3) and (4), and ?
isa weight of smoothing.There are many ways to approximate the priordistribution.
For example, we can have one priorfor all the non-terminals or one for individual non-terminal type.
In practice, we assume ?
?r,x= ?r,x,and approximate ?
?r,xas (?2r,x+ sr,x)12 .In this way, we do not change the mean, butrelax the variance with sr,x.
We tried differ-ent smoothing methods, but the performance didnot change much, therefore we kept this simplestsetup.
We also tried the Poisson distribution, andthe performance is similar to Gaussian distribu-tion, which is about 0.1 point lower in BLEU.When a rule r is applied during decoding, wecompute a penalty for each non-terminal x in raccording toP (l | r, x) =1?r,x?2pie?(l?
?r,x)22?2r,x,where l is length of source span corresponding tox.Our method to address the problem of lengthbias in rule selection is very different from themaximum entropy method used in existing stud-ies, e.g.
(He et al, 2008).752.3 Context Language ModelIn the baseline string-to-dependency system, theprobability a translation rule is selected in decod-ing does not depend on the sentence context.
Inreality, translation is highly context dependent.
Toaddress this defect, we introduce a new feature,called context language model.
The motivation ofthis feature is to exploit surrounding words to in-fluence the selection of the desired transfer rule fora given input span.To illustrate the problem, we use the same ex-ample mentioned in Section 2.1.
Suppose thesource span for rule selection is zhuyao baohan,whose literal translation is mainly and to consistof.
There are many candidate translations for thisphrase, for example, mainly consist of, mainlyconsists of, mainly including, mainly includes, etc.The surrounding words can help to decide whichtranslation is more appropriate for zhuyao bao-han.
We compare the following two context-basedprobabilities:?
P ( jiantao | mainly consist )?
P ( jiantao | mainly consists )Here, jiantao is the source word preceding thesource span zhuyao baohan.In the training data, jiantao is usually trans-lated into the review, third-person singular, thenthe probability P ( jiantao | mainly consists ) willbe higher than P ( jiantao | mainly consist ), sincewe have seen more context events like the formerin the training data.Now we introduce context LM formally.
Let thesource words be f1f2..fi..fj..fn.
Suppose sourcesub-string fi..fjis translated into ep..eq.
We candefine tri-gram probabilities on the left and rightsides of the source span:?
left : PL(fi?1|ep, ep+1)?
right : PR(fj+1|eq, eq?1)In our implementation, the left and right contextLMs are estimated from the training data as partof the rule extraction procedure.
When we exact arule, we collect two 3-gram events, one for the leftside and the other for the right side.In decoding, whenever a partial hypothesis isgenerated, we calculate the context LM scoresbased on the leftmost two words and the rightmosttwo words of the hypothesis as well as the sourcecontext.
The product of the left and right contextLM scores is used as a new feature in the scoringfunction.Please note that our approach is very differentfrom other approaches to context dependent ruleselection such as (Ittycheriah and Roukos, 2007)and (He et al, 2008).
Instead of using a large num-ber of fine grained features with weights optimizedusing the maximum entropy method, we treat con-text dependency as an ngram LM problem, and itis smoothed with Witten-Bell discounting.
The es-timation of the context LMs is very efficient androbust.The benefit is two fold.
The estimation of thecontext LMs is very efficient.
It adds only one newweight to the scoring function.2.4 Source Dependency Language ModelThe context LM proposed in the previous sec-tion only employs source words immediately be-fore and after the current source span in decod-ing.
To exploit more source context, we use asource side dependency language model as an-other feature.
The motivation is to take advantageof the long distance dependency relations betweensource words in scoring a translation theory.We extended string-to-dependency rules inthe baseline system to dependency-to-dependencyrules.
In each dependency-to-dependency rule, wekeep record of the source string as well as thesource dependency structure.
Figure 3 shows ex-amples of dependency-to-dependency rules.We extended the string-to-dependency decod-ing algorithm in the baseline to accommodatedependency-to-dependency theories.
In decoding,we build both the source and the target depen-dency structures simultaneously in chart parsingover the source string.
Thus, we can compute thesource dependency LM score in the same way wecompute the target side score, using a proceduredescribed in (Shen et al, 2008).We introduce two new features for the sourceside dependency LM as follows, in a way similarto the target side.?
Source dependency LM score?
Discount on ill-formed source dependencystructuresThe source dependency LM is trained on thesource side of the bi-lingual training data withWitten-Bell smoothing.
The source dependencyLM score represents the likelihood of the source76X?1 X?2zhuyaobaohantheXXjiantaoreviewstheXXjiantaoreviewXX X?1consistsX?2ofmainlyFigure 3: Dependency-to-dependency translation rulesdependency tree generated by the decoder.
Thesource dependency tree with the highest score isthe one that is most likely to be generated by thedependency model that created the source side ofthe training data.Source dependency trees are composed of frag-ments embedded in the translation rules.
There-fore, a source dependency LM score can beviewed as a measure whether the translation rulesare put together in a way similar to the trainingdata.
Therefore, a source dependency LM scoreserves as a feature to represent structural con-text information that is capable of modeling long-distance relations.However, unlike source context LMs, the struc-tural context information is used only when twopartial dependency structures are combined, whilesource context LMs work as a look-ahead feature.3 ExperimentsWe designed our experiments to show the impactof each feature separately as well as their cumula-tive impact:?
BASE: baseline string-to-dependency system?
SLM: baseline + source dependency LM?
CLM: baseline + context LM?
LEN: baseline + length distribution?
LBL: baseline + syntactic labels?
LBL+LEN: baseline + syntactic labels +length distribution?
LBL+LEN+CLM: baseline + syntactic labels+ length distribution + context LMAll the models were optimized on lower-casedIBM BLEU with Powell?s method (Powell, 1964;Brent, 1973) on n-best translations (Ostendorf etal., 1991), but evaluated on both IBM BLEU andTER.
The motivation is to detect if an improve-ment is artificial, i.e., specific to the tuning met-ric.
For both Arabic-to-English and Chinese-to-English MT, we tuned on NIST MT02-05 andtested on MT06 and MT08 newswire sets.The training data are different from what wasusd at MT06 or MT08.
Our Arabic-to-Englishdata contain 29M Arabic words and 38M En-glish words from 11 corpora: LDC2004T17,LDC2004T18, LDC2005E46, LDC2006E25,LDC2006G05, LDC2005E85, LDC2006E36,LDC2006E82, LDC2006E95, Sakhr-A2E andSakhr-E2A.
The Chinese-to-English data contain107M Chinese words and 132M English wordsfrom eight corpora: LDC2002E18, LDC2005T06,LDC2005T10, LDC2006E26, LDC2006G05,LDC2002L27, LDC2005T34 and LDC2003E07.They are available under the DARPA GALEprogram.
Traditional 3-gram and 5-gram stringLMs were trained on the English side of theparallel data plus the English Gigaword corpusV3.0 in a way described in (Bulyko et al, 2007).The target dependency LMs were trained on theEnglish side of the parallel training data.
For thatpurpose, we parsed the English side of the paralleldata.
Two separate models were trained: one forArabic from the Arabic training data and the otherfor Chinese from the Chinese training data.To compute the source dependency LM forChinese-to-English MT, we parsed the Chineseside of the Chinese-to-English parallel data.
Dueto the lack of a good Arabic parser compatiblewith the Sakhr tokenization that we used on thesource side, we did not test the source dependencyLM for Arabic-to-English MT.When extracting rules with source dependencystructures, we applied the same well-formednessconstraint on the source side as we did on the tar-get side, using a procedure described by (Shenet al, 2008).
Some candidate rules were thrownaway due to the source side constraint.
On the77ModelMT06 MT08BLEU TER BLEU TERlower mixed lower mixed lower mixed lower mixedDecoding (3-gram LM)BASE 48.75 46.74 43.43 45.79 49.58 47.46 42.80 45.08CLM 49.44 47.36 42.96 45.22 49.73 47.53 42.64 44.92LEN 49.37 47.28 43.01 45.35 50.29 48.19 42.32 44.45LBL 49.33 47.07 43.09 45.53 50.46 48.19 42.27 44.57LBL+LEN 49.91 47.70 42.59 45.17 51.10 48.85 41.88 44.16LBL+LEN+CLM 50.75 48.51 42.13 44.50 51.24 49.10 41.63 43.80Rescoring (5-gram LM)BASE 51.24 49.23 42.08 44.42 51.23 49.11 42.01 44.15CLM 51.57 49.54 41.74 43.88 51.44 49.37 41.63 43.74LEN 52.05 50.01 41.50 43.72 51.88 49.89 41.51 43.47LBL 51.80 49.69 41.54 43.76 51.93 49.86 41.27 43.33LBL+LEN 51.90 49.76 41.41 43.70 52.42 50.29 40.93 43.00LBL+LEN+CLM 52.61 50.51 40.77 43.03 52.60 50.56 40.69 42.81Table 1: BLEU and TER percentage scores on MT06 and MT08 Arabic-to-English newswire sets.other hand, one string-to-dependency rule maysplit into several dependency-to-dependency rulesdue to different source dependency structures.
Thesize of the dependency-to-dependency rule set isslightly smaller than the size of the string-to-dependency rule set.Tables 1 and 2 show the BLEU and TER per-centage scores on MT06 and MT08 for Arabic-to-English and Chinese-to-English translation re-spectively.
The context LM feature, the lengthfeature and the syntax label feature all producea small improvement for most of the conditions.When we combined the three features, we ob-served significant improvements over the baseline.For Arabic-to-English MT, the LBL+LEN+CLMsystem improved lower-cased BLEU by 2.0 onMT06 and 1.7 on MT08 on decoding output.For Chinese-to-English MT, the improvements inlower-cased BLEU were 1.0 on MT06 and 0.8 onMT08.
After re-scoring, the improvements be-came smaller, but still noticeable, ranging from 0.7to 1.4.
TER scores were also improved noticeablyfor all conditions, suggesting there was no metricspecific over-tuning.Surprisingly, source dependency LM did notprovide any improvement over the baseline.
Thereare two possible reasons for this.
One is thatthe source and target parse trees were generatedby two stand-alone parsers, which may cause in-compatible structures on the source and targetsides.
By applying the well-formed constraintson both sides, a lot of useful transfer rules arediscarded.
A bi-lingual parser, trained on paral-lel treebanks recently made available to the NLPcommunity, may overcome this problem.
Theother is that the search space of dependency-to-dependency decoding is much larger, since weneed to add source dependency information intothe chart parsing states.
We will explore tech-niques to address these problems in the future.4 DiscussionLinguistic information has been widely used inSMT.
For example, in (Wang et al, 2007), syntac-tic structures were employed to reorder the sourcelanguage as a pre-processing step for phrase-baseddecoding.
In (Koehn and Hoang, 2007), shallowsyntactic analysis such as POS tagging and mor-phological analysis were incorporated in a phrasaldecoder.In ISI?s syntax-based system (Galley et al,2006) and CMU?s Hiero extension (Venugopal etal., 2007), non-terminals in translation rules havelabels, which must be respected by substitutionsduring decoding.
In (Post and Gildea, 2008; Shenet al, 2008), target trees were employed to im-prove the scoring of translation theories.
Mar-ton and Resnik (2008) introduced features definedon constituent labels to improve the Hiero system(Chiang, 2005).
However, due to the limitation ofMER training, only part of the feature space couldused in the system.
This problem was fixed by78ModelMT06 MT08BLEU TER BLEU TERlower mixed lower mixed lower mixed lower mixedDecoding (3-gram LM)BASE 37.44 35.62 54.64 56.47 33.05 31.26 56.79 58.69SLM 37.30 35.48 54.24 55.90 33.03 31.00 56.59 58.46CLM 37.66 35.81 53.45 55.19 32.97 31.01 55.99 57.77LEN 38.09 36.26 53.98 55.81 33.23 31.34 56.51 58.41LBL 38.37 36.53 54.14 55.99 33.25 31.34 56.60 58.49LBL+LEN 38.36 36.59 53.95 55.60 33.72 31.83 56.79 58.65LBL+LEN+CLM 38.41 36.57 53.83 55.70 33.83 31.79 56.55 58.51Rescoring (5-gram LM)BASE 38.91 37.04 53.65 55.45 34.34 32.32 55.60 57.60SLM 38.27 36.38 53.64 55.29 34.25 32.28 55.35 57.21CLM 38.79 36.88 53.09 54.80 35.01 32.98 55.39 57.28LEN 39.22 37.30 53.34 55.06 34.65 32.70 55.61 57.51LBL 39.11 37.30 53.61 55.29 35.02 33.00 55.39 57.48LBL+LEN 38.91 37.17 53.56 55.27 35.03 33.08 55.47 57.46LBL+LEN+CLM 39.58 37.62 53.21 54.94 35.72 33.63 54.88 56.98Table 2: BLEU and TER percentage scores on MT06 and MT08 Chinese-to-English newswire sets.Chiang et al (2008), which used an online learn-ing method (Crammer and Singer, 2003) to handlea large set of features.Most SMT systems assume that translationrules can be applied without paying attention tothe sentence context.
A few studies (Carpuat andWu, 2007; Ittycheriah and Roukos, 2007; He etal., 2008; Hasan et al, 2008) addressed this de-fect by selecting the appropriate translation rulesfor an input span based on its context in the in-put sentence.
The direct translation model in (It-tycheriah and Roukos, 2007) employed syntactic(POS tags) and context information (neighboringwords) within a maximum entropy model to pre-dict the correct transfer rules.
A similar techniquewas applied by He et al (2008) to improve the Hi-ero system.Our model differs from previous work on theway in which linguistic and contextual informa-tion is used.5 Conclusions and Future WorkIn this paper, we proposed four new linguisticand contextual features for hierarchical decoding.The use of non-terminal labels, length distributionand context LM features gave rise to significantimprovement on Arabic-to-English and Chinese-to-English translation on NIST MT06 and MT08newswire data over a state-of-the-art string-to-dependency baseline.
Unlike previous work, weemployed robust probabilistic models to captureuseful linguistic and contextual information.
Ourmethods are more suitable for practical translationtasks.In future, we will continue this work in twodirections.
We will employ a Gaussian modelto unify various linguistic and contextual fea-tures.
We will also improve the dependency-to-dependency method with a better bi-lingual parser.AcknowledgmentsThis work was supported by DARPA/IPTO Con-tract No.
HR0011-06-C-0022 under the GALEprogram.ReferencesR.
P. Brent.
1973.
Algorithms for Minimization With-out Derivatives.
Prentice-Hall.I.
Bulyko, S. Matsoukas, R. Schwartz, L. Nguyen, andJ.
Makhoul.
2007.
Language model adaptation inmachine translation from speech.
In Proceedings ofthe 32nd IEEE International Conference on Acous-tics, Speech, and Signal Processing (ICASSP).M.
Carpuat and D. Wu.
2007.
Context-dependentphrasal translation lexicons for statistical machinetranslation.
In Proceedings of Machine TranslationSummit XI.79D.
Chiang, Y. Marton, and P. Resnik.
2008.
On-line large-margin training of syntactic and structuraltranslation features.
In Proceedings of the 2008Conference of Empirical Methods in Natural Lan-guage Processing.D.
Chiang, K. Knight, and W. Wang.
2009.
11,001new features for statistical machine translation.
InProceedings of the 2009 Human Language Technol-ogy Conference of the North American Chapter ofthe Association for Computational Linguistics.D.
Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedingsof the 43th Annual Meeting of the Association forComputational Linguistics (ACL).D.
Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2).K.
Crammer and Y.
Singer.
2003.
Ultraconservativeonline algorithms for multiclass problems.
Journalof Machine Learning Research, 3:951?991.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,W.
Wang, and I. Thayer.
2006.
Scalable infer-ence and training of context-rich syntactic models.In COLING-ACL ?06: Proceedings of 44th AnnualMeeting of the Association for Computational Lin-guistics and 21st Int.
Conf.
on Computational Lin-guistics.J.-L. Gauvain and Chin-Hui Lee.
1994.
Maximum aposteriori estimation for multivariate gaussian mix-tureobservations of markov chains.
IEEE Transac-tions on Speech and Audio Processing, 2(2).S.
Hasan, J. Ganitkevitch, H. Ney, and J. Andre?s-Ferrer.2008.
Triplet lexicon models for statistical machinetranslation.
In Proceedings of the 2008 Conferenceof Empirical Methods in Natural Language Process-ing.Z.
He, Q. Liu, and S. Lin.
2008.
Improving statisticalmachine translation using lexicalized rule selection.In Proceedings of COLING ?08: The 22nd Int.
Conf.on Computational Linguistics.A.
Ittycheriah and S. Roukos.
2007.
Direct translationmodel 2.
In Proceedings of the 2007 Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics.P.
Koehn and H. Hoang.
2007.
Factored translationmodels.
In Proceedings of the 2007 Conference ofEmpirical Methods in Natural Language Process-ing.P.
Liang, A.
Bouchard-Co?te?, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach to ma-chine translation.
In COLING-ACL ?06: Proceed-ings of 44th Annual Meeting of the Association forComputational Linguistics and 21st Int.
Conf.
onComputational Linguistics.Y.
Marton and P. Resnik.
2008.
Soft syntactic con-straints for hierarchical phrased-based translation.In Proceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics (ACL).M.
Ostendorf, A. Kannan, S. Austin, O. Kimball,R.
Schwartz, and J. R. Rohlicek.
1991.
Integra-tion of diverse recognition methodologies throughreevaluation of nbest sentence hypotheses.
In Pro-ceedings of the DARPA Workshop on Speech andNatural Language.K.
Papineni, S. Roukos, and T. Ward.
2001.
Bleu: amethod for automatic evaluation of machine transla-tion.
IBM Research Report, RC22176.M.
Post and D. Gildea.
2008.
Parsers as languagemodels for statistical machine translation.
In TheEighth Conference of the Association for MachineTranslation in the Americas.M.
J. D. Powell.
1964.
An efficient method for findingthe minimum of a function of several variables with-out calculating derivatives.
The Computer Journal,7(2).L.
Shen, A. Sarkar, and F. J. Och.
2004.
Discriminativereranking for machine translation.
In Proceedings ofthe 2004 Human Language Technology Conferenceof the North American Chapter of the Associationfor Computational Linguistics.L.
Shen, J. Xu, and R. Weischedel.
2008.
A NewString-to-Dependency Machine Translation Algo-rithm with a Target Dependency Language Model.In Proceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics (ACL).M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Proceedings ofAssociation for Machine Translation in the Ameri-cas.C.
Tillmann and T. Zhang.
2006.
A discrimina-tive global training algorithm for statistical mt.
InCOLING-ACL ?06: Proceedings of 44th AnnualMeeting of the Association for Computational Lin-guistics and 21st Int.
Conf.
on Computational Lin-guistics.A.
Venugopal, A. Zollmann, and S. Vogel.
2007.An efficient two-pass approach to synchronous-cfgdriven statistical mt.
In Proceedings of the 2007 Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.C.
Wang, M. Collins, and P. Koehn.
2007.
Chinesesyntactic reordering for statistical machine transla-tion.
In Proceedings of the 2007 Conference of Em-pirical Methods in Natural Language Processing.80
