Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 86?93Manchester, August 2008Toward a cognitive organization for electronic dictionaries, the case forsemantic proxemyBruno Gaume, Karine Duvignau, Laurent Pr?vot, Yann DesalleUniversit?
de Toulouse, CNRS{gaume,duvignau,prevot,desalle}@univ-tlse2.frAbstractWe compare a psycholinguistic approachof mental lexicon organization with a com-putational approach of implicit lexical or-ganization as found in dictionaries.
In thiswork, we associate dictionaries with ?smallworld?
graphs.
This multidisciplinary ap-proach aims at showing that implicit struc-ture of dictionaries, mathematically iden-tified, fits the way young children catego-rize.
These dictionary graphs might there-fore be considered as ?cognitive artifacts?.This shows the importance of semanticproximity both in cognitive and computa-tional organization of verbs lexicon.1 IntroductionAccording to (Dik, 1991) a linguistic theoryshould be compatible with psycholinguistic re-search on language acquisition, treatment, pro-duction, interpretation and memorization of lin-guistic expressions.
We agree with this view andpostulate that elaborating electronic dictionarieson the ground of a linguistic theory, satisfyingDik?s principle, will confer them good ergonomicsthat will increase their usability.
Our approach isto some extent comparable to WordNet initiative(Fellbaum, 1998), in the sense that we are tryingto characterize speakers?
mental lexicon.In this paper, we focus on verb lexical orga-nization through the examination of verbal pivotmetaphorical utterances (VPMU).
Such utterancesinvolve an understudied structural aspect of thelexicon: interdomain co-hyponymy (Duvignau,c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.2002; Duvignau and Gaume, 2008).
In thiscontext, we take semantic proximity as a cen-tral principle for cognitive ergonomics influenc-ing dynamic lexical acquisition and adult lexicalorganization.
VPMU generally consists in substi-tuting elements from different semantic domains.They are usually considered as deviants while theymight constitute a linguistic illustration of the cat-egorial flexibility advocated in (Piaget, 1945; Ny,1979; Hofstadter, 1995).
They might thereforereveal an early lexical structuring mode that mayform a ground for improving electronic dictionar-ies.This paper presents a mathematical method ableto discover the areas in which this structuringmode appears in dictionaries.
Our approach is totake advantage of the mathematical structure of thenetwork generated by verb definitions.
This struc-ture has been mentioned in (Watts and Strogatz,1998), studied for WordNet by (Sigman and Cec-chi, 2002), refined in (Gaume et al, 2002) and ex-ploited in the current proposal.The paper is organized as follows.
The next sec-tion brings evidence of categorization by seman-tic proximity from early lexicon acquisition ex-periments.
Section 3 presents the computationalmodel, hereafter ?proxemy?.
Section 4 details ourwork on lexical graphs while section 5 comparesthe results of experimental studies with those ofthe computational model.2 Toward a categorization by semanticproximity: evidences from early lexiconacquisitionIn order to show the importance of semantic ap-proximation, we have chosen to support our claimwith productions observed at the crucial periodof lexical construction (between 2 and 4 years-of-86age) and to compare these with adult speakers thathave a stabilized lexicon.2.1 Inter-domains vs. intra-domain semanticapproximationsStudies in this field are almost exclusively lim-ited to nominal utterances.
(Duvignau et al,2005) established the existence of the productionof metaphor-like utterances with a verbal pivot in2-4 years-old children and proposed to considerthem, at this stage of language development, as se-mantic approximations and not as mistakes or truemetaphors.
Duvignau distinguished two kinds ofsemantic approximations: Inter-domains proxim-ity and intra domain proximity between verbs (Du-vignau, 2002).- Inter-domains proximity / co-hyponymy be-tween verbs : a ?linguistic approximation?
(1) Elle d?shabille l?orange (She undresses theorange) [Age: 3 years] [movie: a lady peelsan orange]In this category of approximation, the verb usedby the speaker constitutes a reference to a semanticdomain different from the one of element it is com-bined to (?undress?
/ ?orange?).
For this reason, theapproximate character of the verb is understand-able independently of the context of the utterance:detecting the approximation occurs at the linguis-tic level.
We call this type of production ?semanticapproximation?.
They might constitute a metaphoror an ?analogic surextention?.When someone has a conventional verb in themental lexicon (?to peel?)
and use a non conven-tional but relevant verb like ?to undress the orange?for the action [to peel the orange his verbal seman-tic approximation constitutes a metaphor.
On thecontrary when someone does not have a conven-tional verb in the mental lexicon but manages touse a non conventional but relevant verb in saying?to undress?
for this action, his verbal semantic ap-proximation constitutes a ?surextension?
but not anerror because of the lexical relation that links theseverbs.
In fact, according to (Duvignau and Gaume,2008) ?to undress?
and ?to peel?
are related by aninter-domains synonymic relation.- Intra-domain proximity / co-hyponymy be-tween verbs: a ?pragmatic approximation?
Inthis category, illustrated by (2) the approximatecharacter of the verb comes only from a non-correspondence between the verb used and the re-ality it designates.
This happens with utterances inwhich the use of the verbal form does not createany semantic tension within the utterance but des-ignates a way of carrying out an activity that doesnot correspond precisely to the action undertaken.
(2) Elle coupe l?orange (She cuts the or-ange)[age: 3 years][movie: a lady peels anorange]We propose an experimental study of the produc-tion of verbal semantic approximations like (2) or(1) by way of a naming task of 17 action-movieswith young children (from 2 to 4 years old).
Wecompare their performances with adult?s ones.2.2 Experimental DesignIn order to elicit the production of semantic ap-proximations we proposed to all our participantsan action-video naming task.
The population sam-ple consisted of:?
54 non-disturbed children (2-4 years old),monolingual in French?
77 non-disturbed young adults (18-40 yearsold), monolingual in FrenchThe action movies sequences are coming from theApprox protocol (Duvignau et al, 2005).
The ma-terial consists in 17 action-movies sequences de-scribed in table 1.The 17 action movies are presented in randomorder to each participant.
Instructions were givenat the time the action in the movie was completedand its results were visible (e.g when the glass isbroken).
At that moment a question was askedto the participant:?What did the woman do?
(justnow)?2.3 ResultsEach of the children produced between 2 and 5 ap-proximations: ?Elle casse la tomate?
-She breaks atomato ?
[action = to squash], ?Elle ?pluche le bois?- She peels the wood?
[action = to strip the bark offa log].
Globally, children produced semantic ap-proximations for 34 % of the naming tasks, whichwere distributed as follows: 24 % intra-domainsemantic approximations, 10 % inter-domains se-mantic approximations.
They produced them sig-nificantly more frequent than adults : 5 % with4% intra-domain semantic approximations and 1% inter-domains semantic approximations.87Table 1: Approx 17 action moviesThe student Test shows the difference betweenchildren and adults in terms of production of se-mantic approximation is very significant: here p <0, 01 while p < 0, 05 is enough.These results signal the importance of seman-tic approximations and of semantic proximity be-tween verbs in the cognitive organization of verbslexicon.In the rest of the paper we present a computa-tional model of semantic proximity and then com-pare this model with the experimental data ob-tained from the children.3 Proxemy: a computational approachA theory of language useful for computationalwork must account for language statistical regu-larities.
Zipf law (Zipf, 1949) satisfy this obser-vation but provides little insight on lexical struc-tural organization.
More recent graph theory stud-ies (Ferrer-i-Cancho and Sole, 2001; Sigman andCecchi, 2002), capitalizing on results in other sci-entific domains, provided interesting contributionsto the establishment of such a theory of language.All structures discovered in this field research sat-isfy the ?hierarchical small word?
(HSW) defini-tion (see section 3.1).
Our approach takes place inthis general framework.
Our specificities are:?
a new linguistic and psycholinguistic insightthat guides us and help us on our results vali-dation;?
the kind of objects studied (dictionaries);?
our analysis of graph structure resulting ina computational model of semantic proxim-ity among vertices (here vertices are Frenchverbs).The study by (Resnik and Diab, 2000) signaledthat although existing models for verb similarityperformed reasonably well against human judg-ments, none managed to handle certain types ofmetaphorical pairs such as to undress / to peel offthat are nonetheless declared to be rather similarby speakers.
We aim to develop a model address-ing this issue.3.1 Small World NetworksNetworks corresponding to structures found in realworld (henceforth real world networks) are sparse:in a graph with n nodes, the maximum num-ber of possible edges is O(n2) while the numberof edges in real networks is generally inferior toO(nlog(n)).
Watts and Strogatz (Watts and Stro-gatz, 1998) proposed two indicators to characterizea large sparse graph G:?
L : the characteristic path length, i.e the meanof the shortest path between two nodes of G?
C : the clustering coefficient, C ?
[0, 1], itmeasures the graph tendency to host zonesvery dense in edges.
(The more clustered thegraph is, the more the graph?s C approaches1, whereas in random graphs C is very closeto 0).In applying these criteria to different types ofgraphs, Watts and Strogatz found that:?
real world networks have a tendency to havea small L: generally there is at least one shortpath between any two nodes ;?
real world networks have a tendency to havea large C: this reflects a relative tendency fortwo neighbors on the same node to be inter-connected;?
random graphs have a small L: If someonebuilds a graph randomly with a density ofedges comparable to real world networks, itwill obtain graphs with a small L;?
random graphs have a small C: They are notcomposed of aggregates.
In a random graph88there is no reason why neighbors of a samenode are more likely to be connected than anytwo other nodes, hence their poor tendency toform aggregates.Watts and Strogatz proposed to call the graphshaving these two characteristics (a small L anda large C) small worlds (SW).
They recognizedthese SW in all the real world networks they ob-served, and therefore postulated for being a SWwas an universal property of real world networks.A complete presentation of Small Words can befound, for example, in (Newman, 2003).More recent research has shown that most SWalso have a hierarchical structure (hereafter hier-archical small worlds, HSW ).
The distributionof the vertices incidence degrees follows a powerlaw.
The probability P (k) that a given node has kneighbors decreases as a power law, P (k) ?
k?
?,where ?
is a constant characteristic of the graph(Barab?si and Albert, 1999), while random graphsconforms to a Poisson Law.In the next section, we present ?proxemy?, a se-mantic proximity measure based on a distance wedefine.
A interesting particularity of this distanceis to calculate the distance between two vertices onthe ground of the complete graph, and not only ontheir direct neighbors.3.2 The mathematical modelPROX (PROXemy) is a stochastic method de-signed for studying ?Hierarchical Small Worlds?.1This method takes graph as input and transformthem in a Markov chain whose states are graph ver-tices.
Metaphorically, energy particles wander ran-domly from vertex to vertex through the edges ofthe graph.
It is their trajectory dynamics that giveus the structural properties of the graph.PROX takes a graph in input and output a simi-larity measure between the vertices of the graph.Our problem is therefore the opposite than theone of Pathfinder networks (PFNETs see (Schvan-eveldt et al, 1988)).
PFNETs take a full proximitymatrix in input and output a sparse graph.
Theirgoal is to minimize the number of edges requiredin the sparse graph to be able to approximate thefull distance matrix corresponding to the initial fullproximity matrix.1In this paper we will use the term ?proxemy?
to refer to theobtained by PROX algorithm.
It corresponds to some kind ofsemantic proximity.PROX build a similarity measure between thevertices.
The hypothesis is that areas having ahigh density in edges (hereafter, these areas willbe called aggregates) correspond to closely relatedverb meanings (in a graph of verbs).Given a graph with n vertices, G = (V,E), wewill note [G] the matrix n ?
n such that ?r, s ?V , [G]r,s= 0 if {r, s} 6?
E and 1 otherwise.
[G]is called the adjacency matrix of G.Given G = (V,E) a reflexive graph with n ver-tices.
[?G] is a n ?
n matrix defined by ?r, s ?V , [?G]r,s=[G]r,s?x?V{[G]r,x}.
[?G] is the Markovianmatrix of G.[?G] is the n ?
n matrix is a transition matrix ofthe homogeneous Markov chain whose states arethe vertices of the graph such that the probabilityof going from one vertex r ?
V at an instant t ontoanother s ?
V at the instant t+ 1 is equal to:?
0 if {r, s} 6?
E (s is not neighbor of r)?
1/D if {r, s} ?
E and r has D neighbors (sis a neighbor of r)2Given G = (V,E) a reflexive graph with n ver-tices and [?G] its Markovian matrix, ?r, s ?
V,?t ?N?, PROX(G, t, r, s) = [?Gt]r,sPROX(G, t, r, s) is therefore the probabilityfor a particle departing from r at the instant zeroto be on s at the instant t.Therefore when, PROX(G, t, r, s) >PROX(G, t, r, u), the particle has more proba-bility to be, at instant t on s than on u and it isgraph structure that determine these probabilities.For the rest of this paper we will set the valueof t to 4 since L is less than 4 in the kind ofgraph we are concerned with.
Therefore, we takeinto account the global graph simply by calculatingPROX(G; 4; r; s).Now we have defined our model we will presentlexical graphs on which we apply it.4 Lexical graphsSeveral types of lexical graphs can be built accord-ing to the type of the semantic relation used fordefining the graph?s edges.
The two principal typesof relations used are:2In the context of this presentation of the model we do notconsider weighted graphs.
However when building the graphswe do consider information, such as the position of the wordin the definition, for giving weight to the edges.s89?
Syntagmatic relationships, like co-occurrencerelationships: they define edges betweennodes corresponding to words found near toeach other in a corpus.?
Paradigmatic relationships, like synonymy:they define, on the ground of lexical databasessuch as WordNet (Fellbaum, 1998), edges be-tween nodes of words being in a synonymyrelationship in such resource.Moreover, we are interested into less spe-cific relations, called semantic proximity relationsor semantic relatedness, and which covers bothparadigmatic and syntagmatic dimensions.4.1 Dictionary graphsMeaning in dictionary definition is at least partiallybrought by the relations they create between thewords constituting the entries.
Our approach con-sists in exploiting the small word properties of thegraphs corresponding to dictionaries.
More pre-cisely, we are taking advantage of our hypothe-sis that aggregates correspond to areas of closelyrelated senses.
We illustrate our approach ontwo kinds of dictionary, two traditional dictionar-ies, Le Grand Robert3and TLFi4, and an syn-onym dictionary (Dicosyn) made of compilationof synonym relations extracted from seven otherdictionaries (Bailly, Benac, Du Chazaud, Guizot,Lafaye, Larousse et Robert).5We create a graph from a dictionary in the fol-lowing way.
The entries constituted the vertices.Edges between two vertices A and B were addedif and only if B appears in A?s lemmatized defini-tion6as illustrated in Figure 4.1We proceed in this way for each entry and ob-tained a graph of the dictionary.
By extracting thesubgraph composed only of verbs, the ?neighbor-hood?
we get for the verb ??corcer?
is illustratedby Figure 4.1.
Then we render the graph sym-metric and reflexive.
These modifications on thegraphs are allowed thanks to its paradigmatic na-ture.
Graphs created in this way are typical small3A significant amount of work has been done to encode?Le Grand Robert in a graph.4We would like to thank ATILF for making the TLFi re-source available to us.5Dicosyn has been first realized at ATILF (Analyseet Traitement Informatique de la Langue Fran?aise),before being corrected at CRISCO laboratory(http://elsap1.unicaen.fr/dicosyn.html).6Lemmatization has been realizedwith TreeTagger (http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/).Figure 1: Sub-graph near ?
?corcer (to bark ?
atree?)?
from Le Grand Robertworld network.
For example, DicoSyn-Verb has9043 vertices and 50948 edges, its L is 4,1694 andits C 0,3186.Figure 2: Sub-graph of the verbs near ?
?corcer?from Robert(Duvignau, 2002) has shown that co-hyponymyverb lexical organization according fits with apower law distribution of incidence degrees.
In ouropinion, (i) the hierarchical organization of dictio-naries is a consequence of the special role of thehypernymy relation together with the polysemy ofsome specific vertices; (ii) the strong C reflectsthe role of interdomain co-hyponyny (Duvignau,2002; Duvignau and Gaume, 2003).
For example,in French language, ?casser (to break)?
appears inmany definitions: ?
?mietter (to crumble)?, ?frag-menter (to fragment)?, ?d?t?riorer (to damage)?,?r?voquer (to dismiss)?, ?abroger (to abrogate)?.This results in a very high incidence for the vertex?casser (to break)?.
Moreover, many triangles ex-ist ( {casser, ?mietter, fragmenter}, {casser, r?vo-quer, abroger}...,) and they help to create aggre-gates.
These areas that are bringing co-hyponymscloser in the resulting graph.4.2 Disambiguization for creation dictionarygraphsWord Sense Disambiguation is a general issue fornatural language processing that we need to ad-dress when we build our graphs.
We need todisambiguate the verbs we found in the defini-tion facing a similar problem as (Harabagiu et90al., 1999).
For example, in French dictionary LeGrand Robert, there are two distinct entries for theverb ?causer?
: to cause (3) and to chat (4).
(3) CAUSER-1: ?tre la cause de.
(to be thecause of)(4) CAUSER-2: S?entretenir famili?rementavec qqn.
to chat withOf course, the word ?causer?
may appear in otherdefinitions like ?bavarder?
(to chat) .
Althougha French speaker knows that the ?causer?
in (5)refers to the definition (4) our system for buildingthe graph cannot disambiguate.
The solution wepropose is to (i) first create a fictive vertex whichis not a dictionary entry and then (ii) adds twoedges {CAUSER, CAUSER-1} and {CAUSER,CAUSER-2 }.
When ?causer?
is found in anotherdefinition like (5), we add the edge { BAVARDER,CAUSER } as illustrated in Figure (5).
(5) BAVARDER ?Parler beaucoup, longtempsou parler ensemble de choses superfi-cielles.
- Parler; babiller, bavasser (fam.
),cailleter, caqueter, causer, discourir, dis-cuter, jaboter, jacasser, jaser, jaspiner (ar-got), lantiponner (vx), papoter, potiner.Bavarder avec qqn ... ?Figure 3: Disambiguation: ?Causer?, fictive verticeIn Figure (5), many edges are hidden for clarityreasons.
Dashed edges ({Discuter, Causer2}) re-sult from the fact ?Discuter?
and ?Parler?
are in thedefinition of ?Causer-2?.At this stage, we apply PROX to such graph asthe one Figure (5) in order to get a matrix [?G4]as defined in section 3.2.
[?G4]bavarder,causer?1<[?G4]bavarder,causer?2.
This comparison allows usto disambiguate.More generally, let suppose we found a wordwith k entries in a definition, we will then haveS1, .
.
.
, Skvertices corresponding to the entries afictive vertex S. In case there is an edge {A,S}it is replaced by {A,Si} where Siis such that[?G4]A,Si= MAX0<i?k{?G4]A,Si}.
Then we re-move all fictive vertices from the graph to get adisambiguated graph.We can then apply PROX a last time on thedisambiguated graph in order to get the closestword of a word according to our proxemy mea-sure.
For example, the PROX-closest words of?corcer (to bark ?a tree?
), calculated with t =6 are: 1 ECORCER (to bark), 2 D?POUILLER(strip), 3 PELER (peel), 4 TONDRE (mow, shear),5 ?TER (remove), 6 ?PLUCHER (peel, pare),7 RASER (shave), 8 D?MUNIR (divest), 9 D?-CORTIQUER (decorticate), 10 ?GORGER (slitthe throat of), 11 ?CORCHER (skin), 12 ?CALER(husk), 13 VOLER (steal), 14 TAILLER (prune), 15R?PER (grate), 16 PLUMER (pluck), 17 GRAT-TER (scrape), 18 ENLEVER (remove), 19 D?-SOSSER (bone), 20 D?POSS?DER (dispossess),21 COUPER (cut), 22 BRETAUDER (shear slop-pily), 23 INCISER (incise), 24 GEMMER (tap), 25D?MASCLER (remove first layer of cork)75 Proxemy and Experimental studiesProx is a robust method: changing randomly afew edges does not change significantly the results.The repartition of aggregates is not strongly af-fected by a random redistribution of some edges.However the relevance of our proxemy approachof lexical networks is tied to the linguistic rep-resentativity of the networks we use.
Therefore,we tested the PROX model of four different dictio-nary graphs and we compared them to the psycho-linguistic experimental results presented in section2.
The graph we compared were:1.
Graph.TLFI.Verb, a graph built as explainedin 4.1 from TLFi8dictionary,2.
Graph.Robert.Verb, a graph built as explainedin 4.1 from Le Grand Robert dictionary,3.
Graph.DicoSyn.Verb, in which there is a edgebetween two verbs if there are given as syn-7Proposing a translation for such fine grained and some-times polysemous words is impossible since proposing thetranslation include a certain form of disambiguisation as it issuggested by the work of (Gale et al, 1992).8http://atilf.atilf.fr/tlf.htm91onyms by one of the synonym dictionarycomposing DicoSyn4.
Graph.DicoSyn_20 built fromGraph.DicoSyn but in which 20% of theedges are randomly removed and re-added.For each of these graphs we looked at two vari-ables to be related with the psycho-linguistics ex-periments: the answers incidence and the proxim-ity of answers to a ?reference verb?Answers incidence We compare in the graph theaverage incidence degree between adult (IDadult)and children answers (IDchildren).Table 2: Results for ?Answers incidence?The proximity of answers to a ?reference verb?Three linguist judges determined together for eachmovie which was the most appropriate verb to de-scribe the action performed in the movie (hereafterRiis the reference verb for the movie Mi).
Fora given movie Mi, an answer may therefore beranked according to its proxemy according to Ri.For a lexical graph G = (V,E) composed of nwords, and for a reference verb Ri?
V , one candefine rankRifor ranking all the vertices of V indecreasing order resulting from a PROX iterationPROX(G, t,Ri, ?)
on V (see section 3.2).Table 3: Proximity between answers and referenceverbOur first hypothesis was that IDadult<IDchildren.
According to the hypothesis childrenwould learn first words corresponding to high in-cidence vertices.
Then they would use them fortalking about an large lexical area (e.g ?casser?
(tobreak) is used by children while adults use a moreprecise verb like ?d?chirer?
(to tear) which has alower incidence in dictionary graphs).Our second hypothesis was that the mean of therank of the children answers according to the ref-erence verb is higher that the adult ones.
When achild is attempting to communicate an event (e.gd?chirer un livre, to tear a book) for which he doesnot have an already constituted verbal category, hewould do an analogy with a past event (e.g to breaka glass) and use this verb for describing the cur-rent event (e.g casser un livre, to break a book).The adult could use a number of more accurateverbs but their proxemic rank, with regard to thereference verb, is generally lower than the childrenones.The table 2 shows the results concerning an-swers incidence.
Although some variability is ob-served across the graphs, our first hypothesis is val-idated for the 4 graphs.
On the three first graphsthe average incidence of answers is roughly twiceas the adults one.The table 3 illustrates the results concerningproxemic rank of answers according to the ref-erence verb.
Again, in spite of some variabilityacross the graphs our second hypothesis is vali-dated as well.
Moreover, having in mind that thegraph has about 10 000 vertices, we observe thatalthough less close that adults answers, the chil-dren answers remain relatively close to the refer-ence verb according to our proxemic measure.6 ConclusionOur psycholinguistic approach allows us to estab-lish that semantic proximity between verbs play afundamental role during the period of early lexi-cal acquisition.
We signaled the existence in theorganization of the lexicon of a relation of co-hyponymy between verbs.
Based on these firstobservations we consider that productions basedon semantic proximity are particularly interesting:they manifest the existence, at the surface level ofdiscourse, of a lexical relation of inter-domain ?se-mantic proximity?
between verbs not yet consid-ered in linguistics.Moreover we have seen that semantic approxi-mations for verbs appear to fit the proximity valuescalculated by PROX.
On the ground of these firstresults, we postulate that constructing electronicdictionaries on the ground of linguistic theory oflexical semantic organization that fits with earlylexicon acquisition as well with adult lexical orga-nization will provide them interesting ergonomicsproperties.
This should increase their usability and92might be taken into account for normalizing elec-tronic dictionaries.For example, we are developing a ?proxemicelectronic dictionary?
from TLFi.
Such dictionar-ies enable to find an uncommon but precise verblike ?to bark?
by using (i) a common verb like ?toundress?
which is related to ?to bark?
by seman-tic proximity and (ii) a word (e.g ?tree?)
bringing arelevant semantic domain.
Moreover, in the def-inition of ?to bark?
one can find: ?tree?, ?grain??fruit?
which are close from each other accord-ing to PROX ran on nouns.
Finally, when welook for verbs that are close from both ?to un-dress?
and ?tree?, PROX provides the verbs: ?tocut, to ring, to peel, to notch, to bark, to incise,...?which constitute relevant verbs.
Such a dictio-nary can be useful for didactic studies where it cancomplements approaches like and NLP for wordsense desambiguization (Gaume et al, 2004) orde-metaphorization.ReferencesBarab?si, Albert-L?szl?
and R?ka Albert.
1999.
Emer-gence of scaling in random networks.
Science,286:509?512, October.Dik, S. 1991.
Functional grammar.
In Droste, F. andJ.
Joseph., editors, Linguistic theory and grammati-cal description.
Amsterdam : Benjamins.Duvignau, K. and B. Gaume.
2003.
Linguistic, psy-cholinguistic and computational approaches to thelexicon: Contributions to early verb-learning.
Jour-nal of the European Society for the Study of Cogni-tive Systems, 6(1).Duvignau, Karine and Bruno Gaume.
2008.
Betweenwords and world: Verbal ?metaphor?
as semantic orpragmatic approximation?
In Proceedings of In-ternational Conference ?Language, Communicationand Cognition?.Duvignau, K., B. Gaume, and S. Kern.
2005.
Seman-tic approximations intraconcept?avs.
interconcepts inearly verbal lexicon: flexibility against error.
In Pro-ceedings of ELA 2005, Emergence of language abil-ities: ontogeny and phylogeny.Duvignau, K. 2002.
La m?taphore berceau et enfantde la langue.
Ph.D. thesis, Universit?
Toulouse?U Lemirail.Fellbaum, C., editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Ferrer-i-Cancho, Ramon and Ricard V. Sole.
2001.
Thesmall world of human language.
Proceedings of TheRoyal Society of London.
Series B, Biological Sci-ences, 268(1482):2261?2265, November.Gale, W., K. Church, and D. Yarowsky.
1992.
Amethod for disambiguating word senses in a largecorpus.
Computers and the humanities, 26(2):415?439.Gaume, B., K. Duvignau K., O. Gasquet O., and M-D. Gineste.
2002.
Forms of meaning, meaning offorms.
Journal of Experimental and Theoretical Ar-tificial Intelligence, 14:61?74.Gaume, B., N. Hathout, and P. Muller.
2004.
D?sam-biguisation par proximit?
structurelle.
In Proceed-ings of TALN 2004.Harabagiu, Sanda M., George A. Miller, and Dan I.Moldovan.
1999.
Wordnet 2 - a morphologicallyand semantically enhanced resource.
In SIGLEX1999.Hofstadter, D. 1995.
Fluid concepts and creativeanalogies.
New York : Basic Books.Newman, M. 2003.
The structure and function of com-plex networks.Ny, J-F.
Le.
1979.
La s?mantique psychologique.
PUF.Piaget, J.
1945.
La formation du symbole chez l?en-fant,.
Delachaux et Niestl?.Resnik, P. and M. Diab.
2000.
Measuring verb similar-ity.
In Proceedings of the 22nd Annual Meeting ofthe Cognitive Science Society.Schvaneveldt, R. W., D. W D.W Dearholt, and F.TDurso.
1988.
Graph theoric foundations ofpathfinder networks.
Computers and Mathematicswith Applications, 15:337?445.Sigman, M. and G.A.
Cecchi.
2002.
Global organiza-tion of the wordnet lexicon.
Proc.
Natl.
Acad.
Sci.,99(3):1741?1747.Watts, D.J.
and S.H.
Strogatz.
1998.
Collective dynam-ics of small-world networks.
Nature, 393:440?442.Zipf, G. K. 1949.
Human behavior and the principleof least effort.
Addison-Wesley.93
