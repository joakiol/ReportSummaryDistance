Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 297?300,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPMulti-Document Summarization using Sentence-based Topic ModelsDingding Wang1Shenghuo Zhu2Tao Li1Yihong Gong21.
School of Computer Science, Florida International University, Miami, FL, 331992.
NEC Laboratories America, Cupertino, CA 95014, USA.
{dwang003,taoli}@cs.fiu.edu {zsh,ygong}@sv.nec-labs.comAbstractMost of the existing multi-documentsummarization methods decompose thedocuments into sentences and workdirectly in the sentence space using aterm-sentence matrix.
However, theknowledge on the document side, i.e.
thetopics embedded in the documents, canhelp the context understanding and guidethe sentence selection in the summariza-tion procedure.
In this paper, we propose anew Bayesian sentence-based topic modelfor summarization by making use of boththe term-document and term-sentenceassociations.
An efficient variationalBayesian algorithm is derived for modelparameter estimation.
Experimentalresults on benchmark data sets show theeffectiveness of the proposed model forthe multi-document summarization task.1 IntroductionWith the continuing growth of online textresources, document summarization has foundwide-ranging applications in information retrievaland web search.
Many multi-document summa-rization methods have been developed to extractthe most important sentences from the documents.These methods usually represent the documentsas term-sentence matrices (where each row rep-resents a sentence and each column represents aterm) or graphs (where each node is a sentenceand each edge represents the pairwise relationshipamong corresponding sentences), and ranks thesentences according to their scores calculated by aset of predefined features, such as term frequency-inverse sentence frequency (TF-ISF) (Radev et al,2004; Lin and Hovy, 2002), sentence or termposition (Yih et al, 2007), and number of key-words (Yih et al, 2007).
Typical existing summa-rization methods include centroid-based methods(e.g., MEAD (Radev et al, 2004)), graph-rankingbased methods (e.g., LexPageRank (Erkan andRadev, 2004)), non-negative matrix factorization(NMF) based methods (e.g., (Lee and Seung,2001)), Conditional random field (CRF) basedsummarization (Shen et al, 2007), and LSA basedmethods (Gong and Liu, 2001).There are two limitations with most of the exist-ing multi-document summarization methods: (1)They work directly in the sentence space and manymethods treat the sentences as independent of eachother.
Although few work tries to analyze thecontext or sequence information of the sentences,the document side knowledge, i.e.
the topics em-bedded in the documents are ignored.
(2) An-other limitation is that the sentence scores calcu-lated from existing methods usually do not havevery clear and rigorous probabilistic interpreta-tions.
Many if not all of the sentence scoresare computed using various heuristics as few re-search efforts have been reported on using genera-tive models for document summarization.In this paper, to address the above issues,we propose a new Bayesian sentence-based topicmodel for multi-document summarization by mak-ing use of both the term-document and term-sentence associations.
Our proposal explicitlymodels the probability distributions of selectingsentences given topics and provides a principledway for the summarization task.
An efficient vari-ational Bayesian algorithm is derived for estimat-ing model parameters.2 Bayesian Sentence-based Topic Models(BSTM)2.1 Model FormulationThe entire document set is denoted by D. For eachdocument d ?
D, we consider its unigram lan-guage model,297p(Wn1|?d) =n?i=1p(Wi|?d),where ?ddenotes the model parameter for docu-ment d, Wn1denotes the sequence of words {Wi?W}ni=1, i.e.
the content of the document.
W is thevocabulary.
As topic models, we further assumethe unigram model as a mixture of several topicunigram models,p(Wi|?d) =?Ti?Tp(Wi|Ti)p(Ti|?d),where T is the set of topics.
Here, we assumethat given a topic, generating words is independentfrom the document, i.e.p(Wi|Ti, ?d) = p(Wi|Ti).Instead of freely choosing topic unigram mod-els, we further assume that topic unigram modelsare mixtures of some existing base unigram mod-els, i.e.p(Wi|Ti) =?s?Sp(Wi|Si= s)p(Si= s|Ti),where S is the set of base unigram models.
Here,we use sentence language models as the base mod-els.
One benefit of this assumption is that eachtopic is represented by meaningful sentences, in-stead of directly by keywords.
Thus we havep(Wi|?d) =?t?T?s?Sp(Wi|Si= s)p(Si= s|Ti= t)p(Ti= t|?d).Here we use parameter Ustfor the probabilityof choosing base model s given topic t, p(Si=s|Ti= t) = Ust, where?sUst= 1.
We useparameters {?d} for the probability of choosingtopic t given document d, where?t?dt= 1.We assume that the parameters of base models,{Bws}, are given, i.e.
p(Wi= w|Si= s) = Bws,where?wBws= 1.
Usually, we obtain Bwsbyempirical distribution words of sentence s.2.2 Parameter EstimationFor summarization task, we concern how to de-scribe each topic with the given sentences.
Thiscan be answered by the parameter of choosingbase model s given topic t, Ust.
Comparing toparameter Ust, we concern less about the topicdistribution of each document, i.e.
?dt.
Thuswe choose Bayesian framework to estimate Ustbymarginalizing ?dt.
To do so, we assume a Dirich-let prior for ?d??
Dir(?
), where vector ?
is ahyperparameter.
Thus the likelihood isf(U;Y) =?d??ip(Yid|?d)pi(?d|?
)d?d= B(?)?D??id[BU?>]Yidid??dk??k?1dkd?.
(1)As Eq.
(1) is intractable, LDA (Blei et al, 2001)applies variational Bayesian, which is to maximizea variational bound of the integrated likelihood.Here we write the variational bound.Definition 1 The variational bound is?f(U,V;Y) =?dB(?+ ?d,?)B(?
)?vkwd(BwvUvk?vk;wd)Ywd?vk;wd(2)where the domain of V isV = {V ?
RD?K+:?kVdk=1}, ?vk;wd= BwvUvkVdk/[BUV>]wd, ?dk=?wvYwd?vk;wd.We have the following proposition.Proposition 1 f(U;Y) ?
supV?V?f(U,V;Y).Actually the optimum of this variational bound isthe same as that obtained variational Bayesian ap-proach.
Due to the space limit, the proof of theproposition is omitted.3 The Iterative AlgorithmThe LDA algorithm (Blei et al, 2001) em-ployed the variational Bayesian paradigm, whichestimates the optimal variation bound for each U.The algorithm requires an internal Expectation-Maximization (EM) procedure to find the optimalvariational bound.
The nested EM slows downthe optimization procedure.
To avoid the internalEM loop, we can directly optimize the variationalbound to obtain the update rules.3.1 Algorithm DerivationFirst, we define the concept of Dirichlet adjust-ment, which is used in the algorithm for vari-ational update rules involving Dirichlet distribu-tion.
Then, we define some notations for the up-date rules.Definition 2 We call vector y of size K is theDirichlet adjustment of vector x of size K with re-spect to Dirichlet distribution DK(?)
ifyk= exp(?
(?k+ xk)??
(?l(?l+ xl))),where ?(?)
is digamma function.
We denote it byy = PD(x;?
).We denote element-wise product of matrix X andmatrix Y by X ?
Y, element-wise division byXY, obtaining Y via normalizing of each columnof X as Y1?
X, and obtaining Y via Dirich-let adjustment PD(?;?)
and normalization of eachrow of X asPD(?;?),2??
, i.e., z = PD((Xd,?)>;?)
andYd,k= zk/?kzk.
The following is the update rulesfor LDA:U1?
B>[YB?U?V>]?V ?
?U (3)VPD(?;?),2?
[YBU?V>]>(BU) ?
?V (4)298Algorithm 1 Iterative AlgorithmInput: Y : term-document matrixB : term-sentence matrixK : the number of latent topicsOutput: U : sentence-topic matrixV : auxiliary document-topic matrix1: Randomly initialize U and V, and normalize them2: repeat3: Update U using Eq.
(3);4: Update V using Eq.
(4);5: Compute?f using Eq.
(2);6: until?f converges.3.2 Algorithm ProcedureThe detail procedure is listed as Algorithm 1.?From the sentence-topic matrix U, we includethe sentence with the highest probability in eachtopic into the summary.4 Relations with Other ModelsIn this section, we discuss the connections anddifferences of our BSTM model with two relatedmodels.Recently, a new language model, factorizationwith sentence bases (FGB) (Wang et al, 2008) isproposed for document clustering and summariza-tion by making use of both term-document matrixY and term-sentence matrix B.
The FGB modelcomputes two matrices U and V by optimizingU,V = argminU,V`(U,V),where`(U,V) = KL(Y?BUV>)?
ln Pr(U,V).Here, Kullback-Leibler divergence is used to mea-sure the difference between the distributions of Yand the estimated BUV>.
Our BSTM is similarto the FGB summarization since they are all basedon sentence-based topic model.
The difference isthat the document-topic allocation V is marginal-ized out in BSTM.
The marginalization increasesthe stability of the estimation of the sentence-topicparameters.
Actually, from the algorithm we cansee that the difference lies in the Dirichlet adjust-ment.
Experimental results show that our BSTMachieves better summarization results than FGBmodel.Our BSTM model is also related to 3-factor non-negative matrix factorization (NMF)model (Ding et al, 2006) where the problem is tosolve U and V by minimizing`F(U,V) = ?Y ?BUV>?2F.
(5)Both BSTM and NMF models are used for solv-ing U and V and have similar multiplicative up-date rules.
Note that if the matrix B is the identitymatrix, Eq.
(5) leads to the derivation of the NMFalgorithm with Frobenius norm in (Lee and Seung,2001).
However, our BSTM model is a generativeprobabilistic model and makes use of Dirichlet ad-justment.
The results obtained in our model haveclear and rigorous probabilistic interpretations thatthe NMF model lacks.
In addition, by marginaliz-ing out V, our BSTM model leads to better sum-marization results.5 Experimental Results5.1 Data SetTo evaluate the summarization results empirically,we use the DUC2002 and DUC2004 data sets,both of which are open benchmark data sets fromDocument Understanding Conference (DUC) forgeneric automatic summarization evaluation.
Ta-ble 1 gives a brief description of the data sets.DUC2002 DUC2004number ofdocument collections 59 50number of documents ?10 10in each collectiondata source TREC TDTsummary length 200 words 665bytesTable 1: Description of the data sets for multi-documentsummarizationSystems ROUGE-1 ROUGE-2 ROUGE-L ROUGE-SUDUC Best 0.49869 0.25229 0.46803 0.28406Random 0.38475 0.11692 0.37218 0.18057Centroid 0.45379 0.19181 0.43237 0.23629LexPageRank 0.47963 0.22949 0.44332 0.26198LSA 0.43078 0.15022 0.40507 0.20226NMF 0.44587 0.16280 0.41513 0.21687KM 0.43156 0.15135 0.40376 0.20144FGB 0.48507 0.24103 0.45080 0.26860BSTM 0.48812 0.24571 0.45516 0.27018Table 2: Overall performance comparison on DUC2002data using ROUGE evaluation methods.Systems ROUGE-1 ROUGE-2 ROUGE-L ROUGE-SUDUC Best 0.38224 0.09216 0.38687 0.13233Random 0.31865 0.06377 0.34521 0.11779Centroid 0.36728 0.07379 0.36182 0.12511LexPageRank 0.37842 0.08572 0.37531 0.13097LSA 0.34145 0.06538 0.34973 0.11946NMF 0.36747 0.07261 0.36749 0.12918KM 0.34872 0.06937 0.35882 0.12115FGB 0.38724 0.08115 0.38423 0.12957BSTM 0.39065 0.09010 0.38799 0.13218Table 3: Overall performance comparison on DUC2004 data usingROUGE evaluation methods.5.2 Implemented SystemsWe implement the following most widely useddocument summarization methods as the base-line systems to compare with our proposed BSTMmethod.
(1) Random: The method selects sen-tences randomly for each document collection.299(2) Centroid: The method applies MEAD algo-rithm (Radev et al, 2004) to extract sentences ac-cording to the following three parameters: cen-troid value, positional value, and first-sentenceoverlap.
(3) LexPageRank: The method first con-structs a sentence connectivity graph based oncosine similarity and then selects important sen-tences based on the concept of eigenvector cen-trality (Erkan and Radev, 2004).
(4) LSA: Themethod performs latent semantic analysis on termsby sentences matrix to select sentences havingthe greatest combined weights across all impor-tant topics (Gong and Liu, 2001).
(5) NMF: Themethod performs non-negative matrix factoriza-tion (NMF) on terms by sentences matrix and thenranks the sentences by their weighted scores (Leeand Seung, 2001).
(6) KM: The method performsK-means algorithm on terms by sentences matrixto cluster the sentences and then chooses the cen-troids for each sentence cluster.
(7) FGB: TheFGB method is proposed in (Wang et al, 2008).5.3 Evaluation MeasuresWe use ROUGE toolkit (version 1.5.5) to measurethe summarization performance, which is widelyapplied by DUC for performance evaluation.
Itmeasures the quality of a summary by counting theunit overlaps between the candidate summary anda set of reference summaries.
The full explanationof the evaluation toolkit can be found in (Lin andE.Hovy, 2003).
In general, the higher the ROUGEscores, the better summarization performance.5.4 Result AnalysisTable 2 and Table 3 show the comparison resultsbetween BSTM and other implemented systems.From the results, we have the follow observa-tions: (1) Random has the worst performance.The results of LSA, KM, and NMF are similarand they are slightly better than those of Random.Note that LSA and NMF provide continuous so-lutions to the same K-means clustering problemwhile LSA relaxes the non-negativity of the clus-ter indicator of K-means and NMF relaxes theorthogonality of the cluster indicator (Ding andHe, 2004; Ding et al, 2005).
Hence all thesethree summarization methods perform clustering-based summarization: they first generate sentenceclusters and then select representative sentencesfrom each sentence cluster.
(2) The Centroid sys-tem outperforms clustering-based summarizationmethods in most cases.
This is mainly becausethe Centroid based algorithm takes into accountpositional value and first-sentence overlap whichare not used in clustering-based summarization.
(3) LexPageRank outperforms Centroid.
This isdue to the fact that LexPageRank ranks the sen-tence using eigenvector centrality which implic-itly accounts for information subsumption amongall sentences (Erkan and Radev, 2004).
(4) FGBperforms better than LexPageRank.
Note thatFGB model makes use of both term-document andterm-sentence matrices.
Our BSTM model outper-forms FGB since the document-topic allocation ismarginalized out in BSTM and the marginaliza-tion increases the stability of the estimation of thesentence-topic parameters.
(5) Our BSTM methodoutperforms all other implemented systems and itsperformance is close to the results of the best teamin the DUC competition.
Note that the good per-formance of the best team in DUC benefits fromtheir preprocessing on the data using deep naturallanguage analysis which is not applied in our im-plemented systems.The experimental results provide strong evi-dence that our BSTM is a viable method for docu-ment summarization.Acknowledgement: The work is partiallysupported by NSF grants IIS-0546280, DMS-0844513 and CCF-0830659.ReferencesD.
M. Blei, A. Y. Ng, and M. I. Jordan.
Latent dirichlet alocation.
In Advancesin Neural Information Processing Systems 14.C.
Ding and X.
He.
K-means clustering and principal component analysis.
InProdeedings of ICML 2004.Chris Ding, Xiaofeng He, and Horst Simon.
2005.
On the equivalence ofnonnegative matrix factorization and spectral clustering.
In Proceedings ofSiam Data Mining.Chris Ding, Tao Li, Wei Peng, and Haesun Park.
2006.
Orthogonal nonneg-ative matrix tri-factorizations for clustering.
In Proceedings of SIGKDD2006.G.
Erkan and D. Radev.
2004.
Lexpagerank: Prestige in multi-document textsummarization.
In Proceedings of EMNLP 2004.Y.
Gong and X. Liu.
2001.
Generic text summarization using relevance mea-sure and latent semantic analysis.
In Proceedings of SIGIR.Daniel D. Lee and H. Sebastian Seung.
Algorithms for non-negative matrixfactorization.
In Advances in Neural Information Processing Systems 13.C-Y.
Lin and E.Hovy.
Automatic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of NLT-NAACL 2003.C-Y.
Lin and E. Hovy.
2002.
From single to multi-document summarization:A prototype system and its evaluation.
In Proceedings of ACL 2002.I.
Mani.
2001.
Automatic summarization.
John Benjamins Publishing Com-pany.D.
Radev, H. Jing, M. Stys, and D. Tam.
2004.
Centroid-based summarizationof multiple documents.
Information Processing and Management, pages919?938.B.
Ricardo and R. Berthier.
1999.
Modern information retrieval.
ACM Press.D.
Shen, J-T. Sun, H. Li, Q. Yang, and Z. Chen.
2007.
Document summariza-tion using conditional random fields.
In Proceedings of IJCAI 2007.Dingding Wang, Shenghuo Zhu, Tao Li, Yun Chi, and Yihong Gong.
2008.Integrating clustering and multi-document summarization to improve doc-ument understanding.
In Proceedings of CIKM 2008.W-T. Yih, J. Goodman, L. Vanderwende, and H. Suzuki.
2007.
Multi-document summarization by maximizing informative content-words.
InProceedings of IJCAI 2007.300
