Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 218?221,Paris, October 2009. c?2009 Association for Computational LinguisticsWide-coverage parsing of speech transcriptsJeroen GeertzenResearch Centre for English & Applied LinguisticsUniversity of Cambridge, UKjg532@cam.ac.ukAbstractThis paper discusses the performancedifference of wide-coverage parsers onsmall-domain speech transcripts.
Twoparsers (C&C CCG and RASP) are testedon the speech transcripts of two differentdomains (parent-child language, and pic-ture descriptions).The performance difference betweenthe domain-independent parsers andtwo domain-trained parsers (MSTParserand MEGRASP) is substantial, with adifference of at least 30 percent pointin accuracy.
Despite this gap, some ofthe grammatical relations can still berecovered reliably.1 IntroductionEven though wide-coverage, domain-independent1 parser systems may performsufficiently well for the task at hand, obtaininghighly accurate parses of sentences in a par-ticular domain usually requires the parser tobe domain-trained.
Training a parser requiresa sufficient amount of labelled data (a goldstandard), something that is only available forvery few domains.
When accurate parses ofsentences in a new domain are desired, thereare several ways to proceed.
Hand labelling alldata in the new domain is a consideration, but isusually unfeasible as manual annotation is a costlyactivity.
Another possibility is to minimise theamount of annotation effort required to achievegood performance by resorting to semi-automaticannotation or domain adaptation methods.
Inany case, dedicated effort is still required toobtain highly accurate parses, even with recent1In this paper, the terms ?wide-coverage?
and ?domain-independent?
are used synonymously.automated domain adaptation methods (Dredzeet al, 2007).Work that requires parsing in a new domain asbasis of further study or as part of a larger nat-ural language processing system usually involvesa domain-independent parser with the expectationthat parses are sufficiently accurate for the specificpurpose.2 For instance, Bos and Markert (2005)use a wide-coverage CCG-parser (Clark and Cur-ran, 2007) to generate semantic representations forrecognising textual entailment.
Geertzen (2009)uses a HPSG-based dependency parser (Boumaet al, 2001) to obtain the semantic content of utter-ances.
And in the study of child language acqui-sition, Buttery and Korhonen (2007) use RASP, awide-coverage dependency parser (Briscoe et al,2006), to look at lexical acquisition.The goal of this paper is to give an indicationof wide-coverage, domain-independent parser per-formance on specific domains.
Additionally, thestudy gives insight into RASP?s performance onCHILDES, allowing to factor in parsing perfor-mance in the syntax-based study of Buttery andKorhonen (2007).2 Parsing speech transcriptsParsing performance of two domain-independentparsers, C&C CCG en RASP, is evaluated on twospeech domains.
The first domain, CHILDES, in-volves parent-child interactions; the second do-main, CCC, involves a picture description task.2.1 Parsing systemsTwo wide-coverage parser systems are used.RASP (Briscoe et al, 2006) is a parsing system for2Without gold standard there is no way of knowing howwell the parser component performs with respect to a de-sired outcome of syntactic structure.
This may not neces-sarily be a problem, as parsing in such cases is paramount,and application-based evaluation is preferable.
Moreover, itmay be that using linguistically most desired parses does notresult in best application performance.218English that utilises a manually-developed gram-mar and outputs grammatical dependency rela-tions.
The C&C CCG parser (Clark and Cur-ran, 2007) is a parsing system that is based onan automatically extracted grammar from CCG-Bank and uses discriminative training.
Both sys-tems are able to output the exact set of dependencyrelations, and in a comparison on a 560-sentencetest set used by Briscoe and Carroll (2006), Clarkand Curran (2007) report a micro-averaged F -score of 81.14 for the CCG parser, and 76.29 forRASP.
3 Both parsing systems utilise the Gram-matical Relations (GR) annotation scheme pro-posed by Carroll et al (1998).
This scheme is in-tended to cater for parser evaluation, and extendsthe dependency structure based method of eval-uation proposed by Lin (1998).
For the parent-child interaction domain both parsing systems arecompared with two syntactic dependency parsersthat were specifically trained for CHILDES tran-scripts: MEGRASP (Sagae et al, 2007) and MST-parser (McDonald et al, 2005).2.2 Speech phenomenaAs CCC and CHILDES transcripts are describ-ing spoken language, they contain various markersthat encode speech phenomena, particularly dis-fluencies (e.g.
filled pauses, partial words, falsestarts, repetitions) and speech repairs (e.g.
re-tractions and corrections).
Prior to parser eval-uation, such disfluencies have been deleted fromthe transcripts, which slightly improves parser per-formance for all systems mentioned.
Similar per-formance improvements are also reported in stud-ies that address the effect of deletion of repairsand fillers on parsing (e.g.
Charniak and Johnson(2001); Lease and Johnson (2006)).2.3 CHILDES dataThe major part of the evaluation is based onthe parsing of parent-child interactions from theCHILDES database (MacWhinney, 2000).
Alarge portion of CHILDES transcripts was recentlyparsed with a domain-specific parser (Sagae et al,2007), allowing more reliable systematic studiesof syntactic development in child language acqui-sition.
Sagae et al also released their gold stan-dard data, allowing others to train and evaluate3It should be remarked that such cross-formalism compar-isons are difficult in nature.
In this case, training data weredifferent (RASP is not tuned to newspaper text), and CCGutilises a lexicalised parsing model where RASP does not.other parser systems.The gold standard data uses a GR scheme thatis based on that of Carroll et al (1998) but thatdiffers in two respects: the scheme is extendedto suit the specific need of the child language re-search community (cf.
(Sagae et al, 2004)), andthe scheme does not extensively and explicitly usethe GR hierarchy.To compare parsing performance, a mappingfrom RASP GRs to CHILDES GRs was manu-ally constructed, containing 75 rules that involvethe label and optional restrictions on the word orPOS-tag of the head or dependent.3 Parser evaluation3.1 MeasuresSystem performance is reported with accuracymeasures for labelled and unlabelled dependen-cies resulting from 15-fold cross-validation.4 Theperformance on each grammatical relation is ex-pressed by precision, recall, and F1-score.
Punc-tuation has been excluded.3.2 CHILDESThe gold-standard used for evaluation is based on15 (out of 20) files in the Eve section of the Browncorpus.
The annotations that are available weremade with the CHILDES GR scheme, for whichan inter-annotator percentage agreement of 96.5%(N = 2) has been reported by Sagae et al (2004).From all manually annotated utterances initiallyavailable, duplicates, those with less than three to-kens (about 30% of all), and those with missingor incomplete parses (1% of all) were removed,resulting in a set of 14.137 sentences, comprising93,594 tokens with 4.5 tokens per utterance on av-erage.The performance scores that are obtained whenthe parsing systems are compared against the gold-standard are listed in the upper part of Table 1.As can be seen from the accuracy scores,MEGRASP and the MSTParser perform withmore than 30 percent point accuracy considerablybetter than the domain-independent parsers.
How-ever, the list of performance scores for each ofthe grammatical relations in Table 2 shows thatsome relations can be recovered with acceptable4The exception being the MEGRASP, for which becauseof computation problems the full gold standard was used (7%larger than the other training sets), resulting in somewhathigher scores than expected with cross-validation.219Table 1: Parsing accuracy scores.CHILDES labelled unlabelledRASP 60.1 69.2CCG parser 39.1 66.5MSTParser 93.8 95.4MEGRASP 90.7 93.5CCC labelled unlabelledRASP 66.7 72.3CCG parser 60.2 68.5F1-scores, such as auxiliaries, determiners, sub-jects, and objects of prepositions.53.3 CCCThe Cambridge Cookie-theft Corpus (CCC, TOAPPEAR, 2010) contains audio-recorded mono-logues of 196 subjects that were asked to fully de-scribe a scene in a picture.
As a result, the domainis small, but at the same time, sentence bound-aries are difficult to indicate.
From this corpusof 5,628 intonational phrases, a small evaluationset of 80 phrases has been manually annotated6with GRs.
The performance scores for each of theparsers is listed in the lower part of Table 1.
Accu-racy scores are higher than those for CHILDES,and the difference in labelled accuracy betweenthe domain-independent parsers is less than withCHILDES.
Due to space restrictions it is not pos-sible to present performance on individual GRs,but the GRs that are most reliably recovered aresimilar to those mentioned in Section 3.2.4 ConsiderationsIn the work reported here, performance of domain-independent parsers on narrow domains was cal-culated for two domains.
The availability ofmore domain-specific datasets with manually su-pervised GR annotations would allow a better gen-eralisation of parser performance.
Unfortunately,datasets with manually verified annotations thatuse the same set of syntactic dependencies arerare.The CHILDES figures show that the perfor-mance difference between domain-independent5MSTParser scores did not fit in the table, but largelycorrespond in distributional characteristics, and are availableupon request.6Not with multiple coders yet, but percentage agreementfor dependency annotation typically varies from 93-98%.and domain-trained parsers is big.
It should benoted that these results are obtained from speech,which is usually less syntactically well-formedthan written language.
For the speech data anal-ysed, RASP performs better than the CCG parser,whereas Clark and Curran (2007) have shown thatthe CCG parser outperforms RASP on written text.To better explain this difference, it would be in-sightful to compare the confusion matrices of GRassignments.
This would allow assessment on howthe domain-independent parser errors compare tothe domain-trained parser errors.The mapping from RASP GRs to CHILDESGRs that was constructed is exhaustive, but thereis still room for fine-tuning and more refined map-pings, gaining up to about 2% accuracy by esti-mate.5 Conclusions and future workThis paper has provided performance scores ofwide-coverage parsers applied to narrow domainspoken language transcripts to assess the perfor-mance gap with domain-trained parsers.
This gapappears to be considerable (more than 30 percentpoint for CHILDES), but a subset of GRs can stillbe recovered with fair accuracy.We have not yet dealt with comparingdomain-independent and domain-trained parsererrors, which may provide additional insight intothe strengths and weaknesses of wide-coverageparsers for narrow use.AcknowledgementsThis work is supported by UK EPSRC GrantEP/F030061/1.ReferencesBos, J. and Markert, K. (2005).
Recognising tex-tual entailment with logical inference.
In Pro-ceedings of the HLT and EMNLP conference,pages 628?635.Bouma, G., van Noord, G., and Malouf, R. (2001).Alpino: Wide-coverage computational analysisof dutch.
In Proceedings of the CLIN 2000,pages 45?59.Briscoe, T. and Carroll, J.
(2006).
Evaluating theaccuracy of an unlexicalized statistical parser onthe PARC depbank.
In Proceedings of the COL-ING/ACL on Main conference poster sessions,pages 41?48.220Table 2: Performance scores of the parsing systems for major GRs.
Some of the relations could not bereliably be mapped, and are absent for the CCG parser.RASP CCG parser MEGRASPrelation Prec Rec F1 Prec Rec F1 Prec Rec F1aux 89.13 69.87 78.33 90.81 62.21 73.84 98.13 96.21 97.16com 67.80 6.12 11.23 - - - 93.15 88.52 90.78comp 22.73 64.18 33.57 24.53 53.66 33.67 80.00 84.72 82.29coord 70.42 64.31 67.23 82.50 30.62 44.66 75.07 83.93 79.26cpzr 74.67 20.97 32.75 - - - 90.16 85.77 87.91det 90.34 89.38 89.86 60.88 82.54 70.07 96.38 97.27 96.82jct 57.85 56.68 57.26 54.71 5.16 9.42 85.14 83.05 84.08mod 63.04 76.93 69.29 16.89 47.43 24.91 90.00 90.63 90.32obj 73.34 75.50 74.40 46.09 69.25 55.34 91.93 91.10 91.52obj2 32.81 55.13 41.13 53.37 39.16 45.18 83.33 74.14 78.47pobj 88.11 75.51 81.33 - - - 91.94 93.05 92.49pred 54.77 48.94 51.69 64.60 15.55 25.07 90.21 91.08 90.65quant 55.87 68.87 61.69 - - - 83.10 91.46 87.08subj 74.53 67.58 70.89 66.94 66.11 66.52 94.68 95.01 94.84xcomp 52.17 64.97 57.87 1.62 3.35 2.19 92.11 87.13 89.55xmod 12.93 15.32 14.02 2.60 24.19 4.69 56.64 65.32 60.67Briscoe, T., Carroll, J., and Watson, R. (2006).
Thesecond release of the RASP system.
In Proceed-ings of the COLING/ACL on Interactive presen-tation sessions, pages 77?80.Buttery, P. and Korhonen, A.
(2007).
I willshoot your shopping down and you can shootall my tins?automatic lexical acquisition fromthe CHILDES database.
In Proceedings of theWorkshop on Cognitive Aspects of Computa-tional Language Acquisition, pages 33?40.Carroll, J., Briscoe, T., and Sanfilippo, A.
(1998).Parser evaluation: a survey and a new proposal.In Proceedings of the 1st LREC, pages 447?454.Charniak, E. and Johnson, M. (2001).
Edit de-tection and parsing for transcribed speech.
InProceedings of NAACL, pages 118?126.Clark, S. and Curran, J. R. (2007).
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguis-tics, 33(4):493?552.Dredze, M., Blitzer, J., Pratim Talukdar, P.,Ganchev, K., Graca, J. a., and Pereira, F. (2007).Frustratingly hard domain adaptation for depen-dency parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007,pages 1051?1055.Geertzen, J.
(2009).
Semantic interpretation ofDutch spoken dialogue.
In Proceedings of theEight IWCS, pages 286?290.Lease, M. and Johnson, M. (2006).
Early deletionof fillers in processing conversational speech.
InProceedings of the HLT-NAACL, pages 73?76.Lin, D. (1998).
A dependency-based methodfor evaluating broad-coverage parsers.
NaturalLanguage Engineering, 4(2):97?114.MacWhinney, B.
(2000).
The CHILDES project:Tools for analyzing talk.
Lawrence Erlbaum As-sociates, Mahwah, NJ, USA, third edition.McDonald, R., Crammer, K., and Pereira, F.(2005).
Online large-margin training of depen-dency parsers.
In Proceedings of the 43rd An-nual Meeting on ACL, pages 91?98.Sagae, K., Davis, E., Lavie, A., MacWhinney,B., and Wintner, S. (2007).
High-accuracy an-notation and parsing of CHILDES transcripts.In Proceedings of the ACL-2007 workshop onCognitive Aspects of Computational LanguageAcquisition.Sagae, K., MacWhinney, B., and Lavie, A.
(2004).Adding syntactic annotations to transcripts ofparent-child dialogs.
In In Proceedings of theFourth LREC, pages 1815?1818.221
