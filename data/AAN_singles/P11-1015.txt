Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 142?150,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLearning Word Vectors for Sentiment AnalysisAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,Andrew Y. Ng, and Christopher PottsStanford UniversityStanford, CA 94305[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.eduAbstractUnsupervised vector-based approaches to se-mantics can model rich lexical meanings, butthey largely fail to capture sentiment informa-tion that is central to many word meanings andimportant for a wide range of NLP tasks.
Wepresent a model that uses a mix of unsuper-vised and supervised techniques to learn wordvectors capturing semantic term?document in-formation as well as rich sentiment content.The proposed model can leverage both con-tinuous and multi-dimensional sentiment in-formation as well as non-sentiment annota-tions.
We instantiate the model to utilize thedocument-level sentiment polarity annotationspresent in many online documents (e.g.
starratings).
We evaluate the model using small,widely used sentiment and subjectivity cor-pora and find it out-performs several previ-ously introduced methods for sentiment clas-sification.
We also introduce a large datasetof movie reviews to serve as a more robustbenchmark for work in this area.1 IntroductionWord representations are a critical component ofmany natural language processing systems.
It iscommon to represent words as indices in a vocab-ulary, but this fails to capture the rich relationalstructure of the lexicon.
Vector-based models domuch better in this regard.
They encode continu-ous similarities between words as distance or anglebetween word vectors in a high-dimensional space.The general approach has proven useful in taskssuch as word sense disambiguation, named entityrecognition, part of speech tagging, and documentretrieval (Turney and Pantel, 2010; Collobert andWeston, 2008; Turian et al, 2010).In this paper, we present a model to capture bothsemantic and sentiment similarities among words.The semantic component of our model learns wordvectors via an unsupervised probabilistic model ofdocuments.
However, in keeping with linguistic andcognitive research arguing that expressive contentand descriptive semantic content are distinct (Ka-plan, 1999; Jay, 2000; Potts, 2007), we find thatthis basic model misses crucial sentiment informa-tion.
For example, while it learns that wonderfuland amazing are semantically close, it doesn?t cap-ture the fact that these are both very strong positivesentiment words, at the opposite end of the spectrumfrom terrible and awful.Thus, we extend the model with a supervisedsentiment component that is capable of embracingmany social and attitudinal aspects of meaning (Wil-son et al, 2004; Alm et al, 2005; Andreevskaiaand Bergler, 2006; Pang and Lee, 2005; Goldbergand Zhu, 2006; Snyder and Barzilay, 2007).
Thiscomponent of the model uses the vector represen-tation of words to predict the sentiment annotationson contexts in which the words appear.
This causeswords expressing similar sentiment to have similarvector representations.
The full objective functionof the model thus learns semantic vectors that areimbued with nuanced sentiment information.
In ourexperiments, we show how the model can leveragedocument-level sentiment annotations of a sort thatare abundant online in the form of consumer reviewsfor movies, products, etc.
The technique is suffi-142ciently general to work also with continuous andmulti-dimensional notions of sentiment as well asnon-sentiment annotations (e.g., political affiliation,speaker commitment).After presenting the model in detail, we pro-vide illustrative examples of the vectors it learns,and then we systematically evaluate the approachon document-level and sentence-level classificationtasks.
Our experiments involve the small, widelyused sentiment and subjectivity corpora of Pang andLee (2004), which permits us to make comparisonswith a number of related approaches and publishedresults.
We also show that this dataset contains manycorrelations between examples in the training andtesting sets.
This leads us to evaluate on, and makepublicly available, a large dataset of informal moviereviews from the Internet Movie Database (IMDB).2 Related workThe model we present in the next section draws in-spiration from prior work on both probabilistic topicmodeling and vector-spaced models for word mean-ings.Latent Dirichlet Allocation (LDA; (Blei et al,2003)) is a probabilistic document model that as-sumes each document is a mixture of latent top-ics.
For each latent topic T , the model learns aconditional distribution p(w|T ) for the probabilitythat word w occurs in T .
One can obtain a k-dimensional vector representation of words by firsttraining a k-topic model and then filling the matrixwith the p(w|T ) values (normalized to unit length).The result is a word?topic matrix in which the rowsare taken to represent word meanings.
However,because the emphasis in LDA is on modeling top-ics, not word meanings, there is no guarantee thatthe row (word) vectors are sensible as points in ak-dimensional space.
Indeed, we show in section4 that using LDA in this way does not deliver ro-bust word vectors.
The semantic component of ourmodel shares its probabilistic foundation with LDA,but is factored in a manner designed to discoverword vectors rather than latent topics.
Some recentwork introduces extensions of LDA to capture sen-timent in addition to topical information (Li et al,2010; Lin and He, 2009; Boyd-Graber and Resnik,2010).
Like LDA, these methods focus on model-ing sentiment-imbued topics rather than embeddingwords in a vector space.Vector space models (VSMs) seek to model wordsdirectly (Turney and Pantel, 2010).
Latent Seman-tic Analysis (LSA), perhaps the best known VSM,explicitly learns semantic word vectors by apply-ing singular value decomposition (SVD) to factor aterm?document co-occurrence matrix.
It is typicalto weight and normalize the matrix values prior toSVD.
To obtain a k-dimensional representation for agiven word, only the entries corresponding to the klargest singular values are taken from the word?s ba-sis in the factored matrix.
Such matrix factorization-based approaches are extremely successful in prac-tice, but they force the researcher to make a numberof design choices (weighting, normalization, dimen-sionality reduction algorithm) with little theoreticalguidance to suggest which to prefer.Using term frequency (tf) and inverse documentfrequency (idf) weighting to transform the valuesin a VSM often increases the performance of re-trieval and categorization systems.
Delta idf weight-ing (Martineau and Finin, 2009) is a supervised vari-ant of idf weighting in which the idf calculation isdone for each document class and then one valueis subtracted from the other.
Martineau and Fininpresent evidence that this weighting helps with sen-timent classification, and Paltoglou and Thelwall(2010) systematically explore a number of weight-ing schemes in the context of sentiment analysis.The success of delta idf weighting in previous worksuggests that incorporating sentiment informationinto VSM values via supervised methods is help-ful for sentiment analysis.
We adopt this insight,but we are able to incorporate it directly into ourmodel?s objective function.
(Section 4 comparesour approach with a representative sample of suchweighting schemes.
)3 Our ModelTo capture semantic similarities among words, wederive a probabilistic model of documents whichlearns word representations.
This component doesnot require labeled data, and shares its foundationwith probabilistic topic models such as LDA.
Thesentiment component of our model uses sentimentannotations to constrain words expressing similar143sentiment to have similar representations.
We canefficiently learn parameters for the joint objectivefunction using alternating maximization.3.1 Capturing Semantic SimilaritiesWe build a probabilistic model of a document us-ing a continuous mixture distribution over words in-dexed by a multi-dimensional random variable ?.We assume words in a document are conditionallyindependent given the mixture variable ?.
We assigna probability to a document d using a joint distribu-tion over the document and ?.
The model assumeseach word wi ?
d is conditionally independent ofthe other words given ?.
The probability of a docu-ment is thusp(d) =?p(d, ?)d?
=?p(?)N?i=1p(wi|?)d?.
(1)Where N is the number of words in d and wi isthe ith word in d. We use a Gaussian prior on ?.We define the conditional distribution p(wi|?)
us-ing a log-linear model with parameters R and b.The energy function uses a word representation ma-trix R ?
R(?
x |V |) where each word w (representedas a one-on vector) in the vocabulary V has a ?-dimensional vector representation ?w = Rw corre-sponding to that word?s column in R. The randomvariable ?
is also a ?-dimensional vector, ?
?
R?which weights each of the ?
dimensions of words?representation vectors.
We additionally introduce abias bw for each word to capture differences in over-all word frequencies.
The energy assigned to a wordw given these model parameters isE(w; ?, ?w, bw) = ?
?T?w ?
bw.
(2)To obtain the distribution p(w|?)
we use a softmax,p(w|?
;R, b) = exp(?E(w; ?, ?w, bw))?w?
?V exp(?E(w?
; ?, ?w?
, bw?
))(3)= exp(?T?w + bw)?w?
?V exp(?T?w?
+ bw?).
(4)The number of terms in the denominator?s sum-mation grows linearly in |V |, making exact com-putation of the distribution possible.
For a given?, a word w?s occurrence probability is related tohow closely its representation vector ?w matches thescaling direction of ?.
This idea is similar to theword vector inner product used in the log-bilinearlanguage model of Mnih and Hinton (2007).Equation 1 resembles the probabilistic model ofLDA (Blei et al, 2003), which models documentsas mixtures of latent topics.
One could view the en-tries of a word vector ?
as that word?s associationstrength with respect to each latent topic dimension.The random variable ?
then defines a weighting overtopics.
However, our model does not attempt tomodel individual topics, but instead directly modelsword probabilities conditioned on the topic mixturevariable ?.
Because of the log-linear formulation ofthe conditional distribution, ?
is a vector in R?
andnot restricted to the unit simplex as it is in LDA.We now derive maximum likelihood learning forthis model when given a set of unlabeled documentsD.
In maximum likelihood learning we maximizethe probability of the observed data given the modelparameters.
We assume documents dk ?
D are i.i.d.samples.
Thus the learning problem becomesmaxR,bp(D;R, b) =?dk?D?p(?)Nk?i=1p(wi|?
;R, b)d?.
(5)Using maximum a posteriori (MAP) estimates for ?,we approximate this learning problem asmaxR,b?dk?Dp(??k)Nk?i=1p(wi|?
?k;R, b), (6)where ?
?k denotes the MAP estimate of ?
for dk.We introduce a Frobenious norm regularization termfor the word representation matrix R. The word bi-ases b are not regularized reflecting the fact that wewant the biases to capture whatever overall word fre-quency statistics are present in the data.
By takingthe logarithm and simplifying we obtain the final ob-jective,?||R||2F +?dk?D?||?
?k||22 +Nk?i=1log p(wi|?
?k;R, b),(7)which is maximized with respect to R and b. Thehyper-parameters in the model are the regularization144weights (?
and ?
), and the word vector dimension-ality ?.3.2 Capturing Word SentimentThe model presented so far does not explicitly cap-ture sentiment information.
Applying this algorithmto documents will produce representations wherewords that occur together in documents have sim-ilar representations.
However, this unsupervisedapproach has no explicit way of capturing whichwords are predictive of sentiment as opposed tocontent-related.
Much previous work in natural lan-guage processing achieves better representations bylearning from multiple tasks (Collobert and Weston,2008; Finkel and Manning, 2009).
Following thistheme we introduce a second task to utilize labeleddocuments to improve our model?s word representa-tions.Sentiment is a complex, multi-dimensional con-cept.
Depending on which aspects of sentiment wewish to capture, we can give some body of text asentiment label s which can be categorical, continu-ous, or multi-dimensional.
To leverage such labels,we introduce an objective that the word vectors ofour model should predict the sentiment label usingsome appropriate predictor,s?
= f(?w).
(8)Using an appropriate predictor function f(x) wemap a word vector ?w to a predicted sentiment labels?.
We can then improve our word vector ?w to betterpredict the sentiment labels of contexts in which thatword occurs.For simplicity we consider the case where the sen-timent label s is a scalar continuous value repre-senting sentiment polarity of a document.
This cap-tures the case of many online reviews where doc-uments are associated with a label on a star ratingscale.
We linearly map such star values to the inter-val s ?
[0, 1] and treat them as a probability of pos-itive sentiment polarity.
Using this formulation, weemploy a logistic regression as our predictor f(x).We use w?s vector representation ?w and regressionweights ?
to express this asp(s = 1|w;R,?)
= ?
(?T?w + bc), (9)where ?
(x) is the logistic function and ?
?
R?
is thelogistic regression weight vector.
We additionallyintroduce a scalar bias bc for the classifier.The logistic regression weights ?
and bc definea linear hyperplane in the word vector space wherea word vector?s positive sentiment probability de-pends on where it lies with respect to this hyper-plane.
Learning over a collection of documents re-sults in words residing different distances from thishyperplane based on the average polarity of docu-ments in which the words occur.Given a set of labeled documents D where sk isthe sentiment label for document dk, we wish tomaximize the probability of document labels giventhe documents.
We assume documents in the collec-tion and words within a document are i.i.d.
samples.By maximizing the log-objective we obtain,maxR,?,bc|D|?k=1Nk?i=1log p(sk|wi;R,?, bc).
(10)The conditional probability p(sk|wi;R,?, bc) iseasily obtained from equation 9.3.3 LearningThe full learning objective maximizes a sum of thetwo objectives presented.
This produces a final ob-jective function of,?||R||2F +|D|?k=1?||?
?k||22 +Nk?i=1log p(wi|?
?k;R, b)+|D|?k=11|Sk|Nk?i=1log p(sk|wi;R,?, bc).
(11)|Sk| denotes the number of documents in the datasetwith the same rounded value of sk (i.e.
sk < 0.5and sk ?
0.5).
We introduce the weighting 1|Sk| tocombat the well-known imbalance in ratings presentin review collections.
This weighting prevents theoverall distribution of document ratings from affect-ing the estimate of document ratings in which a par-ticular word occurs.
The hyper-parameters of themodel are the regularization weights (?
and ?
), andthe word vector dimensionality ?.Maximizing the objective function with respect toR, b, ?, and bc is a non-convex problem.
We usealternating maximization, which first optimizes the145word representations (R, b, ?, and bc) while leav-ing the MAP estimates (??)
fixed.
Then we find thenew MAP estimate for each document while leav-ing the word representations fixed, and continue thisprocess until convergence.
The optimization algo-rithm quickly finds a global solution for each ?
?k be-cause we have a low-dimensional, convex problemin each ??k.
Because the MAP estimation problemsfor different documents are independent, we cansolve them on separate machines in parallel.
Thisfacilitates scaling the model to document collectionswith hundreds of thousands of documents.4 ExperimentsWe evaluate our model with document-level andsentence-level categorization tasks in the domain ofonline movie reviews.
For document categoriza-tion, we compare our method to previously pub-lished results on a standard dataset, and introducea new dataset for the task.
In both tasks we com-pare our model?s word representations with severalbag of words weighting methods, and alternative ap-proaches to word vector induction.4.1 Word Representation LearningWe induce word representations with our model us-ing 25,000 movie reviews from IMDB.
Becausesome movies receive substantially more reviewsthan others, we limited ourselves to including atmost 30 reviews from any movie in the collection.We build a fixed dictionary of the 5,000 most fre-quent tokens, but ignore the 50 most frequent termsfrom the original full vocabulary.
Traditional stopword removal was not used because certain stopwords (e.g.
negating words) are indicative of senti-ment.
Stemming was not applied because the modellearns similar representations for words of the samestem when the data suggests it.
Additionally, be-cause certain non-word tokens (e.g.
?!?
and ?:-)?
)are indicative of sentiment, we allow them in our vo-cabulary.
Ratings on IMDB are given as star values(?
{1, 2, ..., 10}), which we linearly map to [0, 1] touse as document labels when training our model.The semantic component of our model does notrequire document labels.
We train a variant of ourmodel which uses 50,000 unlabeled reviews in addi-tion to the labeled set of 25,000 reviews.
The unla-beled set of reviews contains neutral reviews as wellas those which are polarized as found in the labeledset.
Training the model with additional unlabeleddata captures a common scenario where the amountof labeled data is small relative to the amount of un-labeled data available.
For all word vector models,we use 50-dimensional vectors.As a qualitative assessment of word represen-tations, we visualize the words most similar to aquery word using vector similarity of the learnedrepresentations.
Given a query word w and an-other word w?
we obtain their vector representations?w and ?w?
, and evaluate their cosine similarity asS(?w, ?w?)
= ?Tw?w?||?w||?||?w?
||.
By assessing the simi-larity of w with all other words w?, we can find thewords deemed most similar by the model.Table 1 shows the most similar words to givenquery words using our model?s word representationsas well as those of LSA.
All of these vectors cap-ture broad semantic similarities.
However, both ver-sions of our model seem to do better than LSA inavoiding accidental distributional similarities (e.g.,screwball and grant as similar to romantic) A com-parison of the two versions of our model also beginsto highlight the importance of adding sentiment in-formation.
In general, words indicative of sentimenttend to have high similarity with words of the samesentiment polarity, so even the purely unsupervisedmodel?s results look promising.
However, they alsoshow more genre and content effects.
For exam-ple, the sentiment enriched vectors for ghastly aretruly semantic alternatives to that word, whereas thevectors without sentiment also contain some contentwords that tend to have ghastly predicated of them.Of course, this is only an impressionistic analysis ofa few cases, but it is helpful in understanding whythe sentiment-enriched model proves superior at thesentiment classification results we report next.4.2 Other Word RepresentationsFor comparison, we implemented several alternativevector space models that are conceptually similar toour own, as discussed in section 2:Latent Semantic Analysis (LSA; Deerwester etal., 1990) We apply truncated SVD to a tf.idfweighted, cosine normalized count matrix, whichis a standard weighting and smoothing scheme for146Our model Our modelSentiment + Semantic Semantic only LSAmelancholybittersweet thoughtful poeticheartbreaking warmth lyricalhappiness layer poetrytenderness gentle profoundcompassionate loneliness vividghastlyembarrassingly predators hideoustrite hideous ineptlaughably tube severelyatrocious baffled grotesqueappalling smack unsuspectinglacklusterlame passable uninspiredlaughable unconvincing flatunimaginative amateurish blanduninspired cliche?d forgettableawful insipid mediocreromanticromance romance romancelove charming screwballsweet delightful grantbeautiful sweet comediesrelationship chemistry comedyTable 1: Similarity of learned word vectors.
Each target word is given with its five most similar words using cosinesimilarity of the vectors determined by each model.
The full version of our model (left) captures both lexical similarityas well as similarity of sentiment strength and orientation.
Our unsupervised semantic component (center) and LSA(right) capture semantic relations.VSM induction (Turney and Pantel, 2010).Latent Dirichlet Allocation (LDA; Blei etal., 2003) We use the method described in sec-tion 2 for inducing word representations from thetopic matrix.
To train the 50-topic LDA model weuse code released by Blei et al (2003).
We use thesame 5,000 term vocabulary for LDA as is used fortraining word vector models.
We leave the LDAhyperparameters at their default values, thoughsome work suggests optimizing over priors for LDAis important (Wallach et al, 2009).Weighting Variants We evaluate both binary (b)term frequency weighting with smoothed delta idf(?t?)
and no idf (n) because these variants workedwell in previous experiments in sentiment (Mar-tineau and Finin, 2009; Pang et al, 2002).
In allcases, we use cosine normalization (c).
Paltoglouand Thelwall (2010) perform an extensive analysisof such weighting variants for sentiment tasks.4.3 Document Polarity ClassificationOur first evaluation task is document-level senti-ment polarity classification.
A classifier must pre-dict whether a given review is positive or negativegiven the review text.Given a document?s bag of words vector v, weobtain features from our model using a matrix-vector product Rv, where v can have arbitrary tf.idfweighting.
We do not cosine normalize v, insteadapplying cosine normalization to the final featurevector Rv.
This procedure is also used to obtainfeatures from the LDA and LSA word vectors.
Inpreliminary experiments, we found ?bnn?
weightingto work best for v when generating document fea-tures via the product Rv.
In all experiments, weuse this weighting to get multi-word representations147Features PL04 Our Dataset SubjectivityBag of Words (bnc) 85.45 87.80 87.77Bag of Words (b?t?c) 85.80 88.23 85.65LDA 66.70 67.42 66.65LSA 84.55 83.96 82.82Our Semantic Only 87.10 87.30 86.65Our Full 84.65 87.44 86.19Our Full, Additional Unlabeled 87.05 87.99 87.22Our Semantic + Bag of Words (bnc) 88.30 88.28 88.58Our Full + Bag of Words (bnc) 87.85 88.33 88.45Our Full, Add?l Unlabeled + Bag of Words (bnc) 88.90 88.89 88.13Bag of Words SVM (Pang and Lee, 2004) 87.15 N/A 90.00Contextual Valence Shifters (Kennedy and Inkpen, 2006) 86.20 N/A N/Atf.
?idf Weighting (Martineau and Finin, 2009) 88.10 N/A N/AAppraisal Taxonomy (Whitelaw et al, 2005) 90.20 N/A N/ATable 2: Classification accuracy on three tasks.
From left to right the datasets are: A collection of 2,000 movie reviewsoften used as a benchmark of sentiment classification (Pang and Lee, 2004), 50,000 reviews we gathered from IMDB,and the sentence subjectivity dataset alo released by (Pang and Lee, 2004).
All tasks are balanced two-class problems.from word vectors.4.3.1 Pang and Lee Movie Review DatasetThe polarity dataset version 2.0 introduced by Pangand Lee (2004) 1 consists of 2,000 movie reviews,where each is associated with a binary sentiment po-larity label.
We report 10-fold cross validation re-sults using the authors?
published folds to make ourresults comparable with others in the literature.
Weuse a linear support vector machine (SVM) classifiertrained with LIBLINEAR (Fan et al, 2008), and setthe SVM regularization parameter to the same valueused by Pang and Lee (2004).Table 2 shows the classification performance ofour method, other VSMs we implemented, and pre-viously reported results from the literature.
Bag ofwords vectors are denoted by their weighting nota-tion.
Features from word vector learner are denotedby the learner name.
As a control, we trained ver-sions of our model with only the unsupervised se-mantic component, and the full model (semantic andsentiment).
We also include results for a version ofour full model trained with 50,000 additional unla-beled examples.
Finally, to test whether our mod-els?
representations complement a standard bag ofwords, we evaluate performance of the two featurerepresentations concatenated.1http://www.cs.cornell.edu/people/pabo/movie-review-dataOur method?s features clearly outperform those ofother VSMs, and perform best when combined withthe original bag of words representation.
The vari-ant of our model trained with additional unlabeleddata performed best, suggesting the model can effec-tively utilize large amounts of unlabeled data alongwith labeled examples.
Our method performs com-petitively with previously reported results in spite ofour restriction to a vocabulary of only 5,000 words.We extracted the movie title associated with eachreview and found that 1,299 of the 2,000 reviews inthe dataset have at least one other review of the samemovie in the dataset.
Of 406 movies with multiplereviews, 249 have the same polarity label for all oftheir reviews.
Overall, these facts suggest that, rela-tive to the size of the dataset, there are highly corre-lated examples with correlated labels.
This is a nat-ural and expected property of this kind of documentcollection, but it can have a substantial impact onperformance in datasets of this scale.
In the randomfolds distributed by the authors, approximately 50%of reviews in each validation fold?s test set have areview of the same movie with the same label in thetraining set.
Because the dataset is small, a learnermay perform well by memorizing the association be-tween label and words unique to a particular movie(e.g., character names or plot terms).We introduce a substantially larger dataset, which148uses disjoint sets of movies for training and testing.These steps minimize the ability of a learner to relyon idiosyncratic word?class associations, therebyfocusing attention on genuine sentiment features.4.3.2 IMDB Review DatasetWe constructed a collection of 50,000 reviews fromIMDB, allowing no more than 30 reviews per movie.The constructed dataset contains an even number ofpositive and negative reviews, so randomly guessingyields 50% accuracy.
Following previous work onpolarity classification, we consider only highly po-larized reviews.
A negative review has a score ?
4out of 10, and a positive review has a score ?
7out of 10.
Neutral reviews are not included in thedataset.
In the interest of providing a benchmark forfuture work in this area, we release this dataset tothe public.2We evenly divided the dataset into training andtest sets.
The training set is the same 25,000 la-beled reviews used to induce word vectors with ourmodel.
We evaluate classifier performance aftercross-validating classifier parameters on the trainingset, again using a linear SVM in all cases.
Table 2shows classification performance on our subset ofIMDB reviews.
Our model showed superior per-formance to other approaches, and performed bestwhen concatenated with bag of words representa-tion.
Again the variant of our model which utilizedextra unlabeled data during training performed best.Differences in accuracy are small, but, becauseour test set contains 25,000 examples, the varianceof the performance estimate is quite low.
For ex-ample, an accuracy increase of 0.1% corresponds tocorrectly classifying an additional 25 reviews.4.4 Subjectivity DetectionAs a second evaluation task, we performed sentence-level subjectivity classification.
In this task, a clas-sifier is trained to decide whether a given sentence issubjective, expressing the writer?s opinions, or ob-jective, expressing purely facts.
We used the datasetof Pang and Lee (2004), which contains subjectivesentences from movie review summaries and objec-tive sentences from movie plot summaries.
This task2Dataset and further details are available online at:http://www.andrew-maas.net/data/sentimentis substantially different from the review classifica-tion task because it uses sentences as opposed to en-tire documents and the target concept is subjectivityinstead of opinion polarity.
We randomly split the10,000 examples into 10 folds and report 10-foldcross validation accuracy using the SVM trainingprotocol of Pang and Lee (2004).Table 2 shows classification accuracies from thesentence subjectivity experiment.
Our model againprovided superior features when compared againstother VSMs.
Improvement over the bag-of-wordsbaseline is obtained by concatenating the two featurevectors.5 DiscussionWe presented a vector space model that learns wordrepresentations captuing semantic and sentiment in-formation.
The model?s probabilistic foundationgives a theoretically justified technique for wordvector induction as an alternative to the overwhelm-ing number of matrix factorization-based techniquescommonly used.
Our model is parametrized as alog-bilinear model following recent success in us-ing similar techniques for language models (Bengioet al, 2003; Collobert and Weston, 2008; Mnih andHinton, 2007), and it is related to probabilistic latenttopic models (Blei et al, 2003; Steyvers and Grif-fiths, 2006).
We parametrize the topical componentof our model in a manner that aims to capture wordrepresentations instead of latent topics.
In our ex-periments, our method performed better than LDA,which models latent topics directly.We extended the unsupervised model to incor-porate sentiment information and showed how thisextended model can leverage the abundance ofsentiment-labeled texts available online to yieldword representations that capture both sentimentand semantic relations.
We demonstrated the util-ity of such representations on two tasks of senti-ment classification, using existing datasets as wellas a larger one that we release for future research.These tasks involve relatively simple sentiment in-formation, but the model is highly flexible in thisregard; it can be used to characterize a wide varietyof annotations, and thus is broadly applicable in thegrowing areas of sentiment analysis and retrieval.149AcknowledgmentsThis work is supported by the DARPA Deep Learn-ing program under contract number FA8650-10-C-7020, an NSF Graduate Fellowship awarded to AM,and ONR grant No.
N00014-10-1-0109 to CP.ReferencesC.
O. Alm, D. Roth, and R. Sproat.
2005.
Emotions fromtext: machine learning for text-based emotion predic-tion.
In Proceedings of HLT/EMNLP, pages 579?586.A.
Andreevskaia and S. Bergler.
2006.
Mining Word-Net for fuzzy sentiment: sentiment tag extraction fromWordNet glosses.
In Proceedings of the EuropeanACL, pages 209?216.Y.
Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003.a neural probabilistic language model.
Journal of Ma-chine Learning Research, 3:1137?1155, August.D.
M. Blei, A. Y. Ng, and M. I. Jordan.
2003.
Latentdirichlet alocation.
Journal of Machine Learning Re-search, 3:993?1022, May.J.
Boyd-Graber and P. Resnik.
2010.
Holistic sentimentanalysis across languages: multilingual supervised la-tent Dirichlet alocation.
In Proceedings of EMNLP,pages 45?55.R.
Collobert and J. Weston.
2008.
A unified architecturefor natural language processing.
In Proceedings of theICML, pages 160?167.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-dauer, and R. Harshman.
1990.
Indexing by latent se-mantic analysis.
Journal of the American Society forInformation Science, 41:391?407, September.R.
E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, andC.
J. Lin.
2008.
LIBLINEAR: A library for large lin-ear classification.
The Journal of Machine LearningResearch, 9:1871?1874, August.J.
R. Finkel and C. D. Manning.
2009.
Joint parsing andnamed entity recognition.
In Proceedings of NAACL,pages 326?334.A.
B. Goldberg and J. Zhu.
2006.
Seeing stars whenthere aren?t many stars: graph-based semi-supervisedlearning for sentiment categorization.
In TextGraphs:HLT/NAACL Workshop on Graph-based Algorithmsfor Natural Language Processing, pages 45?52.T.
Jay.
2000.
Why We Curse: A Neuro-Psycho-Social Theory of Speech.
John Benjamins, Philadel-phia/Amsterdam.D.
Kaplan.
1999.
What is meaning?
Explorations in thetheory of Meaning as Use.
Brief version ?
draft 1.Ms., UCLA.A.
Kennedy and D. Inkpen.
2006.
Sentiment clas-sification of movie reviews using contextual valenceshifters.
Computational Intelligence, 22:110?125,May.F.
Li, M. Huang, and X. Zhu.
2010.
Sentiment analysiswith global topics and local dependency.
In Proceed-ings of AAAI, pages 1371?1376.C.
Lin and Y.
He.
2009.
Joint sentiment/topic model forsentiment analysis.
In Proceeding of the 18th ACMConference on Information and Knowledge Manage-ment, pages 375?384.J.
Martineau and T. Finin.
2009.
Delta tfidf: an improvedfeature space for sentiment analysis.
In Proceedingsof the 3rd AAAI International Conference on Weblogsand Social Media, pages 258?261.A.
Mnih and G. E. Hinton.
2007.
Three new graphicalmodels for statistical language modelling.
In Proceed-ings of the ICML, pages 641?648.G.
Paltoglou and M. Thelwall.
2010.
A study of informa-tion retrieval weighting schemes for sentiment analy-sis.
In Proceedings of the ACL, pages 1386?1395.B.
Pang and L. Lee.
2004.
A sentimental education:sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of the ACL,pages 271?278.B.
Pang and L. Lee.
2005.
Seeing stars: exploiting classrelationships for sentiment categorization with respectto rating scales.
In Proceedings of ACL, pages 115?124.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
sentiment classification using machine learningtechniques.
In Proceedings of EMNLP, pages 79?86.C.
Potts.
2007.
The expressive dimension.
TheoreticalLinguistics, 33:165?197.B.
Snyder and R. Barzilay.
2007.
Multiple aspect rank-ing using the good grief algorithm.
In Proceedings ofNAACL, pages 300?307.M.
Steyvers and T. L. Griffiths.
2006.
Probabilistic topicmodels.
In T. Landauer, D McNamara, S. Dennis, andW.
Kintsch, editors, Latent Semantic Analysis: A Roadto Meaning.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: A simple and general method for semi-supervised learning.
In Proceedings of the ACL, page384394.P.
D. Turney and P. Pantel.
2010.
From frequency tomeaning: vector space models of semantics.
Journalof Artificial Intelligence Research, 37:141?188.H.
Wallach, D. Mimno, and A. McCallum.
2009.
Re-thinking LDA: why priors matter.
In Proceedings ofNIPS, pages 1973?1981.C.
Whitelaw, N. Garg, and S. Argamon.
2005.
Using ap-praisal groups for sentiment analysis.
In Proceedingsof CIKM, pages 625?631.T.
Wilson, J. Wiebe, and R. Hwa.
2004.
Just how madare you?
Finding strong and weak opinion clauses.
InProceedings of AAAI, pages 761?769.150
