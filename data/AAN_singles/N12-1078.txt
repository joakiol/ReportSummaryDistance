2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 621?625,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsExpectations of Word Sense in Parallel CorporaXuchen Yao, Benjamin Van Durme and Chris Callison-BurchCenter for Language and Speech Processing, and HLTCOEJohns Hopkins UniversityAbstractGiven a parallel corpus, if two distinct wordsin language A, a1 and a2, are aligned to thesame word b1 in language B, then this mightsignal that b1 is polysemous, or it might sig-nal a1 and a2 are synonyms.
Both assump-tions with successful work have been put for-ward in the literature.
We investigate theseassumptions, along with other questions ofword sense, by looking at sampled parallelsentences containing tokens of the same typein English, asking how often they mean thesame thing when they are: 1. aligned to thesame foreign type; and 2. aligned to differentforeign types.
Results for French-English andChinese-English parallel corpora show simi-lar behavior: Synonymy is only very weaklythe more prevalent scenario, where both casesregularly occur.1 IntroductionParallel corpora have been used for both paraphraseinduction and word sense disambiguation (WSD).Usually one of the following two assumptions ismade for these tasks:1.
Polysemy If two different words in languageA are aligned to the same word in language B,then the word in language B is polysemous.2.
Synonymy If two different words in languageA are aligned to the same word in language B,then the two words in A are synonyms, and thusis not evidence of polysemy in B.Despite the alternate nature of these assumptions,both have associated articles in which a researcherclaimed success.
Under the polysemy assumption,Gale et al (1992) used French translations as En-glish sense indicators in the task of WSD.
For in-stance, for the English word duty, the French transla-tion droit was taken to signal its tax sense and devoirto signal its obligation sense.
These French wordswere used as labels for different English senses.Similarly, in a cross-lingual WSD setting,1 Lefeveret al (2011) treated each English-foreign alignmentas a so-called ParaSense, using it as a proxy for hu-man labeled training data.Under the synonymy assumption, Diab andResnik (2002) did word sense tagging by groupingtogether all English words that are translated intothe same French word and by further enforcing thatthe majority sense for these English words was pro-jected as the sense for the French word.
Bannard andCallison-Burch (2005) applied the idea that Frenchphrases aligned to the same English phrase are para-phrases in a system that induces paraphrases by piv-oting through aligned foreign phrases.Based on this, and other successful prior work,it seems neither of the assumptions must holduniversally.
Therefore we investigate how oftenwe might expect one or the other to dominate:we sample polysemous words from wide-domain{French,Chinese}-English corpora, and use Ama-zon?s Mechanical Turk (MTurk) to annotate wordsense on the English side.
We calculate empiricalprobabilities based on counting over the competingpolysemous and synonymous scenario labels.A key factor deciding the validity of our conclu-sion is the reliability of the annotations derived viaMTurk.
Thus our first step is to evaluate the abil-ity of Turkers to perform WSD.
After verifying this1E.g., given a sentence ?...
more power, more duty ...?, thetask asks to give a French translation of duty, which should bedevior, after first recognizing the underlying obligation sense.621as a reasonable process for acquiring large amountsof WSD labeled data, we go on to frame the experi-mental design, giving final results in Sec.
4.2 Turker ReliabilityWhile Amazon?s Mechanical Turk (MTurk) hasbeen been considered in the past for constructinglexical semantic resources (e.g., (Snow et al, 2008;Akkaya et al, 2010; Parent and Eskenazi, 2010;Rumshisky, 2011)), word sense annotation is sensi-tive to subjectivity and usually achieves low agree-ment rate even among experts.
Thus we first askedTurkers to re-annotate a sample of existing gold-standard data.
With an eye towards costs saving, wealso considered how many Turkers would be neededper item to produce results of sufficient quality.Turkers were presented sentences from the testportion of the word sense induction task ofSemEval-2007 (Agirre and Soroa, 2007), covering2,559 instances of 35 nouns, expert-annotated withOntoNotes (Hovy et al, 2006) senses.
Two versionsof the task were designed:1. compare: given the same word in differentsentences, tell whether their meaning is THESAME, ALMOST THE SAME, UNLIKELY THESAME or DIFFERENT, where the results werecollapsed post-hoc into a binary same/differentcategorization;2. sense map: map the meaning of a given wordin a sentential context to its proper OntoNotesdefinition.For both tasks, 2, 599 examples were presented.We measure inter-coder agreement using Krip-pendorff?s Alpha (Krippendorff, 2004; Artstein andPoesio, 2008), where ?
?
0.8 is considered to bereliable and 0.667 ?
?
< 0.8 allows for tenta-tive conclusions.
Two points emerge from Table 1:there were greater agreement rates for sense mapthan compare, and 3 Turkers were sufficient.3 Experiment DesignData Selection We used two parallel corpora: theFrench-English 109 corpus (Callison-Burch et al,2009) and the GALE Chinese-English corpus.
?-Turker ?-maj. maj.-agr.compare5 0.47 0.66 0.87compare3 0.44 0.52 0.83sense map5 0.79 0.93 0.95sense map3 0.75 0.87 0.91Table 1: MTurk result on testing Turker reliability.
Krip-pendorff?s Alpha is used to measure agreement.
?-Turker: how Turkers agree among themselves, ?-maj.:how the majority agrees with true value, maj.-agr.
: agree-ment between the majority vote and true value.
?-maj.indicates the confidence level about the maj.-agr.
value.Subscripts denote either 5 Turkers, or 3 randomly se-lected of the 5.For each corpus we selected 50 words, w, at ran-dom from OntoNotes,2 constrained such that w: hadmore than one sense; had a frequency ?
1, 000; andwas not a top 10% most frequent words.Next we sampled 100 instances (aligned English-foreign sentence pairs) for each word based on thefollowing constraints: the aligned foreign word, f ,had a frequency ?
20 in the foreign corpus; f had anon-trivial alignment probability.3 We sampled pro-portionally to the distribution of the aligned foreignwords, ensuring that at least 5 instances from eachforeign translation are sampled.4For each corpus, this results in 100 instances foreach of 50 words, totaling 5,000 instances.
We used3 Turkers per instance for sense annotation, underthe sense map task.
We note that the set of 50randomly selected English words from the Chinese-English corpus were entirely distinct from the 50 se-lected words from the French-English corpus.Probability Estimation Suppose e1 and e2 aretwo tokens of the same English word type e. s(e1)is a function that returns the sense of e1, a(e1) isa function that returns the aligned word of e1.
Letc() be our count function, where: c(e, f) returns the2OntoNotes was used as the sense inventory over alterna-tives, owing to its coarse-grained sense definitions.3Defined as f having index i < k when foreign words areranked by most probable given e, where k is the minimum valuesuch that?ki p(fi | e) > 0.8.
E.g., if we have decreasingprobabilities p(droit | duty) = 0.6, p(devoir | duty) =0.25, p(le | duty) = 0.03, ... then only consider droit anddevoir.
This ruled out many noisy alignments.4Thus, the instances of droit compared to that of devoirwould be 0.6/0.25.622number of times English word e is aligned to foreignword f ; c(es, f) returns the number of times En-glish word e has sense s (tagged by Turkers), whenaligned to foreign word f ; c(e) is the total numberof tokens of English word e; and c(es) is the numberof tokens of e with sense s.We estimate from labeled data the probability ofthree scenarios, with scenario 1 as our primary con-cern: when two English words of the same poly-semous type are aligned to different foreign wordtypes, what is the chance that they have the samesense?
Given the tokens e1 and e2, we calculate P1as follows:P1e = P (s(e1) = s(e2) | a(e1) 6= a(e2))?
?s c2(es)?
?s,f c2(es, f)c2(e)?
?f c2(e, f)P1 says that given two words of the same type(e1 and e2) that are not aligned to the same foreignword type (a(e1) 6= a(e2)), what is the probabil-ity that they have the same sense (s(e1) = s(e2)).We approach this estimation combinatorially.
Forinstance, the number of ways to choose two wordsof the same type is( 2c(e))?
12c2(e) when c(e) islarge.A large value of P1 would be in support of Syn-onymy, as the two foreign aligned words of distincttype would have the same meaning.Scenario 2 asks: given two English words ofthe same polysemous type and aligned to the samewords (a(e1) = a(e2)), what is the probability thatthey have the same sense (s(e1) = s(e2))?P2e = P (s(e1) = s(e2) | a(e1) = a(e2))?
?s,f c2(es, f)?f c2(e, f)Finally, what is the probability of two tokens ofthe same polysemous type agreeing when alignmentinformation is not known (e.g., without a parallelcorpus)?P3e = P (s(e1) = s(e2)) ?
?s c2(es)c2(e)All the above equations are given per English wordtype e. In later sections we report the average valuesover multiple word types and their counts.4 ResultsTurker Experiments To minimize errors fromTurkers, for every HIT we inserted one controlsentence taken from the example sentences ofOntoNotes.
Turker results with either extremely lowfinishing time (<10s), or average accuracy on con-trol sentences lower than accuracy by chance, wererejected.
On average Turkers took 185 seconds tomap 10 sentences in a HIT to their OntoNotes def-inition, receiving $0.10 per HIT.
The total time forannotating 5000 sentences was 22 hours.Turkers had no knowledge about alignments: wehid the aligned French/Chinese sentences from themand these sentences were later processed to computeP1/2/3 values.
Two foreign tokens aligned with thesame source type correspond to two senses of thesame type.
To give an estimate of alignment errors,we manually examined 1/10 of all 5000 sampledChinese-English alignments at random and foundonly 3 of them were wrong: all due to that Englishcontent words were aligned to common Chinesefunction words.
This error rate is much lower thanthat typically reported by alignment tools.
The mainreason is explained in footnote 3: foreign words withtrivial alignment probability were removed beforecalculating P1/2/3 values.
Thus we believe the align-ment was reliable.Probability Estimation Table 2 gives the dis-tribution of senses and word types in the sam-pled words.
Take the second numeric column ofFrench-English as an example: out of 50 words ran-domly sampled, 9 have 2 distinct sense definitionsin OntoNotes.
However, 17 of 50 unique word typeshad exactly 2 distinct senses annotated, out of the100 examples of a given word type: 17 words had2 distinct senses observed.
Of the 9 words with 2official senses, on average 1.9 of those senses wereobserved.Table 3 and Figures 1 and 2 shows the result forP1, P2 and P3 using the {French,Chinese}-Englishcorpora, calculated based on the majority vote ofthree Turkers.
High P2 values suggests that for twotokens of the same type, aligning to the same for-eign type is a reasonable indicator of having thesame meaning.
When working with open domaincorpora, without foreign alignments, the probabil-ity of two English words of the same type having623French-English Chinese-English#senses in OntoNotes 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 18#types in OntoNotes 0 9 7 6 8 6 2 8 4 0 10 6 11 3 8 6 4 1 1#types observed 2 17 9 4 7 7 4 0 0 3 19 9 12 5 2 0 0 0 0avg #senses observed 0 1.9 2.1 3.2 3.8 4.7 6.5 4.9 5.8 0 1.9 2.2 2.9 2.7 4.4 3.8 3.8 3.0 5.0Table 2: Statistics for words sampled from parallel corpora.
Average #senses observed over all words: 2.6 (French-English), and 2.4 (Chinese-English).
The sampled word keep has 18 senses in OntoNotes, with 5 observed.P1 P2 P3 AlphaFrench-English 51.2% 66.7% 59.2% 0.70Chinese-English 59.6% 78.7% 66.7% 0.68Table 3: Expectations of word sense in parallel corpora.Alpha measures how Turkers agreed with themselves.identical meaning is estimated here to be roughly59-67% (59.2% (French), 66.7% (Chinese)).
Thisaccords with results from WSD evaluations, wherethe first-sense heuristic is roughly 75-80% accu-rate (e.g., 80.9% in SemEval?07 (Brody and Lap-ata, 2009)).
Minor algebra translates this into an ex-pected P3 value in a range from 56%?62.5%, up to64%?
68%, which captures our estimates.5Finally for our motivating scenario: values for P1are barely higher than 50%, suggesting that Syn-onymy more regularly holds, but not conclusively.We expect in narrower domains, where words haveless number of senses, this is more noticeable.
Assuggested by Fig.s 1 and 2, less polysemous wordstend to have higher P values.5 ConclusionCurious as to the distinct threads of prior work basedon alternate assumptions of word sense and parallelcorpora, we derived empirical expectations on theshared meaning of tokens of the same type appear-ing in the same corpus.
Our results suggest neitherthe assumption of Polysemy nor Synonymy holdssignificantly more often than the other, at least forindividual words (as opposed to phrases) and for theopen domain corpora used here.
Further, we providean independent data point that supports earlier find-ings as to the expected accuracy of the first senseheuristic in word sense disambiguation.5Assuming worst case: no two tokens that are not the firstsense ever match, and best case: any two tokens not the firstsense always match, then assuming first-sense accuracy of 0.8gives a range on P3 of: (0.82, 0.82 + 0.22) = (0.64, 0.68).Num.Senses.ObservedProbability0.00.20.40.60.81.0lllll1 2 3 4 5 6 7TypeP1 P2 P3Figure 1: French-English values, by number of senses.Num.Senses.ObservedProbability0.20.40.60.81.0ll1 2 3 4 5 6 7TypeP1 P2 P3Figure 2: Chinese-English values, by number of senses.624ReferencesEneko Agirre and Aitor Soroa.
2007.
Semeval-2007Task 02: Evaluating Word Sense Induction And Dis-crimination Systems.
In Proc.
SemEval ?07.Cem Akkaya, Alexander Conrad, Janyce Wiebe, andRada Mihalcea.
2010.
Amazon mechanical turkfor subjectivity word sense disambiguation.
In Proc.NAACL Workshop on CSLDAMT.Ron Artstein and Massimo Poesio.
2008.
Inter-CoderAgreement for Computational Linguistics.
Computa-tional Linguistics, 34(4).Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proc.
ACL.Samuel Brody and Mirella Lapata.
2009.
Bayesian WordSense Induction.
In Proc.
EACL.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings Of The 2009Workshop On Statistical Machine Translation.
InProc.
StatMT.Mona Diab and Philip Resnik.
2002.
An unsupervisedmethod for word sense tagging using parallel corpora.In Proc.
ACL.W.A.
Gale, K.W.
Church, and D. Yarowsky.
1992.
Usingbilingual materials to develop word sense disambigua-tion methods.
In Proc.
TMI.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:the 90% solution.
In Proc.
NAACL-Short.Klaus H. Krippendorff.
2004.
Content Analysis: An In-troduction to Its Methodology.Els Lefever, Ve?ronique Hoste, and Martine De Cock.2011.
Parasense or how to use parallel corpora forword sense disambiguation.
In Proc.
ACL.Gabriel Parent and Maxine Eskenazi.
2010.
Clusteringdictionary definitions using amazon mechanical turk.In Proc.
NAACL Workshop on CSLDAMT.Anna Rumshisky.
2011.
Crowdsourcing word sense def-inition.
In Proc.
LAW V.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In Proc.
EMNLP.625
