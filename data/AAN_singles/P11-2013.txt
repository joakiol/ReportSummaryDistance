Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 71?76,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsInsertion, Deletion, or Substitution?
Normalizing Text Messages withoutPre-categorization nor SupervisionFei Liu1 Fuliang Weng2 Bingqing Wang3 Yang Liu11Computer Science Department, The University of Texas at Dallas2Research and Technology Center, Robert Bosch LLC3School of Computer Science, Fudan University{feiliu, yangl}@hlt.utdallas.edu1fuliang.weng@us.bosch.com2, wbq@fudan.edu.cn3AbstractMost text message normalization approachesare based on supervised learning and rely onhuman labeled training data.
In addition, thenonstandard words are often categorized intodifferent types and specific models are de-signed to tackle each type.
In this paper,we propose a unified letter transformation ap-proach that requires neither pre-categorizationnor human supervision.
Our approach mod-els the generation process from the dictionarywords to nonstandard tokens under a sequencelabeling framework, where each letter in thedictionary word can be retained, removed, orsubstituted by other letters/digits.
To avoidthe expensive and time consuming hand label-ing process, we automatically collected a largeset of noisy training pairs using a novel web-based approach and performed character-levelalignment for model training.
Experiments onboth Twitter and SMS messages show that oursystem significantly outperformed the state-of-the-art deletion-based abbreviation systemand the jazzy spell checker (absolute accuracygain of 21.69% and 18.16% over jazzy spellchecker on the two test sets respectively).1 IntroductionRecent years have witnessed the explosive growthof text message usage, including the mobile phonetext messages (SMS), chat logs, emails, and sta-tus updates from the social network websites suchas Twitter and Facebook.
These text message col-lections serve as valuable information sources, yetthe nonstandard contents within them often degrade2gether (6326) togetha (919) tgthr (250) togeda (20)2getha (1266) togather (207) t0gether (57) toqethaa (10)2gthr (178) togehter (94) togeter (49) 2getter (10)2qetha (46) togethor (29) tagether (18) 2gtr (6)Table 1: Nonstandard tokens originated from ?together?and their frequencies in the Edinburgh Twitter corpus.the existing language processing systems, callingthe need of text normalization before applying thetraditional information extraction, retrieval, senti-ment analysis (Celikyilmaz et al, 2010), or sum-marization techniques.
Text message normalizationis also of crucial importance for building text-to-speech (TTS) systems, which need to determine pro-nunciation for nonstandard words.Text message normalization aims to replace thenon-standard tokens that carry significant mean-ings with the context-appropriate standard Englishwords.
This is a very challenging task due to thevast amount and wide variety of existing nonstan-dard tokens.
We found more than 4 million dis-tinct out-of-vocabulary tokens in the English tweetsof the Edinburgh Twitter corpus (see Section 2.2).Table 1 shows examples of nonstandard tokens orig-inated from the word ?together?.
We can see thatsome variants can be generated by dropping let-ters from the original word (?tgthr?)
or substitut-ing letters with digit (?2gether?
); however, manyvariants are generated by combining the letter in-sertion, deletion, and substitution operations (?to-qethaa?, ?2gthr?).
This shows that it is difficult todivide the nonstandard tokens into exclusive cate-gories.Among the literature of text normalization71(for text messages or other domains), Sproat etal.
(2001), Cook and Stevenson (2009) employed thenoisy channel model to find the most probable wordsequence given the observed noisy message.
Theirapproaches first classified the nonstandard tokensinto various categories (e.g., abbreviation, stylisticvariation, prefix-clipping), then calculated the pos-terior probability of the nonstandard tokens basedon each category.
Choudhury et al (2007) de-veloped a hidden Markov model using hand anno-tated training data.
Yang et al (2009), Pennell andLiu (2010) focused on modeling word abbreviationsformed by dropping characters from the originalword.
Toutanova and Moore (2002) addressed thephonetic substitution problem by extending the ini-tial letter-to-phone model.
Aw et al (2006), Kobuset al (2008) viewed the text message normalizationas a statistical machine translation process from thetexting language to standard English.
Beaufort etal.
(2010) experimented with the weighted finite-state machines for normalizing French SMS mes-sages.
Most of the above approaches rely heavilyon the hand annotated data and involve categorizingthe nonstandard tokens in the first place, which givesrise to three problems: (1) the labeled data is veryexpensive and time consuming to obtain; (2) it ishard to establish a standard taxonomy for categoriz-ing the tokens found in text messages; (3) the lack ofoptimized way to integrate various category-specificmodels often compromises the system performance,as confirmed by (Cook and Stevenson, 2009).In this paper, we propose a general letter trans-formation approach that normalizes nonstandard to-kens without categorizing them.
A large set of noisytraining word pairs were automatically collected viaa novel web-based approach and aligned at the char-acter level for model training.
The system was testedon both Twitter and SMS messages.
Results showthat our system significantly outperformed the jazzyspell checker and the state-of-the-art deletion-basedabbreviation system, and also demonstrated goodcross-domain portability.2 Letter Transformation Approach2.1 General FrameworkGiven a noisy text message T , our goal is to nor-malize it into a standard English word sequence S.b - - - - d a y f - o t o zh u b b i e(1) birthday --> bday(2) photos --> fotoz(4) hubby --> hubbieb i r t h d a yp h o t o sh u b b ys o m e 1 - -(6) someone --> some1s o m e o n en u t h i n -(3) nothing --> nuthinn o t h i n g4 - - e v a -(5) forever --> 4evaf o r e v e rFigure 1: Examples of nonstandard tokens generated byperforming letter transformation on the dictionary words.Under the noisy channel model, this is equivalent tofinding the sequence S?
that maximizes p(S|T ):S?
= argmaxS p(S|T ) = argmaxS(?ip(Ti|Si))p(S)where we assume that each non-standard token Tiis dependent on only one English word Si, that is,we are not considering acronyms (e.g., ?bbl?
for?be back later?)
in this study.
p(S) can be cal-culated using a language model (LM).
We formu-late the process of generating a nonstandard tokenTi from dictionary word Si using a letter transfor-mation model, and use the model confidence as theprobability p(Ti|Si).
Figure 1 shows several exam-ple (word, token) pairs1.
To form a nonstandard to-ken, each letter in the dictionary word can be labeledwith: (a) one of the 0-9 digits; (b) one of the 26 char-acters including itself; (c) the null character ?-?
; (d)a letter combination.
This transformation processfrom dictionary words to nonstandard tokens will belearned automatically through a sequence labelingframework that integrates character-, phonetic-, andsyllable-level information.In general, the letter transformation approach willhandle the nonstandard tokens listed in Table 2 yetwithout explicitly categorizing them.
Note for thetokens with letter repetition, we first generate a setof variants by varying the repetitive letters (e.g.
Ci ={?pleas?, ?pleeas?, ?pleaas?, ?pleeaas?, ?pleeeaas?
}for Ti = {?pleeeaas?
}), then select the maximumposterior probability among all the variants:p(Ti|Si) = maxT?i?Cip(T?i|Si)1The ideal transform for example (5) would be ?for?
to ?4?.But in this study we are treating each letter in the English wordseparately and not considering the phrase-level transformation.72(1) abbreviation tgthr, weeknd, shudnt(2) phonetic sub w/- or w/o digit 4got, sumbody, kulture(3) graphemic sub w/- or w/o digit t0gether, h3r3, 5top, doinq(4) typographic error thimg, macam(5) stylistic variation betta, hubbie, cutie(6) letter repetition pleeeaas, togtherrr(7) any combination of (1) to (6) luvvvin, 2moro, m0rninTable 2: Nonstandard tokens that can be processed by theunified letter transformation approach.2.2 Web based Data Collection w/o SupervisionWe propose to automatically collect training data(annotate nonstandard words with the correspondingEnglish forms) using a web-based approach, there-fore avoiding the expensive human annotation.
Weuse the Edinburgh Twitter corpus (Petrovic et al,2010) for data collection, which contains 97 mil-lion Twitter messages.
The English tweets wereextracted using the TextCat language identificationtoolkit (Cavnar and Trenkle, 1994), and tokenizedinto a sequence of clean tokens consisting of letters,digits, and apostrophe.For the out-of-vocabulary (OOV) tokens consist-ing of letters and apostrophe, we form n Googlequeries for each of them in the form of either?w1 w2 w3?
OOV or OOV ?w1 w2 w3?, where w1to w3 are consecutive context words extracted fromthe tweets that contain this OOV.
n is set to 6 in thisstudy.
The first 32 returned snippets for each queryare parsed and the words in boldface that are differ-ent from both the OOV and the context words arecollected as candidate normalized words.
Amongthem, we further select the words that have longercommon character sequence with the OOV than withthe context words, and pair each of them with theOOV to form the training pairs.
For the OOV tokensconsisting of both letters and digits, we use simplerules to recover possible original words.
These rulesinclude: 1 ?
?one?, ?won?, ?i?
; 2 ?
?to?, ?two?,?too?
; 3 ?
?e?
; 4 ?
?for?, ?fore?, ?four?
; 5 ?
?s?
;6 ?
?b?
; 8 ?
?ate?, ?ait?, ?eat?, ?eate?, ?ight?,?aight?.
The OOV tokens and any resulting wordsfrom the above process are included in the noisytraining pairs.
In addition, we add 932 word pairsof chat slangs and their normalized word forms col-lected from InternetSlang.com that are not coveredby the above training set.These noisy training pairs were further expandedand purged.
We apply the transitive rule on theseinitially collected training pairs.
For example, if thetwo pairs ?
(cause, cauz)?
and ?
(cauz, coz)?
are in thedata set, we will add ?
(cause, coz)?
as another train-ing pair.
We remove the data pairs whose word can-didate is not in the CMU dictionary.
We also removethe pairs whose word candidate and OOV are simplyinflections of each other, e.g., ?
(headed, heading)?,using a set of rules.
In total, this procedure generated62,907 training word pairs including 20,880 uniquecandidate words and 46,356 unique OOVs.22.3 Automatic Letter-level AlignmentGiven a training pair (Si, Ti) consisting of a word Siand its nonstandard variant Ti, we propose a proce-dure to align each letter in Si with zero, one, or moreletters/digits in Ti.
First we align the letters of thelongest common sequence between the dictionaryword and the variant (which gives letter-to-letter cor-respondence in those common subsequences).
Thenfor the letter chunks in between each of the obtainedalignments, we process them based on the followingthree cases:(a) (many-to-0): a chunk in the dictionary wordneeds to be aligned to zero letters in the variant.In this case, we map each letter in the chunk to?-?
(e.g., ?birthday?
to ?bday?
), obtaining letter-level alignments.
(b) (0-to-many): zero letters in the dictionary wordneed to be aligned to a letter/digit chunk in thevariant.
In this case, if the first letter in thechunk can be combined with the previous letterto form a digraph (such as ?wh?
when aligning?sandwich?
to ?sandwhich?
), we combine thesetwo letters.
The remaining letters, or the entirechunk when the first letter does not form a di-graph with the previous letter, are put togetherwith the following aligned letter in the variant.
(c) (many-to-many): non-zero letters in the dictio-nary word need to be aligned to a chunk in thevariant.
Similar to (b), the first letter in the vari-ant chunk is merged with the previous alignmentif they form a digraph.
Then we map the chunkin the dictionary word to the chunk in the vari-ant as one alignment, e.g., ?someone?
aligned to?some1?.2Please contact the first author for the collected word pairs.73The (b) and (c) cases above generate chunk-level(with more than one letter) alignments.
To elimi-nate possible noisy training pairs, such as (?you?,?haveu?
), we keep all data pairs containing digits,but remove the data pairs with chunks involvingthree letters or more in either the dictionary word orthe variant.
For the chunk alignments in the remain-ing pairs, we sequentially align the letters (e.g., ?ph?aligned to ?f-?).
Note that for those 1-to-2 align-ments, we align the single letter in the dictionaryword to a two-letter combination in the variant.
Welimit to the top 5 most frequent letter combinations,which are ?ck?, ?ey?, ?ie?, ?ou?, ?wh?, and the pairsinvolving other combinations are removed.After applying the letter alignment to the col-lected noisy training word pairs, we obtained298,160 letter-level alignments.
Some examplealignments and corresponding word pairs are:e ?
?
?
(have, hav) q ?
k (iraq, irak)e ?
a (another, anotha) q ?
g (iraq, irag)e?
3 (online, 0nlin3) w?wh (watch, whatch)2.4 Sequence Labeling Model for P (Ti|Si)For a letter sequence Si, we use the conditional ran-dom fields (CRF) model to perform sequence tag-ging to generate its variant Ti.
To train the model,we first align the collected dictionary word and itsvariant at the letter level, then construct a featurevector for each letter in the dictionary word, usingits mapped character as the reference label.
This la-beled data set is used to train a CRF model with L-BFGS (Lafferty et al, 2001; Kudo, 2005).
We usethe following features:?
Character-level featuresCharacter n-grams: c?1, c0, c1, (c?2 c?1),(c?1 c0), (c0 c1), (c1 c2), (c?3 c?2 c?1),(c?2 c?1 c0), (c?1 c0 c1), (c0 c1 c2), (c1 c2 c3).The relative position of character in the word.?
Phonetic-level featuresPhoneme n-grams: p?1, p0, p1, (p?1 p0),(p0 p1).
We use the many-to-many letter-phoneme alignment algorithm (Jiampojamarnet al, 2007) to map each letter to multiplephonemes (1-to-2 alignment).
We use three bi-nary features to indicate whether the current,previous, or next character is a vowel.?
Syllable-level featuresRelative position of the current syllable in theword; two binary features indicating whetherthe character is at the beginning or the end ofthe current syllable.
The English hyphenationdictionary (Hindson, 2006) is used to mark allthe syllable information.The trained CRF model can be applied to any En-glish word to generate its variants with probabilities.3 ExperimentsWe evaluate the system performance on both Twitterand SMS message test sets.
The SMS data was usedin previous work (Choudhury et al, 2007; Cook andStevenson, 2009).
It consists of 303 distinct non-standard tokens and their corresponding dictionarywords.
We developed our own Twitter message testset consisting of 6,150 tweets manually annotatedvia the Amazon Mechanical Turk.
3 to 6 turkerswere required to convert the nonstandard tokens inthe tweets to the standard English words.
We extractthe nonstandard tokens whose most frequently nor-malized word consists of letters/digits/apostrophe,and is different from the token itself.
This resultsin 3,802 distinct nonstandard tokens that we use asthe test set.
147 (3.87%) of them have more thanone corresponding standard English words.
Similarto prior work, we use isolated nonstandard tokenswithout any context, that is, the LM probabilitiesP (S) are based on unigrams.We compare our system against three approaches.The first one is a comprehensive list of chat slangs,abbreviations, and acronyms collected by Internet-Slang.com; it contains normalized word forms for6,105 commonly used slangs.
The second is theword-abbreviation lookup table generated by the su-pervised deletion-based abbreviation approach pro-posed in (Pennell and Liu, 2010).
It contains477,941 (word, abbreviation) pairs automaticallygenerated for 54,594 CMU dictionary words.
Thethird is the jazzy spell checker based on the Aspellalgorithm (Idzelis, 2005).
It integrates the phoneticmatching algorithm (DoubleMetaphone) and Leven-shtein distance that enables the interchanging of twoadjacent letters, and changing/deleting/adding of let-ters.
The system performance is measured using then-best accuracy (n=1,3).
For each nonstandard to-ken, the system is considered correct if any of thecorresponding standard words is among the n-bestoutput from the system.74System AccuracyTwitter (3802 pairs) SMS (303 pairs)1-best 3-best 1-best 3-bestInternetSlang 7.94 8.07 4.95 4.95(Pennell et al 2010) 20.02 27.09 21.12 28.05Jazzy Spell Checker 47.19 56.92 43.89 55.45LetterTran (Trim) 57.44 64.89 58.09 70.63LetterTran (All) 59.15 67.02 58.09 70.96LetterTran (All) + Jazzy 68.88 78.27 62.05 75.91(Choudhury et al 2007) n/a n/a 59.9 n/a(Cook et al 2009) n/a n/a 59.4 n/aTable 3: N-best performance on Twitter and SMS datasets using different systems.Results of system accuracies are shown in Ta-ble 3.
For the system ?LetterTran (All)?, we firstgenerate a lookup table by applying the trained CRFmodel to the CMU dictionary to generate up to30 variants for each dictionary word.3 To makethe comparison more meaningful, we also trim ourlookup table to the same size as the deletion ta-ble, namely ?LetterTran (Trim)?.
The trimming wasperformed by selecting the most frequent dictionarywords and their generated variants until the lengthlimit is reached.
Word frequency information wasobtained from the entire Edinburgh corpus.
For boththe deletion and letter transformation lookup tables,we generate a ranked list of candidate words for eachnonstandard token, by sorting the combined scorep(Ti|Si)?C(Si), where p(Ti|Si) is the model con-fidence and C(Si) is the unigram count generatedfrom the Edinburgh corpus (we used counts insteadof unigram probability P (Si)).
Since the string sim-ilarity and letter switching algorithms implementedin jazzy can compensate the letter transformationmodel, we also investigate combining it with our ap-proach, ?LetterTran(All) + Jazzy?.
In this configura-tion, we combine the candidate words from both sys-tems and rerank them according to the unigram fre-quency; since the ?LetterTran?
itself is very effectivein ranking candidate words, we only use the jazzyoutput for tokens where ?LetterTran?
is not veryconfident about its best candidate ((p(Ti|Si)?C(Si)is less than a threshold ?
= 100).We notice the accuracy using the InternetSlanglist is very poor, indicating text message normal-ization is a very challenging task that can hardly3We heuristically choose this large number since the learnedletter/digit insertion, substitution, and deletion patterns tend togenerate many variants for each dictionary word.be tackled by using a hand-crafted list.
The dele-tion table has modest performance given the factthat it covers only deletion-based abbreviations andletter repetitions (see Section 2.1).
The ?Letter-Tran?
approach significantly outperforms all base-lines even after trimming.
This is because it han-dles different ways of forming nonstandard tokensin an unified framework.
Taking the Twitter testset for an example, the lookup table generated by?LetterTran?
covered 69.94% of the total test to-kens, and among them, 96% were correctly normal-ized in the 3-best output, resulting in 67.02% over-all accuracy.
The test tokens that were not coveredby the ?LetterTrans?
model include those generatedby accidentally switching and inserting letters (e.g.,?absolotuely?
for ?absolutely?)
and slangs (?addy?or ?address?).
Adding the output from jazzy com-pensates these problems and boosts the 1-best ac-curacy, achieving 21.69% and 18.16% absolute per-formance gain respectively on the Twitter and SMStest sets, as compared to using jazzy only.
We alsoobserve that the ?LetterTran?
model can be easilyported to the SMS domain.
When combined withthe jazzy module, it achieved 62.05% 1-best accu-racy, outperforming the domain-specific supervisedsystem in (Choudhury et al, 2007) (59.9%) andthe pre-categorized approach by (Cook and Steven-son, 2009) (59.4%).
Regarding different feature cat-egories, we found the character-level features arestrong indicators, and using phonetic- and syllabic-level features also slightly benefits the performance.4 ConclusionIn this paper, we proposed a generic letter trans-formation approach for text message normaliza-tion without pre-categorizing the nonstandard to-kens into insertion, deletion, substitution, etc.
Wealso avoided the expensive and time consuming handlabeling process by automatically collecting a largeset of noisy training pairs.
Results in the Twitterand SMS domains show that our system can signif-icantly outperform the state-of-the-art systems andhave good domain portability.
In the future, wewould like to compare our method with a statisticalmachine translation approach performed at the let-ter level, evaluate the system using sentences by in-corporating context word information, and considermany-to-one letter transformation in the model.755 AcknowledgmentsThe authors thank Deana Pennell for sharing thelook-up table generated using the deletion-based ab-breviation approach.
Thank Sittichai Jiampojamarnfor providing the many-to-many letter-phonemealignment data sets and toolkit.
Part of this workwas done while Fei Liu was working as a researchintern in Bosch Research and Technology Center.ReferencesAiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.
Aphrase-based statistical model for sms text normaliza-tion.
In Proceedings of the COLING/ACL, pages 33?40.Richard Beaufort, Sophie Roekhaut, Louise-Ame?lieCougnon, and Ce?drick Fairon.
2010.
A hybridrule/model-based finite-state framework for normaliz-ing sms messages.
In Proceedings of the ACL, pages770?779.William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Proceedings of Third An-nual Symposium on Document Analysis and Informa-tion Retrieval, pages 161?175.Asli Celikyilmaz, Dilek Hakkani-Tur, and Junlan Feng.2010.
Probabilistic model-based sentiment analysis oftwitter messages.
In Proceedings of the IEEE Work-shop on Spoken Language Technology, pages 79?84.Monojit Choudhury, Rahul Saraf, Vijit Jain, AnimeshMukherjee, Sudeshna Sarkar, and Anupam Basu.2007.
Investigation and modeling of the structure oftexting language.
International Journal on DocumentAnalysis and Recognition, 10(3):157?174.Paul Cook and Suzanne Stevenson.
2009.
An unsuper-vised model for text messages normalization.
In Pro-ceedings of the NAACL HLT Workshop on Computa-tional Approaches to Linguistic Creativity, pages 71?78.Matthew Hindson.
2006.
En-glish language hyphenation dictionary.http://www.hindson.com.au/wordpress/2006/11/11/english-language-hyphenation-dictionary/.Mindaugas Idzelis.
2005.
Jazzy: The java open sourcespell checker.
http://jazzy.sourceforge.net/.Sittichai Jiampojamarn, Grzegorz Kondrak, and TarekSherif.
2007.
Applying many-to-many alignmentsand hidden markov models to letter-to-phoneme con-version.
In Proceedings of the HLT/NAACL, pages372?379.Catherine Kobus, Franc?ois Yvon, and Ge?raldineDamnati.
2008.
Normalizing sms: Are two metaphorsbetter than one?
In Proceedings of the COLING, pages441?448.Taku Kudo.
2005.
CRF++: Yet another CRF took kit.http://crfpp.sourceforge.net/.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of the ICML, pages 282?289.Deana L. Pennell and Yang Liu.
2010.
Normalizationof text messages for text-to-speech.
In Proceedings ofthe ICASSP, pages 4842?4845.Sasa Petrovic, Miles Osborne, and Victor Lavrenko.2010.
The edinburgh twitter corpus.
In Proceedingsof the NAACL HLT Workshop on Computational Lin-guistics in a World of Social Media, pages 25?26.Richard Sproat, Alan W. Black, Stanley Chen, ShankarKumar, Mari Ostendorf, and Christopher Richards.2001.
Normalization of non-standard words.
Com-puter Speech and Language, 15(3):287?333.Kristina Toutanova and Robert C. Moore.
2002.
Pronun-ciation modeling for improved spelling correction.
InProceedings of the ACL, pages 144?151.Dong Yang, Yi cheng Pan, and Sadaoki Furui.
2009.Automatic chinese abbreviation generation using con-ditional random field.
In Proceedings of the NAACLHLT, pages 273?276.76
