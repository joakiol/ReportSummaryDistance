Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2184?2194, Dublin, Ireland, August 23-29 2014.Unsupervised extraction of semantic relations using discourse cuesJuliette Conrath Stergos Afantenos Nicholas Asher Philippe MullerIRIT, Universit?
Toulouse & CNRS, Univ.
Paul Sabatier, 118 Route de Narbonne, 31062 Toulouse{firstname.lastname@irit.fr}AbstractThis paper presents a knowledge base containing triples involving pairs of verbs associated withsemantic or discourse relations.
The relations in these triples are marked by discourse connectorsbetween two adjacent instances of the verbs in the triple in the large French corpus, frWaC.We detail several measures that evaluate the relevance of the triples and the strength of theirassociation.
We use manual annotations to evaluate our method, and also study the coverage ofour resource with respect to the discourse annotated corpus Annodis.
Our positive results showthe potential impact of our resource for discourse analysis tasks as well as other semanticallyoriented tasks like temporal and causal information extraction.1 IntroductionRelational lexical resources, which describe semantic relations between lexical items, have tradition-ally focused on relations like synonymy or similarity in thesauri, perhaps including some hierarchicalsemantic relations like hyperonymy or hyponomy or part-whole relations as in the resource Wordnet (Fel-baum, 1998).
Some distributional thesauri contain more varied relations, see e.g.
(Grefenstette, 1994),however these relations are not typed.
The lexical semantics given by FrameNet (Baker et al., 1998) doesinclude causal and temporal relations, as does Verbocean (Chklovski and Pantel, 2004), but coverage islimited and empirical validation of these resources is partial and still largely remains to be done.Lexical relations, in particular between verbs, are nevertheless crucial for understanding natural lan-guage and for many information processing tasks.
They are needed for textual inference, in which onehas to infer certain relations between eventualities (Hashimoto et al., 2009; Tremper and Frank, 2013),for information extraction tasks, like finding temporal relations between eventualities mentioned in a text(UzZaman et al., 2013), for automatic summarization (Liu et al., 2007), and for discourse parsing in theabsence of explicit discourse markers (Sporleder and Lascarides, 2008).In this paper we report on our efforts to extract semantic relations essential to the analysis of discourseand its interpretation, in which links are made between units of text or rather their semantic representa-tions as in (1) in virtue of semantic information about the two main verbs of those clauses.
(1) The candidate demonstrated his expertise during the interview.
The committee was completelyconvinced.We follow similar work on the extraction of causal, temporal, entailment and presuppositional relationsfrom corpora (Do et al., 2011; Chambers and Jurafsky, 2008; Hashimoto et al., 2009; Tremper and Frank,2013), though our goals and validation methods are different.
While one of our goals is to use thisinformation to improve performance in predicting discourse relations between clauses, we believe thatsuch a lexical resource will have other uses in other tasks in which semantic information is needed.Discourse analysis is a difficult task.
Rhetorical relations are frequently implicit and require for theiridentification inference using diverse sources of lexical and compositional semantic information.
In thePenn Discourse Treebank corpus for example, 52% of the discourse relations are unmarked (Prasad etThis work has been supported by the French agency Agence Nationale de la Recherche (ANR-12-CORD-0004).It is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footer are addedby the organizers.
License details : http://creativecommons.org/licenses/by/4.0/.2184al., 2008).
Accordingly, annotation with discourse structure is a slow and error prone task, and relativelylittle annotated data is currently available ; and so machine learning approaches have had limited suc-cess in this area.
Our approach addresses this problem, using non annotated data with features that canbe automatically detected to find typical contexts (pairs of discourse units) in which various discourserelations occur.
We suppose with (Sporleder and Lascarides, 2008; Braud and Denis, 2013) that suchcontexts display regular lexical associations, in particular with verbs in those discourse units.
An ex-plicit, manually compiled list of all possible associations between two verbs and the semantic relationsthey suggest is infeasible, so we present here an automatic method for compiling such a list, inspired bythe Verbocean project (Chklovski and Pantel, 2004).Our hypothesis, supported by existing corpora, is that adjacent clauses are often arguments of discourserelations.
When these clauses contain certain adverbs or other discourse connectors, we can recoverautomatically one or more discourse relations that we associate with the main verbs of those clauses.
Weextract triples consisting of the two verbs and a semantic relation from a large corpus with the aim ofinferring that such a pair of verbs can suggest the semantic relation even in the absence of an explicitdiscourse marker.
We thus also suppose, with (Sporleder and Lascarides, 2008; Braud and Denis, 2013),that such discourse markers are at least partially redundant ; inferring a discourse relation between twoclauses relies not only the marker but on the two verbs in the related clauses as well.
All of our work hasbeen done on French data.Our paper is organized as follows.
We describe first the knowledge base of verb semantic relationtriples that we have constructed (section 2) ; we then present our methods for isolating verb pairs impli-cating discourse or temporal information (section 3).
A third section describes our methods of evaluation(section 4) and a fourth discusses related work (section 5).2 Exploring relations between verbs in a corpusWe built a knowledge base (V2R)1using the frWaC corpus(Baroni et al., 2009).
frWaC contains about1.6 billion words and was collected on the Web on the .fr domain.
We first parsed the documents in ourcorpus using BONSAI2, which first produced a morpho-syntactic labeling using MElt (Denis and Sagot,2012) and then a syntactic analysis in the form of dependency trees via a French version of the MaltParser(Nivre et al., 2007).Our goal is to find pairs of verbs linked by a relation explicitly marked by a discourse connectorin the corpus, as an indication of a regular semantic relation between the two verbs.
The relations wehave considered are common to most theories of discourse analysis, and they can be grouped into fourclasses (Prasad et al., 2008) : causal (contingency) relations, temporal relations, comparison relations(mainly contrast type relations), and expansion relations (e.g.
elaboration or continuation).To find explicitly marked relations, we used a lexicon of discourse connectors for French, the man-ually constructed LEXCONN resource (Roze et al., 2012)3.
LEXCONN includes 358 connectors andgives their syntactic category as well as associated discourse relations inspired from (Asher and Las-carides, 2003).
Some connectors are ambiguous in that they are associated with several relations.
Weused only the unambiguous connectors (263 in all) in LEXCONN, as a first step.
We regrouped theLEXCONN relations into classes4: explanation relations (parce que/because) and result (ainsi/thus)form the causal class ; temporal relations (puis, apr?s que/then,after that) form the narration group.
Wealso considered other relations like contrast (mais/but), continuation (et, encore/and,again), background(alors que/while), temporal location (quand, pendant que/when), detachment (de toutes fa?ons/anyway),elaboration (en particulier/in particular), alternation (ou/or), commentary (au fait/by the way), rephras-ing (du moins/at least), and evidence (effectivement/indeed).We searched our syntactically parsed corpus for connectors.
When a connector is found and its syn-tactic category verified, if it is close enough to the root of the sentence (at most one dependency linkfrom the root), we look for an inter-sentential link.
The first verb of our pair corresponds in this case1.
Available as an SQLite database at https://dl.dropboxusercontent.com/u/78938139/v2r_db2.
http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html or (Candito et al., 2010)3.
Freely available at : https://gforge.inria.fr/frs/download.php/31052/lexconn.tar.gz.4.
We illustrate each relation with examples of potentially ambiguous markers.2185to the last verb of the previous sentence in the case of connectors for narration, or to its main verb forall the other relations.
We search for the second verb in the pair within a window of two dependencylinks after the connector.
If the connector is not close enough to the root of the sentence, we look fora intra-sentential link.
In this case, we look for the two verbs of the pair in the same sentence within aforward and backward window of two dependency links.If two verbs are found, we examine their local context to better characterize their usage and to improveour results.
If one of the verbs is a modal or support verb, we look for the verb dependent on the modalor support verb and use that as the verb in our pair (if it exists), while keeping the presence of thesupport verb in memory.
Unlike support verbs, we use the presence of a negation or a reflexive particlein the local context to distinguish verbs with different meanings ; e.g., comprendre/understand vs. ne pascomprendre/not understand, agir/act vs. s?agir/concern are all distinct entries.
To get at different verbsenses, we search for idiomatic usage of prepositions using the Dicovalence resource (Van Den Eyndeand Mertens, 2010), which contains valency frames for more than 3700 simple French verbs.
We alsouse the Lefff resource (Sagot, 2010) to find idiomatic verbal locutions.
We also encode other informationthat do not lead to distinct lexical entries : tense, and voice.Once we have obtained a list of verb pairs associated withRelation Distributioncontrast 50,104%cause 33,108%continuation 8,243%narration 6,362%background 1,853%temporal localisation 0.177%detachement 0.149%elaboration 0.002%alternation 0.002%TABLE 1 ?
Distribution of relations inV2R ;commentary, reformulation andevidence occur with negligible fre-quency.a connector, we aggregate this data to get a list of triple types(verb1, verb2, relation).
Given that we have used only unam-biguous connectors (so classified by LEXCONN), the associ-ation of a relation with a connector is immediate.
We asso-ciate to each triple type the number of intra-sentential, inter-sentential and total number of occurrences.
The other featuresmentioned above are stored in a separate table.Our method has isolated more than 1 million distinct typesof triples for V2R and 2 million occurrences, of which 95% areintra-sentential5.
Among these triples, 6.2% have 5 or moreoccurrences.Table 1 summarizes the distribution of triples by relationin V2R.
Note that triples with contrast and causal relationscomprise the majority.
This does not mean that these are themost frequent relations in the corpus but only that they are themost frequently marked by the connectors we considered.
Thismakes for a very different distribution than that of the French manually annotated discourse corpus Ann-odis (Afantenos et al., 2012).3 Measuring the association of a pair of verbs with a relationIn the last section we presented our extraction method.
We now present the measures we have used torank verb pairs with respect to the strength of their association with a particular discourse relation.
Weadapted versions of standard lexical association measures like PMI (pointwise mutual information) andtheir variants, as well as some measures specific to the association of a causal relation between items (Doet al., 2011).
We also experimented with a new measure specifically designed for our knowledge base.Measures of lexical association used in research on co-occurrences in distributional semantics pickout significant associations, taking into account the frequency of the related items.
We examined over10 measures ; we discuss the ones with the best results (see section 4).
One simple measure, PMI, andits variants, normalized, local (Evert, 2005), discounted (Lin and Pantel, 2002), which are designedto reduce biases in the original measure, work well.
The idea behind PMI is to estimate whether theprobability of the co-occurrence of two items is greater than the a priori probability of the two itemsappearing independently.
In distributional semantics, the measure is also used to estimate the significanceof two items co-occurring with a particular grammatical dependency relation like the subject or objectrelation between an NP and a verb.
This use of PMI measures over triples in distributional semanticsfits perfectly with our task of measuring the significance of triples consisting of a pair of verbs and5.
The low proportion of inter-sentential occurrences comes from our conservative scheme for finding these occurrences,which uses only those connectors at the beginning of the second sentence.
Other schemes are possible but would, we fear,introduce too much noise into the data.2186a particular semantic or discourse relation ; our PMI measures estimate whether the co-occurrence oftwo items with a particular discourse relation is higher than the a priori probability of the three itemsoccurring independently.
Our measures consider co-occurrences of two lexical items in a certain relationdenoted by an explicit discourse marker.
PMI and normalized PMI are defined as :PMI = log(P (V1, V2, R)P (V1)?
P (V2)?
P (R))PMI _normalized =PMI?2 log(P (V1, V2, R))Indeed, when we have a complete co-occurrence of the three items, we have : P (V1) = P (V2) =P (R) = P (V1, V2, R), and PMI = ?2 log(P (V1, V2, R)).
The values of normalized PMI lie between?1 and 1, approaching ?1 when the items never appear together, taking the value 0 in the case ofindependence, and the value 1 when they always appear together.
We also considered a weighted PMImeasure (Lin and Pantel, 2002) that corrects the bias of PMI for rare triples.A specificity measure (Mirroshandel et al., 2013), originally used to measure the precision of subcat-egorization frames, also performed well :specificity =13?
(P (V1, V2, R)?iP (V1, Vi, R)+P (V1, V2, R)?iP (Vi, V2, R)+P (V1, V2, R)?iP (V1, V2, Ri))A version of Do et al.
(2011)?s measure for triples involving causal relations did not fare so well onother types of relation.
The definition of the measure can be found in (Do et al., 2011).6Finally, we investigated a measure that evaluates the contribution of each element in the triple to thesignificance measure (this measure is similar to specificity).Wcombined(V1, V2, R) =13(wV1+ wV2+ wR)with : wV1=P (V1,V2,R)maxi(P (Vi,V2,R)), wV2=P (V1,V2,R)maxi(P (V1,Vi,R)), and wR=P (V1,V2,R)maxi(P (V1,V2,Ri)).4 Evaluating extracted relationsWe evaluated V2R in several ways ; we provided : (i) an intrinsic evaluation of the relations betweenverbs (section 4.1) and (ii) an extrinsic evaluation where we evaluated the coverage of the resource on adiscourse annotated corpus and its potential to help in predicting discourse relations in contexts with noexplicit marking (section 4.2).4.1 Intrinsic evaluationOur intrinsic evaluation first evaluates the feasibility of assigning an ?inherent?
semantic link to a verbpair, independently of any linguistic context.
For example, is it possible to judge that there is a typicalcausality link between push and fall, in scenarios where they share some arguments (subject, object, ...),these scenarios being left to the annotator?s imagination (section 4.1.1).
In a second stage, we selectedseveral verb pairs linked with different relations in V2R, and 40 contexts in which these verbs occurtogether in the original corpus, to judge the semantic link in context (section 4.1.2).In both cases we restricted the study to three relation groups : causal, contrastive, and narrative.
Theseare the most often marked relations and correspond to different types of links with a meaningful semanticaspect (as opposed to the ?continuation?
relation for instance, which is often marked too).4.1.1 Out of context evaluationFor out of context judgments, we adopted the following protocol : one of the authors chose for eachrelation 100 verbs with equivalent proportions of good and bad normalized PMI scores.
Then the other6.
We simplified their measure by ignoring IDF (inverse document frequency) and the distance between the verbs, as neithermeasure applies to our task.2187three authors judged the validity of associating each of the 300 pairs with the corresponding relation,without any knowledge of the source of these pairs.We measured the inter-annotator agreements with Cohen?s Kappa (Carletta, 1996), which resulted in :0.17 for cause, 0.42 for narration and 0.56 for contrast as mean values.
If a 0.6 kappa serves a measure fora feasible semantic judgment task, out of context judgments appear very difficult, with only contrastivepairs as a relative exception.
We decided to only consider judgments about contrast, after an adjudicationphase, and we evaluated the measures presented in section 3 to see if they could discriminate betweenthe two verb groups, those judged positively or negatively according to human annotations.
A Mann-Whitney U statistical test showed all of our measures to be discriminative, with the exception of rawco-occurrence counts for which p>0.05.4.1.2 In context evaluationWe also judged associations in context.Verb pair translation association/humanCauseinviter/souhaiter invite/wish 12.8%promettre/?lire promise/elect 25.6%aimer/trouver like/find 38.5%b?n?ficier/cr?er benefit/create 51.3%aider/gagner help/win 53.8%Contrastproposer/refuser propose/refuse 59.0%augmenter/diminuer increase/decrease 64.1%tenter/?chouer try/fail 64.1%gagner/perdre win/lose 71.8%autoriser/interdire authorize/forbid 74.4%Narrationparler/r?fl?chir speak/think 42.5%acheter/essayer buy/try 70.0%atteindre/traverser reach/cross 77.5%commencer/finir begin/end 80.0%envoyer/transmettre send/transmit 82.5%TABLE 2 ?
For each relation, the list of verb pairs manu-ally evaluated in context (and an approximate translation),and the association percentage resulting from the adjudi-cated human annotation.This task was easier and also gave morefine-grained results, because with it we canquantify the degree of association, and thetypicality of the link, as a proportion of con-texts where the two verbs appear togetherin a given semantic relation.
We can thenobserve if this proportion is correlated withthe association measures we already pre-sented.
Nevertheless, this is a costly way ofevaluating a verb pair, as we require a num-ber of judgments on each pair.
It is also noteasy to sample the possible pairs with dif-ferent values to be able to observe signif-icant correlations, because we cannot pre-dict in advance how they will be judged bythe annotators.We selected 40 contexts for each of the15 pairs of verbs we chose, 5 for each of thetarget relation (cause, narration, contrast).Selected pairs range over different values ofnormalized PMI, again chosen by one of theauthors independently of the others, whoannotated the 600 contexts.
Prior to adjudi-cation, raw agreement was 78% on average,for an average kappa of 0.46 (and a max-imum of 0.49).
These values seem moder-ately good, as the task is also rather diffi-cult.Table 2 shows the results after adjudication : for each pair, the proportion of contexts in which theconsidered relation is judged to appear.We computed two correlation values between the association ratio in contexts manually annotatedand each association measure considered : one based on all annotated contexts, and one on the subsetof contexts devoid of explicit markers of a semantic relation (implicit contexts).
The latter is importantto quantify the actual impact of the method, since explicit marking is already used as the basis of verbassociation in the same corpus.
Implicit contexts, however, never appeared in the computation of the verbpair associations.2188normalizedPMIspecificity W_combineddiscountedPMIPMIlocalPMIU_doraw fre-quencyGlobalcorrelation0.749 0.747 0.720 0.716 0.709 0.434 0.376 0.170Correlationfor implicitinstances0.806 0.760 0.738 0.761 0.756 0.553 0.499 0.242TABLE 3 ?
Pearson correlation for the 15 pairs considered and measures from section 3, in decreasingorder.Table 3 shows that mutual information measures are well correlated with human annotations, andthat our W_combined seems useful too.
We also observed results on each relation separately, althoughone should be careful drawing conclusions from these results since the correlations are then computedon 5 points only.
These results (not shown here) show a lot of variation between relations.
The U_domeasure, designed for causal relations, does indeed produce good results for these relations, but does notgeneralize well to our other chosen relations.Also, local PMI seems to work very well on narration and causal relations.
This needs to be confirmedwith more verb pairs.We conclude that the best three measures are : normalized PMI, specificity, and W_combined.
The lasttwo assign their maximal value to several pairs, so we used them in a lexicographical ordering to sort allassociated pairs, using normalized PMI to break ties.Verb pair Translation Relationabandonner / mener abandon / lead backgroundne pas s?arr?ter / rouler not stop / drive narrationdonner satisfaction sur / r?
?lire give satisfaction concerning / re-elect continuationemporter / ne pas cesser take away / not stop summaryemprunter / assurer borrow / insure causene pas manquer / prolonger not miss / prolong detachmentratifier / trembler ratify / tremble backgroundavoir honte / faire piti?
be ashamed / cause pity causeavoir droit / cotiser pour be entitled / contribute to templocne pas repr?senter / st?r?otyper not represent / stereotype templocTABLE 4 ?
Ten best triples in the database.Table 4 shows the best triples with our lexicographical ranking.4.2 Extrinsic evaluationIn order to evaluate the performance of our resource relative to its main intended application?predicting rhetorical relations in text, we intend to use our association measures as additional featuresto an inductive prediction model.
Whether this evaluation produces results depends on the proportionof cases in which this information could help and on the coverage of our resource with respect to thesecases.
We used the Annodis corpus (Afantenos et al., 2012), a set of French texts annotated with rhetori-cal relations, for our study.To improve existing models, a significant number of the predictions to be made must involve a verbpair for which we have information in the resource.
A first indication of its usefulness is also that theverb pair appears most frequently with the relation group to which the annotation belongs, for instancethe fact that two verbs are related with a causal relation whenever we want to predict an explanation.
Thisis interesting only in the absence of an explicit marking of the target relation, i.e for implicit relations.2189Beyond that, it should be interesting to use all the available information about other semantic relationstoo : for instance a potential causal link between two events could indicate the relevance of a temporal linkfor the prediction of a relation.
We relied again on the Lexconn marker database.
As an approximationwe considered that a relation between two discourse units is explicit when a Lexconn marker is presentin any of the two segments, and one of the potential senses of the marker is the annotated relation.This may overestimate the number of explicit instances but ensures that all implicit instances are indeedimplicit (assuming a good enough coverage of the marker resource).
The Annodis corpus lists rhetoricalrelations between elementary discourse units (EDUs), typically clauses, and complex discourse units(sets of EDUs) ; as a simplification we only consider EDUs, since the question of what is a main verb ofa complex unit is difficult to answer.
This is a relatively small corpus, as it includes about 2000 instancesof relations between elementary discourse units.Table 5 present results for coverage, for the main relations in the annotated corpus.
Note that only asmall part of the set of relations between EDUs is considered when we restrict instances to both EDUswith verbs (about 20% of the whole).
It turns out that a lot of EDUs in Annodis are short segments(incises, detached segments, ...).global narration cause contrast elab.
cont.
BG otherAnnodis pairs 427 73 67 41 96 92 24 16Annodis pairs ?
V2R 68.9 71.2 70.8 78.0 68.3 61.9 74.1 62.5Annodis triples ?
V2R 26.5 34.2 50.0 70.7 0.0 20.6 11.1 0.0Implicit Annodis pairs 83.4 71.2 79.2 36.6 99.0 94.8 88.9 100.0Implicit Annodis pairs ?
V2R(any relation)56.9 52.1 54.2 31.7 67.3 58.8 66.7 62.5Implicit Annodis triples ?V2R (with correct relation)17.7 24.7 40.3 31.7 0.0 19.6 11.1 0.0TABLE 5 ?
Coverage of verb pairs in V2R with respect to EDU pairs in the Annodis corpus containingtwo verbs.
Except for the first line, all numbers are percentages.
Pair = verb pairs in the EDUs linkedby a rhetorical relation R, Triple=verb pair associated with a relation R in V2R, BG = Background,cont.=continuation, elab.=elaboration.Our table includes : the proportion of verb pairs found in Annodis EDUs that appear in V2R, theproportion of triples from Annodis that appear in V2R (with the correct relation), and the restrictionof these proportions to implicit contexts in Annodis.
Except for a few exceptions due to lemmatisationerrors, all verbs in Annodis are in V2R in at least one pair, and we can see that the pairs in V2R covermost of the pairs appearing in Annodis (almost 70% globally and between 60 and 80% depending on therelation), and a little less of implicit cases (around 55% on average).
We note that a high proportion of theimplicit cases contains verb pairs that have been collected in a marked context, even for rarely markedrelations like elaboration or continuation?contexts with these relations are the majority in Annodis.Furthermore more than half of these contexts are associated with the right relation in V2R.
Thus thehypothesis of the partial redundancy of connectors appears useful when isolating verbal associationsrelevant for discourse from a large corpus.
We also looked at semantic neighbors of the verbs in V2R butthis did not increase coverage significantly.A good test of the predictive power of the semantic information we gathered is also to include theassociation measures as additional features to a predictive model, to improve classically low resultson implicit discourse relations.
The only available discursive corpus in French, Annodis, is small, andas shown above only about 400 instances have a verb in both related EDUs.
We trained and testeda maximum entropy model with and without the association measures as features, on top of featurespresented in Muller et al.
(2012), who trained a relation model on the same corpus.
We did a 10-foldcross-validation on the 400 instance subset as evaluation, and did not find a significant difference betweenthe two set-ups (F1 score was in the range .40?.42, similar to the cited paper), which is unsurprising2190given the size of the subset.
We plan to evaluate our method relative to discourse parsing by building anEnglish resource like V2R ; we will then be able to use the much larger PDTB corpus (10 times as largeas Annodis) as a source of implicit discourse relations.
This should prove a much more telling evaluationof the usefulness of association measures in predicting implicit discourse relations.5 Related workThere are two different groups of related work.
The first group aims to alleviate the lack of annotateddata for discourse parsing by using a weakly supervised approach, exploiting the presence of discourseconnectors in a large non-annotated corpus.
Each pair of elementary discourse units is automaticallyannotated with the discourse relation triggered by the presence of the connector (connectors are oftenfiltered for non-discursive uses).
Those connectors are afterwards eliminated from the corpus so that themodel trained on this dataset will not be informed by the presence of those connectors.
The pioneeringarticle in this group is Marcu and Echihabi (2002).
Such learning methods with such ?artificial data?obtain low scores, barely above chance as shown in Sporleder and Lascarides (2008).
Braud and Denis(2013) observe that the performance of a classifier for the prediction of implicit relations is much lowerwhen using ?artificial?
data than on ?natural?
data (implicit relations annotated by a human being).
Theypropose a method which exploits these two different kinds of datasets together in various mixtures andon the level of the prediction algorithm, obtaining thus a significant improvement on the Annodis corpus.Our approach is different and complementary ; we isolate the semantic relations between pairs of verbs.We can use that as a feature on discourse units for discourse parsing but it has other uses as well.A second group aims at identifying discourse relations (implicit or not) by focusing on the use of fine-grained lexical relations as another feature during the training phase.
Most of this work focuses mainlyon the use of lexical relations between two verbs.
Chklovski and Pantel (2004), for example, rely onspecific patterns constructed manually for each semantic relation between (similarity, strength, antonymy,enablement and temporal happens-before).
They use the web as a corpus in order to estimate the PMIbetween a pattern and a pair of verbs (a precise measurement cannot be achieved over the web since theprobability of a pattern is not precisely known over all the web).
A threshold on the value of the PMI(manually fixed) permits thus to determine the pairs of verbs that are related to the relation denoted by thepattern.
In the same spirit, Kozareva (2012) is using a weakly supervised approach for the extraction ofpairs of verbs that are potentially implied in a cause-effect relation.
Her method consists in using patternsapplied to the web in order to extract pairs and generate new seeds.
Do et al.
(2011) focus on causalrelations and take into account not only verbs but also event denoting nouns.
According to this paper,an event is denoted by a predicate with a specific number of arguments and thus the association of theevents is the sum of the association between predicates, between predicates and arguments and betweenarguments.
Their association measures are based on PMI and are quite complex.
Our results show thattheir measures do not generalize well to association with all discourse relations.
Using Gigaword as acorpus and a reimplementation of Lin et al.
(2014) they have extracted discourse relations.
An inductivelogic programming approach is finally used exploiting the interaction between causal pairs and discourserelations in order to extract causal links.
Those papers focus on specific relations with the exception ofChklovski and Pantel (2004) who do not present a systematic evaluation of their results.
An importantdifference of our approach is also to consider predicates and their negation as separate entries.Finally, we mention the approaches which while focusing on the learning of discourse structures,nonetheless enrich their systems with lexical information.
Feng and Hirst (2012) have used HILDA (Her-nault et al., 2010) adding more features.
A specific family of features represents lexical similarity basedon the hierarchical distance in VERBNET and WORDNET.
In a similar fashion, Wellner et al.
(2006) fo-cus on intra-sentential discourse relations adding lexical information on the features based on measuresproposed by Lin (1998) calculated on the British National Corpus.
Those approaches use thus only infor-mation on lexical similarity without semantically typing this link.
The impact of this information seemslimited.
As far as evaluation is concerned, our method is similar to that followed in Tremper and Frank(2013) for implication relations combining in and out of context evaluation for verbal associations.
Theirinter-annotator agreement is similar to ours (0.42-0.44 of Kappa) with very different choices : the anno-2191tators were supposed to discriminate verbal links between the different possible sub-cases.
The pairs ofverbs were identified by the system of Lin and Pantel.
These authors also present a classification modelamong the different types of relationships, assuming that two verbs are semantically related.6 ConclusionsWe have presented a knowledge base of triples involving pairs of verbs associated with semantic ordiscourse relations.
We extracted these triples from the large French corpus, frWaC, using discourse con-nectors as markers of relations between two adjacent clauses containing verbs.
We investigated severalmeasures to give the strength of association of a pair of verbs with a relation.
We used manual annotationsto evaluate our method and select the best measures, and we also studied the coverage of our resource onthe discourse annotated corpus Annodis.
Our positive results show our resource has the potential to helpdiscourse analysis as well as other semantically oriented tasks.2192ReferencesStergos Afantenos, Nicholas Asher, Farah Benamara, Myriam Bras, Cecile Fabre, Mai Ho-Dac, Anne Le Draoulec,Philippe Muller, Marie-Paul Pery-Woodley, Laurent Prevot, Josette Rebeyrolles, Ludovic Tanguy, MarianneVergez-Couret, and Laure Vieu.
2012.
An empirical resource for discovering cognitive principles of discourseorganisation : the ANNODIS corpus.
In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet U?gurDo?gan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the EightInternational Conference on Language Resources and Evaluation (LREC?12), Istanbul, Turkey, may.
EuropeanLanguage Resources Association (ELRA).Nicholas Asher and Alex Lascarides.
2003.
Logics of Conversation.
Studies in Natural Language Processing.Cambridge University Press, Cambridge, UK.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998.
The Berkeley FrameNet Project.
In Proceedings ofthe COLING-ACL, Montreal, Canada.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta.
2009.
The wacky wide web : a collectionof very large linguistically processed web-crawled corpora.
Language resources and evaluation, 43(3) :209?226.Chlo?
Braud and Pascal Denis.
2013.
Identification automatique des relations discursives "implicites" ?
partirde donn?es annot?es et de corpus bruts.
In TALN - 20?me conf?rence du Traitement Automatique du LangageNaturel 2013, volume 1, pages 104?117, Sables d?Olonne, France, June.Marie Candito, Beno?t Crabb?, and Pascal Denis.
2010.
Statistical french dependency parsing : Treebank conver-sion and first results.
In LREC.Jean Carletta.
1996.
Assessing agreement on classification tasks : the kappa statistic.
Computational linguistics,22(2) :249?254.Nathanael Chambers and Dan Jurafsky.
2008.
Unsupervised Learning of Narrative Event Chains.
In Proceedingsof ACL-08 : HLT, pages 789?797, Columbus, Ohio, June.
Association for Computational Linguistics, Morris-town, NJ, USA.Timothy Chklovski and Patrick Pantel.
2004.
Verbocean : Mining the web for fine-grained semantic verb relations.In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 33?40, Barcelona, Spain, July.Association for Computational Linguistics.P.
Denis and B. Sagot.
2012.
Coupling an annotated corpus and a lexicon for state-of-the-art pos tagging.
Lan-guage Resources and Evaluation, (46) :721?736.Quang Do, Yee Seng Chan, and Dan Roth.
2011.
Minimally supervised event causality identification.
In Proceed-ings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 294?303, Edinburgh,Scotland, UK., July.
Association for Computational Linguistics.Stefan Evert.
2005.
The statistics of word cooccurrences.
Ph.D. thesis, Stuttgart University.C.
Felbaum.
1998.
Wordnet, an Electronic Lexical Database for English.
Cambridge : MIT Press.Vanessa Wei Feng and Graeme Hirst.
2012.
Text-level discourse parsing with rich linguistic features.
In Proceed-ings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1 : Long Papers),pages 60?68, Jeju Island, Korea, July.
Association for Computational Linguistics.G.
Grefenstette.
1994.
Explorations in automatic thesaurus discovery.
Springer.Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda, Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama.
2009.Large-scale verb entailment acquisition from the Web.
In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages 1172?1181, Singapore, August.
Association for ComputationalLinguistics.Hugo Hernault, Helmut Prendinger, David A. duVerle, and Mitsuru Ishizuka.
2010.
HILDA : A Discourse ParserUsing Support Vector Machine Classification.
Dialogue and Discourse, 1(3) :1?33.Zornitsa Kozareva.
2012.
Cause-effect relation learning.
In Workshop Proceedings of TextGraphs-7 : Graph-based Methods for Natural Language Processing, pages 39?43, Jeju, Republic of Korea, July.
Association forComputational Linguistics.Dekang Lin and Patrick Pantel.
2002.
Concept discovery from text.
In Proceedings of Coling 2002, pages 1?7.Association for Computational Linguistics.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
A PDTB-styled end-to-end discourse parser.
NaturalLanguage Engineering, 20(2) :151?184.Dekang Lin.
1998.
Automatic retrieval and clustering of similar words.
In Proceedings of the 36th ACL and 17thCOLING joint conference, volume 2, pages 768?774, Montreal.Maofu Liu, Wenjie Li, Mingli Wu, and Qin Lu.
2007.
Extractive summarization based on event term clustering.
InProceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion VolumeProceedings of the Demo and Poster Sessions, pages 185?188, Prague, Czech Republic, June.
Association forComputational Linguistics.2193Daniel Marcu and Abdessamad Echihabi.
2002.
An Unsupervised Approach to Recognizing Discourse Relations.In Proceedings of ACL, pages 368?375.Seyed Abolghasem Mirroshandel, Alexis Nasr, and Beno?t Sagot.
2013.
Enforcing subcategorization constraints ina parser using sub-parses recombining.
In Proceedings of the 2013 Conference of the North American Chapterof the Association for Computational Linguistics : Human Language Technologies, pages 239?247, Atlanta,Georgia, June.
Association for Computational Linguistics.Philippe Muller, Stergos Afantenos, Pascal Denis, and Nicholas Asher.
2012.
Constrained decoding for text-level discourse parsing.
In Proceedings of COLING 2012, pages 1883?1900, Mumbai, India, December.
TheCOLING 2012 Organizing Committee.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G?lsen Eryigit, Sandra K?bler, Svetoslav Marinov, andErwin Marsi.
2007.
Maltparser : A language-independent system for data-driven dependency parsing.
NaturalLanguage Engineering, 13(2) :95?135.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie L. Webber.2008.
The Penn Discourse TreeBank 2.0.
In Proceedings of LREC 2008.Charlotte Roze, Laurence Danlos, and Philippe Muller.
2012.
Lexconn : A french lexicon of discourse connectives.Discours, (10).Beno?t Sagot.
2010.
The lefff, a freely available and large-coverage morphological and syntactic lexicon forfrench.
In 7th international conference on Language Resources and Evaluation (LREC 2010).Caroline Sporleder and Alex Lascarides.
2008.
Using Automatically Labelled Examples to Classify RhetoricalRelations : An Assessment.
Natural Language Engineering, 14(3) :369?416, July.Galina Tremper and Anette Frank.
2013.
A discriminative analysis of fine-grained semantic relations includingpresupposition : Annotation and classification.
Dialogue & Discourse, 4(2) :282?322.Naushad UzZaman, Hector Llorens, Leon Derczynski, James Allen, Marc Verhagen, and James Pustejovsky.
2013.Semeval-2013 task 1 : Tempeval-3 : Evaluating time expressions, events, and temporal relations.
In SecondJoint Conference on Lexical and Computational Semantics (*SEM), Volume 2 : Proceedings of the SeventhInternational Workshop on Semantic Evaluation (SemEval 2013), pages 1?9, Atlanta, Georgia, USA, June.Association for Computational Linguistics.K.
Van Den Eynde and P. Mertens, 2010.
Le dictionnaire de valence : Dicovalence.
Leuven : Universit?
deLeuven.
[http ://bach.
arts.
kuleuven.
be/dicovalence/].Ben Wellner, James Pustejovsky, Catherine Havasi, Anna Rumshisky, and Roser Saur?.
2006.
Classification ofdiscourse coherence relations : an exploratory study using multiple knowledge sources.
In Proceedings ofthe 7th SIGdial Workshop on Discourse and Dialogue, SigDIAL ?06, pages 117?125, Stroudsburg, PA, USA.Association for Computational Linguistics.2194
