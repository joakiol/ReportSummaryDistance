Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1733?1743,Beijing, China, July 26-31, 2015. c?2015 Association for Computational LinguisticsAccurate Linear-Time Chinese Word Segmentation via EmbeddingMatchingJianqiang MaSFB 833 and Department of LinguisticsUniversity of T?bingen, Germanyjma@sfs.uni-tuebingen.deErhard HinrichsSFB 833 and Department of LinguisticsUniversity of T?bingen, Germanyeh@sfs.uni-tuebingen.deAbstractThis paper proposes an embedding match-ing approach to Chinese word segmenta-tion, which generalizes the traditional se-quence labeling framework and takes ad-vantage of distributed representations.
Thetraining and prediction algorithms havelinear-time complexity.
Based on the pro-posed model, a greedy segmenter is de-veloped and evaluated on benchmark cor-pora.
Experiments show that our greedysegmenter achieves improved results overprevious neural network-based word seg-menters, and its performance is competi-tive with state-of-the-art methods, despiteits simple feature set and the absence of ex-ternal resources for training.1 IntroductionChinese sentences are written as character se-quences without word delimiters, which makesword segmentation a prerequisite of Chinese lan-guage processing.
Since Xue (2003), most workhas formulated Chineseword segmentation (CWS)as sequence labeling (Peng et al, 2004) with char-acter position tags, which has lent itself to struc-tured discriminative learning with the benefit ofallowing rich features of segmentation configura-tions, including (i) context of character/word n-grams within local windows, (ii) segmentation his-tory of previous characters, or the combinations ofboth.
These feature-based models still form thebackbone of most state-of-the art systems.Nevertheless, many feature weights in suchmodels are inevitably poorly estimated because thenumber of parameters is so large with respect tothe limited amount of training data.
This has mo-tivated the introduction of low-dimensional, real-valued vectors, known as embeddings, as a toolto deal with the sparseness of the input.
Em-beddings allow linguistic units appearing in sim-ilar contexts to share similar vectors.
The suc-cess of embeddings has been observed in manyNLP tasks.
For CWS, Zheng et al (2013) adaptedCollobert et al (2011) and uses character embed-dings in local windows as input for a two-layer net-work.
The network predicts individual characterposition tags, the transitions of which are learnedseparately.
Mansur et al (2013) also developed asimilar architecture, which labels individual char-acters and uses character bigram embeddings asadditional features to compensate the absence ofsentence-level modeling.
Pei et al (2014) im-proved upon Zheng et al (2013) by capturing thecombinations of context and history via a tensorneural network.Despite their differences, these CWS ap-proaches are all sequence labeling models.
In suchmodels, the target character can only influence theprediction as features.
Consider the the segmen-tation configuration in (1), where the dot appearsbefore the target character in consideration and thebox (2) represents any character that can occur inthe configuration.
In that example, the known his-tory is that the first two characters??
?China?
arejoined together, which is denoted by the underline.
(1) ???2??
(where 2 ?
{?,?, ...})(2) ???
??
?China-style especially?
(3) ??
??
?
?besides Chinese spec.
?For possible target characters, ?
?wind?
and ?
?rule?, the correct segmentation decisions for themare opposite, as shown in (2) and (3), respectively.In order to correctly predict both, current modelscan set higher weights for target character-specificfeatures.
However, in general, ?
is more likelyto start a new word instead of joining the exist-ing one as in this example.
Given such conflictingevidence, models can rarely find optimal featureweights, if they exist at all.1733The crux of this conflicting evidence problemis that similar configurations can suggest oppositedecisions, depending on the target character andvice versa.
Thus it might be useful to treat segmen-tation decisions for distinct characters separately.And instead of predicting general segmentation de-cisions given configurations, it could be beneficialto model thematching between configurations andcharacter-specific decisions.To this end, this paper proposes an embed-ding matching approach (Section 2) to CWS, inwhich embeddings for both input and output arelearned and used as representations to counteractsparsities.
Thanks to embeddings of character-specific decisions (actions) serving as both inputfeatures and output, our hidden-layer-free archi-tecture (Section 2.2) is capable of capturing pre-diction histories in similar ways as the hidden lay-ers in recurrent neural networks (Mikolov et al,2010).
We evaluate the effectiveness of the modelvia a linear-time greedy segmenter (Section 3) im-plementation.
The segmenter outperforms previ-ous embedding-based models (Section 4.2) andachieves state-of-the-art results (Section 4.3) on abenchmark dataset.
The main contributions of thispaper are:?
A novel embedding matching model for Chi-nese word segmentation.?
Developing a greedy word segmenter, whichis based on the matching model and achievescompetitive results.?
Introducing the idea of character-specific seg-mentation action embeddings as both featureand output, which are cornerstones of themodel and the segmenter.2 Embedding Matching Models forChinese Word SegmentationWe propose an embedding based matching modelfor CWS, the architecture of which is shown inFigure 1.
The model employs trainable embed-dings to represent both sides of the matching,which will be specified shortly, followed by detailsof the architecture in Section 2.2.2.1 Segmentation as Configuration-ActionMatchingOutput.
The word segmentation output of a char-acter sequence can be described as a sequence ofcharacter-specific segmentation actions.
We useseparation (s) and combination (c) as possibleactions for each character, where a separation ac-tion starts a new word with the current character,while a combination action appends the characterto the preceding ones.
We model character-actioncombinations instead of atomic, character inde-pendent actions.
As a running example, sentence(4b) is the correct segmentation for (4a), which canbe represented as the sequence (?
-s,?
-s,?
-c,?
-s,?
-s,?
-c,?
-c) .
(4) a.
???????b.
?
??
?
???c.
?The cat occupied the crib?Input.
The input are the segmentation configura-tions for each character under consideration, whichare described by context and history features.
Thecontext features of captures the characters that arein the same sentence of the current character andthe history features encode the segmentation ac-tions of previous characters.?
Context features.
These refer to characterunigrams and bigrams that appear in the lo-cal context window of h characters that cen-ters at ci, where ciis ?
in example (4) andh = 5 is used in this paper.
The template forfeatures are shown in Table 1.
For our exam-ple, the uni- and bi-gram features would be:?, ?, ?, ?, ?
and?
?, ?
?, ?
?, ?
?, respectively.?
History features.
To make inferencetractable, we assume that only previous lcharacter-specific actions are relevant, wherel = 2 for this study.
In our example, ?
-sand ?
-s are the history features.
Such fea-tures capture partial information of syntacticand semantic dependencies between previouswords, which are clues for segmentation thatpure character contexts could not provide.
Adummy character START is used to representthe absent (left) context characters in the caseof the first l characters in a sentence.
And thepredicted action for the START symbol is al-ways s.Matching.
CWS is now modeled as the match-ing of the input (segmentation configuration) andoutput (two possible character-specific actions) foreach character.
Formally, a matching model learns1734Figure 1: The architecture of the embedding matching model for CWS.
The model predicts the seg-mentation for the character?
in sentence (4), which is the second character of word??
?occupy?.
Bothfeature and output embeddings are trainable parameters of the model.Group Feature templateunigram ci?2, ci?1, ci, ci+1, ci+2bigram ci?2ci?1, ci?1ci, cici+1, ci+1ci+2Table 1: Uni- and bi-gram feature templatethe following function:g ( b1b2...bn, a1a2...an)=n?j=1f(bj(aj?2, aj?1; cj?h2...cj+h2), aj)(1)where c1c2...cnis the character sequence, bjand ajare the segmentation configuration andaction for character cj, respectively.
In (1),bj(aj?2, aj?1; cj?h2...cj+h2) indicates that the con-figuration for each character is a function that de-pends on the actions of the previous l charactersand the characters in the local window of size h.Why embedding.
The above matching modelwould suffer from sparsity if these outputs(character-specific action aj) were directly en-coded as one-hot vectors, since the matchingmodel can be seen as a sequence labeling modelwithC?L outputs, whereL is the number of orig-inal labels while C is the number of unique char-acters.
For Chinese, C is at the order of 103?104.The use of embeddings, however, can serve thematching model well thanks to their low dimen-sionality.2.2 The ArchitectureThe proposed architecture (Figure 1) has threecomponents, namely look-up table, concatenationand softmax function for matching.
We will gothrough each of them in this section.Look-up table.
The mapping between fea-tures/outputs to their corresponding embeddingsare kept in a look-up table, as in many previousembedding related work (Bengio et al, 2003; Peiet al, 2014).
Such features are extracted from thetraining data.
Formally, the embedding for eachdistinct feature d is denoted as Embed(d) ?
RN ,which is a real valued vector of dimension N .Each feature is retrieved by its unique index.
Theretrieval of the embeddings for the output actionsis similar.Concatenation.
To predict the segmentation forthe target character cj, its feature vectors are con-catenated into a single vector, the input embed-ding, i(bj) ?
RN?K , where K is the number offeatures used to describe the configuration bj.Softmax.
The model then computes the dotproduct of the input embedding i(bj) and each of1735the two output embeddings, o(aj,1) and o(aj,2),which represent the two possible segmentation ac-tions for the target character cj, respectively.
Theexponential of the two raw scores are normalizedto obtain probabilistic values ?
[0, 1].We call the resulting scores matching probabili-ties, which denote probabilities that actions matchthe given segmentation configuration.
In our ex-ample,?
-c has the probability of 0.7 to be the cor-rect action, while?
-s is less likely with a lowerprobability of 0.3.
Formally, the above matchingprocedure can be described as a softmax function,as shown in (2), which is also an individual f termin (1).f( bj, aj,k) =exp (i(bj) ?
o(aj,k))?k?exp(i(bj) ?
o(aj,k?))
(2)In (2), aj,k(1 ?
k ?
2) represent two possibleactions, such as ?
-c and ?
-s for ?
in our ex-ample.
Note that, to ensure the input and output areof the same dimension, for each character specificaction, the model trains two distinct embeddings,one ?
RN as feature and the other ?
RN?K asoutput, whereK is the number of features for eachinput.Best word segmentation of sentence.
Afterplugging (2) into (1) and applying (and then drop-ping) logarithms for computational convenience,finding the best segmentation for a sentence be-comes an optimization problem as shown in (3).
Inthe formula, ?Y is the best action sequence foundby the model among all the possible ones, Y =a1a2...an, where ajis the predicted action for thecharacter cj(1 ?
j ?
n), which is either cj-s orcj-c, such as?
-s and?
-c.?Y = argmaxYn?j=1exp (i(bj) ?
o(aj))?kexp (i(bj) ?
o(aj,k))(3)3 The Greedy SegmenterOur model depends on the actions predicted for theprevious two characters as history features.
Tradi-tionally, such scenarios call for dynamic program-ming for exact inference.
However, preliminaryexperiments showed that, for our model, a Viterbisearch based segmenter, even supported by con-ditional random field (Lafferty et al, 2001) styletraining, yields similar results as the greedy searchbased segmenter in this section.
Since the greedysegmenter is much more efficient in training andtesting, the rest of the paper will focus on the pro-posed greedy segmenter, the details of which willbe described in this section.3.1 Greedy SearchInitialization.
The first character in the sentenceis made to have two left side characters that aredummy symbols of START, whose predicted ac-tions are always START-s, i.e.
separation.Iteration.
The algorithms predicts the action foreach character cj, one at a time, in a left-to-right,incremental manner, where 1 ?
j ?
n and n is thesentence length.
To do so, it first extracts contextfeatures and history features, the latter of which arethe predicted character-specific actions for the pre-vious two characters.
Then the model matches theconcatenated feature embedding with embeddingsof the two possible character-specific actions, cj-sand ci-c.
The onewith highermatching probabilityis predicted as segmentation action for the charac-ter, which is irreversible.
After the action for thelast character is predicted, the segmented word se-quence of the sentence is built from the predictedactions deterministically.Hybrid matching.
Character-specific embed-dings are capable of capturing subtle word forma-tion tendencies of individual characters, but suchrepresentations are incapable of covering match-ing cases for unknown target characters.
An-other minor issue is that the action embeddingsfor certain low frequent characters may not be suf-ficiently trained.
To better deal with these sce-narios, We also train two embeddings to repre-sent character-independent segmentation actions,ALL-s and ALL-c, and use them to average withor substitute embeddings of infrequent or unknowncharacters, which are either insufficiently trainedor nonexistent.
Such strategy is called hybridmatching, which can improve accuracy.Complexity.
Although the total number of ac-tions is large, the matching for each target charac-ter only requires the two actions that correspond tothat specific character, such as?
-s and?
-c for?
in our example.
Each prediction is thus similarto a softmax computation with two outputs, whichcosts constant time C. Greedy search ensures thatthe total time for predicting a sentence of n char-acters is n?C, i.e.
linear time complexity, with aminor overhead for mapping actions to segmenta-tions.17363.2 TrainingThe training procedure first predicts the action forthe current character with current parameters, andthen optimizes the log likelihood of correct seg-mentation actions in the gold segmentations to up-date parameters.
Ideally, the matching probabilityfor the correct action embedding should be 1whilethat of the incorrect one should be 0.
We minimizethe cross-entropy loss function as in (4) for the seg-mentation prediction of each character cjto pursuethis goal.
The loss function is convex, similar tothat of maximum entropy models.J = ?K?k=1?
(aj,k) logexp (i ?
o(aj,k))?k?exp(i ?
o(aj,k?))
(4)where aj,kdenotes a possible action for cjand i is acompact notation for i(bj).
In (4), ?
(aj,k) is an in-dicator function defined by the following formula,where a?jdenotes the correct action.?
(aj,k) ={1, if aj,k= a?j0, otherwiseTo counteract over-fitting, we add L2 regulariza-tion term to the loss function, as follows:J = J +K?k=1?2(||i||2+ ||o(aj,k)||2)(5)The formula in (4) and (5) are similar to that of astandard softmax regression, except that both in-put and output embeddings are parameters to betrained.
We perform stochastic gradient descent toupdate input and output embeddings in turn, eachtime considering the other as constant.
We give thegradient (6) and the update rule (7) for the inputembedding i(bj) (i for short), where okis a shortnotation for o(aj,k).
The gradient and update foroutput embeddings are similar.
The ?
in (7) is thelearning rate, which we use a linear decay schemeto gradually shrink it from its initial value to zero.Note that the update for the input embedding i isactually performed for the feature embeddings thatform i in the concatenation step.
?J?i=?k( f (bj, aj,k) ?
?
(aj,k)) ?
ok+ ?i (6)i = i?
??J?i(7)Complexity.
For each iteration of the training pro-cess, the time complexity is also linear to the inputcharacter number, as compared with search, only afew constant time operations of gradient computa-tion and parameter updates are performed for eachcharacter.4 Experiments4.1 Data and Evaluation MetricIn the experiments, we use two widely used andfreely available1 manually word-segmented cor-pora, namely, PKU and MSR, from the secondSIGHAN international Chinese word segmenta-tion bakeoff (Emerson, 2005).
Table 2 shows thedetails of the two dataset.
All evaluations in thispaper are conducted with official training/testingset split using official scoring script.2PKU MSRWord types 5.5 ?
104 8.8 ?
104Word tokens 1.1 ?
106 2.4 ?
106Character types 5 ?
103 5 ?
103Character tokens 1.8 ?
106 4.1 ?
106Table 2: Corpus details of PKU and MSRThe segmentation accuracy is evaluated by pre-cision (P ), recall (R), F-score and Roov, the re-call for out-of-vocabulary words.
Precision is de-fined as the number of correctly segmented wordsdivided by the total number of words in the seg-mentation result.
Recall is defined as the numberof correctly segmented words divided by the totalnumber of words in the gold standard segmenta-tion.
In particular, Roovreflects the model gen-eralization ability.
The metric for overall perfor-mance, the evenly-weighted F-score is calculatedas in (8):F =2 ?
P ?RP + R(8)To comply with CWS evaluation conventions andmake comparisons fair, we distinguish the follow-ing two settings:?
closed-set : no extra resource other than train-ing corpora is used.?
open-set : additional lexicon, raw corpora, etcare used.1http://www.sighan.org/bakeoff2005/2http://www.sighan.org/bakeoff2003/score1737We will report the final results of our model3 onPKU and MSR corpora in comparison with pre-vious embedding based models (Section 4.2) andstate-of-the-art systems (Section 4.3), before go-ing into detailed experiments for model analyses(Section 4.5).4.2 Comparison with PreviousEmbedding-Based ModelsTable 3 shows the results of our greedy segmenteron the PKU and MSR datasets, which are com-pared with embedding-based segmenters in previ-ous studies.4 In the table, results for both closed-set and open-set setting are shown for previousmodels.
In the open-set evaluations, all threeprevious work use pre-training to train characterngram embeddings from large unsegmented cor-pora to initialize the embeddings, which will belater trained with the manually word-segmentedtraining data.
For our model, we report the close-set results only, as pre-training does not signifi-cant improve the results in our experiments (Sec-tion 4.5).As shown in Table 3, under close-set evaluation,our model significantly outperform previous em-bedding based models in all metrics.
Comparedwith the previous best embedding-based model,our greedy segmenter has achieved up to 2.2% and25.8% absolute improvements (MSR) on F-scoreand Roov, respectively.
Surprisingly, our close-setresults are also comparable to the best open-set re-sults of previous models.
As we will see in (Sec-tion 4.4), when using same or less character uni-and bi-gram features, our model still outperformsprevious embedding based models in closed-setevaluation, which shows the effectiveness of ourmatching model.Significance test.
Table 4 shows the 95% con-fidence intervals (CI) for close-set results of ourmodel and the best performing previousmodel (Peiet al, 2014), which are computed by formula (9),following (Emerson, 2005).CI = 2?F (1 ?
F )N(9)where F is the F-score value and the N is the wordtoken count of the testing set, which is 104,372 and106,873 for PKU and MSR, respectively.
We see3Our implementation: https://zenodo.org/record/17645.4The results for Zheng et al (2013) are from the re-implementation of Pei et al (2014).that the confidence intervals of our results do notoverlap with that of (Pei et al, 2014), meaning thatour improvements are statistically significant.4.3 Comparison with the State-of-the-ArtSystemsTable 5 shows that the results of our greedy seg-menter are competitive with the state-of-the-art su-pervised systems (Best05 closed-set, Zhang andClark, 2007), although our feature set is muchsimpler.
More recent state-of-the-art systems relyon both extensive feature engineering and ex-tra raw corpora to boost performance, which aresemi-supervised learning.
For example, Zhanget al(2013) developed 8 types of static and dy-namic features to maximize the co-training systemthat used extra corpora of Chinese Gigaword andBaike, each of which contains more than 1 bil-lion character tokens.
Such systems are not di-rectly comparable with our supervised model.
Weleave the development of semi-supervised learningmethods for our model as future work.4.4 Features InfluenceTable 6 shows the F-scores of our model onPKU dataset when different features are removed(?w/o?)
or when only a subset of features are used.Features complement each other and removing anygroup of features leads to a limited drop of F-score up to 0.7%.
Note that features of previ-ous (two) actions are even more informative thanall unigram features combined, suggesting thatintra- an inter-word dependencies reflected by ac-tion features are strong evidence for segmentation.Moreover, using same or less character ngram fea-tures, our model outperforms previous embeddingbased models, which shows the effectiveness ofour matching model.4.5 Model AnalysisLearning curve.
Figure 2 shows that the trainingprocedure coverages quickly.
After the first iter-ation, the testing F-scores are already 93.5% and95.7% for PKU andMSR, respectively, which thengradually reach their maximum within the next 9iterations before the curve flats out.Speed.
With an unoptimized single-threadPython implementation running on a laptop withintel Core-i5 CPU (1.9 GHZ), each iteration of thetraining procedure on PKU dataset takes about 5minutes, or 6,000 characters per second.
The pre-1738ModelsPKU Corpus MSR CorpusP R F RoovP R F RoovZheng et al(2013) 92.8 92.0 92.4 63.3 92.9 93.6 93.3 55.7+ pre-training?
93.5 92.2 92.8 69.0 94.2 93.7 93.9 64.1Mansur et al (2013) 93.6 92.8 93.2 57.9 92.3 92.2 92.2 53.7+ pre-training?
94.0 93.9 94.0 69.5 93.1 93.1 93.1 59.7Pei et al (2014) 93.7 93.4 93.5 64.2 94.6 94.2 94.4 61.4+ pre-training?
94.4 93.6 94.0 69.0 95.2 94.6 94.9 64.8+ pre-training & bigram?
- - 95.2 - - - 97.2 -This work (closed-set) 95.5 94.6 95.1 76.0 96.6 96.5 96.6 87.2Table 3: Comparison with previous embedding based models.
Numbers in percentage.
Results with ?used extra corpora for (pre-)training.ModelsPKU MSRF CI F CIPei et al 93.5 ?0.15 94.4 ?0.14This work 95.1 ?0.13 96.6 ?0.11Table 4: Significance test of closed-set results ofPei et al(2014) and our model.Model PKU MSRBest05 closed-set 95.0 96.4Zhang et al (2006) 95.1 97.1Zhang and Clark (2007) 94.5 97.2Wang et al (2012) 94.1 97.2Sun et al (2009) 95.2 97.3Sun et al (2012) 95.4 97.4Zhang et al (2013) ?
96.1 97.4This work 95.1 96.6Table 5: Comparison with the state-of-the-art sys-tems.
Results with ?
used extra lexicon/raw cor-pora for training, i.e.
in open-set setting.
Best05refers to the best closed-set results in 2nd SIGHANbakeoff.diction speed is above 13,000 character per second.Hyper parameters.
The hyper parameters usedin the experiments are shown in Table 7.
We ini-tialized hyper parameters with recommendationsin literature before tuning with dev-set experi-ments, each of which change one parameter by amagnitude.
We fixed the hyper parameter to thecurrent setting without spending too much time ontuning, since that is not the main purpose of thispaper.?
Embedding size determines the number ofparameters to be trained, thus should fit theFeature F-score Feature F-scoreAll features 95.1 uni-&bi-gram 94.6w/o action 94.6 only action 93.3w/o unigram 94.8 only unigram 92.1w/o bigram 94.4 only bigram 94.2Table 6: The influence of features.
F-score in per-centage on the PKU corpus.Figure 2: The learning curve of our model.training data size to achieve good perfor-mance.
We tried the size of 30 and 100, bothof which performs worse than 50.
A possibletuning is to use different embedding size fordifferent groups of features instead of settingN1= 50 for all features.?
Context window size.
A window size of3-5 characters achieves comparable results.Zheng et al (2013) suggested that contextwindow larger than 5 may lead to inferior re-sults.?
Initial Learning rate.
We found that severallearning rates between 0.04 to 0.15 yieldedvery similar results as the one reported here.The training is not very sensitive to reason-1739able values of initial learning rate.
However,Instead of our simple linear decay of learningrate, it might be useful to try more sophisti-cated techniques, such as AdaGrad and expo-nential decaying (Tsuruoka et al, 2009; Sunet al, 2013).?
Regularization.
Our model suffers a littlefrom over-fitting, if no regularization is used.In that case, the F-score on PKU drops from95.1% to 94.7%.?
Pre-training.
We tried pre-training charac-ter embeddings using word2vec5 with Chi-nese Gigaword Corpus6 and use them to ini-tialize the corresponding embeddings in ourmodel, as previous work did.
However, wewere only able to see insignificant F-scoreimprovements within 0.1% and observed thatthe training F-score reached 99.9%much ear-lier.
We hypothesize that pre-training leads tosub-optimal local maximums for our model.?
Hybrid matching.
We tried applying hy-brid matching (Section 3.1) for target char-acters which are less frequent than the topftopcharacters, including unseen characters,which leads to about 0.15% of F-score im-provements.Size of feature embed?
N1= 50Size of output embed?
N2= 550Window size h = 5Initial learning rate ?
= 0.1Regularization ?
= 0.001Hybrid matching ftop= 8%Table 7: Hyper parameters of our model.5 Related WorkWord segmentation.
Most modern segmentersfollowed Xue (2003) to model CWS as sequencelabeling of character position tags, using condi-tional random fields (Peng et al 2004), structuredperceptron (Jiang et al, 2008), etc.
Some notableexceptions are (Zhang and Clark, 2007; Zhang etal., 2012), which exploited rich word-level fea-tures and (Ma et al, 2012; Ma, 2014; Zhang etal., 2014), which explicitly model word structures.Our work generalizes the sequence labeling to a5https://code.google.com/p/word2vec/6https://catalog.ldc.upenn.edu/LDC2005T14more flexible framework of matching, and predictsactions as in (Zhang and Clark, 2007; Zhang et al,2012) instead of position tags to prevent the greedysearch from suffering tag inconsistencies.
To bet-ter utilize resources other than training data, ourmodelmight benefit from techniques used in recentstate-of-the-art systems, such as semi-supervisedlearning (Zhao and Kit, 2008; Sun and Xu, 2011;Zhang et al, 2013; Zeng et al, 2013), joint models(Li and Zhou, 2012; Qian and Liu, 2012), and par-tial annotations (Liu et al, 2014; Yang and Vozila,2014).Distributed representation and CWS.
Dis-tributed representation are useful for various NLPtasks, such as POS tagging (Collobert et al, 2011),machine translation (Devlin et al, 2014) and pars-ing (Socher et al, 2013).
Influenced by Collobertet al (2011), Zheng et al (2013) modeled CWS astagging and treated sentence-level tag sequence asthe combination of individual tag predictions andcontext-independent tag transition.
Mansur et al(2013) was inspired by Bengio et al (2003) andused character bigram embeddings to compensatefor the absence of sentence level optimization.
Tomodel interactions between tags and characters,which are absent in these two CWS models, Pei etal.
(2014) introduced the tag embedding and useda tensor hidden layer in the neural net.
In con-trast, our work uses character-specific action em-beddings to explicitly capture such interactions.
Inaddition, our work gains efficiency by avoidinghidden layers, similar as Mikolov et al (2013).Learning to match.
Matching heterogeneousobjects has been studied in various contexts before,and is currently flourishing, thanks to embedding-based deep (Gao et al, 2014) and convolutional(Huang et al, 2013; Hu et al, 2014) neural net-works.
This work develops a matching model forCWS and differs from others in its?shallow?yeteffective architecture.6 DiscussionSimple architecture.
It is possible to adopt stan-dard feed-forward neural network for our embed-ding matching model with character-action em-beddings as both feature and output.
Nevertheless,we designed the proposed architecture to avoidhidden layers for simplicity, efficiency and easy-tuning, inspired by word2vec.
Our simple archi-tecture is effective, demonstrated by the improvedresults over previous neural-network word seg-1740menters, all of which use feed-forward architecturewith different features and/or layers.
It might beinteresting to directly compare the performancesof our model with same features on the current andfeed-forward architectures, which we leave for fu-ture work.Greedy and exact search-based models.
Asmentioned in Section 3, we implemented and pre-liminarily experimented with a segmenter thattrains a similar model with exact search via Viterbialgorithm.
On the PKU corpus, its F-score is0.944, compared with greedy segmenter?s 0.951.Its training and testing speed are up to 7.8 timesslower than that of the greedy search segmenter.It is counter-intuitive that the performance of theexact-search segmenter is no better or even worsethan that of the greedy-search segmenter.
Wehypothesize that since the training updates pa-rameters with regard to search errors, the finalmodel is ?tailored?
for the specific search methodused, which makes the model-search combinationof greedy search segmenter not necessarily worsethan that of exact search segmenter.
Another wayof looking at it is that search is less importantwhen the model is accurate.
In this case, moststep-wise decisions are correct in the first place,which requires no correction from the search algo-rithm.
Empirically, Zhang and Clark (2011) alsoreported exact-search segmenter performingworsethan beam-search segmenters.Despite that the greedy segmenter is incapableof considering future labels, this rarely causesproblems in practice.
Our greedy segmenter hasgood results, compared with the exact-search seg-menter above and previous approaches, most ofwhich utilize exact search.
Moreover, the greedysegmenter has additional advantages of fastertraining and prediction.Sequence labeling and matching.
A tradi-tional sequence labeling model such as CRF hasK (number of labels) target-character-independentweight vectors, where the target character influ-ences the prediction via the weights of the featuresthat contain it.
In a way, a matching model can beseen as a family of ?sub-models?, which keeps agroup of weight vectors (the output embeddings)for each unique target character.
Different targetcharacters activate different sub-models, allowingopposite predictions for similar input features, asthe target weight vectors used are different.7 Conclusion and Future WorkIn this paper, we have introduced the matchingformulation for Chinese word segmentation andproposed an embedding matching model to takeadvantage of distributed representations.
Basedon the model, we have developed a greedy seg-menter, which outperforms previous embedding-based methods and is competitive with state-of-the-art systems.
These results suggest that it ispromising to model CWS as configuration-actionmatching using distributed representations.
In ad-dition, linear-time training and testing complexityof our simple architecture is very desirable for in-dustrial application.
To the best of our knowledge,this is the first greedy segmenter that is competi-tive with the state-of-the-art discriminative learn-ing models.In the future, we plan to investigate methods forour model to better utilize external resources.
Wewould like to try using convolutional neural net-work to automatically encode ngram-like features,in order to further shrink parameter space.
It is alsointeresting to study whether extending our modelwith deep architectures can benefit CWS.
Lastly,it might be useful to adapt our model to tasks suchas POS tagging and name entity recognition.AcknowledgmentsThe authors would like to thank the anonymousreviewers for their very helpful and constructivesuggestions.
We are indebted to ?a?r?
?
?ltekinfor discussion and comments, to Dale Gerdemann,Cyrus Shaoul, Corina Dima, Sowmya Vajjala andHelmut Schmid for their useful feedback on an ear-lier version of the manuscript.
Financial supportfor the research reported in this paper was providedby the German Research Foundation (DFG) as partof the Collaborative Research Center ?Emergenceof Meaning?
(SFB 833) and by the German Min-istry of Education and Technology (BMBF) as partof the research grant CLARIN-D.ReferencesYoshua Bengio, R?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
The Journal of Machine Learning Re-search, 3:1137?1155.Ronan Collobert, Jason Weston, L?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) from1741scratch.
The Journal of Machine Learning Research,12:2493?2537.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for sta-tistical machine translation.
In Proceedings of ACL,pages 1370?1380.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedings ofthe Fourth SIGHANWorkshop on Chinese LanguageProcessing, volume 133.Jianfeng Gao, Patrick Pantel, Michael Gamon, Xi-aodong He, Li Deng, and Yelong Shen.
2014.
Mod-eling interestingness with deep neural networks.
InProceedings of EMNLP, pages 2?13.Baotian Hu, Zhengdong Lu, Hang Li, and QingcaiChen.
2014.
Convolutional neural network ar-chitectures for matching natural language sentences.In Advances in Neural Information Processing Sys-tems, pages 2042?2050.Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,Alex Acero, and Larry Heck.
2013.
Learning deepstructured semantic models for web search usingclickthrough data.
In Proceedings of the ACM Inter-national Conference on Information & KnowledgeManagement, pages 2333?2338.Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L?.2008.
A cascaded linear model for joint Chineseword segmentation and part-of-speech tagging.
InProceedings of ACL, pages 897?904.John Lafferty, A. McCallum, and F. Pereira.
2001.Conditional random fields: probabilistic models forsegmenting and labeling sequence data.
In Proceed-ings of International Conference on Machine Learn-ing, pages 282?289.Zhongguo Li and Guodong Zhou.
2012.
Unifieddependency parsing of Chinese morphological andsyntactic structures.
In Proceedings of EMNLP,pages 1445?1454.Yijia Liu, Yue Zhang,Wanxiang Che, Ting Liu, and FanWu.
2014.
Domain adaptation for CRF-based Chi-nese word segmentation using free annotations.
InProceedings of EMNLP, pages 864?874.Jianqiang Ma, Chunyu Kit, and Dale Gerdemann.2012.
Semi-automatic annotation of Chinese wordstructure.
In Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese LanguageProcessing, pages 9?17.Jianqiang Ma.
2014.
Automatic refinement of syntac-tic categories in Chinese word structures.
In Pro-ceedings of LREC.Mairgup Mansur, Wenzhe Pei, and Baobao Chang.2013.
Feature-based neural language model andChinese word segmentation.
In Proceedings of IJC-NLP, pages 1271?1277.Tomas Mikolov, Martin Karafi?t, Lukas Burget, JanCernock?, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In Pro-ceedings of INTERSPEECH, pages 1045?1048.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Wenzhe Pei, Tao Ge, and Chang Baobao.
2014.
Max-margin tensor neural network for Chinese word seg-mentation.
In Proceedings of ACL, pages 239?303.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detec-tion using conditional random fields.
In Proceedingsof COLING, pages 562?571.Xian Qian and Yang Liu.
2012.
Joint Chinese wordsegmentation, POS tagging and parsing.
In Proceed-ings of EMNLP-CoNLL, pages 501?511.Richard Socher, John Bauer, Christopher D Manning,and Andrew Y Ng.
2013.
Parsing with compo-sitional vector grammars.
In Proceedings of ACL,pages 455?465.Weiwei Sun and Jia Xu.
2011.
Enhancing Chineseword segmentation using unlabeled data.
In Pro-ceedings of EMNLP, pages 970?979.Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-masa Tsuruoka, and Jun?ichi Tsujii.
2009.
A dis-criminative latent variable Chinese segmenter withhybrid word/character information.
In Proceedingsof NAACL, pages 56?64.Xu Sun, HoufengWang, andWenjie Li.
2012.
Fast on-line training with frequency-adaptive learning ratesfor Chinese word segmentation and new word detec-tion.
In Proceedings of ACL, pages 253?262.Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-masa Tsuruoka, and Jun?ichi Tsujii.
2013.
Prob-abilistic Chinese word segmentation with non-localinformation and stochastic training.
InformationProcessing & Management, 49(3):626?636.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-niadou.
2009.
Stochastic gradient descent trainingfor L1-regularized log-linear models with cumula-tive penalty.
In Proceedings of ACL-IJCNLP, pages477?485.Kun Wang, Chengqing Zong, and Keh-Yih Su.
2012.Integrating generative and discriminative character-based models for Chinese word segmentation.
ACMTransactions on Asian Language Information Pro-cessing (TALIP), 11(2):7.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, 8(1):29?48.1742Fan Yang and Paul Vozila.
2014.
Semi-supervised Chi-nese word segmentation using partial-label learningWith conditional random fields.
In Proceedings ofEMNLP, page 90?98.Xiaodong Zeng, Derek F Wong, Lidia S Chao, and Is-abel Trancoso.
2013.
Graph-based semi-supervisedmodel for joint Chinese word segmentation and part-of-speech tagging.
In Proceedings of ACL, pages770?779.Yue Zhang and Stephen Clark.
2007.
Chinese segmen-tation with a word-based perceptron algorithm.
InProceedings of ACL, pages 840?847.Yue Zhang and Stephen Clark.
2011.
Syntactic pro-cessing using the generalized perceptron and beamsearch.
Computational Linguistics, 37(1):105?151.Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.2006.
Subword-based tagging by conditional ran-dom fields for Chinese word segmentation.
In Pro-ceedings of NAACL, pages 193?196.Kaixu Zhang, Maosong Sun, and Changle Zhou.
2012.Word segmentation on Chinese mirco-blog data witha linear-time incremental model.
In Proceedings ofthe 2nd CIPS-SIGHAN Joint Conference on ChineseLanguage Processing, pages 41?46.Longkai Zhang, Houfeng Wang, Xu Sun, and MaigupMansur.
2013.
Exploring representations from un-labeled data with co-training for Chinese word seg-mentation.
In Proceedings of EMNLP, pages 311?321.Meishan Zhang, Yue Zhang, Wanxiang Che, and TingLiu.
2014.
Character-level Chinese dependencyparsing.
In Proceedings of ACL, pages 1326?1336.Hai Zhao and Chunyu Kit.
2008.
Unsupervised seg-mentation helps supervised learning of character tag-ging for word segmentation and named entity recog-nition.
In Proceedings of IJCNLP, pages 106?111.Xiaoqing Zheng, Hanyang Chen, and TianyuXu.
2013.Deep Learning for Chinese Word Segmentation andPOS Tagging.
In Proceedings of EMNLP, pages647?657.1743
