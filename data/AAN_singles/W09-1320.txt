Proceedings of the Workshop on BioNLP, pages 153?161,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsTowards Retrieving Relevant Information for Answering ClinicalComparison QuestionsAnnette LeonhardSchool of InformaticsUniversity of EdinburghEH8 9AB, Edinburgh, Scotlandannette.leonhard@ed.ac.ukAbstractThis paper introduces the task of automatical-ly  answering  clinical  comparison  questionsusing MEDLINE?
abstracts.
In the beginning,clinical  comparison  questions  and  the  mainchallenges in recognising and extracting theircomponents  are  described.
Then,  differentstrategies for retrieving  MEDLINE?
abstractsare shown.
Finally, the results of an initial ex-periment judging the relevance of MEDLINE?abstracts  retrieved by searching for the com-ponents  of  twelve comparison  questions  willbe shown and discussed.1 IntroductionClinicians  wishing  to  practice  evidence-basedmedicine need to keep up with a vast amount ofever changing research to be able to use the currentbest evidence in individual patient care (Sackett etal.,  1996).
This  can  be difficult  for  time-pressedclinicians, although methods such as systematic re-views, evidence summaries and clinical guidelinescan help to translate research into practice.In a survey commissioned by Doctors.net.uk,97% of  doctors  and  nurses  said that  they  wouldfind  a  Question  Answering  (QA) Service  useful,where they can ask questions in their own words(Bryant  and  Ringrose  2005).
Studies  have  alsoshown that clinicians often want answers to partic-ular questions,  rather than getting information onbroad topics  (Chambliss & Conley,  1996;  Ely  etal., 1999, 2005).A type of question that  clinicians  commonlywant answered are comparison questions.
In a cor-pus of clinical questions collected from the Nation-al  Library  of  Health  (NLH) Question AnsweringService  (http://www.clinicalanswers.nhs.uk),  ap-proximately  16% of  the  4580  questions  concerncomparisons of different drugs, different treatmentmethods or different interventions as in (1).
(1) Have any studies directly compared the ef-fects of Pioglitazone and Rosiglitazone on theliver?Despite the frequency of comparison questionsin  the  clinical  domain,  there  are  no  clinical  QAmethods specially designed to answer them.
Thispaper  introduces  the  task  of  answering  clinicalcomparison questions,  focusing  initially  on ques-tions  involving  comparisons  between drugs.
Sec-tion 2 presents an overview of comparative struc-tures  and  Section  3,  relevant  previous  work  onclinical question answering and the computationalextraction  of  comparisons.
Section  4  discussesstrategies  for  retrieving  MEDLINE?
abstracts  in-volving comparisons.
Section 5 presents the resultsof  an initial  experiment  judging the  relevance ofMEDLINE?
abstracts, which are then discussed inSection 6.2 Background2.1 Indicators of Comparative Constructions153In  order  to  identify  questions  about  comparisonsthat should trigger special purpose search and ex-traction mechanisms, as well as identifying explicitcomparisons made in text, one needs to recognizeconstructions  commonly used to express compar-isons in English (i.e.
similarities and/or differencesbetween two or more entities).
In this  paper,  theterm ?entity?
refers to drugs, treatment methods orinterventions, and the initial focus of the work ison  comparative  questions  in  which  two or  moredrugs or interventions are compared with respect toa particular criterion such as efficacy in treating acertain disease.
This reflects their common occur-rence in the NLH corpus.Comparisons can appear in either a comparativeform or a superlative form.
The comparative formis used to compare two or more entities  with re-spect  to  a  certain  attribute.
The superlative  formcompares or contrasts one entity with a set of otherentities and expresses the end of a spectrum.
Thefollowing examples illustrate the difference:Comparative form:Is  Ibuprofen better  than  Paracetamol fortreating pain?Superlative form:Is Ibuprofen the best treatment for pain?Friedman  (1989)  developed  one  of  the  firstcomputational  treatments  of  comparative  struc-tures.
Comparisons  are  challenging  because  theycorrespond to  a diverse  range of  syntactic  formssuch as coordinate or subordinate conjunctions, ad-verbial  constructions  or  wh-relative-like  clauses.Comparisons are cross-categorical and encompassadjectives, quantifiers, and adverbs.
Adjectives andadverbs  indicating  comparisons  occur  in  the  fol-lowing patterns:Comparative adjectives and adverbs:Regular adjectives and adverbs:ADJ/ADV -er (e.g.
safer) [[as/than]1 X] [for Y]Irregular adjectives and adverbs:e.g.
worse/better [[as/than] X] [for Y]Analytical adjectives and adverbs:e.g.
less/more ADJ/ADV [than X] [for Y]1As/ than are optional.
For example see ?A or B: What issafer?
?Superlative adjectives and adverbs:Regular adjectives and adverbs:ADJ/ADV -est (eg.
safest) X [for Y]Irregular adjectives and adverbs:e.g.
worst/best X [for Y]Analytical adjectives and adverbs:e.g.
least/most ADJ/ADV X [for Y]Comparisons can also be expressed in other partsof speech.
In the NLH corpus the following exam-ples occur:Verbs: compared to/with, differ fromNouns: comparison, differenceConjunctions: versus/vs, or and instead ofWith  respect  to  their  semantics  (and  hence,with respect to other phrases or constructions theymay appear  with)  comparatives  can be  scalar ornon-scalar and express either equality or inequalitybetween  the  compared  entities.
(Superlatives  areabsolute and the notion of scalability and equalitydoes not apply to them).Scalar adjectives and adverbs refer to attributesthat can be measured in degrees, implying a scalealong which entities can be arrayed.
Non-scalar ad-jectives and adverbs refer to attributes that cannotbe  measured  in  degrees.
Equality refers  to  con-structs  where  two or more compared  entities  areequal  in respect  to  a  shared  quality,  whereas  in-equality emphasises the difference between entitiesin respect to a certain quality.Table 1 gives an example showing the four pos-sibilities for drugs and interventions.Scalability Equality Example+ + As efficient as x- + Same intervention as x+ - Better treatment than x- - Drug x differs from drug yTable 1.
Features of comparatives.The difference between  scalar and  non-scalarcomparisons plays an important role as far as auto-matic  processing  of  comparative  constructionswith  SemRep  (Rindflesch  and  Fiszman,  2003;Rindflesch et al, 2005) is concerned.
This will bediscussed in Section 3.1.154Regular expressions based on the given patternsfor adjectives and adverbs and on the other parts ofspeech  shown above,  as  well  as  their  respectivepart-of-speech tags, were used to extract a subsetof  comparison questions  from a corpus collectedfrom the National Library of Health Question An-swering Service website at http://www.clinicalan-swers.nhs.uk, as described in Section 2.3.2.2 The NLH QA ServiceThe NLH Question Answering service (QAS) wasa on-line  service that  clinicians  in the UK coulduse to ask questions, that were then answered by ateam  of  clinical  librarians  from  Trip  DatabaseLtd.2, founded by Jon Brassey and Dr Chris Price.The questions and their answers were then retainedat the website and indexed by major clinical topics(e.g.
Cancer,  Cardiovascular  disease,  Diabetes,etc.)
so  that  clinicians  could  consult  the  QAarchive to  check whether  information relevant  totheir own clinical question was already available.While the NHS QAS service was discontinuedin 2008, its archive of questions and answers wasintegrated  into  ATTRACT3,  the  Welsh  NationalPublic Health Service run by Jon Brassey.
The aimof  both services has been to provide answers in aclinically relevant time frame using the best avail-able evidence.From the NLH QAS archive,  a total of 4580unique Q-A pairs of different degrees of complexi-ty were collected for 34 medical fields representingquestions asked and answered over a 36 month pe-riod.
These were put into an XML format that sep-arated the questions from the answers, while co-in-dexing them to indicate their association.2.3 The Comparison Question CorpusA sub-corpus specifically of comparison ques-tions was created by POS-tagging the questions ofthe  initial  corpus with the Penn Treebank tagset,using the TnT tagger (Brants 1999).
Regular ex-pression  were  then  used  to  search  the  taggedcorpus for  tagged lexical  elements  that  indicatedthe constructions noted in Section 2.2.2http://www.tripdatabase.com/index.html3http://www.attract.wales.nhs.uk/Some  questions  were  initially  retrieved  morethan once because these questions contained morethan  one  tag  which  was  a  comparison  indicator.These duplicates were removed.
There may be oth-er  comparative  questions  that  might  have  beenmissed  because  of  POS  tagging  errors.
A  smallnumber  of  false  positives  were  removed  duringmanual  post-processing.
False positives were dueto the fact that not all words tagged as superlativesare proper comparisons, but idiomatic expressions,such as ?best practise?, or proportional quantifiers(Huddleston  and  Pullum,  2002)  such  as  ?MostNSAIDs?.
(Scheible (2008) distinguishes eight dif-ferent classes in which the superlative constructionis used in English but only five of the eight classesinvolve true comparisons.)
The result is a subset of742 comparison questions out of the the total cor-pus of 4580 Q-A pairs.Table 2. shows the number of occurrences foreach item.POS tag/Lexical item OccurrencesJJR 195RBR 124JJS 207RBS 68versus, instead of 18compared to/with, differ from 45comparison, difference 85Total 742Table 2.
Number of comparison indicators3 Related WorkAs the focus of this paper is biomedical text,  thediscussion here is limited to the work done in thiscontext.
Section 3.1 will present work on findingassertions  involving  comparisons  in  MEDLINE?abstracts  and Section 3.2 will  show work on an-swering clinical questions about comparisons.3.1  Interpretation of Comparative Structures(Fiszman et al, 2007) describes work on auto-matically interpreting comparative constructions inMEDLINE?
abstracts.
They use an extension of an155existing  semantic  processor,  SemRep (Rindfleschand Fiszman, 2003; Rindflesch et al, 2005), fromthe Unified Medical Language System resources toconstruct  semantic  predications  for  the  extractedcomparative expressions.Fiszman et al concentrate on extracting ?struc-tures  in  which  two drugs  are  compared  with re-spect to a shared attribute?, such as a drug?s effica-cy in treating a certain condition, illustrated in thefollowing in example:(3) Losartan was more effective than atenololin reducing cardiovascular morbidity and mor-tality in patients with hyptertension,  diabetes,and LVH.
[Example (20) in (Fiszman et al 2007)]The  drugs'  relative  merits  in  achieving  theirpurpose  is  expressed  by  positions  on  a  scale.Words like than, as, with, and to are cues for iden-tifying compared terms, the comparison scale andthe relative position of the compared entities on thescale.Fiszman et al focused on extracting the drugnames, the scale and the position on the scale as il-lustrated in the SemRep representation from exam-ple (1):(4) Losartan COMPARED_WITH AtenololScale: EffectivnessLosartan HIGHER_THAN Atenolol[Example (21) in (Fiszman et al 2007)]The overall  F-score for the SemRep performanceon the test set is 81% .Fiszman et al do not deal with questions, norwith identifying the basis of the comparison or thepopulation in this paper, both of which are impor-tant  for  generating  relevant  answers  for  clinicalquestions.
However,  as  Fiszman  and  Demner-Fushman have pointed out (personal  communica-tion), it is possible to identify the basis of the com-parison and the population.
Two drugs function asarguments to the TREATS predicate, which identi-fies the disease that is the basis for the comparison.SemRep can also identify the population using thepredicate  PROCESS_OF.
For  the  question  ?Istreatment  A better  than  treatment  B  for  treatingdisease C in  population D?
?, SemRep would pro-duce the following representation for the basis ofthe comparison (C) and the population (D):A TREATS CB TREATS CC PROCESS_OF DThere is an essential limitation to SemRep, how-ever: Its comparative module only considers scalarcomparative constructions, as presented in  Section2.1.
Non-scalar comparisons, e.g.
comparisons like?Is X the same intervention as Y??
or ?How doesdrug X differ from drug Y??
cannot be extractedusing SemRep.
Also, the SemRep algorithm onlyrecognises entities which occur on the left and theright side of the comparison cue and hence cannotrecognize  comparisons  in  which  both  comparedentities are to the right side of the comparative cueas in ?Which is better: X or Y??.
This means thatdifferent  methods are needed in order to  processnon-scalar comparisons  and  scalar comparisonsthat  cannot  be recognized because of their  struc-ture.
In future work, rules will be defined for thedifferent  syntactic  structures  in which  non-scalarcomparisons and scalar comparison with both enti-ties on the same side of a comparative cue can oc-cur to serve as a basis for argument extraction dur-ing parsing.There  may  also  be  problems  with  ?Wh-?
or?anything?
questions (e.g.
?What is better than Xfor treating Y??
or ?Is there anything better than Xfor  treating  Y??
),  because  ?Wh-words?
or  ?any-thing?
do  not  have  a  type  that  can  be  mapped.While  Question  Typing   might  solve  such  prob-lems, the point is that questions involving compar-isons raise somewhat different  problems than as-sertions, which I will have to deal with in the workbeing carried out here.3.2   Answering Clinical QuestionsDemner-Fushman  and  Lin  (2006)  address  su-perlative clinical questions of the type ?What is thebest treatment for X?
by using a hybrid approachconsisting of information retrieval and summariza-tion.Demner-Fushman and Lin?s  task  breaks  downinto subtasks of  identifying the drugs using UMLSconcepts, clustering the abstracts for the drugs us-ing  UMLS semantic  relationships  and  creating  ashort summary for each abstract by  using the ab-stract title and outcome sentence.
They focus  pri-marily on synthesising correct answers from a set156of  search  results  consisting  of  MEDLINE?
cita-tions.The system (Cluster condition) performs wellcompared  to  the  baseline,  which  consists  of  themain interventions from the first three MEDLINE?abstracts retrieved by the manual PubMed queries.In a manual evaluation, only 20% of the drugs forthe  baseline  were  evaluated  as  beneficial,  com-pared to 39% for the Cluster condition.
60% of thePubMed  answers were judged as ?good?
in com-parison to 83% for the  Cluster condition.The system orders the clusters by size, equatingthe most popular drug with the best drug.
Whilethis assumption is not always correct, the authorshave observed that drugs that are studied more aremore likely to be beneficial.
In addition, while thisapproach  might  work  for  questions  of  the  form?What is the best drug for X??
it cannot be used toanswer other superlative questions such as Exam-ples (5) or (6), because looking for the most stud-ied drugs will not provide an answer to the ques-tion which drug has the fewest  side effects  or issafest to use.
(5) Which drug for treating X has the fewestside effects?
(6) Which drug is safest to use for treating X?Despite this shortcoming, however, Demner Fush-man and Lin?s  work of implementing  an end-to-end QA system for superlatives provides a modelfor all future work in this area.4 Strategies  for  Retrieving  MEDLINE?AbstractsAs with (Fiszman et al, 2007) and (Demner-Fush-man and Lin 2006),  the current work starts withinformation  retrieval.
In  particular,  exploratorymanual  searches  were  first  carried  out  via  theOVID?
portal to see if MEDLINE?
abstracts are auseful  resource  for  answering  comparison  ques-tions  such  as  ?Is  drug  A better  than  drug  B fortreating X?
?With the assistance of a medical librarian fromthe  University  of  Edinburgh?s  Information  Ser-vices, different strategies to achieve the best possi-ble  retrieval  of  relevant  abstracts  were  tried  out.Two separate cases were considered: comparisonsinvolving  very  popular,  well-studied  drugs  andones involving other drugs.
First, strategies for theformer  will  be described  and  illustrated  with thefollowing example question:(7) Is paracetamol better than ibuprofen for re-ducing fever?Titles and abstracts were searched for each com-pared entity  (paracetamol and ibuprofen) and thebasis of the comparison (fever).
Then, the resultswere combined to return only abstracts containingboth entities and the basis of the comparison.
Wefound that search precision could be increased bylimiting  the  search  to  comparative  study,  usingOVID's  publication  type  limit.
That  is,   all  ab-stracts that mention all three terms (i.e.
the entitiesand the basis of the comparison) in the title or ab-stract  involve  relevant  comparisons.
The  mostcommon sources that were excluded by constrain-ing the search to comparative studies are reviews,evaluation  studies  and  case  reports.
These  maycontain relevant  information  but  the  initial  focuswas on the study type that was most likely to in-crease  precision.
(As  the  experiment  reported  inSection 5 and discussed in Section 6 shows, the re-striction  to  comparative  studies  is  insufficient  toguarantee relevance.
)Constraining the search to comparative studieshas  somewhat  different  effects,  depending  onwhether  the  drugs  mentioned  in  the  search  arewell-studied or not.For popular, well-studied drugs, looking for thedrug names often leads to hundreds of returned ab-stracts, most of which are not relevant.
By includ-ing the  basis  of  the  comparison and limiting  thestudy type to comparative studies, the number ofreturned abstracts  for a set of 30 questions dropson average to 15% of the size of the original set ofreturned abstracts.
For Example (7) a search for thecombination of both drug names retrieved 593 ab-stracts.
Including the basis of the comparison de-creased  the  number  to  139  abstracts.
After  con-straining  the  results  to  comparative  studies,  thenumber of retrieved abstracts dropped to 24, whichis a reduction of 83%.For  less-studied  drugs,  the  difference  in  num-bers of abstracts retrieved by including the basis ofthe comparison and limiting the search to the com-parative study publication type is smaller compared1571.
Is there any evidence to suggest that torasemide isbetter than furosemide as a diuretic?2.
Is  lansoprazole  better  than  omeprazole  in  treatingdyspepsia?3.
Are  there  any  studies  comparing  topical  di-clofenac gel with ibuprofen gel?4.
Effectiveness of Decapeptyl in treatment of prostatecancer in comparison to Zoladex?5.
Which is more effective ibuprofen or diclofenac forarthritis pain for pain relief?6.
Is calcium citrate better absorbed and a more effec-tive treatment for osteoporosis than calcium carbon-ate?Figure 1.
Questions used in the experiment.to the numbers  retrieved by only looking for thedrug names, because fewer abstracts exist for thesedrugs, but the relevance of the returned abstractsimproves as considerably as for the more studieddrugs.
(Recall was not analyzed during the explo-rations because for answering clinical questions therelevance of the retrieved abstracts is more impor-tant than retrieving all possible abstracts.
)There have also been cases where including thebasis of the comparison leads to the return of norelevant abstracts.
In this case, different strategiesfrom the one discussed above will be necessary.Often drugs are known under generic names orthe basis of the comparison is related to symptomswhich are not explicitly mentioned in the questionbut which are still relevant.
In order to recognisethat different terms are actually related to the samedrug or disease and belong to the same hierarchy,advantage was taken of OVID?s ability to map theentities  to  their  corresponding  MeSH  (MedicalSubject  Headings)  terms and  to  ?explode?
theMeSH terms to include  all of the narrower, morespecific subheadings during the search.So far the focus has been on manual retrievalof abstracts.
The described search strategy of com-bining  search terms  and  restricting  the  results  tothe specific publication type could have been doneusing a  search engine which implements Booleanoperators and is capable of indexing XML docu-ments  However,  the  description  of  the  searchstrategy  and  the  presentation  of  the  intermediatesearches,  which  would  have  been  performed  in-ternally by a search engine, was regarded import-ant to illustrate the impact of adding the basis ofthe comparison and the use of a publication typelimit on the number of retrieved abstracts.7.
Have any studies directly compared the effects ofPioglitazone and Rosiglitazone on the liver?8.
Is  Famvir  (famciclovir)  better  than  acyclovir  forHerpes zoster?9.
Is it true that men on captopril have a better qualityof life than men on enalapril?10.
What is the first choice for Type 2 diabetes patients:sulphonylurea or metformin?11.
Is there any evidence as to which is more effectiveat preventing malaria: Malarone or Doxycyline?12.
In conjunctivitis which is better chloramphenicol orfucithalmic eye drops?5 Judging  the  Relevance  of   MEDLINE?AbstractsA initial experiment was carried out to evaluate therelevance  of  the  abstracts  retrieved  from  MED-LINE?
via Ovid?
using the strategies described inthe previous section.The experimental  subjects  were  eight  4th  yearmedical  students,  who evaluated the abstracts  re-trieved for twelve clinical comparison questions inwhich two drugs were compared to each other withrespect to a particular attribute.
The questions dif-fer in syntactic structure, but they all contain com-parisons of two drugs.
Figure 1 shows the list ofquestions.The material presented to the medical studentsin  the  experiment  was  created  as  follows:The drug names and the basis of the comparisonfrom the natural language questions were manuallymapped  to  their  corresponding  MeSH terms  andused to retrieve abstracts via OVID?
using the finalstrategy described in Section 4..For any question, the maximum number of ab-stracts  given to the  student  judges was 15,  com-prising up-to-15 of the most recent abstracts.
In to-tal,  each judge evaluated 103 abstracts.
Each ab-stract was assigned by each judge into one of threecategories, based on the criteria given after the cat-egory label:1.
Relevant: Both drugs from the question ortheir generic names are mentioned in the abstracts,the drugs are directly compared to each other andthe disease or the attribute with respect to whichthey are being compared is also mentioned and the158same as stated in the question or synonymous to it(e.g.
heartburn and dyspepsia would both count asright because they are closely related).2.
Not  Relevant: The  drugs  or  their  genericnames are not mentioned in the abstract, the drugsare not compared and/or the disease or the attributewith respect to which they are being compared iswrong (as in different  from what is stated in thequestion,  e.g.
effect  on blood pressure instead ofuse as a painkiller).3.
Somewhat  Relevant: The  drugs  or  theirgeneric names are mentioned but there are no sin-gle  sentences  indicating  a  comparison  betweenthem or the disease is not mentioned.
If the wrongdisease is mentioned, the abstract should be labeled?not relevant?.The judges were also asked to explain the rea-son for their choice of labels.The  inter-annotator  agreement  between  thejudges was computed using a variant kappa statis-tic for multiple annotators (Fleiss, 1971).
The nullhypothesis was rejected and it was ensured that theobserved agreement is not accidental.Overall  inter-annotator agreement  for all  threecategories  measured  by  the  kappa  statistic  wasmoderate at 0.58 for a total of 103 judgments.
47judgments were in the ?somewhat relevant?
cate-gory.
If  annotator  agreement  is  only  assessed onthe  remaining  56  judgments  from  the  two  cate-gories ?relevant?
and ?not relevant?, kappa is 0.97,which represents almost perfect agreement.6 Results and DiscussionGraph  1  shows  the  percentage  of  abstracts  thatwere judged relevant by the eight judges for eachquestion.
The  numbers  of  retrieved  abstracts  foreach question were: 15 abstracts for Question 1, 5,8 and 10, 9 abstracts for question 7 and 11, 7 ab-stracts for Question 2, 5 abstracts for Question 9, 4abstracts  for  Question  6  and  12,  3  abstracts  forQuestion 3 and 2 abstracts for Question 4.Question 1, 9 and 12 show a very high percent-age of relevant abstracts (73%, 80% and 100%  re-spectively), whereas no relevant abstracts were re-trieved for questions 4, 5 and 11, and only one rel-evant abstract (out of 15) for question 10.
An ab-stract was considered relevant when at least five ofthe eight judges considered it relevant.Graph 1.
Percentage of abstracts judged relevant by themajority of the judges for each of the twelve questions.The label on the top of each bar is the actual percentage.Here the main sources for these disparate resultsare discussed, based on both the explanations givenby  the  student  judges  and  discussions  with  ourmedical librarian.Approximately 30% (31 of 103) of the abstractswere labeled ?not relevant?
by the judges becausethey  lacked any direct  evidence of a comparisone.g.
at least one sentence that explicitly comparesthe two drugs in question, even though the drugsare  mentioned  in  the  abstract  and  the  study is  acomparative study (as  indicated  in  its  MeSH in-dices).
This  is  illustrated  in  Example  (9),  whichshows the three sentences from one of the abstractsretrieved for Question 1 that explicitly mention thetwo drugs:(9) Piretanide and furosemide have a constantextrarenal elimination and thus accumulate inrenal failure.[...]
Elimination of torasemide isindependent of its renal excretion.
Thus in renalfailure, torasemide is the only loop diuretic inwhich the plasma concentration is strictly dosedependent.159About 10% (10) of the abstracts were judged tobe irrelevant because the drugs were compared aspart of a treatment regime in combination with oth-er drugs, as in Abstract 4 for Question 6 in whichcalcium  citrate  and  calcium  carbonate  are  com-pared co-administered  with different  preparationsof  sodium fluoride.
In two cases  (2% of  the  ab-stracts),  doses  of  a  given  drug  were  comparedagainst  other  dosages  instead  of  the  drugs them-selves,  e.g.
30  mg lansoprazole  versus  20mgomeprazole.A major factor for ?not relevant?
judgments wasthe time frame.
This was relevant when retrievingabstracts  about  well-established  drugs  that  havebeen in existence for a long time, such as ibuprofenor  diclofenac.
All but one of the 18 abstracts re-trieved for the two questions about these two drugswere irrelevant,  even though the two drugs wereexplicitly mentioned in the abstract.
The problemis that they were grouped together as conventionalnon-steroidal  anti-inflammatory  drugs  (NSAIDs)and compared to newer NSAIDs or different painmedication.
Such abstracts could only be excludedby analyzing the abstracts themselves.
Whether toproceed systematically back through the abstractsordered by recency, or to retrieve abstracts from arandom time interval, or from a window of  n-yearsafter the drug came on the market, will be a matterto be assessed empirically.The  final  source  of  ?non  relevant?
judgmentswas a problem with the judges and not with the ab-stracts.
In Question 2 regarding dyspepsia,  two outof seven abstracts were judged irrelevant becausethe drugs were not explicitly compared regardingdyspepsia but  only regarding H. pylori,  which isone of the possible causes for dyspepsia.
Also ab-stracts retrieved for Question 7 about the effect onlipid profiles were wrongly categorised by roughlya third of the judges as not being relevant to theliver.The experiment has shown that searching for thedrugs, the basis of the comparison and studies ofthe  publication  type  comparative  study  is  a  firststep towards retrieving abstracts that can serve asanswer  candidates  for  clinical  comparison  ques-tions, but it has been shown not to be sufficient toguarantee the relevance of the retrieved abstracts.The two main problems discovered during theexperiment  that  need  to  be  addressed  in  furtherprocessing steps for the retrieved abstracts concernabstracts lacking sentences in which the drugs aredirectly compared to each other and the retrieval ofirrelevant  abstracts  for  well-established  drugs,which are used as a reference for comparing newerdrugs to, instead of containing direct comparisonsof the drugs in question.7 Conclusion and Future WorkThis work introduced the task of answering clinicalcomparison questions  and pointed  out  challengesin recognising and extracting their components.
Italso described strategies for retrieving MEDLINE?abstracts  and  showed  that  only  looking  for  thecompared  entities  without  including  the  basis  ofthe comparison is not enough to retrieve useful ab-stracts.The initial experiment evaluating the relevanceof retrieved abstracts for twelve clinical compari-son  questions revealed a number of problems thatneed to be taken into account for future work, es-pecially  the lack of sentences  containing  explicitcomparisons  and  dealing  with  well-establisheddrugs.During the next stages, the process of identify-ing  and  extracting  the  elements  of  a  comparisonquestion as well as the process of retrieving  MED-LINE?
abstracts will be automated using tools fromthe UMLS Knowledge Sources.
Features or ruleswill be defined to augment SemRep to deal withthe  problems  concerning  non-scalar comparisonsand  structurally  different  scalar  comparison  dis-cussed in Section 3.1 to be able to automaticallyextract the relevant comparison components.
Also,possible solutions will be researched to automati-cally overcome the problems of retrieving relevantabstracts  identified  and  discussed  in  Section  6.AcknowledgmentsI would like to thank my advisors Bonnie Webber,Marshall  Dozier  and  Claudia  Pagliari  for  theirhelpful comments and support.
I would also like tothank John Brassey for providing insight into theNLH and ATTRACT QAS.160ReferencesThorsten  Brants.
TnT  ?
A  Statistical  Part-of-Speech  Tagger.
Available  at  http://www.coli.uni-saarland.de/~thorsten/publications/Brants-TR-TnT.pdf Accessed 10 August 2008.Lacey  Sue  Bryant  and  Tim  Ringrose  (2005).Clinical  Question  Answering  Services:  Whatusers want and what providers provide.
Poster.M  Lee  Chambliss  and  Jennifer  Conley  (1996).Answering  Clinical  Questions.
Journal  ofFamily Practice  43: 140?144.Dina Demner-Fushman and  Jimmy Lin (2006).Answer  Extraction,  Semantic  Clustering,  andExtractive Summarization for Clinical QuestionAnswering.
Proc.
COLING/ACL  2006:  841?848.John W Ely, Jerome A Osheroff and Mark H  Ebell(1999).
Analysis of questions asked by familydoctors  regarding  patient  care.
BMJ 319:358?361.John W Ely, Jerome A Osheroff, M Lee Chamb-liss, et al (2005).
Answering physicians?
clinicalquestions:  Obstacles  and  potential  solutions.Journal of the American Medical InformaticsAssociation 12(2): 217?224.Marcelo  Fiszman,  Dina  Demner-Fushman,  Fran-cois M. Lang et.
al.
(2007).
Interpreting comp-arative constructions in biomedical text.
Proc.BioNLP 2007: 137?144.Joseph L Fleiss (1971).
Measuring nominal scaleagreement  among  many  raters.
PsychologicalBulletin 76 (5): 378?382.Carol Friedman (1989).
A general computationaltreatment  of  the  comparative.
Proc.
ACL1989: 161?168.Rodney Huddleston and Geoffrey K Pullum (eds.)2002.
The Cambridge Grammar of the EnglishLanguage.
Cambridge:   Cambridge UniversityPress.Thomas  C  Rindflesch  and  Marcelo  Fiszman(2003).
The interaction of  domain knowledgeand linguistic structure in natural language pro-cessing: Interpreting hypernymic propositions inbiomedical text.
JBI 36(6): 462?77.Thomas C Rindflesch, M Fiszman and BisharahLibbus (2005).
Semantic interpretation for thebiomedical  research  literature.
Medical  infor-matics: Knowledge management and data miningin biomedicine.
Springer, New York, NY.David L Sackett,  William M C Rosenberg,  J  AMuir  Gray,  et  al.
(1996).
Evidence  basedmedicine: what is is and what it isn't: It's aboutintegrating individual clinical expertise and thebest external evidence.
BMJ 312, pp.
71?72.Silke  Scheible  (2008).
Annotating  Superlatives.Proc.
LREC 2008: 28?30.161
