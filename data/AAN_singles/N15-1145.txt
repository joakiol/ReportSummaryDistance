Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1317?1322,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsImproving Update Summarization via Supervised ILP and SentenceRerankingChen Li1, Yang Liu1, Lin Zhao21Computer Science Department, The University of Texas at DallasRichardson, Texas 75080, USA2Research and Technology Center, Robert Bosch LLCPalo Alto, California 94304, USA{chenli,yangl@hlt.utdallas.edu}{lin.zhao@us.bosch.com}AbstractInteger Linear Programming (ILP) based sum-marization methods have been widely adoptedrecently because of their state-of-the-art per-formance.
This paper proposes two new mod-ifications in this framework for update sum-marization.
Our key idea is to use discrimi-native models with a set of features to mea-sure both the salience and the novelty of wordsand sentences.
First, these features are usedin a supervised model to predict the weightsof the concepts used in the ILP model.
Sec-ond, we generate preliminary sentence candi-dates in the ILP model and then rerank themusing sentence level features.
We evaluate ourmethod on different TAC update summariza-tion data sets, and the results show that oursystem performs competitively compared tothe best TAC systems based on the ROUGEevaluation metric.1 IntroductionUpdate summarization has attracted significant re-search focus recently.
Different from generic extrac-tive summarization, update summarization assumesthat users already have some information about agiven topic from an old data set, and thus for anew data set the system aims to generate a summarythat contains as much novel information as possi-ble.
This task was first introduced at DUC 2007 andthen continued until TAC 2011.
It is very useful tochronological events in real applications.Most basic update summarization methods arevariants of multi-document summarization methods,with some consideration of the difference betweenthe earlier and later document sets (Boudin et al,2008; Fisher and Roark, 2008; Long et al, 2010;Bysani, 2010).
One important line is to use graph-based co-ranking.
They rank the sentences in theearlier and later document sets simultaneously byconsidering the sentence relationship.
For example,Li et al (2008) was inspired by the intuition that?a sentence receives a positive influence from thesentences that correlate to it in the same collection,whereas receives a negative influence from the sen-tences that correlates to it in the different (or previ-ously read) collection?, and proposed a graph basedsentence ranking algorithm for update summariza-tion.
Wan (2012) integrated two co-ranking pro-cesses by adding some strict constraints, which ledto more accurate computation of sentences?
scoresfor update summarization.
A similar method wasalso applied earlier by (Wan et al, 2011) for mul-tilingual news summarization.
In addition, genera-tive models, such as topic models, have also beenadopted for this task.
For example, Delort andAlfonseca (2012) proposed a novel nonparametricBayesian approach, a variant of Latent Dirichlet Al-location (LDA), aiming to distinguish between com-mon information and novel information.
Li et al(2012) borrowed the idea of evolutionary cluster-ing and proposed a three-level HDP (HierarchicalDirichlet Process) model to represent the diversityand commonality between aspects discovered fromtwo different document data sets.One of the most competitive summarization meth-ods is based on Integer Linear Programming (ILP).It has been widely adopted in the generic sum-marization task (Martins and Smith, 2009; Berg-Kirkpatrick et al, 2011; Woodsend and Lapata,2012; Li et al, 2013a; Li et al, 2013b; Li et al,2014).
In this paper, we use the ILP summarization1317framework for the update summarization task, andmake improvement from two aspects, with the goalto more discriminatively represent both the salienceand novelty of words and sentences.
First, we usesupervised models and a rich set of features to learnthe weights for the bigram concepts used in theILP model.
Second, we design a sentence rerank-ing component to score the summary candidate sen-tences generated by the ILP model.
This secondreranking approach allows us to explicitly model asentence?s importance and novelty, which comple-ments the bigram centric view in the first step of ILPsentence selection.
Our experimental results on mul-tiple TAC data sets demonstrate the effectiveness ofour proposed method.2 Proposed Update Summarization System2.1 ILP Framework for SummarizationThe core idea of the ILP based summarizationmethod is to select the summary sentences by maxi-mizing the sum of the weights of the language con-cepts that appear in the summary.
Bigrams are of-ten used as the language concepts in this method.Gillick et al (2009) stated that the bigrams gaveconsistently better performance than unigrams or tri-grams for a variety of ROUGE measures.
The as-sociation between the language concepts and sen-tences serves as the constraints.
This ILP methodis formally represented as below (see (Gillick et al,2009) for more details):max?iwici(1)s.t.
ci?
{0, 1} ?i sj?
{0, 1} ?jsjOccij?
ci?jsjOccij?
ci?jljsj?
Lwhere ciand sjare binary variables that indicate thepresence of a concept and a sentence respectively.
ljis the sentence length and L is the maximum length(word number) of the generated summary.
wiis aconcept?s weight and Occijmeans the occurrenceof concept i in sentence j.
The first two inequalitiesassociate the sentences and concepts.
They ensurethat selecting a sentence leads to the selection of allthe concepts it contains, and selecting a concept onlyhappens when it is present in at least one of the se-lected sentences.2.2 Bigrams Weighting for Salience andNoveltyIn the above ILP-based summarization method, howto determine the concepts and measure their weightsis the key factor impacting the system performance.Intuitively, if we can successfully identify the im-portant key bigrams used in the ILP system, or as-sign large weights to those important bigrams, thegenerated summary sentences will contain as manyimportant bigrams as possible, and thus resulting inbetter summarization performance.
The oracle ex-periment in (Gillick et al, 2008) showed that if theyuse the bigrams extracted from the human writtensummaries as the input of the ILP system, much bet-ter ROUGE scores can be obtained than using theautomatically selected bigrams, suggesting the im-portance of using the right concepts.
(Gillick et al,2009) used document frequency as the weight of abigram.
They also provided some justification fordocument frequency as a weighting function in thatpaper.For update summarization, intuitively we need tonot only identify the salience of the bigram, butalso incorporate bigrams?
novelty in their weights.Therefore, only using the document frequency as theweight in the objective function is insufficient.
Wethus propose to use a supervised framework for thebigram weight estimation in the ILP model.
The newobjective function is:max?i(?
?
f(bi)) ci(2)We replace the heuristic wiin Formula (1) with afeature based one: f(bi) represents the features fora bigram bi, and ?
is a weight vector for these fea-tures.
Constraints remain the same as before in theILP method.There are two kinds of features for each bigram:one set is related to the bigrams themselves; theother set is related to the sentences containing thebigram.
Table 1 shows the features we design.For both the bigram and the sentence level fea-tures, we separate the features based on whetherthey represent the importance or the novelty.
For1318Feature 8 and 17, the summary is generated bya general unsupervised ILP-based summarizationsystem from the given old data set.
The idea ofFeature 9 and 10 was first introduced by (Bysani,2010); here we applied it to bigrams.
dfmaxisthe number of documents in the data set (10 in theTAC data), which can be thought of the maximumvalue of document frequency for a bigram.
Fea-ture 11 is interpolated n-gram document frequency,which was first introduced by (Ng et al, 2012):??wu?Sdfnew(wu)+(1??
)?wb?Sdfnew(wb)|S|, where wuand wbare unigrams and bigrams respectively insentence S. Feature 18 and 19 are variants of Fea-tures 11, where instead of document frequency (dfin the formula above), bigram and unigram?s nov-elty and uniqueness values are used.
Among thesefeatures, the feature values of feature 4, 5 and 6 arediscrete.
In this study, we discretized all the othercontinuous values into ten categories according tothe value range in the training data.To train the model (feature weights), we use theaverage perceptron strategy (Collins, 2002) to up-date the feature weights whenever the hypothesis bythe ILP decoding process is incorrect.
Binary classlabels are used for bigrams in the learning process,that is, we only consider whether a bigram is in thesystem generated summary or human summaries,not their term or document frequency.
We use a fixedlearning rate (0.1) in training.2.3 Sentence Reranking on ILP ResultsIn the ILP method, sentence selection is done byconsidering the concepts that a sentence contains.
Itis difficult to add indicative features in this frame-work to explicitly represent the sentence?s salience,and more importantly, its novelty for the updatesummarization task.
This information is only cap-tured by the weights of the bigrams using the methoddescribed above.
Therefore, we propose to use atwo-step approach, where an initial ILP module firstselects some sentences and then a reranking moduleuses sentence level features to rerank them to gener-ate the final summary.
We expect this step of mod-eling sentences directly can complement the bigram1Note that we do not use all the sentences in the ILP module.The ?relevant?
sentences are those that have at least one bigramwith document frequency larger than or equal to three.Bigram Level FeaturesImportance Related Features1.
dfnew(b): document frequency in new dataset2.
normalized term frequency in all filtered rel-evant sentences13.
sentence frequency in all relevant sentences4.
do bigram words appear in topic?s query ?5.
is the bigram in the first 1/2/3 position of thatsentence?6.
is the bigram in the last 1/2/3 position of thatsentence?Novelty Related Features7.
dfold(b): document frequency in old data set8.
normalized term frequency in the summaryfrom old data set9.
bigram novelty value n(b) =dfnew(b)dfold(b)+dfmax10.
bigram uniqueness value u(b) = 0 ifdfold(b) > 0; otherwise u(b) =dfnew(b)dfmaxSentence Level FeaturesImportance Related Features11.
interpolated n-gram document frequency12.
sentence position in that document13.
is the sentence in the first 1/2/3 position inthat document?14.
is the sentence in the last 1/2/3 position inthat document?15.
sentence length16.
sentence similarity with topic?s queryNovelty Related Features17.
sentence similarity with the summary fromold data set18.
interpolated n-gram novelty19.
interpolated n-gram uniquenessTable 1: Features in the supervised ILP model for weight-ing bigrams.centric view in the first ILP summarization module.For the first step, we use the ILP framework withour supervised bigram weighting method to obtaina summary of N words (N is greater than the re-quired summary length L).
Note that the ILP modelselects these output sentences as a set that optimizesthe objective function, and there are no scores foreach individual sentence.
Second, we use sentencelevel features listed in Table 1 to rerank the can-1319didate sentences.
This is expected to better eval-uate the salience and the novelty of the sentences.We use a regression model (SVR) for this rerank-ing purpose.
When training the model, a sentence?sROUGE2 score compared with the human gener-ated summary is used as the regression target.
Af-ter reranking, we just select the top sentences thatsatisfy the length constraint to form the final sum-mary.
In this work we do not use any redundancyremoval (e.g., MMR method).
This is because theILP decoding process tries to find a global optimalset maximizing the concept coverage, subject to thelength constraint, and thus already considers redun-dancy among sentences.
Typically when the initialset (i.e., the output from the first ILP step) is not toobig, redundancy is not a big problem.3 Experiments and Results3.1 Data and Experiment SetupWe evaluate our methods using several recent TACdata sets, from 2008 to 2011.
Every topic has twosets of 10 documents (Set A and B).
The updatetask aims to create a 100-word summary from Set Bgiven a topic query and Set A.
When evaluating onone year?s data, we use the data from the other threeyears as the training set.
This applies to both the su-pervised ILP method and the sentence reranking re-gression model.
All the summaries are evaluated us-ing ROUGE (Lin, 2004).
An academic free solver2does all the ILP decoding and libsvm3is used forSVR implementation.3.2 ResultsTable 2 and Table 3 show the R2 and R-SU4 valueson different TAC data sets for the following systems.?
ILP baseline.
This is the unsupervised ILP-based summarization system (Gillick et al,2009), in which only bigrams with documentfrequency greater than 2 are used in the ILPsummarization process, and weight wiis thedocument frequency of that bigram.?
TAC best.
This is the best result in the TACupdate summarization evaluation.4Note that2http://www.gurobi.com3http://www.csie.ntu.edu.tw/?cjlin/libsvm/4The ID of the TAC best system from 2008 to 2011 is14,40,16 and 43.there is limited research on update summariza-tion and we cannot find better published resultsfor these data sets than the TAC best systems.?
Supervised ILP.
This is our supervised ILPmethod where bigram weights are learned dis-criminately.
It is the one-step system that gen-erates the summary with the target length.
Weuse the same bigram set as the ILP baseline sys-tem.
For this method, we show results usingdifferent features: only using the importancefeatures; and using all the features.
This is usedto evaluate the impact of the novelty features onthe update summarization task.?
Two-step method: supervised ILP followed bysentence reranking.
We generate 200 (value ofN ) words summary in the ILP system.
Twodifferent configurations are also used: with andwithout the sentence novelty features in thesentence ranking module.
All the features (in-cluding the novelty features) are used in the ILPpre-selection step.?
Sentence ranking without ILP.
In this experi-ment, we do not use the ILP summarizationmodule to generate candidate sentences first,but just apply sentence ranking to the entiredata set.
Then MMR is leveraged to select thefinal summary sentences.
Again, we present re-sults using different feature sets.We can see from the tables that the supervised ILPmodel outperforms the unsupervised one.
After in-cluding the novelty related features, the model canassign higher weights for the bigrams with novel in-formation, resulting in improved summarization per-formance.
There is further improvement when us-ing our 2-step approach with the sentence rerank-ing model.
Our proposed method (ILP followed bysentence reranking, and using all the features) out-performs the TAC best result in 2010 and 2011, andalso yields competitive results in the other data sets.The gain of ROUGE-2 of our proposed system com-pared with the ILP baseline is statistically significantbased on ROUGE?s 95% confidence.
When usingsentence ranking on the entire document set, withoutthe ILP pre-selection step, its performance is worsethan our proposed method.
This shows the benefit ofdoing pre-selection using the ILP module.
Finally,13202008 2009 2010 2011ILP Baseline 8.55 8.84 7.04 8.63TAC Best 10.10 10.41 8.00 9.58Supervised ILPw/o novelty features 9.18 9.06 7.39 9.20w all features 9.4 9.28 7.76 9.462-step: Supervised ILP + Sentence Rankingw/o novelty features 9.65 9.47 7.97 9.70w all features 9.99 9.61 8.11 9.99Sentence Ranking w/o ILPw/o novelty features 9.25 9.10 7.41 9.18w all features 9.42 9.32 7.70 9.43Table 2: ROUGE-2 results on TAC 2008-2011 data.2008 2009 2010 2011ILP Baseline 12.17 12.54 10.57 12.01NIST Best 13.66 13.95 11.97 13.08Supervised ILPw/o novelty features 12.57 12.94 11.01 12.76w all features 12.78 13.21 11.61 12.952-step: Supervised ILP + Sentence Rankingw/o novelty features 13.10 13.65 11.98 13.24w all features 13.61 13.77 12.20 13.42Sentence Ranking w/o ILPw/o novelty features 12.60 12.99 11.25 12.73w all features 12.85 13.31 11.50 12.90Table 3: ROUGE-SU4 results on TAC 2008-2011 data.for all the methods, adding the novelty related fea-tures always performs better than that without them,proving the effect of our novelty features for updatesummarization.Lastly we evaluate the effect of the summarylength from the ILP module on the two-step summa-rization systems.
Figure 1 shows the performancewhen N changes from 150 to 400.
We can see thatthere is some difference in the patterns for differentdata sets, and the best results are obtained when Nis around 150 to 250.
When the first ILP moduleproduces many sentence candidates, it is likely thatthere is redundancy among them.
In this case, redun-dancy removal approaches such as MMR need to beused to generate the final summary.
In addition, fora large candidate set, our current regression modelalso faces some challenges due to its limited featuresused in sentence reranking.
Addressing these prob-lems is our future work.7.588.599.51010.5150  200  250  300  350  400ROUGE-2Summary Length of ILP OutputTAC 2008TAC 2009TAC 2010TAC 2011Figure 1: ROUGE-2 results when varying the outputlength for the first ILP selection step.4 ConclusionsIn this paper, we adopt the supervised ILP frame-work for the update summarization task.
A set ofrich features are used to measure the importanceand novelty of the bigram concepts used in the ILPmodel.
In addition, we proposed a re-selection com-ponent to rank candidate sentences generated by theILP model based on sentence level features.
Ourexperiment results show that our features and thereranking procedure both help improve the summa-rization performance.
This pilot research points outnew directions for generic or update summarizationbased on the ILP framework.AcknowledgmentsWe thank the anonymous reviewers for their detailedand insightful comments on earlier drafts of this pa-per.
The work is partially supported by NSF awardIIS-0845484 and DARPA Contract No.
FA8750-13-2-0041.
Any opinions, findings, and conclusions orrecommendations expressed are those of the authorand do not necessarily reflect the views of the fund-ing agencies.ReferencesTaylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of ACL.Florian Boudin, Marc El-B`eze, and Juan-Manuel Torres-Moreno.
2008.
The LIA update summarization sys-tems at tac-2008.
In Proceedings of TAC.1321Praveen Bysani.
2010.
Detecting novelty in the contextof progressive summarization.
In Proceedings of theNAACL Student Research Workshop.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP.Jean-Yves Delort and Enrique Alfonseca.
2012.
Dual-sum: a topic-model based approach for update sum-marization.
In Proceedings of EACL.Seeger Fisher and Brian Roark.
2008.
Query-focusedsupervised sentence ranking for update summaries.
InProceeding of TAC.Dan Gillick, Benoit Favre, and Dilek Hakkani-tur.
2008.The ICSI summarization system at tac 2008.
In Pro-ceedings of TAC.Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, BerndtBohnet, Yang Liu, and Shasha Xie.
2009.
TheICSI/UTD summarization system at tac 2009.
In Pro-ceedings of TAC.Wenjie Li, Furu Wei, Qin Lu, and Yanxiang He.
2008.PNR2: Ranking sentences with positive and negativereinforcement for query-oriented update summariza-tion.
In Proceedings of Coling.Jiwei Li, Sujian Li, Xun Wang, Ye Tian, and BaobaoChang.
2012.
Update summarization using a multi-level hierarchical dirichlet process model.
In Proceed-ings of COLING.Chen Li, Fei Liu, Fuliang Weng, and Yang Liu.
2013a.Document summarization via guided sentence com-pression.
In Proceedings of the EMNLP.Chen Li, Xian Qian, and Yang Liu.
2013b.
Using super-vised bigram-based ilp for extractive summarization.In Proceedings of ACL.Chen Li, Yang Liu, Fei Liu, Lin Zhao, and Fuliang Weng.2014.
Improving multi-documents summarization bysentence compression based on expanded constituentparse trees.
In Proceedings of EMNLP.Chin-Yew Lin.
2004.
Rouge: a package for automaticevaluation of summaries.
In Proceedings of ACL.Chong Long, Minlie Huang, and Xiaoyan Zhu.
2010.Summarizing multidocuments by information dis-tance.
In Proceedings of TAC.Andre F. T. Martins and Noah A. Smith.
2009.
Summa-rization with a joint model for sentence extraction andcompression.
In Proceedings of the ACL Workshopon Integer Linear Programming for Natural LanguageProcessing.Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen Kan,and Chew-Lim Tan.
2012.
Exploiting category-specific information for multi-document summariza-tion.
In Proceedings of COLING.Xiaojun Wan, Houping Jia, Shanshan Huang, and Jian-guo Xiao.
2011.
Summarizing the differences in mul-tilingual news.
In Proceedings of SIGIR.Xiaojun Wan.
2012.
Update summarization based on co-ranking with constraints.
In Proceedings of COLING.Kristian Woodsend and Mirella Lapata.
2012.
Multipleaspect summarization using integer linear program-ming.
In Proceedings of EMNLP-CoNLL.1322
