Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 155?160,Dublin, Ireland, August 23 2014.A Simple Baseline for Discriminating Similar LanguagesMatthew PurverCognitive Science Research GroupSchool of Electronic Engineering and Computer ScienceQueen Mary University of Londonm.purver@qmul.ac.ukAbstractThis paper describes an approach to discriminating similar languages using word- and character-based features, submitted as the Queen Mary University of London entry to the DiscriminatingSimilar Languages shared task.
Our motivation was to investigate how well a simple, data-driven, linguistically naive method could perform, in order to provide a baseline by which morelinguistically complex or knowledge-rich approaches can be judged.
Using a standard supervisedclassifier with word and character n-grams as features, we achieved over 90% accuracy in the test;on fixing simple file handling and feature extraction bugs, this improved to over 95%, comparableto the best submitted systems.
Similar accuracy is achieved using only word unigram features.1 Introduction and ApproachMost approaches to written language detection use character or byte ngram features to capture charac-teristic orthographic sequences ?
see e.g.
(Cavnar and Trenkle, 1994) to (Lui et al., 2014) and manyin between, as well as implementations such as the widely used open-source Chromium Compact Lan-guage Detector.1Some approaches determine these characteristic features from linguistic properties ofthe language (e.g.
(Lins and Gonc?alves, 2004)), while some determine them from data (e.g.
(Cavnar andTrenkle, 1994)).
A wide range of approaches to modelling and classification can be used, ranging fromsimple Na?
?ve Bayes models (Grefenstette, 1995) to more complex generative mixture models for taskswith multilingual texts (Lui et al., 2014).
Our interest in this task was to see how well a naive, entirelydata-driven baseline method would perform in the task of discriminating similar languages (DSL) asposed by the DSL Shared Task (Zampieri et al., 2014).Our approach was intended to capture two basic insights into variation between similar languages.First, that closely related languages often use quite different words for the same concept: e.g.
US Englishelevator vs UK English lift; Croatian tjedan vs Serbian nedelja vs Bosnian sedmica.
Second, that thereare often regular variations in the details of a word?s orthographic or phonological form: e.g.
US Englishcolor, favorite vs UK English colour, favourite; Croatian/Bosnian rijeka, htjeti vs Serbian reka, hteti.The former insight can be approximated by use of word ngrams; the latter via character ngrams.
Whilesuch ngram features cannot capture similarity of meaning or non-sequential dependencies, they may doa reasonable job of capturing similarity of sentential context (often taken to be an indicator of lexicalmeaning) and sequential phenomena.Together with simplicity in method, speed and simplicity of implementation was also an objective.We therefore used only the training and development data available in the shared task ?
see (Tan etal., 2014) ?
together with a standard freely available discriminative SVM classifier and common textpre-processing methods.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1https://code.google.com/p/chromium-compact-language-detector/1552 Background and Related WorkShared Task The Discriminating Similar Languages (DSL) Shared Task was established as part of the2014 VarDial workshop.2The task provided datasets for 13 different languages in 6 groups of closelyrelated languages, shown in Table 1.
Data was divided into training, development and test sets: for eachlanguage, 18,000 labelled training instances and 2,000 labelled development instances were provided;an unlabelled and previously unseen test set containing 1,000 instances per language was then used forevaluation by the organisers ?
see (Tan et al., 2014) for full details of the dataset, and (Zampieri et al.,2014) for the task and evaluation.Group A Bosnian (bs), Croatian (hr), Serbian (sr)Group B Indonesian (id), Malaysian (my)Group C Czech (cz), Slovakian (sk)Group D Brazilian Portuguese (pt-BR), European Portuguese (pt-PT)Group E Peninsular Spain (es-ES), Argentine Spanish (es-AR)Group F American English (en-US), British English (en-GB)Table 1: Languages and groups in the DSL Shared Task.However, problems were discovered in labelling the languages in Group F, and an evaluation forgroups A-E was therefore performed separately; we discuss only this latter task and evaluation here.Related Work Classification approaches based on character or byte sequences have shown success inproviding general models of language identification; see e.g.
(Lui et al., 2014).
In more specific exper-iments into discriminating between pairs or triples of similar languages, many researchers have foundthat word-based features can aid accuracy; but classification method and feature choice vary widely.When distinguishing Malay from Indonesian, Ranaivo-Malanc?on (2006) combines character n-gramfrequencies with heuristics based on number format and lists of words unique to each language.
Ljube?si?cet al.
(2007) use a character trigram-based probabilistic language model, again in combination with aunique-word list, to distinguish between Croatian, Serbian and Slovenian, achieving high accuracies(over 99%); Tiedemann and Ljube?si?c (2012) extend this task to include Bosnian and improve perfor-mance by using a Naive Bayes classifier with unigram word features to achieve accuracies over 95%.Some research suggests that word-based features can even outperform character-based approaches.For Brazilian vs European Portuguese, Zampieri and Gebre (2012) found that word unigrams gave verysimilar performance to character n-gram features when used in a probabilistic language model; Zampieriet al.
(2013) then showed that word 1- or 2-grams outperformed character ngrams of any length from 1to 5 (and that both outperformed features based purely on syntactic part-of-speech), when distinguishingdifferent varieties of Spanish.
Lui and Cook (2013) likewise found that bag-of-words features generallyoutperformed features based on syntax or character sequences when distinguishing between Canadian,Australian and UK English.
However, Zampieri (2013) found that in some cases (e.g French) charactern-grams might give benefits above simple word unigram features.In this work, then our interest was to investigate whether these simple, knowledge-poor approaches cangeneralise and apply across several language groups, using a single integrated approach to classificationincorporating character- and word-based features within one model; and to compare the utility of wordand character features.3 MethodsProcessing and training We tokenise the training texts from (Tan et al., 2014) based on transitionsbetween alphanumeric and non-alphanumeric characters, and remove URLs, email addresses, Twitterusernames and emoticons.
We then form feature vectors with entries for all observed word (token) uni-grams, and character ngrams of lengths 1-3; feature values are counts (raw term frequencies) normalised2http://corporavm.uni-koeln.de/vardial/156by the text length in tokens or characters respectively.
We then train a single multi-class linear-kernel sup-port vector machine using LIBLINEAR (Fan et al., 2008) with the language identifiers (en-US, en-UK,hr, bs, sr etc.)
as labels.
SVMs are well-suited to high-dimensional feature spaces; and SVMs withngrams of these lengths have shown good performance in other language identification work (Baldwinand Lui, 2010).
Features were given numerical indices corresponding to the unique ngram type (i.e.we used a feature dictionary with no hashing).
No feature selection or frequency cutoff was used.
Nopart-of-speech tagging or grammatical analysis was attempted; no external language resources or toolswere used other than described above.Development and testing Development and test set texts were tokenised and featurised using the sameprocess; feature indices were taken from the dictionary generated during training, with unseen ngramtypes ignored.
LIBLINEAR was then used to predict the most likely language identifier label.By re-using a standard set of in-house utilities for tokenisation and featurisation,3the code for train-ing and parameter testing (see below) was written and tested for functionality in around 30 minutes.Pre-processing, featurisation and vectorisation then took around 25 minutes over the training and devel-opment sets, and writing out LIBLINEAR format files around 15 minutes, running on a MacBook Airwith 1.7GHz Intel Core i7 processor and 8Gb memory.
Classifier training then takes around 1 minute,depending on exact parameter settings.
Testing on the development set or test set takes around 1 secondper language group.4 Experiments and ResultsDevelopment We used 10-fold cross-validation on the training set, and testing on the development set,to choose a suitable SVM cost parameter (tradeoff between error and maximum margin criterion).
Wecross-validated over the training set to check overall multi-class accuracy while varying the cost overa range from 1 to 100 ?
see Table 2.
We then trained on the full training set, and tested accuracy onthe development across each language group ?
see Table 3.
Given reported problems with the group Fdataset (en-UK/en-US), we focussed on groups A-E.Cost: 1.0 3.0 10.0 30.0 50.0 100.0Overall A-E 91.36 93.24 94.44 94.83 94.86 94.85Table 2: 10-fold cross-validation accuracy on training set with varying SVM cost.Cost: 1.0 3.0 10.0 30.0 50.0 100.0Group A bs/hr/sr 88.93 91.96 93.20 93.56 93.46 93.26Group B id/my 97.11 97.72 98.14 98.28 98.31 98.42Group C cz/sk 99.90 99.92 99.95 99.97 99.97 99.95Group D pt-BR/pt-PT 89.83 91.99 93.52 94.12 94.00 94.05Group E es-AR/es-ES 82.72 85.82 87.78 89.26 89.24 89.01Overall A-E 91.34 93.28 94.34 94.86 94.81 94.73Table 3: Accuracy on development set with varying SVM cost.A cost parameter value of 30 to 50 appeared to perform best across all groups, so these two valueswere used for separate runs in the shared task test.
Note though that performance appears relativelystable over a cost range of 10-100 (perhaps 30-100 for group E).
The classifier performs worst for groupE (es-AR/es-ES), with only this language group failing to reach 90% accuracy.
Group C (cz/sk)performs best with almost perfect accuracy; this may be due to the existence of characters which arehighly discriminative on their own (e.g.
?o is used in Slovak, but not in Czech, ?u in Czech but not inSlovak ?
although a few dozen examples appear labelled as Slovak in this dataset).3Tools with equivalent functionality are widely available e.g.
as part of NLTK, http://www.nltk.org/.157Test ?
Shared Task A blind run on the test set was then performed and submitted as part of the sharedtask.
Overall accuracy was 90.61% (macro-averaged F-score 92.51%), placing us 5thamongst the taskentrants; results per group are shown in Table 4.Cost: 30.0Group A bs/hr/sr 87.87Group B id/my 93.50Group C cz/sk 96.20Group D pt-BR/pt-PT 90.45Group E es-AR/es-ES 86.45Overall A-E 90.61Table 4: Accuracy on test set as submitted for the shared task.Corrected Test However, after submission of the test run, a bug was discovered in the code whichpaired test sentences with predictions; predictions had been omitted for about 500 of the 11,000 testtexts (i.e.
4.5% of the data) due to an unfortunate combination of unpaired double-quote characters in thetest data with the use of a standard CSV-file handling library.
After release of the gold-standard test setlabels, the classifier was therefore re-run, with resulting accuracies as shown in Table 5.Cost: 1.0 3.0 10.0 30.0 50.0 100.0Group A bs/hr/sr 87.97 90.70 92.40 92.90 93.00 93.17Group B id/my 98.30 98.85 99.05 99.15 99.15 99.15Group C cz/sk 99.90 99.95 99.95 99.95 99.95 99.95Group D pt-BR/pt-PT 88.50 91.45 93.25 93.95 93.90 93.80Group E es-AR/es-ES 83.65 86.70 88.45 89.35 89.45 89.45Overall A-E 91.33 93.27 94.42 94.86 94.90 94.93Table 5: True accuracy on test set after restoring omitted predictions.Accuracies are very similar to those on the development set.
Overall accuracy at the chosen costparameter range of 30-50 is 94.9%, slightly worse than the 1stand slightly better than the 2nd-placedsystems in the official test (95.71% and 94.68% respectively).
Increasing the cost parameter settingcould perhaps give a very slight boost to performance.
Again, group E performs worst, and Group Cbest; per-group and overall accuracies are very similar to those achieved on the development set.A second unintended feature of the feature generation code was subsequently discovered: charactern-grams were being extracted spanning word boundaries (including the whitespace characters separatingwords).
These were removed, leaving only the intended character n-grams within words, and accuraciesare shown in Table 6.
Again, overall performance increases slightly, now to over 95%, although GroupA accuracy shows a slight decrease (0.1%).
Group E accuracy improves by over 1% and is now over90% at the chosen cost parameter.Cost: 1.0 3.0 10.0 30.0 50.0 100.0Group A bs/hr/sr 89.83 92.13 92.73 92.77 92.63 92.67Group B id/my 98.55 99.15 99.25 99.35 99.35 99.35Group C cz/sk 99.95 99.95 99.95 99.95 99.95 99.95Group D pt-BR/pt-PT 91.00 93.25 94.80 95.15 95.10 95.15Group E es-AR/es-ES 86.10 88.35 90.30 90.85 90.95 91.15Overall A-E 92.79 94.35 95.16 95.35 95.33 95.37Table 6: Accuracy on test set after removing spurious character n-grams.158Effect of features To investigate the utility of our chosen feature sets and their insights into lexical andorthographic distinctions, we then compared the overall performance to that achieved when removingcertain features.
Table 7 shows the accuracies achieved without word unigram features (i.e.
using onlycharacter ngrams of lengths 1-3); Table 8 shows accuracies without character ngram features (i.e.
usingonly word unigrams).Cost: 1.0 3.0 10.0 30.0 50.0 100.0Group A bs/hr/sr 81.43 85.40 88.03 89.77 90.10 90.70Group B id/my 91.80 94.15 96.05 96.95 97.20 97.50Group C cz/sk 99.90 99.95 99.95 99.95 99.95 99.95Group D pt-BR/pt-PT 82.75 86.70 89.15 90.80 91.50 91.80Group E es-AR/es-ES 77.80 80.95 83.50 85.20 85.10 85.65Overall A-E 86.25 89.06 91.04 92.28 92.53 92.90Table 7: Accuracy on test set without word unigrams.Cost: 1.0 3.0 10.0 30.0 50.0 100.0Group A bs/hr/sr 86.83 89.63 91.73 92.20 92.43 92.07Group B id/my 97.70 98.65 98.85 99.10 99.15 99.10Group C cz/sk 99.70 99.70 99.80 99.90 99.90 99.90Group D pt-BR/pt-PT 86.65 90.55 92.55 93.25 93.40 93.20Group E es-AR/es-ES 85.10 87.10 88.35 89.35 89.35 89.45Overall A-E 90.80 92.81 94.02 94.53 94.63 94.50Table 8: Accuracy on test set using only word unigrams.Neither system performs as well as the classifier with the full, combined feature set (Table 6).
How-ever, the system with only word unigrams does almost as well as the full system, losing a maximum of2% performance at the extreme range of cost parameter values, and less than 1% at the chosen optimalvalues.
The system with only character ngrams, however, loses noticably more performance, with around3% lost even at optimal cost values.5 ConclusionsA simple approach using ngram features and discriminative classification achieves competitive resultson the task of discriminating similar languages, and the availability of existing language processing andmachine learning tools makes setting up and training such a system easy and extremely quick.
Simpleword unigram features perform well on their own, although combination with character n-gram featuresimproves performance; the choice of classifier parameters is important but seems to generalise wellacross different languages.
Future extensions of this work could include features which take into accountlonger word or character sequences and/or more flexible characterisations and combinations of thosefeatures, for example via the convolutional neural network approach of (Kalchbrenner et al., 2014).ReferencesTimothy Baldwin and Marco Lui.
2010.
Language identification: The long and the short of the matter.
InHuman Language Technologies: The 2010 Annual Conference of the North American Chapter of the Associationfor Computational Linguistics, pages 229?237, Los Angeles, California, June.
Association for ComputationalLinguistics.William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Proceedings of the 3rdSymposium on Document Analysis and Information Retrieval, pages 161?175, Las Vegas.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR: A libraryfor large linear classification.
Journal of Machine Learning Research, 9:1871?1874.159Gregory Grefenstette.
1995.
Comparing two language identification schemes.
In Proceedings of Analisi Statisticadei Dati Testuali (JADT), pages 263?268, Rome.Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom.
2014.
A convolutional neural network for modellingsentences.
In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers), pages 655?665, Baltimore, Maryland, June.
Association for Computational Linguistics.Rafael Dueire Lins and Paulo Gonc?alves.
2004.
Automatic language identification of written texts.
In Proceedingsof the 2004 ACM Symposium on Applied Computing, SAC ?04, pages 1128?1133, New York, NY, USA.
ACM.Nikola Ljube?si?c, Nives Mikeli?c, and Damir Boras.
2007.
Language identification: How to distinguish similarlanguages?
In Proceedings of the 29th International Conference on Information Technology Interfaces (ITI),pages 541?546.Marco Lui and Paul Cook.
2013.
Classifying english documents by national dialect.
In Proceedings of theAustralasian Language Technology Association Workshop 2013 (ALTA 2013), pages 5?15, Brisbane, Australia.Marco Lui, Jey Han Lau, and Timothy Baldwin.
2014.
Automatic detection and language identification of multi-lingual documents.
Transactions of the Association for Computational Linguistics, 2:27?40.Bali Ranaivo-Malanc?on.
2006.
Automatic identification of close languages - case study: Malay and Indonesian.ECTI Transactions on Computer and Information Technology, 2(2):126?134, November.Liling Tan, Marcos Zampieri, Nikola Ljube?si?c, and J?org Tiedemann.
2014.
Merging comparable data sources forthe discrimination of similar languages: The DSL Corpus Collection.
In Proceedings of the 7th Workshop onBuilding and Using Comparable Corpora (BUCC).J?org Tiedemann and Nikola Ljube?si?c.
2012.
Efficient discrimination between closely related languages.
InProceedings of COLING 2012, pages 2619?2634, Mumbai, India, December.Marcos Zampieri and Binyam Gebrekidan Gebre.
2012.
Automatic identification of language varieties: The caseof Portuguese.
In Proceedings of KONVENS2012, pages 233?237, Vienna, September.Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy.
2013.
N-gram language models and POSdistribution for the identification of Spanish varieties.
In Proceedings of TALN2013, pages 580?587, Sablesd?Olonne, France.Marcos Zampieri, Liling Tan, Nikola Ljube?si?c, and J?org Tiedemann.
2014.
A report on the DSL Shared Task2014.
In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects(VarDial).Marcos Zampieri.
2013.
Using bag-of-words to distinguish similar languages: How efficient are they?
InProceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics (CINTI),pages 37?41, Budapest, November.160
