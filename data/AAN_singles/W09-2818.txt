Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 99?100,Suntec, Singapore, 6 August 2009.c?2009 ACL and AFNLPICSI-CRF: The Generation of References to the Main Subject and NamedEntities using Conditional Random FieldsBenoit Favre and Bernd BohnetInternational Computer Science Institute1947 Center Street.
Suite 600Berkeley, CA 94704, USA{favre|bohnet}@icsi.berkeley.eduAbstractIn this paper, we describe our contribution tothe Generation Challenge 2009 for the tasks ofgenerating Referring Expressions to the MainSubject References (MSR) and Named Enti-ties Generation (NEG).
To generate the refer-ring expressions, we employ the ConditionalRandom Fields (CRF) learning technique dueto the fact that the selection of an expres-sion depends on the selection of the previ-ous references.
CRFs fit very well to thistask since they are designed for the labelingof sequences.
For the MSR task, our systemhas a String Accuracy of 0.68 and a REG08-Type Accuracy of 0.76 and for the NEG task aString Accuracy of 0.79 and REG08-Type Ac-curacy of 0.83.1 IntroductionThe GREC Generation Challenge 2009 consists oftwo tasks.
The first task is to generate appropriatereferences to an entity due to a given context whichis longer than a sentence.
In the GREC-MSR task,data sets are provided of possible referring expres-sions which have to be selected.
In the first sharedtask on same topic (Belz and Varges, 2007), the maintask was to select the referring expression type cor-rectly.
In the GREC-MSR 2009 task, the main taskis to select the actual word string correctly, and themain evaluation criterion is String Accuracy.The GREC-NEG task is about the generation ofreferences to all person entities in a context longerthan a sentence.
The NEG data also provides sets ofpossible referring expressions to each entity (?he?
),groups of multiple entities (?they?)
and nested refer-ences (?his father?
).2 System DescriptionOur approach relies in mapping each input expres-sion for a given reference to a class label.
We usethe attributes of the REFEX tags as basic labels sothat, for instance, a REFEX with attributes REG08-TYPE=?pronoun?
CASE=?nominative?
is mappedto the label ?nominative pronoun?.
In order to de-crease the number of potential textual units for a pre-dicted label, we derive extra label information fromthe text itself.
For instance a qualifier ?first name?or ?family name?
is added to the expressions rela-tive to a person.
Similarly, types of pronouns (he,him, his, who, whose, whom, emphasis) are speci-fied in the class label, which is very useful for theNEG task.
Only the person labels have been refinedthis way.
While we experimented with a few ap-proaches to remove the remaining ambiguity (samelabel for different text), they generally did not per-form better than a random selection.
We opted for adeterministic generation with the last element in thelist of possibilities given a class label.For prediction of attributes, our system uses Con-ditional Random Fields, as proposed by (Lafferty etal., 2001).
We use chain CRFs to estimate the prob-ability of a sequence of labels (Y = Y1.
.
.
Yn) givena sequence of observations (X = X1.
.
.
Xm).P (Y |X) ?
exp?
?n?j=1m?i=1?ifi(Yj?1, Yj, X)??
(1)Here, fi(?)
are decision functions that depend on99MSR NEGEvaluation Metric R1 R2 S1 S2 S2R S2O R1 R2 S1 S2 S2R S2OREG08 Type Accuracy 0.36 1.00 0.74 0.75 0.75 0.75 0.40 1.00 0.83 0.83 0.83 0.83String Accuracy 0.12 0.82 0.62 0.67 0.66 0.75 0.12 0.70 0.52 0.79 0.79 0.80Mean Edit Distance 2.52 0.31 0.95 0.85 0.87 0.72 2.38 0.61 1.07 0.53 0.52 0.49Mean Norm.
Edit Dist.
0.79 0.09 0.31 0.28 0.28 0.24 0.84 0.22 0.43 0.19 0.20 0.19BLEU 1 0.19 0.88 0.65 0.69 0.68 0.74 0.17 0.79 0.64 0.81 0.81 0.83BLEU 2 0.14 0.76 0.55 0.60 0.59 0.71 0.18 0.75 0.69 0.83 0.83 0.85BLEU 3 0.10 0.69 0.51 0.56 0.55 0.70 0.18 0.73 0.71 0.83 0.84 0.86Table 1: Results for the GREC MSR and NEG tasks.
Are displayed: a random2output (R1), a random output whenthe attributes are guessed correctly (R2), the CRF system predicting basic attributes (S1), the CRF system predictingrefined attributes (S2), CRF-predicted attributes with random selection of text (S2R) and CRF-predicted attributes withoracle selection of text (S2O).the examples and a clique of boundaries close to Yj,and ?iis the weight of fiestimated on training data.For our experiments, we use the CRF++ toolkit,1which allows binary decision functions dependenton the current label and the previous label.All features are used for both MSR and NEGtasks, where applicable:?
word unigram and bigram before and after thereference?
morphology of the previous and next words (-ed, -ing, -s)?
punctuation type, before and after (comma,parenthesis, period, nothing)?
SYNFUNC, SYNCAT and SEMCAT?
whether or not the previous reference is aboutthe same entity as the current one?
number of occurrence of the entity since the be-ginning of the text (quantized 1,2,3,4+)?
number of occurrence of the entity since thelast change of entity (quantized)?
beginning of paragraph indicatorIn the MSR case, this list is augmented with the fea-tures of the two previous references.
In the NEGcase, we use the features of the previous referenceand those of the previous occurrence of the same en-tity.1http://crfpp.sourceforge.net/3 Results and ConclusionTable 1 shows the results for the GREC MSR andNEG tasks.2We observe that for both tasks, our sys-tem exceeds the performance of a random3selection(columns R1 vs. S2).
In the MSR task, guessingcorrectly the attributes seems more important thanin the NEG task, as suggested by the difference instring accuracy when randomly selecting the refer-ences with the right attributes (columns R2).
Gener-ating more specific attributes from the text is espe-cially important for the NEG task (columns S1 vs.S2).
This was expected because we only refinedthe attributes for person entities.
We also observethat a deterministic disambiguation of the referenceswith the same attributes is not distinguishable froma random selection (columns S2 vs. S2R).
Howeverit seems that selecting the right text, as in the ora-cle experiment, would hardly help in the NEG taskwhile the gap is larger for the MSR task.
This showsthat refined classes work well for person entities butmore refinements are needed for other types (city,mountain, river...).ReferencesJ.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proc.
ofICML, pages 282-289A.
Belz and S. Varges.
2007 Generation of RepeatedReferences to Discourse Entities.
In Proceedings ofthe 11th European Workshop on Natural LanguageGeneration (ENLG07), pages 9-16.2Our system is available http://www.icsi.berkeley.edu/?favre/grec/3All random experiments are averaged over 100 runs.100
