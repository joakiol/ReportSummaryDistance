Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 352?361,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLearning Latent Personas of Film CharactersDavid Bamman Brendan O?Connor Noah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{dbamman,brenocon,nasmith}@cs.cmu.eduAbstractWe present two latent variable models forlearning character types, or personas, infilm, in which a persona is defined as aset of mixtures over latent lexical classes.These lexical classes capture the stereo-typical actions of which a character is theagent and patient, as well as attributes bywhich they are described.
As the firstattempt to solve this problem explicitly,we also present a new dataset for thetext-driven analysis of film, along witha benchmark testbed to help drive futurework in this area.1 IntroductionPhilosophers and dramatists have long arguedwhether the most important element of narrativeis plot or character.
Under a classical Aristotelianperspective, plot is supreme;1 modern theoreticaldramatists and screenwriters disagree.2Without addressing this debate directly, muchcomputational work on narrative has focused onlearning the sequence of events by which a storyis defined; in this tradition we might situate sem-inal work on learning procedural scripts (Schankand Abelson, 1977; Regneri et al, 2010), narrativechains (Chambers and Jurafsky, 2008), and plotstructure (Finlayson, 2011; Elsner, 2012; McIn-tyre and Lapata, 2010; Goyal et al, 2010).We present a complementary perspective thataddresses the importance of character in defining1?Dramatic action .
.
.
is not with a view to the representa-tion of character: character comes in as subsidiary to the ac-tions .
.
.
The Plot, then, is the first principle, and, as it were,the soul of a tragedy: Character holds the second place.?
Po-etics I.VI (Aristotle, 335 BCE).2?Aristotle was mistaken in his time, and our scholars aremistaken today when they accept his rulings concerning char-acter.
Character was a great factor in Aristotle?s time, and nofine play ever was or ever will be written without it?
(Egri,1946, p. 94); ?What the reader wants is fascinating, complexcharacters?
(McKee, 1997, 100).a story.
Our testbed is film.
Under this perspec-tive, a character?s latent internal nature drives theaction we observe.
Articulating narrative in thisway leads to a natural generative story: we first de-cide that we?re going to make a particular kind ofmovie (e.g., a romantic comedy), then decide on aset of character types, or personas, we want to seeinvolved (the PROTAGONIST, the LOVE INTER-EST, the BEST FRIEND).
After picking this set, wefill out each of these roles with specific attributes(female, 28 years old, klutzy); with this cast ofcharacters, we then sketch out the set of events bywhich they interact with the world and with eachother (runs but just misses the train, spills coffeeon her boss) ?
through which they reveal to theviewer those inherent qualities about themselves.This work is inspired by past approaches that in-fer typed semantic arguments along with narra-tive schemas (Chambers and Jurafsky, 2009; Reg-neri et al, 2011), but seeks a more holistic viewof character, one that learns from stereotypical at-tributes in addition to plot events.
This work alsonaturally draws on earlier work on the unsuper-vised learning of verbal arguments and semanticroles (Pereira et al, 1993; Grenager and Manning,2006; Titov and Klementiev, 2012) and unsuper-vised relation discovery (Yao et al, 2011).This character-centric perspective leads to twonatural questions.
First, can we learn what thosestandard personas are by how individual charac-ters (who instantiate those types) are portrayed?Second, can we learn the set of attributes and ac-tions by which we recognize those common types?How do we, as viewers, recognize a VILLIAN?At its most extreme, this perspective reducesto learning the grand archetypes of Joseph Camp-bell (1949) or Carl Jung (1981), such as the HEROor TRICKSTER.
We seek, however, a more fine-grained set that includes not only archetypes, butstereotypes as well ?
characters defined by a fixedset of actions widely known to be representative of352a class.
This work offers a data-driven method foranswering these questions, presenting two proba-blistic generative models for inferring latent char-acter types.This is the first work that attempts to learn ex-plicit character personas in detail; as such, wepresent a new dataset for character type inductionin film and a benchmark testbed for evaluating fu-ture work.32 Data2.1 TextOur primary source of data comes from 42,306movie plot summaries extracted from theNovember 2, 2012 dump of English-languageWikipedia.4 These summaries, which have amedian length of approximately 176 words,5contain a concise synopsis of the movie?s events,along with implicit descriptions of the characters(e.g., ?rebel leader Princess Leia,?
?evil lord DarthVader?).
To extract structure from this data, weuse the Stanford CoreNLP library6 to tag andsyntactically parse the text, extract entities, andresolve coreference within the document.
Withthis structured representation, we extract linguisticfeatures for each character, looking at immediateverb governors and attribute syntactic dependen-cies to all of the entity?s mention headwords,extracted from the typed dependency tuples pro-duced by the parser; we refer to ?CCprocessed?syntactic relations described in de Marneffe andManning (2008):?
Agent verbs.
Verbs for which the entity is anagent argument (nsubj or agent).?
Patient verbs.
Verbs for which the entity isthe patient, theme or other argument (dobj,nsubjpass, iobj, or any prepositional argu-ment prep *).?
Attributes.
Adjectives and common nounwords that relate to the mention as adjecti-val modifiers, noun-noun compounds, appos-itives, or copulas (nsubj or appos governors,or nsubj, appos, amod, nn dependents of anentity mention).3All datasets and software for replication can be found athttp://www.ark.cs.cmu.edu/personas.4http://dumps.wikimedia.org/enwiki/5More popular movies naturally attract more attention onWikipedia and hence more detail: the top 1,000 movies bybox office revenue have a median length of 715 words.6http://nlp.stanford.edu/software/corenlp.shtmlThese three roles capture three different ways inwhich character personas are revealed: the actionsthey take on others, the actions done to them, andthe attributes by which they are described.
For ev-ery character we thus extract a bag of (r, w) tu-ples, where w is the word lemma and r is oneof {agent verb,patient verb, attribute} as iden-tified by the above rules.2.2 MetadataOur second source of information consists of char-acter and movie metadata drawn from the Novem-ber 4, 2012 dump of Freebase.7 At the movielevel, this includes data on the language, country,release date and detailed genre (365 non-mutuallyexclusive categories, including ?Epic Western,??Revenge,?
and ?Hip Hop Movies?).
Many of thecharacters in movies are also associated with theactors who play them; since many actors also havedetailed biographical information, we can groundthe characters in what we know of those real peo-ple ?
including their gender and estimated age atthe time of the movie?s release (the difference be-tween the release date of the movie and the actor?sdate of birth).Across all 42,306 movies, entities average 3.4agent events, 2.0 patient events, and 2.1 attributes.For all experiments described below, we restrictour dataset to only those events that are among the1,000 most frequent overall, and only characterswith at least 3 events.
120,345 characters meet thiscriterion; of these, 33,559 can be matched to Free-base actors with a specified gender, and 29,802 canbe matched to actors with a given date of birth.
Ofall actors in the Freebase data whose age is given,the average age at the time of movie is 37.9 (stan-dard deviation 14.1); of all actors whose genderis known, 66.7% are male.8 The age distributionis strongly bimodal when conditioning on gender:the average age of a female actress at the time of amovie?s release is 33.0 (s.d.
13.4), while that of amale actor is 40.5 (s.d.
13.7).3 PersonasOne way we recognize a character?s latent typeis by observing the stereotypical actions they7http://download.freebase.com/datadumps/8Whether this extreme 2:1 male/female ratio reflects aninherent bias in film or a bias in attention on Freebase (orWikipedia, on which it draws) is an interesting research ques-tion in itself.353perform (e.g., VILLAINS strangle), the actionsdone to them (e.g., VILLAINS are foiled and ar-rested) and the words by which they are described(VILLAINS are evil).
To capture this intuition, wedefine a persona as a set of three typed distribu-tions: one for the words for which the character isthe agent, one for which it is the patient, and onefor words by which the character is attributivelymodified.
Each distribution ranges over a fixed setof latent word classes, or topics.
Figure 1 illus-trates this definition for a toy example: a ZOMBIEpersona may be characterized as being the agentof primarily eating and killing actions, the patientof killing actions, and the object of dead attributes.The topic labeled eat may include words like eat,drink, and devour.eat kill lovedeadhappyagent0.00.20.40.60.81.0eat kill lovedeadhappypatient0.00.20.40.60.81.0eat kill lovedeadhappyattribute0.00.20.40.60.81.0Figure 1: A persona is a set of three distributionsover latent topics.
In this toy example, the ZOM-BIE persona is primarily characterized by beingthe agent of words from the eat and kill topics, thepatient of kill words, and the object of words fromthe dead topic.4 ModelsBoth models that we present here simultaneouslylearn three things: 1.)
a soft clustering over wordsto topics (e.g., the verb ?strangle?
is mostly a typeof Assault word); 2.)
a soft clustering over top-ics to personas (e.g., VILLIANS perform a lot ofAssault actions); and 3.)
a hard clustering overcharacters to personas (e.g., Darth Vader is a VIL-LAIN.)
They each use different evidence: sinceour data includes not only textual features (in theform of actions and attributes of the characters) butalso non-textual information (such as movie genre,age and gender), we design a model that exploitsthis additional source of information in discrimi-nating between character types; since this extra-linguistic information may not always be avail-able, we also design a model that learns only fromthe text itself.
We present the text-only model first??pz?wr??
?WED?p memd???2z?wr??
?WEDP Number of personas (hyperparameter)K Number of word topics (hyperparameter)D Number of movie plot summariesE Number of characters in movie dW Number of (role, word) tuples used by character e?k Topic k?s distribution over V words.r Tuple role: agent verb, patient verb, attribute?p,r Distribution over topics for persona p in role r?d Movie d?s distribution over personaspe Character e?s persona (integer, p ?
{1..P})j A specific (r, w) tuple in the datazj Word topic for tuple jwj Word for tuple j?
Concentration parameter for Dirichlet model?
Feature weights for regression model?, ?2 Gaussian mean and variance (for regularizing ?
)md Movie features (from movie metadata)me Entity features (from movie actor metadata)?r , ?
Dirichlet concentration parametersFigure 2: Above: Dirichlet persona model (left)and persona regression model (right).
Bottom:Definition of variables.for simplicity.
Throughout, V is the word vocab-ulary size, P is the number of personas, and K isthe number of topics.4.1 Dirichlet Persona ModelIn the most basic model, we only use informa-tion from the structured text, which comes as abag of (r, w) tuples for each character in a movie,where w is the word lemma and r is the rela-tion of the word with respect to the character (oneof agent verb, patient verb or attribute, as out-lined in ?2.1 above).
The generative story runs asfollows.
First, let there be K latent word topics;as in LDA (Blei et al, 2003), these are words thatwill be soft-clustered together by virtue of appear-ing in similar contexts.
Each latent word cluster354?k ?
Dir(?)
is a multinomial over the V words inthe vocabulary, drawn from a Dirichlet parameter-ized by ?.
Next, let a persona p be defined as a setof three multinomials ?p over these K topics, onefor each typed role r, each drawn from a Dirichletwith a role-specific hyperparameter (?r).Every document (a movie plot summary) con-tains a set of characters, each of which is associ-ated with a single latent persona p; for every ob-served (r, w) tuple associated with the character,we sample a latent topic k from the role-specific?p,r.
Conditioned on this topic assignment, theobserved word is drawn from ?k.
The distribu-tion of these personas for a given document is de-termined by a document-specific multinomial ?,drawn from a Dirichlet parameterized by ?.Figure 2 (above left) illustrates the form of themodel.
To simplify inference, we collapse out thepersona-topic distributions ?, the topic-word dis-tributions ?
and the persona distribution ?
for eachdocument.
Inference on the remaining latent vari-ables ?
the persona p for each character type andthe topic z for each word associated with that char-acter ?
is conducted via collapsed Gibbs sampling(Griffiths and Steyvers, 2004); at each iteration,for each character e, we sample their persona pe:P (pe = k | p?e, z, ?, ?)
?
(c?ed,k + ?k)?
?j(c?erj ,k,zj+?rj )(c?erj ,k,?+K?rj )(1)Here, c?ed,k is the count of all characters in docu-ment d whose current persona sample is also k(not counting the current character e under con-sideration);9 j ranges over all (rj , wj) tuples asso-ciated with character e. Each c?erj ,k,zj is the countof all tuples with role rj and current topic zj usedwith persona k. c?erj ,k,?
is the same count, summingover all topics z.
In other words, the probabil-ity that character e embodies persona k is propor-tional to the number of other characters in the plotsummary who also embody that persona (plus theDirichlet hyperparameter ?k) times the contribu-tion of each observed word wj for that character,given its current topic assignment zj .Once all personas have been sampled, we sam-9The?e superscript denotes counts taken without consid-ering the current sample for character e.ple the latent topics for each tuple as the following.P (zj = k | p, z?j , w, r, ?, ?)
?
(c?jrj ,p,k+?rj )(c?jrj ,p,?+K?rj )?(c?jk,wj+?
)(c?jk,?+V ?
)(2)Here, conditioned on the current sample p forthe character?s persona, the probability that tuplej originates in topic k is proportional to the num-ber of other tuples with that same role rj drawnfrom the same topic for that persona (c?jrj ,p,k), nor-malized by the number of other rj tuples associ-ated with that persona overall (c?jrj ,p,?
), multipliedby the number of times word wj is associated withthat topic (c?jk,wj ) normalized by the total numberof other words associated with that topic overall(c?jk,?
).We optimize the values of the Dirichlet hyper-parameters ?, ?
and ?
using slice sampling with auniform prior every 20 iterations for the first 500iterations, and every 100 iterations thereafter.
Af-ter a burn-in phase of 10,000 iterations, we collectsamples every 10 iterations (to lessen autocorrela-tion) until a total of 100 have been collected.4.2 Persona RegressionTo incorporate observed metadata in the form ofmovie genre, character age and character gen-der, we adopt an ?upstream?
modeling approach(Mimno and McCallum, 2008), letting those ob-served features influence the conditional probabil-ity with which a given character is expected to as-sume a particular persona, prior to observing anyof their actions.
This captures the increased likeli-hood, for example, that a 25-year-old male actor inan action movie will play an ACTION HERO thanhe will play a VALLEY GIRL.To capture these effects, each character?s la-tent persona is no longer drawn from a document-specific Dirichlet; instead, the P -dimensional sim-plex is the output of a multiclass logistic regres-sion, where the document genre metadata md andthe character age and gender metadatame togetherform a feature vector that combines with persona-specific feature weights to form the following log-linear distribution over personas, with the proba-bility for persona k being:P (p = k | md,me, ?)
= exp([md;me]>?k)1+PP?1j=1 exp([md;me]>?j)(3)The persona-specific ?
coefficients are learnedthrough Monte Carlo Expectation Maximization355(Wei and Tanner, 1990), in which we alternate be-tween the following:1.
Given current values for ?, for all characterse in all plot summaries, sample values of peand zj for all associated tuples.2.
Given input metadata features m and the as-sociated sampled values of p, find the valuesof ?
that maximize the standard multiclass lo-gistic regression log likelihood, subject to `2regularization.Figure 2 (above right) illustrates this model.
Aswith the Dirichlet persona model, inference on pfor step 1 is conducted with collapsed Gibbs sam-pling; the only difference in the sampling prob-ability from equation 1 is the effect of the prior,which here is deterministically fixed as the outputof the regression.P (pe = k | p?e, z, ?,md,me, ?)
?exp([md;me]>?k)?
?j(c?erj ,k,zj+?rj )(c?erj ,k,?+K?rj )(4)The sampling equation for the topic assign-ments z is identical to that in equation 2.
Inpractice we optimize ?
every 1,000 iterations, un-til a burn-in phase of 10,000 iterations has beenreached; at this point we following the same sam-pling regime as for the Dirichlet persona model.5 EvaluationWe evaluate our methods in two quantitative waysby measuring the degree to which we recover twodifferent sets of gold-standard clusterings.
Thisevaluation also helps offer guidance for model se-lection (in choosing the number of latent topicsand personas) by measuring performance on anobjective task.5.1 Character NamesFirst, we consider all character names that occur inat least two separate movies, generally as a conse-quence of remakes or sequels; this includes propernames such as ?Rocky Balboa,?
?Oliver Twist,?and ?Indiana Jones,?
as well as generic type namessuch as ?Gang Member?
and ?The Thief?
; to mini-mize ambiguity, we only consider character namesconsisting of at least two tokens.
Each of thesenames is used by at least two different characters;for example, a character named ?Jason Bourne?is portrayed in The Bourne Identity, The BourneSupremacy, and The Bourne Ultimatum.
Whilethese characters are certainly free to assume dif-ferent roles in different movies, we believe that,in the aggregate, they should tend to embody thesame character type and thus prove to be a natu-ral clustering to recover.
970 character names oc-cur at least twice in our data, and 2,666 individualcharacters use one of those names.
Let those 970character names define 970 unique gold clusterswhose members include the individual characterswho use that name.5.2 TV TropesAs a second external measure of validation, weconsider a manually created clustering presentedat the website TV Tropes,10 a wiki that col-lects user-submitted examples of common tropes(narrative, character and plot devices) found intelevision, film, and fiction, among other me-dia.
While TV Tropes contains a wide range ofsuch conventions, we manually identified a set of72 tropes that could reasonably be labeled char-acter types, including THE CORRUPT CORPO-RATE EXECUTIVE, THE HARDBOILED DETEC-TIVE, THE JERK JOCK, THE KLUTZ and THESURFER DUDE.We manually aligned user-submitted examplesof characters embodying these 72 character typeswith the canonical references in Freebase to cre-ate a test set of 501 individual characters.
Whilethe 72 character tropes represented here are a moresubjective measure, we expect to be able to at leastpartially recover this clustering.5.3 Variation of InformationTo measure the similarity between the two clus-terings of movie characters, gold clusters G andinduced latent persona clusters C, we calculate thevariation of information (Meila?, 2007):V I(G, C) = H(G) +H(C)?
2I(G, C) (5)= H(G|C) +H(C|G) (6)VI measures the information-theoretic distancebetween the two clusterings: a lower value meansgreater similarity, and VI = 0 if they are iden-tical.
Low VI indicates that (induced) clustersand (gold) clusters tend to overlap; i.e., knowing acharacter?s (induced) cluster usually tells us their(gold) cluster, and vice versa.
Variation of infor-mation is a metric (symmetric and obeys triangle10http://tvtropes.org356Character Names ?5.1 TV Tropes ?5.2K Model P = 25 P = 50 P = 100 P = 25 P = 50 P = 10025 Persona regression 7.73 7.32 6.79 6.26 6.13 5.74Dirichlet persona 7.83 7.11 6.44 6.29 6.01 5.5750 Persona regression 7.59 7.08 6.46 6.30 5.99 5.65Dirichlet persona 7.57 7.04 6.35 6.23 5.88 5.60100 Persona regression 7.58 6.95 6.32 6.11 6.05 5.49Dirichlet persona 7.64 6.95 6.25 6.24 5.91 5.42Table 1: Variation of information between learned personas and gold clusters for different numbers oftopics K and personas P .
Lower values are better.
All values are reported in bits.Character Names ?5.1 TV Tropes ?5.2K Model P = 25 P = 50 P = 100 P = 25 P = 50 P = 10025 Persona regression 62.8 (?41%) 59.5 (?40%) 53.7 (?33%) 42.3 (?31%) 38.5 (?24%) 33.1 (?25%)Dirichlet persona 54.7 (?27%) 50.5 (?26%) 45.4 (?17%) 39.5 (?20%) 31.7 (?28%) 25.1 (?21%)50 Persona regression 63.1 (?42%) 59.8 (?42%) 53.6 (?34%) 42.9 (?30%) 39.1 (?33%) 31.3 (?20%)Dirichlet persona 57.2 (?34%) 49.0 (?23%) 44.7 (?16%) 39.7 (?30%) 31.5 (?32%) 24.6 (?22%)100 Persona regression 63.1 (?42%) 57.7 (?39%) 53.0 (?34%) 43.5 (?33%) 32.1 (?28%) 26.5 (?22%)Dirichlet persona 55.3 (?30%) 49.5 (?24%) 45.2 (?18%) 39.7 (?34%) 29.9 (?24%) 23.6 (?19%)Table 2: Purity scores of recovering gold clusters.
Higher values are better.
Each absolute purity scoreis paired with its improvement over a controlled baseline of permuting the learned labels while keepingthe cluster proportions the same.inequality), and has a number of other desirableproperties.Table 1 presents the VI between the learned per-sona clusters and gold clusters, for varying num-bers of personas (P = {25, 50, 100}) and top-ics (K = {25, 50, 100}).
To determine signifi-cance with respect to a random baseline, we con-duct a permutation test (Fisher, 1935; Pitman,1937) in which we randomly shuffle the labels ofthe learned persona clusters and count the num-ber of times in 1,000 such trials that the VI ofthe observed persona labels is lower than the VIof the permuted labels; this defines a nonparamet-ric p-value.
All results presented are significant atp < 0.001 (i.e.
observed VI is never lower thanthe simulation VI).Over all tests in comparison to both gold clus-terings, we see VI improve as both P and, toa lesser extent, K increase.
While this may beexpected as the number of personas increase tomatch the number of distinct types in the goldclusters (970 and 72, respectively), the fact that VIimproves as the number of latent topics increasessuggests that more fine-grained topics are helpfulfor capturing nuanced character types.11The difference between the persona regressionmodel and the Dirichlet persona model here is not11This trend is robust to the choice of cluster metric: hereVI and F -score have a correlation of ?0.87; as more latenttopics and personas are added, clustering improves (causingthe F -score to go up and the VI distance to go down).significant; while VI allows us to compare mod-els with different numbers of latent clusters, its re-quirement that clusterings be mutually informativeplaces a high overhead on models that are funda-mentally unidirectional (in Table 1, for example,the room for improvement between two modelsof the same P and K is naturally smaller thanthe bigger difference between different P or K).While we would naturally prefer a text-only modelto be as expressive as a model that requires po-tentially hard to acquire metadata, we tease apartwhether a distinction actually does exist by evalu-ating the purity of the gold clusters with respect tothe labels assigned them.5.4 PurityFor gold clusters G = {g1 .
.
.
gk} and inferredclusters C = {c1 .
.
.
cj} we calculate purity as:Purity = 1N?kmaxj|gk ?
cj | (7)While purity cannot be used to compare models ofdifferent persona size P , it can help us distinguishbetween models of the same size.
A model canattain perfect purity, however, by placing all char-acters into a single cluster; to control for this, wepresent a controlled baseline in which each char-acter is assigned a latent character type label pro-portional to the size of the latent clusters we havelearned (so that, for example, if one latent per-sona cluster contains 3.2% of the total characters,357BatmanJimGordondark, major, henchmanshoot, aim, overpowersentence, arrest, assignTonyStarkJasonBourneTheJokershoot, aim, overpowertestify, rebuff, confesshatch, vow, undergoVan HelsingColinSullivanDraculaThe DepartedThe DarkKnightIron ManThe BourneIdentityapprove, die, sufferrelent, refuse, agreeinherit live imagineJackDawsonRachelTitanicFigure 3: Dramatis personae of The Dark Knight (2008), illustrating 3 of the 100 character types learnedby the persona regression model, along with links from other characters in those latent classes to othermovies.
Each character type is listed with the top three latent topics with which it is associated.the probability of selecting that persona at randomis 3.2%).
Table 2 presents each model?s absolutepurity score paired with its improvement over itscontrolled permutation (e.g., ?41%).Within each fixed-size partition, the use ofmetadata yields a substantial improvement overthe Dirichlet model, both in terms of absolute pu-rity and in its relative improvement over its sized-controlled baseline.
In practice, we find that whilethe Dirichlet model distinguishes between charac-ter personas in different movies, the persona re-gression model helps distinguish between differ-ent personas within the same movie.6 Exploratory Data AnalysisAs with other generative approaches, latent per-sona models enable exploratory data analysis.
Toillustrate this, we present results from the personaregression model learned above, with 50 latentlexical classes and 100 latent personas.
Figure 3visualizes this data by focusing on a single movie,The Dark Knight (2008); the movie?s protagonist,Batman, belongs to the same latent persona as De-tective Jim Gordon, as well as other action movieprotagonists Jason Bourne and Tony Stark (IronMan).
The movie?s antagonist, The Joker, belongsto the same latent persona as Dracula from VanHelsing and Colin Sullivan from The Departed, il-lustrating the ability of personas to be informedby, but still cut across, different genres.Table 3 presents an exhaustive list of all 50 top-ics, along with an assigned label that consists ofthe single word with the highest PMI for that class.Of note are topics relating to romance (unite,marry, woo, elope, court), commercial transac-tions (purchase, sign, sell, owe, buy), and the clas-sic criminal schema from Chambers (2011) (sen-tence, arrest, assign, convict, promote).Table 4 presents the most frequent 14 personasin our dataset, illustrated with characters fromthe 500 highest grossing movies.
The personaslearned are each three separate mixtures of the50 latent topics (one for agent relations, one forpatient relations, and one for attributes), as illus-trated in figure 1 above.
Rather than presentinga 3 ?
50 histogram for each persona, we illus-trate them by listing the most characteristic top-ics, movie characters, and metadata features asso-ciated with it.
Characteristic actions and featuresare defined as those having the highest smoothedpointwise mutual information with that class; ex-emplary characters are those with the highest pos-terior probability of being drawn from that class.Among the personas learned are canonical maleaction heroes (exemplified by the protagonists ofThe Bourne Supremacy, Speed, and Taken), super-heroes (Hulk, Batman and Robin, Hector of Troy)and several romantic comedy types, largely char-acterized by words drawn from the FLIRT topic,including flirt, reconcile, date, dance and forgive.358Label Most characteristic words Label Most characteristic wordsUNITE unite marry woo elope court SWITCH switch confirm escort report instructPURCHASE purchase sign sell owe buy INFATUATE infatuate obsess acquaint revolve concernSHOOT shoot aim overpower interrogate kill ALIEN alien child governor bandit priestEXPLORE explore investigate uncover deduce CAPTURE capture corner transport imprison trapWOMAN woman friend wife sister husband MAYA maya monster monk goon dragonWITCH witch villager kid boy mom INHERIT inherit live imagine experience shareINVADE invade sail travel land explore TESTIFY testify rebuff confess admit denyDEFEAT defeat destroy transform battle inject APPLY apply struggle earn graduate developCHASE chase scare hit punch eat EXPEL expel inspire humiliate bully grantTALK talk tell reassure assure calm DIG dig take welcome sink revolvePOP pop lift crawl laugh shake COMMAND command abduct invade seize surrenderSING sing perform cast produce dance RELENT relent refuse agree insist hopeAPPROVE approve die suffer forbid collapse EMBARK embark befriend enlist recall meetWEREWOLF werewolf mother parent killer father MANIPULATE manipulate conclude investigate conductDINER diner grandfather brother terrorist ELOPE elope forget succumb pretend likeDECAPITATE decapitate bite impale strangle stalk FLEE flee escape swim hide manageREPLY reply say mention answer shout BABY baby sheriff vampire knight spiritDEMON demon narrator mayor duck crime BIND bind select belong refer representCONGRATULATE congratulate cheer thank recommend REJOIN rejoin fly recruit include disguiseINTRODUCE introduce bring mock read hatch DARK dark major henchman warrior sergeantHATCH hatch don exist vow undergo SENTENCE sentence arrest assign convict promoteFLIRT flirt reconcile date dance forgive DISTURB disturb frighten confuse tease scareADOPT adopt raise bear punish feed RIP rip vanish crawl drive smashFAIRY fairy kidnapper soul slave president INFILTRATE infiltrate deduce leap evade obtainBUG bug zombie warden king princess SCREAM scream faint wake clean hearTable 3: Latent topics learned for K = 50 and P = 100.
The words shown for each class are those withthe highest smoothed PMI, with the label being the single word with the highest PMI.Freq Actions Characters Features0.109 DARKm, SHOOTa,SHOOTpJason Bourne (The Bourne Supremacy), Jack Traven(Speed), Jean-Claude (Taken)Action, Male, Warfilm0.079 CAPTUREp,INFILTRATEa, FLEEaAang (The Last Airbender), Carly (Transformers: Dark ofthe Moon), Susan Murphy/Ginormica (Monsters vs. Aliens)Female, Action,Adventure0.067 DEFEATa, DEFEATp,INFILTRATEaGlenn Talbot (Hulk), Batman (Batman and Robin), Hector(Troy)Action, Animation,Adventure0.060 COMMANDa, DEFEATp,CAPTUREpZoe Neville (I Am Legend), Ursula (The Little Mermaid),Joker (Batman)Action, Adventure,Male0.046 INFILTRATEa,EXPLOREa, EMBARKaPeter Parker (Spider-Man 3), Ethan Hunt (Mission:Impossible), Jason Bourne (The Bourne Ultimatum)Male, Action, Age34-360.036 FLIRTa, FLIRTp,TESTIFYaMark Darcy (Bridget Jones: The Edge of Reason), JerryMaguire (Jerry Maguire), Donna (Mamma Mia!
)Female, RomanceFilm, Comedy0.033 EMBARKa, INFILTRATEa,INVADEaPerseus (Wrath of the Titans), Maximus Decimus Meridius(Gladiator), Julius (Twins)Male, ChineseMovies, Spy0.027 CONGRATULATEa,CONGRATULATEp,SWITCHaProfessor Albus Dumbledore (Harry Potter and thePhilosopher?s Stone), Magic Mirror (Shrek), JosephineAnwhistle (Lemony Snicket?s A Series of UnfortunateEvents)Age 58+, FamilyFilm, Age 51-570.025 SWITCHa, SWITCHp,MANIPULATEaClarice Starling (The Silence of the Lambs), HannibalLecter (The Silence of the Lambs), Colonel Bagley (TheLast Samurai)Age 58+, Male,Age 45-500.022 REPLYa, TALKp, FLIRTp Graham (The Holiday), Abby Richter (The Ugly Truth),Anna Scott (Notting Hill)Female, Comedy,Romance Film0.020 EXPLOREa, EMBARKa,CAPTUREpHarry Potter (Harry Potter and the Philosopher?s Stone),Harry Potter (Harry Potter and the Chamber of Secrets),Captain Leo Davidson (Planet of the Apes)Adventure, FamilyFilm, Horror0.018 FAIRYm, COMMANDa,CAPTUREpCaptain Jack Sparrow (Pirates of the Caribbean: AtWorld?s End), Shrek (Shrek), Shrek (Shrek Forever After)Action, FamilyFilm, Animation0.018 DECAPITATEa,DECAPITATEp, RIPaJericho Cane (End of Days), Martin Riggs (Lethal Weapon2), Gabriel Van Helsing (Van Helsing)Horror, Slasher,Teen0.017 APPLYa, EXPELp,PURCHASEpOscar (Shark Tale), Elizabeth Halsey (Bad Teacher), DreParker (The Karate Kid)Female, Teen,Under Age 22Table 4: Of 100 latent personas learned, we present the top 14 by frequency.
Actions index the latenttopic classes presented in table 3; subscripts denote whether the character is predominantly the agent (a),patient (p) or is modified by an attribute (m).3597 ConclusionWe present a method for automatically inferringlatent character personas from text (and metadata,when available).
While our testbed has been tex-tual synopses of film, this approach is easily ex-tended to other genres (such as novelistic fiction)and to non-fictional domains as well, where thechoice of portraying a real-life person as embody-ing a particular kind of persona may, for instance,give insight into questions of media framing andbias in newswire; self-presentation of individualpersonas likewise has a long history in communi-cation theory (Goffman, 1959) and may be use-ful for inferring user types for personalization sys-tems (El-Arini et al, 2012).
While the goal of thiswork has been to induce a set of latent characterclasses and partition all characters among them,one interesting question that remains is how a spe-cific character?s actions may informatively be atodds with their inferred persona, given the choiceof that persona as the single best fit to explain theactions we observe.
By examining how any indi-vidual character deviates from the behavior indica-tive of their type, we might be able to paint a morenuanced picture of how a character can embody aspecific persona while resisting it at the same time.AcknowledgmentsWe thank Megan Morrison at the CMU School ofDrama for early conversations guiding our work,as well as the anonymous reviewers for helpfulcomments.
The research reported in this articlewas supported by U.S. National Science Founda-tion grant IIS-0915187 and by an ARCS scholar-ship to D.B.
This work was made possible throughthe use of computing resources made available bythe Pittsburgh Supercomputing Center.ReferencesAristotle.
335 BCE.
Poetics, translated by Samuel H.Butcher (1902).
Macmillan, London.David M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent dirichlet alocation.
JMLR, 3:993?1022.Joseph Campbell.
1949.
The Hero with a ThousandFaces.
Pantheon Books.Nathanael Chambers and Dan Jurafsky.
2008.
Unsu-pervised learning of narrative event chains.
In Pro-ceedings of ACL-08: HLT.Nathanael Chambers and Dan Jurafsky.
2009.
Unsu-pervised learning of narrative schemas and their par-ticipants.
In Proceedings of the 47th Annual Meet-ing of the ACL.Nathanael Chambers.
2011.
Inducing Event Schemasand their Participants from Unlabeled Text.
Ph.D.thesis, Stanford University.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
Stanford typed dependencies manual.Technical report, Stanford University.Lajos Egri.
1946.
The Art of Dramatic Writing.
Simonand Schuster, New York.Khalid El-Arini, Ulrich Paquet, Ralf Herbrich, JurgenVan Gael, and Blaise Agu?era y Arcas.
2012.
Trans-parent user models for personalization.
In Proceed-ings of the 18th ACM SIGKDD.Micha Elsner.
2012.
Character-based kernels for nov-elistic plot structure.
In Proceedings of the 13thConference of the EACL.Mark Alan Finlayson.
2011.
Learning NarrativeStructure from Annotated Folktales.
Ph.D. thesis,MIT.R.
A. Fisher.
1935.
The Design of Experiments.
Oliverand Boyde, Edinburgh and London.Erving Goffman.
1959.
The Presentation of the Self inEveryday Life.
Anchor.Amit Goyal, Ellen Riloff, and Hal Daume?, III.
2010.Automatically producing plot unit representationsfor narrative text.
In Proceedings of the 2010 Con-ference on EMNLP.Trond Grenager and Christopher D. Manning.
2006.Unsupervised discovery of a statistical verb lexicon.In Proceedings of the 2006 Conference on EMNLP.Thomas L. Griffiths and Mark Steyvers.
2004.
Findingscientific topics.
PNAS, 101(suppl.
1):5228?5235.Carl Jung.
1981.
The Archetypes and The CollectiveUnconscious, volume 9 of Collected Works.
Bollin-gen, Princeton, NJ, 2nd edition.Neil McIntyre and Mirella Lapata.
2010.
Plot induc-tion and evolutionary search for story generation.
InProceedings of the 48th Annual Meeting of the ACL.Association for Computational Linguistics.Robert McKee.
1997.
Story: Substance, Structure,Style and the Principles of Screenwriting.
Harper-Colllins.Marina Meila?.
2007.
Comparing clusterings?an in-formation based distance.
Journal of MultivariateAnalysis, 98(5):873?895.David Mimno and Andrew McCallum.
2008.
Topicmodels conditioned on arbitrary features withdirichlet-multinomial regression.
In Proceedings ofUAI.360Fernando Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of english words.
InProceedings of the 31st Annual Meeting of the ACL.E.
J. G. Pitman.
1937.
Significance tests which maybe applied to samples from any population.
Supple-ment to the Journal of the Royal Statistical Society,4(1):119?130.Michaela Regneri, Alexander Koller, and ManfredPinkal.
2010.
Learning script knowledge with webexperiments.
In Proceedings of the 48th AnnualMeeting of the ACL.Michaela Regneri, Alexander Koller, Josef Ruppen-hofer, and Manfred Pinkal.
2011.
Learning scriptparticipants from unlabeled data.
In Proceedings ofthe Conference on Recent Advances in Natural Lan-guage Processing.Roger C. Schank and Robert P. Abelson.
1977.
Scripts,plans, goals, and understanding: An inquiry intohuman knowledge structures.
Lawrence Erlbaum,Hillsdale, NJ.Ivan Titov and Alexandre Klementiev.
2012.
Abayesian approach to unsupervised semantic role in-duction.
In Proceedings of the 13th Conference ofEACL.Greg C. G. Wei and Martin A. Tanner.
1990.
A MonteCarlo implementation of the EM algorithm and thepoor man?s data augmentation algorithms.
Journalof the American Statistical Association, 85:699?704.Limin Yao, Aria Haghighi, Sebastian Riedel, and An-drew McCallum.
2011.
Structured relation discov-ery using generative models.
In Proceedings of theConference on EMNLP.361
