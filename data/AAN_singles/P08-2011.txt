Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41?44,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCoreference-inspired Coherence ModelingMicha Elsner and Eugene CharniakBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{melsner,ec}@cs.brown.eduAbstractResearch on coreference resolution and sum-marization has modeled the way entities arerealized as concrete phrases in discourse.
Inparticular there exist models of the nounphrase syntax used for discourse-new versusdiscourse-old referents, and models describ-ing the likely distance between a pronoun andits antecedent.
However, models of discoursecoherence, as applied to information orderingtasks, have ignored these kinds of information.We apply a discourse-new classifier and pro-noun coreference algorithm to the informationordering task, and show significant improve-ments in performance over the entity grid, apopular model of local coherence.1 IntroductionModels of discourse coherence describe the relation-ships between nearby sentences, in which previoussentences help make their successors easier to un-derstand.
Models of coherence have been used toimpose an order on sentences for multidocumentsummarization (Barzilay et al, 2002), to evaluatethe quality of human-authored essays (Miltsakakiand Kukich, 2004), and to insert new informationinto existing documents (Chen et al, 2007).These models typically view a sentence either asa bag of words (Foltz et al, 1998) or as a bag of en-tities associated with various syntactic roles (Lapataand Barzilay, 2005).
However, a mention of an en-tity contains more information than just its head andsyntactic role.
The referring expression itself con-tains discourse-motivated information distinguish-ing familiar entities from unfamiliar and salient fromnon-salient.
These patterns have been studied ex-tensively, by linguists (Prince, 1981; Fraurud, 1990)and in the field of coreference resolution.
We drawon the coreference work, taking two standard modelsfrom the literature and applying them to coherencemodeling.Our first model distinguishes discourse-new fromdiscourse-old noun phrases, using features basedon Uryupina (2003).
Discourse-new NPs are thosewhose referents have not been previously mentionedin the discourse.
As noted by studies since Hawkins(1978), there are marked syntactic differences be-tween the two classes.Our second model describes pronoun coreference.To be intelligible, pronouns must be placed close toappropriate referents with the correct number andgender.
Centering theory (Grosz et al, 1995) de-scribes additional constraints about which entities ina discourse can be pronominalized: if there are pro-nouns in a segment, they must include the backward-looking center.
We use a model which probabilisti-cally attempts to describe these preferences (Ge etal., 1998).These two models can be combined with the en-tity grid described by Lapata and Barzilay (2005)for significant improvement.
The magnitude of theimprovement is particularly interesting given thatBarzilay and Lapata (2005) do use a coreference sys-tem but are unable to derive much advantage from it.2 Discourse-new ModelIn the task of discourse-new classification, the modelis given a referring expression (as in previous work,we consider only NPs) from a document and must41determine whether it is a first mention (discourse-new) or a subsequent mention (discourse-old).
Fea-tures such as full names, appositives, and restrictiverelative clauses are associated with the introductionof unfamiliar entities into discourse (Hawkins, 1978;Fraurud, 1990; Vieira and Poesio, 2000).
Classi-fiers in the literature include (Poesio et al, 2005;Uryupina, 2003; Ng and Cardie, 2002).
The sys-tem of Nenkova and McKeown (2003) works in theopposite direction.
It is designed to rewrite the ref-erences in multi-document summaries, so that theyconform to the common discourse patterns.We construct a maximum-entropy classifier us-ing syntactic and lexical features derived fromUryupina (2003), and a publicly available learningtool (Daume?
III, 2004).
Our system scores 87.4%(F-score of the disc-new class on the MUC-7 for-mal test set); this is comparable to the state-of-the-art system of Uryupina (2003), which scores 86.91.To model coreference with this system, we assigneach NP in a document a label Lnp ?
{new, old}.Since the correct labeling depends on the coref-erence relationships between the NPs, we needsome way to guess at this; we take all NPs withthe same head to be coreferent, as in the non-coreference version of (Barzilay and Lapata, 2005)2.We then take the probability of a document as?np:NPs P (Lnp|np).We must make several small changes to the modelto adapt it to this setting.
For the discourse-new clas-sification task, the model?s most important featureis whether the head word of the NP to be classifiedhas occurred previously (as in Ng and Cardie (2002)and Vieira and Poesio (2000)).
For coherence mod-eling, we must remove this feature, since it dependson document order, which is precisely what we aretrying to predict.
The coreference heuristic will alsofail to resolve any pronouns, so we discard them.Another issue is that NPs whose referents arefamiliar tend to resemble discourse-old NPs, eventhough they have not been previously mentioned(Fraurud, 1990).
These include unique objects likethe FBI or generic ones like danger or percent.
To1Poesio et al (2005) score 90.2%, but on a different corpus.2Unfortunately, this represents a substantial sacrifice; asPoesio and Vieira (1998) show, only about 2/3 of definite de-scriptions which are anaphoric have the same head as their an-tecedent.avoid using these deceptive phrases as examples ofdiscourse-newness, we attempt to heuristically re-move them from the training set by discarding anyNP whose head occurs only once in the document3.The labels we apply to NPs in our test data aresystematically biased by the ?same head?
heuristicwe use for coreference.
This is a disadvantage forour system, but it has a corresponding advantage?we can use training data labeled using the sameheuristic, without any loss in performance on thecoherence task.
NPs we fail to learn about duringtraining are likely to be mislabeled at test time any-way, so performance does not degrade by much.
Tocounter this slight degradation, we can use a muchlarger training corpus, since we no longer requiregold-standard coreference annotations.3 Pronoun Coreference ModelPronoun coreference is another important aspect ofcoherence?
if a pronoun is used too far away fromany natural referent, it becomes hard to interpret,creating confusion.
Too many referents, however,create ambiguity.
To describe this type of restriction,we must model the probability of the text containingpronouns (denoted ri), jointly with their referentsai.
(This takes more work than simply resolving thepronouns conditioned on the text.)
The model of Geet al (1998) provides the requisite probabilities:P (ai, ri|ai?1i ) =P (ai|h(ai), m(ai))Pgen(ai, ri)Pnum(ai, ri)Here h(a) is the Hobbs distance (Hobbs, 1976),which measures distance between a pronoun andprospective antecedent, taking into account variousfactors, such as syntactic constraints on pronouns.m(a) is the number of times the antecedent hasbeen mentioned previously in the document (againusing ?same head?
coreference for full NPs, butalso counting the previous antecedents ai?1i ).
Pgenand Pnum are distributions over gender and num-ber given words.
The model is trained using a smallhand-annotated corpus first used in Ge et al (1998).3Bean and Riloff (1999) and Uryupina (2003) constructquite accurate classifiers to detect unique NPs.
However, somepreliminary experiments convinced us that our heuristic methodworked well enough for the purpose.42Disc.
Acc Disc.
F Ins.Random 50.00 50.00 12.58Entity Grid 76.17 77.55 19.57Disc-New 70.35 73.47 16.27Pronoun 55.77 62.27 13.95EGrid+Disc-New 78.88 80.31 21.93Combined 79.60 81.02 22.98Table 1: Results on 1004 WSJ documents.Finding the probability of a document using thismodel requires us to sum out the antecedents a. Un-fortunately, because each ai is conditioned on theprevious ones, this cannot be done efficiently.
In-stead, we use a greedy search, assigning each pro-noun left to right.
Finally we report the probabilityof the resulting sequence of pronoun assignments.4 Baseline ModelAs a baseline, we adopt the entity grid (Lapata andBarzilay, 2005).
This model outperforms a varietyof word overlap and semantic similarity models, andis used as a component in the state-of-the-art systemof Soricut and Marcu (2006).
The entity grid rep-resents each entity by tracking the syntactic roles inwhich it appears throughout the document.
The in-ternal syntax of the various referring expressions isignored.
Since it also uses the ?same head?
corefer-ence heuristic, it also disregards pronouns.Since the three models use very different featuresets, we combine them by assuming independenceand multiplying the probabilities.5 ExperimentsWe evaluate our models using two tasks, both basedon the assumption that a human-authored documentis coherent, and uses the best possible ordering ofits sentences (see Lapata (2006)).
In the discrimina-tion task (Barzilay and Lapata, 2005), a documentis compared with a random permutation of its sen-tences, and we score the system correct if it indicatesthe original as more coherent4.4Since the model might refuse to make a decision by scor-ing a permutation the same as the original, we also reportF-score, where precision is correct/decisions and recall iscorrect/total.Discrimination becomes easier for longer docu-ments, since a random permutation is likely to bemuch less similar to the original.
Therefore we alsotest our systems on the task of insertion (Chen et al,2007), in which we remove a sentence from a doc-ument, then find the point of insertion which yieldsthe highest coherence score.
The reported score isthe average fraction of sentences per document rein-serted in their original position (averaged over doc-uments, not sentences, so that longer documents donot disproportionally influence the results)5.We test on sections 14-24 of the Penn Treebank(1004 documents total).
Previous work has fo-cused on the AIRPLANE corpus (Barzilay and Lee,2004), which contains short announcements of air-plane crashes written by and for domain experts.These texts use a very constrained style, with fewdiscourse-new markers or pronouns, and so our sys-tem is ineffective; the WSJ corpus is much moretypical of normal informative writing.
Also unlikeprevious work, we do not test the task of completelyreconstructing a document?s order, since this is com-putationally intractable and results on WSJ docu-ments6 would likely be dominated by search errors.Our results are shown in table 5.
When run alone,the entity grid outperforms either of our models.However, all three models are significantly betterthan random.
Combining all three models raises dis-crimination performance by 3.5% over the baselineand insertion by 3.4%.
Even the weakest compo-nent, pronouns, contributes to the joint model; whenit is left out, the resulting EGrid + Disc-New modelis significantly worse than the full combination.
Wetest significance using Wilcoxon?s signed-rank test;all results are significant with p < .001.6 ConclusionsThe use of these coreference-inspired models leadsto significant improvements in the baseline.
Of thetwo, the discourse-new detector is by far more ef-fective.
The pronoun model?s main problem is that,although a pronoun may have been displaced fromits original position, it can often find another seem-ingly acceptable referent nearby.
Despite this issue5Although we designed a metric that distinguishes nearmisses from random performance, it is very well correlated withexact precision, so, for simplicity?s sake, we omit it.6Average 22 sentences, as opposed to 11.5 for AIRPLANE.43it performs significantly better than chance and iscapable of slightly improving the combined model.Both of these models are very different from the lex-ical and entity-based models currently used for thistask (Soricut and Marcu, 2006), and are probablycapable of improving the state of the art.As mentioned, Barzilay and Lapata (2005) uses acoreference system to attempt to improve the entitygrid, but with mixed results.
Their method of com-bination is quite different from ours; they use thesystem?s judgements to define the ?entities?
whoserepetitions the system measures7.
In contrast, we donot attempt to use any proposed coreference links;as Barzilay and Lapata (2005) point out, these linksare often erroneous because the disorded input textis so dissimilar to the training data.
Instead we ex-ploit our models?
ability to measure the probabilityof various aspects of the text.AcknowledgementsChen and Barzilay, reviewers, DARPA, et alReferencesRegina Barzilay and Mirella Lapata.
2005.
Modelinglocal coherence: an entity-based approach.
In ACL2005.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In HLT-NAACL2004, pages 113?120.Regina Barzilay, Noemie Elhadad, and Kathleen McKe-own.
2002.
Inferring strategies for sentence orderingin multidocument news summarization.
Journal of Ar-tificial Intelligence Results (JAIR), 17:35?55.David L. Bean and Ellen Riloff.
1999.
Corpus-basedidentification of non-anaphoric noun phrases.
InACL?99, pages 373?380.Erdong Chen, Benjamin Snyder, and Regina Barzilay.2007.
Incremental text structuring with online hier-archical ranking.
In Proceedings of EMNLP.Hal Daume?
III.
2004.
Notes on CG and LM-BFGSoptimization of logistic regressio n. Paper availableat http://pub.hal3.name#daume04cg-bfgs, implemen-tation available at http://hal3.name/megam/, August.Peter Foltz, Walter Kintsch, and Thomas Landauer.1998.
The measurement of textual coherence with7We attempted this method for pronouns using our model,but found it ineffective.latent semantic analysis.
Discourse Processes,25(2&3):285?307.Kari Fraurud.
1990.
Definiteness and the processing ofnoun phrases in natural discourse.
Journal of Seman-tics, 7(4):395?433.Niyu Ge, John Hale, and Eugene Charniak.
1998.
A sta-tistical approach to anaphora resolution.
In Proceed-ings of the Sixth Workshop on Very Large Corpora,pages 161?171.Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.John A. Hawkins.
1978.
Definiteness and indefinite-ness: a study in reference and grammaticality predic-tion.
Croom Helm Ltd.Jerry R. Hobbs.
1976.
Pronoun resolution.
TechnicalReport 76-1, City College New York.Mirella Lapata and Regina Barzilay.
2005.
Automaticevaluation of text coherence: Models and representa-tions.
In IJCAI, pages 1085?1090.Mirella Lapata.
2006.
Automatic evaluation of informa-tion ordering: Kendall?s tau.
Computational Linguis-tics, 32(4):1?14.E.
Miltsakaki and K. Kukich.
2004.
Evaluation of textcoherence for electronic essay scoring systems.
Nat.Lang.
Eng., 10(1):25?55.Ani Nenkova and Kathleen McKeown.
2003.
Refer-ences to named entities: a corpus study.
In NAACL?03, pages 70?72.Vincent Ng and Claire Cardie.
2002.
Identifyinganaphoric and non-anaphoric noun phrases to improvecoreference resolution.
In COLING.Massimo Poesio and Renata Vieira.
1998.
A corpus-based investigation of definite description use.
Com-putational Linguistics, 24(2):183?216.Massimo Poesio, Mijail Alexandrov-Kabadjov, RenataVieira, Rodrigo Goulart, and Olga Uryupina.
2005.Does discourse-new detection help definite descriptionresolution?
In Proceedings of the Sixth InternationalWorkshop on Computational Semantics, Tillburg.Ellen Prince.
1981.
Toward a taxonomy of given-new in-formation.
In Peter Cole, editor, Radical Pragmatics,pages 223?255.
Academic Press, New York.Radu Soricut and Daniel Marcu.
2006.
Discourse gener-ation using utility-trained coherence models.
In ACL-2006.Olga Uryupina.
2003.
High-precision identification ofdiscourse new and unique noun phrases.
In Proceed-ings of the ACL Student Workshop, Sapporo.Renata Vieira and Massimo Poesio.
2000.
Anempirically-based system for processing definite de-scriptions.
Computational Linguistics, 26(4):539?593.44
