Entropy Rate Constancy in TextDmitriy Genzel and Eugene CharniakBrown Laboratory for Linguistic Information ProcessingDepartment of Computer ScienceBrown UniversityProvidence, RI, USA, 02912fdg,ecg@cs.brown.eduAbstractWe present a constancy rate princi-ple governing language generation.
Weshow that this principle implies that lo-cal measures of entropy (ignoring con-text) should increase with the sentencenumber.
We demonstrate that this isindeed the case by measuring entropyin three dierent ways.
We also showthat this eect has both lexical (whichwords are used) and non-lexical (howthe words are used) causes.1 IntroductionIt is well-known from Information Theory thatthe most ecient way to send informationthrough noisy channels is at a constant rate.
Ifhumans try to communicate in the most ecientway, then they must obey this principle.
Thecommunication medium we examine in this pa-per is text, and we present some evidence thatthis principle holds here.Entropy is a measure of information rst pro-posed by Shannon (1948).
Informally, entropyof a random variable is proportional to the di-culty of correctly guessing the value of this vari-able (when the distribution is known).
Entropyis the highest when all values are equally prob-able, and is lowest (equal to 0) when one of thechoices has probability of 1, i.e.
deterministi-cally known in advance.In this paper we are concerned with entropyof English as exhibited through written text,though these results can easily be extended tospeech as well.
The random variable we dealwith is therefore a unit of text (a word, for ourpurposes1) that a random person who has pro-duced all the previous words in the text streamis likely to produce next.
We have as many ran-dom variables as we have words in a text.
Thedistributions of these variables are obviously dif-ferent and depend on all previous words pro-duced.
We claim, however, that the entropy ofthese random variables is on average the same2.2 Related WorkThere has been work in the speech communityinspired by this constancy rate principle.
Inspeech, distortion of the audio signal is an extrasource of uncertainty, and this principle can byapplied in the following way:A given word in one speech context might becommon, while in another context it might berare.
To keep the entropy rate constant overtime, it would be necessary to take more time(i.e., pronounce more carefully) in less commonsituations.
Aylett (1999) shows that this is in-deed the case.It has also been suggested that the principleof constant entropy rate agrees with biologicalevidence of how human language processing hasevolved (Plotkin and Nowak, 2000).Kontoyiannis (1996) also reports results on 5consecutive blocks of characters from the works1It may seem like an arbitrary choice, but a word is anatural unit of length, after all when one is asked to givethe length of an essay one typically chooses the numberof words as a measure.2Strictly speaking, we want the cross-entropy betweenall words in the sentences number n and the true modelof English to be the same for all n.Computational Linguistics (ACL), Philadelphia, July 2002, pp.
199-206.Proceedings of the 40th Annual Meeting of the Association forof Jane Austen which are in agreement with ourprinciple and, in particular, with its corollary asderived in the following section.3 Problem FormulationLet fXig, i = 1 .
.
.
n be a sequence of randomvariables, with Xi corresponding to word wi inthe corpus.
Let us consider i to be xed.
Therandom variable we are interested in is Yi, a ran-dom variable that has the same distribution asXijX1 = w1, .
.
.
,Xi?1 = wi?1 for some xedwords w1.
.
.
wi?1.
For each word wi there willbe some word wj , (j  i) which is the start-ing word of the sentence wi belongs to.
We willcombine random variables X1.
.
.
Xi?1 into twosets.
The rst, which we call Ci (for context),contains X1through Xj?1, i.e.
all the wordsfrom the preceding sentences.
The remainingset, which we call Li (for local), will containwords Xj through Xi?1 .
Both Li and Ci couldbe empty sets.
We can now write our variableYi as XijCi, Li.Our claim is that the entropy of Yi , H(Yi)stays constant for all i.
By the denition of rel-ative mutual information between Xi and Ci,H(Yi) = H(XijCi, Li)= H(XijLi) ?
I(XijCi, Li)where the last term is the mutual informationbetween the word and context given the sen-tence.
As i increases, so does the set Ci.
Li, onthe other hand, increases until we reach the endof the sentence, and then becomes small again.Intuitively, we expect the mutual informationat, say, word k of each sentence (where Li hasthe same size for all i) to increase as the sen-tence number is increasing.
By our hypothesiswe then expect H(XijLi) to increase with thesentence number as well.Current techniques are not very good at es-timating H(Yi), because we do not have avery good model of context, since this modelmust be mostly semantic in nature.
We haveshown, however, that if we can instead estimateH(XijLi) and show that it increases with thesentence number, we will provide evidence tosupport the constancy rate principle.The latter expression is much easier to esti-mate, because it involves only words from thebeginning of the sentence whose relationshipis largely local and can be successfully cap-tured through something as simple as an n-grammodel.We are only interested in the mean value ofthe H(Xj jLj) for wj 2 Si, where Si is the ithsentence.
This number is equal to 1jSijH(Si),which reduces the problem to the one of esti-mating the entropy of a sentence.We use three dierent ways to estimate theentropy: Estimate H(Si) using an n-gram probabilis-tic model Estimate H(Si) using a probabilistic modelinduced by a statistical parser Estimate H(Xi) directly, using a non-para-metric estimator.
We estimate the entropyfor the beginning of each sentence.
Thisapproach estimates H(Xi), not H(XijLi),i.e.
ignores not only the context, but alsothe local syntactic information.4 Results4.1 N-gramN-gram models make the simplifying assump-tion that the current word depends on a con-stant number of the preceding words (we usethree).
The probability model for sentence Sthus looks as follows:P (S) = P (w1)P (w2jw1)P (w3jw2w1)n?i=4P (wnjwn?1wn?2wn?3)To estimate the entropy of the sentence S, wecompute log P (S).
This is in fact an estimate ofcross entropy between our model and true distri-bution.
Thus we are overestimating the entropy,but if we assume that the overestimation error ismore or less uniform, we should still see our esti-mate increase as the sentence number increases.Penn Treebank corpus (Marcus et al, 1993)sections 0-20 were used for training, sections 21-24 for testing.
Each article was treated as a sep-arate text, results for each sentence number weregrouped together, and the mean value reportedon Figure 1 (dashed line).
Since most articlesare short, there are fewer sentences available forlarger sentence numbers, thus results for largesentence numbers are less reliable.The trend is fairly obvious, especially forsmall sentence numbers: sentences (with no con-text used) get harder as sentence number in-creases, i.e.
the probability of the sentence giventhe model decreases.4.2 Parser ModelWe also computed the log-likelihood of the sen-tence using a statistical parser described inCharniak (2001)3.
The probability model forsentence S with parse tree T is (roughly):P (S) =?x2TP (xjparents(x))where parents(x) are words which are parentsof node x in the the tree T .
This model takesinto account syntactic information present inthe sentence which the previous model does not.The entropy estimate is again log P (S).
Overall,these estimates are lower (closer to the true en-tropy) in this model because the model is closerto the true probability distribution.
The samecorpus, training and testing sets were used.
Theresults are reported on Figure 1 (solid line).
Theestimates are lower (better), but follow the sametrend as the n-gram estimates.4.3 Non-parametric EstimatorFinally we compute the entropy using the esti-mator described in (Kontoyiannis et al, 1998).The estimation is done as follows.
Let T be ourtraining corpus.
Let S = fw1.
.
.
wng be the testsentence.
We nd the largest k  n, such thatsequence of words w1.
.
.
wk occurs in T .
Thenlog Sk is an estimate of the entropy at the wordw1.
We compute such estimates for many rstsentences, second sentences, etc., and take theaverage.3This parser does not proceed in a strictly left-to-rightfashion, but this is not very important since we estimateentropy for the whole sentence, rather than individualwordsFor this experiment we used 3 million words ofthe Wall Street Journal (year 1988) as the train-ing set and 23 million words (full year 1987) asthe testing set4.
The results are shown on Fig-ure 2.
They demonstrate the expected behavior,except for the strong abnormality on the secondsentence.
This abnormality is probably corpus-specic. For example, 1.5% of the second sen-tences in this corpus start with words \the termswere not disclosed", which makes such sentenceseasy to predict and decreases entropy.4.4 Causes of Entropy IncreaseWe have shown that the entropy of a sentence(taken without context) tends to increase withthe sentence number.
We now examine thecauses of this eect.These causes may be split into two categories:lexical (which words are used) and non-lexical(how the words are used).
If the eects are en-tirely lexical, we would expect the per-word en-tropy of the closed-class words not to increasewith sentence number, since presumably thesame set of words gets used in each sentence.For this experiment we use our n-gram estima-tor as described in Section 4.2.
We evaluatethe per-word entropy for nouns, verbs, deter-miners, and prepositions.
The results are givenin Figure 3 (solid lines).
The results indicatethat entropy of the closed class words increaseswith sentence number, which presumably meansthat non-lexical eects (e.g.
usage) are present.We also want to check for presence of lexicaleects.
It has been shown by Kuhn and Mohri(1990) that lexical eects can be easily capturedby caching.
In its simplest form, caching in-volves keeping track of words occurring in theprevious sentences and assigning for each wordw a caching probability Pc(w) =C(w)?wC(w), whereC(w) is the number of times w occurs in theprevious sentences.
This probability is thenmixed with the regular probability (in our case- smoothed trigram) as follows:Pmixed(w) = (1 ?
?
)Pngram(w) + ?Pc(w)4This is not the same training set as the one used intwo previous experiments.
For this experiment we neededa larger, but similar data set0 5 10 15 20 256.877.27.47.67.888.28.4sentence numberentropyestimateparsern?gramFigure 1: N-gram and parser estimates of entropy (in bits per word)0 5 10 15 20 2588.18.28.38.48.58.68.78.88.99sentence numberentropyestimateFigure 2: Non-parametric estimate of entropywhere ?
was picked to be 0.1.
This new prob-ability model is known to have lower entropy.More complex caching techniques are possible(Goodman, 2001), but are not necessary for thisexperiment.Thus, if lexical eects are present, we expectthe model that uses caching to provide lowerentropy estimates.
The results are given in Fig-ure 3 (dashed lines).
We can see that cachinggives a signicant improvement for nouns and asmall one for verbs, and gives no improvementfor the closed-class parts of speech.
This showsthat lexical eects are present for the open-classparts of speech and (as we assumed in the previ-ous experiment) are absent for the closed-classparts of speech.
Since we have proven the pres-ence of the non-lexical eects in the previousexperiment, we can see that both lexical andnon-lexical eects are present.5 Conclusion and Future WorkWe have proposed a fundamental principle oflanguage generation, namely the entropy rateconstancy principle.
We have shown that en-tropy of the sentences taken without context in-creases with the sentence number, which is inagreement with the above principle.
We havealso examined the causes of this increase andshown that they are both lexical (primarily foropen-class parts of speech) and non-lexical.These results are interesting in their ownright, and may have practical implications aswell.
In particular, they suggest that languagemodeling may be a fruitful way to approach is-sues of contextual influence in text.Of course, to some degree language-modelingcaching work has always recognized this, butthis is rather a crude use of context and doesnot address the issues which one normally thinksof when talking about context.
We have seen,however, that entropy measurements can pickup much more subtle influences, as evidencedby the results for determiners and prepositionswhere we see no caching influence at all, but nev-ertheless observe increasing entropy as a func-tion of sentence number.
This suggests thatsuch measurements may be able to pick up moreobviously semantic contextual influences thansimply the repeating words captured by cachingmodels.
For example, sentences will dier inhow much useful contextual information theycarry.
Are there useful generalizations to bemade?
E.g., might the previous sentence alwaysbe the most useful, or, perhaps, for newspa-per articles, the rst sentence?
Can these mea-surements detect such already established con-textual relations as the given-new distinction?What about other pragmatic relations?
All ofthese deserve further study.6 AcknowledgmentsWe would like to acknowledge the members ofthe Brown Laboratory for Linguistic Informa-tion Processing and particularly Mark Johnsonfor many useful discussions.
Also thanks toDaniel Jurafsky who early on suggested the in-terpretation of our data that we present here.This research has been supported in part byNSF grants IIS 0085940, IIS 0112435, and DGE9870676.ReferencesM.
P. Aylett.
1999.
Stochastic suprasegmentals: Re-lationships between redundancy, prosodic struc-ture and syllabic duration.
In Proceedings ofICPhS?99, San Francisco.E.
Charniak.
2001.
A maximum-entropy-inspiredparser.
In Proceedings of ACL?2001, Toulouse.J.
T. Goodman.
2001.
A bit of progress in lan-guage modeling.
Computer Speech and Language,15:403{434.I.
Kontoyiannis, P. H. Algoet, Yu.
M. Suhov, andA.J.
Wyner.
1998.
Nonparametric entropy esti-mation for stationary processes and random elds,with applications to English text.
IEEE Trans.Inform.
Theory, 44:1319{1327, May.I.
Kontoyiannis.
1996.
The complexity and en-tropy of literary styles.
NSF Technical Report No.97, Department of Statistics, Stanford University,June.
[unpublished, can be found at the author?sweb page].R.
Kuhn and R. De Mori.
1990.
A cache-basednatural language model for speech reproduction.IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 12(6):570{583.2 4 6 8 1088.599.5Nounsnormalcaching2 4 6 8 109.51010.511Verbsnormalcaching2 4 6 8 104.64.855.25.4Prepositionsnormalcaching2 4 6 8 103.73.83.944.14.24.34.4DeterminersnormalcachingFigure 3: Comparing Parts of SpeechM.
P. Marcus, B. Santorini, and M. A. Marcin-kiewicz.
1993.
Building a large annotated cor-pus of English: the Penn treebank.
ComputationalLinguistics, 19:313{330.J.
B. Plotkin and M. A. Nowak.
2000.
Languageevolution and information theory.
Journal of The-oretical Biology, pages 147{159.C.
E. Shannon.
1948.
A mathematical theory ofcommunication.
The Bell System Technical Jour-nal, 27:379{423, 623{656, July, October.
