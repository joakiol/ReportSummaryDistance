Applying Explanation-based Learning to Control and Speeding-upNatural Language GenerationGiinter NeumannDFKI GmbHStuh lsatzenhausweg 366123 Saarbr i icken,  Germanyneumann@df k i. uni- sb.
deAbstractThis paper presents a method for the au-tomatic extraction of subgrammars to con-trol and speeding-up natural anguage gen-eration NLG.
The method is based onexplanation-based l arning EBL.
The mainadvantage for the proposed new methodfor NLG is that the complexity of thegrammatical decision making process dur-ing NLG can be vastly reduced, becausethe EBL method supports the adaption ofa NLG system to a particular use of a lan-guage.1 In t roduct ionIn recent years, a Machine Learning tech-nique known as Explanation-based Learning EBL(Mitchell, Keller, and Kedar-Cabelli, 1986; vanHarmelen and Bundy, 1988; Minton et al, 1989) hassuccessfully been applied to control and speeding-upnatural anguage parsing (Rayner, 1988; Samuelssonand Rayner, 1991; Neumann, 1994a; Samuelsson,1994; Srinivas and Joshi, 1995; Rayner and Carter,1996).
The core idea of EBL is to transform thederivations (or explanations) computed by a prob-lem solver (e.g., a parser) to some generalized andcompact forms, which can be used very efficientlyfor solving similar problems in the future.
EBL hasprimarily been used for parsing to automatically spe-cialize a given source grammar to a specific domain.In that case, EBL is used as a method for adapting ageneral grammar and/or parser to the sub-languagedefined by a suitable training corpus (Rayner andCarter, 1996).A specialized grammar can be seen as describ-ing a domain-specific set of prototypical construc-tions.
Therefore, the EBL approach is also veryinteresting for natural language generation (NLG).Informally, NLG is the production of a naturallanguage text from computer-internal representa-tion of information, where NLG can be seen asa complex--potentially cascaded--decision makingprocess.
Commonly, a NLG system is decomposedinto two major components, viz.
the strategic om-ponent which decides 'what to say' and the tacti-cal component which decides 'how to say' the resultof the strategic omponent.
The input of the tacti-cal component is basically a semantic representationcomputed by the strategic omponent.
Using a lexi-con and a grammar, its main task is the computationof potentially all possible strings associated with asemantic input.
Now, in the same sense as EBL isused in parsing as a means to control the range ofpossible strings as well as their degree of ambigu-ity, it can also be used for the tactical componentto control the range of possible semantic input andtheir degree of paraphrases.In this paper, we present a novel method for theautomatic extraction of subgrammars for the controland speeding-up of natural anguage generation.
Itsmain advantage for NLG is that the complexity ofthe (linguistically oriented) decision making processduring natural anguage generation can be vastly re-duced, because the EBL method supports adaptionof a NLG system to a particular language use.
Thecore properties of this new method are:?
prototypical occuring grammatical construc-tions can automatically be extracted;?
generation of these constructions i vastly spedup using simple but efficient mechanisms;the new method supports partial matching, inthe sense that new semantic input need not becompletely covered by previously trained exam-ples;?
it can easily be integrated with recently de-veloped chart-based generators as described in,214e.g., (Neumann, 1994b; Kay, 1996; Shemtov,1996).The method has been completely implementedand tested With a broad-coverage HPSG-basedgrammar for English (see sec.
5 for more details).2 Foundat ionsThe main focus of this paper is tactical generation,i.e., the mapping of structures (usually represent-ing semantic information eventually decorated withsome functional features) to strings using a lexiconand a grammar.
Thus stated, we view tactical gen-eration as the inverse process of parsing.
Informally,EBL can be considered as an intelligent storage unitof example-based generalized parts of the grammat-ical search space determined via training by the tac-tical generator3 Processing of similar new input isthen reduced to simple lookup and matching oper-ations, which circumvent re-computation of this al-ready known search space.We concentrate on constraint-based grammar for-malism following a sign-based approach consider-ing linguistic objects (i.e., words and phrases) asutterance-meaning associations (Pollard and Sag,1994).
Thus viewed, a grammar is a formal state-ment of the relation between utterances in a natu-ral language and representations of their meaningsin some logical or other artificial language, wheresuch representations are usually called logical forms(Shieber, 1993).
The result of the tactical generatoris a feature structure (or a set of such structures inthe case of multiple paraphrases) containing amongothers the input logical form, the computed string,and a representation f the derivation.In our current implementation we are using TDL,a typed feature-based language and inference systemfor constraint-based grammars (Krieger and Sch~ifer,1994).
TDL allows the user to define hierarchically-ordered types consisting of type and feature con-straints.
As shown later, a systematic use of typeinformation leads to a very compact representationof the extracted ata and supports an elegant butefficient generalization step.We are adapting a "flat" representation of log-ical forms as described in (Kay, 1996; Copestakeet al, 1996).
This is a minimally structured, butdescriptively adequate means to represent seman-tic information, which allows for various types ofunder-/overspecification, facilitates generation andthe specification of semantic transfer equivalencesl In case a reversible grammar is used the parser caneven be used for processing the training corpus.used for machine translation (Copestake t al., 1996;Shemtov, 1996).
2Informally, a flat representation is obtained bythe use of extra variables which explicitly repre-sent the relationship between the entities of a logicalform and scope information.
In our current systemwe are using the framework called minimal recur-sion semantics (MRS) described in (Copestake tal., 1996).
Using their typed feature structure nota-tion figure 1 displays a possible MRS of the string"Sandy gives a chair to Kim" (abbreviated whereconvenient).The value of the feature LISZT is actually treatedlike a set, i.e., the relative order of the elements isimmaterial.
The feature HANDEL is used to repre-sent scope information, and INDEX plays much thesame role as a lambda variable in conventional rep-resentations (for more details see (Copestake t al.,1996)).3 Overv iew o f  the  methoda In~tc80P  .s .
I ~BI ,t~ f-" : I :I !
: ' "1  g=,~m~.
l  L :  P" - - "g," , : i  o .........
I, gene_ , - l l  t ze  I ' ,, ', V~ndex~'esulCsFigure 3: A blueprint of the architecture.The above figure displays the overall architectureof the EBL learning method.
The right-hand partof the diagram shows the linguistic competence base(LCB) and the left the EBL-based subgrammar p o-cessing component (SGP).LCB corresponds to the tactical component of ageneral natural anguage generation system NLG.
Inthis paper we assume that the strategic omponentof the NLG has already computed the MRS repre-sentation of the information of an underlying com-puter program.
SGP consists of a training moduleTM, an application module AM, and the subgram-2But note, our approach does not depend on a flatrepresentation f logical forms.
However, in the caseof conventional representation form, the mechanisms forindexing the trained structures would require more com-plex abstract data types (see sec.
4 for more details).215"HANDEL hlINDEX e2LISZT\[.ANDEL hl\] \]/EVEN~ ez \[RANDEL IHANDEL hi \[ACT x5SandyRel LINST ~5 , |PREPARG x6 ' TempOver \[EVENT e2 ,GiveRel LUND x7\[HANDEL hl2\] \[.ANDEL hIJ \] \  \[HANDEL hlO\], |ARG v13| z6 JChairRel L INST x7 J \[PREP x6 J ' KimRel L INST / ToSomeIHANDEL h9 \]BV x7RESTR hlO\[.SCOPE h11JFigure 1: The MRS of the string "Sandy gives a chair to Kim"LISZT (SandyRel \[HANDEL h4 \], GiveRel \[HANDEL hl\], TempOver \[HANDEL hl\], Some \[HANDEL h9\], \]ChairReI\[HANDEL hlO\], To\[HANDEL h12\], KimRel\[HANDEL hi, I )  JFigure 2: The generalized MRS of the string "Sandy gives a chair to Kim"mar, automatically determined by TM and appliedby AM.Briefly, the flow of control is as follows: Duringthe training phase of the system, a new logical formmrs is given as input to the LCB.
After grammaticalprocessing, the resulting feature structure fs(mrs)(i.e., a feature structure that contains among othersthe input MRS, the computed string and a repre-sentation of the derivation tree) is passed to TM.TM extracts and generalizes the derivation tree offs(mrs), which we call the template tempi(mrs)of fs(mrs), tempi(mrs) is then stored in a deci-sion tree, where indices are computed from the MRSfound under the root of tempi(mrs).
During the ap-plication phase, a new semantic input mrs t is usedfor the retrieval of the decision tree.
If a candidatetemplate can be found and successfully instantiated,the resulting feature structure fs (mrd)  constitutesthe generation result of mrs ~.Thus described, the approach seems to facilitateonly exact retrieval and matching of a new seman-tic input.
However, before we describe how partialmatching is realized, we will demonstrate in more de-tail the exact matching strategy using the exampleMRS shown in figure 1.Tra in ing phase The training module TM startsright after the resulting feature structure fs  for theinput MRS mrs has been computed.
In the firstphase, TM extracts and generalizes the derivationtree of fs, called the template of fs.
Each node ofthe template contains the rule name used in the cor-responding derivation step and a generalization ofthe local MRS. A generalized MRS is the abstrac-tion of the LISZT value of a MRS where each elementonly contains the (lexical semantic) type and HAN-DEL information (the HANDEL information is usedfor directing lexical choice (see below)).In our example mrs, figure 2 displays the gener-alized MRS mrsg.
For convenience, we will use themore compact notation:{(SandyRel h4), (Giveael hl),(TempOver hl), (Some h9),(ChairRel hl0), (To h12), (KimRel h14)}Using this notation, figure 4 (see next page) dis-plays the template tempi(mrs) obtained from fs.Note that it memorizes not only the rule applicationstructure of a successful process but also the way thegrammar mutuMly relates the compositional parts ofthe input MRS.In the next step of the training module TM, thegeneralized MRS mrs~ information of the root nodeof tempi(mrs) is used for building up an index ina decision tree.
Remember that the relative orderof the elements of a MRS is immaterial.
For thatreason, the elements of mrsg are alphabetically or-dered, so that we can treat it as a sequence whenused as a new index in the decision tree.The alphabetic ordering has two advantages.Firstly, we can store different templates under acommon prefix, which allows for efficient storage andretrieval.
Secondly, it allows for a simple efficienttreatment of MRS as sets during the retrieval phaseof the application phase.216Sub jhDI (SandyRel h4), (GiveRel h I ), (TempOver hI),(S~une hg).
(ChairRel hi0).
(Tt) h 12), (KimRel h 14)ProperLe HCompNc((SandyRel h4)} {(GiveRel hi), (TempOver hi)(Some hg), (ChairRel hlO).
(T~ h 12), (KimRel hi4)}~ ~ ~ D e t N\[(Ti) hi2), (KimRel hi4)} HCompNc{(GiveRel h I ), (TempOver hI ),(St)me hg), (ChairRel h 10)}P repNoModLe  ProperLe\[ (T<) h 12 ) } { (Ki mRel h 14 ) }MvTo+DitransLe DetN{ (GiveRel h I ).
{ (S()me ht)),(Tem pOve~ h 1 ) } (ChairRel h I(1) \]DetSgLe I n t rNLe{ (Some hg)) { (ChairRel h 11)) }Figure 4: The template tempi(mrs).
Rule namesare in bold.Appl icat ion phase The application module AMbasically performs the following steps:1.
Retrievah For a new MRS mrs' we first con-struct the alphabetically sorted generalized MRSmrsg.
mr% is then used as a path descriptionfor traversing the decision tree.
For reasons wewill explain soon, traversal is directed by type!subsumption.
Traversal is successful if mrsg hasbeen completely processed and if the end nodein the decision tree contains a template.
Notethat because of the alphabetic ordering, the rel-ative order of the elements of new input mrs ~ isimmaterial.2.
Expansion: A successfully retrieved templatetempl is expanded by deterministically applyingthe rules denoted by the non-terminal elementsfrom the top downwards in the order specifiedby tempi.
In some sense, expansion just re-playsthe derivation obtained in the past.
This willresult in a grammatically fully expanded fea-ture structure, where only lexical specific infor-mation is still missing.
But note that throughstructure sharing the terminal elements will al-ready be constrained by syntactic information.
33It is possible to perform the expansion step off-lineas early as the training phase, in which case the applica-tion phase can be sped up, however at the price of morememory being taken up.3.
Lexical lookup: From each terminal element ofthe unexpanded template templ the type andHANDEL information is used to select the cor-responding element from the input MRS mrs'(note that in general the MRS elements of themrs' are much more constrained than their cor-responding elements in the generalized MRSmrs'g).
The chosen input MRS element is thenused for performing lexical lookup, where lexi-cal elements are indexed by their relation name.In general this will lead to a set of lexical can-didates.4.
Lexical instantiation: In the last step of the ap-plication phase, the set of selected lexical el-ements is unified with the constraints of theterminal elements in the order specified by theterminal yield.
We also call this step terminal-matching.
In our current system terminal-matching is performed from left to right.
Sincethe ordering of the terminal yield is given by thetemplate, it is also possible to follow other se-lection strategies, e.g., a semantic head-drivenstrategy, which could lead to more efficientterminal-matching, because the head element issupposed to provide selectional restriction in-formation for its dependents.A template together with its corresponding indexdescribes all sentences of the language that sharethe same derivation and whose MRS are consistentwith that of the index.
Furthermore, the index andthe MRS of a template together define a normaliza-tion for the permutation of the elements of a newinput MRS.
The proposed EBL method guaranteessoundness because retaining and applying the orig-inal derivation in a template nforces the full con-straints of the original grammar.Achieving more general i ty So far, the applica-tion phase will only be able to re-use templates fora semantic input which has the same semantic typeinformation.
However, it is possible to achieve moregenerality, if we apply a further abstraction step ona generalized MRS.
This is simply achieved by se-lecting a supertype of a MRS element instead of thegiven specialized type.The type abstraction step is based on the stan-dard assumption that the word-specific lexical se-mantic types can be grouped into classes represent-ing morpho-syntactic paradigms.
These classes de-fine the upper bounds for the abstraction process.
Inour current system, these upper bounds are directlyused as the supertypes to be considered uring thetype abstraction step.
More precisely, for each el-ement x of a generalized MRS mrsg it is checked217whether its type Tx is subsumed by an upper boundT, (we assume disjoint sets).
Only if this is the case,Ts replaces Tx in mrsg.4 Applying this type abstrac-tion strategy on the MRS of figure 1, we obtain:{(Named h4), (ActUndPrep hl),(TempOver hl), (Some h9),(RegNom hl0), (To h12), (Named h14)}SubjhD{ (Named h4).
(ActUndPrep h1), (TempOver h I),(Some h9).
(RegNom hi0), (To hi2).
(Named hi4)}ProperLe HCompNc{ (Named h4) } { (ActUndPmp hI), (TempOver h l)(Some h9), (RegNom hi0), (To hi2), (Named h)4)}HCompNc / ~ 1  (To h 12), (Name h 14) }{ (ActUndPrep h l ), (TempOver h 1), / ~(Some h9), (RegNom h 10)} / \PrepNoModLe ProperLe{(To hi2)} {(Name hi4)}MvTo+DitransLe DetN{ (ActUndPrep h 1).
{ (Some h9),(TempOver h I )} (RegNom h 10)\]DetSgLe IntrNLe{(Some h9)} ((RegNom hi0)}Figure 5: The more generalized erivation tree dtgof dt.where e.g., NAMED is the common supertype ofSANDYREL and KIMREL, and ACTUNDPREP is thesupertype of GIVEREL.
Figure 5 shows the tem-plate templg obtained from fs  using the more gen-eral MRS information.
Note, that the MRS of theroot node is used for building up an index in thedecision tree.Now, if retrieval of the decision tree is directedby type subsumption, the same template can be re-trieved and potentially instantiated for a wider rangeof new MRS input, namely for those which are typecompatible wrt.
subsumption relation.
Thus, thetemplate templ 9 can now be used to generate, e.g.,the string "Kim gives a table to Peter", as well asthe string "Noam donates a book to Peter".However, it will not be able to generate a sentencelike "A man gives a book to Kim", since the retrieval4 Of course, if a very fine-grained lexical semantic typehierarchy is defined then a more careful selection wouldbe possible to obtained ifferent degrees of type abstrac-tion and to achieve a more domain-sensitive d termina-tion of the subgrammars.
However, more complex typeabstraction strategies are then needed which would beable to find appropriate supertypes automatically.phase will already fail.
In the next section, we willshow how to overcome ven this kind of restriction.4 Partial MatchingThe core idea behind partial matching is that in casean exact match of an input MRS fails we want atleast as many subparts as possible to be instantiated.Since the instantiated template of a MRS subpartcorresponds toa phrasal sign, we also call it a phrasaltemplate.
For example, assuming that the trainingphase has only to be performed for the example infigure 1, then for the MRS of "A man gives a book toKim", a partial match would generate the strings "aman" and "gives a book to Kim".5 The instantiatedphrasal templates are then combined by the tacticalcomponent to produce larger units (if possible, seebelow).Extended tra in ing phase The training moduleis adapted as follows: Starting from a templatetempl obtained for the training example in the man-ner described above, we extract recursively all pos-sible subtrees templs also called phrasal templates.Next, each phrasal template is inserted in the deci-sion tree in the way described above.It is possible to direct the subtree xtraction pro-cess with the application of filters, which are ap-plied to the whole remaining subtree in each recur-sive step.
By using these filters it is possible to re-strict the range of structural properties of candidatephrasal templates (e.g., extract only saturated NPs,or subtrees having at least two daughters, or sub-trees which have no immediate recursive structures).These filters serve the same means as the "chunkingcriteria" described in (Rayner and Carter, 1996).During the training phase it is recognized for eachphrasal template templs whether the decision treealready contains a path pointing to a previously ex-tracted and already stored phrasal template tempi's,such that templs = templ's.
In that case, templ~ isnot inserted and the recursion stops at that branch.Extended appl icat ion phase For the applica-tion module, only the retrieval operation of the de-cision tree need be adapted.Remember that the input of the retrieval opera-tion is the sorted generalized MRS mrsg of the inputMRS mrs.
Therefore, mrsg can be handled like asequence.
The task of the retrieval operation in thecase of a partial match is now to potentially find allsubsequences of mrsg which lead to a template.5If we would allow for an exhaustive partial match(see below) then the strings '% book" and "Kim" wouldadditionally be generated.218In case of exact matching strategy, the decisiontree must be visited only once for a new input.
Inthe case of partial matching, however, the decisiontree describes only possible prefixes for a new input.Hence, we have to recursively repeat retrieval of thedecision tree as long as the remaining suffix is notempty.
In other words, the decision tree is now afinite representation f an infinite structure, becauseimplicitly, each endpoint of an index bears a pointerto the root of the decision tree.Assuming that the following template/index pairshave been inserted into the decision tree: (ab, tl),(abcd, t2), (bcd, t3).
Then retrieval using the pathabcd will return all three templates, retrieval usingaabbcd will return template tl and t3, and abc willonly return tl.6Inter leaving with normal  processing OurEBL method can easily be integrated with normalprocessing, because ach instantiated template canbe used directly as an already found sub-solution.In case of an agenda-driven chart generator of thekind described in (Neumann, 1994a; Kay, 1996), aninstantiated template can be directly added as apassive edge to the generator's agenda.
If passiveedges with a wider span are given higher prioritythan those with a smaller span, the tactical gener-ator would try to combine the largest derivationsbefore smaller ones, i.e., it would prefer those struc-tures determined by EBL.5 Imp lementat ionThe EBL method just described has been fully im-plemented and tested with a broad coverage HPSG-based English grammar including more than 2000fully specified lexical entries.
7 The TDL grammarformalism is very powerful, supporting distributeddisjunction, full negation, as well as full boolean typelogic.In our current system, an efficient chart-basedbidirectional parser is used for performing the train-ing phase.
During training, the user can interac-tively select which of the parser's readings shouldbe considered by the EBL module.
In this way theuser can control which sort of structural ambigui-ties should be avoided because they are known tocause misunderstandings.
For interleaving the EBLapplication phase with normal processing a first pro-6It is possible to parameterize our system to per-form an exhaustive or a non-exhaustive strategy.
In thenon-exhaustive mode, the longest matching prefixes axepreferred.~This grammar has been developed at CSLI, Stan-ford, and kindly be provided to the author.totype of a chart generator has been implementedusing the same grammar as used for parsing.First tests has been carried out using a small testset of 179 sentences.
Currently, a parser is used forprocessing the test set during training.
Generationof the extracted templates is performed solely bythe EBL application phase (i.e., we did not consid-ered integration of EBL and chart generation).
Theapplication phase is very efficient.
The average pro-cessing time for indexing and instantiation of a sen-tence level template (determined through parsing) ofan input MRS is approximately one second.
S Com-pared to parsing the corresponding string the factorof speed up is between 10 to 20.
A closer look tothe four basic EBL-generation steps: indexing, in-stantiation, lexical lookup, and terminal matchingshowed that the latter is the most expensive one (upto 70% of computing time).
The main reasons arethat 1.)
lexical lookup often returns several exicalreadings for an MRS element (which introduces lex-ical non-determinism) and 2.)
the lexical elementsintroduce most of the disjunctive constraints whichmakes unification very complex.
Currently, termi-nal matching is performed left to right.
However,we hope to increase the efficiency of this step by us-ing head-oriented strategies, ince this might help tore-solve disjunctive constraints as early as possible.6 D iscuss ionThe only other approach I am aware of whichalso considers EBL for NLG is (Samuelsson, 1995a;Samuelsson, 1995b).
However, he focuses on thecompilation of a logic grammar using LR-compilingtechniques, where EBL-related methods are used tooptimize the compiled LR tables, in order to avoidspurious non-determinisms during normal genera-tion.
He considers neither the extraction of a spe-cialized grammar for supporting controlled languagegeneration, nor strong integration with the normalgenerator.However, these properties are very important forachieving high applicability.
Automatic grammarextraction is worthwhile because it can be used tosupport he definition of a controlled omain-specificlanguage use on the basis of training with a gen-eral source grammar.
Furthermore, in case exactmatching is requested only the application moduleis needed for processing the subgrammar.
In caseof normal processing, our EBL method serves as aspeed-up mechanism for those structures which haveSEBL-based generation of all possible templates of aninput MRS is less than 2 seconds.
The tests have beenperformed using a Sun UltraSpaxc.219"actually been used or uttered".
However, complete-ness is preserved.We view generation systems which are based on"canned text" and linguistically-based systems im-ply as two endpoints of a contiguous scale of possiblesystem architectures ( ee also (Dale et al, 1994)).Thus viewed, our approach is directed towards theautomatic creation of application-specific generationsystems.7 Conc lus ion  and  Future  D i rec t ionsWe have presented a method of automatic extrac-tion of subgrammars for controlling and speeding upnatural anguage generation (NLG).
The method isbased on explanation-based l arning (EBL), whichhas already been successfully applied for parsing.We showed how the method can be used to traina system to a specific use of grammatical nd lexicalusage.We already have implemented a similar EBLmethod for parsing, which supports on-line learn-ing as well as statistical-based management of ex-tracted ata.
In the future we plan to combine EBL-based generation and parsing to one uniform EBLapproach usable for high-level performance strate-gies which are based on a strict interleaving ofpars-ing and generation (cf.
(Neumann and van Noord,1994; Neumann, 1994a)).8 AcknowledgementThe research underlying this paper was supportedby a research grant from the German Bundesmin-isterium f/Jr Bildung, Wissenschaft, Forschungund Technologie (BMB+F) to the DFKI projectPARADIME FKZ ITW 9704.I would like to thank the HPSG people from CSLI,Stanford for their kind support and for providing theHPSG-based English grammar.
In particular I wantto thank Dan Flickinger and Ivan Sag.
Many thanksalso to Walter Kasper for fruitful discussions.Re ferencesCopestake, A., D. Flickinger, R. Malouf, S. Riehe-mann, and I.
Sag.
1996.
Translation usingminimal recursion semantics.
In Proceedings,6th International Conference on Theoretical andMethodological Issues in Machine Translation.Dale, R., W. Finkler, R. Kittredge, N. Lenke,G.
Neumann, C. Peters, and M. Stede.
1994.
Re-port from working group 2: Lexicalization andarchitecture.
In W. Hoeppner, H. Horacek, andJ.
Moore, editors, Principles of Natural LanguageGeneration, Dagstuhl-Seminar-Report; 93.
SchloflDagstuhl, Saarland, Germany, Europe, pages 30-39.Kay, M. 1996.
Chart generation.
In 3~th An-nual Meeting of the Association for Computa-tional Linguistics, Santa Cruz, Ca.Krieger, Hans-Ulrich and Ulrich Sch~fer.
1994.7"Dt:--a type description language for constraint-based grammars.
In Proceedings of the 15th Inter-national Conference on Computational Linguis-tics, COLING-9~, pages 893-899.Minton, S., J. G. Carbonell, C. A. Knoblock,D.
R.Kuokka, O. Etzioni, and Y.Gi.
1989.Explanation-based learning: A problem solvingperspective.
Artificial Intelligence, 40:63-115.Mitchell, T., R. Keller, and S. Kedar-Cabelli.1986.
Explanation-based generalization: a uni-fying view.
Machine Learning, 1:47-80.Neumann, G. 1994a.
Application of explanation-based learning for efficient processing ofconstraintbased grammars.
In Proceedings of the TenthIEEE Conference on Artificial Intelligence for Ap-plications, pages 208-215, San Antonio, Texas,March.Neumann, G. 1994b.
A Uniform ComputationalModel for Natural Language Parsing and Gener-ation.
Ph.D. thesis, Universit~t des Saarlandes,Germany, Europe, November.Neumann, G. and G. van Noord.
1994.
Re-versibility and self-monitoring in natural languagegeneration.
In Tomek Strzalkowski, editor, Re-versible Grammar in Natural Language Process-ing.
Kluwer, pages 59-96.Pollard, C. and I. M. Sag.
1994.
Head-DrivenPhrase Structure Grammar.
Center for the Studyof Language and Information Stanford.Rayner, M. 1988.
Applying explanation-based gen-eralization to natural language processing.
In Pro-ceedings of the International Conference on FifthGeneration Computer Systems, Tokyo.Rayner, M. and D. Carter.
1996.
Fast parsing us-ing pruning and grammar specialization.
In 34thAnnual Meeting of the Association for Computa-tional Linguistics, Morristown, New Jersey.Samuelsson, C. 1994.
Fast Natural-Language Pars-ing Using Explanation-Based Learning.
Ph.D.thesis, Swedish Institute of Computer Science,Kista, Sweden, Europe.Samuelsson, C. 1995a.
An efficient algorithm forsurface generation.
In Proceedings of the 14th In-ternational Joint Conference on Artificial Intelli-gence, pages 1414-1419, Montreal, Canada.220Samuelsson, C. 1995b.
Example-based optimiza-tion of surface-generation tables.
In Proceedingsof Recent Advances in Natural Language Process-ing, Velingrad, Bulgaria, Europe.Samuelsson, C. and M. Rayner.
1991.
Quantita-tive evaluation of explanation-based l arning asan optimization tool for a large-scale natural lan-guage system.
In IJCAI-91, pages 609-615, Syd-ney, Australia.Shemtov, H. 1996.
Generation of Paraphrases fromAmbiguous Logical Forms.
In Proceedings of the16th International Conference on ComputationalLinguistics (COLING), pages 919-924, Kopen-hagen, Denmark, Europe.Shieber, S. M. 1993.
The problem of logical-formequivalence.
Computational Linguistics, 19:179-190.Srinivas, B. and A. Joshi.
1995.
Some novel ap-plications of explanation-based l arning to pars-ing lexicalized tree-adjoining grammars.
In 33thAnnual Meeting of the Association for Computa-tional Linguistics, Cambridge, MA.van Harmelen, F. and A. Bundy.
1988.
Explanation-based generalization=partial ev luation.
Artifi-cial Intelligence, 36:401-412.221
