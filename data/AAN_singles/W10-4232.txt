UDel: Named Entity Recognition and Reference Regenerationfrom Surface TextNicole L. Sparks, Charles F. Greenbacker, Kathleen F. McCoy, and Che-Yu KuoDepartment of Computer and Information SciencesUniversity of DelawareNewark, Delaware, USA[sparks|charlieg|mccoy|kuo]@cis.udel.eduAbstractThis report describes the methods and re-sults of a system developed for the GRECNamed Entity Recognition and GRECNamed Entity Regeneration Challenges2010.
We explain our process of automat-ically annotating surface text, as well ashow we use this output to select improvedreferring expressions for named entities.1 IntroductionGeneration of References in Context (GREC) is aset of shared task challenges in NLG involving acorpus of introductory sentences from Wikipediaarticles.
The Named Entity Recognition (GREC-NER) task requires participants to recognize allmentions of people in a document and indicatewhich mentions corefer.
In the Named Entity Re-generation (GREC-Full) task, submitted systemsattempt to improve the clarity and fluency of atext by generating improved referring expressions(REs) for all references to people.
Participants areencouraged to use the output from GREC-NER asinput for the GREC-Full task.
To provide ampleopportunities for improvement, a certain portionof REs in the corpus have been replaced by more-specified named references.
Ideally, the GREC-Full output will be more fluent and have greaterreferential clarity than the GREC-NER input.2 MethodThe first step in our process to complete theGREC-NER task is to prepare the corpus for in-put into the parser by stripping all XML tags andsegmenting the text into sentences.
This is accom-plished with a simple script based on common ab-breviations and sentence-final punctuation.Next, the files are run through the StanfordParser (The Stanford Natural Language Process-ing Group, 2010), providing a typed dependencyrepresentation of the input text from which we ex-tract the syntactic functions (SYNFUNC) of, andrelationships between, words in the sentence.The unmarked segmented text is also usedas input for the Stanford Named Entity Recog-nizer (The Stanford Natural Language ProcessingGroup, 2009).
We eliminate named entity tags forlocations and organizations, leaving only personentities behind.
We find the pronouns and com-mon nouns (e.g.
?grandmother?)
referring to per-son entities that the NER tool does not tag.
Wealso identify the REG08-Type and case for eachRE.
Entities found by the NER tool are markedas names, and the additional REs we identifiedare marked as either pronouns or common nouns.Case values are determined by analyzing the as-signed type and any type dependency representa-tion (provided by the parser) involving the entity.At this stage we also note the gender of each pro-noun and common noun, the plurality of each ref-erence, and begin to deal with embedded entities.The next step identifies which tagged mentionscorefer.
We implemented a coreference resolu-tion tool using a shallow rule-based approach in-spired by Lappin and Leass (1994) and Bontchevaet al (2002).
Each mention is compared to allpreviously-seen entities on the basis of case, gen-der, SYNFUNC, plurality, and type.
Each en-tity is then evaluated in order of appearance andcompared to all previous entities starting with themost recent and working back to the first in thetext.
We apply rules to each of these pairs basedon the REG08-Type attribute of the current en-tity.
Names and common nouns are analyzed us-ing string and word token matching.
We collectedextensive, cross-cultural lists of male and femalefirst names to help identify the gender of namedentities, which we use together with SYNFUNCvalues for pronoun resolution.
Separate rules gov-ern gender-neutral pronouns such as ?who.?
Bythe end of this stage, we have all of the resourcesMUC-6 CEAF B-CUBEDCorpus F prec.
recall F prec.
recall F prec.
recallEntire Set 71.984 69.657 74.471 68.893 68.893 68.893 72.882 74.309 71.509Chefs 71.094 65.942 77.119 65.722 65.722 65.722 71.245 69.352 73.244Composers 68.866 66.800 71.064 68.672 68.672 68.672 71.929 73.490 70.433Inventors 76.170 77.155 75.210 72.650 72.650 72.650 75.443 80.721 70.812Table 1: Self-evaluation scores for GREC-NER.necessary to complete the GREC-NER task.As a post-processing step, we remove all extra(non-GREC) tags used in previous steps, re-orderthe remaining attributes in the proper sequence,add the list of REs (ALT-REFEX), and write thefinal output following the GREC format.
At thispoint, the GREC-NER task is concluded and itsoutput is used as input for the GREC-Full task.To improve the fluency and clarity of the textby regenerating the referring expressions, we relyon the system we developed for the GREC NamedEntity Challenge 2010 (NEG), a refined versionof our 2009 submission (Greenbacker and Mc-Coy, 2009a).
This system trains decision treeson a psycholinguistically-inspired feature set (de-scribed by Greenbacker and McCoy (2009b)) ex-tracted from a training corpus.
It predicts the mostappropriate reference type and case for the givencontext, and selects the best match from the list ofavailable REs.
For the GREC-Full task, however,instead of using the files annotated by the GRECorganizers as input, we use the files we annotatedautomatically in the GREC-NER task.
By keep-ing the GREC-NER output in the GREC format,our NEG system was able to successfully run un-modified and generate our output for GREC-Full.3 ResultsScores calculated by the GREC self-evaluationtools are provided in Table 1 for GREC-NER andin Table 2 for GREC-Full.Corpus NIST BLEU-4Entire Set 8.1500 0.7953Chefs 7.5937 0.7895Composers 7.5381 0.8026Inventors 7.5722 0.7936Table 2: Self-evaluation scores for GREC-Full.4 ConclusionsUntil we compare our results with others teams oran oracle, it is difficult to gauge our performance.However, at this first iteration of these tasks, we?repleased just to have end-to-end RE regenerationworking to completion with meaningful output.5 Future WorkFuture improvements to our coreference resolu-tion approach involve analyzing adjacent text, uti-lizing more of the parser output, and applying ma-chine learning to our GREC-NER methods.ReferencesKalina Bontcheva, Marin Dimitrov, Diana Maynard,Valentin Tablan, and Hamish Cunningham.
2002.Shallow Methods for Named Entity CoreferenceResolution.
In Cha?
?nes de re?fe?rences et re?solveursd?anaphores, workshop TALN 2002, Nancy, France.Charles Greenbacker and Kathleen McCoy.
2009a.UDel: Extending reference generation to multipleentities.
In Proceedings of the 2009 Workshopon Language Generation and Summarisation (UC-NLG+Sum 2009), pages 105?106, Suntec, Singa-pore, August.
Association for Computational Lin-guistics.Charles F. Greenbacker and Kathleen F. McCoy.2009b.
Feature selection for reference generation asinformed by psycholinguistic research.
In Proceed-ings of the CogSci 2009 Workshop on Production ofReferring Expressions (PRE-Cogsci 2009), Amster-dam, July.Shalom Lappin and Herbert J. Leass.
1994.
An Algo-rithm for Pronominal Anaphora Resolution.
Com-putational Linguistics, 20(4):535?561.The Stanford Natural Language Processing Group.2009.
Stanford Named Entity Recognizer.http://nlp.stanford.edu/software/CRF-NER.shtml.The Stanford Natural Language Processing Group.2010.
The Stanford Parser: A statistical parser.http://nlp.stanford.edu/software/lex-parser.shtml.
