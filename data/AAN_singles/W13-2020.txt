Proceedings of the BioNLP Shared Task 2013 Workshop, pages 135?138,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsDetecting Relations in the Gene Regulation NetworkThomas ProvoostMarie-Francine MoensDepartment of Computer ScienceKU LeuvenCelestijnenlaan 200A, 3000 Leuven, Belgium{thomas.provoost, sien.moens}@cs.kuleuven.beAbstractThe BioNLP Shared Task 2013 is organ-ised to further advance the field of in-formation extraction in biomedical texts.This paper describes our entry in the GeneRegulation Network in Bacteria (GRN)part, for which our system finished in sec-ond place (out of five).
To tackle this re-lation extraction task, we employ a basicSupport Vector Machine framework.
Wediscuss our findings in constructing localand contextual features, that augment ourprecision with as much as 7.5%.
We touchupon the interaction type hierarchy inher-ent in the problem, and the importance ofthe evaluation procedure to encourage ex-ploration of that structure.1 IntroductionThe increasing number of results in the biomedicalknowledge field has been responsible for attract-ing attention and research efforts towards meth-ods of automated information extraction.
Of par-ticular interest is the recognition of informationfrom sources that are formulated in natural lan-guage, since a great part of our knowledge is stillin this format.
Naturally, the correct detection ofbiomedical events and relations in texts is a matterwhich continues to challenge the scientific com-munity.
Thanks to the BioNLP Shared tasks, al-ready in the third instalment, researchers are givendata sets and evaluation methods to further ad-vance this field.We participated in the Gene Regulation Network(GRN) Task (Bossy et al 2013), which is anextension of the Bacteria Gene Interactions Taskfrom 2011 (Jourde et al 2011).
In this task, ef-forts are made to automatically extract gene inter-actions for sporulation, a specific cellular functionof the bacterium bacillus subtilis for which a sta-ble reference regulatory network exists.
An exam-ple sentence can be seen below.
Note that all en-tities (except for event triggers, i.e.
action entitieslike transcription in figure 1) are given as input inboth training and test phases.
Therefore, this taskmakes abstraction of the entity recognition issue,putting complete focus on the subproblem of rela-tion detection.sspG transcription also requires the DNA binding protein GerE .EventInteraction: RequirementAgentTargetFigure 1: Example sentence: there is an Interac-tion:Requirement relation defined between entitiesGerE and sspG, through the action event of tran-scription.
Full-line entities are given in the testphase, while dashed-lined ones are not.As this is our first participation in this task, wehave built a simple, yet adaptable framework.
Ourcontributions lie therefore more in the domain offeature definition and exploration, rather than indesigning novel machine learning models.Predictions could be given in two ways.
Eitherall events and relations could be predicted, fromwhich the regulation network would then be infer-enced (cfr.
figure 1, detect all dashed-lined enti-ties, and the relations between them).
Or, a speci-fication of the regulation network itself is directlypredicted (in the example, this amounts to findingGerE ?
sspG, and the type (Requirement)).
Wechose to implement the latter method.
In section2 we will lay out the framework we constructed,and the tools we used.
In that section, we will alsolook at some of the design choices for our featureconstruction.
Finally we discuss our results in sec-tion 3, and touch upon opportunities to exploit theavailable interaction hierarchy in this data.1352 ImplementationBasic Framework For this interaction detec-tion task, we implement a Support Vector Ma-chine (SVM) (Vapnik, 1995), with the use of theSVMLight (Joachims, 1999) implementation inthe Shogun Machine Learning Toolbox.
Per givensentence, we construct our data points to be allpairs of genic entities in that sentence, i.e., all pos-sible interaction agent/target pairs.
Note that sincethe regulation network is a directed graph, the or-der of the nodes matters; each such pair thereforeoccurs twice in the data.
It is obvious from thisconstruction that this leads to a great imbalance:there are a lot more negatively labelled data pointsthan positive ones.
To respond to this, we triedapplying differential weighing (as seen in (Shawe-Taylor and Cristianini, 1999) and (Veropoulos etal., 1999)).
This amounts to appointing a big-ger regularisation parameter C to the positive datapoints when training the SVM, thus tightening theboundary constraint on the margin for these points.The results of this were unconvincing however, sowe decided not to implement it.For each interaction type (there are 6 of them),we then train a separate binary (local, hence one-versus-all) SVM classifier1, with a Gaussian Ra-dial Basis Function (RBF) kernel as in (Ai?zermanet al 1964) and (Scho?lkopf et al 1997).
We eval-uated several types of kernels (linear, polynomial,Gaussian) in a 25-fold cross-validation over theunion of training and validation set, and the RBF-kernel consistently gave better results.Feature Construction and Selection Considerour data points (i.e., the agent/target pairs) xijk =(eij , eik), j 6= k, where eij denotes the jth entityof sentence i.
For each such point, the basic (real-valued) feature set-up is this:f(xijk) = fent(eij ) fent(eik) fextra(eij , eik),a concatenation (the operation) of the respectivefeature vectors fent defined separately on the pro-vided entities.
To that we add fextra, which con-tains the Stanford parse tree (Klein and Manning,2003) distance of the two entities, and the locationand count (if any) of Promoter entities: these arenecessary elements for transcription without beingpart of the gene itself.
For any entity, we then con-1There is a lot of scope for leveraging the hierarchy in theinteraction types; we touch upon this in the conclusion.struct the feature vector as:fent(eij ) =1Nij?w?eijfbase(w) fcontext(w, i),whereNij is the number of words in eij .
This is anaverage over all words w that make up entity eij2,with the choice of averaging as a normalisationmechanism, to prevent a consistent assignment ofrelatively higher values to multi-word entities.
In-side the sum is the concatenation of the local fea-ture function on the words (fbase) with fcontext,which will later be seen as encoding the sentencecontext.The base feature function on a word is a vectorcontaining the following dimensions:?
The entity type, as values ?
{0, 1};?
Vocabulary features: for each word in the dic-tionary (consisting of all words encountered),a similarity score ?
[0, 1] is assigned thatmeasures how much of the beginning of theword is shared3.
In using a similarity scor-ing instead of a binary-valued indicator func-tion, we want to respond to the feature spar-sity, aggravated by the low amount of data(134 sentences in training + validation set).While this introduces some additional noisein the feature space, this is greatly offset bya better alignment of dimensions that are ef-fectively related in nature.
Also note that,due to the nature of the English language,this approach of scoring similarities based ona shared beginning, is more or less equiva-lent to stemming (albeit with a bias towardsmore commonly occurring stems).
For ourcross-validations, utilisation of these similar-ity scores attributed to an increase in F-scoreof 7.6% (mainly due to an increase in re-call of 7.0%, without compromising preci-sion) when compared to the standard binaryvocabulary features.?
Part-of-speech information, using thePenn-Treebank (maximum entropy) tagger,through the NLTK Python library (Bird etal., 2009).
These are constructed in the samefashion as the vocabulary features;2Note that one entity can consist of multiple words.3To not overemphasise small similarities (e.g.
one or twoinitial letters in common), we take this as a convex functionof the proportion of common letters.136?
Location of the word in its sentence (nor-malised to be ?
[0, 1]).
Note that next tobeing of potential importance in determiningan entity to be either target or agent, the sub-space of the two location dimensions of therespective entities in the data point xijk =(eij , eik) also encodes the word distance be-tween these.?
Depth in the parse tree (normalised to be ?
[0, 1]).Adding contextual features On top of these ba-sic features, we add some more information aboutthe context in which the entities reside.
To this ef-fect, we concatenate to the basic word features thetree context: a weighted average of all other wordsin the sentence:fcontext(w, i) =1Z?wj?sentencei?di(w,wj)fbase(wj)with fbase the basic word features describedabove, and weights given by ?
?
14 and di(w,wj)the parse tree distance from w to wj .
The normal-isation factor we use isZ =?wj?sentencei?di(w,wj)i.e., the value we would get if a feature would beconsistently equal to 1 for all words.
This nor-malisation makes sure longer sentences are notoverweighted.
For the inner parse tree nodes wethen construct a similar vector (using only part-of-speech and phrasal category information), andappend it to the word context vector.Note that the above definition of fcontext also al-lows us to define di(w,wj) to be the word distancein the sentence, leaving out any grammatical (tree)notion.
We designate this by the term sentencecontext.3 Results and ConclusionCross-validation performance on training dataBecause we have no direct access to the final testdata, we explore the model performance by con-sidering results from a 25-fold cross-validation onthe combined training and validation set.
Table 14We optimised ?
to be 0.4, by tuning on a 25-fold cross-validation, only using training and validation set.shows the numbers of three different implementa-tions5: one with respectively no fcontext concate-nated, and the tree context (the official submissionmethod) and sentence context versions.
We seethat a model based uniquely on information fromthe agent and target entities already performs quitewell; a reason for this could be the limited amountof entities and/or interactions that come into playin the biological process of sporulation, augment-ing the possibility that a pair can already be ob-served in the training data.
Adding context infor-mation increases the F-score by 2%, mainly due toa substantial increase in precision, as high as 7.5%for the sentence context.
Recall performs better inthe tree variant however, pointing to the fact thatgrammatical structure can play a role in identify-ing relations.Note that we only considered the sentence alter-ation after the submission deadline, so the betterresults seen here could no longer implore us to usethis version of the context features.Context SER Prec.
Recall F1None 0.827 0.668 0.266 0.380Tree 0.794 0.709 0.285 0.406Sentence 0.787 0.743 0.278 0.405Table 1: Results of the cross-validation for severalimplementations of context features.
(C = 5, ?
=8.75)We can identify some key focus points to fur-ther improve our performance.
Generally, as canbe seen in the additional results of table 1, a lowrecall is the main weakness in our system.
Theselow numbers can in part be explained by the lackof great variation in the features, mainly due tothe low amount of data we have.
Interesting tonote here, is the great diversity of performance ofthe local classifiers separately: the SVM for Tran-scription attains a recall of 42.0%, in part becausethis type is the most frequent in our data.
However,the worst performers, Requirement and Regulation(with a recall of 0.0% and 3.7% respectively) arenot per se the least frequent; in fact, Regulationis the second most occurring.
Considerable effortshould be put into addressing the general recall is-sue, and gaining further insight into the reasonsbehind the displayed variability.5For simplicity, we keep all other parameters (C, and theRBF kernel parameter ?)
identical across the different entriesof the table.
While in theory a separate parameter optimisa-tion on each model could affect the comparison, this showedto be of little qualitative influence on the results.137Final results on test data On submission of theoutput from the test data, our system achieved aSlot Error Rate (SER) of 0.830 (precision: 0.500,recall: 0.227, F1: 0.313), coming in second placeafter the University of Ljubljana (Zitnik et al2013) who scored a SER of 0.727 (precision:0.682, recall: 0.341, F1: 0.455).Exploring structure One of the main issues ofinterest for future research is the inherent hierar-chical structure in the interactions under consid-eration.
These are not independent of each other,since there are the following inclusions:RegulationInhibition ActivationRequirementBindingTranscriptionSo for example, each interaction of type Tran-scription is also of type Binding, and Regula-tion.
This structure implicates additional knowl-edge about the output space, and we can use thisto our benefit when constructing our classifier.In our initial framework, we make use of localclassifiers, and hence do not leverage this addi-tional knowledge about type structure.
We havealready started exploring the design of techniquesthat can exploit this structure, and preliminary re-sults are promising.One thing we wish to underline in this processis the need for an evaluation procedure that is asaware of the present structures as the classifier.
Forinstance, a system that predicts a Binding interac-tion to be of type Regulation, is more precise thana system that identifies it as an Inhibition.
Bothfor internal as external performance comparison,we feel this differentiation could broaden the fo-cus towards a more knowledge-driven approach ofevaluating.AcknowledgementsWe would like to thank the Research FoundationFlanders (FWO) for funding this research (grantG.0356.12).ReferencesMark A. Ai?zerman, E. M. Braverman, and Lev I. Rozo-noer.
1964.
Theoretical foundations of the potentialfunction method in pattern recognition learning.
Au-tomation and Remote Control, 25:821?837.Steven Bird, Ewan Klein, and Edward Loper.
2009.Natural Language Processing with Python.
OReillyMedia Inc.Robert Bossy, Philippe Bessir`es, and Claire Ne?dellec.2013.
BioNLP shared task 2013 - an overview ofthe genic regulation network task.
In Proceedingsof BioNLP Shared Task 2013 Workshop, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In Bernhard Scho?lkopf, Christo-pher J. C. Burges, and Alexander J. Smola, edi-tors, Advances in Kernel Methods - Support VectorLearning, chapter 11, pages 41?56.
MIT Press.Julien Jourde, Alain-Pierre Manine, Philippe Veber,Kare?n Fort, Robert Bossy, Erick Alphonse, andPhilippe Bessie`res.
2011.
BioNLP shared task 2011- bacteria gene interactions and renaming.
In Pro-ceedings of BioNLP Shared Task 2011 Workshop,pages 56?64.Dan Klein and Christopher D. Manning.
2003.
Fastexact inference with a factored model for naturallanguage parsing.
In Advances in Neural Informa-tion Processing Systems, number 15, pages 3?10.MIT Press.Bernhard Scho?lkopf, Kah-Kay Sung, Christopher J. C.Burges, Federico Girosi, Partha Niyogi, TomasoPoggio, and Vladimir N. Vapnik.
1997.
Compar-ing support vector machines with Gaussian kernelsto radial basis function classifiers.
IEEE Transac-tions on Signal Processing, 45(11):2758?2765.John Shawe-Taylor and Nello Cristianini.
1999.
Fur-ther results on the margin distribution.
In Proceed-ings of the Twelfth Annual Conference on Computa-tional Learning Theory, COLT ?99, pages 278?285,New York, NY, USA.
ACM.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag New York, Inc.,New York, NY, USA.Konstantinos Veropoulos, Colin Campbell, and NelloCristianini.
1999.
Controlling the sensitivity of sup-port vector machines.
In Proceedings of the Inter-national Joint Conference on AI, pages 55?60.Slavko Zitnik, Marinka itnik, Bla Zupan, and MarkoBajec.
2013.
Extracting gene regulation networksusing linear-chain conditional random fields andrules.
In Proceedings of BioNLP Shared Task 2013Workshop, Sofia, Bulgaria, August.
Association forComputational Linguistics.138
