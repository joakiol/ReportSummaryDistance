Proceedings of the EACL 2009 Student Research Workshop, pages 28?36,Athens, Greece, 2 April 2009. c?2009 Association for Computational LinguisticsFinding Word Substitutions Using a Distributional Similarity Baselineand Immediate Context OverlapAurelie HerbelotUniversity of CambridgeComputer LaboratoryJ.J.
Thompson AvenueCambridgeah433@cam.ac.ukAbstractThis paper deals with the task of find-ing generally applicable substitutions for agiven input term.
We show that the outputof a distributional similarity system base-line can be filtered to obtain terms that arenot simply similar but frequently substi-tutable.
Our filter relies on the fact thatwhen two terms are in a common entail-ment relation, it should be possible to sub-stitute one for the other in their most fre-quent surface contexts.
Using the Google5-gram corpus to find such characteris-tic contexts, we show that for the giventask, our filter improves the precision of adistributional similarity system from 41%to 56% on a test set comprising commontransitive verbs.1 IntroductionThis paper looks at the task of finding word substi-tutions for simple statements in the context of KBquerying.
Let us assume that we have a knowl-edge base made of statements of the type ?subject?
verb ?
object?:1.
Bank of America ?
acquire ?
Merrill Lynch2.
Lloyd?s ?
buy ?
HBOS3.
Iceland ?
nationalise ?
KaupthingLet us also assume a simple querying facility,where the user can enter a word and be presentedwith all statements containing that word, in a typ-ical search engine fashion.
If we want to return allacquisition events present in the knowledge baseabove (as opposed to nationalisation events), wemight search for ?acquire?.
This will return thefirst statement (about the acquisition of MerrillLynch) but not the second statement about HBOS.Ideally, we would like a system able to generatewords similar to our query, so that a statementcontaining the verb ?buy?
gets returned when wesearch for ?acquire?.This problem is closely related to the clusteringof semantically similar terms, which has receivedmuch attention in the literature.
Systems thatperform such clustering usually do so under theassumption of distributional similarity (Harris,1954) which state that two words appearingin similar contexts will be close in meaning.This observation is statistically useful and hascontributed to successful systems within twoapproaches: the pattern-based approach and thefeature vector approach (we describe those twoapproaches in the next section).
The definitionof similarity used by those systems is fairlywide, however.
Typically, a query on the verb?produce?
will return verbs such as ?export?, ?im-port?
or ?sell?, for instance (see DIRT demo fromhttp://demo.patrickpantel.com/Content/LexSem/paraphrase.htm, Lin and Pantel, 2001.
)This fairly wide notion of similarity is not fullyappropriate for our word substitutions task: al-though cats and dogs are similar types of enti-ties, querying a knowledge base for ?cat?
shouldn?treturn statements about dogs; statements aboutSiamese, however, should be acceptable.
So, fol-lowing Dagan and Glickman (2004), we refine ourconcept of similarity as that of entailment, definedhere as the relation whereby the meaning of a wordw1 is ?included?
in the meaning of word w2 (prac-tically speaking, we assume that the ?meaning?
ofa word is represented by the contexts in which itappears and require that if w1 entails w2, the con-texts of w2 should be a subset of the contexts ofw1).
Given an input term w, we therefore attemptto extract words which either entail or are entailedby w. (We do not extract directionality at thisstage.
)28The definition of entailment usually implies thatan entailing word must be substitutable for the en-tailed one, in some contexts at least.
Here, we con-sider word substitution queries in cases where noadditional contextual information is given, so wecannot assume that possible, but rare, substitutionswill fit the query intended by the user (?believe?correctly entails ?buy?
in some cases but we canbe reasonably sure that the query ?buy?
is meantin the ?purchase?
sense.)
We thus require that ouroutput will fit the most common contexts.
For in-stance, given the query ?kill?, we want to return?murder?
but not ?stop?.
Given ?produce?, we wantto return both ?release?
and ?generate?
but not ?fab-ricate?
or ?hatch?.1 Taking this into account, wegenerally define substitutability as the ability of aword to replace another one in a given sentencewithout changing the meaning or acceptability ofthe sentence, and this in the most frequent cases.
(By acceptability, we mean whether the sentenceis likely to be uttered by a native speaker of thelanguage under consideration.
)In order to achieve both entailment and generalsubstitutability, we propose to filter the output ofa conventional distributional similarity system us-ing a check for lexical substitutability in frequentcontexts.
The idea of the filter relies on the ob-servation that entailing words tend to share morefrequent immediate contexts than just related ones.For instance, when looking at the top 200 most fre-quent Google 3-gram contexts (Brants and Franz,2006) appearing after the terms ?kill?, ?murder?and ?abduct?, we find that ?kill?
and ?murder?
share54 while ?kill?
and ?abduct?
only share 2, givingus the indication that as far as usage is concerned,?murder?
is closer to ?kill?
than ?abduct?.
Addi-tionally, context frequency provides a way to iden-tify substitutability for the most common uses ofthe word, as required.In what follows, we briefly present relatedwork, and introduce our corpus and algorithm, in-cluding a discussion of our ?immediate contextoverlap?
filter.
We then review the results of anexperiment on the extraction of entailment pairs1In fact, we argue that even in systems where context isavailable, searching for all entailing words is not necessary anadvantage: consider the query ?What does Dole produce??
toa search engine.
The verb ?fabricate?
entails ?produce?
in thecorrect sense of the word, but because of its own polysemy,and unless an expensive layer of WSD is added to the system,it will return sentences such as ?Dole fabricated stories abouther opponent?, which is clearly not the information that theuser was looking for.for 30 input verbs.2 Previous Work2.1 Distributional Similarity2.1.1 PrinciplesSystems using distributional similarity usually fallunder two approaches:1.
The pattern-based approach (e.g.
Ravichad-ran and Hovy, 2002).
The most significantcontexts for an input seed are extracted asfeatures and those features used to discoverwords related to the input (under the assump-tion that words appearing in at least one sig-nificant context are similar to the seed word).There is also a non-distributional strand ofthis approach: it uses Hearst-like patterns(Hearst, 1992) which are supposed to indi-cate the presence of two terms in a certain re-lation - most often hyponymy or meronymy(see Chklovski and Pantel, 2004).2.
The feature vector approach (e.g.
Lin andPantel, 2001).
This method fully embracesthe definition of distributional similarity bymaking the assumption that two words ap-pearing in similar sets of features must be re-lated.2.1.2 LimitationsThe problems of the distributional similarity as-sumption are well-known: the facts that ?a banklends money?
and ?Smith?s brother lent himmoney?
do not imply that banks and brothers aresimilar entities.
This effect becomes particularlyevident in cases where antonyms are returned bythe system; in those cases, a very high distribu-tional similarity actually corresponds to oppositemeanings.
Producing an output ranked accord-ing to distributional similarity scores (weeding outanything under a certain threshold) is thereforenot sufficient to retain good precisions for manytasks.
Some work has thus focused on a re-rankingstrategies (see Geffet and Dagan, 2004 and Gef-fet and Dagan, 2005, who improve the output of adistributional similarity system for an entailmenttask using a web-based feature inclusion check,and comment that their filtering produces betteroutputs than cutting off the similarity pairs withthe lowest ranking.
)292.2 Extraction SystemsProminent entailment rule acquisition systems in-clude DIRT (Lin and Pantel, 2001), which usesdistributional similarity on a 1 GB corpus to iden-tify semantically similar words and expressions,and TEASE (Szpektor et al, 2004), which ex-tracts entailment relations from the web for a givenword by computing characteristic contexts for thatword.Recently, systems that combine both pattern-based and feature vector approaches have alsobeen presented.
Lin et al (2003) and Pantel andRavichandran (2004) have proposed to classify theoutput of systems based on feature vectors usinglexico-syntactic patterns, respectively in order toremove antonyms from a related words list and toname clusters of related terms.Even more related to our work, Mirkin et al(2006) integrate both approaches by constructingfeatures for the output of both a pattern-based anda vector-based systems, and by filtering incorrectentries with a supervised SVM classifier.
(Thepattern-based approach uses a set of manually-constructed patterns applied to a web search.
)In the same vein, Geffet and Dagan (2005) fil-ter the result of a pattern-based system using fea-ture vectors.
They get their features out of an 18million word corpus augmented by a web search.Their idea is that for any pair of potentially simi-lar words, the features of the entailed one shouldcomprise all the features of the entailing one.The main difference between our work and thelast two quoted papers is that we add a new layerof verification: we extract pairs of verbs using au-tomatically derived semantic patterns, perform afirst stage of filtering using the semantic signa-tures of each word and apply a final stage of filter-ing relying on surface substitutability, which wename ?immediate context overlap?
method.
Wealso experiment with a smaller size corpus to pro-duce our distributional similarity baseline (a sub-set of Wikipedia) in an attempt to show that a goodsemantic parse and adequate filtering can providereasonable performance even on domains wheredata is sparse.
Our method does not need man-ually constructed patterns or supervised classifiertraining.2.3 EvaluationThe evaluation of KB or ontology extraction sys-tems is typically done by presenting human judgeswith a subset of extracted data and asking them toannotate it according to certain correctness crite-ria.
For entailment systems, the annotation usu-ally relies on two tests: whether the meaning ofone word entails the other one in some senses ofthose words, and whether the judges can come upwith contexts in which the words are directly sub-stitutable.
Szpektor et al (2007) point out the dif-ficulties in applying those criteria.
They note thelow inter-annotator agreements obtained in previ-ous studies and propose a new evaluation methodbased on precise judgement questions applied toa set of relevant contexts.
Using their methods,they evaluate the DIRT (Lin and Pantel, 2001) andTEASE (Szpektor et al, 2004) algorithms and ob-tain upper bound precisions of 44% and 38% re-spectively on 646 entailment rules for 30 transitiveverbs.
We follow here their methodology to checkthe results obtained via the traditional annotation.3 The DataThe corpus used for our distributional similar-ity baseline consists of a subset of Wikipedia to-talling 500 MB in size, parsed first with RASP2(Briscoe et al, 2006) and then into a Robust Min-imal Recursion Semantics form (RMRS, Copes-take, 2004) using a RASP-to-RMRS converter.The RMRS representation consists of trees (or treefragments when a complete parse is not possible)which comprise, for each phrase in the sentence, asemantic head and its arguments.
For instance, inthe sentence ?Lloyd?s rescues failing bank?, threesubtrees can be extracted:lemma:rescue arg:ARG1 var:Lloyd?swhich indicates that ?Lloyd?s?
is subject of thehead ?rescue?,lemma:rescue arg:ARG2 var:bankwhich indicates that ?bank?
is object of the head?rescue?, andlemma:failing arg:ARG1 var:bankwhich indicates that the argument of ?failing?
is?bank?.Note that any tree can be transformed intoa feature for a particular lexical item by re-placing the slot containing the word with ahole: lemma:rescue arg:ARG2 var:bank be-comes lemma:hole arg:ARG2 var:bank, a po-tentially characteristic context for ?rescue?.All the experiments reported in this paper con-cern transitive verbs.
In order to speed upprocessing, we reduced the RMRS corpus to a30list of relations with a verbal head and at leasttwo arguments: lemma:verb-query arg:ARG1var:subject arg:ARG2 var:object.
Note thatwe did not force noun phrases in the second ar-gument of the relations and for instance, the verb?say?
was both considered as taking a noun or aclause as second argument (?to say a word?, ?tosay that the word is...?
).4 A BaselineWe describe here our baseline, a system based ondistributional similarity.4.1 Step 1 - Pattern-Based Pair ExtractionThe first step of our algorithm uses a pattern-basedapproach to get a list of potential entailing pairs.For each word w presented to the system, we ex-tract all semantic patterns containing w. Those se-mantic patterns are RMRS subtrees consisting of asemantic head and its children (see Section 3).
Wethen calculate the Pointwise Mutual Informationbetween each pattern p and w:pmi(p, w) = log(P (p, w)P (p)P (w))(1)where P (p) and P (w) are the probabilities of oc-currence of the pattern and the instance respec-tively and P (p, w) is the probability that they ap-pear together.PMI is known to have a bias towards less fre-quent events.
In order to counterbalance that bias,we apply a simple logarithm function to the resultsas a discount:d = log (cwp + 1) (2)where cwp is the cooccurrence count of an instanceand a pattern.We multiply the original PMI value by this dis-count to find the final PMI.
We then select the npatterns with highest PMIs and use them as rele-vant semantic contexts to find all terms t that alsoappear in those contexts.
The result of this stepis a list of potential entailment relations, w ?
t1... w ?
tx (we do not know the direction of theentailment).4.2 Step 2 - Feature vector ComparisonThis step takes the output of the pattern-based ex-traction and applies a first filter to the potential en-tailment pairs.
The filter relies on the idea thattwo words that are similar will have similar fea-ture vectors (see Geffet and Dagan, 2005).
We de-fine here the feature vector of word w as the list ofsemantic features containing w, together with thePMI of each feature in relation to w as a weight.For each pair of words (w1, w2) we extract thefeature vectors of both w1 and w2 and calculatetheir similarity using the measure of Lin (1998).Pairs with a similarity under a certain threshold areweeded out.
(We use 0.007 in our experiments ?the value was found by comparing precisions forvarious thresholds in a set of initial experiments.
)As a check of how the Lin measure performedon our Wikipedia subset using RMRS features,we reproduced the Miller and Charles experi-ment (1991) which consists in asking humans torate the similarity of 30 noun pairs.
The experi-ment is a standard test for semantic similarity sys-tems (see Jarmasz and Szpakowicz, 2003; Lin,1998; Resnik, 1995 and Hirst and St Onge, 1998amongst others).
The correlations obtained by pre-vious systems range between the high 0.6 and thehigh 0.8.
Those systems rely on edge counting us-ing manually-created resources such as WordNetand the Roget?s Thesaurus.
We are not actuallyaware of results obtained on totally automated sys-tems (apart from a baseline computed by Strubeand Ponzetto, 2006, using Google hits, which re-turn a correlation of 0.26.
)Applying our feature vector step to the Millerand Charles pairs, we get a correlation of 0.38,way below the edge-counting systems.
It turns out,however, that this low result is at least partially dueto data sparsity: when ignoring the pairs contain-ing at least one word with frequency under 200(8 of them, which means ending up with 22 pairsleft out of the initial 30), the correlation goes upto 0.69.
This is in line with the edge-counting sys-tems and shows that our baseline system producesa decent approximation of human performance, aslong as enough data is supplied.
2Two issues remain, though.
First, fine-grainedresults cannot be obtained over a general corpus:we note that the pairs ?coast-forest?
and ?coast-hill?
get very similar scores using distributionalsimilarity while the latter is ranked twice as highas the former by humans.
Secondly, distribu-2It seems then that in order to maintain precision to ahigher level on our corpus, we could simply disregard pairswith low-frequency words.
(We decided here, however, thatthis would be unacceptable from the point of view of recalland did not attempt to do so.
)31tional methods promise to identify ?semanticallysimilar?
words, as do the Miller and Charles ex-periment and edge-counting systems.
However,as pointed out in the introduction, there is stilla gap between general similarity and entailment:?coast?
and ?hill?
are indeed similar in some waybut never substitutable.
Our baseline is thereforeconstrained by a theoretical problem that furthermodules must solve.5 Immediate Context OverlapOur immediate context overlap module acts as afilter for the system described as our baseline.
Theidea is that, out of all pairs of ?similar?
words,we want to find those that express entailment inat least one direction.
So for instance, given thepairs ?kill ?
murder?
and ?kill ?
abduct?, we wouldlike to keep the former and filter the latter out.
Wecan roughly explain why the second pair is not ac-ceptable by saying that, although the semantics ofthe two words are close (they are both about an actof violence conducted against somebody), they arenot substitutable in a given sentence.To satisfy substitutability, we generally specifythat if w1 entails w2, then there should be surfacecontexts where w2 can replace w1, with the substi-tution still producing an acceptable utterance (seeour definition of acceptability in the introduction).We further suggest that if one word can substitutethe other in frequent immediate contexts, we havethe basis to believe that entailment is possible inat least one common sense of the words ?
whileif substitution is impossible or rare, we can doubtthe presence of an entailment relation, at least incommon senses of the terms.
This can be madeclearer with an example.
We show in Table 1 someof the most frequent trigrams to appear after theverbs ?to kill?, ?to murder?
and ?to abduct?
(thosetrigrams were collected from the Google 5-gramcorpus.)
It is immediately noticeable that somecontexts are not transferable from one term to theother: phrases such as ?to murder and forciblyrecruit someone?, or ?to abduct cancer cells?
areimpossible ?
or at least unconventional.
We alsoshow in italic some common immediate contextsbetween the three words.
As pointed out in the in-troduction, when looking at the top 200 most fre-quent contexts for each term, we find that ?kill?and ?murder?
share 54 while ?kill?
and ?abduct?only share 2, giving us the indication that as far asusage is concerned, ?murder?
is closer to ?kill?
than?abduct?.
Furthermore, by looking at frequency ofoccurrence, we partly answer our need to find sub-stitutions that work in very frequent sentences ofthe language.The Google 5-gram corpus gives the frequencyof each of its n-grams, allowing us to check substi-tutability on the 5-grams with highest occurrencecounts for each potential entailment pair returnedby our baseline.
For each pair (w1, w2) we selectthe m most frequent contexts for both w1 and w2and simply count the overlap between both lists.
Ifthere is any overlap, we keep the pair; if the over-lap is 0, we weed it out (the low threshold helpsour recall to remain acceptable).
We experimentwith left and right contexts, i.e.
with the queryterm at the beginning and the end of the n-gram,and with various combinations (see Section 6).6 ResultsThe results in this section are produced by ran-domly selecting 30 transitive verbs out of the 500most frequent in our Wikipedia corpus and usingour system to extract non-directional entailmentpairs for those verbs, following a similar experi-ment by Szpektor et al (2007).
We use a list ofn = 30 features in Step 1 of the baseline.
We eval-uate the results by first annotating them accordingto a broad definition of entailment: if the annota-tor can think of any context where one word ofthe pair could replace the other, preserving sur-face form and semantics, then the two words arein an entailment relation.
(Note again that we donot consider the directionality of entailment at thisstage.)
We then re-evaluate our best score usingthe Szpektor et al method (2007), which we thinkis more suited for checking true substitutability.
3The baseline described in Section 4 produces301 unique pairs, 124 of which we judge correctusing our broad entailment definition, yielding aprecision of 41%.
The average number of rela-tions extracted for each input term is thus 4.1.Tables 2 and 3 show our results at the end ofthe immediate context overlap step.
Table 2 re-port results using the m = 50 most frequent con-texts for each word in the pair while Table 3 usesan expanded list of 200 contexts.
Precision is the3Although no direct comparison with the worksof Szpektor et al or Lin and Pantel is providedin this paper, we are in the process of evaluatingour results against the TEASE output (available athttp://www.cs.biu.ac.il/?szpekti/TEASE collection.zip) through a web-based annotation task.32Table 1: Immediate Contexts for ?kill?, ?murder?
and ?abduct?kill murder abducttwo birds with babies that life her and makecancer cells and his wife and an innocent mana mocking bird thousands of innocent unsuspecting people andor die for women and children suspects in foreignor be killed her husband and a young girlanother human being in the name and forcibly recruitthousands of people in connection with a teenage girlin the name another human being and kill herhis wife and tens of thousands a child frommembers of the the royal family women and childrennumber of correct relations amongst all those re-turned.
Recall is calculated with regard to the 124pairs judged correct at the end of the previous step(i.e., this is not true recall but recall relative to thebaseline results.
)We experimented with six different set-ups:1- right context: the four words following thequery term are used as context2- left context: the four words preceding thequery term are used as context3- right and left contexts: the best contexts(those with highest frequencies) are selectedout of the concatenation of both right and leftcontext lists4- concatenation: the concatenation of the re-sults obtained from 1 and 25- inclusion: the inclusion set of the results from1 and 2, that is, the pairs judged correct byboth the right context and left context meth-ods.6- right context with ?to?
: identical to 1 but the5-gram is required to start with ?to?.
Thisensures that only the verb form of the queryterm is considered but has the disadvantageof effectively transforming 5-grams into 4-grams.Our best overall results comes from using 50immediate contexts starting with ?to?, right con-text only: we obtain 56% precision on a recall of85% calculated on the results of the previous step.Table 2: Results using 50 immediate contextsContext Used Precision Recall F Returned CorrectLeft 48% 63% 54% 164 78Right 62% 26% 36% 52 32Left and Right 53% 52% 52% 122 65Concatenation 48% 70% 57% 181 87Inclusion 67% 19% 30% 36 24Right + ?to?
56% 85% 68% 187 105Table 3: Results using 200 immediate contextsContext Used Precision Recall F Returned CorrectLeft 44% 86% 58% 244 107Right 54% 60% 57% 137 74Left and Right 46% 85% 60% 228 105Concatenation 44% 92% 60% 260 114Inclusion 55% 53% 54% 121 66Right + ?to?
48% 97% 64% 248 1206.1 Instance-Based EvaluationWe then recalculate our best precision followingthe method introduced in Szpektor et al (2007).This approach consists in extracting, for each po-tential entailment relation X-verb1-Y?X-verb2-Y, 15 sentences in which verb1 appears and askannotators to provide answers to three questions:1.
Is the left-hand side of the relation entailedby the sentence?
If so...2.
When replacing verb1 with verb2, is the sen-tence still likely in English?
If so...333.
Does the sentence with verb1 entail the sen-tence with verb2?We show in Table 4 some potential annotationsat various stages of the process.For each pair, Szpektor et al then calculate alower-bound precision asPlb =nEntailednLeftHandEntailed(3)where nEntailed is the number of entailed sentencepairs (the annotator has answered ?yes?
to the thirdquestion) and nLeftHandEntailed is the number ofsentences where the left-hand relation is entailed(the annotator has answered ?yes?
to the first ques-tion).
They also calculate an upper-bound preci-sion asPub =nEntailednAcceptable(4)where nAcceptable is the number of acceptableverb2 sentences (the annotator has answered ?yes?to the second question).
A pair is deemed to con-tain an entailment relation if the precision for thatparticular pair is over 80%.The authors comment that a large proportion ofextracted sentences lead to a ?left-hand side not en-tailed?
answer.
In order to counteract that effect,we only extract sentences without modals or nega-tion from our Wikipedia corpus and consequentlyonly require 10 sentences per relation (only 11%of our sentences have a ?non-entailed?
left-handside relation against 43% for Szpektor et al).We obtain an upper bound precision of 52%,which is slightly lower than the one initially cal-culated using our broad definition of entailment,showing that the more stringent evaluation is use-ful when checking for general substitutability inthe returned pairs.
When we calculate the lowerbound precision, however, we obtain a low 10%precision due to the large number of sentencesjudged as ?unlikely English sentences?
after sub-stitution (they amount to 33% of all examples witha left-hand side judged ?entailed?).
This result il-lustrates the need for a module able to check sen-tence acceptability when applying the system totrue substitution tasks.
Fortunately, as we explainin the next section, it also takes into account re-quirements that are only necessary for generationtasks, and are therefore irrelevant to our queryingtask.7 DiscussionOur main result is that the immediate context over-lap step dramatically increases our precision (from41% to 56%), showing that a more stringent notionof similarity can be achieved when adequately fil-tering the output of a distributional similarity sys-tem.
However, it also turns out that looking atthe most frequent contexts of the word to substi-tute does not fully solve the issue of surface ac-ceptability (leading to a high number of ?right-hand side not entailed?
annotations).
We argue,though, that the issue of producing an acceptableEnglish sentence is a generation problem separatefrom the extraction task.
Some systems, in fact,are dedicated to related problems, such as identi-fying whether the senses of two synonyms are thesame in a particular lexical context (see Dagan etal., 2006).
As far as our needs are concerned inthe task of KB querying, we only require accuratesearching capabilities as opposed to generationalcapabilities: the expansion of search terms to in-clude impossible strings is not a problem in termsof result.Looking at the immediate context overlaps re-turned for each pair by the system, we find that theoverlap (the similarity) can be situated at variouslinguistic layers:?
in the semantics of the verb?s object: ?anew album?
is something that one would fre-quently ?record?
or ?release?.
The phraseboosts the similarity score between ?record?and ?release?
in their music sense.?
in the clausal information of the right context:a context starting with a clause introduced by?that?
is likely to be preceded by a verb ex-pressing cognition or discourse.
The tri-gram?that there is?
increases the similarity of pairssuch as ?say - argue?.?
in the prepositional information of the rightcontext: ?about?
is the preposition of choiceafter cognition verbs such as ?think?
or ?won-der?.
The context ?about the future?
helps thescore of the pair ?think - speculate?
in the cog-nitive sense (note that ?speculate?
in a finan-cial sense would take the preposition ?on?.
)Some examples of overlaps are shown in Ta-ble 5.We also note that the system returns a fair pro-portion of vacuous contexts such as ?one of the?
or34Table 4: Annotation Examples Following the Szpektor et al MethodWord Pair Sentence Question 1 Question 2 Question 3acquire ?
buy Lloyds acquires HBOS yes yes (Lloyds buys HBOS) yesacquire ?
praise Lloyds acquires HBOS yes yes (Lloyds praises HBOS) noacquire ?
spend Lloyds acquires HBOS yes no (*Lloyds spends HBOS) ?acquire ?
buy Lloyds may acquire HBOS no ?
?Table 5: Sample of Immediate Context Overlapsthink ?
speculate say ?
claim describe ?
characteriseabout the future that it is the nature ofabout what the that there is the effects ofabout how the that it was it as athat they were the effect ofthat they have the role ofthat it has the quality ofthe impact ofthe dynamics of?part of the?
which contribute to the score of manypairs.
Our precision would probably benefit fromexcluding such contexts.We note that as expected, using a larger set ofcontexts leads to better recall and decreased pre-cision.
The best precision is obtained by return-ing the inclusion set of both left and right contextsresults, but at a high cost in recall.
Interestingly,we find that the right context of the verb is farmore telling than the left one (potentially, objectsare more important than subjects).
This is in linewith results reported by Alfonseca and Manandhar(2002).Our best results yield an average of 3.4 relationsfor each input term.
It is in the range reportedby the authors of the TEASE system (Szpektor etal., 2004) but well below the extrapolated figuresof over 20 relations in Szpektor et al, 2007.
Wepoint out, however, that we only search for sin-gle word substitutions, as opposed to single andmulti-word substitutions for Szpektor et al.
Fur-thermore, our experiments are performed on 500MB of text only, against 1 GB of news data forthe DIRT system and the web for the TEASE al-gorithm.
More data may help our recall, as well asbootstrapping over our best precision system.We show a sample of our results in Table 6.
Thepairs with an asterisk were considered incorrect athuman evaluation stage.Table 6: Sample of Extracted Pairsbring ?
attract make - earn*call ?
form *name - delegatechange ?
alter offer - providecreate ?
generate *perform - dischargedescribe ?
characterise produce ?
releasedevelop ?
generate record ?
count*do ?
behave *release ?
announcefeature ?
boast *remain ?
comprise*find ?
indicate require ?
demandfollow ?
adopt say ?
claim*grow ?
contract tell ?
assure*increase - decline think ?
believeleave - abandon *use ?
abandon8 ConclusionWe have presented here a system for the extrac-tion of word substitutions in the context of KBquerying.
We have shown that the output of adistributional similarity baseline can be improvedby filtering it using the idea that two words in anentailment relation are substitutable in immediatesurface contexts.
We obtained a precision of 56%(52% using our most stringent evaluation) on a testset of 30 transitive verbs, and a yield of 3.4 rela-tions per verb.We also point out that relatively good precisionscan be obtained on a parsed medium-sized corpusof 500 MB, although recall is certainly affected.We note that our current implementation doesnot always satisfy the requirement for substi-tutability for generation tasks and point out thatthe system is therefore limited to our intended use,which involves search capabilities only.We would like to concentrate in the future onproviding a direction for the entailment pairs ex-tracted by the system.
We also hope that recallcould possibly improve using a larger set of fea-tures in the pattern-based step (this is suggestedalso by Szpektor et al, 2004), together with ap-35propriate bootstrapping.AcknowledgementsThis work was supported by the UK Engineer-ing and Physical Sciences Research Council (EP-SRC: EP/P502365/1).
I would also like to thankmy supervisor, Dr Ann Copestake, for her supportthroughout this project, as well as the anonymousreviewers who commented on this paper.ReferencesEnrique Alfonseca and Suresh Manandhar.
2002.
Ex-tending a Lexical Ontology by a Combination ofDistributional Semantics Signatures.
In Proceed-ings of EKAW 2002, pp.
1?7, 2002.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramVersion 1.
Linguistic Data Consortium, Philadel-phia, 2006.Edward Briscoe, John Carroll and Rebecca Watson.2006.
The Second Release of the RASP System.
InProceedings of the COLING/ACL 2006 InteractivePresentation Sessions, Sydney, Australia, 2006.Timothy Chklovski and Patrick Pantel.
2004.
Ver-bOcean: Mining The Web for Fine-Grained Se-mantic Verb Relations.
Proceedings of EMNLP-04,Barcelona, Spain, 2004.Ann Copestake.
2004.
Ro-bust Minimal Recursion Semantics.www.cl.cam.ac.uk/?aac10/papers/rmrsdraft.pdf.Ido Dagan and Oren Glickman.
2004.
Probabilis-tic Textual Entailment: Generic Applied Modellingof Language Variability.
Proceedings of The PAS-CAL Workshop on Learning Methods for Text Un-derstanding and Mining, Grenoble, France, 2004.Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-morshtein and Carlo Strapparava.
2006.
DirectWord Sense Matching for Lexical Substitution.
Pro-ceedings of COLING-ACL 2006, 17-21 Jul 2006,Sydney, Australia.Maayan Geffet and Ido Dagan.
2004.
Feature VectorQuality and Distributional Similarity.
ProceedingsOf the 20th International Conference on Computa-tional Linguistics, 2004.Maayan Geffet and Ido Dagan.
2005.
The Distri-butional Inclusion Hypothesises and Lexical Entail-ment.
In Proceedings Of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics, pp.
107?114, 2005.Mario Jarmasz and Stan Szpakowicz.
2003.
Roget?sThesaurus and Semantic Similarity.
In Proceedingsof International Conference RANLP?03, pp.
212?219, 2003.Zelig Harris.
Distributional Structure.
In Word, 10,No.
2?3, pp.
146?162, 1954.Marti Hearst.
1992.
Automatic Acquisition of Hy-ponyms from Large Text Corpora.
Proceedings ofCOLING-92, pp.539?545, 1992.Graeme Hirst and David St-Onge.
1998.
LexicalChains As Representations of Context for the Detec-tion and Correction of Malapropisms.
In ?WordNet?,Ed.
Christiane Fellbaum, Cambridge, MA: The MITPress, 1998.Dekang Lin.
2003.
An Information-Theoretic Defini-tion of Similarity.
In Proceedings of the 15th Inter-national Conference on Machine Learning, pp.
296?304, 1998.Dekang Lin, Shaojun Zhao, Lijuan Qin and MingZhou.
2003.
Identifying Synonyms among Distribu-tionally Similar Words.
In Proceedings of IJCAI-03,Acapulco, Mexico, 2003.Dekang Lin and Patrick Pantel.
2001.
DIRT ?
Discov-ery of Inference Rules from Text.
In Proceedings ofACM 2001, 2001.George Miller and Walter Charles.
2001.
ContextualCorrelates of Semantic Similarity.
In Language andCognitive Processes, 6(1), pp.
1?28, 1991.Shachar Mirkin, Ido Dagan and Maayan Geffet.
2004.Integrating Pattern-Based and Distributional Simi-larity Methods for Lexical Entailment Acquisition.In Proceedings of COLING/ACL, Sydney, Aus-tralia, pp.579?586, 2006.Patrick Pantel and Deepak Ravichandran.
2004.
Auto-matically Labelling Semantic Classes.
In Proceed-ings of HLT/NAACL04, Boston, MA, pp 321328,2004.Deepak Ravichandran and Eduard Hovy.
2002.
Learn-ing Surface Text Patterns for a Question AnsweringSystem.
Proceedings of ACL, 2002.Philip Resnik.
1995.
Using Information Content toEvaluate Semantic Similarity in a Taxonomy.
InProceedings of IJCAI?95, 1995.Idan Szpektor, Hristo Tanev, Ido Dagan and Bonaven-tura Coppola.
2004.
Scaling Web-Based Acquisitionof Entailment Relations.
In Proceedings of EMNLP?2004, pp.
41?48, 2004.Idan Szpektor, Eyal Shnarch and Ido Dagan.
2007.Instance-Based Evaluation of Entailment Rule Ac-quisition.
In Proceedings of ACL?07, 2007.Michael Strube and Simone Ponzetto.
2006.
WikiRe-late!
Computing Semantic Relatedness UsingWikipedia.
In Proceedings of AAAI?06, pp.
1219?1224, 2006.36
