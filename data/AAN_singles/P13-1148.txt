Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1508?1516,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA joint model of word segmentation and phonological variation forEnglish word-final /t/-deletionBenjamin Bo?rschinger1,3 and Mark Johnson1 and Katherine Demuth2(1) Department of Computing, Macquarie University(2) Department of Linguistics, Macquarie University(3) Department of Computational Linguistics, Heidelberg University{benjamin.borschinger, mark.johnson, katherine.demuth}@mq.edu.auAbstractWord-final /t/-deletion refers to a commonphenomenon in spoken English wherewords such as /wEst/ ?west?
are pro-nounced as [wEs] ?wes?
in certain con-texts.
Phonological variation like this iscommon in naturally occurring speech.Current computational models of unsu-pervised word segmentation usually as-sume idealized input that is devoid ofthese kinds of variation.
We extend anon-parametric model of word segmenta-tion by adding phonological rules that mapfrom underlying forms to surface formsto produce a mathematically well-definedjoint model as a first step towards han-dling variation and segmentation in a sin-gle model.
We analyse how our modelhandles /t/-deletion on a large corpus oftranscribed speech, and show that the jointmodel can perform word segmentation andrecover underlying /t/s.
We find that Bi-gram dependencies are important for per-forming well on real data and for learningappropriate deletion probabilities for dif-ferent contexts.11 IntroductionComputational models of word segmentation tryto solve one of the first problems language learn-ers have to face: breaking an unsegmented streamof sound segments into individual words.
Cur-rently, most such models assume that the inputconsists of sequences of phonemes with no pro-nunciation variation across different occurrencesof the same word type.
In this paper we describe1The implementation of our model as well asscripts to prepare the data will be made available athttp://web.science.mq.edu.au/~bborschi.We can?t release our version of the Buckeye Corpus (Pitt etal., 2007) directly because of licensing issues.an extension of the Bayesian models of Gold-water et al (2009) that incorporates phonologi-cal rules to ?explain away?
surface variation.
Asa concrete example, we focus on word-final /t/-deletion in English, although our approach is notlimited to this case.
We choose /t/-deletion be-cause it is a very common and well-studied phe-nomenon (see Coetzee (2004, Chapter 5) for areview) and segmental deletion is an interestingtest-case for our architecture.
Recent work hasfound that /t/-deletion (among other things) is in-deed common in child-directed speech (CDS) and,importantly, that its distribution is similar to that inadult-directed speech (ADS) (Dilley et al, to ap-pear).
This justifies our using ADS to evaluate ourmodel, as discussed below.Our experiments are consistent with long-standing and recent findings in linguistics, in par-ticular that /t/-deletion heavily depends on the im-mediate context and that models ignoring contextwork poorly on real data.
We also examine howwell our models identify the probability of /t/-deletion in different contexts.
We find that modelsthat capture bigram dependencies between under-lying forms provide considerably more accurateestimates of those probabilities than correspond-ing unigram or ?bag of words?
models of underly-ing forms.In section 2 we discuss related work on han-dling variation in computational models and on /t/-deletion.
Section 3 describes our computationalmodel and section 4 discusses its performance forrecovering deleted /t/s.
We look at both a sit-uation where word boundaries are pre-specifiedand only inference for underlying forms has tobe performed; and the problem of jointly findingthe word boundaries and recovering deleted un-derlying /t/s.
Section 5 discusses our findings, andsection 6 concludes with directions for further re-search.15082 Background and related workThe work of Elsner et al (2012) is most closelyrelated to our goal of building a model that han-dles variation.
They propose a pipe-line archi-tecture involving two separate generative models,one for word-segmentation and one for phonolog-ical variation.
They model the mapping to sur-face forms using a probabilistic finite-state trans-ducer.
This allows their architecture to handlevirtually arbitrary pronunciation variation.
How-ever, as they point out, combining the segmenta-tion and the variation model into one joint modelis not straight-forward and usual inference proce-dures are infeasible, which requires the use of sev-eral heuristics.
We pursue an alternative researchstrategy here, starting with a single well-studiedexample of phonological variation.
This permitsus to develop a joint generative model for bothword segmentation and variation which we plan toextend to handle more phenomena in future work.An earlier work that is close to the spirit of ourapproach is Naradowsky and Goldwater (2009),who learn spelling rules jointly with a simplestem-suffix model of English verb morphology.Their model, however, doesn?t naturally extend tothe segmentation of entire utterances./t/-deletion has received a lot of attention withinlinguistics, and we point the interested reader toCoetzee (2004, Chapter 5) for a thorough review.Briefly, the phenomenon is as follows: word-finalinstances of /t/ may undergo deletion in naturalspeech, such that /wEst/ ?west?
is actually pro-nounced as [wEs] ?wes?.2 While the frequency ofthis phenomenon varies across social and dialectalgroups, within groups it has been found to be ro-bust, and the probability of deletion depends onits phonological context: a /t/ is more likely tobe dropped when followed by a consonant thana vowel or a pause, and it is more likely to bedropped when following a consonant than a vowelas well.
We point out two recent publications thatare of direct relevance to our research.
Dilley et al(to appear) study word-final variation in stop con-sonants in CDS, the kind of input we ideally wouldlike to evaluate our models on.
They find that ?in-fants largely experience statistical distributions ofnon-canonical consonantal pronunciation variants[including deletion] that mirror those experiencedby adults.?
This both directly establishes the need2Following the convention in phonology, we give under-lying forms within ?/.
.
.
/?
and surface forms within ?[.
.
.
]?.for computational models to handle this dimensionof variation, and justifies our choice of using ADSfor evaluation, as mentioned above.Coetzee and Kawahara (2013) provide a com-putational study of (among other things) /t/-deletion within the framework of Harmonic Gram-mar.
They do not aim for a joint model that alsohandles word segmentation, however, and ratherthan training their model on an actual corpus, theyevaluate on constructed lists of examples, mimick-ing frequencies of real data.
Overall, our findingsagree with theirs, in particular that capturing theprobability of deletion in different contexts doesnot automatically result in good performance forrecovering individual deleted /t/s.
We will comeback to this point in our discussion at the end ofthe paper.3 The computational modelOur models build on the Unigram and the Bigrammodel introduced in Goldwater et al (2009).
Fig-ure 1 shows the graphical model for our joint Bi-gram model (the Unigram case is trivially recov-ered by generating the Ui,js directly from L ratherthan from LUi,j?1).
Figure 2 gives the mathemati-cal description of the graphical model and Table 1provides a key to the variables of our model.The model generates a latent sequence of un-derlying word-tokens U1, .
.
.
, Un.
Each word to-ken is itself a non-empty sequence of segments orphonemes, and each Uj corresponds to an under-lying word form, prior to the application of anyphonological rule.
This generative process is re-peated for each utterance i, leading to multipleutterances of the form Ui,1, .
.
.
, Ui,ni where ni isthe number of words in the ith utterance, and Ui,jis the jth word in the ith utterance.
Each utter-ance is padded by an observed utterance bound-ary symbol $ to the left and to the right, henceUi,0 = Ui,ni+1 = $.3 Each Ui,j+1 is generatedconditionally on its predecessor Ui,j from LUi,j ,as shown in the first row of the lower plate in Fig-ure 1.
Each Lw is a distribution over the pos-sible words that can follow a token of w and Lis a global distribution over possible words, usedas back-off for all Lw.
Just as in Goldwater etal.
(2009), L is drawn from a Dirichlet Process(DP) with base distribution B and concentration3Each utterance terminates as soon as a $ is generated,thus determining the number of words ni in the ith utterance.See Goldwater et al (2009) for discussion.1509Figure 1: The graphical model for our jointmodel of word-final /t/-deletion and Bigramword segmentation.
The corresponding math-ematical description is given in Figure 2.
Thegenerative process mimics the intuitively plau-sible idea of generating underlying forms fromsome kind of syntactic model (here, a Bi-gram language model) and then mapping theunderlying form to an observed surface-formthrough the application of a phonological rulecomponent, here represented by the collectionof rule probabilities ?c.L |?, ?0 ?DP (?0, B(?
| ?
))Lw |L,?1 ?DP (?1, L)?c |?
?Beta(1, 1)Ui,0 = $Si,0 = $Ui,j+1 |Ui,j , LUi,j ?LUi,jSi,j |Ui,j , Ui,j+1,?
=PR(?
| Ui,j , Ui,j+1)Wi |Si,1, .
.
.
, Si,ni = CAT(Si,0, .
.
.
, Si,ni)Figure 2: Mathematical description of our jointBigram model.
The lexical generator B(?
| ?
)is specified in Figure 3 and PR is explained inthe text below.
CAT stands for concatenationwithout word-boundaries, ni refers to the num-ber of words in utterance i.Variable ExplanationB base distribution over possible wordsL back-off distribution over wordsLw distribution over words following wUi,j underlying form, a wordSi,j surface realization of Ui,j , a word?c /t/-deletion probability in context cWi observed segments for ith utteranceTable 1: Key for the variables in Figure 1 andFigure 2.
See Figure 3 for the definition of B.parameter ?0, and the word type specific distri-butions Lw are drawn from a DP (L,?1), result-ing in a hierarchical DP model (Teh et al, 2006).The base distribution B functions as a lexical gen-erator, defining a prior distribution over possiblewords.
In principle, B can incorporate arbitraryprior knowledge about possible words, for exam-ple syllable structure (cf.
Johnson (2008)).
In-spired by Norris et al (1997), we use a simplerpossible word constraint that only rules out se-quences that lack a vowel (see Figure 3).
Whilethis is clearly a simplification it is a plausible as-sumption for English data.Instead of generating the observed sequence ofsegments W directly by concatenating the under-lying forms as in Goldwater et al (2009), wemap each Ui,j to a corresponding surface-formSi,j by a probabilistic rule component PR.
Thevalues over which the Si,j range are determinedby the available phonological processes.
In themodel we study here, the phonological processesonly include a rule for deleting word-final /t/sbut in principle, PR can be used to encode awide variety of phonological rules.
Here, Si,j ?
{Ui,j ,DELF(Ui,j)} if Ui,j ends in a /t/, and Si,j =Ui,j otherwise, where DELF(u) refers to the sameword as u except that it lacks u?s final segment.We look at three kinds of contexts on which arule?s probability of applying depends:1. a uniform context that applies to every word-final position2.
a right context that also considers the follow-ing segment3.
a left-right context that additionally takes thepreceeding segment into accountFor each possible context c there is a prob-ability ?c which stands for the probability ofthe rule applying in this context.
Writing1510?
?Dir(?0.01, .
.
.
, 0.01?
)B(w = x1:n | ?)
={ [?ni=1 ?xi ]?#Z if V(w)0.0 if ?V(w)Figure 3: Lexical generator with possible word-constraint for words in ?+, ?
being the alphabetof available phonemes.
x1:n is a sequence of ele-ments of ?
of length n. ?
is a probability vectorof length |?| + 1 drawn from a sparse Dirichletprior, giving the probability for each phoneme andthe special word-boundary symbol #.
The pred-icate V holds of all sequences containing at leastone vowel.
Z is a normalization constant that ad-justs for the mass assigned to the empty and non-possible words.contexts in the notation familiar from genera-tive phonology (Chomsky and Halle, 1968), ourmodel can be seen as implementing the fol-lowing rules under the different assumptions:4uniform /t/ ?
?
/ ]wordright /t/ ?
?
/ ]word ?left-right /t/ ?
?
/ ?
]word ?We let ?
range over V(owel), C(onsonant) and $(utterance-boundary), and ?
over V and C. Wedefine a function CONT that maps a pair of ad-jacent underlying forms Ui,j , Ui,j+1 to the con-text of the final segment of Ui,j .
For example,CONT(/wEst/,/@v/) returns ?C ]word V?
in theleft-right setting, or simply ?
]word?
in the uni-form setting.
CONT returns a special NOT con-text if Ui,j doesn?t end in a /t/.
We stipulate that?NOT = 0.0.
Then we can define PR as follows:PR(DELFINAL(u) | u, r)) = ?CONT(u,r)PR(u | u, r) = 1?
?CONT(u,r)Depending on the context setting used, ourmodel includes one (uniform), three (right) or six(left-right) /t/-deletion probabilities ?c.
We place auniform Beta prior on each of those so as to learntheir values in the LEARN-?
experiments below.Finally, the observed unsegmented utterancesWi are generated by concatenating all Si,j usingthe function CAT.We briefly comment on the central intuitionof this model, i.e.
why it can infer underlying4For right there are three and for left-right six differentrules, one for every instantiation of the context-template.from surface forms.
Bayesian word segmentationmodels try to compactly represent the observeddata in terms of a small set of units (word types)and a short analysis (a small number of wordtokens).
Phonological rules such as /t/-deletioncan ?explain away?
an observed surface type suchas [wEs]] in terms of the underlying type /wEst/which is independently needed for surface tokensof [wEst].
Thus, the /t/?
?
rule makes possi-ble a smaller lexicon for a given number of sur-face tokens.
Obviously, human learners have ac-cess to additional cues, such as the meaning ofwords, knowledge of phonological similarity be-tween segments and so forth.
One of the advan-tages of an explicitly defined generative modelsuch as ours is that it is straight-forward to grad-ually extend it by adding more cues, as we pointout in the discussion.3.1 InferenceJust as for the Goldwater et al (2009) segmen-tation models, exact inference is infeasible forour joint model.
We extend the collapsed Gibbsbreakpoint-sampler described in Goldwater et al(2009) to perform inference for our extended mod-els.
We refer the reader to their paper for addi-tional details such as how to calculate the Bigramprobabilities in Figure 4.
Here we focus on therequired changes to the sampler so as to performinference under our richer model.
We consider thecase of a single surface string W , so we drop thei-index in the following discussion.Knowing W , the problem is to recover the un-derlying forms U1, .
.
.
, Un and the surface formsS1, .
.
.
, Sn for unknown n. A major insight inGoldwater?s work is that rather than sampling overthe latent variables in the model directly (the num-ber of which we don?t even know), we can insteadperform Gibbs sampling over a set of boundaryvariables b1, .
.
.
, b|W |?1 that jointly determine thevalues for our variables of interest where |W | isthe length of the surface string W .
For our model,each bj ?
{0, 1, t}, where bj = 0 indicates ab-sence of a word boundary, bj = 1 indicates pres-ence of a boundary and bj = t indicates pres-ence of a boundary with a preceeding underlying/t/.
The relation between the bj and the S1, .
.
.
, Snand U1, .
.
.
, Un is illustrated in Figure 5.
The re-quired sampling equations are given in Figure 4.1511P (bj = 0 | b?j) ?
P (w12,u | wl,u, b?j)?
Pr(w12,s | w12,u, wr,u)?
P (wr,u | w12,u, b?j ?
?wl,u, w12,u?)
(1)P (bj = t | b?j) ?
P (w1,t | wl,u, b?j)?
Pr(w1,s | w1,t, w2,u)?
P (w2,u | w1,t, b?j ?
?wl,u, w1,t?)?
Pr(w2,s | w2,u, wr,u)?
P (wr,u | w2,u, b?j ?
?wl,u, w1,t?
?
?w1,t, w2,u?)
(2)P (bj = 1 | b?j) ?
P (w1,s | wl,u, b?j)?
Pr(w1,s | w1,s, w2,u)?
P (w2,u | w1,s, b?j ?
?wl,u, w1,s?)?
Pr(w2,s | w2,u, wr,u)?
P (wr,u | w2,u, b?j ?
?wl,u, w1,s?
?
?w1,s, w2,u?)
(3)Figure 4: Sampling equations for our Gibbs sampler, see figure 5 for illustration.
bj = 0 correspondsto no boundary at this position, bj = t to a boundary with a preceeding underlying /t/ and bj = 1 to aboundary with no additional underlying /t/.
We use b?j for the statistics determined by all but the jthposition and b?j ?
?r, l?
for these statistics plus an additional count of the bigram ?r, l?.
P (w | l, b)refers to the bigram probability of ?l, w?
given the the statistics b; we refer the reader to Goldwater etal.
(2009) for the details of calculating these bigram probabilities and details about the required statisticsfor the collapsed sampler.
PR is defined in the text.1 10 t 1I h      i  i       t $underlyingsurfaceboundariesobserved I h i i t $I h      i       t  i       t $Figure 5: The relation between the observed se-quence of segments (bottom), the boundary vari-ables b1, .
.
.
, b|W |?1 the Gibbs sampler operatesover (in squares), the latent sequence of sur-face forms and the latent sequence of underly-ing forms.
When sampling a new value forb3 = t, the different word-variables in fig-ure 4 are: w12,u=w12,s=hiit, w1,t=hit and w1,s=hi,w2,u=w2,s=it, wl,u=I, wr,u=$.
Note that we needa boundary variable at the end of the utterance asthere might be an underlying /t/ at this position aswell.
The final boundary variable is set to 1, not t,because the /t/ in it is observed.4 Experiments4.1 The dataWe are interested in how well our model han-dles /t/-deletion in real data.
Ideally, we?d eval-uate it on CDS but as of now, we know of noavailable large enough corpus of accurately hand-transcribed CDS.
Instead, we used the BuckeyeCorpus (Pitt et al, 2007) for our experiments,a large ADS corpus of interviews with Englishspeakers that have been transcribed with relativelyfine phonetic detail, with /t/-deletion among thethings manually annotated.
Pointing to the re-cent work by Dilley et al (to appear) we wantto emphasize that the statistical distribution of /t/-deletion has been found to be similar for ADS andorthographic I don?t intend totranscript /aI R oU n I n t E n d @/idealized /aI d oU n t I n t E n d t U/t-drop /aI d oU n I n t E n d t U/Figure 6: An example fragment from the Buckeye-corpus in orthographic form, the fine transcriptavailable in the Buckeye corpus, a fully idealizedpronunciation with canonical dictionary pronunci-ations and our version of the data with dropped/t/s.CDS, at least for read speech.We automatically derived a corpus of 285,792word tokens across 48,795 utterances from theBuckeye Corpus by collecting utterances across allinterviews and heuristically splitting utterances atspeaker-turn changes and indicated silences.
TheBuckeye corpus lists for each word token a man-ually transcribed pronunciation in context as wellas its canonical pronunciation as given in a pro-nouncing dictionary.
As input to our model, weuse the canonical pronunciation unless the pronun-ciation in context indicates that the final /t/ hasbeen deleted in which case we also delete the final/t/ of the canonical pronunciation Figure 6 showsan example from the Buckeye Corpus, indicatinghow the original data, a fully idealized versionand our derived input that takes into account /t/-deletions looks like.Overall, /t/-deletion is a quite frequent phe-nomenon with roughly 29% of all underlying /t/sbeing dropped.
The probabilities become morepeaked when looking at finer context; see Table 3for the empirical distribution of /t/-dropping forthe six different contexts of the left-right setting.15124.2 Recovering deleted /t/s, given wordboundariesIn this set of experiments we are interested in howwell our model recovers /t/s when it is providedwith the gold word boundaries.
This allows usto investigate the strength of the statistical sig-nal for the deletion rule without confounding itwith the word segmentation performance, and tosee how the different contextual settings uniform,right and left-right handle the data.
Concretely,for the example in Figure 6 this means that we tellthe model that there are boundaries between /aI/,/doUn/, /IntEnd/, /tu/ and /liv/ but we don?t tell itwhether or not these words end in an underlying/t/.
Even in this simple example, there are 5 possi-ble positions for the model to posit an underlying/t/.
We evaluate the model in terms of F-score, theharmonic mean of recall (the fraction of underly-ing /t/s the model correctly recovered) and preci-sion (the fraction of underlying /t/s the model pre-dicted that were correct).In these experiments, we ran a total of 2500 it-erations with a burnin of 2000.
We collect sam-ples with a lag of 10 for the last 500 iterations andperform maximum marginal decoding over thesesamples (Johnson and Goldwater, 2009), as wellas running two chains so as to get an idea of thevariance.5We are also interested in how well the modelcan infer the rule probabilities from the data, thatis, whether it can learn values for the different ?cparameters.
We compare two settings, one wherewe perform inference for these parameters assum-ing a uniform Beta prior on each ?c (LEARN-?
)and one where we provide the model with the em-pirical probabilities for each ?c as estimated offthe gold-data (GOLD-?
), e.g., for the uniform con-dition 0.29.
The results are shown in Table 2.Best performance for both the Unigram andthe Bigram model in the GOLD-?
condition isachieved under the left-right setting, in line withthe standard analyses of /t/-deletion as primarilybeing determined by the preceding and the follow-ing context.
For the LEARN-?
condition, the Bi-gram model still performs best in the left-right set-ting but the Unigram model?s performance drops5As manually setting the hyper-parameters for the DPs inour model proved to be complicated and may be objected toon principled grounds, we perform inference for them undera vague gamma prior, as suggested by Teh et al (2006) andJohnson and Goldwater (2009), using our own implementa-tion of a slice-sampler (Neal, 2003).uniform right left-rightUnigram LEARN-?
56.52 39.28 23.59GOLD-?
62.08 60.80 66.15Bigram LEARN-?
60.85 62.98 77.76GOLD-?
69.06 69.98 73.45Table 2: F-score of recovered /t/s with knownword boundaries on real data for the three differ-ent context settings, averaged over two runs (allstandard errors below 2%).
Note how the Uni-gram model always suffers in the LEARN-?
condi-tion whereas the Bigram model?s performance isactually best for LEARN-?
in the left-right setting.C C C V C $ V C V V V $empirical 0.62 0.42 0.36 0.23 0.15 0.07Unigram 0.41 0.33 0.17 0.07 0.05 0.00Bigram 0.70 0.58 0.43 0.17 0.13 0.06Table 3: Inferred rule-probabilities for differentcontexts in the left-right setting from one of theruns.
?C C?
stands for the context where thedeleted /t/ is preceded and followed by a conso-nant, ?V $?
stands for the context where it is pre-ceded by a vowel and followed by the utteranceboundary.
Note how the Unigram model severelyunder-estimates and the Bigram model slightlyover-estimates the probabilities.in all settings and is now worst in the left-right andbest in the uniform setting.In fact, comparing the inferred probabilitiesto the ?ground truth?
indicates that the Bigrammodel estimates the true probabilities more ac-curately than the Unigram model, as illustratedin Table 3 for the left-right setting.
The Bi-gram model somewhat overestimates the probabil-ity for all post-consonantal contexts but the Uni-gram model severely underestimates the probabil-ity of /t/-deletion across all contexts.4.3 Artificial data experimentsTo test our Gibbs sampling inference procedure,we ran it on artificial data generated according tothe model itself.
If our inference procedure failsto recover the underlying /t/s accurately in this set-ting, we should not expect it to work well on actualdata.
We generated our artificial data as follows.We transformed the sequence of canonical pronun-ciations in the Buckeye corpus (which we take tobe underlying forms here) by randomly deletingfinal /t/s using empirical probabilities as shown inTable 3 to generate a sequence of artificial sur-face forms that serve as input to our models.
We1513uniform right left-rightUnigram LEARN-?
94.35 23.55 (+) 63.06GOLD-?
94.45 94.20 91.83Bigram LEARN-?
92.72 91.64 88.48GOLD-?
92.88 92.33 89.32Table 4: F-score of /t/-recovery with known wordboundaries on artificial data, each condition testedon data that corresponds to the assumption, aver-aged over two runs (standard errors less than 2%except (+) = 3.68%)).Unigram BigramLEARN-?
33.58 55.64GOLD-?
55.92 57.62Table 5: /t/-recovery F-scores when performingjoint word segmention in the left-right setting, av-eraged over two runs (standard errors less than2%).
See Table 6 for the corresponding segmenta-tion F-scores.did this for all three context settings, always es-timating the deletion probability for each contextfrom the gold-standard.
The results of these exper-iments are given in table 4.
Interestingly, perfor-mance on these artificial data is considerably bet-ter than on the real data.
In particular the Bigrammodel is able to get consistently high F-scores forboth the LEARN-?
and the GOLD-?
setting.
Forthe Unigram model, we again observe the severedrop in the LEARN-?
setting for the right and left-right settings although it does remarkably well inthe uniform setting, and performs well across allsettings in the GOLD-?
condition.
We take this toshow that our inference algorithm is in fact work-ing as expected.4.4 Segmentation experimentsFinally, we are also interested to learn how wellwe can do word segmentation and underlying /t/-recovery jointly.
Again, we look at both theLEARN-?
and GOLD-?
conditions but focus on theleft-right setting as this worked best in the exper-iments above.
For these experiments, we performsimulated annealing throughout the initial 2000 it-erations, gradually cooling the temperature from5 to 1, following the observation by Goldwateret al (2009) that without annealing, the Bigrammodel gets stuck in sub-optimal parts of the solu-tion space early on.
During the annealing stage,we prevent the model from performing inferencefor underlying /t/s so that the annealing stage canbe seen as an elaborate initialisation scheme, andwe perform joint inference for the remaining 500iterations, evaluating on the last sample and av-eraging over two runs.
As neither the Unigramnor the Bigram model performs ?perfect?
wordsegmentation, we expect to see a degradation in/t/-recovery performance and this is what we findindeed.
To give an impression of the impact of/t/-deletion, we also report numbers for runningonly the segmentation model on the Buckeye datawith no deleted /t/s and on the data with deleted/t/s.
The /t/-recovery scores are given in Table 5and segmentation scores in Table 6.
Again theUnigram model?s /t/-recovery score degrades dra-matically in the LEARN-?
condition.
Looking atthe segmentation performance this isn?t too sur-prising: the Unigram model?s poorer token F-score, the standard measure of segmentation per-formance on a word token level, suggests that itmisses many more boundaries than the Bigrammodel to begin with and, consequently, can?t re-cover any potential underlying /t/s at these bound-aries.
Also note that in the GOLD-?
condition, ourjoint Bigram model performs almost as well ondata with /t/-deletions as the word segmentationmodel on data that includes no variation at all.The generally worse performance of handlingvariation as measured by /t/-recovery F-scorewhen performing joint segmentation is consistentwith the finding of Elsner et al (2012) who reportconsiderable performance drops for their phono-logical learner when working with induced bound-aries (note, however, that their model does not per-form joint inference, rather the induced boundariesare given to their phonological learner as ground-truth).5 DiscussionThere are two interesting findings from our exper-iments.
First of all, we find a much larger differ-ence between the Unigram and the Bigram modelin the LEARN-?
condition than in the GOLD-?
con-dition.
We suggest that this is due to the Unigrammodel?s lack of dependencies between underlyingforms, depriving it of an important source of ev-idence.
Bigram dependencies provide additionalevidence for underlying /t/ that are deleted on thesurface, and because the Bigram model identifiesthese underlying /t/ more accurately, it can also es-timate the /t/ deletion probability more accurately.1514Unigram BigramLEARN-?
54.53 72.55 (2.3%)GOLD-?
54.51 73.18NO-?
54.61 70.12NO-VAR 54.12 73.99Table 6: Word segmentation F-scores for the /t/-recovery F-scores in Table 5 averaged over tworuns (standard errors less than 2% unless given).NO-?
are scores for running just the word segmen-tation model with no /t/-deletion rule on the datathat includes /t/-deletion, NO-VAR for running justthe word segmentation model on the data with no/t/-deletions.For example, /t/ dropping in ?don?t you?
yieldssurface forms ?don you?.
Because the word bi-gram probability P (you | don?t) is high, the bi-gram model prefers to analyse surface ?don?
asunderlying ?don?t?.
The Unigram model does nothave access to word bigram information so theunderlying forms it posits are less accurate (asshown in Table 2), and hence the estimate of the/t/-deletion probability is also less accurate.
Whenthe probabilities of deletion are pre-specified theUnigram model performs better but still consider-ably worse than the Bigram model when the wordboundaries are known, suggesting the importanceof non-phonological contextual effects that the Bi-gram model but not the Unigram model can cap-ture.
This suggests that for example word pre-dictability in context might be an important factorcontributing to /t/-deletion.The other striking finding is the considerabledrop in performance between running on natural-istic and artificially created data.
This suggeststhat the natural distribution of /t/-deletion is muchmore complex than can be captured by statisticsover the phonological contexts we examined.
Fol-lowing Guy (1991), a finer-grained distinction forthe preceeding segments might address this prob-lem.Yet another suggestion comes from the recentwork in Coetzee and Kawahara (2013) who claimthat ?
[a] model that accounts perfectly for theoverall rate of application of some variable pro-cess therefore does not necessarily account verywell for the actual application of the process to in-dividual words.?
They argue that in particular theextremely high deletion rates typical of high fre-quency items aren?t accurately captured when thedeletion probability is estimated across all types.A look at the error patterns of our model on a sam-ple from the Bigram model in the LEARN-?
settingon the naturalistic data suggests that this is in fact aproblem.
For example, the word ?just?
has an ex-tremely high rate of deletion with 17462442 = 0.71%.While many tokens of ?jus?
are ?explained away?through predicting underlying /t/s, the (literally)extra-ordinary frequency of ?jus?-tokens lets ourmodel still posit it as an underlying form, althoughwith a much dampened frequency (of the 1746 sur-face tokens, 1081 are analysed as being realiza-tions of an underlying ?just?
).The /t/-recovery performance drop when per-forming joint word segmentation isn?t surprisingas even the Bigram model doesn?t deliver a veryhigh-quality segmentation to begin with, leadingto both sparsity (through missed word-boundaries)and potential noise (through misplaced word-boundaries).
Using a more realistic generativeprocess for the underlying forms, for example anAdaptor Grammar (Johnson et al, 2007), couldaddress this shortcoming in future work withoutchanging the overall architecture of the model al-though novel inference algorithms might be re-quired.6 Conclusion and outlookWe presented a joint model for word segmentationand the learning of phonological rule probabili-ties from a corpus of transcribed speech.
We findthat our Bigram model reaches 77% /t/-recoveryF-score when run with knowledge of true word-boundaries and when it can make use of both thepreceeding and the following phonological con-text, and that unlike the Unigram model it is ableto learn the probability of /t/-deletion in differentcontexts.
When performing joint word segmen-tation on the Buckeye corpus, our Bigram modelreaches around above 55% F-score for recoveringdeleted /t/s with a word segmentation F-score ofaround 72% which is 2% better than running a Bi-gram model that does not model /t/-deletion.We identified additional factors that might helphandling /t/-deletion and similar phenomena.
Amajor advantage of our generative model is theease and transparency with which its assump-tions can be modified and extended.
For fu-ture work we plan to incorporate into our modelricher phonological contexts, item- and frequency-specific probabilities and more direct use of word1515predictability.
We also plan to extend our modelto handle additional phenomena, an obvious can-didate being /d/-deletion.Also, the two-level architecture we present isnot limited to the mapping being defined in termsof rules rather than constraints in the spirit of Op-timality Theory (Prince and Smolensky, 2004); weplan to explore this alternative path as well in fu-ture work.To conclude, we presented a model that pro-vides a clean framework to test the usefulness ofdifferent factors for word segmentation and han-dling phonological variation in a controlled man-ner.AcknowledgementsWe thank the anonymous reviewers for theirvaluable comments.
This research was sup-ported under Australian Research Council?s Dis-covery Projects funding scheme (project numbersDP110102506 and DP110102593).ReferencesNoam Chomsky and Morris Halle.
1968.
The SoundPattern of English.
Haper & Row, New York.Andries W. Coetzee and Shigeto Kawahara.
2013.
Fre-quency biases in phonological variation.
NaturalLanguage and Linguisic Theory, 31:47?89.Andries W. Coetzee.
2004.
What it Means to be aLoser: Non-Optimal Candidates in Optimality The-ory.
Ph.D. thesis, University of Massachusetts ,Amherst.Laura Dilley, Amanda Millett, J. Devin McAuley, andTonya R. Bergeson.
to appear.
Phonetic variationin consonants in infant-directed and adult-directedspeech: The case of regressive place assimilationin word-final alveolar stops.
Journal of Child Lan-guage.Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.2012.
Bootstrapping a unified model of lexical andphonetic acquisition.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics, pages 184?193, Jeju Island, Korea.
As-sociation for Computational Linguistics.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2009.
A Bayesian framework for wordsegmentation: Exploring the effects of context.Cognition, 112(1):21?54.Gregory R. Guy.
1991.
Contextual conditioning invariable lexical phonology.
Language Variation andChange, 3(2):223?39.Mark Johnson and Sharon Goldwater.
2009.
Im-proving nonparameteric Bayesian inference: exper-iments on unsupervised word segmentation withadaptor grammars.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associa-tion for Computational Linguistics, pages 317?325,Boulder, Colorado, June.
Association for Computa-tional Linguistics.Mark Johnson, Thomas L. Griffiths, and Sharon Gold-water.
2007.
Adaptor Grammars: A framework forspecifying compositional nonparametric Bayesianmodels.
In B. Scho?lkopf, J. Platt, and T. Hoffman,editors, Advances in Neural Information ProcessingSystems 19, pages 641?648.
MIT Press, Cambridge,MA.Mark Johnson.
2008.
Using Adaptor Grammars toidentify synergies in the unsupervised acquisition oflinguistic structure.
In Proceedings of the 46th An-nual Meeting of the Association of ComputationalLinguistics, pages 398?406, Columbus, Ohio.
Asso-ciation for Computational Linguistics.Jason Naradowsky and Sharon Goldwater.
2009.
Im-proving morphology induction by learning spellingrules.
In Proceedings of the 21st international jontconference on Artifical intelligence, pages 1531?1536, San Francisco, CA, USA.
Morgan KaufmannPublishers Inc.Radford M. Neal.
2003.
Slice sampling.
Annals ofStatistics, 31:705?767.Dennis Norris, James M. Mcqueen, Anne Cutler, andSally Butterfield.
1997.
The possible-word con-straint in the segmentation of continuous speech.Cognitive Psychology, 34(3):191 ?
243.Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-ling, William Raymond, Elizabeth Hume, and EricFosler-Lussier.
2007.
Buckeye corpus of conversa-tional speech.Alan Prince and Paul Smolensky.
2004.
OptimalityTheory: Constraint Interaction in Generative Gram-mar.
Blackwell.Yee Whye Teh, Michael Jordan, Matthew Beal, andDavid Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101:1566?1581.1516
