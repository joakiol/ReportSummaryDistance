Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1177?1187,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLow-Resource Semantic Role LabelingMatthew R. Gormley1Margaret Mitchell2Benjamin Van Durme1Mark Dredze11Human Language Technology Center of ExcellenceJohns Hopkins University, Baltimore, MD 212112Microsoft ResearchRedmond, WA 98052mrg@cs.jhu.edu | memitc@microsoft.com | vandurme@cs.jhu.edu | mdredze@cs.jhu.eduAbstractWe explore the extent to which high-resource manual annotations such as tree-banks are necessary for the task of se-mantic role labeling (SRL).
We examinehow performance changes without syntac-tic supervision, comparing both joint andpipelined methods to induce latent syn-tax.
This work highlights a new applica-tion of unsupervised grammar inductionand demonstrates several approaches toSRL in the absence of supervised syntax.Our best models obtain competitive resultsin the high-resource setting and state-of-the-art results in the low resource setting,reaching 72.48% F1 averaged across lan-guages.
We release our code for this workalong with a larger toolkit for specifyingarbitrary graphical structure.11 IntroductionThe goal of semantic role labeling (SRL) is toidentify predicates and arguments and label theirsemantic contribution in a sentence.
Such labelingdefines who did what to whom, when, where andhow.
For example, in the sentence ?The kids ranthe marathon?, ran assigns a role to kids to denotethat they are the runners; and a role to marathon todenote that it is the race course.Models for SRL have increasingly come to relyon an array of NLP tools (e.g., parsers, lem-matizers) in order to obtain state-of-the-art re-sults (Bj?orkelund et al, 2009; Zhao et al, 2009).Each tool is typically trained on hand-annotateddata, thus placing SRL at the end of a very high-resource NLP pipeline.
However, richly annotateddata such as that provided in parsing treebanks isexpensive to produce, and may be tied to specificdomains (e.g., newswire).
Many languages do1http://www.cs.jhu.edu/?mrg/software/not have such supervised resources (low-resourcelanguages), which makes exploring SRL cross-linguistically difficult.The problem of SRL for low-resource lan-guages is an important one to solve, as solutionspave the way for a wide range of applications: Ac-curate identification of the semantic roles of enti-ties is a critical step for any application sensitive tosemantics, from information retrieval to machinetranslation to question answering.In this work, we explore models that minimizethe need for high-resource supervision.
We ex-amine approaches in a joint setting where wemarginalize over latent syntax to find the optimalsemantic role assignment; and a pipeline settingwhere we first induce an unsupervised grammar.We find that the joint approach is a viable alterna-tive for making reasonable semantic role predic-tions, outperforming the pipeline models.
Thesemodels can be effectively trained with access toonly SRL annotations, and mark a state-of-the-artcontribution for low-resource SRL.To better understand the effect of the low-resource grammars and features used in thesemodels, we further include comparisons with (1)models that use higher-resource versions of thesame features; (2) state-of-the-art high resourcemodels; and (3) previous work on low-resourcegrammar induction.
In sum, this paper makesseveral experimental and modeling contributions,summarized below.Experimental contributions:?
Comparison of pipeline and joint models forSRL.?
Subtractive experiments that consider the re-moval of supervised data.?
Analysis of the induced grammars in un-supervised, distantly-supervised, and jointtraining settings.1177Modeling contributions:?
Simpler joint CRF for syntactic and semanticdependency parsing than previously reported.?
New application of unsupervised grammarinduction: low-resource SRL.?
Constrained grammar induction using SRLfor distant-supervision.?
Use of Brown clusters in place of POS tagsfor low-resource SRL.The pipeline models are introduced in ?
3.1 andjointly-trained models for syntactic and semanticdependencies (similar in form to Naradowsky etal.
(2012)) are introduced in ?
3.2.
In the pipelinemodels, we develop a novel approach to unsu-pervised grammar induction and explore perfor-mance using SRL as distant supervision.
The jointmodels use a non-loopy conditional random field(CRF) with a global factor constraining latent syn-tactic edge variables to form a tree.
Efficient exactmarginal inference is possible by embedding a dy-namic programming algorithm within belief prop-agation as in Smith and Eisner (2008).Even at the expense of no dependency path fea-tures, the joint models best pipeline-trained mod-els for state-of-the-art performance in the low-resource setting (?
4.4).
When the models have ac-cess to observed syntactic trees, they achieve nearstate-of-the-art accuracy in the high-resource set-ting on some languages (?
4.3).Examining the learning curve of the joint andpipeline models in two languages demonstratesthat a small number of labeled SRL examples maybe essential for good end-task performance, butthat the choice of a good model for grammar in-duction has an even greater impact.2 Related WorkOur work builds upon research in both seman-tic role labeling and unsupervised grammar in-duction (Klein and Manning, 2004; Spitkovskyet al, 2010a).
Previous related approaches to se-mantic role labeling include joint classification ofsemantic arguments (Toutanova et al, 2005; Jo-hansson and Nugues, 2008), latent syntax induc-tion (Boxwell et al, 2011; Naradowsky et al,2012), and feature engineering for SRL (Zhao etal., 2009; Bj?orkelund et al, 2009).Toutanova et al (2005) introduced one ofthe first joint approaches for SRL and demon-strated that a model that scores the full predicate-argument structure of a parse tree could lead tosignificant error reduction over independent clas-sifiers for each predicate-argument relation.Johansson and Nugues (2008) and Llu?
?s et al(2013) extend this idea by coupling predictions ofa dependency parser with predictions from a se-mantic role labeler.
In the model from Johans-son and Nugues (2008), the outputs from an SRLpipeline are reranked based on the full predicate-argument structure that they form.
The candidateset of syntactic-semantic structures is reranked us-ing the probability of the syntactic tree and seman-tic structure.
Llu?
?s et al (2013) use a joint arc-factored model that predicts full syntactic pathsalong with predicate-argument structures via dualdecomposition.Boxwell et al (2011) and Naradowsky et al(2012) observe that syntax may be treated as la-tent when a treebank is not available.
Boxwellet al (2011) describe a method for training a se-mantic role labeler by extracting features from apacked CCG parse chart, where the parse weightsare given by a simple ruleset.
Naradowsky etal.
(2012) marginalize over latent syntactic depen-dency parses.Both Boxwell et al (2011) and Naradowskyet al (2012) suggest methods for SRL withoutsupervised syntax, however, their features comelargely from supervised resources.
Even in theirlowest resource setting, Boxwell et al (2011) re-quire an oracle CCG tag dictionary extracted froma treebank.
Naradowsky et al (2012) limit theirexploration to a small set of basic features, andincluded high-resource supervision in the formof lemmas, POS tags, and morphology availablefrom the CoNLL 2009 data.There has not yet been a comparison of tech-niques for SRL that do not rely on a syntactictreebank, and no exploration of probabilistic mod-els for unsupervised grammar induction within anSRL pipeline that we have been able to find.Related work for the unsupervised learning ofdependency structures separately from semanticroles primarily comes from Klein and Manning(2004), who introduced the Dependency Modelwith Valence (DMV).
This is a robust generativemodel that uses a head-outward process over wordclasses, where heads generate arguments.Spitkovsky et al (2010a) show that Viterbi(hard) EM training of the DMV with simple uni-form initialization of the model parameters yieldshigher accuracy models than standard soft-EM1178ParsingModel SemanticDependencyModelCorpusTextText LabeledWith SemanticRolesTrain Time, Constrained Grammar Induction:Observed ConstraintsFigure 1: Pipeline approach to SRL.
In this sim-ple pipeline, the first stage syntactically parses thecorpus, and the second stage predicts semanticpredicate-argument structure for each sentence us-ing the labels of the first stage as features.
In ourlow-resource pipelines, we assume that the syntac-tic parser is given no labeled parses?however, itmay optionally utilize the semantic parses as dis-tant supervision.
Our experiments also consider?longer?
pipelines that include earlier stages: amorphological analyzer, POS tagger, lemmatizer.training.
In Viterbi EM, the E-step finds the max-imum likelihood corpus parse given the currentmodel parameters.
The M-step then finds themaximum likelihood parameters given the corpusparse.
We utilize this approach to produce unsu-pervised syntactic features for the SRL task.Grammar induction work has further demon-strated that distant supervision in the form ofACE-style relations (Naseem and Barzilay, 2011)or HTML markup (Spitkovsky et al, 2010b)can lead to considerable gains.
Recent work infully unsupervised dependency parsing has sup-planted these methods with even higher accuracies(Spitkovsky et al, 2013) by arranging optimiz-ers into networks that suggest informed restartsbased on previously identified local optima.
We donot reimplement these approaches within the SRLpipeline here, but provide comparison of thesemethods against our grammar induction approachin isolation in ?
4.5.In both pipeline and joint models, we use fea-tures adapted from state-of-the-art approaches toSRL.
This includes Zhao et al (2009) features,who use feature templates from combinationsof word properties, syntactic positions includinghead and children, and semantic properties; andfeatures from Bj?orkelund et al (2009), who utilizefeatures on syntactic siblings and the dependencypath concatenated with the direction of each edge.Features are described further in ?
3.3.3 ApproachesWe consider an array of models, varying:1.
Pipeline vs. joint training (Figures 1 and 2)2.
Types of supervision3.
The objective function at the level of syntax3.1 Unsupervised Syntax in the PipelineTypical SRL systems are trained following apipeline where the first component is trained onsupervised data, and each subsequent componentis trained using the 1-best output of the previouscomponents.
A typical pipeline consists of a POStagger, dependency parser, and semantic role la-beler.
In this section, we introduce pipelines thatremove the need for a supervised tagger and parserby training in an unsupervised and distantly super-vised fashion.Brown Clusters We use fully unsupervisedBrown clusters (Brown et al, 1992) in place ofPOS tags.
Brown clusters have been used to goodeffect for various NLP tasks such as named entityrecognition (Miller et al, 2004) and dependencyparsing (Koo et al, 2008; Spitkovsky et al, 2011).The clusters are formed by a greedy hierachi-cal clustering algorithm that finds an assignmentof words to classes by maximizing the likelihoodof the training data under a latent-class bigrammodel.
Each word type is assigned to a fine-grained cluster at a leaf of the hierarchy of clusters.Each cluster can be uniquely identified by the pathfrom the root cluster to that leaf.
Representing thispath as a bit-string (with 1 indicating a left and 0indicating a right child) allows a simple coarsen-ing of the clusters by truncating the bit-strings.
Wetrain 1000 Brown clusters for each of the CoNLL-2009 languages on Wikipedia text.2Unsupervised Grammar Induction Our firstmethod for grammar induction is fully unsuper-vised Viterbi EM training of the DependencyModel with Valence (DMV) (Klein and Manning,2004), with uniform initialization of the model pa-rameters.
We define the DMV such that it gener-ates sequences of word classes: either POS tagsor Brown clusters as in Spitkovsky et al (2011).The DMV is a simple generative model for pro-jective dependency trees.
Children are generatedrecursively for each node.
Conditioned on the par-ent class, the direction (right or left), and the cur-rent valence (first child or not), a coin is flipped todecide whether to generate another child; the dis-tribution over child classes is conditioned on onlythe parent class and direction.2The Wikipedia text was tokenized for Polyglot (Al-Rfou?et al, 2013): http://bit.ly/embeddings1179Constrained Grammar Induction Our secondmethod, which we will refer to as DMV+C, in-duces grammar in a distantly supervised fashionby using a constrained parser in the E-step ofViterbi EM.
Since the parser is part of a pipeline,we constrain it to respect the downstream SRL an-notations during training.
At test time, the parseris unconstrained.Dependency-based semantic role labeling canbe described as a simple structured predictionproblem: the predicted structure is a labeled di-rected graph, where nodes correspond to wordsin the sentence.
Each directed edge indicates thatthere is a predicate-argument relationship betweenthe two words; the parent is the predicate and thechild the argument.
The label on the edge indi-cates the type of semantic relationship.
Unlikesyntactic dependency parsing, the graph is not re-quired to be a tree, nor even a connected graph.Self-loops and crossing arcs are permitted.The constrained syntactic DMV parser treatsthe semantic graph as observed, and constrains thesyntactic parent to be chosen from one of the se-mantic parents, if there are any.
In some cases,imposing this constraint would not permit any pro-jective dependency parses?in this case, we ignorethe semantic constraint for that sentence.
We parsewith the CKY algorithm (Younger, 1967; Aho andUllman, 1972) by utilizing a PCFG correspondingto the DMV (Cohn et al, 2010).
Each chart cell al-lows only non-terminals compatible with the con-strained sets.
This can be viewed as a variation ofPereira and Schabes (1992).Semantic Dependency Model As describedabove, semantic role labeling can be cast as astructured prediction problem where the structureis a labeled semantic dependency graph.
We de-fine a conditional random field (CRF) (Lafferty etal., 2001) for this task.
Because each word in asentence may be in a semantic relationship withany other word (including itself), a sentence oflength n has n2possible edges.
We define a singleL+1-ary variable for each edge, whose value canbe any of L semantic labels or a special label indi-cating there is no predicate-argument relationshipbetween the two words.
In this way, we jointlyperform identification (determining whether a se-mantic relationship exists) and classification (de-termining the semantic label).
This use of an L+1-ary variable is in contrast to the model of Narad-owsky et al (2012), which used a more complexDEPTREEDep1,1Role1,1Role1,2Role1,3Rolen,nDep1,2Dep1,3Depn,n ......Figure 2: Factor graph for the joint syntac-tic/semantic dependency parsing model.set of binary variables and required a constraintfactor permitting AT-MOST-ONE.
We include oneunary factor for each variable.We optionally include additional variables thatperform word sense disambiguation for each pred-icate.
Each has a unary factor and is completelydisconnected from the semantic edge (similar toNaradowsky et al (2012)).
These variables rangeover all the predicate senses observed in the train-ing data for the lemma of that predicate.3.2 Joint Syntactic and Semantic ParsingModelIn Section 3.1, we introduced pipeline-trainedmodels for SRL, which used grammar inductionto predict unlabeled syntactic parses.
In this sec-tion, we define a simple model for joint syntacticand semantic dependency parsing.This model extends the CRF model in Section3.1 to include the projective syntactic dependencyparse for a sentence.
This is done by includ-ing an additional n2binary variables that indicatewhether or not a directed syntactic dependencyedge exists between a pair of words in the sen-tence.
Unlike the semantic dependencies, thesesyntactic variables must be coupled so that theyproduce a projective dependency parse; this re-quires an additional global constraint factor to en-sure that this is the case (Smith and Eisner, 2008).The constraint factor touches all n2syntactic-edgevariables, and multiplies in 1.0 if they form a pro-jective dependency parse, and 0.0 otherwise.
Wecouple each syntactic edge variable to its semanticedge variable with a binary factor.
Figure 2 showsthe factor graph for this joint model.Note that our factor graph does not contain anyloops, thereby permitting efficient exact marginalinference just as in Naradowsky et al (2012).
We1180Property Possible values1 word form all word forms2 lower case word form all lower-case forms3 5-char word form prefixes all 5-char form prefixes4 capitalization True, False5 top-800 word form top-800 word forms6 brown cluster 000, 1100, 010110001, ...7 brown cluster, length 5 length 5 prefixes of brown clusters8 lemma all word lemmas9 POS tag NNP, CD, JJ, DT, ...10 morphological features Gender, Case, Number, ...(different across languages)11 dependency label SBJ, NMOD, LOC, ...12 edge direction Up, DownTable 1: Word and edge properties in templates.i, i-1, i+1 noFarChildren(wi) linePath(wp, wc)parent(wi) rightNearSib(wi) depPath(wp, wc)allChildren(wi) leftNearSib(wi) depPath(wp, wlca)rightNearChild(wi) firstVSupp(wi) depPath(wc, wlca)rightFarChild(wi) lastVSupp(wi) depPath(wlca, wroot)leftNearChild(wi) firstNSupp(wi)leftFarChild(wi) lastNSupp(wi)Table 2: Word positions used in templates.
Basedon current word position (i), positions related tocurrent word wi, possible parent, child (wp, wc),lowest common ancestor between parent/child(wlca), and syntactic root (wroot).train our CRF models by maximizing conditionallog-likelihood using stochastic gradient descentwith an adaptive learning rate (AdaGrad) (Duchiet al, 2011) over mini-batches.The unary and binary factors are defined withexponential family potentials.
In the next section,we consider binary features of the observations(the sentence and labels from previous pipelinestages) which are conjoined with the state of thevariables in the factor.3.3 Features for CRF ModelsOur feature design stems from two key ideas.First, for SRL, it has been observed that fea-ture bigrams (the concatenation of simple fea-tures such as a predicate?s POS tag and an ar-gument?s word) are important for state-of-the-art(Zhao et al, 2009; Bj?orkelund et al, 2009).
Sec-ond, for syntactic dependency parsing, combiningBrown cluster features with word forms or POStags yields high accuracy even with little trainingdata (Koo et al, 2008).We create binary indicator features for eachmodel using feature templates.
Our feature tem-plate definitions build from those used by the topperforming systems in the CoNLL-2009 SharedTask, Zhao et al (2009) and Bj?orkelund et al(2009) and from features in syntactic dependencyparsing (McDonald et al, 2005; Koo et al, 2008).Template Possible valuesrelative position before, after, ondistance, continuity Z+binned distance > 2, 5, 10, 20, 30, or 40geneological relationship parent, child, ancestor, descendantpath-grams the NN wentTable 3: Additional standalone templates.Template Creation Feature templates are de-fined over triples of ?property, positions, order?.Properties, listed in Table 1, are extracted fromword positions within the sentence, shown in Ta-ble 2.
Single positions for a word wiincludeits syntactic parent, its leftmost farthest child(leftFarChild), its rightmost nearest sibling (rightNearSib),etc.
Following Zhao et al (2009), we include thenotion of verb and noun supports and sections ofthe dependency path.
Also following Zhao et al(2009), properties from a set of positions can beput together in three possible orders: as the givensequence, as a sorted list of unique strings, and re-moving all duplicated neighbored strings.
We con-sider both template unigrams and bigrams, com-bining two templates in sequence.Additional templates we include are the relativeposition (Bj?orkelund et al, 2009), geneological re-lationship, distance (Zhao et al, 2009), and binneddistance (Koo et al, 2008) between two words inthe path.
From Llu?
?s et al (2013), we use 1, 2, 3-gram path features of words/POS tags (path-grams),and the number of non-consecutive token pairs ina predicate-argument path (continuity).3.4 Feature SelectionConstructing all feature template unigrams and bi-grams would yield an unwieldy number of fea-tures.
We therefore determine the top N templatebigrams for a dataset and factor a according to aninformation gain measure (Martins et al, 2011):IGa,m=?f?Tm?xap(f, xa) log2p(f, xa)p(f)p(xa)where Tmis the mth feature template, f is a par-ticular instantiation of that template, and xais anassignment to the variables in factor a.
The proba-bilities are empirical estimates computed from thetraining data.
This is simply the mutual informa-tion of the feature template instantiation with thevariable assignment.This filtering approach was treated as a sim-ple baseline in Martins et al (2011) to contrastwith increasingly popular gradient based regular-ization approaches.
Unlike the gradient based ap-1181proaches, this filtering approach easily scales tomany features since we can decompose the mem-ory usage over feature templates.As an additional speedup, we reduce the dimen-sionality of our feature space to 1 million for eachclique using a common trick referred to as fea-ture hashing (Weinberger et al, 2009): we mapeach feature instantiation to an integer using a hashfunction3modulo the desired dimentionality.4 ExperimentsWe are interested in the effects of varied super-vision using pipeline and joint training for SRL.To compare to prior work (i.e., submissions to theCoNLL-2009 Shared Task), we also consider thejoint task of semantic role labeling and predicatesense disambiguation.
Our experiments are sub-tractive, beginning with all supervision availableand then successively removing (a) dependencysyntax, (b) morphological features, (c) POS tags,and (d) lemmas.
Dependency syntax is the mostexpensive and difficult to obtain of these variousforms of supervision.
We explore the importanceof both the labels and structure, and what quantityof supervision is useful.4.1 DataThe CoNLL-2009 Shared Task (Haji?c et al, 2009)dataset contains POS tags, lemmas, morpholog-ical features, syntactic dependencies, predicatesenses, and semantic roles annotations for 7 lan-guages: Catalan, Chinese, Czech, English, Ger-man, Japanese,4Spanish.
The CoNLL-2005 and-2008 Shared Task datasets provide English SRLannotation, and for cross dataset comparability weconsider only verbal predicates (more details in?
4.4).
To compare with prior approaches that usesemantic supervision for grammar induction, weutilize Section 23 of the WSJ portion of the PennTreebank (Marcus et al, 1993).4.2 Feature Template SetsOur primary feature set IGCconsists of 127 tem-plate unigrams that emphasize coarse properties(i.e., properties 7, 9, and 11 in Table 1).
We alsoexplore the 31 template unigrams5IGBdescribed3To reduce hash collisions, We use MurmurHash v3https://code.google.com/p/smhasher.4We do not report results on Japanese as that data wasonly made freely available to researchers that competed inCoNLL 2009.5Because we do not include a binary factor between pred-icate sense and semantic role, we do not include sense as aby Bj?orkelund et al (2009).
Each of IGCand IGBalso include 32 template bigrams selected by in-formation gain on 1000 sentences?we select adifferent set of template bigrams for each dataset.We compare against the language-specific fea-ture sets detailed in the literature on high-resourcetop-performing SRL systems: From Bj?orkelund etal.
(2009), these are feature sets for German, En-glish, Spanish and Chinese, obtained by weeks offorward selection (Bde,en,es,zh); and from Zhao etal.
(2009), these are features for Catalan Zca.64.3 High-resource SRLWe first compare our models trained as a pipeline,using all available supervision (syntax, morphol-ogy, POS tags, lemmas) from the CoNLL-2009data.
Table 4(a) shows the results of our modelwith gold syntax and a richer feature set thanthat of Naradowsky et al (2012), which onlylooked at whether a syntactic dependency edgewas present.
This highlights an important advan-tage of the pipeline trained model: the features canconsider any part of the syntax (e.g., arbitrary sub-trees), whereas the joint model is limited to thosefeatures over which it can efficiently marginalize(e.g., short dependency paths).
This holds trueeven in the pipeline setting where no syntactic su-pervision is available.Table 4(b) contrasts our high-resource resultsfor the task of SRL and sense disambiguationwith the top systems in the CoNLL-2009 SharedTask, giving further insight into the performanceof the simple information gain feature selectiontechnique.
With supervised syntax, our sim-ple information gain feature selection technique(?
3.4) performs admirably.
However, the orig-inal unigram Bj?orkelund features (Bde,en,es,zh),which were tuned for a high-resource model, ob-tain higher F1 than our information gain set us-ing the same features in unigram and bigram tem-plates (IGB).
This suggests that further work onfeature selection may improve the results.
Wefind that IGBobtain higher F1 than the originalBj?orkelund feature sets (Bde,en,es,zh) in the low-resource pipeline setting with constrained gram-mar induction (DMV+C).feature for argument prediction.6This covers all CoNLL languages but Czech, where fea-ture sets were not made publicly available in either work.
InCzech, we disallowed template bigrams involving path-grams.1182(a)(b)(c)SRL Approach Feature Set Dep.
Parser Avg.
ca cs de en es zhPipeline IGCGold 84.98 84.97 87.65 79.14 86.54 84.22 87.35Pipeline IGBGold 84.74 85.15 86.64 79.50 85.77 84.40 86.95Naradowsky et al (2012) Gold 72.73 69.59 74.84 66.49 78.55 68.93 77.97Bj?orkelund et al (2009) Supervised 81.55 80.01 85.41 79.71 85.63 79.91 78.60Zhao et al (2009) Supervised 80.85 80.32 85.19 75.99 85.44 80.46 77.72Pipeline IGCSupervised 78.03 76.24 83.34 74.19 81.96 76.12 76.35Pipeline ZcaSupervised *77.62 77.62 ?
?
?
?
?Pipeline Bde,en,es,zhSupervised *76.49 ?
?
72.17 81.15 76.65 75.99Pipeline IGBSupervised 75.68 74.59 81.61 69.08 78.86 74.51 75.44Joint IGCMarginalized 72.48 71.35 81.03 65.15 76.16 71.03 70.14Joint IGBMarginalized 72.40 71.55 80.04 64.80 75.57 71.21 71.21Naradowsky et al (2012) Marginalized 71.27 67.99 73.16 67.26 76.12 66.74 76.32Pipeline IGCDMV+C (bc) 70.08 68.21 79.63 62.25 73.81 68.73 67.86Pipeline ZcaDMV+C (bc) *69.67 69.67 ?
?
?
?
?Pipeline IGCDMV (bc) 69.26 68.04 79.58 58.47 74.78 68.36 66.35Pipeline IGBDMV (bc) 66.81 63.31 77.38 59.91 72.02 65.96 62.28Pipeline IGBDMV+C (bc) 65.61 61.89 77.48 58.97 69.11 63.31 62.92Pipeline Bde,en,es,zhDMV+C (bc) *63.06 ?
?
57.75 68.32 63.70 62.45Table 4: Test F1 for SRL and sense disambiguation on CoNLL?09 in high-resource and low-resourcesettings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax.
Results areranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
*Indicates partial averages for the language-specific feature sets (Zcaand Bde,en,es,zh), for which we show results only on thelanguages for which the sets were publicly available.traintest2008heads2005spans2005spans(oracletree)X PRY?082005spans84.32 79.44 B?11 (tdc) ?
71.5 B?11 (td) ?
65.0X JN?082008heads85.93 79.90 Joint, IGC72.9 35.0 72.0 Joint, IGB67.3 37.8 67.1Table 5: F1 for SRL approaches (without sensedisambiguation) in matched and mismatchedtrain/test settings for CoNLL 2005 span and 2008head supervision.
We contrast low-resource ()and high-resource settings (X), where latter uses atreebank.
See ?
4.4 for caveats to this comparison.4.4 Low-Resource SRLCoNLL-2009 Table 4(c) includes results for ourlow-resource approaches and Naradowsky et al(2012) on predicting semantic roles as well assense.
In the low-resource setting of the CoNLL-2009 Shared task without syntactic supervision,our joint model (Joint) with marginalized syntaxobtains state-of-the-art results with features IGCdescribed in ?
4.2.
This model outperforms priorwork (Naradowsky et al, 2012) and our pipelinemodel (Pipeline) with contrained (DMV+C) andunconstrained grammar induction (DMV) trainedon brown clusters (bc).In the low-resource setting, training and decod-ing times for the pipeline and joint methods aresimilar as computation time tends to be dominatedby feature extraction.These results begin to answer a key researchquestion in this work: The joint models outper-form the pipeline models in the low-resource set-ting.
This holds even when using the same featureselection process.
Further, the best-performinglow-resource features found in this work are thosebased on coarse feature templates and selectedby information gain.
Templates for these fea-tures generalize well to the high-resource setting.However, analysis of the induced grammars inthe pipeline setting suggests that the book is notclosed on the issue.
We return to this in ?
4.5.CoNLL-2008, -2005 To finish out comparisonswith state-of-the-art SRL, we contrast our ap-proach with that of Boxwell et al (2011), whoevaluate on SRL in isolation (without sense disam-biguation, as in CoNLL-2009).
They report resultson Prop-CCGbank (Boxwell and White, 2008),which uses the same training/testing splits as theCoNLL-2005 Shared Task.
Their results are there-fore loosely7comparable to results on the CoNLL-2005 dataset, which we can compare here.There is an additional complication in com-paring SRL approaches directly: The CoNLL-2005 dataset defines arguments as spans instead of7The comparison is imperfect for two reasons: first, theCCGBank contains only 99.44% of the original PTB sen-tences (Hockenmaier and Steedman, 2007); second, becausePropBank was annotated over CFGs, after converting to CCGonly 99.977% of the argument spans were exact matches(Boxwell and White, 2008).
However, this comparison wasadopted by Boxwell et al (2011), so we use it here.1183heads, which runs counter to our head-based syn-tactic representation.
This creates a mismatchedtrain/test scenario: we must train our model to pre-dict argument heads, but then test on our modelsability to predict argument spans.8We thereforetrain our models on the CoNLL-2008 argumentheads,9and post-process and convert from headsto spans using the conversion algorithm availablefrom Johansson and Nugues (2008).10The headsare either from an MBR tree or an oracle tree.
Thisgives Boxwell et al (2011) the advantage, sinceour syntactic dependency parses are optimized topick out semantic argument heads, not spans.Table 5 presents our results.
Boxwell et al(2011) (B?11) uses additional supervision in theform of a CCG tag dictionary derived from su-pervised data with (tdc) and without (tc) a cut-off.
Our model does very poorly on the ?05 span-based evaluation because the constituent bracket-ing of the marginalized trees are inaccurate.
Thisis elucidated by instead evaluating on the ora-cle spans, where our F1 scores are higher thanBoxwell et al (2011).
We also contrast with rela-vant high-resource methods with span/head con-versions from Johansson and Nugues (2008): Pun-yakanok et al (2008) (PRY?08) and Johansson andNugues (2008) (JN?08).Subtractive Study In our subsequent experi-ments, we study the effectiveness of our modelsas the available supervision is decreased.
We in-crementally remove dependency syntax, morpho-logical features, POS tags, then lemmas.
For theseexperiments, we utilize the coarse-grained featureset (IGC), which includes Brown clusters.Across languages, we find the largest drop inF1 when we remove POS tags; and we find again in F1 when we remove lemmas.
This indi-cates that lemmas, which are a high-resource an-notation, may not provide a significant benefit forthis task.
The effect of removing morphologicalfeatures is different across languages, with littlechange in performance for Catalan and Spanish,8We were unable to obtain the system output of Boxwellet al (2011) in order to convert their spans to dependenciesand evaluate the other mismatched train/test setting.9CoNLL-2005, -2008, and -2009 were derived from Prop-Bank and share the same source text; -2008 and -2009 useargument heads.10Specifically, we use their Algorithm 2, which producesthe span dominated by each argument, with special handlingof the case when the argument head dominates that of thepredicate.
Also following Johansson and Nugues (2008), werecover the ?05 sentences missing from the ?08 evaluation set.Rem #FT ca de es?
127+32 74.46 72.62 74.23Dep 40+32 67.43 64.24 67.18Mor 30+32 67.84 59.78 66.94POS 23+32 64.40 54.68 62.71Lem 21+32 64.85 54.89 63.80Table 6: Subtractive experiments.
Each row con-tains the F1 for SRL only (without sense disam-biguation) where the supervision type of that rowand all above it have been removed.
Removed su-pervision types (Rem) are: syntactic dependencies(Dep), morphology (Mor), POS tags (POS), andlemmas (Lem).
#FT indicates the number of fea-ture templates used (unigrams+bigrams).2030405060700 20000 40000 60000Number of Training SentencesLabeledF1Language / Dependency ParserCatalan / MarginalizedCatalan / DMV+CGerman / MarginalizedGerman / DMV+CFigure 3: Learning curve for semantic dependencysupervision in Catalan and German.
F1 of SRLonly (without sense disambiguation) shown as thenumber of training sentences is increased.but a drop in performance for German.
This mayreflect a difference between the languages, or mayreflect the difference between the annotation of thelanguages: both the Catalan and Spanish data orig-inated from the Ancora project,11while the Ger-man data came from another source.Figure 3 contains the learning curve for SRL su-pervision in our lowest resource setting for twoexample languages, Catalan and German.
Thisshows how F1 of SRL changes as we adjustthe number of training examples.
We find thatthe joint training approach to grammar inductionyields consistently higher SRL performance thanits distantly supervised counterpart.4.5 Analysis of Grammar InductionTable 7 shows grammar induction accuracy inlow-resource settings.
We find that the gap be-tween the supervised parser and the unsupervisedmethods is quite large, despite the reasonable ac-curacy both methods achieve for the SRL end task.11http://clic.ub.edu/corpus/ancora1184DependencyParserAvg.
ca cs de en es zhSupervised* 87.1 89.4 85.3 89.6 88.4 89.2 80.7DMV (pos) 30.2 45.3 22.7 20.9 32.9 41.9 17.2DMV (bc) 22.1 18.8 32.8 19.6 22.4 20.5 18.6DMV+C (pos) 37.5 50.2 34.9 21.5 36.9 49.8 32.0DMV+C (bc) 40.2 46.3 37.5 28.7 40.6 50.4 37.5Marginal, IGC43.8 50.3 45.8 27.2 44.2 46.3 48.5Marginal, IGB50.2 52.4 43.4 41.3 52.6 55.2 56.2Table 7: Unlabeled directed dependency accuracyon CoNLL?09 test set in low-resource settings.DMV models are trained on either POS tags (pos)or Brown clusters (bc).
*Indicates the supervised parseroutputs provided by the CoNLL?09 Shared Task.WSJ?DistantSupervisionSAJM?10 44.8 noneSAJ?13 64.4 noneSJA?10 50.4 HTMLNB?11 59.4 ACE05DMV (bc) 24.8 noneDMV+C (bc) 44.8 SRLMarginalized, IGC48.8 SRLMarginalized, IGB58.9 SRLTable 8: Comparison of grammar induction ap-proaches.
We contrast the DMV trained withViterbi EM+uniform initialization (DMV), ourconstrained DMV (DMV+C), and our model?sMBR decoding of latent syntax (Marginalized)with other recent work: Spitkovsky et al (2010a)(SAJM?10), Spitkovsky et al (2010b) (SJA?10),Naseem and Barzilay (2011) (NB?11), and the CSmodel of Spitkovsky et al (2013) (SAJ?13).This suggests that refining the low-resource gram-mar induction methods may lead to gains in SRL.Interestingly, the marginalized grammars bestthe DMV grammar induction method; however,this difference is less pronounced when the DMVis constrained using SRL labels as distant super-vision.
This could indicate that a better model forgrammar induction would result in better perfor-mance for SRL.
We therefore turn to an analysis ofother approaches to grammar induction in Table 8,evaluated on the Penn Treebank.
We contrast withmethods using distant supervision (Naseem andBarzilay, 2011; Spitkovsky et al, 2010b) and fullyunsupervised dependency parsing (Spitkovsky etal., 2013).
Following prior work, we excludepunctuation from evaluation and convert the con-stituency trees to dependencies.12The approach from Spitkovsky et al (2013)12Naseem and Barzilay (2011) and our results use thePenn converter (Pierre and Heiki-Jaan, 2007).
Spitkovsky etal.
(2010b; 2013) use Collins (1999) head percolation rules.
(SAJ?13) outperforms all other approaches, in-cluding our marginalized settings.
We thereforemay be able to achieve further gains in the pipelinemodel by considering better models of latent syn-tax, or better search techniques that break outof local optima.
Similarly, improving the non-convex optimization of our latent-variable CRF(Marginalized) may offer further gains.5 Discussion and Future WorkWe have compared various approaches for low-resource semantic role labeling at the state-of-the-art level.
We find that we can outperform priorwork in the low-resource setting by coupling theselection of feature templates based on informa-tion gain with a joint model that marginalizes overlatent syntax.We utilize unlabeled data in both generative anddiscriminative models for dependency syntax andin generative word clustering.
Our discriminativejoint models treat latent syntax as a structured-feature to be optimized for the end-task of SRL,while our other grammar induction techniques op-timize for unlabeled data likelihood?optionallywith distant supervision.
We observe that carefuluse of these unlabeled data resources can improveperformance on the end task.Our subtractive experiments suggest that lemmaannotations, a high-resource annotation, may notprovide a large benefit for SRL.
Our grammar in-duction analysis indicates that relatively low accu-racy can still result in reasonable SRL predictions;still, the models do not outperform those that usesupervised syntax, and we aim to explore how wellthe pipeline models in particular improve when weapply higher accuracy unsupervised grammar in-duction techniques.We have utilized well studied datasets in orderto best understand the quality of our models rela-tive to prior work.
In future work, we hope to ex-plore the effectiveness of our approaches on trulylow resource settings by using crowdsourcing todevelop semantic role datasets in other languagesand domains.Acknowledgments We thank Richard Johans-son, Dennis Mehay, and Stephen Boxwell for helpwith data.
We also thank Jason Naradowsky, JasonEisner, and anonymous reviewers for commentson the paper.1185ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
TheTheory of Parsing, Translation, and Compiling.Prentice-Hall, Inc.Rami Al-Rfou?, Bryan Perozzi, and Steven Skiena.2013.
Polyglot: Distributed word representationsfor multilingual NLP.
In Proceedings of the 17thConference on Computational Natural LanguageLearning (CoNLL 2013).
Association for Computa-tional Linguistics.Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.2009.
Multilingual semantic role labeling.
In Pro-ceedings of the 13th Conference on ComputationalNatural Language Learning (CoNLL 2009): SharedTask.
Association for Computational Linguistics.Stephen Boxwell and Michael White.
2008.
Project-ing propbank roles onto the CCGbank.
In Proceed-ings of the International Conference on LanguageResources and Evaluation (LREC 2008).
EuropeanLanguage Resources Association.Stephen Boxwell, Chris Brew, Jason Baldridge, DennisMehay, and Sujith Ravi.
2011.
Semantic role label-ing without treebanks?
In Proceedings of the 5th In-ternational Joint Conference on Natural LanguageProcessing (IJCNLP).
Asian Federation of NaturalLanguage Processing.Peter F. Brown, Peter V. Desouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational linguistics, 18(4).Trevor Cohn, Phil Blunsom, and Sharon Goldwater.2010.
Inducing tree-substitution grammars.
TheJournal of Machine Learning Research, 11.Michael Collins.
1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12.Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?sM`arquez, Adam Meyers, Joakim Nivre, SebastianPad?o, Jan?St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The conll-2009shared task: Syntactic and semantic dependencies inmultiple languages.
In Proceedings of the 13th Con-ference on Computational Natural Language Learn-ing (CoNLL 2009): Shared Task.
Association forComputational Linguistics.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: a corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Com-putational Linguistics, 33(3).Richard Johansson and Pierre Nugues.
2008.Dependency-based semantic role labeling of Prop-Bank.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing(EMNLP 2008).
Association for Computational Lin-guistics.Dan Klein and Christopher Manning.
2004.
Corpus-Based induction of syntactic structure: Models ofdependency and constituency.
In Proceedings of the42nd Meeting of the Association for ComputationalLinguistics (ACL 2004).
Association for Computa-tional Linguistics.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proceedings of ACL-08: HLT.
Association forComputational Linguistics.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceed-ings of the 18th International Conference on Ma-chine Learning (ICML 2001).
Morgan Kaufmann.Xavier Llu?
?s, Xavier Carreras, and Llu?
?s M`arquez.2013.
Joint arc-factored parsing of syntactic and se-mantic dependencies.
Transactions of the Associa-tion for Computational Linguistics (TACL).Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The Penn Treebank.
Com-putational linguistics, 19(2).Andre Martins, Noah Smith, Mario Figueiredo, andPedro Aguiar.
2011.
Structured sparsity in struc-tured prediction.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2011).
Association for Compu-tational Linguistics.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2005).Association for Computational Linguistics.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and dis-criminative training.
In Susan Dumais, DanielMarcu, and Salim Roukos, editors, HLT-NAACL2004: Main Proceedings.
Association for Compu-tational Linguistics.Jason Naradowsky, Sebastian Riedel, and David Smith.2012.
Improving NLP through marginalization ofhidden syntactic structure.
In Proceedings of the2012 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2012).
Associationfor Computational Linguistics.Tahira Naseem and Regina Barzilay.
2011.
Usingsemantic cues to learn syntax.
In Proceedings ofthe 25th AAAI Conference on Artificial Intelligence(AAAI 2011).
AAAI Press.1186Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed cor-pora.
In Proceedings of the 30th Annual Meeting ofthe Association for Computational Linguistics (ACL1992).Nugues Pierre and Kalep Heiki-Jaan.
2007.
Ex-tended constituent-to-dependency conversion for en-glish.
NODALIDA 2007 Proceedings.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2).David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2008).
Association forComputational Linguistics.Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Juraf-sky, and Christopher D Manning.
2010a.
Viterbitraining improves unsupervised dependency parsing.In Proceedings of the 14th Conference on Computa-tional Natural Language Learning (CoNLL 2010).Association for Computational Linguistics.Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-shawi.
2010b.
Profiting from mark-up: Hyper-textannotations for guided parsing.
In Proceedings ofthe 48th Annual Meeting of the Association for Com-putational Linguistics (ACL 2010).
Association forComputational Linguistics.Valentin I. Spitkovsky, Hiyan Alshawi, Angel X.Chang, and Daniel Jurafsky.
2011.
Unsuperviseddependency parsing without gold part-of-speechtags.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing(EMNLP 2011).
Association for Computational Lin-guistics.Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2013.
Breaking out of local optima withcount transforms and model recombination: A studyin grammar induction.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2013).
Association forComputational Linguistics.Kristina Toutanova, Aria Haghighi, and ChristopherManning.
2005.
Joint learning improves semanticrole labeling.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguis-tics (ACL 2005).
Association for Computational Lin-guistics.Kilian Weinberger, Anirban Dasgupta, John Langford,Alex Smola, and Josh Attenberg.
2009.
Featurehashing for large scale multitask learning.
In L?eonBottou and Michael Littman, editors, Proceedingsof the 26th Annual International Conference on Ma-chine Learning (ICML 2009).
Omnipress.Daniel H. Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information andControl, 10(2).Hai Zhao, Wenliang Chen, Chunyu Kity, and GuodongZhou.
2009.
Multilingual dependency learning: Ahuge feature engineering method to semantic depen-dency parsing.
In Proceedings of the 13th Confer-ence on Computational Natural Language Learning(CoNLL 2009): Shared Task.
Association for Com-putational Linguistics.1187
