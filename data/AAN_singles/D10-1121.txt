Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1245?1255,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsWhat?s with the Attitude?
Identifying Sentences with Attitude in OnlineDiscussionsAhmed Hassan Vahed QazvinianUniversity of Michigan Ann ArborAnn Arbor, Michigan, USAhassanam,vahed,radev@umich.eduDragomir RadevAbstractMining sentiment from user generated contentis a very important task in Natural LanguageProcessing.
An example of such content isthreaded discussions which act as a very im-portant tool for communication and collabo-ration in the Web.
Threaded discussions in-clude e-mails, e-mail lists, bulletin boards,newsgroups, and Internet forums.
Most of thework on sentiment analysis has been centeredaround finding the sentiment toward productsor topics.
In this work, we present a methodto identify the attitude of participants in anonline discussion toward one another.
Thiswould enable us to build a signed networkrepresentation of participant interaction whereevery edge has a sign that indicates whetherthe interaction is positive or negative.
Thisis different from most of the research on so-cial networks that has focused almost exclu-sively on positive links.
The method is exper-imentally tested using a manually labeled setof discussion posts.
The results show that theproposed method is capable of identifying at-titudinal sentences, and their signs, with highaccuracy and that it outperforms several otherbaselines.1 IntroductionMining sentiment from text has a wide range ofapplications from mining product reviews on theWeb (Morinaga et al, 2002; Turney and Littman,2003) to analyzing political speeches (Thomas et al,2006).
Automatic methods for sentiment mining arevery important because manual extraction of them isvery costly, and inefficient.
A new application ofsentiment mining is to automatically identify atti-tudes between participants in an online discussion.An automatic tool to identify attitudes will enableus to build a signed network representation of par-ticipant interaction in which the interaction betweentwo participants is represented using a positive ora negative edge.
Even though using signed edgesin social network studies is clearly important, mostof the social networks research has focused only onpositive links between entities.
Some work has re-cently investigated signed networks (Leskovec et al,2010; Kunegis et al, 2009), however this work waslimited to a few number of datasets in which userswere allowed to explicitly add negative, as well aspositive, relations.
This work will pave the way forresearch efforts to examine signed social networksin more detail.
It will also allow us to study the re-lation between explicit relations and the text under-lying those relation.Although similar, identifying sentences that dis-play an attitude in discussions is different from iden-tifying opinionated sentences.
A sentence in a dis-cussion may bear opinions about a definite target(e.g., price of a camera) and yet have no attitude to-ward the other participants in the discussion.
For in-stance, in the following discussion Alice?s sentencehas her opinion against something, yet no attitudetoward the recipient of the sentence, Bob.Alice: ?You know what, he turned out tobe a great disappointment?Bob: ?You are completely unqualified tojudge this great person?However, Bob shows strong attitude toward Alice.In this work, we look at ways to predict whether asentence displays an attitude toward the text recip-ient.
An attitude is the mental position of one par-ticipant with regard to another participant.
it couldbe either positive or negative.
We consider featureswhich takes into account the entire structure of sen-tences at different levels or generalization.
Those1245features include lexical items, part-of-speech tags,and dependency relations.
We use all those patternsto build several pairs of models that represent sen-tences with and without attitude.The rest of the paper is organized as follows.
InSection 2 we review some of the related prior workon identifying polarized words and subjectivity anal-ysis.
We explain the problem definition and discussour approach in Sections 3 & 4.
Finally, in Sec-tions 5 & 6 we introduce our dataset and discuss theexperimental setup.
Finally, we conclude in Section7.2 Related WorkIdentifying the polarity of individual words is a wellstudied problem.
In previous work, Hatzivassiloglouand McKeown (1997) propose a method to iden-tify the polarity of adjectives.
They use a manu-ally labeled corpus to classify each conjunction ofan adjective as ?the same orientation?
as the adjec-tive or ?different orientation?.
Their method canlabel simple in ?simple and well-received?
as thesame orientation and simplistic in ?simplistic butwell-received?
as the opposite orientation of well-received.
Although the results look promising, themethod would only be applicable to adjectives sincenoun conjunctions may collocate regardless of theirsemantic orientations (e.g., ?rise and fall?
).In other work, Turney and Littman (2003) use sta-tistical measures to find the association between agiven word and a set of positive/negative seed words.In order to get word co-occurrence statistics they usethe ?near?
operator from a commercial search en-gine on a given word and a seed word.In more recent work, Takamura et al (2005) usedthe spin model to extract word semantic orientation.First, they construct a network of words using def-initions, thesaurus, and co-occurrence statistics.
Inthis network, each word is regarded as an electron,which has a spin and each spin has a direction tak-ing one of two values: up or down.
Then, they usethe energy point of view to propose that neighboringelectrons tend to have the same spin direction, andtherefore neighboring words tend to have the samepolarity orientations.
Finally, they use the mean fieldmethod to find the optimal solution for electron spindirections.Previous work has also used WordNet, a lexi-cal database of English, to identify word polarity.Specifically, Hu and Liu (2004) use WordNet syn-onyms and antonyms to predict the polarity of anygiven word with unknown polarity.
They label eachword with the polarity of its synonyms and the op-posite polarity of its antonyms.
They continue ina bootstrapping manner to label all unlabeled in-stances.
This work is very similar to (Kamps et al,2004) in which a network of WordNet synonymsis used to find the shortest path between any givenword, and the words ?good?
and ?bad?.
Kim andHovy (Kim and Hovy, 2004) used WordNet syn-onyms and antonyms to expand two lists of positiveand negative seed words.
Similarly, Andreevskaiaand Bergler (2006) used WordNet to expand seedlists with fuzzy sentiment categories, in which wordscould be more central to one category than the other.Finally, Kanayama and Nasukawa (2006) used syn-tactic features and context coherency, defined as thetendency for same polarities to appear successively,to acquire polar atoms.All the work mentioned above focus on the taskof identifying the polarity of individual words.
Ourproposed work is identifying attitudes in sentencesthat appear in online discussions.
Perhaps the mostsimilar work to ours is the prior work on subjectivityanalysis, which is to identify text that present opin-ions as opposed to objective text that present fac-tual information (Wiebe, 2000).
Prior work on sub-jectivity analysis mainly consists of two main cate-gories: The first category is concerned with identify-ing the subjectivity of individual phrases and wordsregardless of the sentence and context they appearin (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000;Banea et al, 2008).
In the second category, sub-jectivity of a phrase or word is analyzed within itscontext (Riloff and Wiebe, 2003; Yu and Hatzivas-siloglou, 2003; Nasukawa and Yi, 2003; Popescuand Etzioni, ).
A good study of the applicationsof subjectivity analysis from review mining to emailclassification is given in (Wiebe, 2000).
Somasun-daran et al (2007) develop genre-speci.c lexiconsusing interesting function word combinations for de-tecting opinions in meetings.
Despite similarities,our work is different from subjectivity analysis be-cause the later only discriminates between opinionsand facts.
A discussion sentence may display an1246opinion about some topic yet no attitude.
The lan-guage constituents considered in opinion detectionmay be different from those used to detect attitude.Moreover, extracting attitudes from online discus-sions is different from targeting subjective expres-sions (Josef Ruppenhofer and Wiebe, 2008; Kimand Hovy, 2004).
The later usually has a limitedset of targets that compete for the subjective expres-sions (for example in movie review, targets could be:director, actors, plot, and so forth).
We cannot usesimilar methods because we are working on an opendomain where anything could be a target.
A very de-tailed survey that covers techniques and approachesin sentiment analysis and opinion mining could befound in (Pang and Lee, 2008).There is also some related work on mining on-line discussions.
Lin et al(2009) proposes a sparsecoding-based model simultaneously model seman-tics and structure of threaded discussions.
Shenet al(2006) proposes three clustering methods forexploiting the temporal information in the streams,as well as an algorithm based on linguistic fea-tures to analyze the discourse structure information.Huang et al(2007) used an SVM classifier to extract(thread-title, reply) pairs as chat knowledge from on-line discussion forums to support the constructionof a chatbot for a certain domain.
Other work hasfocused on the structure of questions and question-answer pairs in online forums and discussions (Dinget al, 2008; Cong et al, 2008).3 Problem DefinitionAssume we have a set of sentences exchanged be-tween participants in an online discussion.
Our ob-jective is to identify sentences that display an atti-tude from the text writer to the text recepient fromthose that do not.
An attitude is the mental posi-tion of one particpant with regard to another partic-ipant.
An attitude may not be directly observable,but rather inferred from what particpants say to oneanother.
The attitude could be either positive or neg-ative.
Strategies for showing a positive attitude mayinclude agreement, and praise, while strategies forshowing a negative attitude may include disagree-ment, insults, and negative slang.
After identifyingsentences that display an attitude, we also predict thesign (positive or negative) of that attitude.4 ApproachIn this section, we describe a model which, given asentence, predicts whether it carries an attitude fromthe text writer toward the text recipient or not.
Anygiven piece of text exchanged between two partici-pants in a discussion could carry an attitude towardthe text recipient, an attitude towards the topic, orno attitude at all.
As we are only interested in at-titudes between participants, we limit our study tosentences that use second person pronouns.
Secondperson pronouns are usually used in conversationalgenre to indicate that the text writer is addressing thetext recipient.
After identifying those sentences, wedo some pre-processing to extract the most relevantfragments.
We examine these fragments to to iden-tify the polarity of every word in the sentence.
Everyword could be assigned a semantic orientation.
Thesemantic orientation could be either positive, nega-tive, or neutral.
The existence of polarized words inany sentence is an important indicator of whether itcarries an attitude or not.The next step is to extract several patterns atdifferent levels of generalization representing anygiven sentence.
We use those patterns to build twoMarkov models for every kind of patterns.
The firstmodel characterizes the relation between differenttokens for all patterns that correspond to sentencesthat have an attitude.
The second model is similar tothe first one, but it uses all patterns that correspondto sentences that do not have an attitude.
Given anew sentence, we extract the corresponding patternsand estimate the likelihood of every pattern beinggenerated from the two corresponding models.
Wethen compare the likelihood of the sentence underthe two models and use this as a feature to predictthe existence of an attitude.
A pair of models willbe built for every kind of patterns.
If we have n dif-ferent patterns, we will have n different likelihoodratios that come from n pairs of models.4.1 Word Polarity IdentificationIdentifying the polarity of words is an important stepfor our method.
Our word identification module issimilar to the work in (Annon, 2010).
We constructa graph where each node represent a word/part-of-speech pair.
Two nodes are linked if the words arerelated.
We use WordNet (Miller, 1995) to link re-1247lated words based on synonyms, hypernyms, andsimilar to relations.
For words that do not appearin Wordnet, we used Wiktionary, a collaborativelyconstructed dictionary.
We also add some linksbased on co-occurrence statistics between words asfrom a large corpus.
The resulting graph is a graphG(W,E) where W is a set of word/part-of-speechpairs, and E is the set of edges connecting relatedwords.We define a random walk model on the graph,where the set of nodes correspond to the state spaceof the random walk.
Transition probabilities are cal-culated by normalizing the weights of the edges outof every node.
Let S+ and S?
be two sets of ver-tices representing seed words that are already la-beled as either positive or negative respectively.
Weused the list of labeled seeds from (Hatzivassiloglouand McKeown, 1997) and (Stone et al, 1966).
Forany given word w, we calculate the mean hittingtime betweenw, and the two seed sets h(w|S+), andh(w|S?).
The mean hitting time h(i|k) is defined asthe average number of steps a random walker, start-ing in state i 6= k, will take to enter state k for thefirst time (Norris, 1997).
If h(w|S+) is greater thanh(w|S?
), the word is classified as negative, oth-erwise it is classified as positive.
We also use themethod described in (Wilson et al, 2005) to deter-mine the contextual polarity of the identified words.The set of features used to predict contextual polar-ity include word, sentence, polarity, structure, andother features.4.2 Identifying Relevant Parts of SentencesThe writing style in online discussion forums is veryinformal.
Some of the sentence are very long, andpunctuation marks are not always properly used.
Tosolve this problem, we decided to use the grammat-ical structure of sentences to identify the most rele-vant part of sentences that would be the subject offurther analysis.
Figure 1 shows a parse tree repre-senting the grammatical structure of a particular sen-tence.
If we closely examine the sentence, we willnotice that we are only interested in a part of thesentence that includes the second person pronoun?you?.
We extract this part, by starting at the wordof interest , in this case ?you?, and go up in the hi-erarchy till we hit the first sentence clause.
Once,we reach a sentence clause, we extract the corre-sponding text if it is grammatical, otherwise we goup one more level to the closest sentence clause.
Weused the Stanford parser to generate the grammaticalstructure of sentences (Klein and Manning, 2003).Figure 1: An example showing how to identify the rele-vant part of a sentence.4.3 Sentences as PatternsThe fragments we extracted earlier are more rele-vant to our task and are more suitable for furtheranalysis.
However, these fragments are completelylexicalized and consequently the performance of anyanalysis based on them will be limited by data spar-sity.
We can alleviate this by using more generalrepresentations of words.
Those general representa-tions can be used a long with words to generate a setof patterns that represent each fragment.
Each pat-tern consists of a sequence of tokens.
Examples ofsuch patterns could use lexical items, part-of-speech(POS) tags, word polarity tags, and dependency re-lations.We use three different patterns to represent eachfragments:?
Lexical patterns: All polarized words are re-places with the corresponding polarity tag, andall other words are left as is.?
Part-of-speech patterns: All words are replacedwith their POS tags.
Second person pronounsare left as is.
Polarized words are replaced withtheir polarity tags and their POS tags.?
Dependency grammar patterns: the shortestpath connecting every second person pronoun1248to the closed polarized word is extracted.
Thesecond person pronoun, the polarized word tag,and the types of the dependency relations alongthe path connecting them are used as a pat-tern.
It has been shown in previous work onrelation extraction that the shortest path be-tween any two entities captures the the in-formation required to assert a relationship be-tween them (Bunescu and Mooney, 2005).
Ev-ery polarized word is assigned to the closestsecond person pronoun in the dependency tree.This is only useful for sentences that have po-larized words.Table 1 shows the different kinds of representa-tions for a particular sentence.
We use text, part-of-speech tags, polarity tags, and dependency rela-tions.
The corresponding patterns for this sentenceare shown in Table 2.4.4 Building the ModelsGiven a set of patterns representing a set of sen-tences, we can build a graph G = V,E,w whereV is the set of all possible token that may appearin the patterns.
E = V ?
V is the set of possibletransitions between any two tokens.
w : E ?
[0..1]is a weighting function that assigns to every pair ofstates (i, j) a weight w(i, j) representing the proba-bility that we have a transition from state i to statej.This graph corresponds to a Markovian model.The set of states are the vocabulary, and the the tran-sition probabilities between states are estimated us-ing Maximum Likelihood estimation as follows:Pij =NijNiwhereNij is the number of times we saw a transitionfrom i to state j, and Ni is the total number of timeswe saw state i in the training data.
This is similar tobuilding a language model over the language of thepatterns.We build two such models for every kind of pat-terns.
The first model is built using all sentences thatappeared in the training dataset and was labeled ashaving an attitude, and the second model is built us-ing all sentences in the training dataset that do nothave an attitude.
If we have n kinds of patterns, wewill build one such pair for every kind of patterns.Hence, we will end up with 2n models.4.5 Identifying Sentences with AttitudeWe split our training data into two splits; the firstcontaining all sentences that have an attitude and thesecond containing all sentences that do not have anattitude.
Given the methodology described in theprevious section, we build n pairs of Markov mod-els.
Given any sentence, we extract the correspond-ing patterns and estimate the log likelihood that thissequence of tokens was generated from every model.Given a model M , and sequence of tokens T =(T1, T2, .
.
.
TSn), the probability of this token se-quence being generated from M is:PM (T ) =n?i=2P (Ti|T1, .
.
.
, Ti?1) =n?i=2W (Ti?1, Ti)where n is the number of tokens in the pattern, andW is the probability transition function.The log likelihood is then defined as:LLM (T ) =n?i=2logW (Ti?1, Ti)For every pair of models, we may use the ratio be-tween the two likelihoods as a feature:f =LLMatt(T )LLMnoatt(T )where T is the token sequence, LLMatt(T ) is the loglikelihood of the sequence given the attitude model,and LLMnoatt(T ) is the log likelihood of the patterngiven the no-attitude model.Given the n kinds of patterns, we can calculatethree different features.
A standard machine learn-ing classifier is then trained using those features topredict whether a given sentence has an attitude ornot.4.6 Identifying the Sign of an AttitudeTo determine the orientation of an attitude sentence,we tried two different methods.
The first method as-sumes that the orientation of an attitude sentence isdirectly related to the polarity of the words it con-tains.
If the sentence has only positive and neutral1249Table 1: Tags used for building patternsText That makes your claims so ignorantPOS That/DT makes/VBZ your/PRP$ claims/NNS so/RB ignorant/JJPolarity That/O makes/O your/O claims/O so/O ignorant/NEGDependency yourposs?
claimsnsubj?
ignorantTable 2: Sample patternsLexical pattern That makes your claims so NEGPOS pattern DT VBZ your PRP$ NNS RB NEG JJDependency pattern your poss nsubj NEGwords, it is classified as positive.
If the sentencehas only negative and neutral words, it is classifiedas negative.
If the sentence has both positive andnegative words, we calculate the summation of thepolarity scores of all positive words and that of allnegative words.
The polarity score of a word is anindicator of how strong of a polarized word it is.
Ifthe former is greater, we classify the sentence as pos-itive,otherwise we classify the sentence as negative.The problem with this method is that it assumesthat all polarized words in a sentence with an atti-tude target the text recipient.
Unfortunately, that isnot always correct.
For example, the sentence ?Youare completely unqualified to judge this great per-son?
has a positive word ?great?
and a negative word?unqualified?.
The first method will not be able topredict whether the sentence is positive or negative.To solve this problem, we use another method thatis based on the paths that connect polarized words tosecond person pronouns in a dependency parse tree.For every positive word w , we identify the shortestpath connecting it to every second person pronounin the sentence then we compute the average lengthof the shortest path connecting every positive wordto the closest second person pronoun.
We repeat fornegative words and compare the two values.
Thesentence is classified as positive if the average lengthof the shortest path connecting positive words to theclosest second person pronoun is smaller than thecorresponding value for negative words.
Otherwise,we classify the sentence as negative.5 DataOur data was randomly collected from a set of dis-cussion groups.
We collected a large number ofthreads from the first quarter of 2009 from a set ofUsenet discussion groups.
All threads were in En-glish, and had 5 posts or more.
We parsed the down-loaded threads to identify the posts and senders.
Wekept posts that have quoted text and discarded allother posts.
The reason behind that is that partici-pants usually quote other participants text when theyreply to them.
This restriction allows us to iden-tify the target of every post, and raises the proba-bility that the post will display an attitude from itswriter to its target.
We plan to use more sophsticatedmethods for reconstructing the reply structure likethe one in (Lin et al, 2009).
From those posts, werandomly selected approximately 10,000 sentencesthat use second person pronouns.
We explained ear-lier how second person pronouns are used in discus-sions genres to indicate the writer is targeting thetext recipient.
Given a random sentence selectedfrom some random discussion thread, the probabil-ity that the sentence does not have an attitude is sig-nificantly larger than the probability that it will havean attitude.
Hence, restricting our dataset to postswith quoted text and sentences with second personpronouns is very important to make sure that wewill have a considerable amount of attitudinal sen-tences.
The data was tokenized, sentence-split, part-of-speech tagged with the OpenNLP toolkit.
It wasparsed with the Stanford dependency parser (Kleinand Manning, 2003).5.1 Annotation SchemeThe goals of the annotation scheme are to distin-guish sentences that display an attitude from thosethat do not.
Sentences could display either a neg-ative or a positive attitude.
Disagreement, insults,and negative slang are indicators of negative attitude.1250A B C DA - 82.7 80.6 82.1B 81.0 - 81.9 82.9C 77.8 78.2 - 83.8D 78.3 77.7 78.6 -Table 3: Inter-annotator agreementAgreement, and praise are indicators of positive at-titude.
Our annotators were instructed to read everysentence and assign two labels to it.
The first speci-fies whether the sentence displays an attitude or not.The existence of an attitude was judged on a threepoint scale: attitude, unsure, and no-attitude.
Thesecond is the sign of the attitude.
If an attitude ex-ists, annotators were asked to specify whether theattitude is positive or negative.
To evaluate inter-annotator agreement, we use the agr operator pre-sented in (Wiebe et al, 2005).
This metric measuresthe precision and recall of one annotator using theannotations of another annotator as a gold standard.The process is repeated for all pairs of annotators,and then the harmonic mean of all values is reported.Formally:agr(A|B) =|A ?B||A|(1)where A, and B are the annotation sets produced bythe two reviewers.
Table 3 shows the value of theagr operator for all pairs of annotators.
The har-monic mean of the agr operator is 80%.
The agroperator was used over the Kappa Statistic becausethe distribution of the data was fairly skewed.6 Experiments6.1 Experimental SetupWe performed experiments on the data described inthe previous section.
The number of sentences withan attitude was around 20% of the entire dataset.The class imbalance caused by the small number ofattitude sentences may hurt the performance of thelearning algorithm (Provost, 2000).
A common wayof addressing this problem is to artificially rebal-ance the training data.
To do this we down-samplethe majority class by randomly selecting, withoutreplacement, a number of sentences without an at-titude that equals the number of sentences with anattitude.
That resulted in a balanced subset, approx-imately 4000 sentences, that we used in our experi-ments.We used Support Vector Machines (SVM) as aclassifier.
We optimized SVM separately for everyexperiment.
We used 10-fold cross validation for alltests.
We evaluate our results in terms of precision,recall, accuracy, and F1.
Statistical significance wastested using a 2-tailed paired t-test.
All reported re-sults are statistically significant at the 0.05 level.
Wecompare the proposed method to several other base-lines that will be described in the next subsection.We also perform experiments to measure the perfor-mance if we mix features from the baselines and theproposed method.6.2 BaselinesThe first baseline is based on the hypothesis that theexistence of polarized words is a strong indicatorthat the sentence has an attitude.
As a result, weuse the number of polarized word in the sentence,the percentage of polarized words to all other words,and whether the sentences has polarized words withmixed or same sign as features to train an SVM clas-sifier to detect attitude.The second baseline is based on the proximity be-tween the polarized words and the second personpronouns.
We assume that every polarized word isassociated with the closest second person pronoun.Let w be a polarized word and p(w) be the closessecond person pronoun, and surf dist(w, p(w)) bethe surface distance betweenw and p(w).
This base-line uses the minimum, maximum, and average ofsurf dist(w, p(w)) for all polarized words as fea-tures to train an SVM classifier to identify sentenceswith attitude.The next baseline uses the dependency tree dis-tance instead of the surface distance.
We assume thatevery polarized word is associated to the second per-son pronoun that is connected to it using the smallestshortest path.
The dep dist(w, p(w)) is calculatedsimilar to the previous baselines but using the de-pendency tree distance.
The minimum, maximum,and average of this distance for all polarized wordsare used as features to train an SVM classifier.1251Figure 2: Accuracy, Precision, and Recall for the Pro-posed Approach and the Baselines.0 10 20 30 40 50 60 70 80 90 1000102030405060708090100RecallPrecisionMMSurfDistDepDistPolFigure 3: Precision Recall Graph.6.3 Results and DiscussionFigure 2 compares the accuracy, precision, and re-call of the proposed method (ML), the polarity basedclassifier (POL), the surface distance based classi-fier (Surf Dist), and the dependency distance basedclassifier (Dep Dist).
The values are selected to opti-mize F1.
The figure shows that the surface distancebased classifier behaves poorly with low accuracy,precision, and recall.
The two other baselines be-have poorly as well in terms of precision and accu-racy, but they do very well in terms of recall.
Welooked at some of the examples to understand whythose two baselines achieve very high recall.
It turnsout that they tend to predict most sentences that havepolarized words as sentences with attitude.
This re-sults in many false positives and low true negativerate.
Achieving high recall at the expense of losingprecision is trivial.
On the other hand, we notice that0 10 20 30 40 50 60 70 80 90 1007576777879808182Training Set Size (%)AccuracyFigure 4: Accuracy Learning Curve for the ProposedMethod.the proposed method results in very close values ofprecision and recall at the optimum F1 point.To better compare the performance of the pro-posed method and the baseline, we study the theprecision-recall curves for all methods in Figure 3.We notice that the proposed method outperforms allbaselines at all operating points.
We also notice thatthe proposed method provides a nice trade-off be-tween precision and recall.
This allows us some flex-ibility in choosing the operating point.
For example,in some applications we might be interested in veryhigh precision even if we lose recall, while in otherapplications we might sacrifice precision in order toget high recall.
On the other hand, we notice thatthe baselines always have low precision regardlessof recall.Table 4 shows the accuracy, precision, recall, andF1 for the proposed method and all baselines.
It alsoshows the performance when we add features fromthe baselines to the proposed method, or merge someof the baselines.
We see that we did not get any im-provement when we added the baseline features tothe proposed method.
We believe that the proposedmethod captures all the information captured by thebaselines and more.Our proposed method uses three different featuresthat correspond to the three types of patterns we useto represent every sentence.
To understand the con-tributions of every feature, we measure the perfor-mance of every feature by itself and also all possiblecombinations of pairs of features.
We compare that1252to the performance we get when using all features inTable 5.
We see that the part-of-speech patterns per-forms better than the text patterns.
This makes sensebecause the former suffers from data sparsity.
De-pendency patterns performs best in terms of recall,while part-of-speech patterns outperform all othersin terms of precision, and accuracy.
All pairs offeatures outperform any single feature that belongto the corresponding pair in terms of F1.
We alsonotice that using the three features results in betterperformance when compared to all other combina-tions.
This shows that every kind of pattern capturesslightly different information when compared to theothers.
It also shows that merging the three featuresimproves performance.One important question is how much data is re-quired to the proposed model.
We constructed alearning curve, shown in Figure 4, by fixing thetest set size at one tenth of the data, and varyingthe training set size.
We carried out ten-fold crossvalidation as with our previous experiments.
We seethat adding more data continues to increase the accu-racy, and that accuracy is quite sensitive to the train-ing data.
This suggests that adding more data to thismodel could lead to even better results.We also measured the accuracy of the two meth-ods we proposed for predicting the sign of attitudes.The accuracy of the first model that only uses thecount and scores of polarized words was 95%.
Theaccuracy of the second method that used depen-dency distance was 97%.6.4 Error AnalysisWe had a closer look at the results to find out whatare the reasons behind incorrect predictions.
Wefound two main reasons.
First, errors in predictingword polarity usually propagates and results in er-rors in attitude prediction.
The reasons behind incor-rect word polarity predictions is ambiguity in wordsenses and infrequent words that have very few con-nection in thesaurus.
A possible solution to this typeof errors is to improve the word polarity identifica-tion module by including word sense disambigua-tion and adding more links to the words graph usingglosses or co-occurrence statistics.
The second rea-son is that some sentences are sarcastic in nature.
Itis so difficult to identify such sentences.
Identify-ing sarcasm should be addressed as a separate prob-Method Accuracy Precision Recall F1ML 80.3 81.0 79.4 80.2POL 73.1 66.4 93.9 77.7ML+POL 79.9 77.9 83.4 80.5SurfDist 70.2 67.1 79.2 72.7DepDist 73.1 66.4 93.8 77.8SurfDist+ 73.1 66.4 93.8 77.7DepDistML+SurfDist 73.9 67.2 93.6 78.2ML+DepDist 72.8 66.1 93.8 77.6ML+SurfDist+ 74.0 67.2 93.4 78.2DepDistSurfDist+ 73.1 66.3 93.8 77.7DepDist+POLML+SurfDist+ 73.0 66.2 93.8 77.6DepDist+POLTable 4: Precision, Recall, F1, and Accuracy for the pro-posed method, the baselines, and different combinationsof proposed method and the baselines featuresMethod Accuracy Precision Recall F1txt 75.5 74.1 78.6 76.2pos 77.7 78.2 76.9 77.5dep 74.7 70.4 85.1 77.0txt+pos 77.8 77.0 79.4 78.1txt+dep 79.4 79.6 79.2 79.4pos+dep 80.4 79.1 82.5 80.7txt+pos+dep 80.3 81.0 79.4 80.2Table 5: Precision, Recall, F1, and Accuracy for differentcombinations of the proposed method?s features.lem.
A method that utilizes holistic approaches thattakes context and previous interactions between dis-cussion participants into consideration could be usedto address it.7 ConclusionsWe have shown that training a supervised Markovmodel of text, part-of-speech, and dependecy pat-terns allows us to identify sentences with attitudesfrom sentences without attitude.
This model is moreaccurate than several other baselines that use fea-tures based on the existence of polarized word, andproximity between polarized words and second per-son pronouns both in text and dependecy trees.
Thismethod allows to extract signed social networksfrom multi-party online discussions.
This opens thedoor to research efforts that go beyond standard so-cial network analysis that is based on positve links1253only.
It also allows us to study dynamics behind in-teractions in online discussions, the relation betweentext and social interactions, and how groups formand break in online discussions.AcknowledgmentsThis research was funded by the Office of the Di-rector of National Intelligence (ODNI), IntelligenceAdvanced Research Projects Activity (IARPA),through the U.S. Army Research Lab.
All state-ments of fact, opinion or conclusions containedherein are those of the authors and should not beconstrued as representing the official views or poli-cies of IARPA, the ODNI or the U.S. Government.ReferencesAlina Andreevskaia and Sabine Bergler.
2006.
Miningwordnet for fuzzy sentiment: Sentiment tag extractionfrom wordnet glosses.
In EACL?06.Carmen Banea, Rada Mihalcea, and Janyce Wiebe.2008.
A bootstrapping method for building subjec-tivity lexicons for languages with scarce resources.
InLREC?08.Razvan C. Bunescu and Raymond J. Mooney.
2005.
Ashortest path dependency kernel for relation extrac-tion.
In HLT ?05, pages 724?731, Morristown, NJ,USA.
Association for Computational Linguistics.Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,and Yueheng Sun.
2008.
Finding question-answerpairs from online forums.
In SIGIR ?08, pages 467?474.Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu.2008.
Using conditional random fields to extract con-texts and answers of questions from online forums.
InACL?08, pages 710?718.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In EACL?97, pages 174?181.Vasileios Hatzivassiloglou and Janyce Wiebe.
2000.
Ef-fects of adjective orientation and gradability on sen-tence subjectivity.
In COLING, pages 299?305.Minqing Hu and Bing Liu.
2004.
Mining and summariz-ing customer reviews.
In KDD?04, pages 168?177.Jizhou Huang, Ming Zhou, and Dan Yang.
2007.
Ex-tracting chatbot knowledge from online discussion fo-rums.
In IJCAI?07, pages 423?428.Swapna Somasundaran Josef Ruppenhofer and JanyceWiebe.
2008.
Finding the sources and targetsof subjective expressions.
In Proceedings of theSixth International Language Resources and Evalua-tion (LREC?08).Jaap Kamps, Maarten Marx, Robert J. Mokken, andMaarten De Rijke.
2004.
Using wordnet to measuresemantic orientations of adjectives.
In National Insti-tute for, pages 1115?1118.Hiroshi Kanayama and Tetsuya Nasukawa.
2006.
Fullyautomatic lexicon expansion for domain-oriented sen-timent analysis.
In EMNLP?06, pages 355?363.Soo-Min Kim and Eduard Hovy.
2004.
Determining thesentiment of opinions.
In COLING, pages 1367?1373.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In ACL?03, pages 423?430.Je?ro?me Kunegis, Andreas Lommatzsch, and ChristianBauckhage.
2009.
The slashdot zoo: mining a so-cial network with negative edges.
In WWW?09, pages741?750, New York, NY, USA.Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.2010.
Predicting positive and negative links in onlinesocial networks.
In WWW ?10, pages 641?650, NewYork, NY, USA.
ACM.Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,and Wei Wang.
2009.
Simultaneously modeling se-mantics and structure of threaded discussions: a sparsecoding approach and its applications.
In SIGIR ?09,pages 131?138.George A. Miller.
1995.
Wordnet: a lexical database forenglish.
Commun.
ACM, 38(11):39?41.Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, andToshikazu Fukushima.
2002.
Mining product reputa-tions on the web.
In KDD?02, pages 341?349.Tetsuya Nasukawa and Jeonghee Yi.
2003.
Sentimentanalysis: capturing favorability using natural languageprocessing.
In K-CAP ?03: Proceedings of the 2ndinternational conference on Knowledge capture, pages70?77.J.
Norris.
1997.
Markov chains.
Cambridge UniversityPress.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.Ana-Maria Popescu and Oren Etzioni.
Extracting prod-uct features and opinions from reviews.
In HLT-EMNLP?05.Foster Provost.
2000.
Machine learning from imbal-anced data sets 101.
In Proceedings of the AAAI Work-shop on Imbalanced Data Sets.Ellen Riloff and Janyce Wiebe.
2003.
Learningextraction patterns for subjective expressions.
InEMNLP?03, pages 105?112.Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.2006.
Thread detection in dynamic text messagestreams.
In SIGIR ?06, pages 35?42.Swapna Somasundaran, Josef Ruppenhofer, and JanyceWiebe.
2007.
Detecting arguing and sentiment in1254meetings.
In Proceedings of the SIGdial Workshop onDiscourse and Dialogue.Philip Stone, Dexter Dunphy, Marchall Smith, and DanielOgilvie.
1966.
The general inquirer: A computer ap-proach to content analysis.
The MIT Press.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting semantic orientations of words usingspin model.
In ACL?05, pages 133?140.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Getout the vote: Determining support or opposition fromCongressional floor-debate transcripts.
In EMNLP2006, pages 327?335.Peter Turney and Michael Littman.
2003.
Measuringpraise and criticism: Inference of semantic orientationfrom association.
ACM Transactions on InformationSystems, 21:315?346.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotionsin language.
Language Resources and Evaluation,1(2):0.Janyce Wiebe.
2000.
Learning subjective adjectivesfrom corpora.
In Proceedings of the SeventeenthNational Conference on Artificial Intelligence andTwelfth Conference on Innovative Applications of Ar-tificial Intelligence, pages 735?740.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In HLT/EMNLP?05, Vancouver,Canada.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: separating facts fromopinions and identifying the polarity of opinion sen-tences.
In EMNLP?03, pages 129?136.1255
