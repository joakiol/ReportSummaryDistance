Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 380?384,Dublin, Ireland, August 23-24, 2014.Kea: Sentiment Analysis of Phrases Within Short TextsAmeeta Agrawal, Aijun AnDepartment of Computer Science and EngineeringYork University, Toronto, Canada M3J 1P3{ameeta, aan}@cse.yorku.caAbstractSentiment Analysis has become an in-creasingly important research topic.
Thispaper describes our approach to building asystem for the Sentiment Analysis in Twit-ter task of the SemEval-2014 evaluation.The goal is to classify a phrase within ashort piece of text as positive, negativeor neutral.
In the evaluation, classifierstrained on Twitter data are tested on datafrom other domains such as SMS, blogs aswell as sarcasm.
The results indicate thatapart from sarcasm, classifiers built forsentiment analysis of phrases from tweetscan be generalized to other short text do-mains quite effectively.
However, in cross-domain experiments, SMS data is found togeneralize even better than Twitter data.1 IntroductionIn recent years, new forms of communication suchas microblogging and text messaging have becomequite popular.
While there is no limit to the rangeof information conveyed by tweets and short texts,people often use these messages to share their sen-timents.
Working with these informal text gen-res presents challenges for natural language pro-cessing beyond those typically encountered whenworking with more traditional text genres.
Tweetsand short texts are shorter, the language is veryinformal, with creative spelling and punctuation,misspellings, slang, new words, URLs, and genre-specific terminology such as, RT for ?re-tweet?and #hashtags for tagging (Rosenthal et al., 2014).Although several systems have tackled the taskof analyzing sentiment from entire tweets, thetask of analyzing sentiments of phrases (a wordThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/or more) within a tweet has remained largely un-explored.
This paper describes the details ofour system that participated in the subtask Aof Semeval-2014 Task 9: Sentiment Analysis inTwitter (Rosenthal et al., 2014).
The goal of thistask is to determine whether a phrase within a mes-sage is positive, negative or neutral in that context.Here, a message indicates any short informal pieceof text such as a tweet, SMS data, or a sentencefrom Live Journal blog, which is a social network-ing service where Internet users keep an online di-ary.
A phrase could be a word or a few consecutivewords within a message.The novelty of this task lies in the fact that amodel built using only Twitter data is used to clas-sify instances from other short text domains suchas SMS and Live Journal.
Moreover, a short testcorpus of sarcastic tweets is also used to test theperformance of the sentiment classifier.The main contributions of this paper includea) developing a sentiment analysis classifer forphrases; b) training on Twitter data and testing onother domains such as SMS and Live Journal datato see how well the classifier generalizes to differ-ent types of text, and c) testing on sarcastic tweets.2 Related WorkSentiment analysis from Twitter data has attractedmuch attention from the research community inthe past few years (Asiaee T. et al., 2012; Go etal., 2009; Pang et al., 2002; Pang and Lee, 2004;Wilson et al., 2005).
However, most of theseapproaches classify entire tweets by their overallsentiment (positive, negative, or neutral).The task at hand is to classify the sentiment of aphrase within a short message.
The challenges ofclassifying contextual polarity of phrases has beenpreviously explored by first determining whetherthe phrase is neutral or polar, and then disam-biguating the polarity of the polar phrases (Wil-son et al., 2005).
Another approach entails using380manually developed patterns (Nasukawa and Yi,2003).
Both these techniques, however, experi-mented with general web pages and online reviewsbut not Twitter data.Previously, a few systems that participated inSemeval-2013: Sentiment Analysis in Twitter task(Wilson et al., 2013; Mohammad et al., 2013;Gunther and Furrer, 2013) tackled the problem ofsentiment analysis of phrases by training on datathat exclusively came from tweets and tested ona corpus made up of tweets and SMS data.
Thistime though, the task is to see how well a systemtrained on tweets will perform on not only SMSdata, but also blog sentences from Live Journal, aswell as sarcastic tweets.3 Task SetupFormally, given a message containing a phrase(one or more words), the task is to determinewhether that phrase is positive, negative or neutralin that context.
We were able to download 8880tweets (7910 for training, and 970 for develop-ment) from the corpus made available by the taskorganizers, where each tweet includes a phrasemarked as positive, negative or neutral.
Keywordsand hashtags were used to identify and collectmessages, which were then annotated using Ama-zon Mechanical Turk.
This task setup is furtherdescribed in the task description paper (Rosenthalet al., 2014).The evaluation consists of Twitter data as wellas surprise genres such as SMS, Live Journal andTwitter Sarcasm.
The purpose of hidden test gen-res was to see how well a system trained on tweetswill perform on previously unseen domains.4 System DescriptionThis section describes the system components.4.1 Supervised Machine LearningDuring development time, we experimented withvarious supervised machine learning classifiers,but the final model was trained using Support Vec-tor Machines (SVM) with a linear kernel as it out-performed all other classifiers.
The c value wasempirically selected and set to 1.4.2 FeaturesFor all tweets, the URL links and @usernamementions are replaced by ?URL?
and ?username?placeholders, respectively.
The following featureswere included in the final model:?
Prior polarities: Previous research (Agrawaland An, 2013; Mohammad et al., 2013) hasshown prior polarities of words to be oneof the most important features in contex-tual sentiment analysis of phrases.
So, forone of the features, the sum of the sentis-cores of all the terms in the phrase was com-puted from SentiWordNet (Esuli and Sebas-tiani, 2006).
For another feature, the priorpolarity of the phrase was estimated by aver-aging the positive/negative strength of all itsterms by looking them up in the SubjectivityClues database (Wilson et al., 2005).?
Emoticons: An emoticon lexicon containingfrequent positive and negative emoticons, aswell as some of their misspellings that aregenerally found in tweets, was created manu-ally1.
The prior positive and negative emoti-con features contain the counts of all positiveand negative emoticons in the phrase.?
Lengths: Counts of the total number of wordsin the phrase, the average number of char-acters in the phrase, and the total number ofwords in the message were included.?
Punctuation: Whether the phrase containspunctuation such as ??
?, ?!
?, ?...
?, etc.?
Clusters: Word cluster IDs were obtained foreach term via unsupervised Brown clusteringof tweets (Owoputi et al., 2013).
For exam-ple, words such as anyone, anybody, any1,ne1 and anyonee are all represented by clus-ter path 0111011110.
This allows groupingmultiple (mis)spellings of a word together,which would otherwise be unique unigrams.?
Unigrams: Each phrase consists of one ormore words, with the average number ofwords in a phrase being 2.
We used only un-igrams as bigrams were found to reduce theaccuracy on the development set.5 Experiments and DiscussionThe task organizers made available a test data setcomposed of 10681 instances.
Table 1 describes1http://goo.gl/fh6Pjr381Test sets (# instances) Sentiment Example Phrase to be classified (in bold)Twitter (6908) positive No school at the Cuse till Wednesday #hypednegative i know it?s in january, but i can?t wait for Winter Jam !neutral Bye bye Kyiv!
See you in December :-*SMS (2334) positive later on wanna catch a movie?negative U had ur dinner already?
She just wont believe wat i said, haiz..neutral Im free on sat ... Ok we watch together lorLiveJournal (1315) positive And Tess you are going to prom too on the same day as us as wellnegative Does not seem likely that there would be any confusion .neutral if i am ever king i will make it up to you .TwitterSarcasm (124) positive @ImagineMore CHEER up.
It?s Monday after all.
#mondaybluesnegative I may or may not be getting sick...perfect.
#idontwantitneutral @Ken Rosenthal mistakes?
C?mon Kenny!!
;)Table 1: Test corpus details.the breakdown of the various types of text, withexample phrases that are to be classified.As expected, Live Journal has a slightly moreformal sentence structure with properly speltwords, whereas Twitter and SMS data includemore creative spellings.
Clearly, the sarcasm cat-egory includes messages with two contradictorysentiments in close proximity.
The challenge ofthis task lies precisely in the fact that one classifiertrained on Twitter data should be able to general-ize reasonably well on different types of text.5.1 Task ResultsWe participated in the constrained version of thetask which meant working with only the providedTwitter training data without any additional an-notated messages.
The macro-average F1-scoresof the positive and negative classes, which werethe evaluation criteria for the task, of our sys-tem (trained on Twitter training data and tested onTwitter test, SMS and Live Journal blog data) arepresented in Table 2.There are two interesting observations here:firstly, even though the classifier was trained solelyon tweets, it performs equally well on SMS andLive Journal data; and secondly, the sarcasm cate-gory has the poorest overall performance, unsur-prisingly.
This suggests that cross-domain sen-timent classification of phrases in short texts isa feasible option.
However, sarcasm seems tobe a subtle sentiment and calls for exploring fea-tures that capture not only semantic but also syn-tactic nuances.
The low recall of the negativesarcastic instances could be due to the fact that30% of the negative phrases are hashtags (e.g.,#don?tjudge, #smartmove, #killmenow, #sadlife,#runninglate, #asthmaticproblems, #idontwantit),that require term-splitting.Further analysis reveals that generally the pos-itive class has better F1-scores than the negativeclass across all domains, except for the SMS data.One possible reason for this could be the fact that,while in all data sets (Twitter train, Twitter test,Sarcasm test) the ratio of positive to negative in-stances is nearly 2:1, the SMS test set is the onlyone with class distribution different from the train-ing set (with less positive instances than negative).The extremely low F1-score for the neutral class isperhaps also due to the skewed class distribution,where in all data sets, the neutral instances onlymake up about 4 to 9% of the data.The positive class also has a better recall thanthe negative class across all domains, which sug-gests that the system is able to identify most of thepositive test instances, perhaps due to the biggerproportion of positive training instances as well aspositive words in the polarity lexicons.
One simpleway of improving the recall of the negative classcould be by increasing the number of negative in-stances in the training set.
In fact, in a prelimi-nary experiment with an increased number of neg-ative instances (resampled using SMOTE (Chawlaet al., 2002)), the macro-average F1-score of theSMS data set improved by 0.5 points and that ofthe Sarcasm set by almost 2 points.
However,there was no notable improvement in the Twitterand Live Journal test sets.We also ran some ablation experiments on thetest corpus after the submission to observe the in-fluence of individual features on the classification382POS.
NEG.
NEU.
AVG.P R F P R F P R FTwitter 87.6 89.7 88.6 82.4 76.2 79.2 23.3 28.2 25.5 83.90SMS 75.9 89.9 82.3 89.8 82.4 86.0 32.7 10.7 16.1 84.14LiveJournal 76.1 87.3 81.3 81.8 80.2 81.0 42.1 16.7 23.9 81.16Sarcasm 77.0 93.9 84.6 72.2 35.1 47.3 16.7 20.0 18.2 65.94Table 2: Macro-average F1-scores.
P, R and F represent precision, recall and F1-score, respectively.process.
Table 3 reports the macro-average F1-scores of the experiments.
The ?all features*?scores here are different from those submitted asthe four test corpora were tested individually hereas opposed to all instances mixed into one data set.The row ?- prior polarities?
indicates a feature setthat excludes the prior polarities feature, and its ef-fect on the F1-score.
MCB is the Majority ClassBaseline, whereas unigrams uses only the phraseunigrams, with no additional features.Twitter SMS Jour.
Sarc.MCB 39.65 31.45 33.40 39.80unigrams 81.85 82.15 79.95 74.85all features* 86.20 87.80 81.90 78.05- prior polarity -1.8 -0.1 -0.05 -1.95- lengths -0.3 0 -0.20 -1.3- punctuation -0.45 -0.45 +0.10 -2.95- emoticon lex -0.15 0 +0.05 0- word clusters -0.15 -1.25 +0.05 -0.25Table 3: Ablation tests: Trained on Twitter only.A few observations from the feature ablationstudy include:?
The prior polarities and lengths seem to betwo of the most distinguishing features forTwitter and Twitter Sarcasm, whereas forSMS data, the word clusters are quite useful.?
While for Twitter Sarcasm, punctuationseems to be the most important feature, ithas the opposite effect on the Live Journalblog data.
This may be because the punctua-tion features learned from Twitter data do nottranslate that well to blog data due to theirdissimilar writing styles.?
Even though the classifier was trained onTwitter data, it has quite a strong performanceon the SMS data, which is rather unsurprisingin retrospect as both genres have similar char-acter limits, which leads to creative spellingsand slang.?
While using all the features leads to almost 5F1-score points improvement over unigramsbaseline in Twitter, SMS and Sarcasm datasets, they increase only 2 F1-score points inLive Journal blog data set, suggesting thatthis feature set is only marginally suited forblog instances.
This prompted us to explorethe hypothesis: how well do SMS and LiveJournal data generalize to other domains, dis-cussed in the following section.5.2 Cross-domain ExperimentsIn this section, we test how well the classifierstrained on one type of text classify other types oftext.
In table 4, for example, the last row shows theresults of a model trained on Journal data (1000 in-stances) and tested on Twitter, SMS and Sarcasmtest sets, and 10-fold cross-validated on Journaldata.
Since this experiment measures the gener-alizability of different data sets, we randomly se-lected 500 positive and 500 negative instances foreach data set, in order to minimize the influenceof the size of the training data set on the classifi-cation process.
Note that this experiment does notinclude the neutral class.
As expected, the bestresults on the test sets are obtained when usingcross-validation (except on Twitter set).
However,the model built using SMS data has the best or thesecond-best result overall, which suggests that outof the three types of text, it is the SMS data thatgeneralize the best.TestTwitter SMS JournalTwitter (1000) 76.4 (cv) 80.2 78.1SMS (1000) 76.8 87.1 (cv) 79.4Journal (1000) 73.8 82.8 85.3 (cv)Table 4: Cross-domain training and tests.3836 ConclusionThis paper presents the details of our system thatparticipated in the subtask A of SemEval:2014:Sentiment Analysis in Twitter.
An SVM classifierwas trained on a feature set consisting of prior po-larities, word clusters and various Twitter-specificfeatures.
Our experiments indicate that prior po-larities are one of the most important features inthe sentiment analysis of phrases from short texts.Furthermore, a classifier trained on just tweets cangeneralize considerably well to other texts suchas SMS and blog sentences, but not to sarcasm,which calls for more research.
Lastly, SMS datageneralizes to other texts better than Twitter data.AcknowledgementsWe would like to thank the organizers of this taskfor their effort and the reviewers for their use-ful feedback.
This research is funded in part bythe Centre for Information Visualization and DataDriven Design (CIV/DDD) established by the On-tario Research Fund.ReferencesAmeeta Agrawal and Aijun An.
2013.
Kea:Expression-level sentiment analysis from Twitterdata.
In Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation, SemEval ?13,June.Amir Asiaee T., Mariano Tepper, Arindam Banerjee,and Guillermo Sapiro.
2012.
If you are happy andyou know it... tweet.
In Proceedings of the 21stACM International Conference on Information andKnowledge Management, CIKM ?12, pages 1602?1606, New York, NY, USA.
ACM.Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.Hall, and W. Philip Kegelmeyer.
2002.
SMOTE:Synthetic minority over-sampling technique.
Jour-nal of Artificial Intelligence Research, 16(1):321?357, June.Andrea Esuli and Fabrizio Sebastiani.
2006.
Senti-wordnet: A publicly available lexical resource foropinion mining.
In Proceedings of the 5th Confer-ence on Language Resources and Evaluation, pages417?422.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.CS224N Project Report, Stanford, pages 1?6.Tobias Gunther and Lenz Furrer.
2013.
Gu-mlt-lt:Sentiment analysis of short messages using linguis-tic features and stochastic gradient descent.
In Pro-ceedings of the Seventh International Workshop onSemantic Evaluation, SemEval ?13, June.Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-dan Zhu.
2013.
NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.
In Pro-ceedings of the seventh international workshop onSemantic Evaluation Exercises (SemEval-2013), At-lanta, Georgia, USA, June.Tetsuya Nasukawa and Jeonghee Yi.
2003.
Sentimentanalysis: capturing favorability using natural lan-guage processing.
In Proceedings of the 2nd inter-national conference on Knowledge capture, K-CAP?03, pages 70?77, New York, NY, USA.
ACM.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL-HLT, pages 380?390.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: sentiment analysis using summarization basedon minimum cuts.
In Proceedings of the 42nd An-nual Meeting on Association for Computational Lin-guistics, ACL ?04, Stroudsburg, PA, USA.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in natu-ral language processing - Volume 10, EMNLP ?02,pages 79?86, Stroudsburg, PA, USA.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
Semeval-2013 task 9:Sentiment analysis in twitter.
In Proceedings ofthe International Workshop on Semantic Evaluation(SemEval-2014), August.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,HLT ?05, pages 347?354, Stroudsburg, PA, USA.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.2013.
Semeval-2013 task 2: Sentiment analysis intwitter.
In Proceedings of the International Work-shop on Semantic Evaluation (SemEval-2013), June.384
