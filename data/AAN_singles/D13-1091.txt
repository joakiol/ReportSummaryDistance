Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 897?902,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsIs Twitter A Better Corpus for Measuring Sentiment Similarity?Shi Feng1, Le Zhang1, Binyang Li2,3, Daling Wang1, Ge Yu1, Kam-Fai Wong31Northeastern University, Shenyang, China2University of International Relations, Beijing, China3The Chinese University of Hong Kong, Shatin, N.T., Hong Kong{fengshi,wangdaling,yuge}@ise.neu.edu.cn, zhang777le@gmail.com{byli,kfwong}@se.cuhk.edu.hkAbstractExtensive experiments have validated the ef-fectiveness of the corpus-based method forclassifying the word?s sentiment polarity.However, no work is done for comparing d-ifferent corpora in the polarity classificationtask.
Nowadays, Twitter has aggregated hugeamount of data that are full of people?s senti-ments.
In this paper, we empirically evaluatethe performance of different corpora in sen-timent similarity measurement, which is thefundamental task for word polarity classifica-tion.
Experiment results show that the Twitterdata can achieve a much better performancethan the Google, Web1T and Wikipedia basedmethods.1 IntroductionMeasuring semantic similarity for words and shorttexts has long been a fundamental problem for manyapplications such as word sense disambiguation,query expansion, search advertising and so on.Determining the word?s polarity plays a criticalrole in opinion mining and sentiment analysis task.Usually we can detect the word?s polarity by mea-suring it?s semantic similarity with a positive seedword sep and a negative seed word sen respectively,as shown in Formula (1):SO(w) = sim(w, sep)?
sim(w, sen) (1)where sim(wi, wj) is the semantic similarity mea-surement method for the given word wi and wj .
Alot of papers have been published for designing ap-propriate similarity measurements.
One direction isto learn similarity from the knowledge base or con-cept taxonomy (Lin, 1998; Resnik, 1999).
Anoth-er direction is to learn semantic similarity with thehelp of large corpus such as Web or Wikipedia da-ta (Sahami and Heilman, 2006; Yih and Meek, 2007;Bollegala et al 2011; Gabrilovich and Markovitch,2007).
The basic assumption of this kind of methodsis that the word with similar semantic meanings of-ten co-occur in the given corpus.
Extensive experi-ments have validated the effectiveness of the corpus-based method in polarity classification task (Turney,2002; Kaji and Kitsuregawa, 2007; Velikovich et al2010).
For example, PMI is a well-known similari-ty measurement (Turney, 2002), which makes use ofthe whole Web as the corpus, and utilizes the searchengine hits number to estimate the co-occurrenceprobability of the give word pairs.
The PMI basedmethod has achieved promising results.
However,according to Kanayama?s investigation, only 60%co-occurrences in the same window in Web pagesreflect the same sentiment orientation (Kanayamaand Nasukawa, 2006).
Therefore, we may ask thequestion whether the choosing of corpus can changethe performance of sim and is there any better cor-pus than the Web page data for measuring the senti-ment similarity?Everyday, enormous numbers of tweets that con-tain people?s rich sentiments are published in Twit-ter.
The Twitter may be a good source for measuringthe sentiment similarity.
Compared with the Webpage data, the tweets have a higher rate of subjectivetext posts.
The length limitation can guarantee thepolarity consistency of each tweet.
Moreover, thetweets contain graphical emoticons, which can be897considered as natural sentiment labels for the corre-sponding tweets in Twitter.
In this paper, we attemptto empirically evaluate the performance of differen-t corpora in sentiment similarity measurement task.As far as we know, no work is done on this topic.2 The Characteristics of Twitter DataAs the world?s second largest SNS website, at theend of 2012 Twitter had aggregated more than 500million registered users, among which 200 millionwere active users .
More than 400 million tweets areposted every day.Several examples of typical posts from Twitter areshown below.
(1) She had a headache and feeling light headedwith no energy.
:((2) @username Nice work!
Looks like you had afun day.
I?m headed there Sat or Sun.
:)(3) I seen the movie on Direc Tv.
I ordered it andI really liked it.
I can?t wait to get it for blu ray!Excellent work Rob!We observe that comparing with the other corpus,the Twitter data has several advantages in measuringthe sentiment similarity.Large.
Users like to record their personal feelingsand talk about the trend topics in Twitter (Java et al2007; Kwak et al 2010).
So there are huge amountof subjective texts with various topics generated inthe millions of tweets everyday.
Further more, theflexible Twitter API makes these data easy to accessand collect.Length Limitation.
Twitter has a length limita-tion of 140 characters.
Users have limited space toexpress their feelings.
So the sentiments in tweet-s are usually concise, straightforward and polarityconsistent.Emoticons.
Users tend to utilize emoticons toemphasize their sentiment feelings.
According tothe statistics, about 8.1% tweets contain at least oneemoticon (Yang and Leskovec, 2011).
Since thetweets have the length limitation, the sentiments ex-pressed in these short texts are usually consistentwith the embedded emoticons, such as the word funand headache in above examples.In addition to the above advantages, there are al-so some disadvantages for measuring sentiment sim-ilarity using Twitter data.
The spam tweets thatcaused by advertisements may add noise and biasduring the similarity measurement.
The short lengthmay also bring in lower co-occurrence probabilityof words.
Some words may not co-occur with eachother when the corpus is small.
These disadvantagesset obstacles for measuring sentiment similarity byusing Twitter data as corpus.
In the experiment sec-tion, we will see if we can overcome these draw-backs and get benefit from the advantages of Twitterdata.3 The Corpus-based Sentiment SimilarityMeasurementsThe intuition behind the corpus-based semantic sim-ilarity measuring method is that the words with sim-ilar meanings tend to co-occur in the corpus.
Giventhe word wi, wj , we use the notation P (wi) to de-note the occurrence counts of word wi in the corpusC.
P (wi, wj) denotes the co-occurrence counts ofword wi and wj in C. In this paper we employ thecorpus-based version of the three well-known simi-larity measurements: Jaccard, Dice and PMI.CorpusJaccard(wi, wj)= P (wi,wj)P (wi)+P (wj)?P (wi,wj)(2)CorpusDice(wi, wj) =2?
P (wi, wj)P (wi) + P (wj)(3)CorpusPMI(wi, wj) = log2(P (wi,wj)NP (wi)NP (wj)N) (4)In Formula (4), N is the number of documents inthe corpus C. The above similarity measurementsmay have their own strengths and weaknesses.
Inthis paper, we utilize these classical measurementsto evaluate the quality of the corpus in polarity clas-sification task.Google is the world?s largest search engine, whichhas indexed a huge number of Web pages.
Us-ing the extreme large indexed Web pages as cor-pus, Cilibrasi and Vitanyi (2007) presented a methodfor measuring similarity between words and phras-es based on information distance and Kolmogorovcomplexity.
The search result page counts of Googlewere utilized to estimate the occurrence frequenciesof the words in the corpus.
Suppose wi, wj rep-resent the candidate words, the Normalized Google898Distance is defined as:NGD(wi, wj) =max{logP (wi),logP (wj)}?logP (wi,wj)logN?min{logP (wi),logP (wj)}(5)where P (wi) denotes page counts returned byGoogle using wi as keyword; P (wi, wj) denotes thepage counts by using wi and wj as joint keywords;N is the number of Web pages indexed by Google.Cilibrasi and Vitanyi have validated the effective-ness of Google distance in measuring the semanticsimilarity between concept words.Based on the above formulas, we compare theTwitter data with the Web and Wikipedia data as thesimilarity measurement corpus.
Given a candidateword w, we firstly measure its sentiment similar-ity with a positive seed word and a negative seedword respectively in Formula (1), and the differenceof sim is used to further detect the polarity of w.The above four similarity measurements serve assim with Web, Wikipedia and Twitter data as cor-pus.
Turney (2002) chose excellent and poor asseed words.
However, using isolated seed word-s may cause the bias problem.
Therefore, we fur-ther select two groups of seed words that are lackof sensitivity to context and form a positive seed setPS and a negative seed set NS (Turney, 2003).
TheFormula (1) can be rewritten as:SO(w) =?sep?PSsim(w, sep)?
?sen?NSsim(w, sen) (6)Based on the Formula(6) and the sentiment seedwords, we can measure the sentiment polarity of thegiven candidate words.4 Experiment4.1 Experiment SetupCorpus Preparing.
The Twitter corpus corre-sponds to the 476 million Twitter tweets (Yang andLeskovec, 2011), which includes over 476 millionTwitter posts from 20 million users, covering a 7month period from June 1, 2009 to December 31,2009.
We filter out the non-English tweets and thespam tweets that have only few words with URLs.The tweets that contain three or more trending topicsare also removed.
Finally, we construct the Twittercorpus that consists of 266.8 million English tweets.For calculating page counts in Web data, the candi-date words were launched to Google from February2013 to April 2013.
We also conduct the experi-ments on the Google Web 1T data that consists ofGoogle n-gram counts (frequency of occurrence ofeach n-gram) for 1 ?
n ?
5 (Brants and Franz,2006).
The Web 1T data provides a nice approxi-mation to the word co-occurrence statistics in Webpages in a predefined window size (1 ?
n ?
5).For example, the 5 gram Web1T data means the co-occurrence window size is 5.
The English Wikipediadump 1 we used was extracted at the end of March2013, which contained more than 13 million articles.We extracted the plain texts of the Wikipedia data asthe training corpus for the Formula (6).EvaluationMethod.
Two well-know sentiment lex-icons are utilized as gold standard for polarity clas-sification task.
The statistics of Liu?s sentiment lex-icon (Liu et al 2005) and MPQA subjectivity lexi-con (Wilson et al 2005) are shown in Table 1.
Foreach word w in the lexicons, we employ the Formu-la (6) to calculate the word?s polarity using differentcorpora.
If SO(w) > 0, the word w is classified in-to the positive category.
Otherwise if SO(w) < 0, itis classified into the negative category.
The accura-cy of the classification result is used to measure thequality of the corpus.Positive# Negative#Liu 2,006 4,783MPQA 2,304 4,153Table 1: Lexicon size4.2 Experiment ResultsFirstly, we chose the seed words excellent and pooras Turney?s (2002) settings.
The polarity classifica-tion accuracies are shown in Table 2.In Table 2, Google, Web1T, Wikipedia, Twitterrepresent the corpora that used in the experiment;CJ, CD, CP, GD represent the Formula (2) to For-mula (5) respectively.
We can see from the Table 2that the Twitter based method can achieve the bestperformance.
The rich sentiment information and1http://en.wikipedia.org/899Lexicon Corpus CJ CD CP GDLiuGoogle 0.5116 0.5117 0.5064 0.5076Web1T-5gram 0.3903 0.3903 0.3897 0.3864Web1T-4gram 0.3771 0.3771 0.3772 0.3227Wikipedia 0.5280 0.5280 0.5350 0.5412Twitter 0.5567 0.5567 0.5635 0.5635MPQAGoogle 0.4897 0.4890 0.4891 0.4864Web1T-5gram 0.3843 0.3843 0.3837 0.3783Web1T-4gram 0.3729 0.3729 0.3714 0.3225Wikipedia 0.5181 0.5181 0.5380 0.5344Twitter 0.5421 0.5421 0.5493 0.5494Table 2: Polarity classification accuracies using excellentand poor as seed wordsnatural window size (140 characters) have a posi-tive impact on determining the word?s polarity.
TheGoogle based method gets a lower accuracy, thismay be due to the length of Web documents whichcan not usually guarantee the semantic consistencyin the returned data.
Even though two words appearin one page (returned by Google), they might not besemantically related.
Furthermore, the Google basedmethod is time-consuming, because we have to peri-odically send queries in order to avoid being blockedby Google.
The Web1T based method gets a muchworse accuracy.
After detailed analysis, we find thatalthough the small window size (4 or 5) can guar-antee the semantic consistency, the short length alsobrings in lower co-occurrence probability.
Statisticsshow that about 38% SO values are zero when usingWeb1T corpus.
Due to the short length, the Twitterdata also suffers from the low co-occurrence prob-lem.To tackle the low co-occurrence problem, the seedword sets are selected as Turney?s (2003) settings.The positive word set PS={good, nice, excellent,positive, fortunate, correct, superior} and negativeword set NS = {bad, nasty, poor, negative, unfortu-nate, wrong, inferior} for the Formula (6).
Theseseed words have been verified to be effective in Tur-ney?s paper for polarity classification.
The experi-ment results are shown in Table 3.Table 3 shows that the performance of Twitter cor-pus is much improved since the multiple seed wordsalleviate the problem of low co-occurrence probabil-ity in tweets.
Generally, when using the seed wordgroups the Twitter can achieve a much better per-formance than all the other corpora.
The improve-ments are statistically significant (p-value < 0.05).Lexicon Corpus CJ CD CP GDLiuGoogle 0.4859 0.4936 0.4884 0.5060Web1T-5gram 0.5785 0.5785 0.3963 0.5782Web1T-4gram 0.5766 0.5766 0.3872 0.5775Wikipedia 0.6226 0.6225 0.5957 0.6145Twitter 0.6678 0.6678 0.6917 0.6457Twitter+ 0.6921 0.6921 0.7273 0.6599MPQAGoogle 0.5108 0.5225 0.5735 0.5763Web1T-5gram 0.5737 0.5737 0.4225 0.5718Web1T-4gram 0.5749 0.5749 0.3329 0.4797Wikipedia 0.6086 0.6085 0.5773 0.5985Twitter 0.6431 0.6431 0.6671 0.6253Twitter+ 0.6665 0.6665 0.7001 0.6383Table 3: Polarity classification accuracies using the seedword groupsWe further add the emoticons ?:)?
and ?:(?
into theseed word groups, denoted by Twitter+ in Table 3.The emoticons are natural sentiment labels.
We cansee that the performances are further improved byconsidering emoticons as seed words.
The aboveexperiment results have validated the effectivenessof Twitter data as a better corpus for measuring thesentiment similarity.
The results also reveal the po-tential usefulness of Twitter corpus in semantic sim-ilarity measurement.5 Related WorkDetecting the polarity of words is the fundamentalproblem for most of sentiment analysis tasks (Hatzi-vassiloglou and McKeown, 1997; Pang and Lee,2007; Feldman, 2013).Many methods have been proposed to measurethe words?
or short texts similarity based on largecorpus (Sahami and Heilman, 2006; Yih and Meek,2007; Gabrilovich and Markovitch, 2007).
Bolle-gala et al(2011) submitted the word to the searchengine, and the related result pages were employedto represent the meaning of the original word.
Mi-halcea et al(2006) proposed a method to measurethe semantic similarity of words or short texts, con-sidering both corpus-based and knowledge-based in-formation.
Although the previous algorithms haveachieved promising results, there are no work doneon evaluating the quality of different corpora.Mohtarami et al(2012; 2013a; 2013b) intro-duced the concept of sentiment similarity, whichwas considered as different from the traditional se-mantic similarity, and more focused on revealing theunderlying sentiment relations between words.
Mo-900htarami et al(2013b) proposed a hidden emotion-al model to calculating the sentiment similarity ofword pairs.
However, the impact of the different cor-pora is not considered for this task.Mohammad et al(2013) generated word-sentiment association lexicons from Tweets with thehelp of hashtags and emoticons.
Pak and Paroubek(2010) collected tweets with happy and sad emoti-cons as training corpus, and built sentiment classi-fier based on traditional machine learning methods.Brody and Diakopoulos (2011) showed that length-ening was strongly associated with subjectivity andsentiment in tweets.
Davidov et al(2010) treated 50Twitter tags and 15 smileys as sentiment labels anda supervised sentiment classification framework wasproposed to classify the tweets.
The previous litera-tures have showed that the emoticons can be treatedas natural sentiment labels of the tweets.6 Conclusion and Future WorkThe quality of corpus may affect the performanceof sentiment similarity measurement.
In this pa-per, we compare the Twitter data with the Google,Web1T and Wikipedia data in polarity classificationtask.
The experiment results validate that when us-ing the seed word groups the Twitter can achieve amuch better performance than the other corpora andadding emoticons as seed words can further improvethe performance.
It is observed that the twitter cor-pus is a potential good source for measuring senti-ment similarity between words.
In future work, weintend to design new similarity measurements thatcan make best of the advantages of Twitter data.AcknowledgmentsThis research is partially supported by Gener-al Research Fund of Hong Kong (No.
417112)and Shenzhen Fundamental Research Program (J-CYJ20130401172046450).
This research is al-so supported by the State Key Development Pro-gram for Basic Research of China (Grant No.2011CB302200-G), State Key Program of Nation-al Natural Science of China (Grant No.
61033007),National Natural Science Foundation of China(Grant No.
61370074, 61100026), and the Funda-mental Research Funds for the Central Universities(N120404007).ReferencesThorsten Brants and Alex Franz.
2006.
Web 1t 5-gramversion 1.
Linguistic Data Consortium, ISBN: 1-58563-397-6, Philadelphia.Samuel Brody and Nicholas Diakopoulos.
2011.Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!
UsingWord Lengthening to Detect Sentiment in Microblogs.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages562?570, Edinburgh, UK, ACL.Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizu-ka.
2011.
A Web Search Engine-Based Approach toMeasure Semantic Similarity between Words.
IEEETransactions on Knowledge and Data Engineering,23(7): 977?990.Rudi Cilibrasi and Paul Vitanyi.
2007.
The Google Simi-larity Distance.
IEEE Transactions on Knowledge andData Engineering, 19(3): 370?383.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced Sentiment Learning Using Twitter Hashtagsand Smileys.
In Proceedings of the 23rd InternationalConference on Computational Linguistics, pages 241?249, Beijing, China, ACL.Ronen Feldman.
2013.
Techniques and Applications forSentiment Analysis.
Communications of the ACM,56(4):82?89.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting Semantic Relatedness Using Wikipedia-basedExplicit Semantic Analysis.
In Proceedings of the20th International Joint Conference on Artificial In-telligence, pages 1606?1611, Hyderabad, India.Vasileios Hatzivassiloglou and Kathleen McKeown.1997.
Predicting the Semantic Orientation of Adjec-tives.
In Proceedings of the 35th Annual Meeting ofthe Association for Computational Linguistics, pages174?181, Madrid, Spain, ACL.Akshay Java, Xiaodan Song, Tim Finin, and Belle L. T-seng.
2007.
Why We Twitter: An Analysis of a Mi-croblogging Community.
In Proceedings of the 9thInternational Workshop on Knowledge Discovery onthe Web and 1st International Workshop on Social Net-works Analysis, pages 118?138, San Jose, CA, USA,Springer.Nobuhiro Kaji and Masaru Kitsuregawa.
Building Lex-icon for Sentiment Analysis from Massive Collec-tion of HTML Documents.
In Proceedings of the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, pp.
1075?1083, Prague, CzechRepublic, ACL.Hiroshi Kanayama and Tetsuya Nasukawa.
2006.
Ful-ly Automatic Lexicon Expansion for Domain-oriented901Sentiment Analysis.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 355?363, Sydney, Australia, ACL.Haewoon Kwak, Changhyun Lee, Hosung Park, and SueB.
Moon.
2010.
What is Twitter, a Social Network or aNews Media?
In Proceedings of the 19th Internation-al Conference on World Wide Web, pages 591?600,Raleigh, North Carolina, USA, ACM.Dekang Lin.
1998.
Automatic Retrieval and Cluster-ing of Similar Words.
In Proceedings of the 17th In-ternational Conference on Computational Linguistics,pages 768?774, Montreal, Quebec, Canada, ACL.Bing Liu, Minqing Hu, and Junsheng Cheng.
2005.Opinion Observer: Analyzing and Comparing Opin-ions on the Web.
In Proceedings of the 14th interna-tional conference on World Wide Web, pages 342?351,Chiba, Japan, ACM.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and Knowledge-based Measuresof Text Semantic Similarity.
In Proceedings of the 21stNational Conference on Artificial Intelligence and the18th Innovative Applications of Artificial IntelligenceConference, pages 775?780, Boston, Massachusetts,USA, AAAI Press.Mitra Mohtarami, Hadi Amiri, Man Lan, Thanh PhuTran, and Chew Lim Tan.
2012.
Sense Sentimen-t Similarity: An Analysis.
In Proceedings of theTwenty-Sixth AAAI Conference on Artificial Intelli-gence, pages 1706?1712, Toronto, Ontario, Canada,AAAI Press.Mitra Mohtarami, Man Lan, and Chew Lim Tan.
2013a.From Semantic to Emotional Space in ProbabilisticSense Sentiment Analysis.
In Proceedings of theTwenty-Seventh AAAI Conference on Artificial Intel-ligence, pages 711?717, Bellevue, Washington, USA,AAAI Press.Mitra Mohtarami, Man Lan, and Chew Lim Tan.
2013b.Probabilistic Sense Sentiment Similarity through Hid-den Emotions.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Linguis-tics, pages 983?992, Sofia, Bulgaria, ACL.Saif M. Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets.
In Proceedingsof the seventh international workshop on Semantic E-valuation Exercises, Atlanta, Georgia, USA, ACL.Alexander Pak and Patrick Paroubek.
2010.
Twitter as aCorpus for Sentiment Analysis and Opinion Mining.In Proceedings of the 2010 International Conferenceon Language Resources and Evaluation, pages 1320?1326, Valletta, Malta, ELRA.Philip Resnik.
1999.
Semantic Similarity in a Taxonomy:An Information based Measure and Its Application toProblems of Ambiguity in Natural Language.
Journalof Artificial Intelligence Research, 11:95?130.Mehran Sahami and Timothy D. Heilman.
2006.
A Web-based Kernel Function for Measuring the Similarity ofShort Text Snippets.
In Proceedings of the 15th inter-national conference on World Wide Web, pages 377?386, Edinburgh, Scotland, UK, ACM.Bo Pang and Lillian Lee.
2007.
Opinion Mining and Sen-timent Analysis.
Foundations and Trends in Informa-tion Retrieval, 2(1-2):1?135.Peter D. Turney.
2002.
Thumbs Up or Thumbs Down?Semantic Orientation Applied to Unsupervised Classi-fication of Reviews.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics, pages 417?424, Philadelphia, PA, USA, ACL.Peter Turney and Michael Littman.
2003.
Measuringpraise and criticism: Inference of semantic orientationfrom association.
ACM Transaction Information Sys-tem, 21(4): 315?346.Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-nan, and Ryan T. McDonald.
The Viability of Web-derived Polarity Lexicons.
In Proceedings of the 2010North American Chapter of the Association of Compu-tational Linguistics, pp.
777?785, Los Angeles, Cali-fornia, USA, ACL.Theresa Wilson, Janyce Wiebe, and Paul Hoffman-n. 2005.
Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.
In Proceedings of the 2005Human Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 347?354, Vancouver, British Columbi-a, Canada, ACL.Jaewon Yang and Jure Leskovec.
2011.
Patterns of Tem-poral Variation in Online Media.
In Proceedings ofthe Forth International Conference on Web Search andWeb Data Mining, pages 177?186, Hong Kong, Chi-na, ACM.Wen-tau Yih and Christopher Meek.
2007.
ImprovingSimilarity Measures for Short Segments of Text.
InProceedings of the 22nd AAAI Conference on ArtificialIntelligence, pages 1489?1494, Vancouver, BritishColumbia, Canada, AAAI Press.902
