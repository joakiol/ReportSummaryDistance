Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 929?936,Sydney, July 2006. c?2006 Association for Computational LinguisticsWord Sense Disambiguation using lexical cohesion in the contextDongqiang Yang | David M.W.
PowersSchool of Informatics and EngineeringFlinders University of South AustraliaPO Box 2100, AdelaideDongqiang.Yang|David.Powers@flinders.edu.auAbstractThis paper designs a novel lexical hub todisambiguate word sense, using both syn-tagmatic and paradigmatic relations ofwords.
It only employs the semantic net-work of WordNet to calculate word simi-larity, and the Edinburgh AssociationThesaurus (EAT) to transform contextualspace for computing syntagmatic andother domain relations with the targetword.
Without any back-off policy theresult on the English lexical sample ofSENSEVAL-21 shows that lexical cohe-sion based on edge-counting techniquesis a good way of unsupervisedly disam-biguating senses.1 IntroductionWord Sense Disambiguation (WSD) is generallytaken as an intermediate task like part-of-speech(POS) tagging in natural language processing,but it has not so far achieved the sufficient preci-sion for application as POS tagging (for the his-tory of WSD, cf.
Ide and V?ronis (1998)).
It ispartly due to the nature of its complexity anddifficulty, and to the widespread disagreementand controversy on its necessity in language en-gineering, and to the representation of the sensesof words, as well as to the validity of its evalua-tion (Kilgarriff and Palmer, 2000).
However theendeavour to automatically achieve WSD hasbeen continuous since the earliest work of the1950?s.In this paper we specifically investigate therole of semantic hierarchies of lexical knowledgeon WSD, using datasets and evaluation methodsfrom SENSEVAL (Kilgarriff and Rosenzweig,1 http://www.senseval.org/2000) as these are well known and accepted inthe community of computational linguistics.With respect to whether or not they employthe training materials provided, SENSEVALroughly categorizes the participating systemsinto ?unsupervised systems?
and ?supervisedsystems?.
Those that don?t use the training dataare not usually truly unsupervised, being basedon lexical knowledge bases such as dictionaries,thesauri or semantic nets to discriminate wordsenses; conversely the ?supervised?
systemslearn from corpora marked up with word senses.The fundamental assumption, in our ?unsu-pervised?
technique for WSD in this paper, isthat the similarity of contextual features of thetarget with the pre-defined features of its sense inthe lexical knowledge base provides a quantita-tive cue for identifying the true sense of the tar-get.The lexical ambiguity of polysemy and ho-monymy, whose distinction is however not abso-lute as sometimes the senses of word may be in-termediate, is the main object of WSD.
Verbs,with their more flexible roles in a sentence, tendto be more polysemous than nouns, so worseningthe computational feasibility.
In this paper wedisambiguated the sense of a word after its POStagging has assigned them either a noun or a verbtag.
Furthermore, we deal with nouns and verbsseparately.2 Some previous work on WSD usingsemantic similaritySussna (1993) utilized the semantic network ofnouns in WordNet to disambiguate term sensesto improve the precision of SMART informationretrieval at the stage of indexing, in which heassigned two different weights for both direc-tions of edges in the network to compute thesimilarity of two nodes.
He then exploited themoving fixed size window to minimize the sum929of all combinations of the shortest distancesamong target and context words.Pedersen et al (2003) extended Lesk?s defini-tion method (1986) to discriminate word sensethrough the definitions of both target and its IS-Arelatives, and achieved a better result in the Eng-lish lexical sample task of SENSEVAL-2, com-pared with other edge-counting or statistical es-timation metrics on WordNet.Humans carefully select words in a sentence toexpress harmony or cohesion in order to ease theambiguity of the sentence.
Halliday and Hasan(1976) argued that cohesive chains unite textstructure together through reiteration of referenceand lexical semantic relations (superordinate andsubordinate).
Morris and Hirst (1991) suggestedbuilding lexical chains is important in the resolu-tion of lexical ambiguity and the determinationof coherence and discourse structure.
They ar-gued that lexical chains, which cover the multi-ple semantic relations (systematic and non-systematic), can transform the context settinginto the computational one to narrow down thespecific meaning of the target, manually realiz-ing this with the help of Roget?s Thesaurus.
Theydefined a lexical chain within Roget?s very gen-eral hierarchy, in which lexical relationships aretraced through a common category.Hirst and St-Onge (1997) define a lexicalchain using the syn/antonym and hyper/hyponymlinks of WordNet to detect and correct malaprop-isms in context, in which they specified threedifferent weights from extra-strong to mediumstrong to score word similarity to decide the in-serting sequence in the lexical chain.
They firstcomputationally employed WordNet to form a?greedy?
lexical chain as a substitute of the con-text to solve the matter of malapropism, wherethe word sense is decided by its preceding words.Around the same time, Barzilay and Elhadad(1997) realized a ?non-greedy?
lexical chain,which determined the word sense after process-ing of all words, in the context of text summari-zation.In this paper we propose an improved lexicalchain, the lexical hub, that holds the target to bedisambiguated as the centre, replacing the usualchain topology used in text summarization andcohesion analysis.
In contrast with previousmethods we only record the lexical hub of eachsense of the target, and we don?t keep track ofother context words.
In other words, after thecomputation of lexical hub of the target, we canimmediately produce the right sense of the targeteven though the senses of the context words arestill in question.
We also transform the contextsurroundings through a word association thesau-rus to explore the effect of other semantic rela-tionships such as syntagmatic relation againstWSD.3 Selection of knowledge basesWordNet (Fellbaum, 1998) provides a fine-grained enumerative semantic net that is com-monly used to tag the instances of English targetwords in the tasks of SENSEVAL with differentsenses (WordNet synset numbers).
WordNetgroups related concepts into synsets and linksthem through IS-A and PART-OF links, empha-sizing the vertical interaction between the con-cepts that is much paradigmatic.Although WordNet can capture the fine-grained paradigmatic relations of words, anothertypical word relationship, syntagmatic connect-edness, is neglected.
The syntagmatic relation-ship, which is often characterized with differentPOS tag, and frequently occurs in corpora orhuman brains, plays a critical part in cross-connecting words from different domains or POStags.It should be noted that WordNet 2.0 makessome efforts to interrelate nouns and verbs usingtheir derived lexical forms, placing associatedwords under the same domain.
Although someverbs have derived noun forms that can bemapped onto the noun taxonomy, this mappingonly relates the morphological forms of verbs,and still lacks syntagmatic links between words.The interrelationship of noun and verb hierar-chies is far from complete and only a supplementto the primary IS-A and PART-OF taxonomiesin WordNet.
Moreover as WordNet generallyconcerns the paradigmatic relations (Fellbaum,1998), we have to seek for other lexical knowl-edge sources to compensate for the shortcomingsof WordNet in WSD.The Edinburgh Association Thesaurus2 (EAT)provides an associative network to account forword relationship in human cognition after col-lecting the first response words for the stimuluswords list (Kiss et al, 1973).
Take the words eatand food for example.
There is no direct pathbetween the concepts of these two words in thetaxonomy of WordNet (both as noun and verb),except in the gloss of the first and third sense ofeat to explain ?take in solid food?, or ?take infood?, which glosses are not regularly or care-2 http://www.eat.rl.ac.uk/930fully organized in WordNet.
However in EATeat is strongly associated with food, and whentaking eat as a stimulus word, 45 out of 100 sub-jects regarded food as the first response.Yarowsky (1993) indicated that the objects ofverbs play a more dominant role than their sub-jects in WSD and nouns acquire more stable dis-ambiguating information from their noun or ad-jective modifiers.In the case of verbs association tests, it is alsoreported that more than half the response wordsof verbs (the stimuli) are syntagmatically related(Fellbaum, 1998).
In experiments of examiningthe psychological plausibility of WordNetrelationships, Chaffin et al (1994) stated thatonly 30.4% of the responses of 75 verb stimulibelongs to verbs, and more than half of the re-sponses are nouns, of which nearly 90% arecategorized as the arguments of the verbs.Sinopalnikova (2004) also reported that thereare multiple relationships found in word associa-tion thesaurus, such as syntagmatic, paradigmaticrelations, domain information etc.In this paper we only use the straightforwardforms of context words separating the effect ofsyntactic dependence on the WSD.
As a supple-ment of enriching word linkage in the WSD, weretrieve the lexical knowledge from both Word-Net and EAT.
We first explore the function ofsemantic hierarchies of WordNet on WSD, andthen we transform the context word with EAT toinvestigate whether other relationships can im-prove WSD.4 System designIn order to find semantically related words tocohesively form lexical hubs, we first employ thetwo word similarity algorithms of Yang andPowers (2005; 2006) that use WordNet to com-pute noun similarity and verb similarity respec-tively.
We next construct the lexical hub for eachtarget sense to assemble the similarity score be-tween the target and its context words together.The maximum score of these lexical hubs spe-cifically predicts the real sense of the target, alsoimplicitly captures the cohesion and real mean-ing of the word in its context.4.1 Similarity metrics on nounsYang and Powers (2005) designed a metric,???
*)2,1( tccSim =utilizing both IS-A and PART-OF taxonomies ofWordNet to measure noun similarity, and theyargued that the similarity of nouns is the maxi-mum of all their concept similarities.
They de-fined the similarity (Sim) of two concepts (c1 andc2) with a link type factor (?t) to specify theweights of different link types (t) (syn/antonym,hyper/ hyponym, and holo/meronym) in theWordNet, and a path type factor (?t) to reducethe uniform distance of the single link, alongwith a depth factor (?)
to restrict the maximumsearching distance between concepts.
Since theirmetric on noun similarity is significantly betterthan some popular measures and even outper-forms some subjects on a standard data set, weselected it as a measure on noun similarity in ourWSD task.4.2 Similarity metrics on verbsYang and Powers (2006) also redesigned theirnoun model,itccDistitstrccSim ???
)2,1(1**)2,1(=?=to accommodate verb case, which is harder todeal with in the shallow and incomplete taxon-omy of verbs in WordNet.
As an enhancement tothe uniqueness of verb similarity they also con-sider three fall-back factors, where if ?str is 1normally but successively falls back to:?
?stm: the verb stem polysemy ignoring senseand form?
?der: the cognate noun hierarchy of the verb?
?gls: the definition of the verbThey also defined two alternate search proto-cols: rich hierarchy exploration (RHE) with nomore than six links and shallow hierarchy explo-ration (SHE) with no more than two links.One minor improvement to the verb model intheir system comes from comparing the similar-ity of verbs and nouns using the noun modelmetric for the derived noun form of verb.
It thusallows us to compare nouns and verbs and avoidsthe limitation of having to have the same POStag.4.3 Depth in WordNetYang and Powers fine-tuned the parameters ofthe noun and verb similarity models, findingthem relatively insensitive to the precise values,and we have elected to use their recommendedvalues for the WSD task.
But it is worthmentioning that their optimal models areachieved in purely verbal data sets, i.e.
thesimilarity score is context-free.931In their models, the depth in the WordNet, i.e.the distance between the synsets of words (?
), isindeed an outside factor which confines thesearching scope to the cost of computation anddepends on the different applications.
If we tunedit using the training data set of SENSEVAL-2 weprobably would assign different values and mightachieve better results.
Note that for both nounsand verbs we employ RHE (rich hierarchy explo-ration) with ?
= 2 making full use of the taxon-omy of WordNet and making no use of glosses.4.4 How to setup the selection standard forthe sensesOther than making the most of WSD results, ourmain motive for this paper is to explore to whatextent the semantic relationships will reach accu-racy, and to fully acknowledge the contributionof this single attribute working on WSD, whichis encouraged by SENSEVAL in order to gainfurther benefits in this field (Kilgarriff andPalmer, 2000).
Without any definition, which ispreviously surveyed by Lesk (1986) and Peder-sen et al (2003), we screen off the definition fac-tor in the metric of verb similarity, with the in-tention of focusing on the taxonomies of Word-Net.Assuming that the lexical hub for the rightsense would maximize the cohesion with otherwords in the discourse, we design six differentstrategies to calculate the lexical hub in its unor-dered contextual surroundings.We first put forward three metrics to measureup the similarity of the senses of the target andthe context word:?
The maximized sense similarity( )),(max),( , jikjikmax CTSimCTSim =where T denotes the target, Tk is the kthsense of the target; Ci is the ith context wordin a fixed window size around the target, Ci,jthe jth sense of Ci.
Note that T and C can beany noun and verb, along with Sim the met-rics of Yang and Powers.?
The average of sense similarity?
?= ==mjmjjikjikikave CTLinksCTSimCTSim1 1,, ),(),(),(where Links(Tk,Ci,j)=1, if Sim(Tk,Ci,j)>0, oth-erwise 0.?
The sum of sense similarity?==mjjikiksum CTSimCTSim1, ),(),(where m is the total sense number of Ci.Subsequently we can define six distinctiveheuristics to score the lexical hub in the follow-ing parts:?
Heuristic 1 ?
Sense Norm  (HSN)???????
?= ?
?= =liliikikmaxkCTLinkwCTSimTSense1 1),(),(maxarg)(where Linkw(Ti)=1 if Simmax(Tk,Ci)>0, oth-erwise 0?
Heuristic 2 ?
Sense Max (HSM)An unnormalized version of HSN is:???????
?= ?=liikmaxkCTSimTSense1),(maxarg)(?
Heuristic 3 ?
Sense Ave (HSA)Taking into account all of the links betweenthe target and its context word, the correctsense of the target is:???????
?= ?=liikavekCTSimTSense1),(maxarg)(?
Heuristic 4 ?
Sense Sum (HSS)The unnormalized version of HSA is:???????
?= ?=liiksumkCTSimTSense1),(maxarg)(?
Heuristic 5 ?
Word Linkage (HWL)The straightforward output of the correctsense of the target in the discourse is to countthe maximum number of context wordswhose similarity scores with the target arelarger than zero:???????
?= ?=liikkCTLinkwTSense1),(maxarg)(?
Heuristic 6 ?
Sense Linkage (HSL)No matter what kind of relations between thetarget and its context are, the sense of thetarget, which is related to the maximumcounts of senses of all its context words, isscored as the right meaning:???????
?= ?
?= =limjjikkCTLinksTSense1 1, ),(maxarg)(Therefore the lexical hub of each sense of thetarget only relies on the interaction of the targetand its each context word, rather than of the con-text words.
The implication is that the lexicalhub only disambiguates the real sense of the tar-932get other than the real meaning of the contextword; the maximum scores or link numbers (onthe level of words or senses) in the six heuristicssuggest that the correct sense of the target shouldcohere with as many words or their senses aspracticable in the discourse.When similarity scores are ties we directlyproduce all of the word senses to prevent us fromguessing results.
Some WSD systems in SEN-SEVAL handle tied scores simply using the firstsense (in WordNet) of the target as the realsense.
It is no doubt that the skewed distributionof word senses in the corpora (the first sense of-ten captures the dominant sense) can benefit theperformance of the systems, but at the same timeit mixes up the contribution of the semantic hier-archy on WSD in our system.5 ResultsWe evaluate the six heuristics on the Englishlexical sample of SENSEVAL-2, in which eachtarget word has been POS-tagged in the trainingpart.
With the absence of taxonomy of adjectivesin WordNet we only extract all 29 nouns and all29 verbs from a total of 73 lexical targets, andthen we subcategorize the test dataset into 1754noun instances and 1806 verb instances.
Sincethe sample of SENSEVAL-2 is manually sense-tagged with the sense number of WordNet 1.7and our metrics are based on its version 2.0, wetranslate the sample and answer format into 2.0in accordance with the system output format.Finally, we find that each noun target has 5.3senses on average and each verb target 16.4senses.
Hence the baseline of random selectionof senses is the reciprocal of each average sensenumber, i.e.
separately 18.9 percent for nounsand 6 percent for verbs.In addition, SENSEVAL-2 provides a scoringsoftware with 3 levels of schemes, i.e.
fine-grained, coarse-grained and mixed-grained toproduce precision and recall rates to evaluate theparticipating systems.
According to the SEN-SEVAL scoring system, as we always give atleast one answer, the precision is identical to therecall under the separate noun and verb datasets.So we just evaluate our systems in light of accu-racy.
We tested the heuristics with fine-grainedprecision, which required the exact match of thekey to each instance.5.1 ContextWithout any knowledge of domain, frequencyand pragmatics to guess, word context is the onlyway of labeling the real meaning of word.
Basi-cally a bag of context words (after morphologicalanalyzing and filtering stop-words) or the fine-grained ones (syntactic role, selection preferenceetc.)
can provide cues for the target.
We proposeto merely use a bag of words to feed into eachheuristic in case of losing any valuable informa-tion in the disambiguation, and preventing fromany interference of other clues except the seman-tic hierarchy of WordNet.The size of the context is not a definitive fac-tor in WSD, Yarowsky (1993) suggested the sizeof 3 or 4 words for the local ambiguity and 20/50words for topic ambiguity.
He also employedRoget?s Thesaurus in 100 words of window toimplement WSD (Yarowsky, 1992).
To investi-gate the role of local context and topic contextwe vary the size of window from one word dis-tance away to the target (left and right) until 100words away in nouns or 60 in verbs, until thereare no increases in the context of each instance.0.250.270.290.310.330.350.370.390.410.430.452 5 10 20 30 40 50 60 70 80 90 100contextaccuracyHSNHSMHSAHSSHWLHSLFigure 1: the result of noun disambiguation withdifferent size of context in SENSEVAL 20.050.070.090.110.130.150.170.190.210.230.250.270.290.310.330.350.371 2 3 4 5 10 20 30 40 50 60contextaccuracyHSNHSMHSAHSSHWLHSLFigure 2: the result of verb disambiguation withdifferent size of context in SENSEVAL 2Noun and verb disambiguation results are re-spectively displayed in Figure 1 and 2.
Since theperformance curves of the heuristics turned intoflat and stable (the average standard deviationsof the six curves of nouns and verbs is around0.02 level before 60 and 20, after that approxi-933mately 0.001 level), optimal performance isreached at 60 context words for nouns and 20words for verbs.
These values are used as pa-rameters in subsequent experiments.5.2 Transformed context (EAT)0.250.270.290.310.330.350.370.390.410.430.450.47context srandrs sr rs srorrsdifferent contextsaccuracyHSNHSMHSAHSSHWLHSLFigure 3: the results of nouns disambiguation ofSENSEVAL-2 in the transformed context spaces0.050.070.090.110.130.150.170.190.210.230.250.270.290.310.330.350.370.39context srandrs sr rs srorrsdifferent contextsaccuracyHSNHSMHSAHSSHWLHSLFigure 4: the results of verbs disambiguationof SENSEVAL-2 in the transformed contextspacesAlthough our metrics can measure the similarityof nouns and verbs through the derived relatedform of verbs (not from the derived verbs ofnouns as a consequence of the shallowness ofverb taxonomy of WordNet), we still can?t com-pletely rely on WordNet, which focuses on theparadigmatic relations of words, to fully coverthe complexity of contextual happenings ofwords.Since the word association norm captures bothsyntagmatic and pragmatic relations in words,we transform the context words of the target intoits associated words, which can be retrieved inthe EAT, to augment the performance of thelexical hub.There are two word lists in the EAT: one listtakes each head word as a stimulus word, andthen collects and ranks all response words ac-cording to their frequency of subject consensus;the other list is in the reverse order with the re-sponse as a head word and followed by the elicit-ing stimuli.
We denote the stimulus/response setof word as SR, respond/stimulus as RS.
Apartfrom that we symbolize SRANDRS as theintersection of SR and RS, along with SRORRSas the union set of SR and RS.
Then for eachcontext word we retrieve its corresponding wordsin each word list and calculate the similarity be-tween the target and these words including thecontext words.As a result we transform the original contextspace of each target into an enriched contextspace under the function of SR, RS, SRANDRSor SRORRS.We take the respective 60 context words ofnouns and 20 words of verbs as the referencepoints for the transferred context experiment,since after that the performance curves of theheuristics turned into flat and stable (the averagestandard deviations of the six curves of nounsand verbs is around 0.02 level before 60, afterthat approximately 0.001 level).After the transformations, the noun and verbresults are respectively demonstrated in Figure 3and 4.6 Comparison with other techniques.0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5Baseline RandomBaseline LeskBaseline Lesk DefJ&CP&L_vectorP&L_extendHWL_ContextHSL_ContextUNED-LS-UDIMAPIIT 1IIT 2HWL_SRORRSHSL_SRORRSaccuracynounverbFigure 5: comparisons of HWL and HSL withother unsupervised systems and similarity met-ricsPedersen et al (2003) in the work of evaluatingdifferent similarity techniques based on Word-Net, realized two variants of Lesk?s methods:extended gloss overlaps (P&L_extend) and glossvector (P&L_vector), as well as evaluating themin the English lexical sample of SENSEVAL-2.The best edge-counting-based metric that theymeasured are from Jiang and Conrath (1997)(J&C).934Accordingly, without the transformation ofEAT, we compare our results of HWL and HSL(denoted as HWL_Context and HSL_Context)with the above methods (picking up their optimalvalues).
The results are illustrated in Figure 5.
Atthe same time we also list three baselines for un-supervised systems (Kilgarriff and Rosenzweig,2000), which are Baseline Random (randomlyselecting one sense of the target), Baseline Lesk(overlapping between the examples and defini-tions of and unsupervised systems in SEN-SEVAL-2 each sense of the target and contextwords), and its reduced version, i.e.
BaselineLesk Def (only definition).We further compare HWL and HSL with theintervention of SRORRS of EAT (denoted asHWL_SRORRS and HSL_ SRORRS) with otherunsupervised systems that employ no trainingmaterials of SENSEVAL-2, which are respec-tively:?
IIT 1 and IIT 2: extended the WordNet glossof each sense of the target, along with its su-perordinate and subordinate node?s glosses,without back-off policies.?
DIMAP: employed both WordNet and theNew Oxford Dictionary of English.
With thefirst sense as a back-off when tied scores oc-curred.?
UNED-LS-U: for each sense of the target,they enriched the sense describer through thefirst five hyponyms of it and a dictionarybuilt from 3200 books from Project Guten-berg.
They adopted a back-off policy to thefirst sense and discarded the senses account-ing for less than 10 percent of files in Sem-Cor).7 Conclusion and discussion7.1 Local context and topic contextOn the analysis of standard deviation of preci-sion on different stage in Figure 1 and 2 we canconclude that the optimum size for HSN to HSSwas ?10 words for nouns, reflecting a sensitivityto only local context, whilst HWL and HSL re-flected significant improvement up to ?60 re-flecting a sensitivity to topical context.
In thecase of verbs HSA showed little significant con-text sensitivity, HSN showed some positive sen-sitivity to local context but increasing beyond ?5had a negative effect, HSM and HSS to HSLshowed some sensitivity to broader topical con-text but this plateaued around ?20 to 30.7.2 The analysis of different heuristics.HWL and HSL were clearly superior for bothnoun and verb tasks, with the superiority of HSLbeing significantly greater and more comparablebetween noun and verb tasks with the differencescarcely reaching significance.
These observa-tions remain true with the addition of the EATinformation.
After transformations with EAT fornouns, HSL and HWL no longer differ signifi-cantly in performance, forming a single groupwith relatively higher precision, whilst the otherheuristics clump together into another group withlower precision, reflecting a negative effect fromEAT.
In the verb case, HWL and HSL, HSM andHSS, and HSN and HSA form three significantlydifferent groups with reference to their precision,reflecting poor performance of both normalizedheuristics (HSN and HSA) and a significantlyimproved result of HWL from the EAT data.All of this implies that in the lexical hub forWSD, the correct meaning of a word should holdas many links as possible with a relatively largenumber of context words.
These links can be inthe level of word form (HWL) or word sense(HSL).
HSL achieved the highest precision inboth nouns and verbs.7.3 The interaction of EAT in WSDFor the noun sense disambiguation, the pairedtwo sample for mean of the t-Test showed us thatRS and SRORRS transformations can signifi-cantly improve the precision of disambiguationof HWL and HSL (P<0.05, at the confidencelevel of 95 percent).
All four transformationsusing EAT for verb disambiguation are signifi-cantly better than its straightforward context caseon HWL and HSL (P<0.05, at the confidencelevel of 95 percent).It demonstrated that both the syntagmatic rela-tion and other domain information in the EATcan help discriminate word sense.
With the trans-formation of context surroundings of the target,the similarity metrics can compare the likenessof nouns and verbs, although we can exploit thederived form of word in WordNet to facilitate thecomparison.7.4 Comparison with other methodsThe lexical hub reached comparatively higherprecision in both nouns (45.8%) and verbs(35.6%).
This contrasted with other similaritybased methods and the unsupervised systems inSENSEVAL-2.
Note that we don?t adopt any935back-off policy such as the commonest sense ofword used by UNED-LS-U and DIMAP.Although the noun and verb similarity metricsin this paper are based on edge-counting withoutany aid of frequency information from corpora,they performed very well in the task of WSD inrelation to other information based metrics anddefinition matching methods.
Especially in theverb case, the metric significantly outperformedother metrics.8 Conclusion and future workIn this paper we defined the lexical hub and pro-posed its use for processing word sense disam-biguation, achieving results that are compara-tively better than most unsupervised systems ofSENSEVAL-2 in the literature.
Since WordNetonly organizes the paradigmatic relations ofwords, unlike previous methods, which are onlybased on WordNet, we fed the syntagmatic rela-tions of words from the EAT into the noun andverb similarity metrics, and significantly im-proved the results of WSD, given that no back-off was applied.
Moreover, we only utilized theunordered raw context information without anypragmatic knowledge and syntactic information;there is still a lot of work to fuse them in the fu-ture research.
In terms of the heuristics evaluated,richness of sense or word connectivity is muchmore important than the strength of individualword or sense linkages.
An interesting questionis whether these results will be borne out in otherdatasets.
In the forthcoming work we will inves-tigate their validity in the lexical task of SEN-SEVAL-3.ReferencesBarzilay, R. and M. Elhadad (1997).
Using LexicalChains for Text Summarization.
In the IntelligentScalable Text Summarization  Workshop (ISTS'97),ACL, Madrid, Spain.Chaffin, R., et al (1994).
The Paradigmatic Organiza-tion of Verbs in the Mental Lexicon.
Trenton StateCollege.Fellbaum, C. (1998).
Wordnet: An Electronic LexicalDatabase.
Cambridge MA, USA, The MIT Press.Halliday, M. A. K. and R. Hasan (1976).
Cohesion inEnglish.
London, London:Longman.Hirst, G. and D. St-Onge (1997).
Lexical Chains asRepresentations of Context for the Detection andCorrection of Malapropisms.
Wordnet.
C. Fell-baum.
Cambridge, MA, The Mit Press.Ide, N. and J. V?ronis (1998).
Word Sense Disam-biguation: The State of the Art.
Computational lin-guistics 24(1).Jiang, J. and D. Conrath (1997).
Semantic SimilarityBased on Corpus Statistics and Lexical Taxonomy.In the 10th International Conference on Researchin Computational Linguistics (ROCLING), Taiwan.Kilgarriff, A. and M. Palmer (2000).
Introduction,Special Issue on Senseval: Evaluating Word SenseDisambiguation Programs.
Computers and theHumanities 34(1-2): 1-13.Kilgarriff, A. and J. Rosenzweig (2000).
Frameworkand Results for English Senseval.
Computers andthe Humanities 34(1-2): 15-48.Kiss, G. R., et al (1973).
The Associative Thesaurusof English and Its Computer Analysis.
Edinburgh,University Press.Lesk, M. (1986).
Automatic Sense DisambiguationUsing Machine Readable Dictionaries: How to Tella Pine Code from an Ice Cream Cone.
In the 5thannual international conference on systems docu-mentation, ACM Press.Morris, J. and G. Hirst (1991).
Lexical CohesionComputed by Thesaural Relations as an Indicatorof the Structure of Text.
Computational linguistics17(1).Pedersen, T., et al (2003).
Maximizing Semantic Re-latedness to Perform Word Sense Disambiguation.Sinopalnikova, A.
(2004).
Word Association Thesau-rus as a Resource for Building Wordnet.
In GWC2004.Sussna, M. (1993).
Word Sense Disambiguation forFree-Text Indexing Using a Massive SemanticNetwork.
In CKIM'93.Yang, D. and D. M. W. Powers (2005).
MeasuringSemantic Similarity in the Taxonomy of Wordnet.In the Twenty-Eighth Australasian Computer Sci-ence Conference (ACSC2005), Newcastle, Austra-lia, ACS.Yang, D. and D. M. W. Powers (2006).
Verb Similar-ity on the Taxonomy of Wordnet.
In the 3rd Inter-national WordNet Conference (GWC-06), Jeju Is-land, Korea.Yarowsky, D. (1992).
Word Sense DisambiguationUsing Statistical Models of Roget's CategoriesTrained on Large Corpora.
In the 14th InternationalConference on Computational Linguistics, Nates,France.Yarowsky, D. (1993).
One Sense Per Collocation.
InARPA Human Language Technology Workshop,Princeton, New Jersey.936
