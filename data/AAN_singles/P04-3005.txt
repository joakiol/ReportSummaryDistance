Customizing Parallel Corpora at the Document LevelMonica ROGATI and Yiming YANGComputer Science Department, Carnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213mrogati@cs.cmu.edu, yiming@cs.cmu.eduAbstractRecent research in cross-lingualinformation retrieval (CLIR) established theneed for properly matching the parallel corpusused for query translation to the target corpus.We propose a document-level approach tosolving this problem: building a custom-madeparallel corpus by automatically assembling itfrom documents taken from other parallelcorpora.
Although the general idea can beapplied to any application that uses parallelcorpora, we present results for CLIR in themedical domain.
In order to extract the best-matched documents from several parallelcorpora, we propose ranking individualdocuments by using a length-normalizedOkapi-based similarity score between them andthe target corpus.
This ranking allows us todiscard 50-90% of the training data, whileavoiding the performance drop caused by agood but mismatched resource, and evenimproving CLIR effectiveness by 4-7% whencompared to using all available training data.1 IntroductionOur recent research in cross-lingual informationretrieval (CLIR) established the need for properlymatching the parallel corpus used for querytranslation to the target corpus (Rogati and Yang,2004).
In particular, we showed that using ageneral purpose machine translation (MT) systemsuch as SYSTRAN, or a general purpose parallelcorpus - both of which perform very well for newsstories (Peters, 2003) - dramatically fails in themedical domain.
To explore solutions to thisproblem, we used cosine similarity betweentraining and target corpora as respective weightswhen building a translation model.
This approachtreats a parallel corpus as a homogeneous entity, anentity that is self-consistent in its domain anddocument quality.
In this paper, we propose thatinstead of weighting entire resources, we can selectindividual documents from these corpora in orderto build a parallel corpus that is tailor-made to fit aspecific target collection.
To avoid confusion, it ishelpful to remember that in IR settings the true testdata are the queries, not the target documents.
Thedocuments are available off-line and can be (andusually are) used for training and systemdevelopment.
In other words, by matching thetraining corpora and the target documents we arenot using test data for training.
(Rogati and Yang, 2004) also discussesindirectly related work, such as query translationdisambiguation and building domain-specificlanguage models for speech recognition.
We arenot aware of any additional related work.In addition to proposing individual documentsas the unit for building custom-made parallelcorpora, in this paper we start exploring the criteriaused for individual document selection byexamining the effect of ranking documents usingthe length-normalized Okapi-based similarity scorebetween them and the target corpus.2 Evaluation Data2.1 Medical Domain Corpus: SpringerThe Springer corpus consists of 9640 documents(titles plus abstracts of medical journal articles)each in English and in German, with 25 queries inboth languages, and relevance judgments made bynative German speakers who are medical expertsand are fluent in English.
We split this parallelcorpus into two subsets, and used the first subset(4,688 documents) for training, and the remainingsubset (4,952 documents) as the test set in all ourexperiments.
This configuration allows us toexperiment with CLIR in both directions (EN-DEand DE-EN).
We applied an alignment algorithmto the training documents, and obtained a sentence-aligned parallel corpus with about 30K sentencesin each language.2.2 Training CorporaIn addition to Springer, we have used four otherEnglish-German parallel corpora for training:?
NEWS is a collection of 59K sentencealigned news stories, downloaded from theweb (1996-2000), and available athttp://www.isi.edu/~koehn/publications/de-news/?
WAC is a small parallel corpus obtained bymining the web (Nie et al, 2000), in noparticular domain?
EUROPARL is a parallel corpus providedby (Koehn).
Its documents are sentencealigned European Parliament proceedings.This is a large collection that has beensuccessfully used for CLEF, when the targetcorpora were collections of news stories(Rogati and Yang, 2003).?
MEDTITLE is an English-German parallelcorpus consisting of 549K paired titles ofmedical journal articles.
These titles weregathered from the PubMed online database(http://www.ncbi.nlm.nih.gov/PubMed/).Table 1 presents a summary of the five trainingcorpora characteristics.Name Size (sent) DomainNEWS 59K newsWAC 60K mixedEUROPARL 665K politicsSPRINGER 30K medicalMEDTITLE 550K medicalTable 1.
Characteristics of Parallel TrainingCorpora3 Selecting Documents from Parallel CorporaWhile selecting and weighing entire trainingcorpora is a problem already explored by (Rogatiand Yang, 2004), in this paper we focus on a lowergranularity level: individual documents in theparallel corpora.
We seek to construct a customparallel corpus, by choosing individual documentswhich best match the testing collection.
Wecompute the similarity between the test collection(in German or English) and each individualdocument in the parallel corpora for that respectivelanguage.
We have a choice of similarity metrics,but since this computation is simply retrieval witha long query, we start with the Okapi model(Robertson, 1993), as implemented by the Lemursystem (Olgivie and Callan, 2001).
Although theOkapi model takes into account average documentlength, we compare it with its length-normalizedversion, measuring per-word similarity.
The twomeasures are identified in the results section by?Okapi?
and ?Normalized?.Once the similarity is computed for eachdocument in the parallel corpora, only the top Nmost similar documents are kept for training.
Theyare an approximation of the domain(s) of the testcollection.
Selecting N has not been an issue forthis corpus   (values between 10-75% were safe).However, more generally, this parameter can betuned to a different test corpus as any otherparameter.
Alternatively, the document score canalso be incorporated into the translation model,eliminating the need for thresholding.4 CLIR MethodWe used a corpus-based approach, similar to thatin (Rogati and Yang, 2003).
Let L1 be the sourcelanguage and L2 be the target language.
The cross-lingual retrieval consists of the following steps:1.
Expanding a query in L1 using blindfeedback2.
Translating the query by taking the dotproduct between the query vector (withweights from step 1) and a translationmatrix obtained by calculating translationprobabilities or term-term similarity usingthe parallel corpus.3.
Expanding the query in L2 using blindfeedback4.
Retrieving documents in L2Here, blind feedback is the process of retrievingdocuments and adding the terms of the top-rankingdocuments to the query for expansion.
We usedsimplified Rocchio positive feedback asimplemented by Lemur (Olgivie and Callan, 2001).For the results in this paper, we have usedPointwise Mutual Information (PMI) instead ofIBM Model 1 (Brown et al, 1993), since (Rogatiand Yang, 2004) found it to be as effective onSpringer, but faster to compute.5 Results and Discussion5.1 Empirical SettingsFor the retrieval part of our system, we adaptedLemur (Ogilvie and Callan, 2001)  to allow the useof weighted queries.
Several parameters weretuned, none of them on the test set.
In our corpus-based approach, the main parameters are thoseused in query expansion based on pseudo-relevance, i.e., the maximum number of documentsand the maximum number of words to be used, andthe relative weight of the expanded portion withrespect to the initial query.
Since the Springertraining set is fairly small, setting aside a subset ofthe data for parameter tuning was not desirable.We instead chose parameter values that were stableon the CLEF collection (Peters, 2003): 5 and 20 asthe maximum numbers of documents and words,respectively.
The relative weight of the expandedportion with respect to the initial query was set to0.5.
The results were evaluated using meanaverage precision (AvgP), a standard performancemeasure for IR evaluations.In the following sections, DE-EN refers toretrieval where the query is in German and thedocuments in English, while EN-DE refers toretrieval in the opposite direction.5.2 Using the Parallel Corpora SeparatelyCan we simply choose a parallel corpus thatperformed very well on news stories, hoping it isrobust across domains?
Natural approaches alsoinclude choosing the largest corpus available, orusing all corpora together.
Figure 1 shows theeffect of these strategies.Figure 1.
CLIR results on the Springer test set byusing PMI with different training corpora.We notice that choosing the largest collection(EUROPARL), using all resources availablewithout weights (ALL), and even choosing a largecollection in the medical domain (MEDTITLE) areall sub-optimal strategies.Given these results, we believe that resourceselection and weighting is necessary.
Thoroughlyexploring weighting strategies is beyond the scopeof this paper and it would involve collection size,genre, and translation quality in addition to ameasure of domain match.
Here, we start byselecting individual documents that match thedomain of the test collection.
We examine theeffect this choice has on domain-specific CLIR.5.3 Using Okapi weights to build a customparallel corpusFigures 2 and 3 compare the two documentselection strategies discussed in Section 3 to usingall available documents, and to the ideal (but nottruly optimal) situation where there exists a ?best?resource to choose and this collection is known.
By?best?, we mean one that can produce optimalresults on the test corpus, with respect to the givenmetric In reality, the true ?best?
resource isunknown: as seen above, many intuitive choicesfor the best collection are not optimal.40455055601 10 100Percent Used (log)AveragePrecisionOkapi NormalizedAll Corpora Best CorpusFigure 2.
CLIR  DE-EN performance vs. Percentof Parallel Documents  Used.
?Best Corpus?
isgiven by an oracle and is usually unknown.50556065701 10 100Percent Used (log)AveragePrecisionOkapi NormalizedAll Corpora Best CorpusFigure 3.
CLIR  EN-DE performance vs. Percentof Parallel Documents Used.
?Best Corpus?
isgiven by an oracle and is usually unknown010203040506070EN-DE DE-ENAvgP.SPRINGER MEDTITLE WACNEWS EUROPARL ALLNotice that the normalized version performs betterand is more stable.
Per-word similarity is, in thiscase, important when the documents are used totrain translation scores: shorter parallel documentsare better when building the translation matrix.
Ourstrategy accounts for a 4-7% improvement overusing all resources with no weights, for bothretrieval directions.
It is also very close to the?oracle?
condition, which chooses the bestcollection in advance.
More importantly, by usingthis strategy we are avoiding the sharpperformance drop when using a mismatched,although very good, resource (such asEUROPARL).6 Future WorkWe are currently exploring weighting strategiesinvolving collection size, genre, and estimatingtranslation quality in addition to a measure ofdomain match.
Another question we areexamining is the granularity level used whenselecting resources, such as selection at thedocument or cluster level.Similarity and overlap between resourcesthemselves is also worth considering whileexploring tradeoffs between redundancy and noise.We are also interested in how these approacheswould apply to other domains.7 ConclusionsWe have examined the issue of selectingappropriate training resources for cross-lingualinformation retrieval.
We have proposed andevaluated a simple method for creating acustomized parallel corpus from other availableparallel corpora by matching the domain of the testdocuments with that of individual paralleldocuments.
We noticed that choosing the largestcollection, using all resources available withoutweights, and even choosing a large collection inthe medical domain are all sub-optimal strategies.The techniques we have presented here are notrestricted to CLIR and can be applied to otherareas where parallel corpora are necessary, such asstatistical machine translation.
The trainedtranslation matrix can also be reused and can beconverted to any of the formats required by suchapplications.8 AcknowledgementsWe would like to thank Ralf Brown for collectingthe MEDTITLE and SPRINGER data.This research is sponsored in part by the NationalScience Foundation (NSF) under grant IIS-9982226, and in part by the DOD under award114008-N66001992891808.
Any opinions andconclusions in this paper are the authors?
and donot necessarily reflect those of the sponsors.ReferencesBrown, P.F, Pietra, D., Pietra, D, Mercer, R.L.
1993.TheMathematics of Statistical Machine Translation:Parameter Estimation.
In Computational Linguistics,19:263-312Koehn, P. Europarl: A Multilingual Corpus forEvaluation of Machine Translation.
Draft,Unpublished.Nie, J. Y., Simard, M. and Foster, G.. 2000.
Usingparallel web pages for multi-lingual IR.
In C.Peters(Ed.
),  Proceedings of the CLEF 2000 forumOgilvie, P. and Callan, J.
2001.
Experiments using theLemur toolkit.
In Proceedings of the Tenth TextRetrieval Conference (TREC-10).Peters, C. 2003.
Results of the CLEF 2003 Cross-LanguageSystem Evaluation Campaign.
Working Notes for theCLEF 2003 Workshop, 21-22 August, Trondheim,NorwayRobertson, S.E.
and all.
1993.
Okapi at TREC.
In TheFirst TREC Retrieval Conference, Gaithersburg, MD.pp.
21-30Rogati, M and Yang, Y.
2003.
Multilingual InformationRetrieval using Open, Transparent Resources inCLEF 2003 .
In C. Peters (Ed.
), Results of theCLEF2003 cross-language evaluation forumRogati, M and Yang, Y.
2004.
Resource Selection forDomain Specific Cross-Lingual IR.
In Proceedings ofACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR'04).
