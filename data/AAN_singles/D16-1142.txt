Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1348?1358,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAll Fingers are not Equal: Intensity of References in Scientific ArticlesTanmoy ChakrabortyDept.
of Computer Science & UMIACSUniversity of Maryland, College Park, USAtanchak@umiacs.umd.eduRamasuri NarayanamIBM Research, Indiaramasurn@in.ibm.comAbstractResearch accomplishment is usually measuredby considering all citations with equal impor-tance, thus ignoring the wide variety of pur-poses an article is being cited for.
Here, weposit that measuring the intensity of a refer-ence is crucial not only to perceive better un-derstanding of research endeavor, but also toimprove the quality of citation-based applica-tions.
To this end, we collect a rich annotateddataset with references labeled by the inten-sity, and propose a novel graph-based semi-supervised model, GraLap to label the in-tensity of references.
Experiments with AANdatasets show a significant improvement com-pared to the baselines to achieve the true labelsof the references (46% better correlation).
Fi-nally, we provide four applications to demon-strate how the knowledge of reference inten-sity leads to design better real-world applica-tions.1 IntroductionWith more than one hundred thousand new schol-arly articles being published each year, there is arapid growth in the number of citations for the rel-evant scientific articles.
In this context, we high-light the following interesting facts about the pro-cess of citing scientific articles: (i) the most com-monly cited paper by Gerard Salton, titled ?A VectorSpace Model for Information Retrieval?
(alleged tohave been published in 1975) does not actually ex-ist in reality (Dubin, 2004), (ii) the scientific authorsread only 20% of the works they cite (Simkin andRoychowdhury, 2003), (iii) one third of the refer-ences in a paper are redundant and 40% are perfunc-tory (Moravcsik and Murugesan, 1975), (iv) 62.7%of the references could not be attributed a specificfunction (definition, tool etc.)
(Teufel et al, 2006).Despite these facts, the existing bibliographic met-rics consider that all citations are equally significant.In this paper, we would emphasize the fact thatall the references of a paper are not equally influ-ential.
For instance, we believe that for our currentpaper, (Wan and Liu, 2014) is more influential refer-ence than (Garfield, 2006), although the former hasreceived lower citations (9) than the latter (1650) sofar1.
Therefore the influence of a cited paper com-pletely depends upon the context of the citing paper,not the overall citation count of the cited paper.
Wefurther took the opinion of the original authors offew selective papers and realized that around 16%of the references in a paper are highly influential,and the rest are trivial (Section 4).
This motivates usto design a prediction model, GraLap to automati-cally label the influence of a cited paper with respectto a citing paper.
Here, we label paper-referencepairs rather than references alone, because a refer-ence that is influential for one citing paper may notbe influential with equal extent for another citing pa-per.We experiment with ACL Anthology Network(AAN) dataset and show that GraLap along withthe novel feature set, quite efficiently, predicts theintensity of references of papers, which achieves(Pearson) correlation of 0.90 with the human anno-tations.
Finally, we present four interesting appli-1The statistics are taken from Google Scholar on June 2,2016.1348cations to show the efficacy of considering unequalintensity of references, compared to the uniform in-tensity.The contributions of the paper are four-fold: (i)we acquire a rich annotated dataset where paper-reference pairs are labeled based on the influencescores (Section 4), which is perhaps the first gold-standard for this kind of task; (ii) we propose agraph-based label propagation model GraLap forsemi-supervised learning which has tremendous po-tential for any task where the training set is lessin number and labels are non-uniformly distributed(Section 3); (iii) we propose a diverse set of features(Section 3.3); most of them turn out to be quite ef-fective to fit into the prediction model and yield im-proved results (Section 5); (iv) we present four ap-plications to show how incorporating the referenceintensity enhances the performance of several state-of-the-art systems (Section 6).2 Defining Intensity of ReferencesAll the references of a paper usually do not carryequal intensity/strength with respect to the citingpaper because some papers have influenced the re-search more than others.
To pin down this intuition,here we discretize the reference intensity by numer-ical values within the range of 1 to 5, (5: most in-fluential, 1: least influential).
The appropriate def-initions of different labels of reference intensity arepresented in Figure 1, which are also the basis ofbuilding the annotated dataset (see Section 4):Note that ?reference intensity?
and ?referencesimilarity?
are two different aspects.
It might hap-pen that two similar reference are used with differ-ent intensity levels in a citing paper ?
while one isjust mentioned somewhere in the paper and otheris used as a baseline.
Here, we address the formerproblem as a semi-supervised learning problem withclues taken from content of the citing and cited pa-pers.3 Reference Intensity Prediction ModelIn this section, we formally define the problem andintroduce our prediction model.?
Label-1: The reference is related to the citing ar-ticle with very limited extent and can be removedwithout compromising the competence of the refer-ences (e.g., (Garfield, 2006) for this paper).?
Label-2: The reference is little mentioned in theciting article and can be replaced by others withoutcompromising the adequacy of the references (e.g.,(Zhu et al, 2015) for this paper).?
Label-3: The reference occurs separately in a sen-tence within the citing article and has no significantimpact on the current problem (e.g., references tometrics, tools) (e.g., (Porter, 1997) for this paper).?
Label-4: The reference is important and highlyrelated to the citing article.
It is usually mentionedseveral times in the article with long reference con-text (e.g., (Singh et al, 2015) for this paper).?
Label-5: The reference is extremely importantand occurs (is emphasized) multiple times withinthe citing article.
It generally points to the cited arti-cle from where the citing article borrows main ideas(and can be treated as a baseline) (e.g., (Wan andLiu, 2014) for this paper).Figure 1: Definitions of the intensity of references.3.1 Problem DefinitionWe are given a set of papers P = {P1, P2, ..., PM}and a sets of references R = {R1, R2, ..., RM},where Ri corresponds to the set of references (orcited papers) of Pi.
There is a set of papers PL ?P whose references RL ?
R are already labeledby ` ?
L = {1, ..., 5} (each reference is labeledwith exactly one value).
Our objective is to de-fine a predictive function f that labels the referencesRU ?
{R \ RL} of the papers PU ?
{P \ PL}whose reference intensities are unknown, i.e., f :(P,R, PL, RL, PU , RL) ??
L.Since the size of the annotated (labeled) data ismuch smaller than unlabeled data (|PL|  |PU |),we consider it as a semi-supervised learning prob-lem.Definition 1.
(Semi-supervised Learning) Givena set of entries X and a set of possible labels YL,let us assume that (x1, y1), (x2, y2),..., (xl, yl) bethe set of labeled data where xi is a data pointand yi ?
YL is its corresponding label.
We as-sume that at least one instance of each class label1349is present in the labeled dataset.
Let (xl+1, yl+1),(xl+2, yl+2),..., (xl+n, yl+u) be the unlabeled datapoints where YU = {yl+1, yl+2, ...yl+u} are un-known.
Each entry x ?
X is represented by a setof features {f1, f2, ..., fD}.
The problem is to deter-mine the unknown labels using X and YL.3.2 GraLap: A Prediction ModelWe propose GraLap, a variant of label propagation(LP) model proposed by (Zhu et al, 2003) wherea node in the graph propagates its associated labelto its neighbors based on the proximity.
We intendto assign same label to the vertices which are closelyconnected.
However unlike the traditional LP modelwhere the original values of the labels continue tofade as the algorithm progresses, we systematicallyhandle this problem in GraLap.
Additionally, wefollow a post-processing in order to handle ?class-imbalance problem?.Graph Creation.
The algorithm starts with thecreation of a fully connected weighted graph G =(X,E) where nodes are data points and the weightwij of each edge eij ?
E is determined by the radialbasis function as follows:wij = exp(?
?Dd=1(xdi ?
xdj )2?2)(1)The weight is controlled by a parameter ?.
Laterin this section, we shall discuss how ?
is selected.Each node is allowed to propagate its label to itsneighbors through edges (the more the edge weight,the easy to propagate).Transition Matrix.
We create a probabilistic transi-tion matrix T|X|?|X|, where each entry Tij indicatesthe probability of jumping from j to i based on thefollowing: Tij = P (j ?
i) = wij?|X|k=1 wkj.Label Matrix.
Here, we allow a soft label (in-terpreted as a distribution of labels) to be associ-ated with each node.
We then define a label matrixY|X|?|L|, where ith row indicates the label distribu-tion for node xi.
Initially, Y contains only the valuesof the labeled data; others are zero.Label Propagation Algorithm.
This algorithmworks as follows:After initializing Y and T , the algorithm starts bydisseminating the label from one node to its neigh-bors (including self-loop) in one step (Step 3).
Thenwe normalize each entry of Y by the sum of its cor-1: Initialize T and Y2: while (Y does not converge) do3: Y ?
TY4: Normalize rows of Y , yij = yij?k yik5: Reassign original labels to XLresponding row in order to maintain the interpreta-tion of label probability (Step 4).
Step 5 is crucial;here we want the labeled sources XL to be persis-tent.
During the iterations, the initial labeled nodesXL may fade away with other labels.
Therefore weforcefully restore their actual label by setting yil = 1(if xi ?
XL is originally labeled as l), and otherentries (?j 6=lyij) by zero.
We keep on ?pushing?the labels from the labeled data points which in turnpushes the class boundary through high density datapoints and settles in low density space.
In this way,our approach intelligently uses the unlabeled data inthe intermediate steps of the learning.Assigning Final Labels.
Once YU is computed, onemay take the most likely label from the label distri-bution for each unlabeled data.
However, this ap-proach does not guarantee the label proportion ob-served in the annotated data (which in this case isnot well-separated as shown in Section 4).
There-fore, we adopt a label-based normalization tech-nique.
Assume that the label proportions in the la-beled data are c1, ..., c|L| (s.t.
?|L|i=1 ci = 1).
Incase of YU , we try to balance the label proportionobserved in the ground-truth.
The label mass is thecolumn sum of YU , denoted by YU.1 , ..., YU.|L| , eachof which is scaled in such a way that YU.1 : ... :YU.|L| = c1 : ... : c|L|.
The label of an unlabeleddata point is finalized as the label with maximumvalue in the row of Y .Convergence.
Here we briefly show that our algo-rithm is guaranteed to converge.
Let us combineSteps 3 and 4 as Y ?
T?
Y , where T?
= Tij/?k Tik.Y is composed of YLl?|L| and YUu?|L| , where YUnever changes because of the reassignment.
We cansplit T?
at the boundary of labeled and unlabeled dataas follows:F?
=[T?ll T?luT?ul T?uu]Therefore, YU ?
T?uuYU+ T?ulYL, which can leadto YU = limn??
T?nuuY 0 + [?ni=1 T?
(i?1)uu ]T?ulYL,where Y 0 is the shape of Y at iteration 0.
We need1350to show T?nuuijY 0 ?
0.
By construction, T?ij ?
0,and since T?
is row-normalized, and T?uu is a partof T?
, it leads to the following condition: ??
<1,?uj=1 T?uuij ?
?, ?i = 1, ..., u.
So,?jT?nuuij =?j?kT?
(n?1)uuik T?uukj=?kT?
(n?1)uuik?jT?uuik??kT?
(n?1)uuik ??
?nTherefore, the sum of each row in T?nuuij convergesto zero, which indicates T?nuuijY 0 ?
0.Selection of ?.
Assuming a spatial representationof data points, we construct a minimum spanningtree using Kruskal?s algorithm (Kruskal, 1956) withdistance between two nodes measured by Euclideandistance.
Initially, no nodes are connected.
Wekeep on adding edges in increasing order of distance.We choose the distance (say, df ) of the first edgewhich connects two components with different la-beled points in them.
We consider df as a heuristicto the minimum distance between two classes, andarbitrarily set ?
= d0/3, following 3?
rule of nor-mal distribution (Pukelsheim, 1994).3.3 Features for Learning ModelWe use a wide range of features that suitably rep-resent a paper-reference pair (Pi, Rij), indicating Pirefers to Pj through reference Rij .
These featurescan be grouped into six general classes.3.3.1 Context-based Features (CF)The ?reference context?
of Rij in Pi is defined bythree-sentence window (sentence where Rij occursand its immediate previous and next sentences).
Formultiple occurrences, we calculate its average score.We refer to ?reference sentence?
to indicate the sen-tence where Rij appears.
(i) CF:Alone.
It indicates whether Rij is mentionedalone in the reference context or together with otherreferences.
(ii) CF:First.
When Rij is grouped with others, thisfeature indicates whether it is mentioned first (e.g.,?[2]?
is first in ?[2,4,6]?
).Next four features are based on the occurrence ofwords in the corresponding lists created manually(see Table 1) to understand different aspects.
(iii) CF:Relevant.
It indicates whether Rij is explic-itly mentioned as relevant in the reference context(Rel in Table 1).
(iv) CF:Recent.
It tells whether the reference con-text indicates that Rij is new (Rec in Table 1).
(v) CF:Extreme.
It implies that Rij is extreme insome way (Ext in Table 1).
(vi) CF:Comp.
It indicates whether the referencecontext makes some kind of comparison with Rij(Comp in Table 1).Note we do not consider any sentiment-based fea-tures as suggested by (Zhu et al, 2015).3.3.2 Similarity-based Features (SF)It is natural that the high degree of semantic simi-larity between the contents of Pi and Pj indicates theinfluence of Pj in Pi.
We assume that although thefull text of Pi is given, we do not have access to thefull text of Pj (may be due to the subscription chargeor the unavailability of the older papers).
Therefore,we consider only the title of Pj as a proxy of itsfull text.
Then we calculate the cosine-similarity2between the title (T) of Pj and (i) SF:TTitle.
the ti-tle, (ii) SF:TAbs.
the abstract, SF:TIntro.
the in-troduction, (iv) SF:TConcl.
the conclusion, and (v)SF:TRest.
the rest of the sections (sections otherthan abstract, introduction and conclusion) of Pi.We further assume that the ?reference context?
(RC) of Pj in Pi might provide an alternate way ofsummarizing the usage of the reference.
Therefore,we take the same similarity based approach men-tioned above, but replace the title of Pj with its RCand obtain five more features: (vi) SF:RCTitle, (vii)SF:RCAbs, (viii) SF:RCIntro, (ix) SF:RCConcl and(x) SF:RCRest.
If a reference appears multiple timesin a citing paper, we consider the aggregation of allRCs together.3.3.3 Frequency-based Feature (FF)The underlying assumption of these features isthat a reference which occurs more frequently ina citing paper is more influential than a single oc-currence (Singh et al, 2015).
We count the fre-quency of Rij in (i) FF:Whole.
the entire content,(ii) FF:Intro.
the introduction, (iii) FF:Rel.
the re-lated work, (iv) FF:Rest.
the rest of the sections (as2We use the vector space based model (Turney and Pantel,2010) after stemming the words using Porter stammer (Porter,1997).1351Rel pivotal, comparable, innovative, relevant, relevantly, inspiring, related, relatedly, similar, similarly, applicable, appropriate,pertinent, influential, influenced, original, originally, useful, suggested, interesting, inspired, likewiserecent, recently, latest, later, late, latest, up-to-date, continuing, continued, upcoming, expected, update, renewed, extendedRec subsequent, subsequently, initial, initially, sudden, current, currently, future, unexpected, previous, previously, old,ongoing, imminent, anticipated, unprecedented, proposed, startling, preliminary, ensuing, repeated, reported, new, earlier,earliest, early, existing, further, revised, improvedExt greatly, awfully, drastically, intensely, acutely, almighty, exceptionally, excessively, exceedingly, tremendously, importantlysignificantly, notably, outstandinglyComp easy, easier, easiest, vague, vaguer, vaguest, weak, weaker, weakest, strong, stronger, strongest, bogus, unclearTable 1: Manually curated lists of words collected from analyzing the reference contexts.
The lists arefurther expanded using the Wordnet:Synonym with different lexical variations.
Note that while searchingthe occurrence of these words in reference contexts, we use different lexical variations of the words insteadof exact matching.mentioned in Section 3.3.2) of Pi.
We also intro-duce (v) FF:Sec.
to measure the fraction of differentsections of Pi where Rij occurs (assuming that ap-pearance of Rij in different sections is more influ-ential).
These features are further normalized usingthe number of sentences in Pi in order to avoid un-necessary bias on the size of the paper.3.3.4 Position-based Features (PF)Position of a reference in a paper might be a pre-dictive clue to measure the influence (Zhu et al,2015).
Intuitively, the earlier the reference appearsin the paper, the more important it seems to us.
Forthe first two features, we divide the entire paper intotwo parts equally based on the sentence count andthen see whether Rij appears (i) PF:Begin.
in thebeginning or (ii) PF:End.
in the end of Pi.
Impor-tantly, if Rij appears multiple times in Pi, we con-sider the fraction of times it occurs in each part.For the other two features, we take the entire pa-per, consider sentences as atomic units, and measureposition of the sentences where Rij appears, includ-ing (iii) PF:Mean.
mean position of appearance, (iv)PF:Std.
standard deviation of different appearances.These features are normalized by the total length(number of sentences) of Pi.
, thus ranging from 0(indicating beginning of Pi) to 1 (indicating the endof Pi).3.3.5 Linguistic Features (LF)The linguistic evidences around the context ofRijsometimes provide clues to understand the intrinsicinfluence of Pj on Pi.
Here we consider word leveland structural features.
(i) LF:NGram.
Different levels of n-grams (1-grams, 2-grams and 3-grams) are extracted from thereference context to see the effect of different wordcombination (Athar and Teufel, 2012).
(ii) LF:POS.
Part-of-speech (POS) tags of thewords in the reference sentence are used as features(Jochim and Schu?tze, 2012).
(iii) LF:Tense.
The main verb of the reference sen-tence is used as a feature (Teufel et al, 2006).
(iv) LF:Modal.
The presence of modal verbs (e.g.,?can?, ?may?)
often indicates the strength of theclaims.
Hence, we check the presence of the modalverbs in the reference sentence.
(v) LF:MainV.
We use the main-verb of the refer-ence sentence as a direct feature in the model.
(vi) LF:hasBut.
We check the presence of conjunc-tion ?but?, which is another clue to show less confi-dence on the cited paper.
(vii) LF:DepRel.
Following (Athar and Teufel,2012) we use all the dependencies present in the ref-erence context, as given by the dependency parser(Marneffe et al, 2006).
(viii) LF:POSP.
(Dong and Schfer, 2011) use sevenregular expression patterns of POS tags to capturesyntactic information; then seven boolean featuresmark the presence of these patterns.
We also utilizethe same regular expressions as shown below 3 withthe examples (the empty parenthesis in each exam-ple indicates the presence of a reference token Rijin the corresponding sentence; while few examplesare complete sentences, few are not):?
?.
*\\(\\) VV[DPZN].*?
: Chen () showed that cohesion is heldin the vast majority of cases for English-French.?
?.
*(VHP|VHZ) VV.*?
: while Cherry and Lin () have shown it tobe a strong feature for word alignment...?
?.
*VH(D|G|N|P|Z) (RB )*VBN.*?
: Inducing features for tag-gers by clustering has been tried by several researchers ().?
?.
*MD (RB )*VB(RB )* VVN.*?
: For example, the likelihood ofthose generative procedures can be accumulated to get the like-lihood of the phrase pair ().3The meaning of each POS tag can be found inhttp://nlp.stanford.edu/software/tagger.shtml(Toutanova and Manning, 2000).1352?
?
[ IW.
]*VB(D|P|Z) (RB )*VV[ND].*?
: Our experimental set-upis modeled after the human evaluation presented in ().?
?
(RB )*PP (RB )*V.*?
: We use CRF () to perform this tagging.?
?.
*VVG (NP )*(CC )*(NP ).*?
: Following (), we provide the an-notators with only short sentences: those with source sentencesbetween 10 and 25 tokens long.These are all considered as Boolean features.
Foreach feature, we take all the possible evidences fromall paper-reference pairs and prepare a vector.
Thenfor each pair, we check the presence (absence) oftokens for the corresponding feature and mark thevector accordingly (which in turn produces a set ofBoolean features).3.3.6 Miscellaneous Features (MS)This group provides other factors to explain whyis a paper being cited.
(i) MS:GCount.
To answerwhether a highly-cited paper has more academic in-fluence on the citing paper than the one which is lesscited, we measure the number of other papers (ex-cept Pi) citing Pj .
(ii) MS:SelfC.
To see the effect of self-citation, wecheck whether at least one author is common in bothPi and Pj .
(iii) MG:Time.
The fact that older papers are rarelycited, may not stipulate that these are less influential.Therefore, we measure the difference of the publica-tion years of Pi and Pj .
(iv) MG:CoCite.
It measures the co-citation countsof Pi and Pj defined by |Ri?Rj ||Ri?Rj | , which in turn an-swers the significance of reference-based similaritydriving the academic influence (Small, 1973).Following (Witten and Frank, 2005), we furthermake one step normalization and divide each featureby its maximum value in all the entires.4 Dataset and AnnotationWe use the AAN dataset (Radev et al, 2009) whichis an assemblage of papers included in ACL relatedvenues.
The texts are preprocessed where sentences,paragraphs and sections are properly separated us-ing different markers.
The filtered dataset contains12,843 papers (on average 6.21 references per paper)and 11,092 unique authors.Next we use Parscit (Councill et al, 2008) toidentify the reference contexts from the dataset andthen extract the section headings from all the pa-pers.
Then each section heading is mapped into oneof the following broad categories using the methodproposed by (Liakata et al, 2012): Abstract, Intro-duction, Related Work, Conclusion and Rest.Dataset Labeling.
The hardest challenge in thistask is that there is no publicly available datasetwhere references are annotated with the intensityvalue.
Therefore, we constructed our own annotateddataset in two different ways.
(i) Expert Annota-tion: we requested members of our research group4to participate in this survey.
To facilitate the labelingprocess, we designed a portal where all the paperspresent in our dataset are enlisted in a drop-downmenu.
Upon selecting a paper, its correspondingreferences were shown with five possible intensityvalues.
The citing and cited papers are also linkedto the original texts so that the annotators can readthe original papers.
A total of 20 researchers partic-ipated and they were asked to label as many paper-reference pairs as they could based on the definitionsof the intensity provided in Section 2.
The annota-tion process went on for one month.
Out of total1640 pairs annotated, 1270 pairs were taken suchthat each pair was annotated by at least two anno-tators, and the final intensity value of the pair wasconsidered to be the average of the scores.
The Pear-son correlation and Kendell?s ?
among the annota-tors are 0.787 and 0.712 respectively.
(ii) AuthorAnnotation: we believe that the authors of a paperare the best experts to judge the intensity of refer-ences present in the paper.
With this intension, welaunched a survey where we requested the authorswhose papers are present in our dataset with signif-icant numbers.
We designed a web portal in similarfashion mentioned earlier; but each author was onlyshown her own papers in the drop-down menu.
Outof 35 requests, 22 authors responded and total 196pairs are annotated.
This time we made sure thateach paper-reference pair was annotated by only oneauthor.
The percentages of labels in the overall an-notated dataset are as follows: 1: 9%, 2: 74%, 3:9%, 4: 3%, 5: 4%.5 Experimental ResultsIn this section, we start with analyzing the impor-tance of the feature sets in predicting the reference4All were researchers with the age between 25-45 workingon document summarization, sentiment analysis, and text min-ing in NLP.1353FF SF CF PF LF MF00.20.40.6FeaturePearson correaltionNon?increasing order of correlation PFRelRestWholeIntroSecLFRestWholeRelIntroSecFFSFAloneRelevantExtremeCompFirstRecentCF TIntroTAbsTRestTTitleTConclRCRestRCIntroRCAbsRCTitleRCConcl CoCiteTimeSelfCGCount(a)(b)MFDepRelModalPOSNGramPOSPMainVTenseFigure 2: Pearson correlation coefficient betweenthe features and the gold-standard annotations.
(a)Group-wise average correlation, and (b) ranking offeatures in each group based on the correlation.intensity, followed by the detailed results.Feature Analysis.
In order to determine which fea-tures highly determine the gold-standard labeling,we measure the Pearson correlation between vari-ous features and the ground-truth labels.
Figure 2(a)shows the average correlation for each feature group,and in each group the rank of features based onthe correlation is shown in Figure 2(b).
Frequency-based features (FF) turn out to be the best, amongwhich FF:Rest is mostly correlated.
This set offeatures is convenient and can be easily computed.Both CF and LF seem to be equally important.
How-ever, PF tends to be less important in this task.Model RMSE ?
R2Uniform 2.09 -0.05 3.21SVR+W 1.95 0.54 1.34SVR+O 1.92 0.56 1.29C4.5SSL 1.99 0.46 2.46GLM 1.98 0.52 1.35(a) BaselinesNo.
Model RMSE ?
R2(1) GraLap+ FF 1.10 0.79 1.05(2) (1) + LF 0.98 0.84 0.95(3) (2) + CF 0.90 0.87 0.87(4) (3) + MF 0.95 0.89 0.84(5) (4) + SF 0.92 0.90 0.82(6) (5) + PF 0.91 0.90 0.80(b) Our modelTable 2: Performance of the competing models.
Thefeatures are added greedily into the GraLap model.Results of Predictive Models.
For the purpose ofevaluation, we report the average results after 10-fold cross-validation.
Here we consider five base-lines to compare with GraLap: (i) Uniform: as-sign 3 to all the references assuming equal inten-sity, (ii) SVR+W: recently proposed Support VectorRegression (SVR) with the feature set mentionedin (Wan and Liu, 2014), (iii) SVR+O: SVR modelwith our feature set, (iv) C4.5SSL: C4.5 semi-supervised algorithm with our feature set (Quinlan,1993), and (v) GLM: the traditional graph-based LPmodel with our feature set (Zhu et al, 2003).
Threemetrics are used to compare the results of the com-peting models with the annotated labels: Root MeanSquare Error (RMSE), Pearson?s correlation coeffi-cient (?
), and coefficient of determination (R2)5.Table 2 shows the performance of the competingmodels.
We incrementally include each feature setinto GraLap greedily on the basis of ranking shownin Figure 2(a).
We observe that GraLap with onlyFF outperforms SVR+O with 41% improvement of?.
As expected, the inclusion of PF into the modelimproves the model marginally.
However, the over-all performance of GraLap is significantly higherthan any of the baselines (p < 0.01).6 Applications of Reference IntensityIn this section, we provide four different applica-tions to show the use of measuring the intensity ofreferences.
To this end, we consider all the labeledentries for training and run GraLap to predict theintensity of rest of the paper-reference pairs.6.1 Discovering Influential ArticlesInfluential papers in a particular area are often dis-covered by considering equal weights to all the ci-tations of a paper.
We anticipate that consideringthe reference intensity would perhaps return moremeaningful results.
To show this, Here we use thefollowing measures individually to compute the in-fluence of a paper: (i) RawCite: total number ofcitations per paper, (ii) RawPR: we construct a ci-tation network (nodes: papers, links: citations), andmeasure PageRank (Page et al, 1998) of each noden: PR(n) = 1?qN + q?m?M(n)PR(m)|L(m)| ; where,q, the damping factor, is set to 0.85, N is the to-tal number of nodes, M(n) is the set of nodes thathave edges to n, and L(m) is the set of nodes thatm has an edge to, (iii) InfCite: the weightedversion of RawCite, measured by the sum of in-tensities of all citations of a paper, (iv) InfPR:the weighted version of RawPR: PR(n) = 1?qN +q?m?M(n)Inf(m?n)PR(m)?a?L(m)Inf(m?a), where Inf indicatesthe influence of a reference.
We rank all the arti-cles based on these four measures separately.
Ta-ble 3(a) shows the Spearman?s rank correlation be-tween pair-wise measures.
As expected, (i) and(ii) have high correlation (same for (iii) and (iv)),whereas across two types of measures the correla-tion is less.
Further, in order to know which mea-5The less (resp.
more) the value of RMSE and R2 (resp.?
), the better the performance of the models.1354sure is more relevant, we conduct a subjective studywhere we select top ten papers from each measureand invite the experts (not authors) who annotatedthe dataset, to make a binary decision whether a rec-ommended paper is relevant.
6.
The average pair-wise inter-annotator?s agreement (based on Cohen?skappa (Cohen, 1960)) is 0.71.
Table 3(b) presentsthat out of 10 recommendations of InfPR, 7 (5) pa-pers are marked as influential by majority (all) of theannotators, which is followed by InfCite.
Theseresults indeed show the utility of measuring refer-ence intensity for discovering influential papers.
Topthree papers based on InfPR from the entire datasetare shown in Table 4.RowCite RowPR InfCite InfPRRowCite 1 0.82 0.61 0.54RowPR 0.82 1 0.52 0.63InfCite 0.61 0.52 1 0.84InfPR 0.54 0.63 0.84 1(a)Metric All MajorityRowCite 2 5RowPR 2 4InfCite 4 5InfPR 5 7(b)Table 3: (a) Spearman?s rank correlation among in-fluence measures and (b) expert evaluation of theranked results (for top 10 recommendations).6.2 Identifying Influential AuthorsH-index, a measure of impact/influence of an author,considers each citation with equal weight (Hirsch,2005).
Here we incorporate the notion of referenceintensity into it and define hif-index.Definition 2.
An author A with a set of papers P (A)has an hif-index equals to h, if h is the largest valuesuch that |{p ?
P (A)|Inf(p) ?
h}| ?
h; where Inf(p)is the sum of intensities of all citations of p.We consider 37 ACL fellows as the list of gold-standard influential authors.
For comparative eval-uation, we consider the total number of papers(TotP), total number of citations (TotC) and av-erage citations per paper (AvgC) as three competingmeasures along with h-index and hif-index.We arrange all the authors in our dataset in de-creasing order of each measure.
Figure 3(a) showsthe Spearman?s rank correlation among the com-mon elements across pair-wise rankings.
Figure 3(b)shows the Precision@k for five competing mea-sures at identifying ACL fellows.
We observe thathif-index performs significantly well with anoverall precision of 0.54, followed by AvgC (0.37),6We choose papers from the area of ?sentiment analysis?
onwhich experts agree on evaluating the papers.h-index (0.35), TotC (0.32) and TotP (0.34).
Thisresult is an encouraging evidence that the reference-intensity could improve the identification of theinfluential authors.
Top three authors based onhif-index are shown in Table 4.5 10 15 20 25 30 3500.20.40.60.81kPrecision@kH?indexHif?indexTopPTopCAvgC(a) (b)H?index 0.34AvgC  1TopP1 0.34 0.24 0.21TopP TopC AvgC H?index0.240.21 0.28 0.38 0.31 10.310.32 0.24  10.24 0.380.27TopC 1 0.35 0.32 0.280.27Hif?index 0.35Hif?indexFigure 3: (a) Sprearman?s rank correlation amongpair-wise ranks, and (b) the performance of all themeasures.6.3 Effect on Recommendation SystemHere we show the effectiveness of reference-intensity by applying it to a real paper recommen-dation system.
To this end, we consider FeRoSA7(Chakraborty et al, 2016), a new (probably the first)framework of faceted recommendation for scien-tific articles, where given a query it provides facet-wise recommendations with each facet representingthe purpose of recommendation (Chakraborty et al,2016).
The methodology is based on random walkwith restarts (RWR) initiated from a query paper.The model is built on AAN dataset and considersboth the citation links and the content informationto produce the most relevant results.
Instead of us-ing the unweighted citation network, here we use theweighted network with each edge labeled by the in-tensity score.
The final recommendation of FeRoSAis obtained by performing RWR with the transitionprobability proportional to the edge-weight (we callit Inf-FeRoSA).
We observe that Inf-FeRoSAachieves an average precision of 0.81 at top 10 rec-ommendations, which is 14% higher then FeRoSAwhile considering the flat version and 12.34% higherthan FeRoSA while considering the faceted version.6.4 Detecting Citation StackingRecently, Thomson Reuters began screening forjournals that exchange large number of anomalouscitations with other journals in a cartel-like arrange-ment, often known as ?citation stacking?
(Jump,2013; Hardcastle, 2015).
This sort of citation stack-ing is much more pernicious and difficult to detect.7www.ferosa.org1355No Paper Author1.
Lexical semantic techniques for corpus analysis (Pustejovsky et al, 1993) Mark Johnson2.
An unsupervised method for detecting grammatical errors (Chodorow and Leacock, 2000) Christopher D. Manning3.
A maximum entropy approach to natural language processing (Berger et al, 1996) Dan KleinTable 4: Top three papers and authors based on InfPR and Hif-index respectively.0 500 1000 1500 2000050010001500Total citationsTotal citations(excluding self?citations)0 1 2 3 4 5012345IFIF if(a) (b)Figure 4: Correlation between (a) IF and IFif and(b) number of citations before and after removingself-journal citations.We anticipate that this behavior can be detected bythe reference intensity.
Since the AAN dataset doesnot have journal information, we use DBLP dataset(Singh et al, 2015) where the complete metadatainformation (along with reference contexts and ab-stract) is available, except the full content of the pa-per (559,338 papers and 681 journals; more detailsin (Chakraborty et al, 2014)).
From this dataset,we extract all the features mentioned in Section 3.3except the ones that require full text, and run ourmodel using the existing annotated dataset as train-ing instances.
We measure the traditional impactfactor (IF ) of the journals and impact factor afterconsidering the reference intensity (IFif ).
Figure4(a) shows that there are few journals whose IFifsignificantly deviates (3?
from the mean) from IF ;out of the suspected journals 70% suffer from the ef-fect of self-journal citations as well (shown in Figure4(b)), example including Expert Systems with Appli-cations (current IF of 2.53).
One of the future workdirections would be to predict such journals as earlyas possible after their first appearance.7 Related WorkAlthough the citation count based metrics are widelyaccepted (Garfield, 2006; Hirsch, 2010), the beliefthat mere counting of citations is dubious has alsobeen a subject of study (Chubin and Moitra, 1975).
(Garfield, 1964) was the first who explained the rea-sons of citing a paper.
(Pham and Hoffmann, 2003)introduced a method for the rapid development ofcomplex rule bases for classifying text segments.
(Dong and Schfer, 2011) focused on a less man-ual approach by learning domain-insensitive fea-tures from textual, physical, and syntactic aspectsTo address concerns about h-index, different alterna-tive measures are proposed (Waltman and van Eck,2012).
However they too could benefit from filteringor weighting references with a model of influence.Several research have been proposed to weight ci-tations based on factors such as the prestige of theciting journal (Ding, 2011; Yan and Ding, 2010),prestige of an author (Balaban, 2012), frequency ofcitations in citing papers (Hou et al, 2011).
Re-cently, (Wan and Liu, 2014) proposed a SVR basedapproach to measure the intensity of citations.
Ourmethodology differs from this approach in at leasefour significant ways: (i) they used six very shallowlevel features; whereas we consider features fromdifferent dimensions, (ii) they labeled the dataset bythe help of independent annotators; here we addi-tionally ask the authors of the citing papers to iden-tify the influential references which is very realistic(Gilbert, 1977); (iii) they adopted SVR for labeling,which does not perform well for small training in-stances; here we propose GraLap , designed specif-ically for small training instances; (iv) four applica-tions of reference intensity mentioned here are com-pletely new and can trigger further to reassessing theexisting bibliometrics.8 ConclusionWe argued that the equal weight of all referencesmight not be a good idea not only to gauge successof a research, but also to track follow-up work or rec-ommending research papers.
The annotated datasetwould have tremendous potential to be utilized forother research.
Moreover, GraLap can be used forany semi-supervised learning problem.
Each appli-cation mentioned here needs separate attention.
Infuture, we shall look into more linguistic evidencesto improve our model.1356ReferencesAwais Athar and Simone Teufel.
2012.
Context-enhanced citation sentiment detection.
In NAACL,pages 597?601, Stroudsburg, PA, USA.
ACL.Alexandru T. Balaban.
2012.
Positive and negative as-pects of citation indices and journal impact factors.Scientometrics, 92(2):241?247.Adam L. Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Comput.
Linguist.,22(1):39?71, March.Tanmoy Chakraborty, Suhansanu Kumar, Pawan Goyal,Niloy Ganguly, and Animesh Mukherjee.
2014.
To-wards a stratified learning approach to predict fu-ture citation counts.
In Proceedings of the 14thACM/IEEE-CS Joint Conference on Digital Libraries,JCDL ?14, pages 351?360, Piscataway, NJ, USA.IEEE Press.Tanmoy Chakraborty, Amrith Krishna, Mayank Singh,Niloy Ganguly, Pawan Goyal, and Animesh Mukher-jee, 2016.
Advances in Knowledge Discovery andData Mining: 20th Pacific-Asia Conference, PAKDD2016, Auckland, New Zealand, April 19-22, 2016, Pro-ceedings, Part II, chapter FeRoSA: A Faceted Recom-mendation System for Scientific Articles, pages 528?541.
Springer International Publishing, Cham.Martin Chodorow and Claudia Leacock.
2000.
An unsu-pervised method for detecting grammatical errors.
InNAACL, pages 140?147, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.D.
E. Chubin and S. D. Moitra.
1975.
Content-Analysisof References Adjunct or Alternative to CitationCounting.
Social studies of science, 5(4):423?441.J.
Cohen.
1960.
A Coefficient of Agreement for NominalScales.
Educational and Psychological Measurement,20(1):37?41.Isaac G Councill, C Lee Giles, and Min-Yen Kan. 2008.Parscit: an open-source crf reference string parsingpackage.
In LREC, pages 28?30, Marrakech, Mo-rocco.Ying Ding.
2011.
Applying weighted pagerank to authorcitation networks.
JASIST, 62(2):236?245.Cailing Dong and Ulrich Schfer.
2011.
Ensemble-styleself-training on citation classification.
In IJCNLP,pages 623?631.
ACL, 11.David Dubin.
2004.
The most influential paper gerardsalton never wrote.
Library Trends, 52(4):748?764.Eugene Garfield.
1964.
Can citation indexing be au-tomated?
Statistical association methods for mecha-nized documentation, Symposium proceedings, pages188?192.Eugene Garfield.
2006.
The History and Meaning of theJournal Impact Factor.
JAMA, 295(1):90?93.G.
N. Gilbert.
1977.
Referencing as persuasion.
SocialStudies of Science, 7(1):113?122.James Hardcastle.
2015.
Citations, self-citations, andcitation stacking, http://editorresources.taylorandfrancisgroup.com/citations-self-citations\\-and-citation-stacking/.J.
E. Hirsch.
2005.
An index to quantify an individual?sscientific research output.
PNAS, 102(46):16569?16572.J.
E. Hirsch.
2010.
An index to quantify an individ-ual?s scientific research output that takes into accountthe effect of multiple coauthorship.
Scientometrics,85(3):741?754, December.Wen-Ru Hou, Ming Li, and Deng-Ke Niu.
2011.
Count-ing citations in texts rather than reference lists to im-prove the accuracy of assessing scientific contribution.BioEssays, 33(10):724?727.Charles Jochim and Hinrich Schu?tze.
2012.
Towardsa generic and flexible citation classifier based on afaceted classification scheme.
In COLING, pages1343?1358, Bombay, India.Paul Jump.
2013.
Journal citationcartels on the rise, https://www.timeshighereducation.com/news/journal-citation-cartels-on-the-rise/2005009.article.J.
B. Kruskal.
1956.
On the Shortest Spanning Subtree ofa Graph and the Traveling Salesman Problem.
In Pro-ceedings of the American Mathematical Society, vol-ume 7, pages 48?50.Maria Liakata, Shyamasree Saha, Simon Dobnik,Colin R. Batchelor, and Dietrich Rebholz-Schuhmann.2012.
Automatic recognition of conceptualizationzones in scientific articles and two life science appli-cations.
Bioinformatics, 28(7):991?1000.M.
Marneffe, B. Maccartney, and C. Manning.
2006.Generating typed dependency parses from phrasestructure parses.
In LREC, pages 449?454, Genoa,Italy, May.
European Language Resources Association(ELRA).M.
J. Moravcsik and P. Murugesan.
1975.
Some resultson the function and quality of citations.
Social studiesof science, 5(1):86?92.L.
Page, S. Brin, R. Motwani, and T. Winograd.
1998.The pagerank citation ranking: Bringing order to theweb.
In WWW, pages 161?172, Brisbane, Australia.Son Bao Pham and Achim Hoffmann.
2003.
A new ap-proach for scientific citation classification using cuephrases.
In Tamas Domonkos Gedeon and LanceChun Che Fung, editors, Advances in Artificial Intelli-gence: 16th Australian Conference on AI, pages 759?771.
Springer Berlin Heidelberg.1357M.
F. Porter.
1997.
Readings in information retrieval.chapter An Algorithm for Suffix Stripping, pages 313?316.
Morgan Kaufmann Publishers Inc., San Fran-cisco, CA, USA.Friedrich Pukelsheim.
1994.
The Three Sigma Rule.The American Statistician, 48(2):88?91.James Pustejovsky, Peter Anick, and Sabine Bergler.1993.
Lexical semantic techniques for corpus analy-sis.
Comput.
Linguist., 19(2):331?358, June.J.
Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann Publishers Inc., SanFrancisco, CA, USA.Dragomir R. Radev, Pradeep Muthukrishnan, and Va-hed Qazvinian.
2009.
The acl anthology networkcorpus.
In Proceedings of the 2009 Workshop onText and Citation Analysis for Scholarly Digital Li-braries, NLPIR4DL, pages 54?61, Stroudsburg, PA,USA.
ACL.Mikhail V. Simkin and V. P. Roychowdhury.
2003.
ReadBefore You Cite!
Complex Systems, 14:269?274.Mayank Singh, Vikas Patidar, Suhansanu Kumar, Tan-moy Chakraborty, Animesh Mukherjee, and PawanGoyal.
2015.
The role of citation context in pre-dicting long-term citation profiles: An experimentalstudy based on a massive bibliographic text dataset.
InCIKM, pages 1271?1280, New York, NY, USA.
ACM.Henry Small.
1973.
Co-citation in the scientific litera-ture: A new measure of the relationship between twodocuments.
JASIST, 24(4):265?269.Simone Teufel, Advaith Siddharthan, and Dan Tidhar.2006.
Automatic classification of citation function.In EMNLP, pages 103?110, Stroudsburg, PA, USA.ACL.Kristina Toutanova and Christopher D. Manning.
2000.Enriching the knowledge sources used in a maximumentropy part-of-speech tagger.
In EMNLP, pages 63?70, Stroudsburg, PA, USA.
ACL.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.J.
Artif.
Int.
Res., 37(1):141?188, January.Ludo Waltman and Nees Jan van Eck.
2012.
The in-consistency of the h-index.
JASIST, 63(2):406?415,February.Xiaojun Wan and Fang Liu.
2014.
Are all literature ci-tations equally important?
automatic citation strengthestimation and its applications.
JASIST, 65(9):1929?1938.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques, SecondEdition (Morgan Kaufmann Series in Data Manage-ment Systems).
Morgan Kaufmann Publishers Inc.,San Francisco, CA, USA.Erjia Yan and Ying Ding.
2010.
Weighted citation: Anindicator of an article?s prestige.
JASIST, 61(8):1635?1643.Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.2003.
Semi-supervised learning using gaussian fieldsand harmonic functions.
In ICML, pages 912?919,Washington D.C.Xiaodan Zhu, Peter Turney, Daniel Lemire, and AndrVellino.
2015.
Measuring academic influence: Notall citations are equal.
JASIST, 66(2):408?427.1358
