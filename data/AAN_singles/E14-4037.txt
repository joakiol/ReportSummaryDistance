Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 190?194,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsAnalysis and Prediction of Unalignable Words in Parallel TextFrances Yung Kevin DuhNara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara, 630-0192 Japanpikyufrances-y|kevinduh|matsu@is.naist.jpYuji MatsumotoAbstractProfessional human translators usually donot employ the concept of word align-ments, producing translations ?sense-for-sense?
instead of ?word-for-word?.
Thissuggests that unalignable words may beprevalent in the parallel text used for ma-chine translation (MT).
We analyze thisphenomenon in-depth for Chinese-Englishtranslation.
We further propose a sim-ple and effective method to improve au-tomatic word alignment by pre-removingunalignable words, and show improve-ments on hierarchical MT systems in bothtranslation directions.1 MotivationIt is generally acknowledged that absolute equiva-lence between two languages is impossible, sinceconcept lexicalization varies across languages.Major translation theories thus argue that textsshould be translated ?sense-for-sense?
instead of?word-for-word?
(Nida, 1964).
This suggests thatunalignable words may be an issue for the paralleltext used to train current statistical machine trans-lation (SMT) systems.
Although existing auto-matic word alignment methods have some mech-anism to handle the lack of exact word-for-wordalignment (e.g.
null probabilities, fertility in theIBM models (Brown et al., 1993)), they may betoo coarse-grained to model the ?sense-for-sense?translations created by professional human trans-lators.For example, the Chinese term ?tai-yang?
liter-ally means ?sun?, yet the concept it represents isequivalent to the English term ?the sun?.
Since theconcept of a definite article is not incorporated inthe morphology of ?tai yang?, the added ?the?
isnot aligned to any Chinese word.
Yet in anothercontext like ?the man?, ?the?
can be the translationof the Chinese demonstrative pronoun ?na?, liter-ally means ?that?.
A potential misunderstanding isthat unalignable words are simply function words;but from the above example, we see that whether aword is alignable depends very much on the con-cept and the linguistic context.As the quantity and quality of professionally-created parallel text increase, we believe there is aneed to examine the question of unalignable wordsin-depth.
Our goal is to gain a better understand-ing of what makes a fluent human translation anduse this insight to build better word aligners andMT systems.
Our contributions are two-fold:1) We analyze 13000 sentences of manually word-aligned Chinese-English parallel text, quantifyingthe characteristics of unalignable words.2) We propose a simple and effective way to im-prove automatic word alignment, based on pre-dicting unalignable words and temporarily remov-ing them during the alignment training procedure.2 Analysis of Unalignable WordsOur manually-aligned data, which we call OR-ACLE data, is a Chinese-to-English corpus re-leased by the LDC (Li et al., 2010)1.
It con-sists of ?13000 Chinese sentences from news andblog domains and their English translation .
En-glish words are manually aligned with the Chinesecharacters.
Characters without an exact counter-part are annotated with categories that state thefunctions of the words.
These characters are ei-ther aligned to ?NULL?, or attached to their depen-dency heads, if any, and aligned together to forma multi-word alignment.
For example, ?the?
is an-notated as [DET], for ?determiner?, and aligned to?tai-yang?
together with ?sun?.In this work, any English word or Chinese char-acter without an exact counterpart are called un-alignable words, since they are not core to the1LDC2012T16, LDC2012T20 and LDC2012T24190word unalignable coretypes tokens tokenscore or 3581 146,693 562,801unalignable (12%) (17%) (66%)always 25320 / 147,373core (88%) (17%)Table 1: Number of core and unalignable words inhand aligned ORACLE corpusmulti-word alignment.
All other English words orChinese characters are referred to as core words.2.1 What kind of words are unalignable?Analyzing the hand aligned corpus, we find thatwords annotated as unalignable do not come froma distinct list.
Table 1 reveals that 88% of theword types are unambiguously core words.
Yetthese word types, including singletons, accountfor only 17% of the word tokens.
On the otherhand, another 17% of the total word tokens areannotated as unalignable.
So, most word types arepossibly unalignable but only in a small portion oftheir occurrence, such as the following examples:(1a) Chi: yi ge di fangone (measure word) placeEng: one place(1b) Chi: ge renpersonalEng: personal(2a) Chi: ming tian zhong wu(tomorrow) (midday)Eng: tomorrow at midday(2b) Chi: zai jiaat/in/on homeEng: at homeIn example (1a), ?ge?
is a measure word that isexclusive in Chinese, but in (1b), it is part of themultiword unit ?ge-ren?
for ?personal?.
Similarly,prepositions, such as ?at?, can either be omitted ortranslated depending on context.Nonetheless, unalignable words are by nomeans evenly distributed among word types.
Ta-ble 2 shows that the top 100 most frequent un-alignable word types already covers 78% and 94%of all Chinese and English unalignable instances,respectively.
Word type is thus an important clue.Intuitively, words with POS defined only in oneof the languages are likely to be unalignable.
Toexamine this, we automatically tagged the ORA-CLE data using the Standford Tagger (ToutanovaMost frequent Token countunalignable word types Chinese EnglishTop 50 34,987 83,905(68%) (88%)Top 100 40,121 89,609(78%) (94%)Table 2: Count of unalignable words by typeset al., 2003).
We find that the unalignable wordsinclude all POS categories of either language,though indeed some POS are more frequent.
Ta-ble 3 lists the top 5 POS categories that most un-alignable words belong to and the percentage theyare annotated as unalignable.
Some POS cate-gories like DEG are mostly unalignable regardlessof context, but other POS tags such as DT and INdepend on context.Chi.
No.
and % of Eng.
No.
and % ofPOS unalign.
POS unalign.DEG 7411(97%) DT 27715 (75%)NN 6138 (4%) IN 19303 (47%)AD 6068 (17%) PRP 5780 (56%)DEC 5572 (97%) TO 5407 (62%)VV 4950 (6%) CC 4145 (36%)Table 3: Top 5 POS categories of Chinese and En-glish unalignable wordsNote also that many Chinese unalignable wordsare nouns (NN) and verbs (VV).
Clearly we cannotindiscriminately consider all nouns as unalignable.Some examples of unalignable content words inChinese are:(3) Chi: can jia hui jian huo dongparticipate meeting activityEng: participate in the meeting(4) Chi: hui yi de yuan man ju xingmeeting ?s successful take placeEng: success of the meetingEnglish verbs and adjectives are often nomi-nalized to abstract nouns (such as ?meeting?
from?meet?, or ?success?
from ?succeed?
), but suchderivation is rare in Chinese morphology.
SincePOS is not morphologically marked in Chinese,?meeting?
and?meet?
are the same word.
To reducethe processing ambiguity and produce more nat-ural translation, extra content words are added tomark the nominalization of abstract concepts.
Forexample, ?hui jian?
is originally ?to meet?.
Adding?huo dong?
(activity) transforms it to a noun phrase191(example 3), similar to the the addition of ?jusing?
(take place) to the adjective ?yuan man?
(ex-ample 4).
These unalignable words are not lexi-cally dependent but are inferred from the context,and thus do not align to any source words.To summarize, a small number of word typescover 17% of word tokens that are unalignable,but whether these words are unalignable dependssignificantly on context.
Although there is no listof ?always unalignable?
words types or POS cat-egories, our analysis shows there are regularitiesthat may be exploited by an automatic classifier.3 Improved Automatic Word AlignmentWe first propose a classifier for predicting whethera word is unalignable.
Let (eJ1, fK1) be a pair ofsentence with length J and K. For each word in(eJ1, fK1) that belongs to a predefined list2of po-tentially unalignable words, we run a binary clas-sifier.
A separate classifier is built for each wordtype in the list, and an additional classifier for allthe remaining words in each language.We train an SVM classifier based on the fol-lowing features: Local context: Unigrams andPOS in window sizes of 1, 3, 5, 7 around theword in question.
Top token-POS pairs: Thisfeature is defined by whether the token in ques-tion and its POS tag is within the top n frequenttoken-POS pairs annotated as unalignable like inTables 2 and 3.
Four features are defined with n =10, 30, 50, 100.
Since the top frequent unalignablewords cover most of the counts as shown in theprevious analysis, being in the top n list is a strongpositive features.
Number of likely unalignablewords per sentence: We hypothesize that thetranslator will not add too many tokens to thetranslation and delete too many from the sourcesentence.
In the ORACLE data, 68% sentenceshave more than 2 unalignable words.
We approx-imate the number of likely unalignable words inthe sentence by counting the number of wordswithin the top 100 token-POS pairs annotated asunalignable.
Sentence length and ratio: Longersentences are more likely to contain unalignablewords than shorter sentences.
Also sentence ra-tios that deviate significantly from the mean arelikely to contain unalignable words.
Presence ofalignment candidate: This is a negative featuredefined by whether there is an alignment candi-2We define the list as the top 100 word types with thehighest count of unalignable words per language accordingto the hand annotated data.date in the target sentence for the source word inquestion, or vice versa.
The candidates are ex-tracted from the top n frequent words aligned toa particular word according to the manual align-ments of the ORACLE data.
Five features are de-fined with n = 5, 10, 20, 50, 100 and one ?withoutlimit?, such that a more possible candidate will bedetected by more features.Next, we propose a simple yet effective mod-ification to the word alignment training pipeline:1.
Predict unalignable words by the classifier2.
Remove these words from the training corpus3.
Train word alignment model (e.g.
GIZA++)34.
Combine the word alignments in both direc-tions with heuristics (grow-diag-final-and)5.
Restore unaligned words to original position6.
Continue with rule extraction and the rest ofthe MT pipeline.The idea is to reduce the difficulty for the wordalignment model by removing unaligned words.4 End-to-End Translation ExperimentsIn our experiments, we first show that removingmanually-annotated unaligned words in ORACLEdata leads to improvements in MT of both trans-lation directions.
Next, we show how a classifiertrained on ORACLE data can be used to improveMT in another large-scale un-annotated dataset.44.1 Experiments on ORACLE dataWe first performed an ORACLE experiment us-ing gold standard unaligned word labels.
Follow-ing the training pipeline in Section 3, we removedgold unalignable words before running GIZA++and restore them afterwards.
90% of the data isused for alignment and MT training, while 10% ofthe data is reserved for testing.The upper half of Table 4 list the alignmentprecision, recall and F1 of the resulting align-ments, and quality of the final MT outputs.
Base-line is the standard MT training pipeline with-out removal of unaligned words.
Our Proposedapproach performs better in alignment, phrase-based (PBMT) and hierarchical (Hiero) systems.The results, evaluated by BLEU, METEOR andTER, support our hypothesis that removing goldunalignable words helps improve word alignmentand the resulting SMT.3We can suppress the NULL probabilities of the model.4All experiments are done using standard settings forMoses PBMT and Hiero with 4-gram LM and mslr-bidirectional-fe reordering (Koehn et al., 2007).
The clas-sifier is trained using LIBSVM (Chang and Lin, 2011).192Align PBMT Hieroacc.
C-E E-C C-E E-CORACLE P .711 B 11.4 17.4 10.3 15.8Baseline R .488 T 70.9 69.0 75.9 72.3F1.579 M 21.8 23.9 21.08 23.7ORACLE P .802 B 11.8+18.3+11.0+17.2+Proposed R .509 T 71.4?65.7+74.7+68.7+(gold) F1.623 M 22.1+24.1+22.0+24.0+REAL B 18.2 18.5 17.0 17.2Baseline T 63.4 67.2 68.0 71.4M 22.9 24.6 22.9 24.8REAL B 18.6 18.5 17.6+18.1+Proposed T 63.8?66.5+67.6 69.7+(predict) M 23.2+24.5 23.4+24.7Table 4: MT results of ORACLE and REAL ex-periments.
Highest score per metric is bolded.{+/?}
indicates statistically significant improve-ment/degradation, p < 0.05.
(P: precision; R: re-call; B: BLEU; M: METEOR; T:TER)For comparison, a naive classifier that labelsall top-30 token-POS combinations as unalignableperforms poorly as expected (PBMT BLEU: 9.87in C-E direction).
We also evaluated our proposedclassifier on this task: the accuracy is 92% and itachieves BLEU of 11.55 for PBMT and 10.84 forHiero in C-E direction, which is between the re-sults of gold-unalign and baseline.4.2 Experiments on large-scale REAL dataWe next performed a more realistic experiment:the classifier trained on ORACLE data is used toautomatically label a large data, which is then usedto train a MT system.
This REAL data consists ofparallel text from the NIST OpenMT2008.5MTexperiments are performed in both directions.The lower half of Table 4 shows the perfor-mance of the resulting MT systems.
We observethat our proposed approach is still able to improveover the baseline.
In particular, Hiero achievedstatistical significant improvements in BLEU andMETEOR.6Comparing to the results of PBMT,this suggests our method may be most effective inimproving systems where rule extraction is sen-5We use the standard MT08 test sets; the trainingdata includes LDC2004T08, 2005E47, 2005T06, 2007T23,2008T06, 2008T08, 2008T18, 2009T02, 2009T06, 2009T15,and 2010T03 (34M English words and 1.1M sentences).Since we do not have access to all OpenMT data, e.g.
FBIS,our results may not be directly comparable to other systemsin the evaluation.6Interestingly, PBMT did better than Hiero in this setup.Chinese English lexical translationword Baseline only Propose onlyxie (bring) him bringingxing (form) and modeldan (but) it, the, they yet, neverthelesspa (scare) that, are, be fears, worriedTable 5: Examples of translations exclusivelyfound in the top 15 lexical translation.Figure 1: Classifier accuracy and MT results V.S.proportion of ORACLE datasitive to the underlying alignments, such as Hi-ero and Syntax-based MT.
Table 5 shows the lex-ical translations for some rare Chinese words: thebaseline tends to incorrectly align these to func-tion words (garbage collection), while the pro-posed method?s translations are more reasonable.To evaluate how much annotation is needed forthe classifier, we repeat experiments using differ-ent proportions of the ORACLE data.
Figure 1shows training by 20% of the data (2600 sents.
)already leads to significant improvements (p <0.05), which is a reasonable annotation effort.5 ConclusionWe analyzed in-depth the phenomenon of un-alignable words in parallel text, and show thatwhat is unalignable depends on the word?s conceptand context.
We argue that this is not a trivial prob-lem, but with an unalignable word classifier anda simple modified MT training pipeline, we canachieve small but significant gains in end-to-endtranslation.
In related work, the issue of droppedpronouns (Chung and Gildea, 2010) and functionwords (Setiawan et al., 2010; Nakazawa and Kuro-hashi, 2012) have been found important in wordalignment, and (Fossum et al., 2008) showed thatsyntax features are helpful for fixing alignments.An interesting avenue of future work is to integratethese ideas with ours, in particular by exploitingsyntax and viewing unalignable words as alignedat a structure above the lexical level.193ReferencesPeter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:Parameter estimation.
Computational Linguistics,19(2).Chih-Chung Chang and Chih-Jen Lin.
2011.
Lib-svm : a library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2(27).Tagyoung Chung and Daniel Gildea.
2010.
Effectsof empty categories on machine translation.
Pro-ceedings of the Conference on Empirical Methodson Natural Language Processing.Victoria Fossum, Kevin Knight, and Steven Abney.2008.
Using syntax to improve word alignment pre-cision for syntax-based machine translation.
Pro-ceedings of the Workshop on Statistical MachineTranslation.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
Proceedings of the Annual Meeting of the As-sociation for Computational Linguistics.Xuansong Li, Niyu Ge, Stephen Grimes, Stephanie M.Strassel, and Kazuaki Maeda.
2010.
Enrichingword alignment with linguistic tags.
Proceedingsof International Conference on Language Resourcesand Evaluation.Toshiaki Nakazawa and Sado Kurohashi.
2012.Alignment by bilingual generation and monolingualderivation.
Proceedings of the International Confer-ence on Computational Linguistics.Eugene A Nida.
1964.
Toward a Science of Translat-ing: with Special Reference to Principles and Pro-cedures Involved in Bible Translating.
BRILL.Hendra Setiawan, Chris Dyer, and Philip Resnik.
2010.Discriminative word alignment with a function wordreordering model.
Proceedings of the Conference onEmpirical Methods on Natural Language Process-ing.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.Proceedings of the Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies.194
