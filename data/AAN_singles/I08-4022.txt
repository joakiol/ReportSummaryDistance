Chinese Named Entity Recognition and Word SegmentationBased on CharacterHe Jingzhou, Wang HoufengInstitution of Computational LinguisticsSchool of Electronics Engineering and Computer Science,Peking University, China, 100871{hejingzhou, wanghf}@pku.edu.cnAbstractChinese word segmentation and namedentity recognition (NER) are both importanttasks in Chinese information processing.This paper presents a character-basedConditional Random Fields (CRFs) modelfor such two tasks.
In The SIGHANBakeoff 2007, this model participated in allclosed tracks for both Chinese NER andword segmentation tasks, and turns out toperform well.
Our system ranks 2nd in theclosed track on NER of MSRA, and 4th inthe closed track on word segmentation ofSXU.1 IntroductionChinese word segmentation and NER are two ofthe most fundamental problems in Chineseinformation processing and have attracted moreand more attentions.
Many methods have beenpresented, of which, machine learning methodshave obviously competitive advantage in suchproblems.
Maximum Entropy (Ng and Low, 2005)and CRFs (Hai Zhao et al 2006, Zhou Junsheng etal.
2006) come to good performance in the formerSIGHAN Bakeoff.We consider both tasks as sequence labelingproblem, and a character-based ConditionalRandom Fields (CRFs) model is applied in thisBakeoff.
Our system used CRF++ packageVersion 0.49 implemented by Taku Kudo fromsourceforge1.2 System DescriptionThe system is mainly based on CRFs, whiledifferent strategies are introduced in wordsegmentation task and NER task.2.1 CRFsCRFs are undirected graphical models which areparticularly well suited to sequence labeling tasks,such as NER & word segmentation.
In these cases,CRFs are often referred to as linear chain CRFs.CRFs are criminative models, which allow aricher feature representation and provide morenatural modeling.1http://www.sourceforge.net/128Sixth SIGHAN Workshop on Chinese Language ProcessingCRFs define the conditional probability of astate sequence given an input sequence asWhere F is a feature function set over itsarguments, ?k is a learned weight for each featurefunction, and Z is the partition function, whichensures that p is appropriately normalized.2.2 Word Segmentation TaskSimilar to (Ng and Low, 2005), a Chinesecharacter comes into four different tags, as inTable 1.Tag MeaningS Character that occurs as asingle-character wordB Character that begins amulti-character wordI Character that continues amulti-character wordE Character that ends amulti-character wordTable 1.
Word segmentation tag set(Ng and Low, 2005) presented feature templatesas:(a) Cn(n = ?2,?1, 0, 1, 2)(b) CnCn+1(n = ?2,?1, 0, 1)(c) C?1C1(d) Pu(C0)(e) T(C?2)T(C?1)T(C0)T(C1)T(C2)In order to find the effect of different features onthe result, some experiments are conducted onthese templates, with 90% of the training dataprovided by The SIGHAN Bakeoff 2007 fortraining, leaving 10% for testing.
Based on ourresults, some templates are adjusted as follows.First, the character-window is reduced to (-1, 0,1) in (a).Second, feature template (d) is not used.
Instead,original sentences are split into clauses ended withpunctuations ??
?, ??
?, ??
?, ??
?, ???
and ??
?.There are two advantages of this processing: (1)the template is simplified but little performance islost; (2) shorter sentences make CRFs trainingquicker.Third, template (e) is separate it to three items:T(C-1), T(C0) and T(C1), in which five types ofthe characters are considered: N stands fornumbers, D for dates, E for English letters, S forpunctuations and C for other characters.
Besides,this feature template does not always contribute tothe segmentation result in our experiments, so itwill be a tradeoff whether to use it or not accordingto experiments.Finally, we use the following feature templates:(a) Cn(n = ?1, 0, 1)(b) CnCn+1(n = ?1, 0)(c) C?1C1(d) T(Cn) (n = ?1, 0, 1)We only took part in word segmentation closedtrack, so no additional corpora, dictionary orlinguistic resources are introduced.1.1 NER TaskMany Chinese NER researches are based on wordsegmentation and even Part-Of-Speech (POS)tagging.
In fact these steps are not necessary.
Therelationship of them is described in Figure 1.129Sixth SIGHAN Workshop on Chinese Language ProcessingIn closed track of both MSRA and CITYU, acharacter-based CRFs model is used in our system.There two reasons as follows:First, no word-level information is provided intraining data of NER tasks in closed track, so it?shard to perform word segmentation with goodaccuracy.Second, we had done some experiments onChinese NER, and found that character-basedmethod outperformed word segmentation and wordsegmentation + POS, if only character sequence isgiven.
Table 2 shows the comparison results.FeatureLevelIntegratedF-measureCharacter 0.8760Word 0.8538POS 0.8635Table 2.
Comparison result among different NERmodels33Train with Annotation Corpora of People's Daily199801 and test with 199806In our NER system, a Chinese character can belabeled as one of four different tags, as in Table 3.Tag MeaningB First character of a NEI Character in a NE but neitherthe first nor the last oneE Last character of a NE except asingle-character oneO Character not in a NETable 3.
NER tag setIt?s similar to the standard of The SIGHANBakeoff 2007 NER track except for an additionaltag ?E?.
Unlike the tag set used in wordsegmentation task, there is no ?S?
tag forsingle-character NEs.
This kind of entities isusually surname of a Chinese person.
In this case,the tag ?B?
will handle it as well.There are actually 3 types of NEs in MSRA andCITYU corpora: PER, LOC and ORG, so the tagset is further divided into 10 sub tags: B-PER,I-PER, E-PER, B-LOC, I-LOC, E-LOC, B-ORG,I-ORG, E-ORG and O.The feature template is similar to the one used inword segmentation task except that here acharacter-window of (-2,-1, 0, 1,2) is applied:(a) Cn(n = ?2,?1, 0, 1, 2)(b) CnCn+1(n = ?2,?1, 0, 1)(c) C?1C1(d) T(Cn) (n = ?1, 0, 1)For CRFs, the precision is usually high whilerecall is low.
To solve this problem, a set of featuretemplates (only differ in window size, orpunctuations) are used to train several differentmodels, and finally achieve a group of results.Merge them as in Table 4 (for the same Chinesecharacter string in result A and B).RecognitionTaggingSegmentationCharacterWordPOSNEsFigure 1.
NER model achitecture130Sixth SIGHAN Workshop on Chinese Language ProcessingA B ResultIs a NE Isn?t a NE Refer to AIsn?t aNEIs a NE Refer to BIsn?t aNEIsn?t a NE Refer to A or BIs a NE Is the sameNE type asARefer to A or BIs a NE Is a NE butnot the sametype as AChoose A or Baccording topredefined rulesTable 4.
Merge strategy of resultsWith a slight loss of precision, an improvementis achieved on recall rate.In open track of MSRA, an additionalsegmentation system is used on the corpora andsome NEs are retrieved based on severalpredefined rules.
It was merged with closed trackresult to form open track result.3 Evaluation ResultsOur word segmentation system is evaluated inclosed track on all 5 corpora of CITYU, CKIP,CTB, NCC and SXU.
Table 5 shows our results onthe best RunID.
Columns R, P, and F show therecall, precision, and F measure, respectively.Column BEST shows best F-measure of allparticipants in the track.Our NER system is evaluated in closed track onboth MSRA and CITYU corpora, and open trackon MSRA corpora only.
Table 6 shows our officialresults on best RunID.
Columns R, P, and F showthe recall, precision, and F measure, respectively.Column BEST shows best F-measure of allparticipants in the track.4 ConclusionIn this paper, a character-based CRFs model isintroduced on both word segmentation and NER.Experiments are done to form our feature templates,and approaches are used to further improve itsperformance on NER.
The evaluation results showits competitive performance in The SIGHANBakeoff 2007.
We?ll launch more research andexperiments on feature picking-up methods andcombination between character-based model andother models in the future.Table 5.
Evaluation results on word segmentationTrack R P F BESTCITYUclosed0.7608 0.8751 0.814 0.8499MSRAclosed0.8862 0.9304 0.9078 0.9281MSRAopen0.9135 0.9321 0.9227 0.9988Table 6.
Evaluation results on NERTrack(allclosed)R P F BESTCITYU 0.9421 0.9339 0.938 0.951CKIP 0.9369 0.927 0.9319 0.947CTB 0.9487 0.9514 0.95 0.9589NCC 0.9278 0.925 0.9264 0.9405SXU 0.9543 0.9568 0.9556 0.9623131Sixth SIGHAN Workshop on Chinese Language ProcessingReferencesHai Zhao, Chang-Ning Huang and Mu Li.
AnImproved Chinese Word SegmentationSystem with Conditional Random Field.
2006.Proceedings of the Fifth SIGHAN Workshopon Chinese Language Processing.Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo.
Amaximum Entropy Approach to ChineseWord Segmentation.
2005.
Proceedings of theFourth SIGHAN Workshop on ChineseLanguage Processing.Wang Xinhao, Lin Xiaojun, Yu Dianhai, TianHao,Wu Xihong.
Chinese WordSegmentation with Maximum Entropy andN-gram Language Model.
2006.
Proceedingsof the Fifth SIGHAN Workshop on ChineseLanguage Processing.Zhou Junsheng, Dai Xinyu, He Liang, Chen Jiajun.Chinese Named Entity Recognition with aMulti-Phase Model.
2006.
Proceedings of theFifth SIGHAN Workshop on ChineseLanguage Processing.AcknowledgementThis paper is supported by National NaturalScience Foundation of China (No.
60675035),National Social Science Foundation of China (No.05BYY043) and Beijing Natural ScienceFoundation (No.
4072012).132Sixth SIGHAN Workshop on Chinese Language Processing
