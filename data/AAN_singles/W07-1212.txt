Proceedings of the 5th Workshop on Important Unresolved Matters, pages 89?96,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsCreating a Systemic Functional Grammar Corpus from the Penn TreebankMatthew Honnibal and James R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australia{mhonn, james}@it.usyd.edu.auAbstractThe lack of a large annotated systemic func-tional grammar (SFG) corpus has posed asignificant challenge for the development ofthe theory.
Automating SFG annotation ischallenging because the theory uses a mini-mal constituency model, allocating as muchof the work as possible to a set of hierarchi-cally organised features.In this paper we show that despite the un-orthodox organisation of SFG, adapting ex-isting resources remains the most practicalway to create an annotated corpus.
Wepresent and analyse SFGBank, an automatedconversion of the Penn Treebank into sys-temic functional grammar.
The corpus iscomparable to those available for other lin-guistic theories, offering many opportunitiesfor new research.1 IntroductionSystemic functional grammar (Halliday andMatthiessen, 2004) aims to describe the set ofmeaningful choices a speaker makes when putting athought into words.
Each of these choices is seen asa resource for shaping the meaning in a particularway, and the selection will have a distinct grammat-ical outcome as well as a semantic implication.
Thechoices are presented hierarchically, so that earlyselections restrict other choices.
For instance, if aspeaker chooses imperative mood for a clause, theycannot choose a tense.
Each selection is linked to asyntactic expression rule.
When imperative moodis selected, the subject of the clause is suppressed;when interrogative mood is selected, the order ofthe subject and first auxiliary are reversed.Systemic grammars are very different from gram-mars influenced by the formalist tradition.
Systemicanalysis locates a constituent within a typology, andyields a set of features that describe its salient prop-erties.
These features have proven useful for re-search in applied linguistics, on topics such as stylis-tics, discourse analysis and translation.
As a gener-ative theory, systemic grammars are less effective.There have been a few attempts, such as those dis-cussed by O?Donnell and Bateman (2005), but as yeta wide coverage systemic grammar that can be usedfor tractable parsing has not been developed.The lack of a corpus and parser has limited re-search on systemic grammars, as corpus studies havebeen restricted to small samples of manually codedexamples, or imprecise queries of unannotated data.The corpus we present, obtained by converting thePenn Treebank, addresses this issue.
It also suggestsa way to automatically code novel text, by convert-ing the output of a parser for a different formalism.This would also allow the use of SFG features forNLP applications to be explored, and support currentresearch using SFG for applied linguistics.The conversion process relies on a set of manuallycoded rules.
The first step of the process is to col-lect SFG clauses and their constituents from parses inthe Penn Treebank.
Each clause constituent is thenassigned up to three function labels, for the three si-multaneous semantic and pragmatic structures Hal-liday (1970) describes.
Finally, the system featuresare calculated, using rules referring to the functionlabels assigned in the previous step.
This paper ex-tends the work described in Honnibal (2004).892 Related WorkConverting the Penn Treebank is the standard ap-proach to creating a corpus annotated according to aspecific linguistic theory.
This has been the methodused to create LTAG (Frank, 2001), LFG (Franket al, 2003) and CCG (Hockenmaier and Steedman,2005) corpora, among others.
We employ a similarmethodology, converting the corpus using manuallyspecified rules.Since the SFG annotation is semantically oriented,the work also bears some resemblance to Prop-bank (Palmer et al, 2005).
However, Propbank isconcerned with manually adding information to thePenn Treebank, rather than automatically reinter-preting the same information through the lens of adifferent linguistic theory.We chose not to base our conversion on the Prop-bank annotation, as it does not currently cover theBrown or Switchboard sections of the Treebank.The wider variety of genres provided by these sec-tions makes the corpus much more useful for SFG,since the theory devotes significant attention to prag-matic phenomena and stylistic variation.3 Systemic Functional GrammarGenerating a constituent using a systemic func-tional grammar involves traversing a decision-tree-like structure referred to as a system network.
Thenodes of this tree are referred to as systems, and theoptions from the systems are referred to as features.At each system, the feature selected may add con-straints on the type, number or order of the internalstructure of the constituent.
When the entire net-work has been traversed, the constraints are unified,and the required constituents generated.In order to annotate a sentence according to a sys-temic functional grammar, we must specify the setof features encountered as the system network is tra-versed, and apply function labels to each constituent.The function labeling is required because the con-straints are always specified according to the childconstituents?
function, rather than their form.Constituents may have more than one functionlabel, as SFG describes three metafunctions, fol-lowing Halliday?s (1969) argument that a clause isstructured simultaneously as a communicative act, apiece of information, and a representation of reality.Interpersonal function labels are assigned to clauseconstituents in determining the clause?s communica-tive status.
The most important interpersonal func-tions are Subject and Finite, since the relative posi-tion of the constituents bearing these labels largelydetermines whether the clause will be a question,statement or command.The textual structure of the clause includes thefunctions Theme and Rheme, following Halliday?s(1970) theory of information structure.Finally, the experiential function of a constituentis its semantic role, described in terms of a smallset of labels that are only minimally sensitive to thesemantics of the predicate.4 Annotation ImplementedWe base our annotation on the clause network inthe Nigel grammar (Mann and Matthiessen, 1983),as it is freely available and discussed at length inMatthiessen (1995).
It is difficult to include annota-tion from the group and phrase networks, because ofthe flat bracketing of constituents in the Penn Tree-bank.
The converted corpus has full coverage overall sections of the Penn Treebank 3 corpus.We implement features from 41 systems from theclause network, out of a possible 62.
The mostprominent missing features relate to process type.The process type system classifies clauses as one offour broad semantic types: material, mental, verbalor relational, with subsequent systems making finergrained distinctions.
This is mostly determined bythe argument structure of the verb, but also dependson its lexical semantics.
Process type assignmenttherefore suffers from word sense ambiguity, so weare unable to select from this system or others whichdepend on its result.
Figure 1 gives an example ofa clause with interpersonal, textual and experientialfunction labels applied to its constituents.5 Creating the CorpusSFG specifies the structure of a clause from ?above?,by setting constraints that are imposed by the set offeatures selected from the system network.
Theseconstraints describe the structure in terms of inter-personal, textual and experiential function labels.These functions then determine the boundaries ofthe clause, by specifying its constituents.90Constituent Interpersonal Textual Ideationaland ?
Txt.
Theme ?last year Adjunct Top.
Theme Circumstanceprices Subject Rheme Participantwere Finite Rheme ?quickly Adjunct Rheme Circumstanceplummeting Predicator Rheme ProcessTable 1: SFG function labels assigned to clause constituents.preprocess(parse)clauses = []for word in parse.words():if isPredicate(word):constituents = getConstituents(word)clauses.append(constituents)Figure 2: Conversion algorithm.The Penn Treebank provides rich syntactic trees,specifying the structure of the sentence.
We there-fore proceed from ?below?, using the Penn Treebankto find clauses and their constituents, then applyingfunction labels to them, and using the function labelsas the basis for rules to traverse the system network.5.1 Finding ConstituentsIn this stage, we search the Treebank parse forSFG clauses, and collect their constituents.
Clausesare identified by searching for predicates that headthem, and constituents are collecting by traversingupwards from the predicate, collecting the nodes?siblings until we hit an S node.There are a few common constructions whichpresent problems for one or both of these pro-cedures.
These exceptions are handled by pre-processing the Treebank tree, changing its structureto be compatible with the predicate and constituentextraction algorithms.
Figure 2 describes the con-version process more formally.5.1.1 Finding predicatesA predicate is the main verb in the clause.
In theTreebank annotation, the predicate will be the wordattached to the lowest node in a VP chain, becauseauxiliaries attach higher up.
Figure 3 describes thefunction to decide whether a word is a predicate.
Es-sentially, we want words that are the last word at-tached to a VP, that do not have a VP sibling.Figure 1 marks the predicates and constituents ina Treebank parse.
The predicates are underlined, andthe constituents numbered to match the predicate.if verb.parent.label == ?VP?
:for sibling in verb.parent.children:if sibling.isWord():if sibling.offset > verb.offset:return Falseif sibling.label == ?VP?
:return Falsereturn TrueFigure 3: Determining whether a word is a predicate.node = predicateconstituents = [predicate]while node.label not in clauseLabels:for sibling in node.parent.children:if sibling != node:constituents.append(sibling)for sibling in node.parent.children:if sibling != nodeand sibling.label in conjOrWHLabels:constituents.append(sibling)Figure 4: Finding constituents.5.1.2 Getting ConstituentsOnce we have a predicate, we can traverse the treearound it to collect the constituents in the clause itheads.
We do this by collecting its siblings and mov-ing up the tree, collecting the ?uncle?
nodes, until wehit the top of the clause.
Figure 4 describes the pro-cess more formally.
The final loop collects conjunc-tions and WH constituents that attach alongside theclause node, such as the ?which?
in Figure 1.5.1.3 Pre-processing Ellipsis and GappingEllipsis and gapping involve two or more pred-icates sharing some constituents.
When the shar-ing can be denoted using the tree structure, by plac-ing the shared items above the point where the VPsfork, we refer to the construction as ellipsis.
Figure5 shows a sentence with a subject and an auxiliaryshared between two predicates.
3.4% of predicatesshare at least one constituent with another clause viaellipsis.
We pre-process ellipsis constructions by in-serting an S node above each VP after the first, andadding traces for the shared constituents.91Shhhhhhhhhhhh((((((((((((NP 1XXXXXNPbb""The plantSBARPPPPWHNP 2whichSVPPPPis 2 VPaaa!!
!owned 2 PP 2HHHby Vose CoVPXXXXwas 1 VPPPPPemployed 1 S-PRP 1VPaaa!!
!to 3 VPbb""make 3 NP 3themFigure 1: A parse tree with predicates underlined and constituents numbered.In gapping constructions, the shared constituentis the predicate itself, and what differs between thetwo clauses are the arguments.
The Treebank usesspecial trace rules to describe which arguments mustbe copied across to the gapped clause.
We createtraces to the shared constituents and add them toeach gapped clause, so that the trace of the verb willbe picked up as a predicate later on.
Gapping is avery rare phenomenon ?
only 0.02% clauses havegapped predicates.5.1.4 Pre-processing Semi-auxiliariesIn Figure 6 the verb ?continue?
will match ourrules for predicate extraction, described in Section5.1.
SFG analyses this and other ?semi-auxiliaries?
(Quirk et al, 1991) as a serial verb construction,rather than a matrix clause and a complement clause.Since we want to treat the finite verb as though itwere an auxiliary, we pre-process these cases bysimply deleting the S node, and attaching its chil-dren directly to the semi-auxiliary?s VP.Defining the semi-auxiliary constructions is notso simple, however.
Quirk et al note that someof these verbs are more like auxiliaries than others,and organise them into a rough gradient accordingto their formal properties.
The problem is that thereis not clear agreement in the SFG literature aboutwhere the line should be drawn.
Matthiessen (1995)describes all non-finite sentential complements asserial-verb constructions.
Martin et al (1997) arguethat verbs such as ?want?
impose selectional restric-SPPPPNPPricesVPHHcontinue SVPll,,to riseFigure 6: Treebank representation of a sentence witha semi-auxiliary.tions on the subject, and therefore should be treatedas full verbs with a clause complement.
Other com-promises are possible as well.Using Matthiessen?s definition, we collect 5.3%fewer predicates than if we treated all semi-auxiliaries as main verbs.
If the complement clausehas a different subject from the parent clause, whenthe two are merged the new verb will seem to haveextra arguments.
58% of these mergings introducean extra argument in this way.
For example,Investors want the market to boomwill be analysed as though boom has two argu-ments, investors and market.
We prevent this fromoccurring by adding an extra condition for merg-ing clauses, stating that the subject of the embeddedclause should be a trace co-indexed with the subjectof the parent clause.92SXXXXXXNPAsbestosVPhhhhhhh(((((((was VPhhhhhhhh@@((((((((VPaaa!!
!used PPPPPPin the early 1950sand VPHHHreplaced PPZZin 1956Figure 5: Treebank representation of ellipsis.
Predicates are underlined, shared items are in bold.5.2 Constituent functionsAs discussed above, we attach up to three functionlabels to each clause constituent, one for each meta-function.
The rules to do this rely on the order ofconstituents and the function dash-tags in the PennTreebank.
Some experiential function rules also re-fer to interpersonal labels, and some textual functionrules refer to experiential labels.5.2.1 Interpersonal Function LabelsThe possible interpersonal function labels we as-sign are Subject, Complement, Adjunct, Finite, andPredicator.
The Finite and Predicator are the firsttensed verb, and the predicate respectively.
If thereare no auxiliary verbs, Halliday and Matthiessen(2004) describes the predicate as functioning bothas Finite and Predicator.
Since this is the only casein which a constituent would receive multiple labelsfrom a single metafunction, we instead assign thesingle label Finite/Predicator.For NPs, Subjects, Complements and Adjunctsare distinguished using the Penn Treebank?s dash-tag function labels.
SFG always assigns preposi-tional phrases the label Adjunct.
All NP constituentsthat are not marked with an adverbial function tag inthe Treebank are labeled Complement.
Conjunctionsare not assigned interpersonal functions.5.2.2 Experiential Function LabelsThe experiential function labels we assign areParticipant, Process and Circumstance.
This is asimplification of the function labels described byHalliday and Matthiessen (2004), as Participants areusually subdivided into what other linguistic theo-ries refer to as semantic roles.
SFG has its own se-mantic role description, which relies on process typefeatures.
For instance, Participants in a verbal pro-cess like ?say?
have the role options Sayer, Target,Receiver and Verbiage.Distinguishing process types requires a wordsense disambiguated corpus and a word sense sen-sitive process type lexicon.
While there is a signifi-cant intersection between the Penn Treebank and theSemcor word sense disambiguated corpus, there iscurrently no suitable process type lexicon.
Conse-quently, Participants have not been subtyped.
TheProcess is simply the verb phrase, while the Subjectand Complements are Participants.5.2.3 Textual Function labelsThe textual metafunction describes the informa-tion structure of the clause.
Halliday?s textual func-tion labels are Textual Theme, Interpersonal Theme,Topical Theme and Rheme.
Theme and Rheme areoften referred to as Topic and Comment in other the-ories of information structure (Vallduvi, 1993).
The-ories also disagree about exactly where to draw theboundary between the two.In Halliday?s theory, the Rheme begins afterthe first full constituent with an experiential func-tion label, and extends to the end of the clause.The first constituent with an experiential functionis labeled Topical Theme.
Constituents before itare labeled either Interpersonal Theme or TextualTheme.
Auxiliaries and vocatives are labeled In-terpersonal Theme, while conjunctions are labeledTextual Theme.93System Null % Feature 1 Feature 2clause class 0% major (86%) minor (13%)agency 13% effective (52%) middle (34%)conjunction 13% non-conjuncted (64%) conjuncted (21%)finiteness 13% finite (67%) non-finite (19%)polarity 13% positive (81%) negative (4%)rank 13% ranking (66%) shifted (19%)secondary/beta clause 13% false (58%) true (28%)status 13% bound (45%) free (41%)deicticity 32% temporal (60%) modal (7%)person 32% non-interactant (54%) interactant (13%)theme selection 32% unmarked (58%) marked (9%)voice 47% active (45%) passive (6%)embed type 80% nominal qualifier (15%) other qualifier (3%)theme role 90% as adjunct (7%) as process (1%)passive agency 93% non-agentive (5%) agentive (1%)Table 2: Selected systems and how often their features are selected.5.3 System SelectionsAs discussed above, the system features are organ-ised into hierarchies, with every feature assuming anull value unless its system?s entry condition is met.We therefore approach the system network muchlike a decision tree, using rules to control how thenetwork is traversed.The rules used to traverse the network cannot beexplained here in full, as there are 41 such decisionfunctions currently implemented.
Table 2 lists a fewof the systems we implement, along with how of-ten their features are selected.
Because the systemnetwork is organised hierarchically, a selection willnot always be made from a given system, since the?entry condition?
may not be met.
For instance, thefeature agency=effective is an entry condition for thevoice system, so if a clause is middle, no voice willbe selected.
The Null % column describes how of-ten the entry condition of the clause is not met.
Sys-tems further down the heirarchy will obviously berelevant less often, as will systems which describe afiner grained distinction for an already rare feature.The following sections describe the system net-work in terms of four general regions.
The systemswithin each region largely sub-categorise each other,or relate to the same grammatical phenomenon.5.4 Mood systemsAssuming the clause is independent, the major moodoptions are declarative, interrogative and imperative.Deciding between these is quite simple: in interrog-ative clauses, the Subject occurs after the first auxil-iary.
Imperative clauses have no Subject.There are a few more granular systems for in-terrogative clauses, recording whether the questionis polar or WH.
If the clause is WH interrogative,there are two further features recording whether therequested constituent functions as Subject, Adjunctor Complement.
The values of these features arequite simple to calculate, by finding the WH elementamong the constituents and retrieving its interper-sonal function.If the clause is not imperative, there are systemsrecording the person (first, second or third) of thesubject, and whether the first auxiliary is modal,present tense, past tense, or future tense.
SFG de-scribes three tenses in English, regarding ?will?
and?shall?
auxiliaries as future tense markers, ratherthan modals.If the clause is imperative, there is a further sys-tem recording whether the clause is the ?jussive?
im-perative with ?let?s?, an ?oblative?
imperative with?let me?, or a second person imperative.
If the im-perative is second person, a further feature recordswhether the ?you?
is explicit or implied.There are also features recording the ?polarity?
ofthe clause: whether it is positive or negative, and, ifnegative, whether the negative marker is full-formedor cliticised as -n?t.5.5 Voice systemsIn the Nigel grammar, the first voice distinctiondrawn is not between active and passive, but be-tween transitive and intransitive clauses.
Intransitiveclauses cannot be passivised, as there is no Comple-ment to shift to Subject.
It therefore makes sense to94carve these off first.
If the clause is transitive, an-other system records whether it is active or passive.The rules to draw this distinction simply look at theverb phrase, checking whether the last auxiliary is aform of the verb ?be?
and the lexical verb has a pastparticiple part-of-speech tag.
Finally, a further sys-tem records whether passive clauses have an agentintroduced by ?by?.5.6 Theme systemsTheme systems record what occurs at the start of theclause.
Typically in English, the first major con-stituent will be the logical subject, and hence alsothe Topical Theme.
A system records whether this isor is not the case.
If the clause is finite and the log-ical subject is not the Topical Theme, the clause issaid to have a ?marked?
theme.
Verb phrase TopicalThemes are considered unmarked if the clause is im-perative.
A further system records whether the Top-ical Theme is the logical object (as in passivisation),or whether it is a fronted Adjunct.
Passive clausesmay have a fronted Adjunct, so does not necessar-ily have a logical object as Topical Theme.
Thereare two further systems recording whether the clausehas a Textual Theme and/or an Interpersonal Theme.5.7 Taxis systemsTaxis systems record dependency relationships be-tween clauses.
There are two types of information:whether the attachment is made through coordina-tion or subordination, and the semantic type of theattachment.
Broadly, semantic type is between ?ex-pansion?
and ?projection?, projection being reported(or quoted) speech or thought.
A further systemrecords the subtype of expansion clauses, which isquite a subtle distinction.
Unfortunately Hallidaychose thoroughly unhelpful terminology for this dis-tinction: his subtypes of expansion are elaboration,enhancement and extension.
Enhancing clauses areessentially adverbial, and are almost always subor-dinate.
Extending clauses, by contrast, are approxi-mately the ?and?
relationship, and are almost alwayscoordinate.
Elaborating clauses qualify or furtherdefine the information in the clause they are attachedto.
Elaborating clauses can be either subordinateor coordinate.
Subordinate elaborating clauses arenon-defining relative clauses, while coordinate elab-orating clauses are usually introduced by a conjunc-tive adjunct, like ?particularly?.6 AccuracyIn order to evaluate the accuracy of the conversionprocess, we manually evaluated the constituencystructure of a randomly selected sample of 200clauses.
The conversion heuristics were developedon section 00 of the Wall Street Journal and section 2of Switchboard, while the evaluation sentences weresampled from the rest of the Penn Treebank.We limited evaluation to the constituency conver-sion process, in order to examine more clauses.
Thefunction labels are calculated from the constituencyconversion, while the system features are calculatedfrom the function labels and other system features.Since the system network is like a decision tree,whether a feature is null-valued depends on priorfeature decisions.
These dependencies in the anno-tation mean that evaluating all of it involves some re-dundancy.
We therefore evaluated the constituencystructure, since it did not depend on any of the otherannotation, and the conversion heuristics involved incalculating it were more complicated than those forthe function labels and system features.In the 200 clause sample, we found three clauseswith faulty constituency structures.
One of thesewas the result of a part-of-speech tag error in theTreebank.
The other two errors were conjunctionsthat were incorrectly replicated in ellipsis clauses.7 ConclusionThe Penn Treebank was designed as a largely the-ory neutral corpus.
In deciding on an annotationscheme, it emphasised the need to have its annota-tors work quickly and consistent, rather than fidelityto any particular linguistic theory (Marcus et al,1994).The fact that it has been successfully converted toso many other annotation schemes suggests that itsannotation is indeed consistent and fine grained.
Itis therefore unsurprising that it is possible to con-vert it to SFG as well.
Despite historically differentconcerns, SFG still fundamentally agrees with othertheories about which constructions are syntacticallydistinct ?
it simply has an unorthodox strategy forrepresenting that variation, delegating more work to95feature structures and less work to the syntactic rep-resentation.Now that a sizable SFG corpus has been created,it can be put to use for linguistic and NLP research.Linguistically, we suggest that it would be interest-ing to use the corpus to explore some of the as-sertions in the literature that have until now beenuntestable.
For instance, Halliday and Matthiessen(2004) suggests that the motivation for passivisationis largely structural ?
what comes first in a clause isan important choice in English.
This implies that thecombination of passive voice and a fronted adjunctshould be unlikely.
There should be many such sim-ple queries that can shed interesting light on abstractclaims in the literature.Large annotated corpora are currently very impor-tant for parsing research, making this work a vitalfirst step towards exploring whether SFG annotationcan be automated.
The fact that Treebank parses canbe converted into SFG annotation suggests it can be,although most parsers do not replicate the dash-tagsattached to Treebank labels, which are necessary todistinguish SFG categories in our conversion system.Even without automating annotation, the corpusoffers some potential for investigating how usefulSFG features are for NLP tasks.
The Penn Treebankincludes texts from a variety of genres, includingnewspaper text, literature and spoken dialogue.
TheSwitchboard section of the corpus also comes withvarious demographic properties about the speakers,and is over a million words.
We therefore suggestthat gold standard SFG features could be used insome simple document classification experiments,such as predicting the gender or education level ofspeakers in the Switchboard corpus.8 AcknowledgmentsWe would like to thanks the anonymous review-ers for their helpful comments.
James Curran wasfunded under ARC Discovery grants DP0453131and DP0665973.ReferencesAnette Frank.
2001.
Treebank conversion: Converting the NE-GRA treebank to an LTAG grammar.
In Proceedings of theEUROLAN Workshop on Multi-layer Corpus-based Analy-sis.
Iasi, Romania.Anette Frank, Louisa Sadler, Josef van Genabith, and AndyWay.
2003.
From Treebank Resources To LFG F-Structures- Automatic F-Structure Annotation of Treebank Trees andCFGs extracted from Treebanks.
Kluwer, Dordrecht.Michael A. K. Halliday.
1969.
Options and functions in theEnglish clause.
Brno Studies in English, 8:82?88.
Reprintedin Halliday and Martin (eds.
)(1981) Readings in SystemicLinguistics, Batsford, London.Michael A. K. Halliday.
1970.
Language structure and languagefunction.
In John Lyons, editor, New Horizons in Linguistics.Penguin, Harmondsworth.Michael A. K. Halliday and Christian M. I. M. Matthiessen.2004.
An Introduction to Functional Grammar.
EdwardArnold, London, third edition.Julia Hockenmaier and Mark Steedman.
2005.
Ccgbank man-ual.
Technical Report MS-CIS-05-09, University of Penn-sylvania.Matthew Honnibal.
2004.
Converting the Penn Treebank toSystemic Sunctional Grammar.
In Proceedings of the Aus-tralasian Language Technology Workshop (ALTW04).William C. Mann and Christian M. I. M. Matthiessen.
1983.
Anoverview of the Nigel text generation grammar.
TechnicalReport RR-83-113, USC/Information Sciences Institute.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated corpus ofEnglish: The Penn Treebank.
Computational Linguistics,19(2):313?330.James R. Martin, Christian M. I. M. Matthiessen, and ClarePainter.
1997.
Working with Functional Grammar.
Arnold,London.Christian Matthiessen.
1995.
Lexicogrammatical Cartography.International Language Sciences Publishers, Tokyo, Taipeiand Dallas.Michael O?Donnell and John A. Bateman.
2005.
SFL in com-putational contexts: a contemporary history.
In J. Webster,R.
Hasan, and C. M. I. M. Matthiessen, editors, Continu-ing Discourse on Language: A functional perspective, pages343?382.
Equinox, London.Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005.
Theproposition bank: An annotated corpus of semantic roles.Computational Linguistics, 31(1):71?106.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and JanSvartvik.
1991.
A Grammar of Contemporary English.Longman, London.Enric Vallduvi.
1993.
Information packing: A survey.
TechnicalReport HCRC/RP-44, Universiy of Edinburgh.96
