Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 50?58,Dublin, Ireland, August 24 2014.Verbal Behaviors and Persuasiveness in Online Multimedia ContentMoitreya Chatterjee, Sunghyun Park*, Han Suk Shim*, Kenji Sagae and Louis-Philippe Morency USC Institute for Creative Technologies Los Angeles, CA 90094 metro.smiles@gmail.com,  { park, hshim, sagae, morency }@ict.usc.edu  Abstract Persuasive communication is an essential component of our daily lives, whether it is negotiat-ing, reviewing a product, or campaigning for the acceptance of a point of view.
With the rapid expansion of social media websites such as YouTube, Vimeo and ExpoTV, it is becoming ev-er more important and useful to understand persuasiveness in social multimedia content.
In this paper we present a novel analysis of verbal behavior, based on lexical usage and para-verbal markers of hesitation, in the context of predicting persuasiveness in online multimedia content.
Toward the end goal of predicting perceived persuasion, this work also explores the potential differences in verbal behavior of people expressing a positive opinion (e.g., a posi-tive movie review) versus a negative one.
The analysis is performed on a multimedia corpus of 1,000 movie review videos annotated for persuasiveness.
Our results show that verbal be-havior can be a significant predictor of persuasiveness in such online multimedia content.
1 Introduction A message that is ?intended to shape, reinforce or change the responses of another or others?
is cate-gorized as persuasive communication (Miller, 1980), and it is particularly important for the role it plays in creating social influence and altering other people?s opinions (Reardon, 1991; Zimbardo and Leippe, 1991).
For instance, a persuasive advertisement could be a potential profit churner.
The growth of social networking sites on the Internet has resulted in an explosion of online content with the purpose of delivering persuasive messages.
Websites such as YouTube, Vimeo and ExpoTV are examples of online media in which these messages propagate mainly in the form of videos.
ExpoTV, in particular, is a repository of a large number of videos dedicated for product reviews in which people try to convince others in favor of or against the use of various products.
This raises an interesting research problem as to what it is that makes certain speakers have a substantial impact on others?
opinions while other speakers are ignored.
In this paper, we present a novel analysis of spoken persuasion in online multimedia content.
Our work is motivated by prior research findings in psychology indicating that verbal behavior is a prom-ising indicator for persuasive communication (Chaiken and Eagly, 1979; Werner, 1982).
Such prior findings allow us to hypothesize that two primary types of verbal features will be predictive of per-suasion: lexical features and paraverbal markers of hesitation.
Additionally we explore the relation-ship of the sentiment of the content and perceived persuasion, by hypothesizing that speakers?
exhibit different verbal behavior when expressing a positive opinion versus a negative one and taking into account these differences will improve prediction performance.
We conduct several experiments in order to validate these hypotheses using a multimedia corpus of 1,000 movie review videos obtained from ExpoTV.com, which is a great source of online reviews.
Our experiments followed by a detailed analysis also reveal a set of predictive features which characterize persuasive online presentations.
In the following section, we present an overview of related work.
Section 3 elaborates on our re-* Both authors contributed equally to this work.
This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footer are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/50search hypotheses.
In Section 4, we present our multimedia corpus.
The details of the experiments with computational descriptors and methodology are described in Section 5.
We discuss the results and findings in Section 6, and finally we conclude our paper and present some future directions of research in Section 7.
2 Related Work Content in the form of written text are omnipresent in our society.
Starting from books, magazines and newspapers to the now prevalent emails and blog posts, text-based content are an invaluable component for effective communication.
Prior research reports possibly greater persuasiveness in written messages compared to visual or acoustic modalities in certain situations (Chaiken and Eagly, 1979; Werner, 1982).
Past research has also revealed that for sophisticated messages, such as those used in a martial setting, written messages are more persuasive (Chaiken and Eagly, 1979).
Although the importance of studying verbal behavior for determining persuasiveness has been un-derscored in prior work in the field of communication sciences (O?Keefe, 2002) and this line of re-search gives us useful pointers to the factors that contribute to persuasiveness in text or verbal com-munication, they present no computational aspect, which is where we put our emphasis in the paper.
In the field of natural language processing, text classification based on bag-of-words has been a long standing approach (Lewis and Gale, 1994; Mitchell, 1997; Dave et al., 2003).
In fact, Young et al.
(2011) have explored lexical features in the specific context of predicting persuasion, but they fo-cus their attention on studying persuasion in dialogue.
Our work draws inspiration from such ap-proaches but explores it in the specific context of predicting persuasiveness in online multimedia con-tent using lexical and paraverbal features.
3 Research Hypotheses Motivated by prior works and theoretical background, we designed our experiments to validate three hypotheses.
Since multiple prior works point to the usefulness of the text modality in persuasive communication and also to the power of text classification with lexical features in various tasks, we explored the fea-sibility of capturing the difference in verbal behavior between persuasive and unpersuasive expres-sions of opinions in online social multimedia content (specifically, movie reviews).
The following is the hypothesis that we specifically tested with our experiments: Hypothesis 1: Verbal behavior, as captured by lexical usage, is indicative of persuasiveness in online social multimedia content, irrespective of whether the opinion expressed is positive or negative.
Paraverbal behaviors indicative of hesitation can constitute important information for predicting persuasiveness.
For instance, a speaker?s stuttering or breaking his/her speech with filled pauses (such as um and uh) has influence on how other people perceive his/her persuasiveness.
Although previous work (DeVault et al, 2013) suggests paraverbal behavior may be indicative of depression, another work on emotion prediction however, (Devillers et al., 2006) raised questions about its predictive power when compared to using standard cues derived from lexical usage.
This leads us to our second hypothesis on paraverbal behaviors in the context of predicting persuasiveness: Hypothesis 2: Paraverbal behaviors related to hesitation are indicative of persuasiveness in online social multimedia content.
Past research highlights the importance of the knowledge of the affective state of a document towards its perceived persuasiveness (Murphy, 2001).
We therefore hypothesize the following: Hypothesis 3: Knowledge of the sentiment polarity of a movie review improves classification of the speaker?s perceived persuasiveness.
4 Dataset ExpoTV.com is a popular website housing videos of product reviews.
Each product review has a vid-eo of a speaker talking about a particular product as well as the speaker?s direct rating of the product on an integral scale from 1 star (for most negative review) to 5 stars (for most positive review).
This direct rating is useful for the purpose of our study because this allows us to study perceived persua-sion under different directions of persuasion (in favor of or against).
For instance, the speaker in a 5-51star movie review video would most likely try to persuade his/her audience in favor of watching the movie while the speaker in a 1-star movie review video would argue against watching the movie.
We therefore collected a total of 1,000 movie review videos that were either highly positive or negative.
The dataset consists of the following:  ?
Positive Reviews: 500 movie review videos with 5-star rating (315 males and 185 females).
?
Negative Reviews: 500 movie review videos with 1 or 2-star rating, consisting of 216 1-star videos (151 males and 65 females) and 284 2-star videos (212 males and 72 females).
We included 2-star videos due to the lack of enough 1-star videos on the website.
Each video in the corpus has a frontal view of one person talking about a particular movie, and the average length of the videos is about 94 seconds.
The corpus contains 372 unique speakers and 600 unique movie titles and is available to the community for purposes of academic research1.
4.1 Evaluation of Persuasiveness Amazon Mechanical Turk (AMT), which is a popular online crowdsourcing platform, was used to obtain subjective evaluation of each speaker?s perceived persuasiveness, following a similar annota-tion scheme as (Mohammadi et al., 2013).
For each video in the corpus, we obtained 3 repeated eval-uations on the level of persuasiveness of the speaker by asking the workers to give direct rating on each speaker?s persuasiveness on a Likert scale from 1 (very unpersuasive) to 7 (very persuasive).
A total of 50 native English-speaking workers based in the United States participated in the evaluation process online, and the task was evenly distributed among the 50 workers.
To minimize gender influ-ence, the task was distributed such that the workers only evaluated speakers of the same gender.
The correlation between the mean score of every movie and the individual ratings was found to be 0.7 on the average (Pearson?s Correlation Coefficient).
Once the evaluation was complete, we used the mean persuasiveness score for each video as the ground-truth measure of the speaker?s perceived persuasiveness.
In this initial effort, we focused on videos that were extremely persuasive or not persuasive at all.
Hence, videos with a mean score of equal to or greater than 5.5 were taken as persuasive while those with a mean score of equal to or less than 2.5 were taken as unpersuasive.
After this, we ended up with a total of 300 videos, specifically 157 videos of positive reviews (75 persuasive and 82 unpersuasive) and 143 videos of negative re-views (62 persuasive and 81 unpersuasive).
4.2 Transcriptions Using AMT and 18 participants from the same worker pool for persuasiveness evaluation, we ob-tained verbatim transcriptions of these filtered 300 videos, including transcriptions for filled pauses and stutters.
Each transcription was reviewed and edited by multiple in-house experienced transcribers for accuracy.
We do not use automatic speech recognition techniques in order to avoid noisy tran-scriptions.
5 Experiments In this section, we give details on the design of our computational descriptors followed by the experi-mental methodology.
5.1 Computational Descriptors In our experiments, our main focus was on devising computational descriptors for verbal behaviors in terms of lexical usage and also in terms of paraverbal markers of hesitation that can capture indica-tions of persuasiveness of the speaker.
Verbal (Lexical) Descriptors: As in many text classification tasks, we designed our verbal de-scriptors based on the bag-of-words representation using term frequency of both unigrams and bi-                                                1 Dataset available online: http://multicomp.ict.usc.edu/52grams.
Using the 300 filtered videos (see Section 4.1) and without feature selection, the numbers of unigrams reach around 4,500 and bigrams around 24,000.
We did not proceed further with higher or-der n-grams because empirical evidence has shown that trigrams and other higher order n-grams do not always show improvement because they introduce problems related to the sparsity of features (Dave et al., 2003).
Paraverbal Descriptors of Hesitation: From the verbatim transcriptions of our corpus, we observed a set of frequent paraverbal cues that could potentially be associated with the level of persuasiveness.
The set of descriptors is inspired from the findings of DeVault et al.
(2013), who explored a similar set of generic paraverbal features in an interactive dialogue setting.
However, we are interested specifically in the ones that capture signs of hesitation.
The following were the descriptors that were used:  ?
Pause-Fillers: The verbal behaviors of reviewers are often characterized with various pause-fillers, such as um or uh.
In order to account for the varying length of each review, we normalized the count of all instances of filled pauses by the number of words spoken in the video.
?
Disfluency Markers: A prominent marker of disfluency in human speech is stuttering.
To capture this disfluency, we counted all instances of stuttering in each video and normalized them by the number of words spoken in the video.
?
Articulation Rate: Articulation rate is defined as the rate of speaking in which all pauses are excluded from calculation (Dankovicova, 1999).
This descriptor was computed by taking the ratio of the number of spoken words in each video to the actual time spent speaking.
?
Mean Span of Silence: Human speech is often interspersed with pauses.
We therefore computed this descriptor, by measuring the total duration of silence during speech, normalized by the total length of the video.
5.2 Methodology We processed all the videos in our dataset and automatically extracted the indicated lexical and paraverbal features.
The extracted features were then used for several classification experiments under three different settings to test our hypotheses: only positive reviews, only negative reviews (called  the sentiment-dependent classifiers) and a combined set of positive and negative reviews (called  the sentiment-independent classifiers).
For each such setting, we divided the set of samples (transcription of movie reviews) into 5 balanced folds that were both speaker-independent and movie-independent.
In other words, in all our experiments, no 2 folds contained samples from the same speaker or movie title.
This was done to remove any form of bias in the classifier based on either the speaker or the movie.
We then performed classification experiments using 5-fold cross-validation using the lexical fea-tures (unigrams and bigrams) on this combined set of reviews (positive and negative reviews togeth-er), each time leaving 1 fold for hold-out testing.
Here, we note that for constructing the dictionary, only data from the training set was used.
On average across 5-fold cross-validation, the number of unigrams was around 4,560 and bigrams around 23,701 for the combined set of movie reviews.
However, since such a feature design typically suffers from problems arising out of the sparsity of the entries of the dictionary in the dataset, we employed a feature selection step.
For feature selection and analysis, we used Information Gain (IG), which is a measure of the number of bits of information obtained for category prediction by knowing the presence of a term in a document.
Prior evaluation of feature-selection methods for text classification has revealed the superiority of IG as a metric over other ones such as Mutual Information, Term Strength or a simple Document Frequency thresholding for document classification tasks (Yang and Pedersen, 1997).
This serves as an inspiring basis for us-ing IG as a metric for feature selection.
The gain score G(t) obtained from IG is a non-zero positive value for features that are strongly in-dicative of the extent of persuasiveness of the document, while ones that are not so informative have a value of 0.
We therefore select only those lexical features (unigrams and bigrams) which have an IG > 0 based on the distribution obtained from the training set.
This allows us to trim the dictionary signifi-cantly and use only meaningful features for classification.53This was then followed up by a 5-fold cross-validation using only the paraverbal features (no fea-ture selection was used here since they were too few in number).
The accuracy of classification based on paraverbal features was then compared with that obtained by classification using only the lexical descriptors and by a majority baseline classifier.
Furthermore, we also tried an early-fusion approach, where we simply use both lexical and para-verbal features together.
Such an approach to fusion seemed more promising here than a decision-level fusion approach because of the few categories of features used (just lexical and paralinguistic, as motivated by the findings of (Gunes and Piccardi, 2005)).
5.3  Classification Model For performing classification experiments we used the Na?ve Bayes classifier.
A well-known issue with using the Na?ve Bayes classifier is its incapability of handling new features, which is handled by performing a conditional uniform smoothing (Puurula, 2012).
6 Results and Discussion Table 1 shows the results for our classification experiments, which confirm the predictive power of lexical features.
Hypothesis 1: The lexical features (unigrams and bigrams) are predictive of persuasiveness.
This is manifested by the fact that they perform significantly better than a majority baseline, which is only 51.04% accurate on the combined set of positive and negative reviews, while the lexical features achieved an accuracy of around 77% (Figure 1).
Considering the positive and the negative reviews individually, we note that the lexical features were accurate for nearly 82% of the test samples for the positive reviews and for 86% of the test samples for the negative reviews, again outperforming a sim-ple majority baseline classifier (Table 1).
An analysis of the features (Table 2) reveals that certain lexical features contribute to the predicta-bility of the persuasiveness of a speaker.
The presence of unigrams such as character or make or bi-grams such as to make or this movie for instance, contributes to the predictability of persuasiveness of the speaker, even though they are not emotionally salient terms.
The high IG scores of such features irrespective of the setting we conduct our experiments in (positive reviews only, negative reviews on-ly or a combined set of positive and negative reviews), highlights their importance.
Moreover, a (+) sign for most of these unigrams or bigrams show that their presence contributes favorably to the speaker being perceived as persuasive.
On the other hand a (-) sign for an informative bigram such as it says is indicative of lack of speaker?s persuasiveness.
This can be explained by the context of the usage of such features.
For instance, the bigram it says in it says that the movie duration is?
is a bi-Feature Group Sentiment Dependent Classifier Sentiment Independent Classifier Mean Positive Reviews Negative Reviews Lexical Features (Unigrams and Bigrams) 83.92% 81.74% 86.09% 76.73% ?
Unigrams Only 77.70% 74.78% 80.62% 73.77% ?
Bigrams  Only 84.05% 81.64% 86.46% 75.81% Para-Linguistic Features 64.23% 65.22% 63.23% 63.04% Early Fusion 84.54% 82.61% 86.46% 78.56% Majority Baseline 52.14% 50.43% 53.85% 51.09%  Table 1: Accuracies for our experiments using a Na?ve Bayes classifier.
The scores in bold indi-cate the dominance of the sentiment-dependent classifier under all circumstances.54gram that is uttered by the reviewers when they refer to the DVD cover of the movie to give some more detailed information about it.
This is identified as a sign of an unpersuasive reviewer.
Such re-sults confirm that the verbal behaviors, as captured by lexical usage, are extremely predictive of per-suasiveness irrespective of whether the opinion expressed is positive or negative, which validates Hy-pothesis 1.
Hypothesis 2: Moreover, our experiments show that while the designed paraverbal features that are markers of hesitation can classify only about 63% of the speakers correctly (see Table 1), howeverFigure 1: Bar graph visualization of the classification accuracies using different types of fea-tures on the combined set of reviews (i.e.
sentiment-independent classifier).
** indicates 2-samples t-test results with p < 0.01 and *** indicates p < 0.001.
The error bars show 1 SD.
Feature Positive Reviews Negative Reviews Both Combined Word IG Score Word IG Score Word IG ScoreUnigramsThe (+) 0.1183 Even (+) 0.11 Make  (+) 0.1117 Make  (+) 0.0816 Make  (-) 0.1082 Just  (+) 0.0728 Everything  (+) 0.0806 Movie  (+) 0.0969 Very  (+) 0.0669 Just  (+) 0.0806 Real  (+) 0.0873 Character  (+) 0.0573 Dollars (+) 0.0722 Not  (+) 0.0867 Becomes  (+) 0.0558 Character  (+) 0.0685 Big  (+) 0.0858 Even (+) 0.0524 Can  (+) 0.0685 One  (+) 0.0817 One  (+) 0.051 Product  (+) 0.0685 Avoid  (+) 0.079 Yourself  (+) 0.05 Famous (+) 0.0609 Feel  (+) 0.079 You  (+) 0.04571 Enjoy  (+) 0.0566 Character  (+) 0.0773 Lot  (+) 0.0456BigramsThere are  (+) 0.1183 This movie  (+) 0.1083 To make  (+) 0.0905 This movie  (+) 0.0816 Do not  (+) 0.1032 A lot  (+) 0.0617 I can?t  (+) 0.0806 I think  (+) 0.1032 This movie  (+) 0.0578 To make  (+) 0.0806 To make  (+) 0.0989 Lot of  (+) 0.0443 Good movie  (+) 0.0722 Not even  (+) 0.091 It says (-) 0.0417 Buy it  (+) 0.0685 Don?t even  (+) 0.091 You will (-) 0.0417 Really a  (+) 0.0685 The story  (+) 0.079 Twenty dollars (+) 0.0368 Definitely one  (+) 0.0685 The film  (+) 0.0672 The character (+) 0.0386 Best movies  (+) 0.0609 At all  (+) 0.0672 So many (+) 0.033 It?s awesome  (+) 0.0566 It?s so (+) 0.0672 See it (+) 0.033  Table 2: Important unigrams and bigrams when they are used individually as lexical features.
(+) indicates that it increases persuasiveness while (-) indicates it contributes to the lack of per-suasiveness.55they are statistically significant features, in terms of their p-values (Figure 2).
While classification performance is lower than that obtained with purely lexical features, it is still far above a majority baseline, and thus confirms our second hypothesis.
Additionally, it is interesting to note from Table 1 that, although a feature-level fusion of the lexical features and paraverbal features gives us an improvement in classification performance, the difference between the results obtained with fusion and those with lexical features alone was minor and was not statistically significant.
Hypothesis 3: We also observe that a sentiment-dependent classifier trained individually on positive reviews or on negative reviews outperforms one that is trained on a combined set of reviews.
This is supported by our empirical results in Table 1 which show that when classification is performed with any of the lexical features, the accuracies are significantly higher for the classifier trained only on the positive or only on the negative reviews (sentiment-dependent classifiers) than for the classifier trained on the combined set of reviews (sentiment-independent classifiers).
For instance, when unigrams and bigrams were both used as our lexical features, we observed that for a sentiment-dependent classifier the classification accuracy jumps to over 84% on average.
This is significantly better than the scenario where the classifier is not aware of the sentiment of the review.
Figure 3 demonstrates this phenomenon.
We resort to feature analysis for an explanation of such an observation (Table 2).
The analysis re-veals that certain sentiment-based lexical features, i.e.
emotionally salient terms, assume an important role in magnifying the discriminative power of language use in persuasiveness prediction, when prior knowledge about the speaker?s opinion is known.
For instance, in the case of a classifier trained only on the positive reviews, unigrams such as enjoy and famous and bigrams such as good movie or it?s awesome become significant.
In the context of persuading against watching the movie prominent sen-timent-based unigrams are not and avoid while bigrams are do not, don?t even and at all.
This pro-vides empirical support for our third hypothesis.
7 Conclusion and Future Work This work presents several interesting findings about perceived persuasiveness prediction in online social multimedia content by analyzing the verbal behavior of the speaker, modeled using lexical fea-tures and paraverbal features of hesitation.
We conducted experiments and showed that verbal behav-ior as captured by lexical descriptors is a strong indicator of persuasiveness, irrespective of whether we persuade in favor of or against something.
Much of this is due to the presence of certain unigrams and bigrams that are either indicative of strong persuasiveness or of lack of persuasiveness.
Our ex-periments further reveal the superiority of classifying with lexical features as compared to with para-Figure 2: Boxplots for the paralinguistic hesitation markers for a classifier trained on the para-linguistic features only.
* and *** indicate p <= 0.05 and 0.001, respectively.56verbal features alone.
Moreover we empirically validate the hypothesis that a sentiment-aware classi-fier outperforms a sentiment-independent one.
As future work, we intend to explore more paraverbal features for persuasiveness prediction and also try more sophisticated prediction models which explic-itly model the temporal dynamic.
Acknowledgments This work was supported by the National Science Foundation under Grant IIS-1118018 and the U.S. Army.
The content does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.
References Shelly Chaiken and Alice H. Eagly.
1979.
Communication modality as a determinant of message persuasiveness and message comprehensibility.
Journal of Personality and Social Psychology, 37:1387-1397.
Jana Dankovicova.
1999.
Articulation rate variation within the intonation phrase in Czech and English.
14th Int.
Congress of Phonetic Sciences, San Francisco, Vol.
1, pp.
269-272.
Kushal Dave, Steve Lawrence, and David M. Pennck.
Mining the Peanut Gallery: Opinion Extraction and semantic Classification of Product Reviews, 2003.
2003 Association for Computational Linguistics (ACL ?03).
David DeVault, Kallirroi Georgila, Ron Artstein, Fabrizio Morbini, David Traum, Stefan, Scherer, Albert (Skip) Rizzo, and Louis-Philippe Morency.
2013.
Verbal indicators of psychological distress in interactive dialogue with a virtual human.
SIGDIAL 2013 Conf, 2013 Association for Computational Linguistics (ACL ?13).
Laurence Devillers and Laurence Vidrascu.
2006.
Real-life emotions detection with lexical and paralinguistric cues on Human-Human call center dialogs.
Interspeech 2006.
Hatice Gunes, and Massimo Piccardi.
2005.
Affect Recognition from face and body: Early fusion vs. Late fusion.
IEEE Int?l Conf.
on Systems, Man and Cybernnetic.
Daniel  J. O?Keefe.
2002.
Persuasion: Theory and research.
(2nd Edition).
Sage Publications, Thousand Oaks, CA.
David D. Lewis and William A. Gale.
1994.
A Sequential Algorithm for Training Text Classifiers.
Special Interest Group in Information Retieval (SIGIR?94 ).
Gerald R. Miller (1980).
On being persuaded: Some basic distinctions.
In M. Roloff, & G. R. Miller (Eds.
), Persuasion: New directions in theory and research, 11?28.
Beverly Hills, CA: Sage.Figure 3: Bar graph visualization of the classification accuracies of lexical features using a senti-ment-dependent classifier (mean) and a sentiment independent one.
** indicates 2-sample t-test results with p < 0.01 and the error bars show 1 SD.57Tom M. Mitchell.
1997.
Machine Learning.
McGraw-Hill.
Gelareh Mohammadi, Sunghyun Park, Kenji Sagae, Alessandro Vinciarelli, and Lois-Phillippe Morency.
2013. Who is persuasive?
The role of perceived personality and Communication modality in social multimedia.
Int?l Conf.
on Multimodal Interfaces (ICMI ?13).
P. Karen Murphy.
2001.
What makes a text persuasive?
Comparing students?
and experts?
conceptions of persuasiveness.
Int?l Journal of Education Research, 35 (2001) 675-698.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002.
Thumbs up?
Sentiment Classification using Machine Learning Techniques.
Conf.
on Empirical Methods in Natural Language Processing.
(EMNLP ?02).
Antti Puurula.
2012.
Combining Modifications to Multinomial Naive Bayes for Text Classification.
Springer, LNCS.
Kathleen Kelley Reardon.
1991.
Persuasion in practice.
Sage Publication, Inc. Carol Werner.
1982.
Intrusiveness and persuasive impact of three communication media.
Journal of Applied Social Psychology, 89:155-181.
Yiming Yang and Jan O. Pedersen.
1997.
A comparative study on feature selection in text categorization.
Int?l Conf.
on Machine Learning (ICML ?97).
Joel Young, Craig Martell, Pranav Anand, Pedro Ortiz and Henry T. Gilbert IV.
2011.
A Microtext Corpus for Persuasion Detection in Dialog.
Analyzing Microtext: AAAI Workshop (AAAI-Workshop ?11).
Phillip G. Zimbardo and Michael R. Leippe.
1991.
The psychology of attitude change and social influence.
McGrew-Hill New York.58
