Named Entity Chunking Techniquesin Supervised Learning for Japanese Named Entity RecognitionManabu SassanoFujitsu Laboratories, Ltd.4-4-1, Kamikodanaka, Nakahara-ku,Kawasaki 211-8588, Japansassano(@ilab.fujitsu.
(:o.j I)Takeh i to  Utsurol )el)artment of Intbrlnationand Computer  Sciences,Toyohashi University of Technologylcnl)aku-cho, ~l~)yohashi 441-8580, Jat)anutsm'og))ics, rut.
ac.j pAbst rac tThis 1)aper focuses on the issue of named entitychunking in Japanese named entity recognition.We apply the SUl)ervised decision list lean>ing method to Japanese named entity recogni-tion.
We also investigate and in(:ori)orate sev-eral named-entity noun phrase chunking tech.-niques and experimentally evaluate and con>t)are their l)erfornlanee, ill addition, we t)rot)osea method for incorporating richer (:ontextua\]ilflbrmation as well as I)atterns of constituentmorphenms within a named entity, which h~venot 1)een considered ill previous research, andshow that the t)roi)osed method outt)erfi)rmsthese t)revious ai)proa('hes.1 I n t roduct ionIt is widely a.greed that named entity recog-nition is an imt)ort;ant ste t) ti)r various al)pli-(:ations of natural language 1)ro(:('.ssing such asintbnnation retrieval, maclfine translation, in-tbrmation extraction and natural language un-derstanding.
In tile English language, thetask of named entity recognition is one of thetasks of the Message Understanding Confer-once (MUC) (e..g., MUC-7 (19!
)8)) and hasbe.on studied intensively.
In the .
}al)anese lan-guage~ several recent conferences, uch as MET(Multilingual Entity Task, MET-I (Maiorano,1996) and MET-2 (MUC, 1998)) and IREX (In-formation l{etriew~l and Extraction Exercise)Workshop (IREX Committee, 1999), focused onnamed entity recognition ms one of their con-test tasks, thus promoting research on Jat)anesenamed entity recognition.In Japanese named entity recognition, it isquite common to apply morphological analy-sis as a t)reprocessing step and to segmentthe sentence string into a sequence of mor-i)henles.
Then, hand-crafted t)attern m~tchingrules and/or statistical named entity recognizerare apt)lied to recognize named entities.
It isofl;en the case that named entities to be rec-ognized have different segmentation boundariesfrom those of morpheums obtained by the mor-phological analysis.
For example, in our anal-ysis of the \]Ill,F,X workshop's training corpus ofllallled entities, about half of the mtmed enti-ties have segmentation boundaries that al'e dif-ferellt \]'rein the result of morphological nalysist)y a .\]al)anese lnorphological nalyzer BI~EAK-FAST (Sassano et al, 1997) (section 2).
Thus, in.Japanese named entity recognition: among themost difficult problems is how to recognize suchnamed entities that have segmentation bound-ary mismatch against he morphemes ot)tainedl)y morphological nalysis.
Furthermore, in al-most 90% of (:ases of those segmentation t)oulld-ary mismatches, named entities to l)e recognizedcan t)e (teconq)osed into several mort)heroes astheir constituents.
This means that the 1)roblemof recognizing named entities in those cases canbe solved by incorporating techniques of basenoun phrase chunking (Ramshaw and Marcus,1995).In this paper, we tbcus on the issue of namedentity chunking in Japanese name.d entity recog-nition.
First, we take a supervised learning ap-proach rather than a hand-crafted rule basedapproach, because the tbnner is nlore promis-ing than the latter with respect o the amomltof  human labor if requires, as well as its adaI)t-abi l i ty to a new domain  or a new def init ion ofnamed entities.
In general, creating trainingdata tbr supervised learning is somewhat easierthan creating pattern matching rules by hand.Next, we apply Yarowsky's method tbr super-vised decision list learning I (Yarowsky, 1994) to1VVe choose tile decision list learning method as the705Table 1: Statistics of NE Types of IREXNE TypeORGANIZATIONPERSONLOCATIONARTIFACTDATETIMEMONEYPERCENTTotalfrequency (%)Training3676 (19.7)3840 (20.6)5463 (29.2)747 (4.0)3567 (19.1)502 (2.7)390 (2.1)492 (2.6)18677Test361 (23.9)338 (22.4)413 (27.4)48 (3.2)260 (17.2)54 (3.5)15 (1.0)21 (1.4)1510Japanese named entity recognition, into whichwe incorporate several noun phrase chunkingtechniques (sections 3 and 4) and experimen-tally evaluate their performance on the IREX, workshop's training and test data (section 5).As one of those noun phrase chunking tech-niques, we propose a method for incorporatingricher contextual information as well as patternsof constituent morphemes within a named en-tity, compared with those considered in tire pre-vious research (Sekine et al, 1998; Borthwick,1999), and show that the proposed method out-perlbrms these approaches.2 Japanese Named Ent i tyRecogn i t ion2.1 Task of the IREX WorkshopThe task of named entity recognition of theIREX workshop is to recognize ight named en-tity types in Table 1 (IREX Conmfittee, 1999).The organizer of the IREX workshop provided1,174 newspaper articles which include 18,677named entities as tire training data.
In the for-mal run (general domain) of the workshop, theparticipating systems were requested to recog-nize 1,510 nanmd entities included in the held-out 71 newspaper articles.2.2 Segmentation Boundaries ofMorphemes and Named EntitiesIn the work presented here, we compare the seg-mentation boundaries of named entities in tireIREX workshop's training corpus with those ofsupervised learning technique mainly because it is easyto implement and quite straightibrward toextend a su-pervised lem'ning version to a milfimally supervised ver-sion (Collins and Singer, 1999; Cucerzan and Yarowsky,1999).
We also reported in (Utsuro and Sassano, 2000)the experimental results of a minimally supervised ver-sion of Japanese named entity recognition.Table 2: Statistics of Boundary Match vs. Mis-lnatch of Morphemes (M) attd Named Entities(NE)Match/Misnmtch II freq.
of NE Tags (%)1 M to 1 NE 10480 (56.1)n(> 2) Msto1 NEn=2n=3n > 44557 (24.4)1658 (8.9) 717596o (5.1) (38.4)other boundary mismatch 1022 (5.5)Total J\[ 18677morphemes which were obtained through mor-phological analysis by a Japanese morphologi-cal attalyzer BREAKFAST (Sassano et al, 1997).
2Detailed statistics of the comparison are pro-vided in 'Fable 2.
Nearly half of the namedentities have bmmdary mismatches against hemorI)hemes and also almost 90% of the namedentities with boundary mismatches can be tie-composed into more than one morpheme.
Fig--ure 1 shows some examples of such cases, a3 Chunk ing  and Tagging NamedEnt i t iesIn this section, we formalize the problem ofnamed entity chunking in Japanese named en-tity recognition.
We describe ~t novel tech-nique as well as those proposed in the previousworks on nan ted entity recognition.
The noveltechnique incorporates richer contextual infor-mation as well as p~tterns of constituent mor-phemes within ~ named entity, compared withthe techniques proposed in previous research onnamed entity recognition and base noun phrasechunking.3.1 Task DefinitionFirst, we will provide out" definition of the taskof Japanese named entity chunking.
Suppose'~The set of part-of-speech tags of lllU.~AKFAST consistsof about 300 tags.
mmAKFaST achieves 99.6% part-of-speech accuracy against newspaper a ticles.aIn most cases of the "other boundary mismatch" inTable 2, one or more named entities have to be rec-ognized as a part of a correctly analyzed morphemeand those cases are not caused by errors of morpholog-ical analysis.
One frequent example of this type is aJapanese verbal noun "hou-bei (visiting United States)"which consists of two characters "hou (visitin.q)" and "bet(United States)", where "bet (United States)" has to berecognized as <LOCATION>.
\Ve believe that 1)ouudarymismatches ofthis type can be easily solved by employ-ink a supervised learning technique such as the decisionlist learning method.706'Dfl)le 3: Exmoding Schemes of Named Entity Chunldng StatesNamed Entity TagMort)heine SequenceIllside/()utside EncodingStmt/End Encoding<0RG> <LOC> <L0C>- .
.
M I M M \] M0 0RG_I 0 L0C_I LOC_I LOC_I LOC_B 00 0RG_U 0 LOC_S L0C_C LOC_E LOC_U 0V Mo i - l )hemes  to 1 Named Ent i ty \ ]<ORGANIZATION>.... Roshia gun -..( S<,s,~i,.
0 ( a,',,.j)<PERSON>.... Murayama IbIni ichi shushOUprimc \]""(last nmne) (first name) ( minister"\[3 Morphemes to1 Named Entity\]<TIME>gozen ku .ii " ?
(AM) (niuc) (o ?1ock)<ARTIFACT>hokubei .jiyuu-1)oueki kyoutei - - ?Norfl~( America ) (flee trade.)
(treaty)Figure 1: Ex~mq)les of B(mndary Mismatch ofMorl)hemes mid Named Entitiesthat a sequen('e of morl)hemes i given as 1)e-low:Left; l{,ight( Context ) (Named Entity) ( Context; )?
.
.~ I ' _ '~ .
.
.~ IL  ~, i~ '~.
.
.M/ ' .
.
.~ , l / ,  ''~ ~1~".
.
.~, f / " .
.
.t(Current Position)Then, given tht~t the current t)osition is atthe morpheme M .N1': the task of tanned elltityeh l l l l k i l lg  is to  ass ign  a, C\] luuki l lg  s ta te  (to })e de-scribed in Section 3.2) as well ~rs a nmned entitytype to the morl)helne Mi NE at tim current po-sition, considering the patterns of surroundingmorl)hemes.
Note that in the SUl)ervised learn-ing phase we can use the (:lmnking iuibnnationon which morphemes constitute a ngune(l entity,and whi(-h morphemes are in the lefl;/right con-texts of tit(; named entity.3.2 Encoding Schemes of NamedEnt i ty  Chunking StatesIn this t)at)er, we evalu~te the following twos('hemes of encoding ctmnking states of nalnedentities.
EXalnples of these encoding s(:hemesare shown in Table 3.3.2.1 Ins ide/Outs ide  EncodingThe Inside/Outside scheme of encoding chunk-ing states of base noun phrases was studied inIbmlshaw and Marcus (1995).
This scheme dis-tinguishes the tbllowing three states: 0 theword at the current position is outside any baseholm phrase.
I the word at the current po-sition is inside some base holm phrase.
B theword at the current position marks the begin-ning of ~ base noml t)hrase that immediately foplows another base noun phrase.
We extend thisscheme to named entity chunking by further dis-tinguishing each of the states I and B into eightnamed entity types.
4 Thus, this scheme distin-guishes 2 x 8 + 1 = 17 states.3.2.2 S tar t /End  EncodingThe Start /End scheme of encoding clmnkingstates of nmned entities was employed in Sekinee,t al.
(1998) and Borthwick (1999).
Thisscheme distinguishes the, following four statesfor each named entity type: S the lllOlTt)\]lellleat the (:urreld; position nmrks the l)eginldng of alUl.in(xt (;lltity consisting of more than one mor-1)\]mme.
C l;he lnOrl)heme ~I; the cm'r(mt )osi -tion marks the middle of a mmmd entity (:onsist-ing of more tlmn one lilOrt)hellle.
E -- the illOft)heme, at the current position ram:ks the endingof a n~mmd entity consisting of more than onemorl)heme.
U - the morpheme at the currentt)osition is a named entity consisting of only one,mort)heine.
The scheme ;dso considers one ad(li-tional state for the position outside any namedentity: 0 t;he mort)heine at the current posi-tion is outside any named entity.
Thus, in oursetting, this scheme distinguishes 4 x 8 + 1 = 33states.a.3 Preced ing /Subsequent  Morphemesas Contextua l  CluesIn this l)aper, we ewfluate the following twol l l ode ls  of  considering preceding/subsequent4\Ve allow the, state :c_B for a named entity tyt)e xonly when the, morl)hcme at t, he current 1)osition marksthe 1)egimdng ofa named entity of the type a" that im-mediately follows a nmned entity of the same type x.707morphemes as contextual clues to named entityclmnking/tagging.
Here we provide a basic out-line of these models, and the details of how toincorporate them into the decision list learningframework will be described in Section 4.2.2.3.a.1 3-gram ModelIn this paper, we refer to the model used inSekine et al (1998) and Borthwick (1999) as a3-gram model.
Suppose that the current posi-tion is at the morpheme M0, as illustrated be-low.
Then, when assigning a chunking state aswell as a named entity type to the morphemeM0, the 3-gram model considers the precedingsingle morpheme M-1 as well as the subsequentsingle morpheme M1 as the contextual clue.Left Current Right( Context ) ( Position ) ( Context )?
.
.
M0 M, .
.
.
(1)The major disadvantage of the 3-gram modelis that in the training phase it does nottake into account whether or not the l)re-ceding/subsequent morphemes constitute onenamed entity together with the mort)heine atthe current position.a.a.2 Variable Length ModelIn order to overcome this disadvantage of the 3-gram model, we propose a novel model, namelythe "Variable Length Model", which incorpo-rates richer contextual intbrmation as well aspatterns of constituent morl)hemes within anamed entity.
In principle, as part of the train-ing phase this model considers which of the pre-ceding/subsequent morphenms constitute onenamed entity together with the morpheme atthe current position.
It also considers sev-eral morphemes in the lefl;/right contexts of thenamed entity.
Here we restrict this model to ex-plicitly considering the cases of named entitiesof the length up to three morphenms and onlyimplicitly considering those longer than threemorphemes.
We also restrict it to consideringtwo morphemes in both left and right contextsof the named entity.Left( Context )... ML2MI_'Ill,ight(Named Entity) ( Context )M# .
.
.
... Mm(<3 )1" (2)(Current Position)4 Superv ised Learning for JapaneseNamed Ent i ty  Recogn i t ionThis section describes how to apply tile deci-sion list learning method to chunking/taggingnamed entities.4.1 Decision List LearningA decision list (Rivest, 1987; Yarowsky, 1994)is a sorted list of decision rules, each of whichdecides the wflue of a decision D given some ev-idence E. Each decision rule in a decision list issorted in descending order with respect o somepreference value, and rules with higher prefer-ence values are applied first when applying thedecision list to some new test; data.First, the random variable D representing adecision w, ries over several possible values, andthe random w~riable E representing some evi-dence varies over '1' and '0' (where '1' denotesthe presence of the corresponding piece of evi-dence, '0' its absence).
Then, given some train-ing data in which the correct value of the deci-sion D is annotated to each instance, the con-ditional probabilities P(D = x I E = 1) of ob-serving the decision D = x under the conditionof the presence of the evidence E (E = 1) arecalculated and the decision list is constructedby the tbllowing procedure.1.
For each piece of evidence, we calculate theIw of likelihood ratio of the largest; condi-tional probability of the decision D = :rl(given the presence of that piece of ev-idence) to the second largest conditionalprobability of the decision D =x2:I E=I)l?g2 P(D=x2 I E=I )Then~ a decision list is constructed withpieces of evidence sorted in descending or-der with respect to their log of likelihoodratios, where the decision of the rule at eachline is D = xl with the largest conditionalprobabil i ty)'~Yarowsky (1994) discusses everal techniques foravoiding the problems which arise when an observedcount is 0. lq-om among those techniques, we employtlm simplest ram, i.e., adding a small constant c~ (0.1 << 0.25) to the numerator and denominator.
Withthis inodification, more frcquent evidence is preferredwhen several evidence candidates exist with the same7082.
The final line of a decision list; ix defined as% default', where the log of likelihood ratiois calculated D<)m the ratio of the largest;marginal )robability of the decision D = x tto the second largest marginal l)rol)at)ilityof the decision D =x2:P(D =:/11)log~ p (D = x'2)The 'default' decision of this final line isD = Xl with the largest lnarginal probabil-ity.4.2 Decision List Learning forChunking/Tagging Named Entities4.2.1 DecisionFor each of the two schemes of enco(li1~g chunk-ing states of nalned entities descrit)ed in Sec-tion 3.2, as the l)ossible values of the <teei-sion D, we consider exactly the same categoriesof chunking states as those described in Sec-tion 3.2.4.2.2 EvidenceThe evidence E used in the decision list learn-ing is a combination of the tbatures of preced-ing/subsequent inorphemes as well as the mor-pheme at; the current position.
The followingdescribes how to form the evidence E fi)r 1)oththe a-gram nlodel and varial)le length model.3-,gram ModelThe evidence E ret)resents a tut)le (F - l ,  F0, F1 ),where F-1 and F1 denote the features of imme-diately t)receding/subsequent morphemes M_~and M1, respectively, F0 the featm:e of the mor-pheme 54o at the current position (see Fonnuta(1) in Section 3.3.1).
The definition of the pos-sible values of those tbatures F_l ,  F0, and 1'~are given below, where Mi denotes the roo f1)\]mnm itself (i.e., including its lexicM tbrm aswell as part-of-sl)eech), C,i the character type(i.e., JaI)anese (hiragana or katakana), Chinese(kanji), numbers, English alphabets, symbols,and all possible combinations of these) of Mi,Ti the part-of-st)eech of Mi:F_  1 ::m_ \]~//--1 I (C -1 ,  T - l )  I T - t  Inu l lmlsmoothed conditional probability P(D = x \[ E = 1).Yarowsky's training Mgoritl,m also ditfcrs omewhat inhis use of the ratio *'(~D=,d*~-j)' which is equivalent inthe case of binary classifications, and also by the interpo-lation between the global probalfilities (used here) andtl,e residual prol)abilities further conditional on higher-ranked patterns failing to match in the list.17'1 ::--~ \]~/-/1 I (C , ,V ; ) I  T* Inu l \ ]F0 ::- M0 I(C0,T0) lT0As the evidence E, we consider each possiblecoml)ination of the values of those three f'ea-lures.Variable Length ModelThe evidence E rel>resents a tuple(FL,FNu, FIt), where FL and Fl~ denotethe features of the morphemes ML_2ML1 andMff'M~ ~ in the left/right contexts of the currentnamed entity, respectively, FNE the featuresof the morphemes MN~""  " MNE " "" MNEm(_<3)constituting the current named entity (seeFormula (2) in Section 3.3.2).
The definition ofthe possible values of those features 1 L, FNI,:,and FI~ arc given below, where F NI~ denotesthe feature of the j - th constituent morphemeM .NJ~ within the current nalne(1 entity, and ak/l NI~ is the morl)heme at the cm'ren~ i)osition:FL ::= M*_'2M~ ~ \ [M~ InullFu ::= M\ ]~M~IM~Inu l lFNE FNEFNE IFNE r.NI'2 7z~NE FNE : := i i+1 i+2 \[ * i-1 * i * i+1\] I~NI'A~NI'21pNE 17NE~NI,2?
, FNE , (3)~NIC MN~c (Cm,: T~VJ~ , ,NI,:As the evidence E, we consider each possit)le(:oml)ination of the wfiues of those three fba-tures, except that the tbllowing three restric-tions are applied.1.
In the cases where the current named en-tity consists of up to three mort)heroes , asthe possible values of the feature FNIi inthe definition (3), we consider only thosewhich are consistent with the requirementthat each nlort)heme M NE is a constituentof the cun'ent named entity.
For exainple,suppose that the cun'ent named entity con-sists of three morphemes, where the cur-rent position is at the middle of those con-stituent morphemes as below:Left Right( Context ) (Named Entity) ( Context )I. L M1N~'M N+~M~u I~ t~ ? "
M1 Mi -'- _/l//_ 2/~//_ 11" (4)(Current Position)Then, as the possible values of the featureFN\],;, we consider only the tbllowing ibm':rN .
::= \[ F.g U.g7092.
II1 the cases where the eurrellt ilalned entityconsists of more than three morphemes,only the three constituent morphemes areregarded as within the current named en-tity and the rest are treated as if theywere outside the named entity.
For exam-pie, suppose that the current named en-tity consists of four morphemes as below:Left Right( Context ) (Named Entity) ( Context )L L$(Current Position)Iit this case, the fourth constitnent mor-pheme M N1c is treated as if it were in theright context of the current named entityas below:Left Right( Context ) (Named Entity) ( Context )'.,~ 1.
~r.,vJ,:~,Nu ~,.,,'.r,_,C M~ZMfft(Curren~ Position)3.
As the evidence E, among the possiblecombination of the values of three t'ea-tures /~,, ENId, and F/t, we only acceptthose in which the positions of the mor-phemes are continuous, and reject thosediscontimmus combinations.
For example,in the case of Formula (4:) above, as theevidence E, we accel)t the combination(Mq,  M 'My , ull), while we r( iect(ML1, M~EM~ 1':, 1,ull).4.3 Procedures  for Training andTestingNext we will briefly describe the entire pro-cesses of learning the decision list tbr etmnk-ing/tagging named entities as well as applyingit to chunking/tagging unseen named entities.4.3.1 TrainingIn the training phase, at the positions wherethe corresponding morpheme is a constitnent ofa named entity, as described in Section 4.2, eachal lowable combination of features is consideredas the evidence E. On the other hand, at thepositions where the corresponding morpheme isoutside any named entity, the way the combi-nation of t~at;ures i  considered is diflbrent inthe variable length model, in that the exception\].
in the previous section is no longer applied.Theretbre, all the possible wflues of the featureFNB in Definition (3) are accepted.
Finally, thefrequency of each decision D and evidence E iscounted and the decision list is learned as de-scribed in Section 4.1.4.3.2 TestingWhen applying the decision list to chunk-ing/tagging nnseen amed entities, first, at eachmorpheme position, the combination of featuresis considered as in the case of the non-entity po-sition in the training phase.
Then, the decisionlist is consulted and all the decisions of the ruleswith a log of likelihood ratio above a certainthreshold are recorded.
Finally, as in the caseof previous research (Sekine et al, 1998; Berth-wick, 1999), the most appropriate sequence ofthe decisions that are consistent throughout thewhole sequence is searched for.
By consistencyof the decisions, we mean requirements such asthat the decision representing the beginning ofsome named entity type has to be followed bythat representing the middle of the same entitytype (in the case of Start /End encoding).
Also,in our case, the appropriateness of the sequenceof the decisions is measured by the stun of thelog of likelihood ratios of 1;t1(; corresponding de-cision rules.5 Exper imenta l  Eva luat ionWe experimentally evaluate the performanceof the supervised learning tbr Japanese nalnedentity recognition on the IREX workshop'straining and test data.
We compare the re-suits of the confl)inations of the two encod-ing schemes of named entity chunking states(the Inside/Outside and the Start /End encod-ing schemes) and the two at)preaches to contex-tual feature design (the 3-gram and the VariableLength models).
For each of those combina-tions, we search tbr an optintal threshold of thelog of likelihood ratio in the decision list.
Theperformance of each combination measured byF-measure (fl = 1) is given in Table 4.In this ewduation, we exclude the namedentities with "other boundary mismatch" inTat)le 2.
We also classify the systemoutput according to the number of con-stitnent lnorphemes of each named entityand evaluate the peribnnance tbr each sub-set of the system output.
For each sub-7103-gramVariableLength' l 'al)le 4: Ewduation \]{esults Measured by F-measm'e (fl = 1)~, Mori)lmlnes to 1 Named Entity- .
,  > J_ll ,,, = I,,.=21,..=311.,>_21.,,.>_31,,>4inside/Outside 72.9 75.9 79.7 51.4 69.4 42.5 29.2Start/End 72.7 76.6 79.6 43.7 68.1 37.8 29.6inside/Outside lJ 74.3 77.6 80.0 55.5 70.9 49.9 41.0Start/End t \ [72.1  77.0 75.6 51.5 67.2 48.6 43.6set, we compare,' the performmme of the fore'combinations of {3-grmn, Vm'iable Length} ?
{Inside/Outside, S|;~n't/EIld} mM show thehighest mrtbrmance with bold-faced font.Several remarkable points of these re, suits of1)erfbrmance omparison can be stated as below:?
Among the four coml)inations, the VariableLength Model with hlside/()utside Ent:od-ing 1)erfi)rms best in tot~fl (n > 1) as wellas in the recognition of named entities con-sisting of more thml one morl)heme (',, --2, 3, n > 2, 3).?
in the re,(:ognil;ion of ilsAll(;d elll;ities con-sisting of more than two mOl"l)henles (~, =3: ?t ~ 3, 4)~ the Vm'ial)le Lellgth Modell)erforlllS signific;mtly t)etter thml the 3-rill "t(.~ {~l'alll mo(le\].
.tn\],' result (:letu'ly SUpl)ortsthe (;l~iin that our modeli\]xg of the Vm'i-nl)le Length Model has an adva,ntnge in therecognition ()f long named entities."
Ill general, the Inside/Outside n(:odingscheme l)erfol'lns slightly t)etl;er th;m theSta\]'t/l'3nd encoding s(:henm, (Well thoughthe tbrmer distinguislms (:onsidera|)ly ti~wersl;ates th;m the latter.6 Conc lus ionIn this 1)~per, we al)plied the supervised eci-si(m list learning method to ,\]at)anese mmmd en-tity recognition, into wlfich we, incorporated sev-eral n(mn phrase chunking teelmiques ~md ex-perimentally evaluated their pertbrmance.
We,showed that a novel technique that we proposedout, performed those using previously considered(;otd;extual fe~tu\]:es.7 AcknowledgmentsThis research was c~rried out while the au-thors were visiting scholars at l)epartment ofComputer Science, Johns Hopkins University.The ~mthors would like to thank Prof  DavidYarowsky of Johns Hopkins University for in-valual)le sut)porl;s to this research.ReferencesA.
Borthwick.
1999.
A JaI)mmse named entity rec-ognizer constructed by a non-speaker ofJapanese.In Proc.
of the II~EX Workshop, pages 187 193.54.
Collins a.nd Y.
Singer.
1999.
Unsupervised mod-els of named entity classification.
In P.roc.
ofthe 1999 Joint SIGDAT Cm@rcncc on Empiri-cal Mcth, ods in Natural Languagc P~vccssing andVery Large Corpora, pages 100 110.S.
Cucerzmt and D. Yarowsky.
1999. l~anguage inde-1)endent named entity recognition combining mor-1)hological mid contextual evideime, in Proc.
ofth, c 1999 Joi'nt SIGDAT Cm@rence on Empiri-cal Methods in Natural Language PTvccssin9 midVery Large Corpora, pages 90 99.IREX Committee, editor.
1!)99.
P~vcecdings of the\]REX Workshop.
(in Japanese).S.
Maiorano.
1996.
The multilingual entity task(MET): Jalmne, se, results.
In ISvc.
of TIPSTEI~,PIH)U1/,AM P HA,5'1'; 11, pa.ges 449 45\].MUC.
1998. l)'rocccdings oJ"l,h,e, 7th Message Unde'r-standing ConJ?rence, (MUC-7).L.
l{alnshaw and M. Ma.rcus.
1995.
Text chunkingusing trmlsforma.tion-based \] mning.
In P,roc.
ofth, c 3rd Work,vh, op on l/cry Larg(: Corpora~ Im.ges83 -94.ILL.
Rive, st. 1987, Le, arning decision lists.
MachineLearning, 2:229 246.M.
Sassano, Y. Saito, and K. Matsui.
1997.,J~l)alle,se morphological mmlyzer for NLP apl)li-(:atioils.
Ill Proc.
of thc, 3rd gn'ttual Meeting ofth, c Association for Natural Language Processing,1)a.ges 441- 444.
(in Jal)anese).S.
Sekine, lL Grishman, and H. Shinnou.
1998.A decision tree method tbr tinding and ('lassit~-ing names in Jat)almse texts.
In Proc.
of the 6thWorkshop on Very La~yc Ctnpora, pages 148-152.T.
Utsuro mid M. Sassano.
2000.
Minimally su-pervised ,\]almnese named e, ntity recognition: I{e-source, s and evahmtion.
In Proc.
of thc 2nd Inter-national Confcrcncc on Lanquaqc Resources andEvahtation, pages 1229 -1236.1).
Yarowsky.
1994.
Decision lists for lexical mnbi-guity resolution: Al)t)lication to accent restora.-tion in Spanish and French.
In Proc.
of the 32ridAnnual Mecl, ing of ACL, 1)ages 88 -95.711
