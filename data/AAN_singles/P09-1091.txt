Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 809?816,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPDependency Based Chinese Sentence RealizationWei He1, Haifeng Wang2, Yuqing Guo2, Ting Liu11Information Retrieval Lab, Harbin Institute of Technology, Harbin, China{whe,tliu}@ir.hit.edu.cn2Toshiba (China) Research and Development Center, Beijing, China{wanghaifeng,guoyuqing}@rdc.toshiba.com.cnAbstractThis paper describes log-linear models for ageneral-purpose sentence realizer based on de-pendency structures.
Unlike traditional realiz-ers using grammar rules, our method realizessentences by linearizing dependency relationsdirectly in two steps.
First, the relative orderbetween head and each dependent is deter-mined by their dependency relation.
Then thebest linearizations compatible with the relativeorder are selected by log-linear models.
Thelog-linear models incorporate three types offeature functions, including dependency rela-tions, surface words and headwords.
Our ap-proach to sentence realization provides sim-plicity, efficiency and competitive accuracy.Trained on 8,975 dependency structures of aChinese Dependency Treebank, the realizerachieves a BLEU score of 0.8874.1 IntroductionSentence realization can be described as theprocess of converting the semantic and syntacticrepresentation of a sentence or series of sen-tences into meaningful, grammatically correctand fluent text of a particular language.Most previous general-purpose realization sys-tems are developed via the application of a set ofgrammar rules based on particular linguistictheories, e.g.
Lexical Functional Grammar (LFG),Head Driven Phrase Structure Grammar (HPSG),Combinatory Categorical Grammar (CCG), TreeAdjoining Grammar (TAG) etc.
The grammarrules are either developed by hand, such as thoseused in LinGo (Carroll et al, 1999), OpenCCG(White, 2004) and XLE (Crouch et al, 2007), orextracted automatically from annotated corpora,like the HPSG (Nakanishi et al, 2005), LFG(Cahill and van Genabith, 2006; Hogan et al,2007) and CCG (White et al, 2007) resourcesderived from the Penn-II Treebank.Over the last decade, there has been a lot of in-terest in a generate-and-select paradigm for sur-face realization.
The paradigm is characterizedby a separation between realization and selection,in which rule-based methods are used to generatea space of possible paraphrases, and statisticalmethods are used to select the most likely reali-zation from the space.
Usually, two statisticalmodels are used to rank the output candidates.One is n-gram model over different units, such asword-level bigram/trigram models (Bangaloreand Rambow, 2000; Langkilde, 2000), or fac-tored language models integrated with syntactictags (White et al 2007).
The other is log-linearmodel with different syntactic and semantic fea-tures (Velldal and Oepen, 2005; Nakanishi et al,2005; Cahill et al, 2007).However, little work has been done on proba-bilistic models learning direct mapping from in-put to surface strings, without the effort to con-struct a grammar.
Guo et al (2008) develop ageneral-purpose realizer couched in the frame-work of Lexical Functional Grammar based onsimple n-gram models.
Wan et al (2009) presenta dependency-spanning tree algorithm for wordordering, which first builds dependency trees todecide linear precedence between heads andmodifiers then uses an n-gram language model toorder siblings.
Compared with n-gram model,log-linear model is more powerful in that it iseasy to integrate a variety of features, and to tunefeature weights to maximize the probability.
Afew papers have presented maximum entropymodels for word or phrase ordering (Ratnaparkhi,2000; Filippova and Strube, 2007).
However,those attempts have been limited to specializedapplications, such as air travel reservation or or-dering constituents of a main clause in German.This paper presents a general-purpose realizerbased on log-linear models for directly lineariz-ing dependency relations given dependencystructures.
We reduce the generation space by809two techniques: the first is dividing the entiredependency tree into one-depth sub-trees andsolving linearization in sub-trees; the second isthe determination of relative positions betweendependents and heads according to dependencyrelations.
Then the best linearization for eachsub-tree is selected by the log-linear model thatincorporates three types of feature functions, in-cluding dependency relations, surface words andheadwords.
The evaluation shows that our realiz-er achieves competitive generation accuracy.The paper is structured as follows.
In Section2, we describe the idea of dividing the realizationprocedure for an entire dependency tree into aseries of sub-procedures for sub-trees.
We de-scribe how to determine the relative positionsbetween dependents and heads according to de-pendency relations in Section 3.
Section 4 givesdetails of the log-linear model and the featurefunctions used for sentence realization.
Section 5explains the experiments and provides the results.2 Sentence Realization from Dependen-cy Structure2.1 The Dependency InputThe input to our sentence realizer is a dependen-cy structure as represented in the HIT ChineseDependency Treebank (HIT-CDT)1.
In our de-pendency tree representations, dependency rela-tions are represented as arcs pointing from a headto a dependent.
The types of dependency arcsindicate the semantic or grammatical relation-ships between the heads and the dependents,which are recorded in the dependent nodes.
Fig-ure 1 gives an example of dependency tree repre-sentation for the sentence:(1) ?
?
??
?
?this is Wuhan Airline??
??
??
?
?first time buy Boeing airliner?This is the first time for Airline Wuhan to buyBoeing airliners.
?In a dependency structure, dependents are un-ordered, i.e.
the string position of each node isnot recorded in the representation.
Our sentencerealizer takes such an unordered dependency treeas input, determines the linear order of the words1 HIT-CDT (http://ir.hit.edu.cn) includes 10,000 sentencesand 215,334 words, which are manually annotated withpart-of-speech tags and dependency labels.
(Liu et al,2006a)as encoded in the nodes of the dependency struc-ture and produces a grammatical sentence.
As thedependency structures input to our realizer havebeen lexicalized, lexical selection is not involvedduring the surface realization.2.2 Divide and Conquer Strategy for Linea-rizationFor determining the linear order of wordsrepresented by nodes of the given dependencystructure, in principle, the sentence realizer hasto produce all possible sequences of the nodesfrom the input tree and selects the most likelylinearization among them.
If the dependency treeconsists of a considerable number of nodes, thisprocedure would be very time-consuming.
Toreduce the number of possible realizations, ourgeneration algorithm adopts a divide-and-conquer strategy, which divides the whole treeinto a set of sub-trees of depth one and recursive-ly linearizes the sub-trees in a bottom-up fashion.As illustrated in Figure 2, sub-trees c and d,which are at the bottom of the tree, are linearizedfirst, then sub-tree b is processed, and finallysub-tree a.The procedure imposes a projective constrainton the dependency structures, viz.
each headdominates a continuous substring of the sentencerealization.
This assumption is feasible in theapplication of the dependency-based generation,because: (i) it has long been observed that thedependency structures of a vast majority of sen-tences in the languages of the world are projec-tive (Igor, 1988) and (ii) non-projective depen-dencies in Chinese, for the most part, are used toaccount for non-local dependency phenomena.Figure 1: The dependency tree for the sentence??????????????????(HED)is??(SBV)this???(VOB)buy???
(ADV)first time???(VOB)airliner???(SBV)airline???(ATT)Wuhan???
(ATT)Boeing810Though non-local dependencies are important foraccurate semantic analysis, they can be easilyconverted to local dependencies conforming tothe projective constraint.
In fact, we find that the10, 000 manually-build dependency trees of theHIT-CDT do not contain any non-projective de-pendencies.3 Relative Position DeterminationIn dependency structures, the semantic or gram-matical roles of the nodes are indicated by typesof dependency relations.
For example, the VOBdependency relation, which stands for the verb-object structure, means that the head is a verband the dependent is an object of the verb; theATT relation, means that the dependent is anattribute of the head.
In languages with fairlyrigid word order, the relative position betweenthe head and dependent of a certain relation is ina fixed order.
For example in Chinese, the objectalmost always occurs behind its dominating verb;the attribute modifier always occurs in front ofits head word.
Therefore, we can draw a conclu-sion that the relative positions between head anddependent of VOB and ATT can be determinedby the types of dependency relations.We make a statistic on the relative positionsbetween head and dependent for each dependen-cy relation type.
Following (Covington, 2001),we call a dependent that precedes its head prede-pendent, a dependent that follows its head post-dependent.
The corpus used to gather appropriatestatistics is HIT-CDT.
Table 1 gives the numbers??(HED)is??(SBV)this?
?
????????????????(VOB)buy???
(ADV)first time?
?????
??
??
???????(VOB)airliner???(ATT)Boeing??
?????(SBV)Airline???(ATT)Wuhan??
?
?sub-tree asub-tree bsub-tree c sub-tree dFigure 2: Illustration of the linearization procedureRelation Description Postdep.
Predep.ADV adverbial 1 25977APP appositive 807 0ATT attribute 0 47040CMP complement 2931 3CNJ conjunctive 0 2124COO coordinate 6818 0DC dep.
clause 197 0DE DE phrase 0 10973DEI DEI phrase 131 3DI DI phrase 0 400IC indep.clause 3230 0IS indep.structure 125 794LAD left adjunct 0 2644MT mood-tense 3203 0POB prep-obj 7513 0QUN quantity 0 6092RAD right adjunct 1332 1SBV subject-verb 6 16016SIM similarity 0 44VOB verb-object 23487 21VV verb-verb 6570 2Table 1: Numbers of pre/post-dependents for eachdependency relation811of predependent/postdependent for each type ofdependency relations and its descriptions.Table 1 shows that 100% dependents of ATTrelation are predependents and 23,487(99.9%)against 21(0.1%) VOB dependents are postde-pendents.
Almost all the dependency relationshave a dominant dependent type?predependentor postdependent.
Although some dependencyrelations have exceptional cases (e.g.
VOB), thenumber is so small that it can be ignored.
Theonly exception is the IS relation, which has794(86.4%) predependents and 125(13.6%)postdependents.
The IS label is an abbreviationfor independent structure.
This type of depen-dency relation is usually used to represent inter-jections or comments set off by brackets, whichusually has little grammatical connection withthe head.
Figure 3 gives an example of indepen-dent structure.
This example is from a news re-port, and the phrase ???????
(set apart bybrackets in the original text) is a supplementaryexplanation for the source of the news.
The con-nection between this phrase and the main clauseis so weak that either it precedes or follows thehead verb is acceptable in grammar.
However,this kind of news-source-explanation is customa-ry to place at the beginning of a sentence in Chi-nese.
This can probably explain the majority ofthe IS-tagged dependents are predependents.If we simply treat all the IS dependents as pre-dependents, we can assume that every dependen-cy relation has only one type of dependent, eitherpredependent or postdependent.
Therefore, therelative position between head and dependentcan be determined just by the types of dependen-cy relations.In the light of this assumption, all dependentsin a sub-tree can be classified into two groups?predependents and postdependents.
The prede-pendents must precede the head, and the postde-pendents must follow the head.
This classifica-tion not only reduces the number of possible se-quences, but also solves the linearization of asub-tree if the sub-tree contains only one depen-dent, or two dependents of different types, viz.one predependent and one postdependent.
In sub-tree c of Figure 2, the dependency relation be-tween the only dependent and the head is ATT,which indicates that the dependent is a prede-pendent.
Therefore, node 7 is bound to precedenode 5, and the only linearization result is ??????.
In sub-tree a of the same figure, the clas-sification for SBV is predependent, and for VOBis postdependent, so the only linearization is<node 2, node 1, node 3>.In HIT-CDT, there are 108,086 sub-trees inthe 10,000 sentences, 65% sub-trees have onlyone dependent, and 7% sub-trees have two de-pendents of different types (one predependentand one postdependent).
This means that therelative position classification can deterministi-cally linearize 72% sub-trees, and only the rest28% sub-trees with more than one predependentor postdependent need to be further determined.4 Log-linear ModelsWe use log-linear models for selecting the se-quence with the highest probability from all thepossible linearizations of a sub-tree.4.1 The Log-linear ModelLog-linear models employ a set of feature func-tions to describe properties of the data, and a setof learned weights to determine the contributionof each feature.
In this framework, we have a setof M feature functions Mmtrhm ,...,1),,( = .For each feature function, there exists a modelparameter Mmtrm ,...,1),,( =?
that is fitted tooptimize the likelihood of the training data.
Aconditional log-linear model for the probabilityof a realization r given the dependency tree t, hasthe general parametric form)],(exp[)(1)|(1trhtZtrp mMmm?== ???
(1)where )(tZ?
is a normalization factor defined as?
??
==)(' 1)],'(exp[)(tYrmMmm trhtZ ??
(2)And Y(t) gives the set of all possible realizationsof the dependency tree t.4.2 Feature FunctionsWe use three types of feature functions for cap-turing relations among nodes on the dependencytree.
In order to better illustrate the feature func-tions used in the log-linear model, we redrawsub-tree b of Figure 2 in Figure 4.
Here we as-sume the linearizations of sub-tree c and d haveFigure 3: Example of independent structure???(HED)serious??????
(IS)Xinhua news?????
(SBV)southern snowstorm812been finished, and the strings of linearizing re-sults are recorded in nodes 5 and 6.The sub-tree in Figure 4 has two predepen-dents (SBV and ADV) and one postdependent(VOB).
As a result of this classification, the onlytwo possible linearizations of the sub-tree are<node 4, node 6, node 3, node 5> and <node 6,node 4, node 3, node 5>.
Then the log-linearmodel that incorporates three types of featurefunctions is used to make further selection.Dependency Relation Model: For a particularsub-tree structure, the task of generating a stringcovered by the nodes on the sub-tree is equiva-lent to linearizing all the dependency relations inthat sub-tree.
We linearize the dependency rela-tions by computing n-gram models, similar totraditional word-based language models, exceptusing the names of dependency relations insteadof words.
For the two linearizations of Figure 4,the corresponding dependency relation sequencesare ?ADV SBV VOB VOB?
and ?SBV ADVVOB VOB?.
The dependency relation modelcalculates the probability of dependency relationn-gram P(DR) according to Eq.(3).
The probabil-ity score is integrated into the log-linear model asa feature.)...
()( 11 mm DRDRPDRP =  (3))|( 1 11?+?=?= k nkmkk DRDRPWord Model: We integrate an n-gram wordmodel into the log-linear model for capturing therelation between adjacent words.
For a string ofwords generated from a possible sequence ofsub-tree nodes, the word models calculate word-based n-gram probabilities of the string.
For ex-ample, in Figure 4, the strings generated by thetwo possible sequences are ?????
??
??
?????
and ???
????
??
?????.
The word model takes these two strings asinput, and calculates the n-gram probabilities.Headword Model: 2 In dependency representa-tions, heads usually play more important rolesthan dependents.
The headword model calculatesthe n-gram probabilities of headwords, withoutregard to the words occurring at dependent nodes,in that dependent words are usually less impor-tant than headwords.
In Figure 4, the two possi-ble sequences of headwords are ???
??
??
???
and ???
??
??
???.
Theheadword strings are usually more generic thanthe strings including all words, and thus theheadword model is more likely to relax the datasparseness.Table 2 gives some examples of all the featuresused in the log-linear model.
The examples listedin the table are features of the linearization<node 6, node 4, node 3, node 5>, extracted fromthe sub-tree in Figure 4.In this paper, all the feature functions used inthe log-linear model are n-gram probabilities.However, the log-linear framework has greatpotential for including other types of features.4.3 Parameter EstimationBLEU score, a method originally proposed toautomatically evaluate machine translation quali-ty (Papineni et al, 2002), has been widely usedas a metric to evaluate general-purpose sentencegeneration (Langkilde, 2002; White et al, 2007;Guo et al 2008, Wan et al 2009).
The BLEUmeasure computes the geometric mean of theprecision of n-grams of various lengths betweena sentence realization and a (set of) reference(s).To estimate the parameters ),...,( 1 M??
for thefeature functions ),...,( 1 Mhh , we use BLEU3 asoptimization objective function and adopt theapproach of minimum error rate training2 Here the term ?headword?
is used to describe the wordthat occurs at head nodes in dependency trees.3 The BLEU scoring script is supplied by NIST Open Ma-chine Translation Evaluation atftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.plFeature function Examples of featuresDependency Relation ?SBV ADV VOB?
?ADV VOB VOB?Word Model ????????
????????
???????????????
?Headword Model ??????
??????
?????
?Table 2: Examples of feature functions???(VOB)buy???
(ADV)first time???(VOB)airliner?????
?airliners of Boeing???(SBV)Airline?????
?Airline WuhanFigure 4: Sub-tree with multiple predependents813(MERT), which is popular in statistical machinetranslation (Och, 2003).4.4 The Realization AlgorithmThe realization algorithm is a recursive proce-dure that starts from the root node of the depen-dency tree, and traverses the tree by depth-firstsearch.
The pseudo code of the realization algo-rithm is shown in Figure 5.5 Experiments5.1 Experimental DesignOur experiments are carried out on HIT-CDT.We randomly select 526 sentences as the test set,and 499 sentences as the development set foroptimizing the model parameters.
The rest 8,975sentences of the HIT-CDT are used for trainingof the dependency relation model.
For training ofword models, we use the Xinhua News part(6,879,644 words) of Chinese Gigaword SecondEdition (LDC2005T14), segmented by the Lan-guage Technology Platform (LTP) 4 .
And fortraining the headword model, we use both theHIT-CDT and the HIT Chinese Skeletal Depen-dency Treebank (HIT-CSDT).
HIT-CSDT is a4 http://ir.hit.edu.cn/demo/ltpcomponent of LTP and contains 49,991 sen-tences in dependency structure representation(without dependency relation labels).As the input dependency representation doesnot contain punctuation information, we simplyremove all punctuation marks in the test and de-velopment sets.5.2 Evaluation MetricsIn addition to BLEU score, percentage of exactlymatched sentences and average NIST simplestring accuracy (SSA) are adopted as evaluationmetrics.
The exact match measure is percentageof the generated string that exactly matches thecorresponding reference sentence.
The averageNIST simple string accuracy score reflects theaverage number of insertion (I), deletion (D), andsubstitution (S) errors between the output sen-tence and the reference sentence.
Formally, SSA= 1 ?
(I + D + S) / R, where R is the number oftokens in the reference sentence.5.3 Experimental ResultsAll the evaluation results are shown in Table 3.The first experiment, which is a baseline experi-ment, ignores the tree structure and randomlychooses position for every word.
From thesecond experiment, we begin to utilize the treestructure and apply the realization algorithm de-scribed in Section 4.4.
In the second experiment,predependents are distinguished from postdepen-dents by the relative position determination me-thod (RPD), then the orders inside predependentsand postdependents are chosen randomly.
Fromthe third experiments, the log-linear models areused for scoring the generated sequences, withthe aid of three types of feature functions as de-scribed in Section 4.2.
First, the feature functionsof trigram dependency relation model (DR), bi-gram word model (Bi-WM), trigram word model(Tri-WM) (with Katz backoff) and trigramheadword model (HW) are used separately inexperiments 3-6.
Then we combine the feature1:procedure SEARCH2:input: sub-tree T {head:H dep.
:D1?Dn}3:  if n = 0 then return4:  for i := 1 to n5:    SEARCH(Di)6:  Apre := {}7:  Apost := {}8:  for i := 1 to n9:    if PRE-DEP(Di) then Apre:=Apre?
{Di}10:   if POST-DEP(Di) then Apost:=Apost?
{Di}11: for all permutations p1 of Apre12:   for all permutations p2 of Apost13:     sequence s := JOIN(p1,H,p2)14:     score r := LOG-LINEAR(s)15:     if best-score(r) then RECORD(r,s)Figure 5: The algorithm for linearizations of sub-treesModel BLEU ExMatch SSA1 Random 0.1478 0.0038 0.20442 RPD + Random 0.5943 0.1274 0.63693 RPD + DR 0.7204 0.2167 0.76834 RPD + Bi-WM 0.8289 0.4125 0.82705 RPD + Tri-WM 0.8508 0.4715 0.84156 RPD + HW 0.7592 0.2909 0.76387 RPD + DR + Bi-WM 0.8615 0.4810 0.87238 RPD + DR + Tri-WM 0.8772 0.5247 0.88179 RPD + DR + Tri-WM + HW 0.8874 0.5475 0.8920Table 3: BLEU, ExMatch and SSA scores on the test set814functions incrementally based on the RPD andDR model.The relative position determination plays animportant role in the realization algorithm.
Weobserve that the BLEU score is boosted from0.1478 to 0.5943 by using the RPD method.
Thiscan be explained by the reason that the lineariza-tions of 72% sub-trees can be definitely deter-mined by the RPD method.
All of the four fea-ture functions we have tested achieve considera-ble improvement in BLEU scores.
The depen-dency relation model achieves 0.7204, the bi-gram word model 0.8289, the trigram word mod-el 0.8508 and the headword model achieves0.7592.
While the combined models perform bet-ter than any of their individual component mod-els.
On the foundation of relative position deter-mination method, the combination of dependen-cy relation and bigram word model achieves aBLEU score of 0.8615, and the combination ofdependency relation and trigram word modelachieves a BLEU score of 0.8772.
Finally thecombination of dependency relation model, tri-gram word model and headword model achievesthe best result 0.8874.5.4 DiscussionWe first inspected the errors made by the relativeposition determination method.
In the treebank-tree test set, there are 7 predependents classifiedas postdependents and 3 postdependents classi-fied as predependents by error.
Among the 9,384dependents, the error rate of the relative positiondetermination method is very small (0.1%).Then we make a classification on the errors inthe experiment of dependency relation model(with relative position determination method).Table 4 shows the distribution of the errors.The first type of errors is caused by duplicatedependency relations, i.e.
a head with two ormore dependents that have the same dependencyrelations.
In this situation, only using the depen-dency relation model cannot generate the rightlinearization.
However, word models, which util-ize the word information, can make distinctionsbetween the dependencies.
The reason for theerrors of SBV-ADV and ATT-QUN is probablybecause the order of these pairs of grammar rolesis somewhat flexible.
For example, the strings of???
(ADV)/today ?(SBV)/I?
and ??(SBV)/I??(ADV)/today?
are both very common andacceptable in Chinese.The word models tend to combine the nodesthat have strong correlation together.
For exam-ple in Figure 6, node 2 is more likely to precedenode 3 because the words ???
/protect?
and???
/future?
have strong correlation, but thecorrect order is <node 3, node 2>.Headword model only consider the words oc-cur at head nodes, which is helpful in the situa-tion like Figure 6.
In our experiments, the head-word model gets a relatively low performance byitself, however, the addition of headword modelto the combination of the other two feature func-tions improves the result from 0.8772 to 0.8874.This indicates that the headword model is com-plementary to the other feature functions.6 ConclusionsWe have presented a general-purpose realizerbased on log-linear models, which directly mapsdependency relations into surface strings.
Thelinearization of a whole dependency tree is di-vided into a series of sub-procedures on sub-trees.The dependents in the sub-trees are classifiedinto two groups, predependents or postdepen-dents, according to their dependency relations.The evaluation shows that this relative positiondetermination method achieves a considerableresult.
The log-linear model, which incorporatesthree types of feature functions, including de-pendency relation, surface words and headwords,successfully captures factors in sentence realiza-tion and demonstrates competitive performance.ReferencesSrinivas Bangalore and Owen Rambow.
2000.
Ex-ploiting a Probabilistic Hierarchical Model forGeneration.
In Proceedings of the 18th Interna-tional Conference on Computational Linguistics,pages 42-48.
Saarbr?cken, Germany.Error types Proportion1 Duplicate dependency relations 60.0%2 SBV-ADV 20.3%3 ATT-QUN 6.3%4 Other 13.4%Table 4: Error types in the RPD+DR experimentFigure 6: Sub-tree for ??????????????work???(ATT)protect???
???
?birds protecting???(SBV)of???
?
?future815Aoife Cahill and Josef van Genabith.
2006.
RobustPCFG-Based Generation Using Automatically Ac-quired LFG Approximations.
In Proceedings of the21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1033-1040.
Sydney, Australia.Aoife Cahill, Martin Forst and Christian Rohrer.
2007.Stochastic Realisation Ranking for a Free WordOrder language.
In Proceedings of 11th EuropeanWorkshop on Natural Language Generation, pages17-24.
Schloss Dagstuhl, Germany.John Carroll, Ann Copestake, Dan Flickinger, andVictor Poznanski.
1999.
An Efficient Chart Gene-rator for (Semi-)Lexicalist Grammars.
In Proceed-ings of the 7th European Workshop on NaturalLanguage Generation, pages 86-95, Toulouse.Michael A. Covington.
2001.
A Fundamental Algo-rithm for Dependency Parsing.
In Proceedings ofthe 39th Annual ACM Southeast Conference, pages95?102.Dick Crouch, Mary Dalrymple, Ron Kaplan, TracyKing, John Maxwell, and Paula Newman.
2007.XLE documentation.
Palo Alto Research Center,CA.Katja Filippova and Michael Strube.
2007.
GeneratingConstituent Order in German Clauses.
In Proceed-ings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 320-327.
Pra-gue, Czech Republic.Yuqing Guo, Haifeng Wang and Josef van Genabith.2008.
Dependency-Based N-Gram Models forGeneral Purpose Sentence Realisation.
In Proceed-ings of the 22th International Conference on Com-putational Linguistics, pages 297-304.
Manchester,UK.Deirdre Hogan, Conor Cafferkey, Aoife Cahill andJosef van Genabith.
2007.
Exploiting Multi-WordUnits in History-Based Probabilistic Generation.
InProceedings of the 2007 Joint Conference on Em-pirical Methods in Natural Language Processingand CoNLL, pages 267-276.
Prague, Czech Repub-lic.Mel'?uk Igor.
1988.
Dependency syntax: Theory andpractice.
In Suny Series in Linguistics.
State Uni-versity of New York Press, New York, USA.Irene Langkilde.
2000.
Forest-Based Statistical Sen-tence Generation.
In Proceedings of 1st Meeting ofthe North American Chapter of the Association forComputational Linguistics, pages 170-177.
Seattle,WA.Irene Langkilde.
2002.
An Empirical Verification ofCoverage and Correctness for a General-PurposeSentence Generator.
In Proceedings of the SecondInternational Conference on Natural LanguageGeneration, pages 17-24.
New York, USA.Ting Liu, Jinshan Ma, and Sheng Li.
2006a.
Buildinga Dependency Treebank for Improving ChineseParser.
Journal of Chinese Language and Compu-ting, 16(4): 207-224.Ting Liu, Jinshan Ma, Huijia Zhu, and Sheng Li.2006b.
Dependency Parsing Based on DynamicLocal Optimization.
In Proceedings of CoNLL-X,pages 211-215, New York, USA.Hiroko Nakanishi, Yusuke Miyao and Jun?ichi Tsujii.2005.
Probabilistic Models for Disambiguation ofan HPSG-Based Chart Generator.
In Proceedingsof the 9th International Workshop on ParsingTechnology, pages 93-102.
Vancouver, British Co-lumbia.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsof the 41st Annual Meeting of the Association forComputational Linguistics, pages 160-167, Sappo-ro, Japan.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a Method for Auto-matic Evaluation of Machine Translation.
In Pro-ceedings of the 40th Annual Meeting of the Associ-ation for Computational Linguistics, pages 311-318.
Philadelphia, PA.Adwait Ratnaparkhi.
2000.
Trainable Methods forNatural Language Generation.
In Proceedings ofNorth American Chapter of the Association forComputational Linguistics, pages 194-201.
Seattle,WA.Erik Velldal and Stephan Oepen.
2005.
MaximumEntropy Models for Realization Ranking.
In Pro-ceedings of the 10th Machine Translation Summit,pages 109-116.
Phuket, Thailand,Stephen Wan, Mark Dras, Robert Dale, C?cile Paris.2009.
Improving Grammaticality in Statistical Sen-tence Generation: Introducing a Dependency Span-ning Tree Algorithm with an Argument Satisfac-tion Model.
In Proceedings of the 12th Conferenceof the European Chapter of the ACL, pages 852-860.
Athens, Greece.Michael White.
2004.
Reining in CCG Chart Realiza-tion.
In Proceedings of the third International Nat-ural Language Generation Conference, pages 182-191.
Hampshire, UK.Michael White, Rajakrishnan Rajkumar and ScottMartin.
2007.
Towards Broad Coverage SurfaceRealization with CCG.
In Proceedings of the Ma-chine Translation Summit XI Workshop, pages 22-30.
Copenhagen, Danmark.816
