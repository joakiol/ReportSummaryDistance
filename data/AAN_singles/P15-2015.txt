Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 89?93,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsSpectral Semi-Supervised Discourse Relation ClassificationRobert FisherCarnegie Mellon University5000 Forbes AvePittsburgh, PA 15213rwfisher@cs.cmu.eduReid SimmonsCarnegie Mellon University5000 Forbes AvePittsburgh, PA 15213reids@cs.cmu.eduAbstractDiscourse parsing is the process of dis-covering the latent relational structure ofa long form piece of text and remains asignificant open challenge.
One of themost difficult tasks in discourse parsing isthe classification of implicit discourse re-lations.
Most state-of-the-art systems donot leverage the great volume of unlabeledtext available on the web?they rely insteadon human annotated training data.
By in-corporating a mixture of labeled and unla-beled data, we are able to improve relationclassification accuracy, reduce the need forannotated data, while still retaining the ca-pacity to use labeled data to ensure thatspecific desired relations are learned.
Weachieve this using a latent variable modelthat is trained in a reduced dimensionalitysubspace using spectral methods.
Our ap-proach achieves an F1score of 0.485 onthe implicit relation labeling task for thePenn Discourse Treebank.1 IntroductionDiscourse parsing is a fundamental task in naturallanguage processing that entails the discovery ofthe latent relational structure in a multi-sentencepiece of text.
Unlike semantic and syntactic pars-ing, which are used for single sentence pars-ing, discourse parsing is used to discover inter-sentential relations in longer pieces of text.
With-out discourse, parsing methods can only be used tounderstand documents as sequences of unrelatedsentences.Unfortunately, manual annotation of discoursestructure in text is costly and time consuming.Multiple annotators are required for each relationto estimate inter-annotator agreement.
The PennDiscourse Treebank (PDTB) (Prasad et al, 2008).is one of the largest annotated discourse parsingdatasets, with 16,224 implicit relations.
However,this pales in comparison to unlabeled datasets thatcan include millions of sentences of text.
By aug-menting a labeled dataset with unlabeled data, wecan use a bootstrapping framework to improvepredictive accuracy, and reduce the need for la-beled data?which could make it much easier toport discourse parsing algorithms to new domains.On the other hand, a fully unsupervised parser maynot be desirable because in many applications spe-cific discourse relations must be identified, whichwould be difficult to achieve without the use of la-beled examples.There has recently been growing interest in abreed of algorithms based on spectral decomposi-tion, which are well suited to training with unla-beled data.
Spectral algorithms utilize matrix fac-torization algorithms such as Singular Value De-composition (SVD) and rank factorization to dis-cover low-rank decompositions of matrices or ten-sors of empirical moments.
In many models, thesedecompositions allow us to identify the subspacespanned by a group of parameter vectors or theactual parameter vectors themselves.
For taskswhere they can be applied, spectral methods pro-vide statistically consistent results that avoid lo-cal maxima.
Also, spectral algorithms tend tobe much faster?sometimes orders of magnitudefaster?than competing approaches, which makesthem ideal for tackling large datasets.
These meth-ods can be viewed as inferring something aboutthe latent structure of a domain?for example, in ahidden Markov model, the number of latent statesand the sparsity pattern of the transition matrix areforms of latent structure, and spectral methods canrecover both in the limit.This paper presents a semi-supervised spectralmodel for a sequential relation labeling task fordiscourse parsing.
Besides the theoretically desir-able properties mentioned above, we also demon-89strate the practical advantages of the model withan empirical evaluation on the Penn DiscourseTreebank (PDTB) (Prasad et al, 2008) dataset,which yields an F1score of 0.485.
This accuracyshows a 7-9 percentage point improvement overapproaches that do not utilize unlabeled trainingdata.2 Related WorkThere has been quite a bit of work concerningfully supervised relation classification with thePDTB (Lin et al, 2014; Feng and Hirst, 2012;Webber et al, 2012).
Semi-supervised relationclassification is much less common however.
Onerecent example of an attempt to leverage unla-beled data appears in (Hernault et al, 2011),which showed that moderate classification accu-racy can be achieved with very small labeleddatasets.
However, this approach is not compet-itive with fully supervised classifiers when moretraining data is available.
Recently there hasalso been some work to use Conditional RandomFields (CRFs) to represent the global properties ofa parse sequence (Joty et al, 2013; Feng and Hirst,2014), though this work has focused on the RST-DT corpus, rather than the PDTB.In addition to requiring a fully supervised train-ing set, most existing discourse parsers use non-spectral optimization that is often slow and inex-act.
However, there has been some work in otherparsing tasks to employ spectral methods in bothsupervised and semi-supervised settings (Parikh etal., 2014; Cohen et al, 2014).
Spectral methodshave also been applied very successfully in manynon-linguistic domains (Hsu et al, 2012; Bootsand Gordon, 2010; Fisher et al, 2014).3 Problem Definition and DatasetThis section defines the discourse parsing prob-lem and discusses the characteristics of the PDTB.The PDTB consists of annotated articles from theWall Street Journal and is used in our empiri-cal evaluations.
This is combined with the NewYork Times Annotated Corpus (Sandhaus, 2008),which includes 1.8 million New York Times arti-cles printed between 1987 and 2007.Discourse parsing can be reduced to three sepa-rate tasks.
First, the text must be decomposed intoelementary discourse units (EDUs), which may ormay not coincide with sentence boundaries.
TheEDUs are often independent clauses that may beconnected with conjunctions.
After the text hasbeen partitioned into EDUs, the discourse struc-ture must be identified.
This requires us to iden-tify all pairs of EDUs that will be connected withsome discourse relation.
These relational links in-duce the skeletal structure of the discourse parsetree.
Finally, each connection identified in the pre-vious step must be labeled using a known set ofrelations.
Examples of these discourse relationsinclude concession, causal, and instantiation rela-tions.
In the PDTB, only adjacent discourse unitsare connected with a discourse relation, so withthis dataset we are considering parse sequencesrather than parse trees.In this work, we focus on the relation labelingtask, as fairly simple methods perform quite wellat the other two tasks (Webber et al, 2012).
Weuse the ground truth parse structures provided bythe PDTB dataset, so as to isolate the error intro-duced by relation labeling in our results, but inpractice a greedy structure learning algorithm canbe used if the parse structures are not known a pri-ori.Some of the relations in the dataset are inducedby specific connective words in the text.
For exam-ple, a contrast relation may be explicitly revealedby the conjunction but.
Simple classifiers usingonly the text of the discourse connective with POStags can find explicit relations with high accu-racy (Lin et al, 2014).
The following sentenceshows an example of a more difficult implicit re-lation.
In this sentence, two EDUs are connectedwith an explanatory relation, shown in bold, al-though the connective word does not occur in thetext.
?But a few funds have taken other defen-sive steps.
Some have raised their cashpositions to record levels.
[BECAUSE]High cash positions help buffer a fundwhen the market falls.
?We focus on the more difficult implicit relationsthat are not induced by coordinating connectivesin the text.
The implicit relations have been shownto require more sophisticated feature sets includ-ing syntactic and linguistic information (Lin et al,2009).
The PDTB dataset includes 16,053 exam-ples of implicit relations.A full list of the PDTB relations is availablein (Prasad et al, 2008).
The relations are orga-nized hierarchically into top level, types, and sub-types.
Our experiments focus on learning only up90This hasn't been Kellogg Co.'s yearThe oat-bran craze has cost the world's largest cereal maker market share, and!the company's president quit suddenly.edu2r12!
(Contingency.Cause.Reason)h12edu1Figure 1: An example of the latent variable dis-course parsing model taken from the Penn Dis-course Treebank Dataset.
The relation here is anexample of a cause attribution relation.to level 2, as the level 3 (sub-type) relations aretoo specific and show only 80% inter-annotatoragreement.
There are 16 level 2 relations in thePDTB, but the 5 least common relations only ap-pear a handful of times in the dataset and are omit-ted from our tests, yielding 11 possible classes.4 ApproachWe incorporate unlabeled data into our spectraldiscourse parsing model using a bootstrappingframework.
The model is trained over several iter-ations, and the most useful unlabeled sequencesare added as labeled training data after each it-eration.
Our method also utilizes Markovian la-tent states to compactly capture global informa-tion about a parse sequence, with one latent vari-able for each relation in the discourse parsing se-quence.
Most discourse parsing frameworks willlabel relations independently of the rest of the ac-companying parse sequence, but this model allowsfor information about the global structure of thediscourse parse to be used when labeling a rela-tion.
A graphical representation of one link in theparsing model is shown in Figure 1.Specifically, each potential relation rijbetweenelementary discourse units eiand ejis accompa-nied by a corresponding latent variable as hij.
Ac-cording to the model assumptions, the followingequality holds:P (rij= r|r1,2, r2,3...rn+1,n) = P (rij= r|hij)To maintain notational consistency with otherlatent variable models, we will denote these re-lation variables as x1...xn, keeping in mind thatthere is one possible relation for each adjacent pairof elementary discourse units.For the Penn Discourse Treebank Dataset, thediscourse parses behave like sequence of randomvariables representing the relations, which allowsus to use an HMM-like latent variable model basedon the framework presented in (Hsu et al, 2012).If the discourse parses were instead trees, such asthose seen in Rhetorical Structure Theory (RST)datasets, we can modify the standard model to in-clude separate parameters for left and right chil-dren, as demonstrated in (Dhillon et al, 2012).4.1 Spectral LearningThis section briefly describes the process of learn-ing a spectral HMM.
Much more detail about theprocess is available in (Hsu et al, 2012).
Learn-ing in this model will occur in a subspace of di-mensionality m, but system dynamics will be thesame if m is not less than the rank of the obser-vation matrix.
If our original feature space hasdimensionality n, we define a transformation ma-trix U ?
Rn?m, which can be computed usingSingular Value Decomposition.
Given the matrixU , coupled with the empirical unigram (P1), bi-gram (P2,1), and trigram matrices (P3,x,1), we areable to estimate the subspace initial state distribu-tion (p?iU) and observable operator (?AU) using thefollowing equalities (wherein the Moore-Penrosepseudo-inverse of matrix X is denoted by X+):p?iU= UTP1?AU= UTP3,x,1(UTP2,1)+?xFor our original feature space, we use therich linguistic discourse parsing features definedin (Feng and Hirst, 2014), which includes syn-tactic and linguistic features taken from depen-dency parsing, POS tagging, and semantic simi-larity measures.
We augment this feature spacewith a vector space representation of semantics.
Aterm-document co-occurrence matrix is computedusing all of Wikipedia and Latent Dirichlet Anal-ysis was performed using this matrix.
The top 200concepts from the vector space representation foreach pair of EDUs in the dataset are included inthe feature space, with a concept regularization pa-rameter of 0.01.4.2 Semi-Supervised TrainingTo begin semi-supervised training, we performa syntactic parse of the unlabeled data and ex-91tract EDU segments using the method described in(Feng and Hirst, 2014).
The model is then trainedusing the labeled dataset, and the unlabeled re-lations are predicted using the Viterbi algorithm.The most informative sequences in the unlabeledtraining set are added to the labeled training set aslabeled examples.
To measure how informative asequence of relations is, we use density-weightedcertainty sampling (DCS).
Specifically for a se-quence of relations r1...rntaken from a document,d, we use the following formula:DCS(d) =1nn?i=1p?
(ri)H(ri)In this equation, H(ri) represented the entropy ofthe distribution of label predictions for the rela-tion rigenerated by the current spectral model,which is a measure of the model?s uncertainty forthe label of the given relation.
Density is de-noted p?
(ri), and this quantity measures the extentto which the text corresponding to this relationis representative of the labeled corpus.
To com-pute this measure, we create a Kernel Density Es-timate (KDE) over a 100 dimensional LDA vectorspace representation of all EDU?s in the labeledcorpus.
We then compute the density of the KDEfor the text associated with relation ri, which givesus p?(ri).
All sequences of relations in the unla-beled dataset are ranked according to their aver-age density-weighted certainty score, and all se-quences scoring above a parameter ?
are addedto the training set.
The model is then retrained,the unlabeled data re-scored, and the process isrepeated for several iterations.
In iteration i, thelabeled data in the training set is weighted wli,and the unlabeled data is weighted wui, with theunlabeled data receiving higher weight in subse-quent iterations.
The KDE kernel bandwidth andthe parameters ?, wli, wui, and the number of hid-den states are chosen in experiments using 10-foldcross validation on the labeled training set, cou-pled with a subset of the unlabeled data.5 ResultsFigure 2 shows the F1scores of the model usingvarious sizes of labeled training sets.
In all cases,the entirety of the unlabeled data is made avail-able, and 7 rounds of bootstrapping is conducted.Sections 2-22 of the PDTB are used for training,with section 23 being withheld for testing, as rec-ommended by the dataset guidelines (Prasad et al,0 10 20 30 40 50 60 70 80 90 100101520253035404550Percentage of Labeled Training Data UsedF1 Prediction ScoreSpectral HMMLin 14BaselineFigure 2: Empirical results for labeling of implicitrelations.2008).
The results are compared against those re-ported in (Lin et al, 2014), as well as a simplebaseline classifier that labels all relations with themost common class, EntRel.
Compared to thesemi-supervised method described in (Hernault etal., 2011), we show significant gains in accuracyat various sizes of dataset, although the unlabeleddataset used in our experiments is much larger.When the spectral HMM is trained using onlythe labeled dataset, with no unlabeled data, it pro-duces an F1score of 41.1%, which is comparableto the results reported in (Lin et al, 2014).
Bycomparison, the semi-supervised classifier is ableto obtain similar accuracy when using approxi-mately 50% of the labeled training data.
Whengiven access to the full labeled dataset, we seean improvement in the F1score of 7-9 percent-age points.
Recent work has shown promising re-sults using CRFs for discourse parsing (Joty et al,2013; Feng and Hirst, 2014), but the results re-ported in this work were taken from the RST-DTcorpus and are not directly comparable.
However,supervised CRFs and HMMs show similar accu-racy in other language tasks (Ponomareva et al,2007; Awasthi et al, 2006).6 ConclusionsIn this work, we have shown that we are ableto outperform fully-supervised relation classifiersby augmenting the training data with unlabeledtext.
The spectral optimization used in this ap-proach makes computation tractable even whenusing over one million documents.
In future work,we would like to further improve the performanceof this method when very small labeled training92sets are available, which would allow discourseanalysis to be applied in many new and interest-ing domains.AcknowledgementsWe give thanks to Carolyn Penstein Ros?e andGeoff Gordon for their helpful discussions andsuggestions.
We also gratefully acknowledgethe National Science Foundation for their supportthrough EAGER grant number IIS1450543.
Thismaterial is also based upon work supported bythe Quality of Life Technology Center and theNational Science Foundation under CooperativeAgreement EEC-0540865.ReferencesPranjal Awasthi, Delip Rao, and Balaraman Ravindran.2006.
Part of speech tagging and chunking withhmm and crf.
Proceedings of NLP Association ofIndia (NLPAI) Machine Learning Contest 2006.Byron Boots and Geoffrey J Gordon.
2010.
Predictivestate temporal difference learning.
arXiv preprintarXiv:1011.0041.Shay B Cohen, Karl Stratos, Michael Collins, Dean PFoster, and Lyle Ungar.
2014.
Spectral learning oflatent-variable pcfgs: Algorithms and sample com-plexity.
The Journal of Machine Learning Research,15(1):2399?2449.Paramveer S Dhillon, Jordan Rodu, Michael Collins,Dean P Foster, and Lyle H Ungar.
2012.
Spectraldependency parsing with latent variables.
In Pro-ceedings of the 2012 joint conference on empiricalmethods in natural language processing and compu-tational natural language learning, pages 205?213.Association for Computational Linguistics.Vanessa Wei Feng and Graeme Hirst.
2012.
Text-level discourse parsing with rich linguistic fea-tures.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics:Long Papers-Volume 1, pages 60?68.
Associationfor Computational Linguistics.Vanessa Wei Feng and Graeme Hirst.
2014.
A linear-time bottom-up discourse parser with constraintsand post-editing.
In Proceedings of The 52nd An-nual Meeting of the Association for ComputationalLinguistics (ACL 2014), Baltimore, USA, June.Robert Fisher, Reid Simmons, Cheng-Shiu Chung,Rory Cooper, Garrett Grindle, Annmarie Kelleher,Hsinyi Liu, and Yu Kuang Wu.
2014.
Spectral ma-chine learning for predicting power wheelchair exer-cise compliance.
In Foundations of Intelligent Sys-tems, pages 174?183.
Springer.Hugo Hernault, Danushka Bollegala, and MitsuruIshizuka.
2011.
Semi-supervised discourse relationclassification with structural learning.
In Compu-tational Linguistics and Intelligent Text Processing,pages 340?352.
Springer.Daniel Hsu, Sham M Kakade, and Tong Zhang.
2012.A spectral algorithm for learning hidden markovmodels.
Journal of Computer and System Sciences,78(5):1460?1480.Shafiq R Joty, Giuseppe Carenini, Raymond T Ng, andYashar Mehdad.
2013.
Combining intra-and multi-sentential rhetorical parsing for document-level dis-course analysis.
In ACL (1), pages 486?496.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing implicit discourse relations in the penndiscourse treebank.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 1-Volume 1, pages 343?351.Association for Computational Linguistics.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.A pdtb-styled end-to-end discourse parser.
NaturalLanguage Engineering, pages 1?34.Ankur Parikh, Shay B Cohen, and Eric Xing.
2014.Spectral unsupervised parsing with additive treemetrics.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers.
Association for ComputationalLinguistics.Natalia Ponomareva, Paolo Rosso, Ferr?an Pla, and An-tonio Molina.
2007.
Conditional random fields vs.hidden markov models in a biomedical named en-tity recognition task.
In Proc.
of Int.
Conf.
RecentAdvances in Natural Language Processing, RANLP,pages 479?483.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind K Joshi, and Bon-nie L Webber.
2008.
The penn discourse treebank2.0.
In LREC.
Citeseer.Evan Sandhaus.
2008.
The new york times annotatedcorpus ldc2008t19.
Linguistic Data Consortium.Bonnie Webber, Markus Egg, and Valia Kordoni.2012.
Discourse structure and language technology.Natural Language Engineering, 18(4):437?490.93
