Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 391?401,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsZero-shot Entity Extraction from Web PagesPanupong PasupatComputer Science DepartmentStanford Universityppasupat@cs.stanford.eduPercy LiangComputer Science DepartmentStanford Universitypliang@cs.stanford.eduAbstractIn order to extract entities of a fine-grainedcategory from semi-structured data in webpages, existing information extraction sys-tems rely on seed examples or redundancyacross multiple web pages.
In this paper,we consider a new zero-shot learning taskof extracting entities specified by a naturallanguage query (in place of seeds) givenonly a single web page.
Our approach de-fines a log-linear model over latent extrac-tion predicates, which select lists of enti-ties from the web page.
The main chal-lenge is to define features on widely vary-ing candidate entity lists.
We tackle this byabstracting list elements and using aggre-gate statistics to define features.
Finally,we created a new dataset of diverse queriesand web pages, and show that our systemachieves significantly better accuracy thana natural baseline.1 IntroductionWe consider the task of extracting entities ofa given category (e.g., hiking trails) from webpages.
Previous approaches either (i) assume thatthe same entities appear on multiple web pages,or (ii) require information such as seed examples(Etzioni et al, 2005; Wang and Cohen, 2009;Dalvi et al, 2012).
These approaches work wellfor common categories but encounter data sparsityproblems for more specific categories, such as theproducts of a small company or the dishes at a lo-cal restaurant.
In this context, we may have only asingle web page that contains the information weneed and no seed examples.In this paper, we propose a novel task, zero-shot entity extraction, where the specificationof the desired entities is provided as a naturallanguage query.
Given a query (e.g., hikingseedsAvalon Super LoopHilton Area traditionalanswersAvalon Super LoopHilton AreaWildlands Loop...queryhiking trailsnear Baltimore our systemanswersAvalon Super LoopHilton AreaWildlands Loop...web pagesweb pagesweb pagesweb pageFigure 1: Entity extraction typically requires ad-ditional knowledge such as a small set of seed ex-amples or depends on multiple web pages.
In oursetting, we take as input a natural language queryand extract entities from a single web page.trails near Baltimore) and a web page (e.g.,http://www.everytrail.com/best/hiking-baltimore-maryland), the goal isto extract all entities corresponding to the queryon that page (e.g., Avalon Super Loop, etc.
).Figure 1 summarizes the task setup.The task introduces two challenges.
Given asingle web page to extract entities from, we canno longer rely on the redundancy of entities acrossmultiple web pages.
Furthermore, in the zero-shotlearning paradigm (Larochelle et al, 2008), whereentire categories might be unseen during training,the system must generalize to new queries and webpages without the additional aid of seed examples.To tackle these challenges, we cast the task asa structured prediction problem where the inputis the query and the web page, and the output isa list of entities, mediated by a latent extractionpredicate.
To generalize across different inputs,we rely on two types of features: structural fea-tures, which look at the layout and placement ofthe entities being extracted; and denotation fea-391tures, which look at the list of entities as a wholeand assess their linguistic coherence.
When defin-ing features on lists, one technical challenge is be-ing robust to widely varying list sizes.
We ap-proach this challenge by defining features over ahistogram of abstract tokens derived from the listelements.For evaluation, we created the OPENWEBdataset comprising natural language queries fromthe Google Suggest API and diverse web pages re-turned from web search.
Despite the variety ofqueries and web pages, our system still achievesa test accuracy of 40.5% and an accuracy at 5 of55.8%.2 Problem statementWe define the zero-shot entity extraction task asfollows: let x be a natural language query (e.g.,hiking trails near Baltimore), and w be a webpage.
Our goal is to construct a mapping from(x,w) to a list of entities y (e.g., [Avalon SuperLoop, Patapsco Valley State Park, .
.
. ])
which areextracted from the web page.Ideally, we would want our data to be anno-tated with the correct entity lists y, but this wouldbe very expensive to obtain.
We instead defineeach training and test example as a triple (x,w, c),where the compatibility function c maps each y toc(y) ?
{0, 1} denoting the (approximate) correct-ness of the list y.
In this paper, an entity list y iscompatible (c(y) = 1) when the first, second, andlast elements of y match the annotation; otherwise,it is incompatible (c(y) = 0).2.1 DatasetTo experiment with a diverse set of queries andweb pages, we created a new dataset, OPENWEB,using web pages from Google search results.1Weuse the method from Berant et al (2013) to gen-erate search queries by performing a breadth-firstsearch over the query space.
Specifically, weuse the Google Suggest API, which takes a par-tial query (e.g., ?list of movies?)
and out-puts several complete queries (e.g., ?list of hor-ror movies?).
We start with seed partial queries?list of ?
?
where ?
is one or two initial let-ters.
In each step, we call the Google Suggest APIon the partial queries to obtain complete queries,1The OPENWEB dataset and our code base are availablefor download at http://www-nlp.stanford.edu/software/web-entity-extractor-ACL2014.Full query New partial querieslist of X IN Y list of Xwhere IN is a preposition list of X(list of [hotels]Xin [Guam]Y) list of X INlist of IN Ylist of X CC Y list of Xwhere CC is a conjunction list of X(list of [food]Xand [drink]Y) list of Ylist of Ylist of X w list of w(list of [good 2012]X[movies]w) list of wlist of XTable 1: Rules for generating new partial queriesfrom complete queries.
(X and Y are sequencesof words; w is a single word.
)and then apply the transformation rules in Table 1to generate more partial queries from completequeries.
We run the procedure until we obtained100K queries.Afterwards, we downloaded the top 2?3 Googlesearch results of each query, sanitized the webpages, and randomly submitted 8000 query / webpage pairs to Amazon Mechanical Turk (AMT).Each AMT worker must either mark the web pageas irrelevant or extract the first, second, and lastentities from the page.
We only included exam-ples where at least two AMT workers agreed onthe answer.The resulting OPENWEB dataset consists of2773 examples from 2269 distinct queries.Among these queries, there are 894 headwordsranging from common categories (e.g., movies,companies, characters) to more specific ones (e.g.,enzymes, proverbs, headgears).
The dataset con-tains web pages from 1438 web domains, of which83% appear only once in our dataset.Figure 2 shows some queries and web pagesfrom the OPENWEB dataset.
Besides the widerange of queries, another main challenge of thedataset comes from the diverse data representa-tion formats, including complex tables, grids, lists,headings, and paragraphs.3 ApproachFigure 3 shows the framework of our system.Given a query x and a web page w, the systemgenerates a set Z(w) of extraction predicates zwhich can extract entities from semi-structureddata in w. Section 3.1 describes extraction pred-icates in more detail.
Afterwards, the systemchooses z ?
Z(w) that maximizes the modelprobability p?
(z | x,w), and then executes z on392Queriesairlines of italynatural causes of global warminglsu football coachesbf3 submachine gunsbadminton tournamentsfoods high in dhatechnical colleges in south carolinasongs on glee season 5singers who use auto tunesan francisco radio stationsactors from bostonExamples (web page, query)airlines of italy natural causes of global warming lsu football coachesFigure 2: Some examples illustrating the diversity of queries and web pages from the OPENWEB dataset.x Generation wZModelz Executionyhiking trailsnear Baltimorehtmlhead...body.../html[1]/body[1]/table[2]/tr/td[1][Avalon Super Loop, Hilton Area, ...]Figure 3: An overview of our system.
The systemuses the input query x and web page w to producea list of entities y via an extraction predicate z.w to get the list of entities y = JzKw.
Section 3.2describes the model and the training procedure,while Section 3.3 presents the features used in ourmodel.3.1 Extraction predicatesWe represent each web page w as a DOM tree, acommon representation among wrapper inductionand web information extraction systems (Sahuguetand Azavant, 1999; Liu et al, 2000; Crescenzi etal., 2001).
The text of any DOM tree node that isshorter than 140 characters is a candidate entity.However, without further restrictions, the numberof possible entity lists grows exponentially withthe number of candidate entities.To make the problem tractable, we introduce anextraction predicate z as an intermediate represen-tation for extracting entities from w. In our sys-tem, we let an extraction predicate be a simplifiedXML path (XPath) such as/html[1]/body[1]/table[2]/tr/td[1]Informally, an extraction predicate is a list ofpath entries.
Each path entry is either a tag (e.g.,tr), which selects all children with that tag; or atag and an index i (e.g., td[1]), which selectsonly the ith child with that tag.
The denotationy = JzKwof an extraction predicate z is the list ofentities selected by the XPath.
Figure 4 illustratesthe execution of the extraction predicate above ona DOM tree.In the literature, many information extractionsystems employ more versatile extraction predi-cates (Wang and Cohen, 2009; Fumarola et al,2011).
However, despite the simplicity, we areable to find an extraction predicate that extractsa compatible entity list in 69.7% of the develop-ment examples.
In some examples, we cannot ex-tract a compatible list due to unrecoverable issuessuch as incorrect annotation.
Section 4.4 providesa detailed analysis of these issues.
Additionally,extraction predicates can be easily extended to in-crease the coverage.
For example, by introduc-ing new index types [1:] (selects all but the firstnode) and [:-1] (selects all but the last node),we can increase the coverage to 76.2%.Extraction predicate generation.
We generatea set Z(w) of extraction predicates for a givenweb page w as follows.
For each node inthe DOM tree, we find an extraction predicatewhich selects only that node, and then gener-alizes the predicate by removing any subset ofthe indices of the last k path entries.
For in-stance, when k = 2, an extraction predicateending in .../tr[5]/td[2] will be general-ized to .../tr[5]/td[2], .../tr/td[2],.../tr[5]/td, and .../tr/td.
In all ex-periments, we use k = 8, which gives at most 28generalized predicates for each original predicate.This generalization step allows the system to se-lect multiple nodes with the same structure (e.g.,393DOM tree whtmlhead bodytabletrtdHome..tdExpl..tdMobi..tdCrea..h1Hiki..tabletrthName..thLoca..trtdAval..td12.7..... trtdGove..td3.1 ..Extraction predicate z/html[1]/body[1]/table[2]/tr/td[1]Rendered web pageHome Explore Mobile Apps Create TripHiking near Baltimore, MarylandName LengthAvalon Super Loop 12.7 milesHilton Area 7.8 milesAvalon Loop 9.4 milesWildlands Loop 4.4 milesMckeldin Area 16.7 milesGreenbury Point 3.7 milesGoverner Bridge Natural Area 3.1 milesFigure 4: A simplified example of a DOM tree w and an extraction predicate z, which selects a list ofentity strings y = JzKwfrom the page (highlighted in red).table cells from the same column or list items fromthe same section of the page).Out of all generalized extraction predicates, weretain the ones that extract at least two entitiesfrom w. Note that several extraction predicatesmay select the same list of nodes and thus producethe same list of entities.The procedure above gives a manageable num-ber of extraction predicates.
Among the devel-opment examples of the OPENWEB dataset, wegenerate an average of 8449 extraction predicatesper example, which evaluate to an average of 1209unique entity lists.3.2 ModelingGiven a query x and a web page w, we definea log-linear distribution over all extraction predi-cates z ?
Z(w) asp?
(z | x,w) ?
exp{?>?
(x,w, z)}, (1)where ?
?
Rdis the parameter vector and?
(x,w, z) is the feature vector, which will be de-fined in Section 3.3.To train the model, we find a parameter vec-tor ?
that maximizes the regularized log marginalprobability of the compatibility function being sat-isfied.
In other words, given training data D ={(x(i), w(i), c(i))}ni=1, we find ?
that maximizesn?i=1log p?
(c(i)= 1 | x(i), w(i))??2???22wherep?
(c = 1 | x,w) =?z?Z(w)p?
(z | x,w) ?
c(JzKw).Note that c(JzKw) = 1 when the entity list y =JzKwselected by z is compatible with the annota-tion; otherwise, c(JzKw) = 0.We use AdaGrad, an online gradient descentwith an adaptive per-feature step size (Duchi et al,2010), making 5 passes over the training data.
Weuse ?
= 0.01 obtained from cross-validation forall experiments.3.3 FeaturesTo construct the log-linear model, we define a fea-ture vector ?
(x,w, z) for each query x, web pagew, and extraction predicate z.
The final featurevector is the concatenation of structural features?s(w, z), which consider the selected nodes inthe DOM tree, and denotation features ?d(x, y),which look at the extracted entities.We will use the query hiking trails near Balti-more and the web page in Figure 4 as a runningexample.
Figure 5 lists some features extractedfrom the example.3.3.1 Recipe for defining features on listsOne main focus of our work is finding good fea-ture representations for a list of objects (DOM treenodes for structural features and entity strings fordenotation features).
One approach is to define thefeature vector of a list to be the sum of the featurevectors of individual elements.
This is commonlydone in structured prediction, where the elementsare local configurations (e.g., rule applications inparsing).
However, this approach raises a normal-ization issue when we have to compare and ranklists of drastically different sizes.As an alternative, we propose a recipe for gen-erating features from a list as follows:394htmlhead bodytable...h1 tabletrth thtrtd tdtrtd td... trtd tdStructural feature ValueFeatures on selected nodes:TAG-MAJORITY = td 1INDEX-ENTROPY 0.0Features on parent nodes:CHILDRENCOUNT-MAJORITY = 2 1PARENT-SINGLE 1INDEX-ENTROPY 1.0HEADHOLE (The first node is skipped) 1Features on grandparent nodes:PAGECOVERAGE 0.6. .
.
.
.
.Selected entitiesAvalon Super LoopHilton AreaAvalon LoopWildlands LoopMckeldin AreaGreenbury PointGoverner Bridge Natural AreaDenotation feature ValueWORDSCOUNT-MEAN 2.42PHRASESHAPE-MAJORITY = Aa Aa 1PHRASESHAPE-MAJORITYRATIO 0.71WORDSHAPE-MAJORITY = Aa 1PHRASEPOS-MAJORITY = NNP NN 1LASTWORD-ENTROPY 0.74WORDPOS = NN (normalized count) 0.53. .
.
.
.
.Figure 5: A small subset of features from the example hiking trails near Baltimore in Figure 4.A B C D E1 2 0 1 00 1 2histogramAbstractionAggregation EntropyMajorityMajorityRatioSingle(Mean)(Variance)Figure 6: The recipe for defining features on alist of objects: (i) the abstraction step converts listelements into abstract tokens; (ii) the aggregationstep defines features using the histogram of the ab-stract tokens.Step 1: Abstraction.
We map each list elementinto an abstract token.
For example, we can mapeach DOM tree node onto an integer equal to thenumber of children, or map each entity string ontoits part-of-speech tag sequence.Step 2: Aggregation.
We create a histogram ofthe abstract tokens and define features on proper-ties of the histogram.
Generally, we use ENTROPY(entropy normalized to the maximum value of 1),MAJORITY (mode), MAJORITYRATIO (percent-age of tokens sharing the majority value), andSINGLE (whether all tokens are identical).
Forabstract tokens with finitely many possible values(e.g., part-of-speech), we also use the normalizedhistogram count of each possible value as a fea-ture.
And for real-valued abstract tokens, we alsouse the mean and the standard deviation.
In theactual system, we convert real-valued features (en-tropy, histogram count, mean, and standard devia-tion) into indicator features by binning.Figure 6 summarizes the steps explained above.We use this recipe for defining both structural anddenotation features, which are discussed below.3.3.2 Structural featuresAlthough different web pages represent data indifferent formats, they still share some commonhierarchical structures in the DOM tree.
To cap-ture this, we define structural features ?s(w, z),which consider the properties of the selected nodesin the DOM tree, as follows:Features on selected nodes.
We apply ourrecipe on the list of nodes in w selected by z usingthe following abstract tokens:?
TAG, ID, CLASS, etc.
(HTML attributes)?
CHILDRENCOUNT and SIBLINGSCOUNT(number of children and siblings)?
INDEX (position among its siblings)?
PARENT (parent node; e.g., PARENT-SINGLEmeans that all nodes share the same parent.
)Additionally, we define the following featuresbased on the coverage of all selected nodes:?
NOHOLE, HEADHOLE, etc.
(node coveragein the same DOM tree level; e.g., HEAD-HOLE activates when the first sibling of theselected nodes is not selected.)395?
PAGECOVERAGE (node coverage relative tothe entire tree; we use depth-first traversaltimestamps to estimate the fraction of nodesin the subtrees of the selected nodes.
)Features on ancestor nodes.
We also define thesame feature set on the list of ancestors of the se-lected nodes in the DOM tree.
In our experiments,we traverse up to 5 levels of ancestors and definefeatures from the nodes in each level.3.3.3 Denotation featuresStructural features are not powerful enough to dis-tinguish between entity lists appearing in similarstructures such as columns of the same table orfields of the same record.
To solve this ambiguity,we introduce denotation features ?d(x, y) whichconsiders the coherence or appropriateness of theselected entity strings y = JzKw.We observe that the correct entities often sharesome linguistic statistics.
For instance, entities inmany categories (e.g., people and place names)usually have only 2?3 word tokens, most of whichare proper nouns.
On the other hand, randomwords on the web page tend to have more diverselengths and part-of-speech tags.We apply our recipe on the list of selected enti-ties using the following abstract tokens:?
WORDSCOUNT (number of words)?
PHRASESHAPE (abstract shape of the phrase;e.g., Barack Obama becomes Aa Aa)?
WORDSHAPE (abstract shape of each word;the number of abstract tokens will be the totalnumber of words over all selected entities)?
FIRSTWORD and LASTWORD?
PHRASEPOS and WORDPOS (part-of-speech tags for whole phrases and individualwords)4 ExperimentsIn this section we evaluate our system on theOPENWEB dataset.4.1 Evaluation metricsAccuracy.
As the main metric, we use a notionof accuracy based on compatibility; specifically,we define the accuracy as the fraction of exampleswhere the system predicts a compatible entity listas defined in Section 2.
We also report accuracyat 5, the fraction of examples where the top fivepredictions contain a compatible entity list.Path suffix pattern (multiset) Count{a, table, tbody, td[*], tr} 1792{a, tbody, td[*], text, tr} 1591{a, table[*], tbody, td[*], tr} 1325{div, table, tbody, td[*], tr} 1259{b, div, div, div, div[*]} 1156{div[*], table, tbody, td[*], tr} 1059{div, table[*], tbody, td[*], tr} 844{table, tbody, td[*], text, tr} 828{div[*], table[*], tbody, td[*], tr} 793{a, table, tbody, td, tr} 743Table 2: Top 10 path suffix patterns found by thebaseline learner in the development data.
Sincewe allow path entries to be permuted, each suffixpattern is represented by a multiset of path entries.The notation [*] denotes any path entry index.To see how our compatibility-based accuracytracks exact correctness, we sampled 100 webpages which have at least one valid extractionpredicate and manually annotated the full list ofentities.
We found that in 85% of the examples,the longest compatible list y is the correct list ofentities, and many lists in the remaining 15% missthe correct list by only a few entities.Oracle.
In some examples, our system cannotfind any list of entities that is compatible with thegold annotation.
The oracle score is the fractionof examples in which the system can find at leastone compatible list.4.2 BaselineAs a baseline, we list the suffixes of the cor-rect extraction predicates in the training data, andthen sort the resulting suffix patterns by frequency.To improve generalization, we treat path entrieswith different indices (e.g., td[1] vs. td[2]) asequivalent and allow path entries to be permuted.Table 2 lists the top 10 suffix patterns from the de-velopment data.
At test time, we choose an extrac-tion predicate with the most frequent suffix pat-tern.
The baseline should work considerably wellif the web pages were relatively homogeneous.4.3 Main resultsWe held out 30% of the dataset as test data.
For theresults on development data, we report the averageacross 10 random 80-20 splits.
Table 3 shows theresults.
The system gets an accuracy of 41.1% and40.5% for the development and test data, respec-tively.
If we consider the top 5 lists of entities, theaccuracy increases to 58.4% on the developmentdata and 55.8% on the test data.396Development data Test dataAcc A@5 Acc A@5Baseline 10.8 ?
1.3 25.6 ?
2.0 10.3 20.9Our system 41.1 ?
3.4 58.4 ?
2.7 40.5 55.8Oracle 68.7 ?
2.4 68.7 ?
2.4 66.6 66.6Table 3: Main results on the OPENWEB datasetusing the default set of features.
(Acc = accuracy,A@5 = accuracy at 5)4.4 Error analysisWe now investigate the errors made by our systemusing the development data.
We classify the er-rors into two types: (i) coverage errors, which arewhen the system cannot find any entity list satis-fying the compatibility function; and (ii) rankingerrors, which are when a compatible list of entitiesexists, but the system outputs an incompatible list.Tables 4 and 5 show the breakdown of cover-age and ranking errors from an experiment on thedevelopment data.Analysis of coverage errors.
From Table 4,about 36% of coverage errors happen when theextraction predicate for the correct entities alsocaptures unrelated parts of the web page (ReasonC1).
For example, many Wikipedia articles havethe See Also section that lists related articles in anunordered list (/ul/li/a), which causes a prob-lem when the entities are also represented in thesame format.Another main source of errors is the in-consistency in HTML tag usage (Reason C2).For instance, some web pages use <b> and<strong> tags for bold texts interchangeably,or switch between <b><a>...</a></b> and<a><b>...</b></a> across entities.
We ex-pect that this problem can be solved by normaliz-ing the web page, using an alternative web pagerepresentation (Cohen et al, 2002; Wang and Co-hen, 2009; Fumarola et al, 2011), or leveragingmore expressive extraction predicates (Dalvi et al,2011).One interesting source of errors is Reason C3,where we need to filter the selected entities tomatch the complex requirement in the query.
Forexample, the query tech companies in China re-quires the system to select only the companynames with China in the corresponding locationcolumn.
To handle such queries, we need a deeperunderstanding of the relation between the linguis-tic structure of the query and the hierarchicalstructure of the web page.
Tackling this error re-Setting Acc A@5All features 41.1 ?
3.4 58.4 ?
2.7Oracle 68.7 ?
2.4 68.7 ?
2.4(Section 4.5)Structural features only 36.2 ?
1.9 54.5 ?
2.5Denotation features only 19.8 ?
2.5 41.7 ?
2.7(Section 4.6)Structural + query-denotation 41.7 ?
2.5 58.1 ?
2.4Query-denotation features only 25.0 ?
2.3 48.0 ?
2.7Concat.
a random web page +structural + denotation 19.3 ?
2.6 41.2 ?
2.3Concat.
a random web page +structural + query-denotation 29.2 ?
1.7 49.2 ?
2.2(Section 4.7)Add 1 seed entity 52.9 ?
3.0 66.5 ?
2.5Table 6: System accuracy with different featureand input settings on the development data.
(Acc= accuracy, A@5 = accuracy at 5)quires compositionality and is critical to general-ize to more complex queries.Analysis of ranking errors.
From Table 5, alarge number of errors are attributed to the systemselecting non-content elements such as navigationlinks and content headings (Reason R1).
Featureanalysis reveals that both structural and linguis-tic statistics of these non-content elements can bemore coherent than those of the correct entities.We suspect that since many of our features try tocapture the coherence of entities, the system some-times erroneously favors the more homonogenousnon-content parts of the page.
To disfavor theseparts, One possible solution is to add visual fea-tures that capture how the web page is renderedand favor more salient parts of the page.
(Liu et al,2003; Song et al, 2004; Zhu et al, 2005; Zheng etal., 2007).4.5 Feature variationsWe now investigate the contribution of each fea-ture type.
The ablation results on the developmentset over 10 random splits are shown in Table 6.We observe that denotation features improves ac-curacy on top of structural features.Table 7 shows an example of an error that iseliminated by each feature type.
Generally, ifthe entities are represented as records (e.g., rowsof a table), then denotation features will help thesystem select the correct field from each record.On the other hand, structural features prevent thesystem from selecting random entities outside themain part of the page.397Reason Short example CountC1 Answers and contextual elements are selectedby the same extraction predicate.Select entries in See Also section in addition to the con-tent because they are all list entries.48C2 HTML tag usage is inconsistent.
The page uses both b and strong for headers.
16C3 The query applies to only some sections of thematching entities.Need to select only companies in China from the tableof all Asian companies.20C4 Answers are embedded in running text.
Answers are in a comma-separated list.
13C5 Text normalization issues.
Selected Silent Night Lyrics instead of Silent Night.
19C6 Other issues.
Incorrect annotation.
/ Entities are permuted when theweb page is rendered.
/ etc.18Total 134Table 4: Breakdown of coverage errors from the development data.Reason Short example CountR1 Select non-content strings.
Select navigation links, headers, footers, or sidebars.
25R2 Select entities from a wrong field.
Select book authors instead of book names.
22R3 Select entities from the wrong section(s).
For the query schools in Texas, select all schools on thepage, or select the schools in Alabama instead.19R4 Also select headers or footers.
Select the table header in addition to the answers.
7R5 Select only entities with a particular formatting.
From a list of answers, select only anchored (a) entities.
4R6 Select headings instead of the contents or viceversa.Select the categories of rums in h2 tags instead of therum names in the tables.2R7 Other issues.
Incorrect annotation.
/ Multiple sets of answers appearon the same page.
/ etc.9Total 88Table 5: Breakdown of ranking errors from the development data.All features Structural only Denotation onlyThe Sun CIRC: 2,279,492 Paperboy AustraliaDaily Mail CIRC: 1,821,684 Paperboy UKDaily Mirror CIRC: 1,032,144 Paperboy Home Page.
.
.
.
.
.
.
.
.Table 7: System outputs for the query UK news-papers with different feature sets.
Without deno-tation features, the system selects the daily circu-lation of each newspaper instead of the newspapernames.
And without structural features, the sys-tem selects the hidden navigation links from thetop of the page.4.6 Incorporating query informationSo far, note that all our features depend only onthe extraction predicate z and not the input queryx.
Remarkably, we were still able to obtain rea-sonable results.
One explanation is that since weobtained the web pages from a search engine, themost prominent entities on the web pages, such asentities in table cells in the middle of the page, arelikely to be good independent of the query.However, different queries often denote enti-ties with different linguistic properties.
For exam-ple, queries mayors of Chicago and universities inChicago will produce entities of different lengths,part-of-speech sequences, and word distributions.This suggests incorporating features that dependon the query.To explore the potential of query informa-tion, we conduct the following oracle experi-ment.
We replace each denotation feature f(y)with a corresponding query-denotation feature(f(y), g(x)), where g(x) is the category of thequery x.
We manually classified all queries in ourdataset into 7 categories: person, media title, loca-tion/organization, abtract entity, word/phrase, ob-ject name, and miscellaneous.Table 8 shows some examples where addingthese query-denotation features improves the se-lected entity lists by favoring answers that aremore suitable to the query category.
However, Ta-ble 6 shows that these new features do not signifi-cantly improve the accuracy of our original systemon the development data.We suspect that any gains offered by the query-denotation features are subsumed by the structuralfeatures.
To test this hypothesis, we conductedtwo experiments, the results of which are shownin Table 6.
First, we removed structural featuresand found that using query-denotation features im-proves accuracy significantly over using denota-tion features alone from 19.8% to 25.0%.
Second,we created a modified dataset where the web pagein each example is a concatenation of the orig-inal web page and an unrelated web page.
On398Query euclid?s elements book titles soft drugs professional athletes with concussionsDefaultfeatures?Prematter?, ?Book I.?,?Book II.
?, ?Book III.
?, .
.
.
?Hard drugs?, ?Soft drugs?,?Some drugs cannot beclassified that way?, .
.
.
?Pistons-Knicks Game Becomes Siteof Incredible Dance Battle?, ?TorontoMayor Rob Ford Attends .
.
.
?, .
.
.Structural+ Query-Denotation(category = media title)?Book I.
The fundamentals .
.
.
?,?Book II.
Geometric algebra?, .
.
.
(category = object name)?methamphetamine?,?psilocybin?, ?caffeine?
(category = person)?Mike Richter?, ?Stu Grimson?,?Geoff Courtnall?, .
.
.Table 8: System outputs after changing denotation features into query-denotation features.this modified dataset, the prominent entities maynot be the answers to the query.
Here, query-denotation features improves accuracy over deno-tation features alone from 19.3% to 29.2%.4.7 Comparison with other problem settingsSince zero-shot entity extraction is a new task,we cannot directly compare our system with othersystems.
However, we can mimic the settings ofother tasks.
In one experiment, we augment eachinput query with a single seed entity (the secondannotated entity in our experiments); this settingis suggestive of Wang and Cohen (2009).
Table 6shows that this augmentation increases accuracyfrom 41.1% to 52.9%, suggesting that our sys-tem can perform substantially better with a smallamount of additional supervision.5 DiscussionOur work shares a base with the wrapper induc-tion literature (Kushmerick, 1997) in that it lever-ages regularities of web page structures.
However,wrapper induction usually focuses on a small setof web domains, where the web pages in each do-main follow a fixed template (Muslea et al, 2001;Crescenzi et al, 2001; Cohen et al, 2002; Arasuand Garcia-Molina, 2003).
Later work in web dataextraction attempts to generalize across differentweb pages, but relies on either restricted data for-mats (Wong et al, 2009) or prior knowledge ofweb page structures with respect to the type of datato extract (Zhang et al, 2013).In our case, we only have the natural languagequery, which presents the more difficult problemof associating the entity class in the query (e.g.,hiking trails) to concrete entities (e.g., Avalon Su-per Loop).
In contrast to information extractionsystems that extract homogeneous records fromweb pages (Liu et al, 2003; Zheng et al, 2009),our system must choose the correct field from eachrecord and also identify the relevant part of thepage based on the query.Another related line of work is information ex-traction from text, which relies on natural lan-guage patterns to extract categories and relationsof entities.
One classic example is Hearst pat-terns (Hearst, 1992; Etzioni et al, 2005), whichcan learn new entities and extraction patterns fromseed examples.
More recent approaches alsoleverage semi-structured data to obtain more ro-bust extraction patterns (Mintz et al, 2009; Hoff-mann et al, 2011; Surdeanu et al, 2012; Riedelet al, 2013).
Although our work focuses on semi-structured web pages rather than raw text, we uselinguistic patterns of queries and entities as a sig-nal for extracting appropriate answers.Additionally, our efforts can be viewed as build-ing a lexicon on the fly.
In recent years, therehas been a drive to scale semantic parsing to largedatabases such as Freebase (Cai and Yates, 2013;Berant et al, 2013; Kwiatkowski et al, 2013).However, despite the best efforts of informationextraction, such databases will always lag behindthe open web.
For example, Berant et al (2013)found that less than 10% of naturally occurringquestions are answerable by a simple Freebasequery.
By using the semi-structured data from theweb as a knowledge base, we hope to increase factcoverage for semantic parsing.Finally, as pointed out in the error analysis, weneed to filter or aggregate the selected entities forcomplex queries (e.g., tech companies in China fora web page with all Asian tech companies).
In fu-ture work, we would like to explore the issue ofcompositionality in queries by aligning linguisticstructures in natural language with the relative po-sition of entities on web pages.AcknowledgementsWe gratefully acknowledge the support of theGoogle Natural Language Understanding FocusedProgram.
In addition, we would like to thankanonymous reviewers for their helpful comments.399ReferencesA.
Arasu and H. Garcia-Molina.
2003.
Extractingstructured data from web pages.
In ACM SIGMODinternational conference on Management of data,pages 337?348.J.
Berant, A. Chou, R. Frostig, and P. Liang.
2013.Semantic parsing on Freebase from question-answerpairs.
In Empirical Methods in Natural LanguageProcessing (EMNLP).Q.
Cai and A. Yates.
2013.
Large-scale semantic pars-ing via schema matching and lexicon extension.
InAssociation for Computational Linguistics (ACL).W.
W. Cohen, M. Hurst, and L. S. Jensen.
2002.
Aflexible learning system for wrapping tables and listsin HTML documents.
In World Wide Web (WWW),pages 232?241.V.
Crescenzi, G. Mecca, P. Merialdo, et al 2001.Roadrunner: Towards automatic data extractionfrom large web sites.
In VLDB, volume 1, pages109?118.N.
Dalvi, R. Kumar, and M. Soliman.
2011.
Auto-matic wrappers for large scale web extraction.
Pro-ceedings of the VLDB Endowment, 4(4):219?230.B.
Dalvi, W. Cohen, and J. Callan.
2012.
Websets:Extracting sets of entities from the web using unsu-pervised information extraction.
In Web Search andData Mining (WSDM), pages 243?252.J.
Duchi, E. Hazan, and Y.
Singer.
2010.
Adaptive sub-gradient methods for online learning and stochasticoptimization.
In Conference on Learning Theory(COLT).O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. S. Weld, and A. Yates.2005.
Unsupervised named-entity extraction fromthe web: An experimental study.
Artificial Intelli-gence, 165(1):91?134.F.
Fumarola, T. Weninger, R. Barber, D. Malerba, andJ.
Han.
2011.
Extracting general lists from web doc-uments: A hybrid approach.
Modern Approaches inApplied Intelligence Springer.M.
A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In InterationalConference on Computational linguistics, pages539?545.R.
Hoffmann, C. Zhang, X. Ling, L. S. Zettlemoyer,and D. S. Weld.
2011.
Knowledge-based weak su-pervision for information extraction of overlappingrelations.
In Association for Computational Lin-guistics (ACL), pages 541?550.N.
Kushmerick.
1997.
Wrapper induction for informa-tion extraction.
Ph.D. thesis, University of Washing-ton.T.
Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.2013.
Scaling semantic parsers with on-the-fly on-tology matching.
In Empirical Methods in NaturalLanguage Processing (EMNLP).H.
Larochelle, D. Erhan, and Y. Bengio.
2008.
Zero-data learning of new tasks.
In AAAI, volume 8,pages 646?651.L.
Liu, C. Pu, and W. Han.
2000.
XWRAP: An XML-enabled wrapper construction system for web infor-mation sources.
In Data Engineering, 2000.
Pro-ceedings.
16th International Conference on, pages611?621.B.
Liu, R. Grossman, and Y. Zhai.
2003.
Mining datarecords in web pages.
In Proceedings of the ninthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 601?606.M.
Mintz, S. Bills, R. Snow, and D. Jurafsky.
2009.Distant supervision for relation extraction withoutlabeled data.
In Association for Computational Lin-guistics (ACL), pages 1003?1011.I.
Muslea, S. Minton, and C. A. Knoblock.
2001.
Hi-erarchical wrapper induction for semistructured in-formation sources.
Autonomous Agents and Multi-Agent Systems, 4(1):93?114.S.
Riedel, L. Yao, and A. McCallum.
2013.
Re-lation extraction with matrix factorization and uni-versal schemas.
In North American Association forComputational Linguistics (NAACL).A.
Sahuguet and F. Azavant.
1999.
WysiWyg webwrapper factory (W4F).
In WWW Conference.R.
Song, H. Liu, J. Wen, and W. Ma.
2004.
Learningblock importance models for web pages.
In WorldWide Web (WWW), pages 203?211.M.
Surdeanu, J. Tibshirani, R. Nallapati, and C. D.Manning.
2012.
Multi-instance multi-label learningfor relation extraction.
In Empirical Methods in Nat-ural Language Processing and Computational Nat-ural Language Learning (EMNLP/CoNLL), pages455?465.R.
C. Wang and W. W. Cohen.
2009.
Character-levelanalysis of semi-structured documents for set expan-sion.
In Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1503?1512.Y.
W. Wong, D. Widdows, T. Lokovic, and K. Nigam.2009.
Scalable attribute-value extraction from semi-structured text.
In IEEE International Conferenceon Data Mining Workshops, pages 302?307.Z.
Zhang, K. Q. Zhu, H. Wang, and H. Li.
2013.
Au-tomatic extraction of top-k lists from the web.
InInternational Conference on Data Engineering.S.
Zheng, R. Song, and J. Wen.
2007.
Template-independent news extraction based on visual consis-tency.
In AAAI, volume 7, pages 1507?1513.400S.
Zheng, R. Song, J. Wen, and C. L. Giles.
2009.
Ef-ficient record-level wrapper induction.
In Proceed-ings of the 18th ACM conference on Information andknowledge management, pages 47?56.J.
Zhu, Z. Nie, J. Wen, B. Zhang, and W. Ma.
2005.2D conditional random fields for web informationextraction.
In International Conference on MachineLearning (ICML), pages 1044?1051.401
