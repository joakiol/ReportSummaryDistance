CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 223?227Manchester, August 2008Discriminative vs. Generative Approaches in Semantic Role LabelingDeniz YuretKoc?
Universitydyuret@ku.edu.trMehmet Ali YatbazKoc?
Universitymyatbaz@ku.edu.trAhmet Engin UralKoc?
Universityaural@ku.edu.trAbstractThis paper describes the two algorithmswe developed for the CoNLL 2008 SharedTask ?Joint learning of syntactic and se-mantic dependencies?.
Both algorithmsstart parsing the sentence using the samesyntactic parser.
The first algorithmuses machine learning methods to identifythe semantic dependencies in four stages:identification and labeling of predicates,identification and labeling of arguments.The second algorithm uses a generativeprobabilistic model, choosing the seman-tic dependencies that maximize the proba-bility with respect to the model.
A hybridalgorithm combining the best stages ofthe two algorithms attains 86.62% labeledsyntactic attachment accuracy, 73.24% la-beled semantic dependency F1 and 79.93%labeled macro F1 score for the combinedWSJ and Brown test sets1.1 IntroductionIn this paper we describe the system we developedfor the CoNLL 2008 Shared Task (Surdeanu et al,2008).
Section 2 describes our approach for iden-tifying syntactic dependencies.
For semantic rolelabeling (SRL), we pursued two independent ap-proaches.
Section 3 describes our first approach,where we treated predicate identification and la-beling, and argument identification and labeling asc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1These numbers are slightly higher than the official resultsdue to a small bug in our submission.four separate machine learning problems.
The fi-nal program consists of four stages, each stage tak-ing the answers from the previous stage as givenand performing its own identification or labelingtask based on a model generated from the train-ing set.
Section 4 describes our second approachwhere we used a generative model based on thejoint distribution of the predicate, the arguments,their labels and the syntactic dependencies con-necting them.
Section 5 summarizes our resultsand suggests possible improvements.2 Syntactic dependenciesWe used a non-projective dependency parser basedon spanning tree algorithms.
The parameters weredetermined based on the experimental results ofthe English task in (McDonald et al, 2005), i.e.
weused projective parsing and a first order feature setduring training.
Due to the new representation ofhyphenated words in both training and testing dataof our shared task and the absence of the gold partof speech (GPOS) column in the test data, the for-mat of the CoNLL08 shared task is slightly differ-ent from the format of the CoNLL05 shared task,which is supported by the McDonald?s parser.
Wereformatted the data accordingly.
The resulting la-beled attachment score on the test set is 87.39% forWSJ and 80.46% for Brown.3 The 4-stage discriminative approachOur first approach to SRL consists of four distinctstages: (1) predicate identification, (2) predicatelabeling, (3) argument identification, and (4) argu-ment labeling.A discriminative machine learning algorithm istrained for each stage using the gold input and out-put values from the training set.
The following sec-223tions describe the machine learning algorithm, thenature of its input/output, and the feature selectionprocess for each stage.
The performance of eachstage is compared to a most frequent class base-line and analyzed separately for the two test setsand for nouns and verbs.
In addition we look at theperformance given the input from the gold data vs.the input from the previous stage.3.1 Predicate identificationThe task of this stage is to determine whether agiven word is a nominal or a verb predicate usingthe dependency-parsed input.
As potential predi-cates we only consider words that appear as a pred-icate in the training data or have a correspondingPropBank or NomBank XML file.
The methodconstructs feature vectors for each occurrence ofa target word in the training and test data.
It as-signs class labels to the target words in the trainingdata depending on whether a target word is a pred-icate or not, and finally classifies the test data.
Weexperimented with combinations of the followingfeatures for each word in a 2k + 1 word windowaround the target: (1) POS(W): the part of speechof the word, (2) DEP(W, HEAD(W)): the syntac-tic dependency of the word, (3) LEMMA(W): thelemma of the word, (4) POS(HEAD(W)): the partof speech of the syntactic head.We empirically selected the combination thatgives the highest accuracy in terms of the precisionand recall scores on the development data.
Themethod achieved its highest score when we usedfeatures 1-3 for the target word and features 1-2 forthe neighbors in a [-3 +3] word window.
TiMBL(Daelemans et al, 2004) was used as the learningalgorithm.Table 1 (4-stage, All1) shows the results of ourlearning method on the WSJ and Brown test data.The noun and verb results are given separately(Verb1, Noun1).
To distinguish the mistakes com-ing from parsing we also give the results of ourmethod after the gold parse (4-stage-gold).
Our re-sults are significantly above the most frequent classbaseline which gives 72.3% on WSJ and 65.3% onBrown.3.2 Predicate labelingThe task of the second stage is deciding the correctframe for a word given that the word is a predicate.The input of the stage is 11-column data, where thecolumns contain part of speech, lemma and syn-tactic dependency for each word.
The first stage?sdecision for the frame is indicated by a string inthe predicate column.
The output of the stage issimply the replacement of that string with the cho-sen frame of the word.
The chosen frame of theword may be word.X, where X is a valid numberin PropBank or NomBank.The statistics of the training data show that bypicking the most frequent frame, the system canpick the correct frame in a large percent of thecases.
Thus we decided to use the most frequentframe baseline for this stage.
If the word is neverseen in the training, first frame of the word ispicked as default.In the test phase, the results are as the follow-ing; in the Brown data, assuming that the stage 1is gold, the score is 80.8%, noting that 11% of thepredicates are not seen in the training phase.
InWSJ, the score based on gold input is 88.3%, andonly 5% of the predicates are not seen in the train-ing phase.
Table 1 gives the full results for Stage 2(4-stage, Verb2, Noun2, All2).3.3 Argument identificationThe input data at this stage contains the syntac-tic dependencies, predicates and their frames.
Welook at the whole sentence for each predicate anddecide whether each word should be an argumentof that predicate or not.
We mark the words wechoose as arguments indicating which predicatethey belong to and leave the labeling of the ar-gument type to the next stage.
Thus, for eachpredicate-word pair we have a yes/no decision tomake.As input to the learning algorithm we experi-mented with representations of the syntactic de-pendency chain between the predicate and theargument at various levels of granularity.
Weidentified the syntactic dependency chain betweenthe predicate and each potential argument usingbreadth-first-search on the dependency tree.
Wetried to represent the chain using various subsetsof the following elements: the argument lemmaand part-of-speech, the predicate frame and part-of-speech, the parts-of-speech and syntactic de-pendencies of the intermediate words linking theargument to the predicate.The syntactic dependencies leading from the ar-gument to the predicate can be in the head-modifieror the modifier-head direction.
We marked the di-rection associated with each dependency relationin the chain description.
We also experimented224with using fine-grained and coarse-grained parts ofspeech.
The coarse-grained part of speech consistsof the first two characters of the Penn Treebankpart of speech given in the training set.We used a simple learning algorithm: choosethe answer that is correct for the majority of theinstances with the same chain description fromthe training set.
Not having enough detail in thechain description leaves crucial information outthat would help with the decision process, whereashaving too much detail results in bad classifica-tions due to sparse data.
In the end, neither the ar-gument lemma, nor the predicate frame improvedthe performance.
The best results were achievedwith a chain description including the coarse partsof speech and syntactic dependencies of each wordleading from the argument to the predicate.
Theresults are summarized in Table 1 (4-stage, Verb3,Noun3, All3).3.4 Argument labelingThe task of this stage is choosing the correct argu-ment tag for a modifier given that it is modifyinga particular predicate.
Input data format has ad-ditional columns indicating which words are argu-ments for which predicates.
There are 54 possiblevalues for a labeled argument.
As a baseline wetake the most frequent argument label in the train-ing data (All1) which gives 37.8% on the WSJ testset and 33.8% on the Brown test set.The features to determine the correct label of anargument are either lexical or syntactic.
In a fewcases, they are combined.
The following list givesthe set we have used.
Link is the type of the syntac-tic dependency.
Direction is left or right, depend-ing the location of the head and the modifier in thesentence.
LastLink is the type of the dependencyat the end of the dependency chain and firstLinkis type of the dependency at the beginning of thedependency chain.Feature1 : modifierStem + headStemFeature2 : modifierStem + coarsePosModifier +headStem + coarsePosHead + directionFeature3 : coarsePosModifier + headPos +firstLink + lastLink + directionFeature4: modifierStem + coarsePosModifierThe training phase includes building simple his-tograms based on four features.
Feature1 and Fea-ture2 are sparser than the other two features andare better features as they include lexical informa-tion.
Last two features are less sparse, coveringmost of the development data, i.e.
their histogramsgive non-zero values in the development phase.
Inorder to match all the instances in the developmentand use the semantic information, a cascade of thefeatures is implemented similar to the one done byGildea and Jurafsky(2002), although no weightingand a kind of back-off smoothing is used.
First,a match is searched in the histogram of the firstfeature, if not found it is searched in the followinghistogram.
After a match, the most frequent argu-ment with that match is returned.
Table 1 gives theperformance (4-stage, Verb4, Noun4, All4).4 The generative approachOne problem with the four-stage approach is thatthe later stages provide no feedback to the earlierones.
Thus, a frame chosen because of its highprior probability will not get corrected when wefail to find appropriate arguments for it.
A gen-erative model, on the other hand, does not sufferfrom this problem.
The probability of the wholeassignment, including predicates, arguments, andtheir labels, is evaluated together and the highestprobability combination is chosen.4.1 The generative modelFigure 1: The graphical model depicting the con-ditional independence assumptions.Our generative model specifies the distributionof the following random variables: P is the lemma(stem+pos) of a candidate predicate.
F is theframe chosen for the predicate (could be null).
Aiis the argument label of word i with respect to agiven predicate (could be null).
Wiis the lemma(stem+pos) of word i. Liis the syntactic depen-dency chain leading from word i to the given pred-icate (similar to Section 3.3).We consider each word in the sentence as a can-didate predicate and use the joint distribution of theabove variables to find the maximum probability F225WSJ Verb1 Verb2 Verb3 Verb4 Noun1 Noun2 Noun3 Noun4 All1 All2 All3 All44-stage 97.1 85.5 85.7 71.7 84.6 78.4 61.1 49.4 90.6 81.8 76.6 63.5generative 96.1 88.4 83.4 74.0 82.8 79.5 69.8 63.2 89.0 83.6 77.4 69.24-stage-gold 97.4 88.3 95.2 82.7 85.2 92.7 70.5 81.9 91.1 90.5 86.0 82.4generative-gold 96.3 92.6 91.1 88.0 83.4 95.5 80.7 86.9 89.4 94.0 86.7 87.5hybrid 97.1 89.3 85.7 74.7 84.6 80.9 70.9 64.0 90.6 84.9 79.5 70.2Brown Verb1 Verb2 Verb3 Verb4 Noun1 Noun2 Noun3 Noun4 All1 All2 All3 All44-stage 93.0 74.5 78.9 59.0 74.4 58.6 52.3 38.8 86.0 68.6 72.8 54.3generative 91.4 71.7 76.1 60.0 70.8 59.3 54.0 45.3 83.1 66.6 69.6 55.74-stage-gold 93.0 80.8 93.7 73.2 75.7 80.3 70.1 70.5 86.5 80.8 88.2 72.4generative-gold 91.6 80.6 85.8 78.05 71.2 85.9 70.5 75.1 83.5 82.6 81.8 77.1hybrid 93.0 73.3 78.9 60.4 74.4 62.9 57.6 47.5 86.0 69.3 73.4 57.0Table 1: The F1 scores for different datasets, models, stages, and predicate parts of speech.
The ?Verb?in the column heading indicates verbal predicates, ?Noun?
indicates nominal predicates, ?All?
indicatesall predicates.
The numbers 1-4 in column headings indicate the 4 stages: (1) predicate identification, (2)predicate labeling, (3) argument identification, (4) argument labeling.
The gold results assume perfectoutput from the previous stages.
The highest number in each column is marked with boldface.and Ailabels given P , Wi, and Li.
The graphicalmodel in Figure 1 specifies the conditional inde-pendence assumptions we make.
Equivalently, wetake the following to be proportional to the jointprobability of a particular assignment:Pr(F |P )?iPr(Ai|F ) Pr(Wi|FAi) Pr(Li|FAi)4.2 Parameter estimationTo estimate the parameters of the generative modelwe used the following methodology:For Pr(F |P ) we use the maximum likelihoodestimate from the training data.
As a consequence,frames that were never observed in the trainingdata have zero probability.
One exception is lem-mas which have not been observed in the trainingdata, for which each frame is considered equallylikely.For Pr(Ai|F ) we also use the maximum like-lihood estimate and normalize it using sentencelength.
For a given argument label we find theexpected number of words in a sentence with thatlabel for frame F .
We divide this expected num-ber with the length of the given sentence to findPr(Ai|F ) for a single word.
Any leftover prob-ability is given to the null label.
If the sentencelength is shorter than the expected number of ar-guments, all probabilities are scaled down propor-tionally.For the remaining two terms Pr(Li|F,Ai) andPr(Wi|F,Ai) using the maximum likelihood esti-mate is not effective because of data sparseness.The arguments in the million word training datacontain about 16,000 unique words and 25,000unique dependency chains.
To handle the sparse-ness problem we smoothed these two estimates us-ing the part-of-speech argument distribution, i.e.Pr(Li|POS, Ai) and Pr(Wi|POS, Ai), where POSrepresents the coarse part of speech of the predi-cate.5 Results and AnalysisTable 1 gives the F1 scores for the two models(4-stage and generative), presented separately fornoun and verb predicates and the four stages ofpredicate identification/labeling, argument identi-fication/labeling.
In order to isolate the perfor-mance of each stage we also give their scores withgold input.
The rest of this section analyzes theseresults and suggests possible improvements.A hybrid algorithm: A comparison of the twoalgorithms show that the 4-stage approach is su-perior in predicate and verbal-argument identifica-tion and the generative algorithm is superior in thelabeling of predicates and arguments and nominal-argument identification.
This suggests a hybrid al-gorithm where we restrict the generative model totake the answers for the better stages from the 4-stage algorithm (Noun1, Verb1, Verb3) as given.Tables 1 and 2 present the results for the hybridalgorithm compared to the 4-stage and generativemodels.Parsing performance: In order to see the effectof syntactic parsing performance, we ran the hy-brid algorithm starting with the gold parse.
Thelabeled semantic score went up to 78.84 for WSJand 67.20 for Brown, showing that better parsing226Data/algorithm Unlabeled LabeledWSJ 4-stage 81.15 69.44WSJ generative 81.01 73.66WSJ hybrid 82.94 74.74Brown 4-stage 76.91 58.76Brown generative 73.76 59.05Brown hybrid 77.22 60.80Table 2: Semantic scores for the 4-stage, genera-tive, and hybrid algorithmscan add about 4-6% to the overall performance.Syntactic vs lexical features: Our algorithmsuse two broad classes of features: informationfrom the dependency parse provides syntactic ev-idence, and the word pairs themselves provide se-mantic evidence for a possible relation.
To iden-tify their relative contributions, we experimentedwith two modifications of the generative algo-rithm: gen-l does not use the Pr(Wi|FAi) termand gen-w does not use the Pr(Li|FAi) term.
gen-l, using only syntactic information and the pred-icate, gets a labeled semantic score of 70.97 forWSJ and 58.83 for Brown, a relatively small de-crease.
In contrast gen-w, using only lexical infor-mation gets 43.06 for WSJ and 33.17 for Browncausing almost a 40% decrease in performance.On the other hand, we find that the lexical fea-tures are essential for certain tasks.
In labeling thearguments of nominal predicates, finding an exactmatch for the lexical pair guarantees a 90% accu-racy.
If there is no exact match, the 4-stage algo-rithm falls back on a syntactic match, which onlygives a 75% accuracy.Future work: The hybrid algorithm shows thestrengths and weaknesses of our two approaches.The generative algorithm allows feedback from thelater stages to the earlier stages and the 4-stage ma-chine learning approach allows the use of betterfeatures.
One way to improve the system could beby adding feedback to the 4-stage algorithm (laterstages can veto input coming from previous ones),or adding more features to the generative model(e.g.
information about neighbor words when pre-dicting F ).
More importantly, there is no feedbackbetween the syntactic parser and the semantic rolelabeling in our systems.
Treating both problemsunder the same framework may lead to better re-sults.Another property of both models is the indepen-dence of the argument label assignments from eachother.
Even though we try to control the number ofarguments of a particular type by adjusting the pa-rameters, there are cases when we end up with noassignments for a mandatory argument or multipleassignments where only one is allowed.
A morestrict enforcement of valence constraints needs tobe studied.The use of smoothing in the generative modelwas critical, it added about 20% to our final F1score.
This raises the question of finding moreeffective smoothing techniques.
In particular, thejump from specific frames to coarse parts of speechis probably not optimal.
There may be interme-diate groups of noun and verb predicates whichshare similar semantic or syntactic argument dis-tributions.
Identifying and using such groups willbe considered in future work.ReferencesDaelemans, W., J. Zavrel, K. van der Sloot, andA.
van den Bosch.
2004.
TiMBL: Tilburg memory-Based Learner.
Tilburg University.Gildea, D. and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245 288.McDonald, R., K. Crammer, and F. Pereira.
2005.
On-line Large-Margin Training of Dependency Parsers.Ann Arbor, 100.Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syntac-tic and semantic dependencies.
In Proceedings ofthe 12th Conference on Computational Natural Lan-guage Learning (CoNLL-2008).227
