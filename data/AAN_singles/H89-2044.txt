ACOUSTICAL PRE-PROCESSING FOR ROBUST SPEECH RECOGNITIONRichard M. Stern and Alejandro Acero 1School of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213ABSTRACTIn this paper we describe our initial efforts to make SPHINX, the CMU continuous peech recognition system,environmentally robust.
Our work has two major goals: to enable SPHINX to adapt o changes in microphone andacoustical environment, and to improve the performance of SPHINX when it is trained and tested using a desk-topmicrophone.
This talk will describe some of our work in acoustical pre-processing techniques, pecifically spectralnormalization and spectral subtraction performed using an efficient pair of algorithms that operate primarily in thecepstral domain.
The effects of these signal processing algorithms on the recognition accuracy of the Sphinx speechrecognition system was compared using speech simultaneously recorded from two types of microphones: thestandard close-talking Sennheiser HMD224 microphone and the desk-top Crown PZM6fs microphone.
A naturally-elicited alphanumeric speech database was used.
In initial results using the stereo alphanumeric database, we foundthat both the spectral subtraction and spectral normalization algorithms were able to provide very substantialimprovements in recognition accuracy when the system was trained on the close-talking microphone and tested onthe desk-top microphone, or vice versa.
Improving the recognition accuracy of the system when trained and testedon the desk-top microphone r mains adifficult problem requinng more sophisticated noise suppression techniques.INTRODUCTIONThe acceptability of any voice interface depends on its ease of use.
Although users in some application domainswill accept he headset-mounted microphones that are commonly used with current speech recognition systems,there are many other applications that require a desk microphone or a wall-mounted microphone.
The use of othertypes of microphones besides the "close-talking" headset generally degrades the performance of spoken-languagesystems.
Even a relatively "quiet" office environment can be expected to provide a significant amount of additivenoise from fans, door slams, as well as competing conversations and reverberation arising from surface reflectionswithin a room.
Applications uch as inspection or inventory on a factory floor, or an outdoor automatic bankingmachine demand an even greater degree of environmental robustness.
Our goal has been to develop practicalspoken-language systems for real-world environments that are robust with respect to changes in acoustical ambienceand microphone type as well as with respect to speaker and dialect.Although a number of techniques have been proposed to improve the quality of degraded speech, researchers haveonly recently begun to evaluate speech-enhancement in terms of the improvement in recognition accuracy that heyprovide for speech-recognition systems operating in natural environments.
We are incorporating into our system acombination of techniques that come into play at different levels of the system, including pre-processing of theacoustical waveform, the development of physiologically and psychophysically motivated peripheral processingmodels (i.e.
"ear models"), adaptive multimicrophone array processing, and dynamic adaptation to new speakers andenvironments by modifying the parameters used to represent the speech sounds.
In this talk we will focus only onour work in the first category, acoustical preprocessing.1This research was sponsored by the Defense Advanced Research Projects Agency (DOD), ARPA Order No.5167, under contract number N00039-85-C-0163.
The views and conclusions contained in this document are thoseof the authors and should not be interpreted as representing the official policies, either expressed or implied, of theDefense Advanced Research Projects Agency or the US Government.311There are many sources of acoustical distortion that can degrade the accuracy of speech-recognition systems.
Forexample, obstacles to robustness include additive noise from machinery, competing talkers, etc., reverberation fromsurface reflections in a room, and spectral shaping by microphones and the vocal tracts of individual speakers.These sources of distortion cluster into two complementary classes: additive noise (as in the first two examples) anddistortions resulting the convolution of the speech signal with an unknown linear system (as in the remaining three).In the classical speech-enhancement literature, two complementary techniques have been proposed to cope withthese problems: spectral subtraction and spectral normalization.
In spectral subtraction one estimates the amount ofbackground noise present during non-speech intervals, and subtracts the estimated spectral density of the noise fromthe incoming signal (e.g.
Boll, 1979; Berouti et al, 1979).
In spectral normalization (sometimes referred to as"blind deconvolution"), one estimates the average spectrum when speech is present and applies a multiplicativenormalization factor with respect to a reference spectrum (e.g.
Stockham et al, 1975).
While these procedures wereonce thought to be of limited practical benefit, based on the results of experiments concerning the human perceptionof speech, results of recent applications of them to automatic speech-recognitions systems have been moreencouraging (e.g.
Porter and Boll, 1984; Van Compernolle, 1987).In this report we will review the database used to evaluate fficient implementations of spectral subtraction andnormalization i  the cepstral domain, discuss the results of analyses of baseline studies of recognition performance,describe the effectiveness of the spectral subtraction and normalization algorithms, and discuss the motivations forsome of our work in progress.THE ALPHANUMERIC  DATABASEAlthough the bulk of research using the Sphinx system at Carnegie Mellon has made use of the well-knownResource Management database, we were forced to use a different database, the Alphanumeric database, for ourevaluations of signal processing.
The primary reason for this is that the Resource Management database with itslarge vocabulary size and many utterances required several weeks to train satisfactorily, which was excessively ongsince the entire system had to be retrained each time a new signal-processing algorithm was introduced.
We alsoperformed these evaluations using a more compact and easily-trained version of Sphinx with only about 650phonetic models, omitting such features as function-word models, between-word triphone models, and correctivetraining.
We were willing to tolerate the somewhat lower absolute recognition accuracy that this version of Sphinxprovided because of the reduced time required by the training process.
Using the Alphanumeric database, the morecompact Sphinx system, and faster computers, we were able to reduce the training time to the point that an entiretrain-and-test cycle could be performed in about 9 hours.A second reason why we resorted to a new database is that we specifically wanted to compare simultaneousrecordings from close-talking and desktop microphones in our evaluations.
We believe that it is very important toevaluate speech-recognition systems in the context of natural acoustical environments with natural noise sources,rather than using speech that is recorded in a quiet environment into which additive noise and spectral tilt areartificially injected.CONTENTS OF THE DATABASEThe Alphanumeric database consists of 1000 training utterances and 140 different testing utterances, that were eachrecorded simultaneously in stereo using both the Sennheiser HMD224 close-talking microphone that has been astandard in previous DARPA evaluations, and a desk-top Crown PZM6fs microphone.
The recordings were madein one of the CMU speech laboratories (the "Agora" lab), which has high ceilings, concrete-block walls, and acarpeted floor.
Although the recordings were made behind an acoustic partition, no attempt was made to silenceother users of the room during recording sessions, and there is consequently a significant amount of audibleinterference from other talkers, key clicks from other workstations, slamming doors, and other sources ofinterference, as well as the reverberation from the room itself.
Since the database was limited in size, it wasnecessary to perform repeated evaluations on the same test utterances.The database consisted of strings of letters, numbers, and a few control words, that were naturally elicited in thecontext of a task in which speakers spelled their names, addresses, and other personal information, and entered somerandom letter and digit strings.
Some sample utterances are N-S-V-H-6-T-49, ENTER-4-5-8-2-1 and312P-I-T-T-S-B-U-R-G-H. A total of 106 vocabulary items appeared in the vocabulary, of which about 40 were rarelyuttered.
Although it contains fewer vocabulary items, the Alphanumeric database is more difficult than theResource Management database with perplexity 60 both because of the greater number of words in the vocabularyand because of their greater intrinsic acoustic onfusibifity.AVERAGE SPEECH AND NOISE SPECTRAFigure 1 compares averaged spectra from the Alphanumeric database for frames believed to contain speech andbackground noise from each of the two microphones.
By comparing these curves, it can be seen that the averagesignal-to-noise ratio (SNR) using the close-talking Sennheiser microphone is about 25 dB.
The signals from theCrown PZM, on the other hand, exhibit an SNR of less than 10 dB for frequencies below 1500 Hz and about 15 dBfor frequencies above 2000 Hz.
Furthermore, the response of the Crown PZM exhibits a greater spectral tilt thanthat of the Sennheiser, perhaps because the noise-cancelling transducer on the Sennheiser also suppresses much ofthe low-frequency omponents ofthe speech signal.~" ' 6000 8000~.
Frequency (Hz) "-110 t" Speech Spectrum-15~ -20-25-30-35Noise SpectrumSennheiser Microphone-25,or, , r  \ , , , , \ 2oo?__4ooo  6ooo  8ooo-51".
~ ~ "~ Frequency (Hz), ."
% % Speech Spectrum-15 %-20 ~ ~Noise Spectrum ~ .
.
.
.
.
.Crown PZM MicrophoneFigure 1 Average speech and noise spectra from the Alphanumeric database obtained using the headset-mountedSennheiser Microphone and the Crown PZM microphone.
The separation of the two curves in each panel providesan indication of signal-to-noise ratio for each microphone.
It can also be seen that he Crown PZM produces greaterspectral f it.313BASELINE RECOGNITION ACCURACYWe first consider the "baseline" recognition accuracy of the Sphinx system obtained using the two microphones withthe standard signal processing routines.
Table I summarizes the recognition accuracy obtained by training andtesting using each of the two microphones.
Recognition accuracy is reported using the standard DARPA scoringprocedure (Pallett, 1989), with penalties for insertions and deletions as well as for substitutions.
It can be seen thattraining and testing on the Crown PZM produces an error rate that is 60% worse than the error rate produced whenthe system is trained and tested on the Sennheiser microphone.
When the system is trained using one microphoneand tested using the other, however, the performance degrades to a very low level.
Hence we can identify two goalsof signal processing for greater obustness: we need to drastically improve the performance of the system for the"cross conditions", and to elevate the absolute performance of the system when it is trained and tested using theCrown PZM.Test CLS Test PZMTrain CLS 85.3 % 18.6%Train PZM 36.9% 76.5%Table I. Baseline performance of the Sphinx system when trained and tested on the Alphanumeric vocabulary usingeach of the two microphones.In order to better understand why performance degraded when the microphone was changed from the Sennheiser tothe Crown PZM, even when the PZM was used for training as well as testing, we studied the spectrograms andlistened carefully to all utterances for which training and testing with the PZM produced errors that did not appearwhen the system was trained and tested on the close-talking Sennheiser microphone.
The estimated causes of the"new" errors using the Crown PZM are summarized in Table II.
Not too surprisingly, the major consequence ofusing the PZM was that the effective SNR was lowered.
As a result, there were many confusions of silence or noisesegments with weak phonetic events.
These confusions accounted for some 58 percent of the additional errors, withcrosstalk (either by competing speakers or key clicks from other workstations) identified as the most significantother cause of new errors.Percent errorsWeak-event insertion 41.5Weak-event deletion 13.2Crosstalk 20.0Others 25.3Table H. Analysis of causes of "new" errors introduced by use of the Crown PZM microphone.We now consider the extent o which the use of acoustical pre-processing can mitigate the effects of the Crown PZMand of the change in microphone.ACOUSTICAL PRE-PROCESSING FOR SPEECH RECOGNITIONIn this section we briefly review the baseline signal procedures used in the Sphinx system, and we describe thespectral normalization and spectral subtraction operations in the cepstral domain.GENERAL SIGNAL PROCESSINGThe first stages of signal processing in the evaluation system are virtually identical to those that have been reportedfor the Sphinx system previously.
Briefly, speech is digitized with a sampling rate of 16 kHz and pre-emphasized,and a Hamming window is applied to produce analysis frames of 20-ms duration every 10 ms. 14 LPC coefficients314are produced for each frame using the autocorrelation method, from which 32 cepstral coefficients are obtainedusing the standard recursion method.
Finally, these cepstral coefficients are frequency warped to a pseudo-mel scaleusing the bilinear-transform ethod with 12 stages, producing a final 12 cepstral coefficients after the frequencywarping.
(We found that increasing the number of cepstral coefficients before the warping from 12 to 32 providedbetter frequency resolution after frequency warping, which led to a 5-percent relative improvement of the baselineSphinx system on the Resource Management task.)
In addition to the LPC cepstral coefficients, differenced LPCcepstral coefficients, power and differenced power are also computed for every frame.
The cepstra, differencedcepstra, and combined power and differenced power parameters are vector quantized into three different codebooks.PROCESSING FOR ROBUSTNESS IN THE CEPSTRAL DOMAINWe describe in this section the procedures we used to achieve spectral normalization and spectral subtraction i thecepstral domain.
Because signal processing and feature xtraction in the Sphinx system was already based oncepstral analysis, these procedures could be implemented with an almost negligible increase in computational loadbeyond that of the existing signal processing procedures.Spectral NormalizationThe goal of spectral normalization is to compensate for distortions to the speech signal produced by linearconvolution, which could be the result of filtering by the vocal tract, room acoustics, or the transfer function of aparticular microphone.
As noted above, compensation for linear convolution could be accomplished by multiplyingthe magnitude of the spectrum by a correction factor.
Since the cepstrum is the log of the magnitude of thespectrum, this corresponds toa simple additive correction of the cepstrum vector.
The major differences betweenvarious spectral normalization algorithms are primarily concerned with how the additive compensation vector isestimated.The most effective form of spectral normalization that we have considered so far is also the simplest.
Specifically, astatic reference vector is estimated by computing the inverse DlZT of the long-term average of the cepstral vector forthe speech frames from the training databases.
(Samples of these averages for the alphanumeric database are shownin Fig.
1.)
The compensation vector is defined to be the difference between the two sets of averaged cepstralcoefficients from the two types of microphones in the training database., Although the compensation vector isdetermined only from averages of spectra in the speech frames, it is applied to both the speech and nonspeechframes.We have also considered other types of spectral normalization in the cepstral domain, including one that determinesthe compensation vector that minimizes the average VQ distortion.
While none of these methods work any better inisolation than the simple static spectral normalization described above, some of them have exhibited betterperformance than the static normalization when used in conjunction with spectral subtraction.Spectral SubtractionSpectral Subtraction is more complex than spectral normalization, both because it cannot be applied to the cepstralcoefficients directly, and because there are more free parameters and arbitrary decisions that must be resolved indetermining the best procedure for a particular system.Spectral subtraction i  the Sphinx system is accomplished by converting from the feature vectors from cepstralcoefficients to log-magnitude coefficients using a 32-point inverse DFT (for the 16 real and even cepstralcoefficients).
These log-magnitude vectors are then exponentiated to produce direct spectral magnitudes, fromwhich a reference vector is subtracted according to the general procedure described below.
The log of the resultingdifference spectrum is then converted once again to a cepstral vector using a 32-point forward DFF.
Although bothan inverse and forward DFF must be performed on the cepstral vectors in this algorithm, little time is consumedbecause only 16 real coefficients are involved in the DFT computations.
In addition, a computationally efficientprocedure similar to the one described by Von Compernolle (1987) can be applied to perform the exponentiation a dlogarithm operations using a single table lookup.The estimated noise spectrum is either over-subtracted or under-subtracted from the input spectrum, depending onthe estimated instantaneous signal-to-noise ratio (of the current analysis frame).
In our current implementation of315spectral subtraction, the estimation of the noise vector and the determination of the amount of subtraction to beinvoked are based on a comparison of the incoming signal energy to two thresholds, representing a putativemaximum power level for noise frames (the "noise threshold") and a putative minimum power level for speechframes (the "speech" threshold").
While these thresholds are presently set empirically, they could easily beestimated from histograms of the average power for the signals in the analysis fxames.
The estimated noise vector isobtained by averaging the cepstra of all frames with a power that falls below the noise threshold.
Once the noisevector is estimated, a magnitude equal to that of the reference spectrum plus 5 dB is subtracted from the magnitudeof the spectrum of the incoming signal, for all frames in which the power of the incoming signal falls below thenoise threshold.
If the power of the incoming signal is above the speech threshold, the magnitude of the referencespectrum minus 2.5 dB is subtracted from the magnitude of the spectrum of the incoming signal.
The amount ofover- or under-subtraction (i  dB) is a linearly interpolated function of the instantaneous signal-to-noise ratio (in dB)for incoming signals whose power is between the two thresholds.
We note that we subtract he magnitudes ofspectra \[as did Berouti et al (1979)\] rather than the more intuitively appealing spectral power because we found thatmagnitude subtraction provides greater recognition accuracy.EXPERIMENTAL RESULTSFigure 2 summarizes the experimental results obtained using the Alphanumeric database when the system wastrained and tested on the two types of microphones, ineither the baseline conditions, or with spectral normalizationand spectral subtraction.
In each of the two panels, the word accuracies obtained for the two baseline conditionswhen the system was trained and tested using the same microphones are indicated by the horizontal dotted lines.
Itcan be seen that in each case, the use of spectral normalization and subtraction provides increasing improvement tothe recognition accuracy obtained in the "cross" conditions, without almost no degradation of the recognitionaccuracy observed when the system is trained and tested using the same microphone.
In fact, the recognitionaccuracy obtained with spectral subtraction i  the "cross" conditions approaches that obtained when the system istrained on the same microphone that it is tested on.
On the other hand, we have not yet been able to significantlyimprove the performance of the system when it is trained and tested on the Crown PZM microphone.
We brieflydescribe some of the strategies we are presently considering toward that end.DISCUSSIONWe demonstrated in the previous section that the spectral subtraction and normalization routines we haveimplemented can greatly increase the robustness of the Sphinx system when it is tested on a different microphonefrom the one with which it was trained.
While we are pleased with these results, we are also continuing our effortsto improve the performance ofthe system when trained and tested using the Crown PZM microphone.
We stronglybelieve that further improvements in performance are possible for this condition using improved acoustical pre-processing, and we briefly describe three techniques to be considered.INTEGRATION OF SPECTRAL SUBTRACTION AND NORMALIZATIONSince spectral subtraction and normalization each provide some improvement i  recognition accuracy when appliedindividually, one would expect hat further improvement should be obtained when they are used simultaneously.Indeed, in pilot experiments using the Resource Management database, training using the Sennheiser microphoneand testing using the Crown PZM, we obtained a 15 percent reduction in relative error rate when spectralnormalization was added to spectral subtraction (Morii, 1987).
Nevertheless, we have found that the effects of thetwo enhancement procedures interact with each other, and simple cascades of the two implementations that workbest in isolation do not produce great improvements in performance.
We are confident hat with betterunderstanding of the nature of these interactions we can more fully exploit he complementary nature of the twotypes of processing.INTRODUCTION OF NON-PHONETIC MODELSIn these Proceedings, Ward (1989) describes a procedure by which the performance of the Sphinx system can beimproved by explicitly developing phonetic models for such non-speech events uch as filled pauses, breath noises,door slams, telephone rings, paper ustling, etc.
Most of these phenomena are highly transitory in nature, and assuch are not directly addressed by either spectral subtraction or normalization.
While Ward was especially316 100 80{ 8o4o2O0Test CLS\[\] D. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.~ Test PZMi I IBaseline Norm Sub1oo806020E\] ~ .
.
.
.
.
.
.
.
~_~_~est PZM~,~ S Test CLSv0 I I I Baseline Norm SubFigure 2 Comparison of recognition accuracy obtained using the baseline signal processing, spectral subtraction,and spectral normalization, and each of the two microphones.
The horizontal dotted lines indicate performanceobtained in the baseline condition when the system is trained and tested using the same microphone.concerned with the non-phonetic events associated with spontaneous speech, there is no reason why thesetechniques cannot be applied to process peech recorded from desk-top microphones as well.
Since it appears thatabout 20 percent of the "new" errors introduced when one replaces the Sennheiser microphone by the Crown PZMare the result of crosstalk, we are optimistic that implementation f Ward's non-phonetic models hould providefurther improvement inrecognition accuracy.CONSIDERATION OF SPECTRAL CORRELATIONS ACROSS FREQUENCYTraditional spectral subtraction techniques assume that all speech frames are statistically independent from eachother, and that every frequency component within a frame is statistically independent from the other frequencies.
Asa result, it is quite possible that the result of a spectral subtraction operation may bear little resemblance to anylegitimate speech spectrum, particularly at low SNRs.
We are exploring several techniques to take advantage ofinformation about correlations across frequency to ensure that the result of the spectral subtraction is likely torepresent a legitimate speech spectrum.SUMMARYWe found that the use of desk-top microphones like the Crown PZM increase the error rate by allowing weakphonetic events to become confused with silences and vice-versa.
The spectral subtraction and normalizationroutines we developed provide considerable improvement inrecognition accuracy when the system is tested using adifferent microphone from the one it was trained on, but further work must be done to improve the absolute level ofperformance obtained when Sphinx is trained and tested using the Crown PZM.317ACKNOWLEDGMENTSMany members of the speech group have contributed tothis work.
We thank Joel Douglas for performing many ofthe calculations, Kai-Fu Lee for helping us understand the mysteries of Sphinx, Fil Alleva and Eric Thayer for manydiscussions about signal processing in the Sphinx system, Bob Weide for providing for database collection andanalysis, Wayne Ward for working with us to introduce non-phonetic models, and (of course) Raj Reddy for hisoverall leadership and support of this work.REFERENCESM.
Berouti, R. Schwartz and J. Makhoul.
(1979).
Enhancement of Speech Corrupted by Acoustic Noise.
InJ.
S. Lira (Ed.
), Speech Enhancement.
Englewood Cliffs, NJ: Prentice Hall, 1983.S.
F. Boll.
(1979).
Suppression of Acoustic Noise in Speech Using Spectral Subtraction.
ASSP, 27, 113-120.S.
Mofii.
(1987).
Performance of the Sphinx System Using Spectral Subtraction and Normalization.
Unpublishedwork, Carnegie Mellon University.D.
Pallett.
(1989).
Benchmark Tests for DARPA Resource Management Database Performance Evaluations.ICASSP89.J.
E. Porter and S. F. Boll.
(1984).
Optimal Estimators for Spectral Restoration of Noisy Speech.
ICASSP84.T.
G. Stockham, T. M. Cannon and R. B. Ingebretsen.
(1975).
Blind Deconvolufion Through Digital SignalProcessing.
Proc.
IEEE, 63,678-692.D.
Van Compernolle.
(1987).
Increased Noise Immunity in Large Vocabulary Speech Recognition with the Aid ofSpectral Subtraction.
ICASSP87.W.
Ward.
(1989).
Modelling Non-Verbal Sounds for Speech Recognition.
These Proceedings.318
