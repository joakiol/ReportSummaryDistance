Discriminative Word Alignment byLinear ModelingYang Liu?Institute of Computing TechnologyChinese Academy of SciencesQun Liu?Institute of Computing TechnologyChinese Academy of SciencesShouxun Lin?Institute of Computing TechnologyChinese Academy of SciencesWord alignment plays an important role in many NLP tasks as it indicates the correspondencebetween words in a parallel text.
Although widely used to align large bilingual corpora, gen-erative models are hard to extend to incorporate arbitrary useful linguistic information.
Thisarticle presents a discriminative framework for word alignment based on a linear model.
Withinthis framework, all knowledge sources are treated as feature functions, which depend on a sourcelanguage sentence, a target language sentence, and the alignment between them.
We describe anumber of features that could produce symmetric alignments.
Our model is easy to extend andcan be optimized with respect to evaluation metrics directly.
The model achieves state-of-the-artalignment quality on three word alignment shared tasks for five language pairs with varyingdivergence and richness of resources.
We further show that our approach improves translationperformance for various statistical machine translation systems.1.
IntroductionWord alignment, which can be defined as an object for indicating the correspondingwords in a parallel text, was first introduced as an intermediate result of statisticalmachine translation (Brown et al 1993).Consider the following Chinese sentence and its English translation:Published under Creative Commons Attribution-NonCommercial 3.0 Unported license     Zhongguo jianzhuye duiwaikaifang chengxian xin gejuThe opening of China?s construction industry to the outside presents a new structure?
Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, ChineseAcademy of Sciences, No.
6 Kexueyuan South Road, Haidian District, P.O.
Box 2704, Beijing 100190,China.
E-mail: {yliu, liuqun, sxlin}@ict.ac.cn.Submission received: 13 September 2007; revised submission received: 7 January 2010; accepted forpublication: 21 February 2010.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 3The Chinese word Zhongguo is aligned to the English word China because they aretranslations of one another.
Similarly, the Chinese word xin is aligned to the Englishword new.
These connections are not necessarily one-to-one.
For example, one Chineseword jianzhuye corresponds to two English words construction industry.
In addition, theEnglish words (e.g., opening ... to the outside) connected to a Chinese word (e.g., dui-waikaifang) could be discontinuous.
Figure 1 shows an alignment for this sentence pair.The Chinese and English words are listed horizontally and vertically, respectively.
Theyare numbered to facilitate identification.
The dark points indicate the correspondencebetween the words in two languages.
The goal of word alignment is to identify suchcorrespondences in a parallel text.Word alignment plays an important role in many NLP tasks.
In statistical machinetranslation, word-aligned corpora serve as an excellent source for translation-relatedknowledge.
The estimation of translation model parameters usually relies heavily onword-aligned corpora, not only for phrase-based and hierarchical phrase-based models(Koehn, Och, and Marcu 2003; Och and Ney 2004; Chiang 2005, 2007), but also forsyntax-based models (Quirk, Menezes, and Cherry 2005; Galley et al 2006; Liu, Liu,and Lin 2006; Marcu et al 2006).
Besides machine translation, many applications forword-aligned corpora have been suggested, including machine-assisted translation,Figure 1Example of a word alignment between a Chinese?English sentence pair.
The Chinese andEnglish words are listed horizontally and vertically, respectively.
They are numbered to facilitateidentification.
The dark points indicate the correspondences between the words in the twolanguages.304Liu, Liu, and Lin Discriminative Word Alignment by Linear Modelingtranslation assessment and critiquing tools, text generation, bilingual lexigraphy, andword sense disambiguation.Various methods have been proposed for finding word alignments between paralleltexts.
Among them, generative alignment models (Brown et al 1993; Vogel and Ney1996) have been widely used to produce word alignments for large bilingual corpora.Describing the relationship of a bilingual sentence pair, a generative model treats wordalignment as a hidden process and maximizes the likelihood of a training corpus usingthe expectation maximization (EM) algorithm.
After the maximization process is com-plete, the unknown model parameters are determined and the word alignments are setto the maximum posterior predictions of the model.However, one drawback of generative models is that they are hard to extend.
Gen-erative models usually impose strong independence assumptions between sub-models,making it very difficult to incorporate arbitrary features explicitly.
For example, whenconsidering whether to align two words, generative models cannot include informationabout lexical and syntactic features such as part of speech and orthographic similarity inan easy way.
Such features would allow for more effective use of sparse data and resultin a model that is more robust in the presence of unseen words.
Extending a generativemodel requires that the interdependence of information sources be modeled explicitly,which often makes the resulting system quite complex.In this article, we introduce a discriminative framework for word alignment basedon the linear modeling approach.
Within this framework, we treat all knowledgesources as feature functions that depend on a source sentence, a target sentence,and the alignment between them.
Each feature function is associated with a featureweight.
The linear combination of features gives an overall score to each candidatealignment.
The best alignment is the one with the highest overall score.
A linearmodel not only allows for easy integration of new features, but also admits optimiz-ing feature weights directly with respect to evaluation metrics.
Experimental resultsshow that our approach improves both alignment quality and translation performancesignificantly.This article is organized as follows.
Section 2 gives a formal description of ourmodel.
We show how to train feature weights by taking evaluation metrics into accountand how to find the most probable alignment in an exponential search space efficiently.Section 3 describes a number of features used in our experiments, focusing on thefeatures that produce symmetric alignments.
In Section 4, we evaluate our model inboth alignment and translation tasks.
Section 5 reviews previous work related to ourapproach and the article closes with a conclusion in Section 6.2.
Approach2.1 The ModelGiven a source language sentence f = f1, .
.
.
, fj, .
.
.
, fJ and a target language sentence e =e1, .
.
.
, ei, .
.
.
, eI, we define a link l = ( j, i) to exist if fj and ei are translations (or part of atranslation) of one another.
Then, an alignment is defined as a subset of the Cartesianproduct of the word positions:a ?
{( j, i) : j = 1, .
.
.
, J; i = 1, .
.
.
, I} (1)305Computational Linguistics Volume 36, Number 3We propose a linear alignment model:score(f, e, a) =M?m=1?mhm(f, e, a) (2)where hm(f, e, a) is a feature function and ?m is its associated feature weight.
The linearcombination of features gives an overall score score(f, e, a) to each candidate alignmenta for a given sentence pair ?f, e?.2.2 TrainingTo achieve good alignment quality, it is essential to find a good set of feature weights?M1 .
Before discussing how to train ?M1 , we first describe two evaluation metrics thatmeasure alignment quality, because we will optimize ?M1 with respect to them directly.2.2.1 Evaluation Metrics.
The first metric is alignment error rate (AER), proposed byOch and Ney (2003).
AER has been used as official evaluation criterion in most wordalignment shared tasks.
Och and Ney define two kinds of links in hand-aligned align-ments: sure links for alignments that are unambiguous and possible links for ambiguousalignments.
Sure links usually connect content words such as Zhongguo and China.In contrast, possible links often align words within idiomatic expressions and freetranslations.An AER score is given byAER(S, P, A) = 1 ?
|A ?
S| + |A ?
P||A| + |S| (3)where S is a set of sure links in a reference alignment that is hand-aligned by humanexperts, P is a set of possible links in the reference alignment, and A is a candidatealignment.
Note that S is a subset of P: S ?
P. The lower the AER score is, the better thealignment quality is.Although widely used, AER has been criticized for correlating poorly with transla-tion quality (Ayan and Dorr 2006a; Fraser and Marcu 2007b).
In other words, lower AERscores do not necessarily lead to better translation quality.1 Fraser and Marcu (2007b)argue that reference alignments should consist of only sure links.
They propose a newmeasure called the balanced F-measure:precision(S, A) =|A ?
S||S| (4)recall(S, A) =|A ?
S||A| (5)F-measure(S, ?, A) = 1?precision(S,A) +1?
?recall(S,A)(6)1 It has not yet been uniformly accepted that better word alignments yield better translations.
Ayan andDorr (2006a) present a detailed discussion of the impact of word alignment on statistical machinetranslation.306Liu, Liu, and Lin Discriminative Word Alignment by Linear Modelingwhere ?
is a parameter that sets the trade-off between precision and recall.
HigherF-measure means better alignment quality.
Obviously, ?
less than 0.5 weights recallhigher, whereas ?
greater than 0.5 weights precision higher.We use both AER and F-measure in our experiments.
AER is used in experimentsevaluating alignment quality (Section 4.1) and F-measure is used in experiments evalu-ating translation performance (Section 4.2).2.2.2 Minimum Error Rate Training.
Suppose we have three candidate alignments: a1, a2,and a3.
Their AER scores are 0.21, 0.20, and 0.22, respectively.
Therefore, a2 is the bestcandidate alignment, a1 is the second best, and a3 is the third best.
We use three featuresto score each candidate.
Table 1 lists the feature values for each candidate.If the set of feature weights is {1.0, 1.0, 1.0}, the model scores (see Equation (2)) ofthe three candidates are ?71, ?74, and ?76, respectively.
Whereas reference alignmentconsiders a2 as the best candidate, a1 has the maximal model score.
This is undesirablebecause the model fails to agree with the reference.
If we change the feature weightsto {1.0,?2.0, 2.0}, the model scores become ?73, ?71, and ?83, respectively.
Now, themodel chooses a2 as the best candidate correctly.If a set of feature weights manages to make model predictions agree with refer-ence alignments in training examples, we would expect the model to achieve goodalignment quality on unseen data as well.
To do this, we adopt the minimum er-ror rate training (MERT) algorithm proposed by Och (2003) to find feature weightsthat minimize AER or maximize F-measure on a representative hand-aligned trainingcorpus.Given a reference alignment r and a candidate alignment a, we use a loss func-tion E(r, a) to measure alignment performance.
Note that E(r, a) can be either AER or1 ?
F-measure.
Given a bilingual corpus ?fS1, eS1?
with a reference alignment rs and a setof K different candidate alignments Cs = {as,1 .
.
.
as,K} for each sentence pair ?fs, es?, ourgoal is to find a set of feature weights ?
?M1 that minimizes the overall loss on the trainingcorpus:?
?M1 = argmin?M1{ S?s=1E(rs, a?
(fs, es; ?M1 ))}(7)= argmin?M1{ S?s=1K?k=1E(rs, as,k)?(a?
(fs, es; ?M1 ), as,k)}(8)Table 1Example feature values and alignment error rates.feature valuescandidate h1 h2 h3 AERa1 ?85 4 10 0.21a2 ?89 3 12 0.20a3 ?93 6 11 0.22307Computational Linguistics Volume 36, Number 3where a?
(fs, es; ?M1 ) is the best candidate alignment produced by the linear model:a?
(fs, es; ?M1 ) = argmaxa{ M?m=1?mhm(fs, es, a)}(9)The basic idea of MERT is to optimize only one parameter (i.e., feature weight)each time and keep all other parameters fixed.
This process runs iteratively over Mparameters until the overall loss on the training corpus does not decrease.Formally, suppose we tune a parameter and keep the other M ?
1 parameters fixed;each candidate alignment corresponds to a line in the plane with ?
as the independentvariable:?
?
?
(f, e, a) + ?
(f, e, a) (10)where ?
denotes the parameter being tuned (i.e., ?m) and ?
(f, e, a) and ?
(f, e, a) areconstants with respect to ?:?
(f, e, a) = hm(f, e, a) (11)?
(f, e, a) =M?m?=1,m?
=m?m?hm?
(f, e, a) (12)The set of candidates in Cs defines a set of lines.
For example, given the candidatealignments in Table 1, suppose we only tune ?2 and keep ?1 and ?3 fixed with an initialset of parameters {1.0, 1.0, 1.0}.
According to Equation (10), a1 corresponds to a line4??
75, a2 corresponds to a line 3??
77, and a3 corresponds to a line 6??
82.The decision rule in Equation (9) states that a?
is the line with the highest modelscore for a given ?.
The selection of ?
for each sentence pair ultimately determines theloss at ?.
How do we find values of ?
that could generate different loss values?As the loss can only change if we move to a ?
where the highest line is differentthan before, Och (2003) suggests only evaluating the loss at values in between theintersections that line the top surface of the cluster of lines.
Figure 2 demonstrates eightFigure 2Candidate alignments in dimension ?
and the critical intersections.
Each candidate alignment isrepresented as a line.
?1,?2, and ?3 are critical intersections where the best candidate a?
(highlighted in bold) will change: a?
is a1 in (?
?, ?1], a2 in (?1, ?2], a7 in (?2, ?3], and a5 in(?3, +?
).308Liu, Liu, and Lin Discriminative Word Alignment by Linear Modelingcandidate alignments.
The sequence of the topmost line segments highlighted in boldconstitutes an upper envelope, which indicates the best candidate alignments the modelpredicts with various values of ?.
Instead of computing all possible K2 intersectionsbetween the lines in Cs, we just need to find the critical intersections where the topmostline changes.
In Figure 2, ?1, ?2, and ?3 are critical intersections.
In the interval (?
?, ?1],a1 has the highest score.
Similarly, the best candidates are a2 for (?1, ?2], a7 for (?2, ?3],and a5 for (?3, +?
), respectively.
The optimal ??
can be found by collecting all criticalintersections on the training corpus and choosing one ?
that results in the minimal lossvalue.
Please refer to Och (2003) for more details.2.3 SearchGiven a source language sentence f and a target language sentence e, we try to find thebest candidate alignment with the highest model score:a?
= argmaxa{score(f, e, a)}(13)= argmaxa{ M?m=1?mhm(f, e, a)}(14)To do this, we begin with an empty alignment and keep adding new links untilthe model score of the current alignment does not increase.
Figure 3 illustrates thissearch process.
Given a source language sentence f1f2 and a target language sentencee1e2, the initial alignment a1 is empty (i.e., all words are unaligned).
Then, we obtain anew alignment a2 by adding a link (1, 1) to a1.
Similarly, the addition of (1, 2) to a1 leadsto a3.
a2 and a3 can be further extended to produce more alignments.Graphically speaking, the search space of a sentence pair can be organized as adirected acyclic graph.
Each node in the graph is a candidate alignment and each edgecorresponds to a link.
We say that alignments that have the same number of linksconstitute a level.
There are 2J?I possible nodes and J ?
I + 1 levels in a graph.
InFigure 3, a2, a3, a4, and a5 belong to the same level because they all contain one link.The maximum level width is given by( J?I J?I2 ).
In Figure 3, the maximal level width is(42)= 6.
Our goal is to find the node with the highest model score in a search graph.As the search space of word alignment is exponential (although enumerable), it iscomputationally prohibitive to explore all the graph.
Instead, we can search efficientlyin a greedy way.
In Figure 3, starting from a1, we add single links to a1 and obtain fournew alignments: a2, a3, a4, and a5.
We retain the best new alignment that has a higherscore than a1, say a3, and discard the others.
Then, we add single links to a3 and obtainthree new alignments: a7, a9, and a11.
After choosing a9 as the current best alignment, thenext candidates are a12 and a14.
Suppose the model scores of both a12 and a14 are lowerthan that of a9.
We terminate the search process and choose a9 as the best candidatealignment.During this search process, we expect that the addition of a single link l to thecurrent best alignment a will result in a new alignment a ?
{l} with a higher score:score(f, e, a ?
{l}) > score(f, e, a) (15)309Computational Linguistics Volume 36, Number 3Figure 3Search space of a sentence pair: f1 f2 and e1e2.
Each node in the directed graph is a candidatealignment and each edge denotes a transition between two nodes by adding a link.that isM?m=1?m(hm(f, e, a ?
{l}) ?
hm(f, e, a))> 0 (16)As a result, we can remove most of the computational overhead by calculating onlythe difference of scores instead of the scores themselves.
The difference of alignmentscores with the addition of a link, which we refer to as a link gain, is defined asG(f, e, a, l) =M?m=1?mgm(f, e, a, l) (17)where gm(f, e, a, l) is a feature gain, which is the incremental feature value after addinga link l to the current alignment a:gm(f, e, a, l) = hm(f, e, a ?
{l}) ?
hm(f, e, a) (18)In our experiments, we use a beam search algorithm that is more general than theabove greedy algorithm.
In the greedy algorithm, we retain at most one candidate ineach level of the space graph while traversing top-down.
In the beam search algorithm,we retain at most b candidates at each level.Algorithm 1 shows the beam search algorithm.
The input is a source languagesentence f and a target language sentence e (line 1).
The algorithm maintains a list of310Liu, Liu, and Lin Discriminative Word Alignment by Linear ModelingAlgorithm 1 A beam search algorithm for word alignment1: procedure ALIGN(f, e)2: open ?
?
 a list of active alignments3: N ?
?
 n-best list4: a ?
?
 begin with an empty alignment5: ADD(open, a, ?, b)  initialize the list6: while open = ?
do7: closed ?
?
 a list of promising alignments8: for all a ?
open do9: for all l ?
J ?
I ?
a do  enumerate all possible new links10: a?
?
a ?
{l}  produce a new alignment11: g ?
GAIN(f, e, a, l)  compute the link gain12: if g > 0 then  ensure that the score will increase13: ADD(closed, a?, ?, b)  update promising alignments14: end if15: ADD(N , a?, 0, n)  update n-best list16: end for17: end for18: open ?
closed  update active alignments19: end while20: return N  return n-best list21: end procedureactive alignments open (line 2) and an n-best list N (line 3).
The aligning process beginswith an empty alignment a (line 4) and the procedure ADD(open, a, ?, b) adds a to open.The procedure prunes the search space by discarding any alignment that has a scoreworse than:1. ?
multiplied with the best score in the list, or2.
the score of b-th best alignment in the list.For each iteration (line 6), we use a list closed to store promising alignments thathave higher scores than the current alignment.
For every possible link l (line 9), weproduce a new alignment a?
(line 10) and calculate the link gain G by calling theprocedure GAIN(f, e, a, l).
If a?
has a higher score (line 12), it is added to closed (line 13).We also update N to keep the top n alignments explored during the search (line 15).
Then-best list will be used in training feature weights by MERT.
This process iteratesuntil there are no promising alignments.
The theoretical running time of this algorithmis O(bJ2I2).3.
Feature FunctionsThe primary art in discriminative modeling is to define useful features that capture var-ious characteristics of word alignments.
Intuitively, we can include generative modelssuch as the IBM Models 1?5 (Brown et al 1993) as features in a discriminative model.A straightforward way is to use a generative model itself as a feature directly (Liu, Liu,311Computational Linguistics Volume 36, Number 3and Lin 2005).
Another way is to treat each sub-model of a generative model as a feature(Fraser and Marcu 2006).
In either case, a generative model can be regarded as a specialcase of a discriminative model where all feature weights are one.
A detailed discussionof the treatment of the IBM models as features can be found in Appendix B.One major drawback of the IBM models is asymmetry.
They are restricted such thateach source word is assigned to exactly one target word.
This is not the case for manylanguage pairs.
For example, in our running example, one Chinese word jianzhuye cor-responds to two English words construction industry.
As a result, our linear model willproduce only one-to-one alignments if the IBM models in two translation directions (i.e.,source-to-target and target-to-source) are both used.
Although some authors would usethe one-to-one assumption to simplify the modeling problem (Melamed 2000; Taskar,Lacoste-Julien, and Klein 2005), many translation phenomena cannot be handled andthe recall cannot reach 100% in principle.A more general way is to model alignment as an arbitrary relation between sourceand target language word positions.
As our linear model is capable of including manyoverlapping features regardless of their interdependencies, it is easy to add featuresthat characterize symmetric alignments.
In the following subsections, we will introducea number of symmetric features used in our experiments.3.1 Translation Probability ProductTo determine the correspondence of words in two languages, word-to-word translationprobabilities are always the most important knowledge source.
To model a symmetricalignment, a straightforward way is to compute the product of the translation probabil-ities of each link in two directions.For example, suppose that there is an alignment {(1, 2)} for a source languagesentence f1 f2 and a target language sentence e1e2; the translation probability prod-uct ist(e2| f1) ?
t( f1|e2)where t(e| f ) is the probability that f is translated to e and t( f |e) is the probability that eis translated to f , respectively.Unfortunately, the underlying model is biased: The more links added, the smallerthe product will be.
For example, if we add a link (2, 2) to the current alignment andobtain a new alignment {(1, 2), (2, 2)}, the resulting product will decrease after beingmultiplied with t(e2| f2) ?
t( f2|e2):t(e2| f1) ?
t( f1|e2) ?
t(e2| f2) ?
t( f2|e2)The problem results from the absence of empty cepts.
Following Brown et al (1993),a cept in an alignment is either a single source word or it is empty.
They assign ceptsto positions in the source sentence and reserve position zero for the empty cept.
Allunaligned target words are assumed to be ?aligned?
to the empty cept.
For example,in the current example alignment {(1, 2)}, the unaligned target word e1 is said to be?aligned?
to the empty cept f0.
As our model is symmetric, we use f0 to denote theempty cept on the source side and use e0 to denote the empty cept on the target side,respectively.312Liu, Liu, and Lin Discriminative Word Alignment by Linear ModelingIf we take empty cepts into account, the product for {(1, 2)} can be rewritten ast(e2| f1) ?
t( f1|e2) ?
t(e1| f0) ?
t( f2|e0)Similarly, the product for {(1, 2), (2, 2)} now becomest(e2| f1) ?
t( f1|e2) ?
t(e2| f2) ?
t( f2|e2) ?
t(e1| f0)Note that after adding the link (2, 2), the new product still has more factors than the oldproduct.
However, the new product is not necessarily always smaller than the old one.In this case, the new product divided by the old product ist(e2| f2) ?
t( f2|e2)t( f2|e0)Whether a new product increases or not depends on actual translation probabilities.2Depending on whether they are aligned or not, we divide the words in a sentencepair into two categories: aligned and unaligned.
For each aligned word, we use trans-lation probabilities conditioned on its counterpart in two directions (i.e., t(ei| fj) andt( fj|ei)).
For each unaligned word, we use translation probabilities conditioned on emptycepts on the other side in two directions (i.e., t(ei| f0) and t( fj|e0)).Formally, the feature function for translation probability product is given by3htpp(f, e, a) =?
( j,i)?a(log(t(ei| fj))+ log(t( fj|ei)))+J?j=1log(?
(?j, 0) ?
t( fj|e0) + 1 ?
?
(?j, 0))+I?i=1log(?
(?i, 0) ?
t(ei| f0) + 1 ?
?
(?i, 0))(19)where ?
(x, y) is the Kronecker function, which is 1 if x = y and 0 otherwise.
We definethe fertility of a source word fj as the number of aligned target words:?j =?
( j?,i)?a?
( j?, j) (20)2 Even though we take empty cepts into account, the bias problem still exists because the product willdecrease by adding new links if there are no unaligned words.
For example, the product will go down ifwe further add a link (1, 1) to {(1, 2), (2, 2)} as all source words are aligned.
This might not be a bad biasbecause reference alignments usually do not have all words aligned and contain too many links.Although translation probability product is degenerate as a generative model, the bias problem can bealleviated when this feature is combined with other features such as link count (see Section 3.8).3 We use the logarithmic form of translation probability product to avoid manipulating very smallnumbers (e.g., 4.3 ?
e?100) just for practical reasons.313Computational Linguistics Volume 36, Number 3Table 2Calculating feature values of translation probability product for a source sentence f1 f2 and atarget sentence e1e2.alignment feature value{} log(t(e1| f0) ?
t(e2| f0) ?
t( f1|e0) ?
t( f2|e0)){(1, 2)} log(t(e1| f0) ?
t(e2| f1) ?
t( f1|e2) ?
t( f2|e0)){(1, 2), (2, 2)} log(t(e1| f0) ?
t(e2| f1) ?
t(e2| f2) ?
t( f1|e2) ?
t( f2|e2))Similarly, the fertility of a target word ei is the number of aligned source words:?i =?
( j,i?
)?a?
(i?, i) (21)For example, as only one English word China is aligned to the first Chinese wordZhongguo in Figure 1, the fertility of Zhongguo is ?1 = 1.
Similarly, the fertility of thethird Chinese word duiwaikaifang is ?3 = 4 because there are four aligned Englishwords.
The fertility of the first English word The is ?1 = 0.
Obviously, the words withzero fertilities (e.g., The, ?s, and a in Figure 1) are unaligned.In Equation (19), the first term calculates the product of aligned words, the secondterm deals with unaligned source words, and the third term deals with unaligned targetwords.
Table 2 shows the feature values for some word alignments.For efficiency, we need to calculate the difference of feature values instead of thevalues themselves, which we call feature gain (see Equation (18)).
The feature gain fortranslation probability product is4gtpp(f, e, a, j, i) = log(t(ei| fj))+ log(t( fj|ei))?log(?
(?j, 0) ?
t( fj|e0) + 1 ?
?
(?j, 0))?log(?
(?i, 0) ?
t(ei| f0) + 1 ?
?
(?i, 0))(22)where ?j and ?i are the fertilities before adding the link ( j, i).Although this feature is symmetric, we obtain the translation probabilities t( f |e) andt(e| f ) by training the IBM models using GIZA++ (Och and Ney 2003).3.2 Exact MatchMotivated by the fact that proper names (e.g., IBM) or specialized terms (e.g., DNA) areoften the same in both languages, Taskar, Lacoste-Julien, and Klein (2005) use a featurethat sums up the number of words linked to identical words.
We adopt this exact matchfeature in our model:hem(f, e, a) =?
( j,i)?a?
( fj, ei) (23)4 For clarity, we use gtpp(f, e, a, j, i) instead of gtpp(f, e, a, l) because j and i appear in the equation.314Liu, Liu, and Lin Discriminative Word Alignment by Linear Modelinggem(f, e, a, j, i) = ?
( fj, ei) (24)3.3 Cross CountDue to the diversity of natural languages, word orders between two languages are usu-ally different.
For example, subject-verb-object (SVO) languages such as Chinese andEnglish often put an object after a verb while subject-object-verb (SOV) languages suchas Japanese and Turkish often put an object before a verb.
Even between SVO languagessuch as Chinese and English, word orders could be quite different too.
In Figure 1,while Zhongguo is the first Chinese word, its counterpart China is the fourth Englishword.
Meanwhile, the third Chinese word duiwaikaifang after Zhongguo is aligned to thesecond English word opening before China.
We say that there is a cross between the twolinks (1, 4) and (3, 2) because (1 ?
3) ?
(4 ?
2) < 0.
In Figure 1, there is only one cross.As a result, we could use the number of crosses in alignments to capture the divergenceof word orders between two languages.Formally, the cross count feature function is given byhcc(f, e, a) =?
( j,i)?a?
( j?,i?
)?a( j?
j?)
?
(i ?
i?)
< 0 (25)gcc(f, e, a, j, i) =?
( j?,i?
)?a( j?
j?)
?
(i ?
i?)
< 0 (26)where expr is an indicator function that takes a boolean expression expr as theargument:expr ={1 if expr is true0 otherwise(27)3.4 Neighbor CountMoore (2005) finds that word alignments between closely related languages tend to beapproximately monotonic.
Even for distantly related languages, the number of crossinglinks is far less than chance since phrases tend to be translated as contiguous chunks.In Figure 1, the dark points are positioned approximately in parallel with the diagonalline, indicating that the alignment is approximately monotonic.To capture such monotonicity, we follow Lacoste-Julien et al (2006) to encouragestrictly monotonic alignments by adding a bonus for any pair of links ( j, i) and ( j?, i?
)such thatj ?
j?
= 1 ?
i ?
i?
= 1In Figure 1, there is one such link pair: (3, 10) and (4, 11).
We call these linksneighbors.
Similarly, (5, 13) and (6, 14) are also neighbors.Formally, the neighbor count feature function is given byhnc(f, e, a) =?
( j,i)?a?
( j?,i?
)?a j ?
j?
= 1 ?
i ?
i?
= 1 (28)315Computational Linguistics Volume 36, Number 3gnc(f, e, a, j, i) =?
( j?,i?
)?a j ?
j?
= 1 ?
i ?
i?
= 1 (29)3.5 Fertility Probability ProductCasual inspection of some word alignments quickly establishes that some Chinesewords such as Zhongguo and chengxian are often aligned to one English word whereasother Chinese words such as duiwaikaifang tend to be translated into multiple Englishwords.
Brown et al (1993) call the number of target words to which a source word f isconnected the fertility of f .
Recall that we have given the formal definition of fertility inthe symmetric scenario in Equation (20) and Equation (21).Besides word association (Sections 3.1 and 3.2) and word distortion (Sections 3.3and 3.4), fertility also proves to be very important in modeling alignment becausesophisticated generative models such as the IBM Models 3?5 parameterize fertilitiesdirectly.
As our goal is to produce symmetric alignments, we calculate the product offertility probabilities in two directions.Given an alignment {(1,2)} for a source sentence f1 f2 and a target sentence e1e2, thefertility probability product isn(1| f0) ?
n(1| f1) ?
n(0| f2) ?
n(1|e0) ?
n(0|e1) ?
n(1|e2)where n(?j| fj) is the probability that fj has a fertility of ?j and n(?i|ei) is the probabilitythat ei has a fertility of ?i, respectively.5 For example, n(1| f0) denotes the probabilitythat one target word is ?aligned?
to the source empty cept f0 and n(1|e2) denotes theprobability that one source word is aligned to e2.If we add a link (2, 2) to the current alignment and obtain a new alignment{(1, 2), (2, 2)}, the resulting product will ben(1| f0) ?
n(1| f1) ?
n(1| f2) ?
n(0|e0) ?
n(0|e1) ?
n(2|e2)The new product divided by the old product isn(1| f2) ?
n(0|e0) ?
n(2|e2)n(0| f2) ?
n(1|e0) ?
n(1|e2)Formally, the feature function for fertility probability product is given byhfpp(f, e, a) =J?j=0log(n(?j| fj)) +I?i=0log(n(?i|ei)) (30)5 Brown et al (1993) treat the empty cept in a different way.
They assume that at most half of the sourcewords in an alignment are not aligned (i.e., ?0 ?
J/2) and define a binomial distribution relying on anauxiliary parameter p0.
Here, we use n(?0|e0) instead of the original form n0(?0|?Ii=1 ?i ) just forsimplicity.
See Appendix B for more details.316Liu, Liu, and Lin Discriminative Word Alignment by Linear ModelingThe corresponding feature gain isgfpp(f, e, a, j, i) = log(n(?0 ?
?
(?i, 0)| f0)) ?
log(n(?0| f0)) +log(n(?j + 1| fj) ?
log(n(?j| fj)) +log(n(?0 ?
?
(?j, 0)|e0)) ?
log(n(?0|e0)) +log(n(?i + 1|ei)) ?
log(n(?i|ei)) (31)where ?j and ?i are the fertilities before adding the link ( j, i).Table 3 gives the feature values for some word alignments.
In practice, we alsoobtain all fertility probabilities n(?j| fj) and n(?i|ei) by using the output of GIZA++directly.3.6 Linked Word CountWe observe that there should not be too many unaligned words in good alignments.For example, there are only three unaligned words on the target side in Figure 1: The,?s, and a. Unaligned words are usually function words that have little lexical meaningbut instead serve to express grammatical relationships with other words or specify theattitude or mood of the speaker.
To control the number of unaligned words, we followMoore, Yih, and Bode (2006) to introduce a linked word count feature that simply countsthe number of aligned words:hlwc(f, e, a) =J?j=1?j > 0 +I?i=1?i > 0 (32)glwc(f, e, a, j, i) = ?
(?j, 0) + ?
(?i, 0) (33)In Equation (33), ?j and ?i are the fertilities before adding l.3.7 Sibling DistanceIn word alignments, there are usually several words connected to the same word on theother side.
For example, in Figure 1, two English words construction and industry arealigned to one Chinese word jianzhuye.
We call the words aligned to the same word onthe other side siblings.
In Figure 1, opening, to, the, and outside are also siblings becausethey are aligned to duiwaikaifang.
A word (e.g., jianzhuye) often tends to produce a seriesof words in another language that belong together, whereas others (e.g., duiwaikaifang)Table 3Calculating feature values of fertility probability product for a source sentence f1 f2 and a targetsentence e1e2.alignment feature value{} log(n(2| f0) ?
n(0| f1) ?
n(0| f2) ?
n(2|e0) ?
n(0|e1) ?
n(0|e2)){(1, 2)} log(n(1| f0) ?
n(1| f1) ?
n(0| f2) ?
n(1|e0) ?
n(0|e1) ?
n(1|e2)){(1, 2), (2, 2)} log(n(1| f0) ?
n(1| f1) ?
n(1| f2) ?
n(0|e0) ?
n(0|e1) ?
n(2|e2))317Computational Linguistics Volume 36, Number 3tend to produce a series of words that should be separate.
To model this tendency, weintroduce a feature that sums up the distances between siblings.Formally, we use ?j,k to denote the position of the k-th target word aligned to asource word fj and use ?i,k to denote the position of the k-th source word aligned to atarget word ei.
For example, jianzhuye is the second source word (i.e., f2) in Figure 1.As the first target word aligned to f2 is construction (i.e., e6), therefore we say that?2,1 = 6.
Similarly, ?2,2 = 7 because industry (i.e., e7) is the second target word alignedto jianzhuye.
Obviously, ?j,k+1 is always greater than ?j,k by definition.As construction and industry are siblings, we define the distance between themas ?2,2 ?
?2,1 ?
1 = 0.
Note that we give no penalty to siblings that belong closelytogether.
In Figure 1, there are four siblings opening, to, the, and outside aligned to thesource word duiwaikaifang.
The sum of distances between them is calculated as?3,2 ?
?3,1 ?
1 + ?3,3 ?
?3,2 ?
1 + ?3,4 ?
?3,3 ?
1= ?3,4 ?
?3,1 ?
3= 10 ?
2 ?
3= 5Therefore, the distance sum of fj can be efficiently calculated as?
( j, ?j) ={?j,?j ?
?j,1 ?
?j + 1 if ?j > 10 otherwise(34)Accordingly, the distance sum of ei is?
(i, ?i) ={?i,?i ?
?i,1 ?
?i + 1 if ?i > 10 otherwise(35)Formally, the feature function for sibling distance is given byhsd(f, e, a) =J?j=1?
( j, ?j) +I?i=1?
(i, ?i) (36)The corresponding feature gain isgsd(f, e, a, j, i) = ?
( j, ?j + 1) ?
?
( j, ?j) +?
(i, ?i + 1) ??
(i, ?i) (37)where ?j and ?i are the fertilities before adding the link ( j, i).3.8 Link CountGiven a source sentence with J words and a target sentence with I words, there areJ ?
I possible links.
However, the actual number of links in a reference alignment isusually far less.
For example, there are only 10 links in Figure 1 although the maximumis 6 ?
14 = 84.
The number of links has an important effect on alignment quality because318Liu, Liu, and Lin Discriminative Word Alignment by Linear Modelingmore links result in higher recall while fewer links result in higher precision.
A goodtrade-off between recall and precision usually results from a reasonable number of links.Using the number of links as a feature could also alleviate the bias problem posed bythe translation probability product feature (see Section 3.1).
A negative weight of thelink count feature often leads to fewer links while a positive weight favors more links.Formally, the feature function for link count ishlc(f, e, a) = |a| (38)glc(f, e, a, l) = 1 (39)where |a| is the cardinality of a (i.e., the number of links in a).3.9 Link Type CountDue to the different fertilities of words, there are different types of links.
For instance,one-to-one links indicate that one source word (e.g., Zhongguo) is translated into ex-actly one target word (e.g., China) while many-to-many links exist for phrase-to-phrasetranslation.
The distribution of link types differs for different language pairs.
For ex-ample, one-to-one links occur more frequently in closely related language pairs (e.g.,French?English) and one-to-many links are more common in distantly related languagepairs (e.g., Chinese?English).
To capture the distribution of link types independent oflanguages, we use features to count different types of links.Following Moore (2005), we divide links in an alignment into four categories:1. one-to-one links, in which neither the source nor the target wordparticipates in other links;2. one-to-many links, in which only the source word participates in otherlinks;3. many-to-one links, in which only the target word participates in otherlinks;4. many-to-many links, in which both the source and target wordsparticipate in other links.In Figure 1, (1, 4), (4, 11), (5, 13), and (6, 14) are one-to-one links and the others areone-to-many links.As a result, we introduce four features:ho2o(f, e, a) =?
( j,i)?a?j = 1 ?
?i = 1 (40)ho2m(f, e, a) =?
( j,i)?a?j > 1 ?
?i = 1 (41)hm2o(f, e, a) =?
( j,i)?a?j = 1 ?
?i > 1 (42)hm2m(f, e, a) =?
( j,i)?a?j > 1 ?
?i > 1 (43)319Computational Linguistics Volume 36, Number 3Their feature gains cannot be calculated in a straightforward way because theaddition of a link might change the link types of its siblings on both the source andtarget sides.
For example, if we align the Chinese word chengxian and the English wordindustry, the newly added link (4, 7) is a many-to-many link.
Its source sibling (2, 7),which was a one-to-many link, now becomes a many-to-many link.
Meanwhile, itstarget sibling (4, 11), which was a one-to-one link, now becomes a one-to-many link.Algorithm 2 shows how to calculate the four feature gains.
After initialization(line 2), we first decide the type of l (lines 3?11).
Then, we consider the siblings of lon the target side (lines 12?24) and those on the source side (lines 25?38), respectively.Note that the feature gains of siblings will not change if ?i = 1 or ?j = 1.3.10 Bilingual DictionaryA conventional bilingual dictionary can be considered an additional knowledge source.The intuition is that a dictionary is expected to be more reliable than an automaticallytrained lexicon.
For example, if Zhongguo and China appear in an entry of a dictionary,they should be more likely to be aligned.
Thus, we use a single indicator feature toencourage linking word pairs that occur in a dictionary D:hbd(f, e, a, D) =?
( j,i)?a( fj, ei) ?
D (44)gbd(f, e, a, D, j, i) = ( fj, ei) ?
D (45)3.11 Link Co-Occurrence CountThe system combination technique that integrates predictions from multiple systemsproves to be effective in machine translation (Rosti, Matsoukas, and Schwartz 2007;He et al 2008).
In word alignment, a link should be aligned if it appears in mostsystem predictions.
Taskar, Lacoste-Julien, and Klein (2005) include the IBM Model 4predictions as features and obtain substantial improvements.To enable system combination, we design a feature to favor links voted by mostsystems.
Given an alignment a?
produced by another system, we use the number oflinks of the intersection of a and a?
as a feature:hlcc(f, e, a, a?)
= |a ?
a?| (46)glcc(f, e, a, a?, j, i) = l ?
a ?
a? (47)4.
ExperimentsIn this section, we try to answer two questions:1.
Does the proposed approach achieve higher alignment quality thangenerative alignment models?2.
Do statistical machine translation systems produce better translations ifwe replace generative alignment models with the proposed approach?In Section 4.1, we evaluate our approach on three word alignment shared tasks forfive language pairs with varying divergence and richness of resources.
Experimental320Liu, Liu, and Lin Discriminative Word Alignment by Linear ModelingAlgorithm 2 Calculating gains for the link type count features1: procedure GAINLINKTYPECOUNT(f, e, a, j, i)2: {go2o, go2m, gm2o, gm2m} ?
{0, 0, 0, 0}  initialize the feature gains3: if ?j = 0 ?
?i = 0 then  consider ( j, i) first4: go2o ?
go2o + 15: else if ?j > 0 ?
?i = 0 then6: go2m ?
go2m + 17: else if ?j = 0 ?
?i > 0 then8: gm2o ?
gm2o + 19: else10: gm2m ?
gm2m + 111: end if12: if ?j = 1 then  consider the siblings of ( j, i) on the target side13: for i?
= 1 .
.
.
I do14: if ( j, i?)
?
a ?
i?
= i then  ( j, i?)
is a sibling of ( j, i) on the target side15: if ?i?
= 1 then  ( j, i?)
was a one-to-one link16: go2o ?
go2o ?
117: go2m ?
go2m + 1  ( j, i?)
now becomes a one-to-many link18: else  ( j, i?)
was a many-to-one link19: gm2o ?
gm2o ?
120: gm2m ?
gm2m + 1  ( j, i?)
now becomes a many-to-many link21: end if22: end if23: end for24: end if25: if ?i = 1 then  consider the siblings of ( j, i) on the source side26: for j?
= 1 .
.
.
J do27: if ( j?, i) ?
a ?
j?
= j then  ( j?, i) is a sibling of ( j, i) on the source side28: if ?j?
= 1 then  ( j?, i) was a one-to-one link29: go2o ?
go2o ?
130: gm2o ?
gm2o + 1  ( j?, i) now becomes a many-to-one link31: else  ( j?, i) was a one-to-many link32: go2m ?
go2m ?
133: gm2m ?
gm2m + 1  ( j?, i) now becomes a many-to-many link34: end if35: end if36: end for37: end if38: return {go2o, go2m, gm2o, gm2m}  return the four feature gains39: end procedureresults show that our system outperforms systems participating in the three sharedtasks significantly and achieves comparable results with other state-of-the-art discrimi-native alignment models.In Section 4.2, we investigate the effect of our model on translation quality.
Bytraining feature weights with respect to F-measure instead of AER, our model resultsin superior translation quality over generative methods for phrase-based, hierarchicalphrase-based, and tree-to-string SMT systems.321Computational Linguistics Volume 36, Number 34.1 Evaluation of Alignment QualityIn this section, we present results of experiments on three word alignment shared tasks:1.
HLT/NAACL 2003 shared task (Mihalcea and Pedersen 2003).
As part ofthe HLT/NAACL 2003 workshop on ?Building and Using Parallel Texts:Data Driven Machine Translation and Beyond,?
this shared task includestwo language pairs: English?French and Romanian?English.
Participantscan use both limited and unlimited resources.2.
ACL 2005 shared task (Martin, Mihalcea, and Pedersen 2005).
As part ofthe ACL 2005 workshop on ?Building and Using Parallel Texts: DataDriven Machine Translation and Beyond,?
this shared task includes threelanguage pairs to cover different language and data characteristics:English?Inuktitut, Romanian?English, and English?Hindi.
Participantscan use both limited and unlimited resources.3.
HTRDP 2005 shared task.
As part of the 2005 HTRDP (National HighTechnology Research and Development Program of China, also called?863?
Program) Evaluation on Chinese Information Processing andIntelligent Human-Machine Interface Technology, this shared taskincluded only one language pair: Chinese?English.
Participants can useunlimited resources.Among these, we choose two tasks, English?French and Chinese?English, to reportdetailed experimental results.
Results for the other tasks can also be found in Table 11.Corpus statistics for the English?French and Chinese?English tasks are shown inTables 4 and 5.
The English?French data from the HLT/NAACL 2003 shared task consistof a training corpus of 1,130,104 sentence pairs, a development corpus of 37 sentencepairs, and a test corpus of 447 sentence pairs.
The development and test sets are manu-ally aligned and marked with both sure and possible labels.
Although the CanadianHansard bilingual corpus is widely used in the community, direct comparisons aredifficult due to the differences in splitting of training data, development data, and testdata.
To make our results more comparable to previous work, we followed Lacoste-Table 4Corpus characteristics of the English?French task.English FrenchTraining corpus Sentences 1,130,104Words 20.01M 23.61MVocabulary 68, 019 86, 591Development corpus Sentences 37Words 661 721Vocabulary 322 344Test corpus Sentences 447Words 7, 020 7, 761Vocabulary 1, 732 1, 943322Liu, Liu, and Lin Discriminative Word Alignment by Linear ModelingTable 5Corpus characteristics of the Chinese?English task.Chinese EnglishTraining corpus Sentences 837,594Words 10.32M 10.71MVocabulary 93, 532 134, 143Development corpus Sentences 502Words 9, 338 9, 364Vocabulary 2, 608 2, 587Test corpus Sentences 505Words 9, 088 10, 224Vocabulary 2, 319 2, 651Julien et al (2006) splitting the original test set into two parts: the first 200 sentencesas the development set and the remaining 247 sentences as the test set.
To comparewith systems participating in the 2003 NAACL shared task, we also used the smalldevelopment set of 37 sentences to optimize feature weights, and ran our system on theoriginal test set of 447 sentences.
The results are shown in Table 11.The Chinese?English data from the HTRDP 2005 shared task contains a develop-ment corpus of 502 sentence pairs and a test corpus of 505 sentence pairs.
We usea training corpus of 837, 594 sentence pairs available from Chinese Linguistic DataConsortium and a bilingual dictionary containing 415, 753 entries.4.1.1 Comparison of the Search Algorithm with GIZA++.
We develop a word alignmentsystem named Vigne based on the linear modeling approach.
As we mentioned before,our model can include the IBM models as features (see Appendix B).
To investigate theeffectiveness of our search algorithm, we compare Vigne with GIZA++ by using thesame models.Table 6 shows the alignment error rate percentages for various IBM models inGIZA++ and Vigne.
To make the results comparable, we ensured that Vigne sharedTable 6Comparison of AER scores for various IBM models in GIZA++ and Vigne.
These models aretrained only on development and test sets.
The pruning setting for Vigne is ?
= 0 and b = 1.
Alldifferences are not statistically significant.English?French Chinese?EnglishModel Training Scheme Direction GIZA++ Vigne GIZA++ VigneModel 1 15 S ?
T 50.6 50.6 58.0 58.0T ?
S 46.2 46.2 56.1 56.1Model 2 1525 S ?
T 47.8 47.8 59.3 59.3T ?
S 43.6 43.6 57.4 57.4Model 3 15H533 S ?
T 31.6 31.4 45.0 44.5T ?
S 27.9 27.9 47.4 46.5Model 4 15H53343 S ?
T 34.5 34.2 44.9 44.6T ?
S 30.8 30.6 46.7 46.4323Computational Linguistics Volume 36, Number 3the same parameters with GIZA++.6 Table 6 also gives the training schemes used forGIZA++.
For example, the training scheme for Model 4 is 15H53343.
This notationindicates that five iterations of Model 1, five iterations of HMM, three iterations ofModel 3, and three iterations of Model 4 are performed.
As the two systems use the samemodel parameters, the amount of training data will have no effect on the comparison.Therefore, we trained the IBM Models only on the development and test sets.
As a re-sult, the AER scores in Table 6 look quite high.In GIZA++, there exist simple polynomial algorithms to find the Viterbi alignmentsfor Models 1 and 2.
We observe that the greedy search algorithm (?
= 0 and b = 1)used by Vigne can also find the optimal alignments.
Note that the two systems achieveidentical AER scores because there are no search errors.For Models 3 and 4, maximization over all alignments cannot be efficiently carriedout as the corresponding search problem is NP-complete.
To alleviate the problem,GIZA++ resorts to a greedy search algorithm.
The basic idea is to compute a Viterbialignment of a simple model such as Model 2 or HMM.
This alignment (an intermediatenode in the search space) is then iteratively improved with respect to the alignmentprobability of the refined model by moving or swapping links.
In contrast, our searchalgorithm starts from an empty alignment and has only one operation: adding a link.In addition, we treat the fertility probability of an empty cept in a different way (seeEquation B.7).
Interestingly, Vigne achieves slightly better results than GIZA++ for bothmodels.
All differences are not statistically significant.4.1.2 Comparison to Generative Models Using Asymmetric Features.
Table 7 compares theAER scores achieved by GIZA++, Cross-EM (Liang, Taskar, and Klein 2006), and Vigne.On both tasks, we lowercased all English words in the training, development, andtest sets as a preprocessing step.
For GIZA++, we used the default training schemeof 15H53545.
We used the three symmetrization heuristics proposed by Och and Ney(2003): intersection, union, and refined method.
For Cross-EM, we also used the defaultconfiguration and jointly trained Model 1 and HMM for five iterations.
For Vigne, weused a greedy search strategy by setting ?
= 0 and b = 1.
Note that both GIZA++ andCross-EM are unsupervised alignment methods.On the English?French task, the refined combination of Model 4 alignments pro-duced by GIZA++ in both translation directions yields an AER of 5.9%.
Cross-EMoutperforms GIZA++ significantly by achieving 5.1%.
For Vigne, we use Model 4 asthe primary feature.
The linear combination of Model 4 in both directions achievesa lower AER than either one separately.
The link count feature controls the numberof links in the resulting alignments and leads to an absolute improvement of 0.1%.With the addition of cross count and neighbor count features, the AER score drops to5.4%.
We attribute this to the fact that the two features are capable of capturing thelocality and monotonicity properties of natural languages, especially for closely relatedlanguage pairs such as English?French.
After adding the linked word count feature, ourmodel achieves an AER of 5.2%.
Finally, Vigne achieves an AER of 4.0% by combiningpredictions from refined Model 4 and jointly trained HMM.On the Chinese?English task, one-to-many and many-to-one relationships occurmore frequently in the reference alignments than the English?French task.
As Cross-EM6 In GIZA++ training, the final parameters are estimated on the final alignments, which are computedusing the parameters obtained in the previous iteration.
As a result, Vigne made use of the parametersgenerated by the iteration before the final iteration.
In other experiments, Vigne used the final parameters.324Liu, Liu, and Lin Discriminative Word Alignment by Linear ModelingTable 7Comparison of GIZA++, Cross-EM, and Vigne on both tasks.
Note that Vigne yields onlyone-to-one alignments if both ?Model 4 s2t?
and ?Model 4 t2s?
features are used.
The pruningsetting for Vigne is ?
= 0 and b = 1.
While the final results of our system are better than the bestbaseline generative models significantly at p < 0.01, adding a single feature will not alwaysproduce a significant improvement, especially for English?French.System Setting English?French Chinese?EnglishModel 4 s2t 7.7 20.9Model 4 t2s 9.2 30.3GIZA++ Intersection 6.8 21.8Union 9.6 28.1Refined method 5.9 18.4Cross-EM HMM, joint 5.1 18.9Model 4 s2t 7.8 20.5+Model 4 t2s 5.6 18.3+link count 5.5 17.7+cross count 5.4 17.6+neighbor count 5.2 17.4Vigne+exact match 5.3 -+linked word count 5.2 17.3+bilingual dictionary - 17.1+link co-occurrence count (GIZA++) 5.1 16.3+link co-occurrence count (Cross-EM) 4.0 15.7is prone to produce one-to-one alignments by encouraging agreement, symmetrizingModel 4 by refined method yields better results than Cross-EM.
We observe that the ad-vantages of adding features such as link count, cross count, neighbor count, and linkedword count to our linear model continue to hold, resulting in a much lower AER thanboth GIZA++ and Cross-EM.
The addition of the bilingual dictionary is beneficial andyields an AER of 17.1%.
Further improvements were obtained by including predictionsfrom GIZA++ and Cross-EM.As the IBM models do not allow a source word to be aligned with more than onetarget word, the activation of the IBM models in both directions always yields one-to-one alignments and thus has a loss in recall.
To alleviate this problem, we use aheuristic postprocessing step to produce many-to-one or one-to-many alignments.
First,we collect links that have higher translation probabilities than corresponding null linksin both directions.
Then, these candidate links are sorted according to their translationprobabilities.
Finally, they are added to the alignments under structural constraintssimilar to those of Och and Ney (2003).On the English?French task, this symmetrization method achieves relatively smallbut very consistent improvements ranging from 0.1% to 0.2%.
On the Chinese?Englishtask, the improvements are more significant, ranging from 0.1% to 0.8%.
This differ-ence also results from the fact that the reference alignments of the Chinese?Englishtask contain more one-to-many and many-to-one relationships than the English?Frenchtask.
After symmetrization, the final AER scores for the two tasks are 3.8% and 15.1%,respectively.4.1.3 Resulting Feature Weights.
Table 8 shows the resulting feature weights of minimumerror rate training.
We observe that adding new features has an effect on the weights325Computational Linguistics Volume 36, Number 3Table 8Resulting feature weights of minimum error rate training on the Chinese?English task (M4ST:Model 4 s2t; M4TS: Model 4 t2s; LC: link count; CC: cross count; NC: neighbor count; LWC:linked word count; BD: bilingual dictionary; LCCG: link co-occurrence count (GIZA++); LCCC:link co-occurrence count (Cross-EM)).M4ST M4TS LC CC NC LWC BD LCCG LCCCM4ST 1.00 - - - - - - - -+M4TS 0.63 0.37 - - - - - - -+LC 0.18 0.07 ?0.75 - - - - - -+CC 0.19 0.07 ?0.56 ?0.18 - - - - -+NC 0.12 0.06 ?0.55 ?0.08 0.17 - - - -+LWC 0.14 0.08 ?0.22 ?0.08 0.25 ?0.26 - - -+BD 0.07 0.02 ?0.35 ?0.05 0.16 0.01 0.34 - -+LCCG 0.03 0.04 ?0.13 ?0.05 0.20 ?0.16 0.28 0.11 -+LCCC 0.02 0.02 0.14 ?0.03 0.10 ?0.26 0.30 0.04 0.09of other features.
The weights of the cross count feature are consistently negative,suggesting that crossing links are always discouraged for Chinese?English.
Also, thepositive weights of the neighbor count feature indicate that monotonic alignmentsare encouraged.
When the bilingual dictionary was included, the weights of Model 4features in both directions dramatically decreased.4.1.4 Results of the Symmetric Alignment Model.
As we mentioned before, the linear modelcan model many-to-many alignments directly without any postprocessing symmetriza-tion heuristics.Table 9 demonstrates the results of the symmetric alignment model on both tasks.As the activation of translation and fertility probability products allows for arbitraryrelationships, the addition of the link count feature excludes most loosely related linksTable 9AER scores achieved by the symmetric alignment model on both tasks.
The pruning setting forVigne is ?
= 0 and b = 1.
Although the final model obviously outperforms the initial modelsignificantly at p < 0.01, adding a single feature will not always result in a significantimprovement, especially for English?French.Features English?French Chinese?Englishtranslation probability product 17.3 23.6+fertility probability product 14.6 22.6+link count 14.5 21.6+cross count 5.8 18.5+neighbor count 5.2 17.2+exact match 5.1 -+linked word count 5.2 17.0+link types 5.0 16.9+sibling distance 4.9 16.2+bilingual dictionary - 15.9+link co-occurrence count (GIZA++) 4.5 15.1+link co-occurrence count (Cross-EM) 3.7 14.5326Liu, Liu, and Lin Discriminative Word Alignment by Linear Modelingand results in more significant improvements than for asymmetric IBM models.
Oneinteresting finding is that the cross count feature is very useful, leading to dramaticabsolute reduction of 8.7% on the English?French task and 3.1% on the Chinese?Englishtask, respectively.
We find that the advantages of adding neighbor count and linkedword count still hold.
By further including predictions from GIZA++ and Cross-EM,our linear model achieves the best result: 3.7% on the English?French task and 14.5%on the Chinese?English task.We find that the symmetric linear model outperforms the asymmetric one, espe-cially on the Chinese?English task.
This suggests that although the asymmetric modelcan produce symmetric alignments via symmetrization heuristics, the ?genuine?
sym-metric model produces many-to-many alignment in a more natural way.4.1.5 Effect of Beam Search.
Table 10 shows the effect of varying beam widths.
The aligningspeed (words per second) decreases almost linearly with the increase of beam width b.For simple alignment models such as using only the translation probability productfeature, enlarging the beam size fails to bring improvements due to modeling errors.When more features are added, the model becomes more expressive.
Therefore, oursystem benefits from larger beam size consistently, although some benefits are notsignificant statistically.
When we set b = 10, the final AER scores for the English?Frenchand Chinese?English tasks are 3.6% and 14.3%, respectively.4.1.6 Effect of Training Corpus Size.
One disadvantage of our approach is that we needa hand-aligned training corpus for training feature weights.
However, compared withbuilding a treebank, manual alignment is far less expensive because one annotator onlyneeds to answer yes?no questions: Should this pair of words be aligned or not?
If welltrained, even a non-linguist who is familiar with both source and target languages couldTable 10Comparison of aligning speed (words per second) and AER score with varying beam widths forthe Chinese?English task.
We fix ?
= 0.01.
Bold numbers refer to the results that are better thanthe baseline but not significantly so.
We use ?+?
to denote the results that outperform the bestbaseline (b = 1) and are statistically significant at p < 0.05.
Similarly, we use ?++?
to denotesignificantly better than baseline at p < 0.01.b=1 b=5 b=10Features w/sec AER w/sec AER w/sec AERtranslation probability product 3, 941 23.6 843 23.6 426 23.7+fertility probability product 1, 418 22.6 300 22.7 150 22.9+link count 1, 557 21.6 330 21.7 166 21.9+cross count 1, 696 18.5 359 18.6 180 18.6+neighbor count 1, 648 17.2 355 16.8+ 178 16.7++linked word count 1, 627 17.0 351 16.4+ 176 16.5++sibling distance 1, 531 16.9 326 16.5+ 165 16.4++link types 899 16.2 187 15.6+ 96 15.5+++bilingual dictionary 890 15.9 187 15.6 94 15.5++link co-occurrence count (GIZA++) 877 15.1 182 15.0 92 14.9+link co-occurrence count (Cross-EM) 867 14.5 183 14.4 92 14.3327Computational Linguistics Volume 36, Number 3produce high-quality alignments.
We estimate that aligning a sentence pair usuallytakes only two minutes on average.An interesting question is: How many training examples are needed to train agood discriminative model?
Figure 4 shows the learning curves with different numbersof features on the Chinese?English task.
We choose four feature groups with varyingnumbers of features: 3, 6, 10, and 14.
There are eight fractions of the training corpus: 10,20, 50, 100, 200, 300, 400, and 502.
Generally, the more features a model uses, the moretraining examples are needed to train feature weights.
Surprisingly, even when we use14 features, 50 sentences seem to be good enough for minimum error rate training.
Thisfinding suggests that our approach could work well even with a quite small trainingcorpus.4.1.7 Summary of Results.
Table 11 summarizes the results on all three shared tasks.Vigne used the same configuration for all tasks.
We used the symmetric linear modeland activated all features.
The pruning setting is ?
= 0.01 and b = 10.
Our systemoutperforms the systems participating in all the three shared tasks significantly.Note that for the English?French task we used the small development set of 37sentences to optimize feature weights, and ran our system on the original test set of447 sentences.
For the Romanian?English language pair, we follow Fraser and Marcu(2006) in reducing the vocabulary by stemming Romanian and English words down totheir first four characters.
For the other language pairs, English?Inuktitut and English?Hindi, the symmetric linear model maintains its superiority over the asymmetric linearmodel and yields better results than the other participants.Figure 4Effect of training corpus size on the Chinese?English task.
We choose four feature groups withvarying numbers of features: 3, 6, 10, and 14.
There are eight training corpora with varyingnumbers of sentence pairs: 10, 20, 50, 100, 200, 300, 400, and 502.328Liu, Liu, and Lin Discriminative Word Alignment by Linear ModelingTable 11Comparison with the systems participating in the three shared tasks.
?non-null?
denotes that thereference alignments have no null links, ?null?
denotes that the reference alignments have nulllinks, ?limited?
denotes only limited resources can be used, and ?unlimited?
denotes that thereare no restrictions on resources used.Shared Task Task Participants VigneRomanian?English, non-null, limited 28.9?52.7 23.5Romanian?English, null, limited 37.4?59.8 26.9HLT-NAACL 2003English?French, non-null, limited 8.5?29.4 4.0English?French, null, limited 18.5?51.7 4.6English?Inuktitut, limited 9.5?71.3 8.9ACL 2005 Romanian?English, limited 26.6?44.5 24.7English?Hindi, limited 51.4 44.8HTRDP 2005 Chinese?English, unlimited 23.5?49.2 14.34.1.8 Comparison to Other Work.
In the word alignment literature, the Canadian Hansardbilingual corpus is the most widely used data set.
Table 12 lists alignment error ratesachieved by previous work and our system.
Note that direct comparisons are problem-atic due to the different configurations of training data, development data, and test data.Our result matches the state-of-the-art performance on the Hansard data (Lacoste-Julienet al 2006; Moore, Yih, and Bode 2006).4.2 Evaluation of Translation QualityIn this section, we report on experiments with Chinese-to-English translation.
To inves-tigate the effect of our discriminative model on translation performance, we used threetranslation systems:1.
Moses (Koehn and Hoang 2007), a state-of-the-art phrase-based SMTsystem;2.
Hiero (Chiang 2007), a state-of-the-art hierarchical phrase-based system;3.
Lynx (Liu, Liu, and Lin 2006), a linguistically syntax-based system thatmakes use of tree-to-string rules.Table 12Comparison of some word alignment systems on the Canadian Hansard data.System Training Test AEROch and Ney (2003) 1.5M 500 5.2Moore (2005) 500K 223 7.5Taskar, Lacoste-Julien, and Klein (2005) 1.1M 347 5.4Liang, Taskar, and Klein (2006) 1.1M 347 4.9Lacoste-Julien et al (2006) 1.1M 247 3.8Blunsom and Cohn (2006) 1.1M 347 5.2Moore, Yih, and Bode (2006) 1.1M 223 3.7This work 1.1M 247 3.6329Computational Linguistics Volume 36, Number 3For all three systems we trained the translation models on the FBIS corpus(7.2M+9.2M words).
For the language model, we used the SRI Language ModelingToolkit (Stolcke 2002) to train a trigram model with modified Kneser-Ney smoothingon the Xinhua portion of the Gigaword corpus.
We used the 2002 NIST MT evaluationtest set as the development set for training feature weights of translation systems, the2005 test set as the devtest set for choosing optimal values of ?
for different translationsystems, and the 2008 test set as the final test set.
Our evaluation metric is case-sensitiveBLEU-4, as defined by NIST, that is, using the shortest (as opposed to closest) referencelength for brevity penalty.We annotated the first 200 sentences of the FBIS corpus using the Blinker guidelines(Melamed 1998).
All links are sure ones.
These hand-aligned sentences served as thetraining corpus for Vigne.
To train the feature weights in our discriminative modelusing minimum-error-rate training (Och 2003), we adopt balanced F-measure (Fraserand Marcu 2007b) as the optimization criterion.The pipeline begins by running GIZA++ and Cross-EM on the FBIS corpus.
Weused seven generative alignment methods based on IBM Model 4 and HMM as baselinesystems: (1) C?E, (2) E?C, (3) intersection, (4) union, (5) refined method (Och andNey 2003), (6) grow-diag-final (Koehn, Och, and Marcu 2003), and (7) Cross-EM (Liang,Taskar, and Klein 2006).
Instead of exploring the entire search space, our linear modelonly searches within the union of baseline predictions, which enables our system toalign large bilingual corpus at a very fast speed of 3, 000 words per second.
In otherwords, our system is able to annotate the FBIS corpus in about 1.5 hours.
Then, we trainthe feature weights of the linear model on the training corpus with respect to F-measureunder different settings of ?.
After that, our system runs on the FBIS corpus to produceword alignments using the optimized weights.
Finally, the three SMT systems train theirmodels on the word-aligned FBIS corpus.Can our approach achieve higher F-measure scores than generative methods withdifferent values of ?
(the weighting factor in F-measure)?
Table 13 shows the results ofall the systems on the development set.
To estimate the loss from restricting the searchTable 13Maximization of F-measure with different settings of ?
(the weighting factor in the balancedF-measure).
We use IBM Model 4 and HMM as baseline systems.
Our system restricts the searchspace by exploring only the union of baseline predictions.
We compute the ?oracle?
alignmentsby intersecting the union with reference alignments.
We use ?+?
to denote the result thatoutperforms the best baseline result with statistical significance at p < 0.05.
Similarly, we use?++?
to denote significantly better than baseline at p < 0.01.?
= 0.1 ?
= 0.3 ?
= 0.5 ?
= 0.7 ?
= 0.9IBM Model 4 C?E 82.6 81.3 80.1 79.0 77.8IBM Model 4 E?C 68.2 70.5 73.0 75.7 78.5IBM Model 4 intersection 63.6 68.9 75.2 82.8 92.1IBM Model 4 union 86.6 82.0 77.9 74.2 70.8IBM Model 4 refined method 75.4 78.5 81.8 85.4 89.4IBM Model 4 grow-diag-final 82.4 82.1 81.7 81.4 81.1Cross-EM HMM 70.4 73.7 77.3 81.2 85.5oracle 91.9 93.6 95.3 97.1 99.0Vigne 87.8++ 85.8++ 86.4++ 88.6++ 93.3++330Liu, Liu, and Lin Discriminative Word Alignment by Linear Modelingspace, we compute oracle alignments by intersecting the union of baseline predictionswith reference alignments.
The F-measures achieved by oracle alignments range from91.9 to 99.0, indicating that the union of baseline predictions is good enough to approx-imate the true search space.
We observe that C?E, union, and grow-diag-final weightrecall higher because F-measure decreases when ?
increases.
On the other hand, E?C,intersection, refined method, and Cross-EM weight precision higher.
In particular, ?has a weak effect on grow-diag-final as its F-measure always keeps above 0.8 when ?is varied.
For each ?, we trained a set of feature weights to maximize the F-measure onthe development set.
We find that our discriminative model outperforms the baselinesystems significantly at all values of ?.Table 14 shows the BLEU scores of the three systems on the devtest set.
For Mosesand Hiero, we used the default setting.
For Lynx, we used the phrase pairs learned byMoses to improve rule coverage (Liu, Liu, and Lin 2006).
The best generative alignmentmethod is grow-diag-final, which is widely used in SMT.
For all the three SMT systems,our system outperforms the baseline systems statistically significantly.
For Moses, thebest value of ?
is 0.5.
For Hiero and Lynx, the best ?
is 0.3, suggesting that recall-oriented alignments yield better translation performance.Table 15 gives the BLEU scores of the three systems on the final test set.
We used theparameters optimized on the dev and devtest sets.
More specifically, Moses used grow-diag-final and ?
= 0.5, Hiero used grow-diag-final and ?
= 0.3, and Lynx used unionand ?
= 0.3.
We find that our discriminative alignment model improves the threesystems significantly.5.
Related WorkThe first generative alignment models were the IBM Models 1?5 proposed by Brownet al (1993).
Vogel and Ney (1996) propose a first-order Hidden Markov model (HMM)for word alignment.
They show that it is beneficial to make the alignment probabilitiesdependent on differences in position rather than on the absolute positions.
Och andTable 14BLEU scores on the devtest set.
We use ?+?
to denote the result that outperforms the bestbaseline result (highlighted in bold) statistically significantly at p < 0.05.
Similarly, we use ?++?to denote significantly better than baseline at p < 0.01.Moses Hiero LynxIBM Model 4 C?E 24.7 25.7 24.8IBM Model 4 E?C 20.6 23.5 21.6IBM Model 4 intersection 20.1 23.2 21.2IBM Model 4 union 24.3 24.1 25.1IBM Model 4 refined method 24.2 24.0 24.2IBM Model 4 grow-diag-final 25.0 25.8 24.3Cross-EM HMM 23.6 24.9 24.8?
= 0.1 tuned 23.9 25.3 26.0++?
= 0.3 tuned 24.9 26.8++ 26.1++Vigne ?
= 0.5 tuned 25.7+ 26.6++ 24.3?
= 0.7 tuned 23.7 25.4 24.7?
= 0.9 tuned 21.9 24.7 23.9331Computational Linguistics Volume 36, Number 3Table 15BLEU scores on the final test set.
We use the parameters optimized on the dev and devtest sets.We use ?+?
to denote the result that outperforms the best baseline result (indicated in bold)statistically significantly at p < 0.05.
Similarly, we use ?++?
to denote significantly better thanbaseline at p < 0.01.Moses Hiero Lynxgenerative 20.1 20.7 19.9discriminative 20.8+ 21.6+ 21.0++Ney (2003) re-implement the IBM models and the HMM model and compare them withheuristic approaches systematically.
The resulting toolkit GIZA++ developed by FranzJ.
Och is the most popular alignment system nowadays.
Liang, Taskar, and Klein (2006)present an unsupervised way to produce symmetric alignments by training two simpleasymmetric models (e.g., IBM Model 1 and the HMM model) jointly to maximize acombination of data likelihood and agreement between the models.
Fraser and Marcu(2007a) introduce a new generative model called LEAF that directly models many-to-many non-consecutive word alignments.
Their model can be trained using bothunsupervised and semi-supervised training methods.Recent years have witnessed the rapid development of discriminative alignmentmethods.
As a first attempt, Och and Ney (2003) proposed the Model 6, which is alog-linear combination of the IBM models and the HMM model.
Cherry and Lin (2003)develop a statistical model to find word alignments, which allows for easy integrationof context-specific features.
Liu, Liu, and Lin (2005) apply the log-linear model usedin SMT (Och and Ney 2002) to word alignment and report significant improvementsover the IBM models.
Moore (2005) presents a discriminative framework for wordalignment and uses averaged perceptron for parameter optimization.
Taskar, Lacoste-Julien, and Klein (2005) treat the alignment prediction task as a maximum weightbipartite matching problem and use the large-margin method to train feature weights.Neural networks and transformation-based learning have also been introduced to wordalignment (Ayan, Dorr, and Monz 2005a, 2005b).
Blunsom and Cohn (2006) propose anew discriminative model based on conditional random fields (CRF).
Fraser and Marcu(2006) use sub-models of IBM Model 4 as features and train feature weights using asemi-supervised algorithm.
Ayan and Dorr (2006b) use a maximum entropy model tocombine word alignments.
Cherry and Lin (2006) show that introducing soft syntacticconstraints through discriminative training can improve alignment quality.
Lacoste-Julien et al (2006) extend the bipartite matching model of Taskar, Lacoste-Julien, andKlein (2005) by including fertility and first-order interactions.
Recently, max-productbelief propagation has been successfully applied to discriminative word alignment(Niehues and Vogel 2008; Cromiere`s and Kurohashi 2009).
Haghighi et al (2009) investi-gate supervised word alignment methods that exploit inversion transduction grammar(ITG) constraints.Our work can be seen as an application of the linear model (Och 2003) in wordalignment.
While aiming at producing symmetric word alignments in a discriminativeway, our approach uses asymmetric generative models (Brown et al 1993) as the majorinformation sources.
Our linear model is similar to that of Moore, Yih, and Bode (2006).They train two linear models called stage 1 and stage 2.
The feature values are extractedfrom word-aligned sentence pairs.
After the stage 1 model aligns the entire trainingcorpus automatically, the stage 2 model uses features based not only on the parallel332Liu, Liu, and Lin Discriminative Word Alignment by Linear Modelingsentences themselves but also on statistics of the alignments produced by the stage 1model.
They use average perceptron and support vector machine (SVM) to train featureweights and use a beam search algorithm to find the most probable alignments.
Table 12shows that the two methods achieve comparable results on the Hansard data, confirm-ing Moore, Yih, and Bode?s (2006) claim that model structure and feature selection aremore important than discriminative training method.6.
Conclusions and Future WorkWe have presented a discriminative framework for word alignment based on the linearmodeling approach.
This framework is easy to extend by including features that char-acterize the aligning process.
In addition, our approach supports symmetric alignmentmodeling that allows for an arbitrary relationship between source and target languagepositions.
As the linear model offers excellent flexibility in using a large variety offeatures and in combining information from various sources, it is able to produce goodpredictions on language pairs that are either closely related (e.g., English?French) or dis-tantly related (e.g., English?Inuktitut), either with rich resources (e.g., Chinese?English)or with scarce resources (e.g., English?Hindi).
We further show that our approach canbenefit different types of SMT systems: phrase-based, hierarchical phrase-based, andsyntax-based.The real benefit of our model does not stem from the use of the linear model, butrather from the discriminative training that optimizes feature weights with respect toevaluation metrics on the gold-standard word alignments.
One disadvantage of ourapproach is the need for annotated training data.
Although we have shown that avery small number of training examples would be enough for parameter estimation(Section 4.1.6), it is difficult to select such a representative training corpus to ensure thatthe model will work well on unseen data, especially when the bilingual corpus to bealigned consists of parallel texts from different domains.Another problem is that it is hard to find an evaluation metric for word alignmentthat correlates well with translation quality because the relationship between alignmentand translation is still not quite clear.
Without a good loss function, discriminativemodels cannot outperform generative models in large-scale applications.
Therefore, it isimportant to investigate how to select training examples and how to choose optimiza-tion criterion.The design of feature functions is most important for a discriminative alignmentmodel.
Often, we need to try various feature groups manually on the development setto determine the optimal feature group.
Furthermore, a feature group optimized for onelanguage pair may not have the same effect on another one.
In the future, we plan toinvestigate an algorithm for automatic feature selection.Appendix A: Table of Notationf source sentencefS1 sequence of source sentences: f1, .
.
.
, fs, .
.
.
, fSf source wordJ length of fj position in f, j = 1, 2, .
.
.
, Jfj the j-th word in ff0 empty cept on the source side333Computational Linguistics Volume 36, Number 3e target sentenceeS1 sequence of target sentences: e1, .
.
.
, es, .
.
.
, eSe target wordI length of ei position in e, i = 1, 2, .
.
.
, Iei the i-th word in ee0 empty cept on the target sidea alignmentl a link ( j, i) in a?j number of positions of e connected to position j of f?i number of positions of f connected to position i of e?j,k position of the k-th target word aligned to fj?i,k position of the k-th source word aligned to ei?
( j, ?j) sum of sibling distances for fj?
(i, ?i) sum of sibling distances for eiscore(f, e, a) a score that indicates how well a is the alignment between f and ea?
the best candidate alignment?
feature weight?
the feature weight being optimizedh(f, e, a) feature functionG(f, e, a, l) link gain after adding l to ag(f, e, a, l) feature gain after adding l to a?
(f, e, a) value of the feature being optimized?
(f, e, a) dot-product of fixed featurest(e| f ) the probability that f is translated to et( f |e) the probability that e is translated to fn(?| f ) the probability that f has a fertility of ?n(?|e) the probability that e has a fertility of ?r reference alignmentCs set of candidate alignments for the s-th training exampleas,k the k-th candidate alignment for the s-th training exampleE(r, a) loss function that measures alignment quality?
the precision/recall weighting factor in balanced F-measure?
pruning threshold in the beam search algorithmb beam size in the beam search algorithm?
(x, y) the Kronecker function, which is 1 if x = y and 0 otherwiseexpr an indicator function taking a boolean expression expr as the argumentAppendix B: Using the IBM Models as Feature FunctionsIn this article, we use IBM Models 1?4 as feature functions by taking the logarithm of themodels themselves rather than the sub-models just for simplicity.
It is easy to separateeach sub-model as a feature as suggested by Fraser and Marcu (2006).
We distinguish334Liu, Liu, and Lin Discriminative Word Alignment by Linear Modelingbetween two translation directions (i.e., source-to-target and target-to-source) to use theIBM models as feature functions.
All model parameters are estimated by GIZA++ (Ochand Ney 2003).The feature function for the IBM Model 1 ishm1 (f, e, a) = log(( J|I)(I + 1)JJ?j=1t( fj|eaj ))(B.1)where ( J|I) predicts the length of the source sentence conditioned on that of the targetsentence, (I + 1)?J defines a uniform distribution of the alignment between source andtarget words, and t( fj|ei) is a translation sub-model.
Note that aj = i, which means thatfj is connected to ei.The corresponding feature gain isgm1 (f, e, a, l) = log(t( fj|ei)) ?
log(t( fj|e0))(B.2)where fj and ei are linked by l and e0 is the empty cept to which all unaligned sourcewords are ?aligned.
?Based on a similar generative story to Model 1, Model 2 replaces the uniformalignment probability distribution with an alignment sub-model a(i| j, I, J).
This sub-model assumes that the position of ei depends on the position of its translation fj andsentence lengths I and J.The feature function for Model 2 ishm2 (f, e, a) = log(( J|I)J?j=1t( fj|eaj )a(aj| j, I, J))(B.3)The corresponding feature gain isgm2 (f, e, a, l) = log(t( fj|ei)) ?
log(t( fj|e0)) +log(a(i| j, I, J)) ?
log(a(0| j, I, J)) (B.4)where fj and ei are linked by l and 0 is the index of the empty cept e0.Model 3 is a fertility-based model that parameterizes fertility of words.
UnlikeModel 2, Model 3 uses a fertility sub-model n(?i|ei) and a distortion sub-model d( j|i, I, J).Formally, the feature function of Model 3 is given byhm3 (a, f, e) = log(n0(?0|I?i=1?i)I?i=1n(?i|ei)?i!J?j=1t( fj|eaj )?
?j:aj =0d( j|i, I, J))(B.5)Brown et al (1993) treat n0(?0|?Ii=1 ?i), the fertility probability of e0, in a differ-ent way.
They assume that at most half of the source words in an alignment are not335Computational Linguistics Volume 36, Number 3aligned (i.e., ?0 ?
J2 ) and define a binomial distribution relying on an auxiliary parame-ter p0:n0(?0|I?i=1?i)={(J?
?0?0)pJ?2?00 (1 ?
p0)?0 if ?0 ?J20 otherwise(B.6)Note that we follow Brown et al (1993) in replacing?Ii=1 ?i with J ?
?0 for simplicity.The original form should be (?Ii=1?i?0)p?Ii=1?i?
?00 (1 ?
p0 )?0 .However, this assumption results in a problem for our search algorithm that beginswith an empty alignment (see Algorithm 1), for which ?0 is J and the feature valuehm3 (f, e, a) is negative infinity.
To alleviate this problem, we modify Equation B.6 slightlyby adding a smoothing parameter pn ?
(0, 1):n0(?0|I?i=1?i)={(J?
?0?0)pJ?2?00 (1 ?
p0)?0pn if ?0 ?J21?pn J2 otherwise(B.7)Therefore, the feature gain for Model 3 isgm3 (f, e, a, l) = log(gn0 ( J, ?0)) +log(n(?i + 1|ei)) ?
log(n(?i|ei)) +log(?i + 1) +log(t( fj|ei)) ?
log(t( fj|e0)) +log(d( j|i, I, J)) (B.8)where fj and ei are linked by l, ?i is the fertility before adding l, and gn0 ( J, ?0) is the gainfor n0(?0|?Ii=1 ?i):gn0 ( J, ?0) =????????0?
( J?
?0+1)( J?2?0+1)?
( J?2?0+2) if ?0 ?J21?pn(J?
?0+1?0?1 )?pJ?2?0+20 ?
(1?p0 )?0?1?pn?J2 J2 < ?0 ?J2 + 11 otherwise(B.9)Model 4 defines a new distortion sub-model D(a) that relies on word classes A andB to capture movement of phrases.
The feature function for Model 4 ishm4 (a, f, e) = log(n0(?0|I?i=1?i)I?i=1n(?i|ei)J?j=1t( fj|eaj )1?0!D(a))(B.10)whereD(a) =I?i=1?i?k=1pik(?ik) (B.11)336Liu, Liu, and Lin Discriminative Word Alignment by Linear Modelingpik( j) ={d1( j ?
c?i |A(e?i ),B(?i1)) if k = 1d>1( j?
?i,k?1|B(?ik)) otherwise(B.12)Brown et al (1993) propose two distortion models for Model 4: d1(?)
for the firstword of a tablet ?
and d>1(?)
for the other words of the tablet.
In Equation B.12, ?i isthe first position to the left of i for which ?i > 0, c?i is the ceiling of the average positionof the words in ?
?, ?ik denotes the k-th French word aligned to ek, ?i,k?1 denotes theposition of the k ?
1-th French word aligned to ei, and A(?)
and B(?)
are word classesfor the source and target languages, respectively.
Please refer to Brown et al (1993) formore details.The corresponding feature gain isgm4 (f, e, a, l) = log(gn0 ( J, ?0)) +log(n(?i + 1|ei)) ?
log(n(?i|ei)) +log(t( fj|ei)) ?
log(t( fj|e0)) +log(?0) +log(D(a ?
{l})) ?
log(D(a)) (B.13)where fj and ei are linked by l and ?i is the fertility before adding l.In Model 4, the addition of a single link might change the distortion probabilitiespik( j) of other links.
As a result, we have to compute the overall distortion probabilitiesD(a) every time.AcknowledgmentsThis work was supported by NationalNatural Science Foundation of China,Contract No.
60603095 and 60573188.Thanks to the three anonymous reviewersfor their insightful and constructivecomments and suggestions.
We aregrateful to Rada Mihalcea for giving usthe Romanian?English training data andDavid Chiang for allowing us to use Hiero.Stephan Vogel, Vamshi Ambati, andKelly Widmaier offered valuable feedbackon an earlier version of this article.ReferencesAyan, Necip Fazil and Bonnie J. Dorr.
2006a.Going beyond AER: An extensive analysisof word alignments and their impact onMT.
In Proceedings of COLING-ACL 2006,pages 9?16, Sydney.Ayan, Necip Fazil and Bonnie J. Dorr.
2006b.A maximum entropy approach tocombining word alignments.
In Proceedingsof HLT-NAACL 2006, pages 96?103, NewYork, NY.Ayan, Necip Fazil, Bonnie J. Dorr, andChristof Monz.
2005a.
Alignment linkprojection using transformation-basedlearning.
In Proceedings of HLT-EMNLP2005, pages 185?192, Vancouver.Ayan, Necip Fazil, Bonnie J. Dorr, andChristof Monz.
2005b.
Neuralign:Combining word alignments using neuralnetworks.
In Proceedings of HLT-EMNLP2005, pages 65?72, Vancouver.Blunsom, Phil and Trevor Cohn.
2006.Discriminative word alignment withconditional random fields.
In Proceedingsof COLING-ACL 2006, pages 65?72,Sydney.Brown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, and Robert L.Mercer.
1993.
The mathematics ofstatistical machine translation: Parameterestimation.
Computational Linguistics,19(2):263?311.Cherry, Colin and Dekang Lin.
2003.
Aprobability model to improve wordalignment.
In Proceedings of ACL 2003,pages 88?95, Sapporo.Cherry, Colin and Dekang Lin.
2006.
Softsyntactic constraints for word alignmentthrough discriminative training.
InProceedings of COLING-ACL 2006 (poster),pages 105?112, Sydney.Chiang, David.
2005.
A hierarchicalphrase-based model for statistical machine337Computational Linguistics Volume 36, Number 3translation.
In Proceedings of ACL 2005,pages 263?270, Ann Arbor, MI.Chiang, David.
2007.
Hierarchicalphrase-based translation.
ComputationalLinguistics, 33(2):201?228.Cromiere`s, Fabien and Sadao Kurohashi.2009.
An alignment algorithm using beliefpropagation and a structure-baseddistortion model.
In Proceedings of EACL2009, pages 166?174, Athens.Fraser, Alexander and Daniel Marcu.
2006.Semi-supervised training for statisticalword alignment.
In Proceedings ofCOLING-ACL 2006, pages 769?776,Sydney.Fraser, Alexander and Daniel Marcu.
2007a.Getting the structure right for wordalignment: LEAF.
In Proceedings ofEMNLP-CoNLL 2007, pages 51?60, Prague.Fraser, Alexander and Daniel Marcu.
2007b.Measuring word alignment quality forstatistical machine translation.Computational Linguistics, 33(3):293?303.Galley, Michel, Jonathan Graehl, KevinKnight, Daniel Marcu, Steve DeNeefe, WeiWang, and Ignacio Thayer.
2006.
Scalableinference and training of context-richsyntactic translation models.
In Proceedingsof COLING-ACL 2006, pages 961?968,Sydney.Haghighi, Aria, John Blitzer, John DeNero,and Dan Klein.
2009.
Better wordalignments with supervised ITG models.In Proceedings of ACL-IJCNLP 2009,pages 923?931, Suntec.He, Xiaodong, Mei Yang, Jianfeng Gao,Patrick Nguyen, and Robert Moore.
2008.Indirect-HMM-based hypothesisalignment for combining outputs frommachine translation systems.
InProceedings of EMNLP 2008, pages 98?107,Honolulu, HI.Koehn, Philipp and Hieu Hoang.
2007.Factored translation models.
In Proceedingsof EMNLP-CoNLL 2007, pages 868?876,Prague.Koehn, Philipp, Franz J. Och, and DanielMarcu.
2003.
Statistical phrase-basedtranslation.
In Proceedings of HLT-NAACL2003, pages 127?133, Edmonton.Lacoste-Julien, Simon, Ben Taskar, Dan Klein,and Michael I. Jordan.
2006.
Wordalignment via quadratic assignment.In Proceedings of HLT-NAACL 2007,pages 112?119, New York, NY.Liang, Percy, Ben Taskar, and Dan Klein.2006.
Alignment by agreement.
InProceedings of HLT-NAACL 2006,pages 104?111, New York, NY.Liu, Yang, Qun Liu, and Shouxun Lin.
2005.Log-linear models for word alignment.
InProceedings of ACL 2005, pages 459?466,Ann Arbor, MI.Liu, Yang, Qun Liu, and Shouxun Lin.
2006.Tree-to-string alignment template forstatistical machine translation.
InProceedings of COLING-ACL 2006,pages 609?616, Sydney.Marcu, Daniel, Wei Wang, AbdessamadEchihabi, and Kevin Knight.
2006.
SPMT:Statistical machine translation withsyntactified target language phrases.
InProceedings of EMNLP 2006, pages 44?52,Sydney.Martin, Joel, Rada Mihalcea, and TedPedersen.
2005.
Word alignment forlanguages with scarce resources.
InProceedings of the ACL 2005 Workshop onBuilding and Using Parallel Texts,pages 65?74, Ann Arbor, MI.Melamed, I. Dan.
1998.
Annotation styleguide for the blinker project.
Technicalreport No.
98-06, University ofPennsylvania, Philadelphia.Melamed, I. Dan.
2000.
Models fortranslational equivalence among words.Computational Linguistics, 26(2):221?249.Mihalcea, Rada and Ted Pedersen.
2003.An evaluation exercise for wordalignment.
In Proceedings of HLT-NAACL2003 Workshop on Building and UsingParallel Texts, pages 1?10, Edmonton.Moore, Robert C. 2005.
A discriminativeframework for bilingual word alignment.In Proceedings of HLT-EMNLP 2005,pages 81?88, Vancouver.Moore, Robert C., Wen-tau Yih, and AndreasBode.
2006.
Improved discriminativebilingual word alignment.
In Proceedingsof COLING-ACL 2006, pages 513?520,Sydney.Niehues, Jan and Stephan Vogel.
2008.Discriminative word alignment viaalignment matrix modeling.
InProceedings of the Third Workshop onStatistical Machine Translation, pages 18?25,Columbus, OH.Och, Franz J.
2003.
Minimum error ratetraining in statistical machine translation.In Proceedings of ACL 2003, pages 160?167,Sapporo.Och, Franz J. and Hermann Ney.
2002.Discriminative training and maximumentropy models for statistical machinetranslation.
In Proceedings of ACL 2002,pages 295?302, Philadephia, PA.Och, Franz J. and Hermann Ney.
2003.
Asystematic comparison of various338Liu, Liu, and Lin Discriminative Word Alignment by Linear Modelingstatistical alignment models.Computational Linguistics, 29(1):19?51.Och, Franz J. and Hermann Ney.
2004.The alignment template approachto statistical machine translation.Computational Linguistics,30(4):417?449.Quirk, Chris, Arul Menezes, and ColinCherry.
2005.
Dependency treelettranslation: Syntactically informedphrasal SMT.
In Proceedings of ACL 2005,pages 271?279, Ann Arbor, MI.Rosti, Antti-Veikko, Spyros Matsoukas, andRichard Schwartz.
2007.
Improvedword-level system combination formachine translation.
In Proceedings of ACL2007, pages 312?319, Prague.Stolcke, Andreas.
2002.
SRILM?anextensible language modeling toolkit.
InProceedings of ICSLP 2002, pages 901?904,Denver, CO.Taskar, Ben, Simon Lacoste-Julien, and DanKlein.
2005.
A discriminative matchingapproach to word alignment.
InProceedings of HLT-EMNLP 2005,pages 73?80, Vancouver.Vogel, Stephan and Hermann Ney.
1996.HMM-based word alignment in statisticaltranslation.
In Proceedings of COLING 1996,pages 836?841, Copenhagen.339
