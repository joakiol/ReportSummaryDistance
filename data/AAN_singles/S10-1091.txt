Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 407?410,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsHIT-CIR: An Unsupervised WSD System Based on Domain MostFrequent Sense EstimationYuhang Guo, Wanxiang Che, Wei He, Ting Liu, Sheng LiHarbin Institute of TechnolgyHarbin, Heilongjiang, PRCyhguo@ir.hit.edu.cnAbstractThis paper presents an unsupervised sys-tem for all-word domain specific wordsense disambiguation task.
This systemtags target word with the most frequentsense which is estimated using a thesaurusand the word distribution information inthe domain.
The thesaurus is automati-cally constructed from bilingual parallelcorpus using paraphrase technique.
Therecall of this system is 43.5% on SemEval-2 task 17 English data set.1 IntroductionTagging polysemous word with its most frequentsense (MFS) is a popular back-off heuristic inword sense disambiguation (WSD) systems whenthe training data is inadequate.
In past evalua-tions, MFS from WordNet performed even bet-ter than most of the unsupervised systems (Snyderand Palmer, 2004; Navigli et al, 2007).MFS is usually obtained from a large scalesense tagged corpus, such as SemCor (Miller et al,1994).
However, some polysemous words havedifferent MFS in different domains.
For example,in the Koeling et al (2005) corpus, target wordcoach means ?manager?
mostly in the SPORTSdomain but means ?bus?
mostly in the FINANCEdomain.
So when the MFS is applied to specificdomains, it needs to be re-estimated.McCarthy et al (2007) proposed an unsuper-vised predominant word sense acquisition methodwhich obtains domain specific MFS without sensetagged corpus.
In their method, a thesaurus, inwhich words are connected with their distribu-tional similarity, is constructed from the domainraw text.
Word senses are ranked by their preva-lence score which is calculated using the thesaurusand the sense inventory.In this paper, we propose another way to con-struct the thesaurus.
We use statistical machineFigure 1: The architecture of HIT-CIRtranslation (SMT) techniques to extract paraphrasepairs from bilingual parallel text.
In this way, weavoid calculating similarities between every pairof words and could find semantic similar words orcompounds which have dissimilar distributions.Our system is comprised of two parts: the wordsense ranking part and the word sense tagging part.Senses are ranked according to their prevalencescore in the target domain, and the predominantsense is used to tag the occurrences of the targetword in the test data.
The architecture of this sys-tem is shown in Figure 1.The word sense ranking part includes followingsteps.1.
Tag the POS of the background text, countthe word frequency in each POS, and get thepolysemous word list of the POS.2.
Using SMT techniques to extract phrase table407Figure 2: Word sense ranking for the noun backbonefrom the bilingual corpus.
Extract the para-phrases (called as neighbor words) with thephrase table for each word in the polysemousword list.3.
Calculate the prevalence score of each senseof the target words, rank the senses with thescore and obtain the predominant sense.We applied our system on the English data setof SemEval-2 specific domain WSD task.
Thistask is an all word WSD task in the environ-mental domain.
We employed the domain back-ground raw text provided by the task organizer aswell as the English WordNet 3.0 (Fellbaum, 1998)and the English-Spanish parallel corpus from Eu-roparl (Koehn, 2005).This paper is organized as follows.
Section 2introduces how to rank word senses.
Section 3presents how to obtain the most related words ofthe target words.
We describe the system settingsin Section 4 and offer some discussions in Sec-tion 5.2 Word Sense RankingIn our method, word senses are ranked accordingto their prevalence score in the specific domain.According to the assumption of McCarthy et al(2007), the prevalence score is affected by the fol-lowing two factors: (1) The relatedness score be-tween a given sense of the target word and thetarget word?s neighbor word.
(2) The similaritybetween the target word and its neighbor word.In addition, we add another factor, (3) the impor-tance of the neighbor word in the specific domain.In this paper, ?neighbor words?
means the wordswhich are most semantically similar to the targetword.Figure 2 illustrates the word sense ranking pro-cess of noun backbone.
The contribution of aneighbor word to a given word sense is measuredby the similarity between them and weighted bythe importance of the neighbor word in the tar-get domain and the relatedness between the neigh-bor word and the target word.
Sum up the con-tributions of each neighbor words, and we get theprevalence score of the word sense.Formally, the prevalence score of sense siof atarget word w is assigned as follows:ps(w, si) =?nj?Nwrs(w, nj) ?
ns(si, nj) ?
dw(nj)(1)wherens(si, nj) =sss(si, nj)?si?
?senses(w)sss(si?, nj), (2)sss(si, nj) = maxsx?senses(nj)sss?
(si, sx).
(3)rs(w, nj) is the relatedness score between w anda neighbor word nj.
Nw= {n1, n2, .
.
.
, nk}is the top k relatedness score neighbor word set.ns(si, nj) is the normalized form of the sense sim-ilarity score between sense siand the neighborword nj(i.e.
sss(si, nj)).
We define this scorewith the maximum WordNet similarity score be-tween siand the senses of nj(i.e.
sss?
(si, nj)).In our system, lesk algorithm is used to measurethe sense similarity score between word senses.408Figure 3: Finding the neighbor words of noun backboneThe similarity of this algorithm is the count ofthe number of overlap words in the gloss or thedefinition of the senses (Banerjee and Pedersen,2002).
The domain importance weight dw(nj) isassigned with the count of njin the domain back-ground corpus.
For the neighbor word that doesnot occur in the domain background text, we usethe add-one strategy.
We will describe how to ob-tain njand rs in Section 3.3 Thesaurus ConstructionThe neighbor words of the target word as well asthe relatedness score are obtained by extractingparaphrases from bilingual parallel texts.
Whena word is translated from source language to tar-get language and then translated back to the sourcelanguage, the final translation may have the samemeaning to the original word but with different ex-pressions (e.g.
different word or compound).
Thetranslation in the same language could be viewedas a paraphrase term or, at least, related term of theoriginal word.For example, in Figure 3, English noun back-bone can be translated to columna, columna verte-bral, pilar and convicciones etc.
in Spanish, andthese words also have other relevant translationsin English, such as vertebral column, column, pil-lar and convictions etc., which are semantically re-lated to the target word backbone.We use a statistical machine translation sys-tem to calculate the translation probability fromEnglish to another language (called as pivot lan-guage) as well as the translation probability fromthat language to English.
By multiplying thesetwo probabilities, we get a paraphrase probabil-ity.
This method was defined in (Bannard andCallison-Burch, 2005).In our system, we choose the top k paraphrasesas the neighbor words of the target word, whichhave the highest paraphrase probability.
Note thatthere are two directions of the paraphrase, fromtarget word to its neighbor word and from theneighbor word to the target word.
We choosethe paraphrase score of the former direction asthe relatedness score (rs).
Because the higherof the score in this direction, the target word ismore likely paraphrased to that neighbor word,and hence the prevalence of the relevant targetword sense will be higher than other senses.
For-mally, the relatedness score is given byrs(w, nj) =?fp(f |w)p(nj|f), (4)where f is the pivot language word.We use the English-Spanish parallel text fromEuroparl (Koehn, 2005).
We choose Spanish asthe pivot language because in the both directionsthe BLEU score of the translation between Englishand Spanish is relatively higher than other Englishand other languages (Koehn, 2005).4 Data set and System SettingsThe organizers of the SemEval-2 specific domainWSD task provide no training data but raw back-ground data in the environmental domain.
The En-glish background data is obtained from the offi-cial web site of World Wide Fund (WWF), Euro-pean Centre for Nature Conservation (ECNC), Eu-ropean Commission and the United Nations Eco-nomic Commission for Europe (UNECE).
Thesize of the raw text is around 15.5MB after sim-ple text cleaning.
The test data is from WWF andECNC, and contains 1398 occurrence of 436 tar-get words.For the implementation, we used bpos (Shen etal., 2007) for the POS tagging.
The maximum409number of the neighbor word of each target word kwas set to 50.
We employed Giza++1and Moses2to get the phrase table from the bilingual paral-lel corpus.
TheWordNet::Similarity package3wasapplied for the implement of the lesk word sensesimilarity algorithm.For the target word that is not in the polysemousword list, we use the MFS from WordNet as theback-off method.5 Discussion and Future WorkThe recall of our system is 43.5%, which is lowerthan that of the MFS baseline, 50.5% (Agirre etal., 2010).
The baseline uses the most frequentsense from the SemCor corpus (i.e.
the MFS ofWordNet).
This means that for some target words,the MFS from SemCor is better than the domainMFS we estimated in the environmental domain.In the future, we will analysis errors in detail tofind the effects of the domain on the MFS.For the domain specific task, it is better to useparallel text in the domain of the test data in ourmethod.
However, we didn?t find any availableparallel text in the environmental domain yet.
Inthe future, we will try some parallel corpus acqui-sition techniques to obtain relevant corpus for en-vironmental domain for our method.AcknowledgmentsThis work was supported by National NaturalScience Foundation of China (NSFC) via grant60803093, 60975055, the ?863?
National High-Tech Research and Development of China viagrant 2008AA01Z144, and Natural Scientific Re-search Innovation Foundation in Harbin Instituteof Technology (HIT.NSRIF.2009069).ReferencesEneko Agirre, Oier Lopez de Lacalle, Christiane Fell-baum, Shu kai Hsieh, Maurizio Tesconi, Mon-ica Monachini, Piek Vossen, and Roxanne Segers.2010.
Semeval-2010 task 17: All-words word sensedisambiguation on a specific domain.
In Proceed-ings of the 5th International Workshop on SemanticEvaluations (SemEval-2010), Association for Com-putational Linguistics.Satanjeev Banerjee and Ted Pedersen.
2002.
Anadapted lesk algorithm for word sense disambigua-tion using wordnet.
In CICLing ?02: Proceedings1http://www.fjoch.com/GIZA++.html2http://www.statmt.org/moses/3http://wn-similarity.sourceforge.net/of the Third International Conference on Compu-tational Linguistics and Intelligent Text Processing,pages 136?145, London, UK.
Springer-Verlag.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In ACL?05: Proceedings of the 43rd Annual Meeting on As-sociation for Computational Linguistics, pages 597?604, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In The Tenth Ma-chine Translation Summit, Phuket, Thailand.Rob Koeling, Diana McCarthy, and John Carroll.2005.
Domain-specific sense distributions and pre-dominant sense acquisition.
In Proceedings of Hu-man Language Technology Conference and Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 419?426, Vancouver, BritishColumbia, Canada, October.
Association for Com-putational Linguistics.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2007.
Unsupervised acquisition of pre-dominant word senses.
Computational Linguistics,33(4):553?590, December.G.
A. Miller, C. Leacock, R. Tengi, and R. Bunker.1994.
A semantic concordance.
In Proc.
ARPAHuman Language Technology Workshop ?93, pages303?308, Princeton, NJ, March.
distributed as Hu-man Language Technology by San Mateo, CA: Mor-gan Kaufmann Publishers.Roberto Navigli, Kenneth C. Litkowski, and Orin Har-graves.
2007.
Semeval-2007 task 07: Coarse-grained english all-words task.
In Proceedings of theFourth International Workshop on Semantic Evalua-tions (SemEval-2007), pages 30?35, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Libin Shen, Giorgio Satta, and Aravind Joshi.
2007.Guided learning for bidirectional sequence classi-fication.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 760?767, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Benjamin Snyder and Martha Palmer.
2004.
The en-glish all-words task.
In Rada Mihalcea and PhilEdmonds, editors, Senseval-3: Third InternationalWorkshop on the Evaluation of Systems for the Se-mantic Analysis of Text, pages 41?43, Barcelona,Spain, July.
Association for Computational Linguis-tics.410
