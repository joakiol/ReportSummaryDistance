Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 66?73,Athens, Greece, 31 March 2009. c?2009 Association for Computational LinguisticsSVD Feature Selection for Probabilistic Taxonomy LearningFallucchi FrancescaDisp, University ?Tor Vergata?Rome, Italyfallucchi@info.uniroma2.itFabio Massimo ZanzottoDisp, University ?Tor Vergata?Rome, Italyzanzotto@info.uniroma2.itAbstractIn this paper, we propose a novel wayto include unsupervised feature selectionmethods in probabilistic taxonomy learn-ing models.
We leverage on the computa-tion of logistic regression to exploit unsu-pervised feature selection of singular valuedecomposition (SVD).
Experiments showthat this way of using SVD for feature se-lection positively affects performances.1 IntroductionTaxonomies are extremely important knowledgerepositories in a variety of applications for nat-ural language processing and knowledge repre-sentation.
Yet, manually built taxonomies suchas WordNet (Miller, 1995) often lack in cover-age when used in specific knowledge domains.Automatically creating or extending taxonomiesfor specific domains is then a very interestingarea of research (O?Sullivan et al, 1995; Magniniand Speranza, 2001; Snow et al, 2006).
Auto-matic methods for learning taxonomies from cor-pora often use distributional hypothesis (Harris,1964) and exploit some induced lexical-syntacticpatterns (Hearst, 1992; Pantel and Pennacchiotti,2006).
In these models, within a very large set,candidate word pairs are selected as new wordpairs in hyperonymy and added to an existing tax-onomy.
Candidate pairs are represented in somefeature space.
Often, these feature spaces arehuge and, then, models may take into considera-tion noisy features.In machine learning, feature selection has beenoften used to reduce the dimensions in huge fea-ture spaces.
This has many advantages, e.g., re-ducing the computational cost and improving per-formances by removing noisy features (Guyon andElisseeff, 2003).In this paper, we propose a novel way to in-clude unsupervised feature selection methods inprobabilistic taxonomy learning models.
Giventhe probabilistic taxonomy learning model intro-duced by (Snow et al, 2006), we leverage on thecomputation of logistic regression to exploit sin-gular value decomposition (SVD) as unsupervisedfeature selection.
SVD is used to compute thepseudo-inverse matrix needed in logistic regres-sion.To describe our idea, we firstly review howSVD can be used as unsupervised feature selec-tion (Sec.
2).
In Section 3 we then describe theprobabilistic taxonomy learning model introducedby (Snow et al, 2006).
We will then shortly re-view the logistic regression used to compute thetaxonomy learning model to describe where SVDcan be naturally used.
We will describe our ex-periments in Sec.
4.
Finally, we will draw someconclusions and describe our future work (Sec.
5).2 Unsupervised feature selection withSingular Value DecompositionSingular value decomposition (SVD) is one of thepossible factorization of a rectangular matrix thathas been largely used in information retrieval forreducing the dimension of the document vectorspace (Deerwester et al, 1990).The decomposition can be defined as follows.Given a generic rectangular n ?
m matrix A, itssingular value decomposition is:A = U?V Twhere U is a matrix n ?
r, V T is a r ?m and ?is a diagonal matrix r ?
r. The two matrices Uand V are unitary, i.e., UTU = I and V TV = I .The diagonal elements of the ?
are the singularvalues such as ?1 ?
?2 ?
... ?
?r > 0 where r isthe rank of the matrix A.
For the decomposition,SVD exploits the linear combination of rows andcolumns of A.A first trivial way of using SVD as unsupervisedfeature reduction is the following.
Given E as set66of training examples represented in a feature spaceof n features, we can observe it as a matrix, i.e.a sequence of examples E = (?
?e1 ...??em).
WithSVD, the n ?
m matrix E can be factorized asE = U?V T .
This factorization implies we canfocus the learning problem on a new space usingthe transformation provided by the matrix U .
Thisnew space is represented by the matrix:E?
= UTE = ?V T (1)where each example is represented with r new fea-tures.
Each new feature is obtained as a linearcombination of the original features, i.e.
each fea-ture vector ?
?el can be seen as a new feature vector??el?
= UT?
?el .
When the target feature space is bigwhereas the cardinality of the training set is small,i.e., n >> m, the application of SVD results in areduction of the original feature space as the rankr of the matrix E is r ?
min(n,m).A more interesting way of using SVD as unsu-pervised feature selection model is to exploit itsapproximated computations, i.e.
:A ?
Ak = Um?k?k?kVTk?nwhere k is smaller than the rank r. The compu-tation algorithm (Golub and Kahan, 1965) is al-lowed to stop at a given k different from the realrank r. The property of the singular values, i.e.,?1 ?
?2 ?
... ?
?r > 0, guarantees that thefirst k are bigger than the discarded ones.
Thereis a direct relation between the informativeness ofthe dimension and the value of the singular value.High singular values correspond to dimensions ofthe new space where examples have more vari-ability whereas low singular values determine di-mensions where examples have a smaller variabil-ity (see (Liu, 2007)).
These dimensions can notbe used as discriminative features in learning al-gorithms.
The possibility of computing the ap-proximated version of the matrix gives a power-ful method for feature selection and filtering aswe can decide in advance how many features or,better, linear combination of original features wewant to use.As feature selection model, SVD is unsuper-vised in the sense that the feature selection is donewithout taking into account the final classes of thetraining examples.
This is not always the case,feature selection models such as those based onInformation Gain largely use the final classes oftraining examples.
SVD as feature selection is in-dependent from the classification problem.3 Probabilistic Taxonomy Learning andSVD feature selectionRecently, Snow et al (2006) introduced a prob-abilistic model for learning taxonomies form cor-pora.
This probabilistic formulation exploits thetwo well known hypotheses: the distributional hy-pothesis (Harris, 1964) and the exploitation ofthe lexico-syntactic patterns as in (Robison, 1970;Hearst, 1992).
Yet, in this formulation, we canpositively and naturally introduce our use of SVDas feature selection model.In the rest of this section we will firstly intro-duce the probabilistic model (Sec.
3.1) and, then,we will describe how SVD is used as feature se-lector in the logistic regression that estimates theprobabilities of the model.
To describe this part weneed to go in depth into the definition of the logis-tic regression (Sec.
3.2) and the way of estimatingthe regression coefficients (Sec.
3.3).
This willopen the possibility of describing how we exploitSVD (Sec.
3.4)3.1 Probabilistic modelIn the probabilistic formulation (Snow et al,2006), the task of learning taxonomies from a cor-pus is seen as a probability maximization prob-lem.
The taxonomy is seen as a set T of asser-tions R over pairs Ri,j .
If Ri,j is in T , i is a con-cept and j is one of its generalization (i.e., the di-rect or the indirect generalization).
For example,Rdog,animal ?
T describes that dog is an animal.The main innovation of this probabilistic methodis the ability of taking into account in a singleprobability the information coming from the cor-pus and an existing taxonomy T .The main probabilities are then: (1) the priorprobability P (Ri,j ?
T ) of an assertion Ri,j tobelong to the taxonomy T and (2) the posteriorprobability P (Ri,j ?
T |?
?e i,j) of an assertion Ri,jto belong to the taxonomy T given a set of evi-dences ?
?e i,j derived from the corpus.
Evidencesis a feature vector associated with a pair (i, j).
Forexamples, a feature may describe how many timesi and j are seen in patterns like ?i as j?
or ?i isa j?.
These among many other features are in-dicators of an is-a relation between i and j (see(Hearst, 1992)).Given a set of evidences E over all the relevantword pairs, in (Snow et al, 2006), the probabilis-tic taxonomy learning task is defined as the prob-lem of finding the taxonomy T?
that maximizes the67probability of having the evidences E, i.e.:T?
= arg maxTP (E|T )In (Snow et al, 2006), this maximization prob-lem is solved with a local search.
What is max-imized at each step is the increase of the probabil-ity P (E|T ) of the taxonomy when the taxonomychanges from T to T ?
= T ?
N where N are therelations added at each step.
This increase of prob-abilities is defined as multiplicative change ?
(N)as follows:?
(N) = P (E|T ?
)/P (E|T ) (2)The main innovation of the model in (Snow et al,2006) is the possibility of adding at each step thebest relation N = {Ri,j} as well as N = I(Ri,j)that is Ri,j with all the relations by the existingtaxonomy.
We will then experiment with our fea-ture selection methodology in the two differentmodels:flat: at each iteration step, a single relation isadded, i.e.
R?i,j = arg maxRi,j ?
(Ri,j)inductive: at each iteration step, a set of re-lations is added, i.e.
I(R?i,j) where R?i,j =arg maxRi,j ?
(I(Ri,j)).The last important fact is that it is possible todemonstrate that?
(Ei,j) = k ?P (Ri,j ?
T |?
?e i,j)1?
P (Ri,j ?
T |?
?e i,j)== k ?
odds(Ri,j)where k is a constant (see (Snow et al, 2006))that will be neglected in the maximization process.This last equation gives the possibility of using thelogistic regression as it is.
In the next sections wewill see how SVD and the related feature selectioncan be used to compute the odds.3.2 Logistic RegressionLogistic Regression (Cox, 1958) is a particulartype of statistical model for relating responses Yto linear combinations of predictor variables X .
Itis a specific kind of Generalized Linear Model (see(Nelder and Wedderburn, 1972)) where its func-tion is the logit function and the independent vari-able Y is a binary or dicothomic variable whichhas a Bernoulli distribution.
The dependent vari-able Y takes value 0 or 1.
The probability thatY has value 1 is function of the regressors x =(1, x1, ..., xk).The probabilistic taxonomy learner model in-troduced in the previous section falls in the cat-egory of probabilistic models where the logisticregression can be applied as Ri,j ?
T is the bi-nary dependent variable and ?
?e i,j is the vector ofits regressors.
In the rest of the section we will seehow the odds, i.e., the multiplicative change, canbe computed.We start from formally describing the LogisticRegression Model.
Given the two stochastic vari-ables Y and X , we can define as p the probabilityof Y to be 1 given that X=x, i.e.
:p = P (Y = 1|X = x)The distribution of the variable Y is a Bernulli dis-tribution, i.e.
:Y ?
Bernoulli(p)Given the definition of the logit(p) as:logit(p) = ln(p1?
p)(3)and given the fact that Y is a Bernoulli distribution,the logistic regression foresees that the logit is alinear combination of the values of the regressors,i.e.,logit(p) = ?0 + ?1x1 + ...+ ?kxk (4)where ?0, ?1, ..., ?k are called regression coeffi-cients of the variables x1, ..., xk respectively.Given the regression coefficients, it is possibleto compute the probability of a given event wherewe observe the regressors x to be Y = 1 or in ourcase to belong to the taxonomy.
This probabilitycan be computed as follows:p(x) =exp(?0 + ?1x1 + ...+ ?kxk)1 + exp(?0 + ?1x1 + ...+ ?kxk)It is obviously trivial to determine theodds(Ri,j) related to the multiplicative changeof the probabilistic taxonomy model.
The oddsis the ratio between the positive and the negativeevent.
It is defined as follows:odds(Ri,j) =P (Ri,j?T |?
?e i,j)1?P (Ri,j?T |?
?e i,j)(5)Then, it is strictly related with the logit, i.e.
:odds(Ri,j) = exp(?0 +??eTi,j?)
(6)The relationship between the possible values ofthe probability, odds and logit is show in the Table1.68Probability Odds Logit0 ?
p < 0.5 [0, 1) (?
?, 0]0.5 < p ?
1 [1,?)
[0,?
)Table 1: Relationship between probability, oddsand logit3.3 Estimating Regression CoefficientsThe remaining problem is how to estimate the re-gression coefficients.
This estimation is done us-ing the maximal likelihood estimation to prepare aset of linear equations using the above logit defini-tion and, then, solving a linear problem.
This willgive us the possibility of introducing the necessityof determining a pseudo-inverse matrix where wewill use the singular value decomposition and itsnatural possibility of performing feature selection.Once we have the regression coefficients, we havethe possibility of assigning estimating a probabil-ity P (Ri,j ?
T |?
?e i,j) given any configuration ofthe values of the regressors?
?e i,j , i.e., the observedvalues of the features.
For sake of simplicity wewill hereafter refer to ?
?e i,j as ?
?e l.Let assume we have a multiset O of observa-tions extracted from Y ?E where Y ?
{0, 1} andwe know that some of them are positive observa-tions (i.e., Y = 1) and some of them are negativeobservations (i.e., Y = 0).For each pairs the relative configuration ?
?e l ?E that appeared at least once in O, we can de-termine using the maximal likelihood estimationP (Y = 1|?
?e l).
Then, from the equation of thelogit (Eq.
4), we have a linear equation system,i.e.:?????
?logit(p) = Q?
(7)where Q is a matrix that includes a constant col-umn of 1, necessary for the ?0 of the linear combi-nation of the values of the regression.
Moreover itincludes the transpose of the evidence matrix, i.e.E = (?
?e 1...?
?e m).
Therefore the matrix will be:Q =?????
?1 e11 e12 ?
?
?
e1n1 e21 e22 ?
?
?
e2n.......... .
....1 em1 em2 ?
?
?
emn?????
?The set of equations in Eq.
7 can be solved us-ing multiple linear regression.In their general form, the equations of multiplelinear regression may be written as (Caron et al,1988):y = X?
+ ?where:?
y is a column vector n ?
1 that includes theobserved values of the dependent variablesY1, ..., Yk;?
X is a matrix n ?m of the values of the re-gressors that we have observed;?
?
is a column vector m?
1 of the regressioncoefficients;?
?
is a column vector including the stochasticcomponents that have not been observed andthat will not be considered later.In the case X is a rectangular and singular matrix,the system y = X?
has not a solution.
Yet, it ispossible to use the principle of the Least SquareEstimation.
This principle determines the solution?
that minimize the residual norm, i.e.:??
= arg min ?X?
?
y?2 (8)This problem can be solved by the Moore-Penrose pseudoinverse X+ (Penrose, 1955).Then, the final equation to determine the ?
is??
= X+yIt is important to remark that if the inverse matrixexist X+ = X?1 and that X+X and XX+ aresymmetric.For our case, the following equation is valid:??
= Q+?????
?logit(p)3.4 Computing Pseudoinverse Matrix withSVD AnalysisWe finally reached the point where it is possibleto explain our idea that is naturally using singularvalue decomposition (SVD) as feature selection ina probabilistic taxonomy learner.
In the previoussections we described how the probabilities of thetaxonomy learner can be estimated using logisticregressions and we concluded that a way to de-termine the regression coefficients ?
is computingthe Moore-Penrose pseudoinverse Q+.
It is pos-sible to compute the Moore-Penrose pseudoin-verse using the SVD in the following way (Pen-rose, 1955).
Given an SVD decomposition of the69matrixQ = U?V T the pseudo-inverse matrix thatminimizes the Eq.
9 is:Q+ = V ?+UT (9)The diagonal matrix ?+ is a matrix r?
r obtainedfirst transposing ?
and then calculating the recip-rocals of the singular value of ?.
So the diagonalelements of the ?+ are 1?1 ,1?2, ..., , 1?r .We have now our opportunity of using SVD asnatural feature selector as we can compute differ-ent approximations of the pseudo-inverse matrix.As we saw in Sec.
2, the algorithm for computingthe singular value decomposition can be stopped adifferent dimensions.
We called k the number ofdimensions.
As we can obtain different SVD asapproximations of the original matrix (Eq.
2), wecan define different approximations of :Q+ ?
Q+k = Vn?k?+k?kUTk?mIn our experiments we will use different valuesof k to explore the benefits of SVD as feature se-lector.4 Experimental EvaluationIn this section, we want to empirically explorewhether our use of SVD feature selection pos-itively affects performances of the probabilistictaxonomy learner.
The best way of determininghow a taxonomy learner is performing is to see if itcan replicate an existing ?taxonomy?.
We will ex-periment with the attempt of replicating a portionof WordNet (Miller, 1995).
In the experiments, wewill address two issues: 1) determining to whatextent SVD feature selection affect performancesof the taxonomy learner; 2) determining if SVDas unsupervised feature selection is better for thetask than some simpler model for taxonomy learn-ing.
We will explore the effects on both the flatand the inductive probabilistic taxonomy learner.The rest of the section is organized as follows.In Sec.
4.1 we will describe the experimental set-up in terms of: how we selected the portion ofWordNet, the description of the corpus used to ex-tract evidences, a description of the feature spacewe used, and, finally, the description of a baselinemodels for taxonomy learning we have used.
InSec.
4.2 we will present the results of the experi-ments in term of performance.4.1 Experimental Set-upTo completely define the experiments we need todescribe some issues: how we defined the taxon-omy to replicate, which corpus we have used toextract evidences for pairs of words, which featurespace we used, and, finally, the baseline model wecompared our feature selection model against.As target taxonomy we selected a portion ofWordNet1 (Miller, 1995).
Namely, we startedfrom the 44 concrete nouns listed in (McRae etal., 2005) and divided in 3 classes: animal, arti-fact, and vegetable.
For sake of comprehension,this set is described in Tab.
2.
For each word w,we selected the synset sw that is compliant withthe class it belongs to.
We then obtained a set S ofsynsets (see Tab.
2).
We then expanded the set toS?
adding the siblings (i.e., the coordinate terms)for each synset in S. The set S?
contains 265 co-ordinate terms plus the 44 original concrete nouns.For each element in S we collected its hyperonym,obtaining the setH .
We then removed from the setH the 4 topmosts: entity, unit, object, and whole.The set H contains 77 hyperonyms.
For the pur-pose of the experiments we both derived from theprevious sets a taxonomy T and produced a set ofnegative examples T .
The two sets have been ob-tained as follows.
The taxonomy T is the portionof WordNet implied by O = H ?
S?, i.e., T con-tains all the (s, h) ?
O ?
O that are in WordNet.On the contrary, T contains all the (s, h) ?
O?Othat are not in WordNet.
We then have 5108 posi-tive pairs in T and 52892 negative pairs in T .We then split the set T ?T in two parts, trainingand testing.
As we want to see if it is possible toattach the set S?
to the right hyperonym, the splithas been done as follows.
We randomly dividedthe set S?
in two parts Str and Sts, respectively,of 70% and 30% of the original S?.
We then se-lected as training Ttr all the pairs in T containinga synset in Str and as testing set Tts those pairs ofT containing a synset of Sts.
For the probabilisticmodel, Ttr is the initial taxonomy whereas Tts?Tis the unknown set.As corpus we used the English Web as Corpus(ukWaC) (Ferraresi et al, 2008).
This is a webextracted corpus of about 2700000 web pages con-taining more than 2 billion words.
The corpus con-tains documents of different topics such as web,computers, education, public sphere, etc..
It hasbeen largely demonstrated that the web documents1We used the version 3.070Concrete nouns Clas Sense Concrete nouns Clas Sense1 banana Vegetable 1 23 boat Artifact 02 bottle Artifact 0 24 bowl Artifact 03 car Artifact 0 25 cat Animal 04 cherry Vegetable 2 26 chicken Animal 15 chisel Artifact 0 27 corn Vegetable 26 cow Animal 0 28 cup Artifact 07 dog Animal 0 29 duck Animal 08 eagle Animal 0 30 elephant Animal 09 hammer Artifact 1 31 helicopter Artifact 010 kettle Artifact 0 32 knife Artifact 011 lettuce Vegetable 2 33 lion Animal 012 motorcycle Artifact 0 34 mushroom Vegetable 413 onion Vegetable 2 35 owl Animal 014 peacock Animal 1 36 pear Vegetable 015 pen Artifact 0 37 pencil Artifact 016 penguin Animal 0 38 pig Animal 017 pineapple Vegetable 1 39 potato Vegetable 218 rocket Artifact 0 40 scissors Artifact 019 screwdriver Artifact 0 41 ship Artifact 020 snail Animal 0 42 spoon Artifact 021 swan Animal 0 43 telephone Artifact 122 truck Artifact 0 44 turtle Animal 1Table 2: Concrete nouns, Classes and senses selected in WordNetare good models for natural language (Lapata andKeller, 2004).As the focus of the paper is the analysis of theeffect of the SVD feature selection, we used as fea-ture spaces both n-grams and bag-of-words.
Outof the T ?
T , we selected only those pairs thatappeared at a distance of at most 3 tokens.
Us-ing these 3 tokens, we generated three spaces:(1) 1-gram that contains monograms, (2) 2-gramthat contains monograms and bigrams, and (3) the3-gram space that contains monograms, bigrams,and trigrams.
For the purpose of this experiment,we used a reduced stop list as classical stop wordsas punctuation, parenthesis, the verb to be are veryrelevant in the context of features for learning ataxonomy.Finally, we want to describe our baseline modelfor taxonomy learning.
This model only containsHeart?s patterns (Hearst, 1992) as features.
Thefeature value is the point-wise mutual information.These features are in some sense the best featuresfor the task as these have been manually selectedafter a process of corpus analysis.
These baselinefeatures are included in our 3-gram model.
We canthen compare our best models with this baselinefeatures in order to see if our SVD feature selec-tion model outperforms manual feature selection.4.2 ResultsIn the first set of experiments we want to focus onthe issue whether or not performances of the prob-abilistic taxonomy learner is positively affectedby the proposed feature selection model based onthe singular value decomposition.
We then deter-mined the performance with respect to differentvalues of k. This latter represents the number ofsurviving dimensions where the pseudo-inverse iscomputed.
Then, it represents the number of fea-tures the model adopts.
We performed this first setof experiments in the 1-gram feature space.
Punc-tuation has been considered.
Figure 1 plots the ac-curacy of the probabilistic learner with respect tothe size of the feature set, i.e.
the number k of sin-gle values considered for computing the pseudo-inverse matrix.
To determine if the effect of thefeature selection is preserved during the iterationof the local search algorithm, we report curves atdifferent sizes of the set of added pairs.
Curves are710.10.20.30.40.50.60 200 400 600 800 1000accuracyk: dimension of the reduced spaceflat20 added pairs40 added pairs60 added pairs80 added pairs100 added pairs0.10.20.30.40.50.60 200 400 600 800 1000accuracyk: dimension of the reduced spaceinductive40 added pairs80 added pairs130 added pairsFigure 1: Accuracy over different cuts of the feature space0.10.20.30.40.50.60 100 200 300 400 500 600accuracyadded pairsbaseline1-gram2-gram3-gramFigure 2: Comparison of different feature spaceswith k=400reported for both the flat model and the inductivemodel.
The flat algorithm adds one pair at eachiteration.
Then, we reported curves for each 20added pairs.
Each curve shows that accuracy doesnot increase after a dimension of k=700.
This sizeof the space is necessary only for the first 20 addedpairs.
Accuracy keeps increasing to k=700 andthen decreases.
When we add more pairs, the opti-mal size of the space is around k=200.
For the in-ductive model we report the accuracies for around40, 80, 130 added pairs.
Here, at each iteration,more than one pair is added.
The optimal dimen-sion of the feature space seems to be around 500as after that value performances decrease or staystable.
SVD feature selection has then a positiveeffect for both the flat and the inductive probabilis-tic taxonomy learners.
This has beneficial effectsboth on the performances and on the computationtime.In the second set of experiments we want to de-termine whether or not SVD feature selection forthe probabilistic taxonomy learner behaves betterthan a reduced set of known features.
We thenfixed the dimension k to 400 and we compared thebaseline model with different probabilistic modelswith different feature sets: 1-gram, 2-gram, and3-gram.
We can consider that the trigram modelbefore the cut on its dimensions contains featuresubsuming the baseline model.
Figure 2 shows re-sults.
Curves report accuracy after n added pairs.All the probabilistic models outperform the base-line model.
As what happened for the first series ofexperiments (see Fig.
1) more informative spacessuch as 3-gram behaves better when the number of72added pairs is small.
Performances of the three re-duced pairs become similar after 100 added pairs.These experiments show that SVD feature selec-tion has a positive effect on performances as re-sulting models are always better with respect tothe baseline.5 Conclusions and Future WorkWe presented a model to naturally introduceSVD feature selection in a probabilistic taxonomylearner.
The method is effective as allows the de-signing of better probabilistic taxonomy learners.We still need to explore at least two issues.
First,we need to determine whether or not the posi-tive effect of SVD feature selection is preservedin more complex feature spaces such as syntacticfeature spaces as those used in (Snow et al, 2006).Second, we need to compare the SVD feature se-lection with other unsupervised feature selectionmodels to determine whether or not this is the bestmethod to use in the case of probabilistic taxon-omy learning.ReferencesD.
Caron, W. Hospital, and P. N. Corey.
1988.Variance estimation of linear regression coefficientsin complex sampling situation.
Sampling Error:Methodology, Software and Application, pages 688?694.D.
R. Cox.
1958.
The regression analysis of binarysequences.
Journal of the Royal Statistical Society.Series B (Methodological), 20(2):215?242.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. L, and Richard Harshman.
1990.
In-dexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41:391?407.A.
Ferraresi, E. Zanchetta, M. Baroni, and S. Bernar-dini.
2008.
Introducing and evaluating ukwac, avery large web-derived corpus of english.
In InProceed-ings of the WAC4 Workshop at LREC 2008,Marrakesh, Morocco.G.
Golub and W. Kahan.
1965.
Calculating the singu-lar values and pseudo-inverse of a matrix.
Journal ofthe Society for Industrial and Applied Mathematics,Series B: Numerical Analysis, 2(2):205?224.Isabelle Guyon and Andre?
Elisseeff.
2003.
An intro-duction to variable and feature selection.
Journal ofMachine Learning Research, 3:1157?1182, March.Zellig Harris.
1964.
Distributional structure.
In Jer-rold J. Katz and Jerry A. Fodor, editors, The Philos-ophy of Linguistics, New York.
Oxford UniversityPress.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 15th International Conference on ComputationalLinguistics (CoLing-92), Nantes, France.Mirella Lapata and Frank Keller.
2004.
The web asa baseline: Evaluating the performance of unsuper-vised web-based models for a range of nlp tasks.In Proceedings of the Human Language TechnologyConference of the North American Chapter of theAssociation for Computational Linguistics, Boston,MA.Bing Liu.
2007.
Web Data Mining: Exploring Hy-perlinks, Contents, and Usage Data.
Data-CentricSystems and Applications.
Springer.Bernardo Magnini and Manuela Speranza.
2001.
In-tegrating generic and specialized wordnets.
In InProceedings of the Euroconference RANLP 2001,Tzigov Chark, Bulgaria.K.
McRae, G.S.
Cree, M.S.
Seidenberg, and C. McNor-gan.
2005.
Semantic feature production norms for alarge set of living and nonliving things.
pages 547?559, Behavioral Research Methods, Instruments,and Computers.George A. Miller.
1995.
WordNet: A lexicaldatabase for English.
Communications of the ACM,38(11):39?41, November.J.
A. Nelder and R. W. M. Wedderburn.
1972.
Gener-alized linear models.
Journal of the Royal StatisticalSociety.
Series A (General), 135(3):370?384.Donie O?Sullivan, A. McElligott, and Richard F. E.Sutcliffe.
1995.
Augmenting the princeton wordnetwith a domain specific ontology.
In Proceedings ofthe Workshop on Basic Issues in Knowledge Sharingat the 14th International Joint Conference on Artifi-cial Intelligence.
Montreal, Canada.Patrick Pantel and Marco Pennacchiotti.
2006.Espresso: Leveraging generic patterns for automati-cally harvesting semantic relations.
In Proceedingsof the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, pages113?120, Sydney, Australia, July.
Association forComputational Linguistics.R.
Penrose.
1955.
A generalized inverse for matrices.In Proc.
Cambridge Philosophical Society.Harold R. Robison.
1970.
Computer-detectable se-mantic structures.
Information Storage and Re-trieval, 6(3):273?288.Rion Snow, Daniel Jurafsky, and A. Y. Ng.
2006.
Se-mantic taxonomy induction from heterogenous evi-dence.
In In ACL, pages 801?808.73
