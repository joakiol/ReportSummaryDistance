Parsing Biomedical LiteratureMatthew Lease and Eugene CharniakBrown Laboratory for Linguistic Information Processing (BLLIP),Brown University, Providence, RI USA{mlease, ec}@cs.brown.eduAbstract.
We present a preliminary study of several parser adaptationtechniques evaluated on the GENIA corpus of MEDLINE abstracts [1,2].We begin by observing that the Penn Treebank (PTB) is lexically im-poverished when measured on various genres of scientific and techni-cal writing, and that this significantly impacts parse accuracy.
To re-solve this without requiring in-domain treebank data, we show how ex-isting domain-specific lexical resources may be leveraged to augmentPTB-training: part-of-speech tags, dictionary collocations, and named-entities.
Using a state-of-the-art statistical parser [3] as our baseline, ourlexically-adapted parser achieves a 14.2% reduction in error.
With oracle-knowledge of named-entities, this error reduction improves to 21.2%.1 IntroductionSince the advent of the Penn Treebank (PTB) [4], statistical approaches to nat-ural language parsing have quickly matured [3,5].
By providing a very largecorpus of manually labeled parsing examples, PTB has played an invaluablerole in enabling the broad analysis, automatic training, and quantitative evalu-ation of parsing techniques.
However, while PTB?s Wall Street Journal (WSJ)corpus has historically served as the canonical benchmark for evaluating statis-tical parsing, the need for broader evaluation has been increasingly recognizedin recent years.
Furthermore, since it is impractical to create a large treebanklike PTB for every genre of interest, significant attention has been directed to-wards maximally reusing existing training data in order to mitigate the needfor domain-specific training examples.
These issues have been most notably ex-plored in parser adaptation studies conducted between PTB?s WSJ and Browncorpora [6,7,8,9].As part of our own exploration of these issues, we have been investigatingstatistical parser adaptation to a novel domain: biomedical literature.
This lit-erature presents a stark contrast to WSJ and Brown: it is suffused with domain-specific vocabulary, has markedly different stylistic constraints, and is often writ-ten by non-native speakers.
Moreover, broader consideration of technical litera-ture shows this challenge and opportunity is not confined to biomedical literature We would like to thank the National Science Foundation for their support of this work(IIS-0112432, LIS-9721276, and DMS-0074276), as well as thank Sharon Goldwaterand our anonymous reviewers for their valuable feeback.R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
58?69, 2005.c?
Springer-Verlag Berlin Heidelberg 2005Parsing Biomedical Literature 59alone, but is also demonstrated by patent literature, engineering manuals, andfield-specific scientific discourse.
Through our work with biomedical literature,we hope to gain insights into effective techniques for adapting statistical parsingto technical literature in general.Our interest in biomedical literature is also motivated by a real need to im-prove information extraction in this domain.
With over 15 million citations inPubMed today, biomedical literature is the largest and fastest growing knowl-edge domain of any science.
As such, simply managing the sheer volume of itsaccumulated information has become a significant problem.
In response to this,a large research community has formed around the challenge of enabling auto-mated mining of the literature [10,11].
While the potential value of parsing hasoften been discussed by this community, attempts to employ it thus far appearto have been limited by the parsing technologies employed.
Reported difficul-ties include poor coverage, inability to resolve syntactic ambiguity, unacceptablememory and speed, and difficulty in hand-crafting rules of grammar [12,13].Perhaps the most telling indicator of community perspective came in a recentsurvey?s bleak observation that efficient and accurate parsing of unrestricted textappears to be out of reach of current techniques [14].In this paper, we show that broad, accurate parsing of biomedical literatureis indeed possible.
Using an off-the-shelf WSJ-trained statistical parser [3] as ourbaseline, we provide the first full-coverage parse accuracy results for biomedi-cal literature, as measured on the GENIA corpus of MEDLINE abstracts [1,2].Furthermore, after showing that PTB is lexically impoverished when measuredon various genres of scientific and technical writing, we describe three methodsfor improving parse accuracy by leveraging lexical resources from the domain:part-of-speech (POS) tags, dictionary collocations, and named-entities.
Our gen-eral hope is that lexically-based techniques such as these can provide alternativeand complementary value to treebank-based adaptation methods such as co-training [9] and sample selection [15].
Our lexically-adapted parser achieves a14.2% reduction in error over the baseline, and in the case of oracle-knowledgeof named-entities, this reduction improves to 21.2%.Section 2 describes the GENIA corpus in detail.
In Section 3, we presentunknown word rate experiments which measure the coverage of PTB?s gram-mar on various genres of scientific and technical writing.
Section 4 describesour methods for lexical adaptation and their corresponding effects on parse ac-curacy.
Section 5 concludes with a discussion challenges and opportunities forfuture work.2 The GENIA CorpusThe GENIA corpus [1,2] consists of MEDLINE abstracts related to transcriptionfactors in human blood cells.
Version 3.02p of the corpus includes 19991 ab-stracts (18,545 sentences, 436,947 words) annotated with part-of-speech (POS)1 The reported total of 2000 abstracts includes repetition of article ID 97218353.60 M. Lease and E. Charniaktags and named-entities.
Named-entities were labelled according to a corpus-defined ontology, and the POS-tagging scheme employed is very similar to thatused in PTB (see Section 4.1).Using these POS annotations and PTB guidelines [16], we hand-parsed 21of these abstracts (215 sentences) to create a pilot treebank for measuring parseaccuracy.
We performed the treebanking using the GRAPH2 tool developed forthe Prague Dependency Treebank.
Initial bracketing was performed without anyform of automation.
Following this, our baseline parser [3] was used to proposealternative parses.
In cases where hand-generated parses conflicted with thoseproposed by the parser, hand-parses were manually corrected, or not corrected,according to PTB bracketing guidelines.
Our pilot treebank is publicly available3.Subsequent to this, the Tsujii lab released its own beta version treebank,which includes 200 abstracts (1761 sentences) from the original corpus.
Thistreebanking was performed largely in accordance with PTB guidelines (perhapsthe most significant difference being constituent labels NAC and NX were excludedin favor of NP).
Because there is no redundancy in the coverage of the Tsuijii lab?streebank and our own pilot treebank (and by chance, NAC and NX do not occurin our pilot treebank either), we have combined the two treebanks to maximizeour evaluation treebank (see Table 3).An additional note is required regarding our use of named-entities (Sec-tion 4.3).
Entity annotations (not available in the treebank) were obtained fromthe earlier 3.02p version of the corpus.
Any sentences that did not match be-tween the two versions of the corpus (due to differences in tokenization or othervariations) were discarded.
The practical impact of this was negligible, as only25 sentences had to be discarded4.3 Unknown WordsCasual reading of technical literature quickly reveals a rich, field-specific vocab-ulary.
For example, consider the following sentence taken from GENIA:The study of NF-kappaB showed that oxLDLs led to a decrease ofactivation-induced p65/p50 NF-kappaB heterodimer binding to DNA,whereas the presence of the constitutive nuclear form of p50 dimer wasunchanged.To quantitatively measure the size and field-specificity of domain vocabulary, weextracted the lexicon contained in WSJ sections 2-21 and evaluated the unknownword rate (by token) for various genres of technical literature.
Results are givenin Table 1.2 http://quest.ms.mff.cuni.cz/pdt/Tools/Tree Editors/Graph3 http://www.cog.brown.edu/Research/nlp4 Because our preliminary use of named-entities assumes oracle-knowledge, this exper-iment was carried out on the development section only, thus only the developmentsection was reduced in this way.Parsing Biomedical Literature 61Table 1.
Unknown word rate on various technical corpora given WSJ 2-21 lexionCorpus Unknown Word RateWSJ sect.
24 2.7Brown-DEV 5.8Brown sect.
J 7.3CRAN 10.0CACM 10.7DOE 16.7GENIA 25.5Brown-DEV corresponds to a balanced sampling of the Brown corpus (see Ta-ble 4).
Section J of Brown contains ?Learned?
writing samples and demonstratedthe highest rate of any single Brown section.
CRAN contains 1400 abstracts inthe field of aerodynamics, and CACM includes 3200 abstracts from Communica-tions of the ACM [17].
DOE contains abstracts from the Department of Energy,released as part of PTB.
GENIA here refers to 333 abstracts (IDs 97449161-99101008) not overlapping our treebank.
As this table shows, unknown wordrate clearly increases as we move to increasingly technical domains.
Annecdotalevaluation on patent literature suggests its unknown rate lies somewhere betweenthat of DOE and GENIA.While these results appear to indicate WSJ is lexically impoverished withrespect to increasingly technical domains, it was also necessary to consider thepossibility that the results were simply symptomatic of technical domains havingvery large lexicons.
If such were the case, we would expect to see these domainsdemonstrate high unknown word rates even in the presence of a domain-specificlexicon.
To test this hypothesis, we contrasted unknown word rates on GENIAusing lexicons extracted from WSJ sections 2-21, Brown (training section fromTable 4), and from GENIA itself (1,333 abstracts: IDs 90110496-97445684)5.Results are presented in Table 2.Table 2.
Unknown word rate on GENIA using lexicons extracted from WSJ, Brown,and GENIALexicon Size Unknown Word RateBrown 25K 28.2WSJ 40K 25.5Brown+WSJ 50K 22.4GENIA 15K 5.3Brown+WSJ+GENIA 60K 4.65 While this set of abstracts does overlap the Tsujii treebank, this experiment was runprior to the treebank?s release.62 M. Lease and E. CharniakAlthough the unknown word rate in the presence of in-domain training forGENIA (5.3%, Table 2) is nearly twice that of out-of-domain training (2.7%,Table 1), suggesting a larger lexicon does indeed exist, it is also strikingly clearthat WSJ and Brown provide almost no lexical value to the domain: expandingGENIAs lexicon by 45,000 new terms found in WSJ and Brown produced only ameager 0.7% reduction in unknown word rate.
Contrast this with the enormousreduction achieved through using GENIA?s lexicon instead of the WSJ or Brownlexicons (Table 2).4 Parser AdaptationIn this section, we present three methods for parser adaptation motivated bythe results of our unknown word rate experiments (Section 3).
The goal ofthese adaptations is to help an off-the-shelf PTB-trained parser compensatefor the large amount of domain-specific vocabulary found in technical liter-ature, specifically biomedical text.
To accomplish this without depending onin-domain treebank data, we consider three alternative (and less expensive)domain-specific knowledge sources: part-of-speech tags, dictionary collocations,and named-entities.
We report on the results of each technique both in isolationand in combination.We adopt as our baseline for these experiments the publicly available Charniakparser [3] trained on WSJ sections 2-21 of the Penn Treebank.
Our division of theGENIA corpus into development and test sets is shown in Table 3.
Analysis wascarried out on the development section, and the test section was reserved for finalevaluation.
Parse accuracy was measured using the standard PARSEVAL met-ric of bracket-bracket scoring, assuming the usual conventions regarding punctua-tion [18].
Statistical significance for eachexperimentwas assessedusing a two-tailedpaired t-test on sentence-averaged f-measure scores.
Since our evaluation treebankexcludes NX and NAC constituent labels in favor ofNP (Section 2), for all experimentsTable 3.
Division of the GENIA combined treebank into development and test sectionsSource Section Abstract IDs SentencesPilot Development 99101510-99120900 215Tsujii Development 91079577-92060325 732Tsujii Test 92062170-94051535 1004Table 4.
Brown corpus division.
Training and evaluation sections were obtained fromGildea [7].
The development (and final training) section was created by extractingevery tenth sentence from Gildea?s training corpus.POS-Train Development TestSentences 19637 2181 2425Parsing Biomedical Literature 63Table 5.
PARSEVAL f-measure scores on the GENIA development section using theadaptation methods described in Section 4.
Statistical significance of individual adap-tations are compared against no adaptation, and combined adaptations are comparedagainst the best prior adaptation.
As the p values indicate, all of the adaptions listedhere produced a significant improvement in parse accuracy.Adaptation F-measure Error reduction Significancenone 78.3 ?
?lexicon 78.6 1.4 p = 0.002no NNP 79.1 3.7 p = 0.002train POS 80.8 11.5 p < 0.001entities 80.9 12.0 p < 0.001no NNP, train POS 81.5 14.7 p = 0.043no NNP, train POS, entities 82.9 21.2 p < 0.001Table 6.
Final PARSEVAL f-measure results on GENIA compared with scores onBrown and WSJ sect.
23.
In all cases, the parser was trained on WSJ sect.
2-21 withthe over-parsing parameter set to 21x over-parsing.
Adapted GENIA results includesPOS adaptations only (oracle-type entity adaptation was not used).
Adapted Brownresults use POS re-training on Brown train section.Corpus F-measure Error reduction SignificanceGENIA-unadapted 76.3 ?
?GENIA-adapted 79.6 14.2 p < 0.001Brown-unadapted 83.4 ?
?Brown-adapted 84.1 4.1 p = 0.002WSJ 89.5 ?
?
(including baseline)wepost-processedparser output to collapse these label distinc-tions6.
Results from our various experiments are summarized in Table 5.Final results of our adapted parser are given in Table 6.
For comparison withstandard benchmarks, parser performance was also evaluated on WSJ section23 and on Brown.
Table 4 shows our division of the Brown corpus.4.1 Using POS TagsPart-of-speech tags provide an important data feature to statistical parsers [3,5].Since technical and scientific texts introduce a significant amount of domain-specific vocabulary (Section 3), a POS-tagger trained only on everyday6 While PTB examples could be similarly pre-processed prior to training, thereby reduc-ing the search spacewhile parsing, the reductionwould beminor andwouldmean givingup a potentially useful distinction in syntactic contexts.64 M. Lease and E. CharniakEnglish is immediately at a disadvantage for tagging such text.
Indeed, ouroff-the-shelf PTB-trained parser achieves only 84.6% tagging accuracy on GE-NIA.
Consequently, our simple first adaptation step was to retrain the parser?sPOS-tagger on the 1,778 GENIA abstracts not present in the combined tree-bank (in addition to WSJ sections 2-21).
This simple fix raised tagging accuracyto 95.9%.
Correspondingly, parsing accuracy improved from 78.3% to 80.8%(Table 5).While such POS-retraining is a direct remedy to learning appropriate tagsfor new vocabulary, it is only a partial fix to a larger problem.
In particular, thetrees found in PTB codify a relationship between PTB POS tags and constituentstructure, and any mismatch between the tagging schemata used in PTB andthat used by our new corpus could result in misapplication or underutilization ofthe bracketing rules acquired by the parser during training.
To overcome this, itis necessary to introduce an additional mapping step which converts between thetwo POS tagging schemata.
For closely related schemata, this mapping may betrivial, but this cannot be assumed without a carefully analysis of tag distributionand usage across the two corpora.In the case of GENIA, the tagging guidelines used were based on PTB andonly subsequently revised (to improve inter-annotator agreement), so while dif-ferences do exist, the problem is much less significant than the general caseof arbitrarily different schemata.
Reported differences include treatment of hy-phenated, partial, and foreign terms, and most notably, the distinction betweenproper (NNP) and common (NN) nouns [2].
In order to quantitatively assessthe degree to which these and other revisions were made to the tagging scheme,we extracted the POS distribution for 333 GENIA abstracts (as used in ourunknown word rate experiments from Section 3).
From this distribution, welearned that NNP almost never occurs in GENIA.
This meant that our PTB-trained parser would be unable to leverage PTB?s constituent structure examplesexamples that involved proper nouns.As a preliminary remedy, we simply relabeled all proper nouns as common inPTBand re-trained the parser.This improved tagging accuracy to 96.4%and pars-ing accuracy to 81.5% (Table 5).
We should note, however, that this solution isnot ideal.
While it does allow use of PTB?s NNP-examples, it does so at the costof confusing legitimate differences in the syntactic distribution of common andproper nouns in English (as reflected by a 0.7% loss in accuracy on WSJ evalua-tion when using this NN-NNP conflated training data).
Clearly it would be betterif GENIA?s nouns could be re-tagged to preserve this distinction while preservinginter-annotator agreement.
A first step in this direction would be to perform thisre-tagging automatically based on determiner usage and GENIA?s entity annota-tions, with success measured by the corresponding impact on parse accuracy.
This,along with a more careful analysis of tagging differences, remains for future work.Wehavealso evaluatedparser performanceunder the oracle conditionofperfecttags.
This was implemented as a soft constraint so that the parser?s joint probabil-ity model could overrule the oracle tag for cases in which no parse could be foundusing it (cases of annotator error or data sparsity).
Using the oracle tag 99.8% ofParsing Biomedical Literature 65the time (in addition to other POS adaptations) had almost no impact on parse ac-curacy, suggesting that further POS-related improvements in parse accuracy willonly come from the sort of careful analysis of the tagging schemata discussed above.4.2 Using a Domain-Specific LexiconAnother strategy we employed for lexical adaptation was the use of a domain-specific dictionary.
For biomedicine, such a dictionary is available from the Na-tional Library of Medicine: the Unified Medical Language System (UMLS) SPE-CIALIST lexicon [19].
Covering both general English as well as biomedical vo-cabulary, the SPECIALIST lexicon contains over 415,000 entries (including or-thographic and morphological variants).
Entries are also assigned one of elevenPOS categories specified as part of the lexicon.Given our finding from Section 4.1 that even oracle POS tags would do littleto improve upon our re-trained POS tagger, we did not make use of lexicon POStags.
Instead, we restricted our use of the lexicon to extracting collocations.
Wethen added a hard-constraint to the parser that these collocations could not becross-bracketed and that each collocation must represent a flat phrase with nointernal sub-constituents.
This approach was motivated by a couple of observa-tions.
On one hand, we observed cases where the parser would be confused bylong compound nouns; in desperation to find the start of a verb phrase, it wouldsometimes use part of the compound to head a new verb phrase.
Unfortunately,WSJ sections 2-21 contain approximately 500 verb phrases headed by present-participle verbs mistagged as nouns, thus making this bizarre bracketing rulestatistically viable.
A second observation was the frequency with which we sawthe terms ?in vivo?
and ?in vitro?
(treebanked as foreign adjverbial or adjecti-val collocations) mis-analyzed.
Even in biomedical texts, ?in?
appears far moreoften as a preposition than as part of such collocations, and as such, is almostalways mis-parsed in these collocational contexts to head a prepositional phrase.Our hope was that by preventing such collocations from being cross-bracketted,we could prevent this class of parsing mistakes.We found use of lexical collocations did yield a small (0.3%) but statisticallysignificant improvement in performance over the unmodified parser (Table 5).However, when combined with either POS or entity adaptations, the lexicon?simpact on parsing accuracy was statistically insignificant.
Our interpretation ofthis latter result is that the primary limitation of the lexicon is coverage, despiteits size.
That is, when either of the other adaptations were used, the lexicondid not offer much beyond them.
It is not surprising that oracle-knowledge ofentities (Section 4.3) provided greater coverage than the generic dictionary, andthe improvement in tagging from POS adaptation (sharper tag probabilities)helped somewhat in preventing the verb-ification of some of the long compoundnouns.
While the lexicon was the only adaptation to correctly fix ?in vivo?
typemistakes, these phrases alone were not sufficiently frequent to provide a statisti-cally significant improvement in parse accuracy on top of other adaptations.
Assuch, the primary value of this method would be in cases where such a lexiconis available but POS tags and labelled entities are not.66 M. Lease and E. Charniak4.3 Using Named-EntitiesThe primary focus of the GENIA corpus is to support training and evaluation ofautomatic named-entity recognition.
As such, a variety of biologically meaningfulterms have been annotated in the corpus according to a corpus-defined ontology.Given the availability of these annotations, we were interested in consideringthe extent to which they could be used as a source of lexical information forparser adaptation.Given the problems described earlier with regard to lexical collocations beingcross-bracketted by our off-the-shelf PTB-trained parser (Section 4.2), our hopewas that named-entities could be used similarly to lexical collocations in helpingto prevent this class of mistakes.
To put it another way, we hoped to exploitthe correlation between named-entities and noun phrase (NP) boundaries.
Acommon preprocessing step in detecting named-entities is to use a chunker tofind NPs.
Our approach was to do the reverse: to use named-entities as a featurefor finding NP boundaries.Our initial plan was to use the same strategy we had used with dictionarycollocations: to add a hard-constraint to the parser that a named-entity couldnot be cross-bracketed and had to represent a flat phrase with no internal sub-constituents.
However, we found upon closer inspection that the entities oftendid contain substructure (primarily parenthetical acronyms), and so we relaxedthe flat-constituent constraint and enforced only the cross-bracketing constraint.As a preliminary step, we evaluated the utility of this method using oracle-knowledge of named-entities.
By itself, this method was roughly equivalent to POSre-training in improving parsing accuracy from78.3%to 80.9%(Table 5).Butwhencombined with POS adaptations, use of named-entities provided another signifi-cant improvement in performance, from81.5%to 82.9%.Clearly this is a promisingavenue for further work, and it will be interesting to see how much of this benefitfrom the oracle case can be realized when using automatically detected entities.5 DiscussionWe have found only limited use of parsing reported to date for biomedical liter-ature, thus it is difficult to compare our parsing results against previous work inparsing this domain.
To the best of our knowledge, only one other wide-coverageparser has been applied to biomedical literature: Grover et al report 99% cov-erage using a hand-written grammar with a statistical ranking component [20].We do not know of any quantitative accuracy figures reported for this domainother than those described here.For those interested in mining the biomedical literature, the next importantstep will be assessing the utility of PTB-style parsing compared to other pars-ing models that have been employed for information extraction.
There has beenpromising work in using PTB-style parses for information extraction by inducingpredicate-argument structures from the output parses [21].
It will be interesting tosee for the biomedical domain how these predicate-argument structures compareto those induced by other grammar formalisms currently in use, such as HPSG [22].Parsing Biomedical Literature 67The next immediate extension of our work is to evaluate use of detectednamed-entities in place of the oracle case described in Section 4.3, replacing thecurrent hard-constraint with a soft-constraint confidence term to be incorporatedinto the parser?s generative model.
Performance of named-entity recognition onGENIA was recently studied as part of a shared task at BioNLP/NLPBA 2004.The best system achieved 72.6% f-measure [23], though note that this task re-quired both detection and classification of named-entities.
As our usage of en-tities does not require classification, this number should be considered a lower-bound in the context of our usage model.
We expect this level of accuracy shouldbe sufficient to improve parse scores, though how much of the oracle benefit wecan realize remains to be seen.There are also interesting POS issues meriting further investigation.
As dis-cussed in Section 4.1, we would like to find a better solution to the lack ofproper noun annotations in GENIA, perhaps by detecting proper nouns usingdeterminers and labelled entities.
More careful analysis of the differences be-tween the PTB and GENIA tagging schemata is also needed.
Additionally, thereare interesting issues regarding how POS tags are used by the parsing model.Whereas the Collins?
model [5] treats POS tagging as an external preprocessingstep (a single best tag is input to the parsing model), the Charniak model [3]generates tag hypotheses as part of its combined generative model, and thusconsiders multiple hypotheses in searching for the best parse.
The significanceof this is that other components of the generative model can influence tag se-lection, and Charniak has reported adding this feature to his simulated versionof the Collins model improved its accuracy by 0.6% [24].
However, this resultwas for in-domain evaluation; the picture becomes more complicated when webegin parsing out-of-domain.
If we have an in-domain trained POS-tagger, wemight not want a combined model trained on out-of-domain data overruling ourtagger?s predictions.
One option may be introducing a weighting factor into thegenerative model to indicate the degree of confidence assigned to our taggerrelative to the other components of the combined model.Another issue for further work is the parsing of paper titles.
In the GENIAdevelopment section, only 28% of the titles are sentences whereas 71% are nounphrases.
This distribution is radically different than the rest of the corpus, whichis heavily dominated by sentence-type utterances.
As headlines are even morerare in our WSJ training data than titles are in GENIA (since WSJ containsfull article text), our parser performs miserably at utterance-type detection (i.e.correctly labelling the top-most node in the parse tree): 58.6%.
Correspondingly,parse accuracy on titles is only 69.1%, which represents a statistically significantdecrease in accuracy in comparison to the entire development section (p = 0.038).In investigating this, we noticed an oddity in GENIA in that most titles wereencoded in the corpus with an ending period that did not exist in the originalpapers the corpus was derived from.
By removing these periods, we improvedutterance-type detection to 77.9%.
While parse accuracy rose to 72.0%, thiswas statistically insignificant (p = 0.082).
The solution we would like to movetowards is to respect the legitimate distributional differences between title and68 M. Lease and E. Charniaknon-title utterances and parameterize the parser differently for the two cases.Generally speaking, such ?contextual parsing?
might allow us to improve parsingaccuracy more widely by parameterizing our parser differently based on wherethe current utterance fits in the larger discourse.
This example of period usagein titles also highlights a broader issue that seemingly innocuous issues in corpuspreparation can have significant impact when parsing.
As a further example ofthis, the choice to (at times) separately tokenize term-embedded parentheses inGENIA creates unnecessary attachment ambiguity in the resulting parentheticalphrases.
For example, in the phrase ?C3a and C3a(desArg)?, ?C3a(desArg)?
istokenized as ?C3a ( desArg )?, which produces ambiguity as to whether theparenthetical should attach low (to the latter ?C3a?)
or high (to the compound?C3a and C3a?).
Issues such as these remind us to be mindful of the relationshipbetween corpus preparation and parsing, as well as downstream processing, andthat some issues which appear difficult to resolve while parsing might be handledmore easily at another stage in the processing pipeline.We view biomedical and other technical texts as providing an interesting setof challenges and questions for future parsing research.
An interesting introduc-tion to some of these challenges, supported by examples drawn from the domain,can be found in [25].
A significant question for consideration is the degree towhich these challenges are related to domain knowledge vs. stylistic norms ofthe genre.
For example, [2] reports that whereas POS determination requireddomain expertise, prepositional phrase (PP)-attachment could be largely deter-mined even by non-biologists.
Our own treebanking experience left us with theopposite impression.
For example, in the phrase ?gene expression and proteinsecretion of IL-6?, should the PP attach high (IL-6 gene expression and proteinsecretion) or low (gene expression and IL-6 protein secretion)?
Domain knowl-edge appears to be necessary here for correct resolution.
In contrast to this, POStags appear to be a distributional rather than a semantic concern.
Issues likethis highlight how little we really understand currently about the parametersof corpus variation.
How do the frequencies of different syntactic constructionsvary by genre, and are there key structural variations at work?
How do we ef-fectively adapt parsers in response?
These issues remain important topics forfuture investigation.References1.
Kim, J.d., Ohta, T., Tateisi, Y., Tsujii, J.: Genia corpus - a semantically annotatedcorpus for bio-textmining.
Bioinformatics (Supplement: Eleventh InternationalConference on Intelligent Systems for Molecular Biology) 19 (2003) i180?i1822.
Tateisi, Y., Ohta, T., dong Kim, J., Hong, H., Jian, S., Tsujii, J.: The genia corpus:Medline abstracts annotated with linguistic information.
In: Third meeting of SIGon Text Mining, Intelligent Systems for Molecular Biology (ISMB).
(2003)3.
Charniak, E.: A maximum-entropy-inspired parser.
In: Proc.
NAACL.
(2000)132?1394.
Marcus, M., Santorini, B., Marcinkiewicz, M.A.
: Building a large annotated corpusof English: The Penn Treebank.
Computational Linguistics 19 (1993) 313?330Parsing Biomedical Literature 695.
Collins, M.: Discriminative reranking for natural language parsing.
In: Proc.
ICML.
(2000) 175?1826.
Ratnaparkhi, A.: Learning to parse natural language with maximum entropy mod-els.
Machine Learning 34 (1999) 151?1757.
Gildea, D.: Corpus variation and parser performance.
In: Proceedings of the 2001Conference on Empirical Methods in Natural Language Processing.
(2001) 167?2028.
Roark, B., Bacchiani, M.: Supervised and unsupervised pcfg adaptation to noveldomains.
In: Proceedings of HLT-NAACL.
(2003) 205?2129.
Steedman, M., Hwa, R., Clark, S., Osborne, M., Sarkar, A., Hockenmaier, J.,Ruhlen, P., Baker, S., Crim, J.: Example selection for bootstrapping statisticalparsers.
In: Proceedings of HLT-NAACL.
(2003) 331?33810.
de Bruijn, B., Martin, J.: Literature mining in molecular biology.
In: Proceedingsof the European Federation for Medical Informatics (EFMI) Workshop on NaturalLanguage Processing in Biomedical Applications.
(2002)11.
Hirschman, L., Park, J., Tsujii, J., Wong, L., Wu, C.: Accomplishments and chal-lenges in literature data mining for biology.
Bioinformatics 18 (2002) 1553?156112.
Yakushiji, A., Tateisi, Y., Miyao, Y., Tsujii, J.: Event extraction from biomedicalpapers using a full parser.
In: Pacific Symposium on Biocomputing.
(2001) 408?41913.
Daraselia, N., Yuryev, A., Egorov, S., Novichkova, S., Nikitin, A., Mazo, I.: Extract-ing human protein interactions from medline using a full-sentence parser.
Bioin-formatics 20 (2004) 604?61114.
Shatkay, H., Feldman, R.: Mining the biomedical literature in the genomic era: Anoverview.
Journal of Computational Biology 10 (2003) 821?85515.
Hwa, R.: Learning Probabilistic Lexicalized Grammars for Natural Language Pro-cessing.
PhD thesis, Harvard University (2001)16.
Bies, A., Ferguson, M., Katz, K., MacIntyre, R.: Bracketting Guideliness for Tree-bank II style Penn Treebank Project.
Linguistic Data Consortium.
(1995)17.
Buckley, C.: Implementation of the smart information retrieval system.
TechnicalReport 85-686, Cornell University (1985)18.
Goodman, J.: Parsing inside-out.
PhD thesis, Harvard University (1998)19.
McCray, A.T., Srinivasan, S., Browne, A.C.: Lexical methods for managing varia-tion in biomedical terminologies.
In: Proceedings of the 18th Annual Symposiumon Computer Applications in Medical Care (SCAMC).
(1994) 235?23920.
Grover, C., Lapata, M., Lascarides, A.: A comparison of parsing technologies forthe biomedical domain.
Journal of Natural Language Engineering (2002)21.
Surdeanu, M., Harabagiu, S., Williams, J., Aarseth, P.: Using predicate-argumentstructures for information extraction.
In: Proceedings of the 41st Annual Meetingof the Association for Computational Linguistics (ACL-03).
(2003) 8?1522.
Miyao, Y., Ninomiya, T., Tsujii, J.: Corpus-oriented grammar development foracquiring a head-driven phrase structure grammar from the penn treebank.
In:Proc.
of IJCNLP-04.
(2004) 684?69323.
Zhou, G., Su, J.: Exploring deep knowledge resources in biomedical name recog-nition.
In: Proceedings of the Joint Workshop on Natural Language Processing inBiomedicine and its Applications (JNLPBA-04).
(2004)24.
Charniak, E.: Statistical parsing with a context-free grammar and word statistics.In: Proceedings of the Fourteenth National Conference on Artificial Intelligence,Menlo Park, AAAI Press/MIT Press (1997)25.
Park, J.C.: Using combinatory categorical grammar to extract biomedical infor-mation.
IEEE Intelligent Systems 16 (2001) 62?67
