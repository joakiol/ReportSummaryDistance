Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1602?1613,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsAbstractive Summarization of Product Reviews Using Discourse StructureShima Gerani??
?Yashar Mehdad?
?Giuseppe Carenini?Raymond T. Ng?Bita Nejat?
?University of Lugano?University of British ColumbiaSwitzerland Vancouver, BC, Canada{gerani,mehdad,carenini,rng,nejatb}@cs.ubc.caAbstractWe propose a novel abstractive summa-rization system for product reviews by tak-ing advantage of their discourse structure.First, we apply a discourse parser to eachreview and obtain a discourse tree repre-sentation for every review.
We then mod-ify the discourse trees such that every leafnode only contains the aspect words.
Sec-ond, we aggregate the aspect discoursetrees and generate a graph.
We then selecta subgraph representing the most impor-tant aspects and the rhetorical relations be-tween them using a PageRank algorithm,and transform the selected subgraph intoan aspect tree.
Finally, we generate anatural language summary by applying atemplate-based NLG framework.
Quan-titative and qualitative analysis of the re-sults, based on two user studies, show thatour approach significantly outperforms ex-tractive and abstractive baselines.1 IntroductionMost existing works on sentiment summarizationfocus on predicting the overall rating on an en-tity (Pang et al., 2002; Pang and Lee, 2004) orestimating ratings for product features (Lu et al.,2009; Lerman et al., 2009; Snyder and Barzilay,2007; Titov and McDonald, 2008)).
However, theopinion summaries in such systems are extractive,meaning that they generate a summary by concate-nating extracts that are representative of opinionon the entity or its aspects.Comparing extractive and abstractive sum-maries for evaluative texts has shown that an ab-stractive approach is more appropriate for sum-marizing evaluative text (Carenini et al., 2013;?The contribution of the first two authors to this paperwas equal.Di Fabbrizio et al., 2014).
This finding is alsosupported by a previous study in the context ofsummarizing news articles (Barzilay et al., 1999).To the best of our knowledge, there are only threeprevious works on abstractive opinion summariza-tion (Ganesan et al., 2010; Carenini et al., 2013;Di Fabbrizio et al., 2014).
The first work (Gane-san et al., 2010) proposes a graph-based methodfor generating ultra concise opinion summariesthat are more suitable for viewing on devices withsmall screens.
This method does not provide awell-formed grammatical abstract and the gener-ated summary only contains words that occur inthe original texts.
Therefore, this approach is moreextractive than abstractive.
Another limitation isthat the generated summaries do not contain anyinformation about the distribution of opinions.In the second work, (Carenini et al., 2013) ad-dresses some of the aforementioned problems andgenerates well-formed grammatical abstracts thatdescribe the distribution of opinion over the en-tity and its features.
However, for each product,this approach requires a feature taxonomy hand-crafted by humans as an input, which is not scal-able.
To partially address this problem (Mukherjeeand Joshi, 2013) has proposed a method for the au-tomatic generation of a product attribute hierarchythat leverages ConceptNet (Liu and Singh, 2004).However, the resulting ontology tree has been usedonly for sentiment classification and not for clas-sification.In the third and most recent study, (Di Fabbrizioet al., 2014) proposed Starlet-H as a hybrid ab-stractive/extractive sentiment summarizer.
Starlet-H uses extractive summarization techniques to se-lect salient quotes from the input reviews and em-beds them into the abstractive summary to exem-plify, justify or provide evidence for the aggregatepositive or negative opinions.
However, Starlet-Hassumes a limited number of aspects as input andneeds a large amount of training data to learn the1602ordering of aspects for summary generation.Highlighting the reasons behind opinions in re-views was also previously proposed in (Kim et al.,2013).
However, their approach is extractive andsimilar to (Ganesan et al., 2010) does not cover thedistribution of opinions.
Furthermore, it aims toexplain the opinion on only one aspect, rather thanexplaining the overall opinion on the product, itsaspects and how they affect each other.To address some of the above mentioned limita-tions , in this paper we propose a novel abstrac-tive summarization framework that generates anaspect-based abstract from multiple reviews of aproduct.
In our framework, anything that is eval-uated in the review is considered an aspect, in-cluding the product itself.
We propose a naturallanguage generation (NLG) framework that takesaspects and their structured relation as input andgenerates an abstractive summary.
However, un-like (Carenini et al., 2013), our method assumes nodomain knowledge about the entity in terms of auser-defined feature taxonomy.
On the other hand,in contrast with Starlet-H, we do not limit the in-put reviews to a small number of aspects and ouraspect ordering method takes advantage of rhetor-ical information and does not require any trainingdata.
Our method relies on the discourse struc-ture and discourse relations of reviews to infer theimportance of aspects as well as the associationbetween them (e.g., which aspects relate to eachother).Researchers have recently started using the dis-course structure of text in sentiment analysis andhave shown its advantage in improving sentimentclassification accuracy (e.g., (Lazaridou et al.,2013; Trivedi and Eisenstein, 2013; Somasun-daran et al., 2009; Asher et al., 2008)).
However,to the best of our knowledge, none of the existingworks have looked into exploiting discourse struc-ture in abstractive review summarization.In our work, importance of aspects, derivedfrom the reviews?
discourse structure and rela-tions, is used to rank and select aspects to be in-cluded in the summary.
More specifically, we startwith the most important (highest ranked) aspectsto generate a summary and add more aspects tothe system until a summary of desired length isobtained.
Aspect association is considered to bet-ter explain how the opinions on aspects affect eachother (e.g., opinion over specific aspects affect theopinion over the more general ones).
Considerthe following sentence as an example summarygenerated by our system for the entity CameraCanon G3: ?All reviewers who commented on thecamera, thought that it was really good mainly be-cause of the photo quality.?
This summary encap-sulates all the following key pieces of information:1) camera and photo quality are the most impor-tant aspects, 2) People have positive opinion oncamera in general and on photo quality as one ofits features, and finally 3) photo quality is the mainreason behind users satisfaction on camera.
Suchsummary helps users understand the reason behinda rating of a product or its aspects without goingthrough all reviews or reading scattered opinionson different aspects in multiple sentences of an ex-tractive summary.This paper makes the following contributions:1.
We propose a novel content selection and struc-turing strategy for review summarization, that as-sumes no prior domain knowledge, by taking ad-vantage of the discourse structure of reviews.2.
We propose a novel product-independenttemplate-based NLG framework to generate an ab-stract based on the selected content, without re-lying on deep syntactic knowledge or sophisti-cated NLG methods.
Our framework, similarly to(Carenini et al., 2013), can effectively convey thedistribution of opinions.3.
We present the first study that investigates theuse of discourse structure information in both con-tent selection and abstract generation for multi-document summarization.Quantitative and qualitative analysis over eval-uation results of two user studies on a set of userreviews on twelve different products show that oursystem is an effective abstractive system for re-view summarization.2 Summarization FrameworkAt a high-level, our summarization framework in-volves generating a summary from multiple in-put reviews based on an Aspect Hierarchy Tree(AHT) that reflects the importance of aspects aswell as the relationships between them.
In ourframework, an AHT is generated automaticallyfrom the set of input reviews, where each sen-tence of every review is marked by the aspects pre-sented in that sentence and the polarity of opin-ions over them.
There are various methods forextracting the aspects and predicting the polar-ity of opinion (Hu and Liu, 2004b; Hu and Liu,16032006; Kim et al., 2011).
In this paper we do notfocus on aspect extraction and sentiment predic-tion but rather consider the aspect and their po-larity/strength (P/S) information given as input tothe system.
P/S scores are integer values in therange [-3, +3], where +3 is the most positive and-3 is the most negative polarity value.
We alsodo not attempt to automatically resolve corefer-ences between aspects.
For example, the aspect?g3?, ?canon g3?
and ?canon?
were manuallycollapsed as into ?camera?.
This preprocessingstep helps to reduce the noise generated by inac-curate aspect labeling in our reviews.
Figure 1shows two sample input reviews where the aspectsand their P/S scores are identified.
For example, inR1, aspects camera, photo quality and auto modeare mentioned.
The P/S values for the three as-pects are [+2], [+3] and [+2] respectively whichindicate positive opinion on all aspects.The first component of our system applies a dis-course parser to each review and obtains a dis-course tree representation for every review (e.g.Figure 1 (a) and (b)).
The discourse trees are thenmodified such that every leaf node only containsthe aspect words.
The output of the first compo-nent is an aspect-based discourse tree (ADT) forevery review (e.g.
Figure 1 (c) and (d)).
In thesecond component, we aggregate the ADTs andgenerate a graph called Aggregated Rhetorical Re-lation Graph (ARRG) (e.g.
Figure 1 (f)).
Thethird component of our framework, is responsi-ble for content selection and structuring.
It takesARRG as input, runs Weighted PageRank, and se-lects a subgraph (e.g.
Figure 1 (g)) representingthe most important aspects.
Finally it transformsthe selected subgraph into a tree and provides anAHT as output (e.g.
Figure 1 (h)).
The gener-ated AHT is the input of the last component whichgenerates a natural language summary by apply-ing micro planning and sentence realization.
Wenow describe each component of our frameworkin more detail.3 Discourse ParsingAny coherent text is structured so that we canderive and interpret the information.
This struc-ture shows how discourse units (text spans suchas sentences or clauses) are connected and relateto each other.
Discourse analysis aims to revealthis structure.
Several theories have been pro-posed in the past to describe the discourse struc-ture, among which the Rhetorical Structure The-ory (RST) (Mann and Thompson, 1988) is one ofthe most popular.
RST divides a text into min-imal atomic units, called Elementary DiscourseUnits (EDUs).
It then forms a tree representa-tion of a discourse called a Discourse Tree (DT)using rhetorical relations (e.g., Elaboration, Ex-planation, etc) as edges, and EDUs as leaves.EDUs linked by a rhetorical relation are also dis-tinguished based on their relative importance inconveying the author?s message: nucleus is thecentral part, whereas satellite is the peripheralpart.We use a publicly available state-of-the-art dis-course parser (Joty et al., 2013)1to generate aDT for each product review.
Figure 1 (a) and (b)show DTs for two sample reviews where dottededges identify the satellite spans.
DT1 in Figure 1(a) shows that review R1 consists of three EDUswith two relations Elaboration and Backgroundbetween them.
It also shows that the first EDU(i.e.
I love camera) is the nucleus (shown by solidline) of the relation Elaboration and so the rest ofthe document (EDUs 2 and 3) is less important andaims at elaborating on what the author meant inthe first EDU.
Similarly, the structure shows thatthe third EDU is mentioned as background infor-mation for EDU2 and so is less important for real-izing the core meaning of the document.After obtaining the DTs, we remove all wordsfrom the text spans of each EDU, except the aspectwords.
Thus, for each review, we have a DT wherea leaf node represents the aspects occurring in thecorresponding EDU.
Note that there may be EDUscontaining no aspects in a review.
In such cases,we keep the corresponding node and mark it withno aspect.
We call the resulting tree an Aspect-based Discourse Tree (ADT) which will be usedin the next components.
Figure 1 (c) and (d) showADTs generated from DTs.4 Aspect Rhetorical Relation Graph(ARRG)In the second component, we aim at generat-ing an ARRG for a product, based on the ADTswhich are the output from the previous compo-nent.
There are two motivations behind aggregat-ing the ADTs and building the ARRG: i) whileeach ADT can be rather noisy because of the infor-mal language of the reviews and inaccuracies from1http://alt.qcri.org/discourse/Discourse Parser Dist.tar.gz1604BackgroundElaborationI love thiscameraI  am amazed atthe quality ofphotosthat I have tooksimply by using theauto modegreat camera !It gives tons ofcontrol forphoto buffsbut still has anauto modefor the noviceto useINPUT:R1: camera[+2], photo quality[+3], auto mode[+2]##I love this camera, I am amazed at the quality of photos that I have took simply using the auto modeR2: camera[+2], control[+2], auto mode [+1]#great camera!
It gives tons of control for photo buffs but still has an auto mode for the novice to usecameraphotoqualityauto modeBackgroundElaborationcameracontrolauto modecamera-(camera, Elaboration, photo quality, 0.5)(camera, Elaboration, auto mode, 0.33)(photo quality, Background, auto mode, 0.75)(camera, Elaboration, auto mode, 0.375)(camera, Elaboration, control, 0.5)(control, Contrast, auto mode, 0.66)ElaborationContrastElaborationElaborationContrastElaborationcameraphoto qualityauto modecontrolElaboration,0.5Background,0.75Elaboration,0.705Elaboration,0.5Contrast,0.668 495cameraphoto qualitycontrolElaboration,0.5Elaboration,0.5cameraphoto qualitycontrolElaborationElaboration(d) ADT2(c) ADT1(a) DT1(b) DT2 (f) ARRG(h) AHT(g) ARRG-subgraph(e) aspect relation tuples extracted from ADTsOUTPUT:All reviewers who commented on the camera, thought that it was really good mainly because of the photo quality.
Accordingly, about half of the reviewers commentedabout the control and they thought it was fine.
(i)MicroplanningSentencerealizationWednesday, May 28, 14Figure 1: A simple example illustrating different components of our summarization framework.automatic discourse parsing, aggregating all theADTs can reveal more reliable information; andii) the aggregated information highlights the mostimportant aspects overall as well as the strongestconnection between the aspects.
This informationcan effectively drive the content selection and ab-stract generation phases.ARRG is a directed graph in which we allowmultiple edges between two vertices.
In ARRG,vertices represent aspects.
We associate to eachaspect/node an importance measure that aggre-gates all the P/S values that the aspect receivesin all the reviews.
By following (Carenini et al.,2013), let PS(a) be the set of P/S values that anaspect a receives.
The direct measure of impor-tance of the aspect is defined as:dir-moi(a) =?ps?PS(a)ps2(1)In ARRG, edges indicate existence of arhetorical relation between text spans of a re-view in which the aspects occurred.
Edges arelabeled with the type of the relation as wellas a weight indicating our confidence in thepresence of the relation between the two aspects.In ARRG, an edge with label r, w from nodeu to node v, ur, w????
v, indicates the existanceof a relation r with confidence w between twoaspects u and v. Also, the direction of the arrowindicates that u and v occurred in the satelliteand nucleus spans respectively.
For example,photo qualityelaboration, 0.8????????????
camera indicatesthat there is a high confidence (0.8) that aspectphoto quality was used in a text span to elaborateaspect camera.
Moreover, camera is a moreimportant aspect compared to photo quality.To build ARRG, we use all the ADTs that areoutput of the previous component (one for eachreview).
From each ADTj, we extract all tuplesof the form (u, r, v, w) in which u is an aspect oc-curring in a satellite span, v is an aspect occurringin a nucleus span, r is a relation type and w is theweight of the tuple computed as follows:w = 1?0.5|EDUs between u and v||total EDUs in ADTj|?0.5drd(2)where, |.| indicates cardinality of a set.
d indi-cates the depth of the ADTjand drindicates thedepth of the sub-tree of ADTjrooted at relationr.
Equation 2 weighs a tuple based on two factors:(i) the relative distance of the EDUs in which thetwo aspects u and v participating in relation r oc-cur.
The intuition is that aspects occurring in closeproximity to each other are more related; and (ii)the depth of the sub-tree at the point of the rela-tion relative to the depth of the whole ADTj.
Thisis because as we move from leaves to the root ofa DT, the accuracy of the rhetorical structure hasbeen shown to decrease.
Also, at higher levelsof an ADT (intra-sentential relations), it is more1605likely that aspects are related through non adjacentEDUs and so are less strongly related.
Figure 1 (e)shows tuples extracted from sample ADTs.Notice that every two aspects u and v may berelated by the same relation more than once in anADT for a review.
Thus, we might have i tupleswith the same u,r, and v but confidence weightswhich are not necessarily the same.
From everyADTj, we extract all (u, r, v, wij) and select theone with maximum confidence.
We then aggre-gate the selected tuples extracted from differentreviews.
Putting these two steps together, for ev-ery two aspects u and v related by relation r, weobtain a single tuple (u, r, v, w?)
wherew?
=?jmaxiwij(3)Figure 1 (f) shows an example ARRG built for thesample reviews.5 Content Selection and StructuringThe content of the summary is selected by extract-ing from ARRG a subgraph containing the mostimportant aspects.
Such content is then structuredby transforming the subgraph into an aspect hier-archy.5.1 Subgraph ExtractionIn ARRG aspects/nodes are weighted by how fre-quently and strongly they are evaluated in the re-views (i.e, dir-moi) and edges are weighted byhow frequently and strongly the corresponding as-pects are rhetorically related in the discourse trees(Equation 3).
In content selection, we want toextract aspects that not only have high weight,but that are also linked with heavy edges to otherheavy aspects.
This problem can be effectivelyaddressed by Weighted Page Rank (WPR) (Xingand Ghorbani, 2004).
WPR takes the importanceof both the in-links and out-links of the aspectsinto account and distributes rank scores based onthe weights of relations between aspects.
In thisway, the heavier aspect nodes, that are either inthe nuclei of many relations or in the satellites ofrelations with other heavy aspects, are promoted.We then update the weight of nodes (aspects) withthe new score from WPR.
Finally, we rank nodesbased on their updated score moi and select thetop N aspects.moi(a) = ?dir-moi(a) + (1??
)WPR(a) (4)Here ?
is a coefficient that can be tuned on a de-velopment set or can be set to 0.5 without tuning.Figure 1 (g) shows an example subgraph selectedfrom the sample ARRG.5.2 Aspects Subgraph to Aspects HierarchyTransformationIn this step, we generate a hierarchical tree struc-ture for aspects.
Such a tree structure helps tonavigate over aspects and can be easily traversedto find certain aspects and their relation to theirparent or children.
The hierarchy of aspects alsomatches the intuition that the root node is the mostfrequent and general aspect (often the product) andas the depth increases, nodes represent more spe-cific aspects of the product with less frequency andweight.To obtain a hierarchical tree structure from theextracted subgraph, we first build an undirectedgraph as follows: we merge the edges connectingtwo nodes and consider the sum of their weightsas the weight of the merged graph.
We also ignorethe relation direction for the purpose of generat-ing the tree.
We then find the Maximum Span-ning Tree of the undirected subgraph and set thehighest weighted aspect as the root of the tree.This process results in a useful knowledge struc-ture of aspects with their associated weight andsentiment polarity connected with the rhetoricalrelations called Aspect Hierarchical Tree (AHT).Figure 1 (h) shows the generated AHT from thesub-graph.6 Abstract GenerationThe automatic generation of a natural languagesummary in our system involves the followingtasks (Reiter and Dale, 2000): (i) microplanning,which covers lexical selection; and (ii) sentencerealization, which produces english text from theoutput of the microplanner.6.1 MicroplanningOnce the content is selected and structured, it ispassed to the microplanning module which per-forms lexical choice.
Lexical choice is an impor-tant component of microplanning.
Lexical choiceis formulated in our system based on a ?formal?style, language ?variability?
and ?fluent?
connec-tivity among other lexical units.
Table 1 demon-strates our lexical choice strategy.1606Quantifiers:if (relative-number == 1) : [?All users (x people) who commented about the aspect?, ?All costumers (x people) that reviewed the aspect?, ...]if (relative-number >= 0.8) : [?Almost all users commented about the aspect and they?, ?Almost all costumers mentioned the aspect and they?, ...]if (relative-number >= 0.6) : [?Most users commented about the aspect and they mainly?, ?Most shoppers mentioned aspect and they?, ...]if (relative-number >= 0.45) : [?Almost half of the users commented about the aspect and they?, ?Almost 50% of the shoppers mentioned the aspect and they?, ...]if (relative-number >= 0.2) : [?About y% of the reviewers commented about the aspect and they?, ?Around y% of the shoppers mentioned the aspect and they?, ...]if (relative-number >= 0.0) : [?z reviewers commented about the aspect and in overall they?, ?z shoppers mentioned about the aspect and they?, ...]Polarity verbs:if (controversial(aspect)) : [?had controversial opinions about it?, ?expressed controversial opinions about this feature?, ...]else: if (average <= ?2) : [?hated it?, ?felt that it was very poor?, ?thought that it was very poor?, ...]if (average <= ?1) : [?disliked it?, ?felt that it was poor?, ?thought that it was poor?, ...]if (average < 0) : [?did not like it?, ?felt that it was weak?, ?thought that it was weak?, ...]if (average == 0) : [?did not express any strong positive or negative opinion about it?, ...]if (average <= +1) : [?liked it?, ?felt that it was fine?, ?thought that it was satisfactory?, ...]if (average <= +2) : [?absolutely liked it?, ?really liked this feature?, ?felt that it was a really good feature?, ?thought that it was really good?, ...]if (average <= +3) : [?loved it?, ?felt that it was great?, ?thought that it was great?, ...]Connectives[?Also, related to the aspect?, ?Accordingly, ?, ?Moreover, regarding the aspect, ?
,?In relation to the aspect, ?, ?Talking about the aspect, ?, ...]Table 1: Microplanning strategy for lexical choice.
The selected lexical items will fill the template in therealization step.Sentence realization templates:First sentence templates:if (polarity-agreement(root,highest-weighted-child) & connecting-relation == [elaboration, explain, cause, summary, same-unit, background, evidence, justify]):?quantifier + polarity-verb + ?mainly because of the?
+ highest-weighted-child?else: ?quantifier + polarity-verb?First level children (aspects) sentences templates:?connective + ?, ?
+ quantifier + ?
?
+ polarity-verb?Supporting sentences templates:if (#children(aspect)==1): ?connective + quantifier + verb ?elseif (#children(aspect)>1 & polarity-agreement(children)): ?connective + quantifier + verb + [and, similarly, while, ...] + quantifier + verb?elseif (#children(aspect)>1 & !polarity-agreement(children)): ?connective + quantifier + verb + [but, in contrast, on contrary, ...] + quantifier + verb?Table 2: Sentence realization templates.Quantifiers: for each aspect, a quantifier is se-lected based on both the absolute and relativenumber of users whose opinions contributed to theevaluation of the aspect.Polarity verbs: for each aspect, a polarity verb isselected based on the average sentiment polaritystrength for that aspect.
Although the average, inmost cases, can be a good metric to evaluate thepolarity of an aspect, it fails when the distributionof evaluations is centered on zero, for instance, ifthere are equal numbers of positive and negativeevaluations (i.e., controversial).
To partially solvethis problem, we first check whether the aspectevaluation is controversial by applying the formulaproposed by (Carenini and Cheung, 2008).
In thecase of controversiality, our microplanner selectsa lexical item to express the controversiality of theaspect.
In other cases, we use the average and se-lect the polarity verb based on that.Connectives: in order to form more fluent andreadable sentences and to increase the languagevariability, we randomly select our connectivesfrom the list shown in Table 1.
Moreover, whena parent aspect (excluding the root in AHT) hastwo children, they are connected by one of the co-ordinating conjunction ?
[and, similarly]?
if theyagree on polarity, and they will be connected bya choice of ?
[on the contrary, in contrast]?
other-wise (see Supporting sentences templates in Table2).
As an alternative we could have selected con-nectives based on the discourse relations specifiedin the aspects tree.
However, this is left as futurework.6.2 Sentence RealizationThe realization of our abstract generation is per-formed by applying a rather simple and compre-hensive template-based strategy.
Depending onthe specific lexical choice in microplanning step,an appropriate template and corresponding fillersare selected as shown in Table 2.
We develop threedifferent templates: i) generates the first abstractsentence; ii) generates the abstract sentence for theaspects with no children; and iii) generates sup-porting sentences for aspects with children.For illustration, assuming that we apply thisstrategy to a 5-node variation of the AHT in Figure1 (h), where the aspect ?control?
has two children?auto mode?
and ?setting?, we obtain ?All review-ers (45 people) who commented on the camera,thought that it was really good mainly because ofthe photo quality.
Accordingly, about 24% of thereviewers commented about the control and they1607thought it was fine.
Also, related to the control, 7users expressed their opinion about the auto modeand they liked it, similarly, 6 shoppers commentedabout the setting and they thought that it was sat-isfactory.
?7 Experimental Setup7.1 Dataset and BaselinesWe conduct our experiments using the customerreviews of twelve products obtained from (Hu andLiu, 2004a): 4 digital cameras, 1 DVD player, 1MP3 player, 2 routers, 2 phones, 1 diaper and 1antivirus.
The reviews were collected from Ama-zon.com and Cnet.com.
We use manually anno-tated aspects and their associated sentiment fromthe same dataset.We compare the summaries generated by oursystem with two state-of-the-art extractive base-lines and a simpler version of our abstractive sys-tem, as follows:1) MEAD-LexRank (LR): we use the LexRank(Erkan and Radev, 2004) implementation insidethe MEAD summarization framework (Radev etal., 2004), which outperforms other algorithmsimplemented in the MEAD framework.2) MEADStar (MEAD*): a state-of-the-art ex-tractive opinion summarization system (Careniniet al., 2013), which is adapted from theopen source summarization framework MEAD.MEAD* orders aspects by the number of sen-tences evaluating that aspect, and selects a sen-tence from each aspect until it reaches the wordlimit.
The sentence that is selected for each aspectis the one with the highest sum of polarity/strengthevaluations for any aspect.3) Simple Abstractive (SA): we sort the aspectsof each product based on dir-moi (Equation 1).Then, for each aspect, we generate a sentencebased on a simple template ?quantifier + polarity-verb?
until the summary reaches the word limit.We limit the length of our summaries to 150words.
In our experiment we use the default pa-rameter in Equation 4 without tuning (i.e.
?
=0.5).
Our system starts the content selection pro-cess with 10 aspects and generates a summarybased on a AHT with 10 aspects.
We add one as-pect, reproduce the AHT and regenerate the sum-mary.
We repeat this process until the word limitis reached.7.2 Evaluation FrameworkOn one hand, the lack of product reviews datasetswith human written summaries, and on the otherhand, the difficulty of generating human-writtensummaries for reviews, makes review summaryevaluation a very challenging task.We evaluate the summaries generated by oursystem by performing two user studies based onpairwise preferences using a popular crowdsourc-ing service.2The user preference evaluation is aneffective method for opinion summarization (e.g.,(Lerman et al., 2009)).
The main motivations be-hind pairwise preferences evaluation is two-fold:i) raters can make a preference decision more ef-ficiently than a scoring judgment; and ii) rateragreement is higher in preference decisions thanin scoring judgments (Ariely et al., 2003).In both user studies, for each product, we runsix pairwise comparisons for four summaries.
Ineach rating assignment, two summaries of thesame product were placed in random order.
Raterswere shown the name of each product along withthe relevant summaries and were asked to expresstheir preference for one summary over the otherusing a simple set of criteria.
For two summariesS1and S2raters should choose one of the follow-ing three options: 1) Prefer S1, 2) Prefer S2, 3) Nopreference.Raters were specifically instructed that their rat-ing should express ?overall satisfaction with theinformation provided by the summary?.
Raterswere also asked to provide a brief comment jus-tifying their choice.
Over 48 raters participated ineach study, and each comparison was evaluated byat least five raters generating more than 360 judg-ments for each user study.
We pre-select the highskilled raters to ensure a higher quality results.The main difference between the two user stud-ies is that in ?user study 1?, we show two sum-maries to the raters and ask them to choose the onethey prefer without showing them the original re-views.
In contrast, in ?user study 2?, we show twosummaries with links to the full text of the reviewsfor the raters to explore.
In order to make sure thatthe raters read the reviews, we ask them to writea short summary of the reviews before rating theautomatic summaries.
We ran two different userstudies because: i) for each product there might bemany reviews to be included; ii) there is no guar-anty that raters, in various evaluation settings, read2www.crowdflower.com1608System I vs System II Agreement No preference Preferred Sys I Preferred Sys IIUser Studies1 2 1 2 1 2 1 2LR vs MEAD*0.33 0.75 7% 6% 35% 20% 58% 74%LR vs SA0.42 0.83 0% 0% 38% 21% 62% 79%LR vs Our System0.50 1.00 0% 3% 26% 13% 74% 84%MEAD* vs SA0.58 0.83 0% 0% 38% 20% 62% 80%MEAD* vs Our System0.67 0.50 0% 3% 25% 30% 75% 67%SA vs Our System0.42 0.50 12% 11% 23% 32% 65% 57%Table 3: Results of pairwise preference user studies.
Statistically significant improvements (p < 0.01)over the baselines are demonstrated by bold fonts.
Italic fonts indicate statistical significance (p < 0.01)of abstractive methods (SA and Our System) over extractive approaches (LR and MEAD*).Systems LR MEAD* SA Our SystemUser Studies1 2 1 2 1 2 1 2Preference33% 18% 41% 41% 49% 63% 71% 69%Table 4: System preference results.
Statistically significant improvements (p < 0.01) over the baselinesare demonstrated by bold fonts.the reviews (partially or completely); and iii) thereis no evidence regarding the depth that each raterwould look into the reviews.
Therefore, choosingbetween user study 1 and 2 is not a straightforwarddecision.
In other words, designing the two userstudies in this way helps us to answer the ques-tion: ?Does the fact that raters can read all thereviews affect their ratings?
?.8 ResultsThis section provides a quantitative and qualitativeanalysis of the evaluation results3.8.1 Quantitative AnalysisQuantitative results for both user studies areshown in Table 3.
The second column indicatesthe percentage of judgments for which the raterswere in agreement.
Agreement here is a weakagreement, where four (out of five) raters are de-fined to be in agreement if they all gave the samerating.
The next three columns indicate the per-centage of judgments for each preference cate-gory, grouped into two user studies.
In addi-tion, we measure the preference for each systemin both user studies (Table 4).
For each system,the preference is the number of times raters preferthe system, divided by the total number of judg-ments for that system (e.g., if A is preferred over3The evaluation results and summaries obtainedfrom CrowdFlower are publicly available and canbe downloaded from: https://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/reviews/user_study_results.zipB 10 out of 30 times, and A is preferred over C15 out of 20 times, the overall preference of A is(10+15)/(30+20)=50%)Abstractive vs. Extractive: the results of our sys-tem and SA in Table 3 show statistically signifi-cant improvements in pairwise preference over ex-tractive baselines (LR and MEAD*) in both userstudies.4Moreover, the results of overall prefer-ence in Table 4 demonstrates that two abstractivesystems are preferred over the extractive ones inboth studies.
This further supports the findings inthe previous studies (e.g., (Carenini et al., 2013))that users prefer abstractive summarization.
Wecan observe that, in both user studies, raters preferour system over other abstractive and extractivebaselines.
Also, the highest pairwise preferencepercentages occur comparing an extractive and anabstractive system (e.g., LR vs Our System).Abstractive Systems: the raters prefer our systemover SA in both user studies (65% and 57%), andour system ranks first in our pairwise preferenceuser studies.
Knowing that both systems are ab-stractive and the differences between them comesfrom using the rhetorical structure in the contentselection and abstract generation phases, provesthe effectiveness of using rhetorical structure andrelations in abstractive summarization of reviews.Extractive Systems: the result in Table 3 and 4demonstrate that raters prefer MEAD* over LR.Although both systems are extractive, the MEAD*system has been proposed for extractive opinion4The statistical significance tests was calculated by ap-proximate randomization, as described in (Yeh, 2000).1609Preference Sys 1 to Sys 2 Reasons Examples of preference justification taken from the raters commentsOur System to LR and MEAD* Readability, coverage of aspects,aggregation of opinionsbetter wording, more objective, more depth, I like the stats, more detail aboutpeople opinion, less personal experience, detail comparison from differentreviews, a summary in a summary, mentions more features, ...LR and MEAD* to Our System Descriptiveness, personal point ofviews, product capabilitiesexplain how the product is positive, good characteristics about the product,has lot more to tell, more descriptive about features, personal perspective,not only characteristics but also ability, more true to the product itself, ...Our System to SA The relations between the aspects,more language variabilityprovides a bit more information, is very complete, not repetitive, more ele-gant, coherent, .....SA to Our System Simpler structure, more aspects written better, has touched variety of features, ...Table 5: System preference results.
The reasons are classified based on raters justifications preferringthe underlined systems.summarization.
In contrast, LR is a generic ex-tractive summarization system which is not opti-mized for opinion summarization.
This also fur-ther demonstrates the need for opinion and reviewssummarization systems.User Study 1 vs.
User Study 2: the first in-teresting observation is that, although the over-all ranking of systems in both user studies doesnot change, there are some changes in the re-sults.
This indicates that reading the reviews ef-fects preference decisions.
We can observe thatin all cases except one (MEAD* vs Our System)the agreement between the raters increases sig-nificantly when they are given the reviews.
Thiscan be interpreted as reading the reviews helpsthe rater to choose a better summary easier andmore effectively.
Moreover, we calculate the over-all agreement for both user studies.5Case study 2reports a higher overall agreement (70%) in com-parison with the user study 1 (65%).
This furtherproves our finding that showing the reviews canhelp the raters with their preference judgment.In Table 3, the preference of sys 2 (last col-umn) significantly rises for all cases when com-pared with the LR system.
This proves that ratersstrongly prefer the summaries that cover opinion-ated sentences, specifically when they are exposedto the reviews.
The same result is reflected in Ta-ble 4, where the overall preference of LR dropswhen the raters are given the reviews.
We also ob-serve a significant rise in preference of sys 2 whenMEAD* is compared with SA (Table 3) and in theoverall preference of SA (Table 4) in user study2.
This proves that raters become more confidentin preferring an abstractive summary over an ex-tractive one when the reviews are given to them.In contrast, we notice that the preference of sys2 drops comparing ?MEAD* vs Our System?
and?SA vs Our System?.
Knowing that the drop is5The agreement is calculated based on 100 randomly sam-pled units selected from our crowdsourcing job.not significant and the the overall ranking of sys-tems remains unchanged, this case is less straightforward to interpret.8.2 Qualitative AnalysisWe collect and group the rater justifications in theresults we obtain by crowdsourcing our evaluationframework, when preferring a summary over an-other, in Table 5.
To make the comparison moreclear, Example 1 shows the summaries generatedby MEAD* and our system.Comparing our system with the extractive base-lines, raters?
justifications are classified in threemain categories.
Although the language of the ex-tractive summaries is less formal, raters often pre-fer our system in terms of presentation and lan-guage.
They justify their selections by expressingphrases such as ?better grammar?
or ?fewer er-rors?.
They also comment about the coverage ofaspects in the summaries generated by our systemand they realize that our system was capable ofaggregating the opinions for each aspect.
In con-trast, when they prefer the extractive summaries,they like the descriptive language of the summaryand the technical details of the products that weremissing in our system summaries.We also notice that raters realize the usage ofstructure (AHT) in our system (both of content se-lection and summary generation) and they appre-ciate it by expressing phrases such as ?very com-plete?, ?more elegant?
or ?related features?.
Incontrast, they sometimes appreciate a simpler lan-guage in summaries generated by SA.
Moreover,few raters prefer the higher coverage in SA sum-maries.
This is mainly because not using connec-tives and structure in SA leaves more space to in-clude more aspects.1610Product: Nikon Coolpix 4300MEAD*: it is very compact but the controls are so well designed thatthey ?re still easy to use .
It ?s easy for beginners to use , but has featuresthat more serious photographers will love , so it ?s an excellent camerato grow into .
But overall this is a good camera with a ?
really good ?picture clarity ; an exceptional close-up shooting capability .The batterylife is very good , i got about 90 minutes with the lcd turned on allthe time , the first time around , and i have been using it with the lcdoff every now and then , and have yet needed to recharge it .
Yes ,the picture quality and features which are too numerous to mention areunmatched for any camera in this price range.Our System: All reviewers (34 people), who commented on the cam-era, felt that it was really good mainly because of the picture.
Around26% of the reviewers expressed their opinion about the picture qualityand they really liked it.
Around 24% of the reviewers noted the useand they thought that it was satisfactory.
Talking about the use, around24% of the reviewers expressed their opinion about the size and theyfelt that it was fine.
Only 6 reviewers commented about the scene modeand in overall they thought that it was satisfactory.
Moreover, regardingthe scene mode, 4 shoppers mentioned about the manual mode and theythought that it was satisfactory, and similarly only 4 reviewers com-mented about the auto mode and in overall they did not express anystrong positive or negative opinion about it.
Only 4 costumers men-tioned the software and they felt that it was really good.Example 1.
Summaries generated by our systemand MEAD* baseline for the Nikon Coolpix 4300camera.
For brevity we exclude other baselines.9 ConclusionsWe have presented a framework for abstractivesummarization of product reviews based on dis-course structure.
For content selection, we pro-pose a graph model based on the importanceand association relations between aspects, that as-sumes no prior domain knowledge, by taking ad-vantage of the discourse structure of reviews.
Forabstract generation, we propose a product inde-pendent template-based natural language genera-tion (NLG) framework that takes aspects and theirstructured relation as input and generates an ab-stractive summary.
Quantitative evaluation results,based on two pairwise preference user studies,show substantial improvement over extractive andabstractive baselines, including MEAD*, whichis considered a state-of-the-art opinion extractivesummarization system, and a simpler version ofour abstractive system.
In future work, we planto extend the microplanning phase by taking ad-vantage of the highly weighted rhetorical relationsbetween the aspects and select connective phrasesbased on the discourse relations specified in theaspects tree.
In addition, we plan to develop andevaluate an end-to-end system, in which the aspectextraction and polarity estimation of aspects areautomated.
In this way, we can achieve an end-to-end automatic summarizaion system for productreviews.AcknowledgmentsThis work was supported in part by Swiss Na-tional Science Foundation (PBTIP2-145659) andNSERC Business Intelligence Network.
Wewould like to thank the anonymous reviewers fortheir valuable comments.
We also acknowledgeShafiq Rayhan Joty for his help regarding rhetori-cal parser.ReferencesDan Ariely, George Loewenstein, and Drazen Prelec.2003.
?Coherent Arbitrariness?
: Stable DemandCurves Without Stable Preferences.
Quarterly Jour-nal of Economics, 118:73?105.Nicholas Asher, Farah Benamara, and Yvette YannickMathieu.
2008.
Distilling opinion in discourse: Apreliminary study.
In Coling 2008: Companion vol-ume: Posters and Demonstrations, pages 5?8.Regina Barzilay, Kathleen R. McKeown, and MichaelElhadad.
1999.
Information fusion in the contextof multi-document summarization.
In Proceedingsof the 37th annual meeting of the Association forComputational Linguistics on Computational Lin-guistics, ACL ?99, pages 550?557, Stroudsburg, PA,USA.
Association for Computational Linguistics.Giuseppe Carenini and Jackie Chi Kit Cheung.
2008.Extractive vs. nlg-based abstractive summarizationof evaluative text: the effect of corpus controversial-ity.
In INLG ?08: Proceedings of the Fifth Inter-national Natural Language Generation Conference,pages 33?41, Morristown, NJ, USA.
Association forComputational Linguistics.Giuseppe Carenini, Jackie Chi Kit Cheung, and AdamPauls.
2013.
Multi-document summarizationof evaluative text.
Computational Intelligence,29(4):545?576.Giuseppe Di Fabbrizio, Amanda Stent, and RobertGaizauskas.
2014.
A hybrid approach to multi-document summarization of opinions in reviews.
InProceedings of the 8th International Natural Lan-guage Generation conference, INLG 2014.G?unes Erkan and Dragomir R. Radev.
2004.
Lexrank:graph-based lexical centrality as salience in textsummarization.
J. Artif.
Int.
Res., 22(1):457?479,December.Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.2010.
Opinosis: a graph-based approach to abstrac-tive summarization of highly redundant opinions.
InProceedings of the 23rd International Conferenceon Computational Linguistics, COLING ?10, pages340?348, Stroudsburg, PA, USA.
Association forComputational Linguistics.1611Minqing Hu and Bing Liu.
2004a.
Mining and sum-marizing customer reviews.
In Proceedings of ACMSIGKDD Conference on Knowledge Discovery andData Mining 2004 (KDD 2004), pages 168?177,Seattle, Washington.Minqing Hu and Bing Liu.
2004b.
Mining opinionfeatures in customer reviews.
In Proceedings of theNineteenth National Conference on Artificial Intelli-gence (AAAI-2004).Minqing Hu and Bing Liu.
2006.
Opinion featureextraction using class sequential rules.
In Pro-ceedings of AAAI 2006 Spring Sympoia on Compu-tational Approaches to Analyzing Weblogs (AAAI-CAAW 2006).Shafiq Joty, Giuseppe Carenini, Raymond Ng, andYashar Mehdad.
2013.
Combining intra- and multi-sentential rhetorical parsing for document-level dis-course analysis.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 486?496,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Hyun Duk Kim, Kavita Ganesan, Parikshit Sondhi, andChengXiang Zhai.
2011.
Comprehensive review ofopinion summarization.Hyun Duk Kim, Malu Castellanos, Meichun Hsu,ChengXiang Zhai, Umeshwar Dayal, and Riddhi-man Ghosh.
2013.
Compact explanatory opinionsummarization.
In Proceedings of the 22Nd ACMInternational Conference on Conference on Infor-mation &#38; Knowledge Management, CIKM ?13,pages 1697?1702, New York, NY, USA.
ACM.Angeliki Lazaridou, Ivan Titov, and CarolineSporleder.
2013.
A bayesian model for jointunsupervised induction of sentiment, aspect anddiscourse representations.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1630?1639, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.Kevin Lerman, Sasha Blair-Goldensohn, and Ryan Mc-Donald.
2009.
Sentiment summarization: Evaluat-ing and learning user preferences.
In Proceedings ofthe 12th Conference of the European Chapter of theAssociation for Computational Linguistics, EACL?09, pages 514?522, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Hugo Liu and Push Singh.
2004.
Conceptnet: A prac-tical commonsense reasoning toolkit.
BT Technol-ogy Journal, 22(4):211?226.Yue Lu, ChengXiang Zhai, and Neel Sundaresan.2009.
Rated aspect summarization of short com-ments.
In Proceedings of the 18th InternationalConference on World Wide Web, WWW ?09, pages131?140, New York, NY, USA.
ACM.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.Subhabrata Mukherjee and Sachindra Joshi.
2013.Sentiment aggregation using conceptnet ontology.In Proceedings of the 6th International Joint Con-ference on Natural Language Processing, IJCNLP2013.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe 42Nd Annual Meeting on Association for Com-putational Linguistics, ACL ?04, Stroudsburg, PA,USA.
Association for Computational Linguistics.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: Sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 Conference on Empirical Methods in Natu-ral Language Processing - Volume 10, EMNLP ?02,pages 79?86, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Dragomir Radev, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer, Arda C?elebi, StankoDimitrov, Elliott Drabek, Ali Hakim, Wai Lam,Danyu Liu, Jahna Otterbacher, Hong Qi, HoracioSaggion, Simone Teufel, Michael Topper, AdamWinkel, and Zhu Zhang.
2004.
MEAD ?
A plat-form for multidocument multilingual text summa-rization.
In Conference on Language Resources andEvaluation (LREC), Lisbon, Portugal.Ehud Reiter and Robert Dale.
2000.
Building NaturalLanguage Generation Systems.
Cambridge Univer-sity Press, New York, NY, USA.Benjamin Snyder and Regina Barzilay.
2007.
Multipleaspect ranking using the good grief algorithm.
InHLT-NAACL, pages 300?307.Swapna Somasundaran, Galileo Namata, JanyceWiebe, and Lise Getoor.
2009.
Supervised andunsupervised methods in employing discourse rela-tions for improving opinion polarity classification.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume1 - Volume 1, EMNLP ?09, pages 170?179, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Ivan Titov and Ryan T. McDonald.
2008.
A jointmodel of text and aspect ratings for sentiment sum-marization.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Lin-guistics, June 15-20, 2008, Columbus, Ohio, USA,ACL 2008, pages 308?316.
Association for Compu-tational Linguistics.Rakshit S. Trivedi and Jacob Eisenstein.
2013.
Dis-course connectors for latent subjectivity in sentimentanalysis.
In HLT-NAACL, pages 808?813.
The As-sociation for Computational Linguistics.1612Wenpu Xing and Ali Ghorbani.
2004.
Weighted pager-ank algorithm.
In Proceedings of the Second AnnualConference on Communication Networks and Ser-vices Research, CNSR ?04, pages 305?314.
IEEEComputer Society.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Pro-ceedings of the 18th conference on Computationallinguistics - Volume 2, COLING ?00, pages 947?953, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.1613
