Proceedings of SPEECHGRAM 2007, pages 41?48,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Bidirectional Grammar-Based Medical Speech TranslatorPierrette Bouillon1, Glenn Flores2, Marianne Starlander1, Nikos Chatzichrisafis1Marianne Santaholma1, Nikos Tsourakis1, Manny Rayner1,3, Beth Ann Hockey41 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, SwitzerlandPierrette.Bouillon@issco.unige.chMarianne.Starlander@eti.unige.ch, Nikos.Chatzichrisafis@vozZup.comMarianne.Santaholma@eti.unige.ch, Nikolaos.Tsourakis@issco.unige.ch2 Medical College of Wisconsin, 8701 Watertown Plank Road, Milwaukee, WI 53226gflores@mcw.edu3 Powerset, Inc., 475 Brannan Street, San Francisco, CA 94107manny@powerset.com4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000bahockey@ucsc.eduAbstractWe describe a bidirectional version of thegrammar-based MedSLT medical speechsystem.
The system supports simple medi-cal examination dialogues about throat painbetween an English-speaking physician anda Spanish-speaking patient.
The physician?sside of the dialogue is assumed to consistmostly of WH-questions, and the patient?s ofelliptical answers.
The paper focusses on thegrammar-based speech processing architec-ture, the ellipsis resolution mechanism, andthe online help system.1 BackgroundThere is an urgent need for medical speech trans-lation systems.
The world?s current populationof 6.6 billion speaks more than 6,000 languages(Graddol, 2004).
Language barriers are associatedwith a wide variety of deleterious consequences inhealthcare, including impaired health status, a lowerlikelihood of having a regular physician, lower ratesof mammograms, pap smears, and other preven-tive services, non-adherence with medications, agreater likelihood of a diagnosis of more severe psy-chopathology and leaving the hospital against med-ical advice among psychiatric patients, a lower like-lihood of being given a follow-up appointment af-ter an emergency department visit, an increased riskof intubation among children with asthma, a greaterrisk of hospital admissions among adults, an in-creased risk of drug complications, longer medicalvisits, higher resource utilization for diagnostic test-ing, lower patient satisfaction, impaired patient un-derstanding of diagnoses, medications, and follow-up, and medical errors and injuries (Flores, 2005;Flores, 2006).
Nevertheless, many patients whoneed medical interpreters do not get them.
For ex-ample, in the United States, where 52 million peo-ple speak a language other than English at homeand 23 million people have limited English profi-ciency (LEP) (Census, 2007), one study found thatabout half of LEP patients presenting to an emer-gency department were not provided with a medicalinterpreter (Baker et al, 1996).
There is thus a sub-stantial gap between the need for and availability oflanguage services in health care, a gap that could bebridged through effective medical speech translationsystems.An ideal system would be able to interpret ac-curately and flexibly between patients and healthcare professionals, using unrestricted language anda large vocabulary.
A system of this kind is, un-fortunately, beyond the current state of the art.It is, however, possible, using today?s technol-ogy, to build speech translation systems for specificscenarios and language-pairs, which can achieveacceptable levels of reliability within the bounds41of a well-defined controlled language.
MedSLT(Bouillon et al, 2005) is an Open Source systemof this type, which has been under construction atGeneva University since 2003.
The system is builton top of Regulus (Rayner et al, 2006), an OpenSource platform which supports development ofgrammar-based speech-enabled applications.
Regu-lus has also been used to build several other systems,including NASA?s Clarissa (Rayner et al, 2005b).The most common architecture for speech trans-lation today uses statistical methods to perform bothspeech recognition and translation, so it is worthclarifying why we have chosen to use grammar-based methods.
Even though statistical architec-tures exhibit many desirable properties (purely data-driven, domain independent), this is not necessar-ily the best alternative in safety-critical medical ap-plications.
Anecdotally, many physicians expressreluctance to trust a translation device whose out-put is not readily predictable, and most of thespeech translation systems which have reached thestage of field testing rely on various types ofgrammar-based recognition and rule-based transla-tion (Phraselator, 2007; Fluential, 2007).Statistical speech recognisers can achieve impres-sive levels of accuracy when trained on enough data,but it is a daunting task to collect training mate-rial in the requisite quantities (usually, tens of thou-sands of high-quality utterances) when trying tobuild practical systems.
Considering that the medi-cal speech translation applications we are interestedin constructing here need to work for multiple lan-guages and subdomains, the problem becomes evenmore challenging.
Our experience is that grammar-based systems which also incorporate probabilisticcontext-free grammar tuning deliver better resultsthan purely statistical ones when training data aresparse (Rayner et al, 2005a).Another common criticism of grammar-basedsystems is that out-of-coverage utterances willneither be recognized nor translated, an objec-tion that critics have sometimes painted as de-cisive.
It is by no means obvious, however,that restricted coverage is such a serious prob-lem.
In text processing, work on several gener-ations of controlled language systems has devel-oped a range of techniques for keeping users withinthe bounds of system coverage (Kittredge, 2003;Mitamura, 1999), and variants of these methods canalso be adapted for spoken language applications.Our experiments with MedSLT show that even aquite simple help system is enough to guide usersquickly towards the intended coverage of a medium-vocabulary grammar-based speech translation appli-cation, with most users appearing confident after justan hour or two of exposure (Starlander et al, 2005;Chatzichrisafis et al, 2006).Until recently, the MedSLT system only sup-ported unidirectional processing in the physicianto patient direction.
The assumption was that thephysician would mostly ask yes/no questions, towhich the patient would respond non-verbally, forexample by nodding or shaking their head.
A uni-directional architecture is easier to make habitablethan a bidirectional one.
It is reasonable to as-sume that the physician will use the system regu-larly enough to learn the coverage, but most patientswill not have used the system before, and it is lessclear that they will be able to acclimatize within thenarrow window at their disposal.
These consider-ations must however be balanced against the factthat a unidirectional system does not allow for apatient-centered interaction characterized by mean-ingful patient-clinician communication or shared de-cision making.
Multiple studies in the medical lit-erature document that patient-centeredness, effec-tive patient-clinician communication, and shared de-cision making are associated with significant im-provements in patient health outcomes, includingreduced anxiety levels, improved functional sta-tus, reduced pain, better control of diabetes melli-tus, blood pressure reduction among hypertensives,improved adherence, increased patient satisfaction,and symptom reduction for a variety of conditions(Stewart, 1995; Michie et al, 2003).
A bidirectionalsystem is considered close to essential from a health-care perspective, since it appropriately addresses thekey issues of patient centeredness and shared de-cision making.
For these reasons, we have overthe last few months developed a bidirectional ver-sion of MedSLT, initially focussing on a throat painscenario with an English-speaking physician and aSpanish-speaking patient.
The physician uses fullsentences, while the patient answers with short re-sponses.One of the strengths of the Regulus approach is42that it is very easy to construct parallel versions ofa grammar; generally, all that is required is to varythe training corpus.
(We will have more to say aboutthis soon).
We have exploited these properties ofthe platform to create two different configurationsof the bidirectional system, so that we can comparecompeting approaches to the problem of accommo-dating patients unfamiliar with speech technology.In Version 1 (less restricted), the patient is allowedto answer using both elliptical utterances and shortsentences, while in Version 2 (more restricted) theyare only permitted to use elliptical utterances.
Thus,for example, if the physician asks the question ?Howlong have you had a sore throat?
?, Version 1 allowsthe patient to respond both ?Desde algunos d??as?
(?For several days?)
and ?Me ha dolido la gargantadesde algunos d??as?
(?I have had a sore throat forseveral days?
), while Version 2 only allows the firstof these.
Both the short and the long versions aretranslated uniformly, with the short version resolvedusing the context from the preceding question.In both versions, if the patient finds it too chal-lenging to use the system to answer WH-questionsdirectly, it is possible to back off to the earlier di-alogue architecture in which the physician uses Y-N questions and the patient responds with simpleyes/no answers, or nonverbally.
Continuing the ex-ample, if the patient is unable to find an appro-priate way to answer the physician?s question, thephysician could ask ?Have you had a sore throat formore than three days??
; if the patient responds nega-tively, they could continue with the follow-on ques-tion ?More than a week?
?, and so on.In the rest of the paper, we first describe thesystem top-level (Section 2), the way in whichgrammar-based processing is used (Section 3), theellipsis processing mechanism (Section 4), and thehelp system (Section 5).
Section 6 presents an ini-tial evaluation, and the final section concludes.2 Top-level architectureThe system is operated through the graphical userinterface (GUI) shown in Figures 1 and 2.
Inaccordance with the basic principles of patient-centeredness and shared decision-making outlinedin Section 1, the patient and the physician each havetheir own headset, use their own mouse, and sharethe same view of the screen.
This is in sharp contrastto the majority of the medical speech translation sys-tems described in the literature (Somers, 2006).As shown in the screenshots, the main GUI win-dow is separated into two tabbed panes, marked?Doctor?
and ?Patient?.
Initially, the ?Doctor?
view(the one shown in Figure 1) is active.
The physicianpresses the ?Push to talk?
button, and speaks intothe headset microphone.
If recognition is success-ful, the GUI displays four separate results, listed onthe right side of the screen.
At the top, immediatelyunder the heading ?Question?, we can see the actualwords returned by speech recognition.
Here, thesewords are ?Have you had rapid strep test?.
Below,we have the help pane: this displays similar ques-tions taken from the help corpus, which are known tobe within system coverage.
The pane marked ?Sys-tem understood?
shows a back-translation, producedby first translating the recognition result into inter-lingua, and then translating it back into English.
Inthe present example, this corrects the minor mistakethe recogniser has made, missing the indefinite ar-ticle ?a?, and confirms that the system has obtaineda correct grammatical analysis and interpretation atthe level of interlingua.
At the bottom, we see thetarget language translation.
The left-hand side of thescreen logs the history of the conversation to date, sothat both sides can refer back to it.If the physician decides that the system has cor-rectly understood what they said, they can now pressthe ?Play?
button.
This results in the system produc-ing a spoken output, using the Vocalizer TTS engine.Simultaneously with speaking, the GUI shifts to the?Patient?
configuration shown in Figure 2.
This dif-fers from the ?Doctor?
configuration in two respects:all text is in the patient language, and the help panepresents its suggestions immediately, based on thepreceding physician question.
The various process-ing components used to support these functionalitiesare described in the following sections.3 Grammar-based processingGrammar-based processing is used for source-language speech recognition and target-side genera-tion.
(Source-language analysis is part of the recog-nition process, since grammar-based recognition in-cludes creating a parse).
All of these functionalities43Figure 1: Screenshot showing the state of the GUI after the physician has spoken, but before he has pressedthe ?Play?
button.
The help pane shows similar queries known to be within coverage.Figure 2: Screenshot showing the state of the GUI after the physician has pressed the ?Play?
button.
Thehelp pane shows known valid responses to similar questions.44are implemented using the Regulus platform, withthe task-specific grammars compiled out of generalfeature grammar resources by the Regulus tools.
Forboth recognition and generation, the first step isto extract a domain-specific feature grammar fromthe general one, using a version of the ExplanationBased Learning (EBL) algorithm.The extraction process is driven by a corpus of ex-amples and a set of ?operationality criteria?, whichdefine how the rules in the original resource gram-mar are recombined into domain-specific ones.
It isimportant to realise that the domain-specific gram-mar is not merely a subset of the resource grammar;a typical domain-specific grammar rule is created bymerging two to five resource grammar rules into asingle ?flatter?
rule.
The result is a feature gram-mar which is less general than the original one, butmore efficient.
For recognition, the grammar is thenprocessed further into a CFG language model, usingan algorithm which alternates expansion of featurevalues and filtering of the partially expanded gram-mar to remove irrelevant rules.
Detailed descrip-tions of the EBL learning and feature grammar ?CFG compilation algorithms can be found in Chap-ters 8 and 10 of (Rayner et al, 2006).
Regulus fea-ture grammars can also be compiled into generatorsusing a version of the Semantic Head Driven algo-rithm (Shieber et al, 1990).The English (physician) side recogniser is com-piled from the large English resource grammar de-scribed in Chapter 9 of (Rayner et al, 2006), andwas constructed in the same way as the one de-scribed in (Rayner et al, 2005a), which was used fora headache examination task.
The operationality cri-teria are the same, and the only changes are a differ-ent training corpus and the addition of new entriesto the lexicon.
The same resources, with a differ-ent training corpus, were used to build the Englishlanguage generator.
It is worth pointing out that, al-though a uniform method was used to build thesevarious grammars, the results were all very differ-ent.
For example, the recognition grammar from(Rayner et al, 2005a) is specialised to cover onlysecond-person questions (?Do you get headachesin the mornings??
), while the generator grammarused in the present application covers only first-person declarative statements (?I visited the doctorlast Monday.?).
In terms of structure, each gram-mar contains several important constructions that theother lacks.
For example, subordinate clauses arecentral in the headache domain (?Do the headachesoccur when you are stressed??)
but are not presentin the sore throat domain; this is because the stan-dard headache examination questions mostly focuson generic conditions, while the sore throat exami-nation questions only relate to concrete ones.
Con-versely, relative clauses are important in the sorethroat domain (?I have recently been in contact withsomeone who has strep throat?
), but are not suffi-ciently important in the headache domain to be cov-ered there.On the Spanish (patient) side, there are fourgrammars involved.
For recognition, we havetwo different grammars, corresponding to the twoversions of the system; the grammar for Ver-sion 2 is essentially a subset of that for Version1.
For generation, there are two separate andquite different grammars: one is used for trans-lating the physician?s questions, while the otherproduces back-translations of the patient?s ques-tions.
All of these grammars are extracted froma general shared resource grammar for Romancelanguages, which currently combines rules forFrench, Spanish and Catalan (Bouillon et al, 2006;Bouillon et al, to appear 2007b).One interesting consequence of our methodologyis related to the fact that Spanish is a prodrop lan-guage, which implies that many sentences are sys-tematically ambiguous between declarative and Y-Nquestion readings.
For example, ?He consultado unme?dico?
could in principle mean either ?I visited adoctor?
or ?Did I visit a doctor??.
When training thespecialised Spanish grammars, it is thus necessary tospecify which readings of the training sentences areto be used.
Continuing the example, if the sentenceoccurred in training material for the answer gram-mar, we would specify that the declarative readingwas the intended one1.4 Ellipsis processing and contextualinterpretationIn Version 1 of the system, the patient is per-mitted to answer using elliptical phrases; in Ver-1The specification can be formulated as a preference thatapplies uniformly to all the training examples in a given group.45sion 2, she is obliged to do so.
Ability to pro-cess elliptical responses makes it easier to guide thepatient towards the intended coverage of the sys-tem, without degrading the quality of recognition(Bouillon et al, to appear 2007a).
The downside isthat ellipses are also harder to translate than full sen-tences.
Even in a limited domain like ours, and in aclosely related language-pair, ellipsis can generallynot be translated word for word, and it is necessaryto look at the preceding context if the rules are tobe applied correctly.
In examples 1 and 2 below,the locative phrase ?In your stomach?
in the Englishsource becomes the subject in the Spanish transla-tion.
This implies that the translation of the ellipsisin the second physician utterance needs to changesyntactic category: ?In your head?
(PP) becomes?La cabeza?
(NP).
(1) Doctor: Do you have a pain in yourstomach?
(Trans): Le duele el estomago?
(2) Doctor: In your head?
(Trans): *En la cabeza?Since examples like this are frequent, our sys-tem implements a solution in which the patient?sreplies are translated in the context of the preced-ing utterance.
If the patient-side recogniser?s outputis classified as an ellipsis (this can done fairly reli-ably thanks to use of suitably specialised grammars;cf.
Section 3), we expand the incomplete phraseinto a full sentence structure by adding appropriatestructural elements from the preceding physician-side question; the expanded semantic structure is theone which is then translated into interlingual form,and thence back to the physician-side language.Since all linguistic representations, includingthose of elliptical phrases and their contexts, are rep-resented as flat attribute-value lists, we are able toimplement the resolution algorithm very simply interms of list manipulation.
In YN-questions, wherethe elliptical answer intuitively adds information tothe question (?Did you visit the doctor??
; ?El lunes??
?I visited the doctor on Monday?
), the repre-sentations are organised so that resolution mainlyamounts to concatenation of the two lists2.
In WH-questions, where the answer intuitively substitutesthe elliptical answer for the WH-phrase (?What is2It is also necessary to replace second-person pronouns withfirst-person counterparts.your temperature??
; ?Cuarenta grados??
?My tem-perature is forty degrees?
), resolution substitutes therepresentation of the elliptical phrase for that of asemantically similar element in the question.The least trivial aspect of this process is provid-ing a suitable definition of ?semantically similar?.This is done using a simple example-based method,in which the grammar developer writes a set of dec-larations, each of which lists a set of semanticallysimilar NPs.
At compile-time, the grammar is usedto parse each NP, and extract a generalised skele-ton, in which specific lexical information is strippedaway; at run-time, two NPs are held to be semanti-cally similar if they can each be unified with skele-tons in the same equivalence class.
This ensures thatthe definition of the semantic similarity relation isstable across most changes to the grammar and lex-icon.
The issues are described in greater detail in(Bouillon et al, to appear 2007a).5 Help systemSince the performance of grammar-based speech un-derstanding is only reliable on in-coverage mate-rial, systems based on this type of architecture mustnecessarily use a controlled language approach, inwhich it is assumed that the user is able to learn therelevant coverage.
As previously noted, the Med-SLT system addresses this problem by incorporat-ing an online help system (Starlander et al, 2005;Chatzichrisafis et al, 2006).On the physician side, the help system offers, af-ter each recognition event, a list of related ques-tions; similarly, on the patient side, it provides ex-amples of known valid answers to the current ques-tion.
In both cases, the help examples are extractedfrom a precompiled corpus of question-answer pairs,which have been judged for correctness by systemdevelopers.
The process of selecting the examplesis slightly different on the two sides.
For questions(physician side), the system performs a second par-allel recognition of the input speech, using a sta-tistical recogniser.
It then compares the recogni-tion result, using an N-gram based metric, againstthe set of known correct in-coverage questions fromthe question-answer corpus, to extract the most sim-ilar ones.
For answers (patient side), the help sys-tem searches the question-answer corpus to find the46questions most similar to the current one, and showsthe list of corresponding valid answers, using thewhole list in the case of Version 1 of the system, andonly the subset consisting of elliptical phrases in thecase of Version 2.6 EvaluationIn previous studies, we have evaluated speechrecognition and speech understanding per-formance for physician-side questions inEnglish (Bouillon et al, 2005) and Spanish(Bouillon et al, to appear 2007b), and investi-gated the impact on performance of the help system(Rayner et al, 2005a; Starlander et al, 2005).
Wehave also carried out recent evaluations designed tocontrast recognition performance on elliptical andfull versions of the same utterance; here, our resultssuggest that elliptical forms of (French-language)MedSLT utterances are slightly easier to recognisein terms of semantic error rate than full sententialforms (Bouillon et al, to appear 2007a).
Our initialevaluation studies on the bidirectional system havefocussed on a specific question which has particularrelevance to this new version of MedSLT.
Sincewe are assuming that the patient will respondusing elliptical utterances, and that these utteranceswill be translated in the context of the precedingphysician-side question, how confident can webe that this context-dependent translation will becorrect?In order to investigate these issues, we performeda small data-collection using Version 2 of the sys-tem, whose results we summarise here.
One of theauthors of the paper played the role of an English-speaking physician, in a simulated medical exam-ination scenario where the goal was to determinewhether or not the ?patient?
was suffering from aviral throat infection.
The six subjects playing therole of the patient were all native speakers of Span-ish, and had had no previous exposure to the system,or indeed any kind of speech technology.
They weregiven cards describing the symptoms they were sup-posed to be displaying, on which they were askedto based their answers.
From a total of 92 cor-rectly recognised patient responses, we obtained 50yes/no answers and 42 examples of real elliptical ut-terances.
Out of these, 36 were judged to have beentranslated completely correctly, and a further 3 werejudged correct in terms of meaning, but less than flu-ent.
Only 3 examples were badly translated: of thesetwo were caused by problems in a translation rule,and one by incorrect treatment of ellipsis resolution.We show representative exchanges below; the last ofthese is the one in which ellipsis processing failed towork correctly.
(3) Doctor: For how long have youhad your sore throat?Patient: Desde hace ma?s deuna semana(Trans): I have had a sorethroat for more than one week(4) Doctor: What were the results?Patient: Negativo(Trans): The results were negative(5) Doctor: Have you seen a doctorfor your sore throat?Patient: S??
el lunes(Trans): I visited the doctorfor my sore throat monday(6) Doctor: Have you been with anyonerecently who has a strep throat?Patient: Si ma?s de dos semanas(Trans): I was in contact with someonemore than two weeks recentlywho had strep throat7 ConclusionsWe have presented a bidirectional grammar-basedEnglish ?
Spanish medical speech translation sys-tem built using a linguistically motivated archi-tecture, where all linguistic information is ulti-mately derived from two resource grammars, onefor each language.
We have shown how this en-ables us to derive the multiple grammars needed,which differ both with respect to function (recog-nition/generation) and to domain (physician ques-tions/patient answers).
The system is currently un-dergoing initial lab testing; we hope to advance toinitial trials on real patients some time towards theend of the year.References[Baker et al1996] D.W. Baker, R.M.
Parker, M.V.Williams, W.C. Coates, and Kathryn Pitkin.
1996.47Use and effectiveness of interpreters in an emer-gency department.
Journal of the American MedicalAssociation, 275:783?8.
[Bouillon et al2005] P. Bouillon, M. Rayner,N.
Chatzichrisafis, B.A.
Hockey, M. Santaholma,M.
Starlander, Y. Nakao, K. Kanzaki, and H. Isahara.2005.
A generic multi-lingual open source platformfor limited-domain medical speech translation.
InProceedings of the 10th Conference of the EuropeanAssociation for Machine Translation (EAMT), pages50?58, Budapest, Hungary.
[Bouillon et al2006] P. Bouillon, M. Rayner, B. Novel-las Vall, Y. Nakao, M. Santaholma, M. Starlander, andN.
Chatzichrisafis.
2006.
Une grammaire multilinguepartage?e pour la traduction automatique de la parole.In Proceedings of TALN 2006, Leuwen, Belgium.
[Bouillon et alto appear 2007a] P. Bouillon, M. Rayner,M.
Santaholma, and M. Starlander.
to appear 2007a.Les ellipses dans un syste`me de traduction automa-tique de la parole.
In Proceedings of TALN 2006,Toulouse, France.
[Bouillon et alto appear 2007b] P. Bouillon, M. Rayner,B.
Novellas Vall, Y. Nakao, M. Santaholma, M. Star-lander, and N. Chatzichrisafis.
to appear 2007b.
Unegrammaire partage?e multi-ta?che pour le traitement dela parole : application aux langues romanes.
Traite-ment Automatique des Langues.
[Census2007] U.S. Census, 2007.
Selected Social Char-acteristics in the United States: 2005.
Data Set: 2005American Community Survey.
Available here.
[Chatzichrisafis et al2006] N. Chatzichrisafis, P. Bouil-lon, M. Rayner, M. Santaholma, M. Starlander, andB.A.
Hockey.
2006.
Evaluating task performance fora unidirectional controlled language medical speechtranslation system.
In Proceedings of the HLT-NAACLInternational Workshop on Medical Speech Transla-tion, pages 9?16, New York.
[Flores2005] G. Flores.
2005.
The impact of medical in-terpreter services on the quality of health care: A sys-tematic review.
Medical Care Research and Review,62:255?299.
[Flores2006] G. Flores.
2006.
Language barriers tohealth care in the united states.
New England Journalof Medicine, 355:229?231.
[Fluential2007] Fluential, 2007.http://www.fluentialinc.com.
As of 24 March2007.
[Graddol2004] D. Graddol.
2004.
The future of lan-guage.
Science, 303:1329?1331.
[Kittredge2003] R. I. Kittredge.
2003.
Sublanguages andcomtrolled languages.
In R. Mitkov, editor, The Ox-ford Handbook of Computational Linguistics, pages430?447.
Oxford University Press.
[Michie et al2003] S. Michie, J.
Miles, and J. Weinman.2003.
Patient-centeredness in chronic illness: what isit and does it matter?
Patient Education and Counsel-ing, 51:197?206.
[Mitamura1999] T. Mitamura.
1999.
Controlled lan-guage for multilingual machine translation.
In Pro-ceedings of Machine Translation Summit VII, Singa-pore.
[Phraselator2007] Phraselator, 2007.http://www.voxtec.com/.
As of 24 March 2007.
[Rayner et al2005a] M. Rayner, P. Bouillon,N.
Chatzichrisafis, B.A.
Hockey, M. Santaholma,M.
Starlander, H. Isahara, K. Kanzaki, and Y. Nakao.2005a.
A methodology for comparing grammar-basedand robust approaches to speech understanding.
InProceedings of the 9th International Conferenceon Spoken Language Processing (ICSLP), pages1103?1107, Lisboa, Portugal.
[Rayner et al2005b] M. Rayner, B.A.
Hockey, J.M.
Ren-ders, N. Chatzichrisafis, and K. Farrell.
2005b.
Avoice enabled procedure browser for the InternationalSpace Station.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics (interactive poster and demo track), Ann Arbor,MI.
[Rayner et al2006] M. Rayner, B.A.
Hockey, andP.
Bouillon.
2006.
Putting Linguistics into SpeechRecognition: The Regulus Grammar Compiler.
CSLIPress, Chicago.
[Shieber et al1990] S. Shieber, G. van Noord, F.C.N.Pereira, and R.C.
Moore.
1990.
Semantic-head-drivengeneration.
Computational Linguistics, 16(1).
[Somers2006] H. Somers.
2006.
Language engineeringand the path to healthcare: a user-oriented view.
InProceedings of the HLT-NAACL International Work-shop on Medical Speech Translation, pages 32?39,New York.
[Starlander et al2005] M. Starlander, P. Bouillon,N.
Chatzichrisafis, M. Santaholma, M. Rayner, B.A.Hockey, H. Isahara, K. Kanzaki, and Y. Nakao.
2005.Practising controlled language through a help systemintegrated into the medical speech translation system(MedSLT).
In Proceedings of MT Summit X, Phuket,Thailand.
[Stewart1995] M.A.
Stewart.
1995.
Effective physician-patient communication and health outcomes: a review.Canadian Medical Association Journal, 152:1423?1433.48
