Words and Pictures in the NewsJaety EdwardsUC Berkeleyjaety@cs.berkeley.eduRyan WhiteUC Berkeleyryanw@cs.berkeley.eduDavid ForsythUC Berkeleydaf@cs.berkeley.eduAbstractWe discuss the properties of a collection ofnews photos and captions, collected from theAssociated Press and Reuters.
Captions havea vocabulary dominated by proper names.
Wehave implemented various text clustering algo-rithms to organize these items by topic, as wellas an iconic matcher that identifies articles thatshare a picture.
We have found that the spe-cial structure of captions allows us to extractsome names of people actually portrayed in theimage quite reliably, using a simple syntacticanalysis.
We have been able to build a directoryof face images of individuals from this collec-tion.1 IntroductionFor the past year we have been building a collection ofcaptioned news photos and illustrated news articles.
Webelieve that, for many applications, words and picturestogether provide very rich information on document con-tent.
Photographs can link articles in ways that pure tex-tual analysis may overlook or underestimate, and textprovides high level descriptions of image contents thatcurrent vision techniques cannot obtain.Our analysis of image captions has revealed variousjournalistic conventions that we believe make this datasetparticularly appealing for a number of applications.
Cap-tions act as concise summaries of events, and we believewe can use them to isolate the most pertinent words fordifferent topics.
We have implemented various clusteringmethods to explore this idea.Captions are also tightly tied to the actual content ofthe image.
The difficulty here on the text side, is in identi-fying those portions of a caption that refer to tangible ob-jects, physically present in the image.
On the image side,we need to isolate objects of interest and solve the corre-spondence problem between caption extracts and imageregions.
We are exploring these issues in the context oftrying to build an automated celebrity directory and faceclassifier.2 The CollectionWe have been collecting news photos with associatedcaptions since Feb. 2002.
These photographs come fromReuters and the AP, and we collect between 800 and 1500per day.
To date, we have collected more than 410,000unique images.
The vast majority of these photos are ofpeople, and the majority of that subset focus on one ortwo individuals.
Since November, we have also collectedmore than 75,000 illustrated news articles from the BBCand CNN.
Both CNN and the BBC, along with the ma-jority of all news outlets, make heavy use of news pho-tographs from the AP and Reuters.
In fact, the field isdominated by just three agencies, Reuters, the AssociatedPress and AFP.Journalistic writing guidelines emphasize clarity.
Thisis certainly necessary in caption writing, where an authorhas an average of 55 words to convey all salient detailsabout an image.
A caption writer?s first responsibility isto identify those portrayed.
A caption will contain thefull names of those represented if they are known.
Sec-ond, the writer must describe the activity portrayed in thephoto.
Finally, with whatever space is left, the authormay provide some broader context.Our collection?s vocabulary reflects this ordering ofpriorities.
First, there is a heavy bias towards propernames, and thus the number of unique terms in this set isvery large (See figure 1).
The vocabulary of other wordclasses, however, reflect the journalistic emphasis on sim-plicity.
People hug rather than embrace, talk rather thanconverse.Captions are also easy for humans to parse becausewriters make use of stylized sentence structures.
JustFigure 1: Caption vocabularies exhibit statistical properties that distinguish them from other collections.
A heavypreponderance of proper names creates a very large vocabulary of capitalized words whose frequency distributionhas heavy tails.
On the other hand, other word classes are somewhat restricted.
In a six month period from July toDecember, 2002, the corpus had 93,457 distinct terms.
Of these, 60,182 were capitalized and 32,059 uncapitalized(with 1216 numerical entries).
About 1000 terms occur in both forms.
Almost a third of all vocabulary terms occuronly once.
On the left we plot term occurrence ordered by frequency for the 1000 most frequent capitalized (solid)and uncapitalized ( dotted) terms.
On the right we analyze the heavy tail, flipping the axes to show the number ofwords that occur between one and twenty times.
Here, capitalized (solid line) words outnumber uncapitalized (dottedline) ones 2-1.
We have used capitalization as a proxy for proper names.
This obviously misrepresents initial wordsof sentences, but these only represent on average 2 words out of every 50-60 word caption.
The richness of ourvocabulary is thus largely due to proper names.over 50% of the captions in our collection, for instance,begin with a starting noun phrase that describes the cen-tral figure of the photograph followed by a present tenseverb that describes the action they are performing (or ifin the passive voice, having performed upon them).
Tex-tual cues such as ?left?,?right?
or a jersey number help toclarify any potential correspondence problems betweenmultiple names in the caption and people in the image.The news photos themselves, like their captions, arealso quite stylized with respect to choice of subject,placement within the image, and photographic tech-niques.
A single individual will generally be centered inthe photo.
Two people will be equally offset to the leftand right of center.
A basketball player will most likelyhave the ball in his hands.
Each of these photographs willemploy a narrow depth of field to blur the backgroundand emphasize their subjects.
Like the caption writer, thephoto journalist must convey a great deal of informationin a small amount of space.
These conventions amountto a shared language between photographer and readerthat places this single image in a far richer context.
Wepresent example captions and their associated images infigure 2.The textual and photographic conventions we have il-lustrated are simply trends we have noticed in our dataset,ones that many individual captions and images in our col-lection break.
One of the benefits of scale, however, isthat we can throw away a great deal of data and still havemeaningful sets with which to work.
These simpler setsin turn may act as foundations from which to attack theexceptions.3 Linking ArticlesWe have linked and clustered articles in a variety of ways.We have looked at how the specialized vocabulary andstatistics of our dataset affect the performance of two dif-ferent textual clustering methods.
We have also examinedhow images might help to link together articles whose re-lationships purely text based clustering overlooks or un-dervalues.
We have also used the clusterings built fromour captions to provide a topic structure for navigatingthe news in general, and investigated various interfacesto make this navigation more natural.3.1 Iconic MatchingSometimes the same photograph is used to illustrate dif-ferent articles.
This establishes links between articles thatmight be difficult to discern solely from the text.
To es-tablish these links, we built an iconic matcher to findcopies of the same image in a database, even if that imagehas been moderately cropped or given a border.Images can change in many subtle ways between theirdistribution and their actual publication.
They may becropped, watermarked, and acquire compression arti-facts.
The news agency may overlay text or add borders.An iconic matcher needs to be robust to these changes.In addition, given a collection of this size, only minimalimage processing is practical.The iconic matcher we have designed first resizes im-ages to 128x128 pixels in order to accommodate differentaspect ratios.
It then applies a Haar wavelet transforma-tion separately to each color channel of the image.
(Ja-U.S. President George W. Bush (news - web sites) ponders a question at a news confer-ence following his meeting with Czech President Vaclav Havel at Prague Castle, Wednes-day, Nov. 20, 2002.
Bush urged the NATO (news - web sites) allies to join a U.S.-led?coalition of the willing?
to ensure Iraq disarms.
(AP Photo/CTK/ Michal Dolezal) Nov 205:52 AMSacramento Monarchs?
Ruthie Bolton, center, grabs a loose ball from Charlotte Sting?sErin Buescher, bottom, as Monarchs?
Andrea Nagy moves in during the first half Friday,July 19, 2002, in Charlotte, N.C. (AP Photo/Rick Havner) Jul 19 8:24 PMFigure 2: News captions follow many conventions.
Individuals are identified with their full names whenever possible.The first sentence describes the actors, and what they are doing.
By looking for proper names immediately followedby present tense verbs, we can reliably find names that reference people actually in the image.
This is important, ascaptions are also full of names (i.e.
Vaclav Havel in caption 1) that do not actually appear.
Often, the captions alsohelp place the person in the frame (i.e.
?center?
in the second image).cobs et al, 1995) The first coefficient in each channel,then, represents the average image intensity; the secondis the deviation from the mean for the right and left side;the third splits the left hand side similarly, and so on.
Ro-tating through the channels, we use the first 64 largestcoefficients as a signature for the image.
We say a pair ofcoefficients match if both are within a given threshold ofeach other.
Images match if their first three coefficients(average color) all match, and if the number of additionalmatching coefficients is above a certain count.
Identicalimages will of course receive a perfect score using thismeasure.To tune the parameters of the matcher, we found sam-ple photos to which CNN or the BBC had added bordersand the corresponding borderless originals from AP andReuters.
We then examined how these changes affectedthe coefficients, and we set the matching thresholds torespond positively even with these changes.
This led tofairly broad ranges, ?5 for the average color coefficientsand ?3 for the rest (out of 0-255 scale).
However, wealso insisted that at least 42 coefficients actually match.A visual scan of the sets returned by our iconic matchershows very few false positives.
The matcher frequentlyaccommodates borders or croppings that change up to10% of the pixels in the original image.
Its response tothese changes, however, is somewhat unpredictable.
Twocrops of similar appearance can have very different ef-fects upon the Haar parameters if one pushes many fea-tures into different quadrants of the image than the other.As Table 1 indicates, a significant percentage (10%) ofthose articles we have collected from the BBC and CNNshare an image with another article.
This phenomenonseems to stem from three main sources.
Across collec-tions, different authors may use the same image to illus-trate similar stories.
Authors may also use the same im-Table 1: Iconic MatchesIconic Matches Collection StatsCollection Total Docs BBC CNNBBC 16990 BBC 3793 223CNN 8397 CNN 1233A shared picture is a very good indication that two ar-ticles are in some way topically related.
Our iconicmatcher establishes those links.
On the left, the figuregives the total number of articles we collected in a threemonth (Oct-Dec, 2002) period from the BBC and CNNcollection, and on the right, how often the same imagewas used for multiple stories.
We have split these iconicmatches out into inter-agency and intra-agency totals.More than 10 % of the articles in our collection sharean image.age over time to provide temporal continuity as the un-derlying story changes.
Finally, the same image may beused to indicate a broad theme, while the articles them-selves discuss quite different topics.
A series of articleson an oil spill of the coast of Spain, for example, movedfrom simply reporting the incident, to investigating thecaptain, to discussing the environmental impact, alwaysusing an illustration of an oil drenched cormorant.3.2 Text ClusteringOur second tool for grouping articles comes from auto-mated clustering of the AP and Reuters caption texts.
Weimplemented two different clustering algorithms.
Eachgenerates a probability distribution over K topics for eachdocument.
Documents are then assigned to the maximumlikelihood cluster.We fit two models to the data.
The first was a simpleunigram mixture model.
This model posits that captionwords are generated by sampling from a multinomial dis-tribution over the corpus vocabulary V .
Topics are sepa-rate distributions over V .
For a given number of topics,we have |K| ?
|V | parameters, where K is the set of top-ics.
The probability of a caption in this model is?k?K(p(k)?w?Vp(w|k))(1)We can fit this model using EM, with the assignmentsof captions to topics as our hidden variables.
In our im-plementation, we initialize p(k) and p(w|k) with randomdistributions.Effectively, we treat each caption as an unordered setof keywords.
Although this is a simple language model,we expected it to fit the captions fairly well.
Given theirextremely short length, we believed captions to have a farhigher percentage of topically important (and thus dis-criminative) words than one would find in a more genericarticle collection.
In longer documents researchers haveinvestigated modelling documents as mixtures of topics(Blei et al, 2001), but we believed captions truly werenarrowly focused around a single topic.Still, our original vocabulary contained over 90,000terms.
To fit this model, we trimmed the tail end ofthe vocabulary.
We applied two heuristics.
removing allwords that happened less than 200 times.
This seems adrastic reduction, but as figure 1 illustrates, the tail of ourvocabulary is primarily proper names.
Moreover, we col-lect more than 1000 captions a day.
A single word, espe-cially a name, could therefore easily occur 200 times ina single day.
Since we are interested in topics of largertemporal extent than a day, this reduction seems at leastsomewhat justified.
We also removed all words of threecharacters or less.
During fitting, we normalized the doc-ument word count vectors to a constant length.Unfortunately, the model was still overwhelmed bycommon words, and the maximum likelihood configura-tion invariably driven to make every topic equally likelyand every topic distribution almost exactly the same.
Wetherefore were forced to additionally remove very com-mon words, namely the 2000 most frequent words fromthe web at large.1 This heuristic almost certainly removessome words strongly associated with specific topics (e.g.?ball?
with sports).
Still, the remaining middle frequencywords do a good job of separating captions into quali-tatively good topics, Our contention is that this middlefrequency is closely aligned with the true statistics of theentire corpus.Our second algorithm, which we call a ?two-level?mixture model, attempts to deal with very common words1Berkeley Digital Library ?Web Term Document Fre-quency?
http://elib.cs.berkeley.edu/docfreq/index.htmlin a more principled way.
The top level is a single multi-nomial distribution ?
shared by all captions.
The secondlevel has K topic distributions, equivalent to the sim-ple unigram model.
For each caption word, we sam-ple a Bernoulli random variable ?
to decide whether todraw from the top-level or second-level distribution.
Thismodel shunts common shared words into the top-level?junk?
distribution, leaving the topic distributions to re-flect truly distinctive words.
The probability of a docu-ment in this model is?k?K(?w?V(?p(w|k)p(k) + (1?
?)p(w|?
)))(2)We used EM once again to estimate these parameters.
Re-gardless of starting position or |K|, ?
consistently con-verges to just over .5 (.53 with std .004).3.3 Quantitative AnalysisWe fit each of these models 10 different times each forK = 10, 20, ...100 where K is the number of topics.
Tocompare the quality of fit between different values of K,we held out 10% of the captions and compared the neg-ative log likelihood of the held out data for each run.
Amodel which assigns the highest probability to the test setwill have the lowest log likelihood.
In figure 3, we plotthe average negative log likelihood for each of these runs.Across all K, EM converged in just a few iterations.3.4 Evaluation and InterfacesQuantitatively our models appear to fit well, but an anal-ysis of their usefulness for interacting with these collec-tions is more difficult.
Our collection has no canonicaltopical structure against which we can compare our re-sults.
However, we have built various tools to aid in thequalitative evaluation of our results.3.4.1 Temporal StructureThe clustering algorithms discussed in the previoussection take no account of the time at which an article orcaption was published.
News topics, however, certainlyhave temporal structure.
Some, like baseball, happen dur-ing a specific season.
Others, like an election day, areevents that are heavily discussed for a brief period of timeand then fade.
Our first interface illustrates these tempo-ral relationships.
We plot each topic over time.
Eachtimestep is approximately a day.2 Each element in theplot of a cluster represents the percentage of captions inthat cluster collected during each time period.
We haveconstructed a web interface that lays out all K topics ascolumns in a matrix, moving through time from top to2A timestep actually represents 1000 chronologically con-tiguous captions, but we collect an average of 1000 captions aday.Figure 3: We fit two clustering models.
The first was a simple unigram mixture model with K topics.
The second?Two Level?
model extended the first with a global distribution such that words could choose to come either from thetopic distribution associated with that document or from the global one.
This figure plots the average and minimumnegative log likelihood attained for values of K = 10, 20, 30, ..., 100.
The simple unigram model uses a vocabularyfiltered of common words, 2000 terms smaller than that used by the ?two level?
model.
This accounts for the higherlog likelihood numbers on the right.
When fit with the full vocabulary, the simple unigram model invariably creates Kidentical topics, losing all topic structure to the noise generated from common words.bottom.
One may click on any individual element to bringup a list of captions specific to that period and cluster, aswell as the word distribution that characterizes this topic.
(Figure 4) The example in this figure utilizes the simpleunigram model.
One striking aspect of this view is thattopics clearly do appear to have time signatures.
Someare periodic, others ramp up, others are extremely peaked.We are contemplating methods to integrate these tempo-ral features into our clustering methods.3.4.2 2D EmbeddingOur second interface attempts to lay out topics on theplane in such a way that distances between them are se-mantically meaningful.
First, we use a symmetrized KLdivergence to define a distance metric between topic dis-tribution pairs.
We define a symmetric KL divergencebetween two topics distributions ti and tj asKLsymmetric = 12(KL(ti||tj) +KL(tj ||ti)) (3)KL(ti||tj) =?w?V((p(w|ti)log( p(w|ti)p(w|tj) )) (4)where V is the corpus vocabulary.We then use Principal Coordinate Analysis to projectinto the plane in a way that in various senses ?best?
pre-serves distances.
We finally calculate the likelihood forall captions in a given cluster and illustrate the topic withthe image associated with the maximum likelihood cap-tion.In our interface, (Figure 5) one may click on any to-ken to bring up a list of all images and articles associ-ated with this topic.
In this example, we have actuallyused the topic structure generated with captions to orga-nize the BBC and CNN combined dataset.
The clusters inthis figure were built using the simple unigram clusteringmodel.One aspect of Principal Coordinate Analysis that is un-desirable in our case is that it tends to emphasize accu-rately representing large distances at the expense of smallones.
In topic space, distances between topics only seemto have semantic meaning up to some threshold.
Beyondthis threshold they are simply unrelated.
We are activelyworking on implementing a modified version of PrincipalCoordinate Analysis that will lend more weight to smallerdistances for this interface.4 A Celebrity Face DetectorOur second area of investigation with this dataset is au-tomated methods for establishing correspondences be-tween textual words or phrases and image regions.
Tothis end, we are investigating the creation of an automatedcelebrity classifier from the AP and Reuters photographs.Brown et al (P. Brown and Mercer, 1993) haveeffectively used co-occurrence statistics to build bilin-gual translation models.
Duygulu, Barnard and Forsyth(Duygulu et al, 2002) have effectively used these ma-chine translation algorithms to establish correspondencesbetween keywords and types of image regions in theCorel image collection.Unlike Corel, our photos are primarily of people, andso it seemed natural to focus first on proper names as op-posed to more general noun classes.
Proper nouns are rel-atively easy to extract from our captions.
Caption writersare quite consistent in how they record individual?s namesand titles.
Simply selecting strings of capitalized wordswith just a few heuristics to accommodate the beginningsof sentences, middle initials, etc.
performs well.
Com-mon names like the President?s will be written in multipleFigure 4: In this figure we see a clustering of 50 topics laid out temporally.
The captions are clustered without respectto time using EM and the simple unigram language model.
In this example we have used 50 clusters.
Each columnrepresents a single topic, and each row is approximately a one day time slice starting July 19, 2002 at top and endingon Dec 8, 2002 at the bottom.
The brightness of the entry reflects the percentage of all captions in this topic thatoccurred during this time slice.
To illustrate, we have labeled certain portions of the figure with the realworld topicsor events with which certain topics/time periods seem most associated.
Topics appear to have time signatures.
Some,like football or championship baseball, are periodic.
Others, like election day, slowly build to a peak and then rapidlyfade.
Other, unexpected events, such as the arrest of the D.C. area snipers and Moscow hostage situation ramp upsuddenly and then slowly fade over time.
We are investigating adding this temporal information into our clusteringmodels.ways, but even in these cases a single form is overwhelm-ingly predominant.
As for the images, face detectors area relatively mature piece of vision technology and we canreliably extract a large set of face cutouts.We could not directly apply the co-occurrence meth-ods used in previous work.
First, our captions are full ofproper nouns (institutions, locations, and other people)that have no visual counterpart in the image.
Duyguluet al faced a similar problem for certain keywords inthe Corel dataset, but to a far smaller degree.
They couldtreat it as a noise problem.
Here, it overwhelms the actualsignal.The image side compounds our problem.
Duygulu etal.
were able to cluster image regions into equivalenceclasses based on a set of extracted features.
We have nosimilarity metric for faces.
Typically, one induces a met-ric by fitting a parametric model to the data and exploringdifferences in parameter space.
Parametric models per-form poorly on faces, however, due to the variability offacial expressions.If one were to manually label our collection withnames and locations of the individuals portrayed in eachphoto, we believe we could generate hundreds or thou-sands of unique face images for many individuals.
Giventhis supervised dataset, it might be possible to generatea non-parametric model of faces, fitting a set of modelsfor each expression.
Manually annotating half a millionphotographs is impractical.
Instead we have leveragedthe special linguistic structure of our captions to create a?photographic entity?
detector.
In other words, a propername finder optimized to return only those names that ac-tually occur in the photograph.4.1 Who Is In The Picture?We are confronted with the problem of trying to identifythose proper names that actually identify people in thephoto.
A small amount of linguistic analysis has beenextremely helpful.
We tried various proper name detec-tors, but it proved difficult to return only people, let alnepeople who actually appear in the images.
Once again,however, we can exploit journalistic conventions.
Withfew exceptions, the first sentence of a caption describesthe activity and participants in the picture itself.
In 51%of our captions, the beginning of the first sentence followsthe pattern [Noun Phrase][Present Tense Action Verb],or less commonly [Noun Phrase][Present Tense PassiveFigure 5: In this interface we have defined a two dimensional distance metric between topics and laid them out on theplane, illustrated with representative photos from the collection.
The right hand side illustrates a closeup of the upperright corner of this space, an area dominated by sports related topics.
In our interface one may click on any tokento bring up a list of additional images and articles associated with this topic.
In this example, the topics have beengenerated using captions from the AP and Reuters photos, but we are actually using this topic structure to navigatearticles from CNN and the BBC.
We contend that clustering with these captions generates topic distributions that focuson words highly relevant to the topic.
Topics are defined as probability distributions across the corpus vocabulary.Our 2D embedding is derived by calculating the symmetrized KL divergence between each pair of topics and usingprinciple coordinate analysis to project into two dimensions in a manner that ?
best?
preserves the distances in theoriginal high dimensional space.
(Ripley, 1996)Verb].
(see figure 2)The detector we have built identifies potential propernames as strings of two or more capitalized words fol-lowed by a potential present tense verb.
Words are clas-sified as possible verbs by first applying a list of morpho-logical rules to possible present tense singular forms, andthen comparing these to a database of known verbs.
Boththe morphological rules and the list of verbs are fromWordNet.
(Wor, 2003)When there is more than one person in the photo, theauthor often inserts a term directly following the propername to help disambiguate the correspondence.
This willeither be a position such as left or right, or an identify-ing characteristic.
This second form is most frequentlyused with sports figures and gives a jersey number.
Ourproper name finder returns the name, the disambiguatorif it exists, and the verb.Our classifier either accepts a caption and returns a pro-posed name or rejects the caption.
We tested it on thesame sample we used for clustering (146,870 captions).The name finder accepts 47% of these captions.
We man-ually examined 400 rejected captions.
Of these, 50%were true misses, where the caption contained a namethat matched a face in the image.
Another 35% were im-ages of people but the caption either contained no propernames or only proper names of people that did not appearin the image.
The final 15% were images that containedno people.We also examined 1000 accepted captions.
In 85% ofthis sample, the classifier accurately extracted the nameof at least one person in the image.
Of these errors,the vast majority still followed our pattern of [NounPhrase][Present Tense Verb], and the subject of the nounphrase actually did appear in the picture.
Our rules sim-ply failed to accurately parse the noun phrase.
Over halfof these mistakes, for instance, were due to the phrase?
[Proper Name] of the United States [Verb],?
and ourclassifier returned ?United States?
instead of the cor-rect individual.
More robust proper name finders shouldlargely eliminate these sources of error.
The more impor-tant point is that captions are so carefully structured thatvery simple rules accurately model most of the collec-tion.
We could conceivably even use our face classifier tolearn some of these structural rules.
If we could use theimages to posit names, and even positions, we might beable to use this as a handle for learning certain types ofcaption structure.
If a photo were to have two strong faceresponses, for instance, we might learn to look for those?
left?,?right?
indicators in the text.
We would also liketo investigate the possible clustering of verbs from imagedata.4.2 Iterative ImprovementsWith aggressive pruning of questionable outputs from thename and face finders, we believe we can generate aneffectively supervised dataset of faces for thousands ofFigure 6: Not every person named in a caption appears in a picture.
However, quite simple syntactic analysis ofcaptions yields some names of persons whose faces are very likely to appear in the picture.
By looking for items wherethe picture has a single, large face detector response ?
so there is only one face present ?
and analysis of the captionproduces a single name, we produce a directory of news personalities that is quite accurate.
The top row shows someentries from this directory: there are relatively few instances of each face, because our tests are quite restrictive, butthe faces corresponding to each name are correct and are seen from a variety of angles, meaning we may be ableto build a non-parametric face model for some individuals by a completely automatic analysis.
The next three rowsshow some possible failure modes of our approach: First, our analysis of the caption could yield more than one name.Second, there may be more than one large face in the image, with only the wrong face producing a face detectorresponse.
Third, the syntax may occasionally follow a syntactic pattern our algorithm does not handle.
We are ableto extract proper names from 68,496 of 146,870 captions, with an estimated 85% of these actually naming a personin the image.
Restricting ourselves solely to large face responses, we are able to produce a gazetteer of 452 distinctnames (621 images total), containing only 60 incorrectly filed images.individuals, many of whom will have hundreds or eventhousands of distinct face images.
From these we hope tobuild a non- parametric model of faces.
The next interest-ing task will be to investigate whether we can then returnto the textual side, using our face models to learn moreabout linguistic structures of the captions.
Bootstrappingby alternating between the two sides of a mixed datasetseems a very powerful model.5 ConclusionNews photo captions are an interesting dataset both fortheir unique textual properties, and for the opportunitiesthey provide to exploit relationships between the text andimage contents.
We have used these captions to illumi-nate underlying topical structure in the collection.
Thisresearch indicates that captions act much like very shortsummaries, emphasizing words that are strongly associ-ated with underlying themes in the news.
We are in-vestigating how well this topical structure translates tomore general collections of news articles.
We have alsoshown that images can provide links between articles thatare missed by textual analysis alone.
Separately, we areinvestigating the possibility of building non-parametricmodels of celebrity faces.
This line of research indicatesthat by combining a face detector with an analysis of thelinguistic conventions of the text, captions can be used asan almost supervised dataset of people in the news.ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2001.
Latent Dirichlet Allocation.
In Neural Informa-tion Processing Systems 14.P.
Duygulu, K. Barnard, J.F.G.
De Freitas, and D.A.Forsyth.
2002.
Object recognition as machine transla-tion: Learning a lexicon for a fixed image vocabulary.Charles E. Jacobs, Adam Finkelstein, and David H.Salesin.
1995.
Fast multiresolution image query-ing.
Computer Graphics, 29(Annual ConferenceSeries):277?286.Christopher D. Manning and Hinrich Schutze.
1999.Foundations of Statistical Natural Language Process-ing.
MIT Press.V.J.
Della Pietra P. Brown, S.A. Della Pietra and R.L.Mercer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Computational Lin-guistics, 32(2):263?311.B.
D. Ripley.
1996.
Pattern Recognition and Neural Net-works.
Cambridge University Press, Cambridge.2003.
Wordnet, a lexical database for the English lan-guage.
