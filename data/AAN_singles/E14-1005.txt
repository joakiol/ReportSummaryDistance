Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 39?48,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsA Joint Model for Quotation Attribution and Coreference ResolutionMariana S. C.
Almeida?
?Miguel B.
Almeida?
?Andr?e F. T.
Martins??
?Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal?Instituto de Telecomunicac?
?oes, Instituto Superior T?ecnico, 1049-001 Lisboa, Portugal{mla,mba,atm}@priberam.ptAbstractWe address the problem of automaticallyattributing quotations to speakers, whichhas great relevance in text mining and me-dia monitoring applications.
While cur-rent systems report high accuracies forthis task, they either work at mention-level (getting credit for detecting uninfor-mative mentions such as pronouns), or as-sume the coreferent mentions have beendetected beforehand; the inaccuracies inthis preprocessing step may lead to errorpropagation.
In this paper, we introduce ajoint model for entity-level quotation attri-bution and coreference resolution, exploit-ing correlations between the two tasks.
Wedesign an evaluation metric for attribu-tion that captures all speakers?
mentions.We present results showing that both tasksbenefit from being treated jointly.1 IntroductionQuotations are a crucial part of news stories, giv-ing the perspectives of the participants in the nar-rated event, and making the news sound objective.The ability of extracting and organizing these quo-tations is highly relevant for text mining applica-tions, as it may aid journalists in fact-checking,help users browse news threads, and reduce humanintervention in media monitoring.
This involvesassigning the correct speaker to each quote?aproblem called quotation attribution (?2).There is significant literature devoted to thistask, both for narrative genres (Mamede andChaleira, 2004; Elson and McKeown, 2010) andnewswire domains (Pouliquen et al., 2007; Sar-mento et al., 2009; Schneider et al., 2010).
Whilethe earliest works focused on devising lexical andsyntactic rules and hand-crafting grammars, therehas been a recent shift toward machine learningapproaches (Fernandes et al., 2011; O?Keefe et al.,2012; Pareti et al., 2013), with latest works re-porting high accuracies for speaker identificationin newswire (in the range 80?95% for direct andmixed quotes, according to O?Keefe et al.
(2012)).Despite these encouraging results, quotation min-ing systems are not yet fully satisfactory, evenwhen only direct quotes are considered.
Part ofthe problem, as we next describe, has to do withinaccuracies in coreference resolution (?3).The ?easiest?
instances of quotation attributionproblems arise when the speaker and the quote aresemantically connected, e.g., through a reportedspeech verb like said.
However, in newswire text,the subject of this verb is commonly a pronoun oranother uninformative anaphoric mention.
Whilethe speaker thus determined may well be correct?being in most cases consistent with human annota-tion choices (Pareti, 2012)?from a practical per-spective, it will be of little use without a corefer-ence system that correctly resolves the anaphora.Since the current state of the art in coreference res-olution is far from perfect, errors at this stage tendto propagate to the quote attribution system.Consider the following examples for illustration(taken from the WSJ-1057 and WSJ-0089 docu-ments in the Penn Treebank), where we have an-notated with subscripts some of the mentions:(a) Rivals carp at ?the principle of [Pilson]M1,?as [NBC?s Arthur Watson]M2once put it ??
[he]M3?s always expounding that rights aretoo high, then [he]M4?s going crazy.?
But [the49-year-old Mr. Pilson]M5is hardly a man toignore the numbers.
(b) [English novelist Dorothy L. Sayers]M1de-scribed [ringing]M2as a ?passion that finds itssatisfaction in [mathematical completeness]M3and [mechanical perfection]M4.?
[Ringers]M5,[she]M6added, are ?filled with the solemn intox-ication that comes of intricate ritual faultlesslyperformed.
?In example (a), the pronoun coreference systemused by O?Keefe et al.
(2012) erroneously clus-ters together mentions M2, M3and M4(insteadof the correct clustering {M1,M3,M4}).
Since itis unlikely that the speaker is co-referent to a third-39person pronoun he inside the quote, a pipeline sys-tem would likely attribute (incorrectly) this quoteto Pilson.
In example (b), there are two quoteswith the same speaker entity (as indicated by thecue she added).
This gives evidence that M1andM6should be coreferent.
A pipeline approachwould not be able to exploit these correlations.We argue that this type of mistakes, amongothers, can be prevented by a system that per-forms quote attribution and coreference resolutionjointly (?4).
Our joint model is inspired by re-cent work in coreference resolution that indepen-dently ranks the possible mention?s antecedents,forming a latent coreference tree structure (Denisand Baldridge, 2008; Fernandes et al., 2012; Dur-rett et al., 2013; Durrett and Klein, 2013).
We con-sider a generalization of these structures which wecall a quotation-coreference tree.
To effectivelycouple the two tasks, we need to go beyond simplearc-factored models and consider paths in the tree.We formulate the resulting problem as a logic pro-gram, which we tackle using a dual decompositionstrategy (?5).
We provide an empirical compari-son between our method and baselines for each ofthe tasks and a pipeline system, defining suitablemetrics for entity-level quotation attribution (?6).2 Quotation AttributionThe task of quotation attribution can be formallydefined as follows.
Given a document containinga sequence of quotations, ?q1, .
.
.
, qL?, and a setof candidate speakers, {s1, .
.
.
, sM}, the goal is toa assign a speaker to every quote.Previous work has handled direct and mixedquotations (Sarmento et al., 2009; O?Keefe et al.,2012), easily extractable with regular expressionsfor detecting quotation marks, as well as indirectquotations (Pareti et al., 2013), which are more in-volved and require syntactic or semantic patterns.In this work, we resort to direct and mixed quo-tations.
Pareti (2012) defines quotation attribu-tions in terms of their content span (the quotationtext itself), their cue (a lexical anchor of the attri-bution relation, such as a reported speech verb),and the source span (the author of the quote).The same reference introduced the PARC dataset,which we use in our experiments (?6) and whichis based on the annotation of a database of attribu-tion relations from the Penn Discourse Treebank(Prasad et al., 2008).
Several machine learningalgorithms have been applied to this task, eitherframing the problem as classification (an indepen-dent decision for each quote), or sequence label-ing (using greedy methods or linear-chain condi-tional random fields); see O?Keefe et al.
(2012)for a comparison among these different methods.In this paper, we distinguish between mention-level quotation attribution, in which the candi-date speakers are individual mentions, and entity-level quotation attribution, in which they are en-tity clusters comprised of one or more mentions.With this distinction, we attempt to clarify howprior work has addressed this task, and design suit-able baselines and evaluation metrics.
For exam-ple, O?Keefe et al.
(2012) applies a coreferenceresolver before quotation attribution, whereas deLa Clergerie et al.
(2011) does it afterwards, as apost-processing stage.
An important issue whenevaluating quotation attribution systems is to pre-vent them from getting credit for detecting unin-formative speakers such as pronouns; we will getback to this topic in ?6.2.3 Coreference ResolutionIn coreference resolution, we are given a set ofmentions M := {m1, .
.
.
,mK}, and the goalis to cluster them into discourse entities, E :={e1, .
.
.
, eJ}, where each ej?
M and ej6= ?.We follow Haghighi and Klein (2007) and distin-guish between proper, nominal, and pronominalmentions.
Each requires different types of infor-mation to be resolved.
Thus, the task involves de-termining anaphoricity, resolving pronouns, andidentifying semantic compatibility among men-tions.
To resolve these references, one typicallyexploits contextual and grammatical clues, as wellas semantic information and world knowledge,to understand whether mentions refer to people,places, organizations, and so on.
The importanceof coreference resolution has led to it being thesubject of recent CoNLL shared tasks (Pradhan etal., 2011; Pradhan et al., 2012).There has been a variety of approaches forthis problem.
Early work used local discrimina-tive classifiers, making independent decisions foreach mention or pair of mentions (Soon et al.,2001; Ng and Cardie, 2002).
Lee et al.
(2011)proposed a competitive non-learned sieve-basedmethod, which constructs clusters by aglomerat-ing mentions in a greedy manner.
Entity-centricmodels define scores for the entire entity clusters(Culotta et al., 2007; Haghighi and Klein, 2010;40Rahman and Ng, 2011) and seek the set of enti-ties that optimize the sum of scores; this can alsobe promoted in a decentralized manner (Durrett etal., 2013).
Pairwise models (Bengtson and Roth,2008; Finkel et al., 2008; Versley et al., 2008), onthe other hand, define scores for each pair of men-tions to be coreferent, and define the clusters asthe transitive closure of these pairwise relations.A disadvantage of these two methods is that theylead to intractable decoding problems, so approx-imate methods must be used.
For comprehensiveoverviews, see Stoyanov et al.
(2009), Ng (2010),Pradhan et al.
(2011) and Pradhan et al.
(2012).Our joint approach (to be fully described in?4) draws inspiration from recent work that shiftsfrom entity clusters to coreference trees (Fernan-des et al., 2012; Durrett and Klein, 2013).
Thesemodels define scores for each mention to link toits antecedent or to an artifical root symbol $ (inwhich case it is not anaphoric).
The computationof the best tree can be done exactly with spanningtree algorithms, or by independently choosing thebest antecedent (or the root) for each mention, ifonly left-to-right arcs are allowed.
The same ideaunderlies the antecedent ranking approach of De-nis and Baldridge (2008).
Once the coreferencetree is computed, the set of entity clusters E is ob-tained by associating each entity set to a branch ofthe tree coming out from the root.
This is illus-trated in Figure 1 (left).4 Joint Quotations and CoreferencesIn this work, we propose that quotation attribu-tion and coreference resolution are solved jointlyby treating both mentions and quotations as nodesin a generalized structure called a quotation-coreference tree (Figure 1, right).
The joint sys-tem?s decoding process consists in creating sucha tree, from which a clustering of the nodes canbe immediatelly obtained.
The clustering is inter-preted as follows:?
All mention nodes in the cluster are coreferent,thus they describe one single entity (just like ina standard coreference tree).?
Quotation nodes that appear together with thosementions in a cluster will be assigned that entityas the speaker.For example, in Figure 1 (right), the en-tity Dorothy L. Sayers (formed by mentions{M1,M6}) is assigned as the speaker of quota-tions Q1and Q2.
We forbid arcs between quotesand from a quote to a mention, effectively con-straining the quotes to be leaves in the tree, withmentions as parents.1We force a tree with onlyleft-to-right arcs, by choosing a total ordering ofthe nodes that places all the quotations in the right-most positions (which implies that any arc con-necting a mention to a quotation will point to theright).
The quotation-coreference tree is obtainedas the best spanning tree that maximizes a scorefunction, to be described next.4.1 Basic ModelOur basic model is a feature-based linear modelwhich assigns a score to each candidate arc linkingtwo mentions (mention-mention arcs), or linking amention to a quote (mention-quotation arcs).
Ourbasic system is called QUOTEBEFORECOREF forreasons we will detail in section 4.2.4.1.1 Coreference featuresFor the mention-mention arcs, we use the samecoreference features as the SURFACE model of theBerkeley Coreference Resolution System (Durrettand Klein, 2013), plus features for gender andnumber obtained through the dataset of Bergsmaand Lin (2006).
This is a very simple lexical-driven model which achieves state-of-the-art re-sults.
The features are shown in Table 1.4.1.2 Quotation featuresFor the quote attribution features, we use featuresinspired by O?Keefe et al.
(2012), shown in Ta-ble 2.
The same set of features works for speakersthat are individual mentions (in the model just de-scribed), and for speakers that are clusters of men-tions (used in ?6 for the baseline QUOTEAFTER-COREF).
These features include various distancesbetween the mention and the quote, the indicationof the speaker being inside the quote span, and var-ious contextual features.4.2 Final ModelWhile the basic model just described puts quo-tations and mentions together, it is not more ex-pressive than having separate models for the twotasks.
In fact, if we just have scores for individualarcs, the two problems are decoupled: the optimal1This is implemented by defining ??
scores for all theoutgoing arcs in a quotation node, as well as incoming arcsoriginating from the root.41Figure 1: Left: A typical coreference tree for the text snippet in ?1, example (b), with mentions M1andM6clustered together and M2and M3left as singletons.
Right: A quotation-coreference tree for thesame example.
Mention nodes are depicted as green circles, and quotation nodes in shaded blue.
Thedashed rectangle represents a branch of the tree, containing the entity cluster associated with the speakerDorothy L. Sayers, as well as the quotes she authored.Features on the child mention[ANAPHORIC (T/F)] + [CHILD HEAD WORD][ANAPHORIC (T/F)] + [CHILD FIRST WORD][ANAPHORIC (T/F)] + [CHILD LAST WORD][ANAPHORIC (T/F)] + [CHILD PRECEDING WORD][ANAPHORIC (T/F)] + [CHILD FOLLOWING WORD][ANAPHORIC (T/F)] + [CHILD LENGTH]Features on the parent mention[PARENT HEAD WORD][PARENT FIRST WORD][PARENT LAST WORD][PARENT PRECEDING WORD][PARENT FOLLOWING WORD][PARENT LENGTH][PARENT GENDER][PARENT NUMBER]Features on the pair[EXACT STRING MATCH (T/F)][HEAD MATCH (T/F)][SENTENCE DISTANCE, CAPPED AT 10][MENTION DISTANCE, CAPPED AT 10]Table 1: Coreference features, associated to eachcandidate mention-mention arc in the tree.
As inDurrett and Klein (2013), we also include con-junctions of each feature with the child and parentmention types (proper, nominal, or, if pronominal,the pronoun word).quotation-coreference tree can be obtained by firstassigning the highest scored mention to each quo-tation, and then building a standard coreferencetree involving only the mention nodes.
This cor-responds to the QUOTEBEFORECOREF baseline,to be used in ?6.To go beyond separate models, we introducea final JOINT model, which includes additionalscores that depend not just on arcs, but also onpaths in the tree.
Concretely, we select certainFeatures on the quote-speaker pair[WORD DISTANCE][SENTENCE DISTANCE][# IN-BETWEEN QUOTES][# IN-BETWEEN SPEAKERS][SPEAKER IN QUOTE, 1ST PERS.
SG.
PRONOUN (T/F)][SPEAKER IN QUOTE, 1ST PERS.
PL.
PRONOUN (T/F)][SPEAKER IN QUOTE, OTHER (T/F)]Features on the speaker[PREVIOUS WORD IS QUOTE (T/F)][PREVIOUS WORD IS SAME QUOTE (T/F)][PREVIOUS WORD IS ANOTHER QUOTE (T/F)][PREVIOUS WORD IS SPEAKER (T/F)][PREVIOUS WORD IS PUNCTUATION (T/F)][PREVIOUS WORD IS REPORTED SPEECH VERB (T/F)][PREVIOUS WORD IS VERB (T/F)][NEXT WORD IS QUOTE (T/F)][NEXT WORD IS SAME QUOTE (T/F)][NEXT WORD IS ANOTHER QUOTE (T/F)][NEXT WORD IS SPEAKER (T/F)][NEXT WORD IS PUNCTUATION (T/F)][NEXT WORD IS REPORTED SPEACH VERB (T/F)][NEXT WORD IS VERB (T/F)]Table 2: Quotation attribution features, associ-ated to each quote-speaker candidate.
Thesefeatures are used in the QUOTEONLY, QUOTE-BEFORECOREF, and JOINT systems (where thespeaker is a mention) and in the QUOTEAFTER-COREF system (where the speaker is an entity).pairs of nodes and introduce scores for the eventthat both nodes are in the same branch of the tree.Rather than doing this for all pairs?which es-sentially would revert to the computationally de-manding pairwise coreference models discussedin ?3?we focus on a small set of pairs that aremostly related with the interaction between thetwo tasks we address jointly.
Namely, we considerthe mention-quotation pairs such that the mention42Mention-inside-quote features[MENTION IS 1ST PERSON, SING.
PRONOUN (T/F)][MENTION IS 1ST PERSON, PLUR.
PRONOUN (T/F)][OTHER MENTION (T/F)]Consecutive quote features[DISTANCE IN NUMBER OF WORDS][DISTANCE IN NUMBER OF SENTENCES]Table 3: Features used in the JOINT system formention-quote pairs (only for mentions insidequotes) and for quote pairs (only for consecutivequotes).
These features are associated to pairs inthe same branch of the quotation-coreference tree.span is within the quotation span (mention-inside-quotation pairs), and pairs of quotations that ap-pear consecutively in the document (consecutive-quotation pairs).
The idea is that, if consecutivequotations appear on the same branch of the tree,they will have the same speaker (the entity classassociated with that branch), even though theyare not necessarily siblings.
These two pairs arealigned with the motivating examples (a) and (b)shown in ?1.4.2.1 Mention-inside-quotation featuresThe top rows of Table 3 show the features we de-fined for mentions inside quotes.
The features in-dicate whether the mention is first-person singularpronominal (I, me, my, myself ), which providesstrong evidence that it co-refers with the quotationauthor, whether it is first-person plural pronominal(we, us, our, ourselves), which provides a weakerevidence (but sometimes works for colective enti-ties that are organizations), and whether none ofthe above happens?in which case, the speaker isunlikely to be co-referent with the mention.4.2.2 Consecutive quotation featuresWe show our consecutive quote features in the bot-tom rows of Table 3.
We use only distance fea-tures, measuring both distance in sentences andin words, with binning.
These simple features areenough to capture the trend of consecutive quotesthat are close apart to have the same speaker.5 Joint Decoding and TrainingWhile decoding in the basic model is easy?as pointed out above, it can even be doneby running a mention-level quotation attribu-tor and the coreference resolver independently(QUOTEBEFORECOREF)?exact decoding withthe JOINT model is in general intractable, sincethis model breaks the independence assumptionbetween the arcs.
However, given the relativelysmall amount of node pairs that have scores (onlymentions inside quotations and consecutive quota-tions), we expect this ?perturbation?
to be smallenough not to affect the quality of an approxi-mate decoder.
The situation resembles other prob-lems in NLP, such as non-projective dependencyparsing, which becomes intractable if higher orderinteractions between the arcs are considered, butcan still be well approximated.
Inspired by workin parsing (Martins et al., 2009) using linear re-laxations with multi-commodity flow models, wepropose a similar strategy by defining auxiliaryvariables and coupling them in a logic program.5.1 Logic FormulationWe next derive the logic program for joint decod-ing of coreferences and quotations.
The input is aset of nodes (including an artificial node), a set ofcandidate arcs with scores, and a set of node pairswith scores.
To make the exposition lighter, weindex nodes by integers (starting by the root node0) and we do not distinguish between mention andquotation nodes.
Only arcs from left to right areallowed.
The variables in our logic program are:?
Arc variables ai?j, which take the value 1 ifthere is an arc from i to j, and 0 otherwise.?
Pair variables pi,j, which indicate that nodes iand j are in the same branch of the tree.?
Path variables pij?
?k, indicating if there is apath from j to k.?
Common ancestor variables ?i?
?j,k, indicatingthat node i is a common ancestor of nodes j andk in the tree.Consistency among these variables is ensured bythe following set of constraints:?
Each node except the root has exactly one par-ent:j?1?i=0ai?j= 1, ?j 6= 0 (1)?
There is a path from each node to itself:pii?
?i= 1, ?i (2)?
There is a path from i to k iff there is some jsuch that i is connected to j and there is path43from j to k:pii??k=?i<j?k(ai?j?
pij?
?k), ?i, k (3)?
Node i is a common ancestor of k and ` iff thereis a path from i to k and from i to `:?i?
?k,`= pii??k?
pii?
?`, ?i, k, ` (4)?
Nodes k and ` are in the same branch if theyhave a common ancestor which is not the root:pk,`=?i 6=0?i?
?k,`, ?k, l. (5)The objective to optimize is linear in the arc andpair variables (hence the problem can be repre-sented as an integer linear program by turning thelogical constraints into linear inequalities).5.2 Dual DecompositionTo decode, we employ the alternating direc-tions dual decomposition algorithm (AD3), whichsolves a relaxation of the ILP above.
AD3hasbeen used successfully in various NLP tasks, suchas dependency parsing (Martins et al., 2011; Mar-tins et al., 2013), semantic role labeling (Das et al.,2012), and compressive summarization (Almeidaand Martins, 2013).
At test time, if the solution isnot integer, we apply a simple rounding procedureto obtain an actual tree: for each node j, obtainthe antecedent (or root) i with the highest ai?j,solving ties arbitrarily.5.3 Learning the ModelWe train the joint model with the max-loss variantof the MIRA algorithm (Crammer et al., 2006),adapted to latent variables (we simply obtain thebest tree consistent with the gold clustering at eachstep of MIRA, before doing cost-augmented de-coding).
The resulting algorithm is very similarto the latent perceptron algorithm in Fernandeset al.
(2011), but it uses the aggressive stepsizeof MIRA.
We set the same costs for coreferencemistakes as Durrett and Klein (2013), and a unitcost for missing the correct speaker of a quota-tion.
For speeding up decoding, we first train a ba-sic pruner for the coreference system (using onlythe features described in ?4.1.1), limiting the num-ber of candidate antecedents to 10, and discardingscores whose difference with respect to the bestantecedent is below a threshold.
We also freezethe best coreference trees consistent with the goldclustering using the pruner model, to eliminate theneed of latent variables in the second stage.6 Experiments6.1 DatasetWe used the 597 documents of the Wall StreetJournal (WSJ) corpus that were disclosed for theCoNLL-2011 coreference shared task (Pradhanet al., 2011) as a dataset for coreference resolu-tion.
This dataset includes train, development andtest partitions, annotated with coreference infor-mation, as well as gold and automatically gener-ated syntactic and semantic information.The CoNLL-2011 corpus does not contain an-notations of quotation attribution.
For that rea-son, we used the WSJ quotation annotations in thePARC dataset (Pareti, 2012).
We used the sameversion of the corpus as O?Keefe et al.
(2012),but with different splits, to match the dataset parti-tions in the coreference resolution data.
This attri-bution corpus contains 279 documents of the 597CoNLL-2011 files, having a total of 1199 anno-tated quotes.
As in that work, we only consid-ered directed speech quotes and the direct part ofmixed quotes (quotes with both direct and undi-rected speech).6.2 Metrics for quotation attributionPrevious evaluations of quotation attribution sys-tems were designed at mention level, and are thusassessed by comparing the predicted speaker men-tion span with the gold one.
This metric assessesthe amount of speaker mentions that were cor-rectly identified.
For compatibility with previousassessments, we report this score, which we callExact Match (EM): this is the percentage of pre-dicted speakers with the same span as the gold one.However, for several quotations (about 30% inthe PARC corpus) this information is of littlevalue, since the gold mention is a pronoun, whichper se does not give any useful information aboutthe actual speaker entity.
Considering this fact,we propose two other metrics that capture infor-mation at the entity level, reflecting the amount ofinformation a system is able to extract about thespeakers:?
Representative Speaker Match (RSM): for eachannotated quote, we obtain the full gold coref-erence set of the gold annotated speaker, and44choose a representative speaker from that clus-ter.
We define this representative speaker asthe proper mention which is the closest to thequote (if available); if the cluster does not con-tain proper mentions, we use the closest nom-inal mention; if only pronominal mentions areavailable, we use the original annotated speaker.The final measure is the percentage of predictedspeakers that match the string of the correspond-ing representative speakers.?
Entity Cluster F1(ECF1).
Considering that asystem outputs a set of mentions coreferent tothe predicted speakers, we compute the F1scorebetween the predicted set and the gold corefer-ence cluster of the correct speaker.The entity level metrics are not only useful forassessing the quality of an quotation attributionsystem?they also reflect the quality of the un-derlying coreference system used to cluster the re-lated mentions.6.3 Attribution baselinesTo analyze the task of entity-level quotation attri-bution, we implemented three baseline systems.?
QUOTEONLY: A quotation attribution systemtrained on the representative speaker, instead ofthe gold speaker.
For fairness, this baseline wastrained with an extra feature indicating the typeof the mention (nominal, pronominal or proper).?
QUOTEAFTERCOREF: An attribution systemdirectly applied to the output of a predictedcoreference chain.
This baseline uses a coref-erence pre-processing, as applied in O?Keefe etal.
(2012).?
QUOTEBEFORECOREF: An attribution systemtrained on the gold speaker, and post-combinedwith the output of a coreference system.
Thissystem should be able to provide a set of infor-mative mentions about a quote, post-resolvingthe problem of the pronominal speakers.
Thiskind of post-coreference approach was used byde La Clergerie et al.
(2011).6.4 Coreference ResolutionWe use the coreference results of our basicQUOTEBEFORECOREF system as a baseline forcoreference resolution.
Since this system effec-tively solves the two problems separately, this canbe considered our implementation of the SURFACEsystem of Durrett and Klein (2013) .
As reportedin Table 4, the perfromance of our baseline iscomparable with the one of the SURFACE systemof Durrett and Klein (2013), which is denoted asSURFACE-DK-2013.2Table 4 also show the CoNLL metrics obtainedfor the proposed system of joint coreference reso-lution and quotation attribution.
Our joint systemoutperformed the baseline with statistical signifi-cance (with p < 0.05 and according to a bootstrapresampling test (Koehn, 2004)) for all metrics ex-pect for the CEAFE F1measure, whose value wasonly slighty improved.
These results confirm thatthe coreference resolution task benefits for beingtackled jointly with quotation attribution.6.5 Quotation attributionWe implemented and trained the three attributionsystems that were described in ?6.3 and the systemfor joint coreference and author attribution that isdetailed in ?4.
For each system, Table 5 shows themention-based and entity-based metrics that weredescribed in ?6.2.Training a quotation attribution system usingrepresentative speakers instead of the gold speak-ers (QUOTEONLY) leads to rather disappointingresults.
As expected, we conclude that assigningthe semantically related speaker is considerablyeasier than selecting another mention that is coref-erent with the correct speaker.Using (predicted) coreference information,both QUOTEAFTERCOREF and QUOTEBE-FORECOREF systems considerably increase ourentity-based metrics.
This was also expected,since the coreference chain allows these baselinesto output a set of related mentions.
We observedthat, using the coreference resolution clusters asthe attribution entity (QUOTEAFTERCOREF) in-fluences the results negatively when compared toa more basic system that runs coreference on topof attribution result of the QUOTEONLY system(QUOTEBEFORECOREF).
These results indicatethat the quotation attribution task performs betterby looking at the speaker mention that connectsmore strongly with the quotation, instead of tryingto match the whole cluster.Finally, the scores achieved by our JOINT2To make the systems comparable, we re-trained Durrettet al.
?s coreference system (version 0.9) on the WSJ portionof the Ontonotes datasets (the portion which has quote anno-tations from Pareti et al.
?s PARC dataset).
For this reason, thevalues in Table 4 differ from those reported in Durrett andKlein (2013), which where trained and tested in the entireOntonotes.45MUC F1BCUB F1CEAFE F1Avg.SURFACE-DK-2013 58.87 62.74 45.46 55.7SURFACE-OURS [QUOTEBEFORECOREF] 57.89 62.50 45.48 55.3JOINT 58.78 63.79 45.50 56.0Table 4: Coreference obtained with the CoNLL scorer (version 5) in the test partition of the WJS cor-pus, for the SURFACE system of Durrett and Klein (2013), our baseline implementation of the that sys-tem (SURFACE-OURS), and our JOINT approach.
All systems were trained in the WSJ portion of theOntonotes.EM RSM ECF1QUOTEONLY 49.1% 49.4% 41.2%QUOTEAFTERCOREF 76.7% 64.6% 70.0%QUOTEBEFORECOREF 88.7% 74.7% 73.7%JOINT 88.1% 76.6% 74.1%Table 5: Attribution results obtained, in the testset, for the three baseline systems and our jointsystem.model are slightly above the best baseline sys-tem QUOTEBEFORECOREF, yielding the best per-formance on the entity-level quotation attributiontask.
The differences, however, were not foundstatistically significant, probably due to the smallnumber of quotes (159) in the test set.The average decoding runtime of the JOINTmodel is 1.6 sec.
per document, against 0.2 sec.for the pipeline system.
This slowdown is ex-pectable given the fact that the pipeline systemonly needs to make independent decisions, whilethe joint version needs to solve a harder combina-torial problem.
Yet, this runtime is within the or-der of magnitude of the time necessary to prepro-cess the documents (which includes tagging andparsing the sentences).6.6 Error AnalysisTo understand the type of errors that are preventedwith the JOINT system, consider the following ex-ample (from document WSJ-2428):?
[Robert Dow, a partner and portfolio managerat Lord, Abbett & Co.]M1, which manages $4billion of high-yield bonds, says [he]M2doesn?t?think there is any fundamental economic ra-tionale (for the junk bond rout).
It was [herdinstinct]M3.?
[He]M4adds: ?The junk markethas witnessed some trouble and now some peo-ple think that if the equity market gets creamedthat means the economy will be terrible andthat?s bad for junk.
?The basic QUOTEBEFORECOREF systemwrongly clusters together M3and M4as corefer-ent, and wrongly assigns M3as the representativespeaker.
On the other hand, the JOINT systemcorrectly clusters M1, M2and M4as coreferent.This is due to the presence of the consecutivequote features which aid in understanding thatboth quotes have the same speaker, and themention-inside-quote features which prevent herdinstinct, which is inside a quote, from beingcoreferent with He, which is very likely the authorof the quotes due to the verb adds.7 ConclusionsWe presented a framework for joint coreferenceresolution and quotation attribution.
We repre-sented the problem as finding an optimal spanningtree in a graph including both quotation nodes andmention nodes.
To couple the two tasks, we intro-duce variables that look at paths in the tree, indi-cating if pairs of nodes are in the same branch, andwe formulate decoding as a logic program.
Eachbranch from the root can then be interpreted as acluster containing all coreferent mentions of an en-tity and all quotes from that entity.In addition, we designed an evaluation metricsuitable for entity-level quotation attribution thattakes into account informative speakers.
Experi-mental results show mutual improvements in thecoreference resolution and quotation attributiontasks.Future work will include extensions to tackle in-direct quotations, possibly exploring connectionsto semantic role labeling.AcknowledgementsWe thank all reviewers for their valuable com-ments, and Silvia Pareti and Tim O?Keefe for pro-viding us the PARC dataset and answering sev-eral questions.
This work was partially supportedby the EU/FEDER programme, QREN/POR Lis-boa (Portugal), under the Intelligo project (con-tract 2012/24803) and by a FCT grant PTDC/EEI-SII/2312/2012.46ReferencesM.
B. Almeida and A. F. T. Martins.
2013.
Fast and ro-bust compressive summarization with dual decom-position and multi-task learning.
In Proc.
of the An-nual Meeting of the Association for ComputationalLinguistics.Eric Bengtson and Dan Roth.
2008.
Understanding thevalue of features for coreference resolution.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 294?303.
As-sociation for Computational Linguistics.Shane Bergsma and Dekang Lin.
2006.
Bootstrappingpath-based pronoun resolution.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, pages 33?40.Association for Computational Linguistics.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online Passive-Aggressive Al-gorithms.
Journal of Machine Learning Research,7:551?585.Aron Culotta, Michael Wick, Robert Hall, and An-drew McCallum.
2007.
First-order probabilisticmodels for coreference resolution.
In Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics (HLT/NAACL), pages 81?88.D.
Das, A. F. T. Martins, and N. A. Smith.
2012.
AnExact Dual Decomposition Algorithm for ShallowSemantic Parsing with Constraints.
In Proc.
of FirstJoint Conference on Lexical and Computational Se-mantics (*SEM).
?Eric de La Clergerie, Beno?
?t Sagot, Rosa Stern, Pas-cal Denis, Ga?elle Recourc?e, and Victor Mignot.2011.
Extracting and visualizing quotations fromnews wires.
In Human Language Technology.
Chal-lenges for Computer Science and Linguistics, pages522?532.
Springer.Pascal Denis and Jason Baldridge.
2008.
Specializedmodels and ranking for coreference resolution.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 660?669.
Association for Computational Linguistics.Greg Durrett and Dan Klein.
2013.
Easy victories anduphill battles in coreference resolution.
In Proceed-ings of the 2013 Conference on Empirical Methodsin Natural Language Processing.Greg Durrett, David Hall, and Dan Klein.
2013.Decentralized entity-level modeling for coreferenceresolution.
In Proc.
of Annual Meeting of the Asso-ciation for Computational Linguistics.David K Elson and Kathleen McKeown.
2010.
Auto-matic attribution of quoted speech in literary narra-tive.
In AAAI.William Paulo Ducca Fernandes, Eduardo Motta, andRuy Luiz Milidi?u.
2011.
Quotation extraction forportuguese.
In Proceedings of the 8th BrazilianSymposium in Information and Human LanguageTechnology (STIL 2011), Cuiab?a, pages 204?208.Eraldo Rezende Fernandes, C?
?cero Nogueira dos San-tos, and Ruy Luiz Milidi?u.
2012.
Latent structureperceptron with feature induction for unrestrictedcoreference resolution.
In Joint Conference onEMNLP and CoNLL-Shared Task, pages 41?48.
As-sociation for Computational Linguistics.J.R.
Finkel, A. Kleeman, and C.D.
Manning.
2008.
Ef-ficient, feature-based, conditional random field pars-ing.
Proc.
of Annual Meeting on Association forComputational Linguistics, pages 959?967.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric bayesianmodel.
In Annual meeting-Association for Compu-tational Linguistics, volume 45, page 848.Aria Haghighi and Dan Klein.
2010.
Coreferenceresolution in a modular, entity-centered model.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, pages385?393.
Association for Computational Linguis-tics.P.
Koehn.
2004.
Statistical signicance tests for ma-chine translation evaluation.
In Proc.
of the AnnualMeeting of the Association for Computational Lin-guistics.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve coref-erence resolution system at the conll-2011 sharedtask.
In Proceedings of the Fifteenth Conference onComputational Natural Language Learning: SharedTask, pages 28?34.
Association for ComputationalLinguistics.Nuno Mamede and Pedro Chaleira.
2004.
Char-acter identification in children stories.
In Ad-vances in Natural Language Processing, pages 82?90.
Springer.A.
F. T. Martins, N. A. Smith, and E. P. Xing.
2009.Concise Integer Linear Programming Formulationsfor Dependency Parsing.
In Proc.
of Annual Meet-ing of the Association for Computational Linguis-tics.A.
F. T. Martins, N. A. Smith, P. M. Q. Aguiar, andM.
A. T. Figueiredo.
2011.
Dual Decompositionwith Many Overlapping Components.
In Proc.
ofEmpirical Methods for Natural Language Process-ing.A.
F. T. Martins, M. B. Almeida, and N. A. Smith.2013.
Turning on the turbo: Fast third-order non-projective turbo parsers.
In Proc.
of the AnnualMeeting of the Association for Computational Lin-guistics.47Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the 40th Annual Meeting on Asso-ciation for Computational Linguistics, pages 104?111.
Association for Computational Linguistics.V.
Ng.
2010.
Supervised noun phrase coreference re-search: The first fifteen years.
In Proc.
of the An-nual Meeting of the Association for ComputationalLinguistics.Tim O?Keefe, Silvia Pareti, James R Curran, Irena Ko-prinska, and Matthew Honnibal.
2012.
A sequencelabelling approach to quote attribution.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 790?799.
Association for Computational Linguistics.Silvia Pareti, Tim O?Keefe, Ioannis Konstas, James R.Curran, and Irena Koprinska.
2013.
Automaticallydetecting and attributing indirect quotations.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing.Silvia Pareti.
2012.
A database of attribution relations.In LREC, pages 3213?3217.Bruno Pouliquen, Ralf Steinberger, and Clive Best.2007.
Automatic detection of quotations in multi-lingual news.
In Proceedings of Recent Advances inNatural Language Processing, pages 487?492.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and NianwenXue.
2011.
Conll-2011 shared task: Modeling un-restricted coreference in ontonotes.
In Proceedingsof the Fifteenth Conference on Computational Nat-ural Language Learning: Shared Task, pages 1?27.Association for Computational Linguistics.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
Conll-2012 shared task: Modeling multilingual unre-stricted coreference in ontonotes.
In Proceedingsof the Joint Conference on EMNLP and CoNLL:Shared Task, pages 1?40.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind K Joshi, and Bon-nie L Webber.
2008.
The penn discourse treebank2.0.
In LREC.
Citeseer.Altaf Rahman and Vincent Ng.
2011.
Narrowing themodeling gap: A cluster-ranking approach to coref-erence resolution.
Journal of Artificial IntelligenceResearch, 40(1):469?521.Luis Sarmento, Sergio Nunes, and E Oliveira.
2009.Automatic extraction of quotes and topics from newsfeeds.
In 4th Doctoral Symposium on InformaticsEngineering.Nathan Schneider, Rebecca Hwa, Philip Gianfortoni,Dipanjan Das, Michael Heilman, Alan W Black,Frederick L Crabbe, and Noah A Smith.
2010.
Vi-sualizing topical quotations over time to understandnews discourse.
Technical report, Technical ReportCMU-LTI-01-103, CMU.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational linguistics, 27(4):521?544.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrasecoreference resolution: Making sense of the state-of-the-art.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 2-Volume2, pages 656?664.
Association for ComputationalLinguistics.Yannick Versley, Simone Paolo Ponzetto, MassimoPoesio, Vladimir Eidelman, Alan Jern, Jason Smith,Xiaofeng Yang, and Alessandro Moschitti.
2008.Bart: A modular toolkit for coreference resolution.In Proceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics on HumanLanguage Technologies: Demo Session, pages 9?12.Association for Computational Linguistics.48
