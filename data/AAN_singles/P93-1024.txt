DISTRIBUT IONAL CLUSTERING OF ENGL ISH WORDSFernando PereiraAT&T Bell Laboratories600 Mountain Ave.Murray Hill, NJ 07974, USApereira@research, att.
comNaftali TishbyDept.
of Computer ScienceHebrew UniversityJerusalem 91904, Israeltishby@cs, hu\]i. ac.
ilL i l l i an  LeeDept .
of Computer  Sc ienceCorne l l  Un ivers i tyI thaca ,  NY  14850, USAllee~cs, cornell, eduAbst rac tWe describe and evaluate experimentally amethod for clustering words according to their dis-tribution in particular syntactic ontexts.
Wordsare represented by the relative frequency distribu-tions of contexts in which they appear, and rela-tive entropy between those distributions i used asthe similarity measure for clustering.
Clusters arerepresented by average context distributions de-rived from the given words according to their prob-abilities of cluster membership.
In many cases,the clusters can be thought of as encoding coarsesense distinctions.
Deterministic annealing is usedto find lowest distortion sets of clusters: as the an-nealing parameter increases, existing clusters be-come unstable and subdivide, yielding a hierarchi-cal "soft" clustering of the data.
Clusters are usedas the basis for class models of word coocurrence,and the models evaluated with respect o held-outtest data.INTRODUCTIONMethods for automatically classifying words ac-cording to their contexts of use have both scien-tific and practical interest.
The scientific ques-tions arise in connection to distributional viewsof linguistic (particularly lexical) structure andalso in relation to the question of lexical acqui-sition both from psychological nd computationallearning perspectives.
From the practical pointof view, word classification addresses questions ofdata sparseness and generalization i  statisticallanguage models, particularly models for decidingamong alternative analyses proposed by a gram-mar.It is well known that a simple tabulation of fre-quencies of certain words participating in certainconfigurations, for example of frequencies of pairsof a transitive main verb and the head noun of itsdirect object, cannot be reliably used for compar-ing the likelihoods of different alternative configu-rations.
The problemis that for large enough cor-pora the number of possible joint events is muchlarger than the number of event occurrences inthe corpus, so many events are seen rarely ornever, making their frequency counts unreliableestimates of their probabilities.Hindle (1990) proposed dealing with thesparseness problem by estimating the likelihood ofunseen events from that of "similar" events thathave been seen.
For instance, one may estimatethe likelihood of a particular direct object for averb from the likelihoods of that direct object forsimilar verbs.
This requires a reasonable defini-tion of verb similarity and a similarity estimationmethod.
In Hindle's proposal, words are similar ifwe have strong statistical evidence that they tendto participate in the same events.
His notion ofsimilarity seems to agree with our intuitions inmany cases, but it is not clear how it can be useddirectly to construct word classes and correspond-ing models of association.Our research addresses some of the same ques-tions and uses similar raw data, but we investigatehow to factor word association tendencies into as-sociations of words to certain hidden senses classesand associations between the classes themselves.While it may be worth basing such a model on pre-existing sense classes (Resnik, 1992), in the workdescribed here we look at how to derive the classesdirectly from distributional data.
More specifi-cally, we model senses as probabilistic onceptsor clusters c with corresponding cluster member-ship probabilities p(clw ) for each word w. Mostother class-based modeling techniques for naturallanguage rely instead on "hard" Boolean classes(Brown et al, 1990).
Class construction is thencombinatorially very demanding and depends onfrequency counts for joint events involving partic-ular words, a potentially unreliable source of in-formation as noted above.
Our approach avoidsboth problems.Prob lem Sett ingIn what follows, we will consider two major wordclasses, 12 and Af, for the verbs and nouns in ourexperiments, and a single relation between them,in our experiments the relation between a tran-sitive main verb and the head noun of its directobject.
Our raw knowledge about the relation con-sists of the frequencies f~n of occurrence of par-ticular pairs (v,n) in the required configurationin a training corpus.
Some form of text analy-sis is required to collect such a collection of pairs.The corpus used in our first experiment was de-rived from newswire text automatically parsed by183Hindle's parser Fidditch (Hindle, 1993).
More re-cently, we have constructed similar tables with thehelp of a statistical part-of-speech tagger (Church,1988) and of tools for regular expression patternmatching on tagged corpora (Yarowsky, 1992).
Wehave not yet compared the accuracy and cover-age of the two methods, or what systematic biasesthey might introduce, although we took care to fil-ter out certain systematic errors, for instance themisparsing of the subject of a complement clauseas the direct object of a main verb for report verbslike "say".We will consider here only the problem of clas-sifying nouns according to their distribution as di-rect objects of verbs; the converse problem is for-mally similar.
More generally, the theoretical ba-sis for our method supports the use of clusteringto build models for any n-ary relation in terms ofassociations between elements in each coordinateand appropriate hidden units (cluster centroids)and associations between thosehidden units.For the noun classification problem, the em-pirical distribution of a noun n is then given bythe conditional distribution p,~(v) = f~.
/  ~v f"~"The problem we study is how to use the Pn to clas-sify the n EAf.
Our classification method will con-struct a set C of clusters and cluster membershipprobabilities p(c\]n).
Each cluster c is associated toa cluster centroid Pc, which is a distribution overl; obtained by averaging appropriately the pn.Distributional SimilarityTo cluster nouns n according to their conditionalverb distributions Pn, we need a measure of simi-larity between distributions.
We use for this pur-pose the relative entropy or Kullback-Leibler (KL)distance between two distributionsO(p I\[ q) = ZP(x)  log p(x): q(x)This is a natural choice for a variety of reasons,which we will just sketch here)First of all, D(p I\[ q) is zero just when p = q,and it increases as the probability decreases thatp is the relative frequency distribution of a ran-dom sample drawn according to q.
More formally,the probability mass given by q to the set of allsamples of length n with relative frequency distri-bution p is bounded by exp-nn(p  I\] q) (Coverand Thomas, 1991).
Therefore, if we are try-ing to distinguish among hypotheses qi when p isthe relative frequency distribution of observations,D(p II ql) gives the relative weight of evidence infavor of qi.
Furthermore, a similar relation holdsbetween D(p IIP') for two empirical distributions pand p' and the probability that p and p~ are drawnfrom the same distribution q.
We can thus use therelative entropy between the context distributionsfor two words to measure how likely they are tobe instances of the same cluster centroid.aA more formal discussion will appear in our paperDistributional Clustering, in preparation.From an information theoretic perspectiveD(p \]1 q) measures how inefficient on average itwould be to use a code based on q to encode avariable distributed according to p. With respectto our problem, D(pn H Pc) thus gives us the infor-mation loss in using cluster centroid Pc instead ofthe actual distribution pn for word n when mod-eling the distributional properties of n.Finally, relative entropy is a natural measureof similarity between distributions for clusteringbecause its minimization leads to cluster centroidsthat are a simple weighted average of member dis-tributions.One technical difficulty is that D(p \[1 p') isnot defined when p'(x) = 0 but p(x) > 0.
Wecould sidestep this problem (as we did initially) bysmoothing zero frequencies appropriately (Churchand Gale, 1991).
However, this is not very sat-isfactory because one of the goals of our work isprecisely to avoid the problems of data sparsenessby grouping words into classes.
It turns out thatthe problem is avoided by our clustering technique,since it does not need to compute the KL distancebetween individual word distributions, but onlybetween a word distribution and average distri-butions, the current cluster centroids, which areguaranteed to be nonzero whenever the word dis-tributions are.
This is a useful advantage of ourmethod compared with agglomerative clusteringtechniques that need to compare individual ob-jects being considered for grouping.THEORETICAL  BAS ISIn general, we are interested in how to organizea set of linguistic objects such as words accordingto the contexts in which they occur, for instancegrammatical constructions or n-grams.
We willshow elsewhere that the theoretical analysis out-lined here applies to that more general problem,but for now we will only address the more specificproblem in which the objects are nouns and thecontexts are verbs that take the nouns as directobjects.Our problem can be seen as that of learning ajoint distribution of pairs from a large sample ofpairs.
The pair coordinates come from two largesets ./kf and 12, with no preexisting internal struc-ture, and the training data is a sequence S of Nindependently drawn pairsSi = (ni, vi) 1 < i < N .From a learning perspective, this problem fallssomewhere in between unsupervised and super-vised learning.
As in unsupervised learning, thegoal is to learn the underlying distribution of thedata.
But in contrast o most unsupervised learn-ing settings, the objects involved have no internalstructure or attributes allowing them to be com-pared with each other.
Instead, the only informa-tion about the objects is the statistics of their jointappearance.
These statistics can thus be seen as aweak form of object labelling analogous to super-vision.184Distributional Cluster ingWhile clusters based on distributional similarityare interesting on their own, they can also be prof-itably seen as a means of summarizing a joint dis-tribution.
In particular, we would like to find aset of clusters C such that each conditional dis-tribution pn(v) can be approximately decomposedasp,(v) = ~p(cln)pc(v) ,cECwhere p(c\[n) is the membership probability of n inc and pc(v) = p(vlc ) is v's conditional probabilitygiven by the centroid distribution for cluster c.The above decomposition can be written in amore symmetric form as~(n,v) = ~_,p(c,n)p(vlc )cEC= ~-~p(c)P(nlc)P(Vlc) (1)cECassuming that p(n) and /5(n) coincide.
We willtake (1) as our basic clustering model.To determine this decomposition we need tosolve the two connected problems of finding suit-able forms for the cluster membership (c\[n) andthe centroid distributions p(vlc), and of maximiz-ing the goodness of fit between the model distri-bution 15(n, v) and the observed ata.Goodness of fit is determined by the model'slikelihood of the observations.
The maximum like-lihood (ML) estimation principle is thus the nat-ural tool to determine the centroid distributionspc(v).As for the membership robabilities, theymust be determined solely by the relevant mea-sure of object-to-cluster similarity, which in thepresent work is the relative entropy between ob-ject and cluster centroid distributions.
Since noother information is available, the membership isdetermined by maximizing the configuration en-tropy for a fixed average distortion.
With the max-imum entropy (ME) membership distribution, MLestimation is equivalent to the minimization of theaverage distortion of the data.
The combined en-tropy maximization entropy and distortion min-imization is carried out by a two-stage iterativeprocess similar to the EM method (Dempster etal., 1977).
The first stage of an iteration is a max-imum likelihood, or minimum distortion, estima-tion of the cluster centroids given fixed member-ship probabilities.
In the second stage of each iter-ation, the entropy of the membership distributionis maximized for a fixed average distortion.
Thisjoint optimization searches for a saddle point inthe distortion-entropy arameters, which is equiv-alent to minimizing a linear combination of thetwo known as free energy in statistical mechanics.This analogy with statistical mechanics i  not co-incidental, and provides a better understanding ofthe clustering procedure.Max imum L ike l ihood  C lus terCent ro idsFor the maximum likelihood argument, we start byestimating the likelihood of the sequence S of Nindependent observations of pairs (ni,vi).
Using(1), the sequence's model log likelihood isNl(S) = log p(c)p(n, le)p(vilc).i= l  cECFixing the number of clusters (model size) Icl, wewant to maximize l(S) with respect o the distri-butions P(nlc ) and p(vlc).
The variation of l(S)with respect o these distributions isN /v(v, Ic)@(n~fl(S) =~ 1 ~..~p(c)| + / (2)i=1 P(ni, vi) c~c \P(nilc)6p(vi Ic)\]with p(nlc ) and p(vlc ) kept normalized.
UsingBayes's formula, we have1 v( lni,~(ni, vi) -- p(c)p(ni\[c)p(vi\[c) (3)for any c. 2 Substituting (3) into (2), we obtainN ( , logp(n,  lc) )~l(S) = ZZp(c ln i ,v i )  + (4)logp(vi Ic) i=1 cECsince ~flogp -- @/p.
This expression is particu-larly useful when the cluster distributions p(n\[c)and p(vlc ) have an exponential form, preciselywhat will be provided by the ME step describedbelow.At this point we need to specify the cluster-ing model in more detail.
In the derivation so farwe have treated, p(n c) and p(v c) symmetrically,corresponding to clusters not of verbs or nounsbut of verb-noun associations.
In principle sucha symmetric model may be more accurate, but inthis paper we will concentrate on asymmetric mod-els in which cluster memberships are associated tojust one of the components of the joint distributionand the cluster centroids are specified only by theother component.
In particular, the model we usein our experiments has noun clusters with clustermemberships determined by p(nlc) and centroiddistributions determined by p(vlc ).The asymmetric model simplifies the estima-tion significantly by dealing with a single compo-nent, but it has the disadvantage that the jointdistribution, p(n, v) has two different and not nec-essarily consistent expressions in terms of asym-metric models for the two coordinates.2As usual in clustering models (Duda and Hart,1973), we assume that the model distribution and theempirical distribution are interchangeable at the solu-tion of the parameter estimation equations, since themodel is assumed to be able to represent correctly thedata at that solution point.
In practice, the data maynot come exactly from the chosen model class, but themodel obtained by solving the estimation equationsmay still be the closest one to the data.185Maximum Ent ropy  C luster  Membersh ipWhile variations of p(nlc ) and p(vlc ) iri equation(4) are not independent, we can treat them sep-arately.
First, for fixed average distortion be-tween the cluster centroid distributions p(vlc ) andthe data p(vln), we find the cluster membershipprobabilities, which are the Bayes inverses of thep(nlc), that maximize the entropy of the clusterdistributions.
With the membership distributionsthus obtained, we then look for the p(vlc ) thatmaximize the log likelihood l(S).
It turns outthat this will also be the values ofp(vlc) that mini-mize the average distortion between the asymmet-ric cluster model and the data.Given any similarity measure din , c) betweennouns and cluster centroids, the average clusterdistortion is(0) = ~_, ~,p(cln)d(n,c ) (5)nEAr tEdIf we maximize the cluster membership entropyH = - ~ Zp(cln)logp(nlc) (6)nEX cEdsubject o normalization ofp(nlc) and fixed (5), weobtain the following standard exponential forms(Jaynes, 1983) for the class and membership dis-tributions1 p(nlc) = Z-?
exp -rid(n, c) (7)1 p(cJn) = ~ exp -rid(n, c) (8)where the normalization sums (partition func-tions) are Z~ = ~,~ exp-fld(n,c) and Zn =~exp- r id (n ,c ) .
Notice that d(n,c) does notneed to be symmetric for this derivation, as thetwo distributions are simply related by Bayes'srule.Returning to the log-likelihood variation (4),we can now use (7) for p(n\[c) and the assumptionfor the asymmetric model that the cluster mem-bership stays fixed as we adjust the centroids, toobtainN61(S) = - ~ ~ p(elni)6rid(n,, c)+ ~ log Z~ (9)i=1 eECwhere the variation of p(v\[c) is now included inthe variation of d(n, e).For a large enough sample, we may replace thesum over observations in (9) by the average over N61(s )  = - p(n) -"p(?ln)6rid(n, ?)
+ 6 logZ?nEN cECwhich, applying Bayes's rule, becomes1 61(S) = - ~ ~(~ ~ p(nlc)6rid(n, c) + 6 log Z?.eEC hENAt the log-likelihood maximum, this variationmust vanish.
We will see below that the use of rel-ative entropy for similarity measure makes 6 log Zcvanish at the maximum as well, so the log likeli-hood can be maximized by minimizing the averagedistortion with respect o the class centroids whileclass membership is kept fixed1p(n jc )6d(n ,e )= o ,cEC nEXor, sufficiently, if each of the inner sums vanish~ p(nlcl6d(n,c)= 0 (10)tee  nEArMin imiz ing  the  Average KL D is tor t ion  Wefirst show that the minimization of the relativeentropy yields the natural expression for clustercentroidsP(vle ) = ~ p(nlc)p(vln ) (11)nEWTo minimize the average distortion (10), we ob-serve that the variation of the KL distance be-tween noun and centroid distributions with re-spect to the centroid istribution p(v\[c), with eachcentroid distribution ormalized by the Lagrangemultiplier Ac, is given by( - ~evP(V\[n)l?gp(v\[c) )~d(n,c) = ~ +A?
(E,~ev p(vlc) - 1)= ~-~( p(vln)+AO,p(vlc ) v(vl )Substituting this expression into (10), we obtain, ,~ v p(vlc)Since the ~p(vlc ) are now independent, we obtainimmediately the desired centroid expression (11),which is the desired weighted average of noun dis-tributions.We can now see that the variation (5 log Z~ van-ishes for centroid distributions given by (11), sinceit follows from (10) that6 log = exp-rid(, , c)6d(n, e)Ze-ri - -  0nThe Free Energy  Funct ion  The combinedminimum distortion and maximum entropy opti-mization is equivalent to the minimization of a sin-gle function, the free energy1log Zn F = -~= <D>-"H l r i  ,where (D) is the average distortion (5) and H isthe cluster membership entropy (6).186The free energy determines both the distor-tion and the membership entropy throughOZF(D)  - O~OFH -OT 'where T =/~-1 is the temperature.The most important property of the free en-ergy is that its minimum determines the balancebetween the "disordering" maximum entropy and"ordering" distortion minimization in which thesystem is most likely to be found.
In fact the prob-ability to find the system at a given configurationis exponential in FPocexp- f l F  ,so a system is most likely to be found in its mini-mal free energy configuration.Hierarchical  C luster ingThe analogy with statistical mechanics uggestsa deterministic annealing procedure for clusteringRose et al, 1990), in which the number of clusterss determined through a sequence of phase transi-tions by continuously increasing the parameter/?following an annealing schedule.The higher is fl, the more local is the influenceof each noun on the definition of centroids.
Dis-tributional similarity plays here the role of distor-tion.
When the scale parameter fl is close to zero,the similarity is almost irrelevant.
All words con-tribute about equally to each centroid, and so thelowest average distortion solution involves just onecluster whose centroid is the average of all worddistributions.
As fl is slowly increased, a criticalpoint is eventually reached for which the lowestF solution involves two distinct centroids.
We saythen that the original cluster has split into the twonew clusters.In general, if we take any cluster c and a twinc' of c such that the centroid Pc' is a small ran-dom perturbation of Pc, below the critical fl atwhich c splits the membership and centroid reesti-mation procedure given by equations (8) and (11)will make pc and Pc, converge, that is, c and c'are really the same cluster.
But with fl above thecritical value for c, the two centroids will diverge,giving rise to two daughters of c.Our clustering procedure is thus as follows.We start with very low /3 and a single clusterwhose centroid is the average of all noun distri-butions.
For any given fl, we have a current set ofleaf clusters corresponding to the current free en-ergy (local) minimum.
To refine such a solution,we search for the lowest fl which is the criticalvalue for some current leaf cluster splits.
Ideally,there is just one split at that critical value, butfor practical performance and numerical accuracyreasons we may have several splits at the new crit-ical point.
The splitting procedure can then berepeated to achieve the desired number of clustersor model cross-entropy.3gunmissileweaponrocketroot1missile 0.835 officerrocket 0.850 aidebullet 0.917 chief0.940 manager40.758 shot 0.8580.786 bullet 0.9250.862 rocket 0.9300.875 missile 1.03720.4840.6120.6490.651Figure 1: Direct object clusters for fireCLUSTERING EXAMPLESAll our experiments involve the asymmetric modeldescribed in the previous section.
As explainedthere, our clustering procedure yields for eachvalue of ~ a set CZ of clusters minimizing the freeenergy F, and the asymmetric model for fl esti-mates the conditional verb distribution for a nounn bycECBwhere p(cln ) also depends on ft.As a first experiment, we used our method toclassify the 64 nouns appearing most frequentlyas heads of direct objects of the verb "fire" in oneyear (1988) of Associated Press newswire.
In thiscorpus, the chosen nouns appear as direct objectheads of a total of 2147 distinct verbs, so eachnoun is represented by a density over the 2147verbs.Figure 1 shows the four words most similar toeach cluster centroid, and the corresponding word-centroid KL distances, for the four clusters result-ing from the first two cluster splits.
It can be seenthat first split separates the objects correspondingto the weaponry sense of "fire" (cluster 1) from theones corresponding to the personnel action (clus-ter 2).
The second split then further refines theweaponry sense into a projectile sense (cluster 3)and a gun sense (cluster 4).
That split is some-what less sharp, possibly because not enough dis-tinguishing contexts occur in the corpus.Figure 2 shows the four closest nouns to thecentroid of each of a set of hierarchical clus-ters derived from verb-object pairs involving the1000 most frequent nouns in the June 1991 elec-tronic version of Grolier's Encyclopedia (10 mil-187grantdistinctionformrepresentationstate 1.320 t residenceally 1.458 stateresidence 1.473 conductor/,..movement 1.534 teacher"-number 0.999 numbermaterial 1.361 materialvariety 1.401 massmass 1.422'~ variety~numberdiversitystructureconcentrationJ control 1 .2011recognition 1.317nomination 1.363~ i~ i~ im 1.3661.392 ent 1.329 _1.554 voyage 1.338 -~-1.571 ~migrat ion  1.4281.577 progress 1.441 ~conductor 0.699 j Istate \]1.279 Ivice-president 0 .756~eop le  I 1.417\]editor 0.814 Imodem 1.418director 0.825 \[farmer 1.4251.082 j complex 1.161 ~aavy 1.096 I1.102 network 1.175_._._~ommunity 1.099 I1.213 community 1.276 \]aetwork 1.2441.233 group 1 .327~ Icomplex 1.259"~omplex \[1.097 IImaterial \[ 0.976 ~network  I 1"21111.026 ~alt \] 1.217\[ lake 11.36011.093 ...------'-'-~mg 1.2441 ~region 11.43511.252 ~aumber 1.250\[ ~ssay \[0.695 Il ' 278~number  1.047 Icomedy 10.8001comedy 1.060..------"~oem \[ 0"8291essay 1.142 f-reatise \[ 0.850\]piece 1 .198"~urnber  11.120 I~?ariety 1.217 I~ater ia l  1.275 IFluster 1.3111~tructure \[ 1.3711~elationship 1.460 I1.429 change 1.561 j .
.
.~P  ect 1.492\[1.537 failure 1.562"-"'- \]system 1.497 I1.577 variation 1 .592~ iaollution 1.187\]1.582, structure 1.592 ~"~ai lu re  1.290 I\ \[re_crease 1.328 I Imtection 1.432\]speed 1.177 ~number 11.4611level 1.315 _.,__Jconcentration 1.478 Ivelocity 1.371 ~trength 1.488 Isize 1 .440~ ~atio 1.488 I~)lspeed 11.130 I~en i th  11.2141epth 1.2441ecognition 0.874\]tcclaim 1.026 Ienown 1.079nomination 1.104form 1.110 I~xplanation 1.255 I:are 1.2911:ontrol 1.295 Ivoyage 0.8611Lrip 0.972\]progress 1.016 Iimprovement 1.114 I)rogram 1.459 I,peration 1.478 I:tudy 1.480 Investigation 1.4811;onductor 0.457\]rice-president 0.474 Ilirector 0.489 I:hairman 0.5001Figure 2: Noun Clusters for Grolier's Encyclopedia188?~3~o-~ ?
train, * - - - - - ,  test pk s- - - -D new- - t t -  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.t t t0 0 100 200 300 400number of dustersFigure 3: Asymmetric Model Evaluation, AP88Verb-Direct Object Pairs0.8"\.m.......~ exceptional3 0.6-o 0.40.2- sL , .
, i0 0 100 200 300number of clusters400Figure 4: Pairwise Verb Comparisons, AP88 Verb-Direct Object Pairslion words).MODEL EVALUATIONThe preceding qualitative discussion providessome indication of what aspects of distributionalrelationships may be discovered by clustering.However, we also need to evaluate clustering morerigorously as a basis for models of distributionalrelationships.
So, far, we have looked at two kindsof measurements of model quality: (i) relative en-tropy between held-out data and the asymmetricmodel, and (ii) performance on the task of decid-ing which of two verbs is more likely to take a givennoun as direct object when the data relating oneof the verbs to the noun has been withheld fromthe training data.The evaluation described below was per-formed on the largest data set we have workedwith so far, extracted from 44 million words of1988 Associated Press newswire with the patternmatching techniques mentioned earlier.
This col-lection process yielded 1112041 verb-object pairs.We selected then the subset involving the 1000most frequent nouns in the corpus for clustering,and randomly divided it into a training set of756721 pairs and a test set of 81240 pairs.Re la t ive  Ent ropyFigure 3 plots the unweighted average relative en-tropy, in bits, of several test sets to asymmet-ric clustered models of different sizes, given by1 ~,,eAr, D(t,,ll/~-), where Aft is the set of di-rect objects in the test set and t,~ is the relativefrequency distribution of verbs taking n as directobject in the test set.
3 For each critical valueof f?, we show the relative entropy with respect oawe use unweighted averages because we are inter-ested her on how well the noun distributions are ap-proximated by the cluster model.
If we were interestedon the total information loss of using the asymmetricmodel to encode a test corpus, we would instead usethe asymmetric model based on gp of the train-ing set (set train), of randomly selected held-outtest set (set test), and of held-out data for a fur-ther 1000 nouns that were not clustered (set new).Unsurprisingly, the training set relative entropydecreases monotonically.
The test set relative en-tropy decreases to a minimum at 206 clusters, andthen starts increasing, suggesting that larger mod-els are overtrained.The new noun test set is intended to testwhether clusters based on the 1000 most frequentnouns are useful classifiers for the selectional prop-erties of nouns in general.
Since the nouns in thetest set pairs do not occur in the training set, wedo not have their cluster membership probabilitiesthat are needed in the asymmetric model.
Instead,for each noun n in the test set, we classify it withrespect o the clusters by settingp(cln) = exp -DD(p,~ I lc)/Z,where p,~ is the empirical conditional verb distri-bution for n given by the test set.
These clustermembership estimates were then used in the asym-metric model and the test set relative entropy cal-culated as before.
As the figure shows, the clustermodel provides over one bit of information aboutthe selectional properties of the new nouns, butthe overtraining effect is even sharper than for theheld-out data involving the 1000 clustered nouns.Dec is ion  TaskWe also evaluated asymmetric luster models ona verb decision task closer to possible applicationsto disambiguation i language analysis.
The taskconsists judging which of two verbs v and v' ismore likely to take a given noun n as object, whenall occurrences of (v, n) in the training set weredeliberately deleted.
Thus this test evaluates howwell the models reconstruct missing data in thethe weighted average ~,~e~t fnD(t,~ll~,,) where f,, isthe relative frequency of n in the test set.189verb distribution for n from the cluster centroidsclose to n.The data for this test was built from the train-ing data for the previous one in the following way,based on a suggestion by Dagan et al (1993).
104noun-verb pairs with a fairly frequent verb (be-tween 500 and 5000 occurrences) were randomlypicked, and all occurrences of each pair in thetraining set were deleted.
The resulting trainingset was used to build a sequence of cluster modelsas before.
Each model was used to decide which oftwo verbs v and v ~ are more likely to appear witha noun n where the (v, n) data was deleted fromthe training set, and the decisions were comparedwith the corresponding ones derived from the orig-inal event frequencies in the initial data set.
Theerror rate for each model is simply the proportionof disagreements for the selected (v, n, v t) triples.Figure 4 shows the error rates for each model forall the selected (v, n, v ~) (al 0 and for just thoseexceptional triples in which the conditional ratiop(n, v)/p(n, v ~) is on the opposite side of 1 fromthe marginal ratio p(v)/p(v~).
In other words, theexceptional cases are those in which predictionsbased just on the marginal frequencies, which theinitial one-cluster model represents, would be con-sistently wrong.Here too we see some overtraining for thelargest models considered, although not for the ex-ceptional verbs.CONCLUSIONSWe have demonstrated that a general divisive clus-tering procedure for probabil ity distributions canbe used to group words according to their partic-ipation in particular grammatical  relations withother words.
The resulting clusters are intuitivelyinformative, and can be used to construct class-based word coocurrence models with substantialpredictive power.While the clusters derived by the proposedmethod seem in many cases semantically signif-icant, this intuition needs to be grounded in amore rigorous assessment.
In addition to predic-tive power evaluations of the kind we have al-ready carried out, it might be worth comparingautomatically-derived clusters with human judge:ments in a suitable experimental setting.Moving further in the direction of class-basedlanguage models, we plan to consider additionaldistributional relations (for instance, adjective-noun) and apply the results of clustering tothe grouping of lexical associations in lexicalizedgrammar frameworks such as stochastic lexicalizedtree-adjoining rammars (Schabes, 1992).ACKNOWLEDGMENTSWe would like to thank Don Hindle for makingavailable the 1988 Associated Press verb-objectdata set, the Fidditch parser and a verb-objectstructure filter, Mats Rooth for selecting the ob-jects of "fire" data set and many discussions,David Yarowsky for help with his stemming andconcordancing tools, and Ido  Dagan for suggestingways of testing cluster models.REFERENCESPeter F. Brown, Vincent J. Della Pietra, Peter V. deS-ouza, Jenifer C. Lal, and Robert L. Mercer.
1990.Class-based n-gram models of natural language.In Proceedings of the IBM Natural Language ITL,pages 283-298, Paris, France, March.Kenneth W. Church and William A. Gale.
1991.A comparison of the enhanced Good-Turing anddeleted estimation methods for estimating proba-bilities of English bigrams.
Computer Speech andLanguage, 5:19-54.Kenneth W. Church.
1988.
A stochastic parts pro-gram and noun phrase parser for unrestrictedtext.
In Proceedings of the Second Conferenceon Applied Natural Language Processing, pages136-143, Austin, Texas.
Association for Compu-tational Linguistics, Morristown, New Jersey.Thomas M. Cover and Joy A. Thomas.
1991.
Ele-ments of Information Theory.
Wiley-Interscience,New York, New York.Ido Dagan, Shaul Markus, and Shaul Markovitch.1993.
Contextual word similarity and estimationfrom sparse data.
In these proceedings.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal StatisticalSociety, Series B, 39(1):1-38.Richard O. Duda and Peter E. Hart.
1973.
Pat-tern Classification and Scene Analysis.
Wiley-Interseience, New York, New York.Donald Hindle.
1990.
Noun classification frompredicate-argument structures.
In 28th AnnualMeeting of the Association for ComputationalLinguistics, pages 268-275, Pittsburgh, Pennsyl-vania.
Association for Computational Linguistics,Morristown, New Jersey.Donald Hindle.
1993.
A parser for text corpora.
InB.T.S.
Atldns and A. Zampoli, editors, Computa-tional Approaches to the Lexicon.
Oxford Univer-sity Press, Oxford, England.
To appear.Edwin T. Jaynes.
1983.
Brandeis lectures.
InRoger D. Rosenkrantz, editor, E. T. Jaynes:Papers on Probability, Statistics and StatisticalPhysics, number 158 in Synthese Library, chap-ter 4, pages 40-76.
D. Reidel, Dordrecht, Holland.Philip Resnik.
1992.
WordNet and distributionalanalysis: A class-based approach to lexical dis-covery.
In AAAI  Workshop on Statistically-Based Natural-Language-Processing Techniques,San Jose, California, July.Kenneth Rose, Eitan Gurewitz, and Geoffrey C. Fox.1990.
Statistical mechanics and phase transitionsin clustering.
Physical Review Letters, 65(8):945-948.Yves Sehabes.
1992.
Stochastic lexicalized tree-adjoining grammars.
In Proceeedings of the 14thInternational Conference on Computational Lin-guistics, Nantes, France.David Yarowsky.
1992.
CONC: Tools for text corpora.Technical Memorandum 11222-921222-29, AT&TBell Laboratories.190
