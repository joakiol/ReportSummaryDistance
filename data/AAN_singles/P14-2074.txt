Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 452?457,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsComparing Automatic Evaluation Measures for Image DescriptionDesmond Elliott and Frank KellerInstitute for Language, Cognition, and ComputationSchool of Informatics, University of Edinburghd.elliott@ed.ac.uk, keller@inf.ed.ac.ukAbstractImage description is a new natural lan-guage generation task, where the aim is togenerate a human-like description of an im-age.
The evaluation of computer-generatedtext is a notoriously difficult problem, how-ever, the quality of image descriptions hastypically been measured using unigramBLEU and human judgements.
The focusof this paper is to determine the correlationof automatic measures with human judge-ments for this task.
We estimate the correla-tion of unigram and Smoothed BLEU, TER,ROUGE-SU4, and Meteor against humanjudgements on two data sets.
The mainfinding is that unigram BLEU has a weakcorrelation, and Meteor has the strongestcorrelation with human judgements.1 IntroductionRecent advances in computer vision and naturallanguage processing have led to an upsurge of re-search on tasks involving both vision and language.State of the art visual detectors have made it possi-ble to hypothesise what is in an image (Guillauminet al, 2009; Felzenszwalb et al, 2010), pavingthe way for automatic image description systems.The aim of such systems is to extract and reasonabout visual aspects of images to generate a human-like description.
An example of the type of imageand gold-standard descriptions available can beseen in Figure 1.
Recent approaches to this taskhave been based on slot-filling (Yang et al, 2011;Elliott and Keller, 2013), combining web-scale n-grams (Li et al, 2011), syntactic tree substitution(Mitchell et al, 2012), and description-by-retrieval(Farhadi et al, 2010; Ordonez et al, 2011; Hodoshet al, 2013).
Image description has been comparedto translating an image into text (Li et al, 2011;Kulkarni et al, 2011) or summarising an image1.
An older woman with a small dog in the snow.2.
A woman and a cat are outside in the snow.3.
A woman in a brown vest is walking on thesnow with an animal.4.
A woman with a red scarf covering her headwalks with her cat on snow-covered ground.5.
Heavy set woman in snow with a cat.Figure 1: An image from the Flickr8K data set andfive human-written descriptions.
These descrip-tions vary in the adjectives or prepositional phrasesthat describe the woman (1, 3, 4, 5), incorrect or un-certain identification of the cat (1, 3), and includea sentence without a verb (5).
(Yang et al, 2011), resulting in the adoption of theevaluation measures from those communities.In this paper we estimate the correlation of hu-man judgements with five automatic evaluationmeasures on two image description data sets.
Ourwork extends previous studies of evaluation mea-sures for image description (Hodosh et al, 2013),which focused on unigram-based measures and re-ported agreement scores such as Cohen?s ?
ratherthan correlations.
The main finding of our analysisis that TER and unigram BLEU are weakly corre-452lated against human judgements, ROUGE-SU4 andSmoothed BLEU are moderately correlated, and thestrongest correlation is found with Meteor.2 MethodologyWe estimate Spearman?s ?
for five different auto-matic evaluation measures against human judge-ments for the automatic image description task.Spearman?s ?
is a non-parametric correlation co-efficient that restricts the ability of outlier datapoints to skew the co-efficient value.
The automaticmeasures are calculated on the sentence level andcorrelated against human judgements of semanticcorrectness.2.1 DataWe perform the correlation analysis on the Flickr8Kdata set of Hodosh et al (2013), and the data set ofElliott and Keller (2013).The test data of the Flickr8K data set contains1,000 images paired with five reference descrip-tions.
The images were retrieved from Flickr, thereference descriptions were collected from Me-chanical Turk, and the human judgements werecollected from expert annotators as follows: eachimage in the test data was paired with the highestscoring sentence(s) retrieved from all possible testsentences by the TRI5SEM model in Hodosh et al(2013).
Each image?description pairing in the testdata was judged for semantic correctness by threeexpert human judges on a scale of 1?4.
We calcu-late automatic measures for each image?retrievedsentence pair against the five reference descriptionsfor the original image.The test data of Elliott and Keller (2013) con-tains 101 images paired with three reference de-scriptions.
The images were taken from the PAS-CAL VOC Action Recognition Task, the referencedescriptions were collected from Mechanical Turk,and the judgements were also collected from Me-chanical Turk.
Elliott and Keller (2013) gener-ated two-sentence descriptions for each of the testimages using four variants of a slot-filling model,and collected five human judgements of the se-mantic correctness and grammatical correctness ofthe description on a scale of 1?5 for each image?description pair, resulting in a total of 2,042 humanjudgement?description pairings.
In this analysis,we use only the first sentence of the description,which describes the event depicted in the image.2.2 Automatic Evaluation MeasuresBLEU measures the effective overlap between areference sentence X and a candidate sentence Y .It is defined as the geometric mean of the effectiven-gram precision scores, multiplied by the brevitypenalty factor BP to penalise short translations.
pnmeasures the effective overlap by calculating theproportion of the maximum number of n-gramsco-occurring between a candidate and a referenceand the total number of n-grams in the candidatetext.
More formally,BLEU = BP ?
exp(N?n=1wnlog pn)pn=?c?cand?ngram?ccountclip(ngram)?c?cand?ngram?ccount(ngram)BP ={1 if c > re(1?r/c)if c?
rUnigram BLEU without a brevity penalty has beenreported by Kulkarni et al (2011), Li et al (2011),Ordonez et al (2011), and Kuznetsova et al (2012);to the best of our knowledge, the only image de-scription work to use higher-order n-grams withBLEU is Elliott and Keller (2013).
In this paper weuse the smoothed BLEU implementation of Clark etal.
(2011) to perform a sentence-level analysis, set-ting n = 1 and no brevity penalty to get the unigramBLEU measure, or n = 4 with the brevity penaltyto get the Smoothed BLEU measure.
We note that ahigher BLEU score is better.ROUGE measures the longest common subse-quence of tokens between a candidate Y and refer-ence X .
There is also a variant that measures the co-occurrence of pairs of tokens in both the candidateand reference (a skip-bigram): ROUGE-SU*.
Theskip-bigram calculation is parameterised with dskip,the maximum number of tokens between the wordsin the skip-bigram.
Setting dskipto 0 is equivalent tobigram overlap and setting dskipto ?
means tokenscan be any distance apart.
If ?
= |SKIP2(X ,Y )|is the number of matching skip-bigrams betweenthe reference and the candidate, then skip-bigramROUGE is formally defined as:RSKIP2= ?
/(?2)453ROUGE has been used by only Yang et al (2011)to measure the quality of generated descriptions,using a variant they describe as ROUGE-1.
We setdskip= 4 and award partial credit for unigram onlymatches, otherwise known as ROUGE-SU4.
We useROUGE v.1.5.5 for the analysis, and configure theevaluation script to return the result for the averagescore for matching between the candidate and thereferences.
A higher ROUGE score is better.TER measures the number of modifications a hu-man would need to make to transform a candidateY into a reference X .
The modifications availableare insertion, deletion, substitute a single word, andshift a word an arbitrary distance.
TER is expressedas the percentage of the sentence that needs to bechanged, and can be greater than 100 if the candi-date is longer than the reference.
More formally,TER =|edits||reference tokens|TER has not yet been used to evaluate image de-scription models.
We use v.0.8.0 of the TER evalu-ation tool, and a lower TER is better.Meteor is the harmonic mean of unigram preci-sion and recall that allows for exact, synonym, andparaphrase matchings between candidates and ref-erences.
It is calculated by generating an alignmentbetween the tokens in the candidate and referencesentences, with the aim of a 1:1 alignment betweentokens and minimising the number of chunks chof contiguous and identically ordered tokens in thesentence pair.
The alignment is based on exact to-ken matching, followed by Wordnet synonyms, andthen stemmed tokens.
We can calculate precision,recall, and F-measure, where m is the number ofaligned unigrams between candidate and reference.Meteor is defined as:M = (1?Pen) ?FmeanPen = ?(chm)?Fmean=PR?P+(1??
)RP =|m||unigrams in candidate|R =|m||unigrams in reference|We calculated the Meteor scores using release 1.4.0with the package-provided free parameter settingsof 0.85, 0.2, 0.6, and 0.75 for the matching compo-nents.
Meteor has not yet been reported to evaluateFlickr 8Kco-efficientn = 17,466E&K (2013)co-efficientn = 2,040METEOR 0.524 0.233ROUGE SU-4 0.435 0.188Smoothed BLEU 0.429 0.177Unigram BLEU 0.345 0.097TER -0.279 -0.044Table 1: Spearman?s correlation co-efficient of au-tomatic evaluation measures against human judge-ments.
All correlations are significant at p < 0.001.the performance of different models on the imagedescription task; a higher Meteor score is better.2.3 ProtocolWe performed the correlation analysis as follows.The sentence-level evaluation measures were cal-culated for each image?description?reference tu-ple.
We collected the BLEU, TER, and Meteorscores using MultEval (Clark et al, 2011), and theROUGE-SU4 scores using the RELEASE-1.5.5.plscript.
The evaluation measure scores were thencompared with the human judgements using Spear-man?s correlation estimated at the sentence-level.3 ResultsTable 1 shows the correlation co-efficients betweenautomatic measures and human judgements andFigures 2(a) and (b) show the distribution of scoresfor each measure against human judgements.
Toclassify the strength of the correlations, we fol-lowed the guidance of Dancey and Reidy (2011),who posit that a co-efficient of 0.0?0.1 is uncor-related, 0.11?0.4 is weak, 0.41?0.7 is moderate,0.71?0.90 is strong, and 0.91?1.0 is perfect.On the Flickr8k data set, all evaluation measurescan be classified as either weakly correlated or mod-erately correlated with human judgements and allresults are significant.
TER is only weakly cor-related with human judgements but could proveuseful in comparing the types of differences be-tween models.
An analysis of the distribution ofTER scores in Figure 2(a) shows that differences incandidate and reference length are prevalent in theimage description task.
Unigram BLEU is also onlyweakly correlated against human judgements, eventhough it has been reported extensively for this task.4540 20 40 60 80 100METEOR ?= 0.52412340.0 0.2 0.4 0.6ROUGE-SU4 ?= 0.43512340 20 40 60 80 100Smoothed BLEU ?= 0.42912340 20 40 60 80 100Unigram BLEU ?= 0.34512340 100 200 300 400TER ?= -0.2791234Sentence-level automated measure scoreHuman Judgement(a) Flick8K data set, n=17,466.0 20 40 60 80 100METEOR ?= 0.2331350.0 0.2 0.4 0.6 0.8ROUGE-SU4 ?= 0.1881350 20 40 60 80 100Smoothed BLEU ?= 0.17713540 50 60 70 80 90 100Unigram BLEU ?= 0.09651350 50 100 150TER ?= -0.0443135Sentence-level automated measure scoreHuman Judgement(b) E&K (2013) data set, n=2,042.Figure 2: Distribution of automatic evaluation measures against human judgements.
?
is the correlationbetween human judgements and the automatic measure.
The intensity of each point indicates the numberof occurrences that fall into that range.Figure 2(a) shows an almost uniform distributionof unigram BLEU scores, regardless of the humanjudgement.
Smoothed BLEU and ROUGE-SU4 aremoderately correlated with human judgements, andthe correlation is stronger than with unigram BLEU.Finally, Meteor is most strongly correlated mea-sure against human judgements.
A similar patternis observed in the Elliott and Keller (2013) data set,though the correlations are lower across all mea-sures.
This could be caused by the smaller samplesize or because the descriptions were generatedby a computer, and not retrieved from a collectionof human-written descriptions containing the gold-standard text, as in the Flickr8K data set.Qualitative AnalysisFigure 3 shows two images from the test collec-tion of the Flickr8K data set with a low Meteorscore and a maximum human judgement of seman-tic correctness.
The main difference between thecandidates and references are in deciding what todescribe (content selection), and how to describe it(realisation).
We can hypothesise that in both trans-lation and summarisation, the source text acts as alexical and semantic framework within which thetranslation or summarisation process takes place.In Figure 3(a), the authors of the descriptions madedifferent decisions on what to describe.
A decisionhas been made to describe the role of the officials inthe candidate text, and not in the reference text.
Theunderlying cause of this is an active area of researchin the human vision literature and can be attributedto bottom-up effects, such as saliency (Itti et al,1998), top-down contextual effects (Torralba et al,2006), or rapidly-obtained scene properties (Olivaand Torralba, 2001).
In (b), we can see the problemof deciding how to describe the selected content.The reference uses a more specific noun to describethe person on the bicycle than the candidate.4 DiscussionThere are several differences between our analysisand that of Hodosh et al (2013).
First, we reportSpearman?s ?
correlation coefficient of automaticmeasures against human judgements, whereas theyreport agreement between judgements and auto-matic measures in terms of Cohen?s ?.
The use of?
requires the transformation of real-valued scoresinto categorical values, and thus loses informa-tion; we use the judgement and evaluation measurescores in their original forms.
Second, our use ofSpearman?s ?
means we can readily use all of theavailable data for the correlation analysis, whereasHodosh et al (2013) report agreement on thresh-olded subsets of the data.
Third, we report the corre-lation coefficients against five evaluation measures,455Candidate: Football players gathering to con-test something to collaborating officials.Reference: A football player in red and whiteis holding both hands up.
(a)Candidate: A man is attempting a stunt with abicycle.Reference: Bmx biker Jumps off of ramp.
(b)Figure 3: Examples in the test data with low Meteor scores and the maximum expert human judgement.
(a) the candidate and reference are from the same image, and show differences in what to describe, in(b) the descriptions are retrieved from different images and show differences in how to describe an image.some of which go beyond unigram matchings be-tween references and candidates, whereas they onlyreport unigram BLEU and unigram ROUGE.
It istherefore difficult to directly compare the resultsof our correlation analysis against Hodosh et al?sagreement analysis, but they also reach the conclu-sion that unigram BLEU is not an appropriate mea-sure of image description performance.
However,we do find stronger correlations with SmoothedBLEU, skip-bigram ROUGE, and Meteor.In contrast to the results presented here, Reiterand Belz (2009) found no significant correlationsof automatic evaluation measures against humanjudgements of the accuracy of machine-generatedweather forecasts.
They did, however, find signif-icant correlations of automatic measures againstfluency judgements.
There are no fluency judge-ments available for Flickr8K, but Elliott and Keller(2013) report grammaticality judgements for theirdata, which are comparable to fluency ratings.
Wefailed to find significant correlations between gram-matlicality judgements and any of the automaticmeasures on the Elliott and Keller (2013) data.
Thisdiscrepancy could be explained in terms of the dif-ferences between the weather forecast generationand image description tasks, or because the imagedescription data sets contain thousands of texts anda few human judgements per text, whereas the datasets of Reiter and Belz (2009) included hundredsof texts with 30 human judges.5 ConclusionsIn this paper we performed a sentence-level corre-lation analysis of automatic evaluation measuresagainst expert human judgements for the automaticimage description task.
We found that sentence-level unigram BLEU is only weakly correlated withhuman judgements, even though it has extensivelyreported in the literature for this task.
Meteor wasfound to have the highest correlation with humanjudgements, but it requires Wordnet and paraphraseresources that are not available for all languages.Our findings held when judgements were made onhuman-written or computer-generated descriptions.The variability in what and how people describeimages will cause problems for all of the measurescompared in this paper.
Nevertheless, we proposethat unigram BLEU should no longer be used asan objective function for automatic image descrip-tion because it has a weak correlation with humanaccuracy judgements.
We recommend adoptingeither Meteor, Smoothed BLEU, or ROUGE-SU4 be-cause they show stronger correlations with humanjudgements.
We believe these suggestions are alsoapplicable to the ranking tasks proposed in Hodoshet al (2013), where automatic evaluation scorescould act as features to a ranking function.AcknowledgmentsAlexandra Birch and R. Calen Walshe, and theanonymous reviewers provided valuable feedbackon this paper.
The research is funded by ERCStarting Grant SYNPROC No.
203427.456ReferencesJonathon H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: Controlling for op-timizer instability.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages176?181, Portland, Oregon, USA.Christine Dancey and John Reidy, 2011.
StatisticsWithout Maths for Psychology, page 175.
PrenticeHall, 5th edition.Desmond Elliott and Frank Keller.
2013.
Image De-scription using Visual Dependency Representations.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages1292?1302, Seattle, Washington, U.S.A.Ali Farhadi, Mohsen Hejrati, Mohammad AminSadeghi, Peter Young, Cyrus Rashtchian, JuliaHockenmaier, and David Forsyth.
2010.
Everypicture tells a story: generating sentences from im-ages.
In Proceedings of the 11th European Confer-ence on Computer Vision, pages 15?29, Heraklion,Crete, Greece.Pedro F. Felzenszwalb, Ross B. Girshick, DavidMcAllester, and Deva Ramanan.
2010.
ObjectDetection with Discriminatively Trained Part-BasedModels.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 32(9):1627?1645.Matthieu Guillaumin, Thomas Mensink, Jakob J. Ver-beek, and Cornelia Schmid.
2009.
Tagprop: Dis-criminative metric learning in nearest neighbor mod-els for image auto-annotation.
In IEEE 12th Interna-tional Conference on Computer Vision, pages 309?316, Kyoto, Japan.Micah Hodosh, Peter Young, and Julia Hockenmaier.2013.
Framing Image Description as a RankingTask : Data , Models and Evaluation Metrics.
Jour-nal of Artificial Intelligence Research, 47:853?899.Laurent Itti, Christof Koch, and Ernst Niebur.
1998.A model of saliency-based visual attention for rapidscene analysis.
IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, 20(11):1254?1259.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, SimingLi, Yejin Choi, Alexander C. Berg, and Tamara L.Berg.
2011.
Baby talk: Understanding and gener-ating simple image descriptions.
In The 24th IEEEConference on Computer Vision and Pattern Recog-nition, pages 1601?1608, Colorado Springs, Col-orado, U.S.A.Polina Kuznetsova, Vicente Ordonez, Alexander C.Berg, Tamara L. Berg, and Yejin Choi.
2012.
Col-lective Generation of Natural Image Descriptions.In Proceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics, pages 359?368, Jeju Island, South Korea.Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-der C. Berg, and Yejin Choi.
2011.
Composingsimple image descriptions using web-scale n-grams.In Fifteenth Conference on Computational NaturalLanguage Learning, pages 220?228, Portland, Ore-gon, U.S.A.Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-maguchi, Karl Stratos, Alyssa Mensch, Alex Berg,Tamara Berg, and Hal Daum?e III.
2012.
Midge :Generating Image Descriptions From Computer Vi-sion Detections.
In Proceedings of the 13th Con-ference of the European Chapter of the Associationfor Computational Linguistics, pages 747?756, Avi-gnon, France.Aude Oliva and Antonio Torralba.
2001.
Modeling theShape of the Scene: A Holistic Representation ofthe Spatial Envelope.
International Journal of Com-puter Vision, 42(3):145?175.Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.2011.
Im2Text: Describing Images Using 1 MillionCaptioned Photographs.
In Advances in Neural In-formation Processing Systems 24, Granada, Spain.Ehud Reiter and A Belz.
2009.
An investigation intothe validity of some metrics for automatically evalu-ating natural language generation systems.
Compu-tational Linguistics, 35(4):529?558.Antonio Torralba, Aude Oliva, Monica S. Castelhano,and John M. Henderson.
2006.
Contextual guid-ance of eye movements and attention in real-worldscenes: the role of global features in object search.Psychologial Review, 113(4):766?786.Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yian-nis Aloimonos.
2011.
Corpus-Guided SentenceGeneration of Natural Images.
In Proceedings of the2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 444?454, Edinburgh,Scotland, UK.457
