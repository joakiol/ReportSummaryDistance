Statistical Machine Translation Models for Personalized SearchRohini U ?AOL India R& DBangalore, IndiaRohini.uppuluri@corp.aol.comVamshi AmbatiLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, USAvamshi@cs.cmu.eduVasudeva VarmaLTRC, IIIT HydHyderabad, Indiavv@iiit.ac.inAbstractWeb search personalization has been wellstudied in the recent few years.
Relevancefeedback has been used in various ways toimprove relevance of search results.
In thispaper, we propose a novel usage of rele-vance feedback to effectively model the pro-cess of query formulation and better char-acterize how a user relates his query to thedocument that he intends to retrieve usinga noisy channel model.
We model a userprofile as the probabilities of translation ofquery to document in this noisy channel us-ing the relevance feedback obtained from theuser.
The user profile thus learnt is appliedin a re-ranking phase to rescore the searchresults retrieved using an underlying searchengine.
We evaluate our approach by con-ducting experiments using relevance feed-back data collected from users using a pop-ular search engine.
The results have shownimprovement over baseline, proving that ourapproach can be applied to personalizationof web search.
The experiments have alsoresulted in some valuable observations thatlearning these user profiles using snippetssurrounding the results for a query gives bet-ter performance than learning from entiredocument collection.1 IntroductionMost existing text retrieval systems, including theweb search engines, suffer from the problem of ?one?This work was done when the first and second authors wereat IIIT Hyderabad, India.size fits all?
: the decision of which documents to re-trieve is made based only on the query posed, with-out consideration of a particular user?s preferencesand search context.
When a query (e.g.
?jaguar?)
isambiguous, the search results are inevitably mixedin content (e.g.
containing documents on the jaguarcat and on the jaguar car), which is certainly non-optimal for a given user, who is burdened by havingto sift through the mixed results.
In order to opti-mize retrieval accuracy, we clearly need to model theuser appropriately and personalize search accordingto each individual user.
The major goal of person-alized search is to accurately model a user?s infor-mation need and store it in the user profile and thenre-rank the results to suit to the user?s interests usingthe user profile.
However, understanding a user?s in-formation need is, unfortunately, a very difficult taskpartly because it is difficult to model the search pro-cess which is a cognitive process and partly becauseit is difficult to characterize a user and his prefer-ences and goals.
Indeed, this has been recognized asa major challenge in information retrieval research(et.
al, 2003).In order to address the problem of personalizationone needs to clearly understand the actual process ofsearch.
First the user has an information need thathe would like to fulfill.
He is the only entity in theprocess that knows the exact information he needsand also has a vague notion of the document thatcan full fill his specific information need.
A querybased search engine is at his disposal for identifyingthis particular document or set of documents fromamong a vast repository of them.
He then formu-lates a query that he thinks is congruent to the doc-ument he imagines to fulfill his need and poses it tothe search engine.
The search engine now returns521a list of results that it calculates as relevant accord-ing to its ranking algorithm.
Every user is differentand has a different information need, perhaps over-lapping sometimes.
The way a user conceives anideal document that fulfills his need also varies.
It isour hypothesis that if one can learn the variations ofeach user in this direction, effective personalizationcan be done.Most approaches to personalization have triedto model the user?s interests by requesting explicitfeedback from the user during the search processand observing these relevance judgments to modelthe user?s interests.
This is called relevance feed-back, and personalization techniques using it havebeen proven to be quite effective for improving re-trieval accuracy (Salton and Buckley, 1990; Roc-chio, 1971).
These approaches to personalizationhave considered, user profile to be a collection ofwords, ontology, a matrix etc.We use relevance feedback for personalization inour approach.
However we propose a novel usage ofrelevance feedback to effectively model the processof query formulation and better characterize how auser relates his query to the document that he in-tends to retrieve as discussed in the web search pro-cess above.
A user profile learnt from the relevancefeedback that captures the query generation processis used as a guide to understand user?s interests overtime and personalize his web search results.Interestingly, a new paradigm has been proposedfor retrieval rooted from statistical language mod-eling recently that views the query generation pro-cess through a Noisy channel model (Berger andLafferty, 1999) .
It was assumed that the docu-ment and query are from different languages andthe query generation process was viewed as a trans-lation from the document language which is moreverbose to the language of the query which is morecompact and brief.
The noisy channel model pro-posed by Berger and Lafferty (Berger and Lafferty,1999) inherently captures the dependencies betweenthe query and document words by learning a trans-lation model between them.
As we intend to achievepersonalized search by personalizing the query for-mulation process, we also perceive the user profilelearning through a Noisy Channel Model.
In themodel, when a user has an information need, he alsohas an ideal document in mind that fulfills his need.The user tries to in a way translate the notion ofthe ideal document into a query that is more com-pact but congruent to the document.
He then posesthis query to the search engine and retrieves the re-sults.
By observing this above process over time,we can capture how the user is generating a queryfrom his ideal document.
By learning this model of auser, we can predict which document best describeshis information need for the query he poses.
Thisis the motive of personalization.
In our approach,we learn a user model which is probabilistic modelfor the noisy channel using statistical translation ap-proaches and from the past queries and their corre-sponding relevant documents provided as feedbackby the user.The rest of the paper is organized as follows.We first describe the related work on personalizedsearch then we provide the background and theframework that our approach is based upon.
wediscuss the modeling of a user profile as a transla-tion model.
after which we describe applying it topersonalized search.
we describe our experimentalresults followed by conclusions with directions tosome future work.2 Related WorkThere has been a growing literature available withregard to personalization of search results.
In thissection, we briefly overview some of the availableliterature.
(Pretschner and Gauch, 1999) used ontology tomodel users interests, which are studied from usersbrowsed web pages.
(Speretta and Gauch, 2004)used users search history to construct user profiles.
(Liu et al, 2002) performed personalized web searchby mapping a query to a set of categories using auser profile and a general profile learned from theuser?s search history and a category hierarchy re-spectively.
(Hatano and Yoshikawa., 2004) consid-ered the unseen factors of the relationship betweenthe web users behaviors and information needs andconstructs user profiles through a memory-basedcollaborative filtering approach.To our knowledge, there has been a very littlework has been done that explicitly uses languagemodels to personalization of search results.
(Croftet al, 2001) discuss about relevance feedback and522query expansion using language modeling.
(Shen etal., 2005) use language modeling for short term per-sonalization by expanding queries.Earlier approaches to personalization have con-sidered, user profile to be a collection of words, on-tology, language model etc.
We perceive the userprofile learning through a Noisy Channel Model.
Inthe model, when a user has an information need, healso has a vague notion of what is the ideal documentthat he would like to retrieve.
The user then createsa compact query that he thinks would retrieve thedocument.
He then poses the query to the search en-gine.
By observing this above process over time, welearn a user profile as the probabilities of translationfor the noisy channel that converts his document tothe query.
We then use this profile in re-ranking theresults of a search engine to provide personalized re-sults.3 BackgroundIn this section, we describe the statistical languagemodeling and the translation model framework forinformation retrieval that form a basis for our re-search.The basic approach for language modeling for IRwas proposed by Ponte and Croft (Ponte and Croft,1998).
It assumes that the user has a reasonable ideaof the terms that are likely to appear in the ideal doc-ument that can satisfy his/her information need, andthat the query terms the user chooses can distinguishthe ideal document from the rest of the collection.The query is thus generated as the piece of text rep-resentative of the ideal document.
The task of thesystem is then to estimate, for each of the documentsin the collection, which is most likely to be the idealdocument.argmaxDP (D|Q) = argmaxDP (Q|D)P (D)where Q is a query and D is a document.
The priorprobability P (D) is usually assumed to be uniformand a language model P (Q|D) is estimated for ev-ery document.
In other words, they estimate a prob-ability distribution over words for each documentand calculate the probability that the query is a sam-ple from that distribution.
Documents are rankedaccording to this probability.
The basic model hasbeen extended in a variety of ways.
Modeling doc-uments as in terms of a noisy channel model byBerger & Lafferty (Berger and Lafferty, 1999), mix-ture of topics, and phrases are considered (Song andCroft., 1999), (Lavrenko and Croft, 2001) explicitlymodels relevance, and a risk minimization frame-work based on Bayesian decision theory has beendeveloped (Zhai and Lafferty, 2001).The noisy channel by Berger and Lafferty (Bergerand Lafferty, 1999) view a query as a distilla-tion or translation from a document describing thequery generation process in terms of a noisy channelmodel.
In formulating a query to a retrieval system,a user begins with an information need.
This infor-mation need is then represented as a fragment of an?ideal document?, a portion of the type of documentthat the user hopes to receive from the system.
Theuser then translates or ?distills?
this ideal documentfragment into a succinct query, selecting key termsand replacing some terms with related terms.To determine the relevance of a document to aquery, their model estimates the probability that thequery would have been generated as a translationof that document.
Documents are then ranked ac-cording to these probabilities.
More specifically,the mapping from a document term w to a queryterm qi is achieved by estimating translation mod-els P (q|w).
Using translation models, the retrievalmodel becomesP (Q|D) =?qi?Q?P (qi|GE)+(1??
)?w?DP (qi|w)P (w|D)where P (qi|GE) is the smoothed or generalprobability obtained from a large general corpus.P (qi|w) is an entry in the translation model.
It repre-sents the probability of generation of the query wordqi for a word w in the document.
P (w|D) is theprobability of the word w in the document and ?
isa weighting parameter which lies between 0 and 1.4 User Profile as a Translation ModelWe perceive the user profile learning as learningthe channel probabilities of a Noisy Channel Modelthat generates the query from the document.
In themodel, when a user has an information need, he alsohas a vague notion of what is the ideal documentthat he would like to retrieve.
The user then creates523a compact query that he thinks would retrieve thedocument.
He then poses the query to the search en-gine.
By observing this above process over time, wecan learn how the user is generating a query fromhis notion of an ideal document.
By learning this,we can predict which document best describes hisinformation need.
The learnt model, called a userprofile, is thus capable of personalizing results forthat particular user.
Hence, the user profile here isa translation model learnt from explicit feedback ofthe user using statistical translation approaches.
Ex-plicit feedback consists of the past queries and theircorresponding relevant documents provided as feed-back by the user.
A translation model is a proba-bilistic model consisting of the triples, the sourceword, the target word and the probability of trans-lation.
The translation model here is between doc-ument words and queries words.
Therefore the userprofile as a translation model in our approach willconsist of triples of a document word, a query wordand the probability of the document word generatingthe query word.5 Personalized SearchIn this section, we describe how we perform person-alized search using the proposed translation modelbased user profile.
First, a user profile is learnt usingthe translation model process then the re-ranking isdone using the learnt user profile.5.1 Learning user profileIn our approach, a user profile consists of a statisti-cal translation model.
A translation model is a prob-abilistic model consisting of the triples, the sourceword, the target word and the probability of trans-lation.
Our user profiles consists of the followingtriples, a document word, a query word and the prob-ability of the document word generating the queryword.Consider a user u, let { {Qi, Di}, i = 1, 2, ..., N}represent the past history of the user u. where Qiis the query and Di is the concatenation of all therelevant documents for the query Qi and let Di ={w1, w2, ..., wn} be the words in it.
The user profilelearnt from the past history of user consists of thefollowing triples of the form (q, wi, p(q|wi)) whereq is a word in the query Qi and wi is a word in thedocument Di.Translation model is typically learnt from paral-lel texts i.e a set of translation pairs consisting ofsource and target language sentences.
In learningthe user profile, we first extract parallel texts fromthe past history of the user and then learn the trans-lation model which is essentially the user profile.
Inthe subsections below, we describe the process in de-tail.5.1.1 Extracting Parallel TextsBy viewing documents as samples of a verboselanguage and the queries as samples of a conciselanguage, we can treat each document-query pair asa translation pair, i.e.
a pair of texts written in theverbose language and the concise language respec-tively.
The extracted parallel texts consists of pairsof the form {Qi, Drel} where Drel is the concatena-tion of contexts extracted from all relevant documentfor the query Qi.We believe that short snippets extracted in thecontext of the query would be better candidates forDrel than using the whole document.
This is be-cause there can be a lot of noisy terms which neednot right in the context of the query.
We believe ashort snippet usually N (we considered 15) wordsto the left and right of the query words, similar to ashort snippet displayed by search engines can bet-ter capture the context of the query.
In deed weexperimented with different context sizes for Drel.The first is using the whole document i.e., consider-ing the query and concatenation of all the relevantdocuments as a pair in the parallel texts extractedwhich is called Ddocuments The second is using justa short text snippet from the document in the con-text of query instead of the whole document whichis called Dsnippets Details are described in the ex-periments section.5.1.2 Learning Translation ModelAccording to the standard statistical translationmodel (Brown et al, 1993), we can find the optimalmodel M?
by maximizing the probability of gener-ating queries from documents orM?
= argmaxMN?i=1P (Qi|Di,M)524qw dw P(qw|dw,u)journal kdd 0.0176journal conference 0.0123journal journal 0.0176journal sigkdd 0.0088journal discovery 0.0211journal mining 0.0017journal acm 0.0088music music 0.0375music purchase 0.0090music mp3 0.0090music listen 0.0180music mp3.com 0.0450music free 0.0008Table 1: Sample user profileTo find the optimal word translation probabilitiesP (qw|dw,M?
), we can use the EM algorithm.
Thedetails of the algorithm can be found in the literaturefor statistical translation models, such as (Brown etal., 1993).IBM Model1 (Brown et al, 1993) is a simplisticmodel which takes no account of the subtler aspectsof language translation including the way word or-der tends to differ across languages.
Similar to ear-lier work (Berger and Lafferty, 1999), we use IBMModel1 because we believe it is more suited for IRbecause the subtler aspects of language used for ma-chine translation can be ignored for IR.
GIZA++(Och and Ney, 2003), an open source tool which im-plements the IBM Models which we have used inour work for computing the translation probabilities.A sample user profile learned is shown in Table 1.5.2 Re-rankingRe-ranking is a phase in personalized search wherethe set of documents matching the query retrievedby a general search engine are re-scored using theuser profile and then re-ranked in descending orderof rank of the document.
We follow a similar ap-proach in our work.Let D be set of all the documents returned by thesearch engine.
The rank of each document D re-turned for a query Q for user u is computing usinghis user profile as shown in Equation 1.P (Q|D,u) =?qi?Q?P (qi|GE)+(1??
)?w?DP (qi|w, u)P (w|D)(1)where P (qi|GE) is the smoothed or generalprobability obtained from a large general corpus.P (qi|w, u) is an entry in the translation model of theuser.
It represents the probability of generation ofthe query word qi for a word w in the document.P (w|D) is the probability of the word w in the doc-ument and ?
is a weighting parameter which lies be-tween 0 and 1.6 ExperimentsWe performed experiments evaluating our approachon data set consisting of 7 users.
Each user submit-ted a number of queries to a search engine (Google).For each query, the user examined the top 10 docu-ments and identified the set of relevant documents.Table 2 gives the statistics of the data sets.
There isno repetition of query for any user though repetitionof some words in the query exists (see Table 2).
Thedocument collection consists of top 20 documentsfrom google which is actually the set of documentsseen by the user while accessing the relevance of thedocuments.
In all, the total size of the documentcollection was 3,469 documents.
We did not includedocuments of type doc and pdf files.To evaluate our approach, we use the 10-foldcross-validation strategy (Mitchell, 1997).
We di-vide the data of each user into 10 sets each hav-ing (approximately) equal number of search queries(For example, for user1 had 37 queries in total, wedivided this into 10 sets with 4 queries each approx-imately).
Learning of user profile is done 10 times,each time leaving out one of the sets from training,but using only the omitted subset for testing.
Per-formance is computed in the testing phase for eachtime and average of the 10 times is taken.
In thetesting phase, we take each query and re rank theresults using the proposed approach using his pro-file learned from nine other sets.
For measuringperformance for each query, we compute Precision@10 (P@10), a widely used metric for evaluatingpersonalized search algorithms.
It is defined as theproportion of relevant documents among the top 10results for the given ranking of documents.
P@10is computed by comparing with the relevant docu-ments present in the data.
All the values presentedin the tables are average values which are averagedover all queries for each user, unless otherwise spec-ified.
We used Lucene1, an open source search en-gine as the general search engine to first retrieve a1http://lucene.apache.org525User No.
Q % of Unique Total Rel Avg.
Relwords in Q1 37 89 236 6.3782 50 68.42 178 3.563 61 82.63 298 4.8854 26 86.95 101 3.8845 33 80.76 134 4.066 29 78.08 98 3.3797 29 88.31 115 3.965Table 2: Statistics of the data set of 7 usersset of results matching the query.6.0.1 Comparison with Contextless RankingWe test the effectiveness of our user profile bycomparing with a contextless ranking algorithm.
Weused a generative language modeling for IR as thecontext less ranking algorithm (Query Likelihoodmodel (Ponte and Croft, 1998; Song and Croft.,1999)).
This is actually the simplest version of themodel described in Equation 1.
Each word w can betranslated only as itself that is the translation proba-bilities (see Equation 1) are ?diagonal?.P (qi|w, u) ={1 if q = w0 OtherwiseThis serves as a good baseline for us to see howwell the translation model actually captured the userinformation.
For fair testing similar to our approach,for each query, we first retrieve results matchinga query using a general search engine (Lucene).Then we rank the results using the formula shownin Equation 2.P (Q|D) =?qi?Q?P (qi|GE)+(1??
)P (qi|D) (2)We used IBM Model1 for learning the translationmodel (i.e., the user profile).
The general Englishprobabilities are computed from all the documents inthe lucene?s index.
Similar to earlier works (Bergerand Lafferty, 1999), we simply set the value of ?
tobe 0.05.
The values reported are P@10 values aver-age over all 10 sets and the queries for the respec-tive user.
Table 3 clearly shows the improvementbrought in by the user profile.6.0.2 Experiments with Different ModelsWe performed an experiment to see if differenttraining models for learning the user profile affectedSet Contextless ProposedUser1 0.1433 0.1421User2 0.1426 0.2445User3 0.1016 0.1216User4 0.0557 0.1541User5 0.1877 0.3933User6 0.1566 0.3941User7 0.1 0.1833Avg 0.1268 0.2332Table 3: Precision @10 results for 7 usersTraining Model Document Test Snippet TestIBM Model1Document Train 0.2062 0.2028Snippet Train 0.2333 0.2488GIZA++Document Train 0.1799 0.1834Snippet Train 0.2075 0.2034Table 4: Summary of Comparison of different Mod-els and Contexts for learning user profilethe performance.
We experimented with two mod-els.
The first is a basic model and used in ear-lier work, IBM Model1.
The second is using theGIZA++ default parameters.
We observed that userprofile learned using IBM Model1 outperformedthat using GIZA++ default parameters.
We believethis is because, IBM Model1 is more suited for IRbecause the subtler aspects of language used for ma-chine translation (which are used in GIZA++ defaultparameters) can be ignored for IR.
We obtained anaverage P@10 value of 0.2333 for IBM Model1 and0.2075 for GIZA++.6.0.3 Snippet Vs DocumentIn extracting parallel texts consists of pairs of theform {Qi, Drel} where Drel is the concatenation ofcontexts extracted from all relevant document forthe queryQi we experimented with different contextsizes for Drel.We believe that a short snippet extracted in thecontext of the query would be better candidate forDrel than using the whole document.
This is be-cause there can be a lot of noisy terms which neednot useful in the context of the query.
We believea short snippet usually N (we considered 15) wordsto the left and right of the query words, similar to ashort snippet displayed by search engines can better526Figure 1: Comparison of Snippet Vs DocumentTraining using IBM Model1 for training.IBM Model1 : I - Document Training and Document Testing,IBM Model1 : II - Document Training and Snippet Testing,IBM Model1 : III - Snippet Training and Document Testing,IBM Model1 : IV - Snippet Training and Snippet Testingcapture the context of the query.We experimented with two context sizes.
Thefirst is using the whole document i.e., consideringthe query and concatenation of all the relevant doc-uments as a pair in the parallel texts extracted whichis calledDdocuments.
The second is using just a shorttext snippet from the document in the context ofquery instead of the whole document which is calledDsnippets.
The user profile learning from pairs ofparallel texts {Q,Ddocuments} is called DocumentTrain.
The user profile learning from pairs of paral-lel texts {Q,Dsnippets} is called Snippet Train.
Theuser profiles are trained using both IBM Model1 andGIZA++ and comparison of the two is shown in Ta-ble 4.We also experimented with the size of the contextused for testing.
Using the document for re-rankingas shown in Equation 1 (called Document Test) 2 andusing just a short snippet extracted from the docu-ment for testing (called Snippet Test).
Table 4 showsthe average P@10 over the 10 sets and all queriesand users.We observed that, not only did the model usedfor training affected P@10, but also the data used intraining and testing, whether it was a snippet or doc-ument, showed a large variation in the performance.Training using IBM Model1 using the snippet and2It is to be noted that Snippet Train and Document Test andtraining using IBM Model1 is the default configuration used forall the reported results unless explicitly specified.Figure 2: Comparison of Snippet Vs DocumentTraining using GIZA++ Default parameters fortraining.GIZA++:I - Document Training and Document Testing,GIZA++:II - Document Training and Snippet Testing,GIZA++:III - Snippet Training and Document Testing,GIZA++:IV - Snippet Training and Snippet Testingtesting using snippet achieved the best results.
Thisis in agreement with the discussion that the snip-pet surrounding the query captures the context ofthe query better than a document which may con-tain many words that could possibly be unrelated tothe query, therefore diluting the strength of the mod-els learnt.
The detailed results for all the users areshown in Figure 1 and Figure 2.7 Conclusions and Future WorkRelevance feedback from the user has been used invarious ways to improve the relevance of the re-sults for the user.
In this paper we have proposeda novel usage of relevance feedback to effectivelymodel the process of query formulation and bettercharacterize how a user relates his query to the doc-ument that he intends to retrieve.
We applied a noisychannel model approach for the query and the doc-uments in a retrieval process.
The user profile wasmodeled using the relevance feedback obtained fromthe user as the probabilities of translation of queryto document in this noisy channel.
The user pro-file thus learnt was applied in a re-ranking phase torescore the search results retrieved using general in-formation retrieval models.
We evaluate the usage ofour approach by conducting experiments using rele-vance feedback data collected from users of a popu-lar search engine.
Our experiments have resulted in527some valuable observations that learning these userprofiles using snippets surrounding the results fora query show better performance than when learn-ing from entire documents.
In this paper, we haveonly evaluated explicit relevance feedback gatheredfrom a user and performed our experiments.
As partof future work, we would like to evaluate our ap-proach on implicit feedback gathered probably asclick-through data in a search engine, or on the clientside using customized browsers.ReferencesAdam Berger and John D. Lafferty.
1999.
Informationretrieval as statistical translation.
In Research and De-velopment in Information Retrieval, pages 222?229.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Comput.
Linguist., 19(2):263?311.W.
Bruce Croft, Stephen Cronen-Townsend, and VictorLarvrenko.
2001.
Relevance feedback and person-alization: A language modeling perspective.
In DE-LOS Workshop: Personalisation and RecommenderSystems in Digital Libraries.Jamie Allan et.
al.
2003.
Challenges in information re-trieval language modeling.
In SIGIR Forum, volume37 Number 1.K.
Sugiyama K. Hatano and M. Yoshikawa.
2004.
Adap-tive web search based on user profile constructed with-out any effort from users.
In Proceedings of WWW2004, page 675 684.Victor Lavrenko and W. Bruce Croft.
2001.
Relevance-based language models.
In Research and Developmentin Information Retrieval, pages 120?127.F.
Liu, C. Yu, and W. Meng.
2002.
Personalized websearch by mapping user queries to categories.
In Pro-ceedings of the eleventh international conference onInformation and knowledge management, ACM Press,pages 558?565.Tom Mitchell.
1997.
Machine Learning.
McGrawHill.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Jay M. Ponte and W. Bruce Croft.
1998.
A lan-guage modeling approach to information retrieval.
InResearch and Development in Information Retrieval,pages 275?281.A.
Pretschner and S. Gauch.
1999.
Ontology based per-sonalized search.
In ICTAI., pages 391?398.J.
J. Rocchio.
1971.
Relevance feedback in informationretrieval, the smart retrieval system.
Experiments inAutomatic Document Processing, pages 313?323.G.
Salton and C. Buckley.
1990.
Improving retrieval per-formance by relevance feedback.
Journal of the Amer-ican Society of Information Science, 41:288?297.Xuehua Shen, Bin Tan, and Chengxiang Zhai.
2005.
Im-plicit user modeling for personalized search.
In Pro-ceedings of CIKM 2005.F.
Song and W. B. Croft.
1999.
A general languagemodel for information retrieval.
In Proceedings onthe 22nd annual international ACM SIGIR conference,page 279280.Micro Speretta and Susan Gauch.
2004.
Personalizingsearch based on user search histories.
In ThirteenthInternational Conference on Information and Knowl-edge Management (CIKM 2004).Chengxiang Zhai and John Lafferty.
2001.
A study ofsmoothing methods for language models applied to adhoc information retrieval.
In Proceedings of ACM SI-GIR?01, pages 334?342.528
