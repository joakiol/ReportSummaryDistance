Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 159?164,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAutomatic Evaluation of Chinese Translation Output:Word-Level or Character-Level?Maoxi Li    Chengqing Zong Hwee Tou NgNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy ofSciences, Beijing, China, 100190Department of Computer ScienceNational University of Singapore13 Computing Drive, Singapore 117417{mxli, cqzong}@nlpr.ia.ac.cn                      nght@comp.nus.edu.sgAbstractWord is usually adopted as the smallest unit inmost tasks of Chinese language processing.However, for automatic evaluation of the quali-ty of Chinese translation output when translat-ing from other languages, either a word-levelapproach or a character-level approach is possi-ble.
So far, there has been no detailed study tocompare the correlations of these two ap-proaches with human assessment.
In this paper,we compare word-level metrics with character-level metrics on the submitted output of Eng-lish-to-Chinese translation systems in theIWSLT?08 CT-EC and NIST?08 EC tasks.
Ourexperimental results reveal that character-levelmetrics correlate with human assessment betterthan word-level metrics.
Our analysis suggestsseveral key reasons behind this finding.1 IntroductionWhite space serves as the word delimiter in Latinalphabet-based languages.
However, in writtenChinese text, there is no word delimiter.
Thus, inalmost all tasks of Chinese natural languageprocessing (NLP), the first step is to segment aChinese sentence into a sequence of words.
This isthe task of Chinese word segmentation (CWS), animportant and challenging task in Chinese NLP.Some linguists believe that word (containing atleast one character) is the appropriate unit for Chi-nese language processing.
When treating CWS as astandalone NLP task, the goal is to segment a sen-tence into words so that the segmentation matchesthe human gold-standard segmentation with thehighest F-measure, but without considering theperformance of the end-to-end NLP applicationthat uses the segmentation output.
In statisticalmachine translation  (SMT), it can happen that themost accurate word segmentation as judged by thehuman gold-standard segmentation may notproduce the best translation output (Zhang et al,2008).
While state-of-the-art Chinese wordsegmenters achieve high accuracy, some errors stillremain.Instead of segmenting a Chinese sentence intowords, an alternative is to split a Chinese sentenceinto characters, which can be readily done withperfect accuracy.
However, it has been reportedthat a Chinese-English phrase-based SMT system(Xu et al, 2004) that relied on characters (withoutCWS) performed slightly worse than when it usedsegmented words.
It has been recognized that vary-ing segmentation granularities are needed for SMT(Chang et al, 2008).To evaluate the quality of Chinese translationoutput, the International Workshop on SpokenLanguage Translation in 2005 (IWSLT'2005) usedthe word-level BLEU metric (Papineni et al,2002).
However, IWSLT'08 and NIST'08 adoptedcharacter-level evaluation metrics to rank the sub-mitted systems.
Although there is much work onautomatic evaluation of machine translation (MT),whether word or character is more suitable for au-tomatic evaluation of Chinese translation outputhas not been systematically investigated.In this paper, we utilize various machine transla-tion evaluation metrics to evaluate the quality ofChinese translation output, and compare their cor-relation with human assessment when the Chinesetranslation output is segmented into words versuscharacters.
Since there are several CWS tools thatcan segment Chinese sentences into words andtheir segmentation results are different, we use fourrepresentative CWS tools in our experiments.
Ourexperimental results reveal that character-level me-159trics correlate with human assessment better thanword-level metrics.
That is, CWS is not essentialfor automatic evaluation of Chinese translationoutput.
Our analysis suggests several key reasonsbehind this finding.2 Chinese Translation EvaluationAutomatic MT evaluation aims at formulating au-tomatic metrics to measure the quality of MT out-put.
Compared with human assessment, automaticevaluation metrics can assess the quality of MToutput quickly and objectively without much hu-man labor.Figure 1.
An example to show an MT system translationand multiple reference translations being segmented intocharacters or words.To evaluate English translation output, automat-ic MT evaluation metrics take an English word asthe smallest unit when matching a system transla-tion and a reference translation.
On the other hand,to evaluate Chinese translation output, the smallestunit to use in matching can be a Chinese word or aChinese character.
As shown in Figure 1, given anEnglish sentence ?how much are the umbrellas??
aChinese system translation (or a reference transla-tion) can be segmented into characters (Figure 1(a))or words (Figure 1(b)).A variety of automatic MT evaluation metricshave been developed over the years, includingBLEU (Papineni et al, 2002), NIST (Doddington,2002), METEOR (exact) (Banerjee and Lavie,2005), GTM (Melamed et al, 2003), and TER(Snover et al, 2006).
Some automatic MT evalua-tion metrics perform deeper linguistic analysis,such as part-of-speech tagging, synonym matching,semantic role labeling, etc.
Since part-of-speechtags are only defined for Chinese words and not forChinese characters, we restrict the automatic MTevaluation metrics explored in this paper to thosemetrics listed above which do not require part-of-speech tagging.3 CWS ToolsSince there are a number of CWS tools and theygive different segmentation results in general, weexperimented with four different CWS tools in thispaper.ICTCLAS: ICTCLAS has been successfully usedin a commercial product (Zhang et al, 2003).
Theversion we adopt in this paper is ICTCLAS2009.NUS Chinese word segmenter (NUS): The NUSChinese word segmenter uses a maximum entropyapproach to Chinese word segmentation, whichachieved the highest F-measure on three of the fourcorpora in the open track of the Second Interna-tional Chinese Word Segmentation Bakeoff (Ngand Low, 2004; Low et al, 2005).
The segmenta-tion standard adopted in this paper is CTB (Chi-nese Treebank).Stanford Chinese word segmenter(STANFORD): The Stanford Chinese word seg-menter is another well-known CWS tool (Tseng etal., 2005).
The version we used was released on2008-05-21 and the standard adopted is CTB.Urheen: Urheen is a CWS tool developed by(Wang et al, 2010a; Wang et al, 2010b), and itoutperformed most of the state-of-the-art CWSsystems in the CIPS-SIGHAN?2010 evaluation.This tool is trained on Chinese Treebank 6.0.4 Experimental Results4.1  DataTo compare the word-level automatic MT evalua-tion metrics with the character-level metrics, weconducted experiments on two datasets, in the spo-ken language translation domain and the newswiretranslation domain.Translation: ?_?_?_?_?_?_?Ref 1:  ?_?_?_?_?_?_?_??
?Ref 7:  ?_?_?_?_?_?_?_?_?_?_?
(a) Segmented into characters.Translation: ?
?_?_?_?_?_?Ref  1:   ??_??_??_?_??
?Ref  7:   ??_??_?_??_?_??_?
(b) Segmented into words by Urheen.160The IWSLT'08 English-to-Chinese ASR chal-lenge task evaluated the translation quality of 7machine translation systems (Paul, 2008).
The testset contained 300 segments with human assess-ment of system translation quality.
Each segmentcame with 7 human reference translations.
Humanassessment of translation quality was carried outon the fluency and adequacy of the translations, aswell as assigning a rank to the output of each sys-tem.
For the rank judgment, human graders wereasked to "rank each whole sentence translationfrom best to worst relative to the other choices"(Paul, 2008).
Due to the high manual cost, the flu-ency and adequacy assessment was limited to theoutput of 4 submitted systems, while the humanrank assessment was applied to all 7 systems.Evaluation based on ranking is reported in this pa-per.
Experimental results on fluency and adequacyjudgment also agree with the results on humanrank assessment, but are not included in this paperdue to length constraint.The NIST'08 English-to-Chinese translation taskevaluated 127 documents with 1,830 segments.Each segment has 4 reference translations and thesystem translations of 11 MT systems, released inthe corpus LDC2010T01.
We asked native speak-ers of Chinese to perform fluency and adequacyjudgment on a five-point scale.
Human assessmentwas done on the first 30 documents (355 segments)(document id ?AFP_ENG_20070701.0026?
to?AFP_ENG_20070731.0115?).
The method ofmanually scoring the 11 submitted Chinese systemtranslations of each segment is the same as thatused in (Callison-Burch et al, 2007).
The adequa-cy score indicates the overlap of the meaning ex-pressed in the reference translations with a systemtranslation, while the fluency score indicates howfluent a system translation is.4.2  Segment-Level Consistency or Correla-tionFor human fluency and adequacy judgments, thePearson correlation coefficient is used to computethe segment-level correlation between humanjudgments and automatic metrics.
Human rankjudgment is not an absolute score and thus Pearsoncorrelation coefficient cannot be used.
We calcu-late segment-level consistency as follows:--The consistent number of pair wise comparisonsThe total number of pair wise comparisons?
?Ties are excluded in pair-wise comparison.Table 1 and 2 show the segment-level consisten-cy or correlation between human judgments andautomatic metrics.
The ?Character?
row shows thesegment-level consistency or correlation betweenhuman judgments and automatic metrics after thesystem and reference translations are segmentedinto characters.
The ?ICTCLAS?, ?NUS?,?STANFORD?, and ?Urheen?
rows show thescores when the system and reference translationsare segmented into words by the respective Chi-nese word segmenters.The character-level metrics outperform the bestword-level metrics by 2?5% on the IWSLT?08CT-EC task, and 4?13% on the NIST?08 EC task.Method BLEU NIST METEOR GTM 1?
TERCharacter 0.69  0.73  0.74  0.71 0.60ICTCLAS 0.64  0.70  0.69  0.66 0.57NUS 0.64 0.71 0.70 0.65 0.55STANFORD 0.64  0.69  0.69  0.64 0.54Urheen 0.63  0.70  0.68  0.65 0.55Table 1.
Segment-level consistency on IWSLT?08 CT-EC.Method BLEU NIST METEOR GTM 1?
TERCharacter 0.63 0.61 0.65 0.61 0.60ICTCLAS 0.49 0.56 0.59 0.55 0.51NUS 0.49 0.57 0.58 0.54 0.51STANFORD 0.50 0.57 0.59 0.55 0.50Urheen 0.49 0.56 0.58 0.54 0.51Table 2.
Average segment-level correlation on NIST?08EC.4.3  System-Level CorrelationWe measure correlation at the system level usingSpearman's rank correlation coefficient.
The sys-tem-level correlations of word-level metrics andcharacter-level metrics are summarized in Table 3and 4.Because there are only 7 systems that have hu-man assessment in the IWSLT?08 CT-EC task, thegap between character-level metrics and word-level metrics is very small.
However, it still showsthat character-level metrics perform no worse thanword-level metrics.
For the NIST?08 EC task, thesystem translations of the 11 submitted MT sys-tems were assessed manually.
Except for the GTMmetric, character-level metrics outperform word-161level metrics.
For BLEU and TER, character-levelmetrics yield up to 6?9% improvement over word-level metrics.
This means the character-level me-trics reduce about 2?3 erroneous system rankings.When the number of systems increases, the differ-ence between the character-level metrics and word-level metrics will become larger.Method BLEU NIST METEOR GTM 1?
TERCharacter 0.96  0.93  0.96  0.93 0.96ICTCLAS 0.96  0.93  0.89  0.93 0.96NUS 0.96 0.93 0.89 0.86 0.96STANFORD 0.96  0.93  0.89  0.86 0.96Urheen 0.96  0.93  0.89  0.86 0.96Table 3.
System-level correlation on IWSLT?08 CT-EC.Method BLEU NIST METEOR GTM 1?
TERCharacter 0.97 0.98 1.0 0.99 0.86ICTCLAS 0.91 0.96 0.99 0.99 0.81NUS 0.91 0.96 0.99 0.99 0.79STANFORD 0.89 0.97 0.99 0.99 0.77Urheen 0.91 0.96 0.99 0.99 0.79Table 4.
System-level correlation on NIST?08 EC.5 AnalysisWe have analyzed the reasons why character-levelmetrics better correlate with human assessmentthan word-level metrics.Compared to word-level metrics, character-levelmetrics can capture more synonym matches.
Forexample, Figure 1 gives the system translation anda reference translation segmented into words:Translation: ?
?_?_?_?_?_?Reference: ??_??_?
?_?_?The word ???
is a synonym for the word ???
?, and both words are translations of the Englishword ?umbrella?.
If a word-level metric is used,the word ???
in the system translation will notmatch the word ????
in the reference translation.However, if the system and reference translationare segmented into characters, the word ???
in thesystem translation shares the same character ??
?with the word ???
?
in the reference.
Thuscharacter-level metrics can better capture synonymmatches.We can classify the semantic relationships ofwords that share some common characters intothree types: exact match, partial match, and nomatch.
The statistics on the output translations ofan MT system are shown in Table 5.
It shows that?exact match?
accounts for 71% (29/41) and ?nomatch?
only accounts for 7% (3/41).
This meansthat words that share some common characters aresynonyms in most cases.
Therefore, character-levelmetrics do a better job at matching Chinese transla-tions.TotalcountExactmatchPartialmatch  No match41  29  9  3Table 5.
Statistics of semantic relationships on wordssharing some common characters.Another reason why word-level metrics performworse is that the segmented words in a systemtranslation may be inconsistent with the segmentedwords in a reference translation, since a statisticalword segmenter may segment the same sequenceof characters differently depending on the contextin a sentence.
For example:Translation: ?_?_??
_?_?Reference:   ?_?_?_ ?
_?_?
?_?Here the word ????
is the Chinese translationof the English word ?Kyoto?.
However, it is seg-mented into two words, ???
and ??
?, in the ref-erence translation by the same CWS tool.
Whenthis happens, a word-level metric will fail to matchthem in the system and reference translation.
Whilethe accuracy of state-of-the-art CWS tools is high,segmentation errors still exist and can cause suchmismatches.To summarize, character-level metrics cancapture more synonym matches and the resultingsegmentation into characters is guaranteed to beconsistent, which makes character-level metricsmore suitable for the automatic evaluation ofChinese translation output.6 ConclusionIn this paper, we conducted a detailed study of therelative merits of word-level versus character-levelmetrics in the automatic evaluation of Chinesetranslation output.
Our experimental results haveshown that character-level metrics correlate betterwith human assessment than word-level metrics.Thus, CWS is not needed for automatic evaluation162of Chinese translation output.
Our study providesthe needed justification for the use of character-level metrics in evaluating SMT systems in whichChinese is the target language.AcknowledgmentsThis research was done for CSIDM Project No.CSIDM-200804 partially funded by a grant fromthe National Research Foundation (NRF) adminis-tered by the Media Development Authority (MDA)of Singapore.
This research has also been fundedby the Natural Science Foundation of China underGrant No.
60975053, 61003160, and 60736014,and also supported by the External CooperationProgram of the Chinese Academy of Sciences.
Wethank Kun Wang, Daniel Dahlmeier, MatthewSnover, and Michael Denkowski for their kind as-sistance.ReferencesSatanjeev Banerjee and Alon Lavie, 2005.
METEOR:An Automatic Metric for MT Evaluation withImproved Correlation with Human Judgments.Proceedings of the ACL Workshop on Intrinsic andExtrinsic Evaluation Measures for MachineTranslation and/or Summarization, pages 65-72, AnnArbor, Michigan, USA.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz and Josh Schroeder, 2007.
(Meta-) Evaluation of Machine Translation.Proceedings of the Second Workshop on StatisticalMachine Translation, pages 136-158, Prague, CzechRepublic.Pi-Chuan Chang, Michel Galley and Christopher D.Manning, 2008.
Optimizing Chinese WordSegmentation for Machine Translation Performance.Proceedings of the Third Workshop on StatisticalMachine Translation, pages 224-232, Columbus,Ohio, USA.George Doddington, 2002.
Automatic Evaluation ofMachine Translation Quality Using N-gram Co-occurrence Statistics.
Proceedings of the SecondInternational Conference on Human LanguageTechnology Research (HLT'02), pages 138-145, SanDiego, California, USA.Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo, 2005.A Maximum Entropy Approach to Chinese WordSegmentation.
Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing, pages161-164, Jeju Island, Korea.I.
Dan Melamed, Ryan Green and Joseph P. Turian,2003.
Precision and Recall of Machine Translation.Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association forComputational Linguistics (HLT-NAACL 2003) -short papers, pages 61-63, Edmonton, Canada.Hwee Tou Ng and Jin Kiat Low, 2004.
Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-Once?Word-Based or Character-Based?
Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2004), pages 277-284, Barcelona, Spain.Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu, 2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
Proceedings ofthe 40th Annual Meeting of the Association forComputational Linguistics, pages 311-318,Philadelphia, Pennsylvania, USA.Michael Paul, 2008.
Overview of the IWSLT 2008Evaluation Campaign.
Proceedings of IWSLT 2008,pages 1-17, Hawaii, USA.Matthew Snover, Bonnie Dorr, Richard Schwartz, JohnMakhoul, Linnea Micciulla and Ralph Makhoul,2006.
A Study of Translation Edit Rate with TargetedHuman Annotation.
Proceedings of the Associationfor Machine Translation in the Americas, pages 223-231, Cambridge.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky and Christopher Manning, 2005.
AConditional Random Field Word Segmenter forSighan Bakeoff 2005.
Proceedings of the FourthSIGHAN Workshop on Chinese LanguageProcessing, pages 168-171, Jeju Island, Korea.Kun Wang, Chengqing Zong and Keh-Yih Su, 2010a.
ACharacter-Based Joint Model for Chinese WordSegmentation.
Proceedings of the 23rd InternationalConference on Computational Linguistics (COLING2010), pages 1173-1181, Beijing, China.Kun Wang, Chengqing Zong and Keh-Yih Su, 2010b.
ACharacter-Based Joint Model for CIPS-SIGHANWord Segmentation Bakeoff 2010.
Proceedings ofCIPS-SIGHAN Joint Conference on ChineseLanguage Processing (CLP2010), pages 245-248,Beijing, China.Jia Xu, Richard Zens and Hermann Ney, 2004.
Do WeNeed Chinese Word Segmentation for StatisticalMachine Translation?
Proceedings of the ACLSIGHAN Workshop 2004, pages 122-128, Barcelona,Spain.Hua-Ping Zhang, Qun Liu, Xue-Qi Cheng, Hao Zhangand Hong-Kui Yu, 2003.
Chinese Lexical Analysis163Using Hierarchical Hidden Markov Model.Proceedings of the Second SIGHAN Workshop onChinese Language Processing, pages 63-70, Sapporo,Japan.Ruiqiang Zhang, Keiji Yasuda and Eiichiro Sumita,2008.
Chinese Word Segmentation and StatisticalMachine Translation.
ACM Transactions on Speechand Language Processing, 5 (2).
pages 1-19.164
