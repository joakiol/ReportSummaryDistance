iText Classification Using WordNet HypernymsSam ScottComputer Science Dept.University of OttawaOttawa, ON KIN 6N5 (Canada)sscot tz~cs l ,  uot  t :awa,  cetAbstractThis paper describes experiments in MachineLearning for text classification using a newrepresentation of text based on WordNethypernyms.
Six binary classification tasks ofvarying difficulty are defined, and the Rippersystem is used to produce discrimination rulesfor each task using the new hypernym densityrepresentation.
Rules are also produced with thecommonly used bag-of-words representation,incorporating no knowledge from WordNet.Experiments how that for some of the moredifficult tasks the hypernym densityrepresentation leads to significantly moreaccurate and more comprehensible rules.1.
IntroductionThe task of Supervised Machine Learning can bestated as follows: given a set of classification labelsC, and set of training examples E, each of which hasbeen assigned one of the class labels from C, thesystem must use E to form a hypothesis that can be~used to predict he class labels of previously unseenexamples of the same type \[Mitchell 97\].
In machinelearning systems that classify text, E is a set oflabeled documents from a corpus such as Reuters-21578.
The labels can signify topic headings, writingstyles, or judgements a to the documents' relevance.Text classification systems are used in a variety ofcontexts, including e-mail and news filtering,personal information agents and assistants,information retrieval, and automatic ndexing.Before a set of documents can be presented to amachine learning system, each document must betransformed into a feature vector.
Typically, eachelement of a feature vector epresents a word from thecorpus.
The feature values may be binary, indicatingpresence or absence of the word in the document, orthey may be integers or real numbers indicating somemeasure of frequency of the word's appearance in theStan MatwinComputer Science Dept.University of OttawaOttawa, ON KIN 6N5 (Canada)st:etn~csl, uot  t :awa,  catext.
This text representation, referred to as the bag-of-words, is used in most typical approaches to textclassification (for recent work see \[Lang 95\],\[Joaehims 97\], and \[Koller & Sahami 97\]).
In theseapproaches, no linguistic processing (other than astop list of most frequent words) is applied to theoriginal text.This paper explores the hypothesis that incorporatinglinguistic knowledge into text representation can leadto improvements in classification accuracy.Specifically, we use part of speech information fromthe Brill tagger \[Brill 92\] and the synonymy andhypernymy relations from WordNet \[Miller 90\] tochange the representation f the text from bag-of-words to hypernym density.
We report results from anongoing study in which the hypernym densityrepresentation at different heights of generalization iscompared to the old bag-of-words model.
We focuson using the new representation of text with aparticular machine learning algorithm (Ripper) thatwas designed with the high dimensionality of textclassification tasks in mind.
The issue of whether ourresults will generalize to other machine learningsystems i left as future work.The only published study comparable to this one is\[Rodffguez et al 97\].
Their study used WordNet toenhance neural network learning algorithms forsignificant improvements in classification accuracyon the Reuters-21578 corpus.
However, theirapproach only made use of synonymy and involved amanual word sense disambiguation step, whereas ourapproach uses synonymy and hypernymy and iscompletely automatic.
Furthermore, their approachtook advantage of the fact that the Reuters topicheadings are themselves good indicators forclassification, whereas our approach makes no suchassumptions.
Finally their approach to usingWordNet focussed on improving the specificalgorithms used by neural networks while retainingthe bag-of-words representation of text.
Ourapproach looks at using WordNet to change therepresentation of the text itself and thus may be45IIiIIIIIiIIIIiIIiIIapplicable to a wider variety of machine learningsystems.The paper proceeds as follows.
In section 2 wepresent the data sets that we work with, theclassification tasks defined on this data, and someinitial experiments with the Ripper learning system.Section 3 discusses the new hypernym densityrepresentation.
Section 4 presents experimentalresults using both bag-of-words and hypernymdensity and discusses the accuracy andcomprehensibility of the rules learned by Ripper.Finally, section 5 presents the conclusion and futurework.2.
Preliminaries: the Corpora,Classification Tasks, and LearningAlgorithmThe classification tasks used in this study are drawnfrom three different corpora: Reuters-21578,USENET, and the Digital Tradition (DigiTrad).
BothReuters and USENET have been the subject ofprevious tudies in machine learning (see \[Koller &Sahami 97\] for a study of Reuters and \[Weiss et al96\] for a study of USENET).
In keeping withprevious tudies, we used topic headings as the basisfor the Reuters classification tasks and newsgroupnames as the basis for the USENET tasks.
The thirdcorpus, DigiTrad is a public domain collection of6500 folk song lyrics \[Greenhaus 96\].
To aidsearching, the owners of DigiTrad have assigned toeach song one or more key words from a fixed list.Some of these key words capture information on theorigin or style of the songs (e.g.
"Irish" or "British")while others relate to subject matter (e.g.
"murder" or"marriage").
The latter type of key words served asthe basis for the classification tasks in this study.Not all types of text are equally difficult to classify.Reuters consists of articles written purely as a sourceof factual information.
The writing style tends to bedirect and to the point, and uses a restrictedvocabulary to aid quick comprehension.
It has beenobserved that the topic headings in Reuters tend toconsist of words that appear frequently in the text,and this observation has been exploited to helpimprove classification accuracy \[Rodtguez etal.
97\].DigiTrad and USENET are good examples of theopposite xtreme.
The texts in DigiTrad make heavyuse of metaphoric, rhyming, unusual and archaiclanguage.
Often the lyrics do not explicitly state whata song is about.
Contributors to USENET often varyin their use of terminology, stray from the topic, or46use unusual anguage.
All of these qualities tend tomake subject-based classification tasks fromDigiTrad and USENET more difficult than those of acomparable size from Reuters.From the three corpora described above, six binaryclassification tasks were defined, as shown in table 1.The tasks were chosen to be roughly the same size,and cover cases in which the classes seemed to besemantically related (REUTER2 and USENET2) aswell as those in which the classes seemed unrelated(REUTER1 and USENETI).
In all cases the classeswere made completely disjoint by removing anyoverlapping examples, tThe machine learning algorithm chosen for this studywas Ripper, a rule-based learner developed byWilliam Cohen \[Cohen 95\].
Ripper was specificallydesigned to handle the high dimensionality of bag-of-words text classification by being fast and using set-valued features \[Cohen 96\].
Table 1 shows that ourintuitions about he difficulty of the three corpora forbag-of-words classification are valid in the case of theRipper algorithm.
Error rates over lO.fold cross-validation 2 for the Reuters tasks were under 5%,while error rates for the other tasks ranged fromapproximately 19% to 38%.
We believe that with thegrowing applications of text classification on theInternet, it is likely that the kinds of texts to beautomatically classified will share many features withthe kinds of texts that are difficult for the bag-of-words approach.It is worth noting that difficult classification tasks forRipper are not necessarily difficult for humans.
Weclassified 200 examples from each of the SONGI andSONG2 by hand (with no special training phase) andcompared our classifications to those from DigiTrad.i USENET articles that were cross-posted or taggedas follow-ups were excluded so that the remainingarticles reflected a wide variety of attempts to launchdiscussions within the given topics.
Non-text objectssuch as uuencoded bitmaps were also removed fromthe postings.z In n-foM cross-validation the articles in the corpusare split into n partitions.
Then the learningalgorithm is executed n times.
On the k ~h run,partition k is used as a testing set and all the otherpartitions make up the training set.
The mean error-rate (percentage of the testing set wrongly classified)on the n runs is taken as an approximate measure ofthe real error-rate of the system on the given corpus.IIIIIITask NameREUTERIREUTER2SONGISONG2USENET1USENET2SourceReuters-21578Reuters-21578Di\[\[iTradDi\[iTradUSENETUSENETTable 1: Information on the class~Classes Size Balance Words Errorlivestock I ~old 224 98/126 154 1.75corn I wheat 313 1301183 173 3.87murder I marriage 424 2001224 331 30.23politicall religion 432 1941238 241 32.64soc.history 249 791170 166 19.92misc.
taxes, moderatedbionet.microbiology 280 1171163 152 37.86bionet.neuroscienceication tasks discussed in this paper.
"Size" refers to total numberof texts in each task.
"Balance" shows number of examples in each class.
"Words" shows theaverage length of the documents in each task.
"Error" show the average percentage error rates foreach task using Ripper with bag-of-words and lO-fold cross-validation.The error rates were approximately I% for SONGIand 4% for SONG2.
Clearly the backgroundknowledge and linguistic competence humans bringto a classification task enables us to overcome thedifficulties posed by the text itself.3.
The  Hypernym Density Representat ionThe algorithm for computing hypernym densityrequires three passes through the corpus.a) During the first pass, the Brill tagger \[Brill 92\]assigns a part of speech tag to each word in thecorpus.b) During the second pass, all nouns and verbs arelooked up in WordNet and a global list of allsynonym and hypernym synsets is assembled.Infrequently occurring synsets are discarded, andthose that remain form the feature set.
(A synset isdefined as infrequent if its frequency of occurrenceover the entire corpus is less than 0.05N, where N isthe number of documents in the corpus.
)c) During the third pass, the density of each synset(defined as the number of occurrences of a synset inthe WordNet output divided by the number of wordsin the document) is computed for each exampleresulting in a set of numerical feature vectors.The calculations of frequency and density areinfluenced by the value of a parameter h that controlsthe height of generalization.
This parameter can beused to limit the number of steps upward through thehypernym hierarchy for each word.
At height h=Oonly the synsets that contain the words in the corpuswill be counted.
At height h>O the same synsets willbe counted as well as all the hypernym synsets thatappear up to h steps above them in the hypernymhierarchy.
A special value of h=max is defined as thelevel in which all hypernym synsets are counted, nomatter how far up in the hierarchy they appear.In the new representation, each feature represents aset of either nouns or verbs.
At h=max, featurescorresponding to synsets higher up in the hypernymhierarchy represent supersets of the nouns or verbsrepresented by the less general features.
At lowervalues of h, the nouns and verbs represented by afeature (synset) will be those that map to synsets up toh steps below it in the hypernym hierarchy.
The bestvalue of h for a given text classification task willdepend on characteristics of the text such as use ofterminology, similarity of topics, and breadth oftopics.
It will also depend on the characteristics ofWordNet itself.
In general, if the value for h is toosmall, the learner will be unable to generalizeeffectively.
If the value for h is too large, the learnerwill suffer from overgeneralization because of theoverlap between the features.Note that no attempt is made at word sensedisambiguation during the computation of hypernymdensity.
Instead all senses returned by WordNet arejudged equally likely to be correct, and all of themare included in the feature set.
The use of the densitymeasurement is an attempt o capture some measureof relevancy.
The learner is aided by the fact thatmany different but synonymous or hyponymouswords will map to common synsets, thus raising thedensities of the "more relevant" synsets.
In otherwords, a relatively low value for a feature indicatesthat little evidence was found for the meaningfulnessof that synset o the document.
(In the \[Rodrfguez etal.
97\] text classification paper,word sense disambiguation was performed by manualinspection.
This approach was feasible in the contexti47of that study because of the small number of wordsinvolved.
In the current study, the words number inthe tens of thousands, making manual disambiguationunfeasible.
Automatic disambiguation is possible andoften obtains good results as in \[Yarowski 95\] or \[Liet al 95\], but this is left as future work.
)Clearly the change of representation process leaves alot of room for inaccuracies to be introduced to thefeature set.
Some sources of potential error are: a)the tagger, b) the lack of true word sensedisambiguation, c) words missing from WordNet, andd) the shallowness of WordNet's emantic hierarchyin some knowledge domains.4.
Experiments and results4.1.
AccuracyThe new hypernym density representation differs inthree important ways from the bag-of-words: a) allwords are discarded except nouns and verbs, b)filtered normalized density vectors replace binaryvectors, and c) hypernym synsets replace words.
Toshow convincingly that improvements in accuracy arederived solely from the use of synsets, twonormalizing experiments were performed using thefollowing representations:a) bag-of-words using only nouns and verbs, andb) filtered normalized ensity vectors for nouns andverbs.The results of these runs were compared to the bag-of-words approach using 10-fold cross-validation (seetable 2) and in no case was any statistically significantdifference found, leading to the conclusion that anyimprovements in accuracy derive mainly from the useof hypernyms.For the main experiments, average rror rates over10-fold cross-validation were compared for all sixclassification tasks using hypernym densityrepresentations with values of h ranging from 0 to 9and h--max.
For each classification task the same 10partitions were used on every run so the results couldbe tested for significance using a paired-t est.
Table3 shows a comparison of three error rates: bag-of-words, hypemym density with h=max, and finallyhypernym density using the best value for h.In the case of the Reuters tasks, no improvementsover bag-of-words were expected and none wereobserved.
On the other hand, a dramatic reduction in48Task Bag ofWordsREUTER 1 1.75REUTER2 3.87SONGI 30.23SONG2 32.64USENETI 19.92USENET2 37.86Bag of Nouns Noun andand Verbs Verb Dens.1.75 1.754.19 5.4827.35 27.6728.23 29.5618.56 20.4537.86 35.00Table 2: Comparison of percentage rror rates overl O-fold cross-validation for the normalizingexperiments.
No statistically significant benefit orharm is derived from any of these changes ofrepresentation.Task Bag of Hypernym DensityWords h error h errorREUTER1 1.75 max 2.38 0 1.75REUTER2 3.87 max 6.13 0 4.84SONG1 30.23 max 22.04 9 16.00SONG2 32.64 max 34.45 4 31.04USENETI 19.92 max 14.36 9 13.11USENET2 37.86 max 40.00 2 36.43Table 3: Comparison of percentage rror rates overlO-fold cross-validation for the six data sets in thestudy.
Statistically significant improvements overbag-of-words are shown in Italics.the error rate was seen for SONGI (47% drop innumber of errors for h=9) and USENETI (34% dropfor h=9).
For the SONG2 and USENET2 data sets,the use of hypernyms produced error ratescomparable tobag-of-words.
The discussion of theseresults is left to section 4.3.4.2 ComprehensibilityIn the machine learning community, increasingweight is given to the idea that classificationhypotheses should be comprehensible to the user.Rule induction systems like Ripper are known forproducing more comprehensible output than, saymulti-layer perceptrons.
A systematic nvestigation ofthe comprehensibility of rules produced usinghypernym density versus bag-of-words i beyond thescope of this work.
However, we often see evidenceof the better comprehensibility of the rules producedfrom the hypernym density representation.
Figure 1shows a comparison of the rules learned by Ripper onthe USENETI data set.
The results for both bag-of-words and h=max hypernym density are shown forthe same fold of data.In the case of hypernyms, Ripper has learned a simplerule saying that if the synset possession has a lowdensity, the document probably belongs in the historyIIIIIIIIIII1Ii1IIIIpossession(synset) < 2.9 ~ soc.historydefault ~ misc.taxes.moderatedRule learned usinghypernym frequencyfor a document D,("tax" ~ D & '~istory" ?
D) OR(',\[tax" ~ D & "s" ~ D & "any" ?
D)OR( tax" ~ D& "is" ~ D& "and" ~ D&"if" ~ D & "roth" ~ D) OR("century" ?
D) OR("great" E D) OR("survey" ?
D) OR Ru/e teared us/rig"war" ?
D) ~ soc.history bag of wordsFigure 1: A comparison of the rules learned byRipper using hypernym density with h=rravi (top)and bog of words (bottom) on a single fold of theUSENETI data.
The bottom rule produced twice asmany errors on the testing set.category.
Over the 10 folds of the data, seven foldsproduced a rule almost identical to the one shown.For the remaining three folds, the possessionhypernym appeared along with other synsets inslightly different rules.
The hyponyms of possessioninclude words such as ownership, asset, and liability -the sorts of words often used during discussions abouttaxes, but rarely during discussions about history.
Onthe other hand, the rules learned on the bag-of-wordsdata seem less comprehensible: they are moreelaborate and less semantically clear.
Furthermore,the rules tended to vary widely across the 10 folds,suggesting that they were less robust and moredependent on the specifics of the training data.4.3 DiscussionHypernym density has been observed to greatlyimprove classification accuracy in some cases, whilein others the improvements are not particularlyspectacular.
In the case of the Reuters tasks, the lackof improvement is not a particular worry.
It is veryunlikely that any change of representation could haveimproved on the accuracy of bag-of-words for thesetasks.
But the cases of the SONG2 and USENET2tasks are worth looking at in more detail.In the SONG2 task, the main problem seems to bethat the classes (political and religion) are moreclosely semantically related than their class labelssuggest.
Visual inspection of these songs revealedthat many of the political songs contain statementsabout religion, make references to religious concepts,or frame their messages in religious terminology.49This was the source of the higher error rate reportedin section 2 when these songs were classified byhand.
Inspection of Ripper's output revealed thatbag-of-words rules make heavy use of religious wordssuch as Jesus, lord, and soul, while the hypernymdensity rules at h=max mostly contained highlyabstract political synsets such as social group andpolitical unit.
It is possible that overgeneralizationoccurred when subtle differences in religiousterminology (for instance between gospel hymns andpolitical parodies of religion) were mapped tocommon synsets in WordNet.In the case of USENET2 the problem is two-fold.The classes are semantically closely related(microbiology and neuroscience) and the writers tendto use highly technical terms that are not found inWordNet 1.5.
Some examples of missing wordsinclude neuroscientist, haemocytometer, HIV, kinase,neurobiology, and retrovirus 3.An attempt was madeto add the missing words manually into the WordNethierarchy, but even then the extended semantichierarchy was not fine-grained enough to allowmeaningful generalizations.
Because of theshallowness of the hierarchy, overgeneralizationquickly becomes a problem as the height ofgeneralization i creases.
This is why the best errorrate for USENET2 using hypernym density was foundat h=2.Clearly the change of representation to hypernymdensity works best only with an appropriate value forthe parameter h. We have introduced a newparameter into the learning task that must somehowbe set by the user.
This is certainly not unheard of inthe machine learning community.
All currentlyavailable machine learning systems contain a largenumber of parameters.
The only difference is that hmodifies the feature set rather than the learningalgorithm itself.
Nevertheless, it is worth addressingthe question of how this parameter could be set inpractice.\[Kohavi & John 95\] describe a "wrapper" method forlearning algorithms that automatically selectsappropriate parameters.
In their system, the set ofparameters i treated as a vector space that can besearched for an optimal setting.
The sets ofparameters are evaluated using 10-fold cross-validation on the training data, and a best-first searchstrategy is employed to search for the parameter setthat minimizes the average rror rate.
This system3 Some of these terms do appear in WordNet 1.6IIiiIIiiIiIcould easily be adapted to include a parameter suchas h that modifies the feature set.
Indeed \[Kohavi &.Iohn 97\] have already extended their method to therelated problem of finding optimal feature subsets forlearning.5.
Conclusions and future work.This paper describes a method of incorporatingWordNet knowledge into text representation that canlead to significant reductions inerror rates on certaintypes of text classification tasks.
The method uses thelexical and semantic knowledge embodied inWordNet to move from a bag-of.wordsrepresentation t  a representation based on hypernymdensity.
The appropriate value for the height ofgeneralization parameter h depends on thecharacteristics of each classification task.
A sidebenefit of the hypernym density representation is thatthe classification rules induced are often simpler andmore comprehensible than rules induced using thebag-of-words.Our experience indicates that the hypernym densityrepresentation can work well for texts that use anextended or unusual vocabulary, or are written bymultiple authors employing different erminologies.It is not likely to work well for text that is guaranteedto be written concisely and efficiently, such as thetext in Reuters-21578.
In particular, hypernymdensity is more likely to perform well onclassification tasks involving narrowly defined and/orsemantically distant classes (such as SONGI andUSENETI).
In the case of classes that are broadlydefined and/or semantically related (such as SONG2and USENET2) hypernym density does not alwaysoutperform bag-of-words.AcknowledgmentsThe authors are grateful to William Cohen for makingthe Ripper system available for this research.
The firstauthor acknowledges the support of the NationalSciences and Engineering Research Council ofCanada nd the Interactive Information Group of theNational Research Council.
The second authoracknowledges the support of the Natural Sciences andEngineering Research Council of Canada.References\[BriU 92\] Eric Brill.
A simple rule-based part ofspeech tagger.
In Proceedings of the ThirdConference on Applied Natural LanguageProcessing, ACL, 1992.\[Cohen 95\] William W. Cohen.
Fast Effective RuleInduction.
In Proc.
ICML-95.
Lake Tahoe, California,1995.\[Cohen 96\] William W. Cohen.
Learning Trees andRules with Set-valued Features.
In Proc.
AAAI-96,1996\[Greenhaus 96\] Dick Greenhaus.
About the DigitalTradition.
(www.mudcat.org/DigiTrad-blurb.html),1996.
4\[Joachims 97\] T..l'oachims.
A Probabilistic Analysisof the Rocchio Algorithm with TFIDF for TextCategorization.
InProc.
ICML-97, 143-146, 1997.\[Kohavi & John 95\] Ron Kohavi and George H. John.Automatic Parameter Selection by MinimizingEstimated Error.
In Proc.
ICML-95, 1995.iIIIIIIOne area for future work is the incorporation ofmoreof the relations from WordNet, such as meronymy.This will give the change of representation a evenmore semantic character.
More sophisticated wordsense disambiguation could produce more accuratehypernym density features.
The use of otherlinguistic resources available in the public domain,such as the Unified Medical Language System?Metathesaurus?
\[NLM 98\], could improve classifierperformance in knowledge domains that aresemantically close and highly expert.
Finally, there isthe task of testing whether the improvements noted inthis study generalize to machine learning systemsother than Ripper.50\[Kohavi & John 97\] Ron Kohavi and George H. John.Wrapers for Feature Subset Selection.
In ArtificialIntelligence Journal, special issue on relevance, May20, 1997.\[Koller & Saharni 96\] D. Koller and M. Sahami.Hierarchically Classifying Documents Using VeryFew Words.
In Proc.
ICML-97, 170-176, 1997.\[Lang 95\] K. Lang.
NewsWeeder: Learning to FilterNews.
In Proc.
ICML-95, 331-336, 1995,\[Li et al 95\] Xiaobin Li, Start Szpakowicz and Start4 The DigiTrad database itself is atwww.deltablues.com/folksearch.htmlIIiIIIIIIMatwin.
A WordNet-based Algorithm for WordSense Disambiguation.
In Proc.
IJCAI-95, Montr6al,Canada, 1995.\[Mitchell 97\] Tom Mitchell, Machine Learning,McGraw Hill, 1997.\[Miller 90\] George A. Miller.
WordNet: an On-lineLexical Database.
International Journal ofLexicography 3(4), 1990.\[NLM 98\] National Library of Medicine.
UnifiedMedical Language System Overview.www.nlrn.nih.gov/research/umls/UMLSDOC.HTML,February, 1998.\[Rodrfguez et al 97\] Manuel de Buenaga Rodrfguez,Jos6 Marfa G6mez-Hidalgo and Bel6n Dfaz-Agudo.Using WordNet o Complement Training Informationin Text Categorization.
In Proc.
RANLP-97,Stanford, March 25-27, 1997\[Weiss et al 96\] Scott A. Weiss, Simon Kasif, andEric Brill.
Text Classification in USENETNewsgroups: A Progress Report.
In Proceedings ofthe AAAI Spring Symposium on Machine Learning inInformation Access, Bulgaria, September 11-13,1996.\[Yarowski 95\] David Yarowski.
Unsupervised WordSense Disambiguation Rivaling Supervised Methods.In Proceedings of the 33 "a Meeting of the ACL,Cambridge, June 26-30, 1995.51
