WRITTEN LANGUAGE SYSTEM EVALUATIONBeth M. SundheimNava l  Command,  Contro l  and Ocean Surve i l lance CenterRDT&E D iv i s ion  (NRaD),  Code 44208San Diego,  CA 92152-7420PROJECT GOALSCurrent efforts support he NLP community in the definitionand implementation f a range of performance evaluations ofwritten language technology.
Goals include offering a varietyof evaluations that will appeal to a broad range of NLP researchgroups, minimizing the task-specific system tailoring requiredof evaluation participants, and ensuring that the task designsfacilitate scoring.RECENT RESULTSThe final evaluation of the information extraction portion ofphase one of the ARPA Tipster Text program was conducted inJuly, 1993.
Participants in this evaluation included not onlythe Tipster-supported information extraction contractors butthirteen other sites as well.
This evaluation was the topic ofthe Fifth Message Understanding Conference (MUC-5), whichwas held in August, 1993, and chaired by NRaD.
Aproceedings will be published in Spring, 1994.With particular respect o the research and development tasksof the Tipster contractors, the goal of the evaluation was toassess uccess in terms of a system's ability to work in bothEnglish and Japanese (BBN, GE/CMU, and NMSU/Brandeis)and/or in both the joint ventures and microelectrorLics domains(BBN, GE/CMU, NMSU/Brandeis, and UMass/Hughes).
Theevaluations measured the completeness and accuracy ofsystems on information extraction tasks and used anexamination of the role of missing, spurious and otherwiseerroneous output as a means of diagnosing the state of the art.Viewed as a set of performance benchmarks for informationextraction technology, the MUC-5 evaluation results on theEnglish joint ventures (EJV) task are at least as good as theMUC-4 level of performance.
This comparison takes intoaccount some measurable differences in difficulty between theEJV task and the MUC-3/MUC-4 terrorism task.However, even a superficial comparison of task difficulty ishard to make because of the change from the flat-format designof the earlier MUC templates to the object-oriented design ofthe MUC-5 templates.
Comparison is also made difficult bythe many changes that were made to the alignment and scoringprocesses and to the performance metrics.
Therefore, it ismore useful to view the results of MUC-5 on its own termsrather than in comparison to previous MUC evaluations.Viewed on its own terms, MUC-5 yielded very impressiveresults for some systems on some tasks.
Error per responsefill scores as low as 34 (GE/CMU optional test run using theCMU TEXTRACT system) and 39 (GE/CMU Shogun system)were obtained on the Japanese joint ventures (JJV) core-template test.
The only other error per response fill scores inthe 30-40 range were achieved by humans, who were tested onthe English microelectronics (EME) task; however, machineperformance on that EME test was only half as good as humanperformance.
Thus, while the IJV core-template st resultsshow that machine performance on a constrained test can bequite high, the EME results show that a similar level ofmachine performance on a more extensive task could not beachieved, at least not in the relatively short developmentperiod allowed for ME.Not only do results such as those cited for the JJV core-template test show how well some approaches to informationextraction work for some tasks, they also show howmanageable anguages other than English can be.
A cross-language comparison of results showed fairly consistentadvantage in favor of Japanese over English.
Comparison ofresults across domains does not show an advantage in favor ofone domain over the other, and it is quite likely thatdifferences in the nature of the texts, the nature and evolutionof the extraction tasks, and the amount of time allowed fordevelopment all had an impact on the results.The quantity and variety of material on which systems weretrained and tested presented challenges far beyond those posedby earlier MUC evaluations.
The scope of the evaluations wasbroad enough to cause most MUC-5 sites to skip pats of theextraction task, especially types of information that appearrelatively rarely in the corpus.
Since no type of information isweighted in the scoring more heavily than any other, thebiases that exist in the evaluation reflect the distribution ofrelevant information i  the text corpus and result in a naturalemphasis on handling the most frequently-occurring slot-filling tasks.
These tasks" turn out to be the ones that are lessidiosyncratic and therefore more important to the developmentof generally useful technology.PLANS FOR THE COMING YEARCollaborate with other representatives of the writtenlanguage NLP community to define and implement ewperformance evaluations.?
Coordinate a dry run of the evaluations.?
Issue call for participation i the formal evaluation andconference.462
