Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 704?709,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsJoint Hebrew Segmentation and Parsingusing a PCFG-LA Lattice ParserYoav Goldberg and Michael ElhadadBen Gurion University of the NegevDepartment of Computer SciencePOB 653 Be?er Sheva, 84105, Israel{yoavg|elhadad}@cs.bgu.ac.ilAbstractWe experiment with extending a lattice pars-ing methodology for parsing Hebrew (Gold-berg and Tsarfaty, 2008; Golderg et al, 2009)to make use of a stronger syntactic model: thePCFG-LA Berkeley Parser.
We show that themethodology is very effective: using a smalltraining set of about 5500 trees, we constructa parser which parses and segments unseg-mented Hebrew text with an F-score of almost80%, an error reduction of over 20% over thebest previous result for this task.
This resultindicates that lattice parsing with the Berkeleyparser is an effective methodology for parsingover uncertain inputs.1 IntroductionMost work on parsing assumes that the lexical itemsin the yield of a parse tree are fully observed, andcorrespond to space delimited tokens, perhaps af-ter a deterministic preprocessing step of tokeniza-tion.
While this is mostly the case for English, thesituation is different in languages such as Chinese,in which word boundaries are not marked, and theSemitic languages of Hebrew and Arabic, in whichvarious particles corresponding to function wordsare agglutinated as affixes to content bearing words,sharing the same space-delimited token.
For exam-ple, the Hebrew token bcl1 can be interpreted asthe single noun meaning ?onion?, or as a sequenceof a preposition and a noun b-cl meaning ?in (the)shadow?.
In such languages, the sequence of lexical1We adopt here the transliteration scheme of (Sima?an et al,2001)items corresponding to an input string is ambiguous,and cannot be determined using a deterministic pro-cedure.
In this work, we focus on constituency pars-ing of Modern Hebrew (henceforth Hebrew) fromraw unsegmented text.A common method of approaching the discrep-ancy between input strings and space delimited to-kens is using a pipeline process, in which the in-put string is pre-segmented prior to handing it to aparser.
The shortcoming of this method, as notedby (Tsarfaty, 2006), is that many segmentation de-cisions cannot be resolved based on local contextalone.
Rather, they may depend on long distance re-lations and interact closely with the syntactic struc-ture of the sentence.
Thus, segmentation deci-sions should be integrated into the parsing processand not performed as an independent preprocess-ing step.
Goldberg and Tsarfaty (2008) demon-strated the effectiveness of lattice parsing for jointlyperforming segmentation and parsing of Hebrewtext.
They experimented with various manual re-finements of unlexicalized, treebank-derived gram-mars, and showed that better grammars contributeto better segmentation accuracies.
Goldberg et al(2009) showed that segmentation and parsing ac-curacies can be further improved by extending thelexical coverage of a lattice-parser using an exter-nal resource.
Recently, Green and Manning (2010)demonstrated the effectiveness of lattice-parsing forparsing Arabic.Here, we report the results of experiments cou-pling lattice parsing together with the currently bestgrammar learning method: the Berkeley PCFG-LAparser (Petrov et al, 2006).7042 Aspects of Modern HebrewSome aspects that make Hebrew challenging from alanguage-processing perspective are:Affixation Common function words are prefixedto the following word.
These include: m(?from?
)f (?who?/?that?)
h(?the?)
w(?and?)
k(?like?)
l(?to?
)and b(?in?).
Several such elements may attach to-gether, producing forms such as wfmhfmf (w-f-m-h-fmf ?and-that-from-the-sun?).
Notice that the lastpart of the token, the noun fmf (?sun?
), when ap-pearing in isolation, can be also interpreted as thesequence f-mf (?who moved?).
The linear orderof such segmental elements within a token is fixed(disallowing the reading w-f-m-h-f-mf in the previ-ous example).
However, the syntactic relations ofthese elements with respect to the rest of the sen-tence is rather free.
The relativizer f (?that?)
forexample may attach to an arbitrarily long relativeclause that goes beyond token boundaries.
To fur-ther complicate matters, the definite article h(?the?
)is not realized in writing when following the par-ticles b(?in?),k(?like?)
and l(?to?).
Thus, the formbbit can be interpreted as either b-bit (?in house?)
orb-h-bit (?in the house?).
In addition, pronominal el-ements may attach to nouns, verbs, adverbs, preposi-tions and others as suffixes (e.g.
lqxn(lqx-hn, ?took-them?
), elihm(eli-hm,?on them?)).
These affixationsresult in highly ambiguous token segmentations.Relatively free constituent order The ordering ofconstituents inside a phrase is relatively free.
Thisis most notably apparent in the verbal phrases andsentential levels.
In particular, while most sentencesfollow an SVO order, OVS and VSO configurationsare also possible.
Verbal arguments can appear be-fore or after the verb, and in many ordering.
Thisresults in long and flat VP and S structures and a fairamount of sparsity.Rich templatic morphology Hebrew has a veryproductive morphological structure, which is basedon a root+template system.
The productive mor-phology results in many distinct word forms and ahigh out-of-vocabulary rate which makes it hard toreliably estimate lexical parameters from annotatedcorpora.
The root+template system (combined withthe unvocalized writing system and rich affixation)makes it hard to guess the morphological analysesof an unknown word based on its prefix and suffix,as usually done in other languages.Unvocalized writing system Most vowels are notmarked in everyday Hebrew text, which results in avery high level of lexical and morphological ambi-guity.
Some tokens can admit as many as 15 distinctreadings.Agreement Hebrew grammar forces morpholog-ical agreement between Adjectives and Nouns(which should agree on Gender and Number anddefiniteness), and between Subjects and Verbs(which should agree on Gender and Number).3 PCFG-LA Grammar EstimationKlein and Manning (2003) demonstrated that lin-guistically informed splitting of non-terminal sym-bols in treebank-derived grammars can result in ac-curate grammars.
Their work triggered investiga-tions in automatic grammar refinement and state-splitting (Matsuzaki et al, 2005; Prescher, 2005),which was then perfected by (Petrov et al, 2006;Petrov, 2009).
The model of (Petrov et al, 2006) andits publicly available implementation, the Berke-ley parser2, works by starting with a bare-bonestreebank derived grammar and automatically refin-ing it in split-merge-smooth cycles.
The learningworks by iteratively (1) splitting each non-terminalcategory in two, (2) merging back non-effectivesplits and (3) smoothing the split non-terminals to-ward their shared ancestor.
Each of the steps isfollowed by an EM-based parameter re-estimation.This process allows learning tree annotations whichcapture many latent syntactic interactions.
At in-ference time, the latent annotations are (approxi-mately) marginalized out, resulting in the (approx-imate) most probable unannotated tree according tothe refined grammar.
This parsing methodology isvery robust, producing state of the art accuracies forEnglish, as well as many other languages includingGerman (Petrov and Klein, 2008), French (Canditoet al, 2009) and Chinese (Huang and Harper, 2009)among others.The grammar learning process is applied to bi-narized parse trees, with 1st-order vertical and 0th-order horizontal markovization.
This means that in2http://code.google.com/p/berkeleyparser/705Figure 1: Lattice representation of the sentence bclm hneim.
Double-circles denote token boundaries.
Lattice arcs correspondto different segments of the token, each lattice path encodes a possible reading of the sentence.
Notice how the token bclm haveanalyses which include segments which are not directly present in the unsegmented form, such as the definite article h (1-3) and thepronominal suffix which is expanded to the sequence fl hm (?of them?, 2-4, 4-5).the initial grammar, each of the non-terminal sym-bols is effectively conditioned on its parent alone,and is independent of its sisters.
This is a verystrong independence assumption.
However, it al-lows the resulting refined grammar to encode its ownset of dependencies between a node and its sisters, aswell as ordering preferences in long, flat rules.
Ourinitial experiments on Hebrew confirm that movingto higher order horizontal markovization degradesparsing performance, while producing much largergrammars.4 Lattice Representation and ParsingFollowing (Goldberg and Tsarfaty, 2008) we dealwith the ambiguous affixation patterns in Hebrew byencoding the input sentence as a segmentation lat-tice.
Each token is encoded as a lattice representingits possible analyses, and the token-lattices are thenconcatenated to form the sentence-lattice.
Figure 1presents the lattice for the two token sentence ?bclmhneim?.
Each lattice arc correspond to a lexical item.Lattice Parsing The CKY parsing algorithm canbe extended to accept a lattice as its input (Chap-pelier et al, 1999).
This works by indexing lexi-cal items by their start and end states in the latticeinstead of by their sentence position, and changingthe initialization procedure of CKY to allow termi-nal and preterminal sybols of spans of sizes > 1.
It isthen relatively straightforward to modify the parsingmechanism to support this change: not giving spe-cial treatments for spans of size 1, and distinguish-ing lexical items from non-terminals by a specifiedmarking instead of by their position in the chart.
Wemodified the PCFG-LA Berkeley parser to acceptlattice input at inference time (training is performedas usual on fully observed treebank trees).Lattice Construction We construct the token lat-tices using MILA, a lexicon-based morphologicalanalyzer which provides a set of possible analysesfor each token (Itai and Wintner, 2008).
While beinga high-coverage lexicon, its coverage is not perfect.For the future, we consider using unknown handlingtechniques such as those proposed in (Adler et al,2008).
Still, the use of the lexicon for lattice con-struction rather than relying on forms seen in thetreebank is essential to achieve parsing accuracy.Lexical Probabilities Estimation Lexical p(t ?w) probabilities are defined over individual seg-ments rather than for complete tokens.
It is the roleof the syntactic model to assign probabilities to con-texts which are larger than a single segment.
Weuse the default lexical probability estimation of theBerkeley parser.3Goldberg et al (2009) suggest to estimate lexi-cal probabilities for rare and unseen segments usingemission probabilities of an HMM tagger trained us-ing EM on large corpora.
Our preliminary exper-iments with this method with the Berkeley parser3Probabilities for robust segments (lexical items observed100 times or more in training) are based on the MLE estimatesresulting from the EM procedure.
Other segments are assignedsmoothed probabilities which combine the p(w|t) MLE esti-mate with unigram tag probabilities.
Segments which were notseen in training are assigned a probability based on a singledistribution of tags for rare words.
Crucially, we restrict eachsegment to appear only with tags which are licensed by a mor-phological analyzer, as encoded in the lattice.706showed mixed results.
Parsing performance on thetest set dropped slightly.When analyzing the parsingresults on out-of-treebank text, we observed caseswhere this estimation method indeed fixed mistakes,and others where it hurt.
We are still uncertain if theslight drop in performance over the test set is due tooverfitting of the treebank vocabulary, or the inade-quacy of the method in general.5 Experiments and ResultsData In all the experiments we use Ver.2 of theHebrew treebank (Guthmann et al, 2009), whichwas converted to use the tagset of the MILA mor-phological analyzer (Golderg et al, 2009).
We usethe same splits as in previous work, with a train-ing set of 5240 sentences (484-5724) and a test setof 483 sentences (1-483).
During development, weevaluated on a random subset of 100 sentences fromthe training set.
Unless otherwise noted, we used thebasic non-terminal categories, without any extendedinformation available in them.Gold Segmentation and Tagging To assess theadequacy of the Berkeley parser for Hebrew, we per-formed baseline experiments in which either goldsegmentation and tagging or just gold segmenta-tion were available to the parser.
The numbers arevery high: an F-measure of about 88.8% for thegold segmentation and tagging, and about 82.8% forgold segmentation only.
This shows the adequacyof the PCFG-LA methodology for parsing the He-brew treebank, but also goes to show the highly am-biguous nature of the tagging.
Our baseline latticeparsing experiment (without the lexicon) results inan F-score of around 76%.4Segmentation ?
Parsing pipeline As anotherbaseline, we experimented with a pipeline systemin which the input text is automatically segmentedand tagged using a state-of-the-art HMM pos-tagger(Goldberg et al, 2008).
We then ignore the pro-duced tagging, and pass the resulting segmented textas input to the PCFG-LA parsing model as a deter-ministic input (here the lattice representation is usedwhile tagging, but the parser sees a deterministic,4For all the joint segmentation and parsing experiments, weuse a generalization of parseval that takes segmentation into ac-count.
See (Tsarfaty, 2006) for the exact details.segmented input).5 In the pipeline setting, we eitherallow the parser to assign all possible POS-tags, orrestrict it to POS-tags licensed by the lexicon.Lattice Parsing Experiments Our initial latticeparsing experiments with the Berkeley parser weredisappointing.
The lattice seemed too permissive,allowing the parser to chose weird analyses.
Erroranalysis suggested the parser failed to distinguishamong the various kinds of VPs: finite, non-finiteand modals.
Once we annotate the treebank verbsinto finite, non-finite and modals6, results improvea lot.
Further improvement was gained by specifi-cally marking the subject-NPs.7 The parser was notable to correctly learn these splits on its own, butonce they were manually provided it did a very goodjob utilizing this information.8 Marking object NPsdid not help on their own, and slightly degraded theperformance when both subjects and objects weremarked.
It appears that the learning procedure man-aged to learn the structure of objects without ourhelp.
In all the experiments, the use of the morpho-logical analyzer in producing the lattice was crucialfor parsing accuracy.Results Our final configuration (marking verbalforms and subject-NPs, using the analyzer to con-struct the lattice and training the parser for 5 itera-tions) produces remarkable parsing accuracy whenparsing from unsegmented text: an F-score of79.9% (prec: 82.3 rec: 77.6) and seg+tagging F of93.8%.
The pipeline systems with the same gram-mar achieve substantially lower F-scores of 75.2%(without the lexicon) and 77.3 (with the lexicon).For comparison, the previous best results for pars-ing Hebrew are 84.1%F assuming gold segmenta-tion and tagging (Tsarfaty and Sima?an, 2010)9, and73.7%F starting from unsegmented text (Golderg et5The segmentation+tagging accuracy of the HMM tagger onthe Treebank data is 91.3%F.6This information is available in both the treebank and themorphological analyzer, but we removed it at first.
Note that theverb-type distinction is specified only on the pre-terminal level,and not on the phrase-level.7Such markings were removed prior to evaluation.8Candito et al (2009) also report improvements in accu-racy when providing the PCFG-LA parser with few manually-devised linguistically-motivated state-splits.9The 84.1 figure is for sentences of length ?
40, and thusnot strictly comparable with all the other numbers in this paper,which are based on the entire test-set.707System Oracle OOV Handling Prec Rec F1Tsarfaty and Sima?an 2010 Gold Seg+Tag ?
- - 84.1Goldberg et al 2009 None Lexicon 73.4 74.0 73.8Seg ?
PCFG-LA Pipeline None Treebank 75.6 74.8 75.2Seg ?
PCFG-LA Pipeline None Lexicon 79.5 75.2 77.3PCFG-LA + Lattice (Joint) None Lexicon 82.3 77.6 79.9Table 1: Parsing scores of the various systemsal., 2009).
The numbers are summarized in Table 1.While the pipeline system already improves over theprevious best results, the lattice-based joint-modelimproves results even further.
Overall, the PCFG-LA+Lattice parser improve results by 6 F-points ab-solute, an error reduction of about 20%.
Taggingaccuracies are also remarkable, and constitute state-of-the-art tagging for Hebrew.The strengths of the system can be attributed tothree factors: (1) performing segmentation, taggingand parsing jointly using lattice parsing, (2) relyingon an external resource (lexicon / morphological an-alyzer) instead of on the Treebank to provide lexicalcoverage and (3) using a strong syntactic model.Running time The lattice representation effec-tively results in longer inputs to the parser.
It isinformative to quantify the effect of the lattice rep-resentation on the parsing time, which is cubic insentence length.
The pipeline parser parsed the483 pre-segmented input sentences in 151 seconds(3.2 sentences/second) not including segmentationtime, while the lattice parser took 175 seconds (2.7sents/second) including lattice construction.
Parsingwith the lattice representation is slower than in thepipeline setup, but not prohibitively so.Analysis and Limitations When analyzing thelearned grammar, we see that it learned to distin-guish short from long constituents, models conjunc-tion parallelism fairly well, and picked up a lotof information regarding the structure of quantities,dates, named and other kinds of NPs.
It also learnedto reasonably model definiteness, and that S ele-ments have at most one Subject.
However, the state-split model exhibits no notion of syntactic agree-ment on gender and number.
This is troubling, aswe encountered a fair amount of parsing mistakeswhich would have been solved if the parser were touse agreement information.6 Conclusions and Future WorkWe demonstrated that the combination of latticeparsing with the PCFG-LA Berkeley parser is highlyeffective.
Lattice parsing allows much needed flexi-bility in providing input to a parser when the yield ofthe tree is not known in advance, and the grammarrefinement and estimation techniques of the Berke-ley parser provide a strong disambiguation compo-nent.
In this work, we applied the Berkeley+Latticeparser to the challenging task of joint segmentationand parsing of Hebrew text.
The result is the firstconstituency parser which can parse naturally occur-ring unsegmented Hebrew text with an acceptableaccuracy (an F1 score of 80%).Many other uses of lattice parsing are possible.These include joint segmentation and parsing ofChinese, empty element prediction (see (Cai et al,2011) for a successful application), and a princi-pled handling of multiword-expressions, idioms andnamed-entities.
The code of the lattice extension tothe Berkeley parser is publicly available.10Despite its strong performance, we observed thatthe Berkeley parser did not learn morphologicalagreement patterns.
Agreement information couldbe very useful for disambiguating various construc-tions in Hebrew and other morphologically rich lan-guages.
We plan to address this point in future work.AcknowledgmentsWe thank Slav Petrov for making available and an-swering questions about the code of his parser, Fed-erico Sangati for pointing out some important detailsregarding the evaluation, and the three anonymousreviewers for their helpful comments.
The work issupported by the Lynn and William Frankel Centerfor Computer Sciences, Ben-Gurion University.10http://www.cs.bgu.ac.il/?yoavg/software/blatt/708ReferencesMeni Adler, Yoav Goldberg, David Gabay, and MichaelElhadad.
2008.
Unsupervised lexicon-based resolu-tion of unknown words for full morphological analy-sis.
In Proc.
of ACL.Shu Cai, David Chiang, and Yoav Goldberg.
2011.Language-independent parsing with empty elements.In Proc.
of ACL (short-paper).Marie Candito, Benoit Crabbe?, and Djame?
Seddah.
2009.On statistical parsing of French with supervised andsemi-supervised strategies.
In EACL 2009 WorkshopGrammatical inference for Computational Linguistics,Athens, Greece.J.
Chappelier, M. Rajman, R. Aragues, and A. Rozen-knop.
1999.
Lattice Parsing for Speech Recognition.In In Sixth Conference sur le Traitement Automatiquedu Langage Naturel (TANL99), pages 95?104.Yoav Goldberg and Reut Tsarfaty.
2008.
A single gener-ative model for joint morphological segmentation andsyntactic parsing.
In Proc.
of ACL.Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008.EM Can find pretty good HMM POS-Taggers (whengiven a good start).
In Proc.
of ACL.Yoav Golderg, Reut Tsarfaty, Meni Adler, and MichaelElhadad.
2009.
Enhancing unlexicalized parsing per-formance using a wide coverage lexicon, fuzzy tag-setmapping, and em-hmm-based lexical probabilities.
InProc.
of EACL.Spence Green and Christopher Manning.
2010.
BetterArabic parsing: Baselines, evaluations, and analysis.In Proc.
of COLING.Noemie Guthmann, Yuval Krymolowski, Adi Milea, andYoad Winter.
2009.
Automatic annotation of morpho-syntactic dependencies in a Modern Hebrew Treebank.In Proc.
of TLT.Zhongqiang Huang and Mary Harper.
2009.
Self-training PCFG grammars with latent annotationsacross languages.
In Proc.
of the EMNLP, pages 832?841.
Association for Computational Linguistics.Alon Itai and Shuly Wintner.
2008.
Language resourcesfor Hebrew.
Language Resources and Evaluation,42(1):75?98, March.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proc.
of ACL, Sapporo,Japan, July.
Association for Computational Linguis-tics.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProc of ACL.Slav Petrov and Dan Klein.
2008.
Parsing German withlatent variable grammars.
In Proceedings of the ACLWorkshop on Parsing German.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proc.
of ACL, Sydney,Australia.Slav Petrov.
2009.
Coarse-to-Fine Natural LanguageProcessing.
Ph.D. thesis, University of California atBekeley, Berkeley, CA, USA.Detlef Prescher.
2005.
Inducing head-driven PCFGswith latent heads: Refining a tree-bank grammar forparsing.
In Proc.
of ECML.Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,and Noa Nativ.
2001.
Building a Tree-Bank ofModern Hebrew text.
Traitement Automatique desLangues, 42(2).Reut Tsarfaty and Khalil Sima?an.
2010.
Model-ing morphosyntactic agreement in constituency-basedparsing of Modern Hebrew.
In Proceedings of theNAACL/HLT Workshop on Statistical Parsing of Mor-phologically Rich Languages (SPMRL 2010), Los An-geles, CA.Reut Tsarfaty.
2006.
Integrated Morphological and Syn-tactic Disambiguation for Modern Hebrew.
In Proc.
ofACL-SRW.709
