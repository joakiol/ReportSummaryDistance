Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 173?181,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsImproving Arabic-Chinese Statistical Machine Translationusing English as Pivot LanguageNizar Habash Jun HuCenter for Computational Learning Systems Computer Science DepartmentColumbia University Columbia UniversityNew York, NY 10115, USA New York, NY 10115, USAhabash@ccls.columbia.edu jh2740@columbia.eduAbstractWe present a comparison of two approaches forArabic-Chinese machine translation using Eng-lish as a pivot language: sentence pivoting andphrase-table pivoting.
Our results show thatusing English as a pivot in either approach out-performs direct translation from Arabic to Chi-nese.
Our best result is the phrase-pivot systemwhich scores higher than direct translation by1.1 BLEU points.
An error analysis of our bestsystem shows that we successfully handle manycomplex Arabic-Chinese syntactic variations.1 IntroductionArabic and Chinese are two languages with avery large global presence; however, there hasnot been, to our knowledge, any work on MTfor this pair.
Given the cost involved in creat-ing parallel corpora for Arabic and Chinese andgiven that there are lots of available resources(in particular parallel corpora) for Arabic andEnglish and for Chinese and English, we areinterested in exploring the role English mightserve as a pivot (or bridge) language.
In thispaper we explore different ways of pivotingthrough English to translate Arabic to Chinese.Our work is similar to previous research onpivot languages except in that our three lan-guages (source, pivot and target) are very dif-ferent and from completely unrelated families.We focus our experiments on a trilingual paral-lel corpus to keep all conditions experimentallyclean.
Our results show that using English as apivot language for translating Arabic to Chineseactually outperforms direct translation.
We be-lieve this may be a result of English being a sortof middle ground between Arabic and Chinesein terms of different linguistic features and, inparticular, word order.Section  2 describes previous work.
Section 3discusses relevant linguistic issues of Arabic,Chinese and English.
Section  4 describes oursystem and different pivoting techniques.
AndSection  5 presents our experimental results.2 Previous WorkThere has been a lot of work on translationfrom Chinese to English (Wang et al, 2007;Crego and Mari?o, 2007; Carpuat and Wu,2007; among others) and from Arabic to Eng-lish (Sadat and Habash, 2006, Al-Onaizan andPapineni, 2006; among others).
There is also afair amount of work on translation into Chinesefrom Japanese, Korean and English (Isahara etal., 2007; Kim et al, 2002; Ye et al, 2007;among others).
In 2008, the National Instituteof Standards and Technology (NIST) MTEvaluation competition introduced English-Chinese as a new evaluation track.1Much work has been done on exploiting multi-lingual corpora for MT or related tasks such aslexical induction or word alignment.
Schaferand Yarowsky (2002) induced translation lexi-cons for languages without common parallelcorpora using a bridge language that is relatedto the target languages.
Simard (1999) de-scribed a sentence aligner that makes simulta-neous decisions in a trilingual parallel text.Kumar et al (2007) improved Arabic-EnglishMT by using available parallel data in otherlanguages.
Callison-Burch et al(2006) ex-ploited the existence of multiple parallel cor-pora to learn paraphrases for Phrase-based MT.Filali and Bilmes (2005) improved word align-ment by leveraging multilingual parallel trans-lations.Most related to our work on pivoting are thefollowing: Utiyama and Isahara (2007) studied1 http://www.nist.gov/speech/tests/mt/2008/doc/173sentence and phrase pivoting strategies usingthree European languages (Spanish, French andGerman).
Their results showed that pivotingdoes not work as well as direct translation.
Wuand Wang (2007) focused on phrase pivoting.They proposed an interpolated scheme that em-ploys two phrase tables: one extracted from asmall amount of direct parallel data; and theother extracted from large amounts of indirectdata with a third pivoting language.
They com-pared results for different European language aswell as Chinese-Japanese translation using Eng-lish as a pivoting language.
Their results showthat simple pivoting does not improve over di-rect MT; however, extending the direct MT sys-tem with phrases learned through pivotinghelps.
Babych et al (2007) compared twomethods for translating into English fromUkrainian: direct Ukrainian-English MT versustranslation via a cognate language, Russian.Their comparison showed that it is possible toachieve better translation quality via pivoting.In this paper we use a standard phrase-basedMT approach (Koehn, 2004) that is in the samespirit of most statistical MT nowadays.
We be-lieve that we are the first to explore the Arabic-Chinese language pair in MT.
We differ fromprevious pivoting research in showing that piv-oting can outperform direct translation evenwhen the source, target and pivot languages areall linguistically unrelated.3 Linguistic IssuesIn this section we discuss different linguisticphenomena in which Arabic, English and Chi-nese are divergent.
We consider orthography,morphology and syntax.
We also present a newmetric for quantifying linguistic differences.3.1 OrthographyArabic is written from right-to-left using an al-phabet of 36 letters and eight optional diacriti-cal marks.
Arabic is written in a cursive mostlyword-internal connected form, but words areseparated by white spaces.
The absence of Ara-bic diacritics adds a lot of ambiguity.Chinese uses a complex orthography that in-cludes around 10,000 characters in commonuse.
Characters convey semantic rather thanphonological information.
Chinese is writtenfrom left-to-right or top-down.
Chinese wordscan be made out of one, two or more charac-ters.
However, words are written without sepa-rating spaces.
Word segmentation is a majorchallenge for processing Chinese (Wu, 1998).English uses the Roman alphabet and its wordsare written with separating white spaces.
Eng-lish orthography is much closer to Arabic thanit is to Chinese.3.2 MorphologyArabic is a morphologically rich language witha large set of morphological features such per-son, number, gender, voice, aspect, mood, case,and state.
Arabic features are realized usingboth concatenative (affixes and stems) andtemplatic (root and patterns) morphology witha variety of morphological, phonological andorthographic adjustments.
In addition, Arabichas a set of very common clitics that are writ-ten attached to the word, e.g., the conjunction?+  w+ ?and?, the preposition ?+  b+2  ?with/in?,the definite article ??
+  Al+ ?the?
and a range ofpronominal clitics that can attach to nouns (aspossessives) or verbs and prepositions (as ob-jects).In stark contrast to Arabic, Chinese is an isolat-ing language with no morphology to talk of.However, what Chinese lacks in morphology itreplaces with a complex system of nominalquantifiers and verbal aspects.
For example, inFigure 1 (at the end of this paper),  Chinesemarks the definiteness and humanness of theword??
Xue Sheng ?student?
using  the twocharacters ??
Zhe Wei  ?this person?, whilethe indefiniteness and book-ness of the word?Shu ?book?
are indicated through the characters??
Yi Ben ?one book-type?.English has a simple limited morphology pri-marily indicating number and tense.
Englishstands in the middle between Arabic and Chi-nese in terms of morphological complexity.3.3 SyntaxArabic is morpho-syntactically complex withmany differences from Chinese and English.We describe here three prominent syntacticissues in which Arabic, Chinese and Englishvary widely: subject-verb order, verb-2  Arabic transliteration is in the Habash-Soudi-Buckwalterscheme (Habash et al 2007).174prepositional phrase order and nominal modifi-cation.First, Arabic verb subjects may be: (a.)
pro-dropped (verb conjugated), (b.)
pre-verbal, or(c.)  post-verbal.
The morphology of the Arabicverb varies in the three cases.
By contrast Eng-lish and Chinese are both generally subject-verblanguages.
When translating from Arabic, thechallenge is to determine whether there is anexplicit subject and, if there is, whether it is pre-or post-verbal.
Since Arabic objects also followthe verb, a sequence of Verb NounPhrase maybe a verb subject or a pro-drop-verb object.
Inthe example in Figure 1, the subject (student)appears after the sentence initial verb in Arabic,but at the beginning of the sentence in Chineseand English.Secondly, as for the word order of prepositionalphrases (PP), Arabic and English are similar inthat PPs generally appear at the end of the sen-tence (after all the verbal arguments) and to alesser extent at its beginning.
In Chinese, how-ever, some PPs (in particular locatives and tem-porals) must appear between subject and verb.Other PPs may appear at end of sentence.
In theexample in Figure 1, the location of the reading,?in the classroom?
appears at the end of theArabic and English sentences; however, it isbetween subject and verb in Chinese.Finally, we distinguish three types of nominalmodification: adjectival (as in ?red book?
),  pos-sessive (as in ?John?s book?)
and relative (as in?the book [which] John gave me?).
All of thesemodification types are handled in a similarmanner in Chinese: using the particle?
De toconnect modifier with modified.
Modifiers al-ways precede the modified.
For example, inFigure 1, ?a book about China?
appears as ????
??
Guan Yu Zhong Guo De Shu ?aboutChina De book?.
Similarly, ?the student?sbook?
would be translated as ??
?
?
XueSheng De Shu ?student DE book?.
Like Chinese,English adjectival modifiers precede what theymodify.
However, relative modifiers follow.Possessive modifiers in English can appear be-fore or after: ?the student?s book?
or ?the bookof the student?.
Unlike English and Chinese,Arabic adjectival modifiers typically followtheir nouns (with a small exception of some su-perlative adjectives).
However, similar to Eng-lish but not Chinese, Arabic relative modifiersfollow what they modify.
As for possessivemodifiers, Arabic has a special constructioncalled Idafa, in which modifiers immediatelyfollow what they modify without connectingparticles.
For example, ?the student?s book?
canonly be translated in Arabic as NOPQO?
?PR?
ktAbAlTAlb ?book the-student?.3These different phenomena are summarized inTable 1.
It is interesting to point out that Eng-lish phenomena are a middle ground for Arabicand Chinese: in some cases English is closer toArabic and in others to Chinese.Arabic English ChineseOrthography reducedalphabetalphabet CharactersMorphology Rich Poor Very PoorSubject-Verb V SubjSubj VVsubjSubj V Subj ?
VVerb-PP V?PP V?PP PP VV PPAdjectivalModifierN Adj Adj N Adj DE NPossessiveModifierN Poss N of PossPoss ?s NPoss DE NRelativeModifierN Rel N Rel Rel DE NTable 1: Comparing different linguistic phenomenain Arabic, English and Chinese3.4 Quantifying Linguistic DifferencesThe previous section described specific typesof linguistic phenomena without distinguishingthem in terms of frequency or effect distance.For example, Arabic nominals (nouns, adjec-tives and adverbs) are seven times as frequentas verbs; and nominal modification phenomenaare more likely local than long distance com-pared to verb-subject order.
A proper quantifi-cation of these different phenomena requirestrilingual parallel treebanks, which are notavailable.
As such, we propose a simple metricto quantify linguistic differences by measuringthe translation complexity of different languagepairs.
The metric is Average Relative Align-ment Length (ARAL):ARAL = 1| L |paSalab ?L?
?
pbSb3  Arabic dialects allow an additional construction.
Wefocus here on Modern Standard Arabic.175We define L as the set of all alignment linkslinking words in a parallel corpus of languagesA and B.
For each alignment link, lab, linkingwords a and b, we define pa and pb as the posi-tion of words a and b in their respective sen-tences.
We also define Sa and Sb as the lengthsof the sentences in which a and b appear, re-spectively.
ARAL is the mean of the absolutedifference in relative word position (pi/Si) of thewords of every alignment link.
The largerARAL is, the more reordering and inser-tions/deletions we expect, and the more com-plexity and difference.
ARAL is a harsh metricsince it ignores syntactic structure facts that ex-plain how clusters of words move together.A-C A-E E-C0.1679 0.0846 0.1531Table 2: Average Relative Alignment Length forpairs of Arabic (A), English (E) and Chinese (C)Table 2 presents the ARAL scores for each lan-guage pair.
These scores are computed over thegrow-diag-final symmetrized alignment we usein our system (Koehn, 2004).
ARALAC is thehighest and ARALAE is the lowest.
The averagelength of sentences is generally close amongthese languages (given the segmentation weuse): Arabic is ~32 words, English is ~31 andChinese is ~29.
Arabic and English are muchcloser to each other than either to Chinese.
Thismay be the result of Arabic tokenization andChinese segmentation technologies which havebeen developed for translation into English.
Weaddress this issue in section  4.1.
The ARALscores agree with our assessment that English iscloser to Arabic and to Chinese than Arabic isto Chinese.
As a result, we believe it may serveas a good pivot language for translating Arabicto Chinese.4 System DescriptionIn this section, we describe the different sys-tems we compare.4.1 DataOur data collection is the United Nations (UN)multilingual corpus, provided by the LDC 4(catalog no.
LDC2004E12).
The UN corpus hasin principle parallel sentences for Arabic, Eng-lish and Chinese.
However, the Arabic-English4 http://www.ldc.upenn.edu(A-E) data and Chinese-English (C-E) data setswere not in synch.
The A-E data set has 3.2Mlines while the C-E data set has 5.0M lines.
Weused the document ID provided in the data tomatch sentences from A-E against those in C-Eto generate a three-way parallel corpus with2.6M lines.We tokenized the Arabic data in the ArabicTreebank scheme (Sadat and Habash, 2006).Chinese was segmented into words using asegmenter developed by Howard Johnson forthe Portage Chinese-English MT system.5 So asentence consists of multiple words with spacesbetween them and each word is comprised ofone or more characters.
English was simplyprocessed to split punctuation and ??s?.
Thesame preprocessing was used in all systemscompared.We are aware of two potentially biased aspectsof our experimental setting.
First, the Arabicand Chinese portions of our data collection, theUN corpus, are known to be generated fromEnglish originals.
And secondly, the preproc-essing techniques we used on Arabic and Chi-nese were developed for translation from theselanguages into English.
These two aspectsmake English potentially more central to ourexperiments than if the data collection and pre-processing were done on Arabic and Chineseindependent of English.
Of course, it must benoted that the data bias is not unique to ourwork but rather a challenge for any bilingualcorpus, in which translation is done from onelanguage to another.
Additionally, we can ar-gue that the English bias in data and preproc-essing does not only affect the Arabic-Englishand English-Chinese pipelines, but it alsomakes the Arabic and Chinese data potentiallycloser.
Finally, given the expense involved increating direct Arabic-Chinese parallel text andgiven the large amounts of Arabic-English andEnglish-Chinese data, we think our results(with English bias) are still valid and interest-ing.
That said, we leave the question of Arabic-Chinese optimization to future work.4.2 Direct A-C MT SystemIn our baseline direct A-C system, we used theArabic and Chinese portions of our parallelcorpus to train a direct phrase-based MT sys-tem.
We use GIZA++ (Och and Ney, 2003) for5 http://iit-iti.nrc-cnrc.gc.ca/projects-projets/portage_e.html176word alignment, and the Pharaoh system suiteto build the phrase table and decode (Koehn,2004).
The Chinese language model (LM) used200M words from the UN corpus segmented ina manner consistent with our training.
The tri-gram LM was built using the SRILM toolkit(Stolcke, 2002).4.3 Sentence Pivoting MT SystemThe sentence pivoting system (A-s-C) usedEnglish as an interface between two separatepharse-based MT systems: an Arabic-Englishdirect system and an English-Chinese directsystem.
When translating Arabic to Chinese, theEnglish top-1 output of the Arabic-English sys-tem was passed as input to the English-Chinesesystem.
The English LM used to train the Ara-bic-English system is built from the counterpartof the Chinese data used to build the ChineseLM in our parallel corpus.
We use 210M Eng-lish words in total.4.4 Phrase Pivoting MT SystemThe phrase pivoting system (A-p-C) extracts anew Arabic to Chinese phrase table using theArabic-English phrase table and the English-Chinese phrase table.
We consider a Chinesephrase a translation of an Arabic phrase only ifsome English phrase can bridge the two.
We usethe following formulae to compute the lexicaland phrase probabilities in the new phrase tablein a similar manner to Utiyama and Isahara(2007).
Here, ?
is the lexical probability andwp is the phrase probability.
'( | ) ( | ) ( | )ea c a e e c?
?
?=?
'( | ) ( | ) ( | )ec a c e e a?
?
?=?
'( | ) ( | ) ( | )w w wep a c p a e p e c=?
'( | ) ( | ) ( | )w w wep c a p c e p e a=?The left hand side of the formulae represents thefour required probabilities in a Pharaoh Arabic-Chinese phrase table.5 EvaluationFor each of the direct system, the sentence-pivoting system and the phrase-pivoting system,we conduct four sets of experiments with dif-ferent data sizes.
Table 3 illustrates the trainingdata size for each experiment.
The training datais collected from the beginning of the sameparallel corpus, so the larger training sets in-clude the smaller ones.Lines Words (Arabic)S 32500 1 MillionM 65000 2 MillionL 130000 4 MillionXL 260000 8 MillionTable 3: Training Data SizeWe use two other data sets (1K lines each) fortuning and testing.
Each sentence in these setshas only one reference.
Tuning and testing datasets are the same across all experiments andsystems.
In all our experiments, we decodeusing Pharaoh (Koehn, 2004) with a distortionlimit of 4 and a maximum phrase length of 7.Tuning is done for each experimental conditionusing Och?s Minimum Error Training (Och,2003).Note that for each set of experiments with thesame data size, we draw Chinese, Arabic andEnglish from the same chunk of three way par-allel corpus.
For example, in S size experi-ments, the two phrase tables used to build anew table in the phrase-pivoting approach areextracted respectively from the A-E and E-Csystems built in the sentence-pivoting approachwith size S corpora.5.1 Direct System ResultsTable 4 shows the results of the direct transla-tion system A-C.
It also includes the result forA-E and E-C direct translation.
As expected, aswe double the size of the data, the BLEU score(Papineni et al, 2002) increases.
However, therate of increase is not always consistent.
In par-ticular, the M and L conditions vary highly inA-E compared to A-C.
This is odd especiallygiven that we are comparing the same set ofdata from the three parallel corpora.
We specu-late that this may have to do with an oddity inthat portion of the data set that may have a dif-ferent quality than the rest.
We see the effect ofthis drop in A-E in the next section.
BLEU ismeasured on English case-insensitively.
BLEUis measured on Chinese using segmented wordsnot characters.177A-C A-E E-CS 11.17 21.89 19.29M 13.43(+20.2%)23.86(+9.0%)20.85(+8.1%)L 14.62(+8.9%)24.86(+4.2%)22.42(+7.5%)XL 16.17(+10.6%)27.96(+12.5%)24.11(+7.5%)Table 4: BLEU-4 scores comparing performance ofdirect translation of Arabic-Chinese (A-C), Arabic-English (A-E) and English-Chinese (E-C) for fourtraining data sizes.
The percentage increases areagainst the immediately lower data size.5.2 Pivoting System ResultsIn Table 5, we present the results of the sen-tence pivoting system (A-s-C) and the phrasepivoting system (A-p-C).
Under all conditions,A-s-C and A-p-C outperform A-C. A-p-C gen-erally outperforms A-s-C except in the M datacondition.
The effect in the S conditions is big-ger than the XL condition.
In our best result(XL), we increase the BLEU score by over 1.12points.
Furthermore, the relative BLEU scoreincrease from the L condition for A-p-C is15.5% as opposed to A-C?s 10.6%.
The A-s-Crelative increase from L to XL is 12.8%.
Thissuggests that we are making better use of theavailable resources.
The differences between A-s-C and A-C and between A-p-C and A-C arestatistically significant at the 95% confidencelevel (Zhang et al, 2004).
The differences be-tween the two pivoting systems are not statisti-cally significant.
Examples from our bestperforming system are shown in Figure 2.A-C A-s-C A-p-CS 11.17 12.24 9.6% 13.12 17.5%M 13.43 14.10 5.0% 13.75 2.4%L 14.62 14.96 2.3% 14.97 2.4%XL 16.17 16.88 4.4% 17.29 6.9%Table 5: Word-based BLEU-4 scores.
A-C is directtranslation.
A-s-C is indirect translation through sen-tence pivoting and A-p-C is indirect translationthrough phrase pivoting.
The percentages indicaterelative improvement over A-C.Our results are consistent with (Utiyama andIsahara, 2007) in that phrase-pivoting generallydoes better than sentence pivoting.
However,we disagree with them in that, for us, directtranslation is not the best system to use.
We be-lieve that this effect is caused by the combina-tion of the very different languages we use.English is truly bridging between Arabic andChinese in many linguistic dimensions.
Wethink it?s English?s middle-ground-ness thatmakes these results possible.A-C A-s-C A-p-CS 53.75 54.38 1.2% 54.64 1.7%M 56.65 57.00 0.6% 55.88 -1.4%L 58.37 57.69 -1.2% 58.79 0.7%BLEU-1XL 59.90 60.34 0.7% 60.28 0.6%S 21.32 21.80 2.3% 22.88 7.3%M 23.84 24.22 1.6% 23.76 -0.3%L 24.98 25.14 0.6% 25.87 3.6%BLEU-4XL 25.95 27.11 4.5% 27.70 6.7%S 9.82 10.02 2.0% 11.42 16.3%M 11.56 11.84 2.4% 11.64 0.7%L 12.23 12.52 2.4% 13.09 7.0%BLEU-7XL 12.69 13.52 6.5% 14.57 14.8%Table 6: Character-based BLEU scores for n-gramsof maximum size 1, 4, and 7.
The percentages arerelative to the direct system.In Table 6, we present additional scores usingBLEU-1, BLEU-4 and BLEU-7 measured atthe character level as opposed to the harshermeasure at word level.
Ignoring the odd behav-ior in M and L conditions, the sentence-pivotand phrase-pivot approaches improve over thedirect translation baseline in terms of fluency(BLEU-7) and accuracy (BLEU-1).
Under thesmall data condition, the phrase-pivot approachincreases the BLEU-4 score three times theincrease of the sentence-pivot approach.
Thatratio reduces to 1.5 times in the XL condition.The relative improvements of the pivoting sys-tems over the direct system are small at BLEU-1 and much bigger at higher BLEU scores.This suggests that differences between the piv-oting systems and the direct system are not interms of lexical coverage but rather in terms ofbetter reordering.The lengths of the outputs of all the systems(direct and pivoting) are larger than the refer-ence length which means no brevity penaltywas applied in BLEU calculation.
Also, noBLEU-gaming was done by OOV deletion: allOOV words were left in the output.5.3 Error AnalysisWe conducted an error analysis of our best per-forming system (Phrase Pivot XL) to under-stand what issues need to be addressed in the178future.
We took a sample of 50 sentences re-stricted in length to be between 15 and 35 Chi-nese words.
A Chinese native speaker comparedour output to the reference translation andjudged its quality in terms of two categories:syntax and lexical choice.In terms of syntax, our judge identified all theoccurrences of (a) subjects and verbs, (b) prepo-sitional phrases and verbs and (c) modifiednouns.
Each case was judged as acceptable orwrong.
Placing a verb before its subject, a pre-verbal prepositional phrase after its verb, or amodifier after the noun it modifies are all con-sidered wrong.
We correctly produce subject-verb order 73% of the time; and we producenominal modification order correctly 64% ofthe time.
Our biggest weakness in terms of syn-tax is prepositional phrase order.
It is worthnoting that the two phenomena we do better onare addressed in translation from Arabic to Eng-lish, unlike prepositional phrase order which iswhere Chinese is different from both Arabic andEnglish.In terms of lexical choice our judge consideredthe translation quality of three classes of words:Nominals (nouns, pronouns, adjectives and ad-verbs), Verbs, and other particles (prepositions,conjunctions and quantifiers).
An incorrectlytranslated or deleted word is considered wrong.We perform on nominals and particles at aboutthe same level of 90%.
Verbs are our biggestchallenge with accuracy below 80%.
The ratioof deleted words among all wrong words israther high at about 30% (for nominals and forverbs).
The detailed results of the error analysisare shown in Table 7.Finally, there are 27 instances of Arabic Out-of-Vocabulary (OOV) words (1.93% of all words)that are not handled.
Ten (37%) of these areproper nouns.
The rest belong to mostly nounsand adjectives.
Orthogonally, 19 (70%) of allOOV words belong to the genre of science re-ports, which is quite different from the data wetrain on.
The OOVs include complex terms like_`aPb?cde?fg`bO?
AlsybrwflwksAsyn ?ciproflox-acin?
and hi?
?kl ?PnPn?
rjAjAt mdAry?
?
[chemi-cal] orbital shakers?.
Other less frequent OOVcases involve bad tokenization and less com-mon morphological constructions.Total Acceptable WrongSubj-Verb 48 35(73%) 13 (27%)Verb-PP 46 17 (37%) 29 (63%)SyntaxNoun-Mod 97 62 (64%) 35 (36%)Nominal 408 368 (90%) 40 (10%)Verb 124 98 (79%) 26 (21%)LexicalChoiceParticle 116 106 (91%) 10 (9%)Table 7: Results of human error analysis on asample from the A-p-C system (XL)6 Conclusion and Future WorkWe presented a comparison of two approachesfor Arabic-Chinese MT using English as a piv-ot language against direct MT.
Our resultsshow that using English as a pivot in either ap-proach outperforms direct translation fromArabic to Chinese.
We believe that this is aresult of English being a sort of middle groundbetween Arabic and Chinese in terms of differ-ent linguistic features (in particular word or-der).
Our best result is the phrase-pivot systemwhich scores higher than direct translation by1.1 BLEU points.
An error analysis of our sys-tem shows that we successfully handle manycomplex Arabic-Chinese syntactic variationsalthough there is a large space for improvementstill.In the future, we plan on exploring tighter cou-pling of Arabic and Chinese through compar-ing different methods of preprocessing Arabicfor Arabic-Chinese MT, in a similar manner toSadat and Habash (2006).
We also plan tostudy how well these results carry on to differ-ent corpora (bilingual Arabic-English and Eng-lish-Chinese) as opposed to the trilingualcorpus used in this paper.
We also plan to in-vestigate whether our findings in Arabic-English-Chinese can be used for other differentlanguage triples.AcknowledgementsWe would like to thank Roland Kuhn, GeorgeFoster and Howard Johnson of the NationalResearch Council Canada for helpful adviceand discussions and for providing us with theChinese preprocessing tools.179Figure 1: An example highlighting Arabic-English-Chinese syntactic differencesFigure 2: Examples of Arabic-Chinese MT output.
English references and Englishglosses for Arabic and Chinese are provided to ease readability.Arabic k`op kq rO??
?PuvO??
?kox?
?
?PbudO hzfol h{`gO?
?
{| ?Pe ?
?O?
rd?
?P?p?
.and-building upon this , therefore  this environment susceptible to-corruption and-lack qualifica-tion to extent big .Eng-Ref Consequently , this environment lends itself to significant degrees of corruption and inefficiency .Chn-Ref ??,????????????????
?Therefore, this kind environment caused have high-degree corruption and efficiency low.Chn-Out ??,???????????
????????
?Therefore , this kind environment inside DE corruption and lack efficiency on big degree top.Arabic  ?c??
?e hpcdQ?O?
?Plcdo?O?
?f?
?O 90????
NdQO?
?
?bi ?
?f??
Plci .and-if did-not arrive information requested in period 90 day other , lapse application .Eng-Ref If the requested information is not received within a further 90 days ,  the application will lapse.Chn-Ref ????
90??????
?????,?????
?If again pass 90 day yet not received requested DE information , then application loose validity.Chn-Out ???????
?????
90?????????
?If not receive requested DE information 90 day within provide more DE request.Arabic ... h`lcv?O?
???n??
_`p ???c?O?
?e ??P?RO?
??Plcdo?O??
?Pg?
f`b`?
.?
facilitation exchanging the-information and-the-sharing in the-resources between the-agenciesthe-governmental .Eng-Ref ?
to facilitate the sharing of information and resources between government agencies .Chn-Ref ????????????????????
?
?for all government agencies among exchanging information and resource offer convenience.Chn-Out ???????????????????
?...purpose in facilitate information exchanging and sharing resource governments among agency .Arabic  ?PbuO?
??P?Rq?
_l rx???
k?O?
rO?
?`d?RdO hOPoe?
hgaP?l f`p?k?
??k?Ra?
?e f???
?
?
?Plcv?dO ?
?g?i?.and-should to-government that look in introducing measures appropriate and-effective to-reduceto extent least from possibilities the-corruptionEng-Ref Governments should consider introducing appropriate and effective measures to minimize thepotential for corruption.Chn-Ref ????????????????,??
???
??????????
?all countries governments should consider adopt appropriate DE effective methods , to-biggest-extent DE reduce producing corruption DE possibility.Chn-Out ?
???????????????,?????????????
?all countries governments should consider build appropriate DE effective methods , to-biggest-extent DE reduce corruption DE possibility.
?f?i1NOPQO2 ?k?R?
?O3 ?PpPR5 ?_4 ?_`?O6 ?
?e 7?
?O8  ?.yqr?1 AlTAlb2 Almjthd3 ktAbA4 ?n5 AlSyn6 fy7 AlSf8 .read1 the-student2 the-diligent3 a-book4 about5 china6 in7 the-classroom8 .?
1?
2??
3              ?
4  ??
5        ?
6??
7            ?
8     ?
9  ?
10      ??
11      ??
12          ?
13?
14?this1 quant2 diligent3 de4  student5     in6    classroom7 read8 one9 quant10 about11     china12           de13    book14Zhe1 Wei2 Qin Fen3        De4   Xue Sheng5 Zai6  Jiao Shi7       Du8     Yi9    Ben10      Guan Yu11 Zhong Guo12 De13   Shu14The diligent student is reading a book about China in the classroom.180ReferencesYaser Al-Onaizan and Kishore Papineni.
2006 Dis-tortion models for statistical machine transla-tion.
In Proceedings of Coling-ACL?06.Sydney, Australia.Bogdan Babych, Anthony Hartley, and SergeSharoff.
2007.
Translating from under-resourced languages: comparing direct transferagainst pivot translation.
In Proceedings of MTSummit XI, Copenhagen, Denmark.Tim Buckwalter.
2002.
Buckwalter Arabic morpho-logical analyzer version 1.0.Chris Callison-Burch, Philipp Koehn, and MilesOsborne.
2006.
Improved statistical machinetranslation using paraphrases.
In Proceedingsof HLT-NAACL?06.
New York, NY, USA.Marine Carpuat and Dekai Wu.
2007.
Context-dependent phrasal translation lexicons for sta-tistical machine translation.
In Proceedings ofMT Summit XI, Copenhagen, Denmark.Josep M. Crego and Jos?
B. Mari?o.
2007.
Syntax-enhanced n-gram-based SMT.
In Proceedingsof MT Summit XI, Copenhagen, Denmark.Karim Filali and Jeff Bilmes.
Leveraging MultipleLanguages to Improve Statistical MT WordAlignments.
In Proceedings of ASRU?05, Can-cun, Mexico.Nizar Habash.
2007.
Syntactic preprocessing forstatistical machine translation.
In Proceedingsof MT Summit XI, Copenhagen, Denmark.Nizar Habash, Abdelhadi Soudi, and Tim Buckwal-ter.
2007.
On Arabic Transliteration.
In A. vanden Bosch and A. Soudi, editors, Arabic Com-putational Morphology: Knowledge-based andEmpirical Methods.
Springer.Hitoshi Isahara, Sadao Kurohashi, Jun?ichi Tsujii,Kiyotaka Uchimoto, Hiroshi Nakagawa, Hiro-yuki Kaji, and Shun?ichi Kikuchi.
2007.
De-velopment of a Japanese-Chinese machinetranslation system.
In Proceedings of MTSummit XI, Copenhagen, Denmark.Philipp Koehn.
2004.
Pharaoh: a Beam Search De-coder for Phrase-based Statistical MachineTranslation Models.
In Proceedings ofAMTA?04, Washington, DC, USA.Shankar Kumar, Franz Och, and Wolfgang Ma-cherey.
2007.
Improving word alignment withbridge languages.
In Proceedings of EMNLP-CoNLL?07, Prague, Czech Republic.Young-Suk Lee.
2004.
Morphological Analysis forStatistical Machine Translation.
In Proceedingsof HLT-NAACL?04, Boston, MA, USA.Franz Josef Och and Hermann Ney.
2003.
A Sys-tematic Comparison of Various StatisticalAlignment Models.
Computational Linguistics,29 (1):19?52.Franz Josef Och.
2003.
Minimum Error Rate Train-ing for Statistical Machine Translation.
In Pro-ceedings of ACL?03, Sapporo, Japan.Fatiha Sadat and Nizar Habash.
2006.
Combinationof Arabic preprocessing schemes for statisticalmachine translation.
In Proceedings of Coling-ACL?06.
Sydney, Australia.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a Method forAutomatic Evaluation of Machine Translation.In Proceedings of ACL?02, Philadelphia, PA,USA.Charles Schafer & David Yarowsky.
2002.
Induc-ing translation lexicons via diverse similaritymeasures and bridge languages.
In Proceedingsof CoNLL?02, Taipei, Taiwan.Micheal.
Simard.
1999.
Text translation alignment:Three languages are better than two.
In Pro-ceedings of EMNLP-VLC?99, College Park,MD, USA.Michel Simard, Nicola Ueffing, Pierre Isabelle, andRoland Kuhn.
2007.
Rule-based translationwith statistical phrase-based post-editing.
InProceedings of the workshop on StatisticalMachine Translation, ACL?07, Prague, CzechRepublic.Andreas Stolcke.
2002.
SRILM - an ExtensibleLanguage Modeling Toolkit.
In Proceedings ofICSLP?02, Denver, CO, USA.Masao Utiyama and Hitoshi Isahara.
2007.
A com-parison of pivot methods for phrase-based sta-tistical machine translation.
In Proceedings ofNAACL-HLT?07, Rochester, NY, USA.Chao Wang, Michael Collins, and Philipp Koehn.2007.
Chinese syntactic reordering for statisti-cal machine translation.
In Proceedings ofEMNLP-CoNLL?07, Prague, Czech Republic.Dekai Wu.
1998.
A Position Statement on ChineseSegmentation.
Presented at the Chinese Lan-guage Processing Workshop.
http://www.cs.ust.hk/~dekai/papers/segmentation.html.Hua Wu and Haifeng Wang.
2007.
Pivot languageaproach for phrase-based statistical machinetranslation.
In Proceedings of ACL?07, Prague,Czech Republic.Yang Ye, Karl-Michael Schneider, and StevenAbney.
2007.
Aspect marker generation inEnglish-to-Chinese machine translation.
InProceedings of MT Summit XI, Copenhagen,Denmark.Ying Zhang, Stephan Vogel and Alex Waibel, In-terpreting Bleu/NIST scores: How much im-provement do we need to have a better system?,In Proceedings of LREC?04, Lisbon, Portugal.181
