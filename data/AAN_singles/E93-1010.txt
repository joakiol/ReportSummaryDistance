Head-driven Parsing for Lexicalist Grammars:Experimental ResultsGosse  Bouma & Ger t jan  van  Noordvakgroep Alfa- informatica, University of GroningenPostbus 716NL 9700 AS GroningenAbstractWe present evidence that head-driven pars-ing strategies lead to efficiency gains overstandard parsing strategies, for lexicalist,concatenative and unification-based gram-mars.
A head-driven parser applies a ruleonly after a phrase matching the head hasbeen derived.
By instantiating the headof the rule important information is ob-tained about the left-hand-side and theother elements of the right-hand-side.
Wehave used two different head-driven parsersand a number of standard parsers to parsewith lexicalist grammars for English andfor Dutch.
The results indicate that forimportant classes of lexicalist grammars itis fruitful to apply parsing strategies whichare sensitive to the linguistic notion 'head'.1 Introduct ionLexicalist grammar formalisms, uch as Head-drivenPhrase Structure Grammar (HPSG) and CategorialUnification Grammar (CUG) have two characteristicproperties.
Lexical elements and phrases are associ-ated with categories that have considerable internalstructure.
Second, instead of construction specificrules, a small set of generic rule schemata is used.Consequently, the set of constituent structures de-fined by a grammar cannot be 'read off' the rule setdirectly, but is defined by the interaction of the ruleschemata and the lexicM categories.Applying standard parsing algorithms to suchgrammars is unsatisfactory for a number of rea-sons.
Earley parsing is intractable in general, as therule set.
is simply too general.
For some grammars,naive top-down prediction may even fail to termi-nate.
\[Shieber, 1985\] therefore proposes a modifiedversion of the Earley-parser, using restricted top-down prediction.
While this modification leads totermination ofthe prediction step, in practice it eas-ily leads to a trivial top-down prediction step, thusleading to inferior performance.Bottom-up arsing is far more attractive for lexi-calist formalisms, as it is driven by the syntactic in-formation associated with lexical elements.
Certaininadequacies remain, however.
Most importantly,the selection of rules to be considered for applicationmay not be very efficient.
Consider, for instance, thefollowing DCG rule:s(\[ \]) -~ Arg, vp(\[Arg\]).
(1)A parser in which application of a rule is driven bythe left-most daughter, as it is for instance in a stan-dard bottom-up active chart parser, will consider theapplication of rule (1) each time an arbitrary con-stituent Arg is derived.
For a bottom-up active chartparser, for instance, this may lead to the introduc-tion of large amounts of active items.
Most of theseitems will be useless.
For instance, if a determineris derived, there is no need to invoke the rule in (1),as there are simply no vP's selecting a determiner assubject.Parsers in which the application of a rule is drivenby the rightmost daughter, such as shift-reduce andinactive bottom-up chart parsers, encounter a similarproblem for rules such as (2).vp(Args) --* vp(\[Arg\[Args\]), Arg.
(2)Each time an arbitrary constituent Arg is derived,the parser will consider applying rule (2), and asearch for a matching vP-constituent will be carriedout.
Again, in many cases (if Arg is instantiated as71a determiner o preposition, for instance) this searchis doomed to fail, as a vp subcategorizing fora cat-egory Arg may simply not be derivable by the gram-mar.
The problem may seem less acute than thatposed by uninstantiated left-most daughters for anactive chart parser, as only a search of the chart iscarried out and no additional items are added to it.Note, however, that the amount of search requiredmay grow exponentially, if more than one uninstan-tiated daughter is present (3) or if the number ofdaughters i not specified by the rule (4), as appearsto be the case for some of the rule-schemata used inHPSG:vp(Args) --* vp(\[A1, A2\]Args\]), A1, A2.
(3)vp\[Ao\]) --+ vp(\[Ao,..., AnD, A1,..., An.
(4)Several authors have suggested parsing algorithmswhich appear to be more suitable for lexicalist gram-mars.
\[Kay, 1989\] discusses the concept of head-driven parsing.
The key idea underlying this conceptis that the linguistic notion head can be used to ob-tain parsing algorithms which are better suited fortypical natural language grammars.
Most linguisticformalisms assume that among the daughters intro-duced by a rule or rule-schema there is one daugh-ter which can be identified as the head of that rule.There are several criteria for deciding which daugh-ter isthe head.
Two of these criteria seem relevantfor parsing.
First of all, the head of a rule deter-mines to a large extent what other daughters may ormust be present, as the head subcategorizes for theother daughters.
Second, the syntactic ategory andmorphological properties of the mother node are, inthe default case, identical to the category and mor-phological properties of the head daughter.
Thesetwo properties uggest hat it might be possible todesign a parsing strategy in which one first identifiesa potential head of a rule, before starting to parsethe non-head aughters.
By starting with the head,important information about the remaining daugh-ters is obtained.
Furthermore, since the head is toa large extent identical to the mother category, ef-fective top-down identification of a potential headshould be possible.
A head-driven parsing strategyis particularly interesting for lexicalist grammars, asthese grammars normally suffer most from the prob-lem that rules or rule-schemata h rdly constrain thesearch-space of the parser.In \[Kay, 1989\] two different head-driven parsersare presented.
First, a 'head-driven' shift-reduceparser is presented which differs from a standardshift-reduce parser in that it considers the applica-tion of a rule (i.e.
a reduce step) only if a categorymatching the head of the rule has been found.
Fur-thermore, it may shift elements onto the parse-stackwhich are in a sense similar to the active items (or'dotted rules') of active chart parsers.
By using thehead of rule to determine whether a rule is appli-cable, the head-driven shift-reduce parser avoids thedisadvantages of parsers in which either the leftmostor rightmost daughter is used to drive the selectionof rules.Kay also presents a 'head-corner' parser.
Thestriking property of this parser is that it does notparse a phrase from left to right, but instead oper-ates 'bidirectionally'.
It starts by locating a poten-tial head of the phrase and then proceeds by parsingthe daughters to the left and the right of the head.Again, this strategy avoids the disadvantages ofparsers in which rule selection is uniformly driven byeither the leftmost or rightmost daughter.
Further-more, by selecting potential heads on the basis of a'head-corner table' (comparable tothe left-corner ta-ble of a left-corner parser) it may use top-down filter-ing to minimize the search-space.
Head-corner pars-ing has also been considered elsewhere.
In \[Satta ndStock, 1989; Sikkel and op den Akker, 1992\] chart-based head-corner parsing for context-free grammaris considered.
It is shown that, in spite of the factthat bidirectional parsing seemingly leads to moreoverhead than left-to-right parsing, the worst-casecomplexity of a head-corner parser does not ex-ceed that of an Earley parser.
\[van Noord, 1991;van Noord, 1993\] argues that head-corner parsing isespecially useful for parsing with non-concatenativegrammar formalisms.
In \[Lavelli and Satta, 1991\]a head-driven parsing strategy for Lexicalized TreeAdjoining Grammars i presented.Although it has been suggested that head-drivenparsing has benefits for lexicalist grammars, thishas not been established in practice.
The poten-tial efficiency gains of a head-driven parser are of-ten outbalanced by the cost of additional overhead.This is particularly true for the (bidirectional) head-corner parser.
The results of the experiment wedescribe in section 3 establish that efficient head-driven parsing is possible.
That is, we show thatfor a radical exicalist grammar (based on CUG) abottom-up head-driven chart parser (a chart-basedbreadth-first implementation f Kay's head-drivenshift-reduce parser) is more efficient han standardpure bottom-up chart parsers.
Also, we show thatfor a lexicalist (definite clause) grammar in which therules still contain a substantial mount of informa-tion, (bidirectional) head-corner parsing, in which abottom-up arsing strategy is guided by top-downprediction, is more efficient than pure bottom-upparsing as well as left-corner parsing.Before discussing the experiment, however, we firstdiscuss the two head-driven parsers used in the ex-periment, and how they relate to standard parsingalgorithms.2 Two Head-dr iven  ParsersIn this section we present wo head-driven parsingalgorithms.
Prolog code for simplifications of the al-gorithms i included in the appendix.
For each gram-mar rule LHS --~ D1,...,  Dh,.. .
,  Dn, it is assumed72goal?
A ?lexgoal.AFigure 1: The head-corner parser.that there is one daughter Dh which has been iden-tified (by the grammar writer) as the head of thatrule.2.1 Head-dr iven  Char t  Pars ingThe head-driven chart parser scans a sentence fromleft to right, storing items representing (partial)derivations in a chart.
Items are of the formitem(Cat, ToParse, BeginPos, EndPos).
If ToParseis empty, the item is inactive, otherwise it is ac-tive.
The parser is a bottom-up active chart parserwithout prediction, in which the addition of an ac-tive item based on a rule R is considered when-ever an inactive item H is entered into the chartwhich matches the head of R. More precisely, ifitem(Cat, \[ \], B, E) is derived, and there is a ruleLHS --* D1,...,Dh-1, Ca~,Dh+l, .... Dn and thereare inactive items matching D1...Dh-1, rangingfrom B0 to B, an iIem(LHS, Dh+I...Dn,Bo, E) isadded to the chart.If the leftmost daughter of each grammar ule isthe head of the rule, then the head-driven chartparser educes to an ordinary bottom-up active chartparser.
If the rightmost daughter of each rule is thehead, then the head-driven chart parser educes to aninactive bottom-up chart parser (i.e.
a breadth-firstimplementation f a shift-reduce parser).The head-driven strategy has a potential advan-tage over active bottom-up chart parsers, as it willassert substantially less active items for grammarsthat contain rules with an underspecified leftmostdaughter (as in rule 1).
In particular it avoids enter-ing active items into the chart for which it is clearthat the missing daughters cannot be derived.The head-driven parser also has a potential advan-tage over inactive bottom-up chart parsers for gram-mars that contain rules with an underspecified right-most daughter.
An inactive chart parser must searchin the chart for items matching the remaining daugh-ter of such a rule each time an arbitrary category isderived.
The head-driven parser on the other handonly needs to search for matching active items.
Thedifference may lead to important efficiency improve-ments, especially if searching the chart is expensive.For example this is the case if the unification opera-tion is expensive.2.2 Head-corner  Pars ingHead-corner parsing is a more radical approach tohead-driven parsing in that it gives up the idea thatparsing should proceed form left to right?
Rather,the order of processing in a head-corner parser isbidirectional, starting from a head outward ('island'-driven)?
A head-corner parser can be thought of as ageneralization of the left-corner parser \[Rosenkrantzand Lewis-II, 1970\].
As in the left-corner parser, theflow of information in a head-corner parser is bothbottom-up and top-down.The basic idea of the head-corner parser is illus-trated in figure 1.
The parser selects the head of thestring (1), and proves that this element is the head-corner of the goal.
To this end, a rule is selected ofwhich this lexical entry is the head daughter.
Thenthe other daughters of the rule are parsed recursivelyin a bidirectional fashion: the daughters left of thehead are parsed from right to left (starting from thehead), and the daughters right of the head are parsedfrom left to right (starting from the head).
The re-sult is a slightly larger head-corner (2).
This processrepeats itself until a head-corner is constructed whichdominates the whole string (3).Note that a rule is triggered only with a fully in-stantiated head-daughter.
The 'generate-and-test'behavior observed for example 1is avoided in a head-corner parser, because the rule is applied only if thevP is found, and hence Arg is instantiated.
For ex-ample if At# = np(sg3, \[\], Snbj), the parser continuesto search for a singular NP, and need not considerother categories.The head-relation holds between two categories hand m with respect o a grammar G iff G contains arule with left hand side m and head daughter h. Therelation 'head-corner' is the reflexive and transitiveclosure of the head relation.
As in the left-corner'parser, a 'linking' table is maintained which repre-sents important aspects of this head-corner relation.For some grammars this table simply represents thefact that the HEAD features of a category and itshead-corner are shared?Note that unlike the left-corner parser, the head-corner parser may need to consider alternative wordsas a possible head-corner of a phrase, e.g.
when pars-ing a sentence which contains several verbs?
Thisproblem is reduced because of the following threeobservations.The  Quicksor t  Effect.
A simplified version of thehead-corner parser is provided in the appendix.
Themain difference with a simple version of the left-corner parser is - -  apart from the head-driven se-73lection of rules - -  the use of two pairs of indices, toimplement the bidirectional way in which the parserproceeds through the string.Observe that each parse-goal in the left-cornerparser is provided with a category and a left-mostposition.
In the head-corner parser a parse-goal isprovided either with a begin or end position (de-pending on whether we parse from the head to theleft or to the right) but also with the extreme posi-tions between which the category should be found.In general the parse predicate is thus provided with acategory and two pair of indices.
The first pair indi-cates the begin and end position of the category, thesecond pair indicates the extreme positions betweenwhich the first pair should lay.
The following figureillustrates this point with an example:vpv np5 6 7 8Suppose we found for a goal category s a possiblehead-corner v from position 5 to 6.
In order to con-struct a complete tree s for this head-comer, a ruleis selected which dictates that a category np shouldbe parsed to the right, starting from position 6.
Toparse np, we predict the head-corner n between 7and 8.
Suppose furthermore that in order to connectn to np a rule is selected which requires a categoryadjp to the left of n. It will be clear that this cat-egory should end in position 7, but can never startbefore position 6.
Hence the only candidate head-corner of this phrase is to be found between 6 and7.
This example illustrates that the use of two pairsof string positions reduces the number of possiblehead-corners for a given goal.String posit ions in head-corner table.
Sec-ondly, the head-corner table includes informationabout begin and end positions, following an idea in\[Sikkel and op den Akker, 1992\].
For example, if thegoal is to parse a phrase with category sbar from po-sition 7, and within positions 7and 12, then for somegrammars it can be concluded that the only possiblehead-corner for this goal should be a complementizerstarting at position 7.
Such information is compiledinto the table as well.
Hence the number of possiblehead-corners is reduced.Well-formed substr ing tables.
Thirdly, theproblem of multiple possible heads is reduced be-cause a well-formed substring table is maintained.This is implemented by a memo-ization technique.This reduces the problem because ven if the wronghead-corner is predicted for a given goal, it may turnout to be the case that the computations based onthis wrong prediction may be useful ater (each lexi-cal category usually is the head of some projection).The well-formed substring table is implementedusing an interesting eneralization f the subsump-tion relation.
A goal need not be investigated any-more if a more general goal has already been com-pleted.
It is easy to see that a certain goal withextreme positions 3 to 6 is more general than an oth-erwise identical goM with extreme positions 4 and 6.Head-dr iven vs. functor-dr lven parsing.
Forcategorial unification grammars in which we choosethe functor as the head of a rule, the head-cornertable is not going to be discriminating, because thegrammar ules in such a grammar may simply be(in DCG notation, given appropriate operator defi-nitions): 1Val ---* Val/ Arg, ArgPal --* Arg, Arg\ Val (5)As no information about word-class or morphologyis stated in the rules, such information will not befound in the head-corner table.A possibly useful approach ere is to compile somelexical information i to the rule set, along the linesproposed in \[Bouma, 1991\].
In that paper it is pro-posed to compile lexical information i to the rule-set,and parse with this 'enriched' rule-set.
What seemsto be most useful here, is to use this enriched gram-mar only for the compilation of the head-corner ta-ble.
The parser then uses the general rule schematathemselves.However, given the usual analysis of modifiers asfunctors, even this approach may fail to yield an in-teresting head-corner table.
Note that some analysesin categorial grammar prescribe that even in suchcases certain morphological features are shared be-tween the functor and its resulting value \[Bouma,1993\].2.3 ComparisonThe important differences between both head-drivenparsing algorithms can be summarized as follows(see Mso table 1).
Firstly the head-driven chartparser proceeds from left-to-right as usual, whereasthe head-corner parser proceeds bidirectionally.
Sec-ondly, the head-driven chart parser is an active chartparser (i.e.
it also stores partial analyses of phrases);1 the second author prefers to write the second rule asVal .-~ Arg, Val~Arg74the head-corner parser uses memo-ization of theparse predicate and the head-corner predicate (i.e.it only stores complete analyses of phrases, and par-tim analyses of head-corners).We also implemented an active head-corner chartparser along the lines of \[Sikkel and op den Akker,1992\], but preliminary experiments indicate that(our implementation f) this parser is not useful forthe grammars used in the experiments to be dis-cussed in the next section.
Note that it is not possibleto incorporate top-down filtering in the head-drivenchart parser in a simple way, because the necessaryactive items may not be available yet.Thirdly, although in both algorithms the way rulesare applied is bottom-up in an important sense, thereis an important flow of information in top-down di-rection in the head-corner parser.
For grammars inwhich the head-corner table is discriminating, thisshould have important effects in practice.
This ex-pectation is confirmed in the experiments discussedin the next section.3 The  exper imentThis section describes experimental results for theparsing algorithms discussed above, in comparisonwith some obvious alternative strategies.
The exper-iment consists of two parts.The first part of the experiment compares pars-ing strategies which proceed in a bottom-up fash-ion without the use of any top-down prediction.
ForCUG such parsers are suitable as no top-down in-formation can be compiled from the rule schematain a simple way.
2 It turns out that the head-drivenbottom-up chart parser performs better than bothan inactive and an active bottom-up chart parser,for a particular CUG for English.
If the cost of uni-fication is relatively high, the use of the head-drivenchart parser pays off.
If unification is cheap, then theinactive chart parser may still be the most efficientchoice.The second part of the experiment concentrates onthe comparison between the head-corner parser andthe left-corner parser.
Both of these parsers proceedin a bottom-up fashion, but use important top-downprediction.
Such parsers are interesting for gram-mars in which interesting top-down information canbe extracted from the rule schemata.
It can be con-cluded from the experiment that for a specific lexi-calist Definite Clause Grammar for Dutch the head-corner parser performs much better than the left-corner parser.These results indicate that at least for some gram-mars it is fruitful to apply parsing strategies whichare sensitive to the linguistic notion 'head'.A CUG for Engl ish.
The first grammar is aCUG for English which includes rules for leftward2but see the discussion on head-driven vs. functor-driven parsing in the previous section.and rightward application and four construction spe-cific rules to implement gap-threading.
The gram-mar covers the basic sentence types (declaratives,WH and yes-no questions, and relative clauses) anda wide range of verbal and adjectival subcategoriza-tion types.
PPs may modify nouns as well as vPs,leading to so-called PP-attachment ambiguities.
Thesyntax of unbounded ependency constructions itreated rather extensively, including accounts of con-straints on extraction, pied-piping, and the possibil-ity of nested dependencies (as in which violin is thissonata easy to play on).
The grammar is defined interms of feature-structures, which may be combinedusing feature-unification.
Furthermore, the treat-ment of nested dependencies u es lists of gaps.
Theinteraction of these lists with certain lexical entries(such as easy) as well as the interaction of these listswith the checking of island-constraints requires thatattempts at cyclic unifications must be detected andmust fail.
Therefore, the feature-unification proce-dure includes an occurs check.If the standard techniques for compiling a left-corner resp.
a head-corner table are applied for thisgrammar, then, at best, the 'trivial' link would re-sult, because the rule schemata do not specify anyinteresting information about morphological featuresetc.A lexical ist DCG for Dutch .
This grammar isa definite clause grammar for Dutch, in which sub-categorization requirements are implemented usingsubcat-lists.
The grammar handles topicalization us-ing gap-threading.
Verb-second is accounted for bya feature-based simulation of head-movement.
Thegrammar analyses cross-serial dependencies by con-catenating subcategorization lists (implemented asdifference lists).
As opposed to the CUG grammar,the second grammar uses actual 'empty elements' tointroduce the traces corresponding to the topicalizedphrases and verbs occurring in second position.
An-other difference with the first grammar is that first-order terms are used, rather than feature structures.The compilation of the left-corner resp.
the head-corner table was done using the same restrictor.
Theleft-corner table contained 94 entries, and the head-corner table contained 25 entries.The parsers.
The parsers used in the experimenthave a number of important properties in common(see table 1).
First of all, they all use a chart to rep-resent (partially or fully developed) analyses of sub-strings.
Second, as categories are feature-structuresor terms, rather than atomic symbols, special re-quirements are needed to ensure that the chart isalways 'minimal'.
That is, items are only added tothe chart if no subsuming item exists, and, if an itemis added to the chart, all more specific items aredeleted from the chart.
Finally, information aboutthe derivational history of phrases is added to thechart in such a way that parse-trees can be recovered.75"well-formed substringspackingsubsumption-checkingactive itemsleft-to-right processingtop-down filteringhead-driven processinginact++++hdc act lc hc+ '+ + ++ + + ++ + + ++ + + -+ + + -- + ++ - +Table 1: The parsers used in the experimentitems recognitionn parses hdc inact act hdc inact act# % % see % %1 25 67 170 .8 63 1911 43 73 180 .9 87 1999 2 89 74 179 2.5 91 20812 3 141 75 181 4.0 102 21115 4 193 79 184 5.5 111 21418 6 254 82 184 7.0 124 21521 32 369 84 181 10.9 135 22424 98 452 86 181 13.7 140 22527 55 472 87 185 14.1 142 23330 95 592 87 179 19.9 144 218parsinghdc inact actsec  % %1.1 67 1681.4 90 1643.6 94 1796.2 101 1758.2 109 18010.8 113 17530.0 117 14787.0 106 12029.7 119 164172.7 107 120Table 2: Results for the English grammarThis is done by using 'packed structures' (also called'parse-forests') to obtain structure sharing in the caseof ambiguities; emantic onstraints (if present) areonly evaluated when the syntactic analysis phase iscompleted.
Our implementation f 'packing' followsthat of \[Moore and Alshawi, 1992\], who implementit for a (unification-based) left-corner parser.Three different bottom-up chart parsers are im-plemented.
The first one (hdc) is the head-drivenchart parser presented above, in which the head ofthe rule is given by the grammar writer.
The ac-tive chart parser (act) is the same as the head-chartparser, but now it is assumed that for each rule theleft-most daughter is the head (active chart).
Theinactive chart parser (inact) is a version of the head-corner parser where each right-most daughter is as-sumed to be the head of the rule.
Since the parserdoes not use active items, some (slight) simplifica-tions of the head-driven chart parser were possible.The left-corner parser is a generalized version ofthe chart-based left-corner parser of \[Rosenkrantzand Lewis-II, 1970\].
As we also add items toconstruct parse-trees using 'packing', the resultingarser should be comparable to the CLE parseroore and Alshawi, 1992\].
The head-corner parseris the parser discussed in the previous ection, aZWe also implemented a generalized Earley parser.This parser was extremely slow for all sentences ofbothgrammars.Results for CUG.
One hundred arbitrarily cho-sen sentences (10 of length 3, 10 of length 6, etc.
)were parsed, using the three pure bottom-up arsers(hde, inact, and act).
The columns in table 2 give, foreach sentence length (column 1), the average num-ber of readings (column 2), the average number ofitems produced by hdc, and the average percentageof items produced by inaet and act, when comparedwith hdc (columns 3-6), the average time it took hdcto parse a sentence without recovering the differentanalyses and the average percentage of time neededfor inact and act to do that (columns 7-9), and fi-nally the average time it took to parse a sentenceand recover all analysis trees for hde and the aver-age percentage of time needed by inact and act to dothat.The number of chart items illustrate clearly thathdc combines features of an inactive chart parserwith that of an active chart-parser.
Note that, inspite of the fact that English is mostly a head-initiallanguage, act produces 80% more items than hdc,whereas inact almost produces 80% of the items pro-duced by hdc.
For languages which are predomi-nantly head-final, the difference between act and hdcwill probably be larger, whereas that between iaactand hdc should be smaller.The recognition times how that an active bottom-up chart parser is two-times slower for this grammarthan a head-driven chart parser.
The difference be-tween the inactive chart parser and the head-driven76n parses hesec3 1 .36 2 .89 6 1.212 5 2.015 9 3.118 16 5.121 20 7.424 23 10.227 61 13.830 87 17.3recognition parsinghdc lc act inact hc hdc lc act% % % % sec % % %2647 80 2804 390 .5 1699 79 i7595407 343 5968 1044 1.6 3698 215 4265550 1170 2.9 334428 2333 3.8 285355 2521 6.7 210248 2408 10.7 160195 1918 15.3 127147 19.8 104209 34.3 131145 62.4 102inact%42813001474Table 3: Results for the Dutch grammar.
For parsers which did not succeed within a given period, the entryin the table has not been filled in.parser is less extreme, and is notably in favor of thehead-driven parser only for relatively long and com-plex (in terms of number of analyses) sentences.
Nev-ertheless, the difference is of enough significance toestablish the superiority of a head-driven strategy inthis case.The final three columns show that if recovery ofparse trees is taken into account as well, the differ-ences are much less extreme.
The reason for this dif-ference is simply that recovery (for which we used anEarley-style top-down algorithm which reconstructsexplicit analysis trees on the basis of inactive items)may take up to eight times as long as doing parsingwithout recovery.
Since the amount of time neededfor recovery is (approximately) equal for all threeparsers, this explains why the relative differences aremuch smaller in this case.The head-corner parser was applied to the samegrammar and sentence set as well.
It behaves muchworse (up to 100 times as slow for recognition of 24-words sentences) than the parsers listed in the ta-bles due to the lack of guiding top-down information.The left-corner parser without top-down predictionreduces to the active chart parser.We also applied the same sentence set to a com-piled version of the same CUG.
In this compiled ver-sion first-order terms were used, rather than featurestructures.
Furthermore, we used ordinary Prologunification on such terms rather than the previouslymentioned feature unification including occurs check.This implied that we had to forbid multiple extrac-tions in the compiled version of the grammar.
Ex-periments indicate that in such cases the inactivechart parser performs consistently better than boththe head-driven chart parser and the active chartparser.
This should not come as a surprise giventhe discussion in section 2.1 where we expected thehead-driven chart parser to be useful for grammarswith an 'expensive' unification operation.Resu l ts  for  the  DCG.
The next table encodesthe results for the Dutch grammar (cf.
table 3).Again, one hundred sentences were chosen (ten ofthree words, ten of six words, etc).The head-corner parser improved with a well-formed substring table and packing beats thebottom-up chart parsers.
This is explained by thefact that these parsers proceed strictly bottom-up,whereas the left-corner and head-corner parser em-ploy both top-down and bottom-up information.The top-down information is available through a left-corner resp.
head-corner table, which turn out to bequite informative for this grammar.The head-corner parser performs considerably bet-ter than the left-corner parser on average, especiallyif we only take the recognition phase into account.For longer sentences the differences are somewhatless extreme than for shorter sentences.
This dif-ference is due to the fact that the left-corner parserseems omewhat better suited for grossly ambiguoussentences.
Furthermore, the number of items usedfor the representation f parse trees is not the samefor the left-corner and head-corner parser.
For am-biguous sentences the head-corner parser producesmore useless items, in the sense that such items carnever be used for the construction of an actual parsetree.
As a consequence, it is more expensive to re-cover the parse trees based on this representation,than it is for the recovery of parse trees based on thesmaller epresentation built by the left-corner parser.A few numbers for three typical (long) sentences areshown in table 4.This is a somewhat puzzling result.
Useless itemsare asserted only in case the parser is following adead-end.
However, the fact that the number of use-less items is larger for the head-corner parser thanfor the left-corner parser implies that the head-cornerparser follows more dead-ends, yet the head-cornerparser is much faster during the recognition phase.A possible explanation for this puzzling fact may bethe overhead involved in keeping track of the ac-77hc# parses "items recognition recovery total items# sec sec sec #26 768 12 12 24 503100 1420 20 37 57 83130 543 9 10 19 430lcrecognitionSeErecoverySeE33 843 2920 8totalsee417228Table 4: Comparison of the size of the parse forest for the left-corner and head-corner parser for a few (longer)sentences.tive items in the left-corner parser whereas no ac-tive items are asserted for the head-corner parser.Clearly for grammars with rules that contain manydaughters (unlike the grammar under consideration)the use of active items may start to pay off.Note that we also implemented a version of thehead-corner parser that asserts less useless items bydelaying the assertion of items until a complete head-corner has been found.
However, given the fact thatthis technique leads to a more complex implementa-tion of the memo-ization of the head-corner relation,it turned out that this immediately leads to longerrecognition times, and an overall worse behavior.4 Conc lus ionThe main conclusion to be drawn from the exper-iments discussed above is that the influence of thegrammar can hardly be underestimated.
The parserthat works best for one grammar may easily turn outto be the most inefficient one for a different gram-mar.
This observation also holds for the grammarsdiscussed above even though these are both lexicalistgrammars.Head-corner parsing appears to be superior forgrammars in which the head-corner table containsdiscriminating information.
A typical DCG gram-mar for a head-final language such as Dutch is anexample of such a grammar.
On the other hand, forgrammars in which top-down filtering is difficult toimplement, strictly bottom-up arsing strategies aremore useful, especially if the number of active itemscan be reduced, either by a lazy strategy which neverenters active items in the chart or, even more success-ful for the CUG grammar for English we considered,a head-driven strategy.Clearly many other factors may be relevant in find-ing the best parser for a particular grammar.
Forexample the cost of unification turns out to be animportant factor.
As indicated above a cheap unifi-cation procedure may favor an inactive chart parser,even if in that parser many useless reductions areattempted.
However, if the cost of unification is rel-atively high, the cost of the use of active items toreduce the number of useless reductions, for exam-ple by a head-driven strategy, may be worthwhile.Another result we obtained during the experi-ments is that the use of a head-corner and left-cornertable may also lead to inefficiency.
It may be thecase that on the basis of the left-corner table (resp.head-comer table) very little derivations are actuallyfiltered out.
Furthermore, the use in the table mayeven lead to more derivations as now certain sub-cases are considered which are considered as a singlederivation in a parser without prediction.
An impor-tant problem thus is to come up with the most use-ful left-corner (resp.
head-corner) table for a givengrammar.A final factor in determining the best parser isthe actual use we want to make of the parser.
Forexample, are we interested in the times needed todo recognition, or do we need to consider the timesused for the recovery of parse trees as well.
In somesystems these different parse trees are never actuallybuilt but the semantic and pragmatic omponentsdirectly work on the items built by the parser \[Mooreand Alshawi, 1992\].
We conjecture that even in suchapplications it is probably a good thing to limit thesize of the parse forest, but the importance may varyfrom application to application.78References\[Bouma, 1991\] Gosse Bouma.
Prediction in chartparsing algorithms for categorial unification gram-mar.
In Fifth Conference of the European Chapterof the Association for Computational Linguistics,Berlin, 1991.\[Bouma, 1993\] Gosse Bouma.
Nonmonotonicity andCategoriai Unification Grammar.
PhD thesis, Uni-versity of Groningen, 1993.\[Kay, 1989\] Martin Kay.
Head driven parsing.
InProceedings of Workshop on Parsing Technologies,Pittsburg, 1989.\[Laveili and Satta, 1991\] Alberto Lavelli and Gior-gio Satta.
Bidirectional parsing of lexicalized treeadjoining rammar.
In Fifth Conference of the Eu-ropean Chapter of the Association for Computa-tional Linguistics, Berlin, 1991.\[Moore and Alshawi, 1992\] Robert C. Moore andHiyan Alshawi.
Syntactic and semantic process-ing.
In Iliyan Alshawi, editor, The Core LanguageEngine, pages 129-148.
ACL-MIT press, 1992.\[Rosenkrantz and Lewis-II, 1970\] D.J.
Rosenkrantzand P.M. Lewis-II.
Deterministic left corner pars-ing.
In 1EEE Conference of the 11th Annual Sym-posium on Switching and Automata Theory, pages139-152, 1970.\[Satta nd Stock, 1989\] Giorgio Satta and OlivieroStock.
Head-driven bidirectional parsing, a tab-ular method.
In Proceedings of the Workshopon Parsing Technologies, pages 43-51, Pittsburg,1989.\[Shieber, 1985\] Stuart M. Shieber.
Using restric-tion to extend parsing algorithms for complex-feature-based formalisms.
In 23th Annual Meetingof the Association for Computational Linguistics,Chicago, 1985.\[Sikkel and op den Akker, 1992\] Klaas Sikkel andRieks op den Akker.
Head-corner chart parsing.In Proceedings Computing Science in the Nether-lands (CSN '92}, Utrecht, 1992.\[van Noord, 1991\] Gertjan van Noord.
Head cornerparsing for discontinuous constituency.
In 29thAnnual Meeting of the Association for Computa-tional Linguistics, Berkeley, 1991.\[van Noord, 1993\] Gertjan van Noord.
Reversibilitgtin Natural Language Processing.
PhD thesis, Uni-versity of Utrecht, 1993.A A head-dr iven  char t  parserThe main omission consists of the administrationconcerning the packed items, for the recovery ofparse-trees.
Also this version assumes that no emptyproductions occur in the grammar.Rules are of the form ruleCBead, LHS, LeftDs,RightDs), where LeftDs is in reversed order.
Thepredicate lex(Cat,  P0, P) is true if the word connect-ing the positions P0 and P has category Cat.The chart consists of (dynamically asserted) factsof the form itemCCat,ToParse,PO,P), indicatingthat if there is a list of categories ToParse from po-sition P to Q then there is category Cat from positionP0 to Q.
The predicate assertz_check is used to as-serts such items.
That predicate asserts its argumentonly if no more general clause exists; furthermore itdeletes all more specific clauses.7.
scan(+P0,+P) parses from P0 to P,P is current positionscanCP,P).scanCP0,P) :-Pl is PO + 1,C lexCCat,pO,pl),add_item(Cat, \[\] ,PO,P1),fail; scanCPl,P)).add_item(+Cat,+ToParse,+Begin,+End)asserts item and computes all itsconsequences, if inactive itemadd_itemCCat,\[\],B,E) "-assertz_checkCitemCCat.\[\].B,E)),closure(Cat,B,E).add_itemCCat,\[H\]T\],B,E) :-assertz_checkCitemCCat,\[H\[T\],B,E)).closureC+Cat,+Begin,+End)computes all the items on basisof item Cat from Begin to Endclosure(Cat,Pi,P) :-itemCLhs,\[CatlToParse\],PO,P1),add itemCLhs,ToParse,PO,P),fail.closureCCat,PI,P) "-ruleCCat,Lhs,Left,Right),leftCLeft,PO,Pl),add_item(Lhs,Right,PO,P),fail.closure(  .
.
.
.
.
).Y, left(+Ds,?Begin,+End) if there are Ds~, from right from Begin to Endle f t (  \[\] ,B0,B0).left( \[DIDs\] ,BO.E) :-itemCD, \[\] ,B,E),left (Ds ,BO,B).79B A head-corner  parserThe main omission of the following version of thehead-corner parser is the administration concerningthe well-formed substring table, packing and the pos-sibility of rules with an empty right hand side.
In thehead-corner parser used in the experiment the parsepredicate and the head-corner predicate are memo-ized.
Furthermore items for the parse forest are as-serted in the head-corner predicate.
Finally somespecial arrangements are made to allow for rules withan empty right hand side, by allowing underspecifi-cation of the string position in the comparison pred-icates.The relation hc_t able (Cat, PO, P, Goal, qO, ?\]) im-plements the head-corner table.
If PO=qO the phraseis head-initial; if P=I~ the phrase is head-final.
Rulesand lexical entries are represented as before.7.
parseCCat,PO,P,EO,E) if there is7, Cat from PO to P, ,ithin range EO,Eparse (Goal, P0, P, EO,E) :-predict (Goal, PO, P, Lex, QO, Q, EO, E),head_corner (Lex, QO, Q, Goal, PO, P, EO, E).7. head_cornerCCat,CO,C,Goal,G0,G,EO,E)7. if Cat from CO to C is a head-corner of7.
Goal from GO to G within EO to E.head_corner(Cat,qO,q,Cat,QO,Q .... ).head_corner(Small,Qi,Q2,Goal,PO,P,E0,E) :-rule(Small,Mid,Left,Right),left(Left,QO,Q1,E0),right(Right,~2,Q,E),hc_table(Mid,QO,Q,Goal,PO,P),head_corner(Mid,QO,Q,Goal,PO,P,EO,E).7. predictCGoal,PO,P,Lex,qO,Q,EO,E)7. i f  Lex from Q0 to Q may be head-corner7.
of Goal from PO to P within EO, E.predict(Goal,POoP,Lex,QO,Q,EO,E) :-hc_table(Lex,QO,Q,Goal,PO,P),lexCLex,QO,Q),EO =< QO,q =< E.7.
left(Ds,PO,P,EO) if (reversed) De exist7.
from P to PO with left-extreme EOl e f t (~ ,p ,p ,_) .IeftC\[HIT\],PO,P,E0) :-parseCH,P1,P,EO,P),IeftCT,PO,P1,EO).7. right(Ds,PO,P,E) if Ds exist from7.
PO to P with right-extreme Er ight ( \ [ \ ] ,P ,P , _ ) .r ight  ( \[H IT\], PO, P, E) :-parse(H,PO,P1,PO,E),right(T,Pi,P,E).80
