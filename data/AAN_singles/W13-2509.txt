Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 69?76,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsImproving MT System Using Extracted Parallel Fragments of Textfrom Comparable CorporaRajdeep Gupta, Santanu Pal, Sivaji BandyopadhyayDepartment of Computer Science & EngineeringJadavpur UniversityKolkata ?
700032, India{rajdeepgupta20, santanu.pal.ju}@gmail.com,sivaji_cse_ju@yahoo.comAbstractIn this article, we present an automated ap-proach of extracting English-Bengali parallelfragments of text from comparable corporacreated using Wikipedia documents.
Our ap-proach exploits the multilingualism of Wiki-pedia.
The most important fact is that this ap-proach does not need any domain specific cor-pus.
We have been able to improve the BLEUscore of an existing domain specific English-Bengali machine translation system by11.14%.1 IntroductionRecently comparable corpora have got great at-tention in the field of NLP.
Extracting parallelfragments of texts, paraphrases or sentences fromcomparable corpora are particularly useful forany statistical machine translation system (SMT)(Smith et al2010) as the size of the parallel cor-pus plays major role in any SMT performance.Extracted parallel phrases from comparable cor-pora are added with the training corpus as addi-tional data that is expected to facilitate better per-formance of machine translation systems specifi-cally for those language pairs which have limitedparallel resources available.
In this work, we tryto extract English-Bengali parallel fragments oftext from comparable corpora.
We have devel-oped an aligned corpus of English-Bengali doc-ument pairs using Wikipedia.
Wikipedia is ahuge collection of documents in many differentlanguages.
We first collect an English documentfrom Wikipedia and then follow the inter-language link to find the same document in Ben-gali (obviously, if such a link exists).
In this way,we create a small corpus.
We assume that suchEnglish-Bengali document pairs from Wikipediaare already comparable since they talk about thesame entity.
Although each English-Bengalidocument pair talks about the same entity, mostof the times they are not exact translation of eachother.
And as a result, parallel fragments of textare rarely found in these document pairs.
Thebigger the size of the fragment the less probableit is to find its parallel version in the target side.Nevertheless, there is always chance of gettingparallel phrase, tokens or even sentences in com-parable documents.
The challenge is to find thoseparallel texts which can be useful in increasingmachine translation performance.In our present work, we have concentrated onfinding small fragments of parallel text instead ofrigidly looking for parallelism at entire sententiallevel.
Munteanu and Marcu (2006) believed thatcomparable corpora tend to have parallel data atsub-sentential level.
This approach is particularlyuseful for this type of corpus underconsideration, because there is a very littlechance of getting exact translation of biggerfragments of text in the target side.
Instead,searching for parallel chunks would be morelogical.
If a sentence in the source side has aparallel sentence in the target side, then all of itschunks need to have their parallel translations inthe target side as well.It is to be noted that, although we havedocument level alignment in our corpus, it issomehow ad-hoc i.e.
the documents in the corpusdo not belong to any particular domain.
Evenwith such a corpus we have been able to improvethe performance of an existing machinetranslation system built on tourism domain.
Thisalso signifies our contribution towards domainadaptation of machine translation systems.The rest of the paper is organized as follows.Section 2 describes the related work.
Section 3describes the preparation of the comparablecorpus.
The system architecture is described insection 4.
Section 5 describes the experiments we69conducted and presents the results.
Finally theconclusion is drawn in section 6.2 Related WorkThere has been a growing interest in approachesfocused on extracting word translations fromcomparable corpora (Fung and McKeown, 1997;Fung and Yee, 1998; Rapp, 1999; Chiao andZweigenbaum, 2002; Dejean et al 2002; Kaji,2005; Gamallo, 2007; Saralegui et al 2008).Most of the strategies follow a standard methodbased on context similarity.
The idea behind thismethod is as follows: A target word t is thetranslation of a source word s if the words withwhich t co-occurs are translations of words withwhich s co-occurs.
The basis of the method is tofind the target words that have the most similardistributions with a given source word.
Thestarting point of this method is a list of bilingualexpressions that are used to build the contextvectors of all words in both languages.
This listis usually provided by an external bilingualdictionary.
In Gamallo (2007), however, thestarting list is provided by bilingual correlationswhich are previously extracted from a parallelcorpus.
In Dejean (2002), the method relies on amultilingual thesaurus instead of an externalbilingual dictionary.
In all cases, the starting listcontains the ?seed expressions?
required to buildcontext vectors of the words in both languages.The works based on this standard approachmainly differ in the coefficients used to measurethe context vector similarity.Otero et al(2010) showed how Wikipediacould be used as a source of comparable corporain different language pairs.
They downloaded theentire Wikipedia for any two language pair andtransformed it into a new collection:CorpusPedia.
However, in our work we haveshowed that only a small ad-hoc corpuscontaining Wikipedia articles could be proved tobe beneficial for existing MT systems.3 Tools and Resources UsedA sentence-aligned English-Bengali parallelcorpus containing 22,242 parallel sentences froma travel and tourism domain was used in thepreparation of the baseline system.
The corpuswas obtained from the consortium-mode project?Development of English to Indian LanguagesMachine Translation (EILMT) System?.
TheStanford Parser and the CRF chunker were usedfor identifying individual chunks in the sourceside of the parallel corpus.
The sentences on thetarget side (Bengali) were POS-tagged/chunkedby using the tools obtained from the consortiummode project ?Development of Indian Languagesto Indian Languages Machine Translation(ILILMT) System?.For building the comparable corpora we havefocused our attention on Wikipedia documents.To collect comparable English-Bengalidocument pairs we designed a crawler.
Thecrawler first visits an English page, saves the rawtext (in HTML format), and then finds the cross-lingual link (if exists) to find the correspondingBengali document.
Thus, we get one English-Bengali document pair.
Moreover, the crawlervisits the links found in each document andrepeats the process.
In this way, we develop asmall aligned corpus of English-Bengalicomparable document pairs.
We retain only thetextual information and all the other details arediscarded.
It is evident that the corpus is notconfined to any particular domain.
The challengeis to exploit this kind of corpus to help machinetranslation systems improve.
The advantage ofusing such corpus is that it can be prepared easilyunlike the one that is domain specific.The effectiveness of the parallel fragments oftext developed from the comparable corpora inthe present work is demonstrated by using thestandard log-linear PB-SMT model as ourbaseline system: GIZA++ implementation ofIBM word alignment model 4, phrase extractionheuristics described in (Koehn et al 2003),minimum-error-rate training (Och, 2003) on aheld-out development set, target language modelwith Kneser-Ney smoothing (Kneser and Ney,1995) trained with SRILM (Stolcke, 2002), andMoses decoder (Koehn et al 2007).4 System Architecture4.1 PB-SMT(Baseline System)Translation is modeled in SMT as a decisionprocess, in which the translation e1I = e1..ei..eI ofa source sentence f1J = f1..fj..fJ  is chosen tomaximize (1))().|(maxarg)|(maxarg 111,11, 11IIJeIJIeIePefPfePII?
(1)where )|( 11 IJ efP  and )( 1IeP  denoterespectively the translation model and the targetlanguage model (Brown et al 1993).
In log-linear phrase-based SMT, the posteriorprobability )|( 11 JI feP  is directly modeled as alog-linear combination of features (Och and Ney,702002), that usually comprise of M translationalfeatures, and the language model, as in (2):??
?MmKIJmmJI sefhfeP111111 ),,()|(log ?
)(log 1ILM eP??
(2)where kk sss ...11 ?
denotes a segmentation of thesource and target sentences respectively into thesequences of phrases )?,...,?
( 1 kee  and )?,...,?
( 1 kffsuch that (we set i0 = 0) (3):,1 Kk ???
sk = (ik, bk, jk),kk iik eee ...?
11???
,kk jbk fff ...?
?
.
(3)and each feature mh?
in (2) can be rewritten as in(4):??
?KkkkkmKIJm sefhsefh1111 ),?,?(?
),,((4)where mh?
is a feature that applies to a singlephrase-pair.
It thus follows (5):?
???
??
?KkKkkkkkkkmMmm sefhsefh1 11),?,?(?),?,?(??
(5)wheremMmmhh ??1???
?.4.2 Chunking of English SentencesWe have used CRF-based chunking algorithm tochunk the English sentences in each document.The chunking breaks the sentences into linguisticphrases.
These phrases may be of different sizes.For example, some phrases may be two wordslong and some phrases may be four words long.According to the linguistic theory, the interme-diate constituents of the chunks do not usuallytake part in long distance reordering when it istranslated, and only intra chunk reordering oc-curs.
Some chunks combine together to make alonger phrase.
And then some phrases againcombine to make a sentence.
The entire processmaintains the linguistic definition of a sentence.Breaking the sentences into N-grams would havealways generated phrases of length N but thesephrases may not be linguistic phrases.
For thisreason, we avoided breaking the sentences intoN-grams.The chunking tool breaks each English sentenceinto chunks.
The following is an example of howthe chunking is done.Sentence: India , officially the Republic of India ,is a country in South Asia.After Chunking: (India ,) (officially) (theRepublic ) (of) (India , ) (is) (a country ) (inSouth Asia ) (.
)We have further merged the chunks to formbigger chunks.
The idea is that, we maysometimes find the translation of the mergedchunk in the target side as well, in which case,we would get a bigger fragment of parallel text.The merging is done in two ways:Strict Merging: We set a value ?V?.
Startingfrom the beginning, chunks are merged such thatthe number of tokens in each merged chunk doesnot exceed V.Figure 1.
Strict-Merging Algorithm.Figure 1 describes the pseudo-code for strictmerging.For example, in our example sentence themerged chunks will be as following, where V=4:(India , officially) (the Republic of ) (India , is)(a country) (in South Asia .
)Figure 2.
Window-Based Merging Algorithm.Procedure Window_Merging()beginSet_Chunk?Set of all English ChunksL?Number of chunks in Set_Chunkfor i = 0 to L-1Words?Set of tokens in i-th Chunk in Set_ChunkCur_wc?number of tokens in WordsOl?i-th chunk in Set_Chunkfor j = (i+1) to (L-1)C?j-th chunk in Set_Chunkw?set of tokens in Cl?number of tokens in wif(Cur_wc + l ?
V)Append C at the end of OlAdd l to Cur_wcend ifend forOutput Ol as the next merged chunkend forendProcedure Strict_Merge()beginOline ?
nullCur_wc ?
0repeatIline?Next ChunkLength?Number of Tokens in Ilineif(Cur_wc + Length > V)Output Oline as the next merged chunkCur_wc?LengthelseAppend Iline at the end of OlineAdd Length to Cur_wcend ifwhile (there are more chunks)end71Figure 3.
System Architecture for Finding Parallel FragmentsWindow-Based Merging: In this type ofchunking also, we set a value ?V?, and for eachchunk we try to merge as many chunks aspossible so that the number of tokens in themerged chunk never exceeds V.So, we slide an imaginary window over thechunks.
For example, for our example sentencethe merged chunks will be as following, where V= 4 :(India , officially) (officially the Republic of)(the Republic of) (of India , is) (India , is) (is acountry) (a country) (in South Asia .
)The pseudo-code of window-based merging isdescribed in Figure 2.4.3 Chunking of Bengali SentencesSince to the best of our knowledge, there is nogood quality chunking tool for Bengali we didnot use chunking explicitly.
Instead, strictmerging is done with consecutive V number oftokens whereas window-based merging is donesliding a virtual window over each token andmerging tokens so that the number of tokensdoes not exceed V.4.4 Finding Parallel ChunksAfter finding the merged English chunks they aretranslated into Bengali using a machinetranslation system that we have alreadydeveloped.
This is also the same machinetranslation system whose performance we wantto improve.
Chunks of each of the documentpairs are then compared to find parallel chunks.Each translated source chunk (translated fromEnglish to Bengali) is compared with all thetarget chunks in the corresponding Bengali-chunk document.
When a translated sourcechunk is considered, we try to align each of itstoken to some token in the target chunk.
Overlapbetween token two Bengali chunks B1 and B2,where B1 is the translated chunk and B2 is thechunk in the Bengali document, is defined asfollows:Overlap(B1,B2) = Number of tokens in B1 forwhich an alignment can be found in B2.It is to be noted that Overlap(B1,B2) ?Overlap(B2 ,B1).
Overlap between chunks isfound in both ways (from translated sourcechunk to target and from target to translatedsource chunk).
If 70% alignment is found in boththe overlap measures then we declare them asparallel.
Two issues are important here: the com-parison of two Bengali tokens and in case analignment is found, which token to retrieve(source or target) and how to reorder them.
Weaddress these two issues in the next two sections.4.5 Comparing Bengali TokensFor our purpose, we first divide the two tokensinto their matra (vowel modifiers) part andconsonant part keeping the relative orders ofcharacters in each part same.
For example,Figure 4 shows the division of the word .EnglishDocumentsEnglishChunksMergingTranslationBengaliDocumentsBengaliChunksFind Parallel Chunks and ReorderMerging72Figure 4.
Division of a Bengali Word.Respective parts of the two words are thencompared.
Orthographic similarities likeminimum edit distance ratio, longest commonsubsequence ratio, and length of the strings areused for the comparison of both parts.Minimum Edit Distance Ratio: It is definedas  follows:where |B| is the length of the string B and ED isthe minimum edit distance or levenshteindistance calculated as the minimum number ofedit operations ?
insert, replace, delete ?
neededto transform B1 into B2.Longest Common Subsequence Ratio: It isdefined as follows:where LCS is the longest common subsequenceof two strings.Threshold for matching is set empirically.
Wedifferentiate between shorter strings and largerstrings.
The idea is that, if the strings are shortwe cannot afford much difference between themto consider them as a match.
In those cases, wecheck for exact match.
Also, the threshold forconsonant part is set stricter because ourassumption is that consonants contribute moretoward the word?s pronunciation.4.6 Reordering of Source ChunksWhen a translated source chunk is comparedwith a target chunk it is often found that theordering of the tokens in the source chunk andthe target chunk is different.
The tokens in thetarget chunk have a different permutation ofpositions with respect to the positions of tokensin the source chunk.
In those cases, we reorderedthe positions of the tokens in the source chunk soas to reflect the positions of tokens in the targetchunk because it is more likely that the tokenswill usually follow the ordering as in the targetchunk.
For example, the machine translationoutput of the English chunk ?from the AtlanticOcean?
is ?
theke  atlantic(mahasagar)?.
We found a targetchunk ?
(atlantic)  (maha-sagar)  (theke)  (ebong)?
with whichwe could align the tokens of the source chunkbut in different relative order.
Figure 5 shows thealignment of tokens.Figure 5.
Alignment of Bengali Tokens.We reordered the tokens of the source chunkand the resulting chunk was ?
?.Also, the token ?
?
in thetarget chunk could not find any alignment andwas discarded.
The system architecture of thepresent system is described in figure 3.5 Experiments And Results5.1 Baseline SystemWe randomly extracted 500 sentences each forthe development set and test set from the initialparallel corpus, and treated the rest as thetraining corpus.
After filtering on the maximumallowable sentence length of 100 and sentencelength ratio of 1:2 (either way), the trainingcorpus contained 22,492 sentences.V=4 V=7Number of EnglishChunks(Strict-Merging)579037 376421Number of EnglishChunks(Window-Merging)890080 949562Number of BengaliChunks(Strict-Merging)69978 44113Number of BengaliChunks(Window-Merging)230025 249330Table 1.
Statistics of the Comparable CorpusV=4 V=7Number of ParallelChunks(Strict-Merging)1032 1225Number of ParallelChunks(Window-Merging)1934 2361Table 2.
Number of Parallel Chunks foundKolkatamatra73BLEU NISTBaseline System(PB-SMT) 10.68 4.12Baseline + ParallelChunks(Strict-Merging)V=4 10.91 4.16V=7 11.01 4.16Baseline + ParallelChunks(Window-Merging)V=4 11.55 4.21V=7 11.87 4.29Table 3.Evaluation of the SystemIn addition to the target side of the parallel cor-pus, a monolingual Bengali corpus containing406,422 words from the tourism domain wasused for the target language model.
Weexperimented with different n-gram settings forthe language model and the maximum phraselength, and found that a 5-gram language modeland a maximum phrase length of 7 produced theoptimum baseline result.
We therefore carriedout the rest of the experiments using thesesettings.5.2 Improving Baseline SystemThe comparable corpus consisted of 582 English-Bengali document pairs.We experimented with the values V=4 andV=7 while doing the merging of chunks both inEnglish and Bengali.
All the single token chunkswere discarded.
Table 1 shows some statisticsabout the merged chunks for V=4 and V=7.It isevident that number of chunks in Englishdocuments is far more than the number of chunksin Bengali documents.
This immediatelysuggests that Bengali documents are lessinformative than English documents.
When theEnglish merged chunks were passed to thetranslation module some of the chunks could notbe translated into Bengali.
Also, some chunkscould be translated only partially, i.e.
sometokens could be translated while some could notbe.
Those chunks were discarded.
Finally, thenumber of (Strict-based) English merged-chunksand number of (Window-based) English merged-chunks were 285756 and 594631 respectively.Two experiments were carried out separately.Strict-based  merged English chunks werecompared with Strict-Based merged Bengalichunks.
Similarly, window-based merged Eng-lish chunks were compared with window-basedmerged Bengali chunks.
While searching forparallel chunks each translated source chunk wascompared with all the target chunks in thecorresponding document.
Table 2 displays thenumber of parallel chunks found.
Compared tothe number of chunks in the original documentsthe number of parallel chunks found was muchless.
Nevertheless, a quick review of the parallellist revealed that most of the chunks were ofgood quality.5.3 EvaluationWe carried out evaluation of the MT qualityusing two automatic MT evaluation metrics:BLEU (Papineni et al 2002) and NIST(Doddington, 2002).
Table 3 presents the ex-perimental results.
For the PB-SMT experiments,inclusion of the extracted strict merged parallelfragments from comparable corpora as additionaltraining data presented some improvements overthe PB-SMT baseline.
Window based extractedfragments are added separately with parallel cor-pus and that also provides some improvementsover the PB baseline; however inclusion of win-dow based extracted phrases in baseline systemwith phrase length 7 improves over both strictand baseline in term of BLEU score and NISTscore.Table 3 shows the performance of the PB-SMT system that shows an improvement overbaseline with both strict and window basedmerging even if,  we change their phrase lengthfrom 4 to 7.
Table 3 shows that the bestimprovement is achieved when we add parallelchunks as window merging with phrase length 7.It gives 1.19 BLEU point, i.e., 11.14% relativeimprovement over baseline system.
The NISTscore could be improved  up to 4.12%.
Bengali isa morphologically rich language and has74relatively free phrase order.
The strict basedextraction does not reflect much improvementcompared to the window based extractionbecause strict-merging (Procedure Strict_Merge)cannot cover up all the segments on either side,so very few parallel extractions have been foundcompared to window based extraction.6 ConclusionIn this work, we tried to find English-Bengaliparallel fragments of text from a comparablecorpus built from Wikipedia documents.
Wehave successfully improved the performance ofan existing machine translation system.
We havealso shown that out-of-domain corpus happenedto be useful for training of a domain specific MTsystem.
The future work consists of working onlarger amount of data.
Another focus could be onbuilding ad-hoc comparable corpus from WEBand using it to improve the performance of anexisting out-of-domain MT system.
This aspectof work is particularly important because themain challenge would be of domain adaptation.AcknowledgementsThis work has been partially supported by a grantfrom the English to Indian language MachineTranslation (EILMT) project funded by theDepartment of Information and Technology(DIT), Government of India.ReferenceChiao, Y. C., & Zweigenbaum, P. (2002, August).Looking for candidate translational equivalents inspecialized, comparable corpora.
In Proceedings ofthe 19th international conference on Computation-al linguistics-Volume 2 (pp.
1-5).
Association forComputational Linguistics.D?jean, H., Gaussier, ?., & Sadat, F. (2002).
Bilin-gual terminology extraction: an approach based ona multilingual thesaurus applicable to comparablecorpora.
In Proceedings of the 19th InternationalConference on Computational Linguistics COLING(pp.
218-224).Doddington, G. (2002, March).
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the secondinternational conference on Human LanguageTechnology Research (pp.
138-145).
MorganKaufmann Publishers Inc..Fung, P., & McKeown, K. (1997, August).
Findingterminology translations from non-parallel corpora.In Proceedings of the 5th Annual Workshop onVery Large Corpora (pp.
192-202).Fung, P., & Yee, L. Y.
(1998, August).
An IR ap-proach for translating new words from nonparallel,comparable texts.
In Proceedings of the 17th inter-national conference on Computational linguistics-Volume 1 (pp.
414-420).
Association for Computa-tional Linguistics.Hiroyuki, K. A. J. I.
(2005).
Extracting translationequivalents from bilingual comparable corpora.IEICE Transactions on information and systems,88(2), 313-323.Kneser, R., & Ney, H. (1995, May).
Improved back-ing-off for m-gram language modeling.
In Acous-tics, Speech, and Signal Processing, 1995.ICASSP-95., 1995 International Conference on(Vol.
1, pp.
181-184).
IEEE.Koehn, P., Hoang, H., Birch, A., Callison-Burch, C.,Federico, M., Bertoldi, N., ... & Herbst, E. (2007,June).
Moses: Open source toolkit for statisticalmachine translation.
In Proceedings of the 45thAnnual Meeting of the ACL on Interactive Posterand Demonstration Sessions (pp.
177-180).Association for Computational Linguistics.Koehn, P., Och, F. J., & Marcu, D. (2003, May).
Sta-tistical phrase-based translation.
In Proceedings ofthe 2003 Conference of the North American Chap-ter of the Association for Computational Linguis-tics on Human Language Technology-Volume 1(pp.
48-54).
Association for Computational Lin-guistics.Munteanu, D. S., & Marcu, D. (2006, July).
Extract-ing parallel sub-sentential fragments from non-parallel corpora.
In Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th annual meeting of the Association forComputational Linguistics (pp.
81-88).
Associationfor Computational Linguistics..Och, F. J.
(2003, July).
Minimum error rate training instatistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Com-putational Linguistics-Volume 1 (pp.
160-167).
As-sociation for Computational Linguistics.Och, F. J., & Ney, H. (2000).
Giza++: Training ofstatistical translation models.Otero, P. G. (2007).
Learning bilingual lexicons fromcomparable english and spanish corpora.
Proceed-ings of MT Summit xI, 191-198.Otero, P. G., & L?pez, I. G. (2010).
Wikipedia asmultilingual source of comparable corpora.
In Pro-ceedings of the 3rd Workshop on Building and Us-ing Comparable Corpora, LREC (pp.
21-25).Papineni, K., Roukos, S., Ward, T., & Zhu, W.
J.
(2002, July).
BLEU: a method for automatic evalu-ation of machine translation.
In Proceedings of the40th annual meeting on association for computa-75tional linguistics (pp.
311-318).
Association forComputational Linguistics.Rapp, R. (1999, June).
Automatic identification ofword translations from unrelated English and Ger-man corpora.
In Proceedings of the 37th annualmeeting of the Association for Computational Lin-guistics on Computational Linguistics (pp.
519-526).
Association for Computational Linguistics.Saralegui, X., San Vicente, I., & Gurrutxaga, A.(2008).
Automatic generation of bilingual lexiconsfrom comparable corpora in a popular science do-main.
In LREC 2008 workshop on building and us-ing comparable corpora.Smith, J. R., Quirk, C., & Toutanova, K. (2010,June).Extracting parallel sentences fromcomparable corpora using document levelalignment.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for ComputationalLinguistics (pp.
403-411).
Association forComputational Linguistics.Stolcke, A.
(2002, September).
SRILM-an extensiblelanguage modeling toolkit.
In Proceedings of theinternational conference on spoken languageprocessing (Vol.
2, pp.
901-904).76
