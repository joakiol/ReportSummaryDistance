Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 52?60,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsMultilingual Sentiment Analysis using Machine Translation?Alexandra Balahur and Marco TurchiEuropean Commission Joint Research CentreInstitute for the Protection and Security of the CitizenVia E. Fermi 2749, Ispra, Italyalexandra.balahur, marco.turchi@jrc.ec.europa.euAbstractThe past years have shown a steady growthin interest in the Natural Language Process-ing task of sentiment analysis.
The researchcommunity in this field has actively proposedand improved methods to detect and classifythe opinions and sentiments expressed in dif-ferent types of text - from traditional press ar-ticles, to blogs, reviews, fora or tweets.
A lessexplored aspect has remained, however, theissue of dealing with sentiment expressed intexts in languages other than English.
To thisaim, the present article deals with the prob-lem of sentiment detection in three differentlanguages - French, German and Spanish - us-ing three distinct Machine Translation (MT)systems - Bing, Google and Moses.
Our ex-tensive evaluation scenarios show that SMTsystems are mature enough to be reliably em-ployed to obtain training data for languagesother than English and that sentiment analysissystems can obtain comparable performancesto the one obtained for English.1 IntroductionTogether with the increase in the access to tech-nology and the Internet, the past years have showna steady growth of the volume of user-generatedcontents on the Web.
The diversity of topics cov-ered by this data (mostly containing subjective andopinionated content) in the new textual types suchas blogs, fora, microblogs, has been proven to beof tremendous value to a whole range of applica-tions, in Economics, Social Science, Political Sci-ence, Marketing, to mention just a few.
Notwith-standing these proven advantages, the high quan-tity of user-generated contents makes this informa-tion hard to access and employ without the use ofautomatic mechanisms.
This issue motivated therapid and steady growth in interest from the NaturalLanguage Processing (NLP) community to developcomputational methods to analyze subjectivity andsentiment in text.
Different methods have been pro-posed to deal with these phenomena for the distincttypes of text and domains, reaching satisfactory lev-els of performance for English.
Nevertheless, forcertain applications, such as news monitoring, theinformation in languages other than English is alsohighly relevant and cannot be disregarded.
Addi-tionally, systems dealing with sentiment analysis inthe context of monitoring must be reliable and per-form at similar levels as the ones implemented forEnglish.Although the most obvious solution to these is-sues of multilingual sentiment analysis would be touse machine translation systems, researchers in sen-timent analysis have been reluctant to using suchtechnologies due to the low performance they usedto have.
However, in the past years, the performanceof Machine Translation systems has steadily im-proved.
Open access solutions (e.g.
Google Trans-late1, Bing Translator2) offer more and more accu-rate translations for frequently used languages.Bearing these thoughts in mind, in this articlewe study the manner in which sentiment analysiscan be done for languages other than English, usingMachine Translation.
In particular, we will study1http://translate.google.it/2http://www.microsofttranslator.com/52this issue in three languages - French, German andSpanish - using three different Machine Translationsystems - Google Translate, Bing Translator andMoses (Koehn et al, 2007).We employ these systems to obtain training andtest data for these three languages and subsequentlyextract features that we employ to build machinelearning models using Support Vector Machines Se-quential Minimal Optimization.
We additionallyemploy meta-classifiers to test the possibility to min-imize the impact of noise (incorrect translations) inthe obtained data.Our experiments show that machine translationsystems are mature enough to be employed for mul-tilingual sentiment analysis and that for some lan-guages (for which the translation quality is highenough) the performance that can be attained is sim-ilar to that of systems implemented for English.2 Related WorkMost of the research in subjectivity and sentimentanalysis was done for English.
However, there weresome authors who developed methods for the map-ping of subjectivity lexicons to other languages.
Tothis aim, (Kim and Hovy, 2006) use a machine trans-lation system and subsequently use a subjectivityanalysis system that was developed for English tocreate subjectivity analysis resources in other lan-guages.
(Mihalcea et al, 2009) propose a methodto learn multilingual subjective language via cross-language projections.
They use the Opinion Finderlexicon (Wilson et al, 2005) and use two bilin-gual English-Romanian dictionaries to translate thewords in the lexicon.
Since word ambiguity can ap-pear (Opinion Finder does not mark word senses),they filter as correct translations only the most fre-quent words.
The problem of translating multi-wordexpressions is solved by translating word-by-wordand filtering those translations that occur at leastthree times on the Web.
Another approach in obtain-ing subjectivity lexicons for other languages thanEnglish was explored by Banea et al (Banea et al,2008b).
To this aim, the authors perform three dif-ferent experiments, obtaining promising results.
Inthe first one, they automatically translate the anno-tations of the MPQA corpus and thus obtain subjec-tivity annotated sentences in Romanian.
In the sec-ond approach, they use the automatically translatedentries in the Opinion Finder lexicon to annotate aset of sentences in Romanian.
In the last experi-ment, they reverse the direction of translation andverify the assumption that subjective language canbe translated and thus new subjectivity lexicons canbe obtained for languages with no such resources.Further on, another approach to building lexiconsfor languages with scarce resources is presented byBanea et al (Banea et al, 2008a).
In this research,the authors apply bootstrapping to build a subjectiv-ity lexicon for Romanian, starting with a set of seedsubjective entries, using electronic bilingual dictio-naries and a training set of words.
They start witha set of 60 words pertaining to the categories ofnoun, verb, adjective and adverb from the transla-tions of words in the Opinion Finder lexicon.
Trans-lations are filtered using a measure of similarity tothe original words, based on Latent Semantic Anal-ysis (LSA) (Deerwester et al, 1990) scores.
Yetanother approach to mapping subjectivity lexica toother languages is proposed by Wan (2009), whouses co-training to classify un-annotated Chinese re-views using a corpus of annotated English reviews.He first translates the English reviews into Chineseand subsequently back to English.
He then performsco-training using all generated corpora.
(Kim et al,2010) create a number of systems consisting of dif-ferent subsystems, each classifying the subjectivityof texts in a different language.
They translate a cor-pus annotated for subjectivity analysis (MPQA), thesubjectivity clues (Opinion finder) lexicon and re-train a Nave Bayes classifier that is implemented inthe Opinion Finder system using the newly gener-ated resources for all the languages considered.
Fi-nally, (Banea et al, 2010) translate the MPQA cor-pus into five other languages (some with a similarethimology, others with a very different structure).Subsequently, they expand the feature space used ina Nave Bayes classifier using the same data trans-lated to 2 or 3 other languages.
Their conclusion isthat by expanding the feature space with data fromother languages performs almost as well as traininga classifier for just one language on a large set oftraining data.Attempts of using machine translation in differ-ent natural language processing tasks have not beenwidely used due to poor quality of translated texts,53but recent advances in Machine Translation havemotivated such attempts.
In Information Retrieval,(Savoy and Dolamic, 2009) proposed a comparisonbetween Web searches using monolingual and trans-lated queries.
On average, the results show a dropin performance when translated queries are used,but it is quite limited, around 15%.
For some lan-guage pairs, the average result obtained is around10% lower than that of a monolingual search whilefor other pairs, the retrieval performance is clearlylower.
In cross-language document summarization,(Wan et al, 2010; Boudin et al, 2010) combinedthe MT quality score with the informativeness scoreof each sentence in a set of documents to automat-ically produce summary in a target language usinga source language texts.
In (Wan et al, 2010), eachsentence of the source document is ranked accord-ing both the scores, the summary is extracted andthen the selected sentences translated to the targetlanguage.
Differently, in (Boudin et al, 2010), sen-tences are first translated, then ranked and selected.Both approaches enhance the readability of the gen-erated summaries without degrading their content.3 Motivation and ContributionThe main motivation for the experiments we presentin this article is the known lack of resources and ap-proaches for sentiment analysos in languages otherthan English.
Although, as we have seen in theRelated Work section, a few attempts were madeto build systems that deal with sentiment analysisin other languages, they mostly employed bilingualdictionaries and used unsupervised approaches.
Thevery few that employed supervised learning usingtranslated data have, in change, concentrated onlyon the issue of sentiment classification and have dis-regarded the impact of the translation quality andthe difference that the use of distinct translation sys-tems can make in this settings.
Moreover, such ap-proaches have usually employed only simple ma-chine learning algorithms.
No attempt has beenmade to study the use of meta-classifiers to enhancethe performance of the classification through the re-moval of noise in the data.Our main contribution in this article is the com-parative study of multilingual sentiment analysisperformance using distinct machine translation sys-tems, with varying levels of translation quality.
Inthis sense, we employ three different systems - BingTranslator, Google Translate and Moses to translatedata from English to three languages - French, Ger-man and Spanish.
We subsequently study the perfor-mance of classifying sentiment from the translateddata and different methods to minimize the effect ofnoise in the data.Our comparative results show, on the one hand,that machine translation can be reliably used formultilingual sentiment analysis and, on the otherhand, which are the main characteristics of the datafor such approaches to be successfully employed.4 Dataset Presentation and AnalysisFor our experiments, we employed the data providedfor English in the NTCIR 8 Multilingual OpinionAnalysis Task (MOAT)3.
In this task, the organiz-ers provided the participants with a set of 20 top-ics (questions) and a set of documents in which sen-tences relevant to these questions could be found,taken from the New York Times Text (2002-2005)corpus.
The documents were given in two differ-ent forms, which had to be used correspondingly,depending on the task to which they participated.The first variant contained the documents split intosentences (6165 in total) and had to be used forthe task of opinionatedness, relevance and answer-ness.
In the second form, the sentences were alsosplit into opinion units (6223 in total) for the opin-ion polarity and the opinion holder and target tasks.For each of the sentences, the participants had toprovide judgments on the opinionatedness (whetherthey contained opinions), relevance (whether theyare relevant to the topic).
For the task of polar-ity classification, the participants had to employ thedataset containing the sentences that were also splitinto opinion units (i.e.
one sentences could containtwo/more opinions, on two/more different targets orfrom two/more different opinion holders).For our experiments, we employed the latter rep-resentation.
From this set, we randomly chose 600opinion units, to serve as test set.
The rest of opin-ion units will be employed as training set.
Subse-quently, we employed the Google Translate, Bing3http://research.nii.ac.jp/ntcir/ntcir-ws8/permission/ntcir8xinhua-nyt-moat.html54Translator and Moses systems to translate, on theone hand, the training set and on the other handthe test set, to French, German and Spanish.
Ad-ditionally, we employed the Yahoo system to trans-late only the test set into these three languages.
Fur-ther on, this translation of the test set by the Yahooservice has been corrected by a person for all thelanguages.
This corrected data serves as Gold Stan-dard4.
Most of these sentences, however, containedno opinion (were neutral).
Due to the fact that theneutral examples are majoritary and can produce alarge bias when classifying, we decided to eliminatethese examples and employ only the positive andnegative sentences in both the training, as well asthe test sets.
After this elimination, the training setcontains 943 examples (333 positive and 610 nega-tive) and the test set and Gold Standard contain 357examples (107 positive and 250 negative).5 Machine TranslationDuring the 1990?s the research community on Ma-chine Translation proposed a new approach thatmade use of statistical tools based on a noisy chan-nel model originally developed for speech recogni-tion (Brown et al, 1994).
In the simplest form, Sta-tistical Machine Translation (SMT) can be formu-lated as follows.
Given a source sentence writtenin a foreign language f , the Bayes rule is appliedto reformulate the probability of translating f into asentence e written in a target language:ebest = argmaxep(e|f) = argmaxep(f |e)pLM (e)where p(f |e) is the probability of translating e to fand pLM (e) is the probability of producing a fluentsentence e. For a full description of the model see(Koehn, 2010).The noisy channel model was extended in differ-ent directions.
In this work, we analyse the mostpopular class of SMT systems: PBSMT.
It is an ex-tension of the noisy channel model using phrasesrather than words.
A source sentence f is segmented4Please note that each sentence may contain more than oneopinion unit.
In order to ensure a contextual translation, wetranslated the whole sentences, not the opinion units separately.In the end, we eliminate duplicates of sentences (due to the factthat they contained multiple opinion units), resulting in around400 sentences in the test and Gold Standard sets and 5700 sen-tences in the training setinto a sequence of I phrases f I = {f1, f2, .
.
.
fI}and the same is done for the target sentence e, wherethe notion of phrase is not related to any grammat-ical assumption; a phrase is an n-gram.
The besttranslation ebest of f is obtained by:ebest = argmaxep(e|f) = argmaxep(f |e)pLM (e)= argmaxeI?i=1?(fi|ei)?
?d(ai ?
bi?1)?d|e|?i=1pLM (ei|e1 .
.
.
ei?1)?LMwhere ?
(fi|ei) is the probability of translating aphrase ei into a phrase fi.
d(ai ?
bi?1) is thedistance-based reordering model that drives the sys-tem to penalise significant reorderings of words dur-ing translation, while allowing some flexibility.
Inthe reordering model, ai denotes the start positionof the source phrase that is translated into the ithtarget phrase, and bi?1 denotes the end position ofthe source phrase translated into the (i ?
1)th targetphrase.
pLM (ei|e1 .
.
.
ei?1) is the language modelprobability that is based on the Markov?s chain as-sumption.
It assigns a higher probability to flu-ent/grammatical sentences.
?
?, ?LM and ?d areused to give a different weight to each element.
Formore details see (Koehn et al, 2003).Three different SMT systems were used to trans-late the human annotated sentences: two existingonline services such as Google Translate and BingTranslator5 and an instance of the open sourcephrase-based statistical machine translation toolkitMoses (Koehn et al, 2007).To train our models based on Moses we used thefreely available corpora: Europarl (Koehn, 2005),JRC-Acquis (Steinberger et al, 2006), Opus (Tiede-mann, 2009), News Corpus (Callison-Burch et al,2009).
This results in 2.7 million sentence pairs forEnglish-French, 3.8 for German and 4.1 for Span-ish.
All the modes are optimized running the MERTalgorithm (Och, 2003) on the development part ofthe News Corpus.
The translated sentences are re-cased and detokonized (for more details on the sys-tem, please see (Turchi et al, 2012).5http://translate.google.com/ and http://www.microsofttranslator.com/55Performances of a SMT system are automati-cally evaluated comparing the output of the systemagainst human produced translations.
Bleu score(Papineni et al, 2001) is the most used metric and itis based on averaging n-gram precisions, combinedwith a length penalty which penalizes short transla-tions containing only sure words.
It ranges between0 and 1, and larger value identifies better translation.6 Sentiment AnalysisIn the field of sentiment analysis, most work hasconcentrated on creating and evaluating methods,tools and resources to discover whether a specific?target?or ?object?
(person, product, organization,event, etc.)
is ?regarded?
in a positive or negativemanner by a specific ?holder?
or ?source?
(i.e.
a per-son, an organization, a community, people in gen-eral, etc.).
This task has been given many names,from opinion mining, to sentiment analysis, reviewmining, attitude analysis, appraisal extraction andmany others.The issue of extracting and classifying sentimentin text has been approached using different methods,depending on the type of text, the domain and thelanguage considered.
Broadly speaking, the meth-ods employed can be classified into unsupervised(knowledge-based), supervised and semi-supervisedmethods.
The first usually employ lexica or dictio-naries of words with associated polarities (and val-ues - e.g.
1, -1) and a set of rules to compute thefinal result.
The second category of approaches em-ploy statistical methods to learn classification mod-els from training data, based on which the test datais then classified.
Finally, semi-supervised methodsemploy knowledge-based approaches to classify aninitial set of examples, after which they use differentmachine learning methods to bootstrap new trainingexamples, which they subsequently use with super-vised methods.The main issue with the first approach is that ob-taining large-enough lexica to deal with the vari-ability of language is very expensive (if it is donemanually) and generally not reliable (if it is doneautomatically).
Additionally, the main problem ofsuch approaches is that words outside contexts arehighly ambiguous.
Semi-supervised approaches, onthe other hand, highly depend on the performance ofthe initial set of examples that is classified.
If we areto employ machine translation, the errors in translat-ing this small initial set would have a high negativeimpact on the subsequently learned examples.
Thechallenge of using statistical methods is that they re-quire training data (e.g.
annotated corpora) and thatthis data must be reliable (i.e.
not contain mistakesor ?noise?).
However, the larger this dataset is, theless influence the translation errors have.Since we want to study whether machine transla-tion can be employed to perform sentiment analy-sis for different languages, we employed statisticalmethods in our experiments.
More specifically, weused Support Vector Machines Sequential MinimalOptimization (SVM SMO) since the literature in thefield has confirmed it as the most appropriate ma-chine learning algorithm for this task.In the case of statistical methods, the most impor-tant aspect to take into consideration is the mannerin which texts are represented - i.e.
the features thatare extracted from it.
For our experiments, we repre-sented the sentences based on the unigrams and thebigrams that were found in the training data.
Al-though there is an ongoing debate on whether bi-grams are useful in the context of sentiment classi-fication, we considered that the quality of the trans-lation can also be best quantified in the process byusing these features (because they give us a measureof the translation correctness, both regarding words,as well as word order).
Higher level n-grams, on theother hand, would only produce more sparse featurevectors, due to the high language variability and themistakes in the traslation.7 ExperimentsIn order to test the performance of sentiment classi-fication when using translated data, we performed aseries of experiments:?
In the first set of experiments, we trained anSVM SMO classifier on the training data ob-tained for each language, with each of the threemachine translations, separately (i.e.
we gen-erated a model for each of the languages con-sidered, for each of the machine translationsystems employed).
Subsequently, we testedthe models thus obtained on the correspond-ing test set (e.g.
training on the Spanish train-56ing set obtained using Google Translate andtesting on the Spanish test set obtained usingGoogle Translate) and on the Gold Standard forthe corresponding language (e.g.
training onthe Spanish training set obtained using GoogleTranslate and testing on the Spanish Gold Stan-dard).
Additionally, in order to study the man-ner in which the noise in the training data canbe removed, we employed two meta-classifiers- AdaBoost and Bagging (with varying sizes ofthe bag).?
In the second set of experiments, we combinedthe translated data from all three machine trans-lation systems for the same language and cre-ated a model based on the unigram and bigramfeatures extracted from this data (e.g.
we cre-ated a Spanish training model using the uni-grams and bigrams present in the training setsgenerated by the translation of the training setto Spanish by Google Translate, Bing Trans-lator and Moses).
We subsequently tested theperformance of the sentiment classification us-ing the Gold Standard for the correspondinglanguage, represented using the features of thismodel.Table 1 presents the number of unigram and bi-gram features employed in each of the cases.In the following subsections, we present the re-sults of these experiments.7.1 Individual Training with Translated DataIn the first experiment, we translated the trainingand test data from English to all the three otherlanguages considered, using each of the three ma-chine translation systems.
Subsequently, we rep-resented, for each of the languages and translationsystems, the sentences as vectors, whose featuresmarked the presence/absence (1 or 0) of the uni-grams and bigrams contained in the correspondingtrainig set (e.g.
we obtained the unigrams and bi-grams in all the sentences in the training set ob-tained by translating the English training data toSpanish using Google and subsequently representedeach sentence in this training set, as well as the testset obtained by translating the test data in English toSpanish using Google marking the presence of theunigram and bigram features).
In order to test theapproach on the Gold Standard (for each language),we represented this set using the corresponding un-igram and bigram features extracted from the cor-responding training set (for the example given, werepresented each sentence in the Gold Standard bymarking the presence/absence of the unigrams andbigrams from the training data for Spanish usingGoogle Translate).The results of these experiments are presented inTable 2, in terms of weighted F1 measure.7.2 Joint Training with Translated DataIn the second set of experiments, we added togetherall the translations of the training data obtained forthe same language, with the three different MT sys-tems.
Subsequently, we represented, for each lan-guage in part, each of the sentences in the joint train-ing corpus as vectors, whose features representedthe presence/absence of the unigrams and bigramscontained in this corpus.
In order to test the perfor-mance of the sentiment classification, we employedthe Gold Standard for the corresponding language,representing each sentence it contains according tothe presence or absence of the unigrams and bigramsin the corresponding joint training corpus for thatlanguage.
Finally, we applied SVM SMO to classifythe sentences according to the polarity of the senti-ment they contained.
Additionally, we applied theAdaBoost and Bagging meta-classifiers to test thepossibilities to minimize the impact of noise in thedata.
The results are presented in Tables 3 and 4,again, in terms of weighter F1 measure.Language SMO AdaBoost M1 BaggingTo German 0.565?
0.563?
0.565?To Spanish 0.419 0.494 0.511To French 0.25 0.255 0.23Table 3: For each language, each classifier has beentrained merging the translated data coming form differ-ent SMT systems, and tested using the Gold Standard.
?Classifier is not able to discriminate between positiveand negative classes, and assigns most of the test pointsto one class, and zero to the other.8 Results and DiscussionGenerally speaking, from our experiments usingSVM, we could see that incorrect translations imply57Bing Google T. MosesTo German 0.57?
0.572?
0.562?To Spanish 0.392 0.511 0.448To French 0.612?
0.571?
0.575?Table 4: For each language, the SMO classifiers havebeen trained merging the translated data coming form dif-ferent SMT systems, and tested using independently thetranslated test sets.
?Classifier is not able to discriminatebetween positive and negative classes, and assigns mostof the test points to one class, and zero to the other.an increment of the features, sparseness and moredifficulties in identifying a hyperplane which sepa-rates the positive and negative examples in the train-ing phase.
Therefore, a low quality of the translationleads to a drop in performance, as the features ex-tracted are not informative enough to allow for theclassifier to learn.From Table 2, we can see that:a) There is a small difference between performancesof the sentiment analysis system using the Englishand translated data, respectively.
In the worst case,there is a maximum drop of 8 percentages.b) Adaboost is sensitive to noisy data, and it isevident in our experiments where in general it doesnot modify the SMO performances or there is adrop.
Vice versa, Bagging, reducing the variancein the estimated models, produces a positive effecton the performances increasing the F-score.
Theseimprovements are larger using the German data,this is due to the poor quality of the translated data,which increases the variance in the data.Looking at the results in Tables 3 and 4, we cansee that:a) Adding all the translated training data togetherdrastically increases the noise level in the trainingdata, creating harmful effects in terms of clas-sification performance: each classifier loses itsdiscriminative capability.b) At language level, clearly the results dependon the translation performance.
Only for Spanish(for which we have the highest Bleu score), eachclassifies is able to properly learn from the trainingdata and try to properly assign the test samples.
Forthe other languages, translated data are so noisythat the classifier is not able to properly learn thecorrect information for the positive and the negativeclasses, this results in the assignment of most ofthe test points to one class and zero to the other.
InTable 3, for the French language we have significantdrop in performance, but the classifier is still ableto learn something from the training and assign thetest points to both the classes.c) The results for Spanish presented in Table 3confirm the capability of Bagging to reduce themodel variance and increase the performance inclassification.d) At system level in Table 4, there is no evidencethat better translated test set alows better classifica-tion performance.9 Conclusions and Future WorkIn this work we propose an extensive evaluation ofthe use of translated data in the context of sentimentanalysis.
Our findings show that SMT systems aremature enough to produce reliably training data forlanguages other than English.
The gap in classifi-cation performance between systems trained on En-glish and translated data is minimal, with a maxi-mum of 8Working with translated data implies an incre-ment number of features, sparseness and noise in thedata points in the classification task.
To limit theseproblems, we test three different classification ap-proaches showing that bagging has a positive impactin the results.In future work, we plan to investigate differentdocument representations, in particular we believethat the projection of our documents in space wherethe features belong to a sentiment lexical and in-clude syntax information can reduce the impact ofthe translation errors.
As well we are interested toevaluate different term weights such as tf-idf.AcknowledgmentsThe authors would like to thank Ivano Azzini, fromthe BriLeMa Artificial Intelligence Studies, for theadvice and support on using meta-classifiers.
Wewould also like to thank the reviewers for their use-ful comments and suggestions on the paper.58ReferencesTurchi, M. and Atkinson, M. and Wilcox, A. and Craw-ley, B. and Bucci, S. and Steinberger, R. and Van derGoot, E. 2012.
ONTS: ?Optima?
News TranslationSystem..
Proceedings of EACL 2012.Banea, C., Mihalcea, R., and Wiebe, J.
2008.
A boot-strapping method for building subjectivity lexicons forlanguages with scarce resources..
Proceedings of theConference on Language Resources and Evaluations(LREC 2008), Maraakesh, Marocco.Banea, C., Mihalcea, R., Wiebe, J., and Hassan, S.2008.
Multilingual subjectivity analysis using ma-chine translation.
Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP 2008), 127-135, Honolulu, Hawaii.Banea, C., Mihalcea, R. and Wiebe, J.
2010.
Multilin-gual subjectivity: are more languages better?.
Pro-ceedings of the International Conference on Computa-tional Linguistics (COLING 2010), p. 28-36, Beijing,China.Boudin, F. and Huet, S. and Torres-Moreno, J.M.
andTorres-Moreno, J.M.
2010.
A Graph-based Ap-proach to Cross-language Multi-document Summa-rization.
Research journal on Computer scienceand computer engineering with applications (Polibits),43:113?118.P.
F. Brown, S. Della Pietra, V. J. Della Pietra and R. L.Mercer.
1994.
The Mathematics of Statistical Ma-chine Translation: Parameter Estimation, Computa-tional Linguistics 19:263?311.C.
Callison-Burch, and P. Koehn and C. Monz and J.Schroeder.
2009.
Findings of the 2009 Workshop onStatistical Machine Translation.
Proceedings of theFourth Workshop on Statistical Machine Translation,pages 1?28.
Athens, Greece.Deerwester, S., Dumais, S., Furnas, G. W., Landauer, T.K., and Harshman, R. 1990.
Indexing by latent se-mantic analysis.
Journal of the American Society forInformation Science, 3(41).Kim, S.-M. and Hovy, E. 2006.
Automatic identificationof pro and con reasons in online reviews.
Proceedingsof the COLING/ACL Main Conference Poster Ses-sions, pages 483490.Kim, J., Li, J.-J.
and Lee, J.-H. 2006.
EvaluatingMultilanguage-Comparability of Subjectivity AnalysisSystems.
Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages595603, Uppsala, Sweden, 11-16 July 2010.P.
Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
Proceedings of theMachine Translation Summit X, pages 79-86.
Phuket,Thailand.P.
Koehn.
2010.
Statistical Machine Translation.
Cam-bridge University Press.P.
Koehn and F. J. Och and D. Marcu.
2003.
StatisticalPhrase-Based Translation, Proceedings of the NorthAmerica Meeting on Association for ComputationalLinguistics, 48?54.P.
Koehn and H. Hoang and A. Birch and C. Callison-Burch and M. Federico and N. Bertoldi and B. Cowanand W. Shen and C. Moran and R. Zens and C. Dyerand O. Bojar and A. Constantin and E. Herbst 2007.Moses: Open source toolkit for statistical machinetranslation.
Proceedings of the Annual Meeting of theAssociation for Computational Linguistics, demon-stration session, pages 177?180.
Columbus, Oh, USA.Mihalcea, R., Banea, C., and Wiebe, J.
2009.
Learn-ing multilingual subjective language via cross-lingualprojections.
Proceedings of the Conference of the An-nual Meeting of the Association for ComputationalLinguistics 2007, pp.976-983, Prague, Czech Repub-lic.F.
J. Och 2003.
Minimum error rate training in statisti-cal machine translation.
Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics, pages 160?167.
Sapporo, Japan.K.
Papineni and S. Roukos and T. Ward and W. J. Zhu2001.
BLEU: a method for automatic evaluation ofmachine translation.
Proceedings of the 40th AnnualMeeting on Association for Computational Linguis-tics, pages 311?318.
Philadelphia, Pennsylvania.J.
Savoy, and L. Dolamic.
2009.
How effective isGoogle?s translation service in search?.
Communi-cations of the ACM, 52(10):139?143.R.
Steinberger and B. Pouliquen and A. Widiger and C.Ignat and T. Erjavec and D. Tufis?
and D. Varga.
2006.The JRC-Acquis: A multilingual aligned parallel cor-pus with 20+ languages.
Proceedings of the 5th Inter-national Conference on Language Resources and Eval-uation, pages 2142?2147.
Genova, Italy.J.
Tiedemann.
2009.
News from OPUS-A Collection ofMultilingual Parallel Corpora with Tools and Inter-faces.
Recent advances in natural language processingV: selected papers from RANLP 2007, pages 309:237.Wan, X. and Li, H. and Xiao, J.
2010.
Cross-languagedocument summarization based on machine transla-tion quality prediction.
Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 917?926.Wilson, T., Wiebe, J., and Hoffmann, P. 2005.
Recogniz-ing contextual polarity in phrase-level sentiment anal-ysis.
Proceedings of HLT-EMNLP 2005, pp.347-354,Vancouver, Canada.59Language SMT system Nr.
of unigrams Nr.
of bigramsFrenchBing 7441 17870Google 7540 18448Moses 6938 18814Bing+Google+Moses 9082 40977GermanBing 7817 16216Google 7900 16078Moses 7429 16078Bing+Google+Moses 9371 36556SpanishBing 7388 17579Google 7803 18895Moses 7528 18354Bing+Google+Moses 8993 39034Table 1: Features employed.Language SMT Test Set SMO AdaBoost M1 Bagging Bleu ScoreEnglish GS 0.685 0.685 0.686To GermanBingGS 0.641 0.631 0.648Tr 0.658 0.636 0.662 0.227To GermanGoogle T.GS 0.646 0.623 0.674Tr 0.687 0.645 0.661 0.209To GermanMosesGS 0.644 0.644 0.676Tr 0.667 0.667 0.674 0.17To SpanishBingGS 0.656 0.658 0.646Tr 0.633 0.633 0.633 0.316To SpanishGoogle T.GS 0.653 0.653 0.665Tr 0.636 0.667 0.636 0.341To SpanishMosesGS 0.664 0.664 0.671Tr 0.649 0.649 0.663 0.298To FrenchBingGS 0.644 0.645 0.664Tr 0.644 0.649 0.652 0.243To FrenchGoogle T.GS 0.64 0.64 0.659Tr 0.652 0.652 0.678 0.274To FrenchMosesGS 0.633 0.633 0.645Tr 0.666 0.666 0.674 0.227Table 2: Results obtained using the individual training sets obtained by translating with each of the three consideredMT systems, to each of the three languages considered.60
