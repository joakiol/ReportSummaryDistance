Tracking Initiative in Collaborative Dialogue InteractionsJ enn i fe r  Chu-Car ro l l  and Michae l  K.  BrownBell  Laborator iesLucent  Technologies600 Mounta in  AvenueMurray Hil l ,  N J  07974, U.S.A.E-mai l :  { jencc,mkb} @ bel l - labs.cornAbstractIn this paper, we argue for the need to dis-tinguish between task and dialogue initiatives,and present a model for tracking shifts in bothtypes of initiatives in dialogue interactions.Our model predicts the initiative holders in thenext dialogue turn based on the current initia-tive holders and the effect that observed cueshave on changing them.
Our evaluation acrossvarious corpora shows that he use of cues con-sistently improves the accuracy in the system' sprediction of task and dialogue initiative hold-ers by 2-4 and 8-13 percentage points, respec-tively, thus illustrating the generality of ourmodel.1 IntroductionNaturally-occurring collaborative dialogues are veryrarely, if ever, one-sided.
Instead, initiative of the in-teraction shifts among participants in a primarily princi-pled fashion, signaled by features uch as linguistic ues,prosodic ues and, in face-to-face interactions, eye gazeaad gestures.
Thus, for a dialogue system to interact withits user in a natural and coherent manner, it must recog-nize the user's cues for initiative shifts and provide ap-propriate cues in its responses to user utterances.Previous work on mixed-initiative dialogues focusedon tracking asingle thread of control among participants.We argue that this view of initiative fails to distinguishbetween task initiative and dialogue initiative, which to-gether determine when and how an agent will addressan issue.
Although physical cues, such as gestures andeye gaze, play an important role in coordinating initia-tive shifts in face-to-face interactions, a great deal ofinformation regarding initiative shifts can be extractedfrom utterances based on linguistic and domain knowl-edge alone.
By taking into account such cues during dia-logue interactions, the system is better able to determinethe task and dialogue initiative holders for each turn andto tailor its response to user utterances accordingly.In this paper, we show how distinguishing betweentask and dialogue initiatives accounts for phenomena incollaborative dialogues that previous models were unableto explain.
We show that a set of cues, which can berecognized based on linguistic and domain knowledgealone, can be utilized by a model for tracking initiativeto predict the task and dialogue initiative holders with99.1% and 87.8% accuracies, respectively, in collabo-rative planning dialogues.
Furthermore, application ofour model to dialogues in various other collaborative en-vironments consistently increases the accuracies in theprediction of task and dialogue initiative holders by 2-4and 8-13 percentage points, respectively, compared to asimple prediction method without he use of cues, thusillustrating the generality of our model.2 Task  In i t ia t ive  vs.
Dialogue Initiative2.1 MotivationPrevious work on mixed-initiative dialogues focused ontracking and allocating a single thread of control, theconversational lead, among participants.
Novick (1988)developed a computational model that utilizes meta-locutionary acts, such as repeat and give-turn, to cap-ture mixed-initiative behavior in dialogues.
Whittakerand Stenton (1988) devised rules for allocating dialoguecontrol based on utterance types, and Walker and Whit-taker (1990) utilized these rules for an analytical studyon discourse segmentation.
Kitano and Van Ess-Dykema(1991) developed a plan-based ialogue understandingmodel that tracks the conversational initiative based onthe domain and discourse plans behind the utterances.Smith and Hipp (1994) developed a dialogue system thatvaries its responses to user utterances based on four di=alogue modes which model different levels of initiativeexhibited by dialogue participants.
However, the dia-logue mode is determined at the outset and cannot bechanged uring the dialogue.
Guinn (1996) subsequentlydeveloped a system that allows change in the level of ini-262tiative based on initiative-changing utterances and eachagent's competency in completing the current subtask.However, we contend that merely maintaining the con-versational lead is insufficient for modeling complex be-havior commonly found in naturally-occurring collabo-rative dialogues (SRI Transcripts, 1992; Gross, Allen,and Tram, 1993; Heeman and Allen, 1995).
For in-stance, consider the alternative responses in utterances(3a)-(3c), given by an advisor to a student's question:(1) S: I want to take NLP to satisfy my seminarcourse requirement.
(2) Who is teaching NLP?
(3a) A: Dr. Smith is teaching NLP.
(3b) A: You can't take NLP because you haven'ttaken AI, which is a prerequisite for NLP(3c) A: You can't take NLP because you haven'ttaken AI, which is a prerequisite for NLPYou should take distributed programmingto satisfy your requirement, and sign upas a listener for NI.~.Suppose we adopt a model that maintains a singlethread of control, such as that of (Whittaker and Stenton,1988).
In utterance (3a), A directly responds to S's ques-tion; thus the conversational lead remains with S. On theother hand, in (3b) and (3c), A takes the lead by initiatinga subdialogue to correct S's invalid proposal.
However,existing models cannot explain the difference in the tworesponses, namely that in (3c), A actively participates inthe planning process by explicitly proposing domain ac-tions, whereas in (3b), she merely conveys the invalid-ity of S's proposal.
Based on this observation, we arguethat it is necessary to distinguish between task initiative,which tracks the lead in the development of the agents'plan, and dialogue initiative, which tracks the lead in de-termining the current discourse focus (Chu-Carroll andBrown, 1997).
1This distinction then allows us to explain?
~/s behavior from a response generation point of view: in(3b), A responds to S's proposal by merely taking overthe dialogue initiative, i.e., informing S of the invalidityof the proposal, while in (3c), A responds by taking overboth the task and dialogue initiatives, i.e., informing S ofthe invalidity and suggesting a possible remedy.An agent is said to have the task initiative if she isdirecting how the agents' task should be accomplished,i.e., if her utterances directly propose actions that the1Although independently conceived, this distinction be-tween task and dialogue initiatives i similar to the notion ofchoice of task and choice of speaker in initiative in (Novickand Sutton, 1997), and the distinction between control and ini-tiative in (Jordan and Di Eugenio, 1997).TI: system37 (3.5%)TI: manager274 (26.3%)727 (69.8%)DI: systemDI: manager 4 (0.4%)Table 1: Distribution of Task and Dialogue Initiativesagents should perform.
The utterances may proposedomain actions (Litman and Allen, 1987) that directlycontribute to achieving the agents' goal, such as "Let'ssend engine E2 to Coming."
On the other hand, theymay propose problem-solving actions (Allen, 1991;Lambert and Carberry, 1991; Ramshaw, 1991) that con-tribute not directly to the agents' domain goal, but to howthey would go about achieving this goal, such as "Let'slook at the first \[problem\]first."
An agent is said to havethe dialogue initiative if she takes the conversationallead in order to establish mutual beliefs, such as mutualbeliefs about a piece of domain knowledge or about hevalidity of a proposal, between the agents.
For instance,in responding to agent Xs proposal of sending a boxcarto Coming via Dansville, agent B may take over the dia-logue initiative (but not the task initiative) by saying "Wecan't go by Dansville because we've got Engine I goingon that track."
Thus, when an agent akes over the taskinitiative, she also takes over the dialogue initiative, sincea proposal of actions can be viewed as an attempt to es-tablish the mutual belief that a set of actions be adopted.On the other hand, an agent may take over the dialogueinitiative but not the task initiative, as in (3b) above.2.2 An Analysis of the TRAINS91 DialoguesTo analyze the distribution of task/dialogue initiativesin collaborative planning dialogues, we annotated theTRAINS91 dialogues (Gross, Allen, and Traum, 1993)as follows: each dialogue turn is given two labels, taskinitiative (TI) and dialogue initiative (DI), each of whichcan be assigned one of two values, system or manager,depending on which agent holds the task/dialogue initia-tive during that turn.
2Table 1 shows the distribution of task and dialogue ini-tiatives in the TRAINS91 dialogues.
It shows that whilein the majority of turns, the task and dialogue initiativesare held by the same agent, in approximately 1/4 of theturns, the agents' behavior can be better accounted forbytracking the two types of initiatives eparately.To assess the reliability of our annotations, approxi-mately 10% of the dialogues were annotated by two ad-ditional coders.
We then used the kappa statistic (Siegeland Castellan, 1988; Carletta, 1996) to assess the level ofagreement between the three coders with respect o the2 An agent holds the task initiative during a turn as long assome utterance during the turn directly proposes how the agentsshould accomplish t eir goal, as in utterance (3c).263task and dialogue initiative holders.
In this experiment,K is 0,57 for the task initiative holder agreement and Kis 0.69 for the dialogue initiative holder agreement.Carletta suggests that content analysis researchersconsider K >.8 as good reliability, with .67< /~" <.8allowing tentative conclusions to be drawn (Carletta,1996).
Strictly based on this metric, our results indicatethat the three coders have a reasonable l vel of agree-ment with respect o the dialogue initiative holders, butdo not have reliable agreement with respect to the taskinitiative holders.
However, the kappa statistic is knownto be highly problematic n measuring inter-coder reli-ability when the likelihood of one category being cho-sen overwhelms that of the other (Grove et al, 1981),which is the case for the task initiative distribution i  theTRAINS91 corpus, as shown in Table 1.
Furthermore, aswill be shown in Table 4, Section 4, the task and dialogueinitiative distributions in TRAINS91 are not at all repre-sentative of collaborative dialogues.
We expect hat bytaking a sample of dialogues whose task/dialogue initia-tive distributions are more representative of all dialogues,we will lower the value of P(E), the probability of chanceagreement, and thus obtain a higher kappa coefficient ofagreement.
However, we leave selecting and annotatingsuch a subset of representative dialogues for future work.3 A Model for Tracking InitiativeOur analysis hows that the task and dialogue initiativesshift between the participants during the course of a di-alogue.
We contend that it is important for the agentsto take into account signals for such initiative shifts fortwo reasons.
First, recognizing and providing signalsfor initiative shifts allow the agents to better coordinatetheir actions, thus leading to more coherent and cooper-ative dialogues.
Second, by determining whether or notit should hold the task and/or dialogue initiatives whenresponding to user utterances, a dialogue system is ableto tailor its responses based on the distribution of initia-tives, as illustrated by the previous dialogue (Chu-Carrolland Brown, 1997).
This section describes our model fortracking initiative using cues identified from the user'sutterances.Our model maintains, for each agent, a task initiativeindex and a dialogue initiative index which measure theamount of evidence available to support he agent hold-ing the task and dialogue initiatives, respectively.
Aftereach turn, new initiative indices are calculated based onthe current indices and the effects of the cues observedduring the turn.
These cues may be explicit requests bythe speaker to give up his initiative, or implicit cues suchas ambiguous proposals.
The new initiative indices thendetermine the initiative holders for the next turn.We adopt the Dempster-Shafer theory of evidence(Sharer, 1976; Gordon and Shortliffe, 1984) as our un-derlying model for inferring the accumulated effect ofmultiple cues on determining the initiative indices.
TheDempster-Shafer theory is a mathematical theory for rea-soning under uncertainty which operates over a set ofpossible outcomes, O.
Associated with each piece ofevidence that may provide support for the possible out-comes is a basic probability assignment (bpa), a func-tion that represents he impact of the piece of evidenceon the subsets of O.
A bpa assigns anumber in the range\[0,1\] to each subset of O such that the numbers um to 1.The number assigned to the subset O1 then denotes theamount of support the evidence directly provides for theconclusions represented by O1.
When multiple piecesof evidence are present, Dempster' s combination rule isused to compute a new bpa from the individual bpa' s torepresent their cumulative ffect.The reasons for selecting the Dempster-Shafer theoryas the basis for our model are twofold.
First, unlikethe Bayesian model, it does not require a complete setof a priori and conditional probabilities, which is dif-ficult to obtain for sparse pieces of evidence.
Second,the Dempster-Shafer theory distinguishes between situ-ations in which no evidence is available to support anyconclusion and those in which equal evidence is avail-able to support each conclusion.
Thus the outcome ofthe model more accurately represents he amount of ev-idence available to support a particular conclusion, i.e.,the provability of the conclusion (Pearl, 1990).3.1 Cues for Tracking InitiativeIn order to utilize the Dempster-Shafer theory for mod-eling initiative, we must first identify the cues that pro-vide evidence for initiative shifts.
Whittaker, Stenton,and Walker (Whittaker and Stenton, 1988; Walker andWhittaker, 1990) have previously identified a set of ut-terance intentions that serve as cues to indicate shifts orlack of shifts in initiative, such as prompts and questions.We analyzed our annotated TRAINS91 corpus and iden-tified additional cues that may have contributed to theshift or lack of shift in task/dialogue initiatives duringthe interactions.
This results in eight cue types, which aregrouped into three classes, based on the kind of knowl-edge needed to recognize them.
Table 2 shows the threeclasses, the eight cue types, their subtypes if any, whethera cue may affect merely the dialogue initiative or boththe task and dialogue initiatives, and the agent expectedto hold the initiative in the next turn.The first cue class, explicit cues, includes explicit re-quests by the speaker to give up or take over the initiative.For instance, the utterance "Any suggestions ?"
indicatesthe speaker's intention for the hearer to take over boththe task and dialogue initiatives.
Such explicit cues canbe recognized by inferring the discourse and/or problem-solving intentions conveyed by the speaker' s utterances.264Class Cue Type SubtypeExplicit Explicit requests give uptake overDiscourse End silenceNo new info repetitionsEffectbothbothbothbothInitiative Examplehearerspeakerhearerhearerprompts both hearerQuestions domain DI speakerevaluation DI hearerObligation task both hearerfulfilleddiscourseactionbeliefDIAnalytical InvaliditySuboptimahty"Any suggestions?"
"Summarize the plan up to this point""Let me handle this one.
"A:hearer A:B:A:Ambiguity actionbeliefA: "Grab the tanker, pick up oranges, go to Elmira,make them into orange juice.
"B: "We go to Elmira, we make orange juice, okay.
'""Yeah ", "Ok", "Right""How far is it from Bath to Coming?
""Can we do the route the banana guy isn't doing?
"A: "Any suggestions ?
"B: "Well, there's a boxcar at Dansville.
""But you have to change your banana plan.
""How long is it from Dansville to Coming ?
""Go ahead and fill up E1 with bananas.
""Well, we have to get a boxcar.""Right.
okay.
It's shorter to Bath from Avon.
"both hearerDI hearerboth hearerboth hearerDI hearerA: "Let's get the tanker car to Elmira anaJill it with OJ.B: "You need to get oranges to the O J factory.
"A: "h' s shorter to Bath from Avon.
"B: "R's shorter to DansvUle.
'""The map is slightly misleading.
"A: "Using Saudi on Thursday the eleventh.
'"B: "It's sold out.
"A: "Is Friday open?
"B: "Economy on Pan Am is open on Thursday.
"A: "Take one of the engines from Coming.
"B: "Let's say engine E2.
"A: "We would get back to Coming at 4.
"B: "4PM?
4AM?
"Table 2: Cues for Modeling InitiativeThe second cue class, discourse cues, includes cuesthat can be recognized using linguistic and discourse in-formation, such as from the surface form of an utterance,or from the discourse relationship between the currentand prior utterances.
It consists of four cue types.
Thefirst type is perceptible silence at the end of an utterance,which suggests that the speaker has nothing more to sayand may intend to give up her initiative.
The second typeincludes utterances that do not contribute informationthat has not been conveyed earlier in the dialogue.
It canbe further classified into two groups: repetitions, a sub-set of the informationally redundant utterances (Walker,1992), in which the speaker paraphrases an utteranceby the hearer or repeats the utterance verbatim, andprompts, in which the speaker merely acknowledges thebearer's previous utterance(s).
Repetitions and promptsalso suggest that the speaker has nothing more to say andindicate that the hearer should take over the initiative(Whittaker and Stenton, 1988).
The third type includesquestions which, based on anticipated responses, aredivided into domain and evaluation questions.
Domainquestions are questions in which the speaker intendsto obtain or verify a piece of domain knowledge.They usually merely require a direct response and thustypically do not result in an initiative shift.
Evaluationquestions, on the other hand, are questions in which thespeaker intends to assess the quality of a proposed plan.They often require an analysis of the proposal, and thusfrequently result in a shift in dialogue initiative.
Thefinal type includes utterances that satisfy an outstandingtask or discourse obligation.
Such obligations may haveresulted from a prior request by the hearer, or from aninterruption initiated by the speaker himself.
In eithercase, when the task/dialogue obligation is fulfilled, theinitiative may be reverted back to the hearer who heldthe initiative prior to the request or interruption.The third cue class, analytical cues, includes cuesthat cannot be recognized without the hearer perform-ing an evaluation on the speaker's proposal using theheater's private knowledge (Chu-Carroll and Carberry,1994; Chu-Carroll and Carberry, 1995).
After the eval-uation, the hearer may find the proposal invalid, subop-timal, or ambiguous.
As a result, he may initiate a sub-dialogue to resolve the problem, resulting in a shift intask/dialogue initiatives.
33 Whittaker, Stenton, and Walker treat subdialogues initiatedas a result of these cues as interruptions, motivated by their col-laborative planning principles (Whittaker and Stenton, 1988;Walker and Whittaker, 1990).2653.2 Utilizing the Dempster-Shafer TheoryAs discussed earlier, at the end of each turn, newtask/dialogue initiative indices are computed based onthe current indices and the effect of the observed cuesto determine the next task/dialogue initiative holders.
Interms of the Dempster-Shafer theory, new task/dialoguebpa's (mt_new/md_netu)  4 are computed by applyingDempster's combination rule to the bpa's representingthe current initiative indices ~ and the bpa of eachobserved cue.Evidently, some cues provide stronger evidence foran initiative shift than others.
Furthermore, a cue mayprovide stronger support for a shift in dialogue initiativethan in task initiative.
Thus, we associate with each cuetwo bpa' s to represent i s effect on changing the currenttask and dialogue initiative indices, respectively.
We ex-tended our annotations of the TRAINS91 dialogues toinclude, in addition to the agent(s) holding the task anddialogue initiatives for each turn, a list of cues observedduring that turn.
Initially, each cue~ is assigned the fol-lowing bpa's: mt- i (O)  ~- I and ma-i(@) = 1, where@ = {speaker,hearer}.
In other words, we assume thatthe cue has no effect on changing the current initiativeindices.
We then developed a training algorithm (Train-bpa, Figure 1) and applied it on the annotated ata toobtain the final bpa' s.For each turn, the task and dialogue bpa's for eachobserved cue are used, along with the current initiativeindices, to determine the new initiative indices (step 2).The combine function utilizes Dempster's combinationrule to combine pairs of bpa' suntil a final bpa is obtainedto represent the cumulative ffect of the given bpa' s. Theresulting bpa's are then used to predict he task/dialogueinitiative holders for the next turn (step 3).
If this pre-diction disagrees with the actual value in the annotateddata, Adjust-bpa is invoked to alter the bpa' s for the ob-served cues, and Reset-current-bpa is invoked to ad-just the current bpa' s to reflect he actual initiative holder(step 4).Adjust-bpa adjusts the bpa's for the observed cuesin favor of the actual initiative holder.
We developedthree adjustment methods by varying the effect that adisagreement between the actual and predicted initiativeholders will have on changing the bpa' s for the observedcues.
The first is constant- increment where each time adisagreement occurs, the value for the actual initiativeholder in the bpa is incremented by a constant (A), while4Bpa's are represented by functions whose names take theform of m,~,b.
The subscript sub may be t-X or d-X, indicat-ing that the function represents he task or dialogue bpa underscenario X.SThe initiative indices are represented as bpa's.
For in-stance, the current ask initiative indices take the followingform: rat .
.
.
.
(speaker) = z and rat .
.
.
.
(hearer) = 1 - z.Train-bpa(annotated-data):1. rat-~.,,r ~ default ask initiative indicesraa-eur - -  default dialogue initiative indicescur-data ,--- read(annotated-data)cue-set .
-  cues in cur-data2.
/* compute new initiative indices */rat-obs * - -  task initiative bpa's for cues in cue-setraa-ob~ ,-- dialogue initiative bpa' s for cues in cue-setmr-nero ~ combine(mr_cur, mt-obs)md .
.
.
.
~ combine(md .
.
.
.
.
ma-ob,)3.
/* determMe predicted next initiative holders */f f  mt  .
.
.
.
(speaker)  > rat_neio(hearer),t-predicted *--- speakerElse, t-predicted *- hearerf fmd .
.
.
.
(speaker)  > tad .
.
.
.
(hearer) ,d-predicted *--- speakerElse, d-predicted ,--- hearer4.
/'* f ind actual initiative holders and compare */new-data -- read(annotated-data)t-actual ,--- actual task initiative holder in new-datad-actual ,--- actual dialogue initiative holder in new-dataIf t-predicted # t-actual,Adjust-bpa(cue-set, task)Reset-current-bpa(mt_c=~)If d-predicted # d-actual,Adjust-bpa(cue-set,dialogue)Reset-current-bpa(ma .
.
.
.
)5.
If end-of-dialogue, returnElse, ,1" swap roles of  speaker and hearer */rat .
.
.
.
(speaker)  ~-- mt  .
.
.
.
(hearer)raa .
.
.
.
(speaker)  - -  ma .
.
.
.
(hearer)rat .
.
.
.
(hearer)  ~ rat  .
.
.
.
(speaker)rad .
.
.
.
(hearer )  ,--- raa .
.
.
.
( speaker )cue-set ,-- cues in new-dataGoto step 2.Figure l: Training Algorithm for Determining BPX sthat for O is decremented by ~.
The second method,constant- increment-with-counter,  associates with eachbpa for each cue a counter which is incremented whena correct prediction is made, and decremented when anincorrect prediction is made.
If the counter is nega-tive, the constant- increment method is invoked, and thecounter is reset o 0.
This method ensures that a bpa willonly be adjusted if it has no "credit" for correct predic-tions in the past.
The third method, variable- increment-with-counter ,  is a variation of constant- increment-with-counter.
However, instead of determining whether anadjustment is needed, the counter determines the amountto be adjusted.
Each time the system makes an incorrectprediction, the value for the actual initiative holder is in-cremented by A /2  c?
'`'~+z, and that for O decremented26610.990.98O.
970.960.95no-pred lc t lon - -const - lncconst - inc -wc  "* ..var - inc -wc  ~t l l i , t l l l0.05 0.I 0.15 0.2 0.25 0,3 0,35 0.4 0.45 0.5de l ta0.90.850.80.750.70.650.6no- red lc t lon  - -const - inc~.._ c< nst -  inc-wc "* ..var - inc -wci t J i ,0 .05 0.i 0.15 0.2 0.25 0.3 0.35 0,4 0.45 0.5de l ta(a) Task Initiative Prediction (b) Dialogue Initiative PredictionFigure 2: Comparison of Three Adjustment Methodsby the same amount.In addition to experimenting with different adjustmentmethods, we also varied the increment constant, A. Foreach adjustment method, we ran 19 training sessionswith A ranging from 0.025 to 0.475, incrementing by0.025 between each session, and evaluated the systembased on its accuracy in predicting the initiative holdersfor each turn.
We divided the TRAINS91 corpus intoeight sets based on speaker/hearer pairs.
For each A,we cross-validated the results by applying the trainingalgorithm to seven dialogue sets and testing the resultingbpa' s on the remaining set.
Figures 2(a) and 2(b) showour system's performance in predicting the task and dia-logue initiative holders, respectively, using the three ad-justment methods.
63.3 DiscussionFigure 2 shows that in the vast majority of cases, ourprediction methods yield better esults than making pre-dictions without cues.
Furthermore, substantial improve-ment is gained by the use of counters ince they preventthe effect of the "exceptions of the rules" from accu-mulating and resulting in erroneous predictions.
By re-stricting the increment to be inversely exponentially re-lated to the "credit" the bpa had in making correct pre-dictions, variable-increment-with-counter obtains bet-ter and more consistent results than constant-increment.However, the exceptions of the rules still resulted in un-desirable ffects, thus the further improved performanceby constant-increment-with-counter.We analyzed the cases in which the system, using6For comparison purposes, the straight lines show the sys-tem's performance without he use of cues, i.e., always predictthat he initiative remains with the current holder.constant-increment-with-counter with A = .35, 7 madeerroneous predictions.
Tables 3(a) and 3(b) summarizethe results of our analysis with respect o task and di-alogue initiatives, respectively.
For each cue type, wegrouped the errors based on whether or not a shift oc-curred in the actual dialogue.
For instance, the first rowin Table 3(a) shows that when the cue invalid action isdetected, the system failed to predict a task initiative shiftin 2 out of 3 cases.
On the other hand, it correctly pre-dicted all 11 cases where no shift in task initiative oc-curred.
Table 3(a) also shows that when an analyticalcue is detected, the system correctly predicted all but onecase in which there was no shift in task initiative.
How-ever, 55% of the time, the system failed to predict a shiftin task initiative, s This suggests that other features needto be taken into account when evaluating user proposalsin order to more accurately model initiative shifts result-ing from such cues.
Similar observations can be madeabout the errors in predicting dialogue initiative shiftswhen analytical cues are observed (Table 3(b)).Table 3(b) shows that when a perceptible silence isdetected at the end of an utterance, when the speakerutters a prompt, or when an outstanding discourseobligation is fulfilled (first three rows in table), thesystem correctly predicted the dialogue initiative holderin the vast majority of cases.
However, for the cue classquestions, when the actual initiative shift differs fromthe norm, i.e., speaker etaining initiative for evaluationquestions and hearer taking over initiative for domainquestions, the system's performance worsens.
In therThis is the value that yields the optimal results (Figure 2).sin the case of suboptimal ctions, we encounter the sparsedata problem.
Since there is only one instance of the cue in theset of dialogues, when the cue is present in the testing set, it isabsent from the training set.267Cue Type Subtype Shift No-Shifterror total error totalInvalidity action 2 3 0 11Suboptimality 1 1 0 0Ambiguity action 3 7 1 5(a) Task Initiative ErrorsCue TypeEnd silence'No new infoQuestionsObligation fulfilledInvalidityf f l ~Subtype Shifterror total13 41prompts 7 193domain 13 31evaluation 8 28discourse 12 19811 341 19 24(b) Dialogue Initiative ErrorsNo-Shifterror total0 53l 60" 985 7l 50 00 00 0Table 3: Summary of Prediction Errorscase of domain questions, errors occur when 1) the re-sponse requires more reasoning than do typical domainquestions, causing the hearer to take over the dialogueinitiative, or 2) the hearer, instead of merely respondingto the question, offers additional helpful information.In the case of evaluation questions, errors occur when1) the result of the evaluation is readily available to thehearer, thus eliminating the need for an initiative shift,or 2) the hearer provides extra information.
We believethat although it is difficult to predict when an agentmay include extra information in response to a question,taking into account he cognitive load that a questionplaces on the hearer may allow us to more accuratelypredict dialogue initiative shifts.4 Applications in Other EnvironmentsTO investigate the generality of our system, we appliedour training algorithm, using the constant-increment-with-counter adjustment method with A = 0.35, onthe TRAINS91 corpus to obtain a set of bpa's.
Wethen evaluated the system on subsets of dialogues fromfour other corpora: the TRAINS93 dialogues (Heemanand Allen, 1995), airline reservation dialogues (SRITranscripts, 1992), instruction-giving dialogues (MapTask Dialogues, 1996), and non-task-oriented dialogues(Switchboard Credit Card Corpus, 1992).
In addition, weapplied our baseline strategy which makes predictionswithout he use of cues to each corpus.Table 4 shows a comparison between the dialoguesfrom the five corpora and the results of this evaluation.Row I in the table shows the number of turns where theexpert 9 holds the task/dialogue initiative, with percent-ages shown in parentheses.
This analysis hows that medistribution of initiatives varies quite significantly acrosscorpora, with the distribution biased toward one agent inthe TRAINS and maptask corpora, and split fairly evenlyin the airline and switchboard ialogues.
Row 2 showsthe results of applying our baseline prediction methodto the various corpora.
The numbers hown are correctpredictions in each instance, with the correspondingpercentages shown in parentheses.
These results indicatethe difficulty of the prediction problem in each corpusthat the task/dialogue initiative distribution (row 1)falls to convey.
For instance, although the dialogueinitiative is distributed approximately 30/70% betweenthe two agents in the TRAINS91 corpus and 40160%in the airline dialogues, the prediction rates in row 2shows that in both cases, the distribution is the result ofshifts in dialogue initiative in approximately 25% of thedialogue turns.
Row 3 in the table shows the predictionresults when applying our training algorithm usingthe constant-increment-with-counter method.
Finally,the last row shows the improvement in percentagepoints between our prediction method and the baseline9The expertis assigned as follows: in the TRAINS domain,the system; in the airline domain, the travel agent; in the map-task domain, the instruction giver; and in the switchboard ia-logues, the agent who holds the dialogue initiative the majorityof the time.268Corpus TRAINS91 ( 042)(# turns) task dialogueExpert 41 311control (3.9%) (29.8%)No cue 1009 780(96.8%) (74.9%)const-inc- 1033 915w-count (99.1%) (87.8%)Improvement 2.3% 12.9%TRAINS93 (256) Airline (332) Maptask (320)task dialogue task dialogue task dialogue37 101 194 193 320 277(14.4%) (39.5%) (58.4%) (58.1%) (100%) (86.6%)239 189 308 247 320 270(93.3%) (73.8%) (92.8%) (74.4%) (100%) (84.4%)250 217 316 281 320 297(97.7%) (84.8%) (95.2%) (84.6%) (100%) (92.8%)4.4% 11.0% 2.4% 10.2% 0.0% 8.4%Table 4: Comparison Across Different Application EnvironmentsSwitchboard (282)task dialogueN/A 166(59.9%)N/A 193(68.4%)N/A 216(76.6%)N/A 8.2%prediction method.
To test the statistical significanceof the differences between the results obtained by thetwo prediction algorithms, for each corpus, we appliedCochran' s Q test (Cochran, 1950) to the results in rows 2and 3.
The tests how that for all corpora, the differencesbetween the two algorithms when predicting the task anddialogue initiative holders are statistically significant atthe levels of p<0.05 and p< 10 -5, respectively.Based on the results of our evaluation, we make thefollowing observations.
First, Table 4 illustrates the gen-erality of our prediction mechanism.
Although the sys-tem's performance varies across environments, the useof cues consistently improves the system's accuracies inpredicting the task and dialogue initiative holders by 2-4 percentage points (with the exception of the maptaskcorpus in which there is no room for improvement) TMand 8-13 percentage points, respectively.
Second, Ta-ble 4 shows the specificity of the trained bpa's with re-spect to application environments.
Using our predic-tion mechanism, the system's performances on the col-laborative planning dialogues (TRAINS91, TRAINS93,and airline reservation) most closely resemble one an-other (last row in table).
This suggests that the bpa'smay be somewhat sensitive to application environmentssince they may affect how agents interpret cues.
Third,our prediction mechanism yields better esults on task-oriented ialogues.
This is because such dialogues areconstrained by the goals; therefore, there are fewer di-gressions and offers of unsolicited opinion as comparedto the switchboard corpus.5 ConclusionsThis paper discussed a model for tracking initiative be-tween participants in mixed-initiative dialogue interac-tions.
We showed that distinguishing between task anddialogue initiatives allows us to model phenomena in col-laborative dialogues that existing systems are unable toexplain.
We presented eight types of cues that affect ini-tiative shifts in dialogues, and showed how our model1?In the maptask domain, the task initiative remains with oneagent, the instruction giver, throughout the dialogue.predicts initiative shifts based on the current initiativeholders and and the effects that observed cues have onchanging them.
Our experiments show that by utilizingthe constant-increment-with-counter adjustment methodin determining the basic probability assignments for eachcue, the system can correctly predict he task and dia-logue initiative holders 99.1% and 87.8% of the time, re-spectively, in the TRAINS91 corpus, compared to 96.8%and 74.9% without he use of cues.
The differences be-tween these results are shown to be statistically signif-icant using Cochran's Q test.
In addition, we demon-strated the generality of our model by applying it to dia-logues in different application environments.
The resultsindicate that although the basic probability assignmentsmay be sensitive to application environments, the use ofcues in the prediction process ignificantly improves thesystem' s performance.AcknowledgmentsWe would like to thank Lyn Walker, Diane Litman, BobCarpenter, and Christer Samuelsson for their commentson earlier drafts of this paper, Bob Carpenter and Christer"Samuelsson for participating in the coding reliability test,as well as Jan van Santen and Lyn Walker for discussionson statistical testing methods.ReferencesAllen, James.
1991.
Discourse structure in the TRAINSproject.
In Darpa Speech and Natural LanguageWorkshop.Carletta, Jean.
1996.
Assessing agreement on classifi-cation tasks: The kappa statistic.
ComputationaILin-guistics, 22:249-254.Chu-Carroll, Jennifer and Michael K. Brown.
1997.
Ini-tiative in collaborative interactions - - its cues and ef-fects.
In Working Notes of the AAAI-97 Spring Sym-posium on Computational Models for Mixed InitiativeInteraction, pages 16-22.Chu-Carroll, Jennifer and Sandra Carberry.
1994.
Aplan-based model for response generation in collab-269orative task-oriented dialogues.
In Proceedings of theTwelfth National Conference on Artificial Intelligence,pages 799-805.Chu-Carroll, Jennifer and Sandra Carberry.
1995.
Re-sponse generation i collaborative n gotiation.
InPro-ceedings of the 33rd Annual Meeting of the Associa-tion for Computational Linguistics, pages 136-143.Cochran, W. G. 1950.
The comparison ofpercentages inmatched samples.
Biometrika, 37:256-266.Gordon, Jean and Edward H. Shortliffe.
1984.
TheDempster-Shafer theory of evidence.
In BruceBuchanan and Edward Shortliffe, editors, Rule-BasedExpert Systems: The MYCIN Experiments of theStanford Heuristic Programming Project.
Addison-Wesley, chapter 13, pages 272-292.Gross, Derek, James F. Allen, and David R. Tranm.1993.
The TRAINS 91 dialogues.
Technical ReportTN92-1, Department ofComputer Science, Universityof Rochester.Grove, William M., Nancy C. Andreasen, PatriciaMcDonald-Scott, Martin B. Keller, and Robert W.Shapiro.
1981.
Reliability studies of psychiatric di-agnosis.
Archives of General Psychiatry., 38:408-413,Guinn, Curry I.
1996.
Mechanisms for mixed-initiative)',m~nJ'c, mputer col!~_b,~_raOve di_scourse.
InProceed-i;;g~ of tiu."
34th Anl;ual Mccti,.
d of the ,ts~,,ciati~,.,forComputational Linguistics, pages 278-285.Heeman, Peter A. and James F. Allen.
1995.
TheTRAINS 93 dialogues.
Technical Report TN94-2, Department of Computer Science, University ofRochester.Jordan, Pamela W. and Barbara Di Eugenio.
1997.
Con-trol and initiative in collaborative problem solving dia-logues.
In Working Notes of the AAA1-97 Spring Sym-posium on Computational Models for Mixed InitiativeInteraction, pages 81-84.Kitano, Hiroaki and Carol Van Ess-Dykema.
1991.
To-ward a plan-based understanding model for mixed-initiative dialogues.
In Proceedings of the 29th An-nual Meeting of the Association for ComputationalLinguistics, pages 25-32.Lambert, Lynn and Sandra Carberry.
1991.
A tripartiteplan-based model of dialogue.
In Proceedings of the29th Annual Meeting of the Association for Computa-tional Linguistics, pages 47-54.Litman, Diane and James Allen.
1987.
A plan recogni-tion model for subdialogues in conversation.
Cogni-tive Science, 11:163-200.Map Task Dialogues.
1996.
Transcripts of DCIEMSleep Deprivation Study, conducted by Defense andCivil Institute of Environmental Medicine, Canada,and Human Communication Research Centre, Uni-versity of Edinburgh and University of Glasgow, UK.Distrubuted by HCRC and LDC.Novick, David G. 1988.
Control of Mixed-lnitiative Dis-course Through Meta-Locutionary Acts: A Computa-tional Model.
Ph.D. thesis, University of Oregon.Novick, David G. and Stephen Sutton.
1997.
What ismixed-initiative interaction?
In Working Notes of theAAAI-97 Spring Symposium on Computational Mod-els for Mixed Initiative Interaction, pages 114-116.Pearl, Judea.
1990, Bayesian and belief-fuctions for-malisms for evidential reasoning: A conceptual naly-sis.
In Glenn Shafer and Judea Pearl, editors, Read-ings in Uncertain Reasoning.
Morgan Kaufmann,pages 540-574.Rmnshaw, Lance A.
1991.
A three-level model for planexploration.
In Proceedings of the 29th Annual Meet-ing of the Association for Computational Linguistics,pages 36--46.Shafer, Glenn.
1976.
A Mathematical Theory of Evi-dence.
Princeton University Press.Siegel, Sidney.
and N. John.
Castellan, Jr. 1988.
Non-parametric Statistics for the Behavioral Sciences.
Mc-Graw Hill.Smith, Ronnie W. and D. Richard Hipp.
1994.
SpokenNatural Language Dialog Systems --  A Practical Ap-proach.
Oxford University Press.SRI Transcripts.
1992.
Transcripts derived from audio-tape conversations made at SRI International, MenloPark, CA.
Prepared by Jacqueline Kowtko under thedirection of Patti Price.Switchboard Credit Card Corpus.
1992.
Transcripts oftelephone conversations on the topic of credit card use,collected at Texas Instruments.
Produced by NIST,available through LDC.Walker, Marilyn and Steve Whittaker.
1990.
Mixedinitiative in dialogue: An investigation i to discoursesegmentation.
In Proceedings of the 28th AnnualMeeting of the Association for Computational Lin-guistics, pages 70-78.Walker, Marilyn A.
1992.
Redundancy in collabora-tive dialogue.
In Proceedings of the 15th InternationalConference on Computational Linguistics, pages 345-351.Whittaker, Steve and Phil Stenton.
1988.
Cues and con-trol in expert-client dialogues.
In Proceedings of the26th Annual Meeting of the Association for Computa-tional Linguistics, pages 123-130.270
