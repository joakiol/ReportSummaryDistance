Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 620?631,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsIncremental Syntactic Language Models for Phrase-based TranslationLane SchwartzAir Force Research LaboratoryWright-Patterson AFB, OH USAlane.schwartz@wpafb.af.milChris Callison-BurchJohns Hopkins UniversityBaltimore, MD USAccb@cs.jhu.eduWilliam SchulerOhio State UniversityColumbus, OH USAschuler@ling.ohio-state.eduStephen WuMayo ClinicRochester, MN USAwu.stephen@mayo.eduAbstractThis paper describes a novel technique for in-corporating syntactic knowledge into phrase-based machine translation through incremen-tal syntactic parsing.
Bottom-up and top-down parsers typically require a completedstring as input.
This requirement makes it dif-ficult to incorporate them into phrase-basedtranslation, which generates partial hypothe-sized translations from left-to-right.
Incre-mental syntactic language models score sen-tences in a similar left-to-right fashion, and aretherefore a good mechanism for incorporat-ing syntax into phrase-based translation.
Wegive a formal definition of one such linear-time syntactic language model, detail its re-lation to phrase-based decoding, and integratethe model with the Moses phrase-based trans-lation system.
We present empirical resultson a constrained Urdu-English translation taskthat demonstrate a significant BLEU score im-provement and a large decrease in perplexity.1 IntroductionEarly work in statistical machine translation viewedtranslation as a noisy channel process comprised ofa translation model, which functioned to posit ad-equate translations of source language words, anda target language model, which guided the fluencyof generated target language strings (Brown et al,This research was supported by NSF CAREER/PECASEaward 0447685, NSF grant IIS-0713448, and the EuropeanCommission through the EuroMatrixPlus project.
Opinions, in-terpretations, conclusions, and recommendations are those ofthe authors and are not necessarily endorsed by the sponsors orthe United States Air Force.
Cleared for public release (CaseNumber 88ABW-2010-6489) on 10 Dec 2010.1990).
Drawing on earlier successes in speechrecognition, research in statistical machine trans-lation has effectively used n-gram word sequencemodels as language models.Modern phrase-based translation using large scalen-gram language models generally performs wellin terms of lexical choice, but still often producesungrammatical output.
Syntactic parsing may helpproduce more grammatical output by better model-ing structural relationships and long-distance depen-dencies.
Bottom-up and top-down parsers typicallyrequire a completed string as input; this requirementmakes it difficult to incorporate these parsers intophrase-based translation, which generates hypothe-sized translations incrementally, from left-to-right.1As a workaround, parsers can rerank the translatedoutput of translation systems (Och et al, 2004).On the other hand, incremental parsers (Roark,2001; Henderson, 2004; Schuler et al, 2010; Huangand Sagae, 2010) process input in a straightforwardleft-to-right manner.
We observe that incrementalparsers, used as structured language models, pro-vide an appropriate algorithmic match to incremen-tal phrase-based decoding.
We directly integrate in-cremental syntactic parsing into phrase-based trans-lation.
This approach re-exerts the role of the lan-guage model as a mechanism for encouraging syn-tactically fluent translations.The contributions of this work are as follows:?
A novel method for integrating syntactic LMsinto phrase-based translation (?3)?
A formal definition of an incremental parser for1While not all languages are written left-to-right, we willrefer to incremental processing which proceeds from the begin-ning of a sentence as left-to-right.620statistical MT that can run in linear-time (?4)?
Integration with Moses (?5) along with empiri-cal results for perplexity and significant transla-tion score improvement on a constrained Urdu-English task (?6)2 Related WorkNeither phrase-based (Koehn et al, 2003) nor hierar-chical phrase-based translation (Chiang, 2005) takeexplicit advantage of the syntactic structure of eithersource or target language.
The translation models inthese techniques define phrases as contiguous wordsequences (with gaps allowed in the case of hierar-chical phrases) which may or may not correspondto any linguistic constituent.
Early work in statisti-cal phrase-based translation considered whether re-stricting translation models to use only syntacticallywell-formed constituents might improve translationquality (Koehn et al, 2003) but found such restric-tions failed to improve translation quality.Significant research has examined the extent towhich syntax can be usefully incorporated into sta-tistical tree-based translation models: string-to-tree(Yamada and Knight, 2001; Gildea, 2003; Imamuraet al, 2004; Galley et al, 2004; Graehl and Knight,2004; Melamed, 2004; Galley et al, 2006; Huanget al, 2006; Shen et al, 2008), tree-to-string (Liuet al, 2006; Liu et al, 2007; Mi et al, 2008; Miand Huang, 2008; Huang and Mi, 2010), tree-to-tree(Abeille?
et al, 1990; Shieber and Schabes, 1990;Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowanet al, 2006; Nesson et al, 2006; Zhang et al, 2007;DeNeefe et al, 2007; DeNeefe and Knight, 2009;Liu et al, 2009; Chiang, 2010), and treelet (Dingand Palmer, 2005; Quirk et al, 2005) techniquesuse syntactic information to inform the translationmodel.
Recent work has shown that parsing-basedmachine translation using syntax-augmented (Zoll-mann and Venugopal, 2006) hierarchical translationgrammars with rich nonterminal sets can demon-strate substantial gains over hierarchical grammarsfor certain language pairs (Baker et al, 2009).
Incontrast to the above tree-based translation models,our approach maintains a standard (non-syntactic)phrase-based translation model.
Instead, we incor-porate syntax into the language model.Traditional approaches to language models inspeech recognition and statistical machine transla-tion focus on the use of n-grams, which provide asimple finite-state model approximation of the tar-get language.
Chelba and Jelinek (1998) proposedthat syntactic structure could be used as an alterna-tive technique in language modeling.
This insighthas been explored in the context of speech recogni-tion (Chelba and Jelinek, 2000; Collins et al, 2005).Hassan et al (2007) and Birch et al (2007) usesupertag n-gram LMs.
Syntactic language modelshave also been explored with tree-based translationmodels.
Charniak et al (2003) use syntactic lan-guage models to rescore the output of a tree-basedtranslation system.
Post and Gildea (2008) investi-gate the integration of parsers as syntactic languagemodels during binary bracketing transduction trans-lation (Wu, 1997); under these conditions, both syn-tactic phrase-structure and dependency parsing lan-guage models were found to improve oracle-besttranslations, but did not improve actual translationresults.
Post and Gildea (2009) use tree substitutiongrammar parsing for language modeling, but do notuse this language model in a translation system.
Ourwork, in contrast to the above approaches, exploresthe use of incremental syntactic language models inconjunction with phrase-based translation models.Our syntactic language model fits into the fam-ily of linear-time dynamic programming parsers de-scribed in (Huang and Sagae, 2010).
Like (Galleyand Manning, 2009) our work implements an in-cremental syntactic language model; our approachdiffers by calculating syntactic LM scores over allavailable phrase-structure parses at each hypothesisinstead of the 1-best dependency parse.The syntax-driven reordering model of Ge (2010)uses syntax-driven features to influence word orderwithin standard phrase-based translation.
The syn-tactic cohesion features of Cherry (2008) encour-ages the use of syntactically well-formed translationphrases.
These approaches are fully orthogonal toour proposed incremental syntactic language model,and could be applied in concert with our work.3 Parser as Syntactic Language Model inPhrase-Based TranslationParsing is the task of selecting the representation ??
(typically a tree) that best models the structure of621????????s???0????????s?
the??11????????s?
that??12????????s?
president??13.
.
.??????
?the president??21??????
?that president??22??????
?president Friday??23.
.
.??????
?president meets??31??????
?Obama met??32.
.
.Figure 1: Partial decoding lattice for standard phrase-based decoding stack algorithm translating the Germansentence Der Pra?sident trifft am Freitag den Vorstand.
Each node h in decoding stack t represents theapplication of a translation option, and includes the source sentence coverage vector, target language n-gram state, and syntactic language model state ?
?th .
Hypothesis combination is also shown, indicatingwhere lattice paths with identical n-gram histories converge.
We use the English translation The presidentmeets the board on Friday as a running example throughout all Figures.sentence e, out of all such possible representations?
.
This set of representations may be all phrasestructure trees or all dependency trees allowed bythe parsing model.
Typically, tree ??
is taken to be:??
= argmax?P(?
| e) (1)We define a syntactic language model P(e) basedon the total probability mass over all possible treesfor string e. This is shown in Equation 2 and decom-posed in Equation 3.P(e) =???
?P(?, e) (2)P(e) =???
?P(e | ?)P(?)
(3)3.1 Incremental syntactic language modelAn incremental parser processes each token of in-put sequentially from the beginning of a sentence tothe end, rather than processing input in a top-down(Earley, 1968) or bottom-up (Cocke and Schwartz,1970; Kasami, 1965; Younger, 1967) fashion.
Afterprocessing the tth token in string e, an incremen-tal parser has some internal representation of possi-ble hypothesized (incomplete) trees, ?t.
The syntac-tic language model probability of a partial sentencee1...et is defined:P(e1...et) =???
?tP(e1...et | ?)P(?)
(4)In practice, a parser may constrain the set of treesunder consideration to ?
?t, that subset of analyses orpartial analyses that remains after any pruning is per-formed.
An incremental syntactic language modelcan then be defined by a probability mass function(Equation 5) and a transition function ?
(Equation6).
The role of ?
is explained in ?3.3 below.
Anyparser which implements these two functions canserve as a syntactic language model.P(e1...et) ?
P(??
t) =?????
tP(e1...et | ?)P(?)
(5)?
(et, ??
t?1)?
??
t (6)6223.2 Decoding in phrase-based translationGiven a source language input sentence f , a trainedsource-to-target translation model, and a target lan-guage model, the task of translation is to find themaximally probable translation e?
using a linearcombination of j feature functions h weighted ac-cording to tuned parameters ?
(Och and Ney, 2002).e?
= argmaxeexp(?j?jhj(e,f)) (7)Phrase-based translation constructs a set of trans-lation options ?
hypothesized translations for con-tiguous portions of the source sentence ?
from atrained phrase table, then incrementally constructs alattice of partial target translations (Koehn, 2010).To prune the search space, lattice nodes are orga-nized into beam stacks (Jelinek, 1969) according tothe number of source words translated.
An n-gramlanguage model history is also maintained at eachnode in the translation lattice.
The search spaceis further trimmed with hypothesis recombination,which collapses lattice nodes that share a commoncoverage vector and n-gram state.3.3 Incorporating a Syntactic Language ModelPhrase-based translation produces target languagewords in an incremental left-to-right fashion, gen-erating words at the beginning of a translation firstand words at the end of a translation last.
Similarly,incremental parsers process sentences in an incre-mental fashion, analyzing words at the beginning ofa sentence first and words at the end of a sentencelast.
As such, an incremental parser with transitionfunction ?
can be incorporated into the phrase-baseddecoding process in a straightforward manner.
Eachnode in the translation lattice is augmented with asyntactic language model state ?
?t.The hypothesis at the root of the translation latticeis initialized with ??
0, representing the internal stateof the incremental parser before any input words areprocessed.
The phrase-based translation decodingprocess adds nodes to the lattice; each new nodecontains one or more target language words.
Eachnode contains a backpointer to its parent node, inwhich ??
t?1 is stored.
Given a new target languageword et and ??
t?1, the incremental parser?s transi-tion function ?
calculates ??
t. Figure 1 illustratesSNPDTTheNNpresidentVPVPVBmeetsNPDTtheNNboardPPINonNPFridayFigure 2: Sample binarized phrase structure tree.SS/NPS/PPS/VPNPNP/NNDTTheNNpresidentVPVP/NNVP/NPVBmeetsDTtheNNboardINonNPFridayFigure 3: Sample binarized phrase structure tree af-ter application of right-corner transform.a sample phrase-based decoding lattice where eachtranslation lattice node is augmented with syntacticlanguage model state ?
?t.In phrase-based translation, many translation lat-tice nodes represent multi-word target languagephrases.
For such translation lattice nodes, ?
willbe called once for each newly hypothesized targetlanguage word in the node.
Only the final syntac-tic language model state in such sequences need bestored in the translation lattice node.4 Incremental Bounded-Memory Parsingwith a Time Series ModelHaving defined the framework by which any in-cremental parser may be incorporated into phrase-based translation, we now formally define a specificincremental parser for use in our experiments.The parser must process target language wordsincrementally as the phrase-based decoder adds hy-potheses to the translation lattice.
To facilitate thisincremental processing, ordinary phrase-structuretrees can be transformed into right-corner recur-623r1t?1r2t?1r3t?1s1t?1s2t?1s3t?1r1tr2tr3ts1ts2ts3tet?1 et.
.
.. .
.. .
.. .
.Figure 4: Graphical representation of the depen-dency structure in a standard Hierarchic HiddenMarkov Model with D = 3 hidden levels that canbe used to parse syntax.
Circles denote random vari-ables, and edges denote conditional dependencies.Shaded circles denote variables with observed val-ues.sive phrase structure trees using the tree transformsin Schuler et al (2010).
Constituent nontermi-nals in right-corner transformed trees take the formof incomplete constituents c?/c??
consisting of an?active?
constituent c?
lacking an ?awaited?
con-stituent c??
yet to come, similar to non-constituentcategories in a Combinatory Categorial Grammar(Ades and Steedman, 1982; Steedman, 2000).
Asan example, the parser might consider VP/NN as apossible category for input ?meets the?.A sample phrase structure tree is shown beforeand after the right-corner transform in Figures 2and 3.
Our parser operates over a right-corner trans-formed probabilistic context-free grammar (PCFG).Parsing runs in linear time on the length of the input.This model of incremental parsing is implementedas a Hierarchical Hidden Markov Model (HHMM)(Murphy and Paskin, 2001), and is equivalent to aprobabilistic pushdown automaton with a boundedpushdown store.
The parser runs in O(n) time,where n is the number of words in the input.
Thismodel is shown graphically in Figure 4 and formallydefined in ?4.1 below.The incremental parser assigns a probability(Eq.
5) for a partial target language hypothesis, usinga bounded store of incomplete constituents c?/c?
?.The phrase-based decoder uses this probability valueas the syntactic language model feature score.4.1 Formal Parsing Model: Scoring PartialTranslation HypothesesThis model is essentially an extension of an HHMM,which obtains a most likely sequence of hidden storestates, s?1..D1..T , of some length T and some maxi-mum depth D, given a sequence of observed tokens(e.g.
generated target language words), e1..T , usingHHMM state transition model ?A and observationsymbol model ?B (Rabiner, 1990):s?1..D1..Tdef= argmaxs1..D1..TT?t=1P?A(s1..Dt | s1..Dt?1 )?P?B(et | s1..Dt )(8)The HHMM parser is equivalent to a probabilis-tic pushdown automaton with a bounded push-down store.
The model generates each successivestore (using store model ?S) only after consideringwhether each nested sequence of incomplete con-stituents has completed and reduced (using reduc-tion model ?R):P?A(s1..Dt | s1..Dt?1 )def=?r1t ..rDtD?d=1P?R(rdt | rd+1t sdt?1sd?1t?1 )?
P?S(sdt | rd+1t rdt sdt?1sd?1t ) (9)Store elements are defined to contain only theactive (c?)
and awaited (c??)
constituent categoriesnecessary to compute an incomplete constituentprobability:sdtdef= ?c?, c???
(10)Reduction states are defined to contain only thecomplete constituent category crdt necessary to com-pute an inside likelihood probability, as well as aflag frdt indicating whether a reduction has takenplace (to end a sequence of incomplete constituents):rdtdef= ?crdt , frdt ?
(11)The model probabilities for these store elementsand reduction states can then be defined (from Mur-phy and Paskin 2001) to expand a new incompleteconstituent after a reduction has taken place (frdt =1; using depth-specific store state expansion model?S-E,d), transition along a sequence of store elements624s11s21s31e1t=1r12r22r32s12s22s32e2t=2r13r23r33s13s23s33e3t=3r14r24r34s14s24s34e4t=4r15r25r35s15s25s35e5t=5r16r26r36s16s26s36e6t=6r17r27r37s17s27s37e7t=7r18r28r38=DT=NP/NN=NP=NN=S/VP=VB=S/VP=VP/NP=DT=VP/NN=S/VP=NN=VP=S/PP=IN=S/NP=S=NP=The =president =meets =the =board =on =FridayFigure 5: Graphical representation of the Hierarchic Hidden Markov Model after parsing input sentence Thepresident meets the board on Friday.
The shaded path through the parse lattice illustrates the recognizedright-corner tree structure of Figure 3.if no reduction has taken place (frdt =0; using depth-specific store state transition model ?S-T,d): 2P?S(sdt | rd+1t rdt sdt?1sd?1t )def=??
?if frd+1t =1, frdt =1 : P?S-E,d(sdt | sd?1t )if frd+1t =1, frdt =0 : P?S-T,d(sdt | rd+1t rdt sdt?1sd?1t )if frd+1t =0, frdt =0 : Jsdt = sdt?1K(12)and possibly reduce a store element (terminatea sequence) if the store state below it has re-duced (frd+1t = 1; using depth-specific reductionmodel ?R,d):P?R(rdt | rd+1t sdt?1sd?1t?1 )def={if frd+1t =0 : Jrdt = r?Kif frd+1t =1 : P?R,d(rdt | rd+1t sdt?1 sd?1t?1 )(13)where r?
is a null state resulting from the failure ofan incomplete constituent to complete, and constantsare defined for the edge conditions of s0t and rD+1t .Figure 5 illustrates this model in action.These pushdown automaton operations are thenrefined for right-corner parsing (Schuler, 2009),distinguishing active transitions (model ?S-T-A,d, inwhich an incomplete constituent is completed, butnot reduced, and then immediately expanded to a2An indicator function J?K is used to denote deterministicprobabilities: J?K = 1 if ?
is true, 0 otherwise.new incomplete constituent in the same store el-ement) from awaited transitions (model ?S-T-W,d,which involve no completion):P?S-T,d(sdt | rd+1t rdt sdt?1sd?1t )def={if rdt 6=r?
: P?S-T-A,d(sdt | sd?1t rdt )if rdt =r?
: P?S-T-W,d(sdt | sdt?1rd+1t )(14)P?R,d(rdt | rd+1t sdt?1sd?1t?1 )def={if crd+1t 6=xt : Jrdt = r?Kif crd+1t =xt : P?R-R,d(rdt | sdt?1sd?1t?1 )(15)These HHMM right-corner parsing operations arethen defined in terms of branch- and depth-specificPCFG probabilities ?G-R,d and ?G-L,d: 33Model probabilities are also defined in terms of left-progeny probability distribution E?G-RL?,d which is itself definedin terms of PCFG probabilities:E?G-RL?,d(c?0?
c?0 ...)def=?c?1P?G-R,d(c?
?
c?0 c?1) (16)E?G-RL?,d(c?k?
c?0k0 ...)def=?c?0kE?G-RL?,d(c?k?1?
c?0k ...)?
?c?0k1P?G-L,d(c?0k ?
c?0k0 c?0k1) (17)E?G-RL?,d(c???
c??
...)def=??k=0E?G-RL?,d(c?k?
c??
...) (18)E?G-RL?,d(c?+?
c??
...)def= E?G-RL?,d(c???
c??
...)?
E?G-RL?,d(c?0?
c??
...) (19)625??????
?president meets??31.
.
.??????
?the board??51.
.
.s13s23s33e3r14r24r34s14s24s34e4r15r25r35s15s25s35e5=meets =the =boardFigure 6: A hypothesis in the phrase-based decoding lattice from Figure 1 is expanded using translation op-tion the board of source phrase den Vorstand.
Syntactic language model state ?
?31 contains random variabless1..33 ; likewise ?
?51 contains s1..35 .
The intervening random variables r1..34 , s1..34 , and r1..35 are calculated bytransition function ?
(Eq.
6, as defined by ?4.1), but are not stored.
Observed random variables (e3..e5) areshown for clarity, but are not explicitly stored in any syntactic language model state.?
for expansions:P?S-E,d(?c?
?, c????
| ?
?, c??)def=E?G-RL?,d(c???
c??
...) ?
Jx??
= c???
= c?
?K (20)?
for awaited transitions:P?S-T-W,d(?c?, c??1?
| ?c?
?, c???
c??0)def=Jc?
= c?
?K ?P?G-R,d(c??
?
c?
?0 c??1)E?G-RL?,d(c??0?
c?
?0 ...)(21)?
for active transitions:P?S-T-A,d(?c?
?, c??1?
| ?
?, c??
c??0)def=E?G-RL?,d(c???
c??
...) ?
P?G-L,d(c??
?
c?
?0 c??1)E?G-RL?,d(c?+?
c?
?0 ...)(22)?
for cross-element reductions:P?R-R,d(c?
?,1 | ?
?, c??
?c???,??)def=Jc??
= c??
?K ?E?G-RL?,d(c?0?
c??
...)E?G-RL?,d(c???
c??
...)(23)?
for in-element reductions:P?R-R,d(c?
?,0 | ?
?, c??
?c???,??)def=Jc??
= c??
?K ?E?G-RL?,d(c?+?
c??
...)E?G-RL?,d(c???
c??
...)(24)We use the parser implementation of (Schuler,2009; Schuler et al, 2010).5 Phrase Based Translation with anIncremental Syntactic Language ModelThe phrase-based decoder is augmented by addingadditional state data to each hypothesis in the de-626coder?s hypothesis stacks.
Figure 1 illustrates an ex-cerpt from a standard phrase-based translation lat-tice.
Within each decoder stack t, each hypothe-sis h is augmented with a syntactic language modelstate ?
?th .
Each syntactic language model state isa random variable store, containing a slice of ran-dom variables from the HHMM.
Specifically, ?
?thcontains those random variables s1..Dt that maintaindistributions over syntactic elements.By maintaining these syntactic random variablestores, each hypothesis has access to the currentlanguage model probability for the partial transla-tion ending at that hypothesis, as calculated by anincremental syntactic language model defined bythe HHMM.
Specifically, the random variable storeat hypothesis h provides P(?
?th) = P(eh1..t, s1..D1..t ),where eh1..t is the sequence of words in a partial hy-pothesis ending at h which contains t target words,and where there are D syntactic random variables ineach random variable store (Eq.
5).During stack decoding, the phrase-based decoderprogressively constructs new hypotheses by extend-ing existing hypotheses.
New hypotheses are placedin appropriate hypothesis stacks.
In the simplestcase, a new hypothesis extends an existing hypothe-sis by exactly one target word.
As the new hypothe-sis is constructed by extending an existing stack ele-ment, the store and reduction state random variablesare processed, along with the newly hypothesizedword.
This results in a new store of syntactic ran-dom variables (Eq.
6) that are associated with thenew stack element.When a new hypothesis extends an existing hy-pothesis by more than one word, this process is firstcarried out for the first new word in the hypothe-sis.
It is then repeated for the remaining words inthe hypothesis extension.
Once the final word inthe hypothesis has been processed, the resulting ran-dom variable store is associated with that hypoth-esis.
The random variable stores created for thenon-final words in the extending hypothesis are dis-carded, and need not be explicitly retained.Figure 6 illustrates this process, showing how asyntactic language model state ?
?51 in a phrase-baseddecoding lattice is obtained from a previous syn-tactic language model state ?
?31 (from Figure 1) byparsing the target language words from a phrase-based translation option.In-domain Out-of-domainLM WSJ 23 ppl ur-en dev pplWSJ 1-gram 1973.57 3581.72WSJ 2-gram 349.18 1312.61WSJ 3-gram 262.04 1264.47WSJ 4-gram 244.12 1261.37WSJ 5-gram 232.08 1261.90WSJ HHMM 384.66 529.41Interpolated WSJ5-gram + HHMM 209.13 225.48Giga 5-gram 258.35 312.28Interp.
Giga 5-gr+ WSJ HHMM 222.39 123.10Interp.
Giga 5-gr+ WSJ 5-gram 174.88 321.05Figure 7: Average per-word perplexity values.HHMM was run with beam size of 2000.
Bold in-dicates best single-model results for LMs trained onWSJ sections 2-21.
Best overall in italics.Our syntactic language model is integrated intothe current version of Moses (Koehn et al, 2007).6 ResultsAs an initial measure to compare language models,average per-word perplexity, ppl, reports how sur-prised a model is by test data.
Equation 25 calculatesppl using log base b for a test set of T tokens.ppl = b?logbP(e1...eT )T (25)We trained the syntactic language model from?4 (HHMM) and an interpolated n-gram languagemodel with modified Kneser-Ney smoothing (Chenand Goodman, 1998); models were trained on sec-tions 2-21 of the Wall Street Journal (WSJ) tree-bank (Marcus et al, 1993).
The HHMM outper-forms the n-gram model in terms of out-of-domaintest set perplexity when trained on the same WSJdata; the best perplexity results for in-domain andout-of-domain test sets4 are found by interpolating4In-domain is WSJ Section 23.
Out-of-domain are the En-glish reference translations of the dev section , set aside in(Baker et al, 2009) for parameter tuning, of the NIST OpenMT 2008 Urdu-English task.627Sentence Moses +HHMM +HHMMlength beam=50 beam=200010 0.21 533 114320 0.53 1193 256230 0.85 1746 374940 1.13 2095 4588Figure 8: Mean per-sentence decoding time (in sec-onds) for dev set using Moses with and without syn-tactic language model.
HHMM parser beam sizesare indicated for the syntactic LM.HHMM and n-gram LMs (Figure 7).
To show theeffects of training an LM on more data, we also re-port perplexity results on the 5-gram LM trained forthe GALE Arabic-English task using the English Gi-gaword corpus.
In all cases, including the HHMMsignificantly reduces perplexity.We trained a phrase-based translation model onthe full NIST Open MT08 Urdu-English translationmodel using the full training data.
We trained theHHMM and n-gram LMs on the WSJ data in orderto make them as similar as possible.
During tuning,Moses was first configured to use just the n-gramLM, then configured to use both the n-gram LM andthe syntactic HHMM LM.
MERT consistently as-signed positive weight to the syntactic LM feature,typically slightly less than the n-gram LM weight.In our integration with Moses, incorporating asyntactic language model dramatically slows the de-coding process.
Figure 8 illustrates a slowdownaround three orders of magnitude.
Although speedremains roughly linear to the size of the source sen-tence (ruling out exponential behavior), it is with anextremely large constant time factor.
Due to thisslowdown, we tuned the parameters using a con-strained dev set (only sentences with 1-20 words),and tested using a constrained devtest set (only sen-tences with 1-20 words).
Figure 9 shows a statis-tically significant improvement to the BLEU scorewhen using the HHMM and the n-gram LMs to-gether on this reduced test set.7 DiscussionThis paper argues that incremental syntactic lan-guages models are a straightforward and appro-Moses LM(s) BLEUn-gram only 18.78HHMM + n-gram 19.78Figure 9: Results for Ur-En devtest (only sentenceswith 1-20 words) with HHMM beam size of 2000and Moses settings of distortion limit 10, stack size200, and ttable limit 20.priate algorithmic fit for incorporating syntax intophrase-based statistical machine translation, sinceboth process sentences in an incremental left-to-right fashion.
This means incremental syntactic LMscores can be calculated during the decoding pro-cess, rather than waiting until a complete sentence isposited, which is typically necessary in top-down orbottom-up parsing.We provided a rigorous formal definition of in-cremental syntactic languages models, and detailedwhat steps are necessary to incorporate such LMsinto phrase-based decoding.
We integrated an incre-mental syntactic language model into Moses.
Thetranslation quality significantly improved on a con-strained task, and the perplexity improvements sug-gest that interpolating between n-gram and syntacticLMs may hold promise on larger data sets.The use of very large n-gram language models istypically a key ingredient in the best-performing ma-chine translation systems (Brants et al, 2007).
Ourn-gram model trained only on WSJ is admittedlysmall.
Our future work seeks to incorporate large-scale n-gram language models in conjunction withincremental syntactic language models.The added decoding time cost of our syntacticlanguage model is very high.
By increasing thebeam size and distortion limit of the baseline sys-tem, future work may examine whether a baselinesystem with comparable runtimes can achieve com-parable translation quality.A more efficient implementation of the HHMMparser would speed decoding and make more exten-sive and conclusive translation experiments possi-ble.
Various additional improvements could includecaching the HHMM LM calculations, and exploitingproperties of the right-corner transform that limit thenumber of decisions between successive time steps.628ReferencesAnne Abeille?, Yves Schabes, and Aravind K. Joshi.1990.
Using lexicalized tree adjoining grammars formachine translation.
In Proceedings of the 13th Inter-national Conference on Computational Linguistics.Anthony E. Ades and Mark Steedman.
1982.
On theorder of words.
Linguistics and Philosophy, 4:517?558.Kathy Baker, Steven Bethard, Michael Bloodgood, RalfBrown, Chris Callison-Burch, Glen Coppersmith,Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,Mike Kayser, Lori Levin, Justin Martineau, Jim May-field, Scott Miller, Aaron Phillips, Andrew Philpot,Christine Piatko, Lane Schwartz, and David Zajic.2009.
Semantically informed machine translation(SIMT).
SCALE summer workshop final report, Hu-man Language Technology Center Of Excellence.Alexandra Birch, Miles Osborne, and Philipp Koehn.2007.
CCG supertags in factored statistical machinetranslation.
In Proceedings of the Second Workshopon Statistical Machine Translation, pages 9?16.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL).Peter Brown, John Cocke, Stephen Della Pietra, Vin-cent Della Pietra, Frederick Jelinek, John Lafferty,Robert Mercer, and Paul Roossin.
1990.
A statisti-cal approach to machine translation.
ComputationalLinguistics, 16(2):79?85.Eugene Charniak, Kevin Knight, and Kenji Yamada.2003.
Syntax-based language models for statisticalmachine translation.
In Proceedings of the Ninth Ma-chine Translation Summit of the International Associ-ation for Machine Translation.Ciprian Chelba and Frederick Jelinek.
1998.
Exploit-ing syntactic structure for language modeling.
In Pro-ceedings of the 36th Annual Meeting of the Associationfor Computational Linguistics and 17th InternationalConference on Computational Linguistics, pages 225?231.Ciprian Chelba and Frederick Jelinek.
2000.
Structuredlanguage modeling.
Computer Speech and Language,14(4):283?332.Stanley F. Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-eling.
Technical report, Harvard University.Colin Cherry.
2008.
Cohesive phrase-based decoding forstatistical machine translation.
In Proceedings of the46th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 72?80.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics, pages 263?270.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 1443?1452.John Cocke and Jacob Schwartz.
1970.
Program-ming languages and their compilers.
Technical report,Courant Institute of Mathematical Sciences, New YorkUniversity.Michael Collins, Brian Roark, and Murat Saraclar.2005.
Discriminative syntactic language modeling forspeech recognition.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics, pages 507?514.Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.2006.
A discriminative model for tree-to-tree trans-lation.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing,pages 232?241.Steve DeNeefe and Kevin Knight.
2009.
Synchronoustree adjoining machine translation.
In Proceedings ofthe 2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 727?736.Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What can syntax-based MT learn fromphrase-based MT?
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 755?763.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependency in-sertion grammars.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics, pages 541?548.Jay Earley.
1968.
An efficient context-free parsing algo-rithm.
Ph.D. thesis, Department of Computer Science,Carnegie Mellon University.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In The Companion Vol-ume to the Proceedings of 41st Annual Meeting ofthe Association for Computational Linguistics, pages205?208.Michel Galley and Christopher D. Manning.
2009.Quadratic-time dependency parsing for machine trans-lation.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP, pages 773?781.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In629Daniel Marcu Susan Dumais and Salim Roukos, edi-tors, Proceedings of the Human Language TechnologyConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of the As-sociation for Computational Linguistics, pages 961?968.Niyu Ge.
2010.
A direct syntax-driven reordering modelfor phrase-based machine translation.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 849?857.Daniel Gildea.
2003.
Loosely tree-based alignment formachine translation.
In Proceedings of the 41st An-nual Meeting of the Association for ComputationalLinguistics, pages 80?87.Jonathan Graehl and Kevin Knight.
2004.
Training treetransducers.
In Proceedings of the Human LanguageTechnology Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 105?112.Hany Hassan, Khalil Sima?an, and Andy Way.
2007.
Su-pertagged phrase-based statistical machine translation.In Proceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 288?295.James Henderson.
2004.
Lookahead in deterministicleft-corner parsing.
In Proceedings of the Workshopon Incremental Parsing: Bringing Engineering andCognition Together, pages 26?33.Liang Huang and Haitao Mi.
2010.
Efficient incrementaldecoding for tree-to-string translation.
In Proceedingsof the 2010 Conference on Empirical Methods in Nat-ural Language Processing, pages 273?283.Liang Huang and Kenji Sagae.
2010.
Dynamic program-ming for linear-time incremental parsing.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 1077?1086.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of the 7th Biennialconference of the Association for Machine Translationin the Americas.Kenji Imamura, Hideo Okuma, Taro Watanabe, and Ei-ichiro Sumita.
2004.
Example-based machine transla-tion based on syntactic transfer with statistical models.In Proceedings of the 20th International Conferenceon Computational Linguistics, pages 99?105.Frederick Jelinek.
1969.
Fast sequential decoding al-gorithm using a stack.
IBM Journal of Research andDevelopment, pages 675?685.T.
Kasami.
1965.
An efficient recognition and syntaxanalysis algorithm for context free languages.
Techni-cal Report AFCRL-65-758, Air Force Cambridge Re-search Laboratory.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Human Language Technology Confer-ence of the North American Chapter of the Associationfor Computational Linguistics, pages 127?133.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics, pages 177?180.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Linguis-tics, pages 609?616.Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007.Forest-to-string statistical translation rules.
In Pro-ceedings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 704?711.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
Improvingtree-to-tree translation with packed forests.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP, pages 558?566.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.I.
Dan Melamed.
2004.
Statistical machine translationby parsing.
In Proceedings of the 42nd Meeting ofthe Association for Computational Linguistics, pages653?660.Haitao Mi and Liang Huang.
2008.
Forest-based transla-tion rule extraction.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 206?214.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 192?199.630Kevin P. Murphy and Mark A. Paskin.
2001.
Linear timeinference in hierarchical HMMs.
In Proceedings ofNeural Information Processing Systems, pages 833?840.Rebecca Nesson, Stuart Shieber, and Alexander Rush.2006.
Induction of probabilistic synchronous tree-insertion grammars for machine translation.
In Pro-ceedings of the 7th Biennial conference of the Associ-ation for Machine Translation in the Americas, pages128?137.Franz Josef Och and Hermann Ney.
2002.
Discrimi-native training and maximum entropy models for sta-tistical machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 295?302.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004.
Asmorgasbord of features for statistical machine trans-lation.
In Proceedings of the Human Language Tech-nology Conference of the North American Chapter ofthe Association for Computational Linguistics, pages161?168.Matt Post and Daniel Gildea.
2008.
Parsers as languagemodels for statistical machine translation.
In Proceed-ings of the Eighth Conference of the Association forMachine Translation in the Americas, pages 172?181.Matt Post and Daniel Gildea.
2009.
Language modelingwith tree substitution grammars.
In NIPS workshop onGrammar Induction, Representation of Language, andLanguage Learning.Arjen Poutsma.
1998.
Data-oriented translation.
InNinth Conference of Computational Linguistics in theNetherlands.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics, pages 271?279.Lawrence R. Rabiner.
1990.
A tutorial on hid-den Markov models and selected applications inspeech recognition.
Readings in speech recognition,53(3):267?296.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.William Schuler, Samir AbdelRahman, Tim Miller, andLane Schwartz.
2010.
Broad-coverage incrementalparsing using human-like memory constraints.
Com-putational Linguistics, 36(1):1?30.William Schuler.
2009.
Positive results for parsing with abounded stack using a model-based right-corner trans-form.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 344?352.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of the 46th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 577?585.Stuart M. Shieber and Yves Schabes.
1990.
Synchronoustree adjoining grammars.
In Proceedings of the 13thInternational Conference on Computational Linguis-tics.Stuart M. Shieber.
2004.
Synchronous grammars as treetransducers.
In Proceedings of the Seventh Interna-tional Workshop on Tree Adjoining Grammar and Re-lated Formalisms.Mark Steedman.
2000.
The syntactic process.
MITPress/Bradford Books, Cambridge, MA.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of 39thAnnual Meeting of the Association for ComputationalLinguistics, pages 523?530.D.H.
Younger.
1967.
Recognition and parsing ofcontext-free languages in time n cubed.
Informationand Control, 10(2):189?208.Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Seng Li,and Chew Lim Tan.
2007.
A tree-to-tree alignment-based model for statistical machine translation.
InProceedings of the 11th Machine Translation Summitof the International Association for Machine Transla-tion, pages 535?542.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of the Workshop on Statistical MachineTranslation, pages 138?141.631
