Session 11: Acoust ic  Model ing and Robust  CSRSteve YoungCambr idge  Un ivers i ty  Eng ineer ing  Depar tmentT rumpington  Street ,  Cambr idge ,  CB2 1PZ, Eng landThe papers in this session are all concerned with im-proving the performance of speech recognition systemsat the acoustic modeling level.
Three papers focus onobtaining very high accuracy under clean matched con-ditions and the remaining papers deal with the problemsof mis-match caused by the use of different microphonesand environmental noise.The first three papers focus on the problem of buildingvery accurate Hidden Markov Models (HMMs) for use inlarge vocabulary speech recognition systems.
Such sys-tems require a very large number of context-dependentmodels, but the available training data is limited and.unevenly spread.
Hence, some way has to be found toshare the data amongst he models.
Furthermore, theapproximations inherent in the use of tied-mixture sys-tems has led to a return to more conventional contin-uous density mixture Gaussian systems.
In their basicform, these have a much greater number of parametersand hence, the data insufficiency problem is exacerbated.The solution adopted in the first two cases is to tie statestogether in order to balance the available data againstmodel complexity.
In the third paper, MAP estimationis used in conjunction with generalised triphones.The first paper, "Tree-based State Tying for High Accu-racy Acoustic Modelling" by Young, Odell 84 Woodland,describes a state-tying method based on phonetic deci-sion trees.
They use an incremental approach in whichuntied single Gaussian models are estimated for everytriphone appearing in the training data, then stateswithin allophone sets are tied and finally, the tied sin-gle Gaussians are converted to mixture Gaussians.
Thekey attractions of this approach are that firstly it is veryefficient since the objective function used for node split-ting depends only on the model parameters and not onthe original data, and secondly, once built, the decisiontrees can be used to synthesise all of the unseen triphonesneeded by the recogniser but not seen in the trainingdata.
The technique described was used to build theHTK Large Vocabulary Recogniser.
This was includedin the November 1993 Wall Street Journal evaluationwhere it returned the lowest error rate in three of thefour tests and the second lowest error rate in the fourth.The second paper, "High-Accuracy Large-VocabularySpeech Recognition using Mixture Tying and Consis-tency Modeling" by Digilakis 84 Murveit follows a similarapproach.
The system here is also built incrementally,however, in this case, HMMs are built starting from aheavily tied system and then mixtures are successivelyuntied using agglomerative clustering followed by a split-ting and pruning procedure.
The paper also contains aninteresting study on time correlation modelling and theuse of linear discriminant analysis.The third paper, "The LIMSI Continuous Speech Dicta-tion System" by Gauvain, Lamel, Adda 84 Adda-Decker,describes a continuous density HMM system which hasa more conventional generalised triphone structure butwhich is trained using MAP estimation.
This yielded thelowest error rate of any 20k word trigram system in theNovember 1993 WSJ evaluation.
In addition to WSJ, thepaper also describes results for French using the BREFcorpus.
This leads to some interesting conclusions on theproperties of the two languages.
In particular, it wouldappear that although higher phoneme recognition ratesare achieved for French, the higher homophone rate re-duces word recognition to similar levels as for English.The systems described in these three papers all use con-tinuous density mixture Gaussian HMMs.
Whilst theseseem to yield the best performance under ideal condi-tions, for robust near real-time operation, tied-mixturesystems continue to offer many advantages.
In thefourth paper of the session, "Adaptation to New Mi-crophones using Tied-Mixture Normalisation" by Anas-tasakos, Kubala, Makhoul 84 Schwartz, a technique isdescribed in which a probabilistic function is used tomap the codebook built in training into one suitable forthe required new microphone.
This mapping function isbuilt using a small amount of stereo adaptation data col-lected using the new microphone.
When combined withcepstral mean subtraction, the technique substantiallyreduced the effects of microphone mis-match.The fifth paper, "Signal Processing for Robust SpeechRecognition" by Liu, Moreno, Stern 84 Acero reviewsa number of cepstral-based compensation procedures305,which have been studied within the Sphinx-II recog-nition system developed at Carnegie Melon University.These include a phone-dependent cepstral normalisationscheme in which compensation vectors are estimatedfrom training data consisting of clean speech and cor-responding noisy speech.
Other techniques studied in-clude VQ codebook adaptation, reduced-band analysisfor telephone speech and silence codebook adaptation.A variety of experimental results are presented for theMicrophone Independence (Spoke 5) and the CalibratedNoise Sources (Spoke 8) November 1993 WSJ evalua-tions.
Error reductions of around 40% were typicallyachieved using combinations of these techniques.The sixth paper, "Microphone-Independent Robust Sig-nal Processing using Probabilistic Optimum Filtering"by Neumeyer 84 Weintraub, also addresses the problemscaused by having a mis-match between the microphoneused for training a speech recognition system and themicrophone used for testing.
In this case, the solutionproposed is to replace ach incoming noisy speech vectorwith an estimate of the underlying clean speech vector.This mapping is achieved using a piecewise non-lineartransformation estimated from stereo training data.
Alarge number of results are presented both for ATIS andWSJ.The seventh and final paper, "Microphone Arrays forRobust Speech Recognition" by Che, Lin, Pcarson, deVries ~4 Flanagan, takes a very different approach todealing with environmental robustness.
In this case, theinput signal is pre-processed using a beamforming mi-crophone array followed by a neural network trained ona small amount of the noisy speech data.
In various testsusing the Sphinx-I recognition system, the combinationof array and neural network in a reverberent acousticenvironment was able to achieve similar performance tothat obtained using a close-talking microphone.306
