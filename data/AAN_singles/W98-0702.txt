IIIIIIIIIIIIIIIIIIIDisambiguating Verbswith the WordNet Category of the Direct ObjectEric V. SiegelDepartment ofComputer ScienceColumbia UniversityNew York, NY 10027evs@cs, columbia,  eduAbstractIn this paper, I demonstrate that verbs can be dis-ambiguated according to aspect by rules that exam?hue the WordNet category of the direct object.
First,when evaluated over a corpus of medical reports, Ishow that WordNet categories correlate with aspec-tual class.
Then, I develop a rule for distinguishingbetween stative and event occurrences of have by theWordNet category of the direct object.
This rule,which is motivated by both linguistic and statisti-cal analysis, is evaluated over an unrestricted set ofnouns.
I also show that WordNet categories improvea system that performs aspectual classification withlinguistically-based numerical indicators.1 IntroductionThe verb have is semantically ambiguous.
It can de-note a possessive r lationship, as in, I had a car, orendow a quality, as in, I had anxiety.
Further, havecan describe an act of creation, as in, I had a baby,or an undertaking, as in, I had lunch.
Broadly, alluses of have either denote a state, i.e., a situationthat is not dynamic, or an event, i.e., a dynamic oc-currence that entails change or activity.
This seman-tic distinction, stativity, is fundamental to many do-mainR, e.g., distinguishing symptoms and diagnosesfrom procedures in the medical domain.Stativity is the first distinction for the semantichierarchy of verb phrases known as aspect.
This hi-erarchy is linguistically established to enable reason-ing about time, i.e., temporal reasoning.
Aspectualclassification further distinguishes events accordingto completedness (i.e., telicity), which determineswhether an event reaches a culmination point in timeat which a new state is introduced.
For example, Imade a fire is culminated, whereas, I gazed at thesunset is non-culminated.Aspectual classification is necessary for interpret-ing temporal modifiers and assessing temporal en-tailments (Moens and Steedman, 1988; Dorr, 1992;Klavans, 1994), and is therefore a necessary com-ponent for applications that perform certain lan-guage interpretation, summarization, informationretrieval, and machine translation tasks.
Aspectualclassification is a diflqcult problem because manyverbs, like have, are aspectually ambiguous.In this paper, I demonstrate that verbs can bedisambiguated according to aspect by the semanticcategory of the direct object.
To this end, WordNet,the largest publicly available on-line lexical database(Miller et al, 1993), is used to provide semantic cat-egories for direct objects.
When applied over a cor-pus of medical reports, I show that WordNet cate-gories correlate with aspectual class.
Furthermore,I develop a rule for aspectual classification by theWordNet category of the direct object.
This ruleis specialized for the verb have, which presents amore prevalent disambiguation problem in medicalreports than any other verb.
The design of this ruleis guided by both linguistic and statistical analy-sis.
The rule is evaluated over an unrestricted setof nouns.
WordNet categories are also shown to im-prove a system that performs aspectual classificationwith linguistically-based numerical indicators.The following section further discusses the seman-tic entailments of aspect and Section 3 discusses theproblem of aspectual ambiguity.
Section 4 describesthe corpus used for this study, and Section 5 de-scribes our approach to disambiguating have.
Sec-tion 6 then evaluates this approach and Section 7 de-scribes the use of WordNet for linguistic indicators.Finally, Section 8 provides conclusions and describesfuture work.2 Aspect in Natural LanguageAspectual classification is a key component of mod-els that assess temporal constraints between clauses(Moens and Steedman, 1988; Hwang and Schubert,1991; Dorr, 1992; Hitzeman et al, 1994).
For ex-ample, stativity must be identified to detect tempo-ral constraints between clauses connected with when.For example, in interpreting (I),(I) She had good strength when objectively tested.the following temporal relationship can hold betweenthe have-state and the tear, event:have-strengthiHowever, in interpreting (2),IIIIIIIIIIIIIIIIIITable I: Four aspectual markers and their linguisticconstraints on aspectual class.I f  a clause can occur:, then it is:with a temporal adverb event(e.g., then)in prvyrwsive - eventwith a duration in-PP c-lminated(e.g., in an hour) eventin the perfect ense c,  lm.
eventor state(2) She had a se/zure when.
objectively tested.the temporal relationship is between two events, andcan be different:have-seizureI ICertain temporal adjuncts and tenses are con-strained by and contribute to the aspectual classof a clause (Vendler, 1967; Dowry, 1979).
Tables 1lists four e.x_ample linguistic constraints.
Each entryin this table describes an aspectual marker and theconstraints on the aspectual category of any clausethat appears with that marker.
For example, if aclause appears in the progressive tense, it must bean event, e.g.,(3) He is prospering.
(event),which contrasts with,(4) *You are resembling your mother.
(state).As a second example, an event must be culminatedto appear in the perfect tense, for example,(5) She had made an attempt.
(culminated),which contrasts with,(6) *He had stared at me.
(non-c-lminated)Such constraints linguistically validate the aspec-tual hierarchy of semantic lasses, provide seman-tic constraints for natural anguage generation andunderstanding, and provide guidelines for aspectualcorpus analysis.3 Aspectually Ambiguous VerbsWhile some verbs appear to connote only one as-pectual class regardless of context, e.g., stare (non-c-lminated event), many verbs are aspectually am.biguous.
For example, shaw denotes a state in, H/$lumbar puncture showed evidence of white cells, butdenotes an event in, He showed me the photographs.This ambiguity presents a di~culty for automati-cally classifying averb because the aspectual class ofa clause is a function of several clausal constituentsin addition to the main verb (Dowry, 1979; Moensand Steedman, 1988; Pustejovsky, 1991).
However,previous work that numerically evaluates aspectualclassification has looked at verbs in isolation (Kla-vans and Chodorow, 1992; Siegel, 1997).10The verb have is particularly problematic.
In themedical domain, have occurs as the main verb ofclauses frequently (8% of clauses) and is aspectu-ally ambiguous, occurring 69.9% of the time as astate, and 30.1% of the time as an event.
Most otherambiguous verbs are more highly dominated by onesense in this domain (Siegel, 1998).In this section, I examine factors contributing toaspectual ambiguity.
First, I exam the interactionbetween a verb and its arguments in determining as-pectual class.
The semantic ategory of open classwords plays a large role in this process.
And sec-ond, I describe a semantic hierarchy of statively am-biguous verb.
This hierarchy groups together verbsthat tend to interact with their arguments in similarways.3.1 How Clausal Const i tuents Contr ibuteto Aspectual  ClassThe presence, syntactic categories, lexical heads,and plurality of a verb's arguments influence as-pectual class.
This is illustrated in Table 2, whichshows example clausal features that influence aspec-tual class.
The effect of each feature is illustratedby showing two similar sentences with distinct as-pectual classes.The number of ways in which clausal constituentsinteractively influence aspect is ,mknown.
However,syntax alone is not sufficient, and the lexical headof multiple constituents (e.g., the verb phrase andthe direct object) are often factors.
Moreover, thesemantic category of these features can also playa role.
For example, Sue played the piano is non-c,lminated, while Sue played the sonata signifies ac-lminated event (this example comes from Moensand Steedman (1988)).3.2 Classes of Ambiguous VerbsPlacing aspectually ambiguous verbs into semanticcategories will help predict how these verbs com-bine with their arguments to determine aspectualclass.
This is because many verbs with related mean-ings combine with their arguments in similar ways.In general, there is a correlation between a verb'ssubcategorization frame and semantic lass (Levin,1993), and this applies to aspect in particular.For example, look and weigh can each appear asevents, e.g.,I looked at the baby.
(event)I we/ghed the baby.
(event)and can also appear as states, as in,The baby ~ heavy.
(state)The baby weighed a lot.
(state)Is this illustrates, these two verbs have similar sub-categorization frames that determine their aspectualclass.
There is also a relationship between theirmeanings, since each describes a type of perceptionor measurement.IIIIIIIIIIIIIIIIIIITable 2: Example clausal features and how they can influence aspectual class.
"P" means process (i.e.,non-culminated event), "C" means culminated event, and "S" means state.Feature:"Predicate adj' ParticleD~r obj cat~, Dir obj headDir obj det, !nd obj det.Ind obj headPrep obj headPrep obj detTenseExample: class:John drove the car.
PJohn drove the car.
PJohn saw Sue.
PJudith p/ayed the piano.
PJohn ate fries.
PKathy sho~ed people her car.
PKathy showed people her car.
PJudith looked around the store.
PKathy shot at deer.
PSal said that it helps.
CContrasting Example:John drove the car ragged.John drove the car up.John saw that  Sue was happy.Judith p/ayed the sonata.John ate the fries.Kathy shorted the people her car.Kathy showed Sal her car.Judith looked around the corner.Kathy shot at the deer.Sal says that it helps.class:CCCCCCCCCSGroup:coemunicationcognitionperceptionpsy .
?h-movuantlocationmetaphoricalTable 3: Groups of verbs that axe statively ambiguous.Example verbs:admit, confirm, indicate, sayjudge, remember, think, wishfeel, see, smell, weighastonish, dismay, please, surlmdsehold, lie, sit, standwork, runcontinue , remainEvent sentence:I said, #Hello.
"I thought about hem.I felt the tablee/oth.You surprised me.I lay on the bed.I worked hard.\[ continued to talk about it.\[ State sentence:\[ say it is correct.I th ink they are nzce.I felt terrible.That suprises me.The book lies on the bed.The machine works.I continued to feel good.Table 3 shows the top level of a hierarchy of startively ambiguous verbs.
Seven semantic groups areshown, each with a set of example verbs, and twosentences illustrating contrasting uses of an exampleverb fxom that group.
Each verb in the first group,communication, can appear as either an event orstate.
Intuitively, this is because each verb can con-vey a communicative act, e.g.,She s.howed me the photos.
(event)or, alternatively, a non-dynamic situation, e.g.,The zrays show no sign ol ~rth .
(state)Verbs in the second group in Table 3, cogn i t ive ,can convey a mental event, e.g.,When he mentioned bananas, she remembered Ed-ward.
(event)or, alternatively, a mental state, e.g.,I'U ahvays remember Disney WorlcL (state)The groups perception and psych-movement aresubgroups of cognition.
The perception andcommunication groups have previously been ides-tiffed with respect to aspect in particular (Vendler,1967; Dowry, 1979), and those and psych-movementfor general purposes beyond aspectual ambiguity(Levin, 1993).
The fifth group, locative, has previ-ously been identified as "lay-verbs.
~ (Dowty, 1979)The group metaphorical in Table 3 contains eventverbs with idiomatic uses that are stative.
These id-iomatic uses correspond to a metaphorical interpre-tation of the event reading (Alexander D. Chaifee,personal communication).
For example,I ra_.nn down the street.
(event)It runs in the family.
(state)Finally, cart/st verbs simply reflect the aspectualclass of their clausal argument.4 Corpus: Medical ReportsOur experiments are performed across a corpus of3,224 medical discharge summaries comprised of1,159,891 words.
A medical discharge s,,mmary de-scribes the symptoms, history, diagnosis, treatmentand outcome of a patient's visit to the hospital.
As-pectual classification is necessary for several medicalreport processing tasks, since these reports describeevents and states that progress over time (Friedmanet al, 1995).These reports were parsed with the EngLishSlot Grammar (McCord, 1990), resulting in 97,973clauses that were parsed fully with no self-diagnosticerrors (error messages were produced on some ofthis corpus' complex sentences).
Parsing is neededto identify the main verb and direct object of eachclause, as well as the presence of aspectual mark-ers for related statistical work, described below inSection 7.Be and have are the two most popular verbs, cov-ering 31.9% of the clauses in this corpus.
Clauseswith be as their main verb, composing 23.9% of thecorpus, always denote a state.
Clauses with have astheir main verb, composing 8.0% of the corpus, arestatively ambiguous.
In this domain, most clauseswith main verbs other than be and have can be aspec-tually classified by the the main verb only, e.g., by11IIIIIIIIIIIIIIIIIIIusing numerical linguistic indicators (Siegel, 1998)In order to produce supertrised ata with whichto develop and evaluate our approach, a batch of206 have-clauses f~om the parsed corpus were man-ually marked according to stativity.
As a linguistictest for marking, each clause was tested for read-ability with, What happened was...
In a separatestudy, a comparison between two human markers us-ing this test to classify clauses over all verbs showedan agreement of approximately 91% (Siegel, 1998).The marked clauses, divided equally into trainingand testing sets of 103 clauses each, were used todevelop and evaluate our approach, respectively.5 App ly ing  WordNetI have manually designed a rule for classifying have-clauses according to stativity by the WordNet cat-egory of the direct object.
To design this rule, thefollowing were observed:?
Distributions of objects of have over the corpus.?
Linguistic intuition regarding WordNet cate-gories and aspectual c ass.?
Correlations between the WordNet category ofthe direct object and stativity over the super-vised training data.To accumulate this information, WordNet wasqueried for each direct object of the parsed corpus.In particular, each noun was placed into one of the25 categories at the top of WordNet's emantic hi-erarchy, listed in Table 4.
Many nouns have mul-tiple entries corresponding to multiple senses.
Asan initial approach, we take the first WordNet cate-gory listed, i.e., the most f~equent sense.
Pronounssuch as him and it were assigned their own category,pronoun.As shown, in Table 5, the most frequent objectsof have are primarily specific to the medical domain.This table shows the high level semantic ategoryassigned by WordNet and the classification of have-clauses with each noun as a direct object.
WordNetis able to handle this technical domain since 89.1%of have-clauses have direct objects that are widely-known medical terms and non-technical terms.The rule shown in Table 6 classifies have-clausesbased on the semantic category of their direct ob-ject.
In particular, clauses with direct objects thatbelong to the categories event, act, phenomenon,communication, possession and food are classifiedas events, and all others are classified as states.Linguistic insights guided the design of this rule.For example, if the direct object of have denotes anevent, such as seizure, the clause describes an event.For this reason, it is clear why the WordNet cate-gories event, act, phenomenon a d communicationeach indicate an event clause.
Note that nominalizedevent verbs, e.g., resolution, are placed in these fourcategories by WordNet.
The category possessionWordNet classlocation 0event 2act 6ar t i fac t  5phenomenon 2entity 2attribute 3meuu:e 3N/A 5cognition IIstate 19t J .u  9substance 5re la t ion  3person 2communication Icausalagent 1posseesion 1group Ifood Ishape 0natural object 0fenlin K 0aJD4m=l 0plant 0mot i vat  ion 0as s ta te  a~ event15631111110000000000000000Table 4: Word_Net categories ofdirect objects of havein the supervised training data.direct object n WordNet c l~history 624 timeep/sode 280 eventpain 192 cognition/@er 123 cognitiontemperature 113 attribute~lev~ 109 statemovement 106 actcourse 96 act<none> 91 <none>symptom 81 cognitioncomplaint 73 states~z~re 72 eventnausea 67 cogn?tionCI?Mmof clauses ta teevents ta tes ta te*states ta te*event*event*state*state*stateevent*stateTable 5: Frequent objects of have, their WordNetcategory, and the aspectual class of have-clauseswith the object.
Asterisks (*) denote classificationsthat were intuitively derived, since these examplesdid not occur in the training cases.was selected since, as shown in Table 6, most occur-rences of possession as a direct object of have areinstances of loss, e.g., The patient had blood loss de-scribes an event.
The category food was selected tocover idioms such as The patient had lunch (event).Furthermore, this classification rule is quantita-tively supported over the supervised training data.12IIIIIIIIIIIIIIIf thenobject is a(n): class is: nact event 1,157event 655phenomenon 242co~unicat ion 194possession 59food 17cognition state 1,146state 875N/A 860time 636art i fact  415attr ibute 349entity 209measuze 205substance 182relation 116person 115group g4location 49feeling 48pronoun 39animal 12Prequent nounsmovement (106) course (96) di~iculty (66) scan (61) admission (60)episode (280) se/zure (72) pulse (28) recurrence (25) on.set (24)pressure (52) z-ray (30) flatus (21) response (19) intake (15)sign (25) resolution (22) effusion (18) section (17) electrocardiogram (12)loss (27) amount (15) res/dua/(5) insurance (4) cut (3)b~ (5) caH~ (2) ~min  (1) ~,g,r (1) sco~ (1)pain (192) l~er (123) ~jmptnm (81) nausea (67) t~t (54)a/ler~ (109) complaint (73) infection (56) disesse (56) problem (40)echocardiogram (51) hematocr/t (41) ultrasound (34) stenosis (29)hist~.~ (624) r~m (8) paa (3) g~t/on (1)catheter (20) stool (19) tube (17) output (16) PPD (15)temperature (113) shortne~8 (46) tenderne.ss (26) levd (22) sound (16)chest (20) head (13) abdomen (13) artery (12) shunt (7)count (41) inc~,~se (18) bout (15) lull (12) day (9)blood (29) thallium (15) sodium (11) urine (10) fluid (9)change (40) rate (32) f~nct/on (12) aspirate (5) relationship (3)ch//d (13) aide (13) son (8) patient (8) temp (6)culture (41) serieJ (7) meet/m 3 (6) progression (4) panel (4)a~ (8) po~t (7) le/e (6) s~te (4) lab (4)appetite (18) relief(7) chill (6) preference (3) feeling (3)which (18) th/a (8) her (4) them (3) it (3)dog (3) paceer (2) pet (1) fetus (1) emu (1)Table 6: Aspectual classification rule for have-clauses.
Counts are over all have-clauses in the medical reportscorpus, from which the supervised training and testing data were extracted.For each WordNet category, Table 4 shows the distri-bution of event and stative have-clauses with a directobject belonging to that category.
As shown, eachWordNet category llnimd to states with our rule oc-curs at least as frequently in stative clauses as theydo in event clauses within the training set, with theexception of co l tmicat ion ,  possess ion  and food.However, these categories occur only one time eachin the training data, which is too sparse to counterlinguistic intuition.6 Resu l tsThere is a strong correlation between the Word-Net category of a direct object, and the aspec-tual class of have-clauses it appears in.
When us-ing the classification rule established in the previ-ous subsection, the WordNet categories that appearmore than five times in the supervised test datacorrectly predict the class of have-clauses with anaverage precision of 82.7?/o.
Specifically, act  andevent predict event have-clauses 85.7% and 66.7%correctly, respectively, and states are predicted witha~'l:ifact (62.5% precision), cogni t ion  (88.2%),s ta te  (93.3%) and t~ne (100.0%).For evaluating the rule's overall performance,there is a baseline of 69.9% and a ceiling of 84.5%accuracy.
The baseline is achieved simply by classi-fying each clause as a state, since this is the domi-nant class over the supervised test cases, t However,XSimilar baselines for comparison have been used for manyclassification problems (Duds and Hart, 1973), e.g., part-of-I I overalll States Eventsacc recall prec recall precC 84.5% 93.1% 85.9% 64.5% 80.0%R 79.6% 84.7% 85.9% 67.7% 65.6%B 69.9% 100.0% 69.9% 0.0% 100.0%Table 7: Performance of a rule (R) that uses theWordNet category of the direct object to aspectuallyclassify have-classes, versus ceiling (C) and baseline(B) approaches.this approach classifies all event clauses incorrectly,achieving an event rr~21 of 0.0%.
The ceiling of84.5% is the maximum achievable by a rule such asours since the first WordNet category of the directobject is not always enough to resolve aspectual am-biguity; the same category appears in both stativeand event test cases.Overall classification performance using Word-Net categories i  greatly improved over the baselinemethod.
As shown in Table 7, an accuracy of 79.6%was achieved.
A binomial test showed that this im-provement over the baseline is significant (p < .04).An event greater improvement over the baselineis illustrated by the increase in the number of eventclauses correctly classified, i.e.
event rr?all.
Asshown in Table 7, an event recall of 67.7% wasachieved by the classification rule, as compared tospeech tagging (Church, 1988; Alien, 1995).13IIIIIIIIIIIIIIIIIIthe 0.0% event recall achieved by the baseline, whilesuffering no loss in overall accuracy.
This differ-ence in recall is more dramatic than the accuracyimprovement because of the dominance of stativeclauses in the test set.
A favorable tradeoff in re-call with no loss in accuracy presents an advantagefor applications that weigh the identification of non-dominant instances more heavily (Cardie and Howe,1997).
For example, it is advantageous for a medicalsystem that identifies medical procedures to identifyevent clauses, since procedures are a type of event.There are several problematic cases that illustratelimitations to our approach.
In particular, lexicalambiguity is mi.qleading for the task of classifyinghave-clauses.
For example, The paticnt had Med/c~/ddenotes a state, but WordNet categorizes Med/ca/das an act.
Similarly, PET, EMUand CATare cate-gorized as animal, as shown in Table 6.
This wouldbe solved by recognizing these as proper nouns oracronyms due to capitalization.
However, other am-biguous objects are more difficult to address.
Fore~Ample, The patient had an enema describes anevent, but WordNet lists enema as artifacl; be-fore act.
As another example, The patient had aurine culture is an event, but WordNet's first senseof cu/tuw is group.
Furthermore, the direct object of10.9% of have-clauses in the medical reports are un-known to WordNet ("N/A").
This includes medicalterminology, e.g., anticont~ants and vitrectomy, aswell as certain expressions parsed by the English SlotGrammar that require further post-processing, suchas bettoeen 39 and 29.7 WordNet for Linguistic IndicatorsAspectual classification is a large-scale, domain-dependent problem.
Although a complete aspectuallexicon of verbs may suffice to classify many clausesby their main verb only, a verb's primary class isoften domain-dependent.
For example, while manydom~inR primarily use show as an event, its appear-ances in medical discharge snmmaxies primarily de-note states.
Therefore, it is necessary to produce aspecialized lexicon for each domain.One statistical approach is to measure linguisticindicators over a corpus (Siegel, 1998).
These in-dicators measure how frequently each verb appearswith markers uch as those in Table 1.
For exam-ple, a verb that appears more frequently in the pro-gressive is more likely to describe an event than astate (Klavans and Chodorow, 1992).
However, thisapproach attempts to classif T verbs independent oftheir context.Incorporating additional constituents of a clausecould alleviate this problem.
For example, indicatorscould be measured over verb-object pairs.
However,since both the main verb and the head of the directobject are open-class categories, indicators would besparsely measured (enjopturnips i rare).To alleviate sparsity, but retain information about\[ \[overall I Culm Non-Culmace redall prec recall precW 71.1% 81.5% 75.0% 53.1% 62.5%V 68.5% 86.2% 70.6% 38.1% 61.4%B 63.3% 100.0% 63.3% 0.0% 100.0%Table 8: Comparison of indicators computed overthe main verb (V), indicators over verb and object'sWordNet category pairs (W), and a baseline (B).the direct object, we measured indicators over verb-object-category pairs, using WordNet to derive thesemantic category of each object.
I describe suchexperiments briefly here; Further details regardingthese experiments is given by Siegel (1998).Fourteen such indicators were evaluated for distin-guishing clauses according to completednese over anunrestricted set of verbs and direct objects.
A cor-pus of 75,289 parsed clauses from ten novels was usedto measure indicator values.
307 training cases (196culminated) and 308 test cases (195 culminated)were manually annotated using linguistic tests.
De-cision tree induction was performed over the trainingcases to combine the indicators.Indicators measured over the main verb and di-rect object category achieved a more favorable re-call tradeoff than those measured over the verbonly, with comparable performance in accuracy.
Asshown in Table 8, indicators measured over the mainverb and direct object category achieved a non-culminated recall of 53.1%, as compared to 38.1%achieved by the verb-only indicators.
The baselineof 63.3% accuracy is achieved by simply classifyingevery clause as culminated.8 Conclusions and Future WorkThe semantic category of the direct object plays amajor role in determining the aspectual class of aclause.
To demonstrate this, a rule was developedthat uses WordNet categories to classify have-clausesaccording to stativity.
When evaluated over an unre-stricted set of nouns, this rule achieved an accuracyof 79.6%, compared to the baseline performance of69.9%.
Moreover, a favorable tradeoff in recall wasachieved, attaining 67.7% event recall, compared tothe the baseline's 0.0%.
More specifically, frequentWordNet categories were shown to predict aspectualclass with an average precision of 82.7%.
These re-sults are impressive, considering the unresolved se-mantic ambiguity of direct objects, and the technicalterminology of the medical domain.WordNet categories also improved the classifica-tion performance of linguistic indicators for com-pletedness.
Although more sparsely measured, theaccuracy achieved by indicators measured over mul-tiple constituents is comparable to that of indicatorsmeasured over the verb only, with a favorable trade-14IIIIIIIIIoff in recall.
Therefore, the noise introduced by thismore sparse measurement of indicators i  more thancompensated forby the ability to resolve aspectuallyambiguous verbs.Furthermore, I have derived a semantic hierar-chy of statively ambiguous verbs in order to predictverbs' subcategorization frames.
This in turn guidesthe disambiguation of such verbs.
Future work willinvestigate whether rules such as that developed forhave could apply over multiple verbs that share sub-categorization behavior.
Additionally, it is possiblethat WordNet's categorization of verbs could auto-matically place verbs into these semantic groups.Finally, disambiguating the direct object accord-ing to WordNet categories, e.g., Resnik (1995),would improve the accuracy of using these categoriesto disambiguate verbs.AcknowledgementsKathieen IL McKeown was extremely helpful regard-ing the formulation of our work and Judith Klavansregarding linguistic techniques.
Alexander D. Char-fee, Vasileios Hatzivassiloglou, Dragomir Radev andDekai Wu provided many helpful insights regard-ing the evaluation and presentation of our results.James Shaw first brought to my attention that haveis statively ambiguous, and, along with Eleazar Es-kin and Regina Barzilay, provided useful feedbackon an earlier draft of this paper.This research is suppoi'ted in part by theColumbia University Center for Advanced Technol-ogy in High Performance Computing and Commu-nications in Healthcare (funded by the New YorkState Science and Technology Foundation), the Of-rice of Naval Research under contract N00014-95-1-0745 and by the National Science Foundation undercontract GER-90-24069.ReferencesJ.
Allen.
1995.
Natural Language Understanding.Benjamin/Cummlngs, Redwood City, CA.C.
Cardie and N. Howe.
1997.
Improving mi-nority class prediction using case-specific featureweights.
In D. Fisher, editor, Proceedings o/theFourteenth International Conference on MachineLearning.
Morgan Kaufmann.K.
Church.
1988.
A stochastic parts program andnoun phrase parser for unrestricted text.
In Pro-ceedim2s ofthe '2nd Conference for Applied NaturalLanguage Prvcessing, pages 136--143.B.J.
Doff.
1992.
A two-level knowledge representa-tion for machine translation: lexical semantics andtense/aspect.
In James Pustejovsky and SabineBergler, editors, ~ Semantics and KnowledgeRepresentation.
Springer Verlag, Berlin.D.R.
Dowty.
1979.
Word Meaning and MontagueGrammar.
D. Reidel, Dordrecht, W. Germany.15R.
O. Duda and P.E.
Hart.
1973.
Pattern Classifi-cation and Scene Analysis.
Wiley, New York.C.
b'~'iedman, G. Hripcsak, W. DuMouchel, S.B.Johann, and P.D.
Clayton.
1995.
Natural lan-guage processing in an operational c inical infor-mation system.
Natural Language Engineering,2(1).J.
Hitzeman, M. Moens, and C. Grover.
1994.
Al-gorithrrm for analysing the temporal structure ofdiscourse.
Technical report, University of Edin-burgh.C.H.
Hwang and L.K.
Schubert.
1991.
Interpretingtemporal adverbials.
Technical report, Universityof Rochester.J.L.
Klavans and M. Chodorow.
1992.
Degrees ofstativity: the lexical representation of verb aspect.In Proceedings of the 1Jth International Confer-enee on Computation Linguistics.J.L.
Klavans.
1994.
Linguistic tests over large cor-pora: aspectual classes in the lexicon.
Technicalreport, Columbia University Dept.
of ComputerScience.
unpublished manuscript.B.
Levin.
1993.
English Verb CIasses and Alterna-tions.
University of Chicago Press, Chicago, 11,.M.C.
McCord.
1990.
SLOT GRAMMAR.
InIt.
Studer, editor, International Symposium onNatural Language and Logic.
Springer Verlag.G.A.
Miller, R. Beckwith, C. Felbaum, D. Gross, andK.
Miller.
1993.
Introduction to wordnet: An on-line lexical database.
Technical report.M.
Moens and M. Steedman.
1988.
Temporal ontol-ogy and temporal reference.
Computational Lin-guist/es, 14(2).J.
Pustejovsky.
1991.
The syntax of event structure.Cognition, 41(103):47-92.P.
Resnik.
1995.
Disambiguating noun groupingswith respect to WorclNet senses.
In Third Work-shop on Very Large Corpora, June.E.V.
Siegel and K.R.
McKcown.
1996.
Gatheringstatistics to aspectually classify sentences with agenetic algorithm.
In K. Oflazer and H. Somers,editors, Proceedings of the Second Inter'nationalConference on New Methods in Language Process-ing, Ankara, Turkey, Sept. Bilkent University.E.V.
Siegel.
1997.
Learning methods for combininglinguistic indicators to classify verbs.
In Prvceed-ings of the Second Conference on Empirical Meth-ads in Natural Language Processing, Providence,RI, August.E.V.
Siegel.
1998.
Linguistic Indicators for Lan-guage Understanding: Using machine learningmethods to combine corpus-based indicators foraspectual classification of clauses.
Ph.D. thesis,Columbia University.Z.
Vendler.
1967.
Verbs and times.
In Linguistics inPhilosophy.
Cornell University Press, Ithaca, NY.
