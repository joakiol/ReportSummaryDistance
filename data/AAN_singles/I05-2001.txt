A Classification-based Algorithm for Consistency Check ofPart-of-Speech Tagging for Chinese CorporaHu Zhang Jia-heng ZhengSchool of Computer& Information TechnologyShanxi UniversityTaiyuan, Shanxi 030006, ChinaYing Zhao Department of Computer ScienceUniversity of MinnesotaMinneapolis, MN 55455, USAyzhao@cs.umn.eduAbstractEnsuring consistency of Part-of-Speech(POS) tagging plays an important rolein constructing high-quality Chinesecorpora.
After analyzing the POS tag-ging of multi-category words in large-scale corpora, we propose a novel con-sistency check method of POS taggingin this paper.
Our method builds avector model of the context of multi-category words, and uses the k-NN al-gorithm to classify context vectors con-structed from POS tagging sequencesand judge their consistency.
The ex-perimental results indicate that the pro-posed method is feasible and effective.1 IntroductionConstructing high-quality and large-scale corporahas always been a fundamental research area inthe field of Chinese natural language process-ing.
In recent years, the rapid development inthe fields of machine translation (MT), phoneticrecognition (PR), information retrieval (IR), webtext mining, and etc., is demanding more Chinesecorpora of higher quality and larger scale.
En-suring consistency of Part-of-Speech (POS) tag-ging plays an important role in constructing high-quality Chinese corpora.
In particular, we fo-cus on consistency check of the POS taggingof multi-tagging words, which consist of sameChinese characters and are near-synonymous, butTo whom correspondence should be addressed.have different grammatical functions.
No mat-ter how many different POS tags a multi-categorywords may be tagged, ensuring consistency ofPOS tagging means to assign the multi-categoryword with the same POS tag when it appears insimilar context.Novel approaches and techniques have beenproposed for automatic rule-based and statistics-based POS tagging, and the ?state-of-the-art?
ap-proaches achieve a tagging precision of 89% and96%, respectively.
A great portion of the wordsappearing in Chinese corpora are multi-categorywords.
We have studied the text data from the2M-word Chinese corpus published by PekingUniversity, and statistics show that multi-categorywords cover 11% of the words, while the percent-age of the occurrence of multi-category words isas high as 47%.
When checking the POS tags,human experts may have disagreements or makemistakes in some cases.
After analyzing 1,042sentences containing the word ????
?, which areextracted from the 2M-word Chinese corpus ofPeking University, the number of incorrect tagsfor the word ?????
is 15, which accounts for around1.3%.So far in the field of POS tagging, most of theworks have focused on novel algorithms or tech-niques for POS tagging.
There are only a lim-ited number of studies has focused on consistencycheck of POS tagging.
Xing (Xing, 1999) ana-lyzed the inconsistency phenomena of word seg-mentation (WS) and POS tagging.
Qu and Chen(Qu and Chen, 2003) improved the corpus qualityby obtaining POS tagging knowledge from pro-cessed corpora, preprocessing, and checking con-1sistency with methods based on rules and statis-tics.
Qian and Zheng (Qian and Zheng, 2003;Qian and Zheng, 2004) introduced a rule-basedconsistency check method that obtained POS tag-ging knowledge automatically from processedcorpora by machine learning (ML) and rough set(RS) methods.
For real corpora, Du and Zheng(Du and Zheng, 2001) proposed a rule-based con-sistency check method and strategy to identify theinconsistency phenomena of POS tagging.
How-ever, the algorithms and techniques for automaticconsistency check of POS tagging proposed in(Qu and Chen, 2003; Qian and Zheng, 2003; Qianand Zheng, 2004; Du and Zheng, 2001) still havesome insufficiencies.
For example, the assign-ment of POS tags of the inconsistent POS taggingthat are not included in the instance set needs tobe conducted manually.In this paper, we propose a novel classification-based method to check the consistency of POStagging.
Compared to Zhang et al (Zhang etal., 2004), the proposed method fully considersthe mutual relation of the POS in POS taggingsequence, and adopts transition probability andemission probability to describe the mutual de-pendencies and k-NN algorithm to weigh the sim-ilarity.
We evaluated our proposed algorithm onour 1.5M-word corpus.
In open test, our methodachieved a precision of 85.24% and a recall of85.84%.The rest of the paper is organized as follows.Section 2 introduces the context vector model ofPOS tagging sequences.
Section 3 describes theproposed classification-based consistency checkalgorithm.
Section 4 discusses the experimentalresults.
Finally, the concluding remarks are givenin Section 5.2 Describing the Context ofMulti-category WordsThe basic idea of our approach is to use thecontext information of multi-category words tojudge whether they are tagged consistently ornot.
In other words, if a multi-category word ap-pears in two locations and the surrounding wordsin those two locations are tagged similarly, themulti-category word should be assigned with thesame POS tag in those two locations as well.Hence, our approach is based on the context ofmulti-category words and we model the contextby looking at a window around a multi-categoryword and the tagging sequence of this window.
Inthe rest of this section, we describe our vector rep-resentation of the context of multi-category wordsand how to determine various parameters in ourvector representations.2.1 Vector Representation of the Context ofMulti-category WordsOur vector representation of context consists ofthree key components: the POS tags of each wordin a context window (POS attribute), the impor-tance of each word to the center multi-categoryword based on distance (position attribute), andthe dependency of POS tags of the center multi-category word and its surrounding words (Depen-dency Attribute).Given a multi-category word and its contextwindow of size l, we represent the words in se-quential order as (w1; w2; :::; wl) and the POStags of each word as (t1; t2; :::; tl).
We also re-fer to the latter vector as POS tagging sequence.In practise, we choose a proper value of l sothat the context window contains sufficient num-ber of words and the complexity of our algorithmremains relatively low.
We will discuss this mat-ter in detail later.
In this study, we set the value ofl to be 7.2.1.1 POS AttributeThe POS tagging sequence contains informa-tion of the POS of each preceding (following)word in a POS tagging sequence as well as theposition of each POS tag.
The POS of surround-ing words may have different effect on determin-ing the POS of the multi-category word, which werefer to as POS attribute and represent it using amatrix as follows.Suppose we have a tag set of size m(1;2; :::;m), given a multi-category word witha context window of size l (w1; w2; :::; wl) and itsPOS tagging sequence, the POS attribute matrixY is an l by m matrix, where the rows indicate thePOS tags of the preceding words, multi-categoryword, and the following words in the context win-dow, while the columns present tags in the tag set.Yi;j= 1 iff the POS tag of wiisj.For example, consider the the POS attributematrix of ?????
in the following sentence:???
?/v ?/a ?
?/n ?/u ?/a ?/n ?/d ?/v ?
?/n ,/w2As we let l = 7, we look at the word ?????
andits 3 preceding and following words.
Hence, thePOS tagging sequence is ( a, n, u, a, n, d, v ).
Inour study, we used a standard tag set that consistsof 25 tags.
Suppose the tag set is ( n, v, a, d, u, p,r, m, q, c, w, I, f, s, t, b, z, e, o, l, j, h, k, g, y), thenthe POS attribute matrix of ?????
in this exampleis:Y=0BBBBBBBBBB0; 0; 1; 0; 0 : : : : : :1; 0; 0; 0; 0 : : : : : :0; 0; 0; 0; 1 : : : : : :0; 0; 1; 0; 0 : : : : : :1; 0; 0; 0; 0 : : : : : :0; 0; 0; 1; 0 : : : : : :0; 1; 0; 0; 0 : : : : : :1CCCCCCCCCCA2.1.2 Position AttributeDue to the different distances from the multi-category word, the POS of the word before (after)the multi-category word may in a POS tagging se-quence have a different influence on the POS tag-ging of the multi-category word, which we referto as position attribute.Given a multi-category word with a con-text window of size l, suppose the number ofpreceding (following) words is n (i.e., l =2n + 1), the position attribute vector VXofthe multi-category word is given by VX=(d1; :::; dn; dn+1; dn+2; :::; dl), where dn+1is thevalue of the position attribute of the multi-category word and dn+1 i(dn+1+i) is the valueof the position attribute of the ith preceding(following) word.
We further require that 8idn+1 i= dn+1+iand dn+1+Pni=1(dn+1 i+dn+1+i) = 1.We choose a proper position attribute vector sothat the multi-category word itself has the high-est weight, and the closer the surrounding word ,the higher its weight is.
If we consider a contextwindow of size 7, based on our preliminary exper-iments, we chose the following position attributevalues: d1= d7= 1=22; d2= d6= 1=11;d3= d5= 2=11; and d4= 4=11.
Hence, the fi-nal position attribute vector used in our study canbe written as follows:VX= (122;111;211;411;211;111;222):Note that if the POS tag in the POS tagging se-quence is incorrect, the position attribute value ofthe corresponding position should be turned intoa negative value, so that when the incorrect POStag appears in a POS tagging sequence, this at-tribute can correctly show that the incorrect POStag has negative effect on generating the final con-text vector.2.1.3 Dependency AttributeThe last attribute we focus on is dependencyattribute, which corresponds to the fact that thereare mutual dependencies on the appearance of ev-ery POS in POS tagging sequences.
In particular,we use transition probability and emission prob-ability in Hidden Markov Model (HMM) (Leek,1997) to capture this dependency.Given a tag set of size m (1;2; :::;m), thetransition probability table T is an m by m ma-trix and given by:Ti;j= PT(i;j) =f(i;j)f(i);where f(i;j) is the frequency of the POS tagjappears after the POS tagiin the entire corpus;f(i) is the frequency of the POS tagiappears inthe entire corpus; and P T is the transition proba-bility.Given a tag set of size m (1;2; :::;m), theemission probability table E is an m by m matrixand given by:Ei;j= PE(i;j) =f(i;j)f(j);where f(i;j) is the frequency of the POS tagiappears before the POS tagjin the entire corpus;f(j) is the frequency of the POS tagjappearsin the entire corpus; and PE is the emission prob-ability.Note that both T and E are constructed fromthe entire corpus and we can look up these two ta-bles easily when we consider the POS tags appearin POS tagging sequences.Now, when we look at a context window of size7 (w1; w2; :::; w7) and its POS tagging sequence(t1; t2; :::; t7), there are three types of probabili-ties we need to take into account.The first one is the probability of the appear-ance of the POS tag t4of the multi-category word,which we can write as follows:PCX(t4) = f(w4is tagged as t4)=f(w4); (1)3where f(w4) is the frequency of the appearanceof the multi-category word w4in the entire corpusand f(w4istaggedast4) is the frequency of theappearance where the word w4is tagged as t4inthe entire corpus.The second one is transition probability, whichis the probability of the appearance of the POS tagti+1in the i + 1 position after the POS tag tiinthe i position and shown in Eqn.
2:PT(i;i+1)= PT(ti; ti+1) = f(ti; ti+1)=f(ti):(2)The last last is emission probability, which isthe probability of the appearance of the POS tagti 1in the i  1 position before the POS tag tiinthe i position and shown in Eqn.
3:PE(i 1;i)= PE(ti 1; ti) = f(ti 1; ti)=f(ti):(3)According to the above three probability for-mulas we can build a seven- dimensional vector,where each dimension corresponds to one POStag, respectively.Given a multi-category word with a contextwindow of size 7 and its POS tagging sequence,the dependency attribute vector VPof the multi-category word is defined as follows:VP= (P1; P2; P3; P4; P5; P6; P7);whereP1= PT(1;2) P2= PT(1;2) PT2;3 PT(3;4) PCX(t4);P2= PT(2;3) P3= PT(2;3) PT(3;4) PCX(t4);P3= PT(3;4) P4= PT(3;4) PCX(t4);P4= PCX(t4);P5= PT(4;5) P4= PE(4;5) PE(4;5) PCX(t4);P6= PT(5;6) P5= PE(5;6) PE(4;5) PCX(t4);P7= PT(6;7) P6= PE(6;7) PE(5;6) P(4;5) PCX(t4):2.1.4 Context Vector of Multi-categoryWordsNow we are ready to define the context vectorof multi-category words.Given a multi-category word with a contextwindow of size l and its POS attribute matrix Y ,position attribute vector VX, and dependency at-tribute vector VP, the context vector VSof themulti-category word is defined as follows:VS= (VX+ VP) Y; (4)where  and  are the weights of the position at-tribute and the dependency attribute, respectively.Note that we require +  = 1, and their opti-mal values are determined by experiments in ourstudy.2.2 Experiment on the Size of the ContextWindowContext vectors can be extended by using 4 to 7preceding (following) words to substitute 3 pre-ceding (following) words in context windows andPOS tagging sequences.
We conducted experi-ments with a context window of size 3 to 7 on oursampled 1M-word training corpus and performedclosed test.
The experimental results are evalu-ated in terms of both the precision of consistencycheck and algorithm complexity simultaneously.We plot the effect on precision in Figure 1.0.870.8720.8740.8760.8780.880.8827 6 5 4 3PrecisionNumber of the preceding (following) wordsFigure 1: Effect on precision of the number ofpreceding (following) words.As shown in Figure 1, the precision of consis-tency check increases as we include more preced-ing (following) words.
In particular, the precisionis improved by 1% when we use 7 preceding (fol-lowing) words.
However, the increase of com-plexity is much higher than that of precision, be-cause the dimensionality of the position attributevector, POS attribute vector, and dependency at-tribute vector doubles.
Hence, we chose 3 as thenumber of preceding (following) words to formcontext windows and calculate context vectors.42.3 Effect on consistency check precision ofandWhen using our sampled 1M-word training cor-pus to conduct closed test, we found that consis-tency check precision changes significantly withthe different values of  and .
Figure 2 showsthe trend when  varies from 0.1 to 0.9.
We used= 0:4 and  = 0:6 in our experiments.0.50.550.60.650.70.750.80.850.90.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1Precision?Figure 2: Effect on consistency check precisionof  and .3 Consistency Check of POS TaggingOur consistency check algorithm is based on clas-sification of context vectors of multi-categorywords.
In particular, we first classify contextvectors of each multi-category word in the train-ing corpus, and then we conduct the consistencycheck of POS tagging based on classification re-sults.3.1 Similarity between Context Cectors ofMulti-category WordsAfter constructing context vectors for all multi-category words from their context windows andPOS tagging sequences, the similarity of two con-text vectors is defined as the Euclidean Distancebetween the two vectors.d(x; y) = kx yk ="nXi=1(xi  yi)2#(1=2); (5)where x and y are two arbitrary context vectors ofn dimensions.3.2 k-NN Classification AlgorithmClassification is a process to assign objects thatneed to be classified to a certain class.
In this pa-per, we used a popular classification method: thek-NN algorithm.Suppose we haveclasses and a class(!i(i = 1; 2; :::;)) has Nisamples (x(i)j(j =1; 2; :::; Ni)).
The idea of the k-NN algorithmis that for each unlabeled object x, compute thedistances between x and all samples whose classis known, and select k samples (k nearest neigh-bors) with the smallest distance.
This object xwill be assigned to the class that contains the mostsamples in the k nearest neighbors.We now formally define the discriminant func-tion and discriminant rule.
Suppose k1; k2; :::; kare the numbers of samples in the k nearest neigh-bors of the object x that belong to the classes!1; !2; :::; !, respectively.
Define the discrimi-nant function of the class !ias di(x) = ki; i =1; 2; :::;: Then, the discriminant rule of deter-mining the class of the object x can be definedas follows:dmx = maxi=1;2;:::;di(x) ) x 2 !m3.3 Consistency Check AlgorithmIn this section, we describe the steps of ourclassification-based consistency check algorithmin detail.Step1: Randomly sampling sentences containing multi-category words and checking their POS tagging manually.For each multi-category word, classifying the context vec-tors of the sampled POS tagging sequences, so that the con-text vectors that have the same POS for the multi-categoryword belong to the same class.Step2: Given a context vector x of a multi-category word, calculating the distances between x and all the contextvectors that contains the multi-category wordin the train-ing corpus, and selecting k context vectors with smallest dis-tances.Step3: According to the k-NN algorithm, checking theclasses of the k nearest context vectors and classifying thevector x.Step4: Comparing the POS of the multi-category wordin the class that the k-NN algorithm assigns x to and the POStag of.
If they are the same, the POS tagging of the multi-category wordis considered to be consistent, otherwise itis inconsistent.The major disadvantage of this algorithm is thedifficulty in selecting the value of k. If k is toosmall, the classification result is unstable.
On theother hand, if k is too big, the classification devi-ation increases.3.4 Selecting k in Classification AlgorithmFigure 3 shows the consistency check precisionvalues obtained with various k values in the k-NN algorithm.
The precision values are closed5Table 1: Experimental ResultsNumber of Number of Number ofTest Test multi-category the true the identified Recall Precisioncorpora type words inconsistencies inconsistencies (%) (%)1M-word closed 127,210 1,147 1,219 (156) 92.67 87.20500K-word open 64,467 579 583 (86) 85.84 85.24test results on our 1M-word training corpus, andwere obtained by using  = 0:4 and  = 0:6 inthe context vector model.0.50.550.60.650.70.750.80.8510 9 8 7 6 5 4 3 2 1AverageprecisionNumber of nearest neighbors (k)Figure 3: Effect on precision of k in the k-NNalgorithm.As shown in Figure 3, when k continues to in-crease from 6, the precision remains the same.When when k reaches to 9, the precision startsdeclining.
Our experiment with other  andvalues also show similar trends.
Hence, we chosek = 6 in this paper.4 Experimental ResultsWe evaluated our consistency check algorithm onour 1.5M-word corpus (including 1M-word train-ing corpus) and conducted open and closed tests.The results are showed in Table 1.The experimental results show two interest-ing trends.
First, the precision and recall ofour consistency check algorithm are 87.20% and92.67% in closed test, respectively, and 85.24%and 85.84% in open test, respectively.
Comparedto Zhang et al (Zhang et al, 2004), the precisionof consistency check is improved by 23%, andthe recall is improved by 10%.
The experimentalresults indicate that the context vector model hasgreat improvements over the one used in Zhanget al (Zhang et al, 2004).
Second, thanks to thegreat improvement of the recall, to some extent,our consistency check algorithm prevents the hap-pening of events with small probabilities in POStagging.5 Conclusion and Future ResearchIn this paper, we propose a new classification-based method to check consistency of POS tag-ging, and evaluated our method on our 1.5M-word corpus (including 1M-word training corpus)with both open and closed tests.In the future, we plan to investigate more typesof word attributes and incorporate linguistic andmathematical knowledge to develop better con-sistency check models, which ultimately providea better means of building high-quality Chinesecorpora.AcknowledgementsThis research was partially supported by the Na-tional Natural Science Foundation of China No.60473139 and the Natural Science Foundation ofShanxi Province No.
20051034.ReferencesY.
Du and J. Zheng.
2001.
The proofreading method studyon consistence of segment and part-of-speech.
ComputerDevelopment and Application, 14(10):16?18.T.
R. Leek.
1997.
Information extraction using hiddenMarkov models.
Master?s thesis, UC San Diego.Y.
Qian and J. Zheng.
2003.
Research on the method ofautomatic correction of chinese pos tagging.
Journal ofChinese Information Processing, 18(2):30?35.Y.
Qian and J. Zheng.
2004.
An approach to improving thequality of part-of-speech tagging of chinese text.
In Pro-ceedings of the 2004 IEEE International Conference onInformation Technology: Coding and Computing (ITCC2004).W.
Qu and X. Chen.
2003.
Analysing on the words classi-fied hard in pos tagging.
In Proceedings of the 9th Na-tional Computational Linguistics (JSCL?03).H.
Xing.
1999.
Analysing on the words classified hard inpos tagging.
In Proceedings of the 5th National Compu-tational Linguistics (JSCL?99).H.
Zhang, J. Zheng, and J. Liu.
2004.
The inspectingmethod study on consistency of pos tagging of corpus.Journal of Chinese Information Processing, 18(5):11?16.6
