A Flexible POS Tagger Using an Automatically AcquiredLanguage Model*L lu fs  MhrquezLS I -  UPCc/Jordi Girona 1-308034 Barcelona.
Catalonialluism?isi, upc.
esL lu /s  Padr6LS I -  UPCc / Jo rd i  G i rona  1-308034 Barce lona.
Cata lon iapadro@isi, upc.
esAbst rac tWe present an algorithm that automati-cally learns context constraints using sta-tistical decision trees.
We then use the ac-quired constraints in a flexible POS tag-ger.
The tagger is able to use informa-tion of any degree: n-grams, automati-cally learned context constraints, linguis-tically motivated manually written con-straints, etc.
The sources and kinds of con-straints are unrestricted, and the languagemodel can be easily extended, improvingthe results.
The tagger has been tested andevaluated on the WSJ corpus.1 In t roduct ionIn NLP, it is necessary to model the language in arepresentation suitable for the task to be performed.The language models more commonly used are basedon two main approaches: first, the linguistic ap-proach, in which the model is written by a linguist,generally in the form of rules or constraints (Vouti-lainen and Jgrvinen, 1995).
Second, the automaticapproach, in which the model is automatically ob-tained from corpora (either raw or annotated) 1 , andconsists of n-grams (Garside et al, 1987; Cuttinget ah, 1992), rules (Hindle, 1989) or neural nets(Schmid, 1994).
In the automatic approach we candistinguish two main trends: The low-level datatrend collects statistics from the training corpora inthe form of n-grams, probabilities, weights, etc.
Thehigh level data trend acquires more sophisticated in-formation, such as context rules, constraints, or de-cision trees (Daelemans et al, 1996; M/~rquez andRodriguez, 1995; Samuelsson et al, 1996).
The ac-quisition methods range from supervised-inductive-learning-from-example a gorithms (Quinlan, 1986;*This research as been partially funded by the Span-ish Research Department (CICYT) and inscribed asTIC96-1243-C03-02I When the model is obtained from annotated corporawe talk about supervised learning, when it is obtainedfrom raw corpora training is considered unsupervised.Aha  et al, 1991) to genetic algorithm strategies(Losee, 1994), through the transformation-basederror-driven algorithm used in (Brill, 1995), Stillanother possibility are the hybrid models, which tryto join the advantages of both approaches (Vouti-lainen and Padr6, 1997).We present in this paper a hybrid approach thatputs together both trends in automatic approachand the linguistic approach.
We describe a POS tag-ger based on the work described in (Padr6, 1996),that is able to use bi/tr igram information, auto-matically learned context constraints and linguisti-cally motivated manually written constraints.
Thesources and kinds of constraints are unrestricted,and the language model can be easily extended.
Thestructure of the tagger is presented in figure 1.Language Model.
I~:.i:;:;~: I / le~ed | t wri.e.
| ...l i.wco usFigure h Tagger architecture.CorpusWe also present a constraint-acquisition algo-rithm that uses statistical decision trees to learn con-text constraints from annotated corpora and we usethe acquired constraints to feed the POS tagger.The paper is organized as follows.
In section 2 wedescribe our language model, in section 3 we describethe constraint acquisition algorithm, and in section4 we expose the tagging algorithm.
Descriptions ofthe corpus used, the experiments performed and theresults obtained can be found in sections 5 and 6.2 Language Mode lWe will use a hybrid language model consisting of anautomatically acquired part and a linguist-writtenpart.238The automatically acquired part is divided in twokinds of information: on the one hand, we have bi-grams and trigrams collected from the annotatedtraining corpus (see section 5 for details).
On theother hand, we have context constraints learnedfrom the same training corpus using statistical deci-sion trees, as described in section 3.The linguistic part is very small --since there wereno available resources to develop it further--  andcovers only very few cases, but it is included to il-lustrate the flexibility of the algorithm.A sample rule of the linguistic part:i0 .0  (XvauxiliarY.
)(-\[VBN IN , : J J  JJS J JR\])+<VBN> ;This rule states that a tag past participle (VBN)  isvery compatible (10.0) with a left context consistingof a %vaux i l i a r% (previously defined macro whichincludes all forms of "have" and "be") provided thatall the words in between don't have any of the tagsin the set \ [VBN IN  , : J J  J J S  J JR\] .
That is,this rule raises the support for the tag past partici-ple when there is an auxiliary verb to the left butonly if there is not another candidate to be a pastparticiple or an adjective inbetween.
The tags \[IN, :\] prevent the rule from being applied when theauxiliary verb and the participle are in two differentphrases (a comma, a colon or a preposition are con-sidered to mark the beginning of another phrase).The constraint language is able to express thesame kind of patterns than the Constraint Gram-mar formalism (Karlsson et al, 1995), although in adifferent formalism.
In addition, each constraint hasa compatibility value that indicates its strength.
Inthe middle run, the system will be adapted to acceptCGs.3 Const ra in t  Acqu is i t ionChoosing, from a set of possible tags, the proper syn-tactic tag for a word in a particular context can beseen as a problem of classification.
Decision trees,recently used in NLP basic tasks such as taggingand parsing (McCarthy and Lehnert, 1995: Daele-mans et al, 1996; Magerman, 1996), are suitable forperforming this task.A decision tree is a n-ary branching tree that rep-resents a classification rule for classifying the objectsof a certain domain into a set of mutually exclusiveclasses.
The domain objects are described as a setof attribute-value pairs, where each attribute mea-sures a relevant feature of an object taking a (ideallysmall) set of discrete, mutually incompatible values.Each non-terminal node of a decision tree representsa question on (usually) one attribute.
For each possi-ble value of this attribute there is a branch to follow.Leaf nodes represent concrete classes.Classify a new object with a decision tree is simplyfollowing the convenient path through the tree untila leaf is reached.Statistical decision trees only differs from commondecision trees in that leaf nodes define a conditionalprobability distribution on the set of classes.It is important o note that decision trees can bedirectly translated to rules considering, for each pathfrom the root to a leaf, the conjunction of all ques-tions involved in this path as a condition and theclass assigned to the leaf as the consequence.
Statis-tical decision trees would generate rules in the samemanner but assigning a certain degree of probabilityto each answer.So the learning process of contextual constraintsis performed by means of learning one statistical de-cision tree for each class of POS ambiguity -~ and con-verting them to constraints (rules) expressing com-patibility/incompatibility of concrete tags in certaincontexts.Learn ing  A lgor i thmThe algorithm we used for constructing the statisti-cal decision trees is a non-incremental supervisedlearning-from-examples algorithm of the TDIDT(Top Down Induction of Decision Trees) family.
Itconstructs the trees in a top-down way, guided bythe distributional information of the examples, butnot on the examples order (Quinlan, 1986).
Briefly.the algorithm works as a recursive process that de-parts from considering the whole set of examples atthe root level and constructs the tree ina  top-downway branching at any non-terminal node accordingto a certain selected attribute.
The different val-ues of this attribute induce a partition of the setof examples in the corresponding subsets, in whichthe process is applied recursively in order to gener-ate the different subtrees.
The recursion ends, in acertain node, either when all (or almost all) the re-maining examples belong to the same class, or whenthe number of examples is too small.
These nodesare the leafs of the tree and contain the conditionalprobability distribution, of its associated subset, ofexamples, on the possible classes.The heuristic function for selecting the mostuseful attribute at each step is of a cru-cial importance in order to obtain simple trees,since no backtracking is performed.
There ex-ist two main families of attribute-selecting func-tions: information-based (Quinlan, 1986: Ldpez,1991) and statistically--based (Breiman et al, 1984;Mingers, 1989).Training SetFor each class of POS ambiguity the initial exam-ple set is built by selecting from the training corpusClasses of ambiguity are determined by the groupsof possible tags for the words in the corpus, i.e, noun-adjective, noun-adjective-verb, preposition-adverb, etc.239all the occurrences of the words belonging to thisambiguity class.
More particularly, the set of at-tributes that describe each example consists of thepart-of-speech tags of the neighbour words, and theinformation about the word itself (orthography andthe proper tag in its context).
The window consid-ered in the experiments reported in section 6 is 3words to the left and 2 to the right.
The follow-ing are two real examples from the training set forthe words that can be preposition and adverb at thesame time (IN-RB conflict).VB DT NN <"as" ,IN> DT JJNN IN NN <"once",RB> VBN TOApproximately 90% of this set of examples is usedfor the construction of the tree.
The remaining 10%is used as fresh test corpus for the pruning process.Attribute Selection FunctionFor the experiments reported in section 6 we used aattribute selection function due to L6pez de Minta-ras (L6pez.
1991), which belongs to the information-based family.
Roughly speaking, it defines a distancemeasure between partitions and selects for branch-ing the attribute that generates the closest partitionto the correc* partaion, namely the one that joinstogether all the examples of the same class.Let X be aset of examples, C the set of classes andPc(X) the partition of X according to the values ofC.
The selected attribute will be the one that gen-erates the closest partition of X to Pc(X).
For thatwe need to define a distance measure between parti-tions.
Let PA(X) be the partition of X induced bythe values of attribute A.
The average informationof such partition is defined as follows:I(PA(X)) = - ~,  p(X,a) log,.p(X,a),aEPa(X)where p(X. a) is the probability for an element of Xbelonging to the set a which is the subset of X whoseexamples have a certain value for the attribute .4,and it is estimated bv the ratio ~ This average?
IX l  'information measure reflects the randomness of dis-tribution of the elements of X between the classes ofthe partition induced by .4..
If we consider now theintersection between two different partitions inducedby attributes .4 and B we obtainI (PA(X)  N PB(X))=- E Z p(X. aMb) log,.p(X, aAb).aEP.a(A'} bEPB;XIConditioned information of PB(X) given PA(X) iSI (PB(X)IPA(X))  =I( PA(X) M Ps(X))  - I (P~(X)) =- Z Z p(X, nb) log, p(X'anb) p(X,a)a~Pa(X  ~, bEPBtX  ~It is easy to show that the measured(Pa(.Y).
PB(X)) =\[(Ps(X)iPA(X)) + I(PA(X) IPB(X))is a distance.
Normalizing we obtaind(PA(X).PB(,\ ' ))d.,v(Pa(X).
PB(.V)) = I (Pa (X)aPB(X) )  "with values in \[0,1\].So the selected attribute will be that one that min-imizes the measure: d.v(Pc(X), PA(X)).Branching StrategyUsual TDIDT algorithms consider a branch for eachvalue of the selected attribute.
This strategy is notfeasible when the number of values is big (or even in-finite).
In our case the greatest number of values foran attribute is 45 --the tag set size-- which is con-siderably big (this means that the branching factorcould be 45 at every level of the tree 3).
Some s.vs-terns perform a previous recasting of the attributesin order to have only binary-valued attributes and todeal with binary trees (Magerman, 1996).
This canalways be done but the resulting features lose theirintuition and direct interpretation, and explode innumber.
We have chosen a mixed approach whichconsist of splitting for all values and afterwards join-ing the resulting subsets into groups for which wehave not enough statistical evidence of being differ-ent distributions.
This statistical evidence is testedwith a X ~" test at a 5% level of significance.
In orderto avoid zero probabilities the following smoothingis performed.
In a certain set of examples, the prob-ability of a tag ti is estimated byI~,l+-~ ri(4) = ,+~where m is the number of possible tags and n thenumber of examples.Additionally.
all the subsets that don't imply areduction in the classification error are joined to-gether in order to have a bigger set of examples tobe treated in the following step of the tree construc-tion.
The classification error of a certain node issimply: I - maxt<i<m (t)(ti)).Experiments reported in (.\I&rquezand Rodriguez.
1995) show that in this way morecompact and predictive trees are obtained.Pruning the TreeDecision trees that correctly classify all examples ofthe training set are not always the most predictiveones.
This is due to the phenomenon known as o,'er-fitting.
It occurs when the training set has a certainamount of misclassified examples, which is obviouslythe case of our training corpus (see section 5).
If we3In real cases the branching factor is much lower sincenot all tags appear always in all positions of the context.240force the learning algorithm to completely classifythe examples then the resulting trees would fit alsothe noisy examples.The usual solutions to this problem are: l) Prunethe tree.
either during the construction process(Quinlan.
1993) or afterwards (Mingers, 1989); 2)Smooth the conditional probability distributions us-ing fresh corpus a (Magerman, 1996).Since another important, requirement of our prob-lem is to have small trees we have implementeda post-pruning technique.
In a first step thetree is completely expanded and afterwards it ispruned following a minimal cost-complexity crite-rion (Breiman et al.
1984).
Roughly speaking thisis a process that iteratively cut those subtrees pro-ducing only marginal benefits in accuracy, obtainingsmaller trees at each step.
The trees of this sequenceare tested using a, comparatively small, fresh part ofthe training set in order to decide which is the onewith the highest degree of accuracy on new exam-ples.
Experimental tests (M&rquez and Rodriguez,1995) have shown that the pruning process reducestree sizes at about 50% and improves their accuracyin a 2-5%.An EzampleFinally, we present a real example of the simple ac-quired contextual constraints for the conflict IN -RB(preposition-adverb).P(IN)=0.$1 \] PnorprobabilityP(RB)=0.19 \[ di~tnbunonT.
.
.
~dngh lm~g er s U-"<C,,.dm,,.wl: P( IN)=0.013  ' ' ' probuiJilmdi.~tnbut.m P~RB~0.987Figure 2: Example of a decision tree branch,The tree branch in figure 2 is translated into thefollowing constraints:-5 .81  <\["as .
.
.
.
As"\],IN> (\[RB'I) (\[IN\]);2.366 <\["as .... As"\],RS> (\[RB\]) (\[IN\]);which express the compatibility (either positive ornegative) of the word-tag pair in angle brackets withthe given context.
The compatibility value for eachconstraint is the mutual information between the tagand the context (Cover and Thomas, 1991).
It isdirectly" computed from the probabilities in the tree.~Of course, this can be done only in the case of sta-tistical decision trees.4 Tagg ing  A lgor i thmUsual tagging algorithms are either n-gram oriented-such as Viterbi algorithm (Viterbi.
1967)- or ad-hoc for every case when they must deal with morecomplex information.We use relaxation labelling as a tagging algorithm.Relaxation labelling is a generic name for a familyof iterative algorithms which perform function opti-mization, based on local information.
See (Torras.1989) for a summary.
Its most remarkable feature isthat it can deal with any kind of constraints, thus themodel can be improved by adding any constraintsavailable and it makes the tagging algorithm inde-pendent of the complexity of the model.The algorithm has been applied to part-of-speechtagging (Padr6, 1996), and to shallow parsing(Voutilainen and Padro.
1997).The algorithm is described as follows:Let.
V = {Vl.t'2 .
.
.
.
.
v,} be a set of variables(words).Let ti = {t\].t~ .
.
.
.
.
t~,} be the set of possiblelabels (POS tags) for variable vi.Let CS be a set of constraints between the labelsof the variables.
Each constraint C E CS states a"compatibility value" C, for a combination of pairsvariable-label.
Any number of variables may be in-volved in a constraint.The aim of the algorithm is to find a weightedlabelling 5 such that "global consistency" is maxi-mized.
Maximizing "global consistency" is definedi is as maximizing for all vi, ~ i  P} x Sii, where pjthe weight for label j in variable vi and Sij the sup-port received by the same combination.
The supportfor the pair variable-label expresses how compatiblethat pair is with the labels of neighbouring variables.according to the constraint set.
It is a vector opti-mization and doesn't maximize only the sum of thesupports of all variables.
It finds a weighted labellingsuch that any other choice wouldn't increase the sup-port for any variable.The support is defined as the sum of the influenceof every constraint on a label.c.. Z In f ( r )r6R, jwhere:l~ij is the set of constraints on label j for variablei, i.e.
the constraints formed by any combination ofvariable-label pairs that includes the pair (ci.
t i ).In f ( r )  = C, x p~'t,"n) x ... x ,v~(m)..  is the prod-uct of the current weights ~ for the labels appearing5A weighted labelling is a weight assignment for eachlabel of each variable such that the weights for the labelsof the same variable add up to one.Gp~(rn) is the weight assigned to label k for variabler at time m.241in the constraint except (vi,t}) (representing howapplicable the constraint is in the current context)multiplied by Cr which is the constraint compatibil-ity value (stating how compatible the pair is with thecontext).Briefly, what the algorithm does is:i.
Start with a random weight assignment r.2.
Compute the support value for each label ofeach variable.3.
Increase the weights of the labels more compat-ible with the context (support greater than 0)and decrease those of the less compatible labels(support less than 0) s, using the updating func-tion:i (m + 1) = p~(m) ?
(1 + s~j)PJ I~,Zp~(m ) x (i + Sit:)k=lwhere - l<S i j  <_+14.
If a stopping/convergence criterion 9 is satisfied,stop, otherwise go to step 2.The cost of the algorithm is proportional to theproduct of the number of words by the number ofconstraints.5 Descr ip t ion  o f  the  corpusWe used the Wall Street Journal corpus to train andtest the system.
We divided it in three parts: 1,100Kw were used as a training set, 20 Kw as a model-tuning set, and 50 Kw as a test set.The tag set size is 45 tags.
36.4% of the words inthe corpus are ambiguous, and the ambiguity ratiois 2.44 tags/word over the ambiguous words, 1.52overall.We used a lexicon derived from training corpora,that contains all possible tags for a word, as wellas their lexical probabilities.
For the words in testcorpora not appearing in the train set, we storedall possible tags, but no lexical probability (i.e.
weassume uniform distribution) l?.The noise in the lexicon was filtered by manuallychecking the lexicon entries for the most frequent 200words in the corpus 11 to eliminate the tags due toerrors in the training set.
For instance the originalZWe use lexical probabilities as a starting point.SNegative values for support indicate incompatibility.9We use the criterion of stopping when there are nomore changes, although more sophisticated heuristic pro-cedures are also used to stop relaxation processes (Ek-lundh and Rosenfeld, 1978; Richards et hi.
, 1981).1?That is, we assumed a morphological analyzer thatprovides all possible tags for unknown words.l~The 200 most frequent words in the corpus coverover half of it.lexicon entry (numbers indicate frequencies in thetraining corpus) for the very common word the was~he CD i DT 47715 JJ 7 NN I NNP 6 VBP 1since it appears in the corpus with the six differ-ent tags: CD (cardinal), DT (determiner), JJ (ad-jective), NN (noun).
NNP (proper noun) and VBP(verb-personal form).
It is obvious that the onlycorrect reading for the is determiner.The training set was used to estimate bi/trigramstatistics and to perform the constraint learning.The model-tuning set was used to tune the algo-rithm parameterizations, and to write the linguisticpart of the model.The resulting models were tested in the fresh testset.6 Exper iments  and  resu l t sThe whole WSJ corpus contains 241 different classesof ambiguity.
The 40 most representative classes t-"were selected for acquiring the corresponding deci-sion trees.
That produced 40 trees totaling up to2995 leaf nodes, and covering 83.95% of the ambigu-ous words.
Given that each tree branch produces asmany constraints as tags its leaf involves, these treeswere translated into 8473 context constraints.We also extracted the 1404 bigram restrictionsand the 17387 trigram restrictions appearing in thetraining corpus.Finally, the model-tuning set was tagged usinga bigram model.
The most common errors com-mited by the bigram tagger were selected for manu-ally writing the sample linguistic part of the model,consisting of a set of 20 hand-written constraints.From now on C will stands for the set of acquiredcontext constraints.
B for the bigram model, T forth.e trigram model, and H for the hand-written con-straints.
Any combination of these letters will indi-cate the joining of the corresponding models (BT,BC, BTC,  etc.
).In addition, ML indicates a baseline model con-raining no constraints (this will result in a most-likely tagger) and HMM stands for a hiddenMarkov model bigram tagger (Elworthy, 1992).We tested the tagger on the 50 Kw test set usingall the combinations ofthe language models.
Resultsare reported below.The effect of the acquired rules on the number oferrors for some of the most common cases is shownin table 1.
XX/Y'Y stands for an error consistingof a word tagged ~t%_" when it should have been XX.Table 2 contains the meaning of all the involved tags.Figures in table 1 show that in all cases the learnedconstraints led to an improvement.It is remarkable that when using C alone, thenumber of errors is lower than with any bigram12In terms of number of examples.242J J /NN+NN/J JVBD/VBN+VBN/VBDIN/RB+RB/INVB/VBP+VBP/VBNN/NNP+NNP/NNNNP/NNPS+NNPS/NNP"'that" 187TotalML C B73+137 70+94 73+112176+190 71+66 88+6931+132 40+69 66+107128+147 30+26 49+4370+11 44+12 72+1745+14 37+19 45+1353 66BC69+10263+5643+1732+2745+1646+1545T I TC57+103 \[ 61+9556+57 55+5777+68 47+6731+32 32+1869+27 50+1854+12 51+1260 I 40BT \ [  BTC67+101 t 62+9365+60 59+6165+98 46-z-8328+32 ') ' ' '} .8,--3.71+20 62+t.553+14 51+1457 .
451341 it 631 II 82?1 630 II 7o3!
603 731 ~s51 iTable 1: Number of some common errors commited by each modelNNJJVBDVBNRBINVBVBPNNPNNPSNoun \[ I ambiguousAdjective B 91.35%Verb - past.
tense T 91.82%'verb - past participle BT 91.92%AdverbPreposition B C 91.96%Verb - base form C 92.72%Verb - personal form TC 92.82%Proper noun BTC 92.55%Plural proper noun Table 4: Results of ourTable 2: Tag meanings of constraint kindsand/or trigram model, that is, the acquired modelperforms better than the others estimated from thesame training corpus.We also find that the cooperation of a bigram ortrigram model with the acquired one, produces evenbetter results.
This is not true in the cooperationof bigrams and trigrams with acquired constraints(BTC) ,  in this case the synergy is not enough to geta better joint result.
This might be due to the factthat the noise in B and T adds up and overwhelmsthe context constraints.The results obtained by the baseline taggers canbe found in table 3 and the results obtained using allthe learned constraints together with the bi/tr igrammodels in table 4.\] ambiguous I overallML \[ 85.31%194.66%HMM 91.75% 97.00%Table 3: Results of the baseline taggersOn the one hand.
the results in tables 3 and 4show that our tagger performs lightly worse than aHMM tagger in the same conditions 13, that is, whenusing only bigram information.13Hand analysis of the errors commited by the algo-rithm suggest hat the worse results may be due to noisein the training and test corpora, i.e., relaxation algo-rithm seems to be more noise-sensitive than a Markovmodel.
Further research is required on this point.overall96.86%97.03%97.06%97.08%97.36%97.39%97.29%tagger using every combinationOn the other hand, those results also show thatsince our tagger is more flexible than a HMM, it caneasily accept more complex information to improveits results up to 97.39% without modifying the algo-rithm.I I ambigu?usH 86.41%BH 91.88%TH 92.04%BTH 92.32%CH 91.97%BCH 92.76%TCH 92.98%BTCH 92.71%overall95.06%97.05%97.11%97.21%97.08%97.37%97.45%97.35%Table .5: Results of our tagger using every combinationof constraint kinds and hand written constraintsTable 5 shows the results adding the hand writtenconstraints.
The hand written set is very small andonly covers a few common error cases.
That pro-duces poor results when using them alone (H).
butthey are good enough to raise the results given bythe automatically acquired models up to 97.-15%.Although the improvement obtained might seemsmall, it must be taken into .account that we aremoving very close to the best achievable result withthese techniques.First, some ambiguities can only be solved withsemantic information, such as the Noun-Adjectiveambiguity for word principal in the phrase lhe prin-cipal office.
It could be an adjective, meaning the243main office, or a noun, meaning the school head of-rice,Second, the WSJ corpus contains noise (mistaggedwords) that affects both the training and the testsets.
The noise in the training set produces noisy-and so less precise- models.
In the test set, it pro-duces a wrong estimation of accuracy, since correctanswers are computed as wrong and vice-versa.For instance, verb participle forms are sometimestagged as such (VBIV) and also as adjectives (J  J) inother sentences with no structural differences:?
.
.
.
fail ing_VBG ~o_TO voluntarily_KBsubmi t_VB the_DT reques~ed_VBNi n fo rma%ion .NN .
.
.?
.
.
.
a_DT large_JJ sample_NN of_INmarried_JJ women_NNS with_IN at_II~least_J JS one_CD child..gN .
.
.Another structure not coherently tagged are nounchains when the nouns are ambiguous and can be?
also adjectives:?
.
.
.
Mr._NNP Hahn_NNP ,_, the_DT62-year -o ld_ J J  cha i rman_NN and_CCchief_NN executive_JJ officer_NN of_INGeorgia-Pacif ic_~NP Corp._NNP .
.
.?
.
.
.
Burger_NgP King_~NP's_POS chief_JJ ezecutive_NN officer_NN ,_,Barry_NNP Gibbons_NNP ,_, stars_VBZin lN  ads_NNS saying_VBG .
.
.?
... and_CC Barrett_NNP B._NNPWeekes_NNP ,_, cha i rma~t -NN ,_,p res ident_NN and_CC chief_JJ ezecutive_JJofficer_NN .
_.?
... the_DT compaay_NN includes_VBZNeiI_NNP Davenport_NNP ,_, 47_CD ,_,president_NN and_CC chief_NN ezecu~ive_NNofficer_NN ;_:All this makes that the performance cannot reach100%, and that an accurate analysis of the noise inWS3 corpus should be performed to estimate theactual upper bound that a tagger can achieve onthese data.
This issue will be addressed in furtherwork.7 Conc lus ionsWe have presented an automatic onstraint learningalgorithm based on statistical decision trees.We have used the acquired constraints in a part-of-speech tagger that allows combining any kind ofconstraints in the language model.The results obtained show a clear improvement inthe performance when the automatically acquiredconstraints are added to the model.
That indicatesthat relaxation labelling is a flexible algorithm ableto combine properly different information kinds, andthat the constraints acquired by the learning algo-rithm capture relevant context information that wasnot included in the n-gram models.It is difficult to compare the results to other works,since the accuracy varies greatly depending on thecorpus, the tag set, and the lexicon or morphologicalanalyzer used.
The more similar conditions reportedin previous work are those experiments performedon the WSJ corpus: (Brill, 1992) reports 3-4% er-ror rate, and (Daelemans et al, 1996) report 96.7%accuracy.
We obtained a 97.39% accuracy with tri-grams plus automatically acquired constraints, and97.45% when hand written constraints were added.8 Fur ther  WorkFurther work is still to be done in the following di-rections:?
Perform a thorough analysis of the noise inthe WSJ corpus to determine a realistic upper?
bound for the performance that can be expectedfrom a POS tagger.On the constraint learning algorithm:?
Consider more complex context features, suchas non-limited istance or barrier rules in thestyle of (Samuelsson et al, 1996).?
Take into account morphological, semantic andother kinds of information.?
Perform a global smoothing to deal with low-frequency ambiguity classes.On the tagging algorithms?
Study the convergence properties of the algo-rithm to decide whether the lower results atconvergence are produced by the noise in thecorpus.?
Use back-off techniques to minimize inter-ferences between statistical and learned con-straints.?
Use the algorithm to perform simultaneouslyPOS tagging and word sense disambiguation,to take advantage of cross influences betweenboth kinds of information.Re ferencesD.W.
Aha, D. Kibler and M. Albert.
1991 Instance-based learning algorithms.
In Machine Learning.7:37-66.
Belmont, California.L.
Breiman, J.H.
Friedman, R.A. Olshen andC.J.
Stone.
1984 Classification and RegressionTrees.
The Wadsworth Statistics/Probability Se-ries.
Wadsworth International Group, Behnont,California.244E.
Brill.
1992 A Simple Rule-Based Part-of-Speech.In Proceedings of the Third Conference on AppliedNatural Language Processing.
ACL.E.
Brill.
1995 Unsupervised Learning of Disam-biguation Rules for Part-of-speech Tagging.
InProceedings of 3rd Workshop on Very Large Cor-pora.
Massachusetts.T.M.
Cover and J.A.
Thomas (Editors) 1991 Ele-ments of information theory.
John Wiley & Sons.D.
Cutting, J. Kupiec, J. Pederson and P. Sibun.1992 A Practical Part-of-Speech Tagger.
In Pro-ceedings of the Third Conference on Applied Nat-ural Language Processing., ACL.J.
Eklundh and A. Rosenfeld.
1978 ConvergenceProperties of Relaxation Labelling.
Technical Re-port no.
701.
Computer Science Center.
Univer-sity of Maryland.D.
Elworthy.
1993 Part-of-Speech and PhrasalTagging.
Technical report, SPRIT BRA-7315 Ac-quilex II, Working Paper WP #10.W.
Daelemans, J. Zavrel, P. Berck and S. Gillis.1996 MTB: A Memory-Based Part-of-SpeechTagger Generator.
In Proceedings of ~th Work-shop on Very Large Corpora.
Copenhagen, Den-mark.R.
Garside, G. Leech and G. Sampson (Editors)1987 The Computational Analysis of English.London and New York: Longman.D.
Hindle.
1989 Acquiring disambiguation rulesfrom text.
In Proceedings ACL'89.F.
Karlsson 1990 Constraint Grammar as a Frame-work for Parsing Running Text.
In H.
Karlgren(ed.
), Papers presented to the 13th InternationalConference on Computational Linguistics, Vol.
3.Helsinki.
168-173.F.
Karlsson, A. Voutilainen, J. Heikkil~ andA.
Anttila.
(Editors) 1995 Constraint Grammar:A Language-Independent System for Parsing Un-restricted Tezt.
Mouton de Gruyter, Berlin andNew York.R.
L6pez.
1991 A Distance-Based Attribute Selec-tion Measure for Decision Tree Induction.
Ma-chine Learning.
Kluwer Academic.R.M.
Losee.
1994 Learning Syntactic Rules andTags with Genetic Algorithms for InformationRetrieval and Filtering: An Empirical Basis forGrammatical Rules.
Information Processing &Management, May.M.
Magerman.
1996 Learning Grammatical Struc-ture Using Statistical Decision-Trees.
In LectureNotes in Artificial Intelligence 11~7.
GrammaticalInference: Learning Syntax from Sentences.
Pro-ceedings ICGI-96.
Springer.L.
M?rquez and H. Rodriguez.
1995 Towards Learn-ing a Constraint Grammar from Annotated Cor-pora Using Decision Trees.
ESPRIT BRA-7315Acquilex II, Working Paper.J.F.
McCarthy and W.G.
Lehnert.
1995 Using De-cision Trees for Coreference Resolution.
In Pro-ceedings of l~th International Joint Conference onArtificial Intelligence (IJCAI'95).J.
Mingers.
1989 An Empirical Comparison of Se-lection Measures for Decision-Tree Induction.
InMachine Learning.
3:319-342.J.
Mingers.
1989 An Empirical Comparison of Prun-ing Methods for Decision-Tree Induction.
In Ma-chine Learning.
4:227-243.L.
Padr6.
1996 POS Tagging Using RelaxationLabelling.
In Proceedings of 16th InternationalConference on Computational Linguistics.
Copen-hagen, Denmark.J.R.
Quinlan.
1986 Induction of Decision Trees.
InMachine Learning.
1:81-106.J.R.
Quinlan.
1993 C4.5: Programs for MachineLearning.
San Mateo, CA.
Morgan Kaufmann.3.
Richards, D. Landgrebe and P. Swain.
1981 Onthe accuracy of pixel relaxation labelling.
IEEETransactions on System, Man and Cybernetics.Vol.
SMC-11C.
Samuelsson, P. Tapanainen and A. Voutilainen.1996 Inducing Constraint Grammars.
In Pro-ceedings of the 3rd International Colloquium onGrammatical Inference.H.
Schmid 1994 Part-of-speech tagging with neu-ral networks.
In Proceedings of 15th InternationalConference on Computational Linguistics.
Kyoto,Japan.C.
Torras.
1989 Relaxation and Neural Learning:Points of Convergence and Divergence.
Journalof Parallel and Distributed Computing.
6:217-244A.J.
Viterbi.
1967 Error bounds for convolutionalcodes and an asymptotically optimal decoding al-gorithm.
In IEEE Transactions on InformationTheory.
pg 260-269, April.A.
Voutilainen and T. J~rvinen.
1995 Specifyinga shallow grammatical representation for parsingpurposes.
In Proceedings of the 7th meeting of theEuropean Association for Computational Linguis-tics.
210-214.A.
Voutilainen and L. Padr6.
1997 Developing aHybrid NP Parser.
In Proceedings of ANLP'97.245
