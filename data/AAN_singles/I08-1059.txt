Using Contextual Speller Techniques and Language Modeling forESL Error CorrectionMichael Gamon*, Jianfeng Gao*, Chris Brockett*, Alexandre Klementiev+, WilliamB.
Dolan*, Dmitriy Belenko*, Lucy Vanderwende**Microsoft ResearchOne Microsoft WayRedmond, WA 98052{mgamon,jfgao,chrisbkt,billdol,dmitryb,lucyv}@microsoft.com+Dept.
of Computer ScienceUniversity of IllinoisUrbana, IL 61801klementi@uiuc.eduAbstractWe present a modular system for detectionand correction of errors made by non-native (English as a Second Language =ESL) writers.
We focus on two error types:the incorrect use of determiners and thechoice of prepositions.
We use a decision-tree approach inspired by contextualspelling systems for detection andcorrection suggestions, and a largelanguage model trained on the Gigawordcorpus to provide additional information tofilter out spurious suggestions.
We showhow this system performs on a corpus ofnon-native English text and discussstrategies for future enhancements.1 IntroductionEnglish is today the de facto lingua franca forcommerce around the globe.
It has been estimatedthat about 750M people use English as a secondlanguage, as opposed to 375M native Englishspeakers (Crystal 1997), while as much as 74% ofwriting in English is done by non-native speakers.However, the errors typically targeted bycommercial proofing tools represent only a subsetof errors that a non-native speaker might make.
Forexample, while many non-native speakers mayencounter difficulty choosing among prepositions,this is typically not a significant problem for nativespeakers and hence remains unaddressed inproofing tools such as the grammar checker inMicrosoft Word (Heidorn 2000).
Plainly there is anopening here for automated proofing tools that arebetter geared to the non-native users.One challenge that automated proofing toolsface is that writing errors often present a semanticdimension that renders it difficult if not impossibleto provide a single correct suggestion.
The choiceof definite versus indefinite determiner?acommon error type among writers with a Japanese,Chinese or Korean language background owing tothe lack of overt markers for definiteness andindefiniteness?is highly dependent on largertextual context and world knowledge.
It seemsdesirable, then, that proofing tools targeting sucherrors be able to offer a range of plausiblesuggestions, enhanced by presenting real-worldexamples that are intended to inform a user?sselection of the most appropriate wording in thecontext1.2 Targeted Error TypesOur system currently targets eight different errortypes:1.
Preposition presence and choice:In the other hand, ... (On the other hand ...)2.
Definite and indefinite determiner presenceand choice:I am teacher... (am a teacher)3.
Gerund/infinitive confusion:I am interesting in this book.
(interested in)4.
Auxiliary verb presence and choice:My teacher does is a good teacher (my teacheris...)1 Liu et al 2000 take a similar approach, retrievingexample sentences from a large corpus.4495.
Over-regularized verb inflection:I writed a letter (wrote)6.
Adjective/noun confusion:This is a China book (Chinese book)7.
Word order (adjective sequences and nominalcompounds):I am a student of university (university student)8.
Noun pluralization:They have many knowledges (much knowledge)In this paper we will focus on the two mostprominent and difficult errors: choice ofdeterminer and prepositions.
Empiricaljustification for targeting these errors comes frominspection of several corpora of non-native writing.In the NICT Japanese Learners of English (JLE)corpus (Izumi et al 2004), 26.6% of all errors aredeterminer related, and about 10% are prepositionrelated, making these two error types the dominantones in the corpus.
Although the JLE corpus isbased on transcripts of spoken language, we haveno reason to believe that the situation in writtenEnglish is substantially different.
The ChineseLearners of English Corpus (CLEC, Gui and Yang2003) has a coarser and somewhat inconsistenterror tagging scheme that makes it harder to isolatethe two errors, but of the non-orthographic errors,more than 10% are determiner and number related.Roughly 2% of errors in the corpus are tagged aspreposition-related, but other preposition errors aresubsumed under the ?collocation error?
categorywhich makes up about 5% of errors.3 Related WorkModels for determiner and preposition selectionhave mostly been investigated in the context ofsentence realization and machine translation(Knight and Chander 1994, Gamon et al 2002,Bond 2005, Suzuki and Toutanova 2006,Toutanova and Suzuki 2007).
Such approachestypically rely on the fact that preposition ordeterminer choice is made in otherwise native-likesentences.
Turner and Charniak (2007), forexample, utilize a language model based on astatistical parser for Penn Tree Bank data.Similarly, De Felice and Pulman (2007) utilize aset of sophisticated syntactic and semantic analysisfeatures to predict 5 common English prepositions.Obviously, this is impractical in a setting wherenoisy non-native text is subjected to proofing.Meanwhile, work on automated error detection onnon-native text focuses primarily on detection oferrors, rather than on the more difficult task ofsupplying viable corrections (e.g., Chodorow andLeacock, 2000).
More recently,  Han et al (2004,2006) use a maximum entropy classifier to proposearticle corrections in TESOL essays, while Izumiet al (2003) and Chodorow et al (2007) presenttechniques of automatic preposition choicemodeling.
These more recent efforts, nevertheless,do not attempt to integrate their methods into amore general proofing application designed toassist non-native speakers when writing English.Finally, Yi et al (2008) designed a system thatuses web counts to determine correct article usagefor a given sentence, targeting ESL users.4 System DescriptionOur system consists of three major components:1.
Suggestion Provider (SP)2.
Language Model (LM)3.
Example Provider (EP)The Suggestion Provider contains modules foreach error type discussed in section 2.
Sentencesare tokenized and part-of-speech tagged beforethey are presented to these modules.
Each moduledetermines parts of the sentence that may containan error of a specific type and one or more possiblecorrections.
Four of the eight error-specificmodules mentioned in section 2 employ machinelearned (classification) techniques, the other fourare based on heuristics.
Gerund/infinitiveconfusion and auxiliary presence/choice each use asingle classifier.
Preposition and determinermodules each use two classifiers, one to determinewhether a preposition/article should be present,and one for the choice of preposition/article.All suggestions from the Suggestion Providerare collected and passed through the LanguageModel.
As a first step, a suggested correction hasto have a higher language model score than theoriginal sentence in order to be a candidate forbeing surfaced to the user.
A second set ofheuristic thresholds is based on a linearcombination of class probability as assigned by theclassifier and language model score.The Example Provider queries the web forexemplary sentences that contain the suggestedcorrection.
The user can choose to consult thisinformation to make an informed decision aboutthe correction.4504.1 Suggestion Provider Modules forDeterminers and PrepositionsThe SP modules for determiner and prepositionchoice are machine learned components.
Ideally,one would train such modules on large data sets ofannotated errors and corrected counterparts.
Such adata set, however, is not currently available.
As asubstitute, we are using native English text fortraining, currently we train on the full text of theEnglish Encarta encyclopedia (560k sentences) anda random set of 1M sentences from a Reuters newsdata set.
The strategy behind these modules issimilar to a contextual speller as described, forexample, in (Golding and Roth 1999).
For eachpotential insertion point of a determiner orpreposition we extract context features within awindow of six tokens to the right and to the left.For each token within the window we extract itsrelative position, the token string, and its part-of-speech tag.
Potential insertion sites are determinedheuristically from the sequence of POS tags.
Basedon these features, we train a classifier forpreposition choice and determiner choice.Currently we train decision tree classifiers with theWinMine toolkit (Chickering 2002).
We alsoexperimented with linear SVMs, but decision treesperformed better overall and training andparameter optimization were considerably moreefficient.
Before training the classifiers, weperform feature ablation by imposing a countcutoff of 10, and by limiting the number of featuresto the top 75K features in terms of log likelihoodratio (Dunning 1993).We train two separate classifiers for bothdeterminers and preposition:?
decision whether or not adeterminer/preposition should be present(presence/absence or pa classifier)?
decision which determiner/preposition isthe most likely choice, given that adeterminer/preposition is present (choiceor ch classifier)In the case of determiners, class values for the chclassifier are a/an and the.
Preposition choice(equivalent to the ?confusion set?
of a contextualspeller) is limited to a set of 13 prepositions thatfigure prominently in the errors observed in theJLE corpus: about, as, at, by, for, from, in, like, of,on, since, to, with, than, "other" (for prepositionsnot in the list).The decision tree classifiers produce probabilitydistributions over class values at their leaf nodes.For a given leaf node, the most likelypreposition/determiner is chosen as a suggestion.
Ifthere are other class values with probabilitiesabove heuristically determined thresholds2, thoseare also included in the list of possible suggestions.Consider the following example of an article-related error:I am teacher from Korea.As explained above, the suggestion providermodule for article errors consists of two classifiers,one for presence/absence of an article, the other forarticle choice.
The string above is first tokenizedand then part-of-speech tagged:0/I/PRP   1/am/VBP   2/teacher/NN   3/from/IN4/Korea/NNP   5/./.Based on the sequence of POS tags andcapitalization of the nouns, a heuristic determinesthat there is one potential noun phrase that couldcontain an article: teacher.
For this possible articleposition, the article presence/absence classifierdetermines the probability of the presence of anarticle, based on a feature vector of pos tags andsurrounding lexical items:p(article + teacher) = 0.54Given that the probability of an article in thisposition is higher than the probability of not havingan article, the second classifier is consulted toprovide the most likely choice of article:p(the) = 0.04p(a/an) = 0.96Given  this probability distribution, a correctionsuggestion I am teacher from Korea -> I am ateacher from Korea is generated and passed on toevaluation by the language model component.4.2 The Language ModelThe language model is a 5-gram model trainedon the English Gigaword corpus (LDC2005T12).In order to preserve (singleton) context informationas much as possible, we used interpolated Kneser-Ney smoothing (Kneser and Ney 1995) withoutcount cutoff.
With a 120K-word vocabulary, thetrained language model contains 54 millionbigrams, 338 million trigrams, 801 million 4-grams2 Again, we are working on learning these thresholdsempirically from data.451and 12 billion 5-grams.
In the example from theprevious section, the two alternative strings  of theoriginal user input and the suggested correction arescored by the language model:I am teacher from Korea.
score = 0.19I am a teacher from Korea.
score = 0.60The score for the suggested correction issignificantly higher than the score for the original,so the suggested correction is provided to the user.4.3 The Example ProviderIn many cases, the SP will produce severalalternative suggestions, from which the user maybe able to pick the appropriate correction reliably.In other cases, however, it may not be clear whichsuggestion is most appropriate.
In this event, theuser can choose to activate the Example Provider(EP) which will then perform a web search toretrieve relevant example sentences illustrating thesuggested correction.
For each suggestion, wecreate an exact string query including a smallwindow of context to the left and to the right of thesuggested correction.
The query is issued to asearch engine, and the retrieved results areseparated into sentences.
Those sentences thatcontain the string query are added to a list ofexample candidates.
The candidates are thenranked by two initially implemented criteria:Sentence length (shorter examples are preferred inorder to reduce cognitive load) and context overlap(sentences that contain additional words from theuser input are preferred).
We have not yetperformed a user study to evaluate the usefulnessof the examples provided by the system.
Someexamples of usage that we retrieve are given belowwith the query string in boldface:Original: I am teacher from Korea.Suggestion: I am a teacher from Korea.All top 3 examples: I am a teacher.Original: So Smokers have to see doctor more oftenthan non-smokers.Suggestion: So Smokers have to see a doctor moreoften than non-smokers.Top 3 examples:1.
Do people going through withdrawal haveto see a doctor?2.
Usually, a couple should wait to see adoctor until after they've tried to getpregnant for a year.3.
If you have had congestion for over aweek, you should see a doctor.Original: I want to travel Disneyland in March.Suggestion: I want to travel to Disneyland inMarch.Top 3 examples:1.
Timothy's wish was to travel toDisneyland in California.2.
Should you travel to Disneyland inCalifornia or to Disney World inFlorida?3.
The tourists who travel to Disneyland inCalifornia can either choose to stay inDisney resorts or in the hotel forDisneyland vacations.5 EvaluationWe perform two different types of evaluation onour system.
Automatic evaluation is performed onnative text, under the assumption that the nativetext does not contain any errors of the type targetedby our system.
For example, the original choice ofpreposition made in the native text would serve assupervision for the evaluation of the prepositionmodule.
Human evaluation is performed on non-native text, with a human rater assessing eachsuggestion provided by the system.5.1 Individual SP ModulesFor evaluation, we split the original training datadiscussed in section 4.1 into training and test sets(70%/30%).
We then retrained the classifiers onthis reduced training set and applied them to theheld-out test set.
Since there are two models, onefor preposition/determiner presence and absence(pa), and one for preposition/determiner choice(ch), we report combined accuracy numbers of thetwo classifiers.
Votes(a) stands for the counts ofvotes for class value = absence from pa, votes(p)stands for counts of votes for presence from pa.Acc(pa) is the accuracy of the pa classifier, acc(ch)the accuracy of the choice classifier.
Combinedaccuracy is defined as in Equation 1.???
??
?
?????(?)
+ ???
??
?
???
??
?
?????(?)?????
????
?Equation 1: Combined accuracy of thepresence/absence and choice models452The total number of cases in the test set is1,578,342 for article correction and 1,828,438 forpreposition correction.5.1.1 Determiner choiceAccuracy of the determiner pa and ch modelsand their combination is shown in Table 1.Model pa ch combinedAccuracy 89.61% 85.97% 86.07%Table 1: Accuracy of the determiner pa, ch, andcombined models.The baseline is 69.9% (choosing the mostfrequent class label none).
The overall accuracy ofthis module is state-of-the-art compared withresults reported in the literature (Knight andChander 1994, Minnen et al 2000, Lee 2004,Turner and Charniak 2007).
Turner and Charniak2007 obtained the best reported accuracy to date of86.74%, using a Charniak language model(Charniak 2001) based on a full statistical parseron the Penn Tree Bank.
These numbers are, ofcourse, not directly comparable, given the differentcorpora.
On the other hand, the distribution ofdeterminers is similar in the PTB (as reported inMinnen et al 2000) and in our data (Table 2).PTB Reuters/Encartamixno determiner 70.0% 69.9%the 20.6% 22.2%a/an 9.4% 7.8%Table 2: distribution of determiners in the PennTree Bank and in our Reuters/Encarta data.Precision and recall numbers for both models onour test set are shown in Table 3 and Table 4.Articlepa classifierprecision recallpresence 84.99% 79.54%absence 91.43% 93.95%Table 3: precision and recall of the article paclassifier.Articlech classifierprecision Recallthe 88.73% 92.81%a/an 76.55% 66.58%Table 4: precision and recall of the article chclassifier.5.1.2 Preposition choiceThe preposition choice model and the combinedmodel achieve lower accuracy than thecorresponding determiner models, a result that canbe expected given the larger choice of candidatesand hardness of the task.
Accuracy numbers arepresented in Table 5.Model pa ch combinedAccuracy 91.06%% 62.32% 86.07%Table 5:Accuracy of the preposition pa, ch, andcombined models.The baseline in this task is 28.94% (using nopreposition).
Precision and recall numbers areshown in Table 6 and Table 7.
From Table 7 it isevident that prepositions show a wide range ofpredictability.
Prepositions such as than and aboutshow high recall and precision, due to the lexicaland morphosyntactic regularities that govern theirdistribution.
At the low end, the semantically moreindependent prepositions since and at show muchlower precision and recall numbers.Prepositionpa classifierprecision recallpresence 90.82% 87.20%absence 91.22% 93.78%Table 6: Precision and recall of the preposition paclassifier.Prepositionch classifierprecision recallother 53.75% 54.41%in 55.93% 62.93%for 56.18% 38.76%of 68.09% 85.85%on 46.94% 24.47%to 79.54% 51.72%with 64.86% 25.00%at 50.00% 29.67%by 42.86% 60.46%as 76.78% 64.18%from 81.13% 39.09%since 50.00% 10.00%about 93.88% 69.70%than 95.24% 90.91%Table 7: Precision and recall of the preposition chclassifier.453Chodorow et al (2007) present numbers on anindependently developed system for detection ofpreposition error in non-native English.
Theirapproach is similar to ours in that they use aclassifier with contextual feature vectors.
Themajor differences between the two systems are theadditional use of a language model in our systemand, from a usability perspective, in the exampleprovider module we added to the correctionprocess.
Since both systems are evaluated ondifferent data sets3, however, the numbers are notdirectly comparable.5.2 Language model ImpactThe language model gives us an additional pieceof information to make a decision as to whether acorrection is indeed valid.
Initially, we used thelanguage model as a simple filter: any correctionthat received a lower language model score thanthe original was filtered out.
As a first approxi-mation, this was an effective step: it reduced thenumber of preposition corrections by 66.8% andthe determiner corrections by 50.7%, and increasedprecision dramatically.
The language model alone,however, does not provide sufficient evidence: ifwe produce a full set of preposition suggestions foreach potential preposition location and rank thesesuggestions by LM score alone, we only achieve58.36% accuracy on Reuters data.Given that we have multiple pieces ofinformation for a correction candidate, namely theclass probability assigned by the classifier and thelanguage model score, it is more effective tocombine these into a single score and impose atunable threshold on the score to maximizeprecision.
Currently, this threshold is manually setby analyzing the flags in a development set.5.3 Human EvaluationA complete human evaluation of our system wouldhave to include a thorough user study and wouldneed to assess a variety of criteria, from theaccuracy of individual error detection andcorrections to the general helpfulness of real web-based example sentences.
For a first humanevaluation of our system prototype, we decided to3 Chodorow et al (2007) evaluate their system onproprietary student essays from non-native students,where they achieve 77.8% precision at 30.4% recall forthe preposition substitution task.simply address the question of accuracy on thedeterminer and preposition choice tasks on asample of non-native text.For this purpose we ran the system over arandom sample of sentences from the CLECcorpus (8k for the preposition evaluation and 6kfor the determiner evaluation).
An independentjudge annotated each flag produced by the systemas belonging to one of the following categories:?
(1) the correction is valid and fixes theproblem?
(2) the error is correctly identified, butthe suggested correction does not fix it?
(3) the original and the rewrite are bothequally good?
(4) the error is at or near the suggestedcorrection, but it is a different kind oferror (not having to do withprepositions/determiners)?
(5) There is a spelling error at or nearthe correction?
(6) the correction is wrong, the originalis correctTable 8 shows the results of this humanassessment for articles and prepositions.Articles (6ksentences)Prepositions(8ksentences)count ratio count ratio(1) correction isvalid240 55% 165 46%(2) error identified,suggestion doesnot fix it10 2% 17 5%(3) original andsuggestion equallygood17 4% 38 10%(4) misdiagnosis 65 15% 46 13%(5) spelling errornear correction37 8% 20 6%(6) original correct 70 16% 76 21%Table 8: Article and preposition correctionaccuracy on CLEC data.The distribution of corrections across deletion,insertion and substitution operations is illustratedin Table 9.
The most common article correction isinsertion of a missing article.
For prepositions,substitution is the most common correction, againan expected result given that the presence of a454preposition is easier to determine for a non-nativespeaker than the actual choice of the correctpreposition.deletion insertion substitutionArticles 8% 79% 13%Prepositions 15% 10% 76%Table 9: Ratio of deletion, insertion andsubstitution operations.6 Conclusion and Future WorkHelping a non-native writer of English with thecorrect choice of prepositions anddefinite/indefinite determiners is a difficultchallenge.
By combining contextual speller basedmethods with language model scoring andproviding web-based examples, we can leveragethe combination of evidence from multiplesources.The human evaluation numbers presented in theprevious section are encouraging.
Article andpreposition errors present the greatest difficulty formany learners as well as machines, but cannevertheless be corrected even in extremely noisytext with reasonable accuracy.
Providingcontextually appropriate real-life examplesalongside with the suggested correction will, webelieve, help the non-native user reach a moreinformed decision than just presenting a correctionwithout additional evidence and information.The greatest challenge we are facing is thereduction of ?false flags?, i.e.
flags where botherror detection and suggested correction areincorrect.
Such flags?especially for a non-nativespeaker?can be confusing, despite the fact that theimpact is mitigated by the set of examples whichmay clarify the picture somewhat and help theusers determine that they are dealing with aninappropriate correction.
In the current system weuse a set of carefully crafted heuristic thresholdsthat are geared towards minimizing false flags on adevelopment set, based on detailed error analysis.As with all manually imposed thresholding, this isboth a laborious and brittle process where eachretraining of a model requires a re-tuning of theheuristics.
We are currently investigating a learnedranker that combines information from languagemodel and classifiers, using web counts as asupervision signal.7 AcknowledgementsWe thank Claudia Leacock (Butler Hill Group) forher meticulous analysis of errors and humanevaluation of the system output, as well as formuch invaluable feedback and discussion.ReferencesBond, Francis.
2005.
Translating the Untranslatable: ASolution to the Problem of Generating EnglishDeterminers.
CSLI Publications.Charniak, Eugene.
2001.
Immediate-head parsing forlanguage models.
In Proceedingsof the 39th AnnualMeeting of the Association for ComputationalLinguistics, pp 116-123.Chickering, David Maxwell.
2002.
The WinMineToolkit.
Microsoft Technical Report 2002-103.Chodorow, Martin, Joel R. Tetreault and Na-Rae Han.2007.
Detection of Grammatical Errors InvolvingPrepositions.
In Proceedings of the 4th ACL-SIGSEMWorkshop on Prepositions, pp 25-30.Crystal, David.
1997.
Global English.
CambridgeUniversity Press.Rachele De Felice and Stephen G Pulman.
2007.Automatically acquiring models of preposition use.Proceedings of the ACL-07 Workshop onPrepositions.Dunning, Ted.
1993.
Accurate Methods for the Statisticsof Surprise and Coincidence.
ComputationalLinguistics, 19:61-74.Gamon, Michael, Eric Ringger, and Simon Corston-Oliver.
2002.
Amalgam: A machine-learnedgeneration module.
Microsoft Technical Report,MSR-TR-2002-57.Golding, Andrew R. and Dan Roth.
1999.
A WinnowBased Approach to Context-Sensitive SpellingCorrection.
Machine Learning, pp.
107-130.Gui, Shicun and Huizhong Yang (eds.).
2003.
ZhongguoXuexizhe Yingyu Yuliaohu.
(Chinese Learner EnglishCorpus).
Shanghai Waiyu Jiaoyu Chubanshe..Han, Na-Rae., Chodorow, Martin and Claudia Leacock.2004.
Detecting errors in English article usage with amaximum entropy classifier trained on a large,diverse corpus.
Proceedings of the 4th internationalconference on language resources and evaluation,Lisbon, Portugal.455Han, Na-Rae.
Chodorow, Martin., and Claudia Leacock.(2006).
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineering,12(2), 115-129.Heidorn, George.
2000.
Intelligent Writing Assistance.In Robert Dale, Herman Moisl, and Harold Somers(eds.).
Handbook of Natural Language Processing.Marcel Dekker.
pp 181 -207.Izumi, Emi, Kiyotaka Uchimoto and Hitoshi Isahara.2004.
The NICT JLE Corpus: Exploiting theLanguage Learner?s Speech Database for Researchand Education.
International Journal of theComputer, the Internet and Management 12:2, pp119 -125.Kneser, Reinhard.
and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.Proceedings of the IEEE International Conferenceon Acoustics, Speech, and Signal Processing, volume1.
1995. pp.
181?184.Knight, Kevin and Ishwar Chander.
1994.
AutomaticPostediting of Documents.
Proceedings of theAmerican Association of Artificial Intelligence, pp779-784.Lee, John.
2004.
Automatic Article Restoration.Proceedings of the Human Language TechnologyConference of the North American Chapter of theAssociation for Computational Linguistics, pp.
31-36.Liu, Ting, Mingh Zhou, JianfengGao, Endong Xun, andChangning Huan.
2000.
PENS: A Machine-AidedEnglish Writing System for Chinese Users.Proceedings of ACL 2000, pp 529-536.Minnen, Guido, Francis Bond and Ann Copestake.2000.
Memory-Based Learning for ArticleGeneration.
Proceedings of the Fourth Conferenceon Computational Natural Language Learning andof the Second Learning Language in LogicWorkshop, pp 43-48.Suzuki, Hisami and Kristina Toutanova.
2006.
Learningto Predict Case Markers in Japanese.
Proceedings ofCOLING-ACL, pp.
1049-1056.Toutanova, Kristina and Hisami Suzuki.
2007Generating Case Markers in Machine Translation.Proceedings of NAACL-HLT.Turner, Jenine and Eugene Charniak.
2007.
LanguageModeling for Determiner Selection.
In HumanLanguage Technologies 2007: The Conference of theNorth American Chapter of the Association forComputational Linguistics; Companion Volume,Short Papers, pp 177-180.Yi, Xing, Jianfeng Gao and William B. Dolan.
2008.Web-Based English Proofing System for English as aSecond Language Users.
To be presented at IJCNLP2008.456
