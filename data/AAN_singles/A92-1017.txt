Extended Spelling Correction for GermanRalf Kese, Friedrich Dudda, Gerhard Heyer, Marianne KuglerTA Triumph-Adler AGTA Research EFFiirther Str.
212D-8500 Nuremberg 80e-mail: ralf@triumph-adler.deAbstractA conscrvativc extension of standardspciling corrcction systems for German isdiscusscd which goes beyond normalchccking of isolated single words bytaking multi-words, linguisticallymotivatcd non-words, as well as contextsinto account.1 MotivationAs indicated by Maurice Gross in his COLING86 lccturc (Gross, 1986), European languagescontain thousands of what he calls "frozen" or"compound words".
In contrast to "free forms",frozen words though being separable intoseveral words and suffixes - lack syntactic and/orsemantic compositionality.
This "lack ofcompositionality is apparent from lexicalrestrictions" (at night, but: *at day, *at evening,ctc.)
as well as "by the impossibility of insertingmaterial that is a priori plausible" (*at {coming,present, cold, dark} night) (Gross, 1986).Since the degree of frozenness can vary, theprocedure for recognizing compound wordswithin a text can be more or less complicated.Yet, at least for the entirely and nearly entirelyfrozen forms, simple string matching operationswill do (Gross, 1986).However, although this clearly indicates thatat least those compound words whose parts have ahigh dcgrce of frozenness are accessible to themethods of standard spelling correction systems,the problem is that these systems at best try tocope with (some) compound nouns (Frisch andZamora, 1988) while they are still ignorant of thebulk of other compound forms and of violationsof lexical and/or cooccurrence (Harris, 1970)restrictions in general.As Zimmermann points out (Zimmermann,1987) with respect to German forms like "inbezug auf" (= frozen) versus "mit Bezug auf" (=free), compounds clearly are out of the scope ofstandard spelling correction systems due to thefact that these systems check for isolated wordsonly and disregard the respective contexts.Following Gross and Zimmermann (Gross,1986; Zimmermann, 1987), we propose to furtherextend standard spelling correction systems ontothe level of compound words by making themcontext-sensitive as well as capable of treatingmore than a single word at a time.Yet even on the level of single words manymore errors could be detected by a spellingcorrector if it possessed at least some rudimentarylinguistic knowledge.
In the case of a word thattakes irregular forms (like the German verb"'laufen" or the English noun "mouse", forexample), a standard system seems to "know" theword and its forms for it is able to verify them,e.g., by simple lexicon lookup.
Yet whenconfronted with a regular though false form ofthe very same word (e.g.
with "laufte" as thelst/3rd pers.
sg.
simple past ind.
act., or with theplural "mouses"), such a system normally fails topropose the corresponding irregular form ("lief"or "mice") as a correction alternative.Following a hint in (Zimmermann, 1987), wepropose to enhance standard spelling correctionsystems on the level of isolated words byintroducing an additional sort of lexicon entriesthat explicitly records those cognitive errors thatare intuitively likely to occur (at least in thewritings of non-native speakers) but which a126standard system fails to treat in an adequate wayfor system intrinsic reasons.Considering that most of these errors give aclear indication of a writer's knowledge of theorthography of a language, and that thereforetheir correction may be of particular importanceto him, the system also is explicitly intended toexemplify how more complex linguisticphenomena that are of importance to a user mayyet be treated by simpler means, thus achievingin the sense of an engineering approach (Heyer,1990) a satisfactory trade-off between theoreticalcosts and practical benefits.Overview of new Phenomena forSpelling CorrectionAs there are irregular forms which neverthelessare well-formed, i.e.
: words, there are alsoregular forms which are ill-formed, i.e.
: non-words.
Whereas words usually are known to aspelling correction system, we have to add thenon-words to its vocabulary in order to improvethe quality of its corrections.
(1)(2.1)(2.3)(2.1)(2.3)(2.2)(2.3)(3)lch lattfe eis.versusIch laufe attf dem Eis.Er d~rfte Bankrott machen.versusEr dfirfte bankrott sein.Sic kann nicht Fahrrad fahren.versusSie kann nicht radfahren.Es war bitter kalt.versusEs war ein bitterkalter Tag.Er liebt Ich-Romane.versusEr liebt Romane in Ichform.BetonblOcke vs. *BetonblocksversusHi~userblocks vs. *HiiuserblOckeOn the level of single words in German, non-words come from various sources and comprise,among others, false feminine derivations ofcertain masculine nouns (*Wandererin, *Abtin),false plurals of nouns (*Thematas, *Tertias), non-licensed inflections (*beigem, *lila(n)es) orcomparisons (*lokaler, *minimalst) of certainadjectives, false comparisons (*nahste,*rentabelerer), wrong names for the citizens oftowns (*Steinhagener, *Stadth~iger), etc.
Someout-dated forms (e.g.
: PreiBelbeere, verk~iufst,abergl~iubig) can likewise be treated as non-words.It's on the level of compounds that wordsrather than non-words come into considerationagain when we look for contextual constraints orcooccurrence restrictions that determineorthography beyond the scope of what can beaccepted or rejected on the basis of isolatedwords alone.For words in German, these restrictionsdetermine, among other things, whether or notcertain forms (1) begin with an upper or lowercase letter; (2) have to be separated by (2.1)blank, (2.2) hyphen, (2.3) or not at all; (3)combine with certain other forms; or even (4)influence punctuation.
Examples are:(4) Er rauchte~ ohne daft sic davon wuBte.versus*Er rauchte ohneL daft sic davon wuBte.3 MethodAccording to a distinction made in the literature(Pollock and Zamora, 1984; Salton, 1989), thereare two main approaches in automatic spellingcorrection: While the 'absolute' approach "consistsof using a dictionary of commonly misspelledwords, and replacing any misspelling detected ina text by the corrected version listed in thedictionary" (Salton, 1989), the 'relative' approachconsists of locating in a conventional dictionarywith correct spellings words "that are most similarto a misspelling and selecting a correction fromthese.
Generally, the selection method is based onmaximizing similarity or minimizing the string-to-string edit distance" (Pollock and Zamora,1984).Although there is some use of 'absolute'methods in some systems (Pollock and Zamora,1984), "referencing a dictionary of correctlyspelled words" (Frisch and Zamora, 1988) isstandard.
On that basis, most of the purelymotoric single word errors, or "typographical127errors" (Berkel and Smedt, 1990), can becorrected.
Some conventional systems additionallytry to cope with a certain subset of cognitive, or"orthographical" (Berkei and Smedt, 1990), errorswhich "result in homophonous strings" andinvolve "some kind of phonemic transcription"(Berkel and Smedt, 1990) for their correction.Sincc the cognitive errors outlined in 1 and 2abovc are non-standard, in the sense that they arencither motoric (by definition) nor phonologicallymotivated, a straightforward method to correctthcm is the 'absolute' one of directly encodingerror pattcrns in a lexicon and replacing eachmatching occurrence in a text by the correctionlistcd in the system lexicon.Now, in ordcr to treat single (non-) wordsand compounds in a uniform way, each entry inthe systcm lcxicon is modelled as a quintuple<W,L,R,C,E> specifying a pattern of a (multi-)word W for which a correction C will beproposed accompanicd by an explanation E iff agiven match of W against some passage in thetext under scrutiny differs significantly from Cand the - possibly empty - left and right contextsL and R of W also match the environment of W'scounterpart in the text.Disrcgarding E for a moment, this istantamount to saying that each such record isinterpreted as a string rewriting ruleW- ->C / L Rreplacing W (e.g.
: Bczug) by C (e.g.
: bezug) in theenvironment L R (e.g.
: in auf).The form of these productions can best becharacterized with an eye to the Chomskyhierarchy as unrestrictcd, since we can have anynon-null number of symbols on the LHS replacedby any numbcr of symbols on the RHS, possiblyby null (Partee et al, 1990).With an eye to semi-Thue or extendedaxiomatic systems one could say that a linearlyordered sequence of strings W, C1, C2, ..., Cm isa derivation of Cm iff (1) W is a (faulty) string(in the text to be corrected) and (2) each Cifollows from the immediately preceding string byone of the productions listed in the lexicon(Partee et al, 1990).Thus, theoretically, a single mistake can becorrected by applying a whole sequence ofproductions, though in practice the default isclearly that a correction be done in a singlederivational step, at least as long as the system isjust operating on strings and not on additionalnon- terminal symbols.Occurrences of W, L, and R in a text arerecognized by pattern matching techniques.
Anerror pattern W ignores the particularly error-prone aspects upper/lower case and wordseparator (see the examples in 2 above).
It thusmatches both the correct and incorrect spellingswith respect o these features.Beside wildcards for characters, like "*", apattern for W, L, or R may contain alsowildcards for words allowing, for example, thespecification of a maximal distance of L or Rwith respect to W. Since the types of errorsdiscussed here only occur within sentences, sucha distant match has to be restricted by thesentence boundaries.
Thus, by having the systemoperate sentencewise, any left or right context isnaturally restricted to be some string within thesame sentence as W or to be a boundary of thatsentence (e.g.
: a punctuation mark).Any left or right context is either a positiveor a negative one, i.e., its components arehomogeneously either required or forbidden inorder for the corresponding rule to fire.
So far ithas not been necessary to allow for mixed modeswithin a left or right context.In case a correction C is proposed to the user,additionally a message will be displayed to himidentifying the reason why C is correct ratherthan W. Depending on the user's knowledge ofthe language under investigation, he can take thiseither as an opportunity to learn or rather as ahelp for deciding whether to finally accept orreject the proposal.There are two kinds of explanations, absoluteand conditional ones.
Whereas absolute rulesindicate that the system has necessary andsufficient evidence for W's deviance, there clearlyare cases where either W or C could be correctand this question cannot be decided on the basisof the system's lexical information alone.
In thesecases, a conditional or i f -then explanation isgiven to the user offering a higher-level decisioncriterion which the system itself is unable toapply.Take, as an example, the sentence128Dieser Film betrifft Alt und Jung.which clearly allows for two readings, one whichrenders "Alt und Jung" as the false spelling of theidiomatic expression "alt und jung" meaning"everybody", and another one which takes "Altund Jung" as the correct form that literallydesignates the old and the young while excludingthe middle-aged.
Thus, substitutability by"jedermann" (i.e.
: "everybody") would be anadequate decision criterion to convey to the user.Although the method described aboveintroduces a new kind of lexical data, its (higher-level) error correction still operates on nothingbut strings.
No deep and time-consuminganalysis, like parsing, is involved.Restricting the system that way makes ourapproach to context-sensitiveness different fromthe one considered in (Rimon and Herz, 1991),where context sensitive spelling verification isproposed to be done with the help of "localconstraints automata (LCAs)" which processcontextual constraints on the level of lexical orsyntactic ategories rather than on the basic levelof strings.
In fact, proof-reading with LCAsrather amounts to genuine grammar checking andas such belongs to a different and higher level oflanguage checking.Context sensitive spelling checking, asproposed here, can be regarded as a checkinglevel in its own right, lying in between anychecking on word level and grammar checking.
Itthus could complement he two-level checkerdiscussed in (Vosse, 1992) by correcting especiallythose errors in idiomatic expressions, like "te alletijden" -> "te allen tijde", which cannot bedetected on word or sentence level; compare(Vosse, 1992).4 A Processing ModelA good model of the system is given by adeterministic multitape Turing machine (Hopcroftand Ullman, 1979) consisting of a finite controlwith, in effect, three tapes and tape heads.
Thefollowing description relates to sentence level:Initially, the input appears on the first tapewith each of the tape's cells containing either aword, a blank (symbolized below by a single "B"),or a left or right sentence boundary symbol.Thus, any input sentence can be stored by afinite sequence of cells.The second tape holds a read-only copy ofthe initial text.
While the first tape will berewritten, the second serves just as a referencetape.
The third tape is also read-only, it holds thefinite sequence of lexicon entries.Consider the following snapshot of the systemTI: B in B Bezug B auf BT2: B in B Bezug B auf BT3: /b/ezug (1in ) lauf  bezugwhere "Bezug" has been scanned on the referencetape T2, and a pattern /b/ezug has been found inthe lexicon T3 that ignores upper/lower case inthe match but requires a lower case "bezug" justin case "in" can be found as 1 word to the left (asis expressed by "(1in") and "auf" can be found 1non-blank cell to the right on T2.Since the corresponding contexts of /b/ezugcan be verified on T2 (by simply moving T2'shead " to the respective cells, scanning theircontents, and comparing these with the relevantinformation on T3), finally the error "Bezug" iscorrected on T1 and a new checking cycle isstarted with the next word:TI: B in B bezug B auf BAT2: B in B Bezug B auf BANote, as should be clear from the outset, thata previous correction on T1 is not available as acontext for any next word under scrutiny, butonly the uncorrected words on T2 are.Thus, if it were counterfactually the case that"auf" had to be corrected somehow whenever itappeared to the right of "bezug" as opposed to"Bezug", and given the input of the aboveexample, our system - though producing "bezug"as the left context of "auf" on T1 - would clearlyfail to correct "auf" since it would still be takingany context from T2.129Although one can think of other, morerealistic, cases (like, e.g., "dab ich Eis laufte" ->"dab ich cislief") which require two or morecorrection steps such that at least one of thesesteps ("Eis lief" -> "eislief") depends on anotherone ("laufte" -> "lief"), there clearly are otheralternatives (like writing clever lexical entries)beside giving up reading from T2.Giving up T2 would mean to give up thesimple working hypothesis that all the higher-Icvcl errors within a givcn input sentence can becorrcctcd indcpendcntly.
As a consequence, thesystcm would become much more complex and,probably, less efficient.For German, we have not yet faced any(significant amout of) data that would justify amore complex redesign of the system.
However,since the data captured in the system's lexiconcovers at present some 50 % of the relevantphenomena compared to the Duden (Berger1985), the ultimate complexity of the system hasto be regarded as an open and empirical question.read beyond a known abbreviation.
This mightresult eventually in taking two sentences to beone, but would, of course, not disturb intra-sentential error correction.
Nothing, however,prevents the system from stopping at an unknownabbreviation and thereby falling short of acontext it otherwise would have recognized.
Fromthis it is clear that the system should at leastknow the most frequent abbreviations of a givenlanguage.Likewise, the formatting information of a textis preserved to a very high degree duringcorrection, as it should be.
Nevertheless, therenaturally are cases where some such informationwill get lost as is clear from the simple fact thatthere can be shrinking productions reducing ndifferently formatted elements on the LHS to melements on the RHS, with m < n. But these areborderline cases.What is less acceptable, for each of theimplcmentations mentioned above, is the lack ofintegration of the checking on the various levels.5 Status of ImplementationA first prototype of the system described abovehas been developed in C under UNIX within theESPRIT II project 2315 "Translator's Workbench"(TWB) as one of several separate moduleschecking basic as well as higher levels of variouslanguages Ilike grammar and style; see (Thurmair,1990) and (Winkelmann, 1990)\].A derived and extended B-release version -covering 3.000 rewriting rules - has beenintegrated into both a proprietary text processingsoftware under DOS and Microsoft's WINWORD1.1 under MS WINDOWS 3.0.
In each case it runsindependently from the built-in standard spellingverifier, although this is not transparent to theuser who perceives just one proofreader checkingeach sentence of a text twice, i.e., on twodifferent levels.On both these implementations, someproblems have received practical solutions to anacceptable degree.For example, the problem of mistaking anabbreviation for the end of a sentence (becauseboth end with a dot), which could prevent acontext from being recognized, is 'solved' byhaving the sentence segmentation routine alwaysThus, it may happen that the checkersrunning one after the other over the same text -disturb each other's results by proposingantagonistic orrections with respect to one andthe same expression: Within the correct passage"in bezug auf", for example, "bezug" will first beregarded as an error by the standard checkerwhich then will propose to rewrite it as "Bezug".If the user accepts this proposal, he will receivethe exactly opposite advice by the contextsensitive checker.On the other hand, checking on differentlevels could nicely go hand in hand and producesynergetic effects: For, clearly, any contextsensitive checking requires that the contextsthemselves be correct and thus possibly have beencorrected in a previous, possibly context free,step.
The checking of a single word could in turnprofit from contextual knowledge in narrowingdown the number of correction alternatives to beproposed for a given error: While there may besome eight or nine plausible candidates a~,corrections of "Bezjg" when regarded in isolation,only one candidate, i.e.
"bezug", is left when thecontext "in auf" is taken into account.Thus, there is a strong demand for arriving ala holistic solution for multi-level languagechecking rather than for just having various level130experts particularistically hooked together inseries.
This will be a task for the future.6 Ongoing WorkAs an inhouse test revealed, it is very importantfor users to have the possibility to add data to thesystem.
As a consequence of that we are currentlydeveloping a higher-level user dictionary that willaccept and support entering context-sensitive(multi-) words of the kind discussed in thispaper.At the same time, data acquisition is stillgoing on.
Since, unfortunately, there is (to ourknowledge) no ready-made corpus of higher-levelerrors available, the collection of data is a timeconsuming process.
The best reference book forGerman seems to be the Duden (Berger 1985), yetit consists of an unstructured mixture of allpossible kinds of errors and often presents aparadigmatic example rather than all the membersof a given error class.As concerns languages other than German, wetake it that a similar approach is feasible forthem as well.
Although in comparison with, e.g.,English, French, Italian, and Spanish, Germanseems to be unique as concerns the relevance ofthe context for upper/lower case spellings in alarge number of cases, there are at least, asindicated in (Gross, 1986), the thousands ofcompounds or frozen words in each of theselanguages which are clearly within reach for themethods discussed.Rudolf Frisch and Antonio Zamora.
Spellingassistance for compound words.
IBM Journal ofResearch and Development, 32(2): 195-200, March1988.Maurice Gross.
Lcxicon Grammar.
TheRepresentation of Compound Words.
InProceedings of the Eleventh InternationalConference on Computational Linguistics, pages1-6, Bonn, Germany, August 1986.
InternationalCommittee on Computational Linguistics.Zellig S. Harris.
Papers in Structural andTransformational Linguistics.
Formal LinguisticsSeries, volume 1.
D. Reidel Publishing Company,Dordrecht, The Netherlands, 1970.Gerhard Heyer.
Probleme und Aufgaben cinerangewandten Computerlinguistik.
KI-KiozstlicheIntelligenz 3(1): 38-42, 1990.John E. Hopcroft and Jeffrey D. Ullman.Introduction to Automata Theory, Languages, andComputation.
Addison -Wesley PublishingCompany, Reading, Massachusetts, 1979.Barbara H. Partee, Alice ter Meulen, and RobertE.
Wall.
Mathematical Methods in Linguistics.Studies in Linguistics and Philosophy, volume 30.Kluwer Academic Publishers, Dordrecht, TheNetherlands 1990.Joseph J. Pollock and Antonio Zamora.
Automaticspelling correction in scientific and scholarly text.Communications of the ACM, 27(4): 358-368,April 1984.ReferencesDieter Berger, editor.
Richtiges und gutesDeutsch.
WOrterbuch der sprachlichenZweifelsfalle.
Der Duden in 10 B~inden.
DasStandardwerk zur deutschen Sprache, volume 9.Bibliographisches Institut, Mannheim, Germany,3rd ed.
1985.Brigitte van Berkel and Koenraad De Smedt 1990.Triphone Analysis: A combined method for thecorrection of orthographical and typographicalerrors.
In Proceedings of the Second Conferenceon Applied Natural Language Processing, pages77-83, Austin, Texas, February 1988.
Associationfor Computational Linguistics.Mori Rimon and Jacky Herz.
The RecognitionCapacity of Local Syntactic Constraints.
InProceedings of the Fifth Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 155-160, Berlin,Germany, April 1991.
Association forComputational Linguistics.Gerard Salton.
Automatic Text Processing.
TheTransformation, Analysis, and Retrieval ofInformation by Computer.
Addison-WesleyPublishing Company, Reading, Massachusetts,1989.Gregor Thurmair.
Parsing for Grammar and StyleChecking.
In Papers presented to the ThirteenthInternational Conference on ConwutationalLinguistics, volume 2, pages 365-370, Helsinki,Finland, August 1990.
Hans Karlgren, editor.131Theo Vosse.
Detecting and Correcting Morpho-syntactic Errors in Real Texts.
In Proceedings ofthe Third Conference on Applied NaturalLanguage Processing, Trento, Italy, March/April1992.
Association for Computational Linguistics.Giintcr Winkelmann.
Semiautomatic InteractiveMultilingual Style Analysis (SIMSA).
In  Paperspresented to the Thirteenth InternationalConference on Computational Linguistics,w)lume 1, pages 79-81, Helsinki, Finland, August1990.
Hans Karigren, editor.Harald Zimmermann.
Textverarbeitung imRahmen moderner Bi~rokommunikationstechniken.PIK.
Praxis der lnformationsverarbeitung undKommunikation 10: 38-45, 1987.132
