Proceedings of the Fourth International Natural Language Generation Conference, pages 3?5,Sydney, July 2006. c?2006 Association for Computational LinguisticsLessons Learned from Large Scale Evaluation of Systems that ProduceText: Nightmares and Pleasant SurprisesKathleen R. McKeownDepartment of Computer ScienceColumbia UniversityNew York, NY 10027kathy@cs.columbia.eduExtended AbstractAs the language generation community exploresthe possibility of an evaluation program for lan-guage generation, it behooves us to examine ourexperience in evaluation of other systems that pro-duce text as output.
Large scale evaluation of sum-marization systems and of question answering sys-tems has been carried out for several years now.Summarization and question answering systemsproduce text output given text as input, while lan-guage generation produces text from a semanticrepresentation.
Given that the output has the sameproperties, we can learn from the mistakes andthe understandings gained in earlier evaluations.In this invited talk, I will discuss what we havelearned in the large scale summarization evalua-tions carried out in the Document UnderstandingConferences (DUC) from 2001 to present, and inthe large scale question answering evaluations car-ried out in TREC (e.g., the definition pilot) as wellas the new large scale evaluations being carried outin the DARPA GALE (Global Autonomous Lan-guage Environment) program.DUC was developed and run by NIST and pro-vides a forum for regular evaluation of summariza-tion systems.
NIST oversees the gathering of data,including both input documents and gold standardsummaries, some of which is done by NIST andsome of which is done by LDC.
Each year, some30 to 50 document sets were gathered as test dataand somewhere between two to nine summarieswere written for each of the input sets.
NIST hascarried out both manual and automatic evaluationby comparing system output against the gold stan-dard summaries written by humans.
The resultsare made public at the annual conference.
In themost recent years, the number of participants hasgrown to 25 or 30 sites from all over the world.TREC is also run by NIST and provides anannual opportunity for evaluating the output ofquestion-answering (QA) systems.
Of the variousQA evaluations, the one that is probably most illu-minating for language generation is the definitionpilot.
In this evaluation, systems generated longanswers (e.g., paragraph length or lists of facts) inresponse to a request for a definition.
In contrast toDUC, no model answers were developed.
Instead,system output was pooled and human judges de-termined which facts within the output were nec-essary (termed ?vital nuggets?)
and which werehelpful, but not absolutely necessary (termed ?OKnuggets?).
Systems could then be scored on theirrecall of nuggets and precision of their response.DARPA GALE is a new program funded byDARPA that is running its own evaluation, carriedout by BAE Systems, an independent contractor.Evaluation more closely resembles that done inTREC, but the systems?
scores will be comparedagainst the scores of human distillers who carryout the same task.
Thus, final numbers will reportpercent of human performance.
In the DARPAGALE evaluation, which is a future event at thetime of this writing, in addition to measuring prop-erties such as precision and recall, BAE will alsomeasure systems?
ability to find all occurrences ofthe same fact in the input (redundancy).One consideration for an evaluation programis the feel of the program.
Does the evalua-tion program motivate researchers or does it causeheadaches?
I liken Columbia?s experience in DUCand currently in GALE to that of Max in Where theWild Things Are by Maurice Sendak.
We beganwith punishment (i.e., if you don?t do well, yourfunding will be in jeopardy), encounter monstersalong the way (seemingly arbitrary methods for3measuring output quality), finally tame the mon-sters and sail back peacefully across time.
DUChas reached the peaceful stage, but GALE has not.The TREC definition pilot had less of a threat ofpunishment.Evaluation in all of these programs began at therequest of the funders, with the goal of comparinghow well different funded systems perform.
Im-provement over the years is also measured in orderto determine if funding is well spent.
This kind ofgoal creates anxiety in participants and makes itmost important to get the details of the evaluationright; errors in how evaluation is carried out canhave great consequences.
Coming to agreementon the metrics used, the methodology for measur-ing output and the tasks on which performance ismeasured can be difficult; the environment doesnot feel friendly.
Even if evaluation within thelanguage generation community was not initiatedwith the same goals, I think it is reasonable to ex-pect a certain amount of disagreement as the pro-gram gets off the ground.However, over time, researchers come to agree-ment on some portion of the task and these fea-tures become accepted.
At this point in time, it ispossible to see the benefits of the program.
Cer-tainly, within DUC, we are at this stage.
DUC hasgenerated large amounts of data, including bothinput document sets and multiple models of goodoutput for each input set, which has spurred stud-ies both on evaluation and summarization.
Hal-teren and Teufel, for example, provide a methodfor annotation of content units and study consen-sus across summarizers (van Halteren and Teufel,2003; Teufel and van Halteren, 2004b).
Nenkovastudies significant differences across DUC04 sys-tems (Nenkova, 2005) as well as the properties ofhuman and system summaries (Nenkova, 2006).We can credit DUC with the emergence of au-tomatic methods for evaluation such as ROUGE(Lin and Hovy, 2003; Lin, 2004) which allowquick measurement of systems during develop-ment and enable evaluation of larger amounts ofdata.
We have seen the development of man-ual methods for evaluation developed both withinDUC (Harman and Over, 2004) and without.
ThePyramid method (Nenkova and Passonneau, 2004)provides a annotation method and metric that ad-dresses the issues of reliability and stability ofscoring.
Thus, research on evaluation of summa-rization has become a field in its own right result-ing in greater understanding of the effect of differ-ent metrics and methodologies.
?From DUC and TREC, we have learned im-portant characteristics of a large-scale evaluation,of which the top three might be:?
Output can be measured by comparisonagainst a human model, but we know thatthis comparison will only be valid if multi-ple models are used.
There are multiple goodsummaries of the same input and if systemoutput is compared against just one, the re-sults will be biased.?
If the task is appealing to a wide audience,the evaluation will spur research and motivateresearchers to join in.
We have seen this withgrowth of participation in DUC.
One benefitof summarization and QA is that the task isdomain-independent and thus, no one site hasan advantage over others through experiencewith a particular domain.?
Given the different ways in which evaluationcan be carried out and the fact that differentresearchers may be biased towards methodswhich favor their own approach, it is impor-tant the evaluation be overseen by a neutralparty which is not deeply involved in researchon the task itself.
On the other hand, someknowledge is necessary if the evaluation is tobe well-designed.While my talk will focus on large scale evalua-tion programs that feature quantitative evaluationthrough comparison with a gold standard, therehas been work on task-based evaluation of sum-marization (McKeown et al 2005).
Task-basedevaluation is more intensive and to date, has notbeen done on a large scale across sites, but showspotential for indicating the usefulness of summa-rization systems.In this brief abstract, I?ve suggested some of thetopics that will be covered in my talk, which willtour the land of the wild things for evaluation, il-luminating monsters and highlighting events thatwill allow more peaceful sailing.
Evaluation canbe a nightmare, but over time and particularlyif carried out away from the influence of fund-ing pressures, it can nurture a community of re-searchers with common goals.4AcknowledgmentsThis material is based upon work supported inpart by the ARDA AQUAINT program (Con-tract No.
MDA908-02-C-0008 and Contract No.NBCHC040040) and the Defense Advanced Re-search Projects Agency (DARPA) under Con-tract No.
HR0011-06-C-0023 and Contract No.N66001-00-1-8919.
Any opinions, findings andconclusions or recommendations expressed in thismaterial are those of the author(s) and do not nec-essarily reflect the views of the DARPA or ARDA.ReferencesHarman, D. and Over, P. 2004.
The effects of humanvariation in duc summarization evaluation.
In TextSummarization Branches Out Workshop, ACL 2004.Lin, C.-Y.
2003.
Rouge: a package for automatic eval-uation of summaries.
In Proceedings of the Work-shop in Text Summarization, ACL?04.Lin, C.-Y.
and Hovy, E. 2003.
Automatic evaluation ofsummaries using n-gram co-occurance statistics.
InProceedings of HLT-NAACL 2003.McKeown, K. and Passonneau, R.J. and Elson, D.K.,and Nenkova, A., and Hirschberg, J.
2005.
A Task-Based Evaluation of Multi-Document Summariza-tion.
In Proceedings of SIGIR 2005.Nenkova, A.
2005.
Automatic text summarization ofnewswire: Lessons learned from the Document Un-derstanding Conference.
In Proceedings of AAAI2004.Nenkova, A.
2006.
Understanding the processof multi-document summarization: content selec-tion, rewrite and evaluation.
Ph.D Dissertation,Columbia University.Nenkova, A. and Passonneau, R. 2004.
Evaluatingcontent selection in summarization: The pyramidmethod.
In Proceedings of HLT/NAACL 2004.Teufel, S. and van Halteren, H. 2004.
Evaluating in-formation content by factoid analysis: human anno-tation and stability.
In EMNLP-04.van Halteren, H. and Teufel, S. 2003.
Examining theconsensus between human summaries: initial exper-iments with factoid analysis.
In HLT-NAACL DUCWorkshop.5
