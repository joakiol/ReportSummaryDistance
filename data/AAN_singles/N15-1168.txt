Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1452?1458,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsExtracting Information about Medication Use from Veterinary DiscussionsHaibo DingSchool of ComputingUniversity of UtahSalt Lake City, UT 84112hbding@cs.utah.eduEllen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112riloff@cs.utah.eduAbstractOur research aims to extract information aboutmedication use from veterinary discussion fo-rums.
We introduce the task of categoriz-ing information about medication use to deter-mine whether a doctor has prescribed medica-tion, changed protocols, observed effects, orstopped use of a medication.
First, we createa medication detector for informal veterinarytexts and show that features derived from theWeb can be very powerful.
Second, we cre-ate classifiers to categorize each medicationmention with respect to six categories.
Wedemonstrate that this task benefits from a richlinguistic feature set, domain-specific seman-tic features produced by a weakly supervisedsemantic tagger, and balanced self-training.1 IntroductionNatural language processing holds great promise forautomatically extracting empirical data about med-ications, from the perspective of both doctors andpatients.
A wealth of information about the admin-istration and effectiveness of medications lies withinunstructured text, including medical records createdby health care professionals (e.g., discharge sum-maries) as well as informal texts written by medicalpractitioners and patients (e.g., Web forums).Previous work has been done on detecting med-ication terms and recognizing relations betweenmedications and other medical concepts, such as dis-eases and symptoms.
Our research explores a newproblem: identifying and categorizing contexts in-volving the administration of medications, which wecall medication use categorization.
For each men-tioned medication, we want to know whether it wasused in a patient?s care, and if so, what actions or ob-servations are being reported.
Our task aims to dis-tinguish between contexts where a doctor prescribeda medication, changed the protocol of a medication(e.g., dosage or frequency), stopped use of a medi-cation, observed effects produced by the medication,or is asking a question about a medication.
Distin-guishing these contexts is an important step towardbeing able to extract empirical data about medica-tion use, such as effectiveness, success under differ-ent protocols, and adverse events.Our research studies veterinary discussion fo-rums, which often contain informal vocabulary suchas shortened and abbreviated medication terms (e.g.?pred?
instead of ?prednisone?, or ?abx?
for ?an-tibiotics?).
The first part of our research addressesthe problem of medication detection from informaltext.
We create an effective medication detector us-ing supervised learning with linguistic features aswell as contextual features acquired from the Web.We show that the Web context features substantiallyimprove recall, and yield an effective medication de-tector even with small amounts of training data.Second, we design supervised classifiers for med-ication use categorization.
We incorporate a rich setof contextual, syntactic, and sentential features aswell as a semantic tagger trained for the veterinarydomain with bootstrapped learning over a large setof unannotated veterinary texts.
We demonstrate ad-ditional performance gains by using balanced self-training with the unannotated texts.2 Related WorkPrevious work on extracting medication informationfrom text has primarily focused on clinical medi-cal text, such as discharge summaries (e.g., (Doanand Xu, 2010; Halgrim et al, 2010; Doan et al,2012; Tang et al, 2013; Segura-Bedmar et al,14522013)).
The Third and Fourth i2b2 Shared Tasksincluded medication detection from clinical texts(Uzuner et al, 2010; Uzuner et al, 2011), andthe Fourth i2b2 Shared Task also included relationclassification between treatments (including medi-cations), problems, and tests.
Recently, there hasbeen growing interest in extracting medication in-formation from other types of text, such as Twitter,online health forums, and drug review sites (e.g.,(Leaman et al, 2010; Bian et al, 2012; Liu et al,2013; Liu and Chen, 2013; Yates and Goharian,2013; Segura-Bedmar et al, 2014)).
Much of thisresearch is geared toward identifying adverse drugevents or drug-drug interactions.Many methods have been used for medication ex-traction, including rule based approaches (Levin etal., 2007; Xu et al, 2010), machine learning (Patrickand Li, 2010; Doan and Xu, 2010; Tang et al, 2013),and hybrid methods (Halgrim et al, 2010; Meystreet al, 2010).
Rule based and hybrid approaches typ-ically rely on manually created lexicons and rules.RxNorm (Nelson et al, 2011; Liu et al, 2005) is alarge knowledge base containing generic and brandnames of drugs and it is often used as a compo-nent in these systems.
We compare our results withthe MedEx system (Xu et al, 2010), which usesRxNorm coupled with manually defined rules.To our knowledge, classifying medication men-tions with respect to administration use categorieshas not yet been studied.
A novel aspect of our workis also the use of Web Context features for medica-tion detection.
Similar Web features have been ex-ploited for fine-grained person classification (Giu-liano, 2009), while we demonstrate that they can behighly beneficial for medical concept extraction.3 Task Description and Data SetWe divide our task into two subproblems: (1)Medication Detection aims to identify words cor-responding to non-food substances used to treatpatients (e.g., drugs, potassium supplements), and(2) Medication Use Categorization aims to classifymedication mentions based on actions and observa-tions related to their administration and to identifyquestion contexts.
We assign each medicationmention to one of the six categories below.Rx: The text indicates that a doctor prescribed themedication for a patient, or that a patient is taking(or has previously taken) the medication.Example: ?I started the dog on abx.
?Change: A change in the administration of the med-ication was made (e.g., dosage, route, frequency).Example: ?I increased the pred to 5mg.
?Stop: Use of the medication was discontinued.Example: ?We took the cat off metacam.
?Effect: The text reports a positive or negative effectfrom the medication on a patient.Example: ?The dog responded well to Vetsulin.
?Question: A question is asked about the medication.Example: ?Do you think we should consider lasix?
?Other: None of the above.
This category primarilycovers contexts not describing patient use.Example: ?Aranesp is expensive.
?Our data consists of discussion forums from theVeterinary Information Network (VIN), which isa web portal (www.vin.com) that hosts messageboards for veterinary professionals to discuss casesand issues in their practice.
To produce gold stan-dard annotations, we collected the initial post of 500randomly selected threads from VIN forums aboutcardiology/pulmonology, endocrinology, and felineinternal medicine.
We defined annotation guide-lines to identify the minimum span of medicationmentions.1Two people independently annotated 50texts, and we measured their inter-annotator agree-ment (IAA) using Cohen?s kappa (?)
statistic.
Formedication detection, their IAA score was ?
= .96.For the medication use categories, we measuredIAA in two ways.
First, we measured agreementon all of the words labeled as a medication by atleast one annotator, yielding ?
= 0.80.
Second, wemeasured agreement only on the words labeled asa medication by both annotators (to more directlyassess agreement on the six categories), yielding?
= .92.
Finally, the annotators labeled an addi-tional 450 texts, producing a gold standard set of500 labeled texts.
Of the annotated medication men-tions, 93% have one word and 6% have two words.The frequency of each category is shown below.Rx Question Effect Change Stop Other908 289 181 52 53 4701Dosage and duration terms were not included.14534 Medication DetectionDetecting medication terms in discussion forums ischallenging because of their informal nature.
As wewill show in Section 4.1, dictionary look-up fromlexicons is not sufficient.
Therefore the first partof our research aims to create an effective medica-tion detector for these informal veterinary texts.
Weused the Stanford CoreNLP tools (Manning et al,2014) for lemmatization, POS tagging and parsing,and created a SVM classifier with a linear kernelusing SVMlin (Sindhwani and Keerthi, 2006).
Theclassifier labels each token as a medication term ornot a medication term.
Adjacent medication tokensare then combined into a single medication mention.We designed three types of features:Word Features include the medication word?sstring, lemma, and part-of-speech tag.
Since drugsoften have common affixes (e.g., ?-sone?
is a com-mon suffix for corticosteroids), we also defined fea-tures for character sequences of length 2-4 at the be-ginning and end of a word.Local Context Features represent the word pre-ceding and the word following each medicationterm.
We replace numbers with the symbol ?CD?.We also defined features to represent the syntacticdependency relations linked to the medication wordusing the Stanford Parser (De Marneffe et al, 2006).Web Context Features capture information fromweb sites that mention a term, which provides exter-nal context beyond the information available in thetraining texts.
During training, we issued a Googlequery for each unique word in our training data andcollected the title and text snippets of the top 10 re-trieved documents.
We then defined binary-valuedfeatures to represent all of the words in the retrievedtexts.2We store the results of each query so that ad-ditional queries are needed only for previously un-seen words.4.1 Medication Detection ResultsWe conducted 10-fold cross-validation experimentson our data set to evaluate our medication detector.First, we created three baselines to assess the diffi-culty of medication detection for this data.
The firstrow of Table 1 shows the performance of a veteri-2We also tried different context windows but found that us-ing the title and entire snippet achieved the best results.nary thesaurus manually created by the VIN.3Weextracted all of the words in the entries categorizedas Pharmacologic Substance and label all instancesof those words as medication terms.
The VIN the-saurus achieved high precision but only 51% recall.Some reasons for the low coverage include abbre-viations, misspellings, general terms (e.g., ?drug?
),and pronouns that refer to medications (which areannotated in our data).
The second row shows theresults of MedEx (Xu et al, 2010), which uses theRxNorm drug lexicon and ranked in second placefor the 2009 i2b2 Medication Extraction challenge.MedEx?s low precision is primarily due to labelingchemical substances (e.g., ?glucose?)
as medica-tions, but in our data they are often test results (e.g.,?the cat?s glucose level...?).
The third row showsthe results of creating a Training Lexicon by collect-ing all nouns annotated as medications in the train-ing data.
We labeled all instances of these nouns asmedication terms in the test data, which producedslightly higher recall and precision than MedEx.Method Precision Recall FVIN thesaurus 90.9 51.3 65.6MedEX 52.5 73.8 61.4Training Lexicon 59.4 76.9 67.0SVM ClassifierWord Features 88.2 79.9 83.9+ Local Context 89.7 81.2 85.3+ Web Context 89.2 86.1 87.6Table 1: Medication Detection ResultsThe last three rows in Table 1 show the resultsfor our classifier.
With only Word Features, theclassifier produced an 83.9% F score, outperformingthe baselines.
Adding the Local Context Featuresyielded small gains in recall and precision.
TheWeb Context Features further increased recall from81% to 86%, raising the F score to 87.6%.
We triedadding features for the VIN thesaurus and MedExsystem, but they did not improve upon the resultsobtained with the Web Context features.We observed that the Web Context Features cancompensate for small amounts of training data.
Todemonstrate how powerful they are, we randomlyselected 100 gold standard texts to use as a test3We used a version provided to us in 2013.1454set, and trained classifiers using different amountsof training data.
Figure 1 shows the results forclassifiers using only the Word Features, Wordand Local Context Features, and all features.
Theclassifier with Web Context Features achieved anF score > 70% using only 10 training texts, andapproached its best performance with just 100training texts.0 50 100 150 200 250 300Number of training documents020406080100F1scoreWordWord+LocalWord+Local+WebFigure 1: Learning curves with different feature sets5 Medication Use CategorizationWe tackled this problem by designing a supervisedclassifier with linguistic features, and incorporated asemantic tagger trained by bootstrapping on a largecollection of veterinary text.
We also used a bal-anced self-training method on unannotated veteri-nary texts to further improve performance.First, we created a one-vs-the-rest binary SVMclassifier for each category using scikit-learn (Pe-dregosa et al, 2011).4If an instance is labeled withmultiple categories, we select the most confident oneusing the distance from the hyperplane.
We de-signed three sets of features.
N-gram Features rep-resent a context window of size eight (+/-4) aroundthe medication mention.
We define features for lexi-cal unigrams, lexical bigrams, lemma unigrams, andlemma bigrams.
Syntactic Features capture verbphrases that participate in a dependency relationwith the medication, using the Stanford parser.
The4Note that for medication detection we used SVMlin, but weswitched to scikit-learn for the medication categorization be-cause it supported additional types of classifiers that we wantedto try.
Ultimately, however, the SVM performed best.
We con-firmed that SVM results from both toolkits were very similar.third set of Sentential Features are for the Questionand Other categories to recognize sentences that donot describe use of the medication on a patient, butask questions, request guidance, describe hypothet-ical scenarios, etc.
The sentential features consistof clause initial part-of-speech (POS) and lemmabigrams; whether the sentence ends with a ques-tion mark; whether the word ?question?
occurs inthe same NP as the medication; whether the sen-tence contains the POS sequence<MD PRP>5; andwhether the medication is separated by a commafrom the ending question mark (for lists).Semantic Tagging.
We hypothesized that identi-fying semantic concepts might be beneficial.
For ex-ample, the presence of an ANIMAL term suggests apatient, and a SYMPTOM term may indicate the rea-son for a prescription or an effect of medication use.First, we used WordNet (Miller, 1995) and identi-fied synsets for 4 semantic classes: ANIMAL, DRUG,DISEASE/SYMPTOM, and HUMAN.
We assigned anynoun phrase with a head in these synsets to the cor-responding semantic type.
Next, we used a boot-strapping method (Huang and Riloff, 2010) to builddomain-specific semantic taggers (SemTaggers) forthe same four semantic classes as well as TEST,TREATMENT and OTHER.
We used 10 seed words6for each category and 10,000 unlabeled veterinaryforum texts for bootstrapping.
Finally, we createdSemantic Features for our medication use classifier.Each noun phrase tagged with a semantic class wasreplaced by a semantic type.
Then we constructedfeatures from pairs of adjacent terms in a contextwindow of size eight (+/-4) around each medica-tion mention.
For example, the word sequence?for a Boston terrier with diabetes?
would be trans-formed into ?for ANIMAL with DISSYM?
and thefeatures for this context would be: <for ANIMAL>,<ANIMAL with>, and <with DISSYM>.5.1 Medication Use Categorization ResultsTable 2 shows the results for medication use clas-sification, applied to the mentions identified by ourmedication detector (from Section 4).
The N-gram5For question phrases such as ?would he?.6We used the same seeds as (Huang and Riloff, 2010).
How-ever we added one semantic class, TREATMENT, so for this cat-egory we manually identified the 10 most frequent words in theunannotated texts that describe treatments.1455MethodRx Question Effect Change Stop Other AverageP R F P R F P R F P R F P R F P R F P R FN-grams 69 74 71 75 65 69 69 37 48 76 44 56 45 23 31 50 54 52 64.0 49.6 55.9+Sentential 68 73 71 79 71 75 74 41 53 85 45 59 49 32 38 51 54 53 67.8 52.7 59.3+Syntactic 69 72 71 78 70 74 70 40 51 77 47 59 70 49 58 51 56 53 69.3 55.7 61.8All+WordNet 69 73 71 80 70 74 73 43 54 86 49 63 72 39 54 50 54 52 71.7 54.6 62.0All+SemTaggers 69 74 71 80 70 75 78 42 55 87 51 64 73 51 60 53 55 54 73.2 57.2 64.2w/Balanced Self-Training 71 73 72 81 69 75 69 49 57 76 64 70 67 69 68 55 56 56 70.0 63.5 66.6Table 2: Medication Use Categorization Results on detected medications (each cell shows Precision, Recall, F)MethodRx Question Effect Change Stop Other AverageP R F P R F P R F P R F P R F P R F P R FN-grams 75 85 80 84 82 83 70 40 51 77 44 56 46 24 32 62 65 63 69.1 56.6 62.3+Sentential 75 83 79 89 88 89 70 45 55 86 45 59 52 38 44 61 64 63 72.4 60.4 65.9+Syntactic 76 82 79 88 87 88 70 44 54 77 57 59 80 55 65 61 65 63 75.4 63.4 68.9All+WordNet 75 83 79 89 86 88 75 46 57 81 54 65 82 45 58 60 64 62 77.2 63.1 69.4All+SemTaggers 76 85 80 90 87 88 79 47 59 89 56 68 81 57 67 65 65 65 79.8 65.9 72.2w/Balanced Self-Training 78 80 79 90 86 88 75 54 63 76 71 74 78 80 79 66 65 65 77.2 73.2 75.1Table 3: Medication Use Categorization Results on gold medications (each cell shows Precision, Recall, and F)features alone yield an average F score of 55.9%.Both the Sentential features and Syntactic features(added cumulatively) further improve performance,raising the average F score to 61.8%.
The next tworows show the effect of adding the semantic fea-tures.
WordNet improves performance for Effectand Change but recall is lower for Stop and Other.In contrast, the SemTaggers improve performanceacross all categories, raising the F score to 64.2%.Our ablation studies show the ANIMAL class con-tributed most to the improvement.In addition, we explored self-training to exploitunannotated texts.
We applied the classifiers to2,000 unlabeled veterinary texts, and used the newlylabeled instances as additional training data.
Thisdid not improve performance, presumably becausethe most common categories dominated the new in-stances.
We then explored a balanced self-trainingmethod that enforces an even distribution of the sixcategories in the new training instances.
For thisapproach, we added exactly k new instances7foreach class, where k was selected to be the size ofthe smallest set of newly labeled instances amongthe six categories.
The last row of Table 2 showsthat this balanced self-training approach improvedthe average F score from 64.2% to 66.6%.7The most confident new instances were selected based onthe differences between the scores for the winning class and theother classes.Table 3 shows the results for medication use clas-sification applied to gold standard medication men-tions.
The same trends hold: the sentential and syn-tactic features improve over n-grams, the SemTaggersemantic features add value and outperform Word-Net, and balanced self-training further improvesperformance.
Overall performance increases from66.6% to 75.1% F score with gold medications.6 ConclusionThis research introduced a new task for classifyingmedication mentions with respect to their use in pa-tient care.
We created an effective medication detec-tor for informal veterinary texts that exploits featuresderived from Web pages, and we created classifiersto recognize six medication use categories.
Theseclassifiers achieved precision ?
75% for all cate-gories except Other, with recall ranging from 54%for Effects to 86% for Questions.
This research is afirst step toward NLP systems that can acquire em-pirical data about the administration and effective-ness of medications from unstructured text.AcknowledgmentsThis material is based upon work supported bythe National Science Foundation under grant IIS-1018314.
We are very grateful to the VeterinaryInformation Network for providing samples of theirdata, and Ashequl Qadir for help annotating the data.1456ReferencesJiang Bian, Umit Topaloglu, and Fan Yu.
2012.
To-wards large-scale twitter mining for drug-related ad-verse events.
In Proceedings of the 2012 internationalworkshop on Smart health and wellbeing, pages 25?32.
ACM.Marie-Catherine De Marneffe, Bill MacCartney, Christo-pher D Manning, et al 2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC.Son Doan and Hua Xu.
2010.
Recognizing medicationrelated entities in hospital discharge summaries usingsupport vector machine.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics: Posters, pages 259?266.
Association for Compu-tational Linguistics.Son Doan, Nigel Collier, Hua Xu, Pham H Duy, andTu M Phuong.
2012.
Recognition of medication in-formation from discharge summaries using ensemblesof classifiers.
BMC medical informatics and decisionmaking, 12(1):36.Claudio Giuliano.
2009.
Fine-grained classification ofnamed entities exploiting latent semantic kernels.
InProceedings of the Thirteenth Conference on Compu-tational Natural Language Learning, pages 201?209.Association for Computational Linguistics.Scott Halgrim, Fei Xia, Imre Solti, Eithon Cadag, and?Ozlem Uzuner.
2010.
Extracting medication infor-mation from discharge summaries.
In Proceedings ofthe NAACL HLT 2010 Second Louhi Workshop on Textand Data Mining of Health Documents, pages 61?67.Association for Computational Linguistics.Ruihong Huang and Ellen Riloff.
2010.
Inducingdomain-specific semantic class taggers from (almost)nothing.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages275?285.
Association for Computational Linguistics.Robert Leaman, Laura Wojtulewicz, Ryan Sullivan, An-nie Skariah, Jian Yang, and Graciela Gonzalez.
2010.Towards internet-age pharmacovigilance: extractingadverse drug reactions from user posts to health-related social networks.
In Proceedings of the 2010workshop on biomedical natural language processing,pages 117?125.
Association for Computational Lin-guistics.Matthew A Levin, Marina Krol, Ankur M Doshi, andDavid L Reich.
2007.
Extraction and mapping ofdrug names from free text to a standardized nomencla-ture.
In AMIA Annual Symposium Proceedings, vol-ume 2007, page 438.
American Medical InformaticsAssociation.Xiao Liu and Hsinchun Chen.
2013.
Azdrugminer:an information extraction system for mining patient-reported adverse drug events in online patient forums.In Smart Health, pages 134?150.
Springer.Simon Liu, Wei Ma, Robin Moore, Vikraman Gane-san, and Stuart Nelson.
2005.
Rxnorm: prescriptionfor electronic drug information exchange.
IT profes-sional, 7(5):17?23.Mei Liu, Ruichu Cai, Yong Hu, Michael E Matheny,Jingchun Sun, Jun Hu, and Hua Xu.
2013.
Deter-mining molecular predictors of adverse drug reactionswith causality analysis based on structure learning.Journal of the American Medical Informatics Associa-tion, pages amiajnl?2013.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Proceedings of 52nd Annual Meet-ing of the Association for Computational Linguistics:System Demonstrations, pages 55?60.St?ephane M Meystre, Julien Thibault, Shuying Shen,John F Hurdle, and Brett R South.
2010.
Textractor:a hybrid system for medications and reason for theirprescription extraction from clinical text documents.Journal of the American Medical Informatics Associa-tion, 17(5):559?562.George A Miller.
1995.
Wordnet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41.Stuart J Nelson, Kelly Zeng, John Kilbourne, TammyPowell, and Robin Moore.
2011.
Normalizednames for clinical drugs: Rxnorm at 6 years.
Jour-nal of the American Medical Informatics Association,18(4):441?448.Jon Patrick and Min Li.
2010.
High accuracy informa-tion extraction of medication information from clini-cal notes: 2009 i2b2 medication extraction challenge.Journal of the American Medical Informatics Associa-tion, 17(5):524?527.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.Isabel Segura-Bedmar, Paloma Mart?nez, and Mar?aHerrero-Zazo.
2013.
Semeval-2013 task 9: Extrac-tion of drug-drug interactions from biomedical texts(ddiextraction 2013).
Proceedings of Semeval, pages341?350.Isabel Segura-Bedmar, Santiago de la Pena, and PalomaMart?nez.
2014.
Extracting drug indications and ad-verse drug reactions from spanish health social media.ACL 2014, page 98.Vikas Sindhwani and S Sathiya Keerthi.
2006.
Largescale semi-supervised linear svms.
In Proceedings of1457the 29th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 477?484.
ACM.Buzhou Tang, Hongxin Cao, Yonghui Wu, Min Jiang,and Hua Xu.
2013.
Recognizing clinical entitiesin hospital discharge summaries using structural sup-port vector machines with word representation fea-tures.
BMC medical informatics and decision making,13(Suppl 1):S1.
?Ozlem Uzuner, Imre Solti, and Eithon Cadag.
2010.Extracting medication information from clinical text.Journal of the American Medical Informatics Associa-tion, 17(5):514?518.
?Ozlem Uzuner, Brett R South, Shuying Shen, and Scott LDuVall.
2011.
2010 i2b2/va challenge on concepts,assertions, and relations in clinical text.
Journal of theAmerican Medical Informatics Association.Hua Xu, Shane P Stenner, Son Doan, Kevin B John-son, Lemuel R Waitman, and Joshua C Denny.
2010.Medex: a medication information extraction systemfor clinical narratives.
Journal of the American Medi-cal Informatics Association, 17(1):19?24.Andrew Yates and Nazli Goharian.
2013.
Adrtrace: de-tecting expected and unexpected adverse drug reac-tions from user reviews on social media sites.
Ad-vances in Information Retrieval, pages 816?819.1458
