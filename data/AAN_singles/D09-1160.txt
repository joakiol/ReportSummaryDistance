Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1542?1551,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPPolynomial to Linear: Efficient Classification with Conjunctive FeaturesNaoki YoshinagaInstitute of Industrial ScienceUniversity of Tokyo4-6-1 Komaba, Meguro-ku, Tokyoynaga@tkl.iis.u-tokyo.ac.jpMasaru KitsuregawaInstitute of Industrial ScienceUniversity of Tokyo4-6-1 Komaba, Meguro-ku, Tokyokitsure@tkl.iis.u-tokyo.ac.jpAbstractThis paper proposes a method that speedsup a classifier trained with many con-junctive features: combinations of (prim-itive) features.
The key idea is to pre-compute as partial results the weights ofprimitive feature vectors that appear fre-quently in the target NLP task.
A triecompactly stores the primitive feature vec-tors with their weights, and it enables theclassifier to find for a given feature vec-tor its longest prefix feature vector whoseweight has already been computed.
Ex-perimental results for a Japanese depen-dency parsing task show that our methodspeeded up the SVM and LLM classifiersof the parsers, which achieved accuracy of90.84/90.71%, by a factor of 10.7/11.6.1 IntroductionDeep and accurate text analysis based on discrimi-native models is not yet efficient enough as a com-ponent of real-time applications, and it is inade-quate to process Web-scale corpora for knowledgeacquisition (Pantel, 2007; Saeger et al, 2009) orsemi-supervised learning (McClosky et al, 2006;Spoustov?
et al, 2009).
One of the main reasonsfor this inefficiency is attributed to the inefficiencyof core classifiers trained with many feature com-binations (e.g., word n-grams).
Hereafter, we referto features that explicitly represent combinationsof features as conjunctive features and the otheratomic features as primitive features.The feature combinations play an essential rolein obtaining a classifier with state-of-the-art ac-curacy for several NLP tasks; recent examples in-clude dependency parsing (Koo et al, 2008), parsere-ranking (McClosky et al, 2006), pronoun reso-lution (Nguyen and Kim, 2008), and semantic rolelabeling (Liu and Sarkar, 2007).
However, ?ex-plicit?
feature combinations significantly increasethe feature space, which slows down not onlytraining but also testing of the classifier.Kernel-based methods such as support vectormachines (SVMs) consider feature combinationsspace-efficiently by using a polynomial kernelfunction (Cortes and Vapnik, 1995).
The kernel-based classification is, however, known to be veryslow in NLP tasks, so efficient classifiers shouldsum up the weights of the explicit conjunctive fea-tures (Isozaki and Kazawa, 2002; Kudo and Mat-sumoto, 2003; Goldberg and Elhadad, 2008).`1-regularized log-linear models (`1-LLMs), onthe other hand, provide sparse solutions, in whichweights of irrelevant features are exactly zero, byassuming a Laplacian prior on the weights (Tibshi-rani, 1996; Kazama and Tsujii, 2003; Goodman,2004; Gao et al, 2007).
However, as Kazama andTsujii (2005) have reported in a text categorizationtask and we later confirm in a dependency pars-ing task, when most features regarded as irrelevantduring training `1-LLMs appear rarely in the task,we cannot greatly reduce the number of active fea-tures in each classification.
In the end, when effi-ciency is a major concern, we must use exhaustivefeature selection (Wu et al, 2007; Okanohara andTsujii, 2009) or even restrict the order of conjunc-tive features at the expense of accuracy.In this study, we provide a simple, but effectivesolution to the inefficiency of classifiers trainedwith higher-order conjunctive features (or polyno-mial kernel), by exploiting the Zipfian nature oflanguage data.
The key idea is to precompute theweights of primitive feature vectors and use themas partial results to compute the weight of a givenfeature vector.
We use a trie called the featuresequence trie to efficiently find for a given fea-ture vector its longest prefix feature vector whoseweight has been computed.
The trie is built fromfeature vectors generated by applying the classifierto actual data in the classification task.
The timecomplexity of the classifier approaches time that1542is linear with respect to the number of primitivefeatures when the retrieved feature vector coversmost of the features in the input feature vector.We implemented our algorithm for SVM andLLM classifiers and evaluated the performance ofthe resulting classifiers in a Japanese dependencyparsing task.
Experimental results show that itsuccessfully speeded up classifiers trained withhigher-order conjunctive features by a factor of 10.The rest of this paper is organized as follows.Section 2 introduces LLMs and SVMs.
Section 3proposes our classification algorithm.
Section 4presents experimental results.
Section 5 concludeswith a summary and addresses future directions.2 PreliminariesIn this paper, we focus on linear classifiers that cal-culate the probability (or score) by summing upweights of individual features.
Examples includenot only log-linear models but also support vec-tor machines with kernel expansion (Isozaki andKazawa, 2002; Kudo and Matsumoto, 2003).
Be-low, we introduce these two classifiers and theirways to consider feature combinations.In classification-based NLP, the target task ismodeled as one or more classification steps.
Forexample in part-of-speech (POS) tagging, eachclassification decides whether to assign a partic-ular label (POS tag) to a given sample (each wordin a given sentence).
Each sample is then repre-sented by a feature vector x, whose element xiisa value of a feature function fi?
F .Here, we assume a binary feature functionfi(x) ?
{0, 1}, in which a non-zero value meansthat particular context data appears in the sample.We say that a feature fiis active in sample xwhenxi= fi(x) = 1 and |x| represents the number ofactive features in x (|x| = |{fi|fi(x) = 1}|).2.1 Log-Linear ModelsThe log-linear model (LLM), or also known asmaximum-entropy model (Berger et al, 1996), isa linear classifier widely used in the NLP literature.Let the training data of LLMs be {?xi, yi?
}Li=1,where xi?
{0, 1}nis a feature vector and yiis aclass label associated with xi.
We assume a binarylabel yi?
{?1} here to simplify the argument.The classifier provides conditional probabilityp(y|x) for a given feature vector x and a label y:p(y|x) =1Z(x)exp?iwi,yfi,y(x, y), (1)where fi,y(x, y) is a feature function that returnsa non-zero value when fi(x) = 1 and the label isy, wi,y?
R is a weight associated with fi,y, andZ(x) =?yexp?iwi,yfi,y(x, y) is the partitionfunction.
We can consider feature combinations inLLMs by explicitly introducing a new conjunctivefeature fF?,y(x, y) that is activated when a partic-ular set of features F??
F to be combined is acti-vated (namely, fF?,y(x, y) =?fi,y?F?fi,y(x, y)).We then introduce an `1-regularized LLM (`1-LLM), in which the weight vector w is tuned soas to maximize the logarithm of the a posterioriprobability of the training data:L(w) =L?i=1log p(yi|xi)?
C?w?1.
(2)Hyper-parameter C thereby controls the degree ofover-fitting (solution sparseness).
Interested read-ers may refer to the cited literature (Andrew andGao, 2007) for the optimization procedures.2.2 Support Vector MachinesA support vector machine (SVM) is a binary clas-sifier (Cortes and Vapnik, 1995).
Training withsamples {?xi, yi?
}Li=1where xi?
{0, 1}nandyi?
{?1} yields the following decision function:y(x) = sgn(g(x) + b)g(x) =?xj?SVyj?j?(xj)T?
(x), (3)where b ?
R, ?
: Rn7?
RHand support vec-tors xj?
SV (subset of training samples), eachof which is associated with weight ?j?
R. Wehereafter call g(x) the weight function.
Nonlinearmapping function ?
is chosen to make the train-ing samples linearly separable in RHspace.
Ker-nel function k(xj,x) = ?(xj)T?
(x) is then in-troduced to compute the dot product in RHspacewithout mapping x to ?
(x).To consider combinations of primitive featuresfj?
F , we use a polynomial kernel kd(xj,x) =(xTjx + 1)d. From Eq.
3, we obtain the weightfunction for the polynomial kernel as:g(x) =?xj?SVyj?j(xTjx+ 1)d. (4)Since we assumed that xiis a binary value repre-senting whether a (primitive) feature fiis activein the sample, the polynomial kernel of degree dimplies a mapping ?dfrom x to ?d(x) that has1543H =?dk=0(nk)dimensions.
Each dimension rep-resents a (weighted) conjunction of d features inthe original sample x.1Kernel Expansion (SVM-KE) The time com-plexity of Eq.
4 is O(|x| ?
|SV|).
This cost is usu-ally high for classifiers used in NLP tasks becausethey often have many support vectors (|SV| >10, 000).
Kernel expansion (KE) was proposedby Isozaki and Kazawa (2002) to convert Eq.
4into the linear sum of the weights in the mappedfeature space as in LLM (p(y|x) in Eq.
1):g(x) = wTxd=?iwixdi, (5)where xdis a binary feature vector whose elementxdihas a non-zero value when (?d(x))i> 0, wis the weight vector for xdin the expanded fea-ture space Fdand is precalculated from the sup-port vectors xjand their weights ?j.
Interestedreaders may refer to Kudo and Matsumoto (2003)for the detailed computation for obtaining w.The time complexity of Eq.
5 (and Eq.
1) isO(|xd|), which is linear with respect to the num-ber of active features in xdwithin the expandedfeature space Fd.Heuristic Kernel Expansion (SVM-HKE) Tomake the weight vector sparse, Kudo and Mat-sumoto (2003) proposed a heuristic method thatfilters out less useful features whose absoluteweight values are less than a pre-defined threshold?.2They reported that increased threshold value ?resulted in a dramatically sparse feature space Fd,which had the side-effects of accuracy degradationand classifier speed-up.3 Proposed MethodIn this section, we propose a method that speedsup a classifier trained with many conjunctive fea-tures.
Below, we focus on a kernel-based classifiertrained with a polynomial kernel of degree d (here,1For example, given an input vector x = (x1, x2)Tand a support vector x?= (x?1, x?2)T, the 2nd-orderpolynomial kernel returns k2(x?,x) = (x?1x1+ x?2x2+1)2= 3x?1x1+ 3x?2x2+ 2x?1x1x?2x2+ 1 (?
x?i, xi?
{0, 1}).
This function thus implies a mapping ?2(x) =(1,?3x1,?3x2,?2x1x2)T. In the following argument, weignore the dimension of the constant in the mapped space andassume constant b is set to include it.2Precisely speaking, they set different thresholds to posi-tive (?j> 0) and negative (?j< 0) support vectors, consid-ering the proportion of positive and negative support vectors.Figure 1: Efficient computation of g(x).SVMs), but an analogous argument is possible forlinear classifiers (e.g., LLMs).3We hereafter represent a binary feature vector xas a set of active features {fi|fi(x) = 1}.
x canthereby be represented as an element of the powerset 2Fof the set of features F .3.1 IdeaLet us remember that weight function g(x) inEq.
5 maps x ?
2Fto W ?
R. If we could cal-culate Wx= g(x) for all possible x in advance,we could obtain g(x) by simply checking |x| ele-ments, namely, in O(|x|) time.
However, because|{x|x ?
2F}| = 2|F|and |F| is likely to be verylarge (often |F| > 10, 000 in NLP tasks), this cal-culation is impractical.We then compute and store weight Wx?=g(x?)
for x??
Vc(?
2F), a certain subset ofthe possible value space, and compute g(x) forx /?
Vcby using precalculated weight Wxcforxc?4x in the following way:g(x) = Wxc+?fi?xd?xdcwi.
(6)Intuitively speaking, starting from partial weightWxc, we add up remaining weights of primitivefeatures f ?
F that are not active in xcbut activein x and conjunctive features that combine f andthe other active features in x.An example of this computation (d = 2) is de-picted in Figure 1.
We can efficiently computeg(x) for a vector x that has four active featuresf1, f2, f3, and f4(and x2has their six conjunc-tive features) using precalculated weight W{1,2,3};we should first check the three features f1, f2, andf3to retrieve W{1,2,3}and next check the remain-ing four features related to f4, namely f4, f1,4,f2,4, and f3,4, in order to add up the remaining3When a feature vector x includes (explicit) conjunctivefeatures f ?
Fd, we assume weight function g?(y|x?)
=g(y|x), where x?is a projection of x (by ?
?1d: Fd?
F ).4This means that all active features in xcare active in x.1544weights, while the normal computation in Eq.
5should check the four primitive and six conjunc-tive features to get the individual weights.Expected time complexity Counting the num-ber of features to be checked in the computation,we obtain the time complexity f(x, d) of Eq.
6 as:f(x, d) = O(|xc|+ |xd| ?
|xdc|), (7)where |xd| =d?k=1(|x|k)(8)(e.g., |x2| =|x|2+|x|2and |x3| =|x|3+5|x|6).5Notethat when |xc| becomes close to |x|, this timecomplexity actually approaches O(|x|).Thus, to minimize this computational cost, xcis to be chosen from Vcas follows:xc= argminx??Vc,x?
?x(|x?|+ |xd| ?
|x?d|).
(9)3.2 Construction of Feature Sequence TrieThere are two issues with speeding up the classi-fier by the computation shown in Eq.
6.
First, sincewe can store weights for only a small fraction ofpossible feature vectors (namely, |Vc|  2|F|), weshould choose Vcso as to maximize its impact onthe speed-up.
Second, we should quickly find anoptimal xcfrom Vcfor a given feature vector x.The solution to the first problem is to enumer-ate partial feature vectors that frequently appear inthe target task.
Note that typical linguistic featuresused in NLP tasks usually consist of disjunctivesets of features (e.g., word surface and POS), inwhich each set is likely to follow Zipf?s law (Zipf,1949) and correlate with each other.
We can ex-pect the distribution of feature vectors, the mixtureof Zipf distributions, to be Zipfian.
This has beenconfirmed for word n-grams (Egghe, 2000) anditemset support distribution (Chuang et al, 2008).We can thereby expect that a small set of partialfeature vectors commonly appear in the task.To solve the second problem, we introduce afeature sequence trie (fstrie), which represents ahierarchy of feature vectors, to enable the clas-sifier to efficiently retrieve (sub-)optimal xc(inEq.
9) for a given feature vector x.
We build anfstrie in the following steps:Step 1: Apply the target classifier to actual (raw)data in the task to enumerate possible featurevectors (hereafter, source feature vectors).5This is the maximum number of conjunctive features.Figure 2: Feature sequence trie and completion ofprefix feature vector weights.Step 2: Sort the features in each source featurevector according to their frequency in thetraining data (in descending order).Step 3: Build a trie from the source feature vec-tors by regarding feature indices as charactersand store weights of all prefix feature vectors.An fstrie built from six source feature vectors isshown in Figure 2.
In fstries, a path from the rootto another node represents a feature vector.
Animportant point here is that the fstrie stores theweights of all prefix feature vectors of the sourcefeature vectors, and the trie structure enables us toretrieve for a given feature vector x the weight ofits longest prefix vector xc?
x in O(|xc|) time.To handle feature functions in LLMs (Eq.
1), westore partial weight Wxc,y=?iwi,yfi,y(xc, y)for each label y on the node that expresses xc.Since we sort the features in the source fea-ture vectors according to their frequency, the pre-fix feature vectors exclude less frequent featuresin the source feature vectors.
Lexical features orfiner-grained features (e.g., POS-subcategory) areusually less frequent than coarse-grained features(e.g., POS), so they lie in the latter part of thefeature vectors.
This sorting helps us to retrievelonger feature vector xcfor input feature vector xthat will have diverse infrequent features.
It alsominimizes the size of fstrie by sharing the com-mon frequent prefix (e.g., {f1, f2} in Figure 2).Pruning nodes from fstrie We have so far de-scribed the way to construct an fstrie from thesource feature vectors.
However, a naive enumer-ation of source feature vectors will result in theexplosion of the fstrie size, and we want to havea principled way to control the fstrie size ratherthan reducing the processed data size.
Below, wepresent a method that prunes useless prefix featurevectors (nodes) from the constructed fstrie to max-imize its impact on the classifier efficiency.1545Algorithm 1 PRUNE NODES FROM FSTRIEInput: fstrie T , node_limit N ?
NOutput: fstrie T1: while # of nodes in T > N do2: xc?
argminx?
?leaf(T )u(x?
)3: remove xc, T4: end while5: return TWe adopt a greedy strategy that iterativelyprunes a leaf node (one prefix feature vector andits weight) from the fstrie built from all the sourcefeature vectors, according to a certain utility scorecalculated for each node.
In this study, we con-sider two metrics for each prefix feature vector xcto calculate its utility score.Probability p(xc), which denotes how often thestored weight Wxcwill be used in the tar-get task.
The maximum-likelihood estima-tion provides probability:p(xc) =?x??xcnx?
?xnx, (10)where nx?
N is the frequency count of asource feature vector x in the processed data.Computation reduction ?d(xc), which denoteshow much computation is reduced by Wxctocalculate a weight of x ?
xc.
This can be es-timated by counting the number of conjunc-tive features we additionally have to checkwhen we remove xc.
Since the fstrie storesthe weight of a prefix feature vector xc-?
xcsuch that |xc-| = |xc| ?
1 (e.g., in Figure 2,xc-= {f1, f2} for xc= {f1, f2, f4}), wecan define the computation reduction as:?d(xc) = (|xdc| ?
|xdc-|)?
(|xc| ?
|xc-|)=d?k=2(|xc|k)?d?k=2(|xc| ?
1k)(?
Eq.
8).
?2(xc) = |xc| ?
1 and ?3(xc) =|xc|2?|xc|2.We calculate utility score of each node xcin thefstrie as u(xc) = p(xc) ?
?d(xc), which meansthe expected computation reduction by xcin thetarget task, and prune the lowest-utility-score leafnodes from the fstrie one by one (Algorithm 1).
Ifseveral prefix vectors have the same utility score,we eliminate them in numerical descending order.Algorithm 2 COMPUTE WEIGHT WITH FSTRIEInput: fstrie T , weight vector w ?
R|Fd|feature vector x ?
2FOutput: weight W = g(x) ?
R1: x?
sort(x)2: ?xc,Wxc?
?
prefix_search(T , x)3: W ?Wxc4: for all feature fj?
xd?
xdcdo5: W ?W + wj6: end for7: return W3.3 Classification AlgorithmOur classification algorithm is shown in detail inAlgorithm 2.
The classifier first sorts the activefeatures in input feature vectorx according to theirfrequency in the training data.
Then, for x, it re-trieves the longest common prefix vector xcfromthe fstrie (line 2 in Algorithm 2).
It then adds theweights of the remaining features to partial weightWxc(line 5 in Algorithm 2).Note that the remaining features whose weightswe sum up (line 4 in Algorithm 2) are primitiveand conjunctive features that relate to f ?
x?xc,which appear less frequently than f??
xcin thetraining data.
Thus, when we apply our algorithmto classifiers with the sparse solution (e.g., SVM-HKEs or `1-LLMs), |xd|?|xdc| can be much smallerthan the theoretical expectation (Eq.
8).
We con-firmed this in the following experiments.4 EvaluationWe applied our algorithm to SVM-KE, SVM-HKE,and `1-LLM classifiers and evaluated the resultingclassifiers in a Japanese dependency parsing task.To the best of our knowledge, there are no previousreports of an exact weight calculation faster thanlinear summation (Eqs.
1 and 5).
We also com-pared our SVM classifier with a classifier calledpolynomial kernel inverted (PKI: Kudo and Mat-sumoto (2003)), which uses the polynomial kernel(Eq.
4) and inverted indexing to support vectors.4.1 Experimental SettingsA Japanese dependency parser inputs bunsetsu-segmented sentences and outputs the correct head(bunsetsu) for each bunsetsu; here, a bunsetsu isa grammatical unit in Japanese consisting of oneor more content words followed by zero or morefunction words.
A parser generates a feature vec-1546Modifier,modifieebunsetsuhead word (surface-form, POS, POS-subcategory,inflection form), functional word (surface-form,POS, POS-subcategory, inflection form), brackets,quotation marks, punctuation marks, position insentence (beginning, end)Betweenbunsetsusdistance (1, 2?5, 6?
), case-particles, brackets,quotation marks, punctuation marksTable 1: Feature set used for experiments.tor for a particular pair of bunsetsus (modifier andmodifiee candidates) by exploiting the head-finaland projective (Nivre, 2003) nature of dependencyrelations in Japanese.
The classifier then outputslabel y = ?+1?
(dependent) or ??1?
(independent).Since our classifier is independent of individ-ual parsing algorithms, we targeted speeding up(a classifier in) the shift-reduce parser proposedby Sassano (2004), which has been reported to bethe most efficient for this task, with almost state-of-the-art accuracy (Iwatate et al, 2008).
Thisparser decreases the number of classification stepsby using the fact that a bunsetsu is likely to modifya bunsetsu close to itself.
Due to space limitations,we omit the details of the parsing algorithm.We used the standard feature set tailored for thistask (Kudo and Matsumoto, 2002; Sassano, 2004;Iwatate et al, 2008) (Table 1).
Note that featureslisted in the ?Between bunsetsus?
row representcontexts between the target pair of bunsetsus andappear independently from other features, whichwill become an obstacle to finding the longest pre-fix vector.
This task is therefore a better measureof our method than simple sequential labeling suchas POS tagging or named-entity recognition.For evaluation, we used Kyoto Text Corpus Ver-sion 4.0 (Kurohashi and Nagao, 2003), Mainichinews articles in 1995 that have been manually an-notated with dependency relations.6The train-ing, development, and test sets included 24,283,4833, and 9284 sentences, and 234,685, 47,571,and 89,874 bunsetsus, respectively.
The trainingsamples generated from the training set included150,064 positive and 146,712 negative samples.The following experiments were performed ona server with an IntelR?
XeonTM3.20-GHz CPU.We used TinySVM7and a simple C++ library formaximum entropy classification8to train SVMsand `1-LLMs, respectively.
We used Darts-Clone,96http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus-e.html7http://chasen.org/?taku/software/TinySVM/8http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/maxent/9http://code.google.com/p/darts-clone/Model type Model statistics Dep.
Sent.Model d ?
/ ?
|Fd| |xd| acc.
acc.SVM-KE 1 0 39712 27.3 88.29 46.49SVM-KE 2 0 1478109 380.6 90.76 53.83SVM-KE 3 0 26194354 3286.7 90.9354.43SVM-HKE 3 0.001 13247675 2725.9 90.9254.39SVM-HKE 3 0.002 2514385 2238.1 90.9154.32>SVM-HKE 3 0.003 793195 1855.4 90.83 54.21SVM-KE 4 0 293416102 20395.4 90.9154.69SVM-HKE 4 0.0002 96522236 15282.1 90.9354.53>SVM-HKE 4 0.0004 19245076 11565.0 90.9654.64SVM-HKE 4 0.0006 7277592 8958.2 90.84 54.48>`1-LLM 1 1.0 9268 26.5 88.22 46.06`1-LLM 2 2.0 32575 309.8 90.62 53.46`1-LLM 3 3.0 129503 2088.3 90.71 54.09>`1-LLM 3 4.0 85419 1803.0 90.61 53.79`1-LLM 3 5.0 63046 1699.5 90.59 53.55Table 2: Specifications of LLMs and SVMs.
Theaccuracy marked with ?
?
or ?>?
was signifi-cantly better than the d = 2 counterpart (p < 0.01or 0.01 ?
p < 0.05 by McNemar?s test).a double-array trie (Aoe, 1989; Yata et al, 2008),as a compact trie implementation.
All these li-braries and algorithms are implemented in C++.The code for building fstries occupies 100 lines,while the code for the classifier occupies 20 lines(except those for kernel expansion).4.2 ResultsSpecifications of SVMs and LLMs used here areshown in Table 2; |Fd| is the number of active fea-tures, while |xd| is the average number of activefeatures in each classification for the test corpus.Dependency accuracy is the ratio of dependencyrelations correctly identified by the parser, whilesentence accuracy is the exact match accuracy ofcomplete dependency relations in a sentence.For LLM training, we designed explicit conjunc-tive features for all the d or lower-order featurecombinations to make the results comparable tothose of SVMs.
We could not train d = 4 LLMsdue to parameter explosion.
We varied SVM softmargin parameter c from 0.1 to 0.000001 and LLMwidth factor parameter ?,10which controls the im-pact of the prior, from 1.0 to 5.0, and adjustedthe values to maximize dependency accuracy forthe development set: (d, c) = (1, 0.1), (2, 0.005),(3, 0.0001), (4, 0.000005) for SVMs and (d, ?)
=(1, 1.0), (2, 2.0), (3, 4.0) for `1-LLMs.The accuracy of around 90.9% (SVM-KE, d =3, 4) is close to the performance of state-of-the-10The parameter C of `1-LLM in Eq.
2 was set to ?/L(referred to in Kazama and Tsujii (2003) as ?single width?
).1547Model PKI Baseline Proposed w/ fstrieSProposed w/ fstrieMProposed w/ fstrieLSpeedtype d classify Mem.
Time [ms/sent.]
Mem.
Time [ms/sent.]
Mem.
Time [ms/sent.]
Mem.
Time [ms/sent.]
up[ms/sent.]
(MB) classify (total) (MB) classify (total) (MB) classify (total) (MB) classify (total)SVM-KE 1 13.480 0.2 0.003 (0.015) +0.6 0.006 (0.018) +20.2 0.007 (0.018) +662.9 0.016 (0.029) NASVM-KE 2 10.313 13.5 0.041 (0.054) +0.5 0.020 (0.032) +18.0 0.021 (0.034) +662.4 0.023 (0.036) 2.1SVM-KE 3 10.945 142.2 0.345 (0.361) +0.5 0.163 (0.178) +18.2 0.108 (0.123) +667.0 0.079 (0.093) 4.4SVM-KE 4 12.603 648.0 2.338 (2.363) +0.5 1.156 (1.178) +18.6 0.671 (0.690) +675.9 0.415 (0.432) 5.6Table 3: Parsing results for test corpus: SVM-KE classifiers with dense feature space.art parsers (Iwatate et al, 2008), and the modelstatistics are considered to be complex (or re-alistic) enough to evaluate our classifier?s util-ity.
The number of support vectors of SVMs was71, 766 ?
9.2%, which is twice as many as thoseused by Kudo and Matsumoto (2003) (34,996) intheir experiments on the same task.We could clearly observe that the number of ac-tive features |xd| increased dramatically accordingto the order d of feature combinations.
The den-sity of |xd| for SVMs was very high (e.g., |x3| =3286.7, close to the maximum shown in Eq.
8:(27.33+ 5?
27.3)/6 ' 3414.For d ?
3 models, we attempted to controlthe size of the feature space |Fd| by changingthe model?s hyper-parameters: threshold ?
for theSVM-HKE and width factor ?
for the `1-LLM.
Al-though we successfully reduced the size of the fea-ture space |Fd|, we could not dramatically reducethe average number of active features |xd| in eachclassification while keeping the accuracy advan-tage.
This confirms that the solution sparsenessdoes not suffice to obtain an efficient classifier.We obtained source feature vectors to buildfstries by applying parsers with the target clas-sifiers to a raw corpus in the target domain,3,258,313 sentences of 1991?94 Mainichi newsarticles that were morphologically analyzed byJUMAN6and segmented into bunsetsus by KNP.6We first built fstrieLusing all the source featurevectors.
We then attempted to reduce the numberof prefix feature vectors in fstrieLto 1/2nthe sizeby Algorithm 1.
We refer to fstries built from 1/32and 1/1024 of the prefix feature vectors in fstrieLas fstrieMand fstrieSin the following experiments.Because we exploited Algorithm 2 to calcu-late the weights of the prefix feature vectors, ittook less than one hour (59 min.
29 sec.)
on the3.20-GHz server to build fstrieL(and calculate theutility score for all the nodes in it) for the slow-est SVM-KE (d = 4) from the 40,409,190 sourcefeature vectors (62,654,549 prefix feature vectors)generated by parsing the 3,258,313 sentences.00.511.522.50 100 200 300 400 500 600 700Ave.classificationtime[ms/sent.
]Size of fstrie [MB]SVM-KE (d = 1)SVM-KE (d = 2)SVM-KE (d = 3)SVM-KE (d = 4)Figure 3: Average classification time per sentenceplotted against size of fstrie: SVM-KE.Results for SVM-KE with dense feature spaceThe performances of parsers having SVM-KE clas-sifiers with and without the fstrie are given in Ta-ble 3.
The ?speed-up?
column shows the speed-upfactor of the most efficient classifier (bold) ver-sus the baseline classifier without fstries.
Sinceeach classifier solved a slightly different num-ber of classification steps (112, 853?
0.15%), weshow the (average) cumulative classification timefor a sentence.
The Mem.
columns show the sizeof weight vectors for SVM-KE classifiers and thesize of fstriesS, fstriesM, and fstriesL, respectively.The fstries successfully speeded up SVM-KEclassifiers with the dense feature space.11TheSVM-KE classifiers without fstries were still fasterthan PKI, but as expected from a large |xd| value,the classifiers with higher conjunctive featureswere much slower than the classifier with onlyprimitive features by factors of 13 (d = 2), 109(d = 3) and 738 (d = 4) and the classificationtime accounted for most of the parsing time.The average classification time of our classifiersplotted against fstrie size is shown in Figure 3.Surprisingly, we obtained a significant speed-upeven with tiny fstrie sizes of < 1 MB.
Further-more, we naively controlled the fstrie size by sim-11The inefficiency of the classifier (d = 1) results from thecost of the additional sort function (line 1 in Algorithm 2) andCPU cache failure due to random accesses to the huge fstries.1548Model Baseline Proposed w/ fstrieSProposed w/ fstrieMProposed w/ fstrieLSpeedtype d ?
/ ?
Mem.
Time [ms/sent.]
Mem.
Time [ms/sent.]
Mem.
Time [ms/sent.]
Mem.
Time [ms/sent.]
up(MB) classify (total) (MB) classify (total) (MB) classify (total) (MB) classify (total)SVM-HKE 3 0.001 64.6 0.348 (0.363) +0.5 0.151 (0.166) +17.6 0.097 (0.111) +638.0 0.070 (0.084) 5.0SVM-HKE 3 0.002 13.9 0.332 (0.346) +0.5 0.123 (0.137) +17.0 0.074 (0.088) +612.2 0.053 (0.067) 6.2SVM-HKE 3 0.003 4.2 0.314 (0.328) +0.4 0.102 (0.115) +14.7 0.057 (0.070) +526.2 0.041 (0.054) 7.8SVM-HKE 4 0.0002 235.0 2.258 (2.280) +0.5 1.022 (1.042) +17.7 0.558 (0.575) +637.1 0.330 (0.346) 6.8SVM-HKE 4 0.0004 82.8 2.038 (2.058) +0.5 0.816 (0.835) +16.8 0.414 (0.430) +601.7 0.234 (0.249) 8.7SVM-HKE 4 0.0006 32.2 1.802 (1.820) +0.4 0.646 (0.662) +15.7 0.311 (0.326) +558.9 0.168 (0.183) 10.7`1-LLM 1 1.0 0.1 0.004 (0.016) +0.8 0.006 (0.018) +25.0 0.007 (0.019) +787.7 0.016 (0.029) NA`1-LLM 2 2.0 0.4 0.043 (0.055) +0.6 0.016 (0.028) +20.5 0.015 (0.027) +698.0 0.018 (0.030) 2.9`1-LLM 3 3.0 1.0 0.314 (0.326) +0.5 0.091 (0.103) +17.8 0.041 (0.054) +601.0 0.027 (0.040) 11.6`1-LLM 3 4.0 0.7 0.300 (0.313) +0.5 0.082 (0.094) +16.3 0.036 (0.049) +550.1 0.024 (0.037) 12.4`1-LLM 3 5.0 0.5 0.290 (0.302) +0.5 0.076 (0.088) +15.1 0.032 (0.045) +510.7 0.022 (0.035) 13.3Table 4: Parsing results for test corpus: SVM-HKE and `1-LLM classifiers with sparse feature space.00.511.522.50 10 20 30 40 50 60 70 80Ave.classificationtime[ms/sent.
]Size of fstrie [MB]0.671 ms/sent.
(18.6 MB)0.680 ms/sent.
(67.1 MB)naiveutility scoreFigure 4: Fstrie reduction: utility score vs. pro-cessed sentence reduction for SVM-KE (d = 4).ply reducing the number of sentences processed to1/2n.
The impact on the speed-up of the resultingfstries (naive) and the fstries constructed by ourutility score (utility-score) on SVM-KE (d = 4)is shown in Figure 4.
The Zipfian nature of lan-guage data let us obtain a substantial speed-upeven when we naively reduced the fstrie size, andthe utility score further decreased the fstrie sizerequired to obtain the same speed-up.
We neededless than 1/3 size fstries to achieve the same speed-up: 0.671 ms./sent.
(18.6 MB) (utility-score) vs.0.680 ms./sent.
(67.1 MB) (naive).Results for SVM-HKE and `1-LLM classifierswith sparse feature space The performances ofparsers having SVM-HKE and `1-LLM classifierswith and without the fstrie are given in Table 4.The fstries successfully speeded up the SVM-HKEand `1-LLM classifiers by factors of 10.7 (SVM-HKE, d = 4, ?
= 0.0006) and 11.6 (`1-LLM,d = 3, ?
= 3.0).
We obtained more speed-up when we used fstries for classifiers with moresparse feature space Fd(Figures 5 and 6).
Theparsing speed with d = 3 models are now compa-rable to the parsing speed with d = 2 models.00.050.10.150.20.250.30.350.40 100 200 300 400 500 600 700Ave.classificationtime[ms/sent.
]Size of fstrie [MB]SVM-KE (d = 3)SVM-HKE (d = 3, ?
= 0.001)SVM-HKE (d = 3, ?
= 0.002)SVM-HKE (d = 3, ?
= 0.003)Figure 5: Average classification time per sentenceplotted against size of fstrie: SVM-HKE (d = 3).00.050.10.150.20.250.30.350.40 100 200 300 400 500 600 700Ave.classificationtime[ms/sent.
]Size of fstrie [MB]?1-LLM (d = 3, ?
= 3.0)?1-LLM (d = 3, ?
= 4.0)?1-LLM (d = 3, ?
= 5.0)Figure 6: Average classification time per sentenceplotted against size of fstrie: `1-LLM (d = 3).Without fstries, little speed-up of SVM-HKEclassifiers versus the SVM-KE classifiers (in Ta-ble 3) was obtained due to the mild reduction inthe average number of active features |xd| in theclassification.
This result conforms to the resultsreported in (Kudo and Matsumoto, 2003).The parsing speed reached 14,937 sentencesper second with accuracy of 90.91% (SVM-HKE,d = 3, ?
= 0.002).
We used this parser to pro-cess 1,005,918 sentences (5,934,184 bunsetsus)randomly extracted from Japanese weblog feeds1549updated in November 2008, to see how much theimpact of fstries lessens when the test data andthe data processed to build fstries mismatch.
Theparsing time was 156.4 sec.
without fstrieL, whileit was just 35.9 sec.
with fstrieL.
The speed-upfactor of 4.4 on weblog feeds was slightly worsethan that on news articles (0.346/0.067 = 5.2)but still evident.
This implies that sorting featuresin building fstries yielded prefix features vectorsthat commonly appear in this task, by excludingdomain-specific features such as lexical features.In summary, our algorithm successfully mini-mized the efficiency gap among classifiers withdifferent degrees of feature combinations andmade accurate classifiers trained with higher-orderfeature combinations practical.5 Conclusion and Future WorkOur simple method speeds up a classifier trainedwith many conjunctive features by using precal-culated weights of (partial) feature vectors storedin a feature sequence trie (fstrie).
We experimen-tally demonstrated that it speeded up SVM andLLM classifiers for a Japanese dependency pars-ing task by a factor of 10.
We also confirmed thatthe sparse feature space provided by `1-LLMs andSVM-HKEs contributed much to size reduction ofthe fstrie required to achieve the same speed-up.The implementations of the proposed algorithmfor LLMs and SVMs (with a polynomial kernel) andthe Japanese dependency parser will be availableat http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/.We plan to apply our method to wider range ofclassifiers used in various NLP tasks.
To speed upclassifiers used in a real-time application, we canbuild fstries incrementally by using feature vec-tors generated from user inputs.
When we run ourclassifiers on resource-tight environments such ascell-phones, we can use a random feature mix-ing technique (Ganchev and Dredze, 2008) or amemory-efficient trie implementation based on asuccinct data structure (Jacobson, 1989; Delprattet al, 2006) to reduce required memory usage.We will combine our method with other tech-niques that provide sparse solutions, for example,kernel methods on a budget (Dekel and Singer,2007; Dekel et al, 2008; Orabona et al, 2008) orkernel approximation (surveyed in Kashima et al(2009)).
It is also easy to combine our methodwith SVMs with partial kernel expansion (Gold-berg and Elhadad, 2008), which will yield slowerbut more space-efficient classifiers.
We will inthe future consider an issue of speeding up decod-ing with structured models (Lafferty et al, 2001;Miyao and Tsujii, 2002; Sutton et al, 2004).Acknowledgment The authors wish to thankSusumu Yata and Yoshimasa Tsuruoka for lettingthe authors to use their pre-release libraries.
Theauthors also thank Nobuhiro Kaji and the anony-mous reviewers for their valuable comments.ReferencesGalen Andrew and Jianfeng Gao.
2007.
Scalable train-ing of L1-regularized log-linear models.
In Proc.ICML 2007, pages 33?40.Jun?ichi Aoe.
1989.
An efficient digital search al-gorithm by using a double-array structure.
IEEETransactions on Software Engineering, 15(9):1066?1077, September.Adam Berger, Stephen Della Pietra, and Vincent DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational Linguis-tics, 22(1):39?71, March.Kun-Ta Chuang, Jiun-Long Huang, and Ming-SyanChen.
2008.
Power-law relationship andself-similarity in the itemset support distribution:analysis and applications.
The VLDB Journal,17(5):1121?1141, August.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine Learning, 20(3):273?297, September.Ofer Dekel and Yoram Singer.
2007.
Support vec-tor machines on a budget.
In Bernhard Sch?lkopf,John Platt, and Thomas Hofmann, editors, Advancesin Neural Information Processing Systems 19, pages345?352.
The MIT Press.Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer.2008.
The forgetron: A kernel-based perceptron ona budget.
SIAM Journal on Computing, 37(5):1342?1372, January.O?Neil Delpratt, Naila Rahman, and Rajeev Raman.2006.
Engineering the LOUDS succinct tree rep-resentation.
In Proc.
WEA 2006, pages 134?145.Leo Egghe.
2000.
The distribution of n-grams.
Scien-tometrics, 47(2):237?252, February.Kuzman Ganchev and Mark Dredze.
2008.
Small sta-tistical models by random feature mixing.
In Proc.ACL 2008 Workshop on Mobile Language Process-ing, pages 19?20.Jianfeng Gao, Galen Andrew, Mark Johnson, andKristina Toutanova.
2007.
A comparative study1550of parameter estimation methods for statistical natu-ral language processing.
In Proc.
ACL 2007, pages824?831.Yoav Goldberg and Michael Elhadad.
2008.splitSVM: Fast, space-efficient, non-heuristic, poly-nomial kernel computation for NLP applications.
InProc.
ACL 2008, Short Papers, pages 237?240.Joshua Goodman.
2004.
Exponential priors for max-imum entropy models.
In Proc.
HLT-NAACL 2004,pages 305?311.Hideki Isozaki and Hideto Kazawa.
2002.
Efficientsupport vector classifiers for named entity recogni-tion.
In Proc.
COLING 2002, pages 1?7.Masakazu Iwatate, Masayuki Asahara, and Yuji Mat-sumoto.
2008.
Japanese dependency parsing usinga tournament model.
In Proc.
COLING 2008, pages361?368.Guy Jacobson.
1989.
Space-efficient static trees andgraphs.
In Proc.
FOCS 1989, pages 549?554.Hisashi Kashima, Tsuyoshi Id?, Tsuyoshi Kato, andMasashi Sugiyama.
2009.
Recent advances andtrends in large-scale kernel methods.
IEICE Trans-actions on on Information and Systems, E92-D. toappear.Jun?ichi Kazama and Jun?ichi Tsujii.
2003.
Evaluationand extension of maximum entropy models with in-equality constraints.
In Proc.
EMNLP 2003, pages137?144.Jun?ichi Kazama and Jun?ichi Tsujii.
2005.
Maxi-mum entropy models with inequality constraints: Acase study on text categorization.
Machine Learn-ing, 60(1-3):159?194.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proc.
ACL 2008, pages 595?603.Taku Kudo and Yuji Matsumoto.
2002.
Japanesedependency analysis using cascaded chunking.
InProc.
CoNLL 2002, pages 1?7.Taku Kudo and Yuji Matsumoto.
2003.
Fast methodsfor kernel-based text analysis.
In Proc.
ACL 2003,pages 24?31.Sadao Kurohashi and Makoto Nagao.
2003.
Build-ing a Japanese parsed corpus.
In Anne Abeill?, edi-tor, Treebank: Building and Using Parsed Corpora,pages 249?260.
Kluwer Academic Publishers.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proc.
ICML 2001, pages 282?289.Yudong Liu and Anoop Sarkar.
2007.
Experimentalevaluation of LTAG-based features for semantic rolelabeling.
In Proc.
EMNLP 2007, pages 590?599.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProc.
HLT-NAACL 2006, pages 152?159.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximumentropy estimation for feature forests.
In Proc.
HLT2002, pages 292?297.Ngan L.T.
Nguyen and Jin-Dong Kim.
2008.
Explor-ing domain differences for the design of a pronounresolution system for biomedical texts.
In Proc.COLING 2008, pages 625?632.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proc.
IWPT 2003,pages 149?160.Daisuke Okanohara and Jun?ichi Tsujii.
2009.
Learn-ing combination features with L1regularization.
InProc.
HLT-NAACL 2009, Short Papers, pages 97?100.Francesco Orabona, Joseph Keshet, and Barbara Ca-puto.
2008.
The projectron: a bounded kernel-basedperceptron.
In Proc.
ICML 2008, pages 720?727.Patrick Pantel.
2007.
Data catalysis: Facilitating large-scale natural language data processing.
In Proc.ISUC, pages 201?204.Stijn De Saeger, Kentaro Torisawa, and Jun?ichiKazama.
2009.
Mining web-scale treebanks.
InProc.
NLP 2009, pages 837?840.Manabu Sassano.
2004.
Linear-time dependency anal-ysis for Japanese.
In Proc.
COLING 2004, pages8?14.Drahom?ra ?Johanka?
Spoustov?, Jan Haji?c, Jan Raab,and Miroslav Spousta.
2009.
Semi-supervisedtraining for the averaged perceptron POS tagger.
InProc.
EACL 2009, pages 763?771.Charles Sutton, Khashayar Rohanimanesh, and An-drew McCallum.
2004.
Dynamic conditional ran-dom fields: factorized probabilistic models for label-ing and segmenting sequence data.
In Proc.
ICML2004, pages 783?790.Robert Tibshirani.
1996.
Regression shrinkage and se-lection via the lasso.
Journal of the Royal StatisticalSociety.
Series B, 58(1):267?288, April.Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee.
2007.An approximate approach for training polynomialkernel SVMs in linear time.
In Proc.
ACL 2007Poster and Demo Sessions, pages 65?68.Susumu Yata, Kazuhiro Morita, Masao Fuketa, andJun?ichi Aoe.
2008.
Fast string matching withspace-efficient word graphs.
In Proc.
Innovationsin Information Technology 2008, pages 79?83.George K. Zipf.
1949.
Human Behavior and the Prin-ciple of Least-Effort.
Addison-Wesley.1551
