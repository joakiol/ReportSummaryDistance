Modeling Conversational Speechfor Speech RecognitionMarie Meteer and Rukmini IyerBBN Systems & Technologies70 Fawcett St.Cambridge MA 02138mmeteer@bbn.com, fiyer@bbn.comAbstractIn language modeling for speech recognition the goal is toconstrain the search of the speech recognizer by providing amodel which can, given a context, indicate what the nextmost likely word will be.
In this paper, we explore how theaddition of information to the text, in particular part ofspeech and dysfluency annotations, can be used to,buildmore complex language models.
In particular, we ask twoquestions.
First, in conversational speech, where there is aless clear notion of "sentence" than in written text, doessegmenting the text into linguistically or semanticallybased units contribute to a better language model thanmerely segmenting based on broad acoustic information,such as pauses.
Second, is the sentence itself a good unit tobe modeling, or should we look at smaller units, forexample, dividing a sentence into a "given" and "new"portion and segmenting out acknowledgments and replies.To answer these questions, we present a variety of kinds ofanalysis, from vocabulary distributions to perplexities onlanguage models.
The next step will be modelingconversations and incorporating those models into a speechrecognizer.1 IntroductionIn language modeling for speech recognition the goal is toconstrain the search of the speech recognizer by providing amodel which can, given a context, indicate what the nextmost likely word will be.
Currently, the field ispredominated bythe use of n-grams, in particular bi-gramsand tri-grams.
One advantage of this type of model is thatonly the text itself is needed to create the model.
In the casewhere what is being modeled is written text in a style forwhich millions of words of text are available, such as theWSJ corpus, this kind of modeling is effective.
However,this is rarely the case since the ultimate goal of speechrecognition is to model extemporaneous spoken language.The interesting problem in language modeling is how tobring generalizations above the level of the wordsthemselves to the text.
One approach is to annotate text,either by hand or semi-automatically, to bring additionalinformation to the text.
Another is to develop algorithmsthat use rules or heuristics that operate over the corpus,again bringing additional information.
It is this additionalinformation that can help create generalizations over thetext and contribute to a model which can go beyond thetraining corpus.In this paper, we describe annotations to the Switchboardcorpus which add part of speech and dysfluency markingsand our latest work using those annotations to create a levelof generalization on top of the annotation to capture thestructure of conversational speech.
In particular, we ask twoquestions.
First, in conversational speech, where there is aless clear notion of "sentence" than in written text, doessegmenting the text into linguistically or semanticallybased units contribute to a better language model thanmerely segmenting based on broad acoustic information,such as pauses.
Second, is the sentence itself a good unit tobe modeling, or should we look at smaller units, forexample, dividing a sentence into a "given" and "new"portion.
To answer these questions, we present a variety ofkinds of analysis, from vocabulary distributions toperplexities on language models.1.1 The Switchboard CorpusThe Switchboard corpus (Godfrey, et al 1992) consists of 2million words of conversational English collected over thephone by having strangers chat with one another about 70different topics ranging from pets and family life toeducation and gun control.
Each conversation is about fiveminutes long and the transcription i cludes information onbackground noises and where conversants overlapped (talkedover one another).33Conversational speech is particularly difficult for speechrecognizers, since not only is the speech often very quickand "sloppy" (e.g.
with "mgonna" for "I am going to"), butit is also very dysfluent, with many fillers, such as "uh"and "you know", and many restarts, such as "I, uh, well, Iwent, I went away last summer".
It has been surmised thatit is the dysfluencies that make the language modelingparticularly difficult, but no studies have been able toconclusively show that that is true or explain the problemssufficiently.
In fact, at the 1995 Language ModelingWorkshop at Johns Hopkins they showed that sentenceswith dysfluencies do not perform any worse in recognitionword error rates than sentences without dysfluencies; theyhave a slightly lower error rate.
Whether this has to do withthe overall sentence length or if different kinds ofdysfluencies correlate differently with error rate still needs tobe explored.
Stolcke and Shriberg (1996) have begun workin this area.
In particular, they show that some results aredifferent depending on whether the data was segmentedlinguistically rather than acoustically (see ?3).The Switchboard annotation effort had four parts: part ofspeech annotation, "sentence" boundaries, dysfluencies, andbracketing.
Part of speech and bracketing were done in theUPenn treebank style (and all annotation was done at Upennby the Linguistic Data Consortium (LDC).
The dysfluencyand sentence boundary annotation stylebook was begun atBBN by the authors of this paper and completed at UPennby Ann Taylor.
We elaborate on these later two types ofannotation i  Section Two.
The main target audience forthis annotation was the 1995 Language ModelingWorkshop at Johns Hopkins, which brought togetherresearchers with diverse backgrounds to tackle the problemof language modeling for conversational speech.
Some ofthe work we report on in this paper was begun at theworkshop.The annotation otation was based on the work of ElizabethShriberg (1994).
We used her work as a starting point forseveral reasons.
First, her notation was quitecomprehensive, covering all (and more) of the phenomenawe were interested in annotating.
Second, she had donemuch of her work on the Switchboard corpus, annotating40,000 words, so we knew that most, if not all, theidiosyncrasies of that corpus would be dealt with.
Third,others can more readily build on the extensive analysis inShriberg's thesis if the annotations are not gratuitouslydifferent.1.2 Goals of the workThe ultimate goal of this work is to improve theperformance ofa speech recognizer.
In the process we hope34to gain a better understanding of the structure ofconversational speech and how styles of speech differ.Annotating data is an enormously expensive task, so beforewe select what information to annotate, we need to thinkabout what use we can make of that information.
In thecase of dysfluency annotation, the sheer number ofdysfluencies in conversational speech, plus the fact thatdysfluencies do not occur at all in written speech and thuscannot be modeled using that source, are indicators that thisannotation can contribute to our language modeling work.Some other questions about the structure of conversationalspeech that can be addressed with annotated ata are asfollows:?
Are dysfluencies more likely to occur in some places inthe sentence than others??
Do dysfluencies carry some useful information??
What is the notion of a sentence or segment inconversational speech??
Is conversational structure different from formal writtenstructure and if it is, can we exploit that difference in ourlanguage models?The most innovative part of this paper is the use of theinformation structure of conversational utterances todevelop targeted language models for different parts of asentence.
According to many theories of discourse inlinguistics (e.g.
Clark and Haviland, 1977, or Haliday andHasan, 1976), sentences have an "information structure" inaddition to the syntactic structure that distinguishes betweengiven information, which is already shared by theconversational participants, either from shared knowledge orinformation contained earlier in the dialog, and newinformation that the speaker is conveying to the hearer.
Inthe unmarked case (e.g.
sentences not marked with emphaticstress or cleft sentences, which are rare in conversationalspeech), given information tends to occur in the beginningof a sentence where the topic is established, whereas newinformation tends to occur at the end, the comment on thetopic.
This work is currently in the data analysis phase,where we are developing heuristics to divide a sentence intoits given and new parts, which roughly correspond to theparts of a sentence before and after the verb.
While thissimple heuristic is far from the complex notion of givenand new in the linguistic literature, it has on mainadvantage: it can be computed automatically given sentenceboundaries, part of speech information, and dysfluencyinformation.
If the simple notion pans out and we can showa significant improvement in recognition performance, wemay want to move to a more complex notion and handannotate it in the corpus.
However, many ideas which areintuitively appealing are difficult to incorporate in alanguage model or do not appear to make a difference inoverall performance.In our work so far, our analysis of the vocabulary and itsdistribution in these two parts of the sentence shows asignificant difference.
We are currently building languagemodels from the two parts and determining ways ofintegrating them that takes advantage of this difference.
Thenext step will be modeling conversations and incorporatingthose models into a speech recognizer.We discuss the dysfluency annotation of Switchboard indetail in Section 2.
In Section 3, we describe work onlinguistic vs. acoustic segmentation and its effect on thelanguage model.
In Section 4, we describe the given/newdistinction as we have implemented it and our analysis ofthe Switchboard corpus.
Finally, in Section 5, we describea model of conversational speech that takes advantage of thegiven/new distinction and how it can be used in a speechrecognition system.2 Annotations of SwitchboardThere were three major kinds of annotations done as part ofthe dysfluency annotation of Switchboard: sentenceboundaries, restarts, and non-sentence elements.
Theassumption underlying the dysfluency annotation was thatwhen it was complete, sentences could be separated, restarts"folded", and non-sentential elements removed and the resultwould be a reasonably grammatical sentence (that is,grammatical for conversational speech, not necessarilycompliant with your third grade English teacher), thoughsome may not be "complete" in that they may be replies oracknowledgments and some may be interrupted by either thespeaker herself or the other conversant and never completed.As mentioned earlier, much of the choice of what toannotate and details of the notation are based on the work ofShriberg (1994).
The main difference is that our work is notas detailed as Shriberg's, since we were not planning as finegrained analysis, and it covered significantly more data(Shriberg annotated 40,000 words, whereas this effortannotated 1.4 million words).
In the next three sections, wedescribe these types of annotations and provide someexamples.2.1 SentencesIn written text, the definition of a sentence is clear andmarked in the text itself by capitalization and punctuation.For conversational speech, the most natural division wouldappear to be the turn, when one speaker stops speaking andanother starts.
However, when we look at the data, we see35that participants often interrupt and talk over one another,so even separating turns is not so simple.
Within a turn aparticipant may ramble on and on, making the utterance toolong for a speech recognizer to handle.In annotating Switchboard, we choose to divide turns into"sentences" consisting each of a single independent clause.When two independent clauses are connected by aconjunction, they are divided with the conjunctions markedas described in ?2.3.4.Sentence units are followed with "P' indicating a sentenceboundary, as shown in example 1.
A sentence is consideredto begin either at turn beginning, or after completion of apreceding sentence.
Any dysfluencies between the end of aprevious sentence and beginning of the current one isconsidered part of the current sentence.In Example 2, there are essentially two sentences.
The firstsentence is across a turn by speaker A, namely "we did getthe wedge cut out by building some kind of a cradle for it".The other sentence is by speaker B which is "A cradle forit".
"You know" at the end of a sentence (Example 3) isconsidered as a part of the current sentence, as described inmore detail in ?2.3.1.Ex 1: A: You interested in woodworking?
/Ex 2: A: we did get the wedge cut out by building some kindof--B: A cradle for it.
/A: -- a cradle for it.
/Ex 3: B: I painted, about eight different, colors, you know.
/the crayons that are sticking up, it will be theheadboard -- /Each sequence of words consisting of only continuers orassessments (expressions such as "uh-huh", "right", "yeah","oh really") is also coded as a sentence, as in Examples 4and 5.Ex 4: Yeah /Ex 5: Right / Right /2 .1 .2  Incomplete  sentencesSentences that do not end normally are treated as incompletesentences.
They are marked with "-P'.
In some cases thespeaker stops a sentence and starts over (in contrast withrestarts where just a few words are repeated, as described in?2.2).
In other cases, the other participant in theconversation interrupts the speaker and the speaker neverfinishes the sentence (in contrast with cases such asexample 2 above, where the first speaker finishes thesentence in the next turn after or during the interruption).Ex 1: B: what I've seen of this kind before is you have the, -/if you're looking at adding on you have, -/Ex 2: A: Perhaps things that we didn't hink of before andjust concentrated on the lawmaking or the resultsthat would be seen in public works or bills that arepassed or, et cetera like that -- -/Ex 3: B: -- it was very unfortunate thing that occurred there /it's, -/A: Where do you live?
/B: we live in Utah.
/2.2 RestartsRestarts are considered to have the following form inShfiberg's work and elsewhere.
The initial part is thereparandum (RM), which is the part that the speaker isgoing to repair.
The interruption point (IP) markes the endof the reparandum and it is followed by an optionalinterregnum (IM), which includes editing phases, such as afilled pause or editing terms.
Finally, the repair (RR) iswhat the speaker intends to replace the RM with.Show me flights from Boston on uh from Denver on MondayI .
.
.
.
.
.
.
.
RM- - -  I IM I  .
.
.
.
.
RR ....IPIn order to simplify the notation, the restart notation wedeveloped marks only the boundaries of the entire restart(RM to RR) with square brackets and the interruption pointwith a "+" .
Partial words are also not marked specially(though in the transcripts they end in a "-"); they appeardirectly to the left of the interruption point.
In contrast withShriberg's work, no internal structure of the restart isincluded (e.g.
which words are repeated, substituted ordeleted).Show me flights \[from Boston on + {F uh } from Denveron \] MondayA restart is "repaired" by deleting the material between theopen bracket and the interruption point (+).
(Note thatfillers such as "uh" in the above example are deleted as aseparate process in cleaning the text.
We discuss them in?2.3) Some examples of restarts and repairs are givenbelow.
Note in Example 1, it is not always clear how muchshould appear in the repair.
"In the book" could also havebeen included.
However, to try to reduce the variation inannotation, annotators were instructed to keep the repair asshort as possible.
In Example 2, it can be seen that a restarthas been marked across a turn, with the RM and IM in oneturn and the RR in the next turn.Ex 1: A: \[ it, + the instructions \] in the book I had said use acoping saw but there's no coping saw big enough \[to, + for \] a fourteen inch wide watermelon /Ex2: B: \ [ I t 's ,+uhA: -- pine?/B: It's, \] plywood face I guess.In the second restart in Example 3, it is not clear what isthe RR for the RM "and".
In cases where there does notappear to be a suitable replacement for the restart,annotators were instructed to place the "\]" as close to the IPas possible.
One rule of thumb that can be followed in caseof marking restarts without repairs is that they are always atthe beginning or in the middle of the sentence, the sentencecontinues after the restart and the restart usually comprisesone to three function words.Ex 3: B: \[ I got, + uh it got \] delayed for a little bit \[ and,+ \]because of work !2.2 .1  Complex  Restar tsMultiple restarts are handled as embedded and are repairedfrom left to right.
Some examples of complex restarts areshown below.
In Example 2, the left to right annotationhas not been strictly observed since "\[ Ber-, + Bermuda \]"appears as a restart with repair within the repair of anotherrestart.Ex 1:Ex 2:A: to keep an inmate in there \[\[ on a, + on a, \] + on a \]life sentence /B: Yeah, / \[\[ they're, + Um you know they're \] like Ber-,+A: Dress shorts.
/B: they're like black corduroy \[ Ber-, + Bermuda \]\]shorts.2.3 Non-Sentence ElementsNon-sentence lements are words or phrases which areinserted in an utterance, disrupting the flow of the sentence.They are simple units with no internal structure and nointerruption point.
There are five types of non-sentenceelements: filled pause {F }, editing term {E }, discoursemarker {P }, conjunction {C }, and aside {A }.36i2.3.1 Filled PauseFil led pauses have unrestricted istribution and no semanticcontent.
A few examples of fillers are "uh .
.
.
.
um", "huh".There can also be other fil led pauses which are rare such as"eh" and "oops".
"oh" can be treated as a filled pause if itappears along with other words for example "oh yeah", "ohreally", as in Example 1.
Otherwise, "oh" is treated as aregular word unit of language if it appears by itself as areply, as in Example 3.Ex 1: B:Ex 2: B:{F Oh }, yeah.
/ Uh-huh.
/Actually, \[ I, + {F uh, } I \] guess I am/\[laughter\].
{Fum }, it just seems kind of funny that this is a topicof discussion.
/ {F uh }, I do, {F uh }, some, {F uh },woodworking myself/\[noise\].
{F uh }, in fact, I'min the middle of a project fight now making a bedfor my son.
/Ex 3: B: Oh/2.3.2 Explicit Editing TermEditing terms are usually restricted to occur between therestart and the repair and have some semantic content (e.g.
"I mean .. .
.
sorry", "excuse me"), as shown in Example 1,through it is possible that editing terms occur outside theRR.Ex 1: A: {F Oh, } yeah, / {F uh, } the whole thing was smalland, \[you, + { E I mean, } you\] actually put it on\[laughter\], !2.3.3 Discourse MarkerDiscourse markers have a wider distribution than explicitediting phrases but are unlike filled pauses in that they arelexical items (e.g.
"well", "you know", "l ike").
"Youknow" is the most frequent discourse marker and is usedvery frequently by some speakers, as shown in Example 3.There are some other terms such as "so" and "actually"which can also serve as discourse markers, as in Example 2;however, "so" can also be a coordinating conjunction or asubordinating conjunction, as discussed in ?2.3.4.
InExample 4, it can be observed that the discourse marker iswithin the RR of a restart.Ex 1: B:Ex 2: A:Ex 3: B:{P Well }, we have a cat who's also about four yearsold.
/he comes back.
/ { P So } \[ he, + he's \] pretty goodabout taking to commands /Yeah, / with, {P you know, } me being at home andjust having the one income, {P you know, } youEx 4: B:don't have, this lot o f extra money \[ to, + to \] do alot of, {Pyou know, } extra things.
/\[ We take, + {P you know, } whenever we take \]them to Showbiz or - /they think it's wonderful justto go to McDonalds, /2.3.4 Coordinating ConjunctionCoordinating conjunctions occur at the inter-sentential leveland generally include "and", "but" and "because".
In somecases it is possible that two words together constitute aconjunction, for example "and then", as in example 2.
Mostof  the conjunctions that appear between two full clauses aremarked as coordinating conjunctions.
The rule of the thumbto be fol lowed is "split sentences whenever possible" exceptwhen the two sentences, if split, are grammatical lyincorrect (for example the second sentence in the spilt doesnot have a subject since it is in the earl ier sentence).Ex 1: A: Yeah, / {C and } we got him when he was abouteight weeks old !
{C and } he's pretty okay, /Ex 2: B: {C and then } I painted, {F uh }, about eightdifferent, {F uh }, colors, /Example 3 is of "so" as a coordinating conjunction.
Notethat in Example 4, the second "and" is NOT treated as acoordinating conjunction, as the two sentences it conjoins("I call him" and "he comes back") are both short and bothappear to be modif ied by the initial "if" clause.Ex3: B: {PWel l} ,{Fuh},we justmovedrecent ly\[laughter\] / {C so} now we're in the, {F uh }, Dallasarea /Ex 4: A: he's pretty good.
/ He stays out of the street / {Cand, } {Fuh }, i f l  catch him I call him and he comesback.
/ {P So } \[he, + he's \] pretty good abouttaking to commands /2.3.5 AsidesThis is a category for "asides" that interrupt he flow of thesentence.
Interjections are rare and are considered only whenthe corresponding sequence of words interrupt the fluentf low of the sentence AND the sentence later picks up fromwhere it left.
The examples below clearly i l lustrate this.Ex 1: B: I, {Fuh }, talked about how a lot of the problemsthey have to \[ come, + overcome \] \[ to, + {F uh, }{A it's a very complex, {F uh, } situation } to \] gointo space.
/Ex2: A: {P So } we built a cradle for it / {C and } \ [wegotth-, + {A once it was turned, } we got \] \[ one s-, + one \]cutout on the table saw, on the radial saw, /373 Linguistic SegmentationsAs explained in the previous section, one of the importantannotations of the Switchboard corpus involved the issue ofsentence boundaries, or segment boundaries.
Sentenceboundaries are easy to detect in the case of read speechwhere there is a distinctive pause at the end of the sentenceand the sentence is usually grammatically complete (thesecond also holds true in case of written speech, where inaddition a period marks the end of a sentence).
However,this is not so in the case of conversational speech as is clearfrom the examples above.
In conversational speech, it ispossible to have incomplete sentences, sentences acrossturns and complex sentences involving restarts and otherdysfluencies.Prior to having annotated ata, the segment boundaries forconversational text data were provided in the form ofacoustic segmentations.
These segmentations were based onpauses, silences, non-speech elements (e.g.
laughs andcoughs) and turn taking.
The differences between the twoforms of segmentations can be observed with the examplegiven below1:Acoustic segmentationsI'm not sure how many active volcanoes there are now andand what the amount of material that they do <s> uh <s> putinto the atmosphere <s> !
think probably the greatest causeis uh <s> vehicles <s> especially around cities <s>Linguistic segmentationsI'm not sure how many active volcanoes there are now andand what the amount of material that they do uh put into theatmosphere <s> I think probably the greatest cause is uhvehicles especially around cities <s>In the n-gram approach to statistical language modeling, thesegment boundary is treated as one of the symbols in thelexical dictionary and modeled similar to other words in thedata stream.
The segment boundaries provide an additionalsource of information to the language model and hence itappears intuitively correct o use linguistic segmentationsfor training language models.
The notion of segmentationis also an important issue if we use higher level languagemodels such as phrase structure models or sentence-levelmixture models (Iyer, et al 1994).
However, given only aspeech signal during recognition with no text cues availablefor segmentation, there will be an inherent mismatchbetween the linguistically segmented training data and theacoustically segmented test data.Thus the segmentation experiments tried to answer threeimportant issues:<s> represents the sentence/segment boundary.?
Does a mismatch in training/testing segmentation hurtlanguage model performance (perplexity and word errorrate)??
Is there any information in segment boundaries??
If no boundary information is available during testing,can we hypothesize this information using a languagemodel trained with segmented training data?3.1 Experimental SetupIn order to analyze the above issues, we first obtained ourbaseline training and testing data.
Since the linguisticsegmentations are available for only two thirds of theSwitchboard ata, we decided to use the corresponding twothirds of the acoustically segmented training data for ourcomparative experiments.
The test set was obtained fromthe Switchboard lattices which served as the baseline for the1995 Language Modeling Workshop at Johns Hopkins.
Thetest set was acoustically segmented.
A correspondinglinguistically segmented test set was also made available 2.3.1.1 Recognition ParadigmWe used the N-best rescoring formalism for recognitionexperiments with the test data (Ostendorf, et al 1991).
TheHTK toolkit was used to generate the top 2500 hypothesesfor each segment.
A weighted combination of scores fromdifferent knowledge sources (HTK acoustic model scores,number of words, different language model scores, etc.)
wasthen used to re-rank the hypotheses.
The top rankinghypothesis was then considered as the recognized output.3.2 Mismatch in Training and Test DataSegmentationsWe trained three trigram language models: two usingacoustic segmentations and linguistic segmentationsrespectively and a third model trained on data with nosegment boundaries.
The models used the Good-Turingback-off for smoothing unseen n-gram estimates (Katz1987).
These models were then used to compute perplexityon the different versions of the test data.
The trigramperplexity numbers are shown in Table 1.382 Unfortunately, the two test sets did not match completely interms of the number of words since the lattice test set had beenhand corrected after the initial transcription to account forsome transcription errors.
Hence, there is a difference of abouttwo hundred words between the acoustically and linguisticallysegmented test sets.Test Trainingacoustic-seg ling-seg no-segacoustic-seg 105 111ling-seg 89 78seg removed 163 174 130Table 1: Tr igram perplexity measurements  onLM95 SWBD dev.
test setAs indicated in Table 1, mismatch between training andtesting segmentation hurts perplexity.
The best perplexitynumbers are obtained under matched conditions.
Though theresults for the linguistically segmented test set (78) aresignificantly better than the corresponding matched case forthe acoustic segmentations (105), we cannot conclusivelystate that this is due to better segmentation since we havenot controlled for the length of the different segments.3.3 Hypothesizing Segment BoundariesA second perplexity experiment that we conducted tried totest whether we can hypothesize segmentations, given thatwe have no boundaries in the test set.
Our segment-hypothesizing algorithm 3 assumed that at any word, wehave two paths possible,?
A transition to the next word.?
A transition to the next word through a segmentboundary.The algorithm was approximate in that we did not keeptrack of all possible segmentations.
Instead at every point,we picked the most likely path as the history for the nextword.As in the first experiment, we trained two language modelson linguistic segmentations and acoustic segmentationsrespectively.
Henceforth, these models are referred to as thel i ng -seg  and acoust ie -seg  models.
Both models try tohypothesize the segment boundaries while computing thelikelihood of the no-segmentation test set.Test Trainingacoustic-seg ling-segno seg 163 127Table 2: Trigram perplexity on LM95 SWBDdev.
test set hypothesiz ing segment boundaries3 This is work done in collaboration with Roni Rosenfeld at the1995 Johns Hopkins Workshop on Language Modeling.39The perplexity results of Table 2 indicate that the ling-segmodel does better than the acoustic-seg model forhypothesizing segment boundaries.
Thus, we can gain asignificant amount of boundary information by this simplescheme of hypothesizing segmentations.3.4 Recognition ExperimentsThere were a couple of experimental constraints to analyzethe aforementioned issues in terms of recognition word errorrate.We were constrained to use the lattices that had beenprovided to the workshop.
Since these lattices were builton acoustic segments, the models had to deal withimplicit acoustic segment boundaries.
The context fromthe previous lattice was not provided for the currentlattice.We tried to alleviate this problem by trying to providethe context for the current lattice by selecting the mostlikely pair of words from the previous lattice using pairoccurrence frequency.
One problem with this approch isthat since the standard Switchboard WER is about 50%,about 73% of the time we were providing incorrectcontext using these lattices.We used our segment hypothesizing scheme for scoringan N-best list corresponding to these lattices (N=2500).While the initial context was provided for the N-bestlists, we had to throw away the final segment boundary.This led to a degradation i  performance.ModelacousticboundariesWER(%)hypothesizingboundariesacoustic seg 50.46 51.85ling seg 50.88 51.72Table 3: N-best rescoring performance (N=2500)measurements on LM95 SWBD dev.
test setAs shown in Table 3, the mismatch between the trainingand test segmentations degrades performance by half apoint, from 50.46% to 50.88%.
Throwing out the endsegment boundary from the N-best lists degradesperformance by slightly more than an absolute 1%.
Also,the ling-seg model does slightly better at hypothesizingsegment boundaries than the acoustic-seg model.3.5 ConclusionsOur experiments indicated the following:?
Mismatch in segmentation hurts language modelperformance, both in terms of perplexity as well as interms of recognition word error rate.?
There is information in the knowledge of segmentboundaries that should be incorporated in our languagemodels.?
If no segment boundaries are known during testing, it isbetter to hypothesize segment boundaries using a modeltrained on linguistic segments than one based on acousticsegments.The notion of linguistic segmentation is important inlanguage modeling because it provides information that isused in many higher order language models, for example the"given-new" model described in the next section, phrasestructure language models, or sentence-level mixturemodels.
However, this information cannot easily be derivedfrom the acoustic signal.
In this section, we have describeda simple technique of hypothesizing segmentations usingan n-gram language model trained on annotated ata.
Weplan to run some controlled perplexity and recognitionexperiments in the future to use this information in ourrecognition system.4.
Information Structure inConversational SpeechIt is well known in the linguistic literature that sentencesare not uniform from beginning to end in the kinds ofwords or structures used.
Sentences have a "given/new" or"topic/comment" structure which is especially pronouncedin conversational speech.
According to discourse theories inlinguistics, given information tends to occur in thebeginning of a sentence where the topic is established,whereas new information tends to occur at the end, thecomment on the topic.
4We are looking at ways of taking advantage of this structurein the language model.
The first stage of the work is todevise a method of dividing sentences into these two parts.Next, treating the before and after portions of the sentencesas separate corpora, we look at the distribution of thevocabulary and the distribution of other phenomena, such asrestarts.
We also build language models using these two4 This tendency is overridden in marked syntactic structures,such as cleft sentences ("It was Suzie who had the book last").These structures are relatively rare in conversational speech.corpora nd test perplexity both within and across corpora.The final step, which we are currently in the process of, isto find a way to integrate these models and use them withinthe speech recognition system to see if these more focusedmodels can actually improve recognition performance.4.1 Dividing the sentenceIn order to divide sentences into their given and newportions, we devised a simple heuristic which determines apivot point for each sentence.
The underlying assumption isthat the dividing line is the verb, or more particularly, thefirst verb that carries content (disregarding "weak" verbssuch as "is", "have", "seems").
The heuristic finds the firststrong verb and if there is none, then the last weak verb,and places a pivot point either before the strong verb orafter the weak one.
Sentences that have no pivot (i.e.
thathave no verb) are put into one of two classes, those that areconsidered complete (such as "Yeah" and "OK") and thosethat are incomplete, that is interrupted by either the speakeror the other conversant (i.e.
"Well, I, I, uh.").
The no pivotcomplete set is very similar the "Back Channel" modeldeveloped by Mark Liberman at the LM95 SummerLanguage Modeling Workshop (Jelinek 1995).
Libermanseparated back channel responses from information-bearingutterances and created a separate language model.
Initialexperiments shows no overall improvement in word errorrate, however, the model was able to identify bothpreviously identified and new backchannel utterances in thetest data.For the purposes of this paper, we will refer to the givenand new parts as before and after (meaning before and afterthe pivot), and "NPC" for no pivot complete and "NPI" forno pivot incomplete sentences.
The following shows anexample dialog and Table 4 shows the correspondingdivision into the four categories:A.I: Okay.
I think the first thing they said, I have written thisdown so it would, is it p-, do you think it's possible tohave honesty in government or an honest government?B.2: Okay.
You're asking what my opinion about,A.3: #Yeah.#B.4: #whether it's# possible \[laughter\] to have honesty ingovernment.
Well, I suspect hat it is possible.
Uh, Ithink it probably is more likely if you have a smallgovernment unit where everybody knows everybody.A.5: Right.
That's a good point.B.6: But, uh, other than that I think maybe it just depends onhow you define honesty.A.7: That's an int-, you know, that's interesting.40NPCOkayYeahOkay9 Right1011NPI BEFOREI i I think the first thing theyI haveso it would,is it p-, do you think it's possible to haveYou'reWell, IUh, I think it probably is more likely if youhave a small government unit where everybodyknowsThat'sBut, uh, other than that I think maybe it justThat's an int-, you know, that'sAFTERsaidwritten this downhonesty in government?asking what my opinion about :#whether it's# possible \[laughter\] tohave honesty in governmentsuspect hat it is possible.everybodya good point.depends on how you define honestyinterestingTable 4: Dialog divided into subpartsNote that this heuristic doesn't always make the correctdivision.
In sentence 8, "is" is the main verb, however, thealgorithm prefers to find a strong verb, so it keeps goinguntil it finds "know", which is actually part of a relativeclause.
A more complex algorithm that finds the main verbgroup and uses the last verb in the verb group rather thanthe last verb in the sentence would remedy this.
However,our goal here is to first determine whether in fact thisdivision is useful in the language model.
As long as errorssuch as this are in the minority, we can evaluate the methodand then go back and refine it if  it proves useful.In order to do the classification, we relied on three kinds ofannotations that were available for the switchboard corpus:sentence boundaries, part of speech, and dysfluencyannotation.
The dysfluency markings are needed since thepivot point is restricted from being inside of a restart.
Thefollowing shows the first two turns in the above discoursewith both of these annotationsS:SpeakerA1/SYM ./.Okay/UH ./.
E_S I/PRP think/VBP the/DTfirst/JJ thing/NN they/PRP said/VBD ,/, N S I/PRPhave/VBP written/VBN this/DT down/RP E_S {C so/RB }it/PRP would/MD ,/, N S \[ is/VBZ it/PRP p-/XX ,/, + do/VBPyou/PRP think/VB it/PRP 's/BES possible/JJ \] to/TOhave/VB honesty/NN in/IN government/NN or/CC an/DThonest/JJ government/NN .9/.
E_SSpeakerB2/SYM ./.
Okay/UH ./.
E_S You/PRP 're/VBPasking/VBG what/WP my/PRP$ opinion/NN about/IN ,/,whether/IN it/PRP 's/BES possible/JJ to/TO have/VBhonesty/NN in/IN government/NN ./.
E_STable 5 shows the breakdown of the data into the fourdivisions, before the pivot (before), after the pivot (after),complete sentences with no pivot (" NPC"), incompletesentences with no pivot (" NPI").Before After NPC NPI TotalTotal words 272,485 298,460 33,954 9364 614,263Number of segments 50,741 47,814 25,483 4112 80,336Avg.
segment length 5.37 6.24 1.33 2.772 7.65Table 5: Switchboard Corpus Divided by Pivot Point5 In this version of the annotation, complete sentences aremarked E_S and incomplete sentences are marked N_S, ratherthan / and -/as described in ?2.
This is to avoid confusion withthe / which delimits words and their part of speech.41Freq.
Rank Word BeforeitAfter23143NPC NPI117Total5481 i 57872 7876 \] 263 1608 676192 and 32482 14135 836 2382 498353 the 14851 27073 802 809 435354 that 22592 17261 301 465 406195 you 22774 15896 684 967 403216 14429 382377 to 13745 19389 102 52 332888 a 8354 24024 556 158 330929 uh 15480 11734 1666 1798 3067810 's 24845 4414 71 31 2936111 of 9244 15615 488 306 2565312 know 12387 9062 329 696 2247413 yeah 156 241 20813 10 2122014 14709 4613 48 376 19746 they3252 do 15 15284 18547Table 6: Totals for 15 Most Frequent wordsIt is interesting to note that the size of the before and aftercorpora are very similar.
Note that this is not necessarilybecause the algorithm is dividing the sentences into twoequal portions, as we can see in the example above.
Somesentences have a rather long introduction with restarts, as insentence 4 and 11, whereas others have just a single wordand a long after portion, as in sentence 6.since pronouns are used generally to refer to participants inthe conversation or things already mentioned, whereasarticles uch as "the" and "a" (rows 3 and 8) are much morefrequent in the after part of the sentence, since they are morefrequently used in full noun phrases describing new entities.
"Yeah" (row 13) occurs almost exclusively in the NPC set,which is comprised mainly of replies.4.2 Vocabulary distributionsIt is clear from the definitions of the given vs. new parts ofthe sentence, that the vocabularies in the corpora resultingfrom the division will have different distributions, giveninformation will be expressed with a larger number ofpronouns whereas the new portion will have more complexdescriptive noun phrases, and thus a wider rangingvocabulary.
Within the verb group, weak (and morecommon) verbs will appear in the given portion, whereasstrong verbs that carry content will appear in the newportion.
But rather than relying on these intuitions, weapply a more careful analysis of the data to determine moreclosely what the differences are.Table 7 plots the 50 most frequent words in the corpus,showing their before and after raw totals.
Note that whilethe values cross, they are rarely the same for the same word.This reinforces our intuition that the use of function words(typically the most common words) in the two parts of thesentences are quite different 6.4.2.1 Comparing most frequent wordsThe most frequent words in the corpus divide rather sharplyacross the data sets.
For example, in Table 6, which showsthe counts for the top 15 words, pronouns such as 'T '  and"it" (rows 1 and 6) are much more frequent in the before set,426 Note that only before and after sets are plotted, so series 2and 3, the lower two lines, do not necessarily sum to series 1,the upper line representing totals in the corpus as a whole.70000600005000040000300002000010000Words-'=Q.~ Series1--II--- Series2\]Ser es3 iTable 7: Before and After Totals for 50 Most Frequent wordsSeriesl: Totals for entire corpusSeries2: Counts for before pivot sentence partsSeries3: Counts for after pivot sentence parts4.2 .2  Differences in vocabularyWe also looked at the differences in the vocabularies of thetwo parts.
Table 8 shows the frequencies of words thatappear in one part of the sentence but do not appear in theother in this corpus.
The raw totals are quite different, with7292 words appearing in the after portion and not appearingin the before portion, while only 1028 appear in after andnot in before.
Also, note that less than half on one percentof the words in the before are uniquely in that part, where as7% of the total words in the after part never occurred inbefore.Row 1 shows the words that only occurred once in thecorpus (which had to occur in just one side or the other).We see that the tail of new words is much longer in theafter set than the before.Frequencies In Before, In After, notnot in After in Before1 877 53282 114 19383-5 40 12026-10 1 38111-50 0 21751-100 0 24100+ (max 214) 0 6Total (diffe~nt words) 1028 9944Total (instances) 1254 30131.046% 7% %ofcorpusTable 8: Vocabulary Differences in Before Pivotand After Pivot Sentence Parts43taken 222 forget 93 notice 75 sold 62teach 185 expect 90 bother 74 continue 60suppose 133 quit 89 moving 74 became 59played 131 follow 85 died 73 mentioned 55caught 129 wondering 80 selling 70 prefer 54rid 112 helping 79 choose 68 drove 52telling 105 born 76 considered 67 depending 52write 97 stopped 75 covered 62 trust 51happening 94 notice 75 staying 62 pickin?
51Table 9: Words occurring more than 50 times in the After Pivot sentence parts andnot at all in the Before Pivot partsTable 9 shows the actual words and counts for those wordsoccurring over fifty times in the after corpus and not at allin the before part.
Note that they are all forms of verbs.as "um" and "uh".
A similar result was reported by Shriberg(1994) who showed that the rate of dysfluencies was muchhigher in sentence initial vs. sentence medial position.4.3 Distr ibution of dysf luenc iesAnother major difference between the two parts is the kindsof dysfluencies that occur, as shown in Table 10.
Thebefore part has significantly more non-sentence elements (asdescribed in ?2.3)-three times as many.
Most of these areconjunctions, which tend to start sentences.
There are twiceas many discourse dysfluencies, but if you look at just theuse of "you know" the totals are nearly the same.
There arealso approximately the same number of filler words, suchTotal Non-SentenceElementsBefore89470After263524.3 Perp lex i tySince the ultimate goal of the work here is to build alanguage model, one significant indicator of the uniformityof a corpus is to run perplexity experiments on the data.Perplexity gives a rough indication of the average numberof alternatives in the grammar based on the computation ofthe entropy.
This is assuming that the training and the testNPC9464NPI9775Total57270Discourse  17796 99445 1110 1646 11940you know 6433 6538 215 602well/uh 7456 646 688 597like 1096 1272 53 68Conj  unct ions  46280 380 896 4771 18277and 25518 209 355 2032but 10493 43 312 1300so/rb 5485 22 148 798iF i l lers  21693 14035 5787 2163 16548Editing Terms 2806 !
534 I 97 206 1392Asides 79 !
121 ~ 0 0 69RestartsBegins 32335 13771 655 952 21614Pivots 32102 13989 660 943 2160825884 20160 653 942 21598 EndsTotal WordsTable 10: Distr ibution of Dysf luencies and Restarts44TrainingNo.
words intrainingFull sentenceTest SetBefore AfterFull Sentences 1.4M 70 48 226Before 631K - 3 4 702108 125 After 693K -Table 11: Perplexity experiments on full sentences, before and after subpartsare well matched.
Perplexity can be used to tell how similaror different wo corpora are by training on one and testingon the other.
It is actually this way of using perplexity thatprovides the information in this case, since we have not yetdeveloped a complete model of the full sentence that takesinto account he given and new portions.We trained three bigram language models, one of fullsentences, one on just the before parts and one on just theafter parts.
The goal here is to find whether this divisionyields more precise models that are a better fit to the data.Of course, because the data is split into before and afterparts, those two models are less well trained than the fullsentences.
Nonetheless, the results are interesting andpromising, as shown in Table 11.5.
Language Modeling forRecognitionThe major challenge of this work is to take what we havelearned from data analysis and perplexity experiments andmake use of them in a language model for speechrecognition.
In Section 5.1 we describe a general approachwhich develops a conversational model of the different partsof a turn and how they interact.
In Section 5.2, we describesome of the other issues and approaches in using this workto improve recognition performance.5.1 Conversational modelsNote that the lowest perplexity is on the match of thebefore pivot training and test and the highest perplexity isthe before model tested on the after model.
The after modelitself is much more robust, with roughly equal perplexitieswhen tested with before and after.
The model trained on thefull sentences has a much lower perplexity on the beforeparts and a much higher perplexity on the after parts.
Thebest combined numbers comes from a match of before onbefore and after on after (in bold in Table 11).
Thisreinforces our intuitions that these subparts of the corpusare quite distinct.
Our next challenge is how to combinethese models in a way that maximizes the separate modelswithout being penalized by having to choose the pivotpoint.Tum Ibeginning I IA more general approach is to develop a higher level modelof a conversations.
We could model a turn with a finitestate machine such as that shown in Figure 1.
Each state isa model trained on the subpart of the corpus.Using the same algorithm that divided the corpus for theanalysis described above, we created a corpus of segmentsequences as shown below.
We first show the dialog andthen the segment sequences, each segment corresponding toa box in the above FSA.A1 Okay.
E_SI think the first thing they said, N_SI have written this down E S{C so } it would, N_S\[ is it p-, + do you think it's possible\] to have honesty ingovernment or an honest government?
E_ScompleteNo pivot ,j~"~"1 incompleteBeforepivot ~ Iv t~ / "1 Turn end \[Figure 1: Finite state model of Conversations45TurnbeginningAcknowledge Lment I ~incomplete I~ , .sentence I ~ ' ~Turn i ,,al kbef?repiv?tl'~ After 1//,''fBefore l,""''7 piv?t Ipivot |Sentence t connector'-~ Turn end \[Figure 2: More complexB2 Okay.
E_SYou're asking what my opinion about, whether it'spossible to have honesty in government.
E_SA3 Yeah.
E_SAI (TB NPC BEF AIFT BEF AFT NPI BEF AFT TE)B2 (TB NPC BEF AFT TE)A3 (TB NPC TE)From this corpus, we can create a bigram language modelof the transitions.
Table 12 shows the transitionprobabilities between the states.
It was trained on about aquarter of the overall data, containing about 133,000 tokensand a vocabulary size of 6 (TB, BEF, AFT, NPC, NPI,TE).
As shown in the table below, turns tend to begin (TB)with either a before pivot segment (BEF) or an NPC(generally an acknowledgment or reply).
Before pivots arevirtually always followed by after segments (AFT).
NPI(incomplete segments that were interrupted before the verb)are most likely to be followed by the end of a turn (TE).BEF AFT NPC NPITB .41 .001 .44 .04BEF 0 .999 0 0AFT .44 0 .01 .03NPC .26 .02 .09 .02NPI .33 .004 .03 .03'IE.1170.51.62.60Table 12: Bigram Transition Probabilities forConversational Model7 A transition from TB (turn beginning) to TE (turn end) occurswhen there was some non-speech, such as a laugh or cough, butno words in the turn.46conversational modelThe divisions used in the above model are fairly gross.
Wecould develop a more precise model taking into accountother differences in the turn.
For example in the networkshown in Figure 2, we distinguish between turn initialbeginnings of sentences and others.
We also separate outsentence connectors and optionally include them betweensentences.5.2 Future WorkThis work can be extended to improve language modelingand thus recognition performance on the Switchboardcorpus.
However, to translate our work on the given/newinformation structure of conversational segments tolanguage modeling, there are some important details.
Someof these issues and possible first-cut approaches are asoutlined below.?
Representation of the split: The split between the givenand new parts of a segment can be represented as a lexicalentity and treated as part of the data stream as givenbelow.
This is similar to the segment boundaryrepresentation in n-gram language models.<s>Uh we were <end-given> <begin-new> thinkingmini van for a while c/s>Another approach is to develop a smooth transition froma given model to a new model while computing thelikelihood of a segment, as described above.
Theadvantage of the second approach is that there will bemore detailed context provided for the beginning of thenew part of the segment and hence the n-gram estimateswill be sharper.!?
Smoothing/Robust parameter estimation: The given andnew models will be trained on subsets of the trainingdata.
This fragmentation f the training data can lead tosparse data problems while estimating the languagemodel parameters.
We will need to explore robustparameter estimation techniques to smooth our modelestimates.
One approach would be to smooth the givenand new models with a general "full sentence" model,using n-gram mixtures,P(w, w,_,)= z,,.
PM.
(w, (wi Jwi_,)where the subscripts M~represents given or new modeland gen represents the general model trained on fullsentences..~,'s represent the interpolation weight and canbe estimated on held-out data.?
Hypothesizing the given~new split: Currently, segmentsare split based on heuristic rules.
We can foresee buildingupon this scheme during training by developing aniterative pivot-hypothesizing algorithm.
In the firstiteration, the training sentences can be split using theheuristic rules described earlier.
Given and new languagen-gram models are estimated using this data.
In thesubsequent iterations, the pivot is selected to maximizethe likelihood of the sentences as estimated by the givenand new language models.
A similar scheme can be usedfor hypothesizing the pivot on the test data.6.
ConclusionsWe have shown a path from annota.tion, through dataanalysis, to an implemented language model for speechrecognition.
Each part of this path is very important.
Theannotation provides a springboard for a wide range ofdifferent research efforts, essentially enabling the work thatwould be impossible without hat effort.
We used a simpleautomatic algorithm for dividing sentences, but based thosedivisions on both the text and the manual annotations.
Ifwe find this approach is a high payoff, then we may wantto experiment with hand correcting some of the divisions tosee if greater accuracy improves our results.
However, it isimportant to show clear progress before investing the time,since manual annotation is very expensive.
The linguisticanalysis of the results can provide indications of theeffectiveness of our algorithm and point to how best to usethe results.
In our work on creating new language modelsbased on our analysis, we have only scratched the surface inways of combining subcorpora nd building an overallconversational model.
We hope to complete recognitionexperiments in the next couple months that show thecontribution of our generalizations over the text on worderror ate.ReferencesClark & Haviland (1977) Comprehension and the Given-new Contract" in Freedle (ed.)
Discourse Production andComprehension, Ablex Publishing Corporation, NewJersey, 1977.Group on "Phrase-structure in Language Modeling",Language Modeling Workshop, Johns Hopkins, 1995.M.A.K.
Haliday and R. Hasan, Cohesion in English,Longman, London, 1976.J.J.
Godfrey, E.C.
Holliman, and J.
McDaniel.
"Switchboard: Telephone speech corpus for research anddevelopment".
In Proceedings of IEEE Conference onAcoustics, Speech, and Signal Processing.
vol.
1 pp.
517-520.
San Francisco, March 1992.R.
Iyer, M. Ostendorf, R. Rohlicek, "An ImprovedLanguage Model Using a Mixture of MarkovComponents", Proceedings of the ARPA Workshop onHuman Language Technology, pp.82-87, March 1994.F.
Jelinek, et.
al 1995.
Report on the LM95 SummerLanguage Modeling Workshop, to appear.
See alsohttp://cspjhu.ece.j hu.edu/lm95 workshop.htmlS.
M. Katz, "Estimation of Probabilities from Sparse Datafor the LM Component of a Speech Recognizer", IEEETransactions on Acoust., Speech, and Signal Proc., Vol.ASSP-35, Number 3, pp.
400-401, March 1987.M.
Ostendorf, A. Kannan, S. Austin, O. Kimball, R.Schwartz and J. R. Rohlicek, "Integration of DiverseRecognition Methodologies Through Reevaluation of N-Best Sentence Hypotheses", Proc.
ARPA Workshop onSpeech and Natural Language, pp.
83-87, February 1991.E.
Shriberg, Preliminaries to a Theory of SpeechDysfluencies, Ph.D. thesis, Department of Psychology,University of California, Berkeley, CA, 1994.A.
Stolcke and E. Shriberg, "Statistical Language Modelingfor Speech Dysfluencies" Proc.
ICASSP-96, May 7-10,Atlanta, GA. 199647
