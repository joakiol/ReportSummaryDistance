Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 109?117,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsMulti-Prototype Vector-Space Models of Word MeaningJoseph ReisingerDepartment of Computer ScienceThe University of Texas at Austin1 University Station C0500Austin, TX 78712-0233joeraii@cs.utexas.eduRaymond J. MooneyDepartment of Computer ScienceThe University of Texas at Austin1 University Station C0500Austin, TX 78712-0233mooney@cs.utexas.eduAbstractCurrent vector-space models of lexical seman-tics create a single ?prototype?
vector to rep-resent the meaning of a word.
However, dueto lexical ambiguity, encoding word mean-ing with a single vector is problematic.
Thispaper presents a method that uses cluster-ing to produce multiple ?sense-specific?
vec-tors for each word.
This approach providesa context-dependent vector representation ofword meaning that naturally accommodateshomonymy and polysemy.
Experimental com-parisons to human judgements of semanticsimilarity for both isolated words as well aswords in sentential contexts demonstrate thesuperiority of this approach over both proto-type and exemplar based vector-space models.1 IntroductionAutomatically judging the degree of semantic sim-ilarity between words is an important task usefulin text classification (Baker and McCallum, 1998),information retrieval (Sanderson, 1994), textual en-tailment, and other language processing tasks.
Thestandard empirical approach to this task exploits thedistributional hypothesis, i.e.
that similar words ap-pear in similar contexts (Curran and Moens, 2002;Lin and Pantel, 2002; Pereira et al, 1993).
Tra-ditionally, word types are represented by a sin-gle vector of contextual features derived from co-occurrence information, and semantic similarity iscomputed using some measure of vector distance(Lee, 1999; Lowe, 2001).However, due to homonymy and polysemy, cap-turing the semantics of a word with a single vector isproblematic.
For example, the word club is similarto both bat and association, which are not at all simi-lar to each other.
Word meaning violates the triangleinequality when viewed at the level of word types,posing a problem for vector-space models (Tver-sky and Gati, 1982).
A single ?prototype?
vectoris simply incapable of capturing phenomena such ashomonymy and polysemy.
Also, most vector-spacemodels are context independent, while the meaningof a word clearly depends on context.
The word clubin ?The caveman picked up the club?
is similar to batin ?John hit the robber with a bat,?
but not in ?Thebat flew out of the cave.
?We present a new resource-lean vector-spacemodel that represents a word?s meaning by a set ofdistinct ?sense specific?
vectors.
The similarity oftwo isolated words A and B is defined as the mini-mum distance between one of A?s vectors and one ofB?s vectors.
In addition, a context-dependent mean-ing for a word is determined by choosing one of thevectors in its set based on minimizing the distanceto the vector representing the current context.
Con-sequently, the model supports judging the similarityof both words in isolation and words in context.The set of vectors for a word is determined by un-supervised word sense discovery (WSD) (Schu?tze,1998), which clusters the contexts in which a wordappears.
In previous work, vector-space lexical sim-ilarity and word sense discovery have been treatedas two separate tasks.
This paper shows how theycan be combined to create an improved vector-spacemodel of lexical semantics.
First, a word?s contextsare clustered to produce groups of similar contextvectors.
An average ?prototype?
vector is then com-puted separately for each cluster, producing a set ofvectors for each word.
Finally, as described above,these cluster vectors can be used to determine the se-109mantic similarity of both isolated words and wordsin context.
The approach is completely modular, andcan integrate any clustering method with any tradi-tional vector-space model.We present experimental comparisons to humanjudgements of semantic similarity for both isolatedwords and words in sentential context.
The resultsdemonstrate the superiority of a clustered approachover both traditional prototype and exemplar-basedvector-space models.
For example, given the iso-lated target word singer our method produces themost similar word vocalist, while using a single pro-totype gives musician.
Given the word cell in thecontext: ?The book was published while Piaseckiwas still in prison, and a copy was delivered to hiscell.?
the standard approach produces protein whileour method yields incarcerated.The remainder of the paper is organized as fol-lows: Section 2 gives relevant background on pro-totype and exemplar methods for lexical semantics,Section 3 presents our multi-prototype method, Sec-tion 4 presents our experimental evaluations, Section5 discusses future work, and Section 6 concludes.2 BackgroundPsychological concept models can be roughly di-vided into two classes:1.
Prototype models represented concepts by anabstract prototypical instance, similar to a clus-ter centroid in parametric density estimation.2.
Exemplar models represent concepts by a con-crete set of observed instances, similar to non-parametric approaches to density estimation instatistics (Ashby and Alfonso-Reese, 1995).Tversky and Gati (1982) famously showed that con-ceptual similarity violates the triangle inequality,lending evidence for exemplar-based models in psy-chology.
Exemplar models have been previouslyused for lexical semantics problems such as selec-tional preference (Erk, 2007) and thematic fit (Van-dekerckhove et al, 2009).
Individual exemplars canbe quite noisy and the model can incur high com-putational overhead at prediction time since naivelycomputing the similarity between two words usingeach occurrence in a textual corpus as an exemplarrequires O(n2) comparisons.
Instead, the standard... chose Zbigniew Brzezinskifor the position of ...... thus the symbol s positionon his clothing was ...... writes call options againstthe stock position ...... offered a position with ...... a position he would holduntil his retirement in ...... endanger their position asa cultural group...... on the chart of the vessel scurrent position ...... not in a position to help...(cluster#2)postappointment, role, job(cluster#4)lineman,tackle, role,scorer(cluster#1)locationimportancebombing(collect contexts) (cluster)(cluster#3)intensity,winds,hour, gust(similarity)singleprototypeFigure 1: Overview of the multi-prototype approachto near-synonym discovery for a single target wordindependent of context.
Occurrences are clusteredand cluster centroids are used as prototype vectors.Note the ?hurricane?
sense of position (cluster 3) isnot typically considered appropriate in WSD.approach is to compute a single prototype vector foreach word from its occurrences.This paper presents a multi-prototype vector spacemodel for lexical semantics with a single parame-ter K (the number of clusters) that generalizes bothprototype (K = 1) and exemplar (K = N , the totalnumber of instances) methods.
Such models havebeen widely studied in the Psychology literature(Griffiths et al, 2007; Love et al, 2004; Rosseel,2002).
By employing multiple prototypes per word,vector space models can account for homonymy,polysemy and thematic variation in word usage.Furthermore, such approaches require only O(K2)comparisons for computing similarity, yielding po-tential computational savings over the exemplar ap-proach when K  N , while reaping many of thesame benefits.Previous work on lexical semantic relatedness hasfocused on two approaches: (1) mining monolin-gual or bilingual dictionaries or other pre-existingresources to construct networks of related words(Agirre and Edmond, 2006; Ramage et al, 2009),and (2) using the distributional hypothesis to au-tomatically infer a vector-space prototype of wordmeaning from large corpora (Agirre et al, 2009;Curran, 2004; Harris, 1954).
The former approachtends to have greater precision, but depends on hand-110crafted dictionaries and cannot, in general, modelsense frequency (Budanitsky and Hirst, 2006).
Thelatter approach is fundamentally more scalable as itdoes not rely on specific resources and can modelcorpus-specific sense distributions.
However, thedistributional approach can suffer from poor preci-sion, as thematically similar words (e.g., singer andactor) and antonyms often occur in similar contexts(Lin et al, 2003).Unsupervised word-sense discovery has beenstudied by number of researchers (Agirre and Ed-mond, 2006; Schu?tze, 1998).
Most work has alsofocused on corpus-based distributional approaches,varying the vector-space representation, e.g.
by in-corporating syntactic and co-occurrence informationfrom the words surrounding the target term (Pereiraet al, 1993; Pantel and Lin, 2002).3 Multi-Prototype Vector-Space ModelsOur approach is similar to standard vector-spacemodels of word meaning, with the addition of a per-word-type clustering step: Occurrences for a spe-cific word type are collected from the corpus andclustered using any appropriate method (?3.1).
Sim-ilarity between two word types is then computed asa function of their cluster centroids (?3.2), instead ofthe centroid of all the word?s occurrences.
Figure 1gives an overview of this process.3.1 Clustering OccurrencesMultiple prototypes for each word w are generatedby clustering feature vectors v(c) derived from eachoccurrence c ?
C(w) in a large textual corpus andcollecting the resulting cluster centroids pik(w), k ?[1,K].
This approach is commonly employed in un-supervised word sense discovery; however, we donot assume that clusters correspond to traditionalword senses.
Rather, we only rely on clusters to cap-ture meaningful variation in word usage.Our experiments employ a mixture of von Mises-Fisher distributions (movMF) clustering methodwith first-order unigram contexts (Banerjee et al,2005).
Feature vectors v(c) are composed of indi-vidual features I(c, f), taken as all unigrams occur-ring f ?
F in a 10-word window around w.Like spherical k-means (Dhillon and Modha,2001), movMF models semantic relatedness usingcosine similarity, a standard measure of textual sim-ilarity.
However, movMF introduces an additionalper-cluster concentration parameter controlling itssemantic breadth, allowing it to more accuratelymodel non-uniformities in the distribution of clustersizes.
Based on preliminary experiments comparingvarious clustering methods, we found movMF gavethe best results.3.2 Measuring Semantic SimilarityThe similarity between two words in a multi-prototype model can be computed straightforwardly,requiring only simple modifications to standard dis-tributional similarity methods such as those pre-sented by Curran (2004).
Given words w and w?, wedefine two noncontextual clustered similarity met-rics to measure similarity of isolated words:AvgSim(w,w?
)def=1K2K?j=1K?k=1d(pik(w), pij(w?))MaxSim(w,w?
)def= max1?j?K,1?k?Kd(pik(w), pij(w?
))where d(?, ?)
is a standard distributional similaritymeasure.
In AvgSim, word similarity is computedas the average similarity of all pairs of prototypevectors; In MaxSim the similarity is the maximumover all pairwise prototype similarities.
All resultsreported in this paper use cosine similarity, 1Cos(w,w?)
=?f?F I(w, f) ?
I(w?, f)?
?f?F I(w, f)2?
?f?F I(w?, f)2We compare across two different feature functionstf-idf weighting and ?2 weighting, chosen due totheir ubiquity in the literature (Agirre et al, 2009;Curran, 2004).In AvgSim, all prototype pairs contribute equallyto the similarity computation, thus two words arejudged as similar if many of their senses are simi-lar.
MaxSim, on the other hand, only requires a sin-gle pair of prototypes to be close for the words to bejudged similar.
Thus, MaxSim models the similarityof words that share only a single sense (e.g.
bat andclub) at the cost of lower robustness to noisy clustersthat might be introduced when K is large.When contextual information is available,AvgSim and MaxSim can be modified to produce1The main results also hold for weighted Jaccard similarity.111more precise similarity computations:AvgSimC(w,w?
)def=1K2K?j=1K?k=1dc,w,kdc?,w?,jd(pik(w), pij(w?))MaxSimC(w,w?
)def= d(p?i(w), p?i(w?
))where dc,w,kdef= d(v(c), pik(w)) is the likelihood ofcontext c belonging to cluster pik(w), and p?i(w)def=piargmax1?k?K dc,w,k(w), the maximum likelihoodcluster for w in context c. Thus, AvgSimC corre-sponds to soft cluster assignment, weighting eachsimilarity term in AvgSim by the likelihood of theword contexts appearing in their respective clus-ters.
MaxSimC corresponds to hard assignment,using only the most probable cluster assignment.Note that AvgSim and MaxSim can be thought of asspecial cases of AvgSimC and MaxSimC with uni-form weight to each cluster; hence AvgSimC andMaxSimC can be used to compare words in contextto isolated words as well.4 Experimental Evaluation4.1 CorporaWe employed two corpora to train our models:1.
A snapshot of English Wikipedia taken on Sept.29th, 2009.
Wikitext markup is removed, asare articles with fewer than 100 words, leaving2.8M articles with a total of 2.05B words.2.
The third edition English Gigaword corpus,with articles containing fewer than 100 wordsremoved, leaving 6.6M articles and 3.9B words(Graff, 2003).Wikipedia covers a wider range of sense distribu-tions, whereas Gigaword contains only newswiretext and tends to employ fewer senses of most am-biguous words.
Our method outperforms baselinemethods even on Gigaword, indicating its advan-tages even when the corpus covers few senses.4.2 Judging Semantic SimilarityTo evaluate the quality of various models, we firstcompared their lexical similarity measurements tohuman similarity judgements from the WordSim-353 data set (Finkelstein et al, 2001).
This testcorpus contains multiple human judgements on 353word pairs, covering both monosemous and poly-semous words, each rated on a 1?10 integer scale.Spearman?s rank correlation (?)
with average humanjudgements (Agirre et al, 2009) was used to mea-sure the quality of various models.Figure 2 plots Spearman?s ?
on WordSim-353against the number of clusters (K) for Wikipediaand Gigaword corpora, using pruned tf-idf and ?2features.2 In general pruned tf-idf features yieldhigher correlation than ?2 features.
Using AvgSim,the multi-prototype approach (K > 1) yields highercorrelation than the single-prototype approach (K =1) across all corpora and feature types, achievingstate-of-the-art results with pruned tf-idf features.This result is statistically significant in all cases fortf-idf and for K ?
[2, 10] on Wikipedia and K > 4on Gigaword for ?2 features.3 MaxSim yields simi-lar performance when K < 10 but performance de-grades as K increases.It is possible to circumvent the model-selectionproblem (choosing the best value of K) by simplycombining the prototypes from clusterings of dif-ferent sizes.
This approach represents words usingboth semantically broad and semantically tight pro-totypes, similar to hierarchical clustering.
Table 1and Figure 2 (squares) show the result of such a com-bined approach, where the prototypes for clusteringsof size 2-5, 10, 20, 50, and 100 are unioned to form asingle large prototype set.
In general, this approachworks about as well as picking the optimal value ofK, even outperforming the single best cluster sizefor Wikipedia.Finally, we also compared our method to a pureexemplar approach, averaging similarity across alloccurrence pairs.4 Table 1 summarizes the results.The exemplar approach yields significantly highercorrelation than the single prototype approach in allcases except Gigaword with tf-idf features (p <0.05).
Furthermore, it performs significantly worse2(Feature pruning) We find that results using tf-idf featuresare extremely sensitive to feature pruning while ?2 features aremore robust.
In all experiments we prune tf-idf features by theiroverall weight, taking the top 5000.
This setting was found tooptimize the performance of the single-prototype approach.3Significance is calculated using the large-sample approxi-mation of the Spearman rank test; (p < 0.05).4Averaging across all pairs was found to yield higher corre-lation than averaging over the most similar pairs.112Spearman?s ?
prototype exemplar multi-prototype (AvgSim) combinedK = 5 K = 20 K = 50Wikipedia tf-idf 0.53?0.02 0.60?0.06 0.69?0.02 0.76?0.01 0.76?0.01 0.77?0.01Wikipedia ?2 0.54?0.03 0.65?0.07 0.58?0.02 0.56?0.02 0.52?0.03 0.59?0.04Gigaword tf-idf 0.49?0.02 0.48?0.10 0.64?0.02 0.61?0.02 0.61?0.02 0.62?0.02Gigaword ?2 0.25?0.03 0.41?0.14 0.32?0.03 0.35?0.03 0.33?0.03 0.34?0.03Table 1: Spearman correlation on the WordSim-353 dataset broken down by corpus and feature type.Figure 2: WordSim-353 rank correlation vs. num-ber of clusters (log scale) for both the Wikipedia(left) and Gigaword (right) corpora.
Horizontal barsshow the performance of single-prototype.
Squaresindicate performance when combining across clus-terings.
Error bars depict 95% confidence intervalsusing the Spearman test.
Squares indicate perfor-mance when combining across clusterings.than combined multi-prototype for tf-idf features,and does not differ significantly for ?2 features.Overall this result indicates that multi-prototype per-forms at least as well as exemplar in the worst case,and significantly outperforms when using the bestfeature representation / corpus pair.4.3 Predicting Near-SynonymsWe next evaluated the multi-prototype approach onits ability to determine the most closely relatedwords for a given target word (using the Wikipediacorpus with tf-idf features).
The top k most simi-lar words were computed for each prototype of eachtarget word.
Using a forced-choice setup, humansubjects were asked to evaluate the quality of thesenear synonyms relative to those produced by a sin-homonymouscarrier, crane, cell, company, issue, interest, match,media, nature, party, practice, plant, racket, recess,reservation, rock, space, valuepolysemouscause, chance, journal, market, network, policy,power, production, series, trading, trainTable 2: Words used in predicting near synonyms.gle prototype.
Participants on Amazon?s Mechani-cal Turk5 (Snow et al, 2008) were asked to choosebetween two possible alternatives (one from a proto-type model and one from a multi-prototype model)as being most similar to a given target word.
Thetarget words were presented either in isolation or ina sentential context randomly selected from the cor-pus.
Table 2 lists the ambiguous words used for thistask.
They are grouped into homonyms (words withvery distinct senses) and polysemes (words with re-lated senses).
All words were chosen such that theirusages occur within the same part of speech.In the non-contextual task, 79 unique raters com-pleted 7,620 comparisons of which 72 were dis-carded due to poor performance on a known test set.6In the contextual task, 127 raters completed 9,930comparisons of which 87 were discarded.For the non-contextual case, Figure 3 left plotsthe fraction of raters preferring the multi-prototypeprediction (using AvgSim) over that of a single pro-totype as the number of clusters is varied.
Whenasked to choose between the single best word for5http://mturk.com6(Rater reliability) The reliability of Mechanical Turkraters is quite variable, so we computed an accuracy score foreach rater by including a control question with a known cor-rect answer in each HIT.
Control questions were generated byselecting a random word from WordNet 3.0 and including aspossible choices a word in the same synset (correct answer) anda word in a synset with a high path distance (incorrect answer).Raters who got less than 50% of these control questions correct,or spent too little time on the HIT were discarded.113Non-contextual Near-Synonym Prediction Contextual Near-Synonym PredictionFigure 3: (left) Near-synonym evaluation for isolated words showing fraction of raters preferring multi-prototype results vs. number of clusters.
Colored squares indicate performance when combining acrossclusterings.
95% confidence intervals computed using the Wald test.
(right) Near-synonym evaluation forwords in a sentential context chosen either from the minority sense or the majority sense.each method (top word), the multi-prototype pre-diction is chosen significantly more frequently (i.e.the result is above 0.5) when the number of clus-ters is small, but the two methods perform sim-ilarly for larger numbers of clusters (Wald test,?
= 0.05.)
Clustering more accurately identi-fies homonyms?
clearly distinct senses and producesprototypes that better capture the different uses ofthese words.
As a result, compared to using a sin-gle prototype, our approach produces better near-synonyms for homonyms compared to polysemes.However, given the right number of clusters, it alsoproduces better results for polysemous words.The near-synonym prediction task highlights oneof the weaknesses of the multi-prototype approach:as the number of clusters increases, the number ofoccurrences assigned to each cluster decreases, in-creasing noise and resulting in some poor prototypesthat mainly cover outliers.
The word similarity taskis somewhat robust to this phenomenon, but syn-onym prediction is more affected since only the toppredicted choice is used.
When raters are forcedto chose between the top three predictions for eachmethod (presented as top set in Figure 3 left), the ef-fect of this noise is reduced and the multi-prototypeapproach remains dominant even for a large num-ber of clusters.
This indicates that although moreclusters can capture finer-grained sense distinctions,they also can introduce noise.When presented with words in context (Figure3 right),7 raters found no significant difference inthe two methods for words used in their majoritysense.8 However, when a minority sense is pre-7Results for the multi-prototype method are generated usingAvgSimC (soft assignment) as this was found to significantlyoutperform MaxSimC.8Sense frequency determined using Google; senses labeledmanually by trained human evaluators.114sented (e.g.
the ?prison?
sense of cell), raters pre-fer the choice predicted by the multi-prototype ap-proach.
This result is to be expected since the sin-gle prototype mainly reflects the majority sense, pre-venting it from predicting appropriate synonyms fora minority sense.
Also, once again, the perfor-mance of the multi-prototype approach is better forhomonyms than polysemes.4.4 Predicting Variation in Human RatingsVariance in pairwise prototype distances can helpexplain the variance in human similarity judgementsfor a given word pair.
We evaluate this hypothe-sis empirically on WordSim-353 by computing theSpearman correlation between the variance of theper-cluster similarity computations, V[D], D def={d(pik(w), pij(w?))
: 1 ?
k, j ?
K}, and the vari-ance of the human annotations for that pair.
Cor-relations for each dataset are shown in Figure 4 left.In general, we find a statistically significant negativecorrelation between these values using ?2 features,indicating that as the entropy of the pairwise clustersimilarities increases (i.e., prototypes become moresimilar, and similarities become uniform), rater dis-agreement increases.
This result is intuitive: if theoccurrences of a particular word cannot be easilyseparated into coherent clusters (perhaps indicatinghigh polysemy instead of homonymy), then humanjudgement will be naturally more difficult.Rater variance depends more directly on the ac-tual word similarity: word pairs at the extremeranges of similarity have significantly lower vari-ance as raters are more certain.
By removing wordpairs with similarity judgements in the middle twoquartile ranges (4.4 to 7.5) we find significantlyhigher variance correlation (Figure 4 right).
Thisresult indicates that multi-prototype similarity vari-ance accounts for a secondary effect separate fromthe primary effect that variance is naturally lower forratings in extreme ranges.Although the entropy of the prototypes correlateswith the variance of the human ratings, we find thatthe individual senses captured by each prototype donot correspond to human intuition for a given word,e.g.
the ?hurricane?
sense of position in Figure 1.This notion is evaluated empirically by computingthe correlation between the predicted similarity us-Figure 4: Plots of variance correlation; lower num-bers indicate higher negative correlation, i.e.
thatprototype entropy predicts rater disagreement.ing the contextual multi-prototype method and hu-man similarity judgements for different usages ofthe same word.
The Usage Similarity (USim) dataset collected in Erk et al (2009) provides such simi-larity scores from human raters.
However, we findno evidence for correlation between USim scoresand their corresponding prototype similarity scores(?
= 0.04), indicating that prototype vectors maynot correspond well to human senses.5 Discussion and Future WorkTable 3 compares the inferred synonyms for severaltarget words, generally demonstrating the ability ofthe multi-prototype model to improve the precisionof inferred near-synonyms (e.g.
in the case of singeror need) as well as its ability to include synonymsfrom less frequent senses (e.g., the experiment senseof research or the verify sense of prove).
However,there are a number of ways it could be improved:Feature representations: Multiple prototypes im-prove Spearman correlation on WordSim-353 com-pared to previous methods using the same under-lying representation (Agirre et al, 2009).
How-ever we have not yet evaluated its performancewhen using more powerful feature representationssuch those based on Latent or Explicit SemanticAnalysis (Deerwester et al, 1990; Gabrilovich andMarkovitch, 2007).
Due to its modularity, the multi-prototype approach can easily incorporate such ad-vances in order to further improve its effectiveness.115Inferred Thesaurusbasssingle guitar, drums, rhythm, piano, acousticmulti basses, contrabass, rhythm, guitar, drumsclaimsingle argue, say, believe, assert, contendmulti assert, contend, allege, argue, insistholdsingle carry, take, receive, reach, maintainmulti carry, maintain, receive, accept, reachmaintainsingle ensure, establish, achieve, improve, promotemulti preserve, ensure, establish, retain, restoreprovesingle demonstrate, reveal, ensure, confirm, saymulti demonstrate, verify, confirm, reveal, admitresearchsingle studies, work, study, training, developmentmulti studies, experiments, study, investigations,trainingsingersingle musician, actress, actor, guitarist, composermulti vocalist, guitarist, musician, singer-songwriter, singersTable 3: Examples of the top 5 inferred near-synonyms using the single- and multi-prototype ap-proaches (with results merged).
In general suchclustering improves the precision and coverage ofthe inferred near-synonyms.Nonparametric clustering: The success of thecombined approach indicates that the optimal num-ber of clusters may vary per word.
A more prin-cipled approach to selecting the number of proto-types per word is to employ a clustering model withinfinite capacity, e.g.
the Dirichlet Process MixtureModel (Rasmussen, 2000).
Such a model would al-low naturally more polysemous words to adopt moreflexible representations.Cluster similarity metrics: Besides AvgSim andMaxSim, there are many similarity metrics overmixture models, e.g.
KL-divergence, which maycorrelate better with human similarity judgements.Comparing to traditional senses: Compared toWordNet, our best-performing clusterings are sig-nificantly more fine-grained.
Furthermore, they of-ten do not correspond to agreed upon semantic dis-tinctions (e.g., the ?hurricane?
sense of position inFig.
1).
We posit that the finer-grained senses actu-ally capture useful aspects of word meaning, leadingto better correlation with WordSim-353.
However, itwould be good to compare prototypes learned fromsupervised sense inventories to prototypes producedby automatic clustering.Joint model: The current method independentlyclusters the contexts of each word, so the senses dis-covered forw cannot influence the senses discoveredfor w?
6= w. Sharing statistical strength across simi-lar words could yield better results for rarer words.6 ConclusionsWe presented a resource-light model for vector-space word meaning that represents words as col-lections of prototype vectors, naturally accountingfor lexical ambiguity.
The multi-prototype approachuses word sense discovery to partition a word?s con-texts and construct ?sense specific?
prototypes foreach cluster.
Doing so significantly increases the ac-curacy of lexical-similarity computation as demon-strated by improved correlation with human similar-ity judgements and generation of better near syn-onyms according to human evaluators.
Further-more, we show that, although performance is sen-sitive to the number of prototypes, combining pro-totypes across a large range of clusterings performsnearly as well as the ex-post best clustering.
Finally,variance in the prototype similarities is found to cor-relate with inter-annotator disagreement, suggestingpsychological plausibility.AcknowledgementsWe would like to thank Katrin Erk for helpful dis-cussions and making the USim data set available.This work was supported by an NSF Graduate Re-search Fellowship and a Google Research Award.Experiments were run on the Mastodon Cluster, pro-vided by NSF Grant EIA-0303609.ReferencesEneko Agirre and Phillip Edmond.
2006.
Word SenseDisambiguation: Algorithms and Applications (Text,Speech and Language Technology).
Springer-VerlagNew York, Inc., Secaucus, NJ, USA.Eneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.
Astudy on similarity and relatedness using distributionaland WordNet-based approaches.
In Proc.
of NAACL-HLT-09, pages 19?27.116F.
Gregory Ashby and Leola A. Alfonso-Reese.
1995.Categorization as probability density estimation.
J.Math.
Psychol., 39(2):216?233.L.
Douglas Baker and Andrew K. McCallum.
1998.
Dis-tributional clustering of words for text classification.In Proceedings of 21st International ACM SIGIR Con-ference on Research and Development in InformationRetrieval, pages 96?103.Arindam Banerjee, Inderjit Dhillon, Joydeep Ghosh, andSuvrit Sra.
2005.
Clustering on the unit hypersphereusing von Mises-Fisher distributions.
Journal of Ma-chine Learning Research, 6:1345?1382.Alexander Budanitsky and Graeme Hirst.
2006.
Evalu-ating wordnet-based measures of lexical semantic re-latedness.
Computational Linguistics, 32(1):13?47.James R. Curran and Marc Moens.
2002.
Improvementsin automatic thesaurus extraction.
In Proceedings ofthe ACL-02 workshop on Unsupervised lexical acqui-sition, pages 59?66.James R. Curran.
2004.
From Distributional to Seman-tic Similarity.
Ph.D. thesis, University of Edinburgh.College of Science.Scott C. Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society for Information Science,41:391?407.Inderjit S. Dhillon and Dharmendra S. Modha.
2001.Concept decompositions for large sparse text data us-ing clustering.
Machine Learning, 42:143?175.Katrin Erk, Diana McCarthy, Nicholas Gaylord Investi-gations on Word Senses, and Word Usages.
2009.
In-vestigations on word senses and word usages.
In Proc.of ACL-09.Katrin Erk.
2007.
A simple, similarity-based model forselectional preferences.
In Proceedings of the 45thAnnual Meeting of the Association for ComputationalLinguistics.
Association for Computer Linguistics.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context: the conceptrevisited.
In Proc.
of WWW-01, pages 406?414, NewYork, NY, USA.
ACM.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness using Wikipedia-based ex-plicit semantic analysis.
In Proc.
of IJCAI-07, pages1606?1611.David Graff.
2003.
English Gigaword.
Linguistic DataConsortium, Philadephia.Tom L. Griffiths, Kevin.
R. Canini, Adam N. Sanborn,and Daniel.
J. Navarro.
2007.
Unifying rational mod-els of categorization via the hierarchical Dirichlet pro-cess.
In Proc.
of CogSci-07.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Lillian Lee.
1999.
Measures of distributional similarity.In 37th Annual Meeting of the Association for Compu-tational Linguistics, pages 25?32.Dekang Lin and Patrick Pantel.
2002.
Concept discoveryfrom text.
In Proc.
of COLING-02, pages 1?7.Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.2003.
Identifying synonyms among distributionallysimilar words.
In Proceedings of the Interational JointConference on Artificial Intelligence, pages 1492?1493.
Morgan Kaufmann.Bradley C. Love, Douglas L. Medin, and Todd M.Gureckis.
2004.
SUSTAIN: A network model of cat-egory learning.
Psych.
Review, 111(2):309?332.Will Lowe.
2001.
Towards a theory of semantic space.In Proceedings of the 23rd Annual Meeting of the Cog-nitive Science Society, pages 576?581.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In Proc.
of SIGKDD-02, pages 613?619, New York, NY, USA.
ACM.Fernando C. N. Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of English words.
InProceedings of the 31st Annual Meeting of the Associ-ation for Computational Linguistics (ACL-93), pages183?190, Columbus, Ohio.Daniel Ramage, Anna N. Rafferty, and Christopher D.Manning.
2009.
Random walks for text seman-tic similarity.
In Proc.
of the 2009 Workshop onGraph-based Methods for Natural Language Process-ing (TextGraphs-4), pages 23?31.Carl E. Rasmussen.
2000.
The infinite Gaussian mixturemodel.
In Advances in Neural Information ProcessingSystems, pages 554?560.
MIT Press.Yves Rosseel.
2002.
Mixture models of categorization.J.
Math.
Psychol., 46(2):178?210.Mark Sanderson.
1994.
Word sense disambiguation andinformation retrieval.
In Proc.
of SIGIR-94, pages142?151.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?123.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Ng.
2008.
Cheap and fast?but is it good?
Eval-uating non-expert annotations for natural languagetasks.
In Proc.
of EMNLP-08.Amos Tversky and Itamar Gati.
1982.
Similarity, sepa-rability, and the triangle inequality.
Psychological Re-view, 89(2):123?154.Bram Vandekerckhove, Dominiek Sandra, and WalterDaelemans.
2009.
A robust and extensible exemplar-based model of thematic fit.
In Proc.
of EACL 2009,pages 826?834.
Association for Computational Lin-guistics.117
