Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 540?549,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsWhat Substitutes Tell Us ?Analysis of an ?All-Words?
Lexical Substitution CorpusGerhard KremerInstitute for Computational LinguisticsUniversity of Heidelberg, Germanykremer@cl.uni-heidelberg.deKatrin ErkDept.
of LinguisticsUniversity of Texas, Austin, U.S.A.katrin.erk@utexas.eduSebastian Pad?Institute for Natural Language ProcessingUniversity of Stuttgart, Germanypado@ims.uni-stuttgart.deStefan ThaterDept.
of Computational LinguisticsSaarland University, Saarbr?cken, Germanystth@coli.uni-sb.deAbstractWe present the first large-scale English ?all-words lexical substitution?
corpus.
Thesize of the corpus provides a rich resourcefor investigations into word meaning.
Weinvestigate the nature of lexical substitutesets, comparing them to WordNet synsets.We find them to be consistent with, butmore fine-grained than, synsets.
We alsoidentify significant differences to resultsfor paraphrase ranking in context reportedfor the SEMEVAL lexical substitution data.This highlights the influence of corpus con-struction approaches on evaluation results.1 IntroductionMany, if not most, words have multiple meanings;for example, the word ?bank?
has a financial anda geographical sense.
One common approach todeal with this lexical ambiguity is supervised wordsense disambiguation, or WSD (McCarthy, 2008;Navigli, 2009), which frames the task as a lemma-level classification problem, to be solved by train-ing classifiers on samples of lemma instances thatare labelled with their correct senses.This approach has its problems, however.
First,it assumes a complete and consistent set of labels.WordNet, used in the majority of studies, doescover several 10,000 lemmas, but has been criti-cised for both its coverage and granularity.
Second,WSD requires annotation for each sense and lemma,leading to an ?annotation bottleneck?.
A numberof technical solutions have been suggested regard-ing the second problem (Ando and Zhang, 2005;Navigli and Ponzetto, 2012), but not for the first.In 2009, McCarthy and Navigli address bothproblems by proposing a fundamentally differentapproach, called Lexical Substitution (McCarthyand Navigli, 2009) which avoids capturing a word?smeaning by a single label.
Instead, annotators areasked to list, for each instance of a word, one ormore alternative words or phrases to be substitutedfor the target in this particular context.
This setupprovides a number of benefits over WSD.
It al-lows characterising word meaning without usingan ontology and can be obtained easily from nativespeakers through crowdsourcing.
Work on mod-elling Lexical Substitution data has also assumed adifferent focus from WSD.
It tends to see the predic-tion of substitutes along the lines of compositionallexical semantics, concentrating on explaining howword meaning is modulated in context (Mitchelland Lapata, 2010).There are, however, important shortcomings ofthe work in the Lexical Substitution paradigm.
Allexisting datasets (McCarthy and Navigli, 2009;Sinha and Mihalcea, 2014; Biemann, 2013; Mc-Carthy et al., 2013) are either comparatively small,are ?lexical sample?
datasets, or both.
?Lexicalsample?
datasets consist of sample sentences foreach target word drawn from large corpora, withjust one target word substituted in each sentence.
InWSD, ?lexical sample?
datasets contrast with ?all-words?
annotation, in which all content words in atext are annotated for sense (Palmer et al., 2001).540In this paper, we present the first large ?all-words?
Lexical Substitution dataset for English.
Itprovides substitutions for more than 30,000 wordsof running text from two domains of MASC (Ide etal., 2008; Ide et al., 2010), a subset of the Ameri-can National Corpus (http://www.anc.org)that is freely available and has (partial) manualannotation.
The main advantage of the all-wordssetting is that it provides a realistic frequency distri-bution of target words and their senses.
We use thisto empirically investigate (a) the nature of lexicalsubstitution and (b) the nature of the corpus, seenthrough the lens of word meaning in context.2 Related Work2.1 Lexical Substitution: DataThe original ?English Lexical Substitution?
dataset(McCarthy and Navigli, 2009) comprises 200 targetcontent words (balanced numbers of nouns, verbs,adjectives and adverbs).
Targets were explicitly se-lected to exhibit interesting ambiguities.
For eachtarget, 10 sentences were chosen (mostly at ran-dom, but in part by hand) from the English InternetCorpus (Sharoff, 2006) and presented to 5 anno-tators to collect substitutes.
Its total size is 2,000target instances.
Sinha and Mihalcea (2014) pro-duced a small pilot dataset (500 target instances) forall-words substitution, asking three annotators tosubstitute all content words in presented sentences.Biemann (2013) first investigated the use ofcrowdsourcing, developing a three-task bootstrap-ping design to control for noise.
His study coversover 50,000 instances, but these correspond only to397 targets, all of which are high-frequency nouns.Biemann clusters the resulting substitutes into wordsenses.
McCarthy et al.
(2013) applied lexical sub-stitution in a cross-lingual setting, annotating 130of the original McCarthy and Navigli targets withSpanish substitutions (i. e., translations).2.2 Lexical Substitution: ModelsThe LexSub task at SEMEVAL 2007 (McCarthyand Navigli, 2009) required systems to both de-termine substitution candidates and choose con-textual substitutions in each case.
Erk and Pad?
(2008) treated the gold substitution candidates asgiven and focused on the context-specific rankingof those candidates.
In this form, the task has beenaddressed through three types of (mostly unsuper-vised) approaches.
The first group computes a sin-gle type representation and modifies it accordingto sentence context (Erk and Pad?, 2008; Thater etal., 2010; Thater et al., 2011; Van de Cruys et al.,2011).
The second group of approaches clustersinstance representations (Reisinger and Mooney,2010; Dinu and Lapata, 2010; Erk and Pad?, 2010;O?S?aghdha and Korhonen, 2011).
The third op-tion is to use a language model (Moon and Erk,2013).
Recently, supervised models have emerged(Biemann 2013; Szarvas et al., 2013a,b).3 COINCO ?
The MASC All-WordsLexical Substitution Corpus1Compared to, e. g., WSD, there still is little gold-annotated data for lexical substitution.
With theexception of the dataset created by Biemann (2013),all existing lexical substitution datasets are fairlysmall, covering at most several thousand instancesand few targets which are manually selected.
Weaim to fill this gap, providing a dataset that mirrorsthe actual corpus distribution of targets in sentencecontext and is sufficiently large to enable a detailed,lexically specific analysis of substitution patterns.3.1 Source Corpus ChoiceFor annotation, we chose a subset of the ?ManuallyAnnotated Sub-Corpus?
MASC (Ide et al., 2008;Ide et al., 2010) which is ?equally distributed across19 genres, with manually produced or validatedannotations for several layers of linguistic phenom-ena?, created with the purpose of being ?free ofusage and redistribution restrictions?.
We chosethis corpus because (a) our analyses can profit fromthe preexisting annotations and (b) we can releaseour annotations as part of MASC.Since we could not annotate the complete MASC,we selected (complete) text documents from twoprominent genres: news (18,942 tokens) and fiction(16,605 tokens).
These two genres are both rele-vant for NLP and provide long, coherent documentsthat are appropriate for all-words annotation.
Weused the MASC part-of-speech annotation to iden-tify all content words (verbs, nouns, adjectives, andadverbs), which resulted in a total of over 15,000targets for annotation.
This method differs fromNavigli and McCarthy?s (2009) in two crucial re-spects: we annotate all instances of each target, andinclude all targets regardless of frequency or levelof lexical ambiguity.
We believe that our corpus isconsiderably more representative of running text.1Available as XML-formatted corpus ?Concepts in Con-text?
(COINCO) from http://goo.gl/5C0jBH.
Alsoscheduled for release as part of MASC.5413.2 CrowdsourcingWe used the Amazon Mechanical Turk (AMT) plat-form to obtain substitutes by crowdsourcing.
Inter-annotator variability and quality issues due to non-expert annotators are well-known difficulties (see,e.
g., Fossati et al.
(2013)).
Our design choiceswere shaped by ?best practices in AMT?, includingMason and Suri (2012) and Biemann (2013).Defining HITs.
An AMT task consists of HumanIntelligence Tasks (HITs), each of which is sup-posed to represent a minimal, self-contained task.In our case, potential HITs were annotations of(all target words in) one sentence, or just one tar-get word.
The two main advantages of annotatinga complete sentence at a time are (a) less over-head, because the sentence has only to be readonce; (b) higher reliability, since all words within asentence will be annotated by the same person.Unfortunately, presenting individual sentencesas HITs also means that all sentences pay the sameamount irrespective of their length.
Since long sen-tences require more effort, they are likely to receiveless attention.
We therefore decided to generallypresent two random target words per HIT, and oneword in the case of ?leftover?
singleton targets.In the HITs, AMT workers (?turkers?)
saw thehighlighted target word in context.
Since one sen-tence was often insufficient to understand the targetfully, we also showed the preceding and the follow-ing sentence.
The task description asked turkers toprovide (preferably single-word) substitutes for thetarget that ?would not change the meaning?.
Theywere explicitly allowed to use a ?more general term?in case a substitute was hard to find (e. g., dog forthe target dachshund, cf.
basic level effects: Roschet al.
(1976)).
Turkers were encouraged to produceas many replacements as possible (up to 5).
If theycould not find a substitute, they had to check one ofthe following radio buttons: ?proper name?, ?partof a fixed expression?, ?no replacement possible?,?other problem (with description)?.Improving Reliability.
Another major problemis reliability.
Ideally, the complete dataset shouldbe annotated by the same group of annotators, butturkers tend to work only on a few HITs beforeswitching to other AMT jobs.
Following an ideaof Biemann and Nygaard (2010), we introduced atwo-tier system of jobs aimed at boosting turkerloyalty.
A tier of ?open tasks?
served to identifyreliable turkers by manually checking their givensubstitutes for plausibility.
Such turkers were theninvited to the second, ?closed task?
tier, with ahigher payment.
In both tiers, bonus paymentswere offered to those completing full HIT sets.For each target, we asked 6 turkers to providesubstitutions.
In total, 847 turkers participated suc-cessfully.
In the open tasks, 839 turkers submitted12,158 HITs (an average of 14.5 HITs).
In theclosed tasks, 25 turkers submitted 42,827 HITs (anaverage of 1,713 HITs), indicating the substantialsuccess of our turker retention scheme.Cost.
In the open task, each HIT was paid forwith $ 0.03, in the closed task the wage was $ 0.05per HIT.
The bonus payment for completing a HITset amounted to $ 2 ($ 1) in the open (closed) tasks.The average cost for annotations was $ 0.22 for onetarget word instance and $ 0.02 for one substitute.The total cost with fees was ~$ 3,400.3.3 COINCO: Corpus and Paraset StatisticsWe POS-tagged and lemmatised targets and substi-tutes in sentence context with TreeTagger (Schmid,1994).
We manually lemmatised unknown words.Our annotated dataset comprises a total of 167,336responses by turkers for 15,629 target instances in2,474 sentences (7,117 nouns, 4,617 verbs, 2,470adjectives, and 1,425 adverbs).
As outlined above,targets are roughly balanced across the two gen-res (news: 8,030 instances in 984 sentences; fic-tion: 7,599 instances in 1,490 sentences).
There are3,874 unique target lemmas; 1,963 of these occurmore than once.
On this subset, there is a mean of6.99 instances per target lemma.
To our knowledge,our corpus is the largest lexical substitution datasetin terms of lemma coverage.Each target instance is associated with a paraset(i. e., the set of substitutions or paraphrases pro-duced for a target in its context) with an averagesize of 10.71.
Turkers produced an average of1.68 substitutions per target instance.2Despiteour instructions to provide single-word substitutes,11,337 substitutions contain more than one word.3.4 Inter-Annotator AgreementMcCarthy and Navigli (2009) introduced two inter-annotator agreement (IAA) measures for theirdataset.
The first one is pairwise agreement (PA),2Note that a small portion of the corpus was annotated bymore than 6 annotators.542dataset # targets PA mode-% PAmMN09 1,703 27.7 73.9 50.7SM13 550 15.5 N/A N/ACOINCO (complete) 15,400 19.3 70.9 44.7COINCO (subset) 2,828 24.6 76.4 50.9Table 1: Pairwise turker agreement (mode-%: per-centage of target instances with a mode)measuring the overlap of produced substitutions:PA =?t?T??st,s?t?
?Ct|st?
s?t||st?
s?t|?1|Ct| ?
|T |where t is a target in our target set T , stis theparaset provided by one turker for t, and Ctis theset comprising all pairs of turker-specific parasetsfor t. Only targets with non-empty parasets (i. e.,not marked by turkers as a problematic target) fromat least two turkers are included.
The second oneis mode agreement (PAm), the agreement of an-notators?
parasets with the mode (the unique mostfrequent substitute) for all targets where one exists:PAm=?t?Tm?st?St[m ?
st] ?1|st| ?
|Tm|where Tmis the set of all targets with some modem and Stis the set of all parasets for target t. TheIverson bracket notation [m ?
st] denotes 1 ifmode m is included in st(otherwise 0).Table 1 compares our dataset to the results byMcCarthy and Navigli (2009, MN09) and Sinhaand Mihalcea (2014, SM13).
The scores forour complete dataset (row 3) are lower than Mc-Carthy and Navigli?s both for PA (?8 %) and PAm(?6 %), but higher than Sinha and Mihalcea?s, whoalso note the apparent drop in agreement.3We believe that this is a result of differences inthe setup rather than an indicator of low quality:Note that PA will tend to decrease both in the faceof more annotators and of more substitutes.
Bothof these factors are present in our setup.
To test thisinterpretation, we extracted a subset of our data thatis comparable to McCarthy and Navigli?s regard-ing these factors.
It comprises all target instanceswhere (a) exactly 6 turkers gave responses (9,521targets), and (b) every turker produced between oneand three substitutes (5,734 targets).
The results forthis subset (row 4) are much more similar to thoseof McCarthy and Navigli: the pairwise agreement3Please see McCarthy and Navigli (2009) for a possibleexplanation of the generally low IAA numbers in this field.relation all verb noun adj advsyn 9.4 12.5 7.7 8.0 10.4direct-hyper 6.6 9.3 7.6 N/A N/Adirect-hypo 7.5 11.6 8.0 N/A N/Atrans-hyper 3.2 2.8 4.7 N/A N/Atrans-hypo 3.0 3.7 3.8 N/A N/Awn-other 68.9 60.7 66.5 88.5 85.4not-in-wn 2.1 0.9 2.2 3.4 4.2Table 2: Target?substitute relations in percentages,overall (all) and by POS.
Note: WordNet containsno hypo-/hypernyms for adjectives and adverbs.differs only by 3 %, and the mode agreement isalmost identical.
We take these figures as indica-tion that crowdsourcing can serve as a sufficientlyreliable way to create substitution data; note thatSinha and Mihalcea?s annotation was carried out?traditionally?
by three annotators.Investigating IAA numbers by target POS and bygenre, we found only small differences (?
2.6 %)among the various subsets, and no patterns.4 Characterising Lexical SubstitutionsThis section examines the collected lexical substi-tutions, both quantitatively and qualitatively.
Weexplore three questions: (a) What lexical relationshold between targets and their substitutes?
(b) Doparasets resemble word senses?
(c) How similarare the parasets that correspond to the same wordsense of a target?
These questions have not beenaddressed before, and we would argue that theycould not be addressed before, because previouscorpora were either too small or were sampled in away that was not conducive to this analysis.We use WordNet (Fellbaum, 1998), release 3.1,as a source for both lexical relations and wordsenses.
WordNet is the de facto standard in NLPand is used for both WSD and broader investiga-tions of word meaning (Navigli and Ponzetto, 2012;Erk and McCarthy, 2009).
Multi-word substitutesare excluded from all analyses.44.1 Relating Targets and SubstitutesWe first look at the most canonical lexical relationsbetween a target and its substitutes.
Table 2 lists thepercentage of substitutes that are synonyms (syn),direct/transitive (direct-/trans-) hypernyms (hyper)4All automatic lexical substitution approaches, includingSection 5, omit multi-word expressions.
Also, they can beexpected to have WordNet coverage and normalisation issues,which would constitute a source of noise for this analysis.543sentence substitutesNow, how can I help the elegantly mannered friend ofmy Nepthys and his surprising young charge ?dependent, person, task, lass, prot?g?, effort, companionThe distinctive whuffle of pleasure rippled through thebetas on the bridge, and Rakal let loose a small growl,as if to caution his charges against false hope.dependent, command, accusation, private, companion, follower,subordinate, prisoner, teammate, ward, junior, underling, enemy,group, crew, squad, troop, team, kidTable 3: Context effects below the sense level: target noun ?charge?
(wn-other shown in italics)and hyponyms (hypo) of the target.
If a substitutehad multiple relations to the target, the shortest pathfrom any of its senses to any sense of the targetwas chosen.
The table also lists the percentage ofsubstitutes that are elsewhere in WordNet but notrelated to the target (wn-other) and substitutes thatare not covered by WordNet (not-in-wn).We make three main observations.
First, Word-Net shows very high coverage throughout ?
thereare very few not-in-wn substitutes.
Second, the per-centages of synonyms, hypernyms and hyponymsare relatively similar (even though the annotationguidelines encouraged the annotation of hyponymsover hypernyms), but relatively small.
Finally, andmost surprisingly, the vast majority of substitutesacross all parts of speech are wn-other.A full analysis of wn-other is beyond the cur-rent paper.
But a manual analysis of wn-othersubstitutes for 10 lemmas5showed that most ofthem were context-specific substitutes that can dif-fer even when the sense of the target is the same.This is illustrated in Table 3, which features twooccurrences of the noun ?charge?
in the sense of?person committed to your care?.
But because ofthe sentence context, the first occurrence got sub-stitutes like ?prot?g?
?, while the second one wasparaphrased by words like ?underling?.
We alsosee evidence of annotator error (e. g., ?command?and ?accusation?
in the second sentence).6Dis-counting such instances still leaves a prominentrole for correct wn-other cases.But are these indeed contextual modulation ef-fects below the sense level, or are parasets funda-mentally different from word senses?
We performtwo quantitative analyses to explore this question.4.2 Comparing Parasets to SynsetsTo what extent do parasets follow the boundariesof WordNet senses?
To address this question, we5We used the nouns business, charge, place, way and theverbs call, feel, keep, leave, show, stand.6A manual analysis of the same 10 lemmas showed only38 out of 1,398 (0.027) of the substitutes to be erroneous.paraset?sense mapping class verb noun adj advmappable 90.3 73.5 33.0 49.6uniquely mappable 63.1 57.5 24.3 41.3Table 4: Ratios of (uniquely) mappable parasetsestablish a mapping between parasets and synsets.Since gold standard word senses in MASC are lim-ited to high-frequency lemmas and cover only asmall part of our data, we create a heuristic map-ping that assigns each paraset to that synset of itstarget with which it has the largest intersection.
Weuse extended WordNet synsets that include directhypo- and hypernyms to achieve better matcheswith parasets.
We call a paraset uniquely mappableif it has a unique best WordNet match, and map-pable if one or more best matches exist.
Table 4shows that most parasets are mappable for nounsand verbs, but not for adjectives or adverbs.We now focus on mappable parasets for nounsand verbs.
To ensure that this does not lead to aconfounding bias, we performed a small manualstudy on the 10 noun and verb targets mentionedabove (247 parasets).
We found 25 non-mappableparasets, which were due to several roughly equallyimportant reasons: gaps in WordNet, multi-wordexpressions, metaphor, problems of sense granular-ity, and annotator error.
We also found 66 parasetswith multiple best matches.
The two dominantsources were target occurrences that evoked morethan one sense and WordNet synset pairs with veryclose meanings.
We conclude that excluding non-mappable parasets does not invalidate our analysis.To test whether parasets tend to map to a singlesynset, we use a cluster purity test that comparesa set of clusters C to a set of gold standard classesC?.
Purity measures the accuracy of each clusterwith respect to its best matching gold class:purity(C,C?)
=1NK?k=1maxk?|Ck?
C?k?|where N is the total number of data points, K is the544measure verbs nounscluster purity (%) 75.1 81.2common core size within sense 1.84 2.21common core size across senses 0.39 0.41paraset size 6.89 6.29Table 5: Comparing uniquely mappable parasets tosenses: overlap with best WordNet match as clusterpurity (top), and intersection size of parasets withand without the same WordNet match (bottom)number of clusters, and C?k?is the gold class thathas the largest overlap with cluster Ck.
In our case,C is the set of mappable parasets7, C?the set ofextended WordNet synsets, and we only considersubstitutes that occur in one of the target?s extendedsynsets (these are the data points).
This makes thecurrent analysis complementary to the relationalanalysis in Table 2.8The result, listed in the first row of Table 5,shows that parasets for both verbs and nouns havea high purity, that is, substitutes tend to focus on asingle sense.
This can be interpreted as saying thatannotators tend to agree on the general sense of atarget.
Roughly 20?25 % of substitutes, however,tend to stem from a synset of the target that is notthe best WordNet match.
This result comes withthe caveat that it only applies to substitutes thatare synonyms or direct hypo- and hypernyms ofthe target.
So in the next section, we perform ananalysis that also includes wn-other substitutes.4.3 Similarity Between Same-Sense ParasetsWe now use the WordNet mappings from the pre-vious section to ask how (dis-)similar parasets arethat represent the same word sense.
We also try toidentify the major sources for dissimilarity.We quantify paraset similarity as the commoncore, that is, the intersection of all parasets forthe same target that map onto the same extendedWordNet synset.
Surprisingly, the common coreis mostly non-empty (in 85.6 % of all cases), andcontains on average around two elements, as thesecond row in Table 5 shows.
For this analysis, weonly use uniquely mappable parasets.
In relationto the average paraset size (see row 4), this meansthat one quarter to one third of the substitutes are7For non-uniquely mappable parasets, the purity is thesame for all best-matching synsets.8Including wn-other substitutes would obscure whetherlow purity means substitutes from a mixture of senses (whichwe are currently interested in) or simply a large number ofwn-other substitutes (which we have explored above).set elementssynset \ core feel, perceive, comprehendsynset ?
core sensecore \ synset noticenon-core substitutes detect, recall, perceive, experi-ence, note, realize, discernTable 6: Target feel.v.03: synset and common coreshared among all instances of the same target?sensecombination.
In contrast, the common core forall parasets of targets that map onto two or moresynsets contains only around 0.4 substitutes (seerow 3) ?
that is, it is empty more often than not.At the same time, if about one quarter to onethird of the substitutes are shared, this means thatthere are more non-shared than shared substituteseven for same-sense parasets.
Some of these casesresult from small samples: Even 6 annotators can-not always exhaust all possible substitutes.
Forexample, the phrase ?I?m starting to see more busi-ness transactions?
occurs twice in the corpus.
Thetwo parasets for ?business?
share the same bestWordNet sense match, but they have only 3 sharedand 7 non-shared substitutes.
This is even thoughthe substitutes are all valid and apply to both in-stances.
Other cases are instances of the contextsensitivity of the Lexical Substitution task as dis-cussed above.
Table 6 illustrates on an examplehow the common core of a target sense relates tothe corresponding synset; note the many context-specific substitutes outside the common core.5 Ranking ParaphrasesWhile there are several studies on modelling lexi-cal substitutes, almost all reported results use Mc-Carthy and Navigli?s SEMEVAL 2007 dataset.
Wenow compare the results of three recent computa-tional models on COINCO (our work) and on theSEMEVAL 2007 dataset to highlight similaritiesand differences between the two datasets.Models.
We consider the paraphrase rankingmodels of Erk and Pad?
(2008, EP08), Thater etal.
(2010, TFP10) and Thater et al.
(2011, TFP11).These models have been analysed by Dinu et al.
(2012) as instances of the same general frameworkand have been shown to deliver state-of-the-art per-formance on the SEMEVAL 2007 dataset, with bestresults for Thater et al.
(2011).The three models share the idea to represent themeaning of a target word in a specific context by545corpus syntactically structured syntactically filtered bag of words randomTFP11 TFP10 EP08 TFP11/EP08 TFP10 TFP11/EP08 TFP10COINCOcontext 47.8 46.0 47.4 47.4 41.9 46.2 40.833.0baseline 46.2 44.6 46.2 45.8 38.8 44.7 37.5SEMEVAL 2007context 52.5 48.6 49.4 50.1 44.7 48.0 42.630.0baseline 43.7 42.7 43.7 44.4 38.0 42.7 35.8COINCO Subsetcontext 40.3 37.7 39.0 39.2 34.1 37.7 32.523.7baseline 36.7 35.7 36.7 36.4 30.6 35.4 28.0Table 7: Corpus comparison in terms of paraphrase ranking quality (GAP percentage).
SEMEVAL resultsfrom Thater et al.
(2011).
?Context?
: full models, ?baseline?
: uncontextualised target-substitute similarity.modifying the target?s basic meaning vector withinformation from the vectors of the words in thetarget?s direct syntactic context.
For instance, thevector of ?coach?
in the phrase ?the coach derailed?is obtained by modifying the basic vector represen-tation of ?coach?
through the vector of ?derail?, sothat the resulting contextualised vector reflects thetrain car sense of ?coach?.We replicate the setup of Thater et al.
(2011)to make our numbers directly comparable.
Weconsider three versions of each model: (a) syntacti-cally structured models use vectors which recordco-occurrences based on dependency triples, ex-plicitly recording syntactic role information withinthe vectors; (b) syntactically filtered models alsouse dependency-based co-occurrence information,but the syntactic role is not explicitly represented inthe vector representations; (c) bag-of-words mod-els use a window of ?
5 words.
All co-occur-rence counts are extracted from the English Giga-word corpus (http://catalog.ldc.upenn.edu/LDC2003T05), analysed with Stanford de-pendencies (de Marneffe et al., 2006).We apply the models to our dataset as follows:We first collect all substitutes for all occurrences ofa target word in the corpus.
The task of our modelsfor each target instance is then to rank the candi-dates so that the actual substitutes are ranked higherthan the rest.
We rank candidates according to thecosine similarity between the contextualised vec-tor of the target and the vectors of the candidates.Like most previous approaches, we compare theresulting ranked list with the gold standard annota-tion (the paraset of the target instance), using gen-eralised average precision (Kishida, 2005, GAP),and using substitution frequency as weights.
GAPscores range between 0 and 1; a score of 1 indicatesa perfect ranking in which all correct substitutesprecede all incorrect ones, and correct high-weightsubstitutes precede low-weight substitutes.Results.
The upper part of Table 7 shows resultsfor our COINCO corpus and the previous stan-dard dataset, SEMEVAL 2007.
?Context?
refers tothe full models, and ?baseline?
to global, context-unaware ranking based on the semantic similaritybetween target and substitute.
Baselines are model-specific since they re-use the models?
vector repre-sentations.
Note that EP08 and TFP11 are identicalunless syntactically structured vectors are used, andtheir baselines are identical.The behaviour of the baselines on the two cor-pora is quite similar: random baselines have GAPsaround 0.3, and uncontextualised baselines haveGAPs between 0.35 and 0.46.
The order of themodels is also highly parallel: the syntacticallystructured TFP11 is the best model, followed byits syntactically filtered version and syntacticallystructured EP08.
All differences between thesemodels are significant (p< 0.01) for both corpora,as computed with bootstrap resampling (Efron andTibshirani, 1993).
That is, the model ranking onSEMEVAL is replicated on COINCO.There are also substantial differences betweenthe two corpora, though.
Most notably, all modelsperform substantially worse on COINCO.
Thisis true in absolute terms (we observe a loss of 2?5 % GAP) but even more dramatic expressed as thegain over the uninformed baselines (almost 9 % forTFP11 on SEMEVAL but only 1.2 % on COINCO).All differences between COINCO and SEMEVALare again significant (p< 0.01).We see three major possible reasons for thesedifferences: variations in (a) the annotation setup(crowdsourcing, multiple substitutes); (b) the sensedistribution; (c) frequency and POS distributionsbetween the two corpora.
We focus on (c) since itcan be manipulated most easily.
SEMEVAL con-tains exactly 10 instances for all targets, while CO-INCO reflects the Zipf distribution of ?natural?
cor-pora, with many targets occurring only once.
Such546corpora are easier to model in terms of absoluteperformance, because the paraphrase lists for raretargets contain less false positives for each instance.For hapax legomena, the set of substitution candi-dates is identical to the gold standard, and the onlyway to receive a GAP score lower than 1 for suchtargets is to rank low-weight substitutes ahead ofhigh-weight substitutes.
Not surprisingly, the meanGAP score of the syntactically structured TFP11for hapax legomena is 0.863.
At the same time,such corpora make it harder for full models to out-perform uncontextualised baselines; the best model(TFP11) only outperforms the baseline by 1.6 %.To neutralise this structural bias, we created?SEMEVAL-like?
subsets of COINCO (collectivelyreferred to as the COINCO Subset) by extractingall COINCO targets with at least 10 instances (141nouns, 101 verbs, 50 adjectives, 36 adverbs) andbuilding 5 random samples by drawing 10 instancesfor each target.
These samples match SEMEVAL inthe frequency distribution of its targets.
To accountfor the unequal distribution of POS in the samples,we compute GAP scores for each POS separatelyand calculate these GAP scores?
average.The results for the various models on the CO-INCO Subset in the bottom part of Table 7 showthat the differences between COINCO and SE-MEVAL are not primarily due to the differencesin target frequencies and POS distribution ?
theCOINCO Subset is actually more different to SE-MEVAL than the complete COINCO.
Strikingly,the COINCO Subset is very difficult, with a ran-dom baseline of 24 % and model performances be-low 37 % (baselines) and up to 40 % (full models),which indicates that the set of substitutes in CO-INCO is more varied than in SEMEVAL as an effectof the annotation setup.
Encouragingly, the marginbetween full models and baselines is larger than onthe complete COINCO and generally amounts to2?4 % (3.6 % for TFP11).
That is, the full modelsare more useful on the COINCO corpus than theyappeared at first glance; however, their effect stillremains much smaller than on SEMEVAL.6 ConclusionThis paper describes COINCO, the first large-scale?all-words?
lexical substitution corpus for English.It was constructed through crowdsourcing on thebasis of MASC, a corpus of American English.The corpus has two major advantages over previ-ous lexical substitution corpora.
First, it covers con-tiguous documents rather than selected instances.We believe that analyses on our corpus generalisebetter to the application domain of lexical substitu-tion models, namely random unseen text.
In fact,we find substantial differences between the perfor-mances of paraphrase ranking models for COINCOand the original SEMEVAL 2007 LexSub dataset:the margin of informed methods over the baselinesare much smaller, even when controlling for targetfrequencies and POS distribution.
We attribute thisdivergence at least in part to the partially manual se-lection strategy of SEMEVAL 2007 (cf.
Section 2.1)which favours a more uniform distribution acrosssenses, while our whole-document annotation facesthe ?natural?
distribution skewed towards predom-inant senses.
This favours the non-contextualisedbaseline models, consistent with our observations.At the very least, our findings demonstrate the sen-sitivity of evaluation results on corpus properties.The second benefit of our corpus is that its sizeenables more detailed analyses of lexical substi-tution data than previously possible.
We are ableto investigate the nature of the paraset, i. e., theset of lexical substitutes given for one target in-stance, finding that lexical substitution sets corre-spond fairly well to WordNet sense distinctions(parasets for the same synset show high similarity,while those for different senses do not).
In addition,however, we observe a striking degree of context-dependent variation below the sense level: the ma-jority of lexical substitutions picks up fine-grained,situation-specific meaning components that do notqualify as sense distinctions in WordNet.Avenues for future work include a more detailedanalysis of the substitution data to uncover genre-and domain-specific patterns and the developmentof lexical substitution models that take advantageof the all-words substitutes for global optimisation.AcknowledgementsWe are grateful to Jan Pawellek for implementingthe AMT task, extracting MASC data, and preparingHITs.
Furthermore, we thank Georgiana Dinu forher support with the word meaning models.ReferencesRie Kubota Ando and Tong Zhang.
2005.
A frame-work for learning predictive structures from multipletasks and unlabeled data.
Journal of Machine Learn-ing Research, 6:1817?1853.547Chris Biemann and Valerie Nygaard.
2010.
Crowd-sourcing WordNet.
In Proceedings of the 5th GlobalWordNet conference, Mumbai, India.Chris Biemann.
2013.
Creating a system for lexi-cal substitutions from scratch using crowdsourcing.Language Resources and Evaluation, 47(1):97?122.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of LREC, pages 449?454, Genoa, Italy.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedingsof EMNLP, pages 1162?1172, Cambridge, MA.Georgiana Dinu, Stefan Thater, and S?ren Laue.
2012.A comparison of models of word meaning in con-text.
In Proceedings of NAACL, pages 611?615,Montr?al, Canada.Bradley Efron and Robert J. Tibshirani.
1993.
AnIntroduction to the Bootstrap.
Chapman and Hall,New York.Katrin Erk and Diana McCarthy.
2009.
Graded wordsense assignment.
In Proceedings of EMNLP, pages440?449, Singapore.Katrin Erk and Sebastian Pad?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of EMNLP, pages 897?906, Honolulu,HI.Katrin Erk and Sebastian Pad?.
2010.
Exemplar-basedmodels for word meaning in context.
In Proceedingsof ACL, pages 92?97, Uppsala, Sweden.Christiane Fellbaum, editor.
1998.
WordNet: Anelectronic lexical database.
MIT Press, Cambridge,MA.Marco Fossati, Claudio Giuliano, and Sara Tonelli.2013.
Outsourcing FrameNet to the crowd.
In Pro-ceedings of ACL, pages 742?747, Sofia, Bulgaria.Nancy Ide, Collin F. Baker, Christiane Fellbaum,Charles Fillmore, and Rebecca Passonneau.
2008.MASC: The manually annotated sub-corpus ofAmerican English.
In Proceedings of LREC, pages2455?2461, Marrakech, Morocco.Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-becca Passonneau.
2010.
The manually annotatedsub-corpus: A community resource for and by thepeople.
In Proceedings of ACL, pages 68?73, Upp-sala, Sweden.Kazuaki Kishida.
2005.
Property of average precisionand its generalization: An examination of evalua-tion indicator for information retrieval experiments.Technical Report NII-2005-014E, Japanese NationalInstitute of Informatics.Winter Mason and Siddharth Suri.
2012.
Conductingbehavioral research on Amazon?s Mechanical Turk.Behavior Research Methods, 44(1):1?23.Diana McCarthy and Roberto Navigli.
2009.
The En-glish lexical substitution task.
Language Resourcesand Evaluation, 43(2):139?159.Diana McCarthy, Ravi Sinha, and Rada Mihalcea.2013.
The cross-lingual lexical substitution task.Language Resources and Evaluation, 47(3):607?638.Diana McCarthy.
2008.
Word sense disambiguation.In Linguistics and Language Compass.
Blackwell.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Taesun Moon and Katrin Erk.
2013.
An inference-based model of word meaning in context as a para-phrase distribution.
ACM Transactions on Intelli-gent Systems and Technology, 4(3).Roberto Navigli and Simone Paolo Ponzetto.
2012.Babelnet: The automatic construction, evaluationand application of a wide-coverage multilingual se-mantic network.
Artificial Intelligence, 193:217?250.Roberto Navigli.
2009.
Word sense disambiguation: Asurvey.
ACM Computing Surveys, 41:1?69.Diarmuid O?S?aghdha and Anna Korhonen.
2011.Probabilistic models of similarity in syntactic con-text.
In Proceedings of EMNLP, pages 1047?1057,Edinburgh, UK.Martha Palmer, Christiane Fellbaum, Scott Cotton,Lauren Delfs, and Hoa Trang Dang.
2001.
Englishtasks: All-words and verb lexical sample.
In Pro-ceedings of the SENSEVAL-2 workshop, pages 21?24, Toulouse, France.Joseph Reisinger and Raymond J. Mooney.
2010.Multi-prototype vector-space models of word mean-ing.
In Proceeding of NAACL, pages 109?117, LosAngeles, CA.Eleanor Rosch, Carolyn B. Mervis, Wayne D. Gray,David M. Johnson, and Penny Boyes-Braem.
1976.Basic objects in natural categories.
Cognitive Psy-chology, 8(3):382?439.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings ofNEMLAP, pages 44?49, Manchester, UK.Serge Sharoff.
2006.
Open-source corpora: Using thenet to fish for linguistic data.
International Journalof Corpus Linguistics, 11(4):435?462.Ravi Sinha and Rada Mihalcea.
2014.
Explorationsin lexical sample and all-words lexical substitution.Natural Language Engineering, 20(1):99?129.548Gy?rgy Szarvas, Chris Biemann, and Iryna Gurevych.2013a.
Supervised all-words lexical substitutionusing delexicalized features.
In Proceedings ofNAACL-HLT, pages 1131?1141, Atlanta, GA.Gy?rgy Szarvas, R?bert Busa-Fekete, and Eyke H?ller-meier.
2013b.
Learning to rank lexical substitutions.In Proceedings of EMLNP, pages 1926?1932, Seat-tle, WA.Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations us-ing syntactically enriched vector models.
In Pro-ceedings of ACL, pages 948?957, Uppsala, Sweden.Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.2011.
Word meaning in context: A simple and effec-tive vector model.
In Proceedings of IJCNLP, pages1134?1143, Chiang Mai, Thailand.Tim Van de Cruys, Thierry Poibeau, and Anna Korho-nen.
2011.
Latent vector weighting for word mean-ing in context.
In Proceedings of EMNLP, pages1012?1022, Edinburgh, Scotland.549
