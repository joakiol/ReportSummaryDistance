Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 46?52,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational LinguisticsThe LIGM-Alpage Architecture for the SPMRL 2013 Shared Task:Multiword Expression Analysis and Dependency ParsingMatthieu ConstantUniversite?
Paris-EstLIGMCNRSMarie CanditoAlpageParis Diderot UnivINRIADjame?
SeddahAlpageParis Sorbonne UnivINRIAAbstractThis paper describes the LIGM-Alpage sys-tem for the SPMRL 2013 Shared Task.
Weonly participated to the French part of the de-pendency parsing track, focusing on the real-istic setting where the system is informed nei-ther with gold tagging and morphology nor(more importantly) with gold grouping of to-kens into multi-word expressions (MWEs).While the realistic scenario of predicting bothMWEs and syntax has already been investi-gated for constituency parsing, the SPMRL2013 shared task datasets offer the possibil-ity to investigate it in the dependency frame-work.
We obtain the best results for French,both for overall parsing and for MWE recog-nition, using a reparsing architecture that com-bines several parsers, with both pipeline archi-tecture (MWE recognition followed by pars-ing), and joint architecture (MWE recognitionperformed by the parser).1 IntroductionAs shown by the remarkable permanence over theyears of specialized workshops, multiword expres-sions (MWEs) identification is still receiving consid-erable attention.
For some languages, such as Ara-bic, French, English, or German, a large quantity ofMWE resources have been generated (Baldwin andNam, 2010).
Yet, while special treatment of com-plex lexical units, such as MWEs, has been shown toboost performance in tasks such as machine transla-tion (Pal et al 2011), there has been relatively littlework exploiting MWE recognition to improve pars-ing performance.Indeed, a classical parsing scenario is to pre-group MWEs using gold MWE annotation (Arunand Keller, 2005).
This non-realistic scenario hasbeen shown to help parsing (Nivre and Nilsson,2004; Eryigit et al 2011), but the situation is quitedifferent when switching to automatic MWE predic-tion.
In that case, errors in MWE recognition al-leviate their positive effect on parsing performance(Constant et al 2012).
While the realistic scenarioof syntactic parsing with automatic MWE recogni-tion (either done jointly or in a pipeline) has alreadybeen investigated in constituency parsing (Caffer-key et al 2007; Green et al 2011; Constant et al2012; Green et al 2013), the French dataset of theSPMRL 2013 Shared Task (Seddah et al 2013) of-fers one of the first opportunities to evaluate this sce-nario within the framework of dependency syntax.In this paper, we discuss the systems we submit-ted to the SPMRL 2013 shared task.
We focusedour participation on the French dependency parsingtrack using the predicted morphology scenario, be-cause it is the only data set that massively containsMWEs.
Our best system ranked first on that track(for all training set sizes).
It is a reparsing systemthat makes use of predicted parses obtained bothwith pipeline and joint architectures.
We applied itto the French data set only, as we focused on MWEanalysis for dependency parsing.
Section 2 gives itsgeneral description, section 3 describes the handlingof MWEs.
We detail the underlying parsers in sec-tion 4 and their combination in section 5.
Experi-ments are described and discussed in sections 6 and7.2 System OverviewOur whole system is made of several single statisti-cal dependency parsing systems whose outputs arecombined into a reparser.
We use two types of sin-46gle parsing architecture: (a) pipeline systems; (b)?joint?
systems.The pipeline systems first perform MWE analy-sis before parsing.
The MWE analyzer (section 3)merges recognized MWEs into single tokens andthe parser is then applied on the sentences with thisnew tokenization.
The parsing model is learned ona gold training set where all marked MWEs havebeen merged into single tokens.
For evaluation, themerged MWEs appearing in the resulting parses areexpanded, so that the tokens are exactly the same ingold and predicted parses.The ?joint?
systems directly output dependencytrees whose structure comply with the Frenchdataset annotation scheme.
As shown in Figure 1,such trees contain not only syntactic dependencies,but also the grouping of tokens into MWEs, since thefirst component of an MWE bears dependencies tothe subsequent components of the MWE with a spe-cific label dep_cpd.
At that stage, the only missinginformation is the POS of the MWEs, which we pre-dict by applying a MWE tagger in a post-processingstep.la caisse d?
e?pargne avait ferme?
la veillesujdetdepcpddep cpdauxtpsmoddepcpdFigure 1: French dependency tree for La caissed?e?pargne avait ferme?
la veille (The savings bank hadclosed the day before), containing two MWEs (in red).3 MWE Analyzer and MWE TaggerThe MWE analyzer we used in the pipeline sys-tems is based on Conditional Random Fields (CRF)(Lafferty et al 2001) and on external lexicons fol-lowing (Constant and Tellier, 2012).
Given a tok-enized text, it jointly performs MWE segmentationand POS tagging (of simple tokens and of MWEs),both tasks mutually helping each other1.
CRF is aprominent statistical model for sequence segmenta-1Note though that we keep only the MWE segmentation, anduse rather the Morfette tagger-lemmatizer, cf.
section 4.tion and labelling.
External lexicons used as sourcesof features greatly improve POS tagging (Denisand Sagot, 2009) and MWE segmentation (Constantand Tellier, 2012).
Our lexical resources are com-posed of two large-coverage general-language lexi-cons: the Lefff2 lexicon (Sagot, 2010), which con-tains approx.
half a million inflected word forms,among which approx.
25, 000 are MWEs; and theDELA3 (Courtois, 2009; Courtois et al 1997) lex-icon, which contains approx.
one million inflectedforms, among which about 110, 000 are MWEs.These resources are completed with specific lexi-cons freely available in the platform Unitex4: thetoponym dictionary Prolex (Piton et al 1999) and adictionary of first names.The MWE tagger we used in the joint systemstakes as input a MWE within a dependency tree, andoutputs its POS.
It is a pointwise classifier, basedon a MaxEnt model that integrates different featurescapturing the MWE local syntactic context, and inparticular the POS at the token level (and not atthe MWE level).
The features comprise: the MWEform, its lemma, the sequence of POS of its compo-nents, the POS of its first component, its governor?sPOS in the syntactic parse, the POS following theMWE, the POS preceding the MWE, the bigram ofthe POS following and preceding the MWE.4 Dependency ParsersFor our development, we trained 3 types of parsers,both for the pipeline and the joint architecture:?
MALT, a pure linear-complexity transition-based parser (Nivre et al 2006)?
Mate-tools 1, the graph-based parser availablein Mate-tools5 (Bohnet, 2010)?
Mate-tools 2, the joint POS tagger andtransition-based parser with graph-based com-pletion available in Mate-tools (Bohnet andNivre, 2012).2We use the version available in the POS tagger MElt (Denisand Sagot, 2009).3We use the version in the platform Unitex (http://igm.univ-mlv.fr/?unitex).
We had to convert the DELA POS tagset to theFTB one.4http://igm.univ-mlv.fr/?unitex5Available at http://code.google.com/p/mate-tools/.
Weused the Anna3.3 version.47Such parsers require some preprocessing of theinput text: lemmatization, POS tagging, morphol-ogy analyzer (except the joint POS tagger andtransition-based parser that does not require prepro-cessed POS tagging).
We competed for the scenarioin which this information is not gold but predicted.Instead of using the predicted POS, lemma and mor-phological features provided by the shared task orga-nizers, we decided to retrain the tagger-lemmatizerMorfette (Chrupa?a et al 2008; Seddah et al 2010),in order to apply a jackknifing on the training set, sothat parsers are made less sensitive to tagging errors.Note that no feature pertaining to MWEs are used atthis stage.5 ReparserThe reparser is an adaptation to labeled dependencyparsing of the simplest6 system proposed in (Sagaeand Lavie, 2006).
The principle is to build an arc-factored merge of the parses produced by n inputparsers, and then to find the maximum spanningtree among the resulting merged graph7.
We im-plemented the maximum spanning tree algorithm8of (Eisner, 1996) devoted to projective dependencyparsing.
During the parse merging, each arc is unla-beled, and is given a weight, which is the frequencyit appears in the n input parses.
Once the maxi-mum spanning tree is found, each arc is labeled byits most voted label among the m input parses con-taining such an arc (with arbitrary choice in case ofties).6 Experiments6.1 SettingsMWE Analysis and TaggingFor the MWE analyzer, we used the tool lgtag-ger9 (version 1.1) with its default set of feature tem-6The other more complex systems were producing equiva-lent scores.7In order to account for labeled MWE recognition, we in-tegrated in the ?dep cpd?
arcs the POS of the correspondingMWE.
For instance, if the label ?dep cpd?
corresponds to an arcin a multiword preposition (P), the arc is relabeled ?dep cpd P?.At evaluation time, the output parse labels are remapped to theofficial annotation scheme.8More precisely, we based our implementation on thepseudo-code given in (McDonald, 2006).9http://igm.univ-mlv.fr/?mconstanplates.
The MWE tagger model was trained usingthe Wapiti software(Lavergne et al 2010).
We usedthe default parameters and we forced the MaxEntmode.ParsersFor MALT (version 1.7.2), we used the arceageralgorithm, and the liblinear library for training.
Asfar as the features are concerned, we started withthe feature templates given in Bonsai10 (Candito etal., 2010), and we added some templates (essentiallylemma bigrams) during the development tests, thatslightly improved performance.
For the two Mate-tools parsers, we used the default feature sets andparameters proposed in the documentation.Morphological predictionPredicted lemmas, POS and morphology featuresare computed with Morfette version 0.3.5 (Chrupa?aet al 2008; Seddah et al 2010)11, using 10 iter-ations for the tagging perceptron, 3 iterations forthe lemmatization perceptron, default beam size forthe decoding of the joint prediction, and the Lefff(Sagot, 2010) as external lexicon used for out-of-vocabulary words.
We performed a jackknifing onthe training corpus, with 10 folds for the full corpus,and 20 folds for the 5k track12.6.2 ResultsWe first provide the results on the development cor-pus.
Table 1 shows the general parsing accuracy ofour different systems.
Results are displayed in threedifferent groups corresponding to each kind of sys-tems: the two single parser architectures ones (jointand pipeline) and the reparsing one.
Each systemwas tested both when learned on the full trainingdata set and on the 5k one.
The joint and pipelinesystems were evaluated with the three parsers de-scribed in section 4.
For the reparser, we tested dif-ferent combinations of parsers in the full trainingdata set mode.
We found that the best combinationincludes all parsers but MALT in joint mode.
We didnot tune our reparsing system in the 5k training dataset mode.
We assumed that the best combination inthis mode was the same as with full training.10http://alpage.inria.fr/statgram/frdep/fr stat dep parsing.html11Available at https://sites.google.com/site/morfetteweb/12Note that for the 5k track, we retrained Morfette using the5k training corpus only, whereas the official 5k training set con-tains predicted morphology trained on the full training set.48full 5ktype parser LAS UAS LaS LAS UAS LaSJointMALT 80.91 84.74 89.18 78.61 83.16 87.51Mate-tools 1 84.60 88.21 91.43 82.02 86.23 90.02Mate-tools 2 84.40 88.08 91.02 81.66 85.97 89.38PipelineMALT 82.56 86.22 90.22 80.79 84.71 89.19Mate-tools 1 85.28 88.73 91.85 83.23 86.97 90.67Mate-tools 2 84.82 88.31 91.45 82.79 86.56 90.26Reparserjoint only 85.28 88.77 91.70 - - -pipeline only 85.79 89.17 91.94 - - -all 86.12 89.36 92.22 - - -best ensemble 86.23 89.55 92.21 84.25 87.88 91.17Table 1: Parsing results on development corpus (38820 tokens)COMP MWE MWE+POSR P F R P F R P Fjoint Mate-tools 1 76.3 82.4 79.2 74.3 80.6 77.3 70.7 76.7 73.6pipeline Mate-tools 1 80.8 82.7 81.7 79.0 83.6 81.2 75.6 80.1 77.8best reparser 81.1 82.5 81.8 79.2 83.0 81.0 76.1 79.8 77.9Table 2: MWE Results on the development corpus (2119 MWEs) with full training.Table 2 contains the MWE results on the devel-opment data set with full training, for three systems:the best single-parser joint and pipeline systems (i.e.with Mate-tools 1) and the best reparser.
We do notprovide results for the 5k training because they showsimilar trends.
We provide the 9 MWE-related mea-sures defined in the shared task.
The symbols R, Pand F respectively correspond to recall, precisionand F-measure.
COMP corresponds to evaluationof the non-head MWE components (i.e.
the non-firstMWE components, cf.
Figure 1).
MWE correspondsto the recognition of a complete MWE.
MWE+POSstands for the recognition of a complete MWE asso-ciated with its correct POS.We submitted to the shared task our best(reparser) system according to the tuning describedabove.
We also sent the two best pipeline systems(Mate-tools 1 and Mate-tools 2) and the best jointsystem (Mate-tools 1), in order to compare our sin-gle systems to the other competitors.
The official re-sults of our systems are provided in table 3 for gen-eral parsing and in table 4 for MWE recognition.
Wealso show the ranking of each of these systems in thecompetition.7 DiscussionIn table 3, we can note that for the 5k training setscenario, there is a general drop of parsing perfor-mance (approximately 2 points), but the trends areexactly the same as for the full training set sce-nario.
Concerning the performance on MWE analy-sis (table 4), the pipeline Mate-tools-1 system veryslightly outperforms the best reparser system in the5k scenario, contrary to the full training set scenario,but the difference is not significant.
In the following,we focus on the full training set scenario.Let us first discuss the overall parsing perfor-mance, by looking at the results on the develop-ment corpus (table 1).
As far as the single-parsersystems are concerned, we can note that for boththe joint and pipeline systems, MALT achieveslower performance than the graph-based (Mate-tools-1) and the joint tagger-parser (Mate-tools-2),which have comparable performance.
Moreover,the pipeline systems achieve overall better than theirjoint counterpart, though the increase between jointand pipeline architecture is much bigger for MALTthan for the Mate parsers (for MALT, compare49training type parser LAS UAS LaS RankFullReparser best 85.86 89.19 92.20 1Pipeline Mate-tools 1 84.91 88.35 91.73 3Pipeline Mate-tools 2 84.87 88.40 91.51 4Joint Mate-tools 1 84.14 87.67 91.24 75kReparser best 83.60 87.40 90.76 1Pipeline Mate-tools 1 82.53 86.51 90.14 4Pipeline Mate-tools 2 82.15 86.18 89.79 6Joint Mate-tools 1 81.63 85.76 89.56 7Table 3: Official parsing results on the evaluation corpus (75216 tokens)training type parser COMP MWE MWE+POS RankFullReparser best ensemble 81.3 80.7 77.5 1Pipeline Mate-tools 1 81.2 80.8 77.4 2Pipeline Mate-tools 2 81.2 80.8 76.6 3Joint Mate-tools 1 79.6 77.4 74.1 65kPipeline Mate-tools 1 78.7 77.7 74.0 1Reparser best ensemble 78.9 77.2 73.8 2Pipeline Mate-tools 2 78.7 77.7 73.3 5Joint Mate-tools 1 75.9 72.2 75.9 10Table 4: Official MWE results on the evaluation corpus (4043 MWEs).
The scores correspond to the F-measure.LAS=80.91 for the joint system, and LAS=82.56for the pipeline architecture, while for Mate-tools-1, compare LAS=84.60 with LAS=85.28).
The bestreparser system provides a performance increase ofapproximately one point over the best single-parsersystem (Mate-tools-1), both for LAS and UAS,which suggests that the parsers have complementarystrengths.When looking at performance on MWE recog-nition and tagging (2), we can note greater varia-tion between the F-measures obtained by the single-parser systems, but this is due to the much lowernumber of MWEs with respect to the number oftokens (there are 38820 tokens and 2119 MWEsin the dev set).
The MWE analyzer used in thepipeline systems leads to better MWE recognition(F ?
measure = 81.2 on dev set) than when theanalysis is left to the bare ?joint?
parsers (joint Mate-tools 1 achieves F-measure= 77.3).Contrary to the situation for overall parsing per-formance, the reparser system does not lead to betterMWE recognition with respect to the MWE analyzerof the pipeline systems.
Indeed the performance onMWEs are quite similar between the reparser sys-tem and the MWE analyzer (for the MWE metric,on the dev set we get F=81.0 versus 81.2 for bestreparser and pipeline systems respectively, whereaswe get 80.7 and 80.8 on the test set.
These differ-ences are not significant).
This is because the MWEspredicted by the MWE analyzer are present in threeof the single-parser systems taken into account in thereparsing process, and are thus much favored in thevoting.In order to understand better our parsing systems?performance on MWE recognition, we provide in ta-ble 5 the MWE+POS results broken down by MWEpart-of-speech, for the dev set.
Not surprisingly,we can note that performance varies greatly de-pending on the POS, with better performance onclosed classes (conjunctions, determiners, preposi-tions, pronouns) than on open classes.
The lowestperformance is on adjectives and verbs, but given theraw numbers of gold MWEs, the major impact onoverall performance is given by the results on nom-inal MWEs (either common or proper nouns).
A lit-tle less than one third of the nominal gold MWEs50R P F Nb gold Nb predicted Nb correctadjectives 46.9 75.0 57.7 32 20 15adverbs 74.7 83.0 78.7 360 324 269conjunctions 90.1 83.7 86.8 91 98 82clitics - 0.00 - 0 1 0determiners 96.0 96.8 96.4 252 250 242nouns 72.7 76.2 74.4 973 928 707prepositions 84.6 84.9 84.8 345 344 292pronouns 75.0 87.5 80.8 28 24 21verbs 66.7 66.7 66.67 33 33 22unknown 0 0 0 5 0 0ALL 77.9 81.6 79.7 2119 2022 1650Table 5: MWE+POS results on the development corpus, broken down by POS (recall, precision, F-measure, numberof gold MWEs, predicted MWEs, correct MWEs with such POS.is not recognized (R = 72.7), and about one quar-ter of the predicted nominal MWEs are wrong (P =76.2).
Though these results can be partly explainedby some inconsistencies in MWE annotation in theFrench Treebank (Constant et al 2012), there re-mains room for improvement for open class MWErecognition.8 ConclusionWe have described the LIGM-Alpage system for theSPMRL 2013 shared task, restricted to the Frenchtrack.
We provide the best results for the realisticscenario of predicting both MWEs and dependencysyntax, using a reparsing architecture that combinesseveral parsers, both pipeline (MWE recognitionfollowed by parsing) and joint (MWE recognitionperformed by the parser).
In the future, we plan tointegrate features specific to MWEs into the jointsystem, so that the reparser outperforms both thejoint and pipeline systems, not only on parsing (asit is currently the case) but also on MWE recogni-tion.ReferencesA.
Arun and F. Keller.
2005.
Lexicalization in crosslin-guistic probabilistic parsing: The case of french.
InProceedings of the Annual Meeting of the AssociationFor Computational Linguistics (ACL?05), pages 306?313.T.
Baldwin and K.S.
Nam.
2010.
Multiword expressions.In Handbook of Natural Language Processing, SecondEdition.
CRC Press, Taylor and Francis Group.Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Proceed-ings of the 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning, pages 1455?1465,Jeju Island, Korea, July.
Association for Computa-tional Linguistics.Bernd Bohnet.
2010.
Top accuracy and fast dependencyparsing is not a contradiction.
In Proceedings of the23rd International Conference on Computational Lin-guistics (COLING?10), Beijing, China.C.
Cafferkey, D. Hogan, and J. van Genabith.
2007.Multi-word units in treebank-based probabilistic pars-ing and generation.
In Proceedings of the 10th Inter-national Conference on Recent Advances in NaturalLanguage Processing (RANLP?07).M.-H. Candito, J. Nivre, P. Denis, and E. Henestroza An-guiano.
2010.
Benchmarking of statistical depen-dency parsers for french.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics (COLING?10), Beijing, China.Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-abith.
2008.
Learning morphology with morfette.In In Proc.
of LREC 2008, Marrakech, Morocco.ELDA/ELRA.Matthieu Constant and Isabelle Tellier.
2012.
Evaluat-ing the impact of external lexical resources into a crf-based multiword segmenter and part-of-speech tagger.In Proceedings of the 8th conference on Language Re-sources and Evaluation (LREC?12).Matthieu Constant, Anthony Sigogne, and Patrick Wa-trin.
2012.
Discriminative strategies to integrate mul-51tiword expression recognition and parsing.
In Pro-ceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics: Long Papers - Volume1, ACL ?12, pages 204?212, Stroudsburg, PA, USA.Association for Computational Linguistics.B.
Courtois, M. Garrigues, G. Gross, M. Gross,R.
Jung, M. Mathieu-Colas, A. Monceaux, A. Poncet-Montange, M. Silberztein, and R. Vive?s.
1997.
Dic-tionnaire e?lectronique DELAC : les mots compose?sbinaires.
Technical Report 56, University Paris 7,LADL.B.
Courtois.
2009.
Un syste`me de dictionnairese?lectroniques pour les mots simples du franc?ais.Langue Franc?aise, 87:11?22.P.
Denis and B. Sagot.
2009.
Coupling an annotated cor-pus and a morphosyntactic lexicon for state-of-the-artPOS tagging with less human effort.
In Proceedingsof the 23rd Pacific Asia Conference on Language, In-formation and Computation (PACLIC?09), pages 110?119.Jason M. Eisner.
1996.
Three new probabilistic mod-els for dependency parsing: an exploration.
In Pro-ceedings of the 16th conference on Computational lin-guistics - Volume 1, COLING ?96, pages 340?345,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.G.
Eryigit, T. Ilbay, and O. Arkan Can.
2011.
Multiwordexpressions in statistical dependency parsing.
In Pro-ceedings of the IWPT Workshop on Statistical Pars-ing of Morphologically-Rich Languages (SPRML?11),pages 45?55.S.
Green, M.-C. de Marneffe, J. Bauer, and C. D. Man-ning.
2011.
Multiword expression identification withtree substitution grammars: A parsing tour de forcewith french.
In Proceedings of the conference onEmpirical Method for Natural Language Processing(EMNLP?11), pages 725?735.Spence Green, Marie-Catherine de Marneffe, andChristopher D Manning.
2013.
Parsing models foridentifying multiword expressions.
ComputationalLinguistics, 39(1):195?227.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedings ofthe Eighteenth International Conference on MachineLearning (ICML?01), pages 282?289.Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.2010.
Practical very large scale CRFs.
In Proceed-ings the 48th Annual Meeting of the Association forComputational Linguistics (ACL?10), pages 504?513.Ryan McDonald.
2006.
Discriminative Training andSpanning Tree Algorithms for Dependency Parsing.Ph.D.
thesis, University of Pennsylvania.J.
Nivre and J. Nilsson.
2004.
Multiword units in syn-tactic parsing.
In Proceedings of Methodologies andEvaluation of Multiword Units in Real-World Applica-tions (MEMURA).J.
Nivre, J.
Hall, and J. Nilsson.
2006.
Maltparser: Adata-driven parser-generator for dependency parsing.In Proceedings of the fifth international conferenceon Language Resources and Evaluation (LREC?06),pages 2216?2219, Genoa, Italy.Santanu Pal, Tanmoy Chkraborty, and Sivaji Bandy-opadhyay.
2011.
Handling multiword expressionsin phrase-based statistical machine translation.
InProceedings of the Machine Translation Summit XIII,pages 215?224.O.
Piton, D. Maurel, and C. Belleil.
1999.
The prolexdata base : Toponyms and gentiles for nlp.
In Proceed-ings of the Third International Workshop on Applica-tions of Natural Language to Data Bases (NLDB?99),pages 233?237.Kenji Sagae and Alon Lavie.
2006.
Parser combinationby reparsing.
In Proceedings of the Human LanguageTechnology Conference of the NAACL, CompanionVolume: Short Papers, NAACL-Short ?06, pages 129?132, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.B.
Sagot.
2010.
The lefff, a freely available, accurateand large-coverage lexicon for french.
In Proceedingsof the 7th International Conference on Language Re-sources and Evaluation (LREC?10).Djame?
Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,Josef van Genabith, and Marie Candito.
2010.Lemmatization and statistical lexicalized parsing ofmorphologically-rich languages.
In Proc.
of theNAACL/HLT Workshop on Statistical Parsing of Mor-phologically Rich Languages (SPMRL 2010), Los An-geles, CA.Djame?
Seddah, Reut Tsarfaty, Sandra K?
?ubler, MarieCandito, Jinho Choi, Richa?rd Farkas, Jennifer Fos-ter, Iakes Goenaga, Koldo Gojenola, Yoav Gold-berg, Spence Green, Nizar Habash, Marco Kuhlmann,Wolfgang Maier, Joakim Nivre, Adam Przepi-orkowski, Ryan Roth, Wolfgang Seeker, YannickVersley, Veronika Vincze, Marcin Wolin?ski, AlinaWro?blewska, and Eric Villemonte de la Cle?rgerie.2013.
Overview of the spmrl 2013 shared task: Across-framework evaluation of parsing morphologi-cally rich languages.
In Proceedings of the 4th Work-shop on Statistical Parsing of Morphologically RichLanguages: Shared Task, Seattle, WA.52
