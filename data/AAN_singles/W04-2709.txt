Interlingual Annotation of Multilingual Text CorporaStephen HelmreichDavid FarwellComputing Research LaboratoryNew Mexico State Universitydavid@crl.nmsu.edushelmrei@crl.nmsu.eduFlorence ReederKeith MillerInformation Discovery & UnderstandingMITRE Corporationfreeder@mitre.orgkeith@mitre.orgBonnie DorrNizar HabashInstitute for Advanced Computer StudiesUniversity of Marylandbonnie@umiacs.umd.eduhabash@umiacs.umd.eduEduard HovyInformation Sciences InstituteUniversity of Southern Californiahovy@isi.eduLori LevinTeruko MitamuraLanguage Technologies InstituteCarnegie Mellon Universitylsl@cs.cmu.eduteruko@cs.cmu.eduOwen RambowAdvaith SiddharthanDepartment of Computer ScienceColumbia Universityrambow@cs.columbia.eduas372@cs.columbia.eduAbstractThis paper describes a multi-site project toannotate six sizable bilingual parallel corporafor interlingual content.
After presenting thebackground and objectives of the effort, wedescribe the data set that is being annotated,the interlingua representation language used,an interface environment that supports the an-notation task and the annotation process itself.We will then present a preliminary version ofour evaluation methodology and concludewith a summary of the current status of theproject along with a number of issues whichhave arisen.1 IntroductionThis paper describes a multi-site National ScienceFoundation project focusing on the annotation of sixsizable bilingual parallel corpora for interlingual contentwith the goal of providing a significant data set for im-proving knowledge-based approaches to machine trans-lation (MT) and a range of other Natural LanguageProcessing (NLP) applications.
The project participantsinclude the Computing Research Laboratory at NMSU,the Language Technologies Institute at CMU, the In-formation Science Institute at USC, UMIACS at theUniversity of Maryland, the MITRE Corporation andColumbia University.
In the remainder of the paper, wefirst present the background and objectives of the pro-ject.
We then describe the data set that is being anno-tated, the interlingual representation language beingused, an interface environment that is designed to sup-port the annotation task, and the process of annotationitself.
We will then outline a preliminary version of ourevaluation methodology and conclude with a summaryof the current status of the project along with a set ofissues that have arisen since the project began.2 Project Goals and Expected OutcomesThe central goals of the project are:?
to produce a practical, commonly-shared systemfor representing the information conveyed by atext, or ?interlingua?,?
to develop a methodology for accurately andconsistently assigning such representations totexts across languages and across annotators,?
to annotate a sizable multilingual parallel corpusof source language texts and translations for ILcontent.This corpus is expected to serve as a basis for improvingmeaning-based approaches to MT and a range of othernatural language technologies.
The tools and annotationstandards will serve to facilitate more rapid annotationof texts in the future.3 CorpusThe target data set is modeled on and an extension ofthe DARPA MT Evaluation data set (White andO?Connell 1994) and includes data from the LinguisticData Consortium (LDC) Multiple Translation Arabic,Part 1 (Walker et al, 2003).
The data set consists of 6bilingual parallel corpora.
Each corpus is made up of125 source language news articles along with threetranslations into English, each produced independentlyby different human translators.
However, the sourcenews articles for each individual language corpus aredifferent from the source articles in the other languagecorpora.
Thus, the 6 corpora themselves are comparableto each other rather than parallel.
The source languagesare Japanese, Korean, Hindi, Arabic, French and Span-ish.
Typically, each article is between 300 and 400words long (or the equivalent) and thus each corpus hasbetween 150,00 and 200,000 words.
Consequently, thesize of the entire data set is around 1,000,000 words.Thus, for any given corpus, the annotation effort isto assign interlingual content to a set of 4 parallel texts,3 of which are in the same language, English, and all ofwhich theoretically communicate the same information.The following is an example set from the Spanish cor-pus:S: Atribuy?
esto en gran parte auna pol?tica que durante muchos a?ostuvo un "sesgo concentrador" y repre-sent?
desventajas para las clases me-nos favorecidas.T1: He attributed this in greatpart to a type of politics thatthroughout many years possessed a"concentrated bias" and representeddisadvantages for the less favoredclasses.T2: To a large extent, he attrib-uted that fact to a policy which hadfor many years had a "bias towardconcentration" and represented disad-vantages for the less favoredclasses.T3: He attributed this in greatpart to a policy that had a "centristslant" for many years and representeddisadvantages for the less-favoredclasses.The annotation process involves identifying thevariations between the translations and then assessingwhether these differences are significant.
In this case,the translations are, for the most part, the same althoughthere are a few interesting variations.For instance, where this appears as the translationof esto in the first and third translations, that factappears in the second.
The translator choice potentiallyrepresents an elaboration of the semantic content of thesource expression and the question arises as to whetherthe annotation of the variation in expressions should bedifferent or the same.More striking perhaps is the variation betweenconcentrated bias, bias toward concen-tration and centrist slant as the translationfor sesgo concentrador.
Here, the third transla-tion offers a clear interpretation of the source text au-thor?s intent.
The first two attempt to carry over thevagueness of the source expression assuming that thetarget text reader will be able to figure it out.
But evenhere, the two translators appear to differ as to what thesource language text author?s intent actually was, theformer referring to bias of a certain degree of strengthand the second to a bias  in a certain direction.
Seem-ingly, then, the annotation of each of these expressionsshould differ.Furthermore, each source language has differentmethods of encoding meaning linguistically.
The resul-tant differing types of translation mismatch with Englishshould provide insight into the appropriate structure andcontent for an interlingual representation.The point is that a multilingual parallel data set ofsource language texts and English translations offers aunique perspective and unique problem for annotatingtexts for meaning.4 InterlinguaDue to the complexity of an interlingual annotation asindicated by the differences described in the previoussection, the representation has developed through threelevels and incorporates knowledge from sources such asthe Omega ontology and theta grids.
Since this is anevolving standard, the three levels will be presented inorder as building on one another.
Then the additionaldata components will be described.4.1 Three Levels of RepresentationWe now describe three levels of representation, referredto as IL0, IL1 and IL2.
The aim is to perform the annota-tion process incrementally, with each level of represen-tation incorporating additional semantic features andremoving existing syntactic ones.
IL2 is intended as theinterlingua, that abstracts away from (most) syntacticidiosyncrasies of the source language.
IL0 and IL1 areintermediate representations that are useful startingpoints for annotating at the next level.4.1.1 IL0IL0 is a deep syntactic dependency representation.
Itincludes part-of-speech tags for words and a parse treethat makes explicit the syntactic predicate-argumentstructure of verbs.
The parse tree is labeled with syntac-tic categories such as Subject or Object , which refer todeep-syntactic grammatical function (normalized forvoice alternations).
IL0 does not contain function words(determiners, auxiliaries, and the like): their contribu-tion is represented as features.
Furthermore, semanti-cally void punctuation has been removed.
While thisrepresentation is purely syntactic, many disambiguationdecisions, relative clause and PP attachment for exam-ple, have been made, and the presentation abstracts asmuch as possible from surface-syntactic phenomena.Thus, our IL0 is intermediate between the analytical andtectogrammatical levels of the Prague School (Haji?
etal 2001).
IL0 is constructed by hand-correcting the out-put of a dependency parser (details in section 6) and is auseful starting point for semantic annotation at  IL1,since it allows annotators to see how textual units relatesyntactically when making semantic judgments.4.1.2 IL1IL1 is an intermediate semantic representation.
It asso-ciates semantic concepts with lexical units like nouns,adjectives,  adverbs and verbs (details of the ontology insection 4.2).
It also replaces the syntactic relations inIL0, like subject and object, with thematic roles, likeagent, theme and goal (details in section 4.3).
Thus, likePropBank (Kingsbury et al2002), IL1 neutralizes dif-ferent alternations for argument realization.
However,IL1 is not an interlingua; it does not normalize over alllinguistic realizations of the same semantics.
In particu-lar, it does not address how the meanings of individuallexical units combine to form the meaning of a phrase orclause.
It also does not address idioms, metaphors andother non-literal uses of language.
Further, IL1 does notassign semantic features to prepositions; these continueto be encoded as syntactic heads of their phrases, al-though these might have been annotated with thematicroles such as location or time.4.1.3 IL2IL2 is intended to be an interlingua, a representation ofmeaning that is reasonably independent of language.
IL2is intended to capture similarities in meaning acrosslanguages and across different lexical/syntactic realiza-tions within a language.
For example, IL2 is expected tonormalize over conversives (e.g.
X bought a book fromY vs. Y sold a book to X)  (as does FrameNet (Baker etal 1998)) and non-literal language usage (e.g.
X startedits business vs. X opened its doors to customers).
Theexact definition of IL2 will be the major research con-tribution of this project.4.2 The Omega OntologyIn progressing from IL0 to IL1, annotators have to se-lect semantic terms (concepts) to represent the nouns,verbs, adjectives, and adverbs present in each sentence.These terms are represented in the 110,000-node ontol-ogy Omega (Philpot et al, 2003), under construction atISI.
Omega has been built semi-automatically from avariety of sources, including Princeton's WordNet (Fell-baum, 1998), NMSU?s Mikrokosmos (Mahesh and Ni-renburg, 1995), ISI's Upper Model (Bateman et al,1989) and ISI's SENSUS (Knight and Luk, 1994).
Afterthe uppermost region of Omega was created by hand,these various resources?
contents were incorporated and,to some extent, reconciled.
After that, several millioninstances of people, locations, and other facts wereadded (Fleischman et al, 2003).
The ontology, whichhas been used in several projects in recent years (Hovyet al, 2001), can be browsed using the DINO browser athttp://blombos.isi.edu:8000/dino; this browser forms apart of the annotation environment.
Omega remainsunder continued development and extension.4.3 The Theta GridsEach verb in Omega is assigned one or more theta gridsspecifying the arguments associated with a verb andtheir theta roles (or thematic role).
Theta roles are ab-stractions of deep semantic relations that generalizeover verb classes.
They are by far the most commonapproach in the field to represent predicate-argumentstructure.
However, there are numerous variations withlittle agreement even on terminology (Fillmore, 1968;Stowell, 1981; Jackendoff, 1972; Levin and Rappaport-Hovav, 1998).The theta grids used in our project were extractedfrom the Lexical Conceptual Structure Verb Database(LVD) (Dorr, 2001).
The WordNet senses assigned toeach entry in the LVD were then used to link the thetagrids to the verbs in the Omega ontology.
In addition tothe theta roles, the theta grids specify the mapping be-tween theta roles and their syntactic realization in argu-ments, such as Subject, Object or Prepositional Phrase,and the Obligatory/Optional nature of the argument,thus facilitating IL1 annotation.
For example, one of thetheta grids for the verb ?load?
is listed in Table 1 (at theend of the paper).Although based on research in LCS-based MT(Dorr, 1993; Habash et al 2002), the set of theta rolesused has been simplified for this project.
This list (seeTable 2 at the end of the paper), was used in the Inter-lingua Annotation Experiment 2002 (Habash andDorr).14.4 Incremental AnnotationAs described earlier, the development and annota-tion of the interlingual notation is incremental in nature.This necessitates constraining the types and categoriesof attributes included in the annotation during the be-ginning phases.
Other topics not addressed here, butconsidered for future work include time, aspect, loca-tion, modality, type of reference, types of speech act,causality, etc.Thus, IL2 itself is not a final interlingual representa-tion, but one step along the way.
IL0 and IL1 are alsointermediate representations, and as such are an occa-sionally awkward mixture of syntactic and semanticinformation.
The decisions as to what to annotate, whatto normalize, what to represent as features at each levelare semantically and syntactically principled, but alsogoverned by expectations about reasonable annotatortasks.
What is important is that at each stage of trans-formation, no information is lost, and the original lan-guage recoverable in principle from the representation.5 Annotation ToolWe have assembled a suite of tools to be used in theannotation process.
Some of these tools are previouslyexisting resources that were gathered for use in the pro-ject, and others have been developed specifically withthe annotation goals of this project in mind.
Since weare gathering our corpora from disparate sources, weneed to standardize the text before presenting it toautomated procedures.
For English, this involves sen-tence boundary detection, but for other languages, itmay involve segmentation, chunking of text, or other?text ecology?
operations.
The text is then processedwith a dependency parser, the output of which is viewedand corrected in TrED (Haji?, et al, 2001), a graphi-cally-based tree editing program, written in Perl/Tk2.The revised deep dependency structure produced by thisprocess is the IL0 representation for that sentence.In order to derive IL1 from the IL0 representation,annotators use Tiamat, a tool developed specifically for1 Other contributors to this list are Dan Gildea and KarinKipper Schuler.2 http://quest.ms.mff.cuni.cz/pdt/Tools/Tree_Editors/Tred/this project.
This tool enables viewing of the IL0 treewith easy reference to all of the IL resources describedin section 4 (the current IL representation, the ontology,and the theta grids).
This tool provides the ability toannotate text via simple point-and-click selections ofwords, concepts, and theta-roles.
The IL0 is displayedin the top left pane, ontological concepts and their asso-ciated theta grids, if applicable, are located in the topright, and the sentence itself is located in the bottomright pane.
An annotator may select a lexical item (leafnode) to be annotated in the sentence view; this word ishighlighted, and the relevant portion of the Omega on-tology is displayed in the pane on the left.
In addition,if this word has dependents, they are automatically un-derlined in red in the sentence view.
Annotators canview all information pertinent to the process of decidingon appropriate ontological concepts in this view.
Fol-lowing the procedures described in section 6, selectionof concepts, theta grids and roles appropriate to thatlexical item can then be made in the appropriate panes.Evaluation of the annotators?
output would be daunt-ing based solely on a visual inspection of the annotatedIL1 files.
Thus, a tool was also developed to comparethe output and to generate the evaluation measures thatare described in section 7.
The reports generated by theevaluation tool allow the researchers to look at bothgross-level phenomena, such as inter-annotator agree-ment, and at more detailed points of interest, such aslexical items on which agreement was particularly low,possibly indicating gaps or other inconsistencies in theontology being used.6 Annotation TaskTo describe the annotation task, we first present theannotation process and tools used with it as well as theannotation manuals.
Finally, setup issues relating tonegotiating multi-site annotations are discussed.6.1 Annotation processThe annotation process was identical for each text.
Forthe initial testing period, only English texts were anno-tated, and the process described here is for English text.The process for non-English texts will be, mutatis mu-tandis, the same.Each sentence of the text is parsed into a depend-ency tree structure.
For English texts, these trees werefirst provided by the Connexor parser at UMIACS(Tapanainen and Jarvinen, 1997), and then corrected byone of the team PIs.
For the initial testing period, anno-tators were not permitted to alter these structures.
Al-ready at this stage, some of the lexical items arereplaced by features (e.g., tense), morphological formsare replaced by features on the citation form, and certainconstructions are regularized (e.g., passive) and emptyarguments inserted.
It is this dependency structure thatis loaded into the annotation tool and which each anno-tator then marks up.The annotator was instructed to annotate all nouns,verbs, adjectives, and adverbs.
This involves annotatingeach word twice ?
once with a concept from WordnetSYNSET and once with a Mikrokosmos concept; thesetwo units of information are merged, or at least inter-twined in Omega.
One of the goals and results of thisannotation process will be a simultaneous coding ofconcepts in both ontologies, facilitating a closer unionbetween them.In addition, users were instructed to provide a se-mantic case role for each dependent of a verb.
In manycases this was ?NONE?
since adverbs and conjunctionswere dependents of verbs in the dependency tree.
LCSverbs were identified with Wordnet classes and the LCScase frames supplied where possible.
The user, how-ever, was often required to determine the set of roles oralter them to suit the text.
In both cases, the revised ornew set of case roles was noted and sent to a guru forevaluation and possible permanent inclusion.
Thus theset of event concepts in the ontology supplied with roleswill grow through the course of the project.6.2 The annotation manualsMarkup instructions are contained in three manuals: ausers guide for Tiamat (including procedural instruc-tions), a definitional guide to semantic roles, and amanual for creating a dependency structure (IL0).
To-gether these manuals allow the annotator to (1) under-stand the intention behind aspects of the dependencystructure; (2) how to use Tiamat to mark up texts; and(3) how to determine appropriate semantic roles andontological concepts.
In choosing a set of appropriateontological concepts, annotators were encouraged tolook at the name of the concept and its definition, thename and definition of the parent node, example sen-tences, lexical synonyms attached to the same node, andsub- and super-classes of the node.
All these manualsare available on the IAMTC website3.6.3 The multi-site set upFor the initial testing phase of the project, all annotatorsat all sites worked on the same texts.
Two texts wereprovided by each site as were two translations of thesame source language (non-English) text.
To test for theeffects of coding two texts that are semantically close,since they are both translations of the same sourcedocument, the order in which the texts were annotateddiffered from site to site, with half the sites marking onetranslation first, and the other half of the sites markingthe second translation first.
Another variant tested was3 http://sparky.umiacs.umd.edu:8000/IAMTC/annotation_manual.wiki?cmd=get&anchor=Annotation+Manualto interleave the two translations, so that two similarsentences were coded consecutively.During the later production phase, a more complexschedule will be followed, making sure that many textsare annotated by two annotators, often from differentsites, and that regularly all annotators will mark thesame text.
This will help ensure continued inter-coderreliability.In the period leading up to the initial test phase,weekly conversations were held at each site by the an-notators, going over the texts coded.
This was followedby a weekly conference call among all the annotators.During the test phase, no discussion was permitted.One of the issues that arose in discussion was howcertain constructions should be displayed and whethereach word should have a separate node or whether cer-tain words should be combined into a single node.
Inview of the fact that the goal was not to tag individualwords, but entities and relations, in many cases wordswere combined into single nodes to facilitate this proc-ess.
For instance, verb-particle constructions were com-bined into a single node.
In a sentence like ?He threw itup?, ?throw?
and ?up?
were combined into a singlenode ?throw up?
since one action is described by thecombined words.
Similarly, proper nouns, compoundnouns and copular constructions required specializedhandling.
In addition, issues arose about whether an-notators should change dependency trees; and in in-structing the annotators on how best to determine anappropriate ontology node.7 EvaluationThe evaluation criteria and metrics continue to evolveand are in the early stages of formation and implementa-tion.
Several possible courses for evaluating the annota-tions and resulting structures exist.
In the first of these,the annotations are measured according to inter-annotator agreement.
For this purpose, data is collectedreflecting the annotations selected, the Omega nodesselected and the theta roles assigned.
Then, inter-coderagreement is measured by a straightforward match, withagreement calculated by a Kappa measure (Carletta,1993) and a Wood standard similarity (Habash andDorr, 2002).
This is done for three agreement points:annotations, Omega selection and theta role selection.At this time, the Kappa statistic?s expected agreement isdefined as 1/(N+1) where N is the number of choices ata given data point.
In the case of Omega nodes, thismeans the number of matched Omega nodes (by stringmatch) plus one for the possibility of the annotator trav-ersing up or down the hierarchy.
Multiple measures areused because it is important to have a mechanism forevaluating inter-coder consistency in the use of the ILrepresentation language which does not depend on theassumption that there is a single correct annotation of agiven text.
The tools for evaluation have been modifiedfrom pervious use (Habash and Dorr, 2002).Second, the accuracy of the annotation is measured.Here accuracy is defined as correspondence to a prede-fined baseline.
In the initial development phase, allsites annotated the same texts and many of the varia-tions were discussed at that time, permitting the devel-opment of a baseline annotation.
While not a usefullong-term strategy, this produced a consensus baselinefor the purpose of measuring the annotators?
task andthe solidity of the annotation standard.The final measurement technique derives from theultimate goal of using the IL representation for MT,therefore, we are measuring the ability to generate accu-rate surface texts from the IL representation as anno-tated.
At this stage, we are using an available generator,Halogen (Knight and Langkilde, 2000).
A tool to con-vert the representation to meet Halogen requirements isbeing built.
Following the conversion, surface formswill be generated and then compared with the originalsthrough a variety of standard MT metrics (ISLE, 2003).8 Accomplishments and IssuesIn a short amount of time, we have identified languagesand collected corpora with translations.
We have se-lected representation elements, from parser outputs toontologies, and have developed an understanding ofhow their component elements fit together.
A coremarkup vocabulary (e.g., entity-types, event-types andparticipant relations) was selected.
An initial version ofthe annotator?s toolkit (Tiamat) has been developed andhas gone through alpha testing.
The multi-layered ap-proach to annotation  decided upon reduces the burdenon the annotators for any given text as annotations buildupon one another.
In addition to developing individualtools, an infrastructure exists for carrying out a multi-site annotation project.In the coming months we will be fleshing out thecurrent procedures for evaluating the accuracy of anannotation and measuring inter-coder consistency.From this, a multi-site evaluation will be produced andresults reported.
Regression testing, from the interme-diate stages and representations will be able to be car-ried out.
Finally, a growing corpus of annotated textswill become available.In addition to the issues discussed throughout thepaper, a few others have not yet been identified.
From acontent standpoint, looking at IL systems for time andlocation should utilize work in personal name, temporaland spatial annotation (e.g., Ferro et al, 2001).
Also, anideal IL representation would also account for causality,co-reference, aspectual content, modality, speech acts,etc.
At the same time, while incorporating these items,vagueness and redundancy must be eliminated from theannotation language.
Many inter-event relations wouldneed to be captured such as entity reference, time refer-ence, place reference, causal relationships, associativerelationships, etc.
Finally, to incorporate these, cross-sentence phenomena remain a challenge.From an MT perspective, issues include evaluatingthe consistency in the use of an annotation languagegiven that any source text can result in multiple, differ-ent, legitimate translations (see Farwell and Helmreich,2003) for discussion of evaluation in this light.
Alongthese lines, there is the problem of annotating texts fortranslation without including in the annotations infer-ences from the source text.9 ConclusionsThis is a radically different annotation project fromthose that have focused on morphology, syntax or evencertain types of semantic content (e.g., for word sensedisambiguation competitions).
It is most similar toPropBank (Kingsbury et al2002) and FrameNet (Bakeret al1998).
However, it is novel in its emphasis on:  (1)a more abstract level of mark-up (interpretation); (2) theassignment of a well-defined meaning representation toconcrete texts; and (3) issues of a community-wide con-sistent and accurate annotation of meaning.By providing an essential, and heretofore non-existent, data set for training and evaluating natural lan-guage processing systems, the resultant annotated multi-lingual corpus of translations is expected to lead tosignificant research and development opportunities forMachine Translation and a host of other Natural Lan-guage Processing technologies including Question-Answering and Information Extraction.ReferencesBaker, C., J. Fillmore and J B. Lowe, 1998.
The Berke-ley FrameNet Project.
Proceedings of ACL.Bateman, J.A., R. Kasper, J. Moore, and R. Whitney.1989.
A General Organization of Knowledge forNatural Language Processing: The Penman UpperModel.
Unpublished research report, USC / Informa-tion Sciences Institute, Marina del Rey, CA.Carletta, J. C. 1996.
Assessing agreement on classifica-tion tasks: the kappa statistic.
Computational Lin-guistics, 22(2), 249-254Conceptual Structures and Documentation, UMCP.http://www.umiacs.umd.edu/~bonnie/LCS_Database_Documentation.htmlDorr, B. J.
2001.
LCS Verb Database, Online SoftwareDatabase of LexicalDorr, B. J., 1993.
Machine Translation: A View from theLexicon, MIT Press, Cambridge, MA.Farwell, D., and S. Helmreich.
2003.
Pragmatics-basedTranslation and MT Evaluation.
In Proceedings ofTowards Systematizing MT Evaluation.
MT-SummitWorkshop, New Orleans, LA.Fellbaum, C.
(ed.).
1998.
WordNet: An On-line LexicalDatabase and Some of its Applications.
MIT Press,Cambridge, MA.Ferro, L., I. Mani, B. Sundheim and G. Wilson.
2001.TIDES Temporal Annotation Guidelines.
Version1.0.2 MITRE Technical Report, MTR 01W0000041Fillmore, C..  1968.
The Case for Case.
In E. Bach andR.
Harms, editors, Universals in Linguistic Theory,pages 1--88.
Holt, Rinehart, and Winston.Fleischman, M., A. Echihabi, and E.H. Hovy.
2003.Offline Strategies for Online Question Answering:Answering Questions Before They Are Asked.
Pro-ceedings of the ACL Conference.
Sapporo, Japan.Habash, N. and B. Dorr.
2002.
Interlingua AnnotationExperiment Results.
AMTA-2002 Interlingua Reli-ability Workshop.
Tiburon, California, USA.Habash, N., B. J. Dorr, and D. Traum, 2002.
"EfficientLanguage Independent Generation from LexicalConceptual Structures," Machine Translation, 17:4.Haji?, J.; B.
Vidov?-Hladk?
; P. Pajas.
2001: The Pra-gue Dependency Treebank: Annotation Structure andSupport.
In Proceeding of the IRCS Workshop onLinguistic Databases, pp.
.
University of Pennsyl-vania, Philadelphia, USA, pp.
105-114.Hovy, E., A. Philpot, J. Ambite, Y. Arens, J. Klavans,W.
Bourne, and D. Saroz.
2001.
Data Acquisitionand Integration in the DGRC's Energy Data Collec-tion Project, in Proceedings of the NSF's dg.o 2001.Los Angeles, CA.ISLE 2003.
Framework for Evaluation of MachineTranslation in ISLE.http://www.issco.unige.ch/projects/isle/femti/Jackendoff, R. 1972.
Grammatical Relations and Func-tional Structure.
Semantic Interpretation in Genera-tive Grammar.
The MIT Press, Cambridge, MA.Kingsbury, P and M Palmer and M Marcus , 2002.Adding Semantic Annotation to the Penn TreeBank.Proceedings of the Human Language TechnologyConference (HLT 2002).Knight, K., and I. Langkilde.
2000.
Preserving Ambi-guities in Generation via Automata Intersection.American Association for Artificial Intelligence con-ference (AAAI).Knight, K, and S. K. Luk.
1994.
Building a Large-ScaleKnowledge Base for Machine Translation.
Proceed-ings of AAAI.
Seattle, WA.Levin, B. and M. Rappaport-Hovav.
1998.
From LexicalSemantics to Argument Realization.
Borer, H.
(ed.
)Handbook of Morphosyntax and Argument Structure.Dordrecht: Kluwer Academic Publishers.Mahesh, K., and Nirenberg, S.  1995.
A Situated Ontol-ogy for Practical NLP, in Proceedings on the Work-shop on Basic Ontological Issues in KnowledgeSharing at IJCAI-95.
Montreal, Canada.Philpot, A., M. Fleischman, E.H. Hovy.
2003.
Semi-Automatic Construction of a General Purpose Ontol-ogy.
Proceedings of the International Lisp Confer-ence.
New York, NY.
Invited.Stowell, T. 1981.
Origins of Phrase Structure.
PhD the-sis, MIT, Cambridge, MA.Tapanainen, P. and T Jarvinen.
1997.
A non-projectivedependency parser.
In the 5th Conference on AppliedNatural Language Processing / Association for Com-putational Linguistics, Washington, DC.White, J., and T. O?Connell.
1994.
The ARPA MTevaluation methodologies: evolution, lessons, and fu-ture approaches.
Proceedings of the 1994 Confer-ence, Association for Machine Translation in theAmericasWalker, K., M. Bamba, D. Miller, X. Ma, C. Cieri, andG.
Doddington 2003.
Multiple-Translation ArabicCorpus, Part 1.
Linguistic Data Consortium (LDC)catalog num.
LDC2003T18 & ISBN 1-58563-276-7.Role Description Grid Syntax TypeAgent The entity that does the action Agent:  loadTheme  with possessedSUBJ OBLIGATORYTheme The entity that is worked on Agent:  loadTheme with possessedOBJ OBLIGATORYPossessed The entity controlled or owned Agent:  loadTheme  with possessedPP OPTIONALTable 1 :  A theta grid for the verb "load"Role and Definition ExamplesAgent:  Agents have the features of volition, sentience, causation andindependent exist?
Henry pushed/broke the vase.Instrument: An instrument should have causation but no volition.
Itssentience and existence are not relevant.?
The Hammer broke the vase.?
She hit him with a baseball batExperiencer: An experiencer has no causation but is sentient andexists independently.
Typically an experiencer is the subject of verbslike feel, hear, see, sense, smell, notice, detect, etc.?
John heard the vase shatter.?
John shivered.Theme: The theme is typically causally affected or experiences amovement and/or change in state.
The theme can appear as the infor-mation in verbs like acquire, learn, memorize, read, study, etc.
It canalso be a thing, event or state (clausal complement).?
John went to school.?
John broke the vase.?
John memorized his lines.?
She buttered the bread with marga-rine.Perceived: Refers to a perceived entity that isn't required by the verbbut further characterizes the situation.
The perceived is neither caus-ally affected nor causative.
It doesn't experience a movement orchange in state.
Its volition and sentience are irrelevant.
Its existenceis independent of an experiencer.?
He saw the play.?
He looked into the room.?
The cat's fur feels good to John.?
She imagined the movie to be loud.Predicate: Indicates new modifying information about other thematicroles.?
We considered him a fool.?
She acted happy.Source: Indicates where/when the theme started in its motion, orwhat its original state was, or where its original (possibly abstract)location/time was.?
John left the house.Goal: Indicates where the theme ends up in its motion, or what itsfinal state is, or where/when its final (possibly abstract) location/timeis.
It also can indicate the thing/event resulting from the verb's occur-rence (the result).?
John ran home.?
John ran to the store.?
John gave a book to Mary.?
John gave Mary a book.Location: Indicates static locations---as opposed to a source or goal,i.e., the (possibly abstract) location of the theme or event.?
He lived in France.?
The water fills the box.?
This cabin sleeps five peopleTime Indicates time.
?
John sleeps for five hours.?
Mary ate during the meeting.Beneficiary: Indicates the thing that receives the benefit/result of theevent/state.?
John baked the cake for Mary.?
John baked Mary a cake.?
An accident happened to him.Purpose: Indicates the purpose/reason behind an event/state ?
He studied for the exam.?
He searched for rabbits.Possessed: Indicates the possessed entity in verbs such as own, have,possess, fit, buy, and carry.?
John has five bucks.?
He loaded the cart with hay.?
He bought it for five dollarsProposition: Indicates the secondary event/state ?
He wanted to study for the exam.Modifier: Indicates a property of a thing such as color, taste, size,etc.?
The red book sitting on the table isold.Null Indicates no thematic contribution.
Typical examples are imper-sonal it and there.?
It was raining all morning in Miami.TABLE 2:  List of Theta Roles
