Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1055?1065,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsIntegrating surprisal and uncertain-input models in online sentencecomprehension: formal techniques and empirical resultsRoger LevyDepartment of LinguisticsUniversity of California at San Diego9500 Gilman Drive # 0108La Jolla, CA 92093-0108rlevy@ucsd.eduAbstractA system making optimal use of available in-formation in incremental language compre-hension might be expected to use linguisticknowledge together with current input to re-vise beliefs about previous input.
Under somecircumstances, such an error-correction capa-bility might induce comprehenders to adoptgrammatical analyses that are inconsistentwith the true input.
Here we present a for-mal model of how such input-unfaithful gar-den paths may be adopted and the difficultyincurred by their subsequent disconfirmation,combining a rational noisy-channel model ofsyntactic comprehension under uncertain in-put with the surprisal theory of incrementalprocessing difficulty.
We also present a behav-ioral experiment confirming the key empiricalpredictions of the theory.1 IntroductionIn most formal theories of human sentence compre-hension, input recognition and syntactic analysis aretaken to be distinct processes, with the only feed-back from syntax to recognition being prospectiveprediction of likely upcoming input (Jurafsky, 1996;Narayanan and Jurafsky, 1998, 2002; Hale, 2001,2006; Levy, 2008a).
Yet a system making optimaluse of all available information might be expectedto perform fully joint inference on sentence identityand structure given perceptual input, using linguisticknowledge both prospectively and retrospectively indrawing inferences as to how raw input should besegmented and recognized as a sequence of linguis-tic tokens, and about the degree to which each inputtoken should be trusted during grammatical analysis.Formal models of such joint inference over uncer-tain input have been proposed (Levy, 2008b), andcorroborative empirical evidence exists that strongcoherence of current input with a perceptual neigh-bor of previous input may induce confusion in com-prehenders as to the identity of that previous input(Connine et al, 1991; Levy et al, 2009).In this paper we explore a more dramatic predic-tion of such an uncertain-input theory: that, whenfaced with sufficiently biasing input, comprehen-ders might under some circumstances adopt a gram-matical analysis inconsistent with the true raw in-put comprising a sentence they are presented with,but consistent with a slightly perturbed version ofthe input that has higher prior probability.
If this isthe case, then subsequent input strongly disconfirm-ing this ?hallucinated?
garden-path analysis mightbe expected to induce the same effects as seen inclassic cases of garden-path disambiguation tradi-tionally studied in the psycholinguistic literature.We explore this prediction by extending the ratio-nal uncertain-input model of Levy (2008b), integrat-ing it with SURPRISAL THEORY (Hale, 2001; Levy,2008a), which successfully accounts for and quan-tifies traditional garden-path disambiguation effects;and by testing predictions of the extended model in aself-paced reading study.
Section 2 reviews surprisaltheory and how it accounts for traditional garden-path effects.
Section 3 provides background infor-mation on garden-path effects relevant to the currentstudy, describes how we might hope to reveal com-prehenders?
use of grammatical knowledge to revisebeliefs about the identity of previous linguistic sur-1055face input and adopt grammatical analyses incon-sistent with true input through a controlled experi-ment, and informally outlines how such belief revi-sions might arise as a side effect in a general the-ory of rational comprehension under uncertain in-put.
Section 4 defines and estimates parameters for amodel instantiating the general theory, and describesthe predictions of the model for the experiment de-scribed in Section 3 (along with the inference proce-dures required to determine those predictions).
Sec-tion 5 reports the results of the experiment.
Section 6concludes.2 Garden-path disambiguation undersurprisalThe SURPRISAL THEORY of incremental sentence-processing difficulty (Hale, 2001; Levy, 2008a)posits that the cognitive effort required to process agiven word wi of a sentence in its context is given bythe simple information-theoretic measure of the logof the inverse of the word?s conditional probability(also called its ?surprisal?
or ?Shannon informationcontent?)
in its intra-sentential context w1,...,i?1 andextra-sentential context Ctxt:Effort(wi) ?
log1P (wi|w1...i?1, Ctxt)(In the rest of this paper, we consider isolated-sentence comprehension and ignore Ctxt.)
The the-ory derives empirical support not only from con-trolled experiments manipulating grammatical con-text but also from broad-coverage studies of read-ing times for naturalistic text (Demberg and Keller,2008; Boston et al, 2008; Frank, 2009; Roark et al,2009), including demonstration that the shape of therelationship between word probability and readingtime is indeed log-linear (Smith and Levy, 2008).Surprisal has had considerable success in ac-counting for one of the best-known phenomena inpsycholinguistics, the GARDEN-PATH SENTENCE(Frazier, 1979), in which a local ambiguity biasesthe comprehender?s incremental syntactic interpre-tation so strongly that upon encountering disam-biguating input the correct interpretation can onlybe recovered with great effort, if at all.
The mostfamous example is (1) below (Bever, 1970):(1) The horse raced past the barn fell.where the context before the final word is stronglybiased toward an interpretation where raced is themain verb of the sentence (MV; Figure 1a), the in-tended interpretation, where raced begins a reducedrelative clause (RR; Figure 1b) and fell is the mainverb, is extremely difficult to recover.
Letting Tjrange over the possible incremental syntactic analy-ses of words w1...6 preceding fell, under surprisal theconditional probability of the disambiguating con-tinuation fell can be approximated asP (fell|w1...6) =?jP (fell|Tj , w1...6)P (Tj |w1...6)(I)For all possible predisambiguation analyses Tj ,either the analysis is disfavored by the context(P (Tj |w1...6) is low) or the analysis makes thedisambiguating word unlikely (P (fell|Tj , w1...6) islow).
Since every summand in the marginalizationof Equation (I) has a very small term in it, the totalmarginal probability is thus small and the surprisalis high.
Hale (2001) demonstrated that surprisal thuspredicts strong garden-pathing effects in the classicsentence The horse raced past the barn fell on ba-sis of the overall rarity of reduced relative clausesalone.
More generally, Jurafsky (1996) used a com-bination of syntactic probabilities (reduced RCs arerare) and argument-structure probabilities (raced isusually intransitive) to estimate the probability ratioof the two analyses of pre-disambiguation contextin Figure 1 as roughly 82:1, putting a lower boundon the additional surprisal incurred at fell for thereduced-RC variant over the unreduced variant (Thehorse that was raced past the barn fell) of 6.4 bits.13 Garden-pathing and input uncertaintyWe now move on to cases where garden-pathing canapparently be blocked by only small changes to thesurface input, which we will take as a starting pointfor developing an integrated theory of uncertain-input inference and surprisal.
The backdrop is whatis known in the psycholinguistic literature as theNP/Z ambiguity, exemplified in (2) below:1We say that this is a ?lower bound?
because incorporat-ing even finer-grained information?such as the fact that horseis a canonical subject for intransitive raced?into the estimatewould almost certainly push the probability ratio even farther infavor of the main-clause analysis.1056SNPDTTheNNhorseVPVBDracedPPINpastNPDTtheNNbarn...(a) MV interpretationSNPDTTheNNhorseRRCSVPVBNracedPPINpastNPDTtheNNbarnVP...(b) RR interpretationFigure 1: Classic garden pathing(2) While Mary was mending the socks fell off her lap.In incremental comprehension, the phrase the socksis ambiguous between being the NP object of thepreceding subordinate-clause verb mending versusbeing the subject of the main clause (in whichcase mending has a Zero object); in sentences like(2) the initial bias is toward the NP interpreta-tion.
The main-clause verb fell disambiguates, rul-ing out the initially favored NP analysis.
It hasbeen known since Frazier and Rayner (1982) thatthis effect of garden-path disambiguation can bemeasured in reading times on the main-clause verb(see also Mitchell, 1987; Ferreira and Henderson,1993; Adams et al, 1998; Sturt et al, 1999; Hilland Murray, 2000; Christianson et al, 2001; vanGompel and Pickering, 2001; Tabor and Hutchins,2004; Staub, 2007).
Small changes to the contextcan have huge effects on comprehenders?
initial in-terpretations, however.
It is unusual for sentence-initial subordinate clauses not to end with a commaor some other type of punctuation (searches in theparsed Brown corpus put the rate at about 18%); em-pirically it has consistently been found that a commaeliminates the garden-path effect in NP/Z sentences:(3) While Mary was mending, the socks fell off her lap.Understanding sentences like (3) is intuitively mucheasier, and reading times at the disambiguating verbare reliably lower when compared with (2).
Fodor(2002) summarized the power of this effect suc-cinctly:[w]ith a comma after mending, therewould be no syntactic garden path left tobe studied.
(Fodor, 2002)In a surprisal model with clean, veridical input,Fodor?s conclusion is exactly what is predicted: sep-arating a verb from its direct object with a commaeffectively never happens in edited, published writ-ten English, so the conditional probability of theNP analysis should be close to zero.2 When uncer-tainty about surface input is introduced, however?due to visual noise, imperfect memory representa-tions, and/or beliefs about possible speaker error?analyses come into play in which some parts of thetrue string are treated as if they were absent.
Inparticular, because the two sentences are perceptualneighbors, the pre-disambiguation garden-path anal-ysis of (2) may be entertained in (3).We can get a tighter handle on the effect of inputuncertainty by extending Levy (2008b)?s analysis ofthe expected beliefs of a comprehender about the se-quence of words constituting an input sentence tojoint inference over both sentence identity and sen-tence structure.
For a true sentence w?
which yieldsperceptual input I , joint inference on sentence iden-tity w and structure T marginalizing over I yields:PC(T,w|w?)
=?IPC(T,w|I,w?
)PT (I|w?)
dIwhere PT (I|w?)
is the true model of noise (percep-tual inputs derived from the true sentence) and PC(?
)terms reflect the comprehender?s linguistic knowl-edge and beliefs about the noise processes interven-ing between intended sentences and perceptual in-put.
w?
and w must be conditionally independentgiven I since w?
is not observed by the comprehen-der, giving us (through Bayes?
Rule):P (T,w|w?)
=?IPC(I|T,w)PC(T,w)PC(I)PT (I|w?)
dIFor present purposes we constrain the comprehen-der?s model of noise so that T and I are condition-ally independent given w, an assumption that can berelaxed in future work.3 This allows us the further2A handful of VP -> V , NP ... rules can be foundin the Penn Treebank, but they all involve appositives (It [VPran, this apocalyptic beast .
.
.
]), vocatives (You should [VP un-derstand, Jack, .
.
.
]), cognate objects (She [VP smiled, a smilewithout humor]), or indirect speech (I [VP thought, you nastybrute.
.
.
]); none involve true direct objects of the type in (3).3This assumption is effectively saying that noise processesare syntax-insensitive, which is clearly sensible for environmen-tal noise but would need to be relaxed for some types of speakererror.1057simplification toP (T,w|w?)
=(i)?
??
?PC(T,w)(ii)?
??
?
?IPC(I|w)PT (I|w?
)PC(I)dI(II)That is, a comprehender?s average inferences aboutsentence identity and structure involve a tradeoffbetween (i) the prior probability of a grammati-cal derivation given a speaker?s linguistic knowl-edge and (ii) the fidelity of the derivation?s yield tothe true sentence, as measured by a combination oftrue noise processes and the comprehender?s beliefsabout those processes.3.1 Inducing hallucinated garden pathsthrough manipulating prior grammaticalprobabilitiesReturning to our discussion of the NP/Z ambigu-ity, the relative ease of comprehending (3) entailsan interpretation in the uncertain-input model thatthe cost of infidelity to surface input is sufficient toprevent comprehenders from deriving strong beliefin a hallucinated garden-path analysis of (3) pre-disambiguation in which the comma is ignored.
Atthe same time, the uncertain-input theory predictsthat if we manipulate the balance of prior grammat-ical probabilities PC(T,w) strongly enough (term(i) in Equation (II)), it may shift the comprehender?sbeliefs toward a garden-path interpretation.
This ob-servation sets the stage for our experimental manip-ulation, illustrated below:(4) As the soldiers marched, toward the tank lurched aninjured enemy combatant.Example (4) is qualitatively similar to (3), but withtwo crucial differences.
First, there has been LOCA-TIVE INVERSION (Bolinger, 1971; Bresnan, 1994)in the main clause: a locative PP has been frontedbefore the verb, and the subject NP is realizedpostverbally.
Locative inversion is a low-frequencyconstruction, hence it is crucially disfavored bythe comprehender?s prior over possible grammaticalstructures.
Second, the subordinate-clause verb isno longer transitive, as in (3); instead it is intran-sitive but could itself take the main-clause frontedPP as a dependent.
Taken together, these prop-erties should shift comprehenders?
posterior infer-ences given prior grammatical knowledge and pre-disambiguation input more sharply than in (3) to-ward the input-unfaithful interpretation in which theimmediately preverbal main-clause constituent (to-ward the tank in (4)) is interpreted as a dependent ofthe subordinate-clause verb, as if the comma wereabsent.If comprehenders do indeed seriously entertainsuch interpretations, then we should be able tofind the empirical hallmarks (e.g., elevated readingtimes) of garden-path disambiguation at the main-clause verb lurched, which is incompatible with the?hallucinated?
garden-path interpretation.
Empiri-cally, however, it is important to disentangle theseempirical hallmarks of garden-path disambiguationfrom more general disruption that may be inducedby encountering locative inversion itself.
We ad-dress this issue by introducing a control conditionin which a postverbal PP is placed within the subor-dinate clause:(5) As the soldiers marched into the bunker, toward thetank lurched an injured enemy combatant.
[+PP]Crucially, this PP fills a similar thematic rolefor the subordinate-clause verb marched as themain-clause fronted PP would, reducing the ex-tent to which the comprehender?s prior favors theinput-unfaithful interpretation (that is, the prior ra-tio P (marched into the bunker toward the tank|VP)P (marched into the bunker|VP) for (5) ismuch lower than the corresponding prior ratioP (marched toward the tank|VP)P (marched|VP) for (4)), while leavinglocative inversion present.
Finally, to ensure thatsentence length itself does not create a confounddriving any observed processing-time difference, wecross presence/absence of the subordinate-clause PPwith inversion in the main clause:(6)a.
As the soldiers marched, the tank lurched towardan injured enemy combatant.
[Uninverted,?PP]b.
As the soldiers marched into the bunker, thetank lurched toward an injured enemy combatant.
[Uninverted,+PP]4 Model instantiation and predictionsTo determine the predictions of our uncertain-input/surprisal model for the above sentence types,we extracted a small grammar from the parsed1058TOP ?
S .
1.000000S ?
INVERTED NP 0.003257S ?
SBAR S 0.012289S ?
SBAR , S 0.041753S ?
NP VP 0.942701INVERTED ?
PP VBD 1.000000SBAR ?
INSBAR S 1.000000VP ?
VBD RB 0.002149VP ?
VBD PP 0.202024VP ?
VBD NP 0.393660VP ?
VBD PP PP 0.028029VP ?
VBD RP 0.005731VP ?
VBD 0.222441VP ?
VBD JJ 0.145966PP ?
IN NP 1.000000NP ?
DT NN 0.274566NP ?
NNS 0.047505NP ?
NNP 0.101198NP ?
DT NNS 0.045082NP ?
PRP 0.412192NP ?
NN 0.119456Table 1: A small PCFG (lexical rewrite rules omit-ted) covering the constructions used in (4)?
(6), withprobabilities estimated from the parsed Brown cor-pus.Brown corpus (Kuc?era and Francis, 1967; Marcuset al, 1994), covering sentence-initial subordinateclause and locative-inversion constructions.4,5 Thenon-terminal rewrite rules are shown in Table 1,along with their probabilities; of terminal rewriterules for all words which either appear in the sen-tences to be parsed or appeared at least five times inthe corpus, with probabilities estimated by relativefrequency.As we describe in the following two sections, un-4Rule counts were obtained using tgrep2/Tregex pat-terns (Rohde, 2005; Levy and Andrew, 2006); the probabilitiesgiven are relative frequency estimates.
The patterns used can befound at http://idiom.ucsd.edu/?rlevy/papers/acl2011/tregex_patterns.txt.5Similar to the case noted in Footnote 2, a small number ofVP -> V , PP ... rules can be found in the parsed Browncorpus.
However, the PPs involved are overwhelmingly (i) setexpressions, such as for example, in essence, and of course, or(ii) manner or temporal adjuncts.
The handful of true loca-tive PPs (5 in total) are all parentheticals intervening betweenthe verb and a complement strongly selected by the verb (e.g.,[VP means, in my country, homosexual]); none fulfill one of theverb?s thematic requirements.certain input is represented as a weighted finite-stateautomaton (WFSA), allowing us to represent the in-cremental inferences of the comprehender throughintersection of the input WFSA with the PCFGabove (Bar-Hillel et al, 1964; Nederhof and Satta,2003, 2008).4.1 Uncertain-input representationsLevy (2008a) introduced the LEVENSHTEIN-DISTANCE KERNEL as a model of the average effectof noise in uncertain-input probabilistic sentencecomprehension; this corresponds to term (ii) inour Equation (II).
This kernel had a single noiseparameter governing scaling of the cost of consid-ering word substitutions, insertions, and deletionsare considered, with the cost of a word substitutionfalling off exponentially with Levenshtein distancebetween the true word and the substituted word,and the cost of word insertion or deletion falling offexponentially with word length.
The distributionover the infinite set of strings w can be encodedin a weighted finite-state automaton, facilitatingefficient inference.We use the Levenshtein-distance kernel here tocapture the effects of perceptual noise, but make twomodifications necessary for incremental inferenceand for the correct computation of surprisal valuesfor new input: the distribution over already-seen in-put must be proper, and possible future inputs mustbe costless.
The resulting weighted finite-state rep-resentation of noisy input for a true sentence prefixw?
= w1...j is a j + 1-state automaton with arcs asfollows:?
For each i ?
1, .
.
.
, j:?
A substitution arc from i?1 to i with costproportional to exp[?LD(w?, wi) ?]
foreach word w?
in the lexicon, where ?
> 0is a noise parameter and LD(w?, wi) is theLevenshtein distance between w?
and wi(when w?
= wi there is no change to theword);?
A deletion arc from i?1 to i labeled ?
withcost proportional to exp[?len(wi)/?];?
An insertion loop arc from i ?
1to i ?
1 with cost proportional toexp[?len(w?)/?]
for every word w?
in thelexicon;?
A loop arc from j to j for each word w?
in1059?/0.063it/0.467hit/0.172him/0.063it/0.135hit/0.050him/0.050it/0.135hit/0.050him/0.050?/0.021it/0.158hit/0.428him/0.158it/1.000hit/1.000him/1.00010 2Figure 2: Noisy WFSA for partial input it hit.
.
.with lexicon {it,hit,him}, noise parameter ?=1the lexicon, with zero cost (value 1 in the realsemiring);?
State j is a zero-cost final state; no other statesare final.The addition of loop arcs at state n allows mod-eling of incremental comprehension through the au-tomaton/grammar intersection (see also Hale, 2006);and the fact that these arcs are costless ensures thatthe partition function of the intersection reflects onlythe grammatical prior plus the costs of input alreadyseen.
In order to ensure that the distribution overalready-seen input is proper, we normalize the costson outgoing arcs from all states but j.6 Figure 2gives an example of a simple WFSA representationfor a short partial input with a small lexicon.4.2 InferenceComputing the surprisal incurred by the disam-biguating element given an uncertain-input repre-sentation of the sentence involves a standard appli-cation of the definition of conditional probability(Hale, 2001):log1P (I1...i|I1...i?1)= logP (I1...i?1)P (I1...i)(III)Since our uncertain inputs I1...k are encoded by aWFSA, the probability P (I1...k) is equal to the par-tition function of the intersection of this WFSA withthe PCFG given in Table 1.7 PCFGs are a specialclass of weighted context-free grammars (WCFGs),6If a state?s total unnormalized cost of insertion arcs is ?
andthat of deletion and insertion arcs is ?, its normalizing constantis ?1??
.
Note that we must have ?
< 1, placing a constraint onthe value that ?
can take (above which the normalizing constantdiverges).7Using the WFSA representation of average noise effectshere actually involves one simplifying assumption, that the av-which are closed under intersection with WFSAs; aconstructive procedure exists for finding the inter-section (Bar-Hillel et al, 1964; Nederhof and Satta,2003).
Hence we are left with finding the partitionfunction of a WCFG, which cannot be computed ex-actly, but a number of approximation methods areknown (Stolcke, 1995; Smith and Johnson, 2007;Nederhof and Satta, 2008).
In practice, the com-putation required to compute the partition functionunder any of these methods increases with the sizeof the WCFG resulting from the intersection, whichfor a binarized PCFG with R rules and an n-stateWFSA is Rn2.
To increase efficiency we imple-mented what is to our knowledge a novel methodfor finding the minimal grammar including all rulesthat will have non-zero probability in the intersec-tion.
We first parse the WFSA bottom-up withthe item-based method of Goodman (1999) in theBoolean semiring, storing partial results in a chart.After completion of this bottom-up parse, every rulethat will have non-zero probability in the intersec-tion PCFG will be identifiable with a set of entriesin the chart, but not all entries in this chart willhave non-zero probability, since some are not con-nected to the root.
Hence we perform a second, top-down Boolean-semiring parsing pass on the bottom-up chart, throwing out entries that cannot be derivedfrom the root.
We can then include in the intersec-tion grammar only those rules from the classic con-struction that can be identified with a set of surviv-ing entries in the final parse chart.8 The partitionfunctions for each category in this intersection gram-mar can then be computed; we used a fixed-pointmethod preceded by a topological sort on the gram-mar?s ruleset, as described by Nederhof and Satta(2008).
To obtain the surprisal of the input deriv-ing from a word wi in its context, we can thus com-erage surprisal of Ii, or EPT[log 1PC(Ii|I1...i?1)], is well ap-proximated by the log of the ratio of the expected probabilitiesof the noisy inputs I1...i?1 and I1...i, since as discussed in Sec-tion 3 the quantities P (I1...i?1) and P (I1...i) are expectationsunder the true noise distribution.
This simplifying assumptionhas the advantage of bypassing commitment to a specific repre-sentation of perceptual input and should be justifiable for rea-sonable noise functions, but the issue is worth further scrutiny.8Note that a standard top-down algorithm such as Earleyparsing cannot be used to avoid the need for both bottom-upand top-down passes, since the presence of loops in the WFSAbreaks the ability to operate strictly left-to-right.10600.10 0.15 0.20 0.258.59.09.510.010.511.0Noise level ?
(high=noisy)Surprisalat main?clauseverb Inverted, +PPUninverted, +PPInverted, ?PPUninverted, ?PPFigure 3: Model predictions for (4)?
(6)pute the partition functions for noisy inputs I1...i?1and I1...i corresponding to words w1...i?1 and wordsw1...i respectively, and take the log of their ratio asin Equation (III).4.3 PredictionsThe noise level ?
is a free parameter in this model, sowe plot model predictions?the expected surprisalof input from the main-clause verb for each vari-ant of the target sentence in (4)?
(6)?over a widerange of its possible values (Figure 3).
The far left ofthe graph asymptotes toward the predictions of cleansurprisal, or noise-free input.
With little to no inputuncertainty, the presence of the comma rules out thegarden-path analysis of the fronted PP toward thetank, and the surprisal at the main-clause verb is thesame across condition (here reflecting only the un-certainty of verb identity for this small grammar).As input uncertainty increases, however, surprisalin the [Inverted, ?PP] condition increases, reflect-ing the stronger belief given preceding context in aninput-unfaithful interpretation.5 Empirical resultsTo test these predictions we conducted a word-by-word self-paced reading study, in which partici-pants read by pressing a button to reveal each suc-cessive word in a sentence; times between but-ton presses are recorded and analyzed as an in-dex of incremental processing difficulty (Mitchell,1984).
Forty monolingual native-English speakerparticipants read twenty-four sentence quadruplets(?items?)
on the pattern of (4)?
(6), with a Latin-square design so that each participant saw an equalInverted Uninverted-PP 0.76 0.93+PP 0.85 0.92Table 2: Question-answering accuracynumber of sentences in each condition and saw eachitem only once.
Experimental items were pseudo-randomly interspersed with 62 filler sentences; notwo experimental items were ever adjacent.
Punctu-ation was presented with the word to its left, so thatfor (4) the four and fifth button presses would yield--------------- marched, ---------------and------------------------ toward --------respectively (right-truncated here for reasons ofspace).
Every sentence was followed by a yes/nocomprehension question (e.g., Did the tank lurch to-ward an injured enemy combatant?
); participants re-ceived feedback whenever they answered a questionincorrectly.Reading-time results are shown in Figure 4.
Ascan be seen, the model?s predictions are matchedat the main-clause verb: reading times are highestin the [Inverted, ?PP] condition, and there is aninteraction between main-clause inversion and pres-ence of a subordinate-clause PP such that presenceof the latter reduces reading times more for invertedthan for uninverted main clauses.
This interactionis significant in both by-participants and by-itemsANOVAs (both p < 0.05) and in a linear mixed-effects analysis with participants- and item-specificrandom interactions (t > 2; see Baayen et al, 2008).The same pattern persists and remains significantthrough to the end of the sentence, indicating con-siderable processing disruption, and is also observedin question-answering accuracies for experimentalsentences, which are superadditively lowest in the[Inverted, ?PP] condition (Table 2).The inflated reading times for the [Inverted,?PP] condition beginning at the main-clauseverb confirm the predictions of the uncertain-input/surprisal theory.
Crucially, the input thatwould on our theory induce the comprehender toquestion the comma (the fronted main-clause PP)1061400500600700Readingtime(ms)As the soldiers marched(,)  into thebunker,toward the tank lurched toward an enemy combatant.Inverted, +PPUninverted, +PPInverted, ?PPUninverted, ?PPFigure 4: Average reading times for each part of thesentence, broken down by experimental conditionis not seen until after the comma is no longer visi-ble (and presumably has been integrated into beliefsabout syntactic analysis on veridical-input theories).This empirical result is hence difficult to accommo-date in accounts which do not share our theory?s cru-cial property that comprehenders can revise their be-lief in previous input on the basis of current input.6 ConclusionLanguage is redundant: the content of one part of asentence carries predictive value both for what willprecede and what will follow it.
For this reason, andbecause the path from a speaker?s intended utteranceto a comprehender?s perceived input is noisy anderror-prone, a comprehension system making opti-mal use of available information would use currentinput not only for forward prediction but also to as-sess the veracity of previously encountered input.Here we have developed a theory of how such anadaptive error-correcting capacity is a consequenceof noisy-channel inference, with a comprehender?sbeliefs regarding sentence form and structure at anymoment in incremental comprehension reflecting abalance between fidelity to perceptual input and apreference for structures with higher prior proba-bility.
As a consequence of this theory, certaintypes of sentence contexts will cause the drive to-ward higher prior-probability analyses to overcomethe drive to maintain fidelity to input, undermin-ing the comprehender?s belief in an earlier part ofthe input actually perceived in favor of an analy-sis unfaithful to part of the true input.
If subse-quent input strongly disconfirms this incorrect in-terpretation, we should see behavioral signatures ofclassic garden-path disambiguation.
Within the the-ory, the size of this ?hallucinated?
garden-path ef-fect is indexed by the surprisal value under uncer-tain input, marginalizing over the actual sentenceobserved.
Based on a model implementing the-ory we designed a controlled psycholinguistic ex-periment making specific predictions regarding therole of fine-grained grammatical context in modu-lating comprehenders?
strength of belief in a highlyspecific bit of linguistic input?a comma markingthe end of a sentence-initial subordinate clause?and tested those predictions in a self-paced read-ing experiment.
As predicted by the theory, read-ing times at the word disambiguating the ?halluci-nated?
garden-path were inflated relative to controlconditions.
These results contribute to the theory ofuncertain-input effects in online sentence process-ing by suggesting that comprehenders may be in-duced not only to entertain but to adopt relativelystrong beliefs in grammatical analyses that requiremodification of the surface input itself.
Our resultsalso bring a new degree of nuance to surprisal the-ory, demonstrating that perceptual neighbors of truepreceding input may need to be taken into accountin order to estimate how surprising a comprehenderwill find subsequent input to be.Beyond the domain of psycholinguistics, themethods employed here might also be usefully ap-plied to practical problems such as parsing of de-graded or fragmentary sentence input, allowing jointconstraint derived from grammar and available inputto fill in gaps (Lang, 1988).
Of course, practical ap-plications of this sort would raise challenges of theirown, such as extending the grammar to broader cov-erage, which is delicate here since the surface in-put places a weaker check on overgeneration fromthe grammar than in traditional probabilistic pars-ing.
Larger grammars also impose a technical bur-den since parsing uncertain input is in practice morecomputationally intensive than parsing clean input,raising the question of what approximate-inferencealgorithms might be well-suited to processing un-certain input with grammatical knowledge.
Answersto this question might in turn be of interest for sen-tence processing, since the exhaustive-parsing ideal-ization employed here is not psychologically plausi-ble.
It seems likely that human comprehension in-1062volves approximate inference with severely limitedmemory that is nonetheless highly optimized to re-cover something close to the intended meaning ofan utterance, even when the recovered meaning isnot completely faithful to the input itself.
Arriving atmodels that closely approximate this capacity wouldbe of both theoretical and practical value.AcknowledgmentsParts of this work have benefited from presentationat the 2009 Annual Meeting of the Linguistic Soci-ety of America and the 2009 CUNY Sentence Pro-cessing Conference.
I am grateful to Natalie Katzand Henry Lu for assistance in preparing materialsand collecting data for the self-paced reading exper-iment described here.
This work was supported by aUCSD Academic Senate grant, NSF CAREER grant0953870, and NIH grant 1R01HD065829-01.ReferencesAdams, B. C., Clifton, Jr., C., and Mitchell, D. C.(1998).
Lexical guidance in sentence processing?Psychonomic Bulletin & Review, 5(2):265?270.Baayen, R. H., Davidson, D. J., and Bates, D. M.(2008).
Mixed-effects modeling with crossed ran-dom effects for subjects and items.
Journal ofMemory and Language, 59(4):390?412.Bar-Hillel, Y., Perles, M., and Shamir, E. (1964).On formal properties of simple phrase structuregrammars.
In Language and Information: Se-lected Essays on their Theory and Application.Addison-Wesley.Bever, T. (1970).
The cognitive basis for linguisticstructures.
In Hayes, J., editor, Cognition and theDevelopment of Language, pages 279?362.
JohnWiley & Sons.Bolinger, D. (1971).
A further note on the nominalin the progressive.
Linguistic Inquiry, 2(4):584?586.Boston, M. F., Hale, J. T., Kliegl, R., Patil, U., andVasishth, S. (2008).
Parsing costs as predictors ofreading difficulty: An evaluation using the Pots-dam sentence corpus.
Journal of Eye MovementResearch, 2(1):1?12.Bresnan, J.
(1994).
Locative inversion and thearchitecture of universal grammar.
Language,70(1):72?131.Christianson, K., Hollingworth, A., Halliwell, J. F.,and Ferreira, F. (2001).
Thematic roles assignedalong the garden path linger.
Cognitive Psychol-ogy, 42:368?407.Connine, C. M., Blasko, D. G., and Hall, M. (1991).Effects of subsequent sentence context in audi-tory word recognition: Temporal and linguisticconstraints.
Journal of Memory and Language,30(2):234?250.Demberg, V. and Keller, F. (2008).
Data fromeye-tracking corpora as evidence for theoriesof syntactic processing complexity.
Cognition,109(2):193?210.Ferreira, F. and Henderson, J. M. (1993).
Readingprocesses during syntactic analysis and reanaly-sis.
Canadian Journal of Experimental Psychol-ogy, 16:555?568.Fodor, J. D. (2002).
Psycholinguistics cannot escapeprosody.
In Proceedings of the Speech ProsodyConference.Frank, S. L. (2009).
Surprisal-based comparison be-tween a symbolic and a connectionist model ofsentence processing.
In Proceedings of the 31stAnnual Conference of the Cognitive Science Soci-ety, pages 1139?1144.Frazier, L. (1979).
On Comprehending Sentences:Syntactic Parsing Strategies.
PhD thesis, Univer-sity of Massachusetts.Frazier, L. and Rayner, K. (1982).
Making andcorrecting errors during sentence comprehension:Eye movements in the analysis of structurallyambiguous sentences.
Cognitive Psychology,14:178?210.Goodman, J.
(1999).
Semiring parsing.
Computa-tional Linguistics, 25(4):573?605.Hale, J.
(2001).
A probabilistic Earley parser asa psycholinguistic model.
In Proceedings of theSecond Meeting of the North American Chapterof the Association for Computational Linguistics,pages 159?166.Hale, J.
(2006).
Uncertainty about the rest of thesentence.
Cognitive Science, 30(4):609?642.1063Hill, R. L. and Murray, W. S. (2000).
Commas andspaces: Effects of punctuation on eye movementsand sentence parsing.
In Kennedy, A., Radach,R., Heller, D., and Pynte, J., editors, Reading as aPerceptual Process.
Elsevier.Jurafsky, D. (1996).
A probabilistic model of lexicaland syntactic access and disambiguation.
Cogni-tive Science, 20(2):137?194.Kuc?era, H. and Francis, W. N. (1967).
Computa-tional Analysis of Present-day American English.Providence, RI: Brown University Press.Lang, B.
(1988).
Parsing incomplete sentences.
InProceedings of COLING.Levy, R. (2008a).
Expectation-based syntactic com-prehension.
Cognition, 106:1126?1177.Levy, R. (2008b).
A noisy-channel model of ratio-nal human sentence comprehension under uncer-tain input.
In Proceedings of the 13th Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 234?243.Levy, R. and Andrew, G. (2006).
Tregex and Tsur-geon: tools for querying and manipulating treedata structures.
In Proceedings of the 2006 con-ference on Language Resources and Evaluation.Levy, R., Bicknell, K., Slattery, T., and Rayner,K.
(2009).
Eye movement evidence that read-ers maintain and act on uncertainty about pastlinguistic input.
Proceedings of the NationalAcademy of Sciences, 106(50):21086?21090.Marcus, M. P., Santorini, B., and Marcinkiewicz,M.
A.
(1994).
Building a large annotated corpusof English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Mitchell, D. C. (1984).
An evaluation of subject-paced reading tasks and other methods for investi-gating immediate processes in reading.
In Kieras,D.
and Just, M. A., editors, New methods in read-ing comprehension.
Hillsdale, NJ: Earlbaum.Mitchell, D. C. (1987).
Lexical guidance in hu-man parsing: Locus and processing characteris-tics.
In Coltheart, M., editor, Attention and Per-formance XII: The psychology of reading.
Lon-don: Erlbaum.Narayanan, S. and Jurafsky, D. (1998).
Bayesianmodels of human sentence processing.
In Pro-ceedings of the Twelfth Annual Meeting of theCognitive Science Society.Narayanan, S. and Jurafsky, D. (2002).
A Bayesianmodel predicts human parse preference and read-ing time in sentence processing.
In Advancesin Neural Information Processing Systems, vol-ume 14, pages 59?65.Nederhof, M.-J.
and Satta, G. (2003).
Probabilis-tic parsing as intersection.
In Proceedings of theInternational Workshop on Parsing Technologies.Nederhof, M.-J.
and Satta, G. (2008).
Computingpartition functions of PCFGs.
Research on Logicand Computation, 6:139?162.Roark, B., Bachrach, A., Cardenas, C., and Pal-lier, C. (2009).
Deriving lexical and syntacticexpectation-based measures for psycholinguisticmodeling via incremental top-down parsing.
InProceedings of EMNLP.Rohde, D. (2005).
TGrep2 User Manual, version1.15 edition.Smith, N. A. and Johnson, M. (2007).
Weightedand probabilistic context-free grammars areequally expressive.
Computational Linguistics,33(4):477?491.Smith, N. J. and Levy, R. (2008).
Optimal process-ing times in reading: a formal model and empiri-cal investigation.
In Proceedings of the 30th An-nual Meeting of the Cognitive Science Society.Staub, A.
(2007).
The parser doesn?t ignore intransi-tivity, after all.
Journal of Experimental Psychol-ogy: Learning, Memory, & Cognition, 33(3):550?569.Stolcke, A.
(1995).
An efficient probabilisticcontext-free parsing algorithm that computes pre-fix probabilities.
Computational Linguistics,21(2):165?201.Sturt, P., Pickering, M. J., and Crocker, M. W.(1999).
Structural change and reanalysis difficultyin language comprehension.
Journal of Memoryand Language, 40:136?150.Tabor, W. and Hutchins, S. (2004).
Evidence forself-organized sentence processing: Digging ineffects.
Journal of Experimental Psychology:Learning, Memory, & Cognition, 30(2):431?450.1064van Gompel, R. P. G. and Pickering, M. J.
(2001).Lexical guidance in sentence processing: A noteon Adams, Clifton, and Mitchell (1998).
Psycho-nomic Bulletin & Review, 8(4):851?857.1065
