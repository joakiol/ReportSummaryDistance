Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 48?52, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsHsH: Estimating Semantic Similarity of Words and Short Phrases withFrequency Normalized Distance MeasuresChristian WartenaHochschule Hannover ?
University of Applied Sciences and ArtsDepartment of Information and CommunicationExpo Plaza 12, 30539 Hannover, GermanyChristian.Wartena@hs-hannover.deAbstractThis paper describes the approach of theHochschule Hannover to the SemEval 2013Task Evaluating Phrasal Semantics.
In or-der to compare a single word with a two wordphrase we compute various distributional sim-ilarities, among which a new similarity mea-sure, based on Jensen-Shannon Divergencewith a correction for frequency effects.
Theclassification is done by a support vector ma-chine that uses all similarities as features.
Theapproach turned out to be the most successfulone in the task.1 IntroductionThe task Evaluating Phrasal Semantics of the 2013International Workshop on Semantic Evaluation(Manandhar and Yuret, 2013) consists of two sub-tasks.
For the first subtask a list of pairs consistingof a single word and a two word phrase are given.For the English task a labeled list of 11,722 pairswas provided for training and a test set with 3,906unlabeled examples.
For German the training setcontains 2,202 and the test set 732 pairs.
The systemshould be able to tell whether the two word phraseis a definition of the single word or not.
This taskis somewhat different from the usual perspective offinding synonyms, since definitions are usually moregeneral than the words they define.In distributional semantics words are representedby context vectors and similarities of these con-text vectors are assumed to reflect similarities ofthe words they represent.
We compute context vec-tors for all words using the lemmatized version ofthe Wacky Corpora for English (UKWaC, approxi-mately 2,2 billion words) and German (DeWaC, 1,7billion words) (Baroni et al 2009).
For the phraseswe compute the context vectors as well directly onthe base of occurrences of that phrase, as well asby construction from the context vectors of the twocomponents.
For the similarities between the vec-tors we use Jensen-Shannon divergence (JSD) andcosine similarity.
Since the JSD is extremely depen-dent on the number of occurrences of the words, wedefine a new similarity measure that corrects for thisdependency.
Since none of the measures gives satis-factory results, we use all measures to train a supportvector machine that classifies the pairs.The remainder of this paper is organized as fol-lows.
We start with an overview of related work.
Insection 3 we discuss the dependence of JSD on wordfrequency and introduce a new similarity measure.Section 4 then describes the system.
The results aregiven in section 5 and are discussed in section 6.2 Related WorkThough distributional similarity has widely beenstudied and has become an established method tofind similar words, there is no consensus on the waythe context of a word has to be defined and on thebest way to compute the similarity between two con-texts.
In the most general definitions the context ofa word consists of a number of words and their re-lation to the given word (Grefenstette, 1992; Curranand Moens, 2002).
In the following we will onlyconsider the simplest case in which there is only onerelation: the relation of being in the same sentence.Each word can be represented by a so called con-48text vector in a high dimensional word space.
Sincethese vectors will be sparse, often dimensionality re-duction techniques are applied.
In the present pa-per we use random indexing, introduced by Karlgrenand Sahlgren (2001) and Sahlgren (2005) to reducethe size of the context vectors.The way in which the context vectors are con-structed also determines what similarity measuresare suited.
For random indexing Go?rnerup and Karl-gren (2010) found that best results are obtained us-ing L1-norm or Jensen-Shannon divergence (JSD).they also report that these measures highly correlate.We could confirm this in a preliminary experimentand therefore only use JSD in the following.Recently, the question whether and how an ap-propriate context vector for a phrase can be de-rived from the context vectors of its componentshas become a central issue in distributional seman-tics (Clark and Pulman, 2007; Mitchell and Lap-ata, 2008; Widdows, 2008; Clarke et al 2008).
Itis not yet clear which way of combining the vec-tors of the components is best suited for what goals.Giesbrecht (2010) and Mitchell and Lapata (2008)e.g.
find that for noun-noun compounds the prod-uct of context vectors (corresponding to the intersec-tion of contexts) and more complex tensor productsgive best results, while Guevara (2011) obtains bestresults for adjective-noun phrases with addition ofvectors (corresponding to union of contexts).
Sincewe do not (yet) have a single best similarity mea-sure to distinguish definitions from non-definitions,we use a combination of similarity measures to traina model as e.g.
also was done by Ba?r et al(2012).3 Frequency Dependency Correction ofJensen-Shannon DivergenceWeeds et al(2004) observed that in tasks in whichrelated words have to be found, some measures pre-fer words with a frequency similar to that of the tar-get word while others prefer high frequent words,regardless of the frequency of the target word.
SinceGo?rnerup and Karlgren (2010) found that L1-normand JSD give best results for similarity of randomindex vectors, we are especially interested in JSD.The JSD of two distributions p and q is given byJSD(p, q) = 12D(p||12p+12q)+12D(q||12p+12q) (1)where D(p||q) = ?ip(i)log p(i)log q(i) is the Kullback-Leibler divergence.
We will follow the usual termi-nology of context vectors.
However, we will alwaysnormalize the vectors, such that they can be inter-preted as probability mass distributions.
Accordingto Weeds et al(2004) the JSD belongs to the cat-egory of distance measures that tends to give smalldistances for highly frequent words.
In Wartena etal.
(2010) we also made this observation and there-fore we added an additional constraint on the selec-tion of keywords that should avoid the selection oftoo general words.
In the present paper we try to ex-plicitly model the dependency between the JSD andthe number of occurrences of the involved words.We then use the difference between the JSD of theco-occurrence vectors of two words and the JSD ex-pected on the base of the frequency of these wordsas a similarity measure.
In the following we will usethe dependency between the JSD and the frequencyof the words directly.
In (Wartena, 2013) we modelthe JSD instead as a function of the number of nonzero values in the context vectors.
The latter depen-dency can be modeled by a simpler function, but didnot work as well with the SemEval data set.Given two words w1 and w2 the JSD of their con-text vectors can be modeled as a function of the min-imum of the number of occurrences of w1 and w2.Figure 3 shows the JSD of the context vectors of thewords of the training set and the context vector ofthe definition phrase.
In this figure the JSD of thepositive and the negative examples is marked withdifferent marks.
The lower bound of the negativeexamples is roughly marked by a (red) curve, that isdefined for context vectors c1 and c2 for words w1and w2, respectively, byJSDexp(c1, c2) = a +1n?b + c(2)where n?
= min(n(w1), n(w2)) with n(w) the num-ber of occurrences of w in the corpus and with a,b and c constants that are estimated for each set ofword pairs.
For the pairs from the English trainingand test set the values are: a = 0.15, b = 0.3 andc = 0.5.
Experiments on the training data showedthat the final results are not very dependent on theexact values of these constants.Finally, our new measure is simply defined byJSDnorm(p, q) = JSD(p, q)?
JSDexp(p, q).
(3)49Figure 1: JSD (y-axis) of all pairs in the English training set versus the number of occurrences of the definition phrase(x-axis) in the UkWaC-Corpus.
The positives examples are marked by a +, the negative examples by a ?.
Mostpositive examples are hidden behind the negative ones.
The solid (red) line gives the expected JSD.4 System DescriptionThe main assumption for our approach is, that aword and its definition are distributionally more sim-ilar than a word and an arbitrary definition.
We userandom indexing to capture distributional propertiesof words and phrases.
Since similarity measures forrandom index vectors have biases for frequent or in-frequent pairs, we use a combination of differentmeasures.
For the two-word definition phrases wecan either estimate the context vector on the base ofthe two words that make up the phrase, or compute itdirectly from occurrences of the whole phrase in thecorpus.
The latter method has the advantage of beingindependent of assumptions about semantic compo-sition, but might have the problem that it is basedon a few examples only.
Thus we use both distribu-tions, and also include the similarities between thesingle word and each of the words of the definition.4.1 DistributionsConsider a pair (w, d) with w a word and d a defi-nition consisting of two words: d = (d1, d2).
Nowfor each of the words w, d1, d2 and the multiwordd we compute context vectors using the random in-dexing technique.
The context vectors are computedover the complete Wacky corpus.
The context usedfor a word are all open-class words (i.e.
Noun, Verb,Adjective, Adverb, etc.
but not Auxiliary, Pronoun,etc.)
in a sentence.
Each word is represented by arandom index vector of 10 000 dimensions in which8 random positions have a non-zero value.
The ran-dom vectors of all words in all contexts are summedup to construct context vectors (with length 10 000),denoted vw, vd, vd1 , vd2 .
In many cases there areonly very few occurrences of d, making the contextvector vd very unreliable.
Thus we also compute thevectors vaddd = vd1 + vd2 and vmultd = vd1 ?
vd2 .
Fi-nally, we also compute the general context vector (orbackground distribution) vgen which is the contextvector obtained by aggregating all used contexts.4.2 SimilaritiesTable 1 gives an overview of the similarities com-puted for the context vector vw.
In addition we alsocompute D(vw||vgen), D(vd||vgen), D(vd1 ||vgen),D(vd2 ||vgen).
The original intuition was that the def-inition of a word is usual given as a more generalterm or hypernym.
It turned out that this is not thecase.
However, in combination with other featuresthese divergences proved to be useful for the ma-chine learning algorithm.
Finally, we also use thedirect (first-order) co-occurrence between w and dby computing the ratio between the probability withwhich we expect w and d to co-occur in one sentenceif they would be independent, and the real probabil-ity of co-occurrence found in the corpus:co-occurrence-ratio(w, d) =p(w, d)p(w) ?
p(d)(4)50Table 1: Similarity measures used to compute the simi-larity of a context vector of some word to various contextvectors for a phrase d = (d1, d2).vd vd1 vd2 vaddd vmultdjsd X X X Xjsd-norm X X X Xcossim X XTable 2: Results for English and German (no namesdataset).
Results on train sets are averaged results from10-fold cross validation.
Results on the test set are theofficial task results.AUC Accuracy F-MeasureTrain English 0.88 0.80 0.79Test English - 0.80 0.79Train German 0.90 0.83 0.82Test German - 0.83 0.81where p(w, d) is the probability that w and d arefound in the same sentence, and p(w), with w a wordor phrase, the probability that a sentence contains w.For the computation of JSDnorm(vw, vaddd ) weneed the number of occurrences on which vaddd isbased.
As an estimate for this number we usemax(n(d1), n(d2)).
The constants a, b and c inequation 2 are set to the following values: forall cases a = 0.15; for JSDnorm(vw, vd) we letb = 0.3 and c = 0.5; for JSDnorm(vw, vd1) andJSDnorm(vw, vd2) we let b = 0.35 and c = ?0.1; forJSDnorm(vw, vaddd ) we let b = 0.4 and c = ?0.1.
Forthe German subtask a = 0.28 and slightly differentvalues for b and c were used to account for slightlydifferent frequency dependencies.4.3 Combining SimilaritiesThe 15 attributes for each pair obtained in this wayare used to train a support vector machine (SVM)using LibSVM (Chang and Lin, 2011).
Optimal pa-rameters for the SVM were found by grid-search and10-fold cross validation on the training data.5 ResultsIn Table 2 the results are summarized.
Since thetask can also be seen as a ranking task, we includethe Area Under the ROC-Curve (AUC) as a classi-cal measure for ranking quality.
We can observe thatthe results are highly stable between training set andTable 3: Results for English train set (average from 10-fold cross validation) using one featurefeature Accuracy AUCjsd(vw, vd) 0.50 0.57jsdnorm(vw, vd) 0.59 0.70jsd(vw, vd1) 0.54 0.63jsdnorm(vw, vd1) 0.61 0.69jsd(vw, vd2) 0.57 0.65jsdnorm(vw, vd2) 0.63 0.71jsd(vw, vaddd ) 0.59 0.67jsdnorm(vw, vaddd ) 0.66 0.74cossim(vw, vaddd ) 0.69 0.76cossim(vw, vmultd ) 0.62 0.71co-occ-ratio(w, d) 0.61 0.71test set and across languages.
Table 3 gives the re-sults that are obtained on the training set using onefeature.
We can observe that the normalized versionsof the JSD always perform better than the JSD itself.Furthermore, we see that for the composed vectorsthe cosine performs better than the normalized JSD,while it performs worse than JSD for the other vec-tors (not displayed in the table).
This eventually canbe explained by the fact that we have to estimate thenumber of contexts for the calculation of jsdexp.6 ConclusionThough there are a number of ad-hoc decisions inthe system the approach was very successful andperformed best in the SemEval task on phrasal se-mantics.
The main insight from the developmentof the system is, that there is not yet a single bestsimilarity measure to compare random index vec-tors.
The normalized JSD turns out to be a usefulimprovement of the JSD but is problematic for con-structed context vectors, the formula in equation (2)is rather ad hoc and the constants are just rough esti-mates.
The formulation in (Wartena, 2013) might bea step in the right direction, but also there we are stillfar away from a unbiased similarity measure with awell founded theoretical basis.Finally, it is unclear, what is the best way to rep-resent a phrase in distributional similarity.
Here weuse three different vectors in parallel.
It would bemore elegant if we had a way to merge context vec-tors based on direct observations of the phrase witha constructed context vector.51ReferencesDaniel Ba?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
Ukp: Computing semantic textual sim-ilarity by combining multiple content similarity mea-sures.
In Proceedings of the Sixth International Work-shop on Semantic Evaluation (SemEval-2012), pages435?440.M.
Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.2009.
The wacky wide web: A collection of very largelinguistically processed web-crawled corpora.
Lan-guage Resources and Evaluation 43 (3): 209-226,43(3):209?226.C.-C. Chang and C.-J.
Lin.
2011.
Libsvm : a libraryfor support vector machines.
ACM Transactions onIntelligent Systems and Technology, 2(3):1?27.Stephen Clark and Stephen Pulman.
2007.
Combiningsymbolic and distributional models of meaning.
InProceedings of the AAAI Spring Symposium on Quan-tum Interaction, pages 52?55.Daoud Clarke, Rudi Lutz, and David Weir.
2008.
Se-mantic composition with quotient algebras.
In Pro-ceedings of the 9th International Conference on Com-putational Semantics (IWCS 2011).James R. Curran and Marc Moens.
2002.
Improve-ments in automatic thesaurus extraction.
In Unsuper-vised Lexical Acquisition: Proceedings of the Work-shop of the ACL Special Interest Group on the Lexi-con (SIGLAX)., pages 59?66.
Association of Compu-tational Linguistics.Eugenie Giesbrecht.
2010.
Towards a matrix-based dis-tributional model of meaning.
In Proceedings of theNAACL HLT 2010 Student Research Workshop, pages23?28, Los Angeles, California.
ACL.Olaf Go?rnerup and Jussi Karlgren.
2010.
Cross-lingualcomparison between distributionally determined wordsimilarity networks.
In Proceedings of the 2010 Work-shop on Graph-based Methods for Natural LanguageProcessing, pages 48?54.
ACL.Gregory Grefenstette.
1992.
Use of syntactic context toproduce term association lists for text retrieval.
In SI-GIR ?92: Proceedings of the 15th annual internationalACM SIGIR conference on Research and developmentin information retrieval, pages 89?97.
ACM.Emiliano Guevara.
2011.
Computing semantic compo-sitionality in distributional semantics.
In Proceedingsof the 9th International Conference on ComputationalSemantics (IWCS 2011), pages 135?144.Jussi Karlgren and Magnus Sahlgren.
2001.
From wordsto understanding.
In Foundations of Real-World Intel-ligence, pages 294?308.
CSLI Publications, Stanford,California.Suresh Manandhar and Deniz Yuret, editors.
2013.
Pro-ceedings of the 7th International Workshop on Seman-tic Evaluation (SemEval 2013), in conjunction withthe Second Joint Conference on Lexical and Compu-tational Semantcis (*SEM 2013), Atlanta.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In ACL 2008, Pro-ceedings of the 46th Annual Meeting of the Associationfor Computational Linguistics, pages 236?244.Magnus Sahlgren.
2005.
An introduction to randomindexing.
In Methods and Applications of Seman-tic Indexing Workshop at the 7th International Con-ference on Terminology and Knowledge Engineering,TKE, volume 5.Christian Wartena, Rogier Brussee, and WouterSlakhorst.
2010.
Keyword extraction using wordco-occurrence.
In Database and Expert SystemsApplications (DEXA), 2010 Workshop on, pages54?58.
IEEE.Christian Wartena.
2013.
Distributional similarity ofwords with different frequencies.
In Proceedings ofthe Dutch-Belgian Information Retrieval Workshop,Delft.
To Appear.Julie Weeds, David J. Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributional simi-larity.
In COLING 2004, Proceedings of the 20th In-ternational Conference on Computational Linguistics.Dominic Widdows.
2008.
Semantic vector products:Some initial investigations.
In Second Conference onQuantum Interaction, Oxford, 26th 28th March 2008.52
