Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 1?10,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsEmotiBlog: a finer-grained and more precise learning of subjectivity expression modelsEster Boldrini University of Alicante, Department of Software and Computing Systems eboldrini@dlsi.ua.esAlexandra Balahur University of Alicante, Department of Software and Computing Systems abalahur@dlsi.ua.es Patricio Mart?nez-Barco University of Alicante, Department of Software and Computing Systems patricio@dlsi.ua.esAndr?s Montoyo University of Alicante, Department of Software and Computing Systems montoyo@dlsi.ua.es  AbstractThe exponential growth of the subjective in-formation in the framework of the Web 2.0 has led to the need to create Natural Language Processing tools able to analyse and process such data for multiple practical applications.
They require training on specifically annotated corpora, whose level of detail must be fine enough to capture the phenomena involved.
This paper presents EmotiBlog ?
a fine-grained annotation scheme for subjectivity.
We show the manner in which it is built and demonstrate the benefits it brings to the sys-tems using it for training, through the experi-ments we carried out on opinion mining and emotion detection.
We employ corpora of dif-ferent textual genres ?a set of annotated re-ported speech extracted from news articles, the set of news titles annotated with polarity and emotion from the SemEval 2007 (Task 14) and ISEAR, a corpus of real-life self-expressed emotion.
We also show how the model built from the EmotiBlog annotations can be enhanced with external resources.
The results demonstrate that EmotiBlog, through its structure and annotation paradigm, offers high quality training data for systems dealing both with opinion mining, as well as emotion detec-tion.
1 Credits This paper has been supported by Ministe-rio de Ciencia e Innovaci?n- Spanish Gov-ernment (grant no.
TIN2009-13391-C04-01), and Conselleria d'Educaci?n-Generalitat Valenciana (grant no.
PRO-METEO/2009/119 and A-COMP/2010/288).2 Introduction The exponential growth of the subjective infor-mation with Web 2.0 created the need to develop new Natural Language Processing (NLP) tools to automatically process and manage the content available on the Internet.
Apart from the tradi-tional textual genres, at present we have new ones such as blogs, forums and reviews.
The main difference between them is that the latter are predominantly subjective, containing per-sonal judgments.
At the moment, NLP tools and methods for analyzing objective information have a better performance than the new ones the research community is creating for managing the subjective content.
The survey called ?The State of the Blogosphere 2009?, published by Tech-norati 1 , demonstrates that users are blogging more than ever.
Furthermore, in contrast to the general idea about bloggers, each day it is more and more the number of professionals who de-cide to use this means of communication, contra-dicting the common belief about the predomi-nance of an informal editing (Balahur et al, 2009).
Due to the growing interest in this text type, the subjective data of the Web is increasing on a daily basis, becoming a reflection of peo-ple?s opinion about a wide range of topics.
(Cui, Mittal and Datar, 2006).
Blogs represent an im-portant source of real-time, unbiased informa-tion, useful for the development of many applica-tions for concrete purposes.
Given the proved importance of automatically processing this data, a new task has appeared in NLP task, dealing with the treatment of subjective data: Sentiment Analysis (SA).
The main objective of this paper is to present EmotiBlog (Boldrini et al, 2009), a fine-grained annotation scheme for labeling sub-jectivity in the new textual genres.
Subjectivity                                                 1 http://technorati.com/1can be reflected in text through expressions of emotions beliefs, views (a way of considering something) 2  and opinions, generally denomi-nated ?private states?
(Uspensky, 1973), not open to verification (Wiebe, 1994).
We per-formed a series of experiments focused on dem-onstrating that EmotiBlog represents a step for-ward to previous research in this field; its use allows a finer-grained and more precise learning of subjectivity expression models.
Starting form (Wiebe, Wilson and Cardie, 2005) we created an annotation schema able to capture a wide range and key elements, which give subjectivity, mov-ing a step forward the mere polarity recognition.
In particular, the experiments concern expres-sions of emotion, as a finer-grained analysis of affect in text and a subsequent task to opinion mining (OM) and classification.
To that aim, we employ corpora of different textual genres?
a set of annotated reported speech extracted from news articles (denominated JRC quotes) (Bala-hur et al, 2010) and the set of news titles anno-tated with polarity and emotion from the SemE-val 2007 Task No.
14 (Strapparava and Mihal-cea, 2007), as well as a corpus of real-life self-expressed emotion entitled ISEAR (Scherer and Walbott, 1999).
We subsequently show, through the quality of the results obtained, that Emoti-Blog, through its structure and annotation para-digm, offers high quality training for systems dealing both with opinion mining, as well as emotion detection.
3 Motivation and Contribution The main motivation of this research is the dem-onstrated necessity to work towards the harmoni-zation and interoperability of the increasingly large number of tools and frameworks that sup-port the creation, instantiation, manipulation, querying, and exploitation of annotated resource.
This necessity is stressed by the new tools and resources, which have been recently created for processing the subjectivity in the new-textual genres born with the Web 2.0.
Such predomi-nantly subjective data is increasing at an expo-nential rate (about 75000 new blogs are reported to be created every day) and contains opinions on the most diverse set of topics.
Given its world-wide availability, the subjective data on the Web has become a primary source of information (Balahur et al, 2009).
As a consequence, new mechanisms have to be implemented so that this                                                 2 http://dictionary.cambridge.org/data is effectively analyzed and processed.
The main challenge of the opinionated content is that, unlike the objective one, which presents facts, the subjective information is most of the times difficult and complex to extract and classify us-ing in grammatically static and fixed rules.
Ex-pression of subjectivity is more spontaneous and even if the majority is quite formal, new means of expressivity can be encountered, such as the use of colloquialisms, sayings, collocations or anomalies in the use of punctuation; this is moti-vated by the fact that subjectivity expression is part of our daily life.
For example, at the time of taking a decision, people search for information and opinions expressed on the Web on their mat-ter of interest and base their final decision on the information found.
At the same time, when using a product, people often write reviews on it, so that others can have a better idea of the perform-ance of that product before purchasing it.
There-fore, on the one hand, the growing volume of opinion information available on the Web allows for better and more informed decisions of the users.
On the other hand, the amount of data to be analyzed requires the development of special-ized NLP systems that automatically extract, classify and summarize the data available on the Web on different topics.
(Esuli and Sebastiani, 2006) define OM as a recent discipline at the crossroads of Information Retrieval and Compu-tational Linguistics, which is concerned not with the topic a document is about, but with the opin-ion it expresses.
Research in this field has proven the task to be very difficult, due to the high se-mantic variability of affective language.
Differ-ent authors have addressed the problem of ex-tracting and classifying opinion from different perspectives and at different levels, depending on a series of factors which can be level of interest (overall/specific), querying formula (?Nokia E65?/?Why do people buy Nokia E65??
), type of text (review on forum/blog/dialogue/press arti-cle), and manner of expression of opinion - di-rectly (using opinion statements, e.g.
?I think this product is wonderful!
?/?This is a bright initia-tive?
), indirectly (using affect vocabulary, e.g.
?I love the pictures this camera takes!
?/?Personally, I am shocked one can pro-pose such a law!?)
or implicitly (using adjectives and evaluative expressions, e.g.
?It?s light as a feather and fits right into my pocket!?).
While determining the overall opinion on a movie is sufficient for taking the decision to watch it or not, when buying a product, people are interested in the individual opinions on the different prod-2uct characteristics.
When discussing a person, one can judge and give opinion on the person?s actions.
Moreover, the approaches taken can vary depending on the manner in which a user asks for the data (general formula such as ?opinions on X?
or a specific question ?Why do people like X??
and the text source that needs to be queried).
Retrieving opinion information in newspaper articles or blogs posts is more complex, because it involves the detection of different discussion topics, the subjective phrases present and subse-quently their classification according to polarity.
Especially in the blog area, determining points of view expressed in dialogues together with the mixture of quotes and pastes from newspapers on a topic can, additionally, involve determining the persons present and whether or not the opinion expressed is on the required topic or on a point previously made by another speaker.
This diffi-cult NLP problem requires the use of specialized data for system training and tuning, gathered, annotated and tested within the different text spheres.
At the present moment, these specialized resources are scarce and when they exist, they are rather simplistically annotated or highly domain-dependent.
Moreover, most of these resources created are for the English.
The contribution we describe in this paper intends to propose solutions to the above-mentioned problems, and consists of the following points: first of all, we overcome the problem of corpora scarcity in other languages except English and also improve the English ones; we present the manner in which we compiled a multilingual corpus of blog posts on different topics of interest in three languages-Spanish, Italian and English.
The second issue we tried to solve was the coarse-grained annotation schemas employed in other annotation schema.
Thus, we describe the new annotation model, EmotiBlog built up in order to capture the different subjectivity/objectivity, emotion/opinion/attitude aspects we are interested in at a finer-grained level.
We justify the need for a more detailed annotation model, the sources and the reasons taken into consideration when constructing the corpus and its annotation.
Thirdly, we address an aspect strongly related to blogs annotation: due the presence of ?copy and pastes?
from news articles or other blogs, the frequent quotes, we include the annotation of both the directly indicated source, as well as the anaphoric references at cross-document level.
We discuss on the problems encountered at different stages and comment upon some of the conclusions we have reached while performing this research.this research.
Finally, we conclude on our ap-proach and propose the lines for future work.
4 Related Work In recent years, different researchers have ad-dressed the needs and possible methodologies from the linguistic, theoretical and practical points of view.
Thus, the first step involved re-sided in building lexical resources of affect, such as WordNet Affect (Strapparava and Valitutti, 2004), SentiWordNet (Esuli and Sebastiani, 2006), Micro-WNOP (Cerini et.
Al, 2007) or ?emotion triggers?
(Balahur and Montoyo, 2009).
All these lexicons contain single words, whose polarity and emotions are not necessarily the ones annotated within the resource in a larger context.
We also employed the ISEAR corpus, consisting of phrases where people describe a situation when they felt a certain emotion.
Our work, therefore, concentrates on annotating larger text spans, in order to consider the undeni-able influence of the context.
The starting point of research in emotion is represented by (Balahur and Montoyo, 2008), who centered the idea of subjectivity around that of private states, and set the benchmark for subjectivity analysis as the recognition of opinion-oriented language in order to distinguish it from objective language and giv-ing a method to annotate a corpus depending on these two aspects ?
MPQA (Wiebe, Wilson and Cardie, 2005).
Furthermore, authors show that this initial discrimination is crucial for the senti-ment task, as part of Opinion Information Re-trieval  (last three editions of the TREC Blog tracks 3  competitions, the TAC 2008 competi-tion4), Information Extraction (Riloff and Wiebe, 2003) and Question Answering (Stoyanov et al, 2004) systems.
Once this discrimination is done, or in the case of texts containing only or mostly subjective language (such as e-reviews), opinion mining becomes a polarity classification task.
Our work takes into consideration this initial dis-crimination, but we also add a deeper level of emotion annotation.
Since expressions of emo-tion are also highly related to opinions, related work also includes customer review classifica-tion at a document level, sentiment classification using unsupervised methods (Turney, 2002), Machine Learning techniques (Pang and Lee, 2002), scoring of features (Dave, Lawrence and Pennock, 2003), using PMI, syntactic relations                                                 3 http://trec.nist.gov/data/blog.html 4 http://www.nist.gov/tac/3and other attributes with SVM (Mullena and Col-lier, 2004), sentiment classification considering rating scales (Pang and Lee, 2002), supervised and unsupervised methods (Chaovalit and Zhou, 2005) and semisupervised learning (Goldberg and Zhou, 2006).
Research in classification at a document level included sentiment classification of reviews (Ng, Dasgupta and Arifin, 2006), sen-timent classification on customer feedback data (Gamon, Aue, Corston-Oliver, Ringger, 2005), comparative experiments (Cui, Mittal and Datar, 2006).
Other research has been conducted in ana-lysing sentiment at a sentence level using boot-strapping techniques (Riloff, Wiebe, 2003), con-sidering gradable adjectives (Hatzivassiloglou, Wiebe, 2000), semisupervised learning with the initial training some strong patterns and then ap-plying NB or self-training (Wiebe and Riloff, 2005) finding strength of opinions (Wolson, Wiebe, Hwa, 2004) sum up orientations of opin-ion words in a sentence (or within some word window) (Kim and Hovy, 2004), (Wilson and Wiebe, 2004), determining the semantic orienta-tion of words and phrases (Turney and Littman, 2003), identifying opinion holders (Stoyanov and Cardie, 2006), comparative sentence and relation extraction and feature-based opinion mining and summarization (Turney, 2002).
Finally, fine-grained, feature-based opinion summarization is defined in (Hu and Liu, 2004) and researched in (Turney, 2002) or (Pang and Lee, 2002).
All these approaches concentrate on finding and classifying the polarity of opinion words, which are mostly adjectives, without taking into ac-count modifiers or the context in general.
Our work, on the other hand, represents the first step towards achieving a contextual comprehension of the linguistic roots of emotion expression.
5 Corpora It is well known that nowadays blogs are the second way of communication most used after the e-mail.
They are extremely useful and a poll for discussing about any topic with the world.
For this reason, the first corpus object of our study is a collection of blog posts extracted from the Web.
The texts we selected have distinctive features, extremely different from traditional tex-tual ones.
In fact people writing a post can use an informal language colloquialism, emoticons, etc.
to express their feelings and it is not rare to find a mix of sources in the same post; people usually mention some facts or discourses and then they give their opinion about them.
As we can deduce,the source detection represents one of the most complex tasks.
As we mentioned above, we car-ried out a multilingual research, collecting texts in three languages: Spanish, Italian, and English about three subjects of interest.
The first one contains blog posts commenting upon the signing of the Kyoto Protocol against global warming, the second collection consists of blog entries about the Mugabe government in Zimbabwe, and finally we selected a series of blog posts discuss-ing the issues related to the 2008 USA presiden-tial elections.
For each of the abovementioned topics, we have gathered 100 texts, summing up a total of 30.000 words approximately for each language.
However in this research we start with English but consider as future work labeling the other languages we have.
The second corpus we employed for this research is a collection of 1592 quotes extracted from the news in April 2008.
As a consequence they are about many different top-ics and in English (Balahur and Steinberg, 2009).
Both of these corpora have been annotated with EmotiBlog that is presented in the next section.
6 EmotiBlog Annotation Model Our annotation schema can be defined as a fine-grained model for labelling subjectivity of the new-textual genres born with the Web 2.0.
As mentioned above, it represents a step forward to previous research and it is focused on detecting the linguistic elements, which give subjectivity to the text.
The EmotiBlog annotation is divided into different levels (Figure 1).Figure 1: General structure of EmotiBlog.
As we can observe in Figure 1, the first distinc-tion to be made is between objective and subjec-tive speech.
If we are labelling an objective sen-tence, we insert the source element, while if we are annotating a subjective discourse, a list of elements with the corresponding attributes have to be added.
We select among the list of subjec-tive elements and specify the element?s attrib-4utes.
Table 1 presents the annotation model in detail.
Elem.
Description Obj.
speech Confidence, comment, source, target.
Subj.
speech Confidence, comment, level, emotion, phenomenon, polarity, source and target.
Adjectives Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target.
Adverbs Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target.
Verbs Confidence, comment, level, emotion, phenomenon, polarity, mode, source and target.
Anaphora Confidence, comment, type, source and target.
Capital letter Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target.
Punctuation Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target.
Names Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, and source.
Phenomenon Confidence, comment, type: collocation, saying, slang, title, and rhetoric.
Reader Inter-pretation Confidence, comment, level, emotion, phenomenon, polarity, source and target.
Author Inter-pretation Confidence, comment, level, emotion, phenomenon, polarity, source and target.
Emotions Confidence, comment, accept, anger, anticipation, anxiety, appreciation, bad, bewilderment, comfort, ?
Table 1: EmotiBlog structure  Each element of the discourse has its own attrib-utes with a series of features, which have to be annotated.
Due to space reasons it is impossible to detail each one of them, however we would like to underline the most innovative and rel-evant.
For each element we are labelling the an-notator has to insert his level of confidence.
In this way we will assign each label a weight that will be computed for future evaluations.
More-over, the annotator has to insert the polarity, which can be positive or negative, the level (high, medium, low) and also the sentiment this element is expressing.
Table 2 presents a com-plete list of the emotions we selected to be part of EmotiBlog.
We grouped all sentiments into subgroups in order to help the evaluation pro-cess.
In fact emotions of the same subgroup will have less impact when calculating the inter-annotation agreement.
In order to make this sub-division proper and effective division, we were inspired by (Scherer, 2005) who created an alter-native dimensional structure of the semantic space for emotions.
The graph below represents the mapping of the term Russell (1983) uses for his claim of an emotion circumflex in two-dimensional valence by activity/arousal space (upper-case terms).
As we can appreciate, the circle is divided by 4 axes.
Moreover, Scherer distinguishes between positive and negative sen-timents and after that between active and passive.
Furthermore emotions are grouped between ob-structive and conductive, and finally between high power and low power control.
We started form this classification, grouping sentiments into positive and negative, but we divided them as high/low power control, obstructive/conductive and active/passive.
Further on, we distributed the sentiments within our list into the Scherer slots creating other smaller categories included in the abovementioned general ones.
The result of this division is shown in Table 2:Table 2: Alternative dimensional structures of the semantic space for emotions  Following with the description of the model, we said that the first distinction to be made is be-tween objective and subjective speech.
Analys-ing the texts we collected, we realised that even if the writer uses an objective speech, sometimes it is just apparently objective and for this reason we added two elements: reader and author inter-pretation.
The first one is the impres-sion/feeling/reaction the reader has reading the intervention and what s/he can deduce from the piece of text and the author interpretation is what we can understand from the author (politic orien-tation, preferences).
All this information can be deduced form some linguistic elements that ap-parently are not so objective as they may appear.
Another innovative element we inserted in the model is the coreference but just at a cross-post level.
It is necessary because blogs are composed by posts linked between them and thus cross-Group Emotions Criticism Sarcasm, irony, incorrect, criticism, objection, opposition, scepticism.
Happiness Joy, joke.
Support Accept, correct, good, hope, support, trust, rapture, respect, patience, appreciation, excuse.
Importance Important, interesting, will, justice, longing, anticipation, revenge.
Gratitude Thank.
Guilt Guilt, vexation.
Fear Fear, fright, troubledness, anxiety.
Surprise Surprise, bewilderment, disappoint-ment, consternation.
Anger Rage, hatred, enmity, wrath, force, anger, revendication.
Envy Envy, rivalry, jealousy.
Indifference Unimportant, yield, sluggishness.
Pity Compassion, shame, grief.
Pain Sadness, lament, remorse, mourning, depression, despondency.
Shyness Timidity.
Bad Bad, malice, disgust, greed.5document coreference can help the reader to fol-low the conversations.
We also label the unusual usage of capital letters and repeated punctuation.
In fact, it is very common in blogs to find words written in capital letter or with no conventional usage of punctuation; these features usually mean shouts or a particular mood of the writer.
Using EmotiBlog, we annotate the single ele-ments, but we also mark sayings or collocations, representative of each language.
A saying is a well-known and wise statement, which often has a meaning, different from the simple meanings of the words it contains5; while a collocation is a word or phrase, which is frequently used with another word or phrase, in a way that sounds cor-rect to native speakers, but might not be expected from the individual words?
meanings6.
Finally we insert for each element the source and topic.
An example of annotation can be:  <phenomenon target="Kyoto Protocol" category="phrase" degree="medium" source="w" polarity="positive" emotion="good">The Onion has a <adjective target="Kyoto Protocol" phenomenon="phrase" de-gree="medium" polarity="positive" emotion="good" source="w" ismodifier="yes">great</adjective> story today titled ?Bush Told to Sign Birthday Treaty for Someone Named Kyoto."
</phenomenon> 7 Experiments and Evaluation In order to evaluate the appropriateness of the EmotiBlog annotation scheme and to prove that the fine-grained level it aims at has a positive impact on the performance of the systems em-ploying it as training, we performed several ex-periments.
Given that a) EmotiBlog contains an-notations for individual words, as well as for multi-word expressions and at a sentence level, and b) they are labeled with polarity, but also emotion, our experiments show how the anno-tated elements can be used as training for the opinion mining and polarity classification task, as well as for emotion detection.
Moreover, tak-ing into consideration the fact that EmotiBlog labels the intensity level of the annotated ele-ments, we performed a brief experiment on de-termining the sentiment intensity, measured on a three-level scale: low, medium and high.
In order to perform these three different evaluations, we chose three different corpora.
The first one is a collection of quotes (reported speech) from newspaper articles presented in (Balahur et al, 2010), enriched with the manual fine-grained5  Definition according to the Cambridge Advanced Learner?s Dictionary 6   Definition according to the Cambridge Advanced Learner?s Dictionaryannotation of EmotiBlog7; the second one is the collection of newspaper titles in the test set of the SemEval 2007 task number 14 ?
Affective Text.
Finally, the third one is a corpus of self-reported emotional response ?
ISEAR (Scherer and Wal-bott, 1999).
The intensity classification task is evaluated only on the second corpus, given that it is the only one in which scores between -100 and 0 and 0 and 100, respectively, are given for the polarity of the titles.
6.1 Creation of training models For the OM and polarity classification task, we first extracted the Named Entities contained in the annotations using Lingpipe and united through a ?_?
all the tokens pertaining to the NE.
All the annotations of punctuation signs that had a specific meaning together were also united un-der a single punctuation sign.
Subsequently, we processed the annotated data, using Minipar.
We compute, for each word in a sentence, a series of features (some of these features are used in (Choi et al, 2005): ?
the part of speech (POS)  ?
capitalization (if all letters are in capitals, if only the first letter is in capitals, and if it is a NE or not) ?
opinionatedness/intensity/emotion - if the word is annotated as opinion word, its polar-ity, i.e.
1 and -1 if the word is positive or negative, respectively and 0 if it is not an opinion word, its intensity (1.2 or 3) and 0 if it is not a subjective word, its emotion (if it has, none otherwise) ?
syntactic relatedness with other opinion word ?
if it is directly dependent of an opin-ion word or modifier (0 or 1), plus the polar-ity/intensity and emotion of this word (0 for all the components otherwise) ?
role in 2-word, 3-word and 4-word annota-tions: opinionatedness, intensity and emo-tion of the other words contained in the an-notation, direct dependency relations with them if they exist and 0 otherwise.
We compute the length of the longest sentence in EmotiBlog.
The feature vector for each of the sentences contains the feature vectors of each of its words and 0s for the corresponding feature vectors of the words, which the current sentence has less than the longest annotated sentence.
Fi-nally, we add for each sentence as feature binary features for subjectivity and polarity, the value corresponding to the intensity of opinion and the                                                 7 Freely available on request to the authors.6general emotion.
These feature vectors are fed into the Weka8 SVM SMO ML algorithm and a model is created (EmotiBlog I).
A second model (EmotiBlog II) is created by adding to the collec-tion of single opinion and emotion words anno-tated in EmotiBlog, the Opinion Finder lexicon and the opinion words found in MicroWordNet, the General Inquirer resource and WordNet Af-fect.
6.2 Evaluation of models on test sets  In order to evaluate the performance of the mod-els extracted from the features of the annotations in EmotiBlog, we performed different tests.
The first one regarded the evaluation of the polarity and intensity classification task using the Emoit-blog I and II constructed models on two test sets ?
the JRC quotes collection and the SemEval 2007 Task Number 14 test set.
Since the quotes often contain more than a sentence, we consider the polarity and intensity of the entire quote as the most frequent result in each class, corre-sponding to its constituent sentences.
Also, given the fact that the SemEval Affective Text head-lines were given intensity values between -100 and 100, we mapped the values contained in the Gold Standard of the task into three categories: [-100, -67] is high (value 3 in intensity) and nega-tive (value -1 in polarity), [-66, 34] medium negative and [33, 1] is low negative.
The values between [1 and 100] are mapped in the same manner to the positive category.
0 was consid-ered objective, so containing the value 0 for in-tensity.
The results are presented in Table 3 (the values I and II correspond to the models Emoti-Blog I and EmotiBlog II):   Test  Corpus Evaluation type Precision Recall Polarity 32.13 54.09 JRC quotes I Intensity 36.00 53.2 Polarity 36.4 51.00 JRC quotes II Intensity 38.7 57.81 Polarity 38.57 51.3 SemEval I Intensity 37.39 50.9 Polarity 35.8 58.68 SemEval II Intensity 32.3 50.4 Table 3.
Results for polarity and intensity classifi-cation using the models built from the EmotiBlog annotations The results shown in Table 2 show a signifi-cantly high improvement over the results ob-tained in the SemEval task in 2007.
This is ex-plainable, on the one hand, by the fact that sys-                                                8 http://www.cs.waikato.ac.nz/ml/weka/tems performing the opinion task did not have at their disposal the lexical resources for opinion employed in the EmotiBlog II model, but also because of the fact that they did not use machine learning on a corpus comparable to EmotiBlog (as seen from the results obtained when using solely the EmotiBlog I corpus).
Compared to the NTCIR 8 Multilingual Analysis Task this year, we obtained significant improvements in preci-sion, with a recall that is comparable to most of the participating systems.
In the second experi-ment, we tested the performance of emotion clas-sification using the two models built using Emo-tiBlog on the three corpora ?
JRC quotes, SemE-val 2007 Task No.14 test set and the ISEAR cor-pus.
The JRC quotes are labeled using Emoti-Blog; however, the other two are labeled with a small set of emotions ?
6 in the case of the Se-mEval data (joy, surprise, anger, fear, sadness, disgust) and 7 in ISEAR (joy, sadness, anger, fear, guilt, shame, disgust).
Moreover, the Se-mEval data contains more than one emotion per title in the Gold Standard, therefore we consider as correct any of the classifications containing one of them.
In order to unify the results and ob-tain comparable evaluations, we assessed the performance of the system using the alternative dimensional structures defined in Table 1.
The ones not overlapping with the category of any of the 8 different emotions in SemEval and ISEAR are considered as ?Other?
and are not included either in the training, nor test set.
The results of the evaluation are presented in Table 4.
Again, the values I and II correspond to the models EmotiBlog I and II.
The ?Emotions?
category contains the following emotions: joy, sadness, anger, fear, guilt, shame, disgust, surprise.
Test  corpus Evaluation  type Precision Recall JRC quotes I Emotions   24.7 15.08JRC quotes II Emotions  33.65 18.98SemEval I Emotions 29.03 18.89 SemEval II Emotions 32.98 18.45 ISEAR I Emotions 22.31 15.01 ISEAR II Emotions 25.62 17.83 Table 4.
Results for emotion classification using the models built from the EmotiBlog annotations.
The best results for emotion detection were ob-tained for the ?anger?
category, where the preci-sion was around 35 percent, for a recall of 19 percent.
The worst results obtained were for the ISEAR category of ?shame?, where precision was around 12 percent, with a recall of 15 per-7cent.
We believe this is due to the fact that the latter emotion is a combination of more complex affective states and it can be easily misclassified to other categories of emotion.
Moreover, from the analysis performed on the errors, we realized that many of the affective phenomena presented were more explicit in the case of texts expressing strong emotions such as ?joy?
and ?anger?, and were mostly related to common-sense interpreta-tion of the facts presented in the weaker ones.
As it can be seen in Table 3, results for the texts per-taining to the news category obtain better results, most of all news titles.
This is due to the fact that such texts, although they contain a few words, have a more direct and stronger emotional charge than direct speech (which may be biased by the need to be diplomatic, find the best suited words etc.).
Finally, the error analysis showed that emo-tion that is directly reported by the persons expe-riencing is more ?hidden?, in the use of words carrying special signification or related to gen-eral human experience.
This fact makes emotion detection in such texts a harder task.
Neverthe-less, the results in all corpora are comparable, showing that the approach is robust enough to handle different text types.
All in all, the results obtained using the fine and coarse-grained anno-tations in EmotiBlog increased the performance of emotion detection as compared to the systems in the SemEval competition.
6.3 Discussion on the overall results  From the results obtained, we can see that this approach combining the features extracted from the EmotiBlog fine and coarse-grained annota-tions helps to balance between the results ob-tained for precision and recall.
The impact of using additional resources that contain opinion words is that of increasing the recall of the sys-tem, at the cost of a slight drop in precision, which proves that the approach is robust enough so that additional knowledge sources can be added.
Although the corpus is small, the results obtained show that the phenomena it captures is relevant in the OM task, not only for the blog sphere, but also for other types of text (newspa-per articles, self-reported affect).
8 Conclusions and future work Due to the exponential increase of the subjective information result of the high-level usage of the Internet and the Web 2.0, NLP able to process this data are required.
In this paper we presentedthe procedure by which we compiled a multilin-gual corpus of blog posts on different topics of interest in three languages: Spanish, Italian and English.
Further on, we explained the need to create a finer-grained annotation schema that can be used to improve the performance of subjectiv-ity mining systems.
Thus, we presented the new annotation model, EmotiBlog and justified the benefits of this detailed annotation schema, pre-senting the sources and the reasons taken into consideration when building up the corpus and its labeling.
Furthermore, we addressed the pres-ence of ?copy and pastes?
from news articles or other blogs, the frequent quotes.
For solving this possible ambiguity we included the annotation of both the directly indicated source, as well as the anaphoric references at cross-document level.
We performed several experiments on three dif-ferent corpora, aimed at finding and classifying both the opinion, as well as the expressions of emotion they contained; we showed that the fine and coarse-grained levels of annotation that EmotiBlog contains offers important information on the structure of affective texts, leading to an improvement of the performance of systems trained on it.
Although the EmotiBlog corpus is small, the results obtained are promising and show that the phenomena it captures are relevant in the OM task, not only for the blog sphere, but also for other textual-genres.
It is well known that OM is an extremely challenging task and a young discipline, thus there is room for im-provement above all to solve linguistic phenom-ena such as the correference resolution at a cross document level, temporal expression recognition.
In addition to this, more experiments would need to be done in order to verify the complete ro-bustness of EmotiBlog.
Last but not least, our idea is to include the existing tools for a more effective semi-supervised annotation.
After the training of the ML system we obtain automati-cally some markables which have to be validated or not by the annotator and the ideal option would be to connect these terms the system de-tects automatically with tools, such as the map-ping with an opinion lexicon based on WordNet (SentiWordNet, WordNet Affect, MicroWord-Net), in order to automatically annotate all the synonyms and antonyms with the same or the opposite polarity respectively and assigning them some other elements contemplated into the Emo-tiBlog annotation schema.
This would mean an important step forward for saving time during the annotation process and it will also assure a high quality annotation due to the human supervision.8References Balahur A., Steinberger R., Kabadjov M., Zavarella V., van der Goot E., Halkia M., Pouliquen B., and Belyaeva J.
2010.
Sentiment Analysis in the News.
In Proceedings of LREC 2010.
Balahur A., Boldrini E., Montoyo A., Mart?nez-Barco P. 2009.
A Comparative Study of Open Domain and Opinion Question Answering Systems for Fac-tual and Opinionated Queries.
In Proceedings of the Recent Advances in Natural Language Proc-essing.
Balahur A., Montoyo A.
2008.
Applying a Culture Dependent Emotion Triggers Database for Text Valence and Emotion Classification.
In Proceed-ings of the AISB 2008 Symposium on Affective Language in Human and Machine, Aberdeen, Scot-land.
Balahur A., Steinberger R., Rethinking Sentiment Analysis in the News: from Theory to Practice and back.
In Proceeding of WOMSA 2009.
Seville.
Balahur A., Boldrini E., Montoyo A., Mart?nez-Barco P. 2009.
Summarizing Threads in Blogs Using Opinion Polarity.
In Proceedings of ETTS work-shop.
RANLP.
2009.
Boldrini E., Balahur A., Mart?nez-Barco P., Montoyo A.
2009.
EmotiBlog: a fine-grained model for emotion detection in non-traditional textual gen-res.
In Proceedings of WOMSA.
Seville, Spain.
Boldrini E., Fern?ndez J., G?mez J.M., Mart?nez-Barco P. 2009.
Machine Learning Techniques for Automatic Opinion Detection in Non-Traditional Textual Genres.
In Proceedings of WOMSA 2009.
Seville, Spain.
Chaovalit P, Zhou L. 2005.
Movie Review Mining: a Comparison between Supervised and Unsupervised Classification Approaches.
In Proceedings of HICSS-05.
Carletta J.
1996.
Assessing agreement on classification task: the kappa statistic.
Computa-tional Linguistics, 22(2): 249?254.
Cui H., Mittal V., Datar M. 2006.
Comparative Ex-periments on Sentiment Classification for Online Product Reviews.
In Proceedings of the 21st Na-tional Conference on Artificial Intelligence AAAI.
Cerini S., Compagnoni V., Demontis A., Formentelli M., and Gandini G. 2007.
Language resources and linguistic theory: Typology, second language ac-quisition.
English linguistics (Forthcoming), chap-ter Micro-WNOp: A gold standard for the evalua-tion of automatically compiled lexical resources for opinion mining.
Franco Angeli Editore, Milano, IT.
Choi Y., Cardie C., Rilloff E., Padwardhan S. 2005.
Identifying Sources of Opinions with Conditional Random  Fields and Extraction Patterns.
In Pro-ceedings of the HLT/EMNLP.
Dave K., Lawrence S., Pennock, D. ?Mining the Pea-nut Gallery: Opinion Extraction and Semantic Classification of Product Reviews?.
In Proceedings of WWW-03.
2003.Esuli A., Sebastiani F. 2006.
SentiWordNet: A Pub-licly Available Resource for Opinion Mining.
In Proceedings of the 6th International Conference on Language Resources and Evaluation, LREC 2006, Genoa, Italy.
Gamon M., Aue S., Corston-Oliver S., Ringger E. 2005.
Mining Customer Opinions from Free Text.
Lecture Notes in Computer Science.
Goldberg A.B., Zhu J.
2006.
Seeing stars when there aren?t many stars: Graph-based semi-supervised learning for sentiment categorization.
In HLT-NAACL 2006 Workshop on Textgraphs: Graph-based Algorithms for Natural Language Process-ing.
Hu M., Liu B.
2004.
Mining Opinion Features in Cus-tomer Reviews.
In Proceedings of Nineteenth Na-tional Conference on Artificial Intelligence AAAI.
Hatzivassiloglou V., Wiebe J.
2000.
Effects of adjec-tive orientation and gradability on sentence subjec-tivity.
In Proceedings of COLING.
Kim S.M., Hovy E. 2004.
Determining the Sentiment of Opinions.
In Proceedings of COLING.
Mullen T., Collier N. 2006.
Sentiment Analysis Using Support Vector Machines with Diverse Information Sources.
In Proceedings of EMNLP.
2004.
Lin, W.H., Wilson, T., Wiebe, J., Hauptman, A.
?Which Side are You On?
Identifying Perspectives at the Document and Sentence Levels?.
In Proceedings of the Tenth Conference on Natural Language Learn-ing CoNLL.2006.
Ng V., Dasgupta S. and Arifin S. M. 2006.
Examining the Role of Linguistics Knowledge Sources in the Automatic Identification and Classification of Re-views.
In the proceedings of the ACL, Sydney.
Pang B., Lee L., Vaithyanathan S. 2002.
Thumbs up?
Sentiment classification using machine learning techniques.
In Proceedings of EMNLP-02, the Conference on Empirical Methods in Natural Lan-guage Processing.
Riloff E., Wiebe J.
2003.
Learning Extraction Pat-terns for Subjective Expressions.
In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.
Strapparava C. Valitutti A.
2004.
WordNet-Affect: an affective extension of WordNet.
In Proceedings ofthe 4th International  Conference on Language Resources and Evaluation, LREC.
Russell J.A.
1983.
Pancultural aspects of the human conceptual organization of emotions.
Journal of Personality and Social Psychology 45: 1281?8.
Scherer K. R. 2005.
What are emotions?
And how can they be measured?
Social Science Information, 44(4), 693?727.
Stoyanov V. and Cardie C. 2006.
Toward Opinion Summarization: Linking the Sources.
COLING-ACL.
Workshop on Sentiment and Subjectivity in Text.
Stoyanov V., Cardie C., Litman D., and Wiebe J.
2004.
Evaluating an Opinion Annotation Scheme Using a New Multi-Perspective Question and An-9swer Corpus.
AAAI Spring Symposium on Explor-ing Attitude and Affect in Text.
Strapparava and Mihalcea, 2007 - SemEval 2007 Task 14: Affective Text.
In  Proceedings of the ACL.
Turney P. 2002.
Thumbs Up or Thumbs Down?
Se-mantic Orientation Applied to Unsupervised Clas-sification of Reviews.
ACL 2002: 417-424.
Turney P., Littman M. 2003.
Measuring praise and criticism: Inference of semantic orientation from association.
ACM Transactions on Information Systems 21.
Uspensky B.
1973.
A Poetics of Composition.
Univer-sity of California Press, Berkeley, California.
Wiebe J. M. 1994.
Tracking point of view in narra-tive.
Computational Linguistics, vol.
20, pp.
233?287.
Wiebe J., Wilson T. and Cardie C. 2005.
Annotating expressions of opinions and emotions in language.
Language Resources and Evaluation.
Wilson T., Wiebe J., Hwa R. 2004.
Just how mad are you?
Finding strong and weak opinion clauses.
In: Proceedings of AAAI.
Wiebe J., Wilson T. and Cardie C. 2005.
?Annotation Expressions of Opinions and Emotions in Lan-guage.
Language Resources and Evaluation.
Wiebe J., Riloff E. 2005.
Creating Subjective and Objective Sentence Classifiers from Unannotated Texts.
In Proceedings of the 6th International Con-ference on Computational Linguistics and Intelli-gent Text Processing (CICLing).10
