Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 57?65,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsRepresenting words as regions in vector spaceKatrin ErkUniversity of Texas at Austinkatrin.erk@mail.utexas.eduAbstractVector space models of word meaning typi-cally represent the meaning of a word as a vec-tor computed by summing over all its corpusoccurrences.
Words close to this point in spacecan be assumed to be similar to it in meaning.But how far around this point does the regionof similar meaning extend?
In this paper wediscuss two models that represent word mean-ing as regions in vector space.
Both represen-tations can be computed from traditional pointrepresentations in vector space.
We find thatboth models perform at over 95% F-score ona token classification task.1 IntroductionVector space models of word meaning (Lund andBurgess, 1996; Landauer and Dumais, 1997; Lowe,2001; Jones and Mewhort, 2007; Sahlgren and Karl-gren, 2005) represent words as points in a high-dimensional semantic space.
The dimensions of thespace represent the contexts in which each targetword has been observed.
Distance between vec-tors in semantic space predicts the degree of seman-tic similarity between the corresponding words, aswords with similar meaning tend to occur in simi-lar contexts.
Because of this property, vector spacemodels have been used successfully both in com-putational linguistics (Manning et al, 2008; Snowet al, 2006; Gorman and Curran, 2006; Schu?tze,1998) and in cognitive science (Landauer and Du-mais, 1997; Lowe and McDonald, 2000; McDon-ald and Ramscar, 2001).
Given the known problemswith defining globally appropriate senses (Kilgarriff,1997; Hanks, 2000), vector space models are espe-cially interesting for their ability to represent wordmeaning without relying on dictionary senses.Vector space models typically compute one vec-tor per target word (what we will call word type vec-tors), summing co-occurrence counts over all corpustokens of the target.
If the target word is polyse-mous, the representation will constitute a union overthe uses or senses of the word.
Such a model doesnot provide information on the amount of variancein each dimension: Do values on each dimensionvary a lot across occurrences of the target?
Also, itdoes not provide information on co-occurrences offeature values in occurrences of the target.
To en-code these two types of information, we study richermodels of word meaning in vector space beyond sin-gle point representations.Many models of categorization in psychologyrepresent a concept as a region, characterized byfeature vectors with dimension weights (Smith etal., 1988; Hampton, 1991; Nosofsky, 1986).
Tak-ing our cue from these approaches, we study twomodels that represent a word as a region in vectorspace rather than a point.
The first model is onethat we have recently introduced for representing hy-ponymy in vector space (Erk, 2009).
We now testits suitability as a general region model for wordmeaning.
This model can be viewed as a prototype-style model that induces a region surrounding a cen-tral vector.
As it does not record co-occurrences offeature values, we contrast it with a second model,an exemplar-style model using a k-nearest neighboranalysis, which can represent both degree of vari-ance in each dimension and value co-occurrences.Both models induce regions representations with-out labeled data.
The idea on which both modelsare based is to use word token vectors to estimate a57region representation.
We evaluate the two regionmodels on a task of token classification: Given apoint in vector space, the task is predict the wordof which it is a token vector.By representing the meaning of words as regionsin vector space, we can describe areas in whichpoints encode similar meanings.
This description isflexible, depending on the target word in question,rather than uniform for all words through a fixeddistance threshold from the target?s type vector.
Onepossible application of region models of word mean-ing is in the task of determining the appropriatenessof a paraphrase in a given context (Connor and Roth,2007).
This task is highly relevant for textual entail-ment (Szpektor et al, 2008).
Current vector spaceapproaches typically compare the target word?s to-ken vector to the type vector of the potential para-phrase (Mitchell and Lapata, 2008; Erk and Pado,2008).
A region model could instead test the tar-get?s token vector for inclusion in the potential para-phrase?s region.2 Related workThis section discusses existing vector space modelsand compares vector space models in computationallinguistics to feature-based models of human con-cept representation in psychology.Vector space models.
Vector space models rep-resent the meaning of a target word as a vector in ahigh-dimensional space (Lund and Burgess, 1996;Landauer and Dumais, 1997; Sahlgren and Karl-gren, 2005; Pado?
and Lapata, 2007; Jones and Me-whort, 2007).
Dimensions stand for context itemswhich which the target word has been observedto co-occur, for example other words (Lund andBurgess, 1996) or syntactic paths (Pado?
and Lapata,2007).
In the simplest case, the value on a dimensionis the raw co-occurrence count between the targetword and the context item for which the dimensionstands.
Raw counts are often transformed, for ex-ample using a log-likelihood transformation (Lowe,2001).
Sometimes the vector space as a whole istransformed using dimensionality reduction (Lan-dauer and Dumais, 1997).In NLP, vector space models have featured mostprominently in information retrieval (Manning etal., 2008), but have also been used for ontologylearning (Lin, 1998; Snow et al, 2006; Gormanand Curran, 2006) and word sense-related tasks(McCarthy et al, 2004; Schu?tze, 1998).
In psy-chology, vector space models have been used tomodel synonymy (Landauer and Dumais, 1997;Pado?
and Lapata, 2007), lexical priming phenom-ena (Lowe and McDonald, 2000), and similarityjudgments (McDonald and Ramscar, 2001).
Therehave also been studies on inducing hyponymy in-formation from vector space representations.
Gef-fet and Dagan (2005) use a dimension re-weightingscheme, then predict entailment when the mosthighly weighted dimensions of two verbs stand ina subset relation.
However, they find that while re-call of this method is good (whenever some sensesof two words stand in an entailment relation, top-weighted dimensions of their vectors stand in a sub-set relation), precision is problematic.
Weeds, Weirand McCarthy (2004) introduce the notion of distri-butional generality (x is more distributionally gen-eral than y if x occurs in more contexts than y) andfind that for hyponym-hypernym pairs from Word-Net, hyponyms are typically more distributionallygeneral.
(As they study only word pairs that areknown to be related by hyponymy, they test for recallbut not precision.)
Erk (2009) suggests that while itmay not be possible to induce hyponymy informa-tion from a vector space representation, it is possibleto encode it in a vector space representation after ithas been obtained through some other means.Vector space models of word tokens.
Vectorspace models have mostly been used to representthe meaning of a word type by summing its co-occurrence counts over a complete corpus.
Thereare several approaches to computing vectors for in-dividual word tokens.
All of them compute wordtype vectors first, then combine them into token vec-tors.
Kintsch (2001) and Mitchell and Lapata (2008)combine the target?s type vector with that of a sin-gle word in the target?s syntactic context.
Lan-dauer and Dumais (Landauer and Dumais, 1997)and Schu?tze (1998) combine the type vectors ofall the words surrounding the target token.
Erkand Pado?
(2008) combine the target?s type vectorwith a vector representing the selectional preferenceof a single word in the target?s syntactic context.Smolensky (1990) focuses on integrating syntacticinformation in the vector representation rather than58on representing the lexical meaning of the target.Feature-based models of human concept rep-resentation.
Many models of human concept rep-resentation in psychology are based on vectors offeatures (e.g.
(Smith et al, 1988; Hampton, 1991;Nosofsky, 1986)).
Features in these models aretypically weighted to represent their importance tothe concept in question.
Similarity to a given fea-ture vector is usually taken to decrease exponentiallywith distance from that vector, following Shepard?slaw (Shepard, 1987).
Categorization involves com-petition between categories.
Feature-based modelsof human concept representation can be broadly cat-egorized into prototype models, which represent aconcept by a single summary representation, and ex-emplar models, which assume that categorization isby comparison to remembered exemplars.
As an ex-ample of a feature-based model of concept represen-tation, we show the definition of Nosofsky?s (1986)Generalized Context Model (GCM).
This exemplarmodel estimates the probability of categorizing anexemplar ~e as a member of a concept C asP (C|~e) =?~?
?C w~?sim(~?,~e)?concept C??~??C?
w~?sim(~?,~e)(1)where the concept C is a set of remembered exem-plars, w~?
is an exemplar weight, and the similaritysim(~?,~e) between ~?
and ~e is defined assim(~?,~e) = exp(z ?
?dimension iwi(?i ?
ei)2) (2)Here, z is a general sensitivity parameter, wi is aweight for dimension i, and ?i, ei are the valuesof ~?
and ~e on dimension i.
This model shows allthe properties listed above: It has weighted dimen-sions through the wi.
It incorporates Shepard?s lawthrough the exponential relation between sim andthe sum of squared value distances wi(?i ?
ei)2.Competition between categories arises through thenormalization of ~e?s similarity to C by the similar-ity to all other categories in Eq.
(1).
While feature-based models of concept representation talk aboutconcepts rather than word meaning, Murphy (2002)argues that there is ?overwhelming empirical evi-dence for the conceptual basis of word meaning?through experimental results on conceptual phenom-ena that have also been shown to hold for words.Ga?rdenfors (2004) proposes a model that repre-sents concepts as convex regions in a conceptualspace.
Feature structures play no central role in thismodel, but Ga?rdenfors suggests that concepts maybe represented by a central point, such that cate-gorization could simply be determining the nearestcentral point (without positing an exponential rela-tion between distance and similarity).3 ModelsIn this section, we present two models for represent-ing word meaning as regions in vector space.The centered model.
The first model that we de-fine, which we call the centered model, is prototype-like.
As the representation for a target word, it in-duces a region surrounding the target?s type vec-tor (Erk, 2009).
Let w be the target word and~w its type vector.
Let ~x be a point in the samevector space.
To predict whether ~x represents thesame meaning as ~w, we estimate the probabilityP (IN(~x, ~w)) that ~x is in the region around ~w, usinga log-linear model:P (IN(~x, ~w)) = 1Z exp(?i?
INi fi(~x, ~w)) (3)where the fi are features that characterize the point~x, and the ?
INi are weights identifying the impor-tance of the different features for the class IN.
Z is anormalizing factor that ensures that P is a probabilitydistribution: If P (OUT(~x, ~w)) = 1?P (IN(~x, ~w)) isthe probability that ~x is not in the region around ~w,with associated weights ?OUTi for the same featuresfi, then Z =?`=IN,OUT exp(?i ?`i fi(~x, ~w)).We define the features fi as follows: If ~w =?w1, .
.
.
, wn?, we define the feature fi(~x, ~w), for1 ?
i ?
n, as the squared distance between ~w and ~xon dimension i:fi(~x) = (wi ?
xi)2 (4)This model, like feature-based models of catego-rization from psychology, has weighted dimensionsthrough the ?i.
It follows Shepard?s law ?
the ex-ponential relation between similarity and distance ?through the exponential function in Eq.
(3).
Compe-tition between categories is implicit in the estimationof P (OUT(~x, ~w)).59Most of the weights ?
INi can reasonably be ex-pected to be negative, since a negative ?
INi indicatesthat membership of a point ~x in the w-region getsless likely as the distance (wi?xi)2 increases.
If ?
INihas a large negative value, categorization is highlysensitive to changes in the ith dimension.
If on theother hand, ?
INi is negative but close to zero, thismeans that vector entries in dimension i can varygreatly without much influence on categorization.The parameters ?
INi and ?OUTi need to be estimatedfrom training data.
Although the log-likelihoodmodel is a supervised learning scheme, we do notneed to take recourse to labeled data.
Instead, weuse token vectors: Token vectors of w will serveas positive training data for estimating P (IN(~x, ~w)),and token vectors of other words than w will con-stitute negative training data.
The amount of pre-processing needed depends on the approach to com-puting token vectors that we use.
We will use anapproach that combines w?s type vector with thatof a single word in its syntactic context.
This pre-supposes a syntactic parse of the corpus.
Note thatwe could just as well have used a Schu?tze-style ap-proach, which does not rely on parsing.The distributed model.
The second model thatwe consider is an exemplar-style, instance-basedmodel.
The simplest instance-based models are k-nearest neighbor classifiers, which assign to a testitem the majority label of its k nearest neighborsamong the training items.
We will here use a verysimple model, doing k nearest neighbor classifica-tion where the distance between two vectors ~w and~x is the sum of dimension distances ?i with?i = ?i|wi ?
xi|maxi ?minimaxi and mini are the maximum and minimumvalues observed for dimension i, and ?i is a fea-ture weight.
We use a standard feature weightingmethod, gain ratio, which is information gain nor-malized by the entropy of feature values.
Informa-tion gain on its own has a bias towards features withmany values, which gain ratio attenuates in favor offeatures with lower entropy:?i =H(C) ?
?y?val(i) P (y)H(C|y)?
?y?val(i) P (y) log2 P (y)(5)for the set C = {IN, OUT} of classes and sets val(i)of values seen for dimension i.
We call this the dis-tributed model.
As with the centered model, wecompare it to models of concept representation: Ithas weighted dimensions (Eq.
(5)), and it incorpo-rates competition between categories by storing bothpositive and negative exemplars and categorizing ac-cording to the majority among the k nearest neigh-bors.
However, it does not implement Shepard?s law.It additionally differs from the GCM (Eq.
(1)) in bas-ing categorization on the k nearest neighbors ratherthan summed similarity to all neighbors.Like the centered model, the distributed modelneeds both positive and negative training data.Again, labeled data is not necessary as we can useword token vectors.
Positive training data consists oftokens of the target word, and tokens of other wordsare negative training data.
This model does not makeuse of the target?s type vector.Above we have discussed two pieces of informa-tion that region models can encode and that are hardto encode in single-point models of word meaning:variance in each dimension and co-occurrence offeature values.
The centered model encodes the vari-ance in the values of each dimension through theweights ?
INi , but it does not retain information onfeature values of different dimensions that tend toco-occur.
The distributed model encodes both vari-ance in each dimension and co-occurrence of fea-ture values through the remembered exemplars.
Sothe centered model should do well for monosemouswords, since it seems reasonable that their tokenvectors should form a single region around the typevector.
For polysemous words, token vectors couldbe more scattered in semantic space, in which casethe distributed model should do better.Note that neither the centered nor the distributedmodel is a clustering model: Both are supervisedmodels learning the distinctions between tokens ofthe target word and other vectors.
Neither of themgroups vectors in an unsupervised fashion.Hard versus soft region boundaries.
In thecurrent paper, we consider only regions with sharpboundaries.
In the centered model, a point ~xwill be considered a member of the w-region ifP (IN(~x, ~w)) ?
0.5.
In the distributed model, ~xwill be considered a member if the majority of itsk nearest neighbors are members.
However, it is im-60portant that both models can also be used to repre-sent regions with soft boundaries.
In the centeredmodel, we can use P (IN(~x, ~w)) without a thresh-old.
In the distributed model, we can use the fractionof k that are positive instances, or we can computesummed similarity to the positive instances like theGCM does.
So both models can be used to estimatedegrees of membership in a target word?s region.4 Task, Data, and ImplementationThis section describes the task used for evaluation,the data, and the implementation of the models.Task.
The main task will be for a model trainedon a target word w to predict, for a given point ~x insemantic space, whether ~x is a token vector of w ornot.
This task is a direct test of whether the regioninduced for w succeeds in characterizing the regionin semantic space in which tokens of w will occur.As an example, consider the target word super-sede: Region models of supersede will be trainedon tokens of supersede in a training dataset.
Onesuch token is supersede knowledge (i.e., knowledgeas the direct object of supersede).
We compute a to-ken vector for this occurrence by combining the typevectors of supersede and knowledge.
After train-ing a model, we test it on tokens occurring in a testdataset.
Positive test items are tokens of supersede,and negative test items are tokens of other words, forexample guard.
An example of a positive test itemis supersede collection.
The test items will consistsolely of tokens that do not occur in the training data.Data.
We focus on verbs in this paper since para-phrase appropriateness for verbs is an important taskin the context of textual entailment.
Since we sus-pect that the centered model will be better suited tomodeling monosemous words while the distributedmodel should do equally well on monosemous andpolysemous words, we first test a group of monose-mous verbs, then a mixed group.
We use WordNet3.0 to form the two groups.
The first group consistsof all verbs listed in WordNet 3.0 as being monose-mous.
We refer to this set as Mon.
Since we alsowant to compare the two region models on the taskof hyponymy encoding (Erk, 2009), we use as ourset of mixed monosemous and polysemous verbs theverbs used there to test hyponymy encoding: the setof all verbs that are hypernyms of the Mon verbs ac-cording to WordNet 3.0.
We call this set Hyp.We use the British National Corpus (BNC) tocompute the vector space and as our source of tar-get word tokens.
We need token vectors for trainingthe two region models, and we need separate, previ-ously unseen token vectors as test data.
So we splitthe written portion of the BNC in half at random,leaving files intact.
This yielded a training and a testset.
We computed word type vectors from the train-ing half of the BNC, using a syntax-based vectorspace (Pado?
and Lapata, 2007) of 500 dimensions,with raw co-occurrence counts as dimension values.We used the dv package1 to compute type vectorsfrom a Minipar (Lin, 1993) parse of the BNC.We computed token vectors by combining the tar-get verb?s type vector with the type vector of theword occurring as the target?s direct object.
Wetest three methods for combining type vectors: First,component-wise multiplication (below called mult),which showed best results in Mitchell and Lapata?s(2008) analysis.
Second, component-wise averag-ing (below called avg), a variant of type vector addi-tion, a method often used for computing token vec-tors.
Third, we consider component-wise minimum(min), which can be viewed as a kind of intersectionof the contexts with which the two words have beenobserved.
We used the training half of the BNC toextract training tokens of the target verbs, and thetest half for extracting test tokens.
We used onlythose verb/object pairs as test tokens that did not alsooccur in the training data.We restricted the set of verbs to avoid data sparse-ness issues, using only verbs that occurred with atleast 50 different direct objects in the training part ofthe BNC.
The direct objects, in turn, were restrictedto exclude overly rare and overly frequent (and thuspotentially uninformative) items.
We restricted thedirect objects to those with no more than 6,500 andno less than 270 occurrences in Mon ?
Hyp.
Theresulting set Mon consisted of 120 verbs, and Hypconsisted of 430 verbs.Model implementation.
We implemented thecentered model using the OpenNLP maxent pack-age2, and the distributed model using TiMBL3 in theIB1 setting with k = 5 nearest neighbors.
We use bi-1http://www.nlpado.de/?sebastian/dv.html2http://maxent.sourceforge.net/3http://ilk.uvt.nl/timbl/61centered distributedPrec Rec F Prec Rec Fmult 100 73.2 84.5 29.4 47.5 36.3avg 99.6 91.3 95.3 71.1 99.9 83.1min 97.9 85.4 91.2 21.0 90.3 34.1Table 1: Results: token classification for monosemousverbs.
Random baseline: Prec 0.8, Rec 49.8, F 1.6.nary models throughout, such that the classificationtask is always between IN and OUT.
In training andtesting, each token vector was presented to a modelonly once, ignoring the frequency of direct objects.5 ExperimentsThis section reports on experiments that test the per-formance of the two region models of word meaningin vector space that we have presented in Sec.
3, thecentered and the distributed model.Experiment 1: Token classification formonosemous verbsIn the first experiment, we test whether the tworegion models can identify novel tokens of themonosemous verbs in Mon.
The task is the one de-scribed in Sec.
4.
We focus on monosemous verbsfirst because we suspect that the centered modelshould do better here than on polysemous verbs.Both models were trained using token vectors com-puted from the training half of the BNC.
Token vec-tors of the target verb were treated as positive data,and token vectors of other verbs as negative data.4We used resampling to restrict the number of nega-tive items used during training, using 3% of the neg-ative items, randomly sampled.5 We use for test-ing only those direct objects that do not also ap-pear in the training data, yielding 6,339 positive and1,396,552 negative test items summed over all tar-get verbs.
The case of supersede discussed in Sec.
4is an example of a monosemous verb according toWordNet 3.0.Table 1 summarizes precision, recall and F-scoreresults.
Both models easily beat the random base-4This simplification breaks down for 6 of the 120 verbs(5%), which are in fact synonyms.
We consider this an accept-able level of noise.5The number of 3% was determined on a development setconstructed by further splitting the training set into training anddevelopment portion.centered distributedfreq.
Prec Rec F Prec Rec Fmult 50-100 100 59.3 74.5 20.8 47.2 28.9100-200 100 89.4 94.4 57.4 49.7 53.2200-500 100 97.4 98.7 92.1 41.1 56.9avg 50 - 100 99.5 86.6 92.6 61.6 99.8 76.2100-200 99.7 96.6 98.1 86.3 100 92.6200-500 100 100 100 99.1 100 99.6min 50-100 100 82.9 90.6 17.9 92.6 30.1100-200 98.2 88.2 93.0 25.4 89.2 39.6200-500 86.4 90.3 88.3 42.9 80.0 55.9Table 2: Results: token classification for monosemousverbs, by target frequencycentered distributed# senses Prec Rec F Prec Rec Fall 100 92.9 96.3 99.6 99.8 99.71 100 86.1 92.5 99.0 99.5 99.22-5 100 90.8 95.2 99.4 99.6 99.56-10 100 93.5 96.7 99.9 99.9 99.911-20 100 96.6 98.3 100 100 100?
21 100 99.5 99.7 100 100 100Table 3: Results: Token classification for polysemousverbs, avg token computation.
Random baseline: Prec8.2, Rec 50.4, F 14.0.line.
The centered model shows better performanceoverall than the distributed one, and the avg methodof computing token vector worked best for bothmodels.
The centered model has extremely highprecision throughout, while the distributed modelhas better recall for conditions avg and min.
Ta-ble 2 breaks down the results by the frequency ofthe target verb, measured in the number of differentverb/object tokens in the training data.Experiment 2: Token classification forpolysemous verbsWe now test how the centered and distributed mod-els fare on the same task, but with a mixture ofmonosemous and polysemous verbs.
We use theverbs in Hyp, which in WordNet 3.0 have on aver-age 6.79 senses.
For example, follow is a WordNethypernym of the monosemous supersede.
It has 24senses, among them comply and postdate.
Amongits training tokens are follow instruction and followdinner.
The first is probably the comply sense of fol-low, the second the postdate sense.
An example of atest token (i.e., occurring in the test but not the train-62ing data) is follow tea.
(If tea is tea time, this is alsothe postdate sense.
)We computed type vectors for the Hyp verbs andtheir objects from the training half of the BNC, andcomputed token vectors using the best method fromExp.
1, avg.
Again, we use for testing only those to-kens that do not also appear in the training data.
Dueto the larger amount of data, we used resampling inthe training as well as the test data, using only a ran-dom 3% of negative tokens for testing.
This yielded25,736 positive and 670,630 negative test items.Table 3 shows the results: The first line has theoverall results, and the following lines break downthe results by the number of senses each lemma hasin WordNet 3.0.6 Both models, centered and dis-tributed, easily beat the random baseline.
The cen-tered model has comparable results for the Hyp asfor the Mon verbs (cf.
Table 1), while the distributedmodel has better results for this dataset, and betterresults than the centered model.
The centered modelshows a marked improvement in recall as the num-ber of senses increases.Experiment 3: Encoding hyponymyWe first proposed the centered model as a methodfor encoding hyponymy information in a vectorspace representation (Erk, 2009).
Hyponymy infor-mation from another source, in this case WordNet,was encoded in a centered region representation ofa target verb by using tokens of the verb itself aswell as tokens from its direct hyponyms in trainingthe model.
Negative data consisted of training datatokens that were not occurrences of the target verbor its direct hyponyms.
In the example of the verbfollow, the positive training data would contain to-kens of follow along with tokens of supersede andguard, another direct hyponym of follow.
Negativetraining tokens would include, for example, tokensof the word destroy.
The resulting centered model,in this case of follow, was then tested on previouslyunseen tokens, for example guard purpose (a tokenof a hyponym) and destroy lawn (a token of a non-hyponym), with the task of predicting whether theywere tokens of direct hyponyms of follow or not.6The one-sense items in Table 3 are a 43 verb subset of Mon.The reason for the difference in performance in comparison toTable 1 is unclear, as the two sets have similar distributions oflemma frequencies.centered distributedPrec Rec F Prec Rec F95.2 43.4 59.6 68.3 58.6 63.1Table 4: Results: Identifying hyponyms based on ex-tended hypernym representations, avg token computa-tion.
Random baseline: Prec 11.0, Rec 50.2, F 18.0We now repeat this experiment with the dis-tributed model.
We use the direct hypernyms of theverbs in Mon, with the same frequency restrictionsas above.
We refer to this set of 273 verbs as DHyp.We train one centered and one distributed model foreach verb w in DHyp.
Positive training tokens fortraining a model for a verb w ?
DHyp are tokensof w and of all sufficiently frequent children of win WordNet 3.0.
Negative training tokens are to-kens of other verbs in DHyp and their children.
Weagain sample a random 3% of the negative data dur-ing both training and testing.Table 4 shows the results.
Both models again beatthe baseline.
The distributed model shows slightlybetter results overall, while the centered model hasby far the highest precision.DiscussionPerformance on monosemous verbs.
For themonosemous verbs in Exp.
1, both models succeedin inducing regions that characterize tokens of a tar-get word with high precision as well as high recall.The extremely high precision of the centered modelshows that in general the region surrounding the typevector does not contain any tokens of other verbsthan the target.
Concerning the distributed model, itis to be expected that in min, and even more so inmult, dimension values will vary more than in avg;this could explain the huge difference between avgand the other two conditions for this model.
It isinteresting to note that the centered model achievesbetter precision, while the distributed model reacheshigher recall.
Maybe it will be possible in later mod-els to combine their strengths.
The breakdown byfrequency bands in Table 2 shows that in mult andavg, the models get strictly better with more data,while min has a precision/recall tradeoff.Performance on polysemous verbs.
For the pol-ysemous verbs in Exp.
2, like for the monosemousverbs in Exp.
1, both models show excellent per-63formance in distinguishing tokens of the target verbfrom tokens of other verbs.7 The distributed modelsurpasses the centered one on this dataset.
However,it is not clear that this is because the contiguous re-gion that the centered model infers is inappropriatefor polysemous verbs.
After all the centered model,too, achieves better performance on this dataset thanon Mon.
The fact that results get better with the de-gree of polysemy, at first surprising, may indicatethat the centered model draws an overly tight bound-ary around the type vector and that this boundaryimproves when token vectors differ more, and areat greater distance from the type vector, as shouldbe the case for more polysemous lemmas.
Anotherpossible reason for the better performance of bothmodels is that this dataset is larger and in particularprovides a larger set of negative data.Encoding external information in a regionmodel.
In the hyponymy encoding task in Exp.
3,both models successfully encode hyponymy infor-mation in vector space representations.
The cen-tered model manages to derive a high-precision re-gion around the type vector, while the distributedmodel makes use of outliers in the training data toachieve higher recall.Comparing region representations to pointrepresentations.
We now compare the two regionmodels to existing variants of point-based vectorspace models.
Both region models have dimen-sion weights, whose function is somewhat similar tothat of log-likelihood or mutual information trans-formations of raw co-occurrence counts: to estimatethe importance of each dimension for characteriz-ing the target word in question.
However, dimensionweights in region models are computed based on to-ken vectors, while all co-occurrence count transfor-mations work on type vectors.The distributed model additionally has the abilityto represent typical co-occurrences of feature valuesbecause the training tokens are remembered in theirentirety.
The most similar mechanism in point-basedvector space models is probably dimensionality re-duction, which strives to find latent dimensions thatexplain most of the variance in the data.
But again,dimensionality reduction uses type vectors while the7The near-perfect performance in particular of the dis-tributed model has been confirmed on a separate noun dataset.distributed model stores token vectors, which canshow more variance than the type vectors alone.Applications of region models.
Region modelsof word meaning are interesting for the task of test-ing the appropriateness of paraphrases in context.Previous models either used competition betweenparaphrase candidates or a global similarity thresh-old to decide whether to accept a paraphrase can-didate (Mitchell and Lapata, 2008; Szpektor et al,2008).
A region model of word meaning used forthe same task would still require a threshold, in thiscase a threshold on membership probability, but theregions for which membership is tested could dif-fer in their size, and the extent of each region wouldbe learned individually from the data.
To use themodel, for example to test whether trickle is a goodparaphrase for run in the color ran, we would testwhether the sentence-specific token vector for runfalls into the region of trickle.6 Conclusion and outlookIn this paper, we have proposed using region modelsfor word meaning in vector space, predicting regionsin space in which points can be assumed to carry thesame meaning.
We have studied two models, theprototype-like centered models and the exemplar-like distributed model, both of which are learnedwithout labeled data by making use of token vectorsof the target word in question.
Both models showexcellent performance, with F-scores of 83%-99%,on the task of identifying previously unseen occur-rences of the target word.Our aim is to to test the usability of region mod-els for predicting paraphrase appropriateness in con-text.
The next step towards that will be to test regionmodels on the task of identifying synonym tokens.Acknowledgements.
Many thanks to JasonBaldridge, David Beaver, Graham Katz, AlexanderKoller, Ray Mooney, and Manfred Pinkal for helpfuldiscussions.
This work was supported by the Mor-ris Memorial Grant from the New York CommunityTrust.ReferencesM.
Connor and D. Roth.
2007.
Context sensitive para-phrasing with a single unsupervised classifier.
In Pro-ceedings of ECML-07, Warsaw, Poland.64K.
Erk and S. Pado.
2008.
A structured vector spacemodel for word meaning in context.
In Proceedings ofEMNLP-08, Hawaii.K.
Erk.
2009.
Supporting inferences in semantic space:representing words as regions.
In Proceedings ofIWCS-8, Tilburg, Netherlands.P.
Ga?rdenfors.
2004.
Conceptual spaces.
MIT press,Cambridge, MA.M.
Geffet and I. Dagan.
2005.
The distributional inclu-sion hypotheses and lexical entailment.
In Proceed-ings of ACL-05, Ann Arbor, MI.J.
Gorman and J. R. Curran.
2006.
Scaling distributionalsimilarity to large corpora.
In Proceedings of ACL ?06,Sydney.J.
A. Hampton.
1991.
The combination of prototype con-cepts.
In P. Schwanenflugel, editor, The psychology ofword meanings.
Lawrence Erlbaum Associates.P.
Hanks.
2000.
Do word meanings exist?
Computersand the Humanities, 34(1-2):205?215(11).M.
Jones and D. Mewhort.
2007.
Representing wordmenaing and order information in a composite holo-graphic lexicon.
Psychological Review, 114:1?37.A.
Kilgarriff.
1997.
I don?t believe in word senses.
Com-puters and the Humanities, 31(2):91?113.W.
Kintsch.
2001.
Predication.
Cognitive Science,25:173?202.T.
Landauer and S. Dumais.
1997.
A solution to Platosproblem: the latent semantic analysis theory of ac-quisition, induction, and representation of knowledge.Psychological Review, 104(2):211?240.D.
Lin.
1993.
Principle-based parsing without overgen-eration.
In Proceedings of ACL?93, Columbus, Ohio.D.
Lin.
1998.
Automatic retrieval and clustering of sim-ilar words.
In COLING-ACL98, Montreal, Canada.W.
Lowe and S. McDonald.
2000.
The direct route: Me-diated priming in semantic space.
In Proceedings ofthe Cognitive Science Society.W.
Lowe.
2001.
Towards a theory of semantic space.
InProceedings of the Cognitive Science Society.K.
Lund and C. Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instruments,and Computers, 28:203?208.C.
D. Manning, P. Raghavan, and H. Schu?tze.
2008.
In-troduction to Information Retrieval.
Cambridge Uni-versity Press.D.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004.Finding predominant senses in untagged text.
In Pro-ceedings of ACL?04, Barcelona, Spain.S.
McDonald and M. Ramscar.
2001.
Testing the dis-tributional hypothesis: The influence of context onjudgements of semantic similarity.
In Proceedings ofthe Cognitive Science Society.J.
Mitchell and M. Lapata.
2008.
Vector-based modelsof semantic composition.
In Proceedings of ACL-08,Columbus, OH.G.
L. Murphy.
2002.
The Big Book of Concepts.
MITPress.R.
M. Nosofsky.
1986.
Attention, similarity, and theidentification-categorization relationship.
Journal ofExperimental Psychology: General, 115:39?57.S.
Pado?
and M. Lapata.
2007.
Dependency-based con-struction of semantic space models.
ComputationalLinguistics, 33(2):161?199.M.
Sahlgren and J. Karlgren.
2005.
Automatic bilinguallexicon acquisition using random indexing of parallelcorpora.
Journal of Natural Language Engineering,Special Issue on Parallel Texts, 11(3).H.
Schu?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1).R.
Shepard.
1987.
Towards a universal law ofgeneralization for psychological science.
Science,237(4820):1317?1323.E.
E. Smith, D. Osherson, L. J. Rips, and M. Keane.1988.
Combining prototypes: A selective modifica-tion model.
Cognitive Science, 12(4):485?527.P.
Smolensky.
1990.
Tensor product variable binding andthe representation of symbolic structures in connec-tionist systems.
Artificial Intelligence, 46:159?216.R.
Snow, D. Jurafsky, and A. Y. Ng.
2006.
Semantictaxonomy induction from heterogenous evidence.
InProceedings of COLING/ACL?06.I.
Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.2008.
Contextual preferences.
In Proceedings ofACL-08, Columbus, OH.J.
Weeds, D. Weir, and D. McCarthy.
2004.
Character-ising measures of lexical distributional similarity.
InProceedings of COLING-04, Geneva, Switzerland.65
