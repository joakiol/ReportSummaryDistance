Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 161?168, Vancouver, October 2005. c?2005 Association for Computational LinguisticsLocal Phrase Reordering Models for Statistical Machine TranslationShankar Kumar, William Byrne?Center for Language and Speech Processing, Johns Hopkins University,3400 North Charles Street, Baltimore, MD 21218, U.S.A.Machine Intelligence Lab, Cambridge University Engineering Department,Trumpington Street, Cambridge CB2 1PZ, U.K.skumar@jhu.edu , wjb31@cam.ac.ukAbstractWe describe stochastic models of localphrase movement that can be incorpo-rated into a Statistical Machine Transla-tion (SMT) system.
These models pro-vide properly formulated, non-deficient,probability distributions over reorderedphrase sequences.
They are imple-mented by Weighted Finite State Trans-ducers.
We describe EM-style parameterre-estimation procedures based on phrasealignment under the complete translationmodel incorporating reordering.
Our ex-periments show that the reordering modelyields substantial improvements in trans-lation performance on Arabic-to-Englishand Chinese-to-English MT tasks.
Wealso show that the procedure scales as thebitext size is increased.1 IntroductionWord and Phrase Reordering is a crucial componentof Statistical Machine Translation (SMT) systems.However allowing reordering in translation is com-putationally expensive and in some cases even prov-ably NP-complete (Knight, 1999).
Therefore anytranslation scheme that incorporates reordering mustnecessarily balance model complexity against theability to realize the model without approximation.In this paper our goal is to formulate models of lo-cal phrase reordering in such a way that they can beembedded inside a generative phrase-based model?
This work was supported by an ONR MURI GrantN00014-01-1-0685.of translation (Kumar et al, 2005).
Although thismodel of reordering is somewhat limited and can-not capture all possible phrase movement, it formsa proper parameterized probability distribution overreorderings of phrase sequences.
We show that withthis model it is possible to perform Maximum APosteriori (MAP) decoding (with pruning) and Ex-pectation Maximization (EM) style re-estimation ofmodel parameters over large bitext collections.We now discuss prior work on word and phrasereordering in translation.
We focus on SMT systemsthat do not require phrases to form syntactic con-stituents.The IBM translation models (Brown et al, 1993)describe word reordering via a distortion model de-fined over word positions within sentence pairs.
TheAlignment Template Model (Och et al, 1999) usesphrases rather than words as the basis for transla-tion, and defines movement at the level of phrases.Phrase reordering is modeled as a first order Markovprocess with a single parameter that controls the de-gree of movement.Our current work is inspired by the block(phrase-pair) orientation model introduced by Till-mann (2004) in which reordering allows neighbor-ing blocks to swap.
This is described as a sequenceof orientations (left, right, neutral) relative to themonotone block order.
Model parameters are block-specific and estimated over word aligned trained bi-text using simple heuristics.Other researchers (Vogel, 2003; Zens and Ney,2003; Zens et al, 2004) have reported performancegains in translation by allowing deviations frommonotone word and phrase order.
In these cases,1610c 4c 5c0d 1d1v 2v 3v 4v 5v 6v 7v1f 2f 3f 4f 5f 6f 7f 8f 9f2d 3d 4d 5d2c 3c1cx 1 x 2 x 3 x 4 x 51e 5e 7e2e 3e 4e 6e 9e8eu 1 u 2 u 3 u 4 u 5y 1 y 5y 4y 3y 2doivent de_25_%exportationsgrains fl?chirexportations grains de_25_%doivent fl?chir1les exportations deles  exportations  de  grains  doivent  fl?chir de  25  %grains doivent fl?chir de_25_%1.exportations doiventgrains fl?chir de_25_%grain exports are_projected_to by_25_%grain  exports  are  projected  to  fall  by  25  %SentencefallSource LanguageTarget Language SentenceFigure 1: TTM generative translation process; here,I = 9,K = 5, R = 7, J = 9.reordering is not governed by an explicit probabilis-tic model over reordered phrases; a language modelis employed to select the translation hypothesis.
Wealso note the prior work of Wu (1996), closely re-lated to Tillmann?s model.2 The WFST Reordering ModelThe Translation Template Model (TTM) is a genera-tive model of phrase-based translation (Brown et al,1993).
Bitext is described via a stochastic processthat generates source (English) sentences and trans-forms them into target (French) sentences (Fig 1 andEqn 1).P (fJ1 , vR1 , dK0 , cK0 , yK1 , xK1 , uK1 ,K, eI1) =P (eI1)?Source Language Model GP (uK1 ,K|eI1)?Source Phrase Segmentation WP (xK1 |uK1 ,K, eI1)?Phrase Translation and Reordering RP (vR1 , dK0 , cK0 , yK1 |xK1 , uK1 ,K, eI1)?Target Phrase Insertion ?P (fJ1 |vR1 , dK0 , cK0 , yK1 , xK1 , uK1 ,K, eI1)Target Phrase Segmentation ?
(1)The TTM relies on a Phrase-Pair Inventory (PPI)consisting of target language phrases and theirsource language translations.
Translation is mod-eled via component distributions realized as WFSTs(Fig 1 and Eqn 1) : Source Language Model (G),Source Phrase Segmentation (W ), Phrase Transla-tion and Reordering (R), Target Phrase Insertion(?
), and Target Phrase Segmentation (?)
(Kumar etal., 2005).TTM Reordering Previously, the TTM was for-mulated with reordering prior to translation; here,we perform reordering of phrase sequences follow-ing translation.
Reordering prior to translation wasfound to be memory intensive and unwieldy (Kumaret al, 2005).
In contrast, we will show that the cur-rent model can be used for both phrase alignmentand translation.2.1 The Phrase Reordering ModelWe now describe two WFSTs that allow local re-ordering within phrase sequences.
The simplest al-lows swapping of adjacent phrases.
The second al-lows phrase movement within a three phrase win-dow.
Our formulation ensures that the overall modelprovides a proper parameterized probability distri-bution over reordered phrase sequences; we empha-size that the resulting distribution is not degenerate.Phrase reordering (Fig 2) takes as its input aFrench phrase sequence in English phrase orderx1, x2, ..., xK .
This is then reordered into Frenchphrase order y1, y2, ..., yK .
Note that words withinphrases are not affected.We make the following conditional independenceassumption:P (yK1 |xK1 , uK1 ,K, eI1) = P (yK1 |xK1 , uK1 ).
(2)Given an input phrase sequence xK1 we now as-sociate a unique jump sequence bK1 with each per-missible output phrase sequence yK1 .
The jump bkmeasures the displacement of the kth phrase xk, i.e.xk ?
yk+bk , k ?
{1, 2, ...,K}.
(3)The jump sequence bK1 is constructed such that yK1is a permutation of xK1 .
This is enforced by con-structing all models so that?Kk=1 bk = 0.We now redefine the model in terms of the jumpsequenceP (yK1 |xK1 , uK1 ) (4)={P (bK1 |xK1 , uK1 ) yk+bk = xk ?k0 otherwise,162x 2 x 3 x 4 x 5x 1y 2 y 3 y 4 y 5y 13b = 01b = +12b = ?1 4b = 0 5b = 0doivent de_25_%exportations fl?chirexportations grains de_25_%doivent fl?chirgrainsFigure 2: Phrase reordering and jump sequence.-where yK1 is determined by xK1 and bK1 .Each jump bk depends on the phrase-pair (xk, uk)and preceding jumps bk?11P (bK1 |xK1 , uK1 ) =K?k=1P (bk|xk, uk, ?k?1), (5)where ?k?1 is an equivalence classification (state)of the jump sequence bk?11 .The jump sequence bK1 can be described by adeterministic finite state machine.
?
(bk?11 ) is thestate arrived at by bk?11 ; we will use ?k?1 to denote?
(bk?11 ).We will investigate phrase reordering by restrict-ing the maximum allowable jump to 1 phrase andto 2 phrases; we will refer to these reorderingmodels as MJ-1 and MJ-2.
In the first case,bk ?
{0,+1,?1} while in the second case, bk ?
{0,+1,?1,+2,?2}.2.2 Reordering WFST for MJ-1We first present the Finite State Machine of thephrase reordering process (Fig 3) which has twoequivalence classes (FSM states) for any given his-tory bk?11 ; ?
(bk?11 ) ?
{1, 2}.
A jump of +1 has tobe followed by a jump of ?1, and 1 is the start andend state; this ensures?Kk=1 bk = 0.1 b=+1b=?1b=02Figure 3: Phrase reordering process for MJ-1.Under this restriction, the probability of the jumpbk (Eqn 5) can be simplified asP (bk|xk, uk, ?
(bk?11 )) = (6)?????
?1(xk, uk) bk = +1, ?k?1 = 11 ?
?1(xk, uk) bk = 0, ?k?1 = 11 bk = ?1, ?k?1 = 2.There is a single parameter jump probability?1(x, u) = P (b = +1|x, u) associated with eachphrase-pair (x, u) in the phrase-pair inventory.
Thisis the probability that the phrase-pair (x, u) appearsout of order in the transformed phrase sequence.We now describe the MJ-1 WFST.
In the presen-tation, we use upper-case letters to denote the En-glish phrases (uk) and lower-case letters to denotethe French phrases (xk and yk).The PPI for this example is given in Table 1.English French Parametersu x P (x|u) ?1(x, u)A a 0.5 0.2A d 0.5 0.2B b 1.0 0.4C c 1.0 0.3D d 1.0 0.8Table 1: Example phrase-pair inventory with trans-lation and reordering probabilities.The input to the WFST (Fig 4) is a lattice ofFrench phrase sequences derived from the Frenchsentence to be translated.
The outputs are the cor-responding English phrase sequences.
Note that thereordering is performed on the English side.The WFST is constructed by adding a self-loopfor each French phrase in the input lattice, anda 2-arc path for every pair of adjacent Frenchphrases in the lattice.
The WFST incorporates thetranslation model P (x|u) and the reordering modelP (b|x, u).
The score on a self-loop with labels(u, x) is P (x|u) ?
(1 ?
?1(x, u)); on a 2-arc pathwith labels (u1, x1) and (u2, x2), the score on the1st arc is P (x2|u1) ?
?1(x2, u1) and on the 2nd arcis P (x1|u2).In this example, the input to this transducer is asingle French phrase sequence V : a, b, c. We per-form the WFST composition R?V , project the resulton the input labels, and remove the epsilons to formthe acceptor (R?V )1 which contains the six Englishphrase sequences (Fig 4).Translation Given a French sentence, a lattice oftranslations is obtained using the weighted finitestate composition: T = G ?
W ?
R ?
?
?
?
?
T .The most-likely translation is obtained as the pathwith the highest probability in T .Alignment Given a sentence-pair (E,F ), a latticeof phrase alignments is obtained by the finite statecomposition: B = S ?
W ?
R ?
?
?
?
?
T , where163A : b / 0.1A B D 0.4 x 0.6 x 0.2 = 0.480B A D 0.4 x 0.5 x 0.2 = 0.040A D B 0.4 x 0.8 x 0.4 = 0.128A A B 0.4 x 0.1 x 0.4 = 0.016A B A 0.4 x 0.6 x 0.4 = 0.096B A A 0.4 x 0.5 x 0.4 = 0.080VR 1( )A : b / 0.5RV a b dB : b / 0.6D : d / 0.2A : d / 0.4A : a / 0.4B : a / 0.4B : d / 0.4D : b / 0.8Figure 4: WFST for the MJ-1 model.S is an acceptor for the English sentence E, andT is an acceptor for the French sentence F .
TheViterbi alignment is found as the path with the high-est probability in B.
The WFST composition givesthe word-to-word alignments between the sentences.However, to obtain the phrase alignments, we needto construct additional FSTs not described here.2.3 Reordering WFST for MJ-2MJ-2 reordering restricts the maximum allowablejump to 2 phrases and also insists that the reorder-ing take place within a window of 3 phrases.
Thislatter condition implies that for an input sequence{a, b, c, d}, we disallow the three output sequences:{b, d, a, c; c, a, d, b; c, d, a, b; }.
In the MJ-2 finitestate machine, a given history bk?11 can lead to oneof the six states in Fig 5.b=0123456b=?1b=+1b=?1b=+2b=0b=?2b=?1b=+1 b=?2Figure 5: Phrase reordering process for MJ-2.The jump probability of Eqn 5 becomesP (bk|xk, uk, ?k?1) =?????????
?1(xk, uk) bk = 1, ?k?1 = 1?2(xk, uk) bk = 2, ?k?1 = 1{1 ?
?1(xk, uk)?
?2(xk, uk)bk = 0, ?k?1 = 1(7){?1(xk, uk) bk = 1, ?k?1 = 21 ?
?1(xk, uk) bk = ?1, ?k?1 = 2(8){0.5 bk = 0, ?k?1 = 30.5 bk = ?1, ?k?1 = 3.
(9){1 bk = ?2, ?k?1 = 4 (10){1 bk = ?2, ?k?1 = 5 (11){1 bk = ?1, ?k?1 = 6 (12)We note that the distributions (Eqns 7 and 8) arebased on two parameters ?1(x, u) and ?2(x, u) foreach phrase-pair (x, u).Suppose the input is a phrase sequence a, b, c, theMJ-2 model (Fig 5) allows 6 possible reorderings:a, b, c; a, c, b; b, a, c; b, c, a; c, a, b; c, b, a.
The distri-bution Eqn 9 ensures that the sequences b, c, a andc, b, a are assigned equal probability.
The distribu-tions in Eqns 10-12 ensure that the maximum jumpis 2 phrases and the reordering happens within awindow of 3 phrases.
By insisting that the pro-cess start and end at state 1 (Fig 5), we ensure thatthe model is not deficient.
A WFST implementingthe MJ-2 model can be easily constructed for bothphrase alignment and translation, following the con-struction described for the MJ-1 model.3 Estimation of the Reordering ModelsThe Translation Template Model relies on an in-ventory of target language phrases and their sourcelanguage translations.
Our goal is to estimate thereordering model parameters P (b|x, u) for eachphrase-pair (x, u) in this inventory.
However, whentranslating a given test set, only a subset of thephrase-pairs is needed.
Although there may be anadvantage in estimating the model parameters underan inventory that covers all the training bitext, we fixthe phrase-pair inventory to cover only the phraseson the test set.
Estimation of the reordering modelparameters over the training bitext is then performedunder this test-set specific inventory.164We employ the EM algorithm to obtain MaximumLikelihood (ML) estimates of the reordering modelparameters.
Applying EM to the MJ-1 reorderingmodel gives the following ML parameter estimatesfor each phrase-pair (u, x).?
?1(x, u) =Cx,u(0,+1)Cx,u(0,+1) + Cx,u(0, 0).
(13)Cx,u(?, b) is defined for ?
= 1, 2 and b =?1, 0,+1.
Any permissible phrase alignment of asentence pair corresponds to a bK1 sequence, whichin turn specifies a ?K1 sequence.
Cx,u(?, b) is theexpected number of times the phrase-pair x, u isaligned with a jump of b phrases when the jump his-tory is ?.
We do not use full EM but a Viterbi train-ing procedure that obtains the counts for the best(Viterbi) alignments.
If a phrase-pair (x, u) is neverseen in the Viterbi alignments, we back-off to a flatparameter ?1(x, u) = 0.05.The ML parameter estimates for the MJ-2 modelare given in Table 2, with Cx,u(?, b) defined sim-ilarly.
In our training scenario, we use WFST op-erations to obtain Viterbi phrase alignments of thetraining bitext where the initial reordering modelparameters (?0(x, u)) are set to a uniform value of0.05.
The counts Cx,u(s, b) are then obtained overthe phrase alignments.
Finally the ML estimates ofthe parameters are computed using Eqn 13 (MJ-1) orEqn 14 (MJ-2).
We will refer to the Viterbi trainedmodels as MJ-1 VT and MJ-2 VT. Table 3 shows theMJ-1 VT parameters for some example phrase-pairsin the Arabic-English (A-E) task.u x ?1(x, u)which is the closest Aqrb 1.0international trade tjArp EAlmyp 0.8the foreign ministry wzArp xArjyp 0.6arab league jAmEp dwl Erbyp 0.4Table 3: MJ-1 parameters for A-E phrase-pairs.To validate alignment under a PPI, we mea-sure performance of the TTM word alignmentson French-English (500 sent-pairs) and Chinese-English (124 sent-pairs) (Table 4).
As desired, theAlignment Recall (AR) and Alignment Error Rate(AER) improve modestly while Alignment Preci-sion (AP) remains constant.
This suggests that themodels allow more words to be aligned and thus im-prove the recall; MJ-2 gives a further improvementin AR and AER relative to MJ-1.
Alignment preci-Reordering Metrics (%)Frn-Eng Chn-EngAP AR AER AP AR AERNone 94.2 84.8 10.0 85.1 47.1 39.3MJ-1 VT 94.1 86.8 9.1 85.3 49.4 37.5MJ-2 VT 93.9 87.4 8.9 85.3 50.9 36.3Table 4: Alignment Performance with Reordering.sion depends on the quality of the word alignmentswithin the phrase-pairs and does not change muchby allowing phrase reordering.
This experiment val-idates the estimation procedure based on the phrasealignments; however, we do not advocate the use ofTTM as an alternate word alignment technique.4 Translation ExperimentsWe perform our translation experiments on the largedata track of the NIST Arabic-to-English (A-E) andChinese-to-English (C-E) MT tasks; we report re-sults on the NIST 2002, 2003, and 2004 evaluationtest sets 1.4.1 Exploratory ExperimentsIn these experiments the training data is restricted toFBIS bitext in C-E and the news bitexts in A-E. Thebitext consists of chunk pairs aligned at sentenceand sub-sentence level (Deng et al, 2004).
In A-E,the training bitext consists of 3.8M English words,3.2M Arabic words and 137K chunk pairs.
In C-E,the training bitext consists of 11.7M English words,8.9M Chinese words and 674K chunk pairs.Our Chinese text processing consists of word seg-mentation (using the LDC segmenter) followed bygrouping of numbers.
For Arabic our text pro-cessing consisted of a modified Buckwalter analysis(LDC2002L49) followed by post processing to sep-arate conjunctions, prepostions and pronouns, andAl-/w- deletion.
The English text is processed us-ing a simple tokenizer based on the text processingutility available in the the NIST MT-eval toolkit.The Language Model (LM) training data consistsof approximately 400M words of English text de-rived from Xinhua and AFP (English Gigaword), theEnglish side of FBIS, the UN and A-E News texts,and the online archives of The People?s Daily.Table 5 gives the performance of the MJ-1 andMJ-2 reordering models when translation is per-formed using a 4-gram LM.
We report performanceon the 02, 03, 04 test sets and the combined test set1http://www.nist.gov/speech/tests/mt/165?
?1(x, u) =Cx,u(1,+1) + Cx,u(2,+1)Cx,u(1,+1) + Cx,u(1, 0) + Cx,u(1,+2) + Cx,u(2,+1) + Cx,u(2,?1)?
?2(x, u) =(Cx,u(1, 0) + Cx,u(2,?1) + Cx,u(1,+2))Cx,u(1,+2)(Cx,u(1,+1) + Cx,u(1, 0) + Cx,u(1,+2) + Cx,u(2,+1) + Cx,u(2,?1))(Cx,u(1,+2) + Cx,u(1, 0))Table 2: ML parameter estimates for MJ-2 model.Reordering BLEU (%)Arabic-English Chinese-English02 03 04 ALL 02 03 04 ALLNone 37.5 40.3 36.8 37.8 ?
0.6 24.2 23.7 26.0 25.0 ?
0.5MJ-1 flat 40.4 43.9 39.4 40.7 ?
0.6 25.7 24.5 27.4 26.2 ?
0.5MJ-1 VT 41.3 44.8 40.3 41.6 ?
0.6 25.8 24.5 27.8 26.5 ?
0.5MJ-2 flat 41.0 44.4 39.7 41.1 ?
0.6 26.4 24.9 27.7 26.7 ?
0.5MJ-2 VT 41.7 45.3 40.6 42.0 ?
0.6 26.5 24.9 27.9 26.8 ?
0.5Table 5: Performance of MJ-1 and MJ-2 reordering models with a 4-gram LM.(ALL=02+03+04).
For the combined set (ALL), wealso show the 95% BLEU confidence interval com-puted using bootstrap resampling (Och, 2003).Row 1 gives the performance when no reorder-ing model is used.
The next two rows show the in-fluence of the MJ-1 reordering model; in row 2, aflat probability of ?1(x, u) = 0.05 is used for allphrase-pairs; in row 3, a reordering probability isestimated for each phrase-pair using Viterbi Train-ing (Eqn 13).
The last two rows show the effect ofthe MJ-2 reordering model; row 4 uses flat proba-bilities (?1(x, u) = 0.05, ?2(x, u) = 0.01) for allphrase-pairs; row 5 applies reordering probabilitiesestimating with Viterbi Training for each phrase-pair(Table 2).On both language-pairs, we observe that reorder-ing yields significant improvements.
The gains fromphrase reordering are much higher on A-E relativeto C-E; this could be related to the fact that the wordorder differences between English and Arabic aremuch higher than the differences between Englishand Chinese.
MJ-1 VT outperforms flat MJ-1 show-ing that there is value in estimating the reorderingparameters from bitext.
Finally, the MJ-2 VT modelperforms better than the flat MJ-2 model, but onlymarginally better than the MJ-1 VT model.
There-fore estimation does improve the MJ-2 model butallowing reordering beyond a window of 1 phrase isnot useful when translating either Arabic or Chineseinto English in this framework.The flat MJ-1 model outperforms the no-reordering case and the flat MJ-2 model is betterthan the flat MJ-1 model; we hypothesize that phrasereordering increases search space of translations thatallows the language model to select a higher qual-ity hypothesis.
This suggests that these models ofphrase reordering actually require strong languagemodels to be effective.
We now investigate the inter-action between language models and reordering.Our goal here is to measure translation perfor-mance of reordering models over variable span n-gram LMs (Table 6).
We observe that both MJ-1and MJ-2 models yield higher improvements underhigher order LMs: e.g.
on A-E, gains under 3g(3.6 BLEU points on MJ-1, 0.2 points on MJ-2) arehigher than the gains with 2g (2.4 BLEU points onMJ-1, 0.1 points on MJ-2).Reordering BLEU (%)A-E C-E2g 3g 4g 2g 3g 4gNone 21.0 36.8 37.8 16.1 24.8 25.0MJ-1 VT 23.4 40.4 41.6 16.2 25.9 26.5MJ-2 VT 23.5 40.6 42.0 16.0 26.1 26.8Table 6: Reordering with variable span n-gram LMson Eval02+03+04 set.We now measure performance of the reorder-ing models across the three test set genres used inthe NIST 2004 evaluation: news, editorials, andspeeches.
On A-E, MJ-1 and MJ-2 yield larger im-provements on News relative to the other genres;on C-E, the gains are larger on Speeches and Ed-itorials relative to News.
We hypothesize that thePhrase-Pair Inventory, reordering models and lan-guage models could all have been biased away fromthe test set due to the training data.
There may alsobe less movement across these other genres.166Reordering BLEU (%)A-E C-ENews Eds Sphs News Eds SphsNone 41.1 30.8 33.3 23.6 25.9 30.8MJ-1 VT 45.6 32.6 35.7 24.8 27.8 33.3MJ-2 VT 46.2 32.7 35.5 24.8 27.8 33.7Table 7: Performance across Eval 04 test genres.BLEU (%)Arabic-English Chinese-EnglishReordering 02 03 04n 02 03 04nNone 40.2 42.3 43.3 28.9 27.4 27.3MJ-1 VT 43.1 45.0 45.6 30.2 28.2 28.9MET-Basic 44.8 47.2 48.2 31.3 30.3 30.3MET-IBM1 45.2 48.2 49.7 31.8 30.7 31.0Table 8: Translation Performance on Large Bitexts.4.2 Scaling to Large Bitext Training SetsWe here describe the integration of the phrase re-ordering model in an MT system trained on largebitexts.
The text processing and language mod-els have been described in ?
4.1.
Alignment Mod-els are trained on all available bitext (7.6M chunkpairs/207.4M English words/175.7M Chinese wordson C-E and 5.1M chunk pairs/132.6M Englishwords/123.0M Arabic words on A-E), and wordalignments are obtained over the bitext.
Phrase-pairsare then extracted from the word alignments (Koehnet al, 2003).
MJ-1 model parameters are estimatedover all bitext on A-E and over the non-UN bitexton C-E.
Finally we use Minimum Error Training(MET) (Och, 2003) to train log-linear scaling fac-tors that are applied to the WFSTs in Equation 1.04news (04n) is used as the MET training set.Table 8 reports the performance of the system.Row 1 gives the performance without phrase re-ordering and Row 2 shows the effect of the MJ-1VT model.
The MJ-1 VT model is used in an initialdecoding pass with the four-gram LM to generatetranslation lattices.
These lattices are then rescoredunder parameters obtained using MET (MET-basic),and 1000-best lists are generated.
The 1000-bestlists are augmented with IBM Model-1 (Brown etal., 1993) scores and then rescored with a second setof MET parameters.
Rows 3 and 4 show the perfor-mance of the MET-basic and MET-IBM1 models.We observe that the maximum likelihood phrasereordering model (MJ-1 VT) yields significantly im-proved translation performance relative to the mono-tone phrase order translation baseline.
This confirmsthe translation performance improvements foundover smaller training bitexts.We also find additional gains by applying MET tooptimize the scaling parameters that are applied tothe WFST component distributions within the TTM(Equation 1).
In this procedure, the scale factor ap-plied to the MJ-1 VT Phrase Translation and Re-ordering component is estimated along with scalefactors applied to the other model components; inother words, the ML-estimated phrase reorderingmodel itself is not affected by MET, but the likeli-hood that it assigns to a phrase sequence is scaledby a single, discriminatively optimized weight.
Theimprovements from MET (see rows MET-Basic andMET- IBM1) demonstrate that the MJ-1 VT reorder-ing models can be incorporated within a discrimi-native optimized translation system incorporating avariety of models and estimation procedures.5 DiscussionIn this paper we have described local phrase reorder-ing models developed for use in statistical machinetranslation.
The models are carefully formulatedso that they can be implemented as WFSTs, andwe show how the models can be incorporated intothe Translation Template Model to perform phrasealignment and translation using standard WFST op-erations.
Previous approaches to WFST-based re-ordering (Knight and Al-Onaizan, 1998; Kumarand Byrne, 2003; Tsukada and Nagata, 2004) con-structed permutation acceptors whose state spacesgrow exponentially with the length of the sentence tobe translated.
As a result, these acceptors have to bepruned heavily for use in translation.
In contrast, ourmodels of local phrase movement do not grow ex-plosively and do not require any pruning or approx-imation in their construction.
In other related work,Bangalore and Ricardi (2001) have trained WF-STs for modeling reordering within translation; theirWFST parses word sequences into trees containingreordering information, which are then checked forwell-formed brackets.
Unlike this approach, ourmodel formulation does not use a tree representationand also ensures that the output sequences are validpermutations of input phrase sequences; we empha-size again that the probability distribution inducedover reordered phrase sequences is not degenerate.Our reordering models do resemble those of (Till-mann, 2004; Tillmann and Zhang, 2005) in that we167treat the reordering as a sequence of jumps relativeto the original phrase sequence, and that the likeli-hood of the reordering is assigned through phrase-pair specific parameterized models.
We note thatour implementation allows phrase reordering be-yond simply a 1-phrase window, as was done by Till-mann.
More importantly, our model implements agenerative model of phrase reordering which can beincorporated directly into a generative model of theoverall translation process.
This allows us to per-form ?embedded?
EM-style parameter estimation,in which the parameters of the phrase reorderingmodel are estimated using statistics gathered underthe complete model that will actually be used intranslation.
We believe that this estimation of modelparameters directly from phrase alignments obtainedunder the phrase translation model is a novel contri-bution; prior approaches derived the parameters ofthe reordering models from word aligned bitext, e.g.within the phrase pair extraction procedure.We have shown that these models yield improve-ments in alignment and translation performance onArabic-English and Chinese-English tasks, and thatthe reordering model can be integrated into largeevaluation systems.
Our experiments show that dis-criminative training procedures such Minimum Er-ror Training also yield additive improvements bytuning TTM systems which incorporate ML-trainedreordering models.
This is essential for integratingour reordering model inside an evaluation system,where a variety of techniques are applied simultane-ously.The MJ-1 and MJ-2 models are extremely sim-ple models of phrase reordering.
Despite their sim-plicity, these models provide large improvementsin BLEU score when incorporated into a monotonephrase order translation system.
Moreover, theycan be used to produced translation lattices for useby more sophisticated reordering models that allowlonger phrase order movement.
Future work willbuild on these simple structures to produce morepowerful models of word and phrase movement intranslation.ReferencesP.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311.Y.
Deng, S. Kumar, and W. Byrne.
2004.
Bitext chunkalignment for statistical machine translation.
In Re-search Note, Center for Language and Speech Pro-cessing, Johns Hopkins University.K.
Knight and Y. Al-Onaizan.
1998.
Translationwith finite-state devices.
In AMTA, pages 421?437,Langhorne, PA, USA.K.
Knight.
1999.
Decoding complexity in word-replacement translation models.
Computational Lin-guistics, Squibs & Discussion, 25(4).P.
Koehn, F. Och, and D. Marcu.
2003.
Statistical phrase-based translation.
In HLT-NAACL, pages 127?133,Edmonton, Canada.S.
Kumar and W. Byrne.
2003.
A weighted finite statetransducer implementation of the alignment templatemodel for statistical machine translation.
In HLT-NAACL, pages 142?149, Edmonton, Canada.S.
Kumar, Y. Deng, and W. Byrne.
2005.
A weighted fi-nite state transducer translation template model for sta-tistical machine translation.
Journal of Natural Lan-guage Engineering, 11(4).F.
Och, C. Tillmann, and H. Ney.
1999.
Improved align-ment models for statistical machine translation.
InEMNLP-VLC, pages 20?28, College Park, MD, USA.F.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In ACL, Sapporo, Japan.C.
Tillmann and T. Zhang.
2005.
A localized predictionmodel for statistical machine translation.
In ACL, AnnArbor, Michigan, USA.C.
Tillmann.
2004.
A block orientation model for sta-tistical machine translation.
In HLT-NAACL, Boston,MA, USA.H.
Tsukada and M. Nagata.
2004.
Efficient decoding forstatistical machine translation with a fully expandedWFST model.
In EMNLP, Barcelona, Spain.S.
Vogel.
2003.
SMT Decoder Dissected: Word Reorder-ing.
In NLPKE, Beijing, China.D.
Wu.
1996.
A polynomial-time algorithm for sta-tistical machine translation.
In ACL, pages 152?158,Santa Cruz, CA, USA.R.
Zens and H. Ney.
2003.
A comparative study on re-ordering constraints in statistical machine translation.In ACL, pages 144?151, Sapporo, Japan.R.
Zens, H. Ney, T. Watanabe, and E. Sumita.
2004.Reordering constraints for phrase-based statistical ma-chine translation.
In COLING, pages 205?211,Boston, MA, USA.168
