Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 97?102,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsDemonstration of IlluMe: Creating AmbientAccording to Instant Message LogsLun-Wei Ku Cheng-Wei Sun Ya-Hsin HsuehNational Yunlin University of Science and Technology123 University Road, Section 3Douliou, Yunlin 64002, Taiwanlwku@yuntech.edu.tw;chengwei.kenny.sun@gmail.com;hsuehyh@yuntech.edu.twAbstractWe present IlluMe, a software tool packwhich creates a personalized ambient usingthe music and lighting.
IlluMe includes anemotion analysis software, the small spaceambient lighting, and a multimediacontroller.
The software analyzesemotional changes from instant messagelogs and corresponds the detected emotionto the best sound and light settings.
Theambient lighting can sparkle with differentforms of light and the smart phone canbroadcast music respectively according todifferent atmosphere.
All settings can bemodified by the multimedia controller atany time and the new settings will befeedback to the emotion analysis software.The IlluMe system, equipped with thelearning function, provides a link betweenresidential situation and personal emotion.It works in a Chinese chatting environmentto illustrate the language technology in life.1 IntroductionEmotion analysis as well as recommendationtechnology has drawn a lot attention in the naturallanguage processing research community.
Thedevelopment of fundamental approaches as well asapplications has been proposed (Das, 2011; Sarwaret al, 2001; Zheng et al, 2010).
However, most ofthem were Internet applications, and to the bestknowledge of the authors, these technologies havenot yet been involved in the ambient creation.
Tocreate an intelligent living space, some researchersutilized the facial expression and speech recognizerto detect emotions (Busso et al, 2004), but thenthe accompanied cameras and microphones werenecessary.
Some researchers tried to use sensors towatch the heart beat and the body temperature ofresidents to know their current emotion for furtherapplications, but the problem was that users had towear sensors and it was inconvenient.
Instead ofwatching body signals, we postulate that thecommunications among people is one of theimportant factors to influence their emotions.Therefore, we tried to find clues from the textualconversations of the residents in order to detecttheir psychological state.There are many ways to categorize emotions.Different emotion states were used for experimentsin previous research (Bellegarda, 2010).
To findsuitable categories of emotions, we adopted thethree-layered emotion hierarchy proposed byParrott (2001)1.
Six emotions are in the first layer,including love, joy, surprise, anger, sadness andfear.
The second layer includes 25 emotions, andthe third layer includes 135 emotions.
Using thishierarchical classification benefits the system.
Wecan categorize emotions from rough to finegranularities and degrade to the upper level whenthe experimental materials are insufficient.
How tomap categories in other researches to ours becomesclearer, and annotators have more informationwhen marking their current emotion.As to the music, most researchers looked for theemotions in songs or rhythms (Yang and Chen,2011; Zbikowski, 2011).
They classified musicinto different emotional categories and developedthe system to tell what emotion a song might bringto a listener.
However, if the aim is to create a1  http://changingminds.org/explanations/emotions/basic%20emotions.htm97comfortable ambient, what songs a person in acertain emotional state wants to listen to becomesthe question.
A happy user does not always enjoyhappy songs, and vice versa.
In this case, thetechnology developed in the previous work did notmeet the new requirement.IlluMe was designed for a small space personalenvironment.
We expect that users would like touse it because this system could interactivelyrespond to their personal status to provide a feelingof the companion.
We view the IlluMe system as arealization of detecting emotions from users?textual conversations and then recommending thebest ambient accordingly.
There are three majorcontributions in the development of the system.First, a corpus for ambient creation according toemotions was constructed.
Second, IlluMedemonstrates a way to apply the state of the arttechnology of emotion analysis andrecommendation to create an intelligent livingspace.
Third, along with the developed technology,several further applications utilizing thecomponents of IlluMe become feasible.2 System DescriptionThe potential working area for IlluMe is home or asmall space.
The system was designed to fit in withthe modern people?s life style: programs areinstalled in users?
personal computer and smartphone.
The smart phone functions as the remotecontrol and the music player, while all settingsignals are sent out from the personal computer.The smart phone and the personal computercommunicate through the wireless network.
Theonly additional hardware requirement is thelighting set.2.1 System FeaturesEmotion Detection Switch: The system detectsusers?
current emotion according to messenger logsonce a preset time period.
It is ON/OFF switchableif users do not want the conversations to berecorded or utilized when determining the ambient.Auto Ambient Setting: The system sets thecurrent ambient by a specific combination of asong and a light group which corresponds to theemotion or represents a special atmosphere.Manual Ambient Adjustment: IlluMe providesa friendly user interface to change the settings ofmusic and lighting at any time.Personal Preference Learning: When userschange the settings, the new ones are recorded.IlluMe learns the preference and then performs theuser adaptation.
After a period of time users willhave their unique ambient creating system.Unlimited Melodies and Rich Light Colors:Users can add their songs in the smart phone forselection at any time.
The learning process willhelp propose the new songs to create ambient later.Instant State Update: IlluMe watches the userinput from messenger when the software is on.Therefore, it is able to change the music andlighting according to the detected emotion within apreset time period and users will feel like theenvironment is interacting with them.2.2 System FrameworkFigure 1 demonstrates the system framework ofIlluMe.
The system automatically watches theUser Messages from messenger logs.
The EmotionAnalysis component detects the emotion of users,while the Ambient Learning Model determines themusic and lighting accordingly, considering alsothe Personal Information of users.After the lights are on and the music is played,the user can change the settings they are notsatisfying.
A smart phone (Mobile Device) is usedto change the settings, with two controllers on it:the Preference Controller and the AmbientController.
The former takes the User Input fornew settings, and then the music and lighting arechanged by the latter.
At the same time, thePreference Controller also sends the new settingsto Ambient Learning Model to be recorded for useradaptation when creating the next ambient.The Emotion Analysis Component and AmbientLearning Model are two programs in a personalcomputer, and the Personal Info is saved in thepersonal computer, too.
ANT wireless personalnetwork protocol (Dynastream) is adopted to sendthe control signals to the Lighting.
The LEDlighting board is utilized to implement the Lightingof 65,536 colors.2.3 Operation Flowchart of User InterfaceThe IlluMe system provides a user interface tochange the settings by a smart phone (MobileDevice), functioning as a remote control.
Users canselect the location of music or the lighting, e.g.
theliving room or the bedroom, and the control mode,98Modei.e.
manual or automatic.
In the manual mode,users can set the color of a specific light; in theautomatic mode, users select an emotional color setor a desired atmosphere for the lighting.
Figure 2shows the operational flow of the user interface.2.4 Ambient Lighting and Music PlayingTo design the ambient lighting, one has to takeLED array board, controlling mode and the light-mixing effect of the lampshade into consideration.The LED lamp should sprinkle the LEDcomponents of red, cyan, green, white and orangelights equally onto the LED array board, so as toachieve uniform distribution.
The controllingmodule distinguishes each lamp by its own code tomodify the brightness of different colored LEDswithin.Figure 1.
System Framework of IlluMeAs the LED lighting changes its color accordingto the controlling signals from the remotecontroller, the system transfer appropriate RFsignals from the user?s personal computer to theANT board, and then the ANT board controls theLED lighting board to change the color of lights.Music is broadcasted according to the detectedemotional state.
The broadcasting function and thecontrolling function are both realized by thesoftware in the smart phone.
Music is broadcasteddirectly through the phone, which conforms to thehabits of modern people.
Figure 3 shows theillustration of the usage of IlluMe.Figure 2.
Operation FlowchartFigure 3.
Usage Illustration3 Emotion AnalysisThe emotion analysis that IlluMe performed is tofind the emotions that texts in messenger logs bearin order to create a comfort ambient by sound andlighting accordingly.
To achieve this, the systemneeds to understand the Internet language first, andthen detect emotions and categorize them.
Thesystem works on the Chinese chatting environmentand analyzes Chinese texts to detect emotions.
Thematerials, approaches, and preliminary results inthe development phase are described in this section.3.1 Experimental MaterialsTwo dictionaries, the Chinese sentiment dictionaryNTUSD (Ku and Chen, 2007) and the Chineseemotion dictionary (Lin et al, 2008), were adoptedfor detecting emotions.
The former categorizedsentiment words into positive and negative, whilethe latter into eight emotion types: awesome,heartwarming, surprising, sad, useful, happy,boring, and angry.
Notice that these eight emotionAmbient LearningModelLightingANTMobile DeviceMusicSettingsUser MessagesEmotion Analysis ComponentPersonal InfoPreferenceControllerAmbientControllerUser  InputMediaControlLocationAutoPlay MusicLocationManualDisplay Lighting EffectBedroom Living RoomLightsColor EmotionSetAtmosphere99types appeared in Yahoo!
News Taiwan in the year2008 and not all of them were general emotionstates.
Therefore, we tried to find Lin?s emotioncategories in Parrott?s emotion hierarchy beforeusing this dictionary.
Those could not be foundwere categorized in the Other class.Messenger logs were used as the source to detectemotions.
We collected texts from Yahoo!Messenger and MSN Messenger logs of 8annotators.
When the installed collecting programin their computers was on, it ran as a service andcontinuously logged their messages.
Wheneverthere was at least one new message, once an hourthe collecting program would pop up the menu andask them to annotate the current emotion togetherwith the preferred settings of the music andlighting.
There were 3,290 songs, 15 emotionallighting colors and 6 atmospheres for selection.When selecting the settings of lighting, a full-screen colored photo would be displayed to helpannotators make their decisions.
A total of 150records are annotated for experiments and statisticsare shown in Table 1.Emo 1 2 3 4 5 611 80 1 15 39 4Color 1 2 3 4 5 6 714 6 5 25 9 5 11 148 9 10 11 12 13 14 1511 7 4 13 7 15 5 13Atm 1 2 3 4 5 628 40 16 33 17 16Table 1.
Statistics of Annotated Materials(Emo: Emotions, 1=Love, 2=Joy, 3=Surprise, 4=Angry,5=Sad, 6=Fear; Color:15 color sets; Atm:6 atmospheres)3.2 Interpretation of Zhuyin WenWhen processing Internet Chinese texts, IlluMetransformed messenger logs and sentimentdictionaries into zhuyin (Su, 2003) before lookingfor emotions2.
There were many reasons to do this.Zhuyin Wen (???)
is one of many creative usesof writing systems in the Internet language.
AsBlakeman (2004) found in his study of English,Internet language is fraught with initializations.However, as to the traditional Chinese, bothWikipedia and Zhang and Dai (2006) indicatedthat stylized initials and stylized numbers are2 Lookup Table: http://cclookup.cctserver.com/rarely used in Taiwan.
Su reported that the mostpopular type of creative use of writing systems is?Zhuyin Wen?
(???).
In ?Zhuyin Wen?
thecomplete phonetic representation of a character isreduced to a consonant, or sometimes a vowel.This creative use appeared commonly in thecollected conversations.
Generally we had to figureout the missing vowels to understand the word, butin our system a reversed approach (droppingvowels) was adopted to make sure the system didnot miss any possible match of dictionary termsobserved in the conversations.When messenger users typed characters by theirphonetics (consonants and vowels), very often theyselected the wrong one from candidates of thesame pronunciation, or they were just too lazy toselect so the writing system chose the defaultcandidate for them.
In these cases, the systemcould not find a match because of wrongcomposite characters.
Transforming characters inboth dictionaries and conversations into theirzhuyin representations before detecting emotionsalso help recover this kind of errors.3.3 Emotion Detection from TextsSection 3.2 shows how the system dealt with theerror prone Internet texts and found thedictionaries terms.
Ku and Chen?s (2007) approachfor calculating sentiment scores was then adoptedto give scores to these terms.
The scores of termsof different emotional categories were summed upand the emotion category of the highest score wasselected as the detected emotion.
The AmbientLearning Model takes the detected emotion andselects the corresponding music and lighting by theNa?ve Bayes classifier trained by the annotatedmaterials.3.4 Experiment and Preliminary ResultsTable 2 shows that using enhanced NTUSD (anaugmented version of NTUSD) together withzhuyin transformation achieves the best results foremotion classification (positive/negative).Ku (2008) reported the set precision of theirapproach was 0.489 when texts were categorizedinto positive, neutral and negative.
Though theyhad one additional neutral category, our systemachieved the precision of 0.620 when processingthe noisy Internet texts without word segmentationand part of speech tagging, which was satisfactory.100Because IlluMe would always recommend anew or unchanged ambient setting, it would alwaysfind the closest emotion category of the user?scurrent emotion.
In other words, the chattingcontent would always be connected to one of sixemotion categories, so precision is the best metricto evaluate the performance of the system.
Themicro-average precision of the emotion detectionwas 0.207, while the macro-average precision was0.338.
Bellegarda reported that his best f-measurewas 0.340 also for 6 categories.
Notice that thecategories in Lin?s Chinese emotional dictionarywere not identical to ours and hence we could notfind terms for some categories in it.
Therefore,though Bellegarda?s and our results were done ondifferent datasets and evaluated by differentmetrics, considering our system suffered for thelack of terms in some categories and theambiguous texts from the creative writing, theperformance was considered acceptable.For the ambient recommendation, the micro-average precision of selecting the settings oflighting according to the detected emotion was0.441 for 15 color sets and 0.461 for 6 atmospheres.Positive Negative TotalA 0.489 0.534 0.507B 0.902 0.155 0.613A+C 0.902 0.172 0.620Table 2.
Precision of Emotion Detection(A: NTUSD; B: Enhanced NTUSD; C:Zhuyintransformation)3.5 Ambient Learning FunctionBecause bringing up the settings to users is like abehavior of recommendation, we adopted theconcept of collaborative filtering to design thefunction of the Ambient Learning Model.
In theearly stage of using IlluMe, it proposes the mostfrequently selected settings, that is, the choice of agroup of people in the specific emotional state.
Ifthe user is connected to the Internet, the userexperience will be transferred back to the serversto help recommend a better ambient to other users.The user experience optimization was feasible inthis system because of the use of the smart phone,and this function was also implemented.
As theusers update the settings, the system knows theirpreference.
In the later stage of using IlluMe, theAmbient Learning Model considers the preferenceof both the individual and the group to create aunique ambient for each user.4 Conclusion and Future WorkThrough the work we aim to apply the languagetechnology to redefine the concept of a small houseor working space.
They should be a family-likeexistence which possesses the intellectual capacityto observe human behavior and emotion, andcreate consoling spaces according to the residents?different status.
Therefore we implementedemotion analysis technique to equip a space withthe ability to observe the status of its residents andinteract with them accordingly.
The instant interiorlightings and music change can be viewed as a newform of ?conversation?.
Residents can not onlytake the ambient provided by IlluMe, but can alsogive feedbacks.
The concept of collaborativefiltering was also implemented as we viewed theproposing of ambient as a kind of recommendation.Through the demonstration of the IlluMe system,we hope to show another way to apply languagetechnology in life and retrieve the positive andrelaxing atmosphere to rebuild our sense of trustand safety toward space, and finally recollect thelong-lost attachment toward it.We will continue collecting annotatedmaterials and user feedbacks for learning, andmake the materials a corpus for the researchcommunity.
Facebook will be a source of textcollection to gather more complete personalconversations for emotion detection.
Making theIlluMe components real products like the homelighting system, the intelligent table lamp, or themusic album promoter is also a future plan.5 DemonstrationAs demonstrating the IlluMe system by ouroriginal model house may be difficult intransportation and it may need a large space fordemonstration, we will demonstrate the lightingsby several table lamps, in which the LED lightingboard resides.
Other software will be performed onthe smart phone and the personal computer.5.1 Demonstration OutlineThere are three purposes of the demonstration: first,to show how we apply the emotion analysis andrecommendation technique in an ambient creatingsystem; second, to illustrate actual and live101operation of the system to the potential users; third,to show the annotation process of the experimentmaterials and the underlying algorithms for thoseinterested in the technical details.Potential users might be interested in how thesystem will work if they have it in their personalcomputers and smart phones.
Therefore, wedemonstrate the whole IlluMe system with theactual music and lighting.
Users can type Chinesewords in messengers from the personal computer,and then the IlluMe system will change the musicand lighting according to the proposed settings in ashort time.
The user can also control the music andlighting from the interface by the smart phone.In addition to demonstrating the functionality ofthe system, we will also provide accompanyingvisual aids that illustrate the underlying algorithmsand the technical details.
For example, zhuyin,terms found in the dictionaries, emotion scores, thedetected emotion and the suggested settings.AcknowledgementsResearch of this paper was partially supported byNational Science Council, Taiwan, under thecontract NSC100-2218-E-224-013-.ReferencesBellegarda, Jerome R. 2010.
Emotion Analysis UsingLatent Affective Folding and Embedding.Proceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis andGeneration of Emotion in Text, Los Angeles, 1-9.Blakeman, Adam.
2004.
An Investigation of theLanguage of Internet Chat Rooms.http://www.lancs.ac.uk/fss/courses/ling/ling201/res/dissertations.html.Busso, Carlos, Deng, Zhigang, Yildirim, Serdar, Bulut,Murtaza, Lee, Chul Min, Kazemzadeh, Abe, Lee,Sungbok, Neumann, Ulrich and Narayanan,Shrikanth.
2004.
Analysis of Emotion Recognitionusing Facial Expressions, Speech and MultimodalInformation.
Proceedings of ACM 6th InternationalConference on Mutlmodal Interfaces (ICMI 2004),State College, PA, Oct 2004Das, Dipankar, 2011.
Analysis and Tracking ofEmotions in English and Bengali Texts: AComputational Approach.
Proceedings of theInternational World Wide Web Conference (WWW2011), Ph.
D. Symposium.
343-347.Dynastream Innovations Inc., ANT AT3 RF TransceiverChipset_Datasheet_Rev1.2,http://www.thisisant.com/.Ku, Lun-Wei and Chen, Hsin-Hsi.
2007.
MiningOpinions from the Web: Beyond Relevance Retrieval.Journal of American Society for Information Scienceand Technology, Special Issue on Mining WebResources for Enhancing Information Retrieval,58(12), 1838-1850.Ku, Lun-Wei, Liu, I-Chien, Lee, Chia-Ying, Chen,Kuan-hua.
and Chen, Hsin-His.
2008.
Sentence-Level Opinion Analysis by CopeOpi in NTCIR-7.Proceedings of the 7th NTCIR Workshop Meeting,Tokyo, Japan.
260-267.Lin, Kevin Hsin-Yih, Yang, Changhua, and Chen, Hsin-His.
2008.
Emotion Classification of Online NewsArticles from the Reader?s Perspective.
Proceedingsof the 2008 IEEE/WIC/ACM InternationalConference on Web Intelligence.
220-226.Ortony, A. and Turner, T. J.
1990.
What's basic aboutbasic emotions?
Psychological Review, 97, 315-331.Parrott, W. 2001.
Emotions in Social Psychology,Psychology Press, Philadelphia.Sarwar, Badrul, Karypis, George, Konstan, Joseph, andRiedl, John.
2001.
ItemBased Collaborative FilteringRecommendation Algorithms.
Proceedings of theInternational World Wide Web Conference (WWW2001), 285-295.Su, Hsi-Yao.
2003.
The Multilingual and Multi-Orthographic Taiwan-Based Internet: Creative Usesof Writing Systems on College-Affiliated BBSs.Journal of Computer-Mediated Communication 9(1).http://jcmc.indiana.edu/vol9/issue1/su.html.Yang, Yi-Hsuan and Chen, Homer H., Fellow, IEEE.2011.
Ranking-Based Emotion Recognition forMusic Organization and Retrieval.
IEEETransactions on audio, speech, and languageprocessing, 19(4).Zbikowski, Lawrence M., 2011.
Music, Emotion,Analysis.
Music Analysis, Blackwell Publishing Ltd,Oxford, UK.Zhang, Jiawei and Dai, Jiaxing.
2006.
Qiantan shixiaqingnian wangluo yongyu ?huoxing wen ?????
?
?
?
?
?
?
?
?
?
?http://www.shs.edu.tw/works/essay/2006/03/2006032816043532.pdfZheng, Vincent W., Cao, Bin, Zheng, Yu, Xie, Xing andYang, Qiang.
2010.
Collaborative Filtering MeetsMobile Recommendation: A User-centered ApproachProceedings of Twenty-Fourth National Conferenceon Artificial Intelligence (AAAI-10).102
