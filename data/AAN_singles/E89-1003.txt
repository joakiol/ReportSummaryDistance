EFF IC IENT PROCESSING OFFLEX IBLE  CATEGORIAL  GRAMMARGosse BoumaResearch Institute for Knowledge SystemsPostbus 463, 6200 AL MaastrichtThe Netherlandse-mail(earn): exriksgb@hmarl5ABSTRACT*From a processing point of view, however,flexible categorial systems are problematic,since they introduce spurious ambiguity.
Inthis paper, we present a flexible categorialgrammar which makes extensive use of theproduct-operator ,  f irst introduced byLambek (1958).
The grammar has the prop-erty that for every reading of a sentence, astrictly left-branching derivation can begiven.
This leads to the definition of a subsetof the grammar, for which the spurious ambi-guity problem does not arise and efficientprocessing is possible.1.
Flexibility vs. AmbiguityCategorial Grammars owe much of theirpopularity to the fact that they allow forvarious degrees of flexibility with respect toconstituent structure.
From a processingpoint of view, however, flexible categorialsystems are problematic, since they intro-duce spurious ambiguity.The best known example of a flexiblecategorial grammar is a grammar containingthe reduction rules application and compo-sition, and the category changing rule rais-ing 1 ?
* I would like to thank Esther K0nig,Erik-Jan van der Linden, Michael Moortgat,Adriaan van Paassen and the participants ofthe Edinburgh Categorial Grammar Weekend,who made useful comments to earlierpresentations of this material.
All remainingerrors and misconceptions are of course myown.1 Throughout his paper we will be us-ing the notation of Lambek (1958), in whichA/B and B\A are a right-directional nd a( i)  application : A/B B ==> AB B\A ==> Acomposition: A/B B/C ==> NCC\B B~, ==> C~raising : A ==> (B/A)\BA ==> B/(A\B)With this grammar many alternative con-stituent structures for a sentence can begenerated, even where this does not corre-spond to semantic ambiguities.
From a lin-guistic point of view, this has many advan-tages.
Various kind of arguments for givingup traditional conceptions of constituentstructure can be given, but the most con-vincing and well-documented case in favourof flexible constituent structure is coordi-nation (see Steedman (1985), Dowty (1988),and Zwarts (1986)).The standard assumption in generativegrammar is that coordination always takesplace between between constituents.
Right-node raising constructions and other in-stances of non-constituent conjunction areproblematic, because it is not clear what thestatus of the coordinated elements in theseconstructions is.
Flexible categorial gram-mar presents an elegant solution for suchcases, since, next to canonical constituentstructures, it also admits various other con-stituent structures.
Therefore, the sentencesin (2) can be considered to be ordinary in-stances of coordination (of two categoriesslap and (vp /ap) \vp ,  respectively).
(2) a. John sold and Mary bought a books/vp vp/np s/vp vp/np nps/np s/npleft-directional functor respectively, lookingfor an argument of category B.- 19 -b. J. loves Mary madly and Sue wildlyvp/np np vp\vp np vp\vpMary madlynp vp\vp(vplnp)\vp(vp/np)WpA somewhat different type of argumentfor flexible phrase structure is based on theway humans process natural language.
InAdes & Steedman (1982) it is pointed outthat humans process natural language in aleft-to-right, incremental, manner.
This pro-cessing aspect is accounted for in a flexiblecategorial system, where constituents can bebuilt for any part of a sentence.
Since syn-tactic rules operate in parallel with semanticinterpretation rules, building a syntacticstructure for an initial part of a sentence,implies that a corresponding semantic struc-ture can also be constructed.These and other arguments suggest thatthere is no such thing as a fixed constituentstructure, but that the order in which ele-ments combine with eachother is rather free.From a parsing point of view, however,flexibility appears to be a disadvantage.Flexible categorial grammars produce largenumbers of, often semantically equivalent,derivations for a given phrase.
This spuriousambiguity problem (Wittenburg (1986))makes efficient processing of flexible catego-rial grammar problematic, since quite oftenthere is an exponential growth of the numberof possible derivations, relative to the lengthof the string to be parsed.There have been two proposals foreliminating spurious ambiguity from thegrammar.
The first is Wittenburg (1987).
Inthis paper, a categorial grammar with compo-sition and heavily restricted versions ofraising (for subject n p's only) is considered.Wittenburg proposes to eliminate spuriousambiguity by redefining composition.
Hispredictive composition rules apply only inthose cases where they are really needed tomake a derivation possible.
A disadvantageof this method, noticed by Wittenburg, isthat one may have to add special predictivecomposition rules for all general combina-- 20  -tory rules in the grammar.
Some carefulrewriting of the original grammar has to takeplace, before things work as desired.Pareschi & Steedman (1987) propose anefficient chart-parsing algorithm for catego-rial grammars with spurious ambiguity.
In-stead of the usual strategy, in which all pos-sible subconstituents are added to the chart,Pareschi & Steedman restrict themselves toadding only those constituents that may leadto a difference in semantics.
Thus, in (3)only the underlined constituents are in thechart.
The "---" constituent is not.
(3) John loves Mary madlys/vp vp/np np vp\vpCombining 'madly' with the rest would beimpossible or lead to backtracking in thenormal case.
Here, the Pareschi & Steedmanalgorithm starts looking for a constituentleft adjacent of madly, which contains an el-ement X/vp as a leftmost category.
If such aconstituent can be found, it can be concludedthat the rest of that constituent must(implicitly) be a v p, and thus the validity ofcombining vp \vp  with this constituent hasbeen established.
Therefore, Pareschi &Steedman are able to work with only a mini-mal amount of items in the chart.Both Wittenburg and Pareschi & Steed-man work with categorial grammars, whichcontain restricted versions of compositionand raising.
Although they can be processedefficiently, there is linguistic evidence thatthey are not fully adequate for analysis ofsuch phenomena as coordination.
Sinceatomic categories can in general not beraised in these grammars, sentence (2b) (inwhich the category n p has to be raised)cannot be derived.
Furthermore, sincecomposition is not generalized, as in Ades &Steedman (1982), a sentence such as Johnsold but Mary donated a book to the librarywould not be derivable.
The possibilities forleft-to-right, incremental, processing arealso limited.
Therefore, there is reason tolook for a more flexible system, for whichefficient parsing is still possible.2.
S t ruc tura l  CompletenessIn the next section we present a gram-matical calculus, which is more flexible thanthe systems considered by Wittenburg(1987) and Pareschi & Steedman (1987), andtherefore is attractive for linguistic pur-poses.
At the same time, it offers a solutionto the spurious ambiguity problem.Spurious ambiguity causes problems forparsing in the systems mentioned above, be-cause there is no systematic relationshipbetween syntactic structures and semanticrepresentations.
That is, there is no way toidentify in advance, for a given sentence S, aproper subset of the set of all possible syn-tactic structures and associated semanticrepresentations, for wh ich  it holds that itwill contain all possible semantic represen-tations of S.(5) Strong Structural Complete-nessIf a sequence of categories XI .. X nreduces to Y, with semantics Y',there is a reduction to Y, with se-mantics Y', for any bracketing ofXI..Xn into constituents.Grammars with this property, can poten-tially circumvent the spurious ambiguityproblem, since for these grammars, we onlyhave to inspect all left-branching syntaxtrees, to find all possible readings.
Thismethod will only fail if the set of left-branching trees itself would contain spuri-ous ambiguous derivations.
In section 4 wewill show that these can be eliminated fromthe calculus presented below.3.
The P -ca lcu lusConsider now a grammar for which thefollowing property holds:(4) Structural CompletenessIf a sequence of categories X1 .. X nreduces to Y, there is a reduction toY for any bracketing of X1..Xn intoconstituents.
(Moortgat, 1987:5) 2Structural complete grammars are interest-ing linguistically, since they are able tohandle, for instance, all kinds of non-con-stituent conjunction, and also because theyallow for strict left-to-right processing (seeMoortgat, 1988).The latter observation has consequencesfor parsing as well,, since, if we can parseevery sentence in a strict left-to-right man-ner (that is, we produce only strictly left-branching syntax trees), the parsing algo-rithm can be greatly simplified.
Notice,however, that such a parsing strategy is onlyvalid, if we also guarantee that all possiblereadings of a sentence can be found in thisway.
Thus, instead of (4), we are looking forgrammars with the following, slightlystronger, property:2 Buszkowski (1988) provides aslightly different definition in terms offunctor-argument structures.The P(roduct)-calculus is a categorialgrammar, based on Lambek (1958), whichhas the property of strong structural com-pleteness.In Lambek (1958), the foundations offlexible categorial grammar are formulatedin the form of a calculus for syntactic cate-gories.
Well-known categorial rules, such asapplication, composition and category-rais-ing, are theorems of this calculus.
A largelyneglected aspect of this calculus, is the useof the product-operator.The calculus we present below, wasdeveloped as an alternative for Moortgat's(1988) M-system.
The M-system is a subsetof the Lambek-calculus, which uses, next toapplication, only a very general form ofcomposition.
Since it has no raising, it seemsto be an attractive candidate for investigat-ing the possibilities of left-associativeparsing for categorial grammar.
It is notcompletely satisfactory, however, sincestructural completeness i  not fully guaran-teed, and also, since it is unknown whetherthe strong structural completeness propertyholds for this system.
In our calculus, wehope to overcome these problems, by usingproduct-introduction and -elimination rulesinstead of composition.The kernel of the P-calculus is right-and left-application, as usual.
Next to these,-21  -we use a rule for introducing the product-operator, and two inference rules for elimi-nating products :(6) RA : A/B B => ALA : B B~, => A(product) introduction:I : A B => A*Binference rules :P : AB=>C,  DC=>ED*A B => EP ' :  AB=> C, CD=>EA B*D => EWe can use this calculus to produce left-branching syntax trees for any given(grammatical) sentence.
(7) is a simple ex-ample 3(7) John loves Mary madlys/vp vp/np np vp\vpIs /vp*vp/np(a)s/vp*vp(b)S --> NP VP), or (in CG) with a reductionrule (application or composit ion, for in-stance), we now have the freedom toconcatenate arbitrary categories, completelyirrespective of their internal structure.The P-calculus is structurally complete.To prove this, we prove that for any fourcategor ies  A ,B ,C ,D ,  it holds that :(AB)C --> D <==> A(BC) --> D, where -->is the derivabil ity relation.
From this,structural completeness may be concluded,since any bracketing (or branching of syntaxtrees) can be obtained by applying thisequivalence an arbitrary number of times.Proof : From (AB)C --> D it follows thatthere exists a category E such that AB -->E and EC --> D. BC --> B 'C ,  by I. NowA(B*C) --> D, by P', since AB --> E andEC --> D. Therefore, by transitivity of - ->,A(BC) --> D. To prove that A(BC) --> D==> (AB)C --> D use P instead of P'.Semantics can be added to the grammar,by giving a semantic counterpart (in lowercase) for each of the rules in (6):(8) RA :I_A:A/B:a B:b => A:a(b)B:b B~A:a => A:a(b)(product) introduction:I :  A:a B:b => A*B:a*b(a) vp/np np => vp,s/vp vp => s/vp*vps/vp*vp/np np ffi> s/vp*vp(b) vp vp\vp => vp ,  s/vp vp => ss/vp*vp vp => sThe first step in the derivation of (7) is theapplication of rule I.
The other two reduc-tions ((a) and (b)) are instantiations of theinference rule P. As the example shows, the*-operator (more in particular its use in I)does something like concatenation,  butwhereas such operations are normally asso-ciated with particular grammatical rules (i.e.you may concatenate two elements of categoryN P and V P,  respectively, if there is a rule3 To improve readability, we assumethat the operators / and \ take precedenceover * (X*Y/Z should be read as X*(Y/Z)  ).inference rules :P : A:a B:b => C:c, D:d C:c => E:eD*A:d*a B:b => E:eP' : A:a B:b => C:c, C:c D:d => E:eA:a B*D:b*d => E:eWe can now include semantics in theproof given above, and from this, we may con-clude that strong structural completenessholds for the P-calculus as well.4.
Eliminating SpuriousAmbiguityIn this section we outline a subset of theP-calculus, for which efficient processing ispossible.
As was noted above, in the P-cal-culus there is always a str ict ly left-branching derivation for any reading of a- 22  -sentence S. The restrictions we add in thissection are needed to eliminate spurious am-biguities from these left-branching deriva-t ions.Restricting a parser so that it will onlyaccept left-branching derivations will notdirectly lead to an efficient parsing proce-dure for the P-calculus.
The reason istwofold.First, nothing in the P-calculus excludesspurious ambiguity which occurs within theset of left-branching analysis trees.
Con-sider again example (7).
This sentence isunambiguous, but nevertheless we can give aleft-branching derivation for it which dif-fers from the one given earlier :(9) John loves Mary madlys/vp vp/np np vp\vpIs /vp*vp/npI(s/vp*vp/np)* np(**)sThe inference step (**) can be proven to bevalid, if we use P' as well as P.An even more serious problem is causedby the interaction between I and P,P'.
(10) AB=>A*B A*B C=>DBC=>B*C A B*C=>DPp,A*B C => DIf we try to prove that A*B and C can be re-duced to a category D, we could use P, with Iin the left premise.
To prove the secondpremise, we could use P', also with using I inthe left premise.
But now the right premiseof P' is identical to our initial problem; andthus we have made a useless loop, whichcould even lead to an infinite regress.These problems can be eliminated, if werestrict the grammar in two ways.
First ofall, we consider only derivations of the formC1, .
.
.
,Cn  ==> S, where C1 , .
.
.
,Cn ,S  do notcontain the product-operator.
This means werequire that the start symbol of the grammar,and the set lexical categories must be prod-uct-free.
Notice that this restriction can beeasily made, since most categorial lexiconsdo not contain the product-operator anyway.Given this restriction, the inference ruleP can be restricted: we require that the leftpremise of this rules always is an instance ofeither left- or r ight-appl icat ion.
Considerwhat would happen if we used I here :(11)BC=>B*C A B*C=>DPA*B C => DSince the lexicon is product-free, and we areinterested in strictly left-branching deriva-tions only, we know that C must be a prod-uct-free category.
If  we combine B and Cthrough I, we are faced with the problem in***.
At this point we could use I again for in-stance, thereby instantiating D as A* (B*C) .But this will lead to a spurious ambiguity,since we know that:A*(B*C) E => F iff (A*B)*C E--> F 4 .A category (A*B)*C can be obtained by ap-plying I directly to A*B and C.If we apply P' at point ***, we find our-selves trying to find a solution for A B =>E, and then E C => D. But this is nothingelse than trying to find a left-branchingderivation for A,B,C => D, and therefore,the inference step in (11) has not led toanything new.In fact, given that the lexicon is productfree and only application may be used in theleft premise of P, P' is never needed to derivea left-branching tree.As a result, we get (12), where we havemade a distinction between reduction rules(right and left-application) and other rules.This enables us to restrict the left premiseof P. The fact that every reduction rule isalso a general rule of the grammar, is ex-pressed by R. P' has been eliminated.4 In the P-calculus, this follows fromthe fact that E must be product-free.
It is atheorem of the Lambek-calculus as well.c , -o  - 23  -(12) RA : A/B B -> ALA : B B~A -> A(product) introduction:I :  A B =>A*Binference rules :P : AB->C,  DC=>ED*A B => ER : AB->Csentence like (7) using a shift-reduce pars-ing technique, and having only right- andleft-application as syntax rules.
(1 4) (remaining) input stacks /vp ,vp /np ,np ,vp \vp  _S vp /np ,np ,vp\vp  s /vpS np,vp\vp s /vp ,vp /npS vp \vp  s /vp ,vp /np ,npR vp \vp  s /vp ,vpS _ s /vp ,vp ,vp \vpR _ s /vp ,vpR sAB=>CThe system in (12) is a subset of the P-calculus, which is able to generate a strictlyleft-branching derivation for every readingof a given sentence of the grammar.The Prolog fragment in (13) shows howthe restricted system in (12) can be used todefine a simple left-associative parsing algo-r i thm.
(13) parse(\[C\] ==> C) :- !.parse(\[Cl,C21Rest\] ==> S) :-rule(\[C1,C2\] ==> C3),parse(\[C31Rest\] ==> S).%R:rule(X --=> Y) :-reduction_rule(X ==> Y).%P:% '+' is used instead of '*' to avoid% unnecessary bracketingrule(\[X+Y,Z\] ==> W) :-reduction_rule(\[Y,Z\] ==> V),rule(\[X,V\] ==> W).% I:rule(\[X,Y\] ==> X+Y).% appl icat ion ?reduction_rule(\[X/Y,Y\] ==> X).reduction_rule(\[Y,Y\X\] ==> X).5.
Shift-reduce parsingIt has sometimes been noted that aderivation tree in categorial grammar (suchas (7)) does not really reflect constituentstructure in the traditional sense, but that itreflects a particular parse process.
This maybe true for categorial systems in general, butit is particularly clear for the P-calculus.Consider for instance how one would parse aShifting an element onto the stack (apartform the first one maybe) seems to be equiv-alent to combining elements by means of I.The stack is after all nothing but a somewhatdifferent representation of the product typeswe used earlier.
The fact that adding one el-ement to the stack (vp \vp)  induces two re-duction steps, is comparable to the fact thatthe inference rule P may have the effect ofeliminating more than one slash at time.The s imi lar i ty between sh i f t - reduceparsing and the derivations in P brings inanother interesting aspect.
The shift-reducealgorithm is a correct parsing strategy, be-cause it will produce all (syntactic) ambigu-ities for a given input string.
This meansthat in the example above, a shift-reduceparser would only produce one syntax tree(assuming that the grammar has only appli-cation).If the input was potentially ambiguous,as in (15), there are two different deriva-t ions.
(15) aJa a a\aIt is after shifting a on the stack that adifference arises.
Here, one can either re-duce or shift one more step.
The first choiceleads to the left-branching derivation, thesecond to the right-branching one.The choice between shifting or reducinghas a categorial equivalent.
In the P-calcu-lus, one can either produce a left-branchingderivation tree for (15) by using applicationonly, or as indicated in (16).- 24  -(16) a/a a a\aI~a*aPNote that the P-calculus thus is able tofind genuine syntactic (or potentially se-mantic) ambiguities, without producing adifferent branching phrase structure.
Thecorrespondence to shift-reduce parsing al-ready suggests this of course, since weshould consider the phrase structure pro-duced by a structurally complete grammarmuch more as a record of the parse processthan as a constituent structure in the tradi-tional sense.6.
Coord inat ionThe P-calculus is structurally complete,and therefore, all the arguments that havebeen presented in favour of a categorialanalysis of coordination, hold for the P-cal-culus as well.
Coordination introduces poly-morphism in the grammar, however, and thisleads to some complications for the re-stricted P-calculus presented in (12).Adding a category X\(X/X) for coordina-tors to the P-calculus, enables us to handlenon-constituent conjunction, as is exempli-fied in (17) and (18).
(17) John loves and Peter adores Sues/vp vp/np X\(X/X) s/vp vp/np npIs/vp*vp/np s/vp*vp/nps/vp*vp/npS(18)  J. loves Mary madly and Sue wildlyvp/np np vp\vp X~(X/X)np vp\vpnp*vp\vp np*vp\vpnp*vp\vpvpp,The restricted-calculus of (12) was de-signed to enable efficient left-associativeparsing.
We assumed that lexieal categorieswould always be product-free, but this as-sumption no longer holds, if we add X\(X/X)to the grammar (since X can be instantiated,for instance as s /vp*vp/np) .
This meansthat left-associative derivations are notalways possible for coordinated sentences.Our solution to this problem, is to addrules such as (19) to the grammar, which cantransform certain product-categories intoproduct-free categories.
(19) A/(B*C) ~> (A/C)/BA number of such rules are needed to restoreleft-associat ivity.Next to syntactical additions, somemodifications to the semantic part of theinference rule P had to be made, in order tocope with the polymorphic semanticsproposed for coordination by Partee & Rooth(1983).7.
Conc lud ing  remarks .The spurious ambiguity problem hasbeen solved in this paper in a rather para-doxical manner.
Whereas Wittenburg (1987)tries to do away with ambiguous phrasestructure as much as possible (it only ariseswhere you need it) and Pareschi & Steedman(1987) use a chart parsing technique to re-cover implicit constituents efficiently, thestrategy in this paper has been to go forcomplete ambiguity.
It is in fact thismassive ambiguity,  which tr iv ial izesconstituent structure to such an extent thatone might as well ignore it, and choose a con-stituent structure that fits ones purposesbest (left-branching in this case).
It seemsthat as far as processing is concerned, thehalf-way flexible systems of Steedman(having generalized composition, and heavilyrestricted forms of raising) are in fact thehardest case.
Simple AB-grammars are in allrespects similar to CF-grammars, and can di-rectly be parsed by any bottom-upalgorithm.
For strong structurally completesystems such as P, spurious ambiguity canbe eliminated by inspecting left-branchingtrees only.
For flexible but not structurallycomplete systems, it is much harder to pre-dict which derivations are interesting andwhich ones are not, and therefore the onlysolution is often to inspect all possibilities.- 25  -R e f e r e n c e sAdes, Antony & Mark Steedman (1982), Onthe Order of Words, Linguistics & Phi-losophy 4, 517-518.Buszkowski, Wojciech (1988).
GenerativePower of Categorial Grammars.
In R. Oehrle,E.
Bach, and D. Wheeler (eds.
), CategorialGrammars and Natural Language Structures,Reidel, Dordrecht, 69-94.Wittenburg, Kent (1987), PredictiveCombinators: A Method for Efficient Pro-cessing of Combinatory Categorial Grammars.Proceedings of the 25th Annual Meeting ofthe Association for Computational Linguis-tics, Stanford University, 73-80.Zwarts, Frans (1986), Categoriale Grammaticaen Algebraische Semantiek, Dissertation,State University Groningen.Dowty, David (1988), Type Raising, Func-tional Composition, and Non-constituentConjunction.
In R. Oehrle, E. Bach, and D.Wheeler (eds.
), Categorial Grammars andNatural Language Structures, Reidel, Dor-drecht, 153-197.Lambek, Joachim (1958), The mathematics ofsentence structure.
American MathematicalMonthly 65, 154-170.Moortgat, Michael (1987), Lambek CategorialGrammar and the Autonomy Thesis.
INLworking papers 87-03.Moortgat, Michael (1988), Categorial Inves-tigations : Logical and Linguistic Aspects ofthe Lambek Calculus.
Dissertation, Univer-sity of Amsterdam.Pareschi, Remo and Mark Steedman (1987), ALazy Way to Chart-Parse with CategorialGrammars.
Proceedings of the 25th AnnualMeeting of the Association for ComputationalLinguistics, Stanford University, 81-88.Partee, Barbara and Mats Rooth (1983), Gen-eralized Conjunction and Type Ambiguity.
InR.
Bliuerle, C. Schwarze and A. yon Stechow(eds.)
Meaning, Use, and Interpretation ofLanguage, de Oruyter, Berlin, 361-383.Steedman, Mark (1985), Dependency andCoordination in the Grammar of Dutch andEnglish.
Language 61, 523-568.Wittenburg, Kent (1986), Natural LanguageParsing with Combinatory Categorial Gram-mars in a Graph-Unification.Based Formal-ism.
Ph.
D. dissertation, University of Texasat Austin.- 26  -
