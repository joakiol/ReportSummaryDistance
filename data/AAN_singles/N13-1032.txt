Proceedings of NAACL-HLT 2013, pages 315?324,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsImproving reordering performance using higher order and structuralfeaturesMitesh M. KhapraIBM Research Indiamikhapra@in.ibm.comAnanthakrishnan RamanathanIBM Research Indiaanandr42@gmail.comKarthik VisweswariahIBM Research Indiav-karthik@in.ibm.comAbstractRecent work has shown that word aligned datacan be used to learn a model for reorderingsource sentences to match the target order.This model learns the cost of putting a wordimmediately before another word and finds thebest reordering by solving an instance of theTraveling Salesman Problem (TSP).
However,for efficiently solving the TSP, the model isrestricted to pairwise features which examineonly a pair of words and their neighborhood.In this work, we go beyond these pairwise fea-tures and learn a model to rerank the n-bestreorderings produced by the TSP model us-ing higher order and structural features whichhelp in capturing longer range dependencies.In addition to using a more informative setof source side features, we also capture targetside features indirectly by using the transla-tion score assigned to a reordering.
Our exper-iments, involving Urdu-English, show that theproposed approach outperforms a state-of-the-art PBSMT system which uses the TSP modelfor reordering by 1.3 BLEU points, and a pub-licly available state-of-the-art MT system, Hi-ero, by 3 BLEU points.1 IntroductionHandling the differences in word orders betweenpairs of languages is crucial in producing good ma-chine translation.
This is especially true for lan-guage pairs such as Urdu-English which have sig-nificantly different sentence structures.
For exam-ple, the typical word order in Urdu is Subject ObjectVerb whereas the typical word order in English isSubject Verb Object.
Phrase based systems (Koehnet al 2003) rely on a lexicalized distortion model(Al-Onaizan and Papineni, 2006; Tillman, 2004)and the target language model to produce outputwords in the correct order.
This is known to be in-adequate when the languages are very different interms of word order (refer to Table 3 in Section 3).Pre-ordering source sentences while training andtesting has become a popular approach in overcom-ing the word ordering challenge.
Most techniquesfor pre-ordering (Collins et al 2005; Wang et al2007; Ramanathan et al 2009) depend on a highquality source language parser, which means thesemethods work only if the source language has aparser (this rules out many languages).
Recent work(Visweswariah et al 2011) has shown that it is pos-sible to learn a reordering model from a relativelysmall number of hand aligned sentences .
This elim-inates the need of a source or target parser.In this work, we build upon the work ofVisweswariah et al(2011) which solves the reorder-ing problem by treating it as an instance of theTraveling Salesman Problem (TSP).
They learn amodel which assigns costs to all pairs of words ina sentence, where the cost represents the penalty ofputting a word immediately preceding another word.The best permutation is found via the chained Lin-Kernighan heuristic for solving a TSP.
Since thismodel relies on solving a TSP efficiently, it cannotcapture features other than pairwise features that ex-amine the words and neighborhood for each pair ofwords in the source sentence.
In the remainder ofthis paper we refer to this model as the TSP model.Our aim is to go beyond this limitation of the TSPmodel and use a richer set of features instead of us-ing pairwise features only.
In particular, we are in-terested in features that allow us to examine triplesof words/POS tags in the candidate reordering per-315mutation (this is akin to going from bigram to tri-gram language models), and also structural featuresthat allow us to examine the properties of the seg-mentation induced by the candidate permutation.
Togo beyond the set of features incorporated by theTSP model, we do not solve the search problemwhich would be NP-hard.
Instead, we restrict our-selves to an n-best list produced by the base TSPmodel and then search in that list.
Using a richerset of features, we learn a model to rerank these n-best reorderings.
The parameters of the model arelearned using the averaged perceptron algorithm.
Inaddition to using a richer set of source side featureswe also indirectly capture target side features by in-terpolating the score assigned by our model with thescore assigned by the decoder of a MT system.To justify the use of these informative features,we point to the example in Table 1.
Here, the head(driver) of the underlined English Noun Phrase (Thedriver of the car) appears to the left of the NounPhrase whereas the head (chaalak {driver}) of thecorresponding Urdu Noun Phrase (gaadi {car} ka{of} chaalak {driver}) appears to the right of theNoun Phrase.
To produce the correct reordering ofthe source Urdu sentence the model has to make anunusual choice of putting gaadi {car} before bola{said}.
We say this is an unusual choice because themodel examines only pairwise features and it is un-likely that it would have seen sentences having thebigram ?car said?.
If the exact segmentation of thesource sentence was known, then the model couldhave used the information that the word gaadi {car}appears in a segment whose head is the noun chaalak{driver} and hence its not unusual to put gaadi {car}before bola {said} (because the construct ?NP said?is not unusual).
However, since the segmentationof the source sentence is not known in advance, weuse a heuristic (explained later) to find the segmen-tation induced by a reordering.
We then extractfeatures (such as first word current segment,end word current segment) to approximate theselong range dependencies.Using this richer set of features with Urdu-English as the source language pair, our approachoutperforms the following state of the art systems:(i) a PBSMT system which uses TSP model for re-ordering (by 1.3 BLEU points), (ii) a hierarchicalPBSMT system (by 3 BLEU points).
The overallInput Urdu: fir gaadi ka chaalak kuch bolaGloss: then car of driver said somethingEnglish: Then the driver of the car said something.Ref.
reordering: fir chaalak ka gaadi bola kuchTable 1: Example motivating the use of structural featuresgain is 6.3 BLEU points when compared to a stan-dard PBSMT system which uses a lexicalized distor-tion model (Al-Onaizan and Papineni, 2006).The rest of this paper is organized as follows.
InSection 2 we discuss our approach of re-ranking then-best reorderings produced by the TSP model.
Thisincludes a discussion of the model used, the featuresused and the algorithm used for learning the parame-ters of the model.
It also includes a discussion on themodification to the Chained Lin-Kernighan heuris-tic to produce n-best reorderings.
Next, in Section3 we describe our experimental setup and report theresults of our experiments.
In Section 4 we presentsome discussions based on our study.
In section 5 webriefly describe some prior related work.
Finally, inSection 6, we present some concluding remarks andhighlight possible directions for future work.2 Re-ranking using higher order andstructural featuresAs mentioned earlier, the TSP model (Visweswariahet al 2011) looks only at local features for a wordpair (wi, wj).
We believe that for better reorder-ing it is essential to look at higher order and struc-tural features (i.e., features which look at the overallstructure of a sentence).
The primary reason whyVisweswariah et al(2011) consider only pairwisebigram features is that with higher order features thereordering problem can no longer be cast as a TSPand hence cannot be solved using existing efficientheuristic solvers.
However, we do not have to dealwith an NP-Hard search problem because instead ofconsidering all possible reorderings we restrict oursearch space to only the n-best reorderings producedby the base TSP model.
Formally, given a set ofreorderings, ?
= [pi1, pi2, pi3, ...., pin], for a sourcesentence s, we are interesting in assigning a score,score(pi), to each of these reorderings and pick thereordering which has the highest score.
In this paper,we parametrize this score as:score(pi) = ?T?
(pi) (1)316where, ?
is the weight vector and ?
(pi) is a vectorof features extracted from the reordering pi.
The aimthen is to find,pi?
= arg maxpi?
?score(pi) (2)In the following sub-sections, we first brieflydescribe our overall approach towards finding pi?.Next, we describe our modification to the Lin-Kernighan heuristic for producing n-best outputsfor TSP instead of the 1-best output used by(Visweswariah et al 2011).
We then discuss the fea-tures used for re-ranking these n-best outputs, fol-lowed by a discussion on the learning algorithm usedfor estimating the parameters of the model.
Finally,we describe how we interpolate the score assignedby our model with the score assigned by the decoderof a SMT engine to indirectly capture target side fea-tures.2.1 Overall approachThe training stage of our approach involves twophases : (i) Training a TSP model which will beused to generate n-best reorderings and (ii) Traininga re-ranking model using these n-best reorderings.For training both the models we need a collectionof sentences where the desired reordering pi?
(x) foreach input sentence x is known.
These reference or-derings are derived from word aligned source-targetsentence pairs (see first 4 rows of Figure 1).
We firstdivide this word aligned data into N parts and useA?i to denote the alignments leaving out the i-thpart.
We then train a TSP model M?i using refer-ence reorderings derived from A?i as described in(Visweswariah et al 2011).
Next, we produce n-best reorderings for the source sentences using thealgorithm getNBestReorderings(sentence) de-scribed later.
Dividing the data into N parts is nec-essary to ensure that the re-ranking model is trainedusing a realistic n-best list rather than a very opti-mistic n-best list (which would be the case if part iis reordered using a model which has already seenpart i during training).Each of the n-best reorderings is then repre-sented as a feature vector comprising of higherorder and structural features.
The weightsof these features are then estimated using theaveraged perceptron method.
At test time,getNBestReorderings(sentence) is used to gen-erate the n-best reorderings for the test sentence us-ing the trained TSP model.
These reorderings arethen represented using higher order and structuralfeatures and re-ranked using the weights learned ear-lier.
We now describe the different stages of our al-gorithm.2.2 Generating n-best reorderings for the TSPmodelThe first stage of our approach is to train a TSPmodel and generate n-best reorderings using it.
Thedecoder used by Visweswariah et al(2011) relieson the Chained Lin-Kernighan heuristic (Lin andKernighan, 1973) to produce the 1-best permutationfor the TSP problem.
Since our algorithm aims atre-ranking an n-best list of permutations (reorder-ings), we made a modification to the Chained Lin-Kernighan heuristic to produce this n-best list asshown in Algorithm 1 .Algorithm 1 getNBestReorderings(sentence)NbestSet = ?pi?
= Identity permutationpi?
= linkernighan(pi?
)insert(NbestSet, pi?
)for i = 1?
nIter dopi?= perturb(pi?
)pi?= linkernighan(pi?
)if C(pi?)
< maxpi?NbestSetC(pi) thenInsertOrReplace(NbestSet, pi?
)end ifif C(pi?)
< C(pi?)
thenpi?
= pi?end ifend forIn Algorithm 1 perturb() is a four-edge pertur-bation described in (Applegate et al 2003), andlinkernighan() is the Lin-Kernighan heuristic thatapplies a sequence of flips that potentially returnsa lower cost permutation as described in (Lin andKernighan, 1973).
The cost C(pi) is calculated us-ing a trained TSP model.2.3 FeaturesWe represent each of the n-best reorderings obtainedabove as a vector of features which can be dividedinto two sets : (i) higher order features and (ii) struc-317Segmentation Based Features(extracted for every segment inthe induced segmentation)Features fired for the seg-ment [mere(PRP) ghar(NN)]in Figure1end lex current segment gharend lex prev segment Shyamend pos current segment NNend pos prev segment NNlength of current segment 2first lex current segment merefirst lex next segment aayefirst pos current segment PRPfirst pos next segment V RBHigher order features Features fired for the tripletShyam(NN) the(Vaux)aaye(VRB) in Figure1lex triplet jumps lex triplet = ?Shyam theaaye?
&& jumps = [4,?1]pos triplet jumps pos triplet = ?NN VauxVRB?
&& jumps = [4,?1]Table 2: Features used in our model.tural features.
The higher order features are es-sentially trigram lexical and pos features whereasthe structural features are derived from the sentencestructure induced by a reordering (explained later).2.3.1 Higher Order FeaturesSince deriving a good reordering would essen-tially require analyzing the syntactic structure of thesource sentence, the tasks of reordering and parsingare often considered to be related.
The main motiva-tion for using higher order features thus comes froma related work on parsing (Koo and Collins, 2010)where the performance of a state of the art parserwas improved by considering higher order depen-dencies.
In our model we use trigram features (seeTable 2) of the following form:?
(rui, rui+1, rui+2, J(rui, rui+1), J(rui+1, rui+2))where rui =word at position i in the reorderedsource sentence and J(x, y) = difference betweenthe positions of x and y in the original sourcesentence.Figure 1 shows an example of jumps between dif-ferent word pairs in an Urdu sentence.
Since suchhigher order features will typically be sparse, wealso use some back-off features.
For example, in-stead of using the absolute values of jumps we di-vide the jumps into 3 buckets, viz., high, low andmedium and use these buckets in conjunction withthe triplets as back-off features.Figure 1: Segmentation induced on the Urdu sentencewhen it is reordered according to its English translation.Note that the words Shyam and mere are adjacent to eachother in the original Urdu sentence but not in the re-ordered Urdu sentence.
Hence, the word mere marks thebeginning of a new segment.2.3.2 Structural FeaturesThe second set of features is based on the hy-pothesis that any reordering of the source sentenceinduces a segmentation on the sentence.
This seg-mentation is based on the following heuristic: if wiand wi+1 appear next to each other in the originalsentence but do not appear next to each other in thereordered sentence then wi marks the end of a seg-ment and wi+1 marks the beginning of the next seg-ment.
To understand this better please refer to Fig-ure 1 which shows the correct reordering of an Urdusentence based on its English translation and the cor-responding segmentation induced on the Urdu sen-tence.
If the correct segmentation of a sentence isknown in advance then one could use a hierarchicalmodel where the goal would be to reorder segmentsinstead of reordering words individually (basically,instead of words, treat segments as units of reorder-ing.
In principle, this is similar to what is done byparser based reordering methods).
Since the TSPmodel does not explicitly use segmentation basedfeatures it often produces wrong reorderings (referto the motivating example in Section 1).Reordering such sentences correctly requiressome knowledge about the hierarchical structure ofthe sentence.
To capture such hierarchical informa-tion, we use features which look at the elements318(words, pos tags) of a segment and its neighboringsegments.
These features along with examples arelisted in Table 2.
These features should help us inselecting a reordering which induces a segmentationwhich is closest to the correct segmentation inducedby the reference reordering.
Note that every featurelisted in Table 2 is a binary feature which takes onthe value 1 if it fires for the given reordering andvalue 0 if it does not fire for the given reordering.
Inaddition to the features listed in Table 2 we also usethe score assigned by the TSP model as a feature.2.4 Estimating model parametersWe use perceptron as the learning algorithm for es-timating the parameters of our model described inEquation 1.
To begin with, all parameters are ini-tialized to 0 and the learning algorithm is run for Niterations.
During each iteration the parameters areupdated after every training instance is seen.
For ex-ample, during the i-th iteration, after seeing the j-thtraining sentence, we update the k-th parameter ?kusing the following update rule:?
(i,j)k = ?
(i,j?1)k + ?k(pigoldj )?
?k(pi?j ) (3)where, ?
(i,j)k = value of the k-th parameter afterseeing sentence j in iteration i?k = k-th featurepigoldj = gold reordering for the j-th sentencepi?j = arg maxpi??j?(i,j?1)T?
(pi)where ?j is the set of n-best reorderings for the j-th sentence.
pi?j is thus the highest-scoring reorder-ing for the j-th sentence under the current parame-ter vector.
Since the averaged perceptron method isknown to perform better than the perceptron method,we used the averaged values of the parameters at theend of N iterations, calculated as:?avgk =1N ?
tN?i=1t?j=1?
(i,j)k (4)where, N = Number of iterationst = Number of training instancesWe observed that in most cases the reference re-ordering in not a part of the n-best list producedby the TSP model.
In such cases instead of using?k(pigoldj ) for updating the weights in Equation 3 weuse ?k(piclosest to goldj ) as this is known to be a betterstrategy for learning a re-ranking model (Arun andKoehn, 2007).
piclosest to goldj is given by:arg maxpiij?
?j# of common bigram pairs in piij and pigoldjlen(pigoldj )where, ?j = set of n-best reorderings for jth sentencepiclosest to goldj is thus the reordering which has themaximum overlap with pigoldj in terms of the numberof word pairs (wm, wn) where wn is put next to wm.2.5 Interpolating with MT scoreThe approach described above aims at producing abetter reordering by extracting richer features fromthe source sentence.
Since the final aim is to im-prove the performance of an MT system, it wouldpotentially be beneficial to interpolate the scores as-signed by Equation 1 to a given reordering with thescore assigned by the decoder of an MT system tothe translation of the source sentence under this re-ordering.
Intuitively, the MT score would allow usto capture features from the target sentence whichare obviously not available to our model.
With thismotivation, we use the following interpolated score(scoreI ) to select the best translation.scoreI(ti) = ??score?
(pii) + (1?
?)
?
scoreMT (ti)where, ti =translation produced under the i-threordering of the source sentencescore?
(pii) =score assigned by our model to thei-th reorderingscoreMT (ti) =score assigned by the MT system to tiThe weight ?
is used to ensure that score?
(pii) andscoreMT (pii) are in the same range (it just serves asa normalization constant).
We acknowledge that theabove process is expensive because it requires theMT system to decode n reorderings for every sourcesentence.
However, the aim of this work is to showthat interpolating with the MT score which implic-itly captures features from the target sentence helpsin improving the performance.
Ideally, this interpo-lation should (and can) be done at decode time with-out having to decode n reorderings for every source319sentence (for example by expressing the n reorder-ings as a lattice), but, we leave this as future work.3 Empirical evaluationWe evaluated our reordering approach on Urdu-English.
We use two types of evaluation, one in-trinsic and one extrinsic.
For intrinsic evaluation,we compare the reordered source sentence in Urduwith a reference reordering obtained from the handalignments using BLEU (referred to as monolingualBLEU or mBLEU by (Visweswariah et al 2011) ).Additionally, we evaluate the effect of reordering onMT performance using BLEU (extrinsic evaluation).As mentioned earlier, our training process in-volves two phases : (i) Generating n-best reorder-ings for the training data and (ii) using these n-bestreorderings to train a perceptron model.
We use thesame data for training the reordering model as wellas our perceptron model.
This data contains 180Kwords of manual alignments (part of the NIST MT-08 training data) and 3.9M words of automaticallygenerated machine alignments (1.7M words fromthe NIST MT-08 training data1 and 2.2M words ex-tracted from sources on the web2).
The machinealignments were generated using a supervised maxi-mum entropy model (Ittycheriah and Roukos, 2005)and then corrected using an improved correctionmodel (McCarley et al 2011).
We first divide thetraining data into 10 folds.
The n-best reorder-ings for each fold are then generated using a modeltrained on the remaining 9 folds.
This division into10 folds is done for reasons explained earlier in Sec-tion 2.1.
These n-best reorderings are then used totrain the perceptron model as described in Section2.4.
Note that Visweswariah et al(2011) used onlymanually aligned data for training the TSP model.However, we use machine aligned data in additionto manually aligned data for training the TSP modelas it leads to better performance.
We used this im-provised TSP model as the state of the art baseline(rows 2 and 3 in Tables 3 and 4 respectively) forcomparing with our approach.We observed that the perceptron algorithm con-verges after 5 iterations beyond which there is verylittle (<1%) improvement in the bigram precision on1http://www.ldc.upenn.edu2http://centralasiaonline.comthe training data itself (bigram precision is the frac-tion of word pairs which are correctly put next toeach other).
Hence, for all the numbers reported inthis paper, we used 5 iterations of perceptron train-ing.
Similarly, while generating the n-best reorder-ings, we experimented with following values of n :10, 25, 50, 100 and 200.
We observed that, by re-stricting the search space to the top-50 reorderingswe get the best reordering performance (mBLEU)on a development set.
Hence, we used n=50 for ourMT experiments.For intrinsic evaluation we use a development setof 8017 Urdu tokens reordered manually.
Table 3compares the performance of the top-1 reorderingoutput by our algorithm with the top-1 reorderinggenerated by the improved TSP model in terms ofmBLEU.
We see a gain of 1.8 mBLEU points withour approach.Next, we see the impact of the better reorderingsproduced by our system on the performance ofa state-of-the-art MT system.
For this, we useda standard phrase based system (Al-Onaizan andPapineni, 2006) with a lexicalized distortion modelwith a window size of +/-4 words (Tillmann andNey, 2003).
As mentioned earlier, our training dataconsisted of 3.9M words including the NIST MT-08training data.
We use HMM alignments along withhigher quality alignments from a supervised aligner(McCarley et al 2011).
The Gigaword Englishcorpus was used for building the English languagemodel.
We report results on the NIST MT-08evaluation set, averaging BLEU scores from theNews and Web conditions to provide a single BLEUscore.
Table 4 compares the MT performanceobtained by reordering the training and test datausing the following approaches:1.
No pre-ordering: A baseline system whichdoes not use any source side reordering as a pre-processing step2.
HIERO : A state of the art hierarchical phrasebased translation system (Chiang, 2007)3.
TSP: A system which uses the 1-best reorderingproduced by the TSP model4.
Higher order & structural features: A system320Approach mBLEUUnreordered 31.2TSP 56.6Higher order & structural features 58.4Table 3: mBLEU scores for Urdu to English reorderingusing different models.Approach BLEUNo pre-ordering 21.9HIERO 25.2TSP 26.9Higher order & structural features 27.5Interpolating with MT score 28.2Table 4: MT performance for Urdu to English without re-ordering and with reordering using different approaches.which reranks n-best reorderings produced by TSPusing higher order and structural features5.
Interpolating with MT score : A system whichinterpolates the score assigned to a reordering byour model with the score assigned by a MT systemWe used Joshua 4.0 (Ganitkevitch et al 2012)which provides an open source implementation ofHIERO.
For training, tuning and testing HIEROwe used the same experimental setup as describedabove.
As seen in Table 4, we get an overall gain of6.2 BLEU points with our approach as compared toa baseline system which does not use any reordering.More importantly, we outperform (i) a PBSMT sys-tem which uses the TSP model by 1.3 BLEU pointsand (ii) a state of the art hierarchical phrase basedtranslation system by 3 points.4 DiscussionsWe now discuss some error corrections and ablationtests.4.1 Example of error correctionWe first give an example where the proposed ap-proach performed better than the TSP model.
In theexample below, I = input sentence, E= gold Englishtranslation, T = incorrect reordering produced byTSP and O = correct reordering produced by ourapproach.
Note that the words roman catholic aurprotestant in the input sentence get translated asSentence length mBLEUUnreordered TSP Ourapproach1-14 words (small) 29.7 58.7 57.815-22 words (med.)
28.2 56.8 59.223+ words (long) 33.4 55.8 58.2All 31.2 56.6 58.4Table 5: mBLEU improvements on sentences of differentlengthsa continuous phrase in English (Roman Catholicand Protestant) and hence should be treated as asingle unit by the reordering model.
The TSP modelfails to keep this segment intact whereas our model(which uses segmentation based features) does soand matches the reference reordering.I: ab roman catholic aur protestant ke darmiyaanikhtilafat khatam ho chuke haiE: The differences between Roman Catholics andProtestants have now endedT: ab roman ikhtilafat ke darmiyaan catholic aurprotestant hai khatam ho chukeO: ab ikhtilafat ke darmiyaan roman catholic aurprotestant hai khatam ho chuke4.2 Performance based on sentence lengthWe split the test data into roughly three equal partsbased on length, and calculated the mBLEU im-provements on each of these parts as reported inTable 5.
These results show that the model worksmuch better for medium-to-long sentences.
In fact,we see a drop in performance for small sentences.
Apossible reason for this could be that the structuralfeatures that we use are derived through a heuristicthat is error-prone, and in shorter sentences, wherethere would be fewer reordering problems, these er-rors hurt more than they help.
While this needs to beanalyzed further, we could meanwhile combine thetwo models fruitfully by using the base TSP modelfor small sentences and the new model for longersentences.321Disabled feature mBLEUend lex current segment 57.6end lex prev segment 57.6end pos current segment 57.8end pos prev segment 57.4length 57.6lex triplet jumps 58.0pos triplet jumps 56.1first lex current segment 58.2first lex next segment 58.2first pos current segment 57.6first pos next segment 57.6NONE 58.4Table 6: Ablation test indicating the contribution of eachfeature to the reordering performance.4.3 Ablation testTo study the contribution of each feature to thereordering performance, we did an ablation testwherein we disabled one feature at a time and mea-sured the change in the mBLEU scores.
Table 6summarizes the results of our ablation test.
Themaximum drop in performance is obtained when thepos triplet jumps feature is disabled.
This obser-vation supports our claim that higher order features(more than bigrams) are essential for better reorder-ing.
The lex triplet jumps feature has the leastimpact on the performance mainly because it is alexicalized feature and hence very sparse.
Also notethat there is a high correlation between the perfor-mances obtained by dropping one feature from eachof the following pairs :i) first lex current segment, first lex next segmentii) first pos current segment, first pos next segmentiii) end lex current segment, end lex next segment.This is because these pairs of features arehighly dependent features.
Note that similar tothe pos triplet jumps feature we also tried apos quadruplet jumps feature but it did not help(mainly due to overfitting and sparsity).5 Related WorkThere are several studies which have shown that re-ordering the source side sentence to match the targetside order leads to improvements in Machine Trans-lation.
These approaches can be broadly classifiedinto three types.
First, approaches which reordersource sentences by applying rules to the source sideparse; the rules are either hand-written (Collins etal., 2005; Wang et al 2007; Ramanathan et al2009) or learned from data (Xia and McCord, 2004;Genzel, 2010; Visweswariah et al 2010).
Theseapproaches require a source side parser which isnot available for many languages.
The second typeof approaches treat machine translation decodingas a parsing problem by using source and/or tar-get side syntax in a Context Free Grammar frame-work.
These include Hierarchical models (Chi-ang, 2007) and syntax based models (Yamada andKnight, 2002; Galley et al 2006; Liu et al 2006;Zollmann and Venugopal, 2006).
The third type ofapproaches, avoid the use of a parser (as requiredby syntax based models) and instead train a reorder-ing model using reference reorderings derived fromaligned data.
These approaches (Tromble and Eis-ner, 2009; Visweswariah et al 2011; DeNero andUszkoreit, 2011; Neubig et al 2012) have a low de-code time complexity as reordering is done as a pre-processing step and not integrated with the decoder.Our work falls under the third category, as it im-proves upon the work of (Visweswariah et al 2011)which is closely related to the work of (Trombleand Eisner, 2009) but performs better.
The focusof our work is to use higher order and structuralfeatures (based on segmentation of the source sen-tence) which are not captured by their model.
Someother works have used collocation based segmenta-tion (Henr?
?quez Q. et al 2010) and Multiword Ex-pressions as segments (Bouamor et al 2012) to im-prove the performance of SMT but without muchsuccess.
The idea of improving performance by re-ranking a n-best list of outputs has been used re-cently for the related task of parsing (Katz-Brown etal., 2011) using targeted self-training for improvingthe performance of reordering.
However, in contrast,in our work we directly aim at improving the perfor-mance of a reordering model.6 ConclusionIn this work, we proposed a model for re-rankingthe n-best reorderings produced by a state of theart reordering model (TSP model) which is limitedto pair wise features.
Our model uses a more in-formative set of features consisting of higher orderfeatures, structural features and target side features322(captured indirectly using translation scores).
Theproblem of intractability is solved by restricting thesearch space to the n-best reorderings produced bythe TSP model.
A detailed ablation test shows thatof all the features used, the pos triplet features aremost informative for reordering.
A gain of 1.3 and 3BLEU points over a state of the art phrase based andhierarchical machine translation system respectivelyprovides good extrinsic validation of our claim thatsuch long range features are useful.As future work, we would like to evaluate our al-gorithm on other language pairs.
We also plan tointegrate the score assigned by our model into thedecoder to avoid having to do n decodings for ev-ery source sentence.
Also, it would be interestingto model the segmentation explicitly, where the aimwould be to first segment the sentence and then usea two level hierarchical reordering model which firstreorders these segments and then reorders the wordswithin the segment.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Dis-tortion models for statistical machine translation.
InProceedings of ACL, ACL-44, pages 529?536, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.David Applegate, William Cook, and Andre Rohe.
2003.Chained lin-kernighan for large traveling salesmanproblems.
In INFORMS Journal On Computing.Abhishek Arun and Philipp Koehn.
2007.
Onlinelearning methods for discriminative training of phrasebased statistical machine translation.
In In Proceed-ings of MT Summit.Dhouha Bouamor, Nasredine Semmar, and PierreZweigenbaum.
2012.
Identifying bilingual multi-word expressions for statistical machine translation.In Nicoletta Calzolari (Conference Chair), KhalidChoukri, Thierry Declerck, Mehmet Uur Doan, BenteMaegaard, Joseph Mariani, Jan Odijk, and SteliosPiperidis, editors, Proceedings of the Eight Interna-tional Conference on Language Resources and Eval-uation (LREC?12), Istanbul, Turkey, may.
EuropeanLanguage Resources Association (ELRA).David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Comput.
Linguist., 33(2):201?228, June.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL, pages 531?540,Morristown, NJ, USA.
Association for ComputationalLinguistics.John DeNero and Jakob Uszkoreit.
2011.
Inducing sen-tence structure from parallel corpora for reordering.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?11,pages 193?203, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and the 44th annual meeting of theAssociation for Computational Linguistics, ACL-44,pages 961?968, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,and Chris Callison-Burch.
2012.
Joshua 4.0: Pack-ing, pro, and paraphrases.
In Proceedings of theSeventh Workshop on Statistical Machine Translation,pages 283?291, Montre?al, Canada, June.
Associationfor Computational Linguistics.Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine transla-tion.
In Proceedings of the 23rd International Con-ference on Computational Linguistics, COLING ?10,pages 376?384, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.A.
Carlos Henr?
?quez Q., R. Marta Costa-jussa`, VidasDaudaravicius, E. Rafael Banchs, and B. Jose?
Marin?o.2010.
Using collocation segmentation to augment thephrase table.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Metric-sMATR, WMT ?10, pages 98?102, Stroudsburg, PA,USA.
Association for Computational Linguistics.Abraham Ittycheriah and Salim Roukos.
2005.
A max-imum entropy word aligner for Arabic-English ma-chine translation.
In Proceedings of HLT/EMNLP,HLT ?05, pages 89?96, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Jason Katz-Brown, Slav Petrov, Ryan McDonald, FranzOch, David Talbot, Hiroshi Ichikawa, Masakazu Seno,and Hideto Kazawa.
2011.
Training a parser formachine translation reordering.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?11, pages 183?192,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT-NAACL.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the 48th323Annual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 1?11, Stroudsburg, PA,USA.
Association for Computational Linguistics.S.
Lin and B. W. Kernighan.
1973.
An effective heuristicalgorithm for the travelling-salesman problem.
Oper-ations Research, pages 498?516.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thannual meeting of the Association for ComputationalLinguistics, ACL-44, pages 609?616, Stroudsburg,PA, USA.
Association for Computational Linguistics.J.
Scott McCarley, Abraham Ittycheriah, Salim Roukos,Bing Xiang, and Jian-ming Xu.
2011.
A correc-tion model for word alignments.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?11, pages 889?898,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Graham Neubig, Taro Watanabe, and Shinsuke Mori.2012.
Inducing a discriminative parser to optimizemachine translation reordering.
In Proceedings of the2012 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, pages 843?853, Jeju Island, Ko-rea, July.
Association for Computational Linguistics.Ananthakrishnan Ramanathan, Hansraj Choudhary,Avishek Ghosh, and Pushpak Bhattacharyya.
2009.Case markers and morphology: addressing the cruxof the fluency problem in English-Hindi smt.
InProceedings of ACL-IJCNLP.Christoph Tillman.
2004.
A unigram orientation modelfor statistical machine translation.
In Proceedings ofHLT-NAACL.Christoph Tillmann and Hermann Ney.
2003.
Word re-ordering and a dynamic programming beam search al-gorithm for statistical machine translation.
Computa-tional Linguistics, 29(1):97?133.Roy Tromble and Jason Eisner.
2009.
Learning linear or-dering problems for better translation.
In Proceedingsof EMNLP.Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,Vijil Chenthamarakshan, and Nandakishore Kamb-hatla.
2010.
Syntax based reordering with automat-ically derived rules for improved statistical machinetranslation.
In Proceedings of the 23rd InternationalConference on Computational Linguistics.Karthik Visweswariah, Rajakrishnan Rajkumar, AnkurGandhe, Ananthakrishnan Ramanathan, and JiriNavratil.
2011.
A word reordering model forimproved machine translation.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?11, pages 486?496,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Chao Wang, Michael Collins, and Philipp Koehn.
2007.Chinese syntactic reordering for statistical machinetranslation.
In Proceedings of EMNLP-CoNLL.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical MT system with automatically learned rewritepatterns.
In COLING.Kenji Yamada and Kevin Knight.
2002.
A decoder forsyntax-based statistical mt.
In Proceedings of ACL.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings on the Workshop on Statistical MachineTranslation.324
