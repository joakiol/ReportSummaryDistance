Proceedings of NAACL HLT 2007, pages 121?130,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsUnsupervised Resolution of Objects and Relations on the WebAlexander YatesTuring CenterComputer Science and EngineeringUniversity of WashingtonBox 352350Seattle, WA 98195, USAayates@cs.washington.eduOren EtzioniTuring CenterComputer Science and EngineeringUniversity of WashingtonBox 352350Seattle, WA 98195, USAetzioni@cs.washington.eduAbstractThe task of identifying synonymous re-lations and objects, or Synonym Resolu-tion (SR), is critical for high-quality infor-mation extraction.
The bulk of previousSR work assumed strong domain knowl-edge or hand-tagged training examples.This paper investigates SR in the con-text of unsupervised information extrac-tion, where neither is available.
The pa-per presents a scalable, fully-implementedsystem for SR that runs in O(KN log N)time in the number of extractions N andthe maximum number of synonyms perword, K. The system, called RESOLVER,introduces a probabilistic relational modelfor predicting whether two strings areco-referential based on the similarity ofthe assertions containing them.
Giventwo million assertions extracted from theWeb, RESOLVER resolves objects with78% precision and an estimated 68% re-call and resolves relations with 90% pre-cision and 35% recall.1 IntroductionWeb Information Extraction (WIE) sys-tems extract assertions that describe a rela-tion and its arguments from Web text (e.g.,(is capital of,D.C.,United States)).
WIE systemscan extract hundreds of millions of assertionscontaining millions of different strings from theWeb (e.g., the TEXTRUNNER system (Banko et al,2007)).1 WIE systems often extract assertions thatdescribe the same real-world object or relation usingdifferent names.
For example, a WIE system mightextract (is capital city of,Washington,U.S.
),which describes the same relationship as above butcontains a different name for the relation and eachargument.Synonyms are prevalent in text, and the Web cor-pus is no exception.
Our data set of two million as-sertions extracted from a Web crawl contained overa half-dozen different names each for the UnitedStates and Washington, D.C., and three for the ?iscapital of?
relation.
The top 80 most commonlyextracted objects had an average of 2.9 extractednames per entity, and several had as many as 10names.
The top 100 most commonly extracted re-lations had an average of 4.9 synonyms per relation.We refer to the problem of identifying synony-mous object and relation names as Synonym Res-olution (SR).2 An SR system for WIE takes a set ofassertions as input and returns a set of clusters, witheach cluster containing coreferential object stringsor relation strings.
Previous techniques for SR havefocused on one particular aspect of the problem, ei-ther objects or relations.
In addition, the techniqueseither depend on a large set of training examples, orare tailored to a specific domain by assuming knowl-edge of the domain?s schema.
Due to the numberand diversity of the relations extracted, these tech-1For a demo see www.cs.washington.edu/research/textrunner.2Ironically, SR has a number of synonyms in the literature,including Entity Resolution, Record Linkage, and Deduplica-tion.121niques are not feasible for WIE systems.
Schemataare not available for the Web, and hand-labelingtraining examples for each relation would require aprohibitive manual effort.In response, we present RESOLVER, a novel,domain-independent, unsupervised synonym resolu-tion system that applies to both objects and relations.RESOLVER clusters coreferential names together us-ing a probabilistic model informed by string similar-ity and the similarity of the assertions containing thenames.
Our contributions are:1.
A scalable clustering algorithm that runs intime O(KN log N) in the number of extrac-tions N and maximum number of synonymsper word, K, without discarding any poten-tially matching pair, under exceptionally weakassumptions about the data.2.
An unsupervised probabilistic model for pre-dicting whether two object or relation namesco-refer.3.
An empirical demonstration that RESOLVERcan resolve objects with 78% precision and68% recall, and relations with 90% precisionand 35% recall.The next section discusses previous work.
Section3 introduces our probabilistic model for SR. Section4 describes our clustering algorithm.
Section 5 de-scribes extensions to our basic SR system.
Section6 presents our experiments, and section 7 discussesour conclusions and areas for future work.2 Previous WorkThe DIRT algorithm (Lin and Pantel, 2001) ad-dresses a piece of the unsupervised SR problem.DIRT is a heuristic method for finding synonymousrelations, or ?inference rules.?
DIRT uses a depen-dency parser and mutual information statistics overa corpus to identify relations that have similar sets ofarguments.
In contrast, our algorithm provides a for-mal probabilistic model that applies equally well torelations and objects, and we provide an evaluationof the algorithm in terms of precision and recall.There are many unsupervised approaches for ob-ject resolution in databases, but unlike our algo-rithm these approaches depend on a known, fixedschema.
Ravikumar and Cohen (Ravikumar and Co-hen, 2004) present an unsupervised approach to ob-ject resolution using Expectation-Maximization ona hierarchical graphical model.
Several other re-cent approaches leverage domain-specific informa-tion and heuristics for object resolution.
For ex-ample, many (Dong et al, 2005; Bhattacharya andGetoor, 2005; Bhattacharya and Getoor, 2006) relyon evidence from observing which strings appear asarguments to the same relation simultaneously (e.g.,co-authors of the same publication).
While this isuseful information when resolving authors in the ci-tation domain, it is extremely rare to find relationswith similar properties in extracted assertions.
Noneof these approaches applies to the problem of resolv-ing relations.
See (Winkler, 1999) for a survey ofthis area.Several supervised learning techniques make en-tity resolution decisions (Kehler, 1997; McCallumand Wellner, 2004; Singla and Domingos, 2006), butof course these systems depend on the availabilityof training data, and often on a significant numberof labeled examples per relation of interest.
Theseapproaches also depend on complex probabilisticmodels and learning algorithms, and they have orderO(n3) time complexity, or worse.
They currently donot scale to the amounts of data extracted from theWeb.
Previous systems were tested on at most a fewthousand examples, compared with millions or hun-dreds of millions of extractions from WIE systemssuch as TEXTRUNNER.Coreference resolution systems (e.g., (Lappin andLeass, 1994; Ng and Cardie, 2002)), like SR sys-tems, try to merge references to the same object (typ-ically pronouns, but potentially other types of nounphrases).
This problem differs from the SR problemin several ways: first, it deals with unstructered textinput, possibly with syntactic annotation, rather thanrelational input.
Second, it deals only with resolv-ing objects.
Finally, it requires local decisions aboutstrings; that is, the same word may appear twice in atext and refer to two different things, so each occur-rence of a word must be treated separately.The PASCAL Recognising Textual EntailmentChallenge proposes the task of recognizing whentwo sentences entail one another, and many authorshave submitted responses to this challenge (Dagan etal., 2006).
Synonym resolution is a subtask of thisproblem.
Our task differs significantly from the tex-tual entailment task in that it has no labeled training122data, and its input is in the form of relational extrac-tions rather than raw text.Two probabilistic models for information extrac-tion have a connection with ours.
Our probabilisticmodel is partly inspired by the ball-and-urns abstrac-tion of information extraction presented by Downeyet al (2005) Our task and probability model are dif-ferent from theirs, but we make many of the samemodeling assumptions.
Second, we follow Snow etal.
?s work (2006) on taxonomy induction in incorpo-rating transitive closure constraints in our probabil-ity calculations, as explained below.3 Probabilistic ModelOur probabilistic model provides a formal, rigorousmethod for resolving synonyms in the absence oftraining data.
It has two sources of evidence: thesimilarity of the strings themselves (i.e., edit dis-tance) and the similarity of the assertions they ap-pear in.
This second source of evidence is some-times referred to as ?distributional similarity?
(Hin-dle, 1990).Section 3.2 presents a simple model for predict-ing whether a pair of strings co-refer based on stringsimilarity.
Section 3.3 then presents a model calledthe Extracted Shared Property (ESP) Model for pre-dicting whether a pair of strings co-refer based ontheir distributional similarity.
Finally, a method ispresented for combining these models to come upwith an overall prediction for coreference decisionsbetween two clusters of strings.3.1 Terminology and NotationWe use the following notation to describe the proba-bilistic models.
The input is a data set D containingextracted assertions of the form a = (r, o1, .
.
.
, on),where r is a relation string and each oi is an objectstring representing the arguments to the relation.
Inour data, all of the extracted assertions are binary, son = 2.
The subset of all assertions in D containinga string s is called Ds.For strings si and sj , let Ri,j be the random vari-able for the event that si and sj refer to the sameentity.
Let Rti,j denote the event that Ri,j is true,and Rfi,j denote the event that it is false.A pair of strings (r, s2) is called a property ofa string s1 if there is an assertion (r, s1, s2) ?
Dor (r, s2, s1) ?
D. A pair of strings (s1, s2) isan instance of a string r if there is an assertion(r, s1, s2) ?
D. Equivalently, the property p =(r, s2) applies to s1, and the relation r applies tothe instance i = (s1, s2).
Finally, two strings x andy share a property (or instance) if both x and y areextracted with the same property (or instance).3.2 String Similarity ModelMany objects appear with multiple names that aresubstrings, acronyms, abbreviations, or other sim-ple variations of one another.
Thus string similaritycan be an important source of evidence for whethertwo strings co-refer.
Our probabilistic String Sim-ilarity Model (SSM) assumes a similarity functionsim(s1, s2): STRING?
STRING ?
[0, 1].
Themodel sets the probability of s1 co-referring with s2to a smoothed version of the similarity:P (Rti,j |sim(s1, s2)) =?
?
sim(s1, s2) + 1?+ ?The particular choice of ?
and ?
make little differ-ence to our results, so long as they are chosen suchthat the resulting probability can never be one orzero.
In our experiments ?
= 20 and ?
= 5, and weuse the well-known Monge-Elkan string similarityfunction for objects and the Levenshtein string edit-distance function for relations (Cohen et al, 2003).3.3 The Extracted Shared Property ModelThe Extracted Shared Property (ESP) Model out-puts the probability that s1 and s2 co-referbased on how many properties (or instances) theyshare.
As an example, consider the strings?Mars?
and ?Red Planet?, which appear in ourdata 659 and 26 times respectively.
Out ofthese extracted assertions, they share four proper-ties.
For example, (lacks,Mars, ozone layer) and(lacks,Red P lanet, ozone layer) both appear asassertions in our data.
The ESP model determinesthe probability that ?Mars?
and ?Red Planet?
referto the same entity after observing k, the number ofproperties that apply to both, n1, the total numberof extracted properties for ?Mars?, and n2, the totalnumber of extracted properties for ?Red Planet.
?ESP models the extraction of assertions as agenerative process, much like the URNS model(Downey et al, 2005).
For each string si, a certain123number, Pi, of properties of the string are written onballs and placed in an urn.
Extracting ni assertionsthat contain si amounts to selecting a subset of sizeni from these labeled balls.3 Properties in the urn arecalled potential properties to distinguish them fromextracted properties.To model coreference decisions, ESP uses a pairof urns, containing Pi and Pj balls respectively, forthe two strings si and sj .
Some subset of the Piballs have the exact same labels as an equal-sizedsubset of the Pj balls.
Let the size of this sub-set be Si,j .
The ESP model assumes that corefer-ential strings share as many potential properties aspossible, though only a few of the potential proper-ties will be extracted for both.
For non-coreferentialstrings, the number of shared potential properties is astrict subset of the potential properties of each string.Thus if Ri,j is true then Si,j = min(Pi, Pj), and ifRi,j is false then Si,j < min(Pi, Pj).The ESP model makes several simplifying as-sumptions in order to make probability predictions.As is suggested by the ball-and-urn abstraction, itassumes that each ball for a string is equally likelyto be selected from its urn.
Because of data sparsity,almost all properties are very rare, so it would be dif-ficult to get a better estimate for the prior probabilityof selecting a particular potential property.
Second,it assumes that without knowing the value of k, ev-ery value of Si,j is equally likely, since we have nobetter information.
Finally, it assumes that all sub-sets of potential properties are equally likely to beshared by two non-coreferential objects, regardlessof the particular labels on the balls, given the size ofthe shared subset.Given these assumptions, we can derive an ex-pression for P (Rti,j).
First, note that there are(Pini)(Pjnj) total ways of extracting ni and nj asser-tions for si and sj .
Given a particular value of Si,j ,the number of ways in which ni and nj assertionscan be extracted such that they share exactly k isgiven byCount(k, ni, nj |Pi, Pj , Si,j) =(Si,jk)?r,s?0(Si,j?kr+s)(r+sr)( Pi?Si,jni?
(k+r))( Pj?Si,jnj?
(k+s))By our assumptions,3Unlike the URNS model, balls are drawn without replace-ment because each extracted property is distinct in our data.P (k|ni, nj , Pi, Pj , Si,j) =Count(k, ni, nj |Pi, Pj , Si,j)(Pini)(Pjnj) (1)Let Pmin = min(Pi, Pj).
The result below fol-lows from Bayes?
Rule and our assumptions above:Proposition 1 If two strings si and sj have Pi andPj potential properties (or instances), and they ap-pear in extracted assertions Di and Dj such that|Di| = ni and |Dj | = nj , and they share k extractedproperties (or instances), the probability that si andsj co-refer is:P (Rti,j |Di, Dj , Pi, Pj) =P (k|ni, nj , Pi, Pj , Si,j = Pmin)?k?Si,j?Pmin P (k|ni, nj , Pi, Pj , Si,j)(2)Substituting equation 1 into equation 2 gives us acomplete expression for the probability we are look-ing for.Note that the probability for Ri,j depends on justtwo hidden parameters, Pi and Pj .
Since we haveno labeled data to estimate these parameters from,we tie these parameters to the number of times therespective strings si and sj are extracted.
Thus weset Pi = N ?
ni, and we set N = 50 in our experi-ments.3.4 Combining the EvidenceFor each potential coreference relationship Ri,j ,there are now two pieces of probabilistic evidence.Let Eei,j be the evidence for ESP, and let Esi,j be theevidence for SSM.
Our method for combining thetwo uses the Na?
?ve Bayes assumption that each pieceof evidence is conditionally independent, given thecoreference relation:P (Esi,j , Eei,j |Ri,j) = P (Esi,j |Ri,j)P (Eei,j |Ri,j)Given this simplifying assumption, we can com-bine the evidence to find the probability of a cofer-ence relationship by applying Bayes?
Rule to bothsides (we omit the i, j indices for brevity):P (Rt|Es, Ee) =P (Rt|Es)P (Rt|Ee)(1?
P (Rt))?i?
{t,f} P (Ri|Es)P (Ri|Ee)(1?
P (Ri))1243.5 Comparing Clusters of StringsOur algorithm merges clusters of strings with oneanother, using one of the above models.
However,these models give probabilities for coreference deci-sions between two individual strings, not two clus-ters of strings.We follow the work of Snow et al (2006) in in-corporating transitive closure constraints in proba-bilistic modeling, and make the same independenceassumptions.
The benefit of this approach is that thecalculation for merging two clusters depends onlyon coreference decisions between individual strings,which can be calculated independently.Let a clustering be a set of coreference relation-ships between pairs of strings such that the corefer-ence relationships obey the transitive closure prop-erty.
We let the probability of a set of assertions Dgiven a clustering C be:P (D|C) =?Rti,j?CP (Di ?Dj |Rti,j)?
?Rfi,j?CP (Di ?Dj |Rfi,j)The metric used to determine if two clustersshould be merged is the likelihood ratio, or the prob-ability for the set of assertions given the mergedclusters over the probability given the original clus-tering.
Let C ?
be a clustering that differs from Conly in that two clusters in C have been merged inC ?, and let ?C be the set of coreference relation-ships in C ?
that are true, but the corresponding onesin C are false.
This metric is given by:P (D|C ?
)/P (D|C) =?Rti,j?
?C P (Rti,j |Di ?Dj)(1?
P (Rti,j))?Rti,j??C(1?
P (Rti,j |Di ?Dj))P (Rti,j)The probability P (Rti,j |Di?Dj) may be suppliedby the SSM, ESP, or combination model.
In our ex-periments, we let the prior for the SSM model be0.5.
For the ESP and combined models, we set theprior to P (Rti,j) = 1min(P1,P2) .4 RESOLVER?s Clustering AlgorithmOur clustering algorithm iteratively merges clustersof co-referential names, making each iteration inS := set of all stringsFor each property or instance p,Sp := {s ?
S|s has property p}1.
Scores := {}2.
Build index mapping properties (and instances)to strings with those properties (instances)3.
For each property or instance p:If |Sp| < Max:For each pair {s1, s2} ?
Sp:Add mergeScore(s1, s2) to Scores4.
Repeat until no merges can be performed:Sort ScoresUsedClusters := {}While score of top clusters c1, c2is above Threshold:Skip if either is in UsedClustersMerge c1 and c2Add c1, c2 to UsedClustersMerge properties containing c1, c2Recalculate merge scores as in Steps 1-3Figure 1: RESOLVER?s Clustering Algorithmtime O(N log N) in the number of extracted as-sertions.
The algorithm requires only basic assump-tions about which strings to compare.
Previous workon speeding up clustering algorithms for SR has ei-ther required far stronger assumptions, or else it hasfocused on heuristic methods that remain, in theworst case, O(N2) in the number of distinct objects.Our algorithm, a greedy agglomerative clusteringmethod, is outlined in Figure 1.
The first novel partof the algorithm, step 3, compares pairs of stringsthat share the same property or instance, so long asno more than Max strings share that same propertyor instance.
After the scores for all comparisons aremade, each string is assigned its own cluster.
Thenthe scores are sorted and the best cluster pairs aremerged until no pair of clusters has a score abovethreshold.
The second novel aspect of this algorithmis that as it merges clusters in Step 4, it merges prop-erties containing those clusters in a process we callmutual recursion, which is discussed below.This algorithm compares every pair of clustersthat have the potential to be merged, assuming twoproperties of the data.
First, it assumes that pairsof clusters with no shared properties are not worth125comparing.
Since the number of shared propertiesis a key source of evidence for our approach, theseclusters almost certainly will not be merged, even ifthey are compared, so the assumption is quite rea-sonable.
Second, the approach assumes that clus-ters sharing only properties that apply to very manystrings (more than Max) need not be compared.Since properties shared by many strings provide lit-tle evidence that the strings are coreferential, this as-sumption is reasonable for SR. We use Max = 50in our experiments.
Less than 0.1% of the propertiesare thrown out using this cutoff.4.1 Algorithm AnalysisLet D be the set of extracted assertions.
The follow-ing analysis shows that one iteration of merges takestime O(N log N), where N = |D|.
Let NC bethe number of comparisons between strings in step3.
To simplify the analysis, we consider only thoseproperties that contain a relation string and an argu-ment 1 string.
Let A be the set of all such properties.NC is linear in N :4NC =?p?A|Sp| ?
(|Sp| ?
1)2?
(Max?
1)2 ?
?p?A|Sp|= (Max?
1)2 ?NNote that this bound is quite loose because mostproperties apply to only a few strings.
Step 4 re-quires time O(N log N) to sort the comparisonscores and perform one iteration of merges.
If thelargest cluster has size K, in the worst case the al-gorithm will take K iterations.
In our experiments,the algorithm never took more than 9 iterations.4.2 Relation to other speed-up techniquesThe merge/purge algorithm (Hernandez and Stolfo,1995) assumes the existence of a particular attributesuch that when the data set is sorted on this attribute,matching pairs will all appear within a narrow win-dow of one another.
This algorithm is O(M log M)where M is the number of distinct strings.
However,there is no attribute or set of attributes that comes4If the Max parameter is allowed to vary with log|D|,rather than remaining constant, the same analysis leads to aslightly looser bound that is still better than O(N2).close to satisfying this assumption in the context ofdomain-independent information extraction.There are several techniques that often providespeed-ups in practice, but in the worst case theymake O(M2) comparisons at each merge iteration,where M is the number of distinct strings.
This cancause problems on very large data sets.
Notably,McCallum et al (2000) use a cheap comparisonmetric to place objects into overlapping ?canopies,?and then use a more expensive metric to cluster ob-jects appearing in the same canopy.
The RESOLVERclustering algorithm is in fact an adaptation of thecanopy method; it adds the restriction that strings arenot compared when they share only high-frequencyproperties.
The canopy method works well on high-dimensional data with many clusters, which is thecase with our problem, but its time complexity isworse than ours.For information extraction data, a complexity ofO(M2) in the number of distinct strings turns outto be considerably worse than our algorithm?s com-plexity of O(N log N) in the number of extractedassertions.
This is because the data obeys a Zipf lawrelationship between the frequency of a string and itsrank, so the number of distinct strings grows linearlyor almost linearly with the number of assertions.54.3 Mutual RecursionMutual recursion refers to the novel property ofour algorithm that as it clusters relation strings to-gether into sets of synonyms, it collapses proper-ties together for object strings and potentially findsmore shared properties between coreferential objectstrings.
Likewise, as it clusters objects together intosets of coreferential names, it collapses instances ofrelations together and potentially finds more sharedinstances between coreferential relations.
Thus theclustering decisions for relations and objects mutu-ally depend on one another.For example, the strings ?Kennedy?
and ?Pres-ident Kennedy?
appear in 430 and 97 assertionsin our data, respectively, but none of their ex-tracted properties match exactly.
Many properties,5The exact relationship depends on the shape parameter zof the Zipf curve.
If z < 1, as it is for our data set, the num-ber of total extractions grows linearly with the number of dis-tinct strings extracted.
If z = 1, then n extractions will contain?
( nln n ) distinct strings.126however, almost match.
For example, the asser-tions (challenged,Kennedy,Premier Krushchev)and (stood up to,President Kennedy,Kruschev)both appear in our data.
Because ?challenged?
and?stood up to?
are similar, and ?Krushchev?
and ?Pre-mier Krushchev?
are similar, our algorithm is ableto merge these pairs into two clusters, thereby creat-ing a new shared property between ?Kennedy?
and?President Kennedy.?
Eventually it can merge thesetwo strings as well.5 Extensions to RESOLVERWhile the basic RESOLVER system can cluster syn-onyms accurately and quickly, there is one type oferror that it frequently makes.
In some cases, it hasdifficulty distinguishing between similar pairs of ob-jects and identical pairs.
For example, ?Virginia?and ?West Virginia?
share several extractions be-cause they have the same type, and they have highstring similarity.
As a result, RESOLVER clustersthese two together.
The next two sections describetwo extensions to RESOLVER that address the prob-lem of similarity vs. identity.5.1 Function FilteringRESOLVER can use functions and one-to-one rela-tions to help distinguish between similar and identi-cal pairs.
For example, West Virginia and Virginiahave different capitals: Richmond and Charleston,respectively.
If both of these facts are extracted, andif RESOLVER knows that the ?capital of?
relation isfunctional, it should prevent Virginia and West Vir-ginia from merging.The Function Filter prevents merges betweenstrings that have different values for the same func-tion.
More precisely, it decides that two strings y1and y2 match if their string similarity is above a highthreshold.
It prevents a merge between strings x1and x2 if there exist a function f and extractionsf(x1, y1) and f(x2, y2), and there are no such ex-tractions such that y1 and y2 match (and vice versafor one-to-one relations).
Experiments described insection 6 show that the Function Filter can improvethe precision of RESOLVER without significantly af-fecting its recall.While the Function Filter currently uses func-tions and one-to-one relations as negative evidence,it is also possible to use them as positive evidence.For example, the relation ?married?
is not strictlyone-to-one, but for most people the set of spousesis very small.
If a pair of strings are extractedwith the same spouse?e.g., ?FDR?
and ?PresidentRoosevelt?
share the property (?married?, ?EleanorRoosevelt?
)?this is far stronger evidence that thetwo strings are identical than if they shared somerandom property.Unfortunately, various techniques that attemptedto model this insight, including a TF-IDF weightingof properties, yielded essentially no improvement ofRESOLVER.
One major reason is that there are rel-atively few examples of shared functional or one-to-one properties because of sparsity.
This idea de-serves more investigation, however, and is an areafor future work.5.2 Using Web HitcountsWhile names for two similar objects may often ap-pear together in the same sentence, it is relativelyrare for two different names of the same object toappear in the same sentence.
RESOLVER exploitsthis fact by querying the Web to determine how oftena pair of strings appears together in a large corpus.When the hitcount is high, RESOLVER can preventthe merge.Specifically, the Coordination-Phrase Filtersearches for hitcounts of the phrase ?s1 and s2?,where s1 and s2 are a candidate pair for merging.It then computes a variant of pointwise mutualinformation, given bycoordination score(s1, s2) = hits(s1 and s2)2hits(s1)?
hits(s2)The filter prevents any merge for which the coor-dination score is above a threshold, which is de-termined on a development set.
The results ofCoordination-Phrase filtering are discussed in thenext section.6 ExperimentsOur experiments demonstrate that the ESP modelis significantly better at resolving synonyms than awidely-used distributional similarity metric, the co-sine similarity metric (CSM) (Salton and McGill,1983), and that RESOLVER is significantly better at127resolving synonyms than either of its components,SSM or ESP.We test these models on a data set of 2.1 millionassertions extracted from a Web crawl.6 All modelsran over all assertions, but compared only those ob-jects or relations that appeared at least 25 times inthe data, to give the ESP and CSM models sufficientdata for estimating similarity.
However, the mod-els do use strings that appear less than 25 times asfeatures.
In all, the data contains 9,797 distinct ob-ject strings and 10,151 distinct relation strings thatappear at least 25 times.We judged the precision of each model by manu-ally labeling all of the clusters that each model out-puts.
Judging recall would require inspecting notjust the clusters that the system outputs, but the en-tire data set, to find all of the true clusters.
Be-cause of the size of the data set, we instead esti-mated recall over a smaller subset of the data.
Wetook the top 200 most frequent object strings and top200 most frequent relation strings in the data.
Foreach one of these high-frequency strings, we man-ually searched through all strings with frequencyover 25 that shared at least one property, as wellas all strings that contained one of the keywords inthe high-frequency strings or obvious variations ofthem.
We manually clustered the resulting matches.The top 200 object strings formed 51 clusters of sizegreater than one, with an average cluster size of 2.9.For relations, the top 200 strings and their matchesformed 110 clusters with size greater than one, withan average cluster size of 4.9.
We measured the re-call of our models by comparing the set of all clus-ters containing at least one of the high-frequencywords against these gold standard clusters.For our precision and recall measures, we onlycompare clusters of size two or more, in order tofocus on the interesting cases.
Using the term hy-pothesis cluster for clusters created by one of themodels, we define the precision of a model to be thenumber of elements in all hypothesis clusters whichare correct divided by the total number of elementsin hypothesis clusters.
An element s is marked cor-rect if a plurality of the elements in s?s cluster referto the same entity as s; we break ties arbitrarily, as6The data is made available athttp://www.cs.washington.edu/homes/ayates/.they do not affect results.
We define recall as thesum over gold standard clusters of the most num-ber of elements found in a single hypothesis cluster,divided by the total number of elements in gold stan-dard clusters.For the ESP and SSM models in our experiment,we prevented mutual recursion by clustering rela-tions and objects separately.
Only the full RE-SOLVER system uses mutual recursion.
For the CSMmodel, we create for each distinct string a row vec-tor, with each column representing a property.
If thatproperty applies to the string, we set the value ofthat column to the inverse frequency of the propertyand zero otherwise.
CSM finds the cosine of the an-gle between the vectors for each pair of strings, andmerges the best pairs that score above threshold.Each model requires a threshold parameter to de-termine which scores are suitable for merging.
Forthese experiments we arbitrarily chose a thresholdof 3 for the ESP model (that is, the data needs tobe 3 times more likely given the merged cluster thanthe unmerged clusters in order to perform the merge)and chose thresholds for the other models by hand sothat the difference between them and ESP would beroughly even between precision and recall, althoughfor relations it was harder to improve the recall.
It isan important item for future work to be able to esti-mate these thresholds and perhaps other parametersof our models from unlabeled data, but the chosenparameters worked well enough for the experiments.Table 1 shows the precision and recall of our models.6.1 DiscussionESP significantly outperforms CSM on both objectand relation clustering.
CSM had particular troublewith lower-frequency strings, judging far too manyof them to be co-referential on too little evidence.
Ifthe threshold for clustering using CSM is increased,however, the recall begins to approach zero.ESP and CSM make predictions based on a verynoisy signal.
?Canada,?
for example, shares moreproperties with ?United States?
in our data than?U.S.?
does, even though ?Canada?
appears less of-ten than ?U.S.?
The results show that both modelsperform below the SSM model on its own for objectmerging, and both perform slightly better than SSMon relations because of SSM?s poor recall.We found a significant improvement in both pre-128Objects RelationsModel Prec.
Rec.
F1 Prec.
Rec.
F1CSM 0.51 0.36 0.42 0.62 0.29 0.40ESP 0.56 0.41 0.47 0.79 0.33 0.47SSM 0.62 0.53 0.57 0.85 0.25 0.39RESOLVER 0.71 0.66 0.68 0.90 0.35 0.50Table 1: Comparison of the cosine similarity metric (CSM), RESOLVER components (SSM and ESP), and the RESOLVERsystem.
Bold indicates the score is significantly different from the score in the row above at p < 0.05 using the chi-squared testwith one degree of freedom.
Using the same test, RESOLVER is also significantly different from ESP and CSM in recall on objects,and from CSM and SSM in recall on relations.
RESOLVER?s F1 on objects is a 19% increase over SSM?s F1.
RESOLVER?s F1 onrelations is a 28% increase over SSM?s F1.cision and recall when using a combined model overusing SSM alone.
RESOLVER?s F1 is 19% higherthan SSM?s on objects, and 28% higher on relations.In a separate experiment we found that mutual re-cursion provides mixed results.
A combination ofSSM and ESP without mutual recursion had a preci-sion of 0.76 and recall of 0.59 on objects, and a pre-cision of 0.91 and recall of 0.35 on relations.
Mutualrecursion increased recall and decreased precisionfor both objects and relations.
None of the differ-ences were statistically significant, however.There is clearly room for improvement on the SRtask.
Except for the problem of confusing similarand identical pairs (see section 5), error analysisshows that most of RESOLVER?s mistakes are be-cause of two kinds of errors:1.
Extraction errors.
For example, ?US News?gets extracted separately from ?World Report?, andthen RESOLVER clusters them together because theyshare almost all of the same properties.2.
Multiple word senses.
For example, there are twoPresident Bushes; also, there are many terms like?President?
and ?Army?
that can refer to many dif-ferent entities.6.2 Experiments with ExtensionsThe extensions to RESOLVER attempt to addressthe confusion between similar and identical pairs.Experiments with the extensions, using the samedatasets and metrics as above, demonstrate that theFunction Filter (FF) and the Coordination-PhraseFilter (CPF) boost RESOLVER?s performance.FF requires as input the set of functional and one-to-one relations in the data.
Table 2 contains a sam-is capital of is capital city ofnamed after was named afterheadquartered in is headquartered inTable 2: A sample of the set of functions used by the Func-tion Filter.Model Prec.
Rec.
F1RESOLVER 0.71 0.66 0.68RESOLVER+FF 0.74 0.66 0.70RESOLVER+CPF 0.78 0.68 0.73RESOLVER+FF+CPF 0.78 0.68 0.73Table 3: Comparison of object merging results for theRESOLVER system, RESOLVER plus Function Filtering (RE-SOLVER+FF), RESOLVER plus Coordination-Phrase Filter-ing (RESOLVER+CPF), and RESOLVER plus both types of fil-tering (RESOLVER+FF+CPF).
Bold indicates the score is sig-nificantly different from RESOLVER?s score at p < 0.05 us-ing the chi-squared test with one degree of freedom.
RE-SOLVER+CPF?s F1 on objects is a 28% increase over SSM?sF1, and a 7% increase over RESOLVER?s F1.pling of the manually-selected functions used in ourexperiment.
Automatically discovering such func-tions from extractions has been addressed in Ana-Maria Popescu?s dissertation (Popescu, 2007), andwe did not attempt to duplicate this effort in RE-SOLVER.Table 3 contains the results of our experiments.With coordination-phrase filtering, RESOLVER?s F1is 28% higher than SSM?s on objects, and 6% higherthan RESOLVER?s F1 without filtering.
While func-tion filtering is a promising idea, FF provides asmaller benefit than CPF on this dataset, and the129merges that it prevents are, with a few exceptions,a subset of the merges prevented by CPF.
This is inpart due to the limited number of functions availablein the data.
In addition to outperforming FF on thisdataset, CPF has the added advantage that it does notrequire additional input, like a set of functions.7 Conclusion and Future WorkWe have shown that the unsupervised and scalableRESOLVER system is able to find clusters of co-referential object names in extracted relations witha precision of 78% and a recall of 68% with the aidof coordination-phrase filtering, and can find clus-ters of co-referential relation names with precisionof 90% and recall of 35%.
We have demonstratedsignificant improvements over using simple similar-ity metrics for this task by employing a novel prob-abilistic model of coreference.In future work, we plan to use RESOLVER on amuch larger data set of over a hundred million as-sertions, further testing its scalability and its abil-ity to improve in accuracy given additional data.We also plan to add techniques for handling mul-tiple word senses.
Finally, to make the probabilisticmodel more accurate and easier to use, we plan toinvestigate methods for automatically estimating itsparameters from unlabeled data.AcknowledgementsThis research was supported in part by NSF grantsIIS-0535284 and IIS-0312988, DARPA contractNBCHD030010, ONR grant N00014-05-1-0185 aswell as gifts from Google, and carried out at the Uni-versity of Washington?s Turing Center.
We thankDoug Downey, Michele Banko, Stef Schoenmack-ers, Dan Weld, Fei Wu, and the anonymous review-ers for their helpful comments on previous drafts.ReferencesM.
Banko, M. J. Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extractionfrom the web.
In IJCAI.I.
Bhattacharya and L. Getoor.
2005.
Relational Clus-tering for Multi-type Entity Resolution.
In 11th ACMSIGKDD Workshop on Multi Relational Data Mining.I.
Bhattacharya and L. Getoor.
2006.
Query-time entityresolution.
In KDD.W.W.
Cohen, P. Ravikumar, and S.E.
Fienberg.
2003.A comparison of string distance metrics for name-matching tasks.
In IIWeb.I.
Dagan, O. Glickman, and B. Magnini.
2006.
The PAS-CAL Recognising Textual Entailment Challenge.
Lec-ture Notes in Computer Science, 3944:177?190.X.
Dong, A.Y.
Halevy, and J. Madhavan.
2005.
Refer-ence reconciliation in complex information spaces.
InSIGMOD.D.
Downey, O. Etzioni, and S. Soderland.
2005.
A Prob-abilistic Model of Redundancy in Information Extrac-tion.
In IJCAI.M.
A. Hernandez and S. J. Stolfo.
1995.
Themerge/purge problem for large databases.
In SIG-MOD.D.
Hindle.
1990.
Noun classification from predicage-argument structures.
In ACL.A.
Kehler.
1997.
Probabilistic coreference in informa-tion extraction.
In EMNLP.S.
Lappin and H. J. Leass.
1994.
An algorithm forpronominal anaphora resolution.
Computational Lin-guistics, 20(4):535?561.D.
Lin and P. Pantel.
2001.
DIRT ?
Discovery of Infer-ence Rules from Text.
In KDD.A.
McCallum and B. Wellner.
2004.
Conditional modelsof identity uncertainty with application to noun coref-erence.
In NIPS.A.
McCallum, K. Nigam, and L. Ungar.
2000.
Efficientclustering of high-dimensional data sets with applica-tion to reference matching.
In KDD.V.
Ng and C. Cardie.
2002.
Improving machine learningapproaches to coreference resolution.
In ACL.Ana-Maria Popescu.
2007.
Information Extraction fromUnstructured Web Text.
University of Washington.P.
Ravikumar and W. W. Cohen.
2004.
A hierarchicalgraphical model for record linkage.
In UAI.G.
Salton and M.J. McGill.
1983.
Introduction to Mod-ern Information Retrieval.
McGraw-Hill.P.
Singla and P. Domingos.
2006.
Entity Resolution withMarkov Logic.
In ICDM.R.
Snow, D. Jurafsky, and A. Y. Ng.
2006.
Semantictaxonomy induction from heterogenous evidence.
InCOLING/ACL.W.E.
Winkler.
1999.
The state of record linkage and cur-rent research problems.
Technical report, U.S. Bureauof the Census, Washington, D.C.130
