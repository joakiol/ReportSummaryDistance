An Architecture for Voice Dialog SystemsBased on Prolog-Style Theorem ProvingRonnie W. Smith*East Carolina UniversityAlan W. Biermann *Duke UniversityD.
Richard Hipp tHipp, Wyrick & Company, Inc.A pragmatic architecture for voice dialog machines aimed at the equipment repair problem hasbeen implemented.
This architecture exhibits anumber of behaviors required for efficient human-machine dialog.
These behaviors include:(1)(2)(3)(4)(5)problem solving to achieve a target goalthe ability to carry out subdialogs to achieve appropriate subgoals and to passcontrol arbitrarily from one subdialog to anotherthe use of a user model to enable useful verbal exchanges and to inhibitunnecessary onesthe ability to change initiative from strongly computer controlled to strongly usercontrolled or to some level in betweenthe ability to use context dependent expectations tocorrect speech recognition andtrack user movement tonew subdialogs.The paper gives a description of the dialog theory, presents examples of its capabilities, andincludes adetailed trace of one of those xamples showing all significant mechanisms.
The papergives performance data for a series of 141 problem-solving dialogs carried out with human subjects.1.
A Theory of Voice Dialog SystemsThis paper presents a theory of voice dialog systems that integrates into a single,self-consistent architecture a variety of capabilities necessary for successful dialog.These capabilities include problem solving, natural anguage input and output, a usermodel, variable initiative, and the use of expectation for speech error correction andplan recognition.
The theory revolves around a Prolog-style theorem-proving modelwith a variety of special features to accommodate he needs of a dialog system.
Thepaper describes the target behaviors of the processor, the theory that achieves them,an implementation f the theory in a system to aid in electric circuit repair, and per-formance statistics gathered as human subjects used it in a series of tests.?
Department of Mathematics, East Carolina University, Greenville, N.C. 27858.f Hipp, Wyrick & Company, Inc., 6200 Maple Cove Lane, Charlotte, N.C. 28269.Department of Computer Science, Duke University, Durham, N.C.
27706.
(~) 1995 Association for Computational LinguisticsComputational Linguistics Volume 21, Number 32.
Target BehaviorsThe purpose of the architecture is to deliver to users in real time the behaviors neededfor efficient human-machine dialog.
Specifically, it is aimed at achieving the following:Convergence to a goal.
Efficient dialog requires that each participant understandthe purpose of the interaction and have the necessary prerequisites to cooperate inits achievement.
This is the intentional structure of Grosz and Sidner (1986), the goal-oriented mechanism that gives direction to the interaction.
The primary required fa-cilities are a problem solver that can deduce the necessary action sequences and a setof subsystems capable of carrying out those sequences.Subdialogs and effective movement between them.
Efficient human dialog is usually seg-mented into utterance sequences, ubdialogs, that are individually aimed at achievingrelevant subgoals (Grosz 1978; Linde and Goguen 1978; Polanyi and Scha 1983; Reich-man 1985).
These are called "segments" by Grosz and Sidner (1986) and constitute thelinguistic structure defined in their paper.
The global goal is approached by a series ofattempts at subgoals each of which involves a set of interactions, the subdialogs.An aggressive strategy for global success is to choose the subgoals judged mostlikely to lead to success and carry out their associated subdialogs.
As the systemproceeds on a given subdialog, it should always be ready to drop it abruptly if someother subdialog suddenly seems more appropriate.
This leads to the fragmented stylethat so commonly appears in efficient human communication.
A subdialog is opened,leading to another, then another, then a jump to a previously opened subdialog, andso forth, in an unpredictable order until the necessary subgoals have been solved foran overall success.An accounting for user knowledge and abilities.
Cooperative problem solving involvesmaintaining a dynamic profile of user knowledge, termed a user model.
This conceptis described, for example, in Kobsa and Wahlster (1988, 1989), Chin (1989), Cohenand Jones (1989), Finin (1989), Lehman and Carbonell (1989), Morik (1989), and Paris(1988).
The user model specifies information eeded for efficient interaction with theconversational partner.
Its purpose is to indicate what needs to be said to the user toenable the user to function effectively.
It also indicates what should be omitted becauseof existing user knowledge.Because considerable information is exchanged during the dialog, the user modelchanges continuously.
Mentioned facts are stored in the model as known to the userand are not repeated.
Previously unmentioned information may be assumed to beunknown and may be explained as needed.
Questions from the user may indicatelack of knowledge and result in the removal of items from the user model.Change of initiative.
A real possibility in a cooperative interaction is that the user'sproblem-solving ability, either on a given subgoal or on the global task, may exceedthat of the machine.
When this occurs, an efficient interaction requires that the machineyield control so that the more competent partner can lead the way to the fastest possiblesolution.
Thus, the machine must be able to carry out its own problem-solving processand direct the actions to task completion or yield to the user's control and respondcooperatively to his or her requests.
This is a variable initiative dialog, as studied byKitano and Van Ess-Dykema (1991), Novick (1988), Whittaker and Stenton (1988), andWalker and Whittaker (1990).
As a pragmatic issue, we have found that at least fourinitiative modes are useful:(1) directive.
The computer has complete dialog control.
It recommends asubgoal for completion and will use whatever dialog is necessary toobtain the needed item of knowledge related to the subgoal.282Smith, Hipp, and Biermann An Architecture for Voice Dialog Systems(2) suggestive.
The computer still has dialog control, but not as strongly.
Thecomputer will make suggestions about the subgoal to perform next, butit is also willing to change the direction of the dialog according to stateduser preferences.
(3) declarative.
The user has dialog control, but the computer is free tomention relevant, though not required, facts as a response to the user'sstatements.
(4) passive.
The user has complete dialog control.
The computer respondsdirectly to user questions and passively acknowledges user statementswithout recommending a subgoal as the next course of action.Expectation of user input.
Since all interactions occur in the context of a current sub-dialog, the user's input is far more predictable than would be indicated by a generalgrammar for English.
In fact, the current subdialog specifies the focus of the interac-tion, the set of all objects and actions that are locally appropriate.
This is the attentionalstructure described by Grosz and Sidner (1986), and its most important function in oursystem is to predict the meaning structures the user is likely to communicate in aninput.
For illustration, the opening of a chassis cover plate will often evoke com-ments about the objects behind the cover; the measurement of a voltage is likely toinclude references to a voltmeter, leads, voltage range, and the locations of measure-ment points.Thus  the subdialog structure provides a set of expected utterances at each pointin the conversation, and these have two important roles:(1)(2)The expected utterances provide strong guidance for the speechrecognition system so that error correction can be enhanced.
Whereambiguity arises, recognition can be biased in the direction ofmeaningful statements in the current context.
Earlier researchers whohave investigated this insight are Erman et al (1980), Walker (1978), Finkand Biermann (1986), Mudler and Paulus (1988), Carbonell and Pierrel(1988), Young (1990), and Young et al (1989).The expected utterances from subdialogs other than the current one canindicate that a shift from the current subdialog is occurring.
Thus,expectations are one of the primary mechanisms needed for tracking theconversation as it jumps from subdialog to subdialog.
This is knownelsewhere as the plan recognition problem, and it has received muchattention in recent years.
See, for example, Allen and Perrault (1980),Allen (1983), Kautz (1991), Litman and Allen (1987), Pollack (1986), andCarberry (1988, 1990).Systems capable of all of the above behaviors are rare, as has been observed byAllen et al (1989): "no one knows how to fit all of the pieces together."
An impressiveearly example along these lines is the MINDS system of Young et al (1989).
Thissystem maintains an AND-OR goal tree to represent the problem-solving space, andit engages in dialog in the process of trying to achieve subgoals in the tree.
A seriesof interactions related to a given subgoal constitute a subdialog, and expectationsassociated with currently active goals are used to predict incoming user utterances.These predictions are further sharpened by a user model and then are passed downto the signal-processing level to improve speech recognition.
The resulting system283Computational Linguistics Volume 21, Number 3demonstrated dramatic improvements over performance l vels that had been observedwithout such predictive capabilities.
For example, the effective perplexity in one testwas reduced from 242.4 to 18.3 using dialog level constraints, while word accuracyrecognition was increased from 82.1 percent o 97.0 percent.In another dialog project, Allen et al (1989) describe an architecture that con-centrates on representations for subdialog mechanisms and their interactions withsentence-level processing.
Their mechanism uses the blackboard organization, whichdisplays at a global level all pertinent information and has subroutines with specialtyfunctions to update the blackboard.
There are subroutines, for example, to do process-ing at the lexical, syntactic, and semantic levels, to handle referencing problems, tomanage discourse structure and speech act issues, tense, and much more.
A typicaltask for their system is to properly parse and analyze a given dialog.A third interesting project has produced the TINA system (Seneff 1992), whichuses probabilistic networks to parse token sequences provided by a speech recognitionsystem, SUMMIT, created by Zue et al (1989).
The networks and their probabilities arecreated automatically from grammatical rules and text samples input by the designer.Their main utility is to provide expectation for error correction as we do in our system.However, their expectation is primarily syntax-based while ours uses structure fromall levels, subdialog (or focus-based), semantic, and syntactic.
Their semantics i  builtdirectly into the parse trees and translated into SQL for access to a database.Our approach resembles that of Young et al (1989), except hat we employ Prolog-style rules for the knowledge base and the associated proofs for directing the goal-oriented behavior.
This leads to the "missing axiom theory" we describe for processingdiscourse, and some rather simple mechanisms for employing the user model, formanaging multiple subdialogs, and for creating and using expectation.
Our theoryalso shows how variable initiative is built into the same simple architecture.
Ourimplementation shows that the mechanisms are efficient enough to run in real timeand sufficiently well designed to yield successful dialogs with humans.3.
Sample DialogsMany of the points discussed above are illustrated by the following excerpts fromrepair dialogs using our system.
In the following example, the system is in direc-tive mode.
It has determined that a particular knob should be set to 10 and then avoltage measurement should be made.
The movement to subdialogs is indicated byindentation.1.
Computer:2.
User:3.
Computer:4.
User:5.
Computer:6.
User:7.
Computer:8.
User:9.
Computer:10.
User:Put the knob to one zero.Okay.What is the voltage between connector one two oneand connector three four?I do not know.Locate the voltmeter.Done.Add a wire between the minus com hole on thevoltmeter and the connector one two one.How do I do that?Connect he end of the black wire with the smallplug to the minus corn hole on the voltmeter.Done.284Smith, Hipp, and Biermann An Architecture for Voice Dialog Systems11.
Computer: Connect he end of the black wire with the largeplug to connector one two one.12.
User: Which knob?13.
Computer: Near the top boundary and in the center is a greenregion.
Now, in the middle right of this greenregion is the control knob.14.
User: Okay.15.
Computer: Put the knob to zero one.16.
User: Done.17.
Computer: Connect he end of the black wire with the largeplug to connector one two one.18.
User: Where is connector three four?19.
Computer: Near the left boundary and toward the middle is agreen region.
Now, in the bottom center of thisgreen region is connector three four.20.
User: Okay.21.
Computer: What is the voltage between connector one two oneand connector three four?22.
User: Zero.This dialog fragment is interesting in that it illustrates goal-oriented behavior whilesimultaneously jumping between subdialogs.
The dialog also illustrates the effects ofthe user model.
For example, the user model at the beginning of the dialog indicatesthat the user can find the knob (find(knob)).
This assertion prevents the system fromgiving any explanation about he location of the knob.
However, utterance 12 indicatesan error in the current model and the system removes this assertion from the usermodel.
This leads to a clarification subdialog explaining the position of the knob anda response from the user, "Okay."
The user's affirmation then enables the system toadd the assertion find(knob) back into the user model and proceed.The use of expectation can also be illustrated by the example.
When the user ispreparing to respond with utterance 2, the system expects the response to pertain tothe command "Put the knob to one zero."
The primary expectations are for an as-sertion that this request has been done or for a question about how to do this task.Consequently, speech recognition can be biased to accept his limited class of utter-ances.
A different use of expectation appears in sentence 12, where local expectationswere related to the connection of a wire.
Here, local expectations failed but a previousexpectation from sentence 1 is fulfilled.
So control is returned to the previously activesubdialog where the goal was to "Put the knob to one zero.
"The following dialog segment illustrates the behavior of the system in declarativemode.1.
Computer: This is the circuit-fixit-shop.
How may I help you?2.
User: RSlll is not working.3.
Computer: I am familiar with that circuit.
The LED is supposed to bedisplaying alternately flashing one and seven.4.
User: There is no wire on one zero four.5.
Computer: There is supposed to be a wire between connectorone zero four and connector one zero two.6.
User: The circuit is working.7.
Computer: Glad to have been of assistance.
Good-bye.Here the machine diagnoses as well as it can the current subdialog from usercomments.
Then it presents information it surmises may be helpful, specifically facts285Computational Linguistics Volume 21, Number 3from the currently active subdialog that are not in the user model.
The user mayignore suggestions by following his or her own preferred ialog paths, as occurs instatements 4 and 6.
In each case, the system tracks the selected topic and responds inan appropriate manner.4.
Mechanisms for Achieving the BehaviorsThis paper presents a single self-consistent mechanism capable of achieving simulta-neously the above-described behavior.
We will examine sequentially (1) a theory oftask-oriented language, (2) an implementation f the subdialog feature, (3) a methodfor accounting for user knowledge, (4) mechanisms needed to obtain variable initia-tive, and (5) the implementation a d uses of expectation.
The section following thisone will give a detailed example showing all of these mechanisms working together.4.1 A Theory of Task-Oriented LanguageThe central mechanism of our architecture is a Prolog-style theorem-proving system.The goal of the dialog is stated in a Prolog-style goal and rules are invoked to "provethe theorem" or "achieve the goal" in a normal top-down fashion.
If the proof succeedsusing internally available knowledge, the dialog terminates without any interactionwith the user.
Thus it completes a dialog of length zero.
More typically, however, theproof fails, and the system finds itself in need of more information before it can pro-ceed.
In this case, it looks for so-called "missing axioms," which would help completethe proof, and it engages in dialog to try to acquire them.As an example, the system might have the goal of determining the position, upor down, of a certain switch swl.
This might appear in the proof tree asobserveposition(swl,X) *-- find(swl), reportposition(swl,X)That is, it is necessary to find swl and then to report its position X.
It is possible thatin the process of the interaction the user has both found the switch and reported itsposition and that both find(swl) and report position(swl,up) appear in the database.Then the system will achieve observeposition(swl,up) and answer its question with-out interaction with the user.
It is also possible that the user has previously found theswitch but not recently reported its position.
Here theorem proving would succeedwith the first goal, find(swl), but fail to find reportposition(swl,-).
Then reportpo-sition(swl,X) would become a missing axiom and could be returned to the dialogcontroller for possible vocalization.
The third possibility is that neither find(swl) orreportposition(swl,-) would exist in the database, in which case both could be sent tothe controller as missing axioms for possible vocalization.
(The decision as to whetherto send a missing axiom to the controller depends on whether the axiom representsan answerable question by the user.
The system maintains a mechanism for indicatingwhat is reasonable to ask and what is not, as described below.
)Thus our system is built around a theorem prover at its core, and the role oflanguage is to supply missing axioms.
Our system engages in dialog only for thepurpose of enabling theorem proving, and voice interactions do not otherwise occur.
(A later publication will propose other uses of voice interactions, but our currentsystem uses them for only this purpose.
)The user may not always respond with the desired information.
He or she mayrespond with a request for a clarifcation such as "Where is the switch?"
or with anunanticipated comment such as "There is no wire connected to terminal 102."
Thusthe theorem prover needs to be immensely more flexible than ordinary Prolog, andthis is the topic of the next section.286Smith, Hipp, and Biermann An Architecture for Voice Dialog Systems4.2 Implementing the Subdialog FeatureInstead of turning control over to a depth first policy, theorem proving in this sys-tem must allow for abrupt freezing of any proof and transfer of control to any otherpartially completed subproof or function of the system.
The implementation f thiswas the IPSIM (Interruptible Prolog SIMulator) theorem prover, which can maintaina set of partially completed proofs and jump to the appropriate one as dialog pro-ceeds.The processing of IPSIM proceeds with normal theorem proving, but is interrupt-able in two ways.
First, IPSIM may discover that there could be outside informationthat could be used to supply missing axioms, as described above.
When this occurs,IPSIM can halt and pass control to the dialog controller, indicating an opportunity toengage in dialog.
The dialog controller may choose to invoke the proposed interac-tion or it may select another action.
Second, IPSIM may be interrupted by the dialogcontroller to inquire about proof status.
In a real-time system, a theorem prover cannever be released arbitrarily.
Timing considerations by the controller may dictate thehalting of a given proof and resorting to other action.Since voice dialog is always tied to proving a given subgoal, the set of all inter-actions related to that goal comprise a subdialog.
The set of logical rules leading tothe subgoal are, by definition, related to that subgoal, and the voice interactions willnecessarily have the coherence that theorists (Hobbs 1979) have often discussed.The partial or completed proof of a subgoal is not erased or popped from anystack when processing moves to another part of the proof tree.
This makes it possibleto reopen any subdialog at a later time to clarify, revise, or continue that interaction.The reopening may be initiated either by the system because of a change in prioritiesin its agenda or by the user.Subdialogs are entered in several ways.
First, normal theorem proving may createa new subgoal to be proved, and its related voice interactions will yield a subdialog.Second, the controller may halt an interaction that it deems unfruitful and send thesystem in pursuit of a new subgoal.
Third, the user may initiate dialog on a newsubgoal in ways that will be discussed below.4.3 Accounting for User KnowledgeTheorem proving will often reach goals that can only be satisfied by interactions withthe user.
For example, if the machine has no means to manipulate some variable andthe user does, the only alternative is to make a request of the user.
It is necessarythat the knowledge base store information related to what the user can be expectedto do, and the system only should make requests that will be within this repertoire.Of course, the abilities of the user will depend on his or her level of expertise andexperience in the current environment.
Thus most users will know how to adjust aknob to a specified level even if they are novices; but they may be able to measurea voltage only after they have been told all of the steps at least once in the currentsituation.It is necessary to style outputs to the user to account for these variations, andthe Prolog theorem-proving tree easily adapts to this requirement.
The example aboverelated to finding the position of a switch shows how this works.
If the user knows,for example (according to the user model), how to find the switch, the model preventsthe useless interaction related to finding the switch from occurring.
If the user doesnot know (according to the user model) where the switch is, the model releases thesystem to so inform the user.
The theory of user modeling thus is simply to specifyuser capabilities in the Prolog-style rules and let the natural execution of IPSIM selectwhat to say or not say.287Computational Linguistics Volume 21, Number 3The user model must change continuously during a dialog as the interactionsoccur.
Almost every statement will change the useVs knowledge base, and futureinteractions will be ill-conceived if the appropriate updates are not made.
Acquisitionof user model axioms is made from inferences based on user inputs.
A description ofthe inferences i given below:(1)(2)(3)(4)(5)(6)(7)If the input indicates that the user has a goal to learn some information,then conclude that the user does not know about the information.If the input indicates that an action to achieve or observe a physical statewas completed, then conclude that the user knows how to perform theaction.If the input describes ome physical state, then conclude that the userknows how to observe this physical state.
In addition, if the physicalstate is a property, then infer that the user knows how to locate theobject hat has the property.If the input indicates that the user has not performed some primitiveaction, make the appropriate inference about the user's knowledge abouthow to perform this action.If the user has completed an action by completing each substep, thenconclude that the user knows how to do the action.Infer that the user has intensional knowledge about a physical state ifthe user has knowledge on how to observe or achieve the physical state.Infer that the user has knowledge on how to observe a physical state ifhe or she has knowledge on how to achieve the physical state.The basic implementation f these rules is a "compute_inferences" predicate in Prologthat takes the meaning of the user's current utterance and causes inferences to beasserted into the axiom base.
Here is an example from the Prolog code.
It is theimplementation f statement (2) given above:/* Inference 2:If we have learned that an action to achieve or observe a physical state was com-pleted, then conclude that the physical state has the appropriate status and that theuser knows how to perform the action.
*/compute_inferences(ProofNum,Meaning,_) :-Meaning = phys~state(prop(GoalAction,actionAttribute,done),true),( GoalAction = ach(phys_state(StateDes,TS)) ;GoalAction = obs(phys_state(StateDes,TS))),make_inference(ProofNum,phys~state(StateDes,TS),infer(Meaning)),makeAnference(ProofNum, mentaLstate(user, int_know~action(how_to_do(GoalAction)),true),infer(Meaning)).In typical dialogs, the user modeling system added a net of about 1.2 Prolog-styleassertions to the user model per user utterance.
There are both additions and deletions288Smith, Hipp, and Biermann An Architecture for Voice Dialog Systemsoccurring after each utterance, and this figure gives the average increase in assertionsper user utterance.Formalizing a theory of what constitutes appropriate inferences could be a separateresearch project.
What this research as contributed is a theory of usage for theseinferences, which is usage by the theorem prover as it attempts to complete taskgoals.4.4 Mechanisms for Obtaining Variable InitiativeVariable initiative dialog allows either participant to have control, and it allows theinitiative to change between participants during the exchange.
It also allows interme-diate levels of control where one participant may gently rather than strongly lead theinteractions.A system can participate in variable initiative dialog if it properly manages (1) theselection of the current subdialog, (2) the level of assertiveness in its outputs, and (3)the interpretation of its inputs.
We discuss each in the following paragraphs.Selection of Subdialog.
The most important aspect of dialog control is the ability toselect he next subdialog to be entered.
Very strong control means that the participantwill select the subdialog and will ignore attempts by the partner to vary from it.Weaker control allows the partner to introduce minor but not major variations fromthe selected path.
Loss of control means that the partner will select unconditionallythe next subgoal.In our system, the four implemented levels of initiative follow these guidelines:(1) Directive Mode.
Unless the user explicitly needs some type ofclarification, the computer will select its response solely according to itsnext goal for the task.
If the user expresses need for clarification aboutthe previous goal, this must be addressed first.
No interruptions to othersubdialogs are allowed.
(2) Suggestive Mode.
The computer will again select its response accordingto its next goal for the task, but it will allow minor interruptions tosubdialogs about closely related goals.
As before, user requests forclarification of the previous goal have priority.
(3) Declarative Mode.
The user has dialog control.
Consequently, the usercan interrupt o any desired subdialog at any time, but the computer isfree to mention relevant, though not required, facts as a response to theuser's statements.
(4) Passive Mode.
The user has complete dialog control.
Consequently, thecomputer will passively acknowledge user statements.
It will provideinformation only as a direct response to a user question.Level of Assertiveness of Outputs.
The system must output statements hat are com-patible with its level of initiative.
This means that the output generator must have aparameter that enables the system to specify assertiveness.
Examples of the request o"turn the switch up" at various levels of assertiveness are as follows:Turn the switch up.Would you \[please\] turn the switch up?Can you turn the switch up?The switch can be turned up.Turning the switch up is necessary.
(command)(request)(indirect request)(statement of fact)(statement of fact)289Computational Linguistics Volume 21, Number 3Two examples of querying for the switch position are as follows:What is the switch position?I need to know the switch position.
(request)(indirect request)Interpretation ofInputs.
Inputs from a passive participant can be expected to bemore predictable and well behaved than those from a directive one.
Our system doesnot account for this effect at this time.
The only implemented variation in behaviorconcerns the treatment of silence.
The system may allow rather longer silences whenit is in passive mode than when it is in directive mode.4.5 The Implementation and Uses of ExpectationThe response received after a given input is likely to be related to the currently activesubdialog.
If it is not, then it may be related to a nearby active subdialog or, withless probability, a more remote one.
The expectation facility provides a list of expectedmeanings organized in a hierarchy, and it is used for two purposes: (1) If the incom-ing utterance is syntactically near the syntax for an expected meaning in the activesubdialog, the expectation provides a powerful error-correction mechanism.
(2) If theincoming utterance is not in the locally active subdialog, the expectations of other ac-tive subdialogs provide a means for tracing the movement to those subdialogs.
(Thisis known as "plan recognition" in the literature.
See, for example, Allen and Perrault(1980), Allen (1983) and Carberry (1990).
Expectations are specified in GADL (Goaland Action Description Language) form, an internal anguage for representing pred-icates.
For example, the expectation that the user is going to report the setting of aswitch would be represented asobs(phys_state(prop(switchl,state, PropValue), TruthStatus)).Expectation of user responses provides a model of the attentional state describedby Grosz and Sidner (1986).
It contains the list of semantic structures that have mean-ing for the current subdialog and for other active subdialogs.
For example, after thecomputer produces an utterance that is an attempt o have a specific task step S per-formed, there are expectations for any of the following types of responses:(1)(2)(3)(4)(5)(6)A statement about missing or uncertain background knowledgenecessary for the accomplishment of S.A statement about a subgoal of S.A statement about the underlying purpose for S.A statement about ancestor task steps of which accomplishment of S is apart.A statement about another task step which, along with S, is needed toaccomplish some ancestor task step.A statement indicating accomplishment of S.The central pragmatic issues for the management of expectation are, what are thesources of expectation (or how is the expectation list created) and how is it used.
Thefollowing paragraphs describe both.290Smith, Hipp, and Biermann An Architecture for Voice Dialog SystemsSources of expectation.
The first source of expectation is the domain processor de-scribed below.
One of the main tasks of this system is to supply the dialog ma-chine with debugging queries such as "What is the LED showing?"
A secondarytask is to associate with each such query a list of expected answers.
Thus the queryabout the LED would yield as expectations some possible descriptions for the LED.These are called situation_specific_expectations.
The domain processor also supplies itu-ation_related_expectations, which are not directly connected to the observation but whichcould naturally occur.
For example, the LED query might also result in observationsabout the presence or absence of wires, the position of the power switch, and thepresence or absence of a battery.The other source of expectations is the dialog_controller, also described below, whichprovides coordination for the complete system.
The dialog controller manages a num-ber of generic rules related to dialog and can provide the associated expectations.
Forexample, "What is the LED showing?"
can represent the action "observe the value forthe display property of the object LED."
The associated task~specific_expectations wouldrepresent expectations based on this general notion with values for property and objectinstantiated to the situation values.
Thus the task-specific expectations for the sampletopic would include questions on the location of the object, on the definition of theproperty, and on how to perform the action.
In addition, these expectations wouldinclude responses that can be interpreted as state descriptions of the relevant prop-erty.
In general, they include potential questions and statements about subtasks of thecurrent ask.
There are rules for 12 different generic actions (Smith 1991).
The rules arebased on a characterization f response types obtained uring a Wizard-of-Oz studyon the effects of restricted vocabulary (Moody 1988).The dialog controller also provides a broader class of expectations, called task-related expectations, which are based on general principles about the performance ofactions.
Example task-related expectations would include general requests for helpor questions about the purpose of an action.
Another important member of the task-related expectations are the expectations for topics that are ancestors of the currenttopic in the discourse structure.
For example, the current opic could be the locationof a connector, which could be a subtopic of connecting a voltmeter wire, which couldbe a subtopic of performing a voltage measurement.
The task-related expectations forthe location of this connector would include all the expectations related to the topicsof connecting a voltmeter wire and performing a voltage measurement.Utilizing expectation.
After the semantic expectations are computed, they are trans-lated into linguistic expectations according to grammatical rules.
Once the linguisticexpectations are produced, they are labeled with an expectation cost, which is a measureof how strongly each is anticipated at the current point in the dialog.
The situation-specific expectations are the most strongly anticipated, followed by the other threetypes.
Meanings output by the minimum distance parsing algorithm (described be-low) have a corresponding utterance cost, which is the distance between the user'sinput and an equivalent well-formed phrase.
Each meaning is matched with its cor-responding dialog expectation, and its expectation and utterance costs are combinedinto a total cost by an expectation function.
The meaning with the smallest otal costis selected to be the final output of the parser.
An important side effect of matchingmeanings with expectations is the ability to interpret an utterance whose content doesnot fully specify its meaning.
These semantic and linguistic expectations can providethe necessary context.
Some examples are as follows:(a) The referent of pronouns (In the implemented system, the only pronoun is "it.
")The parser leaves the slot for the referent of "it" unspecified in its interpretation.
If thisinterpretation of the utterance can be matched to the linguistic expectation, the value291Computational Linguistics Volume 21, Number 3for "it" is filled with the value provided by the expectation.
Consider the followingexample:(1)(2)Computer: Turn the switch up.User: Where is it?In computing the task-specific expectations for the user utterance, one expectation isfor a statement asking about the location of the object of interest in the current opic--in this case the switch.
The parser interprets the user statement as a statement askingabout the location of an unspecified object.
The linguistic expectation provides thevalue of the unspecified object.
(b) The meaning of short answers (In the implemented system, these include suchresponses as "yes," "no," and "okay.")
The idea for each of these is similar.
Theseutterances may have any of several meanings.
The proper choice is determined by theexpectations produced based on the situation.
In the following example, it is likelythat "okay" denotes affirmation of completion of the goal to turn up the switch:(1) Computer: Turn the switch up.
(2) User: Okay.It is less likely that it denotes comprehension f the request.
In any case, after statement(1), the situation-specific expectations include an expectation for an affirming utteranceindicating completion, and this becomes the interpretation given to "okay."
Contrastthis with the following:(1) Computer: Turn up the switch.
(2) User: Where is it?
(3) Computer: In the lower left corner.
(4) User: Okay.In this case, the interpretation of "okay" could be either that the location description(3) has been understood or that the original goal (1) has been accomplished.
Theexpectation system scores the likelihood of each meaning and selects the most likelyone using its scoring method.
(c) Maintain dialog coherence.
Consider the following subdialog taken from usage ofthe implemented system:(1) Computer: What is the voltage between connector 121 and connector120?
(2) User: I need help.
(3) Computer: Locate the voltmeter.
(4) User: Done.
(5) Computer: Add a wire between the "- corn" hole on the voltmeter andconnector 121.
(6) User: Done.292Smith, Hipp, and Biermann An Architecture for Voice Dialog Systemsa(7) Computer: Add a wire between the "+ v omega a" hole on the voltmeterand connector 120.
(8) User: Nine.When utterance (8) is spoken, there are two active task steps: (1) performing thevoltage measurement and (2) connecting a wire between the "+ v omega a" hole andconnector 120.
The user response does not satisfy the missing axiom for completingthe substep (7).
The expectations for the response to (7) are checked, but this utteranceis not one of them.
However, (8) does satisfy the missing axiom for completing themain task step (1).
It has meaning in that context and it is so interpreted.5.
The Zero-Level ModelThe system built in our laboratory (described at length in Smith \[1991\] and Hipp \[1992\]and Smith and Hipp \[1995\]) implements the theory given above.
Figure 1 presents azero-level model of the main processor, which illustrates the system principles of oper-ation without burdening the reader with too much detail.
This model is the recursivesubroutine ZmodSubdialog, and it is entered with a single argument, a goal to beproven.
Its actions are to carry out a Prolog-style proof of the goal.
A side effect ofthe proof may be some voice interactions with the user to supply missing axioms asdescribed above.
In fact, the only voice interactions the system undertakes are thosecalled for by the theorem-proving machinery.The ZmodSubdialog routine is a Prolog-style interpreter with a number of specialfeatures designed for the dialog processing application.
It is typical of such interpretersin that it lifts goals from a priority queue and applies rules from the knowledge baseto try to satisfy them.
See, for example, the first and third branches beginning with "IfR..." where respectively, the trivial case and the general case for applying a rule arehandled.
The deviations from a standard such interpreter are (1) in the second branch"If R...", which handles the case of a missing axiom where voice interaction is to beinvoked, (2) in three steps (marked "mode") where processing will vary according tothe level of the initiative the system is in, and (3) in the controlling module for theinterpreter, which may freeze execution of this computation atany time to initiate orcontinue some other such computation.Typical execution of ZmodSubdialog involves opening aproof tree and proceedingwith a computation until an interrupt or clarification subdialog occurs.
This may come,for example, from a new goal suggested by the domain processor or from a statementby the user causing movement toa different subdialog.
The interrupt will cause controlto pass to another existing proof tree (that was previously frozen) or to a new oneaimed at the newly presented goal.
Thus a set of partially completed trees will exist atall times, and control will jump back and forth between them.
Of course, many of thesetrees will invoke associated voice interactions--and these constitute the subdialogs ofthe conversation.The process of dialog described here is a kind of interactive theorem proving wherethe guidance down paths can come from either the user's knowledge or system knowl-edge.
However, the emphasis in the traditional interactive theorem-proving literatureis on giving the user substantial opportunity to propose the notations and individualsteps of the proof in a way that is not possible or desirable in our environment.293Computational Linguistics Volume 21, Number 3oRecursive subdialog routine (enter with a goal to prove)ZmodSubdialog(Goal)Create subdialog data structuresWhile there are rules available which may achieve GoalGrab next available rule R from knowledge; unify with GoalIf R trivially satisfies Goal, return with successIf R is vocalize(X) thenExecute verbal output X (mode)Record expectationReceive response (mode)Record implicit and explicit meanings for responseTransfer control depending on which expected response was receivedSuccess response: Return with successNegative response: No actionConfused response: Modify rule for clarification; prioritize for executionInterrupt: Match response to expected response of another subdialog;Go to that subdialog (mode)If R is a general rule thenStore its antecedentsWhile there are more antecedents o processGrab the next one and enter ZmodSubdialog with itIf the ZmodSubdialog exits with failure then terminate processing of RIf all antecedents of R succeed, return with successHalt with failureNOTE: SUCCESSFUL COMPLETION OF THIS ROUTINE DOES NOT NECESSARILYMEAN TRANSFER OF CONTROL TO THE CALLING ROUTINE.
CONTROL PASSESTO THE SUBDIALOG SELECTED BY THE DIALOG CONTROLLER.Figure 1The zero-level model of the main subdialog processing algorithm.6.
Executing an Example SubdialogThe operation of ZmodSubdialog (and similarly our implemented system) becomesclear if a complete xample subdialog is carried out.
Here we will trace the executionof the example 22 utterance subdialog given in Section 3 and thereby illustrate thetheory of operation in detail.
An overview of the computation is given here and adetailed trace of all significant details appears in Appendix A.
The following databaseof Prolog-like rules is needed for proper system operation.Specific Device Debugging RuleTl_circuit_Test2(V) ~ set(knob,10), measurevoltage(121, 34, V).General Debugging Rulesset(knob,Y) ~ find(knob), adjust(knob,Y).measurevoltage(X,Y,V) *-- find(voltmeter),294Smith, Hipp, and Biermann An Architecture for Voice Dialog Systemsset(voltmeterscale,20),connect(bw, com,X),connect(rw,+,Y),vocalize(read(voltmeter, V))set(voltmeterscale,20)connect(W,X,Y) +- connectend(W,X),connectend(W,Y)General Dialog RulesY +-- usercan(Y), vocalize(Y)vocalize(Y)find(Y) ~ vocalize(find(Y))User Modeling Rulesfind(knob)usercan(adjust(knob,X))usercan(measurevoltage(X,Y,V))usercan(find(voltmeter))usercan(connect(X,Y,Z))usercan(connectend(X,Y))We assume that the machine has selected a new goal that comes from the domainprocessor: Tl_circuit_Test2(V), where V is a voltage to be returned by the test.
Thus, thedomain processor is asking that test 2 on circuit T1 be performed returning a voltageV.
ZmodSubdialog begins in Prolog fashion looking for a rule in the database to provethe goal Tl_circuit_Test2(V), and it finds the debugging rule Tl_circuit_Test2(V)set(knob,10), measurevoltage(121,34,V).
Referring to Figure 1, this rule R is a generalrule resulting in the third choice branch.
Here the algorithm selects the first subgoalset(knob,10) ofR and creates another subdialog by entering ZmodSubdialog with thissubgoal.The new subdialog finds the ruleset(knob,Y) *-- find(knob), adjust(knob,Y),which says the way to set the knob is to first find it and then do the adjustment.
Execu-tion of this rule demonstrates the mechanisms related to the use of the user model andthe initiation of voice interaction.
For example, the first new subgoal find(knob) causesa new entry into ZmodSubdialog, where it is immediately satisfied by find(knob) inthe user model.
That is, the user has achieved find(knob) (knows how to find theknob), and no further consideration of this subgoal is needed.
If the user did notknow (according to the user model) how to find the knob, the system might haveinvoked a voice interaction to try to achieve this subgoal.
Moving to the second goal,adjust(knob,10), again it might have occurred that the user has just achieved this also.
(Since the algorithm does unification as a rule is invoked, the variable Y has been setto 10.)
Entry of ZmodSubdialog with this subgoal, however, finds no trivial resolutionfor this subgoal.
But it can invokeY ~ usercan(Y), vocalize(Y),which says that a goal Y can be achieved if the user is capable of doing Y (whichis represented as usercan(Y)) and if we vocalize Y.
(This type of rule has not been295Computational Linguistics Volume 21, Number 3explicitly implemented in our system, but we include it here as a model of what doeshappen.)
Further ecurrences on ZmodSubdialog discover usercan(adjust(knob,X)) andundertake vocalize(adjust(knob,10)).Entry of ZmodSubdialog with vocalize(adjust(knob,10)) sends control down itssecond path.
Sentence generation and voice output produces the statement"Put the knob to one zero.
"Next a set of expected responses i compiled.
Some of these include:question(location,knob).question(ACTION,how-to-do).assertion(knob,status,10).assertion(ACTION,done).When a vocalized response comes back, parsing and error correction will be biased torecognize one of these meanings.After a response meaning has been resolved, it is entered into the database with allits presuppositions.
For example, the mention of an object is assumed to indicate thatthe user can recognize and find that object if needed.
These assertions are entered intothe user model.
The user model thus changes on almost every interaction to note newfacts that are probably known to the user or to remove facts that the user apparentlydoes not know.
Finally, control changes because of the response ither to (1) returnfrom this subdialog with success, (2) continue in this subdialog searching for a success(having received an unsuccessful response), (3) enter special processing to deal witha need for clarification, or (4) interrupt processing to jump to some other subdialog.In the example subdialog, the user responds "OK," which corresponds to one ofthe expected meanings: assertion(ACTION, done).
Expectations equate ACTION to thecurrent action.
Consequently, there is a successful exit of the current subdialog andachievement of the set(knob,10) goal.The first invoked rule,Tl_circuit_Test2(V) ~ set(knob,10), measurevoltage(121,34,V),is thus half satisfied, and the goal measurevoltage(121,34,V) is undertaken.The new goal leads to vocalization i  the same manner described above.
But theresponse in this case is negative: "I do not know."
So attempts to achieve measure-voltage(121,34,V) continue and involve the rulemeasurevoltage(X,Y,V) *-- find(voltmeter),set(voltmeter,20),connect(bw, com,X),connect(rw,+,Y),vocalize(read(voltmeter, V)).This leads to a number of interactions, as traced in detail in Appendix A.
The readershould note in this continued interaction the manner in which the theorem provingdrives the dialog and the user model inhibits or enables voice interactions appropriateto the situation.The next interesting action occurs in utterance 12, when the user answers a re-quest o connect a wire with the question "Which knob?"
Here the response is parsedagainst expected meanings without success.
So the system looks for expectations ofother subdialogs that either have been invoked or might be invoked.
In this example,296Smith, Hipp, and Biermann An Architecture for Voice Dialog Systems"which knob" is an expected response for the first utterance, so control returns tothat subdialog.
In fact, in that subdialog, this response corresponds to a request forclarification.
In our studies, we have found such requests for clarification to be routineand have designed a special mechanism for handling them.
Our system dynamicallymodifies the active ruleadjust(knob,10) <-- usercan(adjust(knob,10)),vocalize(adjust(knob,10)).to include a clarification subdialog:adjust(knob,10) +-- find(knob),usercan(adjust(knob,10)),vocalize(adjust(knob,10)).Thus, the original user model was incorrect where it included find(knob), and thisassertion is deleted.
Next, as specified in Figure 1, the system reattempts he compu-tation with this revised rule.
The newly inserted subgoal causes the voice output ofutterance 13.This discussion shows the operation of all parts of the ZmodSubdialog model andillustrates the mechanisms u ed in our dialog machine.
Appendix A gives the detailedsteps required for completing the first part of the 22-utterance sample dialog.7.
An Overview of the System ArchitectureThe architecture of the system is given in Figure 2, where five major subsystems areshown: the dialog controller, the domain processor, the knowledge base, the generalreasoning system, and the linguistic interface.
These modules will be described next.Dialog controller.
This is the overall "boss" of the dialog processing system.
It formu-lates goals at the top level to be passed on to the theorem-proving stage.
It determinesthe role that the computer plays in the dialog by determining how user inputs relateto already established ialog as well as determining the type of response given.
Italso maintains all dialog information shared by the other modules and controls theiractivation.
Its control algorithm is the highest-level dialog processing algorithm.The basic cycle followed by the dialog controller is shown below.
(1)(2)(3)(4)(5)(6)Obtain suggested goal from the domain processor.Based on the suggested goal and the current state of the dialog, selectthe next goal to be pursued by the computer and determine theexpectations a sociated with that goal.
(The goal may thus be selectedfrom one of the active subdialogs.
The choice is partially dependent onthe current level of initiative.
)Attempt o complete the goal using the IPSIM system, possibly invokingvoice interactions.Update system knowledge based on efforts at goal completion.Determine next operations to be performed by the domain processor inproviding a suggested goal.Go to step 1.297Computational Linguistics Volume 21, Number 3DOMAIN PROCESSOR GENERAL REASONINGC, enera Oomph Kno.kx~don~n dependent por tk~ o f t=kkno.k~e0eometn~ bo=rd desc~nfundamental control str~eg~sOrc~ ~cDIALOG CONTROLLER /Coordination Algodthm for Other Modules- Invocation of domain processor- selection of action-invocation of IPSIM- expectation production- Invocation of I/O- ~=dge updateMode O~Uon/Jgodthm~out bteq~taUon/~godthm- Map Input o "world meaning"- Determine its relationship to discourse structure /UNGUISTIC INTERFACERecognizerRecdves and parses k~put basedon dlalo O expectations and contextby ?ontronerGeneratorkOSlMIntenuptible theorem provercontrolled externally which permitsoutside interaction (Le.
dialog) forobtaining missing =doms.
Externalcontrol can also be used to dynamicallyalter and/or suspend proofs.KNOWLEDGETask Knowledge=?Uon deconlx~Uongenera ~pectaUon=theorem= for goal ?=n~eUonproducUen of kacaUve descdptJomI~gulstic realizatiom of taskexpe=aUem?Jscocrse structure Takes utterance spedr~auen prowledby contrdler and produces utteranceto be spokenKno~Figure 2The system architecture.The domain processor.
This is the primary application-dependent portion of the sys-tem.
It contains much of the information about the application domain.
It receivesfrom the controller a request for the next suggested goal to be undertaken, and it re-turns to the controller its suggestion along with expected results from attempting thetest.
It then receives the results of the interaction and appropriately updates its datastructures.In the implemented system, the domain processor assists in electronic equipmentrepair and contains a debugging tree that organizes the debugging task.
The debug-298Smith, Hipp, and Biermann An Architecture for Voice Dialog Systemsging tree has as its root a node representing the whole device to be debugged andother nodes representing all of the subsystems.
It is organized using the "part of"relationship with each node that represents a part of a subsystem connected as a childnode below that subsystem.Device/ I \Subsystem1 Subsystem 2 Subsystem 3For example, in the implemented system, the top level device is a circuit called theRSl11; its subsystems are the power circuit, the T1 and T2 circuits, and the LED circuit.Their subsystems are wires, switches, transistors, and so forth.
The lowest level of thistree is the atomic element that can be addressed in a dialog.Each node has as its primary constituents a set of specifications and a set of flagsrepresenting its state.
The specifications have three parts, giving the observation to bemade, the conditions to be satisfied before making the observation, and the actions tobe taken depending on what is observed.
An example of one of the specifications forthe LED is as follows:Observation: Observe the behavior of the LED.Conditions: The switch must be on, and the control must be set to 10.Actions: IF the LED is alternately displaying a 1 and 7 with frequency greater thanonce per second THEN assert he specification is satisfiedELSE IF the LED is not onTHEN assert he battery is suspicious, in which case the power circuitis suspiciousELSE IF the LED is on but not blinkingTHEN assert hat the transistor circuits are suspiciousELSE IF the LED is blinking but not alternately displaying a 1 and a 7THEN assert hat the LED circuit is suspiciousELSE IF the LED is damagedTHEN assert hat the LED device should be replacedELSE IF .
.
.The actions may be to assert hat the specification is checked, to set suspicion flags onother nodes in the tree, or to replace parts.The other major component of a node is a set of status flags for the subsys-tem represented by the node.
One flag indicates whether the subsystem is checked,unchecked, partially checked, or suspicious.
Other flags give for each individual spec-ification its status and a counter for the number of iterations that the specification hasbeen checked.The domain processor algorithm chooses the node (subsystem) with the greatestsuspicion and the specification on that node with greatest suspicion and sends it tothe dialog controller for possible checking.
When two nodes or specifications are tiedfor being most suspicious, finer-grained criteria are used to break the tie.The algorithm is designed to guarantee that false information that may be enteredinto the tree will be eventually found.
This is done by allowing the iterations counteron each specification to reduce its effective level of suspicion.
Thus in a problematicdebugging situation, specifications with the smallest count will be checked again andagain until all have been checked the same number of times.
Additional checking will299Computational Linguistics Volume 21, Number 3then make the rounds of all specifications.
If erroneous information has been enteredafter any observation, that observation will eventually be repeated, enabling progressand guaranteeing ultimate success.The set of possible observations provides the situation-specific and situation-related expectations discussed in the section on expectations.
Thus, in the examplelisted above where the LED is being observed, the expectations are as follows:(1)(2)(3)(4)(5)the LED is alternately displaying a 1 and 7 with frequency greater thanonce per secondthe LED is not onthe LED is on but not blinkingthe LED is blinking but not alternately displaying a 1 and a 7the LED is damagedetc.These are passed to the controller at the time of the request for the LED observation.The reasoning system.
The IPSIM system receives as input goals to be proven andcommands to start, stop, and furnish information.
It yields as output theorems thatare proven and status reports on the proof in progress.
Its processing follows the usualmechanisms of Prolog-style theorem-proving, and it is modeled by the ZmodSubdialogroutine given above.
It uses the rules in the knowledge base described below.The knowledge base.
This is the repository of information about ask-oriented dialogs.This includes the following general knowledge about the performance of actions andgoals:(1) Knowledge about the decompositions of actions into substeps.
(2) Knowledge about theorems for proving completion of goals.
(3) Knowledge about the expectations for responses when performing anaction.There is also general task knowledge about completing locative descriptions.
Generaldialog knowledge includes knowledge about the linguistic realizations of task expec-tations as well as discourse structure information maintained on the current dialog.Finally, there is also knowledge about the user that is acquired during the course ofthe dialog.
Note that the predefined information of this module is easily modifiedwithout requiring changes to the dialog controller.Linguistic interface.
This system receives the spoken inputs from the user and re-turns spoken outputs.
We will examine first the voice input system.The inputs to the voice input system for each utterance are the set of expectationsfrom the dialog controller and the speech utterance from the user.
The output fromthis system is a GADL meaning representation.The core of the processor is a syntax-directed translator (Aho and Ullman 1969)with rules of the form A --* Wl:W2, where Wl should be thought of as an ordinarycontext-free grammar ighbhand side.
If the terminal string w can be generated bythe grammar ules of the from A --* Wl, then the analogous derivation using the rulesA ~ w2 will produce the translated output in GADL form.
As an example, suppose300Smith, Hipp, and Biermann An Architecture for Voice Dialog Systemsthe utterance "no wire" has been received and the following rules are in the systemgrammar:StartState ~ A : AA ~ TIS NOT WIRE : assertion(NOT, state(exist,WIRE,present))TIS --* :WIRE --* DET WIRE2 : WIRE2DET ~ ?NOT --* no : falseWIRE2 --* wire : wire(+,+)Then the derivation of the source string yields the meaning string as shown.StartStateATIS NOT WIRENOT WIRENOT DET WIRE2NOT WIRE2no WIRE2no wireStartStateAassertion(NOT, state(exist,WIRE,present))assertion(NOT, state(exist,WIRE,present))assertion(NOT, state(exist,WIRE2,present))assertion(NOT, state(exist,WIRE2,present))assertion(false,state(exist,WIRE2,present))assertion(false,state(exist, wire(+,+),present))That is, the meaning representation from the input "no wire" isassertion(false,state(exist,wire(+,+),present)).The dialog controller, of course, will provide an expectation for each receivedinput.
In the current example, assume the machine has previously output "there shouldbe a wire from terminal 102 to terminal 104."
Then the expectations would beassertion(true, state(exist,wire(102,104),present))assertion(false, tate(exist, wire(102,104),present))assertion(true,do(user, achieve(state(exist, wire(102,104),present))))question(yn,do_action(achieve(state(exist,wire(102,104),present))))assertion(true,comprehension)assertion(false, comprehension)question(wh,location(102,_))question(wh,location(104,_))question(wh,how_to_do(achieve(state(exist,wire(102,104),present))))The selected meaning of the incoming utterance will be the least-cost match between anoutput for the translation grammar and the expectations.
The mechanisms for findingthis least-cost match will be described next.The cost C of selecting a given expectation is a function of two parameters: (1) theutterance cost U, which measures the distance of the perceived voice signal from agrammatical utterance as defined by the system grammar, and (2) the expectation costE, which measures the degree of locality of the selected expectation with respect o301Computational Linguistics Volume 21, Number 3the current subdialog.
The utterance cost U will be small if the voiced signal preciselymatches a token sequence generated by the system grammar.
The expectation cost Ewill be small if the meaning of the utterance as generated by the translation grammarprecisely matches an expected meaning for the currently active subdialog.
Thus thetotal cost can be represented asC =f(U,E)where the exact nature of the function f is a problem to be solved.In this project, it was assumed that the speech recognizer would provide a graphof alternate guesses of the current input.
Thus it might pass the following to the parserafter a user had spoken "no wire.
"The parser then searches for paths through the graph that match as closely as possiblegrammatical inputs.
In searching for a path, the parser may delete or insert words toachieve a match.
Each such edit operation has an associated cost, depending on thesignificance of the word being edited.
Some words, such as "not" or major nouns,make a large difference in sentence meaning; other words, such as articles, may notcarry significant meaning in a given context.
U is the sum of the edit costs requiredto traverse the graph path and match a grammatical input.
In the example, the pathno-a-wire matches the grammatical "no wire" with the deletion of only an article "a.
"The computation of E was simple in our project.
A low value was assigned for allexpectations atthe current subdialog.
The sequential levels of more distant subdialogseach were given higher expectation costs.The original hypothesized combining function for U and E wasC = flU+ (1 - fl)Ewhere fl is a weighting factor between 0 and 1.
A fl near 0 will tend to prefer matchesto local expectation, regardless of the values of U; a fl near 1 will place most weighton getting a good match between the input graph and a grammatical string, regard-less of the value of E. Our experimentation, as described in Hipp (1992), indicatedthat fl should be near 1.
This supports the intuition that definitive source data atthe time of the utterance should be the preferred evidence regardless of expectation.Consequently, the usefulness of the expectation is for selecting between grammaticalutterances derived from the perceived voice signal that have minimal utterance cost.Reflecting this experimentally determined result, the cost computation was revised toC= ~ E i fU=U~n\ oo Otherwisewhere U~n is the smallest observed utterance cost for the given utterance.
This func-tion selects the meaning with minimum utterance cost and uses expectation to breakties.
A big advantage to using this form comes from the fact that any partial parsethat exceeds the currently known minimum can be abandoned immediately at greatsavings in computation time.The details of the minimization algorithm are given in Hipp (1992).
It followssome of the ideas of Aho and Peterson (1972), Levinson (1985), and Lyon (1974), and302Smith, Hipp, and Biermann An Architecture for Voice Dialog Systemswill not be described here.
It finds optimum answers in less than two seconds' timefor most utterances of lengths used in the environment of our system when runningon a Sun Sparc Station 2.The voice output system will not be discussed here.
It receives a GADL specifi-cation for an output and some parameters egarding the statement context, and usesa grammar to generate the desired word sequence.
It uses the context information toadapt outputs to their environment and sends the sequence to a DECtalk system forvoicing.8.
Some Implementation DetailsThe system has been implemented on a Sun 4 workstation with the majority of the codewritten in Quintus Prolog.
The parser is coded in C. Speech recognition is performedby a Verbex 6000 user-dependent connected-speech recognizer unning on an IBMPC, and the vocabulary is currently restricted to 125 words.
The users are requiredto begin each utterance with the word "verbie" and end with the word "over."
TheVerbex machine acknowledges each input with a small beep sound.
These sentinelinteractions help to keep the user and machine in synchronization.
The grammar usedby the parser consists of 491 rules and 263 dictionary entries.
The dictionary entriesdefine insertion and deletion costs for individual words as well as substitution costsfor phonetically similar words (such as "which" and "switch").The dialog system exclusive of the parser and error correction code consists ofabout 17,000 lines of Prolog (and this includes ome comments) apportioned as follows:Dialog Controller procedural mechanisms (including IPSIM) 15%, Dialog Controllerknowledge base 11%, Domain Processing procedural mechanisms 25%, Domain Pro-cessing knowledge base 14%, Linguistic Interface including much language generationcode 30%, miscellaneous 5%.The implemented omain processor was loaded with a model for a particularcircuit assembled on a Radio Shack 160-in-One Electronic Project Kit.
The model wascomplete nough to solve any problem of the circuit that involved missing wires.For example, if the system were asked to debug the circuit with no wires, it wouldsystematically discover each missing wire and request hat the user install it.The speech output was done with a DECtalk (trademark of Digital EquipmentCorp.)
DTCO1 text-to-speech onverter.9.
Testing the SystemA reasonable test of the theory and implementation described here is to bring humansubjects to the laboratory and determine whether they can converse sufficiently wellwith the machine to effectively solve problems.
The purpose of the testing was togather general statistics on system performance and timing, to study the effects ofmode, and to judge the human factors issues, learnability, and user response.
Thehypotheses were that the system would function acceptably, that with a reasonableamount of user training, machine directive mode would yield longer completion timesand less complex verbal behaviors than a more passive mode, and that users wouldrespond positively to using the system.
This section describes the design of the testsand the results obtained.9.1 Experimental DesignThree experimental sessions were used for each subject.
The first was to train thesubject and register subject pronunciations on the Verbex machine.
The second session303Computational Linguistics Volume 21, Number 3was a data-gathering test in which the subject could attempt up to ten problems withthe dialog system locked in either directive or declarative mode.
The third sessionallowed the subject o attempt up to ten additional problems.
It was similar to thesecond except hat the system was placed in the mode that was not used in session 2(either declarative or directive).
Experimentation was thus to be limited to just twomodes even though four were operative.Eight subjects were recruited from computer science classes.
They were selectedon the criteria that they (1) have demonstrated problem-solving skills by having suc-cessfully completed a computer science course and having enrolled in another, (2) nothave excessive familiarity with artificial intelligence or natural language processing aswould occur, for example, if they had had a course on one of these topics, and (3)not be an electrical engineering major (in which case they could probably repair thecircuits without aid).
They were told they would receive $36.00 for participating inthe three-part experiment.
All selected subjects were used and all collected ata arereported regardless of the level of success achieved.Session 1 introduced the subjects to the voice equipment and required that theyspeak at least two examples of each of the 125 vocabulary words.
They then wereasked to speak 239 sentences to train the system for coarticulation.
Repetitions ineither of these sessions were used as needed to obtain acceptable recognition rates.Next the subjects were told about the dialog system and its functions and capabili-ties in brief and simple terms.
They were given the basic rules on how to speak tothe system, including the need for carefully enunciated speech, the requirement forverbie-over bracketing, the importance of hearing the acknowledging beep, and spe-cial requirements for stating numbers.
They were told not to direct any comments tothe experimenter; however, the experimenter would occasionally give them help, aswill be described below.
The subjects were asked to listen to and repeat four sentencesspoken by the DECtalk system; this exercise was repeated until they overcame anydifficulties in understanding.
They were shown the target LED displays and givensuggestions on how to successfully describe such displays to the system; specifically,the user should tell what they see present on the display (as in "the top of a seven isdisplaying") and not describe what does not appear (as in "the bottom of the sevenis missing").
The subjects were provided with a list of the allowed vocabulary wordsand charts on a poster board suggesting implemented syntax if they wished to use it.Finally, they were given four practice problems and allowed to try solving them withthe machine operating in directive mode.
The complete session lasted up to two andone half hours.Session 2 was scheduled for three or four days later.
It began with a reorientation,60 practice sentences on the speech recognizer, and some review questions on thegeneral instructions.
If this session was in directive mode, the subjects were told thesystem would act like a teacher and that they should follow its instructions.
If thissession was in declarative mode, they were told the system would act like an assistantso that they could control the dialog, and they were given an example of a shortinteraction so that they could observe the kind of control that can be achieved.
Thenthey were released to do up to ten problems.Session 3 was scheduled for three or four days after the second session.
Appro-priate instructions were given to change the subject expectations to the new mode,and ten more sample problems were given.
Finally, the subjects were asked to fill ina short form and describe their reactions to using the system.An important issue in such tests, as has been observed elsewhere (Biermann, Fine-man, and Heidlage 1992), is the problem of giving the subject sufficient error messagesto enable satisfactory progress.
Users may wander aimlessly in their behaviors without304Smith, Hipp, and Biermann An Architecture for Voice Dialog Systemssome guidance when things go wrong.
If the input speech is discrete with a pauseafter every word, an automatic system can confirm words individually and give theuser adequate f edback (Biermann et al 1985).
But with connected speech, the systemcannot easily pinpoint he source of errors and may not provide satisfactory guidance.The user may receive an unexpected response from the system and then speak againbut with increased volume or nonstandard vocabulary; this may yield worse machineresponses and even more extreme behavior from the user.
Our answer to this prob-lem was to post the experimenter nearby and to allow him or her to give the subjectseveral different standard error messages if they were needed.
The experimenter wasallowed to deliver any of the following messages if certain criteria were met:1.
Due to misrecognition your words came out as .
(Mostmisrecognitions were corrected automatically and thus resulted in nosuch messge.
This message was given if the interpreted meaningcontradicted the intended meaning or referenced the wrong object.)2.
Please be patient.
The system is taking a long time to respond.3.
The system is ready for your next utterance.
(Or other synchronizationwarnings.)4.
Please remember to start/end utterances with verbie/over.5.
Recognition is indicated by a beep.6.
The word __  is not in the vocabulary.7.
(a number) must be spoken as digits.8.
Please restrict your utterances to one sentence.9.
Please keep your tone/volume/rhythm si ilar to the way you trained.10.
Please focus on interaction with the computer.
(In case of a comment tothe experimenter.)11.
Please follow the computer's guidance.
(After three repetitions caused bysubject's refusal to cooperate.
)Statistics were kept on the number of such messages that were delivered uring thetest sessions, as reported below.9.2 Test ProblemsThe circuit to be repaired was a multivibrator circuit constructed on a Radio Shack160 in One Project Kit.
It contained twenty wires and used a number of componentson the board: a switch, potentiometer, light-emitting diode (LED), battery, and twotransistors.
Its correct behavior was to alternately display a 1 and a 7 on the LED withthe rate of alternation being adjustable by the potentiometer.
For the purposes of theexperiment, failures were introduced by removing one or two wires.
The first eightproblems for the two sessions were matched in difficulty as well as possible in order togive balance between sessions and to prevent avarying difficulty from overshadowingimportant effects.
The last two problems were repeats of the practice problems fromthe first session.9.3 Test Dialog SystemThe dialog system being tested was the version that was operative at the time of thetest, mid February, 1991.
At that time, it was running on a Sun 4 machine, which caused305Computational Linguistics Volume 21, Number 3Table 1Experimental results for eight sClbjects operating at two levels of machine initiative;declarative and directive.
Numbers in parentheses are for the first eight problems only; thelast two problems were repeats of the practice problems from Session 1.Declarative Mode Directive ModeNumber of problems attempted 75 66Number of problems diagnosed 66 61Number of problems completed 60 58Average diagnosis time 199.9 (225.0) 258.9 (277.3)Average completion time 270.6 (303.8) 511.3 (541.0)Average number of utterances per dialog 10.7 (12.0) 27.6 (28.8)Average subject response time (sec.)
17.0 11.8Average number of utterances per minute 2.3 3.1Percent of nontrivial utterances 62.9 39.7Average length of nontrivial utterances 5.4 4.8Number of different words used 100 84Number of utterances 1011 1829Number of experimenter interactions 174 100Number of misrecognition i teractions 118 69Recognition rate (parser) 75.3 85.0Recognition rate (Verbex) 44.3 53.1Overall recognition rate (parser) 81.5Overall recognition rate (Verbex) 50.0significant problems with execution time.
A few responses during the experiment wereas slow as 10 seconds or in some cases as much as 30 seconds, which hampered theflow of the interaction.
These slow responses were primarily due to the computationalcosts of parsing long utterances containing many misrecognized words.
The systemwas later enhanced by moving it to Sparc-2 machine.The ability of the system to respond to silence as a legitimate input was disabledbecause it had earlier confused our pilot subjects.
If the user was silent for a periodof time, the system patiently waited for his or her input.
A small number of grammaromissions and other minor system shortcomings were noticed in the early subjectsand fixed for later subjects.Later versions of the system, such as the one on our demonstration tape (Hipp andSmith 1991), included some error message capabilities that were not available in theexperiment.
Those would have led to substantially better performance if they couldhave been used.9.4 ResultsThe results from the experiment are summarized in Table 1.Each entry in the table will be explained:Number of problems attempted.
Each subject had the opportunity to do 10 problemsat each initiative level.
Therefore if time constraints had not existed, a total of 80problems would have been attempted in each mode.
However, subjects progressedslowly enough so that the two-hour session (session 2 or 3) ended before all 10 wereattempted in some cases.Number of problems diagnosed.
The diagnosis of a problem involved discovering whatwires were missing.
However, substantial more interaction was needed to correctlyfix the problem and to test the circuit to be sure its complete functioning had beenrestored.306Smith, Hipp, and Biermann An Architecture for Voice Dialog SystemsNumber of problems completed.
A dialog was completed when the machine was sat-isfied that the repair was complete and had signed off by saying "good-bye" to theuser.
Most of the failures to complete were simply the result of limited time.
If adialog exceeded 10 minutes without a successful diagnosis or 15 minutes without asuccessful completion, and there was no expectation that progress would occur in thenext couple of minutes, the dialog was halted and scored as a failure.
The percent ofproblems completed might have been nearly 100 percent if unlimited time had beenavailable.Average diagnosis time.
The elapsed time from the beginning of the dialog to dis-covery of the missing wire(s).Average completion time.
The elapsed time from the beginning of the dialog to thecompletion of the task.Average number of utterances per dialog.
The average number of utterances by thesubject per dialog.Average subject response time.
The average lapsed time (in seconds) from the com-puter's utterance to the subject's response.
It is believed that this is a better indicatorfor the "speech rate" of a subject han average utterances per minute because the num-ber of utterances per minute is dependent on the speed of the computer system aswell, and long utterances by the subject required long response times for the computer.Average number of utterances per minute.
The average number of utterances perminute spoken by the subject on all dialogs, even those that were not completed.Percent of nontrivial utterances.
The percent of utterances spoken by the subject hatincluded more than one word and the bracketing words (verbie and over).Average length of nontrivial utterances.
The average number of words in a nontrivialutterance by the subject not including the bracketing words.Number of different words used.
The total number of different words used by thesubjects that were implemented in the system.Number of utterances.
The total number of utterances spoken by all subjects in alldialogs.Number of experimenter interactions.
The total number of times the experimenterspoke to a subject during the dialogs.Number ofmisrecognition i teractions.
The number of experimenter interactions thatwere caused by misrecognitions.
This figure shows how many of the interactionsspecified in the previous row were due to the single cause of misrecognition.
Themessage in this case was "Due to misrecognition, your words came out as .. .
.
"Recognition rate (parser).
The percentage of subject utterances that were correctlyinterpreted by the parser after error correction and the use of expectation.Recognition rate (Verbex).
The percentage of subject utterances that were correctlyrecognized word-for word by the speech recognition system.Overall recognition rate.
The recognition rate obtained by combining data acrossboth modes.After subjects completed session 3, they were given a form to gather their reac-tions to the use of the system.
Table 2 summarizes their answers on a scale from 1(representing "strongly disagree") to 5 (representing "strongly agree").
Additional in-formation about user responses, including their extensive remarks, is given in Smith(1991).9.5 Test Data Analysis9.5.1 General results.
Overall system functioning was good.
All subjects were ableto use the system, and they solved the problems at the rates of 80 and 88 percent(that is 60 out of 75 trials and 58 out of 66 trials) in the declarative and directive307Computational Linguistics Volume 21, Number 3Table 2User reactions to system use.Subjects Using Directive Subjects Using Declarativethen Declarative Modes then Directive ModesEasy to learn to use 4.5 4.2Enjoyed using 4.2 3.9Easy to use 3.8 4.0Tiring 3.8 4.0System had too much control initially 3.0 2.6System had too much control at end 2.5 4.1modes, respectively.
Most of the failures to finish were from time outs rather than anyultimate system failure.
Subject speech rates were around two to three utterances perminute, but this might have risen to four to six sentences per minute if the systemresponse time had been better.
(This higher speech rate occurred in an earlier systembuilt in this lab (Biermann, Fineman, and Heidlage 1992), and might also be possiblewith our current faster version of this system.)
The powerful error correction from thenearest neighbor algorithm using expectation improved overall recognition from 50.0to 81.5 percent.
The number of experimenter interactions was about 1 for every 18user utterances in the directive mode, and 1 for every 6 user utterances in the morecomplex declarative mode.The effects of mode were clear and as predicted.
During session 1, subjects hadgained substantial information about how to debug this circuit, so the extremelypedantic directive mode was too detailed for them in most cases.
They functionedmuch better in declarative mode, in which the machine stated relevant facts but al-lowed the user to lead the interaction.
The completion times and numbers of utteranceswere far smaller for the declarative mode.
In addition, the complexity of the sentencesand the time required for the user to respond was larger for the declarative mode.
Thedirective mode involves far more questions from the machine with only one-wordanswers, yielding faster responses and higher recognition rates.
The directive moderequires far fewer experimenter interactions to help the subject hrough the task.Users responded positively to the system and definitely noticed the effects of themode change.The posters for providing syntax assistance were not used extensively.
Based onexit interviews with subjects, two of them did not use them at all during the formalexperimental sessions (sessions 2 and 3).
Three subjects used them only for a specificsituation (such as notifying the system that the circuit was working), while the remain-ing three subjects used the charts for assistance with more than one class of utterances(such as voltage statements and LED description statements).
However, even thesesubjects did not continually refer to the charts, but only used them infrequently.Additional study of the data yielded some interesting information about the sub-jects' rate of learning of the system.
One might wonder how users' behaviors improvedwith time.
The total number of problems attempted in both modes in session 2 was67, and this rose to 74 in session 3.
The rate of success rose from 79 percent o 88percent.
It is also interesting to speculate how long it would be necessary to keep theexperimenter at hand to give error messages.
The number of experimenter interactionsdropped from 162 in session 2 to 112 in session 3.308Smith, Hipp, and Biermann An Architecture for Voice Dialog SystemsTable 3The statistical results for the eight problems.Completion Time Utterances % Non TrivialProb.
n t p value t p value t p value1 5 .686 -- 3.340 .05* -3.619 .05*2 5 -.018 -- 1.170 .40 -3.267 .05*3 4 1.547 .30 3.633 .05* -3.441 .05*4 4 2.971 .10 4.586 .02* -3.118 .105 4 1.342 .30 4.557 .02* -2.281 .206 7 3.136 .05* 4.516 .01"* -1.874 .207 5 2.145 .10 3.783 .02* -2.665 .108 4 1.876 .20 3.235 .05* -1.811 .20Another phenomenon ften noticed in the speech recognition world is the "sheep-goats" dichotomy that seems to divide users.
Some users seem to function well withvoice recognition equipment ("sheep") while others ("goats") have considerable diffi-culties.
We noticed this in that four of our subjects were able to attempt all 80 problemsgiven them and solved 79 of the 80.
Their recognition rates tended to be in the high80s or low 90s.
The much lower reported average figures given above are the resultof averaging in performance figures from the less successful class of users.A final issue of interest is the level of software reliability achieveable by sucha system.
It has been observed in previous research systems that the code is suffi-ciently complex and the level of development in a research laboratory sufficientlyincomplete that substantial numbers of software failures do occur in such tests.
Forexample, Damerau (1981), Miller, Herschman, and Kelly (1978) on the LADDER sys-tem, and Biermann, Ballard, and Sigmon (1983) reported software failures at the ratesof approximately 5, 2, and 2 percents, respectively.
In the current est, of the 2, 840 ut-terances that were processed, only one resulted in such a software failure.
The reasonsfor the high reliability were (1) the quality of the code was high and (2) the nature ofthe design that finds a nearest neighbor to an input string among expected inputs isone in which high reliability is much more easily obtained.9.5.2 Statistical Analysis.
Although the summary results provide a strong indicationthat there are behavioral differences as a function of initiative, there are two majorproblems in demonstrating the statistical significai~ce of these differences: (1) onlyfour subjects completed a high enough percentage of the dialogs to enable a within-subject comparison of these differences across all problems, and (2) there were largesample variances in the data.To alleviate these problems, behavioral differences were analyzed separately foreach problem.
This was possible because of the balancing of problems between sessionsaccording to problem type.
That is, problem "k" of each session had the same erroneousLED display.
Consequently, the system's diagnosis procedure was virtually identicalfor problem "k" in each session.
The only difference came in identifying the missingwire or wires that caused the circuit failure.
The missing wires were different in eachsession.Three performance criteria were examined: (1) completion time, (2) number ofutterances spoken, and (3) percentage of nontrivial utterances.
By examining the dif-309Computational Linguistics Volume 21, Number 3ference in a subject's performance on problem "k" as a function of initiative, a pairedt test (Larsen and Marx 1981) could be conducted.
The null hypothesis would say thatthe average of the paired differences i 0.
Table 3 shows the results.
For example, forproblem 1, there were five subjects who completed the dialog in both sessions, andtheir paired difference for each criterion is obtained by the following formula:"directive mode value" - "declarative mode value"For example, the first subject spoke 18 utterances in directive mode for problem 1,but only 11 utterances when in declarative mode, a paired difference of +7.
The set ofpaired differences for a given problem provides the sample data for the test statistic.The t statistic and the individual observed significance levels are shown for eachphenomena, one row per problem.
In the results for number of utterances poken,all problems but problem 2 showed an observed significance in user behavior as afunction of initiative at the p = 0.05 level or less.
There are also strong trends to-ward statistically significant behavioral differences with respect o completion timeand nontrivial utterances for several of the problems.
One difficulty with respect ocompletion time is the fact that the implementation of the parsing algorithm was notyet optimized during the experiment, and the more prevalent nontrivial utterancesof declarative mode would require a longer time to parse.
Note that while almostall of the significance levels support the hypothesis of a real difference, because theproblems are interrelated, and the tested subjects overlap from problem to problem,we cannot claim overwhelming statistical significance.
Nevertheless, we believe theseresults provide a strong indication that our system demonstrates variable initiativebehavior, and that user behavior differs according to the level of initiative.10.
Theoretical Issues from the LiteratureGrosz and Sidner (1986) have given a high-level theory of dialog.
The theory specifiesthree components, the linguistic, intentional, and attentional structures, and describestheir nature and relationships to each other.
However, their theory leaves a wholevariety of issues undetermined; without a full implementation, the question of itsapplicability remains unanswered.
This project shows a successful instantiation ofthese ideas and provides some verification of their correctness.
In fact, it presentsa working speech dialog system that follows their theory in most of its details.
Indoing so, the project clarifies the nature of the three components and increases ourunderstanding of them.The linguistic component is the actual sequence of dialog interactions, and its mostsignificant characteristic is that it can be naturally divided into what Grosz and Sidnercall segments.
(We have used the term "subdialog" instead of "segment" because wefeel the latter term connotes contiguity, which may be misleading.)
These segmentsare semantically coherent subparts of the dialog, and their union constitutes the wholedialog.
An individual segment may be a contiguous sequence of interactions, or it maybe broken into several sequences, as described in earlier sections of this paper.
Manyresearchers have studied the properties of such segments, including linguistic andintonational markers to delineate them (Hirschberg and Litman 1993; Hirschberg andPierrehumbert 1986), the resolution of noun phrases within the context of a segment(Grosz 1978; Reichman 1985), and their semantic self-consistency (Hobbs 1979).
Ourtheory provides an automatic mechanism for participating in dialogs, for followingsegments initiated by the user, for initiating segments on its own, and for properlyutilizing segment semantics for many language-processing purposes, such as nounphrase resolution, error correction in speech recognition, and so forth.310Smith, Hipp, and Biermann An Architecture for Voice Dialog SystemsThe intentional component specifies the purpose of the dialog.
Grosz and Sidner(1986) use the notation DP and DSP to stand for "discourse purpose" and "discoursesegment purpose."
These entities correspond to the predicate goals that our systemposes and then builds the dialog around.
Our system, in fact, constructs a set of partialproofs and these are our instantiation of the intentional component.
Grosz and Sidnerintroduce the relation of "dominance" between two DSPs; one DSP dominates anotherif the other "is intended to provide part of the satisfaction" of the first.
This relationcorresponds to the natural precedence in our system that a Horn clause proof givesto the predicates in the proof tree.
Grosz and Sidner emphasize that the number ofpossible discourse purposes must be infinite in a dialog system, an assertion thatwe agree with.
Our predicates allow for arbitrarily complex nesting of functionalstructures.
As a trivial example, in our theory applied to the block-stacking world, theDSP buildtower (A on B on C on ... on X) is a legal goal regardless of how manyblocks are specified to be stacked.
Another issue they discuss is the possibility ofmultiple simultaneous goals.
Thus the speaker may wish to both convey informationand impress the hearer with his or her intelligence.
This is a problem we have notattempted to deal with in our theory, and we do not offer any solutions to it.
Oursystem deals with multiple goals but each subdialog addresses only one at a time.The attentional structure is a stack in the theory of Grosz and Sidner and in manyother theories as well (Litman and Allen 1987).
The current incoming sentence ismatched to local expectation, the entities at the top of the stack, and its meaning willbe processed with respect to these structures in focus unless ome kind of inconsistencyoccurs.
If the sentence does not have meaning within this context, the stack may bepushed to introduce a new topic, or it may be popped and processing can occurwithin the next context in the priority.
In our theory, the local expectation or focus isrepresented by the set of predicates that are related by our Prolog-style rules to thecurrently active predicate or DP.
If no successful match is made, then other nonlocalbut recently active DPs are tried.The fact that our representation f local focus is a set of predicates on the proof tree(the intentional structure) may seem to contradict the Grosz-Sidner assertions that thefocus-space hierarchy and the intentional structure should not be conflated.
However, acloser look reveals that our focus stack exists independently of the intentional structureeven though the items on the stack are listed on the intentional structure.
The focusstructure may be thought of as a stack of pointers; those pointers give addresses onthe intentional structure, but the independence of the two entities is not compromised.An example given by Grosz and Sidner (which in turn was borrowed from L. Pol-anyi and R. Scha) illustrates many of these points.DI: John came by and left the groceriesD2: Stop that you kidsD3: and I put them away after he leftFigure 3 shows our representation f the interaction after the utterance D1.
Two goalsare currently active, DSP=describe prepare dinner and DSP=reduce stress; their partialproof trees are shown.
The focus stack contains the description of getting food as itstop entry, and previous topics have been pushed down to lower levels.
The speaker'spriority system, however, in the next instant has pushed the goal "reduce stress" intothe foreground with the associated utterance D2, as shown in Figure 4.
Here the stackreceives an additional entry, a pointer to the control-children DSP.
With the control-children DSP achieved, the focus stack can pop back to the previous interaction andcontinue, as shown in Figure 5.
Actually, we deviate from Grosz and Sidner here andkeep the recent subdialog active near the top of the stack.
It may be returned to; for311Computational Linguistics Volume 21, Number 3Other DSPIA DS\]P=de~ribefocus ~ de.,m:ril~ romp findsm,:k bring put dimulmon al~nmiveawayFigure 3Intentional and attentional structures after "John came by and left the groceries.
"Other DSP.focm ~ de, m:ril~ slop findstack Ixing put ~ ~vegroceries awayFigure 4The speaker's priority intervenes to reduce stress: "Stop that you kids.
"example, the next utterance could beD4: Have you finished your homework?We do this because we do not want our system to have to look far to find the appro-priate referents in any recently active subdialog.312Smith, Hipp, and Biermann An Architecture for Voice Dialog SystemsOther DSPm" I1\  .. .
.
I1\  -awayFigure 5Returning to the DSP=describe g t food intention: "and I put them away after he left."
Therecent focus on DSP=control children is retained on the focus stack.Litman and Allen (1987) introduce the concept of a "discourse plan" that mesheswith the domain plans of the traditional literature (Fikes and Nilsson 1971).
The ideaof discourse plans is that they operate on domain plans and act as meta operators.But they still are plans and are interpreted by the same mechanism as traditionalplanning systems.
Discourse plans fit into three classes: the continue, clarification, andtopic shift classes.
As each incoming utterance arrives, one of these discourse planscomes into action to interpret hat utterance in terms of the existing domain plans.A continue class discourse plan relates the utterance to the existing domain plan asa continuation of that domain plan.
A clarification class plan relates the utterance tothe existing domain plan as a clarification of that domain plan.
A topic shift planrejects any relationship to the current domain plan and introduces a new domainplan.
Our system has corresponding behaviors, but they are coded into the interpreterfor the Prolog-style rules.
(Litman and Allen comment that the discourse plans area small set and that they are domain-independent, so we find their inclusion in theinterpreter to be a realistic choice.)
Continuations and clarifications are handled asstandard mechanisms of our interpreter.
Topic shifts are handled by the mechanismthat seeks matches of an incoming meaning with other subdialogs if the meaning hasno local match.Webber and Baldwin (1992) have examined the phenomenon called "contextchange" in discourse, which refers to the movement from subdialog to subdialog (orfrom segment to segment).
Their discussion focuses on the concept of the working set,which specifies "the set of entities in the discourse context."
As discourse proceeds,this working set is updated and it provides at each point in time the set of objectsfor definite noun phrase resolution.
They describe two mechanisms for updating theworking set; one, called context change by entity introduction, adds entities to the work-ing set as they appear in the discourse.
The other is called context change by eventsimulation, and it assumes that operators within the reasoning system add and delete313Computational Linguistics Volume 21, Number 3items from the working set as a side effect of planning.
In our system, the analog tothe working set is the set of objects, relationships, and actions pecified by the rulesin the current subdialog.
However, our system does not increment or decrement itsworking set in a gradual manner, one object at a time or one noun phrase at a time.When a subdialog is invoked, all of its objects, relationships, and actions come intofocus at once; when the subdialog is abandoned, all of these entities are demoted tononprimary but still near-at-hand availability.
Our system does not typically changesubdialog merely because an unexpected entity has been introduced.
The specificationof a new entity without any sentential context would probably confuse our system; itlooks for utterances that will be meaningful in the current subdialog and it changessubdialog when an incoming utterance fails to have meaning locally but does havemeaning in another available subdialog.The semantic interpretation f sentences in realistic situations ometimes can beenabled by abduction mechanisms, as described by Hobbs et al (1988).
Abduction isneeded when a literal interpretation f incoming tokens will not match the items orrelationships in the existing database; it provides a mechanism to "coerce" a specificand literal meaning into a meaning that accounts for the sentence context.
For example,Hobbs et al (1988) address the issue of finding a referent for the implied fluid in thefirst sentence below.Flow obstructed.
Metal particles in lube oil filter.According to Hobbs et al (1988), the second sentence will require proving there existsx lube-oil (x), and the problem is to find a logical connection between the fluid ofsentence one and the "lube oil."
Their solution is to propose the identityV x ((fluid (x) A etq (x)) - lube-oil (x))where etcl(x) specifies a set of additional characteristics of x needed to ensure theidentity.
These properties etcl(x) need not be known; they are what Hobbs et al call"assumable," and the act of making the assumption is the abductive act central to the"coercion" being carried out.
This identity and the associated assumption make it pos-sible to equate the fluid of the first sentence with the lube oil of the second sentence.The mechanism of our system achieves a similar result but via different means.
Somecurrently active subdialog, which might or might not be at the highest level on thefocus stack, would contain the lube oil and a variety of its characteristics and rela-tionships.
As incoming sentences arrive, their interpretations would be matched to theobjects and relationships in the focus spaces using the minimum distance algorithmaccording to the stack priority until an acceptable match is found.
In this example,there would be a focus space with an object "lube oil" that has the property of being a"fluid" that can "flow" and so forth.
The words in the given sentences would naturallymatch to these without any special mechanisms.
In summary, the abduction takes careof the situation where the semantic model is inadequate, and our design attempts toavoid such inadequacies.A number of important contributions have been made in the area of user modeling(Kobsa and Wahlster 1988; Rich 1988; Paris 1988; McCoy 1988).
One of the most com-prehensive is the "General User Modeling Shell" (GUMS), described by Finin (1989).He lists five important features for user models:(1)(2)Separate knowledge base.
The user model is kept as a separate module.Logical representation.
The user model knowledge is encoded so as to bedirectly usable in inferential processes.314Smith, Hipp, and Biermann An Architecture for Voice Dialog Systems(3)(4)(5)Declarative knowledge.
The user knowledge should be in a declarativerather than procedural form.Support for abstraction.
The user knowledge should be able to representgeneralizations.
For example, it should be possible to representinformation about classes of users as well as individuals.Multiple use.
The user model knowledge should be represented in aform such that it can be reasoned about as well as reasoned with.An important feature of this model is a hierarchical structure for stereotypes aboutusers.
Each level in the structure assumes a set of "definite" knowledge and a set of"default" knowledge.
The given user in an interaction is placed somewhere on thehierarchy according to the best information.
Then, as new information is gathered, heor she can be moved to a more general level (less definite information) if the user'sknowledge is in contradiction with the required efinite knowledge of that level.In addition to a user's stereotype, an individual model of the user will also bedeveloped.
As specific information is acquired about the user, the system will beginto rely more on the individual model and less on the stereotype model.?
Our system supports the first three features described by Finin.
These are sideeffects of the Prolog-style representation.
We could easily support he ideas related toclasses of users and the hierarchy of stereotypes, but they were not addressed in thisproject.We do not know of any systems that claim to support nontrivial variable initiativeas described in our system.
However, there is some literature on the topic.
Whittakerand Stenton (1988) propose a definition of dialog control based on the utterance typeof the speaker: question, assertion, command, prompt.
In the first three, the speakerusually has control unless the utterance is a direct response to a question.
In thelast, control is usually being relinquished.
They note that there is a close relationshipbetween topic shift and control shift, and thus dialog control is useful in identifyingthe discourse structure.Kitano and Van Ess-Dykema (1991) extend the plan recognition model of Litmanand Allen (1987) to consider variable initiative dialog.
They note that the two partici-pants may have different domain plans.
Consequently, there must be speaker-specificplans as well as the joint plans proposed in the Litman and Allen model.
This sep-aration of plans enables a more flexible plan recognition process.
In addition, theyextend the initiative control rules of Whittaker and Stenton to consider utterance con-tent.
They note that when a speaker makes an utterance that is relevant to his or herspeaker-specific domain plan, then that speaker has dialog control.
Their observationthat there are speaker-specific plans or goals is crucial to our proposed model for par-ticipating in variable initiative dialogs.
Selecting the next subdialog to be entered mayrequire selecting between differing computer and user goals.
This selection processmust be a function of the current level of dialog initiative.11.
SummaryA voice-interactive dialog architecture that achieves imultaneously a variety of behav-iors believed to be necessary for efficient human-machine dialog has been developed.Goal-oriented behavior is supplied by the theorem-proving paradigm, and the missingaxiom theory provides the machinery for voice interactions.
Subdialogs and movementbetween them is implemented with an interruptible theorem prover that maintains aset of partially completed proofs and can work on the most appropriate one at any315Computational Linguistics Volume 21, Number 3given time.
A user model is provided by a continuously changing set of rules thatare referenced uring theorem proving either to enable or to inhibit voice interaction.Variable initiative is made possible by variable types of processing by the input andoutput routines and by restricting or releasing the ability to interrupt o a new subdi-alog.
Expectation is associated with individual subdialogs, is compiled from domainand dialog information related to each specific output, and is used to improve voicerecognition and enable movement between subdialogs.While this system simultaneously achieves behaviors that have been studied in-dividually by a variety of earlier researchers, it cannot be reasonably characterized asan amalgamation of other systems.
The behaviors are achieved by a new mechanism,namely the system-controlled Prolog-style inference machine, and the contribution ofthe work is in the demonstration that these behaviors can be done with this mech-anism and in its description of how to do it.
The contribution of other researchershas been to investigate the individual issues related to achieving dialog so that as wefaced them, we had ideas about what to do using our mechanisms.The implementation of the system shows the viability of the architecture andprovides information regarding achievable behaviors in real-time voice dialogs.
Eightsubjects were able to solve 84 percent of attempted problems and reported a positiveexperience in doing it.A number of important unsolved problems need attention to enable furtherprogress.
For example, an automatic mechanism is needed for setting the appropriatelevel of initiative as an interaction proceeds.
Also, techniques are required that willenable the system to better diagnose user problems o that adequate messages can beprovided automatically rather than depending on the experimenter.
And the system,as it stands, can probably be improved greatly by increasing its knowledge base andrefining its mechanisms.The dialog theory described here has been aimed at implementing voice interac-tions.
However, the theory could be used in other contexts using menus and multi-media or other interaction styles.
This is another area for future work.Appendix A: Trace of ZmodSubdialog Processing the 22-Sentence Example Dialogfrom Section 31.
12.
13.
24.
25.
26.
27.
28.
29.
210.
311.
312.
3Retrieve goal from dialog controllerTl_circuit_Test2(V)Enter subdialog with that goalCreate subdialog data structuresGrab next available rule from knowledgeTl_circuit_Test~(V) ~ set(knob,10),measurevoltage(121,34,V)Unify with current goalThis is a general ruleStore its right hand side as goalsGrab next one and enter a subdialogset (knob,10)Enter subdialog with that goalCreate subdialog data structuresGrab next available rule from knowledgeset(knob,Y)+--find(knob),adjust(knob,Y)Unify with current goal316Smith, Hipp, and Biermann An Architecture for Voice Dialog Systems13.
3 This is a general rule14.
3 Store its right hand side goals15.
3 Grab next one and enter a subdialogfind(knob)16.
3 Enter subdialog with that goal17.
4 Create subdialog data structures18.
4 Grab next available rule from knowledgefind(knob)19.
4 Unify with current goal20.
4 Trivially answers question21.
4 Exit subdialogCurrent rule: set(knob,Y)*-find(knob),adjust(knob,Y)22.
3 Grab next right hand side and enter a subdialogadjust(knob,10)23.
3 Enter subdialog with that goal24.
4 Create subdialog data structures25.
4 Grab next available rule from knowledgeY ~ usercan(Y),vocalize(Y)26.
4 Unify with current goal27.
4 This is a general rule28.
4 Store its right hand side goals29.
4 Grab next one and enter a subdialogusercan(adjust(knob,10))30.
4 Enter subdialog with that goal31.
5 Create subdialog data structures32.
5 Grab next available rule from knowledgeusercan(adjust(knob,X))33.
5 Unify with current goal34.
5 Trivially answers question35.
5 Exit subdialogCurrent rule: Y~usercan(Y),vocalize(Y)36.
4 Grab next right hand side and enter a subdialogvocalize(adjust(knob,10))37.
4 Enter subdialog with that goal38.
5 Create subdialog data structures39.
5 Grab next available rule from knowledgevocalize(Y)40.
5 Unify with current goal41.
5 Goal is to vocalize42.
5 Execute verbal outputPut the knob to one zero.43.
5 Record expectation Aaffirmativenegativeuser put knob to 10where is knobetc.44.
5 Receive responseOK.317Computational Linguistics Volume 21, Number 345.
5 Record implicit, explicit meanings for response46.
5 Transfer control depending on response47.
5 Successful response: Return48.
5 Exit subdialogCurrent rule: Y ~ usercan(Y),vocalize(Y)49.
4 Exit subdialogCurrent rule: set(knob, Y) ~ find(knob),adjust(knob, Y)50.
3 Exit subdialogCurrent rule: Tl_circuit_Test2(V) ~ set(knob,10), measurevoltage(121, 34, V)And so forth.
The rest of the 330-step trace for processing the example 22-sentencedialog is available from the third author on request.AcknowledgmentsWe would like to acknowledgecollaboration with Robert Rodman duringthe early years of this project.
In particular,we acknowledge his student Robin Gambill,who wrote the sentence-generation softwarefor the system.
Dania Egedi wrote some ofthe grammars for recognition and helpedwith the subject esting.
Ruth S. Dayadvised us on the design of the humanfactors testing, and Robert M. Hoekstrasuggested the statistical tests.
The NationalScience Foundation supported the workunder Grants IRI 88-03802 and IRI 92-21842.Duke University supported the work undera special grant authorized by Dr. Charles E.Putman.We are very appreciative of the carefulreading and suggestions provided byanonymous reviewers of this paper.ReferencesAho, Alfred V., and Peterson, Thomas G.(1972).
"A minimum distance rrorcorrecting parser for context freelanguages."
SIAM Journal on Computing,1(4), 305-312.Aho, Alfred V., and Ullman, Jeffrey D.(1969).
"Properties of syntax directedtranslations."
Journal of Computer andSystem Sciences, 3(3), 319-334.Allen, James F. (1983).
"Recognizingintentions from natural anguageutterances."
In Computational Models ofDiscourse, edited by M. Brady andR.
C.
Berwick., 107-166.
M1T Press.Allen, James F., and Perrault, C. Raymond(1980).
"Analyzing intention indialogues."
Artificial Intelligence, 15(3),143-178.Allen, James; Guez, Stephen; Hoebel, Louis;Hinkelman, Elizabeth; Jackson, Keri;Kyburg, Alice; and Traum, David (1989).
"The discourse system project."
TechnicalReport 317, University of Rochester, NewYork.Biermann, Alan W.; Ballard, Bruce W.; andSigmon, Anne H. (1983).
"Anexperimental study of natural anguageprogramming."
International Journal ofMan-Machine Studies, 18, 71-87.Biermann, Alan W.; Fineman, Linda; andHeidlage, John F. (1992).
"A voice- andtouch-driven atural anguage ditor andits performance."
International Journal ofMan-Machine Studies, 37, 1-21.Biermann, Alan W.; Rodman, Robert; Rubin,David; and Heidlage, John E (1985).
"Natural anguage with discrete speech asa mode for human to machinecommunication."
Communication ftheACM, 18(6), 628--636.Carberry, Sandra (1988).
"Modeling theuser's plans and goals."
ComputationalLinguistics, 14(3), 23-37.Carberry, Sandra (1990).
Plan Recognition iNatural Language Dialogue.
MIT Press.Carbonnel, N., and Pierrel, J. M.
(1988).
"Task-oriented dialogue processing inhuman-computer voice communication.
"In Recent Advances in Speech Understandingand Dialog Systems, edited by H. Niemann,M.
Lang, and G. Sagerer, 74--107.Springer-Verlag.Chin, David N. (1989).
"KNOME: Modelingwhat the user knows in UC."
In UserModels in Dialog Systems, edited by AlfredKobsa and Wolfgang Wahlster, 74-107.Springer-Verlag.Cohen, Robin, and Jones, Marlene (1989).
"Incorporating user models into expertsystems for educational diagnosis."
InUser Models in Dialog Systems, edited byAlfred Kobsa and Wolfgang Wahlster,313-333.
Springer-Verlag.Damerau, Fred J.
(1981).
"Operatingstatistics for the transformational questionanswering system."
ComputationalLinguistics, 7(1), 30-42.318Smith, Hipp, and Biermann An Architecture for Voice Dialog SystemsErman, Lee D.; Hayes-Roth, Frederick;Lesser, Victor R.; and Reddy, D. Raj(1980).
"The Hearsay?IIspeech-understanding system: Integratingknowledge to resolve uncertainty."
ACMComputing Surveys, 12, 213-253.Fikes, Richard E., and Nilsson, Nils (1971).
"STRIPS: A new approach to theapplication of theorem proving toproblem solving."
Artificial Intelligence 2,2(3/4), 189-208.Fink, Pamela E., and Biermann, Alan W.(1986).
"The correction of ill-formed inputusing history-based expectation withapplications to speech understanding.
"Computational Linguistics, 12(1), 13-36.Finin, Timothy W. (1989).
"GUMS: AGeneral User Modeling Shell."
In UserModels in Dialog Systems, edited by AlfredKobsa and Wolfgang Wahlster, 411-430.Springer-Verlag.Grosz, Barbara J.
(1978).
"Discourseanalysis."
In Understanding SpokenLanguage, edited by D. E. Walker, 235-268.North Holland.Grosz, Barbara J., and Sidner, Candace L.(1986).
"Attentions, intentions, and thestructure of discourse."
ComputationalLinguistics, 12(3), 175-204.Hipp, D. Richard (1992).
A new techniquefor parsing ill-formed spokennatural-language dialog.
Doctoraldissertation, Duke University, Durham,North Carolina.Hipp, D. Richard, and Smith, Ronnie W.(1991).
A demonstration f the "Circuit Fix-ItShoppe" \[Twelve-minute videotape\].Department of Computer Science, DukeUniversity, Durham, North Carolina.Hirschberg, Julia, and Litman, Diane (1993).
"Empirical studies on the disambiguationof cue phrases."
Computational Linguistics,19(3), 501-530.Hirschberg, Julia, and Pierrehumbert, JanetB.
(1986).
"The intonational structuring ofdiscourse."
In Proceedings, 24th AnnualMeeting of the Association for ComputationalLinguistics.
New York.
136--144.Hobbs, Jerry R. (1979).
"Coherence andcoreference."
Cognitive Science, 3, 67-90.Hobbs, Jerry R.; Stickel, Mark; Martin, Paul;and Edwards, Douglas (1988).
"Interpretation asabduction."
InProceedings, 26th Annual Meeting of theAssociation for Computational Linguistics.95-103.Kautz, Henry A.
(1991).
"A formal theory ofplan recognition and its implementation.
"In Reasoning about Plans, edited byJ.
E Allen, H. A. Kautz, R. N. Pelavin,and J. D. Tenenberg, 69-125.
MorganKaufmann.Kitano, Hiroaki, and Van Ess-Dykema,Carol (1991).
"Toward a plan-basedunderstanding model for mixed-initiativedialogues."
In Proceedings, 29th AnnualMeeting of the Association for ComputationalLinguistics.
25-32.Kobsa, Alfred, and Wahlster, Wolfgang,editors.
(1988).
Special Issue on UserModelling.
Computational Linguistics, 14(3).Kobsa, Alfred, and Wahlster, Wolfgang(1989).
User Models in Dialog Systems.SpringeroVerlag.Larsen, Richard J., and Marx, Morris L.(1981).
An Introduction to MathematicalStatistics and Its Applications.
Prentice-Hall.Lehman, Jill Fain, and Carbonell, Jaime G.(1989).
"Learning the user's language: Astep towards automated creation of usermodels."
In User Models in Dialog Systems,edited by Alfred Kobsa and WolfgangWahlster, 163-194.
Springer-Verlag.Levinson, Steven E. (1985).
"Structuralmethods in automatic speechrecognition."
In Proceedings ofthe IEEE,73(11), 1625-1650.Linde, Charlotte, and Goguen, J.
A.
(1978).
"Structure of planning discourse."
J. SocialBiol.
Struct., 1,219-251.Litman, Diane J., and Allen, James F.
(1987).
"A plan recognition model forsubdialogues in conversations."
CognitiveScience, 11(2), 163-200.Lyon, Gordon (1974).
"Syntax-directed leasterrors analysis for context-freelanguages."
Communications of the ACM,17(1), 3-14.McCoy, Kathleen E (1988).
"Reasoning on ahighlighted user model to respond tomisconceptions."
Computational Linguistics,14(3), 52-63.Miller, H. G.; Hershman, R. L.; and Kelly,R.
J.
(1978).
"Performance of a naturallanguage query system in a simulatedcommand control environment."
U.S.Naval Report NOSC ACCAT.Moody, Terence S. (1988).
The effects ofrestricted vocabulary size on voice interactivediscourse structure.
Doctoral dissertation,North Carolina State University, Raleigh,NC.Morik, Katharina (1989).
"User models andconversational settings: Modeling theuser's wants."
In User Models in DialogSystems, edited by Alfred Kobsa andWolfgang Wahlster, 364-385.Springer-Verlag.Mudler, J. and Paulus, E.
(1988).
"Expectation-based peech recognition.
"In Recent Advances in Speech Understandingand Dialog Systems, edited by H. Niemann,319Computational Linguistics Volume 21, Number 3M.
Lang, and G. Sagerer, 473-477.Springer-Verlag.Novick, David G. (1988).
Control ofmixed-initiative discourse throughmeta-locutionary cts: A computational model.Doctoral dissertation, University ofOregon.Paris, Cecile L. (1988).
"Tailoring objectdescriptions to a user's level of expertise.
"Computational Linguistics, 14(3), 64-78.Polanyi, Livia, and Scha, Remko J. H.
(1983).
"On the recursive structure of discourse.
"In Connectedness in Sentence, Discourse andText, edited by K. Ehlich and H. vanRiemsdijk, 141-178.
Tilburg University.Pollack, Martha E. (1986).
"A model of planinference that distinguishes between thebeliefs of factors and observers."
InProceedings, 24th Annual Meeting of theAssociation for Computational Linguistics,207-214.Reichman, Rachel (1985).
Getting ComputersTo Talk Like You and Me.
MIT Press.Rich, Elaine (1989).
"Stereotypes and UserModelling."
In User Models in DialogSystems, edited by Kobsa, Alfred,Wahlster, and Wolfgang.
Springer-Verlag.35-51.Seneff, Stephanie (1992).
"TINA: A naturallanguage system for spoken languageapplications."
Computational Linguistics,18(1), 61-86.Smith, Ronnie W. (1991).
A computationalmodel of expectation-driven mixed-initiativedialog processing.
Doctoral dissertation,Duke University, Durham, NC.Smith, Ronnie W., and Hipp, D. Richard(1994).
Spoken Natural Language DialogSystems: A Practical Approach.
OxfordUniversity Press.Walker, Donald E. (1978).
UnderstandingSpoken Language.
North-Holland.Walker, Marilyn, and Whittaker, Steve(1990).
"Mixed initiative in dialogue: Aninvestigation i to discoursesegmentation."
In Proceedings, 28th AnnualMeeting of the Association for ComputationalLinguistics, 70-78.Webber, Bonnie L., and Baldwin, Breck(1992).
"Accommodating context change.
"In Proceedings, 30th Annual Meeting of theAssociation for Computational Linguistics,96-103.Whittaker, Steve, and Stenton, Phil (1988).
"Cues and control in expert-clientdialogues."
In Proceedings, 26th AnnualMeeting of the Association for ComputationalLinguistics, 123-130.Young, Sheryl R. (1990).
"Use of dialogue,pragmatics and semantics to enhancespeech recognition."
SpeechCommunication, 9, 551-564.Young, Sheryl R.; Hauptmann, AlexanderG.
; Ward, Wayne H.; Smith, Edward T.;and Werner, Philip (1989).
"High levelknowledge sources in usable speechrecognition systems."
Communications ofthe ACM, 32(2), 183-194.Zue, Victor; Glass, James; Phillips, Michael;and Seneff, Stephanie (1989).
"The MITSUMMIT speech recognition system, aprogress report."
In Proceedings, DARPASpeech and Natural Language Workshop.Philadelphia, 21-23.320
