Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 101?106,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSemantic classifications for detection of verb metaphorsBeata Beigman Klebanov1and Chee Wee Leong1and Elkin Dario Gutierrez2and Ekaterina Shutova3and Michael Flor11Educational Testing Service2University of California, San Diego3University of Cambridge{bbeigmanklebanov,cleong,mflor}@ets.orgedg@icsi.berkeley.edu, ekaterina.shutova@cl.cam.ac.ukAbstractWe investigate the effectiveness of se-mantic generalizations/classifications forcapturing the regularities of the behaviorof verbs in terms of their metaphoric-ity.
Starting from orthographic wordunigrams, we experiment with variousways of defining semantic classes forverbs (grammatical, resource-based, dis-tributional) and measure the effectivenessof these classes for classifying all verbsin a running text as metaphor or nonmetaphor.1 IntroductionAccording to the Conceptual Metaphor theory(Lakoff and Johnson, 1980), metaphoricity is aproperty of concepts in a particular context of use,not of specific words.
The notion of a concept is afluid one, however.
While write and wrote wouldlikely constitute instances of the same concept ac-cording to any definition, it is less clear whethereat and gobble would.
Furthermore, the Con-ceptual Metaphor theory typically operates withwhole semantic domains that certainly generalizebeyond narrowly-conceived concepts; thus, saveand waste share a very general semantic feature ofapplying to finite resources ?
it is this meaning el-ement that accounts for the observation that theytend to be used metaphorically in similar contexts.In this paper, we investigate which kinds of gen-eralizations are the most effective for capturingregularities of metaphor usage.2 Related WorkMost previous supervised approaches to verbmetaphor classification evaluated their systems onselected examples or in small-scale experiments(Tsvetkov et al, 2014; Heintz et al, 2013; Tur-ney et al, 2011; Birke and Sarkar, 2007; Gediganet al, 2006), rather than using naturally occurringcontinuous text, as done here.
Beigman Klebanovet al (2014) and Beigman Klebanov et al (2015)are the exceptions, used as a baseline in the currentpaper.Features that have been used so far in super-vised metaphor classification address concretenessand abstractness, topic models, orthographic uni-grams, sensorial features, semantic classificationsusing WordNet, among others (Beigman Klebanovet al, 2015; Tekiroglu et al, 2015; Tsvetkov et al,2014; Dunn, 2014; Heintz et al, 2013; Turney etal., 2011).
Of the feature sets presented in this pa-per, all but WordNet features are novel.3 Semantic ClassificationsIn the following subsections, we describe the dif-ferent types of semantic classifications; Table 1summarizes the feature sets.Name Description #FeaturesU orthographic unigram variesUL lemma unigram variesVN-Raw VN frames 270VN-Pred VN predicate 145VN-Role VN thematic role 30VN-RoRe VN them.
role filler 128WordNet WN lexicographer files 15Corpus distributional clustering 150Table 1: Summary of feature sets.
All features arebinary features indicating class membership.3.1 Grammar-basedThe most minimal level of semantic generalizationis that of putting together verbs that share the samelemma (lemma unigrams, UL).
We use NLTK(Bird et al, 2009) for identifying verb lemmas.1013.2 Resource-basedVerbNet: The VerbNet database (Kipper et al,2006) provides a classification of verbs accord-ing to their participation in frames ?
syntactic pat-terns with semantic components, based on Levin?sclasses (Levin, 1993).
Each verb class is anno-tated with its member verb lemmas, syntactic con-structions in which these participate (such as tran-sitive, intransitive, diathesis alternations), seman-tic predicates expressed by the verbs in the class(such as motion or contact), thematic roles (suchas agent, patient, instrument), and restrictions onthe fillers of these semantic roles (such as pointedinstrument).VerbNet can thus be thought of as providing anumber of different classifications over the sameset of nearly 4,000 English verb lemmas.
Themain classification is based on syntactic frames, asenacted in VerbNet classes.
We will refer to themas VN-Raw classes.
An alternative classificationis based on the predicative meaning of the verbs;for example, the verbs assemble and introduce arein different classes based on their syntactic beha-vior, but both have the meaning component of to-gether, marked in VerbNet as a possible value ofthe Predicate variable.
Similarly, shiver and faintbelong to different VerbNet classes in terms ofsyntactic behavior, but both have the meaning el-ement of describing an involuntary action.
Usingthe different values of the Predicate variable, wecreated a set of VN-Pred classes.
We note that thesame verb lemma can occur in multiple classes,since different senses of the same lemma can havedifferent meanings, and even a single sense canexpress more than one predicate.
For example, theverb stew participates in the following classes ofvarious degrees of granularity: cause (shared with2,912 other verbs), use (with 700 other verbs), ap-ply heat (with 49 other verbs), cooked (with 49other verbs).Each VerbNet class is marked with the thematicroles its members take, such as agent or benefi-ciary.
Here again, verbs that differ in syntacticbehavior and in the predicate they express couldshare thematic roles.
For example, stew and prickbelong to different VerbNet classes and share onlythe most general predicative meanings of causeand use, yet both share a thematic role of instru-ment.
We create a class for each thematic role(VN-Role).Finally, VerbNet provides annotations of the re-strictions that apply to fillers of various thematicroles.
For example, verbs that have a thematicrole of instrument can have the filler restrictedto being inanimate, body part, concrete, pointy,solid, and others.
Across the various VerbNetclasses, there are 128 restricted roles (such as in-strument pointy).
We used those to generate VN-RoRe classes.WordNet: We use lexicographer files to clas-sify verbs into 15 classes based on their generalmeaning, such as verbs of communication, con-sumption, weather, and so on.3.3 Corpus-basedWe also experimented with automatically-generated verb clusters as semantic classes.
Weclustered VerbNet verbs using a spectral cluster-ing algorithm and lexico-syntactic features.
Weselected the verbs that occur more than 150 timesin the British National Corpus, 1,610 in total, andclustered them into 150 clusters (Corpus).We used verb subcategorization frames (SCF)and the verb?s nominal arguments as features forclustering, as they have proved successful in pre-vious verb classification experiments (Shutova etal., 2010).
We extracted our features from the Gi-gaword corpus (Graff et al, 2003) using the SCFclassification system of Preiss et al (2007) to iden-tify verb SCFs and the RASP parser (Briscoe et al,2006) to extract the verb?s nominal arguments.Spectral clustering partitions the data relyingon a similarity matrix that records similarities be-tween all pairs of data points.
We use Jensen-Shannon divergence (dJS) to measure similaritybetween feature vectors for two verbs, viand vj,and construct a similarity matrix Sij:Sij= exp(?dJS(vi, vj)) (1)The matrix S encodes a similarity graph G overour verbs.
The clustering problem can then be de-fined as identifying the optimal partition, or cut, ofthe graph into clusters.
We use the multiway nor-malized cut (MNCut) algorithm of Meila and Shi(2001) for this purpose.
The algorithm transformsS into a stochastic matrix P containing transitionprobabilities between the vertices in the graph asP = D?1S, where the degree matrix D is a dia-gonal matrix with Dii=?Nj=1Sij.
It then com-putes the K leading eigenvectors of P , where K isthe desired number of clusters.
The graph is par-titioned by finding approximately equal elements102in the eigenvectors using a simpler clustering al-gorithm, such as k-means.
Meila and Shi (2001)have shown that the partition I derived in this wayminimizes the MNCut criterion:MNCut(I) =K?k=1[1?
P (Ik?
Ik|Ik)], (2)which is the sum of transition probabilities acrossdifferent clusters.
Since k-means starts from a ran-dom cluster assignment, we ran the algorithm mul-tiple times and used the partition that minimizesthe cluster distortion, that is, distances to clustercentroid.We tried expanding the coverage of VerbNetverbs and the number of clusters using grid searchon the training data, with coverage grid ={2,500;3,000; 4,000} and #clusters grid = {200; 250; 300;350; 400}, but obtained no improvement in perfor-mance over our original setting.4 Experiment setup4.1 DataWe use the VU Amsterdam Metaphor Corpus(Steen et al, 2010).1The corpus contains anno-tations of all tokens in running text as metaphor ornon metaphor, according to a protocol similar toMIP (Pragglejaz, 2007).
The data come from theBNC, across 4 genres: news (N), academic writing(A), fiction (F), and conversation (C).
We addresseach genre separately.
We consider all verbs apartfrom have, be, and do.We use the same training and testing partitionsas Beigman Klebanov et al (2015).
Table 2 sum-marizes the data.2Data Training Testing#T #I %M #T #INews 49 3,513 42% 14 1,230Fict.
11 4,651 25% 3 1,386Acad.
12 4,905 31% 6 1,260Conv.
18 4,181 15% 4 2,002Table 2: Summary of the data.
#T = # of texts; #I= # of instances; %M = percentage of metaphors.4.2 Machine Learning MethodsOur setting is that of supervised machine learn-ing for binary classification.
We experimentedwith a number of classifiers using VU-News train-ing data, including those used in relevant priorwork: Logistic Regression (Beigman Klebanov et1available at http://metaphorlab.org/metcor/search/2Data and features will be made available athttps://github.com/EducationalTestingService/metaphor.al., 2015), Random Forest (Tsvetkov et al, 2014),Linear Support Vector Classifier.
We found thatLogistic Regression was better for unigram fea-tures, Random Forest was better for features usingWordNet and VerbNet classifications, whereas thecorpus-based features yielded similar performanceacross classifiers.
We therefore ran all evaluationswith both Logistic Regression and Random For-est classifiers.
We use the skll and scikit-learntoolkits (Blanchard et al, 2013; Pedregosa et al,2011).
During training, each class is weighted ininverse proportion to its frequency.
The optimiza-tion function is F1 (metaphor).5 ResultsWe first consider the performance of each type ofsemantic classification separately as well as var-ious combinations using cross-validation on thetraining set.
Table 3 shows the results with theclassifier that yields the best performance for thegiven feature set.Name N F A C Av.U .64 .51 .55 .39 .52UL .65 .51 .61 .39 .54VN-Raw .64 .49 .60 .38 .53VN-Pred .62 .47 .58 .39 .52VN-Role .61 .46 .55 .40 .50VN-RoRe .59 .47 .54 .36 .49WN .64 .50 .60 .38 .53Corpus .59 .49 .53 .36 .49VN-RawToCorpus .63 .49 .59 .38 .53UL+WN .67 .52 .63 .40 .56UL+Corpus .66 .53 .62 .39 .55Table 3: Performance (F1) of each of the featuresets, xval on training data.
U = unigram baseline.Of all types of semantic classification, only thegrammatical one (lemma unigrams, UL) showsan overall improvement over the unigram base-line with no detriment for any of the genres.VN-Raw and WordNet show improved performancefor Academic but lower performance on Fictionthan the unigram baseline.
Other versions ofVerbNet-based semantic classifications are gener-ally worse than VN-Raw, with some exceptionsfor the Conversation genre.
Distributional clus-ters (Corpus) generally perform worse than theresource-based classifications, even when the re-source is restricted to the exact same set of verbs asthat covered in the Corpus clusters (compare Cor-pus to VN-RawToCorpus).The distributional features are, however, aboutas effective as WordNet features when combined103with the lemma unigrams (UL); the combinationsimprove the performance over UL alone for everygenre.
We also note that the better performancefor these combinations is generally attained by theLogistic Regression classifier.
We experimentedwith additional combinations of feature sets, butobserved no further improvements.To assess the consistency of metaphoricitybehavior of semantic classes across genres, wecalculated correlations between the weights as-signed by the UL+WN model to the 15 WordNetfeatures.
All pairwise correlations between News,Academic, and Fiction were strong (r > 0.7),while Conversation had low to negative correlationwith other genres.
The low correlations with Con-versation was largely due to a highly discrepantbehavior of verbs of weather3?
these are con-sistently used metaphorically in all genres apartfrom Conversation.
This discrepancy, however, isnot so much due to genre-specific differences inbehavior of the same verbs as to the differencein the identity of the weather verbs that occur inthe data from the different genres.
While burn,pour, reflect, fall are common in the other genres,the most common weather verb in Conversation israin, and none of its occurrences is metaphoric; itssingle occurrence in the other genres is likewisenot metaphoric.
More than a difference acrossgenres, this case underscores the complementarityof lemma-based and semantic class-based infor-mation ?
it is possible for weather verbs to tendtowards metaphoricity as a class, yet some verbsmight not share the tendency ?
verb-specific infor-mation can help correct the class-based pattern.5.1 Blind Test BenchmarkTo compare the results against state-of-art, weshow the performance of Beigman Klebanov etal.
(2015) system (SOA?15) on the test data (seeTable 2 for the sizes of the test sets per genre).Their system uses Logistic Regression classifierand a set of features that includes orthographicunigrams, part of speech tags, concreteness, anddifference in concreteness between the verb and itsdirect object.
Against this benchmark, we evaluatethe performance of the best combination identifiedduring the cross-validation runs, namely, UL+WNfeature set using Logistic Regression classifier.We also show the performance of the resource-3Removing verbs of weather propelled the correlationswith Conversation to a moderate range, r = 0.25-0.45 acrossgenres.lean model, UL+Corpus.
The top three rows ofTable 4 show the results.
The UL+WN model out-performs the state of art for every genre; the im-provement is statistically significant ( p<0.05).4The improvement of UL+Corpus over SOA?15 isnot significant.Following the observation of the similarity be-tween weights of semantic class features acrossgenres, we also trained the three systems on all theavailable training data across all genres (all data inthe Train column in Table 2), and tested on testdata for the specific genre.
This resulted in perfor-mance improvements for all systems in all genres,including Conversation (see the bottom 3 rows inTable 4).
The significance of the improvement ofUL+WN over SOA?15 was preserved; UL+Corpusnow significantly outperformed SOA?15.Feature Set N F A C Av.Train SOA?15 .64 .47 .71 .43 .56in UL+WN .68 .49 .72 .44 .58genre UL+Corpus .65 .49 .71 .43 .57Train SOA?15 .66 .48 .74 .44 .58on all UL+WN .69 .50 .77 .45 .60genres UL+Corpus .67 .51 .76 .45 .60Table 4: Benchmark performance, F1 score.6 ConclusionThe goal of this paper was to investigatethe effectiveness of semantic generaliza-tions/classifications for metaphoricity classi-fication of verbs.
We found that generalizationfrom orthographic unigrams to lemmas is effec-tive.
Further, lemma unigrams and semantic classfeatures based on WordNet combine effectively,producing a significant improvement over thestate of the art.
We observed that semantic classfeatures were weighted largely consistently acrossgenres; adding training data from other genres ishelpful.
Finally, we found that a resource-leanmodel where lemma unigram features werecombined with clusters generated automaticallyusing a large corpus yielded a competitive perfor-mance.
This latter result is encouraging, as theknowledge-lean system is relatively easy to adaptto a new domain or language.4We used McNemar?s test of significance of differencebetween correlated proportions (McNemar, 1947), 2-tailed.We combined data from all genres into on a 2X2 matrix:both SOA?15 and UL+WN correct in (1,1), both wrong(0,0), SOA?15 correct UL+WN wrong (0,1), UL+WN correctSOA?15 wrong (1,0)).104AcknowledgmentWe are grateful to the ACL reviewers for theirhelpful feedback.
Ekaterina Shutova?s research issupported by the Leverhulme Trust Early CareerFellowship.ReferencesBeata Beigman Klebanov, Chee Wee Leong, MichaelHeilman, and Michael Flor.
2014.
Different texts,same metaphors: Unigrams and beyond.
In Pro-ceedings of the Second Workshop on Metaphor inNLP, pages 11?17, Baltimore, MD, June.
Associa-tion for Computational Linguistics.Beata Beigman Klebanov, Chee Wee Leong, andMichael Flor.
2015.
Supervised word-levelmetaphor detection: Experiments with concretenessand reweighting of examples.
In Proceedings of theThird Workshop on Metaphor in NLP, pages 11?20,Denver, Colorado, June.
Association for Computa-tional Linguistics.Steven Bird, Edward Loper, and Ewan Klein.
2009.Natural Language Processing with Python.
OReillyMedia Inc.Julia Birke and Anoop Sarkar.
2007.
Active learn-ing for the identification of nonliteral language.In Proceedings of the Workshop on ComputationalApproaches to Figurative Language, pages 21?28,Rochester, New York.Daniel Blanchard, Michael Heilman,and Nitin Madnani.
2013.
SciKit-Learn Laboratory.
GitHub repository,https://github.com/EducationalTestingService/skll.E.
Briscoe, J. Carroll, and R. Watson.
2006.
The sec-ond release of the rasp system.
In Proceedings of theCOLING/ACL on Interactive presentation sessions,pages 77?80.Jonathan Dunn.
2014.
Multi-dimensional abstractnessin cross-domain mappings.
In Proceedings of theSecond Workshop on Metaphor in NLP, pages 27?32, Baltimore, MD, June.
Association for Computa-tional Linguistics.M.
Gedigan, J. Bryant, S. Narayanan, and B. Ciric.2006.
Catching metaphors.
In PProceedings of the3rd Workshop on Scalable Natural Language Un-derstanding, pages 41?48, New York.D.
Graff, J. Kong, K. Chen, and K. Maeda.
2003.English Gigaword.
Linguistic Data Consortium,Philadelphia.Ilana Heintz, Ryan Gabbard, Mahesh Srivastava, DaveBarner, Donald Black, Majorie Friedman, and RalphWeischedel.
2013.
Automatic Extraction of Lin-guistic Metaphors with LDA Topic Modeling.
InProceedings of the First Workshop on Metaphor inNLP, pages 58?66, Atlanta, Georgia, June.
Associa-tion for Computational Linguistics.Karin Kipper, Anna Korhonen, Neville Ryant, andMartha Palmer.
2006.
Extensive classifications ofenglish verbs.
In Proceedings of the 12th EURALEXInternational Congress, Turin, Italy, September.George Lakoff and Mark Johnson.
1980.
Metaphorswe live by.
University of Chicago Press, Chicago.Beth Levin.
1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
Chicago, IL:University of Chicago Press.Quinn McNemar.
1947.
Note on the sampling errorof the difference between correlated proportions orpercentages.
Psychometrika, 12(2).M.
Meila and J. Shi.
2001.
A random walks view ofspectral segmentation.
In Proceedings of AISTATS.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine Learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Group Pragglejaz.
2007.
MIP: A Method for Iden-tifying Metaphorically Used Words in Discourse.Metaphor and Symbol, 22(1):1?39.Judita Preiss, Ted Briscoe, and Anna Korhonen.
2007.A system for large-scale acquisition of verbal, nom-inal and adjectival subcategorization frames fromcorpora.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 912?919, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Ekaterina Shutova, Lin Sun, and Anna Korhonen.2010.
Metaphor identification using verb and nounclustering.
In Proceedings of the 23rd InternationalConference on Computational Linguistics (COL-ING), pages 1002?1010.Gerard Steen, Aletta Dorst, Berenike Herrmann, AnnaKaal, Tina Krennmayr, and Trijntje Pasma.
2010.
AMethod for Linguistic Metaphor Identification.
Am-sterdam: John Benjamins.Serra Sinem Tekiroglu, G?ozde?Ozbal, and Carlo Strap-parava.
2015.
Exploring sensorial features formetaphor identification.
In Proceedings of the ThirdWorkshop on Metaphor in NLP, pages 31?39, Den-ver, Colorado, June.
Association for ComputationalLinguistics.Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman,Eric Nyberg, and Chris Dyer.
2014.
Metaphor de-tection with cross-lingual model transfer.
In Pro-ceedings of the 52nd Annual Meeting of the Asso-ciation for Computational Linguistics (Volume 1:Long Papers), pages 248?258, Baltimore, Maryland,June.
Association for Computational Linguistics.105Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-hen.
2011.
Literal and metaphorical sense identi-fication through concrete and abstract context.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 680?690, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.106
