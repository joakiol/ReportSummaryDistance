Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 873?882, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsLearning Constraints for Consistent Timeline ExtractionDavid McClosky and Christopher D. ManningNatural Language Processing GroupComputer Science DepartmentStanford University, Stanford, CA, USA{mcclosky,manning}@stanford.eduAbstractWe present a distantly supervised system forextracting the temporal bounds of fluents (re-lations which only hold during certain times,such as attends school).
Unlike previouspipelined approaches, our model does not as-sume independence between each fluent oreven between named entities with known con-nections (parent, spouse, employer, etc.).
In-stead, we model what makes timelines of flu-ents consistent by learning cross-fluent con-straints, potentially spanning entities as well.For example, our model learns that someoneis unlikely to start a job at age two or to marrysomeone who hasn?t been born yet.
Our sys-tem achieves a 36% error reduction over apipelined baseline.1 IntroductionMany information extraction (IE) systems tradition-ally extracted just relations, but a great many realworld relations such as attends school or has spousevary over time.
To capture this, some recent IEsystems have extended their focus from relations tofluents (relations combined with temporal bounds).This can be seen in the temporal slot filling track inthe TAC-KBP 2011 shared task (Ji et al2011).
Adirect application of this work is the automatic im-provement of online resources such as Freebase andWikipedia infoboxes.
Indirect applications includequestion answering systems.Fluents can be grouped together to form time-lines (see Figure 1 for an example) and provide eas-ily capturable consistency constraints.
Our goal isFigure 1: A timeline of two named entities.
Each timespan represents a fluent (a relation with temporal bounds).Temporal bounds are denoted by spans on the timeline.Fluents can create links between entities (e.g., marriage).to learn these constraints and use them to producemore accurate timelines of significant events forpeople and organizations.
For example, it is com-mon knowledge that someone cannot attend a schoolif they haven?t been born yet.
Constraints on con-sistent timelines do not need to be hard constraints,though: it is rare, although possible, to become theCEO of a company at the age of 21.Despite the rich constraints on valid timelines,there is relatively little work on exploiting these con-straints for mutual disambiguation.
Many existingsystems extract different parts of a timeline sepa-rately and use heuristics to combine them.
Theseheuristics tend to optimize only local consistency(within a single fluent) but ignore more global con-straints across fluents (e.g., attending a school be-fore being born) or across fluents of two linkedentities (e.g., attending a school before the schoolwas founded).
In this work, we explore using jointinference to enforce these constraints.
We showthat these techniques can yield substantial improve-ments.
Additionally, our general approach is notspecific to extracting temporal boundaries of fluents.It could easily be applied to other IE systems which873employ independent extractions followed by heuris-tics to improve consistency.2 The timelining taskAs a basis for our task, we first describe the Tempo-ral KBP task (Ji et al2011).
As input, one is givena list of queries, a database of example fluents, andsource documents.
Queries are named entities (peo-ple or organizations) with their gold relations but notemporal bounds.
The database consists of trainingentities with their fluents, including known tempo-ral bounds for each fluent.
Example fluents can beseen in Table 1.
Note that the database may be in-complete.
In addition to missing fluents for an en-tity, some temporal bounds may be missing fromthe database; missing bounds are unfortunately in-distinguishable from unbounded ranges.
As a result,we can only trust concrete temporal boundaries inthe database.
Source documents consist of raw textfrom news, blogs, and Wikipedia articles.
For eachfluent, systems must output their predicted temporalbounds, along with references to source documentsto provide provenance.Our task is a variation of the Temporal KBP task.In our case, the database is a collection of Freebase1entities and their fluents, merged with Wikipedia in-foboxes.
Each entity has a unique ID, allowing usto avoid some coreference issues (though there canstill be issues in document retrieval).
In TemporalKBP, the temporal representation allows for upperand lower bounds on both the event start and end:?sl, su, el, eu?
where sl ?
start ?
su, el ?
end ?eu.
However, it is difficult to obtain these boundswithout manual annotation.
As a result, we opted forthe simpler representation which can be easily foundin databases like Freebase.
Our temporal represen-tation is limited to bounds of the form ?start, end?where either can be unbounded or unknown (bothrepresented as ??
).Our set of fluents is closely related to those inthe Temporal KBP task.
Our goal was to useas much temporal information as possible, withthe hope of each fluent providing additional poten-tial constraints.
While we omit the resides in andmember of fluents,2 we add several others.
For1http://freebase.org2This is because these fluents are rarely present in Freebasepeople and organizations, we add a special fluent,lifespan, which doesn?t take a slot value.3 A list offluents we use are listed in Table 3.3 ModelTo operate on a set of queries, we first collect can-didate temporal expression mentions for each fluentfrom our source documents.
This limits us to us-ing temporal expression mentions which appear nearfluent mentions in text.
It also ensures that we canprovide provenance for each temporal boundary as-sertion.
This process is described in ?3.1.Our model contains two components, both ofwhich assign probabilities to timelines.
The clas-sifier component determines how each candidatetemporal expression mention connects to its fluent(?3.2).
For example, the mention may indicate theSTART of the fluent, the END, both its START ANDEND (for instantaneous events), or be UNRELATED.These connections involve relations between tempo-ral expression mentions and relations and we refer tothem as metarelations.4 For features, the classifieruses the surrounding textual and syntactic context oftemporal expression and fluent mentions.
Each clas-sification decision is made independently, allowingfor inconsistency at multiple levels (within a fluent,across fluents, or across entities).
However, usingjoint inference, the classifier component can deter-mine the best overall span for each fluent.The consistency component learns what makestimelines consistent (?3.3).
It is similar in nature toa language model for timelines instead of sentences.Given a candidate timeline, the consistency compo-nent estimates its probability of occurring.
This isdone by decomposing timelines into a series of ques-tions (such as ?did the entity go to school beforestarting a job??)
and learning the probabilities ofdifferent answers from training data.Unlike the classifier component, the consistencycomponent is blind to the underlying text in thesource documents.
The two components work to-gether to find a global timeline that is both based ontextual evidence and coherent across entities usingwith temporal bounds.3Note that this is a relation in the non-temporal KBP task.4Other metarelations are possible under more complex tem-poral representations.
For example, Artiles et al2011) usesthe HOLDS metarelation.874Entity Relation Slot value Temporal boundsJon Stewart lifespan ?
[1962-11-28, +?
)/en/jon stewartJon Stewart has parent Donald Leibowitz [1962-11-28, +?
)/en/jon stewart /en/donald leibowitzJon Stewartattends school College of William and Mary (?
?, 1984]/en/jon stewart /en/college of william and maryJon Stewart has spouse Tracey McShane [2000-11, +?
)/en/jon stewart /en/tracy mcshaneTable 1: Example relations with their temporal bounds.
Freebase IDs are shown in monospace.
Note that temporalbounds differ in their resolution (some are days of the year, others are only years).
Some bounds are unknown (e.g.,the start of the attends school fluent) and indistinguishable from unbounded.
The lifespan fluent is a unary relation.joint inference (note that they are trained indepen-dently).
The inference process is described in ?3.4.3.1 Temporal expression retrievalGiven a fluent, we search for all textual mentionsof the fluent and collect nearby temporal expressionmentions.
These temporal expressions are used ascandidate boundaries for the fluent in later steps.The search process assumes that if a fluent?s entityand slot value co-occur in a sentence,5 that sentenceis typically a positive example of the fluent.6 Thisis sometimes known as distant supervision (Cravenand Kumlien, 1999; Mintz et al2009).
We usethe Stanford Core NLP suite (Toutanova et al2003;Finkel et al2005; Klein and Manning, 2003; Lee etal., 2011) to annotate each document with POS andNER tags, parse trees, and coreference chains.
Ontop of this, we apply a rule-based temporal expres-sion extractor (Chang and Manning, 2012).
Sincewe have coreference links, we also search docu-ments for anything coreferent with the fluent?s en-tity.The temporal expression extractor handles moststandard date and time formats.
For each document,one can provide an optional reference time.
Forunderspecified dates, the reference time is used to5While we limit our scope to sentences in this work, it istrivial to extend this to larger regions such as paragraphs.6The lifespan fluent requires special handling.
Ideally, itscandidates would be provided by a relation extraction mentiondetector (e.g., a KBP system).
For this work, we use the goldlifespan bounds as slot values for the purpose of document re-trieval.
While this does heavily bias the system towards usinggold bounds, the system still must predict the correct associa-tions (START, END, etc.)
making the lifespan fluent non-trivial.resolve these dates to full expressions if possible.Some of our documents are news articles, where weuse the publication date as the reference time.
Otherdocuments, e.g., Wikipedia articles, are undated andwe typically omit a reference time for these.
We ex-clude dates which are not uniquely resolvable (e.g.,?September 15th,?
when the reference date is un-known) since our task requires us to output unam-biguous dates.We create training datums by computing themetarelation between each temporal expression andits gold fluent.
For example, for the temporalexpression mention ?September 15th, 1981?
andgold lifespan relation that spans [1981-09-15,+?
), we would assign the START metarelation.
Asa heuristic, we allow for underspecified matches.Thus, both ?1981?
and ?September 1981?
wouldhave the START metarelation but ?September 2nd,1981?
would be assigned UNRELATED.3.2 Classifier componentWe use a classifier to determine the nature of thelink between fluents and candidate temporal expres-sion mentions.
Our classifier (a standard multi-class maximum entropy classifier) learns a functionC : (t, f) ?
M where t is a temporal expressionmention, f = ?entity, relation name, slot value?
isa fluent from the database, and M is the set of thefour possible metarelations.Features for the classifier include many of thosein Artiles et al2011).
These include standard re-lation extraction features such as the dependencypaths between the temporal expression and the en-tity or slot value.
We use both the original depen-875dency paths and their collapsed Stanford Dependen-cies forms (de Marneffe and Manning, 2008).
Weinclude the lengths of each path and, if the path isshorter than four edges, the grammatical relations,words, POS tags, and NER labels along the path.We extract the same sorts of features from surfacepaths (i.e., the words and tags between the entity andthe temporal expression) if the path is five tokens orshorter.
For temporal expressions, we include theircentury and decade as features.
These features act asa crude prior over when valid temporal expressionsoccur.
There are also features for the precision ofthe temporal expression (year only, has month, andhas day).
Lastly, we include the relation name itselfas a feature.Previous work (Artiles et al2011) heuristicallyaggregates the hard decisions from their classifier tocreate a locally consistent span.
The basic aggre-gation model (described in ?4.2) is similar to theirmethod.
In contrast, our method uses the likeli-hood of complete spans to ensure both boundariesare consistent with the text.To calculate the likelihood of a specific temporalspan for a fluent f , we represent the span as aseries of metarelations and take the product of theirprobabilities.
For example, if the candidate span is[1981-09-15, +?)
and we have two temporalexpressions, ?September 15th, 1981?
and ?2012?
:P(span(f) = [1981-09-15, +?)
| f)=P(C(?September 15th, 1981?, f) = START)?P(C(?2012?, f) = UNRELATED)This can easily be extended to calculating the jointprobability of an entire timeline, represented as a listof ?fluent , span?
pairs:PCC(?f1, s1?, .
.
.
)=?iP(span(fi) = si | fi)We refer to this model as the Combined Classifier(CC) since it uses the probabilities of all timelinesboundaries rather than aggregating hard local deci-sions.3.3 Consistency componentWhile distant supervision can be used to create im-plicit negative examples for the classifier component(time expressions marked as UNRELATED), we donot have an equivalent technique to reliably createnegative examples for the consistency component(examples of inconsistent timelines).
Instead, weonly have positive examples of consistent timelinesfrom the database.
As a result, we must treat predict-ing consistency as a density estimation rather than aclassification problem.Our consistency component is designed to be asgeneral as possible ?
it does not even include basicconstraints about timelines such as ?starts are beforeends.?
Instead, we provide several simple templatesfor temporal constraints to allow it to learn these ba-sic tendencies as well as more complex ones.
Ex-amples include whether one typically goes to schoolfirst or starts their first job, how many jobs peopletypically have at one time, or if it is possible to marrysomeone who hasn?t been born yet.We achieve this by decomposing timelinesinto a series of probabilistic events, or ques-tions.
As an example, one question aboutthe timeline shown in Table 1 is whether JonStewart graduated from the College of Williamand Mary BEFORE marrying Tracey McShane,i.e., end(attends school) < start(has spouse).
In thiscase, the answer is ?yes.?
More generally, wecan apply the BEFORE template to all bound-aries of all fluents: boundary1(fluent1) <boundary2(fluent2).
We use templates like these(denoted by SMALL CAPS) to generate all possiblequestions to ask about a specific entity.Other questions can be asked at the fluent levelrather than the boundary level (Allen, 1983).
Oneset of fluent level questions asks whether two flu-ents?
spans OVERLAP.
For example, in Table 1, JonStewart?s lifespan OVERLAPs with the span of hishas spouse fluent.
Other sets of fluent level ques-tions ask whether the span of a fluent completelyCONTAINS the span of another one, whether a flu-ent is COMPLETELY BEFORE another fluent, andwhether two fluents TOUCH (the start of one fluentis the same as the end of another).Since all of these questions involve ordering butignore the actual differences in time, we create onemore set of questions asking whether two bound-aries are WITHIN a certain number of years:|boundary1(fluent1)?
boundary2(fluent2)| ?
K876for K ?
{1, 2, 4, 8, 16}.
The aim is to approxi-mate the typical lengths of a single fluent or amountof time between boundaries from different fluents.There is nothing which requires that the flu-ents in question come from a single entity.
Thus,we can trivially ask questions about two entitieswhich are linked by a fluent.
For example, sinceJon Stewart is linked to Tracey McShane by thehas spouse fluent (Table 1), we could ask the ques-tion of whether Jon Stewart?s lifespan OVERLAPSTracey McShane?s lifespan.
We can ask any typeof question about two linked entities and distinguishthe questions by prefixing them with the nature ofthe link (has spouse in this case).Note that not all questions can be answered sincethey may rely on comparing unknown values.
Thisis because (for our setup) infinite values are indistin-guishable from unknown values.
For example, thestart of the Jon Stewart?s attends school fluent is un-defined in the database, but clearly not actually ?
?.Thus, we add a third possible answer to each ques-tion: unknown.
The answers to boundary level ques-tions are defined only if both boundaries are finite.Fluent level questions have known answers as longas both fluents have at least one finite value.To train our model, we gather the answers to ques-tions over all the fluents from training entities.
Eachquestion forms a multinomial over the three possiblevalues (yes, no, unknown).
To determine the proba-bility of a complete timeline:Pconsistency(timeline) =?(q,a)?Q(timeline){(1?
c)P?
(a | q) q is oldc q is newwhere Q(?)
generates all possible?question, answer?
pairs which are consistentwith the fluents in the timeline, ?
is a vector of themodel parameters, and c is a smoothing parameter(described below).To learn the model parameters, we start by us-ing maximum-likelihood estimation for these multi-nomials from training entities.
However, somesmoothing is required since new entities may con-tain previously unseen answers to existing ques-tions.
To address this, we apply add-?
smoothingto each multinomial, P?
(a | q).
Additionally, it ispossible to see entirely new questions when we seea new combination of fluent types.
We reserve anamount of probability mass for new questions, c. cand ?
are estimated in turn by picking the value thatmaximizes the likelihood of the timeline made bythe development entities.To adjust the weight of the consistency compo-nent relative to the classifier component, we takethe geometric mean of the likelihood using the to-tal number of questions, |Q(t)|, as the exponent andraise the resulting mean to an exponent, ?.
This isnecessary since the two components essentially op-erate on different scales.
The Joint Classifier withConsistency (JCC) model calculates the score of atimeline, t, according to both components:scoreJCC (t) = PCC (t)[Pconsistency(t)?|Q(t)|]3.4 InferenceInference for the CC model is relatively simple:Simply pick the most likely span for each fluent.Since it assumes all fluents are independent, thebounds for each fluent can be inferred separately.To perform inference on a specific fluent, we con-sider all of its possible temporal spans, limited bythe temporal expression mentions found by the re-trieval system (?3.1).
Each possible span assigns oneof the four metarelations to each candidate temporalexpression for the fluent.
For example, if we foundonly the temporal expression mention ?1981?
for aspecific fluent, there are four possible spans:UNRELATED: (?
?, +?
)START: [1981-01-01, +?
)END: (?
?, 1981-12-31]START AND END: [1981-01-01, 1981-12-31]Note that when we assign ?1981?
as a start, weuse the earliest possible time (January 1st) whilewhen we assign it as an end, we use the latest pos-sible time (December 31st).
Of course, we typi-cally have multiple candidate temporal expressionsand thus potentially many more than four possiblespans.
All temporal expression mentions that re-solve to the same time are grouped together, since itwouldn?t make sense to assign ?August 28th, 2010?one metarelation and a different one to ?8/28/2010.
?Joint inference for the JCC model is a little moreinvolved since the consistency model does not as-sume independence across fluents.
Thus, we need877to apply techniques like Gibbs sampling or random-restart hillclimbing (RRHC) to determine the opti-mal temporal spans for each fluent.
For our task,the two methods obtain similar performance whileRRHC requires many fewer iterations so our discus-sion focuses on the latter.
RRHC involves loopingover all fluents in our testing entities, shuffling theorder of the fluents at the beginning of each pass.We maintain a working timeline, t, with our currentguesses of the spans for each fluent.
For each fluentand span ?f, s?
?
t, we pick the optimal span for f :s?
= argmaxs?
?S(f)scoreJCC (ts?
)where S(f) determines all possible temporalspans for the fluent f and ts?
= (t?
?f, s??)?
?f, s?is a copy of t where s?
is the span for f insteadof s. After selecting s?, we add it to our timeline:tnew = (t ?
?f, s??)
?
?f, s?.
Rather than calculat-ing the score of the full timeline, we can save time byusing only the relevant fluents in ts?
.
For example,if our fluent is the has spouse fluent for Jon Stew-art, we include all the fluents involving Jon Stewartand any relevant linked entities.
In this case, we alsoinclude all the fluents for Tracey McShane.Each round of RRHC consists of two passesthrough the fluents we are inferring: An argmaxpass followed by a randomization pass where werandomly choose spans for a random fraction of thefluents.
When finished, we return the highest scor-ing timeline seen during either of these passes.4 ExperimentsWe evaluate our models (CC and JCC) according totheir ability to predict the temporal bounds of flu-ents from Freebase.
This is similar to the DiagnosticTrack in the Temporal KBP task, where gold rela-tions are provided as inputs.
We provide three base-lines for comparison, discussed further in ?4.2.
Toform our database, we scraped a random sample ofpeople and organization entities from Freebase us-ing their API.
Since our consistency model has lim-ited effect if entities do not have any links to otherentities, we restrict our attention to entities linkedto at least one other entity ?
this eliminates a largeportion of possible entities.
Our corpus7 consists of8,450 entities for training, 1,072 for development,and 1,067 for test.
Entities have approximately 2.0fluents on average.From experiments on the development set, we setthe relative strength of the consistency component?
= 10.
For the JCC model, we perform three runsfor each experiment with different random seeds.Each experiment performs 10 rounds of RRHC,8 ini-tializing from an empty timeline.4.1 Evaluation metricOur evaluation metric is adapted from the TemporalKBP metric (Ji et al2011) to work with 2-tuplesfor temporal representations rather than the 4-tuplesin Temporal KBP.
The metric favors tighter boundson fluents while giving partial credit.
All dates needto be given at day resolution.
Thus, for gold fluentswith only year- or month-level resolution, we treatthem as their earliest (for starts) or latest (for ends)possible day.
To score a boundary, we take the dif-ference between the predicted and gold values: Ifthey?re both unbounded (??
), the boundary?s scoreis 1.
If only one is unbounded, the score is 0.
Ifboth are finite, the score is 1/(1 + |d|) where d isthe difference between the values in years.
To scorea fluent, we average the scores of its start and endboundaries.
In rare cases, we have multiple spansfor the same relation (e.g., Elizabeth Taylor marriedRichard Burton twice).
In these cases, we give sys-tems the benefit of the doubt and greedily align flu-ents in such a way as to maximize the metric.
Thetotal metric computes the score of each fluent di-vided by the number of fluents.
The official metricincludes precision and recall components, but sinceour setup provides gold relations, our precision andrecall are be equal.
This allows us to report a singlenumber.4.2 Baselines and oracleThe simplest baseline is the null baseline, proposedin Surdeanu et al2011).
This baseline assumes thatall fluents are unbounded in their spans.
The purpose7http://nlp.stanford.edu/?mcclosky/data/freebase-temporal-relations.tar.gz8There was no significant difference in accuracy betweenrunning 10 and 200 rounds of RRHC.878Figure 2: Performance of models and baselines on devel-opment data while varying amount of training data.
Notpictured: The null baseline at 58.8%.of this baseline is primarily to show the approximateminimal value for the temporal metric.We provide two other baselines to describe heuris-tic methods of aggregating the hard decisions fromthe classifier function C learned in ?3.2.
These areunlike the CC model which uses the soft decisionsof C. Both of these baselines maintain lists of pos-sible starts and ends for each fluent.
If the classifierassigns START AND END, we add the candidate tem-poral expression to both.
The first baseline, basicaggregation, is along the same lines as the aggrega-tion method used in Artiles et al2011), a state-of-the-art system.
Our baseline assigns the earliest startand the latest end as the bounds for each fluent, as-signing ??
for empty lists.
The second baseline,basic aggregation (modes), is the same except that ituses the mode from each list.To determine the best possible score given ourtemporal expression retrieval system, we calculatethe oracle score by assigning each fluent the spanwhich maximizes the temporal metric.
The oraclescore can differ from a perfect score since we canonly use candidate temporal expressions as valuesfor a fluent if (a) mentions of the fluent are retriev-able in our source documents, (b) the temporal ex-pression mention appears nearby, and (c) our tem-poral expression extractor is able to recognize it cor-rectly.
Nevertheless, it is still a reasonable upperbound in our setting.Model Dev TestOracle 78.1 75.2Joint Classifier with Consistency 76.1 72.2Combined Classifier 75.8 71.5Basic aggregation (modes) 75.3 71.2Basic aggregation 74.7 70.5null baseline 58.8 55.6Table 2: Performance of systems on development and testdivisions.
The Joint classifier with Consistency is the av-erage of three runs with negligible variance (?
?
0.02).4.3 ResultsWe present the performance of our models, base-lines, and the oracle in Figure 2 while varying thepercentage of training entities.
The JCC model(76.1% on development with 100% training enti-ties) is consistently the best non-oracle system.
Itsgains are larger when the amount of training data islow.
This is presumably because the classifier suf-fers from insufficient data and the consistency com-ponent is able to learn consistency rules to recoverfrom this.
Both the CC and JCC models outperformthe basic aggregation models.
This shows the valueof incorporating all marginal probabilities.
On thetest set (Table 2), the JCC model performs even bet-ter in comparison to the simple models, despite thetest set being clearly more difficult than the develop-ment set.
In this case, the JCC achieves a 36% errorreduction over the basic aggregation model.9 On theofficial KBP entities, the oracle score is 92%.
Sincewe use a different set of entities, there is a mismatchbetween our entities and the source documents re-sulting in a lower oracle score.
Addressing this isfuture work.5 DiscussionTable 3 shows the performance of four systemsand baselines on individual fluent types.
The JCCmodel derives most of its improvement from thetwo lifespan fluents and other high frequency flu-ents.
The lifespan fluents provide the most roomfor improvement since they tend to contain non-nullvalues a reasonable amount of the time (note howthese relations have a large gap between their ora-9This counts errors relative to the oracle score since we treatthe retrieval system as fixed in this work.879ModelFluent Count null Basic Basic (modes) CC JCC Oracleorganization: lifespan 266 49.2 71.0 70.7 71.1 71.7 73.4organization: top employees 150 88.0 88.0 88.0 88.0 88.0 88.3organization: founders 31 0.0 5.4 5.4 10.8 11.1 16.3organization: acquires company 14 21.4 21.4 21.4 21.4 21.4 38.5person: lifespan 806 28.6 63.1 64.6 65.6 66.1 69.1person: has spouse 582 92.2 92.1 92.1 92.2 92.3 93.1person: attends school 107 97.7 97.7 97.7 97.7 98.1 98.1person: has job 85 78.8 79.4 79.4 78.8 78.8 80.3person: holds government position 45 16.7 19.7 19.7 19.7 19.7 25.1person: romantic partner 5 50.0 52.9 52.9 52.9 52.9 71.2Table 3: Fluent-level performance of models and baselines on development data.
Scores are calculated with thetemporal metric.
CC stands for Combined Classifier and JCC for Joint Classifier with Consistency.
The JCC modelobtains most of its benefits on the two lifespan relations.
For attends school, it is the only system able to achieveoracle-level performance.
The null baseline is especially strong for several fluents since these tend to be unbounded or(more likely) missing their values in Freebase.
The two basic aggregation models differ primarily on their predictionsfor the lifespan fluents.cle and null scores).
Additionally, the lifespan fluentis always present for entities while other fluents aresparser.
For attends school, JCC is the only systemable to achieve oracle-level performance.
No systemimproves on the null baseline for acquires company.This is likely due to its sparsity.Inspecting the multinomials in the consistencycomponent, we can see that the model learns reason-able answers to questions such as whether an entity?was born before getting married??
(yes: 14.8%,no: 0.04%),10 ?died before their parents were born??
(yes: 0.3%, no: 53.7%) and ?finished a job beforestarting a job (not necessarily the same one)??
(yes:72.5%, no: 20.5%).
Despite some unavoidable noisein the data, it is clear these constraints are useful.6 Related workThere is a large body of related work that focuseson ordering events or classifying temporal relationsbetween them (Ling and Weld, 2010; Yoshikawa etal., 2009; Chambers and Jurafsky, 2008; Mani etal., 2006, inter alia).
Much of this work uses theAllen interval relations (Allen, 1983) which richlydescribe partial orderings of fluents.
We use severalof these as fluent-level question templates.Joint inference has been applied successfully10Percentages for ?unknown?
are omitted here.to other NLP problems (Roth and Yih, 2004;Toutanova et al2008; Martins et al2009; Changet al2010; Koo et al2010; Berant et al2011).
Two recent examples in information ex-traction include using Markov Logic for temporalordering (Ling and Weld, 2010) and using dual-decomposition for event extraction (Riedel and Mc-Callum, 2011).Our work is closest to Temporal KBP slot fillingsystems.
The CUNY and UNED systems (Artileset al2011; Garrido et al2011) for this task usedclassifiers to determine the relation between tempo-ral expressions and fluents.
These systems use thehard decisions from the classifier and combine thedecisions by finding a span that includes all temporalexpressions.
In contrast, our system uses the classi-fier?s marginal probabilities along with the consis-tency component to incorporate global consistencyconstraints.
Other participants used rule-based andpattern matching approaches (Byrne and Dunnion,2011; Surdeanu et al2011; Burman et al2011).Outside of Temporal KBP, there are several workson the task of extracting fluents from text.
Wanget al2011) which uses label propagation, a graph-based semi-supervised method to extend positiveand negative seed examples over the graph.
Taluk-dar et al2012) apply a similar approach by ag-gregating local classification decisions using tempo-880ral constraints (e.g., mutual exclusion, containment,and succession) and joint inference.
One key dif-ference is that their constraints are included as inputrather than learned by the system.7 Conclusion and future WorkJoint inference can be effectively applied to the taskof inferring timelines about named entities.
Ratherthan using hard coded heuristics, our model learnsand applies consistency constraints which captureinter-entity and cross-entity rules.
Simple inferencetechniques such as random-restart hillclimbing scorewell and run efficiently.
Both of our models (CC andJCC) obtain a substantial error reductions over sim-pler heuristics-based consistency approaches.The overall framework can easily be applied toother information extraction tasks.
Rather than list-ing rules for consistency, these can be learned andenforced via joint inference.
While simple joint in-ference methods such as random-restart hillclimb-ing and Gibbs sampling worked well in our case,more complex inference methods may be requiredwith more elaborate constraints.A prime direction for future work is combiningour model with a probabilistic relation extractionsystem.
This could be accomplished by using themarginal probabilities on the extracted relations andmultiplying them with the probabilities from theclassifier and consistency components.
Inferencewould require an additional step which could add ordrop candidate fluents.
Furthermore, the consistencycomponent can be extended with new question typesto incorporate non-temporal constraints as well.AcknowledgmentsThe authors would like to thank the Stanford NLPgroup (with special thanks to Gabor Angeli andMihai Surdeanu), William Headden, Micha Elsner,Pontus Stenetorp, and our anonymous reviewers fortheir helpful comments and feedback.We gratefully acknowledge the support ofDefense Advanced Research Projects Agency(DARPA) Machine Reading Program under AirForce Research Laboratory (AFRL) prime contractno.
FA8750-09-C-0181.
Any opinions, findings,and conclusion or recommendations expressed inthis material are those of the author(s) and do notnecessarily reflect the view of the DARPA, AFRL,or the US government.ReferencesJames F. Allen.
1983.
Maintaining knowledge abouttemporal intervals.
Communications of the ACM,26(11):832?843.Javier Artiles, Qi Li, Taylor Cassidy, Suzanne Tamang,and Heng Ji.
2011.
CUNY BLENDER TAC-KBP2011 Temporal Slot Filling System Description.In Proceedings of Text Analysis Conference (TAC),November.Jonathan Berant, Ido Dagan, and Jacob Goldberger.2011.
Global learning of typed entailment rules.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 610?619, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Amev Burman, Arun Jayapal, Sathish Kannan, MadhuKavilikatta, Ayman Alhelbawy, Leon Derczynski, andRobert Gaizauskas.
2011.
USFD at KBP 2011: EntityLinking, Slot Filling and Temporal Bounding.
In Pro-ceedings of Text Analysis Conference (TAC), Novem-ber.Lorna Byrne and John Dunnion.
2011.
UCD IIRG atTAC 2011.
In Proceedings of Text Analysis Confer-ence (TAC), November.Nathanael Chambers and Dan Jurafsky.
2008.
Jointlycombining implicit constraints improves temporal or-dering.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing, pages698?706.
Association for Computational Linguistics.Angel X. Chang and Christopher D. Manning.
2012.SUTIME: A library for recognizing and normalizingtime expressions.
In 8th International Conference onLanguage Resources and Evaluation (LREC 2012),May.Ming-Wei Chang, Dan Goldwasser, Dan Roth, and VivekSrikumar.
2010.
Discriminative learning over con-strained latent representations.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 429?437.
Association forComputational Linguistics.Mark Craven and Johan Kumlien.
1999.
Constructingbiological knowledge bases by extracting informationfrom text sources.
In Proceedings of the Seventh Inter-national Conference on Intelligent Systems for Molec-ular Biology, pages 77?86.
Heidelberg, Germany.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies repre-sentation.
In Proceedings of the COLING Workshop881on Cross-framework and Cross-domain Parser Evalu-ation.Jenny R. Finkel, Teg Grenager, and Christopher D. Man-ning.
2005.
Incorporating non-local information intoinformation extraction systems by Gibbs sampling.
InProceedings of the 43rd Annual Meeting on Associ-ation for Computational Linguistics, pages 363?370.Association for Computational Linguistics.Guillermo Garrido, Bernardo Cabaleiro, Anselmo Pe nas,Alvaro Rodrigo, and Damiano Spina.
2011.
A distantsupervised learning system for the TAC-KBP Slot Fill-ing and Temporal Slot Filling Tasks.
In Proceedings ofText Analysis Conference (TAC), November.Heng Ji, Ralph Grishman, and Hoa Trang Dang.
2011.Overview of the TAC 2011 Knowledge Base Popula-tion track.
In Proceedings of Text Analysis Conference(TAC), November.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics, pages 423?430.
Association for Computa-tional Linguistics.Terry Koo, Alexander M. Rush, Michael Collins, TommiJaakkola, and David Sontag.
2010.
Dual decompo-sition for parsing with non-projective head automata.In Proceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages1288?1298.
Association for Computational Linguis-tics.Heeyoung Lee, Yves Peirsman, Angel X. Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Juraf-sky.
2011.
Stanford?s Multi-Pass Sieve CoreferenceResolution System at the CoNLL-2011 Shared Task.In CoNLL 2011, page 28.Xiao Ling and Daniel S. Weld.
2010.
Temporal infor-mation extraction.
In Proceedings of the Twenty FifthNational Conference on Artificial Intelligence.Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong MinLee, and James Pustejovsky.
2006.
Machine learningof temporal relations.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand the 44th Annual Meeting of the Association forComputational Linguistics, pages 753?760.
Associa-tion for Computational Linguistics.Andre?
F. T. Martins, Noah A. Smith, and Eric P. Xing.2009.
Concise integer linear programming formula-tions for dependency parsing.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages342?350.
Association for Computational Linguistics.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.2009.
Distant supervision for relation extraction with-out labeled data.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP, pages 1003?1011.
Associa-tion for Computational Linguistics.Sebastian Riedel and Andrew McCallum.
2011.
Fast androbust joint models for biomedical event extraction.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP ?11), July.Dan Roth and Wen-tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
In Hwee Tou Ng and Ellen Riloff, editors,HLT-NAACL 2004 Workshop: Eighth Conference onComputational Natural Language Learning (CoNLL-2004), pages 1?8, Boston, Massachusetts, USA, May6 - May 7.
Association for Computational Linguistics.Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-Closky, Angel X. Chang, Valentin I. Spitkovsky, andChristopher D. Manning.
2011.
Stanford?s Distantly-Supervised Slot-Filling System.
In Proceedings ofText Analysis Conference (TAC), November.Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell.2012.
Coupled temporal scoping of relational facts.
InProceedings of the Fifth ACM International Confer-ence on Web Search and Data Mining, pages 73?82.ACM.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology, pages 173?180.
Association for Computational Linguistics.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A global joint model for semanticrole labeling.
Computational Linguistics, 34(2):161?191.Yafang Wang, Bing Yang, Lizhen Qu, Marc Spaniol, andGerhard Weikum.
2011.
Harvesting facts from textualweb sources by constrained label propagation.
In Pro-ceedings of the 20th ACM International Conference onInformation and Knowledge Management, pages 837?846.
ACM.Katsumasa Yoshikawa, Sebastian Riedel, Yuji Mat-sumoto, and Masayuki Asahara.
2009.
Jointly iden-tifying temporal relations with Markov Logic.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP, pages 405?413.
Association for Computa-tional Linguistics.882
