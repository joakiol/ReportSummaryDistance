Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1424?1433,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsIdentifying Causal Relations Using Parallel Wikipedia ArticlesChristopher HideyDepartment of Computer ScienceColumbia UniversityNew York, NY 10027chidey@cs.columbia.eduKathleen McKeownDepartment of Computer ScienceColumbia UniversityNew York, NY 10027kathy@cs.columbia.eduAbstractThe automatic detection of causal relation-ships in text is important for natural lan-guage understanding.
This task has provento be difficult, however, due to the need forworld knowledge and inference.
We fo-cus on a sub-task of this problem wherean open class set of linguistic markerscan provide clues towards understandingcausality.
Unlike the explicit markers, aclosed class, these markers vary signifi-cantly in their linguistic forms.
We lever-age parallel Wikipedia corpora to identifynew markers that are variations on knowncausal phrases, creating a training set viadistant supervision.
We also train a causalclassifier using features from the openclass markers and semantic features pro-viding contextual information.
The resultsshow that our features provide an 11.05point absolute increase over the baselineon the task of identifying causality in text.1 IntroductionThe automatic detection of causal relationships intext is an important but difficult problem.
Theidentification of causality is useful for the under-standing and description of events.
Causal in-ference may also aid upstream applications suchas question answering and text summarization.Knowledge of causal relationships can improveperformance in question answering for ?why?questions.
Summarization of event descriptionscan be improved by selecting causally motivatedsentences.
However, causality is frequently ex-pressed implicitly, which requires world knowl-edge and inference.
Even when causality is ex-plicit, there is a wide variety in how it is expressed.Causality is one type of relation in the Penn Dis-course Tree Bank (PDTB) (Prasad et al 2008).In general, discourse relations indicate how twotext spans are logically connected.
In PDTB the-ory, these discourse relations can be marked ex-plicitly or conveyed implicitly.
In the PDTB, thereare 102 known explicit discourse markers such as?and?, ?but?, ?after?, ?in contrast?, or ?in addi-tion?.
Of these, 28 explicitly mark causal relations(e.g., ?because?, ?as a result?, ?consequently?
).In addition to explicit markers, PDTB re-searchers recognize the existence of an open classof markers, which they call AltLex.
There is atremendous amount of variation in how AltLexesare expressed and so the set of AltLexes is ar-guably infinite in size.
In the PDTB, non-causalAltLexes include ?That compares with?
and ?Inany event.?
Causal AltLexes include ?This mayhelp explain why?
and ?This activity produced.
?Discourse relations with explicit discoursemarkers can be identified with high precision(Pitler and Nenkova, 2009) but they are also rela-tively rare.
Implicit relations are much more com-mon but very difficult to identify.
AltLexes fallin the middle; their linguistic variety makes themdifficult to identify but their presence improves theidentification of causality.One issue with causality identification is thelack of data.
Unsupervised identification onopen domain data yields low precision (Do et al2011) and while supervised methods on the PDTBhave improved (Ji and Eisenstein, 2015), creatingenough labeled data is difficult.
Here, we presenta distant supervision method for causality identifi-cation that uses parallel data to identify new causalconnectives given a seed set.
We train a classi-fier on this data and self-train to obtain new data.Our novel approach uses AltLexes that were auto-matically identified using semi-supervised learn-ing over a parallel corpus.
Since we do not know1424a priori what these phrases are, we used a mono-lingual parallel corpus to identify new phrases thatare aligned with known causal connectives.
Aslarge corpora of this type are rare, we used Sim-ple and English Wikipedia to create one.Section 2 discusses prior research in causalityand discourse.
Section 4 describes how we createda new corpus from Wikipedia for causality and ex-tracted a subset of relations with AltLexes.
In sec-tion 5, we recount the semantic and marker fea-tures and how they were incorporated into a classi-fier for causality.
We show that these features im-prove causal inference by an 11.05 point increasein F-measure over a naive baseline in 6.
Finally,we discuss the results and future work in 7.2 Related WorkRecent work on causality involved a combinationof supervised discourse classification with unsu-pervised metrics such as PMI (Do et al 2011).They used a minimally supervised approach us-ing integer linear programming to infer causality.Other work focused on specific causal construc-tions events paired by verb/verb and verb/noun(Riaz and Girju, 2013) (Riaz and Girju, 2014).Their work considered semantic properties ofnouns and verbs as well as text-only features.There has also been significant research intodiscourse semantics over the past few years.
Onetheory of discourse structure is represented in thePDTB (Prasad et al 2008).
The PDTB repre-sents discourse relationships as connectives be-tween two arguments.
Early work with the PDTB(Pitler and Nenkova, 2009) showed that discourseclasses with explicit discourse connectives canbe identified with high accuracy using a combi-nation of the connective and syntactic features.Further work (Pitler et al 2009) resulted in theidentification of implicit discourse relations usingword pair features; this approach extended ear-lier work using word pairs to identify rhetoricalrelations (Marcu, 2001) (Blair-Goldensohn et al2007).
These word pairs were created from textby taking the cross product of words from the Gi-gaword corpus for explicit causal and contrast re-lations.
Others built on this work by aggregat-ing word pairs for every explicit discourse con-nective (Biran and McKeown, 2013).
They thenused the cosine similarity between a prospectiverelation and these word pairs as a feature.
Re-cently, the first end-to-end discourse parser wascompleted (Lin et al 2012).
This parser jointlyinfers both argument spans and relations.
The cur-rent state-of-the-art discourse relation classifier isa constituent parse recursive neural network withcoreference (Ji and Eisenstein, 2015).Our work is similar to previous work to identifydiscourse connectives using unsupervised meth-ods (Laali, 2014).
In their research, they used theEuroParl parallel corpus to find discourse connec-tives in French using known English connectivesand filtering connectives using patterns.
Unlikethis effort, we created our own parallel corpus andwe determined new English connectives.Compared to previous work on causality, we fo-cus specifically on causality and the AltLex.
Thework by Do and Riaz used minimally supervised(Do et al 2011) or unsupervised (Riaz and Girju,2013) approaches and a slightly different defini-tion of causality, similar to co-ocurrence.
Thework of Riaz and Girju (2013) is most similar toour own.
We also examine causality as expressedby the author of the text.
However, they focus onintra-sentence constructions between noun or verbphrases directly whereas we attempt to examinehow the AltLex connectives express causality incontext.
Lastly, Riaz and Girju used FrameNet andWordNet to identify training instances for causalverb-verb and verb-noun pairs (Riaz and Girju,2014) whereas we use them as features for an an-notated training set.
Overall our contributions area new dataset created using a distant supervisionapproach and new features for causality identifi-cation.
One major advantage is that our methodrequires very little prior knowledge about the dataand requires only a small seed set of known con-nectives.3 Linguistic BackgroundOne disadvantage of the PDTB is that the markedAltLexes are limited only to discourse relationsacross sentences.
We know that there are addi-tional phrases that indicate causality within sen-tences but these phrases are neither found in theset of Explicit connectives nor AltLexes.
Thus weexpand our definition of AltLex to include thesemarkers when they occur within a sentence.
Al-though some phrases or words could be identifiedby consulting a thesaurus or the Penn ParaphraseDatabase (Ganitkevitch et al 2013), we still needthe context of the phrase to identify causality.We hypothesize that there is significant linguis-1425tic variety in causal AltLexes.
In the set of knownexplicit connectives there are adjectives (?subse-quent?
), adverbs (?consequently?
), and preposi-tions and prepositional phrases (?as a result?).
Weconsider that these parts of speech and syntacticclasses can be found in AltLexes as well.
In addi-tion, verbs and nouns often indicate causality butare not considered explicit connectives.Some obvious cases of AltLexes are the verbalforms of connectives such as ?cause?
and ?result?.In addition to these verbs, there exist other verbsthat can occur in causal contexts but are ambigu-ous.
Consider that ?make?
and ?force?
can replace?cause?
in this context:The explosion made people evacuatethe building.The explosion forced people to evacuatethe building.The explosion caused people to evacu-ate the building.However, the words can not be substituted in thefollowing sentence:The baker made a cake.
*The baker caused a cake.
*The baker forced a cake.Furthermore, verbs such as ?given?
may replaceadditional causal markers:It?s not surprising he is tired since he didnot get any sleep.It?s not surprising he is tired given thathe did not get any sleep.There are also some phrases with the samestructure as partial prepositional phrases like ?asa result?
or ?as a result of?, where the patternis preposition and noun phrase followed by anoptional preposition.
Some examples of thesephrases include ?on the basis of,?
?with the goalof,?
and ?with the idea of.
?We may also see phrases that are only causalwhen ending in a preposition such as ?thanks to?or ?owing to.?
?Lead?
may only be causal as apart of ?lead to?
and the same for ?develop?
ver-sus ?develop from.?
In addition, prepositions canaffect the direction of the causality.
Comparing?resulting in?
versus ?resulting from?, the prepo-sition determines that the latter is of the ?reason?class and the former is of the ?result?
class.Ultimately, we want to be able to detect thesephrases automatically and determine whether theyare a large/small and open/closed class of markers.4 DataIn order to discover new causal connectives, wecan leverage existing information about knowncausal connectives.
It should be the case thatif a phrase is a causal AltLex, it will occur insome context as a replacement for at least oneknown explicit connective.
Thus, given a largedataset, we would expect to find some pairs ofsentences where the words are very similar ex-cept for the connective.
This approach requiresa parallel corpus to identify new AltLexes.
Aslarge English paraphrase corpora are rare, we drawfrom previous work identifying paraphrase pairs inWikipedia (Hwang et al 2015).The dataset we used was created from the En-glish and Simple Wikipedias from September 11,2015.
We used the software WikiExtractor to con-vert the XML into plain text.
All articles withthe same title were paired and any extra arti-cles were ignored.
Each article was lemmatized,parsed (both constituent and dependency), andnamed-entity tagged using the Stanford CoreNLPsuite (Manning et al 2014).
We wish to identifyparaphrase pairs where one element is in EnglishWikipedia and one is in Simple Wikipedia.
Fur-thermore, we do not limit these elements to be sin-gle sentences because an AltLex can occur withina sentence or across sentences.Previous work (Hwang et al 2015) created ascore for similarity (WikNet) between EnglishWikipedia and Simple Wikipedia.
Many similarityscores are of the following form comparing sen-tences W and W?:s(W,W?)
=1Z?w?Wmaxw??W??(w,w?
)idf(w) (1)where ?(w,w?)
is a score1between 2 words andZ is a normalizer ensuring the score is between 0and 1.
For their work, they created a score where?(w,w?)
= ?wk(w,w?)
+ ?wk(h, h?
)?r(r, r?).
?wkis a distance function derived from Wik-tionary by creating a graph based on words appear-ing in a definition.
h and h?are the governors ofw and w?in a dependency parse and r and r?arethe relation.
Similar sentences should have similarstructure and the governors of two words in differ-ent sentences should also be similar.
?ris 0.5 if hand h?have the same relation and 0 otherwise.For this work, we also include partial matches,as we only need the connective and the immediate1The score is not a metric, as it is not symmetric.1426Method Max F1WikNet 0.4850WikNet, ?
= 0.75 0.5981Doc2Vec 0.6226Combined 0.6263Table 1: Paraphrase Resultssurrounding context on both sides.
If one sentencecontains an additional clause, it does not affectwhether it contains a connective.
Thus, one dis-advantage to this score is that when determiningwhether a sentence is a partial match to a longersentence or a shorter sentence, the longer sentencewill often be higher as there is no penalty for un-matched words between the two elements.
Weexperimented with penalizing content words thatdo not match any element in the other sentence.The modified score, where W and W?are nouns,verbs, adjectives, or adverbs, is then:s(W,W?)
=1Z?w?Wmaxw??W??(w,w?)idf(w)??(|W?
?W |+ |W ?W?|)(2)We also compared results with a model trainedusing doc2vec (Le and Mikolov, 2014) on eachsentence and sentence pair and identifying para-phrases with their cosine similarity.As these methods are unsupervised, only asmall amount of annotated data is needed to tunethe similarity thresholds.
Two graduate com-puter science students annotated a total of 45 Sim-ple/English article pairs.
There are 3,891 total sen-tences in the English articles and 794 total sen-tences in the Simple Wikipedia articles.
Inter-annotator agreement (IAA) was 0.9626, computedon five of the article pairs using Cohen?s Kappa.We tune the threshold for each possible score: fordoc2vec the cosine similarity and for WikNet thescoring function.
We also tune the lambda penaltyfor WikNet.
F1 scores were calculated via gridsearch over these parameters and the best settingsare a combined score using doc2vec and penalizedWikNet with ?
= 0.75 where a pair is consideredto be a paraphrase if either threshold is greater than0.69 or 0.65 respectively.Using the combined score we obtain 187,590paraphrase pairs.
After combining and dedupingthis dataset with the publicly available dataset re-leased by (Hwang et al 2015), we obtain 265,627pairs, about 6 times as large as the PDTB.In order to use this dataset for training a modelto distinguish between causal and non-causal in-Class Type SubtypeTemporalContingency Cause reasonresultPragmatic causeConditionPragmatic conditionComparisonExpansionTable 2: PDTB Discourse Classesstances, we use the paired data to identify pairswhere an explicit connective appears in at leastone element of the pair.
The explicit connectivecan appear in a Simple Wikipedia sentence or anEnglish Wikipedia sentence.
We then use patternsto find new phrases that align with these connec-tives in the matching sentence.To identify a set of seed words that unambigu-ously identify causal and non-causal phrases weexamine the PDTB.
As seen in Table 2, causal re-lations fall under the Contingency class and Causetype.
We consider connectives from the PDTB thateither only or never appear as that type.
The con-nective ?because?
is the only connective to be al-most always a ?reason?
connective, whereas thereare 11 unambiguous connectives for ?result?, in-cluding ?accordingly?, ?as a consequence?, ?as aresult?, and ?thus?.
There were many markersthat were unambiguously not causal (e.g.
?but?,?though?, ?still?, ?in addition?
).In order to label paraphrase data, we use con-straints to identify possible AltLexes.2We usedMoses (Koehn et al 2007) to train an alignmentmodel on the created paraphrase dataset.
Then forevery paraphrase pair we identify any connectivesthat match with any potential AltLexes.
Based onour linguistic analysis, we require these phrases tocontain at least one content word, which we iden-tify based on part of speech.
We also draw on pre-vious work (Pitler and Nenkova, 2009) that usedthe left and right sibling of a phrase.
Therefore,we use the following rules to label new AltLexes:1.
Must be less than 7 words.2.
Must contain at least one content word:(a) A non-proper noun(b) A non-modal and non-auxiliary verb(c) An adjective or adverb3.
Left sibling of the connective must be a nounphrase, verb phrase, or sentence.4.
Right sibling of the connective must be anoun phrase, verb phrase, or sentence.2We do not attempt to label arguments at this point.14275.
May not contain a modal or auxilary verb.Because connectives identify causality betweenevents or agents, we require that each potentialconnective link 2 events/agents.
We define anevent or agent as a noun, verb, or an entire sen-tence.
This means that we require the left sib-ling of the first word in a phrase and the right sib-ling of the last word in a phrase to be an event,where a sibling is the node at the same level inthe constituent parse.
We also require the left andright sibling rule for the explicit connectives, butwe allow additional non-content words (for exam-ple, we would mark ?because of?
as a connectiverather than ?because.?
We then mark the AltLexas causal or not causal.Given that the paraphrases and word alignmentsare noisy, we use the syntactic rules to decrease theamount of noise in the data by more precisely de-termining phrase boundaries.
These rules are thesame features used by Pitler and Nenkova (2009)for the early work on the PDTB on explicit con-nectives.
These features were successful on theWall Street Journal and they are applicable forother corpora as well.
Also, they are highly in-dicative of discourse/non-discourse usage so webelieve that we are improving on noisy align-ments without losing valuable data.
In the future,however, we would certainly like to move awayfrom encoding these constraints using a rule-basedmethod and use a machine learning approach toautomatically induce rules.This method yields 72,135 non-causal and9,190 causal training examples.
Although theseexamples are noisy, the dataset is larger than thePDTB and was derived automatically.
There are35,136 argument pairs in the PDTB marked withone of the 3 relations that implies a discourse con-nective (Implicit, Explicit, and AltLex), and ofthese 6,289 are causal.
Of the 6,289 causal pairs,2,099 are explicit and 273 contain an AltLex.5 MethodsGiven training data labeled by this distant supervi-sion technique, we can now treat this problem as asupervised learning problem and create a classifierto identify causality.We consider two classes of features: featuresderived from the parallel corpus data and lexicalsemantic features.
The parallel corpus featuresare created based on where AltLexes are used asparaphrases for causal indicators and in what con-text.
The lexical semantic features use FrameNet,WordNet, and VerbNet to derive features from allthe text in the sentence pair.
These lexical re-sources exploit different perspectives on the datain complementary ways.The parallel corpus features encourage the clas-sifier to select examples with AltLexes that arelikely to be causal whereas the lexical semanticfeatures allow the classifier to consider context fordisambiguation.
In addition to the dataset, the par-allel corpus and lexical semantic features are themain contributions of this effort.5.1 Parallel Corpus FeaturesWe create a subclass of features from the parallelcorpus: a KL-divergence score to encourage theidentification of phrases that replace causal con-nectives.
Consider the following datapoints andassume that they are aligned in the parallel corpus:I was late because of traffic.I was late due to traffic.We want both of these examples to have a highscore for causality because they are interchange-able causal phrases.
Similarly, we want non-causalphrases that are often aligned to have a high scorefor non-causality.We define several distributions in order to de-termine whether an AltLex is likely to replace aknown causal or non-causal connective.
We con-sider all aligned phrases, not just ones contain-ing a causal or non-causal connective to attemptto reduce noisy matches.
The idea is that non-connective paraphrases will occur often and inother contexts.The following conditional Bernoulli distribu-tions are calculated for every aligned phrase in thedataset, where w is the phrase, s is the sentence itoccurs in, c is ?causal?
and nc is ?not causal?
:p1= p(w1?
s1|rel(s1) ?
{c}, w1/?
s2) (3)p2= p(w1?
s1|rel(s1) ?
{nc}, w1/?
s2) (4)We compare these two distributions to other dis-tributions with the same word and in a differentcontext (where o represents ?other?
):q1= p(w1?
s1|rel(s1) ?
{nc, o}, w1/?
s2) (5)q2= p(w1?
s1|rel(s1) ?
{c, o}, w1/?
s2) (6)We then calculate DKL(p1||q1) andDKL(p2||q2).
In order to use KL-divergenceas a feature, we multiply the score by (?1)p<qand add a feature for causal and one for non-causal.14285.2 Lexical Semantic FeaturesAs events are composed of predicates and ar-guments and these are usually formed by nounsand verbs, we consider using lexical semantic re-sources that have defined hierarchies for nounsand verbs.
We thus use the lexical resourcesFrameNet, WordNet, and VerbNet as complemen-tary resources from which to derive features.
Wehypothesize that these semantic features providecontext not present in the text; from these we areable to infer causal and anti-causal properties.FrameNet is a resource for frame semantics,defining how objects and relations interact, andprovides an annotated corpus of English sen-tences.
WordNet provides a hierarchy of wordsenses and we show that the top-level class ofverbs is useful for indicating causality.
VerbNetprovides a more fine-grained approach to verb cat-egorization that complements the views providedby FrameNet and WordNet.In FrameNet, a semantic frame is a concep-tual construction describing events or relationsand their participants (Ruppenhofer et al 2010).Frame semantics abstracts away from specific ut-terances and ordering of words in order to repre-sent events at a higher level.
There are over 1,200semantic frames in FrameNet and some of thesecan be used as evidence or counter-evidence forcausality (Riaz and Girju, 2013).
In Riaz?s work,they identified 18 frames as causal (e.g.
?Pur-pose?, ?Internal cause?, ?Reason?, ?Trigger?
).We use these same frames to create a lexicalscore based on the FrameNet 1.5 corpus.
Thiscorpus contains 170,000 sentences manually an-notated with frames.
We used a part-of-speechtagged version of the FrameNet corpus and foreach word and tag, we count how often it occursin the span of one of the given frames.
We onlyconsidered nouns, verbs, adjectives, and adverbs.We then calculate pw(c|t) and cwct, the probabilitythat a wordw is causal given its tag t and its count,respectively.
The lexical score of a word i is calcu-lated by using the assigned part-of-speech tag andis given by CSi= pwi(c|ti) log cwicti.
The totalscore of a sequence of words is then?ni=0CSi.We also took this further and determined whatframes are likely to be anti-causal.
We startedwith a small set of seed words derived directlyfrom 11 discourse classes (types and subtypesfrom Table 2), such as ?Compare?, ?Contrast?,?Explain?, ?Concede?, and ?List?.
We expandedthis list using WordNet synonyms for the seedwords.
We then extracted every frame associatedwith their stems in the stemmed FrameNet corpus.These derived frames were manually examined todevelop a list of 48 anti-causal frames, including?Statement?, ?Occasion?, ?Relative time?, ?Evi-dence?, and ?Explaining the facts?.We create an anti-causal score using theFrameNet corpus just as we did for the causalscore.
The total anti-causal score of a se-quence of words is?ni=0ACSiwhere ACSi=pwi(a|ti) log cwiatifor anti-causal probabilitiesand counts.
We split each example into three parts:the text before the AltLex, the AltLex, and the textafter.
Each section is given a causal score and ananti-causal score.
Overall, there are six featuresderived using FrameNet: causal score and anti-causal score for each part of the example.In WordNet, words are grouped into ?synsets,?which represent all synonyms of a particular wordsense.
Each word sense in the WordNet hierarchyhas a top-level category based on part of speech(Miller, 1995).
Every word sense tagged as noun,verb, adjective, or adverb is categorized.
Someexamples of categories are ?change?, ?stative?,or ?communication?.
We only include the toplevel because of the polysemous nature of Word-Net synsets.
We theorize that words having to dowith change or state should be causal indicatorsand words for communication or emotion may beanti-causal indicators.Similar to the FrameNet features, we split theexample into three sections.
However, we alsoconsider the dependency parse of the data.
We be-lieve that causal relations are between events andagents which are represented by nouns and verbs.Events can also be represented by predicates andtheir arguments, which is captured by the depen-dency parse.
As the root of a dependency parse isoften a verb and sometimes a noun or adjective, weconsider the category of the root of a dependencyparse and its arguments.We include a categorical feature indicating thetop-level category of the root of each of the threesections, including the AltLex.
For both sides ofthe AltLex, we include the top-level category ofall arguments as well.
If a noun has no category,we mark it using its named-entity tag.
If there isstill no tag, we mark the category as ?none.
?VerbNet VerbNet is a resource devoted to stor-ing information for verbs (Kipper et al 2000).1429In contrast to WordNet, VerbNet provides a morefine-grained description of events while focusingless on polysemy.
Some examples of VerbNetclasses are ?force?, ?indicate?, and ?wish?.
InVerbNet, there are 273 verb classes, and we in-clude their presence as a categorical feature.
Sim-ilar to WordNet, we use VerbNet categories forthree sections of the sentence: the text pre-AltLex,the AltLex, and the text post-AltLex.
UnlikeWordNet, we only mark the verbs in the AltLex,root, or arguments.Interaction Finally, we consider interactionsbetween the WordNet and VerbNet features.
Asprevious work (Marcu, 2001) (Biran and McKe-own, 2013) used word pairs successfully, we hy-pothesize that pairs of higher-level categories willimprove classification without being penalized asheavily by the sparsity of dealing with individualwords.
Thus we include interaction features be-tween every categorical feature for the pre-AltLextext and every feature for the post-AltLex text.In all, we include the following features (Lrefers to the AltLex, B refers to the text before theAltLex and A refers to the text after the AltLex):1.
FrameNet causal score for L, B, and A.2.
FrameNet anti-causal score for L, B, and A.3.
WordNet top-level of L.4.
WordNet top-level of the root of B and A.5.
WordNet top-level for arguments of B and A.6.
VerbNet category for verb at the root of L.7.
VerbNet top-level category for any verb in theroot of B and A.8.
VerbNet top-level category for any verbs inthe arguments of B and A.9.
Categorical interaction features between thefeatures from B and the features from A.6 ResultsWe evaluated our methods on two manually anno-tated test sets.
We used one of these test sets fordevelopment only.
For this set, one graduate com-puter science student and two students from theEnglish department annotated a set of Wikipediaarticles by marking any phrases they consideredto indicate a causal relationship and marking thephrase as ?reason?
or ?result.?
Wikipedia articlesfrom the following categories were chosen as webelieve they are more likely to contain causal re-lationships: science, medicine, disasters, history,television, and film.
For each article in this cate-gory, both the English and Simple Wikipedia ar-ticles were annotated.
A total of 12 article pairswere annotated.
IAA was computed to be 0.31 ontwo article pairs using Kripendorff?s alpha.IAA was very low and we also noticed thatannotators seemed to miss sentences containingcausal connectives.
It is easy for an annotator tooverlook a causal relation when reading througha large quantity of text.
Thus, we created a newtask that required labeling a connective as causalor not when provided with the sentence contain-ing the connective.
For testing, we used Crowd-Flower to annotate the output of the system usingthis method.
We created a balanced test set by an-notating 600 examples, where the system labeled300 as causal and 300 as non-causal.
Contribu-tors were limited to the highest level of quality andfrom English-speaking countries.
We required 7annotators for each data point.
The IAA was com-puted on the qualification task that all annotatorswere required to complete.
There were 15 ques-tions on this task and 410 annotators.
On this sim-plified task, the IAA improved to 0.69.We also considered evaluating the results onthe PDTB but encountered several issues.
Asthe PDTB only has a limited set of explicit intra-sentence connectives marked, this would not showthe full strength of our method.
Many causal con-nectives that we discovered are not annotated inthe PDTB.
Alternatively, we considered evaluat-ing on the AltLexes in the PDTB but these ex-amples are only limited to inter-sentence cases,whereas the vast majority of our automatically an-notated training data was for the intra-sentencecase.
Thus we concluded that any evaluation onthe PDTB would require additional annotation.Our goal in this work was to identify new waysin which causality is expressed, unlike the PDTBwhere annotators were given a list of connectivesand asked to determine discourse relations.We tested our hypothesis by training a binary3classifier on our data using the full set of featureswe just described.
We used a linear Support VectorMachine (SVM) classifier (Vapnik, 1998) trainedusing stochastic gradient descent (SGD) throughthe sci-kit learn package.
(Pedregosa et al 2011)4We used elasticnet to encourage sparsity and tunedthe regularization constant ?
through grid search.We use two baselines.
The first baseline is the3We combine ?reason?
and ?result?
into one ?causal?class and plan to work on distinguishing between non-causal,reason, and result in the future.4We also considered a logistic regression classifier.1430Accuracy True Precision True Recall True F-measureMost Common Class 63.50 60.32 82.96 69.85CONN 62.21 78.47 35.64 49.02LS 67.68 61.98 58.51 60.19KLD 58.03 91.17 19.55 32.20LS ?KLD 73.95 80.63 64.35 71.57LS ?
LSinter72.99 78.54 64.66 70.93KLD ?
LS ?
LSinter70.09 76.95 58.99 66.78LS ?KLD ?
CONN 71.86 70.28 77.60 73.76Bootstrapping179.26 77.97 82.64 80.24Bootstrapping279.58 77.29 84.85 80.90Table 3: Experimental Resultsmost common class of each AltLex according toits class in the initial training set.
For example,?caused by?
is almost always a causal AltLex.
Asecond baseline uses the AltLex itself as a cate-gorical feature and is shown as CONN in Ta-ble 3.
For comparison, this is the same baselineused in (Pitler and Nenkova, 2009) on the ex-plicit discourse relations in the PDTB.
We com-pare these two baselines to ablated versions of oursystem.
We evaluate on the KLD (KLD) andsemantic (LS and LSinter) features described insections 5 and 5.1.
LS consists of features 1-8,all the FrameNet, VerbNet, and WordNet features.LSinterincludes only the interaction between cat-egorical features from WordNet and VerbNet.We calculate accuracy and true precision, recall,and F-measure for the causal class.
As seen in Ta-ble 3, the best system (LS ?
KLD ?
CONN )outperforms the baselines.5The lexical semanticfeatures by themselves (LS) are similar to thoseused by (Riaz and Girju, 2014) although on a dif-ferent task and with the WordNet and VerbNet fea-tures included.
Note that the addition of the Altlexwords and KL divergence (LS?KLD?CONN )yields an absolute increase in f-measure of 13.57points over lexical semantic features alone.6.1 BootstrappingOur method for labeling AltLexes lends itself nat-urally to a bootstrapping approach.
As we are us-ing explicit connectives to identify new AltLexes,we can also use these new AltLexes to identify ad-ditional ones.
We then consider any paraphrasepairs where at least one of the phrases contains oneof our newly discovered AltLexes.
We also use5These results are statistically significant by a binomialtest with p < 7 ?
10?6.our classifier to automatically label these new datapoints and remove any phrases where the classifierdid not agree on both elements in the pair.
The setof features used were theKLD?LS?LSinterfea-tures as these performed best on the developmentset.
We use early stopping on the developmentdata to identify the point when adding additionaldata is not worthwhile.
The bootstrapping methodconverges quickly.
After 2 iterations we see a de-crease in the F-measure of the development data.The increase in performance on the test data issignificant.
In Table 3, Bootstrappingnrefersto results after n rounds of bootstrapping.
Boot-strapping yields improvement over the supervisedmethod with an absolute gain of 7.14 points.6.2 DiscussionOf note is that the systems without connectives(combinations of LS, LSinter, and KLD) per-form well on the development set without usingany lexical features.
Using this system enables thediscovery of new AltLexes during bootstrapping,as we cannot rely on having a closed class of con-nectives but need a way of classifying connectivesnot seen in the initial training set.Also important is that the Altlex by itself(CONN ) performs poorly.
In comparison, in thetask of identifying discourse relations in the PDTBthese features yield an 75.33 F-score and 85.85%accuracy in distinguishing between discourse andnon-discourse usage (Pitler and Nenkova, 2009)and an accuracy of 93.67% when distinguishingbetween discourse classes.
Although this is a dif-ferent data set, this shows that identifying causal-ity when there is an open class of connectivesis much more difficult.
We believe the connec-tive by itself performs poorly because of the wide1431True Precision True Recall True F-measureFrameNet 67.88 53.14 59.61WordNet 76.92 9.52 16.94V erbNet 38.70 3.80 6.92Table 4: Semantic Feature Ablationlinguistic variation in these alternative lexicaliza-tions.
Many connectives appear only once or notat all in the training set, so the additional featuresare required to improve performance.In addition, the ?most common class?
baselineis a strong baseline.
The strength of this perfor-mance provides some indication of the quality ofthe training data, as the majority of the time theconnective is very indicative of its class in theheld-out test data.
However, the the overall accu-racy is still much lower than if we use informativefeatures.The KLD and LS feature sets appear to becomplementary.
The KLD feature sets havehigher precision on a smaller section of the data,whereas the LS system has higher recall over-all.
These lexical semantic features likely havehigher recall because these resources are designedto represent classes of words rather than individ-ual words.
Some connectives occur very rarely, soit is necessary to generalize the key aspects of theconnectives and class-based resources provide thiscapability.In order to determine the contribution of eachlexical resource, we perform additional feature ab-lation for each of FrameNet, WordNet, and Verb-Net.
As seen in Table 4, the lexical semantic re-sources each contribute uniquely to the classifier.The FrameNet features provide most of the perfor-mance of the classifier.
The WordNet and Verb-Net features, though not strong individually, sup-ply complementary information and improve theoverall performance of the LS system (see Table3) compared to just using FrameNet alne.Finally, the model (LS?KLD?CONN ) cor-rectly identifies some causal relations that neitherbaseline identifies, such as:Language is reduced to simple phrasesor even single words, eventually leadingto complete loss of speech.Kulap quickly accelerated north,prompting the PAGASA to issue theirfinal advisory on the system.These examples do not contain standard causalconnectives and occur infrequently in the data, sothe lexical semantic features help to identify them.After two rounds of bootstrapping, the system isable to recover additional examples that were notfound previously, such as:When he finally changed back, Buffystabbed him in order to once again savethe world.This connective occurs rarely or not at all in theinitial training data and is only recovered becauseof the improvements in the model.7 ConclusionWe have shown a method for identifying and clas-sifying phrases that indicate causality.
Our methodfor automatically building a training set for causal-ity is a new contribution.
We have shown statisti-cally significant improvement over the naive base-line using semantic and parallel corpus features.The text in the AltLex alone is not sufficient to ac-curately identify causality.
We show that our fea-tures are informative by themselves and performwell even on rarely occurring examples.Ultimately, the focus of this work is to improvedetection of causal relations.
Thus, we did notevaluate some intermediate steps, such as the qual-ity of the automatically annotated corpus.
Our useof distant supervision demonstrates that we canuse a large amount of possibly noisy data to de-velop an accurate classifer.
To evaluate on the in-termediate step would have required an additionalannotation process.
In the future, we may improvethis step using a machine learning approach.Although we have focused exclusively onWikipedia, these methods could be adapted toother domains and languages.
Causality is noteasily expressed in English using a fixed set ofphrases, so we would expect these methods to ap-ply to formal and informal text ranging from newsand journals to social media.
Linguistic expres-sions of causality in other languages is another av-enue for future research, and it would be interest-ing to note if other languages have the same vari-ety of expression.1432ReferencesOr Biran and Kathleen McKeown.
2013.
AggregatedWord Pair Features for Implicit Discourse RelationDisambiguation.
Proceedings of ACL.Sasha Blair-Goldensohn, Kathleen McKeown, andOwen Rambow.
2007.
Building and RefiningRhetorical-Semantic Relation Models.
Proceedingsof NAACL-HLT.Nathanael Chambers and Dan Jurafsky.
2008.
Unsu-pervised Learning of Narrative Event Chains.
Stan-ford University.
Stanford, CA 94305.Quang Xuan Do, Yee Seng Chan, and Dan Roth.
2011.Minimally Supervised Event Causality Identifica-tion.
Transactions of ACL, 3:329344.William Hwang, Hannaneh Hajishirzi, Mari Ostendorf,and Wei Wu.
2015.
Aligning Sentences from Stan-dard Wikipedia to Simple Wikipedia.
Proceedings ofNAACL-HLT.Yangfeng Ji and Jacob Eisenstein.
2015.
One Vec-tor is Not Enough: Entity-Augmented DistributedSemantics for Discourse Relations.
Proceedings ofEMNLP.Karin Kipper, Hoa Trang Dan, and Martha Palmer.2000.
Class-Based Construction of a Verb Lexicon.American Association for Artifical Intelligence.Majid Laali and Leila Kosseim.
2014.
Inducing Dis-course Connectives from Parallel Texts.
Proceed-ings of COLING: Technical Papers.Quoc Le and Tomas Mikolov.
2014.
Distributed Rep-resentations of Sentences and Documents.
Proceed-ings of ICML.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012.A PDTB-Styled End-to-End Discourse Parser.
De-partment of Computer Science, National Universityof Singapore.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP Natural Lan-guage Processing Toolkit.
Proceedings of ACL:System Demonstrations.Daniel Marcu and Abdessamad Echihabi 2001.
AnUnsupervised Approach to Recognizing DiscourseRelations.
Proceedings of ACL.George A. Miller.
1995.
WordNet: A Lexical Databasefor English.
Communications of the ACM.The PDTB Research Group 2008.
The PDTB 2.0.
An-notation Manual.
Technical Report IRCS-08-01.
In-stitute for Research in Cognitive Science, Universityof Pennsylvania.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.Automatic sense prediction for implicit discourse re-lations in text Proceedings of ACL.Emily Pitler and Ani Nenkova.
2009.
Using Syntaxto Disambiguate Explicit Discourse Connectives inText.
Proceedings of ACL-IJCNLP Short Papers.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi and BonnieWebber.
2008.
The Penn Discourse Treebank 2.0.Proceedings of LREC.Rashmi Prasad, Aravind Joshi, and Bonnie Webber.2010.
Realization of Discourse Relations by OtherMeans: Alternative Lexicalizations.
Proceedings ofCOLING.Kira Radinsky and Eric Horvitz.
2013.
Mining the Webto Predict Future Events.
Proceedings of WSDM.Mehwish Riaz and Roxana Girju.
2013.
Towarda Better Understanding of Causality between Ver-bal Events: Extraction and Analysis of the CausalPower of Verb-Verb Associations.
Proceedings ofSIGDIAL.Mehwish Riaz and Roxana Girju.
2014.
RecognizingCausality in Verb-Noun Pairs via Noun and Verb Se-mantics.
Proceedings of the EACL 2014 Workshopon Computational Approaches to Causality in Lan-guage.Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.Petruck, Christopher R. Johnson, and Jan Schef-fczyk.
2010.
FrameNet II: Extended Theory andPractice.
University of California, Berkeley.Vladimir N. Vapnik.
1998.
Statistical Learning The-ory.
Wiley-Interscience.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The ParaphraseDatabase.
Proceedings of NAACL-HLT.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos, D.Cournapeau, M. Brucher, M. Perrot, E. Duchesnay.2011.
Scikit-learn: Machine Learning in Python.Journal of Machine Learning Research Volume 12.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:open source toolkit for statistical machine transla-tion.
Proceedings of ACL: Interactive Poster andDemonstration Sessions.1433
