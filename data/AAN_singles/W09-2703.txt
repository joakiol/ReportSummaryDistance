Proceedings of the 2009 Workshop on Knowledge and Reasoning for Answering Questions, ACL-IJCNLP 2009, pages 11?14,Suntec, Singapore, 6 August 2009.c?2009 ACL and AFNLPQAST: Question Answering System for Thai WikipediaWittawat Jitkrittum?
Choochart Haruechaiyasak?
Thanaruk Theeramunkong?
?School of Information, Computer and Communication Technology (ICT)Sirindhorn International Institute of Technology (SIIT)131 Moo 5 Tiwanont Rd., Bangkadi, Muang, Phathumthani, Thailand, 12000wittawatj@gmail.com, thanaruk@siit.tu.ac.th?Human Language Technology Laboratory (HLT)National Electronics and Computer Technology Center (NECTEC)Thailand Science Park, Klong Luang, Pathumthani 12120, Thailandchoochart.haruechaiyasak@nectec.or.thAbstractWe propose an open-domain question an-swering system using Thai Wikipedia asthe knowledge base.
Two types of in-formation are used for answering a ques-tion: (1) structured information extractedand stored in the form of Resource De-scription Framework (RDF), and (2) un-structured texts stored as a search index.For the structured information, SPARQLtransformed query is applied to retrieve ashort answer from the RDF base.
For theunstructured information, keyword-basedquery is used to retrieve the shortest textspan containing the questions?s key terms.From the experimental results, the systemwhich integrates both approaches couldachieve an average MRR of 0.47 based on215 test questions.1 IntroductionMost keyword-based search engines available on-line do not support the retrieval of precise infor-mation.
They only return a list of URLs, each re-ferring to a web page, sorted by relevancy to theuser?s query.
Users then have to manually scanthose documents for needed information.
Due tothis limitation, many techniques for implement-ing QA systems have been proposed in the pastdecades.From the literature reviews, previous and exist-ing QA systems can be broadly categorized intotwo types:1.
Knowledge Intensive: Knowledge intensivesystems focus on analyzing and understand-ing the input questions.
The system knowsexactly what to be answered, and also whattype the answer should be.
The analysisphase usually depends on an ontology or asemantic lexicon like WordNet.
The an-swer is retrieved from a predefined organizedknowledge base.
Natural Language Process-ing (NLP) techniques are heavily used in aknowledge intensive system.2.
Data Intensive: Data intensive systems,which do not fully analyze the input ques-tions, rely on the redundancy of huge amountof data (Dumais et al, 2002).
The idea is thatif we have a huge amount of data, a piece ofinformation is likely to be stated more thanonce in different forms.
As a result, the data-intensive QA systems are not required to per-form many complex NLP techniques.In this paper, we propose an open-domain QAsystem for Thai Wikipedia called QAST.
The sys-tem supports five types of close-ended questions:person, organization, place, quantity, and date/-time.
Our system can be classified as a data in-tensive type with an additional support of struc-tured information.
Structured information in ThaiWikipedia is extracted and represented in the formof RDF.
We use SPARQL to retrieve specific in-formation from the RDF base.
If using SPARQLcannot answer a given question, the system will re-trieve answer candidates from the pre-constructedsearch index using a technique based on MinimalSpan Weighting (Monz, 2003).2 System ArchitectureFigure 1 shows the system architecture of QASTwhich consists of three main sub-systems: DataRepresentation , Question Processor , and Answer11Figure 1: The system architecture of QASTProcessor .2.1 Data RepresentationThe Data Representation part is a storage for allinformation contained in Thai Wikipedia.
Twomodules constitute this sub-system.RDF Base: In QAST, RDF triples are generatedfrom Wikipedia?s infoboxes following similar ap-proaches described in the works of Isbell and But-ler (2007) and Auer and Lehmann (2007).
To gen-erate RDF triples from an infobox, we would havethe article title as the subject.
The predicates arethe keys in the first column.
The objects are thevalues in the second column.
Altogether, the num-ber of generated triples corresponds to the numberof rows in the infobox.In addition to the infoboxes, we also store syn-onyms in the form of RDF triples.
The synonymsare extracted from redirect pages in Wikipedia.For example, a request for the Wikipedia articletitled ?Car?
will result in another article titled ?Au-tomobile?
to be shown up.
The former page usu-ally has no content and only acts as a pointer toanother page which contains the full content.
Therelationship of these two pages implies that ?Car?and ?Automobile?
are synonymous.
Synonymsare useful in retrieving the same piece of informa-tion with different texual expressions.Search Index: QAST stores the textual contentas a search index.
We used the well-known IR li-brary, Lucene1, for our search backend.
We in-dexed 41,512 articles (as of February 5, 2009)from a Thai Wikipedia dump with full term posi-tions.
Firstly, all template constructs and the Wiki-Text markups are removed, leaving with only theplain texts.
A dictionary-based longest-matchingword segmentation is then performed to tokenizethe plain texts into series of terms.
Finally, theresulted list of non-stopwords are passed to theLucene indexing engine.
The dictionary used forword segmentation is a combination of word listfrom the LEXiTRON2and all article titles fromThai Wikipedia.
In total, there are 81,345 wordsin the dictionary.2.2 Question ProcessorQuestion processor sub-system consists of fourmodules as follows.1.
Question Normalizer ?
This first module isto change the way the question is formed intoa normal form to ease the processing at lat-ter stages.
This includes correcting mistypedwords or unusual spelling such as f33t forfeet.2.
Word Segmenter ?
This module performstokenizing on the normalized question to ob-tain a list of non-stopwords.3.
Question Analyzer ?
The question ana-lyzer determines the expected type of answer(i.e., quantity, person, organization, location,date/time and unknown) and constructs anappropriate query.
Normally, a SPARQLquery is generated and used to retrieve a can-didate answer from the RDF base.
When theSPARQL fails to find an answer, the systemwill switch to the index search.
In that case,the module also defines a set of hint terms tohelp in locating candidate answers.4.
Searcher ?
This module executes the queryand retrieves candidate answers from the datarepresentation part.To generate a SPARQL query, the input ques-tion is compared against a set of predefined regu-lar expression patterns.
Currently, the system hastwo types of patterns: pattern for definitional ques-tions, and pattern for questions asking for a prop-1Apache Lucene, http://lucene.apache.org2LEXiTRON, http://lexitron.nectec.or.th12erty of an entity.
The pattern for definitional ques-tion is of the form a-rai-kue-X ?What is X ??
orX-kue-a-rai ?X is what ??.
After X is determinedfrom a user?s question, the first paragraph of thearticle titled X is retrieved and directly returned tothe user.
Since the first paragraph in any article isusually the summary, it is appropriate to use thefirst paragraph to answer a definitional question.Questions asking for a property of an entity areof the form a-rai-kue-P-kong-X ?What is P of X ?
?e.g., ?When was SIIT established ??
which can beanswered by looking for the right information inthe RDF base.
A simplified SPARQL query usedto retrieve an answer for this type of question is asfollows.SELECT ?
oWHERE {?
tempPage h a s I n f o b o x ?
tempBox .?
tempPage r d f s : l a b e l ?X?
.?
tempBox ?P ?
o .
} The query matches an object of a RDF triple withthe predicate P (e.g., ?date of establishment?
), pro-vided that the triple is generated from an infoboxtitled X (e.g.,?SIIT?)
.
The object of the year 1992is then correctly returned as the answer.When SPARQL fails, i.e., the question doesnot match any known pattern or the answer doesnot exist in the RDF base, the system switches tothe index search which performs the following thesteps.1.
Word Segmenter tokenizes the question intoa list of keywords q.2.
Question analyzer analyzes q, generates a ba-sic Lucene?
TermQuery, and defines a set ofhint terms H .3.
Retrieve the most relevant c documents usingLucene?s default search scoring function3.Denote D as the set of retrieved documents.4.
For each document d in D where d ={t1, t2, .
.
.
, t|d|} (t is a term),(a) Find in d the start term indexmmsStart and end term indexmmsEnd of the shortest term spancontaining all terms in q (Monz, 2003).
(b) spanLength ?
1 + mmsEnd ?mmsStart(c) If spanLength > 30, skip current d.Go to the next document.3http://lucene.apache.org/java/2_3_0/scoring.html(d) Find minimal span weightingscore msw (Monz, 2003).
If|q ?
d| = 1 then, msw =RSVn(q, d).
Otherwise, msw =0.4 ?RSVn(q, d)+0.6 ?(|q?d|spanLength)1/8?
(|q?d||q|) where RSVn(q, d) =lucene(q, d)/maxdlucene(q, d)(e) mmsStart?
max(mmsStart?s, 1)(f) mmsEnd?
min(mmsEnd + s, |d|)(g) Find the weighting for hint terms hw(0 ?
hw ?
1).
(h) Calculate the span scoresp = msw ?
(1 + hw)(i) Add the text span to the span set P (SortP by sp in descending order).5.
Return the top k spans in P as answers.In the actual implementation, we set c equal to500 so that only the top 500 documents are con-sidered.
Although retrieving more texts from thecorpus would likely increase the chance of find-ing the answer (Moldovan et al, 2002), our trial-and-error showed that 500 documents seem to bea good trade-off between speed and content cov-erage.
To look for an occurrence of hint terms,each span is stretched backward and forward for10 terms (i.e., s = 10).
Finally, we set k equal to5 to return only the top five spans as the answers.2.3 Answer ProcessorThis sub-system contains two modules: AnswerRanker and Answer Generator.Answer Ranker concerns with how to rank theretrieved answer candidates.
In the case whereSPARQL query is used, this module is not re-quired since most of the time there will be onlyone result returned.In the case when the search index is used, allcandidate answers are sorted by the heuristic spanscore (i.e., sp = msw ?
(1 + hw)).
The func-tion mostly relies on regular expressions definingexpected answer patterns.
If a span has an occur-rence of one of the defined patterns (i.e., hw > 0),it is directly proportional to the suitability of theoccurrence with respect to the question, length andrareness of the pattern occurrence.
For example,the hint terms of questions asking for a personwould be personal titles such as Ms. and Dr.As for the final step, the Answer Generatormodule formats the top five candidate answers intoan HTML table and returns the results to the user.13Question Type Index & RDF IndexPerson 0.47 0.37Organization 0.56 0.46Place/Location 0.43 0.36Quantity 0.51 0.44Date/Time 0.39 0.34Average MRR 0.47 0.39Table 1: QAST?s performance comparison be-tween (1) using both index and RDF and (2) usingonly the index.3 Evaluation MetricTo evaluate the system, 215 test questions (43questions for each question type) and their cor-rect answers were constructed based on the con-tents of random articles in Thai Wikipedia.
MeanReciprocal Rank (MRR), the official measurementused for QA systems in TREC (Voorhees and Tice,2000), is used as the performance measurement.To evaluate the system, a question is said to becorrectly answered only when at least one of theproduced five ranked candidates contained the trueanswer with the right context.
Out-of-context can-didate phrases which happen to contain the trueanswers are not counted.
If there is no correct an-swer in any candidate, the score for that questionis equal to zero.4 Experimental Results and DiscussionTable 1 shows a comparison of the MRR valueswhen using both index and RDF, and using onlythe index.
The approach of using only the index,the overall MRR is equal to 0.39 which is fairlyhigh with respect to the answer retrieval method-ology.
The index search approach simply relies onthe fact that if the question keywords in a rankedcandidate document occur close together and atleast one occurrence of expected answer patternexists, then there is a high chance that the termspan contains an answer.The MRR significantly increases to 0.47 (20.5%improvement) when RDF (structured information)is used together with the index.
A thorough analy-sis showed that out of 215 questions, 21 questionstriggered the RDF base.
Among these, 18 ques-tions were correctly answered.
Therefore, usingthe additional structured information helps answerthe definitional and factoid questions.
We expect ahigher improvement when more structured infor-mation is incorporated into the system.5 Conclusions and Future WorksWe proposed an open-domain QA system calledQAST.
The system uses Thai Wikipedia as the cor-pus and does not rely on any complex NLP tech-nique in retrieving an answer.As for future works, some possiblities for im-proving the current QAST are as follows.?
An information extraction module may beadded to extract and generate RDF triplesfrom unstructured text.?
Infoboxes, wikipedia categories and internalarticle links may be further explored to con-struct an ontology which will allow an auto-matic type inference of entities.?
More question patterns and the correspond-ing SPARQL queries can be added so thatSPARQL is used more often.AcknowledgementThe financial support from Young Scientist andTechnologist Programme, NSTDA (YSTP : SP-51-NT-15) is gratefully acknowledged.ReferencesSoren Auer and Jens Lehmann.
2007.
What HaveInnsbruck and Leipzig in Common?
Extracting Se-mantics from Wiki Content.
In Proc.
of the 4thEu-ropean conference on The Semantic Web: Researchand Applications, pp.
503-517.Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,and Andrew Ng.
2002.
Web Question Answering:Is More Always Better?.
In Proc.
of the 25th ACMSIGIR, pp.
291-298.Jonathan Isbell and Mark H. Butler.
2007.
Extractingand Re-using Structured Data from Wikis.
TechnicalReport HPL-2007-182, Hewlett-Packard.Dan Moldovan, Marius Pasca, Sanda Harabagiu, andMihai Surdeanu .
2002.
Performance Issues and Er-ror Analysis in an Open-Domain Question Answer-ing System.
Proc.
of the 40thACL, pp.
33-40.Christof Monz.
2003.
From Document Retrieval toQuestion Answering.
Ph.D. Thesis.
University ofAmsterdam.Ellen M. Voorhees and Dawn Tice.
2000.
Building aQuestion Answering Test Collection.
In 23rdACMSIGIR, pp.
200-207.14
