Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 55?64,Dublin, Ireland, August 23-29 2014.Developing an interlingual translation lexicon using WordNetsand Grammatical FrameworkShafqat Mumtaz VirkUniversity of Gothenburg,University of Eng.
& Tech.
Lahorevirk.shafqat@gmail.comK.V.S.
PrasadChalmers University of Technologyprasad@chalmers.seAarne RantaUniversity of Gothenburgaarne@chalmers.seKrasimir AngelovUniversity of Gothenburgkrasimir@chalmers.seAbstractThe Grammatical Framework (GF) offers perfect translation between controlled subsetsof natural languages.
E.g., an abstract syntax for a set of sentences in school mathematicsis the interlingua between the corresponding sentences in English and Hindi, say.
GF?resource grammars?
specify how to say something in English or Hindi; these are re-used with ?application grammars?
that specify what can be said (mathematics, touristphrases, etc.).
More recent robust parsing and parse-tree disambiguation allow GF toparse arbitrary English text.
We report here an experiment to linearise the resultingtree directly to other languages (e.g.
Hindi, German, etc.
), i.e., we use a language-independent resource grammar as the interlingua.
We focus particularly on the last partof the translation system, the interlingual lexicon and word sense disambiguation (WSD).We improved the quality of the wide coverage interlingual translation lexicon by usingthe Princeton and Universal WordNet data.
We then integrated an existing WSD tooland replaced the usual GF style lexicons, which give one target word per source word,by the WordNet based lexicons.
These new lexicons and WSD improve the quality oftranslation in most cases, as we show by examples.
Both WordNets and WSD in generalare well known, but this is the first use of these tools with GF.1 Introduction1.1 Translation via an interlinguaInterlingual translation scales easily up to a large number of languages.
Google translate, forexample, deals with all pairs of 60 languages mostly by using English as a pivot language.
Inthis way, it can do with just 2 * 59 = 118 sets of bilingual training data, instead of 60 * 59 =3540 sets.
It would be hard to collect and maintain so many pairs, and in many cases, there isvery little data to be found.The roots of an inter-lingua are perhaps in the medieval idea of a universal grammar (Lyons,1968), in which a universal representation of meaning can be expressed.
Translating via thisinterlingua then also means that meaning is conserved in going from the source to the tar-get language.
In recent decades, this idea appears in (Curry, 1961) where the interlingua iscalled tectogrammar, in the Rosetta project (Rosetta, 1994), building on the semantic modelsof (Montague, 1974), and in the UNL (Universal Networking Language) project.Incidentally, interlingua is also the heart of modern compiler technology.
For instance, theGNU Compiler Collection (Stallman, 2001) uses a shared tree representation to factor out themajority of compilation phases between a large number of source and target languages.
Compilerwriters save work, and semantics is preserved by design.
A compiler, then, is built as a pipelinewith parsing from a source language to an abstract syntax tree, which is analyzed andoptimized in the language-independent phases, and finally linearized to a target language.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers andproceedings footer are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/55It is easy to see an analogy between this pipeline and the way a human language translatorcould work.
But how to make it real?
How to scale up to the full size of natural languages?1.2 WordNetsIn current machine translation research, interlingual methods are marginal, despite the wide useof pivot languages in systems like Google translate.
Closest to the mainstream perhaps is thedevelopment of linked WordNets.
The original Princeton Wordnet for English (Miller, 1995) de-fines a set of word senses, which many other wordnets map to other languages.
Implementationsof this idea are Finnish (Lind?n and Carlson., 2010) and Hindi (Hindi-WordNet, 2012).In the linked Wordnet approach, the Princeton WordNet senses work as an interlingua, albeitonly on the level of the lexicon.
(Lind?n and Carlson., 2010) give strong arguments why in factthis is a good way to go, despite the often emphasized fact that different languages divide theworld in different ways, so that the senses of their word don?t map one to one.
The evidence fromthe English-Finnish case shows that 80% of the mappings are one-to-one and un-problematic.As this part of the lexicon can be easily reused, linguists and system builders can concentratetheir effort on the remaining 20%.The Universal WordNet (de Melo and Weikum, 2009) works on the same lines.
Building onthe Princeton WordNet, it populates the mappings to over 200 different languages by collectingdata from different sources (such as the Wikipedia) and using supervised machine learningtechniques to propagate the knowledge and infer more of it.
What makes it a particularlyinteresting resource is that it is freely available under the most liberal licenses, as is the originalPrinceton WordNet,1.3 GFGrammatical Framework (GF)(Ranta, 2004) is a grammar formalism tool based on MartinL?f?s type theory (Martin-L?f, 1982).
It can be seen as a tool to build interlingua based trans-lation systems.
GF works like a compiler: the source language is parsed to an abstract syntaxtree, which is then linearized to the target language.
The parsing and linearization componentare defined by using Parallel Multiple Context-Free Grammars (PMCFG, (Seki et al., 1991),(Ljungl?f, 2004)), which give GF an expressive power between mildly and fully context-sensitivegrammars.
Thus GF can easily handle with language-specific variations in morphology, wordorder, and discontinuous constituents, while maintaining a shared abstract syntax.Historically, the main use of GF has been in controlled language implementations, e.g., (Rantaand Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural languagegeneration, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15parallel languages.
In recent years, the coverage of GF grammars and the processing performancehas enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translationof patents (Enache et al., 2012).
The general purpose Resource Grammar Library (RGL)(Ranta,2011) has grown to 30 languages.
It includes the major European languages, South Asianlanguages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al.,2011), the Southeast Asian language Thai, and Japanese and Chinese.However, GF has yet not been exploited for arbitrary text parsing and translation.
To dothis, we have to meet several challenges: robust parsing, parse-tree disambiguation, word sensedisambiguation, and development of a wide-coverage interlingual translation lexicon.
This paperfocuses on the latter two.
We report first a method of using the WordNets (Princeton andUniversal) to build an interlingual full-form, multiple sense translation lexicon.
Then, we showhow these lexicons together with a word sense disambiguation tool can be plugged in a translationpipeline.
Finally, we describe an experimental setup and give many examples to highlight theeffects of this work.561.4 South Asian languagesWhile the work described here can apply to any language, it is particularly interesting for SouthAsian languages.
In these languages, statistical tools do not have much bilingual training data towork on, so Google translate and similar tools are not as useful as they are with better resourcedlanguages.
At the same time, there is an urgent and widely recognised need for translations fromEnglish to the various languages of South Asia.
Fortunately, word nets are being built for manyof them, so that the techniques described here can be applied.2 From Universal WordNet to a GF LexiconThe original Princeton WordNet (Miller, 1995) defines a set of word senses, and the UniversalWordNet (de Melo and Weikum, 2009) maps them to different languages.
In this multilingualscenario, the Princeton WordNet senses can be seen as an abstract representation, while theUniversal WordNet mappings can be seen as concrete representation of those senses in differentlanguages.
GF grammars use very much the same technique of one common abstract andmultiple parallel concrete representations to achieve multilingualism.
Due to this compatibility,it is easy to build a multilingual GF lexicon using data from those two resources (i.e.
Princetonand Universal WordNets).
This section briefly describes the experiment we did to build oneabstract and multiple concrete GF lexicons for a number of languages including German, French,Finnish, Swedish, Hindi, and Bulgarian.
The method is very general, so can be used to build asimilar lexicon for any other language for which data is available in the Universal WordNet.2.1 GF Abstract LexiconThe Princeton WordNet data is distributed in the form of different database files.
For each ofthe four lexical categories (i.e.
noun, verb, adjective, and adverb), two files named ?index.pos?and ?data.pos?
are provided, where ?pos?
is noun, verb, adj and adv.
Each of the ?index.pos?files contains all words, including synonyms of the words, found in the corresponding part ofspeech category.
Each of the ?data.pos?
files contains information about unique senses belongingto the corresponding part of speech category.
For our purposes, there were two possible choicesto build an abstract representation of the lexicon:1.
To include all words of the four lexical categories, and also their synonyms (i.e.
to buildthe lexicon from ?index.pos?
files)2.
To include only unique senses of the four categories with one word per sense, but not thesynonyms (i.e.
to build the lexicon from the data.pos?
files)To better understand this difference, consider the words ?brother?
and ?buddy?.
The word?brother?
has five senses with sense offsets ?08111676?, ?08112052?, ?08112961?, ?08112265?
and?08111905?
in the Princeton WordNet 1.7.11, while the word ?buddy?
has only one sense with thesense offset ?08112961?.
Choosing option (1) means that we have to include the following entriesin our abstract lexicon.brother_08111676_Nbrother_08112052_Nbrother_08112961_Nbrother_08112265_Nbrother_08111905_Nbuddy_08112961_NWe can see that the sense with the offset ?08112961?
is duplicated in the lexicon: once withthe lemma ?brother?
and then with the lemma ?buddy?.
However, if we choose option (2), weend up with the following entries.1We choose WordNet 1.7.1, because the word sense disambiguator that we are using in our translation pipelineis based on WordNet 1.7.157brother_08111676_Nbrother_08112052_Nbrother_08112265_Nbrother_08111905_Nbuddy_08112961_NSince the file ?data.noun?
lists the unique senses rather than the words, their will be noduplication of the senses.
However, the choice has an obvious effect on the lexicon coverage, anddepending on whether we want to use it as a parsing or as a linearization lexicon, the choicebecomes critical.
Currently, we choose option (2) for the following two reasons:1.
The Universal WordNet provides mappings for synsets (i.e.
unique senses) but not for theindividual synonyms of the synsets.
If we choose option (1), as mentioned previously, wehave to list all synonyms in our abstract representation.
But, as translations are availableonly for synsets, we have to put the same translation against each of the synonyms of thesynset in our concrete representations.
This will not gain us anything (as long as we usethese lexicon as linearization lexicons), but will increase the size of the lexicon and hencemay have reduce the processing speed of the translation system.2.
At the current stage of our experiments we are using these lexicons as linearization lexicons,so one translation of each unique sense is enough.Our abstract GF lexicon covers 91516 synsets out of around 111,273 synsets in the WordNet1.7.1.
We exclude some of the synsets with multi-word lemmas.
We consider them more of asyntactic category rather than a lexical category, and hence deal with them at the syntax level.Here, we give a small segment of our abstract GF lexicon.abstract LinkedDictAbs = Cat ** {fun consecutive_01624944_A : A ;fun consequently_00061939_Adv : Adv ;fun conservation_06171333_N : N ;fun conspire_00562077_V : V ;fun sing_01362553_V2 : V2 ;........}The first line in the above given code states that the module ?LinkedDictAbs?
is an abstractrepresentation (note the keyword ?abstract?).
This module extends (achieved by ?**?
operator)another module labeled ?Cat2?
which, in this case, has definitions for the morphological categories?A?, ?Adv?, ?N?
and ?V?.
These categories correspond to the ?adjective?, ?adverb?, ?noun?, and ?verb?categories in the WordNet respectively.
However, note that in GF resource grammars we havea fine-grained morphological division for verbs.
We sub-categorize them according to theirvalencies i.e ?V?
is for intransitive, and ?V2?
for transitive verbs.
We refer to (Bringert et al.,2011) for more details on these divisions.Each entry in this module is of the following general type:fun lemma_senseOffset_t : t ;Keyword ?fun?
declares each entry as a function of the type ?t?.
The function name is composedof lemma, sense offset and a type ?t?, where lemma and sense offset are same as in the PrincetonWordNet, while ?t?
is one of the morphological types in GF resource grammars.This abstract representation will serve as a pivot for all concrete representations, which aredescribed next.2This module has definitions of different morphological and syntactic categories in the GF resource grammarlibrary582.2 GF Concrete LexiconsWe build the concrete representations for different languages using the translations obtainedfrom the Universal WordNet data and GF morphological paradigms (D?trez and Ranta, 2012;Bringert et al., 2011).
The Universal WordNet translations are tagged with a sense offset fromWordNet 3.03 and also with a confidence score.
As, an example consider the following segmentform the Universal WordNet data, showing German translations for the noun synset with offset?13810818?
and lemma ?rest?
(in the sense of ?remainder?
).n13810818 Rest 1.052756n13810818 Abbrand 0.95462n13810818 Ruckstand 0.924376Each entry is of the following general type.posSenseOffset translation confidence-scoreIf we have more than one candidate translation for the same sense (as in the above case),we select the best one (i.e.
with the maximum confidence score) and put it in the concretegrammar.
Next, we give a small segment from the German concrete lexicon.concrete LinkedDictGer of LinkedDictAbs = CatGer ** openParadigmsGer, IrregGer,Prelude in {lin consecutive_01624944_A = mkA "aufeinanderfolgend" ;lin consequently_00061939_Adv = mkAdv "infolgedessen" ;lin conservation_06171333_N = mkN "Konservierung" ;lin conspire_00562077_V = mkV "anzetteln" ;lin sing_01362553_V2 = mkV2 (mkV "singen" ) ;......}The first line declares ?LinkedDictGer?
to be the concrete representation of the previouslydefined abstract representation (note the keyword ?concrete?
at the start of the line).
Each entryin this representation is of the following general type:lin lemma_senseOffset_t = paradigmName "translation" ;Keyword ?lin?
declares each entry to be a linearization of the corresponding function in theabstract representation.
?paradigmName?
is one of the morphological paradigms defined in the?ParadigmsGer?
module.
So in the above code, ?mkA?, ?mkAdv?, ?mkN?, ?mkV?
and ?mkV2?
arethe German morphological paradigms4 for different lexical categories of ?adjective?, ?adverb?,?noun?, ?intransitive verb?, and ?transitive verb?
respectively.
?translation?
is the best possibletranslation obtained from the Universal WordNet.
This translation is passed to a paradigm asa base word, which then builds a full-form inflection table.
These tables are then used in thelinearization phase of the translation system (see section 3)Concrete lexicons for all other languages were developed using the same procedure.
Table 1gives some statistics about the coverage of these lexicons.Language Number of Entries Language Number of EntriesAbstract 91516 German 49439French 38261 Finnish 27673Swedish 23862 Hindi 16654Bulgarian 12425Table 1: Lexicon Coverage Statistics3However, in our concrete lexicons we match them to WordNet 1.7.1 for the reasons mentioned previously4See (Bringert et al., 2011) for more details on these paradigms593 System architectureFigure 1 shows an architecture of the translation pipeline.
The architecture is inter-lingualand uses the Resource Grammar Library (RGL) of Grammatical Framework (Ranta, 2011) asthe syntax and semantics component, Penn Treebank data for parse-tree disambiguation andIMS(It Makes Sense)(Zhong and Ng, 2010) as a word sense disambiguation tool.
Even thoughthe syntax, semantics and parse-tree disambiguation are not the main topics of this paper,we give the full architecture to show where the work reported in this paper fits.
Internal GFresources (e.g.
resource grammars and dictionaries) are shown in rectangles while the externalcomponents (e.g.
PennTreebank and IMS(Zhong and Ng, 2010): a wide coverage word sensedisambiguation system for arbitrary text.)
are shown in double-stroked rectangles.With reference to Figure 1: The input is parsed using English resource grammar (EngRG)and a comprehensive English dictionary (DictEng).
If the input is syntactically ambiguous theparser will return more than one parse-tree.
These trees are disambiguated using a statisticalmodel build from the PennTreebank data.
The best tree is further processed using the inputfrom the IMS to tag the lexical nodes with best sense identifiers.
This tree is finally linearizedto the target language using the target language resource grammar (TLRG) together with thetarget language lexicon (LinkedDict) discussed in section 2.InputParsingEngRG+DictEngParse-TreesParse TreeDisambiguationBest-TreeWordSenseDisambiguationLinearizationSense-Tagged-TreeIMSTLRG+LinkedDictOutputPenn TreebankEngRG: English Resource GrammarTLRG: Target Language Resource GrammarFigure 1: The translation pipeline.4 Experimental Setup and EvaluationOur experimental setup is as follows: We take some English text as source, and translate it to atarget language (German and Hindi in these experiments) by passing it through the translationpipeline described in section 3.
To show the usefulness of the lexicons described in section 2 andfor comparison, we translate the same source twice: with and without word sense disambiguation.For the first attempt, we used exactly the same translation pipeline as shown in Figure 1,except that to overcome the deficiencies of our existing parse-tree disambiguator, for some ofthe examples, we used trees directly from the PennTreebank, which are supposed to be correct.However, this should not damage the claims made in this paper which is about developingwide coverage interlingual translation lexicons and then using them for WSD in an interlingualtranslation pipeline.For the second attempt, we plugged out the word sense disambiguation form the translationpipeline and used our old GF style lexicons (one target word per source word irrespective of itssense) in the linearization phase.Finally, we compared both candidate translations to see if we have gained anything.
We didboth manual and automatic evaluations to confirm our findings.For a set of 25 sentences for English-German pair we got marginal BLEU score improvements(from 0.3904 to 0.399 with ?old?
and ?new?
dictionaries).
Manual inspection, however, was muchmore encouraging, and explained the reasons for very low improvements in the BLEU scores insome cases.
The reason was that even if the word sense disambiguation, and hence, our new60lexicon gives a better lexical choice, it will still be considered ?wrong?
by the evaluation tool if thegold-standard has a different choice.
It was also observed that there were cases where the ?old?lexicon produced a much better translation than the ?new?
one.
The reasons for this are obvious.The word sense disambiguator has its own limitations and is known to make mistakes.
Also, asexplained in Section 5, the lexicon cannot be guaranteed to always give the right translation.Next, we give a number of example sentence with comments5 to show that how the newlexicons improved the quality of translations, and also give some examples where it worked theother way around.4.1 German1.
Source He increases the board to sevenWithout WSD er erh?ht das Brett nach einigen siebenWith WSD er vergr?
?ert die Beh?rde nach einigen siebenComments das Brett is a wooden board (wrong); erh?ht means ?to raise?.
whilevergr?
?ert means ?increases the size?.
Note the wrong preposition choice (?to?
shouldbe zu rather than nach).
Also, an indefinite determiner (einige, some) has beenwrongly added to the cardinal number is used as a noun phrase.2.
Source the index uses a base of 100 in 1,982Without WSD das Verzeichnis verwendet eine Base nach einige 100 in einigen1982With WSD der [index_11688271_N] nutzt einen Operationsbasis von einigen100 in einigen 1982Comments Note the untranslated word in the WSD version.
Base means a chemical base,the wrong meaning here.
Operationsbasis is not the best choice, but acceptable.3.
Source fear is the father of panicWith WSD Angst ist der Papa von AngstComment The traditional hilarious example, saying ?fear is fear?s daddy?.4.2 HindiTo represent Hindi, we use an IPA style alphabet, with the usual values and conventions.Retroflexed sounds are written with a dot under the letter: ?, ?, and ?
(a flap) are com-mon, while ?
and ?
occur in Sanskritised Hindi (though many dialects pronounce them n and?).
The palatalised spirant is shown ?
and aspirated stops are shown thus: kh.
A macron over avowel denotes a long vowel, and ?, nasalisation.
In Hindi, e and o are always long, so the macronis dropped.
Finally, we use ?
to mean the nasal homorganic with the following consonant.Here are examples from our evaluation showing that the WSD system works well; the versionswithout WSD merely pick the first synonym in the lexicon.1.
Source Mr Baris is a lawyer in New York .Without WSD Mr Baris New York m?
k?n?n k?
pa?
?it h?With WSD Mr Baris New York m?
vak?l h?Word order Mr Baris New York in lawyer isComments k?n?n k?
pa?
?it is ?expert/teacher in law?, while vak?l means ?lawyer?.2.
Source we don?t depend on pharmaceutical companies for our supportWithout WSD ham au?adh?ya sahy?g?
par ham?re bhara?
p??a?
ke liye nah?
?nirte h?
?.5For the comments on German, we are indebted to Erzsebet Galgoczy and Wolfgang Ahrendt, our colleaguesand German informants.61With WSD ham au?adh?ya ka?pan?
par ham?re nirv?h vyay ke liye nah??
?te h?
?.Word order We pharmaceutical companies on our subsistence expenditure fornot ???
doComments sahy?g?
means ?company?
in the sense of ?colleagues?, nirv?h vyay means?subsistence expenditure?
, while bhara?
p??a?
means ?weight bearing?.
The penul-timate word in both versions is nonsense, and the lexicons need to be debugged.3.
Source you may recall that a triangle is also a polygonWithout WSD tum "recall may" ho ki ?r?yengl "also" bahubhuj h?With WSD tum smara?
kar sakte ho ki triko?
bh?
bahubhuj h?Word order You recall do can that triangle also polygon isComments The version without WSD has several missing words.
The WSD version of?recall?
is not idiomatic, but understandable.It should be noted that the coverage of the Hindi lexicon is lowest of all the lexicons givenin Table 1.
The result is that many sentences have missing words in the translations.
Also,there is considerable interference with Urdu words (some stemming from the shared basegrammar (Prasad and Shafqat, 2012)).
Further, some mappings coming from the UniversalWordNet data are in roman, as opposed to Devanagari (the usual script for Hindi, and whatthe grammar is based on), so these need to be transcribed.
Finally, idiomatic phrases area problem (?before the law?
is likely to be rendered ?
(temporally) before the law?
ratherthan ?in the eyes of the law?
).5 The next stepsSince the Universal WordNet mappings are produced from parallel data by machine learningtechniques, the translations are not always accurate and do not always make the best possiblechoice.
This leaves a window for improvement in the quality of the reported lexicons.
Oneway of improvement is the manual inspection/correction, not an easy task for a wide-coveragelexicon with around 100 thousand entries, but not impossible either.
This would be a one-timetask with a strong impact on the quality of the lexicon.
Another way is to use manually builtWordNets, such as the Finnish and Hindi WordNets.
In our work, the availability of some ofthese resources was an issue, so we leave it for the future.
Further, as mentioned in Section 4,the Hindi lexicon has some script-related issues which should be fixed in future.When it comes to interlingua-based arbitrary machine translation, an important concern isthe size of lexicons.
We are aware of the fact that the size of our lexicons is not comparable tosome of the other similar systems such as ATLAS-II (Fujitsu), where the size of lexicons is inmillions.
We have plan to extend the size of lexicons using some of the other publicly availableresources (such as Hindi WordNet) and/or using parallel corpus.
The development of bilinguallexicons form parallel corpus have been previously explored (Delpech et al., 2012; Qian et al.,2012), and the same ideas can be applied in our case.6 ConclusionWe have shown how to use existing lexical resources such as WordNets to develop an interlingualtranslation lexicon in GF, and how to use it for the WSD task in an arbitrary text translationpipeline.
The improvements in the translation quality (lexical), shown by examples in Section4, are encouraging and motivate further work in this direction.
However, it should be notedthat there is still a lot of work to be done (especially in the open domain text parsing andparse-tree disambiguation phases of the translation pipeline) to bring the translation system toa competitive level.
For the reasons noted in the introduction, we expect our techniques to beparticularly useful for South Asian languages.62ReferencesAngelov, K. (2011).
The Mechanics of the Grammatical Framework.
PhD thesis, Chalmers UniversityOf Technology.
ISBN 978-91-7385-605-8.Angelov, K. and Enache, R. (2010).
Typeful Ontologies with Direct Multilingual Verbalization.
In Fuchs,N.
and Rosner, M., editors, CNL 2010, Controlled Natural Language.Bringert, B., Hallgren, T., and Ranta., A.
(2011).
GF resource grammar library synopsis.www.grammaticalframework.org/lib/doc/synopsis.html.Curry, H. B.
(1961).
Some logical aspects of grammatical structure.
In Jakobson, R., editor, Structure ofLanguage and its Mathematical Aspects: Proceedings of the Twelfth Symposium in Applied Mathematics,pages 56?68.
American Mathematical Society.de Melo, G. and Weikum, G. (2009).
Towards a Universal Wordnet by learning from combined evidence.In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM 2009),pages 513?522, New York, NY, USA.
ACM.Delpech, E., Daille, B., Morin, E., and Lemaire, C. (2012).
Extraction of domain-specific bilingual lexiconfrom comparable corpora: Compositional translation and ranking.
In Proceedings of COLING 2012,pages 745?762, Mumbai, India.
The COLING 2012 Organizing Committee.D?trez, G. and Ranta, A.
(2012).
Smart paradigms and the predictability and complexity of inflectionalmorphology.
In EACL, pages 645?653.Dymetman, M., Lux, V., and Ranta, A.
(2000).
XML and multilingual document authoring: Conver-gent trends.
In Proc.
Computational Linguistics COLING, Saarbr?cken, Germany, pages 243?249.International Committee on Computational Linguistics.Enache, R., Espa?a-Bonet, C., Ranta, A., and M?rquez, L. (2012).
A hybrid system for patent translation.In Proceedings of the 16th Annual Conference of the European Association for Machine Translation(EAMT12), Trento, Italy.Hindi-WordNet (2012).
Hindi Wordnet.
2012.
Universal Word?Hindi Lexicon.http://www.cfilt.iitb.ac.in.Lind?n, K. and Carlson., L. (2010).
Finnwordnet?wordnet p?
finska via ?vers?ttning.
Lexi-coNordica?Nordic Journal of Lexicography, 17:119?140.Ljungl?f, P. (2004).
The Expressivity and Complexity of Grammatical Framework.
PhD thesis, Dept.
ofComputing Science, Chalmers University of Technology and Gothenburg University.
http://www.cs.chalmers.se/~peb/pubs/p04-PhD-thesis.pdf.Lyons, J.
(1968).
Introduction to theoretical linguistics.
Cambridge: Cambridge University Press.Martin-L?f, P. (1982).
Constructive mathematics and computer programming.
In Cohen, Los, Pfeif-fer, and Podewski, editors, Logic, Methodology and Philosophy of Science VI, pages 153?175.
North-Holland, Amsterdam.Miller, G. A.
(1995).
Wordnet: A lexical database for English.
Communications of the ACM, 38:39?41.Montague, R. (1974).
Formal Philosophy.
Yale University Press, New Haven.
Collected papers editedby Richmond Thomason.Prasad, K. V. S. and Shafqat, M. V. (2012).
Computational evidence that Hindi and Urdu share agrammar but not the lexicon.
In The 3rd Workshop on South and Southeast Asian NLP, COLING.Qian, L., Wang, H., Zhou, G., and Zhu, Q.
(2012).
Bilingual lexicon construction from comparablecorpora via dependency mapping.
In Proceedings of COLING 2012, pages 2275?2290, Mumbai, India.The COLING 2012 Organizing Committee.Ranta, A.
(2004).
Grammatical Framework: A Type-Theoretical Grammar Formalism.
The Journal ofFunctional Programming, 14(2):145?189.
http://www.cse.chalmers.se/~aarne/articles/gf-jfp.pdf.Ranta, A.
(2011).
Grammatical Framework: Programming with Multilingual Grammars.
CSLI Publica-tions, Stanford.
ISBN-10: 1-57586-626-9 (Paper), 1-57586-627-7 (Cloth).63Ranta, A. and Angelov, K. (2010).
Implementing Controlled Languages in GF.
In Proceedings of CNL-2009, Athens, volume 5972 of LNCS, pages 82?101.Ranta, A., D?trez, G., and Enache, R. (2012).
Controlled language for everyday use: the MOLTOphrasebook.
In CNL 2012: Controlled Natural Language, volume 7175 of LNCS/LNAI.Rosetta, M. T. (1994).
Compositional Translation.
Kluwer, Dordrecht.Seki, H., Matsumura, T., Fujii, M., and Kasami, T. (1991).
On multiple context-free grammars.
Theo-retical Computer Science, 88:191?229.Shafqat, M., Humayoun, M., and Aarne, R. (2011).
An open source Punjabi resource grammar.
In Pro-ceedings of the International Conference Recent Advances in Natural Language Processing 2011, pages70?76, Hissar, Bulgaria.
RANLP 2011 Organising Committee.
http://aclweb.org/anthology/R11-1010.Stallman, R. (2001).
Using and Porting the GNU Compiler Collection.
Free Software Foundation.Zhong, Z. and Ng, H. T. (2010).
It makes sense: A wide-coverage word sense disambiguation systemfor free text.
In Proceedings of the ACL 2010 System Demonstrations, pages 78?83, Uppsala, Sweden.Association for Computational Linguistics.
http://www.aclweb.org/anthology/P10-4014.64
