Proceedings of the SIGDIAL 2014 Conference, pages 60?68,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsLearning non-cooperative dialogue behavioursIoannis EfstathiouInteraction LabHeriot-Watt Universityie24@hw.ac.ukOliver LemonInteraction LabHeriot-Watt Universityo.lemon@hw.ac.ukAbstractNon-cooperative dialogue behaviour hasbeen identified as important in a vari-ety of application areas, including educa-tion, military operations, video games andhealthcare.
However, it has not been ad-dressed using statistical approaches to di-alogue management, which have alwaysbeen trained for co-operative dialogue.We develop and evaluate a statistical dia-logue agent which learns to perform non-cooperative dialogue moves in order tocomplete its own objectives in a stochas-tic trading game.
We show that, whengiven the ability to perform both coopera-tive and non-cooperative dialogue moves,such an agent can learn to bluff and to lieso as to win games more often ?
againsta variety of adversaries, and under var-ious conditions such as risking penaltiesfor being caught in deception.
For exam-ple, we show that a non-cooperative dia-logue agent can learn to win an additional15.47% of games against a strong rule-based adversary, when compared to an op-timised agent which cannot perform non-cooperative moves.
This work is the firstto show how an agent can learn to use non-cooperative dialogue to effectively meetits own goals.1 IntroductionResearch in automated conversational systems hasalmost exclusively focused on the case of coopera-tive dialogue, where a dialogue system?s core goalis to assist humans in particular tasks, such as buy-ing airline tickets (Walker et al., 2001) or findinga place to eat (Young et al., 2010).
Gricean co-operative principles have been shown to emergefrom multi-agent decision theory, in a languagetask modelled using Decentralised Partially Ob-servable Markov Decision Processes (Vogel et al.,2013a), and in related work conversational impli-cature was argued to be a by-product of agentswho maximise joint utility (Vogel et al., 2013b).However, non-cooperative dialogues, where anagent may act to satisfy its own goals rather thanthose of other participants, are also of practi-cal and theoretical interest (Georgila and Traum,2011), and the game-theoretic underpinnings ofnon-Gricean behaviour are actively being investi-gated (Asher and Lascarides, 2008).
For example,it may be advantageous for an automated agent notto be fully cooperative when trying to gather in-formation from a human, and when trying to per-suade, argue, or debate, when trying to sell themsomething, when trying to detect illegal activity(for example on internet chat sites), or in the areaof believable characters in video games and edu-cational simulations (Georgila and Traum, 2011;Shim and Arkin, 2013).
Another arena in whichnon-cooperative dialogue behaviour is desirable isin negotiation (Traum, 2008), where hiding infor-mation (and even outright lying) can be advanta-geous.
Furthermore, deception is considered to bean essential part of successful military operations.According to Sun Tzu ?All warfare is based on de-ception?
and Machiavelli clearly states in The Dis-courses that ?Although deceit is detestable in allother things, yet in the conduct of war it is laud-able and honorable?
(Arkin, 2010).
Indeed, Den-nett argues that deception capability is required forhigher-order intentionality in AI (Dennett, 1997).A complementary research direction in recentyears has been the use of machine learning meth-ods to automatically optimise cooperative dia-logue management - i.e.
the decision of what di-alogue move to make next in a conversation, inorder to maximise an agent?s overall long-term ex-pected utility, which is usually defined in terms ofmeeting a user?s goals (Young et al., 2010; Rieser60and Lemon, 2011).
This research has shown howrobust and efficient dialogue management strate-gies can be learned from data, but has only ad-dressed the case of cooperative dialogue.
Theseapproaches use Reinforcement Learning with a re-ward function that gives positive feedback to theagent only when it meets the user?s goals.An example of the type of non-cooperative dia-logue behaviour which we are generating in thiswork is given by agent B in the following dia-logue:A: ?I will give you a sheep if you give me a wheat?B: ?No?B: ?I really need rock?
[B actually needs wheat]A: ?OK...
I?ll give you a wheat if you give merock?B: ?OK?Here, A is deceived into providing the wheatthat B actually needs, because A believes that Bneeds rock rather than wheat.
Similar behaviourcan be observed in trading games such as Settlersof Catan (Afantenos et al., 2012).1.1 Non-cooperative dialogue andimplicatureOur trading dialogues are linguistically coop-erative (according to the Cooperative Principle(Grice, 1975)) since their linguistic meaning isclear from both sides and successful informationexchange occurs.
Non-linguistically though theyare non-cooperative, since they they aim for per-sonal goals.
Hence they violate Attardo?s Per-locutionary Cooperative Principle (PCP) (Attardo,1997).
In our non-cooperative environment, themanipulative utterances such as ?I really needsheep?
can imply that ?I don?t really need any ofthe other two resources?, as both of the players arefully aware that three different resources exist intotal and more than one is needed to win the game,so therefore they serve as scalar implicatures (Vo-gel et al., 2013b).
Hence we will show that theLA learns how to include scalar implicatures inits dialogue to successfully deceive its adversaryby being cooperative on the locutionary level andnon-cooperative on the perlocutionary level.1.2 Structure of the paperIn this paper we investigate whether a learn-ing agent endowed with non-cooperative dialoguemoves and a ?personal?
reward function can learnhow to perform non-cooperative dialogue.
Notethat the reward will not be given for perform-ing non-cooperative moves themselves, but onlyfor winning trading games.
We therefore explorewhether the agent can learn the advantages of be-ing non-cooperative in dialogue, in a variety ofsettings.
This is similar to (Vogel et al., 2013a)who show how cooperativity emerges from multi-agent decision making, though in our case weshow the emergence of non-cooperative dialoguebehaviours.We begin with the case of a simple but challeng-ing 2-player trading game, which is stochastic andinvolves hidden information.In section 2 we describe and motivate the trad-ing game used in this work, and in section 3 wedescribe the Learning Agent.
In section 4 we ex-plain the different adversaries for experimentation,in section 5 we provide results, and in section 6 weconclude and discuss areas for future work.2 The Trading GameTo investigate non-cooperative dialogues in a con-trolled setting we created a 2-player, sequential,non-zero-sum game with imperfect informationcalled ?Taikun?.
Motivated by the principle ofOccam?s razor we shaped this game as simply aspossible, while including the key features of a re-source trading game.
The precise goal was also toimplement mechanics that are not restrictive forthe future of this research and therefore can beflexibly extended to capture different aspects oftrading and negotiation.
We call the 2 players the?adversary?
and the ?learning agent?
(LA).The two players can trade three kinds of re-sources with each other sequentially, in a 1-for-1manner, in order to reach a specific number of re-sources that is their individual goal.
The playerwho first attains their goal resources wins.
Bothplayers start the game with one resource of eachtype (wheat, rock, and sheep).
At the beginningof each round the game updates the number of re-sources of both players by either removing one ofthem or adding two of them, thereby making theopponent?s state (i.e.
the cards that they hold) un-observable.
In the long run, someone will eventu-ally win even if no player ever trades.
However,effective trading can provide a faster victory.2.1 ?Taikun?
game characteristicsTaikun is a sequential, non-cooperative, non-zero-sum game, with imperfect information, where:61?
The goal is to reach either 4 or 5 of twospecific resources (4 wheat and 5 rocksfor the learning agent and 4 wheat and5 sheep for the adversary).
The playersshare a goal resource (wheat).?
Each round consists of an update of re-sources turn, the learning agent?s trad-ing proposal turn (and adversary?s ac-ceptance or rejection), and finally theadversary?s trading proposal turn (andLA?s acceptance or rejection).?
The update turn, which is a hidden ac-tion, changes one of the resources ofeach player at random by +2 or -1.?
When a resource is ?capped?, that is ifits number is 5 or more, then no updaterule can be applied to it.
Trading canstill change its quantity though.2.2 Actions (Trading Proposals)Trade occurs through trading proposals that maylead to acceptance from the other player.
In anagent?s turn only one ?1-for-1?
trading proposalmay occur, or nothing (7 actions in total):1.
I will do nothing2.
I will give you a wheat if you give me a rock3.
I will give you a wheat if you give me a sheep4.
I will give you a rock if you give me a wheat5.
I will give you a rock if you give me a sheep6.
I will give you a sheep if you give me a wheat7.
I will give you a sheep if you give me a rockAgents respond by either saying ?No?
or ?OK?in order to reject or accept the other agent?s pro-posal.2.3 Non-cooperative dialogue movesIn our second experiment three manipulative ac-tions are added to the learning agent?s set of ac-tions:1.
?I really need wheat?2.
?I really need rock?3.
?I really need sheep?The adversary believes these statements, result-ing in modifying their probabilities of making cer-tain trades.Note that in the current model we assume thatonly these 3 manipulative actions potentially havean effect on the adversary?s reasoning about thegame.
An alternative would be to allow allthe trading utterances to have some manipulativepower.
For example the LA?s uttering ?I will giveyou a wheat if you give me a rock?
could lead theadversary to believe that the LA currently needsrock.
For the present work, we prefer to sepa-rate out the manipulative actions explicitly, so asto first study their effects in the presence of non-manipulative dialogue actions.
In future work, wewill consider the case where all trading proposalscan cause adversaries to change their game strat-egy.3 The Learning Agent (LA)The game state can be represented by the learningagent?s set of resources, its adversary?s set of re-sources, and a trading proposal (if any) currentlyunder consideration.
We track up to 19 of eachtype of resource, and have a binary variable repre-senting whose turn it is.
Therefore there are 20 x20 x 20 x 2 = 16,000 states.The learning agent (LA) plays the game andlearns while perceiving only its own set of re-sources.
This initial state space can later be ex-tended with elements of history (previous dialoguemoves) and estimates of the other agent?s state(e.g.
beliefs about what the adversary needs).The LA is aware of its winning condition (to ob-tain 4 wheat and 5 rocks) in as much as it experi-ences a large final reward when reaching this state.It learns how to achieve the goal state throughtrial-and-error exploration while playing repeatedgames.The LA is modelled as a Markov Decision Pro-cess (Sutton and Barto, 1998): it observes states,selects actions according to a policy, transitions toa new state (due to the adversary?s move and/or aupdate of resources), and receives rewards at theend of each game.
This reward is then used to up-date the policy followed by the agent.The rewards that were used in these experi-ments were 1,000 for the winning case, 500 fora draw and -100 when losing a game.
The win-ning and draw cases have the same goal states andthat would initially suggest the same reward butthey can be achieved through different strategies.Experiments that we have conducted using eitherthe above rewards or the same rewards for win and62draw have verified this.
The learning agent?s per-formance is slightly better when the reward for awin is 1000 and 500 for a draw.The LA was trained using a custom SARSA(?
)learning method (Sutton and Barto, 1998) with aninitial exploration rate of 0.2 that gradually decaysto 0 at the end of the training games.
After exper-imenting with the learning parameters we foundthat with ?
equal to 0.4 and ?
equal to 0.9 we ob-tain the best results for our problem and thereforethese values have been used in all of the experi-ments that follow.4 AdversariesWe investigated performance with several differ-ent adversaries.
As a baseline, we first needto know how well a LA which does not havenon-cooperative moves at its disposal can per-form against a rational rule-based adversary.
Ourhypothesis is then that a LA with additionalnon-cooperative moves can outperform this base-line case when the adversary becomes somewhatgullible.A ?gullible?
adversary is one who believes state-ments such as ?I really need rock?
and then acts soas to restrict the relevant resource(s) from the LA.Our experiments (see experiments 3.1-3.3) showthat this gullible behaviour may originate fromsound reasoning.
The adversary confronts in thiscase a very important dilemma.
It suddenly doesnot know if it should stay with its goal-orientedstrategy (baseline) or instead it should boycott theLA?s stated needed resources.
A priori, both ofthese strategies might be equally successful, andwe will show that their performances are indeedvery close to each other.4.1 Rule-based adversary: experiment 1This strategy was designed to form a challengingrational adversary for measuring baseline perfor-mance.
It cannot be manipulated at all, and non-cooperative dialogue moves will have no effect onit ?
it simply ignores statements like ?I really needwheat?.The strict rule-based strategy of the adversarywill never ask for a resource that it does not need(in this case rocks).
Furthermore, if it has an avail-able non-goal resource to give then it will offer it.It only asks for resources that it needs (goal re-sources: wheat and sheep).
In the case where itdoes not have a non-goal resource (rocks) to offerthen it offers a goal resource only if its quantityis more than it needs, and it asks for another goalresource if it is needed.Following the same reasoning, when replyingto the LA?s trading proposals, the adversary willnever agree to receive a non-goal resource (rock).It only gives a non-goal resource (rock) for anotherone that it needs (wheat or sheep).
It also agrees tomake a trade in the special case where it will givea goal resource of which it has more than it needsfor another one that it still needs.
This is a strongstrategy that wins a significant number of games.In fact, it takes about 100,000 training games be-fore the LA is able to start winning more gamesthan this adversary, and a random LA policy loses66% of games against this adversary (See Table 1,LA policy ?Random?
).4.2 Gullible adversary: experiment 2The adversary in this case retains the above strictbase-line policy but it is also susceptible to thenon-cooperative moves of the LA, as explainedabove.
For example, if the LA utters ?I really needrock?, weights of actions which transfer rock fromthe adversary will decrease, and the adversary willthen be less likely to give rock to the LA.
Con-versely, the adversary is then more likely to givethe other two resources to the LA.
In this way theLA has the potential to mislead the adversary intotrading resources that it really needs.4.3 The restrictive adversaries: experiments3.1, 3.2, 3.3Here we investigate performance against adver-saries who cannot be manipulated, but their strat-egy is to always restrict the LA from gaining a spe-cific type of resource.
We need to explore howwell a manipulated adversary (for example onewho will no longer give rocks that only its op-ponent needs) performs.
This will show us thepotential advantage to be gained by manipulationand most important, it will generalise our prob-lem by showing that the restriction (boycott) of aresource that only the opponent needs, or of a re-source that both of the players need, are actuallyreasonably good strategies compared to the base-line case (Experiment 1).
Hence, the manipulatedadversary has indeed a reason for choosing to re-strict resources (Experiment 2) rather than stayingwith its rule-based strategy.
In other words it hasa rational reason to become gullible and fall in thelearning agent?s trap.634.4 Gullible-based adversary with risk ofexposure: experiments 4.1, 4.2Here we extend the problem to include possi-ble negative consequences of manipulative LA ac-tions.
The adversary begins each game with aprobability of detecting manipulation, that expo-nentially increases after every one of the LA?s ma-nipulative actions.
In more detail, every time theLA performs a manipulation, there is an additionalchance that the adversary notices this (starts at 1-in-10 and increases after every manipulative move,up to 100% in the case of the 10th manipulativeattempt).
The consequence of being detected isthat the adversary will refuse to trade with the LAany further in that game (experiment 4.1), or thatthe adversary automatically wins the game (exper-iment 4.2).
In these two cases there is always arisk associated with attempting to manipulate, andthe LA has to learn how to balance the potentialrewards with this risk.5 ResultsThe LA was trained over 1.5 million games againsteach adversary for the cases of the rule-based (ex-periment 1), gullible (experiment 2) and restrictiveadversaries (experiments 3.1, 3.2, 3.3).
The result-ing policies were tested in 20 thousand games.For reasons of time, the LA was trained for only35 thousand games for the case of the gullibleadversary who stops trading when the LA be-comes exposed (experiment 4.1), and 350 thou-sand games for the gullible adversary who winsthe game when the LA becomes exposed (experi-ment 4.2).
In the former case we used 2 thousandtesting games and in the latter 20 thousand.5.1 Baseline performance: Experiment 1The LA scored a winning performance of 49.5%against 45.555% for the adversary, with 4.945%draws (Table 1), in the 20 thousand test games,see Figure 1.
This represents the baseline perfor-mance that the LA is able to achieve against anadversary who cannot be manipulated at all.
Thisshows that the game is ?solvable?
as an MDP prob-lem, and that a reinforcement learning agent canoutperform a strict hand-coded adversary.Here, the learning agent?s strategy mainly fo-cuses on offering the sheep resource that it doesnot need for the rocks that does need (for exampleaction7 > action2 > action6 > action3 Ta-ble 2).
It is also interesting to notice that the LAlearnt not to use action 3 at all (gives 1 wheat thatthey both need for 1 sheep that only the adversaryneeds).
Hence its frequency is 0.
The actions 4 and5 are never accepted by the adversary so their rolein both of the experiments is similar to that of theaction 1 (do nothing).
The rejections of the adver-sary?s trades dominate the acceptances with a ratioof 94 to 1 as our learning agent learns to becomenegative towards the adversarial trading proposalsand therefore to prohibit its strategy.Figure 1: Learning Agent?s reward-victory graphover 1.5 million training games of Experiment 1.5.2 Non-cooperative actions: Experiment 2In Experiment 2 the learning agent scored awinning performance of 59.17% against only39.755% of its adversary, having 1.075% draws(Table 1), in the 20 thousand test games, see Fig-ure 2.Similarly to the previous experiment, the LA?sstrategy focuses again mainly on action 7, by of-fering the sheep resource that it does not need forrocks that it needs (Table 2).
However in this casewe also notice that the LA has learnt to use ac-tion 2 very often, exploiting cases where it willwin by giving the wheat resource that they bothneed for a rock that only it needs.
This is a resultof its current manipulation capabilities.
The highfrequency manipulative actions 8 (?I really needwheat?)
and 9 (?I really need rock?)
assist in de-ceiving its adversary by hiding information, there-fore significantly reinforcing its strategy as theyboth indirectly result in gaining sheep that only theadversary needs (experiment 3.2).Rejections to adversarial trading offers over the64acceptances were again the majority in this exper-iment.
However in this case they are significantlyfewer than before, with a ratio of only 2.5 to 1,as our learning agent is now more eager to acceptsome trades because it has triggered them itself byappropriately manipulating its adversary.Figure 2: Learning Agent?s reward-victory graphin 1.5 million training games of Experiment 2.In Experiment 1 the LA?s dominating strategy(mainly based on requiring the rocks resourcefrom its adversary) provides it with a differencein winning performance of +3.945%.
In Experi-ment 2 the adversary, further being deceived by thelearning agent?s hiding information actions, loses19.415% more often than the learning agent.Action Exp.
1 Exp.
2number frequency frequency1.
Do nothing 81969 1447272.
Give wheat for rock 8077 460283.
Give wheat for sheep 0 103584.
Give rock for wheat 80578 628745.
Give rock for sheep 78542 556276.
Give sheep for wheat 6429 246877.
Give sheep for rock 23888 311328.
I really need wheat - 689749.
I really need rock - 8712310.
I really need sheep - 18Table 2: Frequencies of LA actions.Table 2 shows that the LA?s strategy in Experi-ment 1 mainly focuses on requiring rocks from theadversary by offering sheep (for example action 7> action 2 or 6).
In Experiment 2 the agent?s strat-egy is similar.
However, it is now enhanced by thefrequent use of the manipulative actions 8 and 9(both hide information).
The LA gathers mainlysheep (8 and 9) through its manipulation and thenwheat (9 > 8) that the adversary needs to win.
Italso offers them ?selectively?
back (2 and 7) forrock that only it needs in order to win.5.3 Restrictive adversaries: Experiment 3In experiment 3 the LA uses no manipulative ac-tions.
It is the same LA as that of Experiment 1.
Itis trained and then tested against 3 different typesof restrictive adversaries.
The first one (Experi-ment 3.1) never gives wheat, the second one (Ex-periment 3.2) never gives rocks, and the third onenever gives sheep (Experiment 3.3).
They all actrandomly regarding the other 2 resources whichare not restricted.
In the first case (adversary re-stricts wheat that they both need), the LA scoreda winning performance of 50.015% against 47.9%of its adversary, having 2.085% draws in the 20thousand test games.
In the second case (adver-sary restricts rocks that the LA only needs), the LAscored a winning performance of 53.375% against44.525% of its adversary, having 2.1% draws inthe 20 thousand test games.
In the third case(adversary restricts sheep that only itself needs),the LA scored a winning performance of 62.21%against 35.13% of its adversary, having 2.66%draws in the 20 thousand test games.
These re-sults show that restricting the resource that onlythe opponent needs (i.e.
LA only needs rocks) andespecially the resource that they both need (i.e.wheat) can be as effective as the strategy followedby the rule-based adversary (see Table 1).
The dif-ference in the performances for the former case(rock) is +8.85% and for the latter (wheat) only+2.115%.
That means the adversary has indeeda reason to believe that boycotting its opponent?sresources could be a winning opposing strategy,motivating its gullibility in experiment 2 (section5.2).15.4 Non-cooperative actions and risk ofexposure: Experiment 4.1 (adversarystops trading)In this case when the LA is exposed by the adver-sary then the latter does not trade for the rest of the1Further experiments showed that having the same num-ber of goal resources (i.e.
both need 4 of their own goal re-sources, rather than 5) still produces similar results.65Exp.
Learning Agent policy Adversary policy LA wins Adversary wins DrawsRandom Baseline 32% 66% 2%1 SARSA Baseline 49.5% 45.555% 4.945%2 SARSA + Manipulation Baseline + Gullible 59.17%* 39.755% 1.075%3.1 SARSA Restrict wheat 50.015%* 47.9% 2.085%3.2 SARSA Restrict rock 53.375%* 44.525% 2.1%3.3 SARSA Restrict sheep 62.21%* 35.13% 2.66%4.1 SARSA + Manipulation Basel.
+ Gull.
+ Expos.
(no trade) 53.2%* 45.15% 1.65%4.2 SARSA + Manipulation Basel.
+ Gull.
+ Expos.
(win game) 36.125% 61.15% 2.725%Table 1: Performance (% wins) in testing games, after training.
(*= significant improvement over base-line, p < 0.05)game.
The LA scored a winning performance of53.2% against 45.15% for this adversary, having1.65% draws in the 2 thousand test games, see Fig-ure 3.
This shows that the LA managed to locate asuccessful strategy that balances the use of the ma-nipulative actions and the normal trading actionswith the risk of exposure (Table 3).
In more de-tail, the strategy that the LA uses here makes fre-quent use of the manipulative actions 8 (?I reallyneed wheat?)
and 9 (?I really need rock?)
againwhich mainly result in the collection of sheep thatonly its adversary needs to win.
Restriction of aresource that the opponent only needs is a goodstrategy (as our experiment 3.2 suggests) and theLA managed to locate that and exploit it.
The nexthighest frequency action (excluding actions 4 and5 that mostly lead to rejection from the adversaryas it also follows its rule-based strategy) is 7 (?Iwill give you a sheep if you give me a rock?)
that isexclusively based on the LA?s goal and along with6 they ?selectively?
give back the sheep for goal re-sources.
Rejections to adversary?s proposals overthe acceptances were in a ratio of approximately4 to 1.
The LA is quite eager (in contrast to thebaseline case of experiment 1) to accept the adver-sary?s proposals as it has already triggered themby itself through deception.5.5 Non-cooperative actions and risk ofexposure: Experiment 4.2 (adversarywins the game)In this case if the LA becomes exposed by the ad-versary then the latter wins the game.
The LAscored a winning performance of 36.125% against61.15% of its adversary, having 2.725% draws in20 thousand test games, see Figure 4.
It is theonly case where the LA so far has not yet founda strategy that wins more often than its adversary,and therefore in future work a larger set of traininggames will be used.
Note that this was only trainedfor 350 thousand games ?
we expect better perfor-mance with more training.
In fact, here we wouldexpect a good policy to perform at least as well asexperiment 1, which would be the case of learningnever to use manipulative actions, since they areso dangerous.
Indeed, a good policy could be tolie (action 10) only once, at the start of a dialogue,and then to follow the policy of experiment 2.
Thiswould lead to a winning percentage of about 49%(the 59% of experiment 2 minus a 10% loss for thechance of being detected after 1 manipulation).The LA has so far managed to locate a strat-egy that again balances the use of the manipula-tive actions and that of the normal ones with therisk of losing the game as a result of exposure(Table 3).
According to Figure 4 we notice thatthe LA gradually learns how to do that.
How-ever its performance is not yet desirable, as it isstill only slightly better than that of the Randomcase against the Baseline (Table 1).
It is interest-ing though to see that the strategy that the LA useshere makes frequent use of the action 10 (?I reallyneed sheep?)
that lies.
On the other hand, the ac-tions 8 and 9 are almost non-existent.
That resultsin accepting wheat that they both need and rocksthat it only needs, showing that the main focus ofthe manipulation is on the personal goal.
The LAhas learned so far in this case that by lying it canget closer to its personal goal.
Rejections to adver-sary?s proposals over the acceptances resulted in aratio of approximately 1.7 to 1, meaning that theLA is again quite eager to accept the adversarialtrading proposals that it has triggered already byitself through lying.We report further results on this scenario inan updated version of this paper (Efstathiou and66Lemon, 2014).Action Exp.
4.1 Exp.
4.2number frequency frequency1 Do nothing 8254 741452 Give wheat for rock 2314 35373 Give wheat for sheep 1915 46334 Give rock for wheat 5564 461205 Give rock for sheep 4603 570316 Give sheep for wheat 2639 27377 Give sheep for rock 3132 31058 I really need wheat 7200 49 I really need rock 7577 710 I really need sheep 548 19435Table 3: Frequencies of LA actions.Figure 3: Learning Agent?s reward-victory graphin 35 thousand training games of Experiment 4.1.6 Conclusion & Future WorkWe showed that a statistical dialogue agent canlearn to perform non-cooperative dialogue movesin order to enhance its performance in trad-ing negotiations.
This demonstrates that non-cooperative dialogue strategies can emerge fromstatistical approaches to dialogue management,similarly to the emergence of cooperative be-haviour from multi-agent decision theory (Vogelet al., 2013a).In future work we will investigate more com-plex non-cooperative situations.
For example areal dialogue example of this kind is taken fromFigure 4: Learning Agent?s reward-victory graphin 350 thousand training games of Experiment 4.2.the ?Settlers of Catan?
game corpus (Afantenos etal., 2012):?
A: Do you have rock??
B: I?ve got lots of wheat [in fact, B hasa rock]?
A: I?ll give you 2 clay for a rock?
B: How about 2 clay for a wheat??
A: I?ll give 1 clay for 3 wheat?
B: Ok, it?s a deal.In future more adversarial strategies will also beapplied, and the learning problem will be mademore complex (e.g.
studying ?when?
and ?howoften?
an agent should try to manipulate its ad-versary).
Alternative methods will also be con-sidered such as adversarial belief modelling withthe application of interactive POMDPs (PartiallyObservable Markov Decision Processes) (Gmy-trasiewicz and Doshi, 2005).
The long-term goalof this work is to develop intelligent agents thatwill be able to assist (or even replace) users in in-teraction with other human or artificial agents invarious non-cooperative settings (Shim and Arkin,2013), such as education, military operations, vir-tual worlds and healthcare.ReferencesStergos Afantenos, Nicholas Asher, Farah Benamara,Anais Cadilhac, Cedric Degremont, Pascal De-nis, Markus Guhe, Simon Keizer, Alex Lascarides,Oliver Lemon, Philippe Muller, Soumya Paul, Ver-ena Rieser, and Laure Vieu.
2012.
Developing a67corpus of strategic conversation in The Settlers ofCatan.
In Proceedings of SemDial 2012.R.
Arkin.
2010.
The ethics of robotics deception.
In1st International Conference of International Asso-ciation for Computing and Philosophy, pages 1?3.N.
Asher and A. Lascarides.
2008.
Commitments, be-liefs and intentions in dialogue.
In Proc.
of SemDial,pages 35?42.S.
Attardo.
1997.
Locutionary and perlocutionary co-operation: The perlocutionary cooperative principle.Journal of Pragmatics, 27(6):753?779.Daniel Dennett.
1997.
When Hal Kills, Who?s toBlame?
Computer Ethics.
In Hal?s Legacy:2001?sComputer as Dream and Reality.Ioannis Efstathiou and Oliver Lemon.
2014.
Learn-ing to manage risk in non-cooperative dialogues.
Inunder review.Kallirroi Georgila and David Traum.
2011.
Reinforce-ment learning of argumentation dialogue policies innegotiation.
In Proc.
INTERSPEECH.Piotr J. Gmytrasiewicz and Prashant Doshi.
2005.
Aframework for sequential planning in multi-agentsettings.
Journal of Artificial Intelligence Research,24:49?79.Paul Grice.
1975.
Logic and conversation.
Syntax andSemantics, 3.Verena Rieser and Oliver Lemon.
2011.
Reinforce-ment Learning for Adaptive Dialogue Systems: AData-driven Methodology for Dialogue Manage-ment and Natural Language Generation.
Theoryand Applications of Natural Language Processing.Springer.J.
Shim and R.C.
Arkin.
2013.
A Taxonomy of RobotDeception and its Benefits in HRI.
In Proc.
IEEESystems, Man, and Cybernetics Conference.R.
Sutton and A. Barto.
1998.
Reinforcement Learn-ing.
MIT Press.David Traum.
2008.
Exrended abstract: Computa-tional models of non-cooperative dialogue.
In Proc.of SIGdial Workshop on Discourse and Dialogue.Adam Vogel, Max Bodoia, Christopher Potts, and DanJurafsky.
2013a.
Emergence of Gricean Maximsfrom Multi-Agent Decision Theory.
In Proceedingsof NAACL 2013.Adam Vogel, Christopher Potts, and Dan Jurafsky.2013b.
Implicatures and Nested Beliefs in Approx-imate Decentralized-POMDPs.
In Proceedings ofACL 2013.M.
Walker, R. Passonneau, and J. Boland.
2001.
Quan-titative and qualitative evaluation of DARPA Com-municator spoken dialogue systems.
In Proc.
ofthe Annual Meeting of the Association for Compu-tational Linguistics (ACL).Steve Young, M. Gasic, S. Keizer, F. Mairesse,J.
Schatzmann, B. Thomson, and K. Yu.
2010.
TheHidden Information State Model: a practical frame-work for POMDP-based spoken dialogue manage-ment.
Computer Speech and Language, 24(2):150?174.68
