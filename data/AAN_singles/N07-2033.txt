Proceedings of NAACL HLT 2007, Companion Volume, pages 129?132,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsADVANCES IN THE CMU/INTERACT ARABIC GALE TRANSCRIPTION SYSTEMMohamed Noamany, Thomas Schaaf*, Tanja SchultzInterACT, Language Technologies Institute, Carnegie Mellon UniversityPittsburgh, PA 15213{mfn,tschaaf,tanja@cs.cmu.edu}* Now with Toshiba Research Europe Ltd, Cambridge, United KingdomABSTRACTThis paper describes the CMU/InterACT effort indeveloping an Arabic Automatic Speech Recognition(ASR) system for broadcast news and conversationswithin the GALE 2006 evaluation.
Through the span of9 month in preparation for this evaluation we improvedour system by 40% relative compared to our legacysystem.
These improvements have been achieved byvarious steps, such as developing a vowelized system,combining this system with a non-vowelized one,harvesting transcripts of TV shows from the web forslightly supervised training of acoustic models, as wellas language model adaptation, and finally fine-tuningthe overall ASR system.Index Terms?
Speech recognition, Vowelization,GALE, Arabic, Slightly supervised training, web data.1.
INTRODUCTIONThe goal of the GALE (Global Autonomous LanguageExploitation) program is to develop and apply computersoftware technologies to absorb, analyze and interprethuge volumes of speech and text in multiple languagesand make them available in English.
In a long run thisrequires to combine techniques from textsummarization, information retrieval, machinetranslation, and automatic speech recognition.
NISTwill perform regular evaluations and the first evaluationtook place recently.
This paper describes improvementsin the CMU Modern Standard Arabic (MSA) systemthrough the span of 9 months in preparation for thisevaluation.One of the language characteristics and challenges ofArabic is that some vowels are omitted in the writtenform.
These vowels carry grammatical case informationand may change the meaning of a word.
Modeling thevowels in the pronunciation dictionary was found togive improvements over un-vowelized pronunciations[4].
In this paper we achieved another significantimprovement by combining a vowelized with a non-vowelized system.
Furthermore, we got gains bycollecting and utilizing web transcripts from TV show,which include broadcast conversations.2.
SYSTEM DESCRIPTIONOur MSA speech recognition system is based on theJanus Recognition Toolkit JRTk [9] and the IBISdecoder [10].Before decoding the audio, an automatic segmentationstep and a speaker clustering step is performed.
Thesegmentation step aims at excluding those segments thatcontain no speech, such as music or background noise.The remaining segments are clustered into speakerclusters such that all adaptation and normalization stepscan be processed on clusters as batches.From the incoming 16 kHz audio signal we extract foreach segment power spectral features using a FFT witha 10ms frame-shift and a 16ms Hamming window.
Fromthese we compute 13 Mel-Frequency CepstralCoefficients (MFCC) per frame and perform a cepstralmean as well as variance normalization on a clusterbasis.
To incorporate dynamic features we concatenate15 adjacent MFCC frames (?7) and project these 195dimensional features into a 42 dimensional space usinga transform found by linear discriminate analysis(LDA).
We use the context-dependent codebooks asclasses for finding the LDA transform [2].
On top of theLDA we apply a single maximum likelihood trainedSemi-Tied-Covariance (STC) matrix.The general decoding setup employs a first pass inwhich a speaker independent acoustic model withoutvocal tract length normalization (VTLN) and noadaptation is used.
The hypotheses of a cluster from thefirst pass are then used to estimate the VTLN warpingfactors to warp the power spectrum using the maximumlikelihood approach described in [8].
After the VTLNfactors are found, the same hypotheses are considered toestimate a feature space adaptation (FSA) using aconstrained MLLR (CMLLR) transform.
Then a modelspace adaptation is performed using maximumlikelihood linear regression with multiple regressionclasses.
The regression classes are found throughclustering of the Gaussians in the acoustic model.
Thesecond pass decoding uses a speaker adaptive trained129acoustic model, in which the adaptation was performedusing a single CMLLR transform per speaker.For the non-vowelized system, we applied a grapheme-to-phoneme approach to automatically generate thepronunciation dictionary.
For the vowelized system weused the same phoneme set as in the non-vowelizedsystem but extended it with the 3 short vowels, whichdo not appear in the writing system.
Both systems are 2-pass system as described above and employ CepstralMean Normalization (CMN), MLLR, Semi-tiedcovariance (STC), and Feature space adaptation (FSA).For the development of context dependent acousticmodels we applied an entropy-based polyphonedecision tree clustering process using context questionsof maximum width ?2, resulting in shared quin-phones.In addition we included word-boundary tags into thepronunciation dictionary, which can be asked for in thedecision tree can ask for word-boundary tags.
The non-vowelized system uses 4000 phonetically-tied quin-phones with a total of 305,000 Gaussians.
The non-vowelized system has 5000 codebooks with a total of308,000 Gaussians.In total we used 190 hours for acoustic training.These consist of 40 hours Broadcast news (BN) frommanually transcribed FBIS data, 50 hours BN LDC-TDT4 selected from 85 hours using a slightlysupervised approach as described in [3], and 30 hoursBroadcast conversation (BC) recorded from Al-jazeeraTV, and 70 hours (40hrs BN, 30hrs BC) from LDC-GALE data.
For quality reasons we removed some ofthe most recent GALE data from acoustic modeltraining.4.
LANGUAGE MODELINGThe Arabic Giga word corpus distributed by LDC iscurrently the major Arabic text resource for languagemodeling.
Since this corpus only covers broadcast news,we spidered the web to cover broadcast conversationaldata.
We found transcripts for Arabic talk shows on theAl-jazeera web site www.al-jazeera.net and collected alldata available from 1998 to 2005.
We excluded allmaterial from 2006 to comply the evaluation ruleswhich prohibit the use of any data starting February2006.
In addition to the mentioned data we collectedBN data from the following source: Al-Akhbar(Egyptian daily newspaper 08/2000 to 12/2005) andAkhbar Elyom (Egyptian weekly newspaper 08/2000 to12/2005).
Furthermore, we used unsupervised trainingtranscripts from 750 hours BN created and shared byIBM.For language modeling building we used the SRILMtool kit from SRI [5].
Since we have 2 kinds of data,Broadcast News and Conversation, we built variousindividual 4-grams language models.
11 models werethen interpolated to create one language model.
Theinterpolation weights were selected based on a held outdata set from BN and BC sources.
We found that thedata from Al-jazeera (both BN & BC) has the highestweight comparing to other sources.
The resulting finallanguage model uses a total number of n-grams is 126Mand a vocabulary of 219k words.
The perplexity of thelanguage model is 212 on a test set containing BC andBN data.5.
TV WEB TRANSCRIPTSMost of our acoustic and language model training datacomes from broadcast news.
However, since GALEtargets broadcast news as well as conversations welooked for an effective method to increase the trainingdata for Arabic BC.
We made use of the fact that someArabic TV stations place transcripts for their programon the web.
These transcripts lack time stamp butinclude acceptable quality of the transcription.However, one challenge is that the transcriptions are notcomplete in that they do not include transcripts ofcommercials or any news break that may interrupt theshow.
In total we recorded 50 hours of Broadcastconversation shows from Al-jazeera and used them inour acoustic model and language model training byperforming the following procedures:?
We manually selected shows from Al-jazeera TV?
We used a scheduler to automatically start therecording of the selected shows.?
We spidered the web to collect corresponding showtranscripts from their web site www.aljazeera.net.?
We automatically processed the transcripts toconvert the html files to text, convert numbers towords and remove any non-Arabic words in theshows.?
We added these shows to our LM data with highweight, built a biased LM, and used this LM todecode the recorded shows.?
We aligned the reference (transcripts without timestamps) with the decoder output that may containspeech recognition errors.?
We selected only the portions that are correct; wedid not select any portion with number of wordsless than 3 correct consecutive words.?
Based on the above criteria we finally selected 30hours out of the total 40 hours recordings.?
We clustered utterances based on BIC criteriaapproach described in [7].As a result, we managed to project the time stamp in theoriginal transcript such that it can be used for training.Using these 30 hours of data resulted in a 7% relativeimprovement on RT04.
Since RT04 is broadcast news,we expect even higher gains on broadcastconversational data.
It is worth mentioning that we130applied the same slightly supervised approach to theTDT4 data which is a low quality quick transcription.We selected 50 out of 80 hours and achieved animprovement of 12% relative.
The gain was highersince at the time of these experiments we had only 40hours of training from FBIS data, therefore more thandoubled the amount of training data by adding TDT4.6.
NON-VOWELIZED SYSTEMArabic spelling is mostly phonemic; there is a closeletter-to-sound correspondence.
We used a grapheme-to-phoneme approach similar to [1].
Our phoneme setcontains 37 phonemes plus three special phonemes forsilence, non-speech events, and non-verbal effects, suchas hesitation.We preprocessed the text by mapping the 3 shapes ofthe grapheme for glottal stops to one shape at thebeginning of the word since these are frequently miss-transcribed.
This preprocessing step leads to 20%reduction in perplexity of our language model and 0.9%improvements in the final WER performance on RT04.Preprocessing of this kind appears to be appropriatesince the target of the project is not transcription butspeech translation and the translation communityapplies the same pre-processing.
We used a vocabularyof 220K words selected by including all wordsappearing in the acoustic transcripts and the mostfrequent words occurring in the LM.
The OOV rate is1.7% on RT04.
Table 1 shows the performance of ourSpeaker-Independent (SI) and Speaker-Adaptive (SA)non-vowelized system on the RT04 set.Table 1: Non-vowelized System ResultsSystem     WER on RT04 (%)Non-Vowelized         (SI)           25.3Non-Vowelized         (SA)           20.87.
VOWELIZED SYSTEMWritten MSA lacks vowels, thus native speakers addthem during reading.
Vowels are written only inchildren books or traditional religious books.
To restorevowels for a 129K vocabulary [4], we performed thefollowing steps:?
Buckwalter morphological analyzer (BMA) (found106K out of 129K entries).?
If a word is not vowelized by the analyzer, wecheck for its vowelization in the LDC Arabic Tree-Bank (additional 5k entries found).?
If the word did not appear in any of those, we usedthe written non-vowelized word form.In total 11k entries could not be resolved by either theBMA or the Treebank.This vowelization step resulted in 559,035pronunciations for the 129k words in our vocabulary,i.e.
we have on average 5 pronunciations per word.
Toreduce the number of pronunciation variants weperformed a forced alignment and excludedpronunciations which did not occur in the trainingcorpus.
This results in 407,754 pronunciations, which isa relative reduction of about 27%.
For system trainingwe used the same vocabulary and applied the sametraining procedure as in the non-vowelized system foracoustic model training.As Table 2 shows, we achieved a very good gain of1.3% absolute on the SI pass and 1.5% on the SA pass,both benchmarked on RT04 (compare Table 1).
Weenvision to seeing even higher improvements afterestimating and applying probability priors to multiplepronunciation and after vowelizing the remainder 11kwords that had not been covered by BMA or the Tree-Bank.Table2: Vowelized System ResultsSystem  WER on RT04 (%)Vowelized         (SI)        24.0Vowelized         (SA)       19.38.
COMBINING VOWELIZED & NON-VOWELIZED SYSTEMAfter seeing significant improvements by vowelization,we investigated the performance gain through cross-adapting the vowelized system with the non-vowelizedsystem.
The vowelized system cross adapted with theSA non-vowelized gave us 1.3 over the vowelizedsystem  adapted on the SI vowelized system.
We used a3-pass decoding strategy, in which the first pass uses thespeaker independent (SI) vowelized system, the secondpass uses the speaker adaptive (SA) non-vowelizedsystem, and the third, final pass, uses the speakeradaptive vowelized system.
Some challenges for thecross-adaptations had to be overcomed, for instance tocross adapt the non-vowelized system on the vowelizedsystem, we had to remove the vowels to have a non-vowelized transcript.
Since the phoneme set of the non-vowelized system is a subset of the phoneme set of thevowelized system, we could simply exclude the vowelphonemes from the vowelized system.
Furthermore, thesearch vocabulary is the same and so is the languagemodel.The main changes are the pronunciation dictionary andthe decision tree.
We tried different combinationschemes, e.g.
by starting with the non-vowelizedsystem, then the vowelized, and then the non-vowelizedbut found that none outperforms the combinationreported here in terms of WER.
In addition starting withthe non-vowelized SI pass is much faster than thevowelized SI system (4.5RT compared to 9RT).131Table 3: Non-vowelized & vowelized SystemCombinationSystem    WER on RT04 (%)Vowelized         (SI)           24.0Non-Vowelized (SA)           19.9Vowelized         (SA)           18.39.
ACOUSTIC MODEL PARAMETER TUNINGWe started our legacy system with 40 hours and until itreached 90 hours we were using the same number ofcodebooks (3000) and same number of Gaussians (64)per codebook.
With the increase of training data from90 hours to 190 hours we investigated the effect ofincreasing the number of codebooks and Gaussians.Also, we were using merge and split training (MAS)and STC only for the adapted pass; we furthermoreinvestigated the effect of using it for the SI pass.
Wefound that using MAS & STC on the SI pass gave us again of 5% relative on the SI pass.
In addition we foundthat the ideal number of codebooks is 5000 for the non-vowelized system resulting in a gain of 5.3% relative onthe SI pass.
We expect to see further gains on the SApass.
Table 4 summarizes the system performance usingdifferent parameter sizes and training schemes.Table 4: System Performance vs.Model Size#codebooks MAS #Gausians Voc System WER(%)3K - 64K 129 Non-vow(NV)29.63K Mas 64K 129      NV 28.35K Mas 64K 129      NV 27.95K Mas 100K 129      NV 27.65K Mas 100K 200 nv+tvTRANS26.33K Mas 100K 200 vow+tvTRANS24.010.
SYSTEM EVOLUTIONTable 5 shows the gains we achieved at major milestonestages while building the system.
The key improvementsare due to adding data collected from the web,Vowelization, and combining the vowelized and non-vowelized systems.
Tuning the acoustic modelsparameters gave us a good gain and finally theinterpolation of different language model for differentsources gave additional improvements.
The real-timebehavior of the system improved from 20RT to 10 RTwhile loosing only 0.2% which is in acceptable trade-off.
Recently, we gained 3.5% relative applyingdiscriminative training (MMIE).11.
CONCLUSIONWe presented the CMU 2006 GALE ASR Arabicsystem.
It can be seen that we achieved 40%improvements over our legacy system.Table 5: System Progress WER (%)LEGACY SYSTEM  32.7STC+VTLN 30.1SPEED FROM 20RT TO 10RT 30.3FROM 3 TO 4GM+BETTERSEGMENTATION28.4TDT4 TRANSCIPTS SELECTIONREFINEMENT26.3CLUSTERING REFINEMENT &RETRAINING25.5MORE LM DATA +INTERPOLATING 11 LMS 24.2ADDITION Q3 OF LDC DATA 23.6ACOUSTIC MODEL PARAMETER TUNING 20.7MMIE 20.0COMBINED SYSTEMS (VOW+NON-VOW) 18.3We combined a vowelized and a non-vowelized systemand achieved 4.0% relative over the vowelized system.Also, we managed to use TV web transcript as a methodto cover the shortage of training data specially thebroadcast conversation.
Currently, we are exploringmore on the vowelized system by adding weights todifferent multiple pronunciations and addingvowelization to words not covered by the morphologicalanalyzer or the tree-bank.12.
ACKNOWLEGMENTSWe also would like to thank Qin Jin for applying herautomatic clustering techniques to the web data.13.
REFERENCES[1] J. Billa, M. Noamany, A. Srivastava,  D. Liu,  R. Stone, J.Xu,  J. Makhoul, and F. Kubala, ?Audio Indexing ofArabic Broadcast News?,   International Conference onAcoustics, Speech, and Signal Processing  (ICASSP),May 2002.
[2] H. Yu, Y-C. Tam, T. Schaaf, S. St?ker, Q. Jin, M.Noamany, and T. Schultz ?The ISL RT04 MandarinBroadcast News Evaluation System?, EARS RichTranscription workshop, Palisades, NY, 2004.
[3] L. Nguyen et al, ?Light supervision in acoustic modeltraining,?
ICASSP , Montreal, QC, Canada, May 2004.
[4] M. Afify et al,"Recent progress in Arabic broadcast newstranscription at BBN", In INTERSPEECH-2005.
[5] A. Stolcke SRILM- An Extensible Language ModelingToolKit  ICSLP.
2002, Denver, Colorado.
[6] T.Buckwalter, ?Issues in Arabic Orthography andmorphology Analysis?, COLING 2004, Geneva, 2004.
[7] Q. Jin, T. Schultz, ?Speaker segmentation and clusteringin meetings?, ICSLP, 2004.
[8] P. Zhan, M. Westphal, "Speaker Normalization Based OnFrequency Warping", ICASSP 1997, Munich, Germany.
[9] M. Finke, et al, "The Karlsruhe Verbmobil SpeechRecognition Engine," International Conference onAcoustics, Speech, and Signal Processing, ICASSP,1997.
[10] H. Soltau, et al, "A One Pass-Decoder Based OnPolymorphic Linguistic Context",  ASRU 2001, Trento,Italy, 2001132
