First Joint Conference on Lexical and Computational Semantics (*SEM), pages 586?590,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsWeiwei: A Simple Unsupervised Latent Semantics based Approach forSentence SimilarityWeiwei GuoDepartment of Computer Science,Columbia University,weiwei@cs.columbia.eduMona DiabCenter for Computational Learning Systems,Columbia University,mdiab@ccls.columbia.eduAbstractThe Semantic Textual Similarity (STS) sharedtask (Agirre et al, 2012) computes the degreeof semantic equivalence between two sen-tences.1 We show that a simple unsupervisedlatent semantics based approach, WeightedTextual Matrix Factorization that only exploitsbag-of-words features, can outperform mostsystems for this task.
The key to the approachis to carefully handle missing words that arenot in the sentence, and thus rendering it su-perior to Latent Semantic Analysis (LSA) andLatent Dirichlet Allocation (LDA).
Our sys-tem ranks 20 out of 89 systems according tothe official evaluation metric for the task, Pear-son correlation, and it ranks 10/89 and 19/89in the other two evaluation metrics employedby the organizers.1 IntroductionIdentifying the degree of semantic similarity [SS]between two sentences is helpful for many NLP top-ics.
In Machine Translation (Kauchak and Barzi-lay, 2006) and Text Summarization (Zhou et al,2006), results are automatically evaluated based onsentence comparison.
In Text Coherence Detection(Lapata and Barzilay, 2005), sentences are linked to-gether by similar or related words.
For Word SenseDisambiguation, researchers (Banerjee and Peder-sen, 2003; Guo and Diab, 2012a) construct a sensesimilarity measure from the sentence similarity ofthe sense definitions.Almost all SS approaches decompose the task intoword pairwise similarity problems.
For example, Is-1Mona Diab, co-author of this paper, is one of the task orga-nizerslam and Inkpen (2008) create a matrix for each sen-tence pair, where columns are the words in the firstsentence and rows are the words in the second sen-tence, and each cell stores the distributional similar-ity of the two words.
Then they create an alignmentbetween words in two sentences, and sentence simi-larity is calculated based on the sum of the similarityof aligned word pairs.
There are two disadvantageswith word similarity based approaches: 1. lexicalambiguity as the word pairwise similarity ignoresthe semantic interaction between the word and sen-tence/context.
2. word co-occurrence informationis not as sufficiently exploited as they are in latentvariable models such as Latent Semantic Analysis(LSA) (Landauer et al, 1998) and Latent DirichiletAllocation (LDA) (Blei et al, 2003).
On the otherhand, latent variable models can solve the two issuesnaturally by modeling the semantics of words andsentences simultaneously in the low-dimensional la-tent space.However, attempts at addressing SS using LSAperform significantly below word similarity basedmodels (Mihalcea et al, 2006; O?Shea et al, 2008).We believe the reason is that the observed wordsin a sentence are too few for latent variable mod-els to learn robust semantics.
For example, giventhe two sentences of WordNet sense definitions forbank#n#1 and stock#n#1:bank#n#1: a financial institution that accepts de-posits and channels the money into lending activitiesstock#n#1: the capital raised by a corporationthrough the issue of shares entitling holders to anownership interest (equity)LDA can only find the dominant topic (thefinancial topic) based on the observed words with-out further discernibility.
In this case, many sen-586tences will share the same latent semantics profile,as long as they are in the same topic/domain.In our work (Guo and Diab, 2012b), we proposeto model the missing words (words that are not inthe sentence) to address the sparseness issue for theSS task.
Our intuition is since observed words in asentence are too few to tell us what the sentence isabout, missing words can be used to tell us what thesentence is not about.
We assume that the semanticspace of both the observed and missing words makeup the complete semantic profile of a sentence.
Weimplement our idea using a weighted matrix factor-ization approach (Srebro and Jaakkola, 2003), whichallows us to treat observed words and missing wordsdifferently.It should be noted that our approach is very gen-eral (similar to LSA/LDA) in that it can be applied toany genre of short texts, in a manner different fromexisting work that models short texts by using addi-tional data, e.g., Ramage et al (2010) model tweetsusing their metadata (author, hashtag, etc).
Also wedo not extract additional features such as multiwordsexpression or syntax from sentences ?
all we use isbag-of-words feature.2 Related WorkAlmost all current SS methods work in the high-dimensional word space, and rely heavily onword/sense similarity measures.
The word/sensesimilarity measure is either knowledge based (Li etal., 2006; Feng et al, 2008; Ho et al, 2010; Tsatsa-ronis et al, 2010), corpus-based (Islam and Inkpen,2008) or hybrid (Mihalcea et al, 2006).
Almost allof them are evaluated on a data set introduced in (Liet al, 2006).
The LI06 data set consists of 65 pairsof noun definitions selected from the Collin CobuildDictionary.
A subset of 30 pairs is further selectedby LI06 to render the similarity scores evenly dis-tributed.
Our approach has outperformed most of theprevious methods on LI06 achieving the second bestPearson?s correlation and the best Spearman corre-lation (Guo and Diab, 2012b).3 Learning Latent Semantics of Sentences3.1 IntuitionGiven only a few observed words in a sentence, thereare many hypotheses of latent vectors that are highlyrelated to the observed words.
Therefore, missingFigure 1: Matrix Factorizationwords can be used to prune the hypotheses that arealso highly related to the missing words.Consider the hypotheses of latent vectors in Ta-ble 1 for the sentence of the WordNet definitionof bank#n#1.
Assume there are 3 dimensions inour latent model: financial, sport, institution.
Weuse Rvo to denote the sum of relatedness betweenlatent vector v and all observed words; similarly,Rvm is the sum of relatedness between the vectorv and all missing words.
Hypothesis v1 is givenby topic models, where only the financial sen-tence is found, and it has the maximum relatednessto observed words in bank#n#1 sentence Rv1o =20.v2 is the ideal latent vector, since it also detectsthat bank#n#1 is related to institution.
It has aslightly smaller Rv2o =18, but more importantly, re-latedness to missing words Rv2m=300 is substantiallysmaller than Rv1m=600.However, we cannot simply choose a hypothesiswith the maximum Ro ?Rm value, since v3, whichis clearly not related to bank#n#1 but with a min-imum Rm=100, will be our final answer.
The so-lution is straightforward: give a smaller weight tomissing words, e.g., so that the algorithm tries toselect a hypothesis with maximum value of Ro ?0.01 ?
Rm.
To implement this idea, we model themissing words in the weighted matrix factorizationframework [WMF] (Srebro and Jaakkola, 2003).3.2 Modeling Missing Words by WeightedMatrix FactorizationGiven a corpus we represent the corpus as anM ?
N matrix X .
The row entries of the matrixare the unique N words in the corpus, and the Mcolumns are the sentence ids of all the sentences.The yielded N ?M co-occurrence matrix X com-prises the TF-IDF values in each Xij cell, namelythat TF-IDF value of word wi in sentence sj .In WMF, the original matrix X is factorized intotwo matrices such thatX ?
P>Q, where P is aK?M matrix, and Q is a K ?
N matrix (Figure 1).
Inthis scenario, the latent semantics of each wordwi orsentence sj is represented as a K-dimension vector587financial sport institution Ro Rm Ro ?Rm Ro ?
0.01Rmv1 1 0 0 20 600 -580 14v2 0.6 0 0.1 18 300 -282 15v3 0.2 0.3 0.2 5 100 -95 4Table 1: Three possible hypotheses of latent vectors for definition of bank#n#1P?,i or Q?,j .
Note that the inner product of P?,i andQ?,j is used to approximate the semantic relatednessof word wi and sentence sj : Xij ?
P?,i ?Q?,j , as theshaded parts in Figure 1.In WMF each cell is associated with a weight, somissing words cells (Xij=0) can have a much lesscontribution than observed words.
Assume wm isthe weight for missing words cells.
The latent vec-tors of words P and sentences Q are estimated byminimizing the objective function:?i?jWij (P?,i ?Q?,j ?Xij)2 + ?||P ||22 + ?||Q||22where Wi,j ={1, if Xij 6= 0wm, if Xij = 0(1)Equation 1 explicitly requires the latent vector ofsentence Q?,j to be not related to missing words(P?,i ?
Q?,j should be close to 0 for missing wordsXij = 0).
Also weight wm for missing words isvery small to make sure latent vectors such as v3 inTable 1 will not be chosen.
In experiments we setwm = 0.01.
We refer to our approach as WeightedTextual Matrix Factorization (WTMF).After we run WTMF on the sentence corpus, thesimilarity of the two sentences sj and sk can be com-puted by the inner product of Q?,j and Q?,k.3.3 InferenceThe latent vectors in P and Q are first randomlyinitialized, then can be computed iteratively by thefollowing equations (derivation is omitted due tolimited space, but can be found in (Srebro andJaakkola, 2003)):P?,i =(QW?
(i)Q> + ?I)?1QW?
(i)X>i,?Q?,j =(PW?
(j)P> + ?I)?1PW?
(i)X?,j(2)where W?
(i) = diag(W?,i) is an M ?
M diagonalmatrix containing ith row of weight matrixW .
Sim-ilarly, W?
(j) = diag(W?,j) is an N ?
N diagonalmatrix containing jth column of W .Since most of the cells have the same value of 0,the inference can be further optimized to save com-putation, which has been described in (Steck, 2010).4 Data PreprocessingThe data sets for WTMF comprises two dictionar-ies WordNet (Fellbaum, 1998), Wiktionary,2 andthe Brown corpus.
We did not link the senses be-tween WordNet and Wiktionary, therefore the defini-tion sentences are simply treated as individual docu-ments.
We crawl Wiktionary and remove the entriesthat are not tagged as noun, verb, adjective, or ad-verb, resulting in 220,000 entries.
For both WordNetand Wiktionary, target words are added to the defini-tion (e.g.
the word bank is added into the definitionsentence of bank#n#1).
Also usage examples areappended to definition sentences (hence sentencesbecome short texts).
For the Brown corpus, eachsentence is treated as a document in order to createmore co-occurrence.
The importance of words in asentence is estimated by the TF-IDF schema.All data is tokenized, pos-tagged3, and lemma-tized4.
To reduce word sparsity issue, we takean additional preprocessing step: for each lemma-tized word, we find all its possible lemmas, andchoose the most frequent lemma according to Word-Net::QueryData.
For example, the word thinkings isfirst lemmatized as thinking, then we discover think-ing has possible lemmas thinking and think, finallywe choose think as targeted lemma.
The STS data isalso preprocessed using the same pipeline.5 Experiments5.1 SettingSTS data: The sentence pair data in the STStask is collected from five sources: 1.
MSR Para-phrase corpus (Dolan et al, 2004), 2.
MSR videodata (Chen and Dolan, 2011), 3.
SMT europarl data,2http://en.wiktionary.org/wiki/Wiktionary:Main Page3http://nlp.stanford.edu/software/tagger.shtml4http://wn-similarity.sourceforge.net, WordNet::QueryData588models MSRpar MSRvid SMT-eur ON-WN SMT-newsLDA 0.274 0.7682 0.452 0.619 0.366WTMF 0.411(67/89) 0.835(11/89) 0.513(10/89) 0.727(1/89) 0.438(28/89)Table 2: Performance of LDA and WTMF on each individual test set of Task 6 STS dataALL ALLnrm Mean0.695(20/89) 0.830(10/89) 0.608(19/89)Table 3: Performance of WTMF on all test sets4.
OntoNotes-WordNet data (Hovy et al, 2006), 5.SMT news data.Evaluation Metrics: Since the systems are re-quired to assigned a similarity score to each sentencepair, Pearson?s correlation is used to measure theperformance of systems on each of the 5 data sets.However, measuring the overall performance on theconcatenation of 5 data sets is rarely discussed inprevious work.
Accordingly the organizers of STStask provide three evaluation metrics: 1.
ALL: Pear-son correlation with the gold standard for the com-bined 5 data sets.
2.
ALLnrm: Pearson correlationafter the system outputs for each data set are fittedto the gold standard using least squares.
3.
Mean:Weighted mean across the 5 data sets, where theweight depends on the number of pairs in the dataset.WTMF Model: Our model is built on Word-Net+Wiktionary+Brown+training data of STS.
Eachsentence of STS test data is transformed into a latentvector using Equation 2.
Then sentence pair similar-ity is computed by the cosine similarity of the twolatent vectors.
We employ the parameters used in(Guo and Diab, 2012b) (?
= 20, wm = 0.01).5.2 ResultsTable 3 summarizes the overall performance ofWTMF on the concatenation of 5 data sets followedby the corresponding rank among all participatingsystems.5 There are 88 submitted results in total and1 baseline which is simply the cosine similarity ofsurface word vectors.Table 2 compares the individual performance ofLDA (trained on the same corpus) and WTMF oneach data set.
WTMF outperforms LDA by a largemargin.
This is because LDA only uses 10 observedwords to infer a 100 dimension vector, while WTMFtakes advantage of much more missing words to5http://www.cs.york.ac.uk/semeval-2012/task6/index.php?id=results-updatelearn more robust latent semantic vectors.WTMF model achieves great overall perfor-mance, with ranks 20, 10, 19 out of 89 reported re-sults in three evaluation metrics respectively.
It isworth noting that WTMF is unsupervised in that itdoes not use the training data similarity values, alsothe only feature WTMF uses is bag-of-words fea-tures without other information such as syntax, sen-timent, etc.
indicating that these additional featurescould lead to even more improvement.Observing the individual performance on each ofthe 5 data set, we find WTMF ranks relatively highin the four data sets: MSRvid (11/89), SMT-eur(11/89), ON-WN (1/89), SMT-news (28/89).
How-ever, WTMF is outperformed by most of the systemson MSRpar data set (67/89).
We analyze the data setand find that different from the other four data sets,MSRpar is related to a lot of other NLP topics suchas textual entailment or sentiment coherence.
There-fore, our feature set (bag of words) is too shallow forthis data set indicating that using syntax and moresemantically oriented features could be helpful.6 ConclusionsWe introduce a new latent variable model WTMFthat is competitive with high dimensional ap-proaches to the STS task.
In WTMF model, we ex-plicitly model missing words to alleviate the sparsityproblem in modeling short texts.
For future work,we would like to combine our methods with existingword similarity based approaches and add more nu-anced features incorporating syntax and semanticsin the latent model.AcknowledgmentsThis research was funded by the Office of the Di-rector of National Intelligence (ODNI), IntelligenceAdvanced Research Projects Activity (IARPA),through the U.S. Army Research Lab.
All state-ments of fact, opinion or conclusions containedherein are those of the authors and should not beconstrued as representing the official views or poli-cies of IARPA, the ODNI or the U.S. Government.589ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: A piloton semantic textual similarity.
In Proceedings of the6th International Workshop on Semantic Evaluation(SemEval 2012), in conjunction with the First JointConference on Lexical and Computational Semantics(*SEM 2012).Satanjeev Banerjee and Ted Pedersen.
2003.
Extendedgloss overlaps as a measure of semantic relatedness.In Proceedings of the 18th International Joint Confer-ence on Artificial Intelligence, pages 805?810.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of MachineLearning Research, 3.David L. Chen and William B. Dolan.
2011.
Collectinghighly parallel data for paraphrase evaluation.
In Pro-ceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics.William Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.
InProceedings of the 20th International Conference onComputational Linguistics.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Jin Feng, Yi-Ming Zhou, and Trevor Martin.
2008.
Sen-tence similarity based on relevance.
In Proceedings ofIPMU.Weiwei Guo and Mona Diab.
2012a.
Learning the latentsemantics of a concept from its definition.
In Proceed-ings of the 50th Annual Meeting of the Association forComputational Linguistics.Weiwei Guo and Mona Diab.
2012b.
Modeling sen-tences in the latent space.
In Proceedings of the 50thAnnual Meeting of the Association for ComputationalLinguistics.Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Ab-dul Kadir, and Shyamala C. Doraisamy.
2010.
Wordsense disambiguation-based sentence similarity.
InProceedings of the 23rd International Conference onComputational Linguistics.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:The 90% solution.
In Proceedings of the Human Lan-guage Technology Conference of the North AmericanChapter of the ACL.Aminul Islam and Diana Inkpen.
2008.
Semantictext similarity using corpus-based word similarity andstring similarity.
ACM Transactions on KnowledgeDiscovery from Data, 2.David Kauchak and Regina Barzilay.
2006.
Paraphras-ing for automatic evaluation.
In Proceedings of theHuman Language Technology Conference of the NorthAmerican Chapter of the ACL.Thomas K Landauer, Peter W. Foltz, and Darrell Laham.1998.
An introduction to latent semantic analysis.Discourse Processes, 25.Mirella Lapata and Regina Barzilay.
2005.
Automaticevaluation of text coherence: Models and representa-tions.
In Proceedings of the 19th International JointConference on Artificial Intelligence.Yuhua Li, Davi d McLean, Zuhair A. Bandar, James D. OShea, and Keeley Crockett.
2006.
Sentence similar-ity based on semantic nets and corpus statistics.
IEEETransaction on Knowledge and Data Engineering, 18.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In Proceedings of the 21stNational Conference on Articial Intelligence.James O?Shea, Zuhair Bandar, Keeley Crockett, andDavid McLean.
2008.
A comparative study of twoshort text semantic similarity measures.
In Proceed-ings of the Agent and Multi-Agent Systems: Technolo-gies and Applications, Second KES International Sym-posium (KES-AMSTA).Daniel Ramage, Susan Dumais, and Dan Liebling.
2010.Characterizing microblogs with topic models.
In Pro-ceedings of the Fourth International AAAI Conferenceon Weblogs and Social Media.Nathan Srebro and Tommi Jaakkola.
2003.
Weightedlow-rank approximations.
In Proceedings of the Twen-tieth International Conference on Machine Learning.Harald Steck.
2010.
Training and testing of recom-mender systems on data missing not at random.
InProceedings of the 16th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing.George Tsatsaronis, Iraklis Varlamis, and Michalis Vazir-giannis.
2010.
Text relatedness based on a word the-saurus.
Journal of Articial Intelligence Research, 37.Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,and Eduard Hovy.
2006.
Paraeval: Using paraphrasesto evaluate summaries automatically.
In Proceedingsof Human Language Tech-nology Conference of theNorth American Chapter of the ACL,.590
