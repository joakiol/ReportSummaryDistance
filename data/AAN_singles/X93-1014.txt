CORPORA AND DATA PREPARATION FOR INFORMATION EXTRACTIONLynn CarlsonBoyan OnyshkevychMary Ellen OkurowskiU.
S. Department of  DefenseFt.
Meade, MD 20755em~dl: {imcarls, baonysh, meokuro}@afterlife.ncsc.mil1.
BACKGROUNDThe data selection and data preparation efforts whichled to the TIPSTER and Fifth Message Understanding Con-ference (MUC-5) corpora involved substantial effort, timeand resources.
The Government commitment to theseselection and preparation efforts stems from four TIPSTERProgram objectives: (1) to provide training 4ata that wouldpromote the development of information extraction tech-nology, (2) to provide accurate test data to evaluate andbaseline system performance inan objective manner, (3) toprovide baseline data for human performance tounderstandand interpret machine performance, and (4) to support helarger Natural Language Processing community by makingavailable a unique set of texts and templates in multipledomains and languages under ARPA support.
This commit-ment was demonstrated through the managerial, technical,and administrative support o these efforts from variousGovernment agencies, as well as through the contractualefforts with the Institute for Defense Analyses for datapreparation and New Mexico State University for softwaretool development.2.
DOCUMENT CORPORAFour language-domain pairs were used in the TIP-STER exercise, abbreviated as EJV, JJV, EME, JME toreflect he language (English or Japanese) and the domain(Joint Ventures or MicroElectronics).
Each of the four lan-guage-domain pairs has an associated set of 1200 to 1600documents (a corpus), divided into a development set andthe multiple test sets.
During the course of the TIPSTERprogram, up to three test sets were prepared for each lan-guage-domain pair, in addition to approximately 1000development set documents for each corpus.
These testsets, which were used for the TIPSTER 12-, 18-, and 24-month evaluations, ranged from 50 to 300 documents each.For MUC-5, the first test set was added tothe developmentcorpus, the second test set was used for the MUC-5 dry run,and the third test set was used for the MUC-5 evaluation.Randomly selected from the overall pool of documents, thetest sets reflect a similar distribution of sources, relevancy,and other document attributes as the development sets.There are a few exceptions, e.g., the first EJV test set doesnot contain documents from one of the sources added to thedevelopment and subsequent test sets.These corpora consist of documents from a variety ofnewswire or newspaper sources, selected by a combinationof automatic retrieval and manual filtering techniques.
Forexample, the EJV corpus was retrieved from three text datasources (LEXUS/NEXUS, PROMT, and Wall Street Jour-nal from ACL/DCI or TIPSTER Detection databaseCDROMs) by using traditional keyword-based documentretrieval systems.
These keywords for EJV included suchsterns as joint venture, joint, venture, tie-up, collaborate,cooperate.
Though the majority of the documents werepulled by the keyword method, additional candidates wereretrieved by random browsing through the corpora sourcesand identifying documents which appeared to be relevant.After a large pool of candidate documents was retrieved,these documents were manually scanned and separated intotwo groups: relevant or irrelevant.
In order to test whetherthe Information Extraction systems were able to discrimi-nate between relevant and irrelevant documents, the fourcorpora were then seeded with a certain number of irrele-vant documents.
The percentage of irrelevant documentsfunctioning as "distractors" ranges from about 5% (forEnglish Joint Ventures) to 30% (for Japanese Microelec-tronics).
By comparison, the corpora used for previousMUCs used up to 50% irrelevant documents, tressing thedocument detection aspect of the task more strenuouslythan in TIPSTER/MUC-5.The 200+ different sources used to build the English-language corpora include the Wall Street Journal, JijiPress, New York Iimes, Financial imes, Kyodo News Ser-vice, and a variety of technical publications in fields suchas communications, airline transportation, rubber & plas-tics, and food marketing.
The Japanese-language sources135Table 1: Template Countsused for the Japanese corpora include Asahi, Nikkei, andYomiuri.Each document in the four development corpora hasan associated filled-in template (see appendices to "Tasks,Domains and Languages for Information Extraction" in thisvolume), representing the correct template or "answer key"for that document.
The development corpora along withtheir associated templates were provided to the programparticipants during the course of the program.3.
TEMPLATE CORPORAIn order to provide the system developers with train-ing data to illustrate the task and benchmark their development, filled-out templates for the approximately 1000documents of each training set were provided as "keys".
Inaddition, templates were produced for the initial TIPSTERprogram test cycles (12 and 18 months) and for the finaljoint TIPSTER (24 month)/MUC-5 test.
Table 1 providesthe number of templates in each development and test setThe templates were filled by experienced human ana-lysts according to the same fill rules document (see below)and other supporting documentation that was provided tothe system developers to define the exact syntax andsemantics of the template fills.4.
FILL RULESIn addition to the template definition itself, which onlydefines the syntax in a BNF-like notation (see "TemplateDesign for Information Extraction" in this volume), theanalysts and participants in TIPSTER and MUC-5 wereprovided with fill rules for each domain.
At the highestlevel, the fill rules specify the reporting conditions for agiven domain; that is, what information in the document isto be extracted and coded as part of the template.
Thesereporting conditions correspond to the general goals of theextraction task.
For example, the fill rules document for theJV domain defines what information in a text constitutessufficient evidence to report a joint venture.
Of note is thatthe conditions enumerated in the fill rules were determinedfrom the document corpus and refined through the actualapplication of (earlier versions of) the fill rules to the cor-pus.
At a more specific level, the fill rules delineate the con-ditions for instantiating an object, object by object, and forfilling a slot, slot by slot.
At the object and dot levels, therules specify (1) what kind of evidence in the text isrequired for instantiation or fills and what, if anything, canbe inferred, (2) the formatting conditions for data represen-tation, and (3) the semantics of the data elements.
Examplesare often provided to highlight any one of these aspects.The fill rules served as guidelines for two very differ-ent sets of users--the analysts and the system developers.Since the evolution of a Fill Rule document was driven to alarge extent by its application to a text corpus, the analystswere key contributors to the fill rules in that they applied therules and in so doing identified discrepancies, omissions,and exceptions to the rules.
System developers, on the otherhand, were mainly "consumers" of the rules, even thoughthe TIPSTER participants did provide substantial input tothe fill rules through questions and comments.
Althoughreporting conditions as well as object and slot specificationsneed to be implemented in the extraction systems, thedevelopers of those systems also relied on the text corpusitself and analyst-filled templates to direct development.In support of the fill rules document, other specializeddocuments were also provided, for example, expanding onthe definition of a joint venture, or on the semantics of rep-resenting time expressions.
programs, and the test sets were used to measure systemwrformance at six-month intervals (see above under TEM-5.
OTHER SUPPORTING MATERIALThe Government also supplied on-line supportingmaterials to the analysts and the TIPSTERIMUC-5 partici-pants.
In many cases, this material was used to regularize ornormalize the template fills.
For example, it was necessaryto use the English language Gazetteer to regularize geo-graphic locations.
Compiled from a variety of sources, thisresource provides place names for more than 240,000 loca-tions around the world.
For example, Baltimore is identifiedas a CITY, located in the PROVINCE (state) of Maryland,which is in the COUNTRY USA.
The entire gazetteer entryfor a location is used as the normalized fill for locationalinformation in the template.
Due to the small number of on-line geographic resources available for Japanese, a muchmore limited version of a Japanese gazetteer was manuallyproduced by one of the Japanese analysts, with entries forall of the countries in the world, detailed listings for Japa-nese provinces, U. S. states, major cities for both countries,and other major cities worldwide that appeared in the JJVcorpus.
The Japanese language Gazetteer contains 1882 dif-ferent locations.In the Joint Venture domain, the reporting of the prod-ucts or business of the joint venture included classifying theproduct or service using the Standard Industrial Classifica-tion Manual compiled by the U. S. Office of Managementand Budget.
This resource contains a hierarchical classifica-tion of all the industry or business types in the U. S., forexample, avocado farms, electric popcorn popper sales,management consulting.
The template-filling task requiredthat products or services be coded as a two-digit classifica-tion representing the second level in the hierarchy.Other supporting resources for fill regularizationinclude lists of currency names and abbreviations (e.g., theDutch guilder is abbreviated NLG), lists of corporate abbre-viations (e.g., Inc, GMBH, and Ltd.) along with lists ofcountries where those abbreviations are typically used, andnationality adjectives (e.g., Iraqi, Irish).
An additional set ofresources was provided to system developers to assist in theextraction task, for example, lists of people's first names.All of these resources have been made available to theresearch community through the Consortium for LexicalResearch at New Mexico State University.6.
DATA PREPARATIONThe goal of data preparation was to have human ana-lysts produce sets of development and test templates foreach of the four corpora.
The development templates servedas models for system developers in the TIPSTER and MUCPLATE CORPORA).
For each of the four languageldomainpairs, a group of experienced analysts was hired.
These ana-lysts met regularly over the course of 12 - 21 months(depending on the domain) to discuss domain and language-specific issues, iron out differences, and provide input to thefill rules, which evolved over time.
The human analystsused a window-based tool for Sun Microsystems worksta-.tions, developed for the template-filling task by New Mex-ico State University's Computing Research Laboratory.
Oneadditional sub-task undertaken as part of the data prepara-tion was the establishment of a performance baseline bymeasuring the performance of human analysts against eachother and against the final "correct" version of various tem-plates (see Table 2 below; for more &tail, see also "Com-paring Human and Machine Performance for NaturalLanguage Information Extraction: Results from the TipsterText Evaluation" in this volume).Eleven of the nineteen analysts which comprised thefour teams were hied by the Institute for Defense Analyses(IDA) in Virginia; additional analysts from various Govern-ment facilities joined these teams.
The Government techni-cal management team (including the authors) led the effortto specify the domain, the definition of the templates, andthe development of fill rulesfill rules and other supportingmaterials, in addition to directing IDA, which was responsi-ble for tracking template production and delivering pre-pared materials to the contractor sites, among other tasks.In order to ensure maximal consistency and correct-ness in the analyst-produced keys, a variety of template-fill-ing schemes were tried.
Essentially, the schemes useddifferent degrees of redundancy in producing each filledtemplate, then used different methods to compare thosetemplate versions and to produce one final "most correct"version.
Table 2 summarizes the different strategies thatwere tried.
Most templates were produced using AB+B orAB+C; JME was entirely produced using A+A.
For theother three corpora, the A, B, C, and D positions wererotated among the analysts.
Even though redundant codingand checking methods were utilized, the templates that wereproduced were not perfect; anomalies found by systemdevelopers were reviewed and changes were incorporatedinto the templates as appropriate.7.
TEMPLATE-FILLING STRATEGIESThe methodology used by the human analysts in fill-ing templates was studied during the course of the task,partly to drive redesign of the tools and documentation tosupport the analysts' efforts.
Although available resourcesdid not permit extensive cognitive study of the mechanismsTable 2: Template Coding SchemesScheme Name CODERS CHECKERS DESCRIPTIONA A One analyst produces template in onepassA+A A AA+BAB+BAB+CABCD +com-mitteeABCD + EAA andBA andBA,B,C,DA,B,C,DBCall togetherEOne analyst codes, then checks it in a sep-arate pass at a later timeOne analyst codes template, anotherchecks itTwo analysts independently produce cod-ings, then one of them reviews both andproduces composite versionTwo analysts independently produce cod-ings, then a third analyst reviews thosetwo and produces composite version.Each of four analysts produces codingindependently, then final version pro-duced by entire committeeEach of four analysts produces codingindependently, then final version pro-duced by the fifth personused by analysts, we did make some general observationsabout he strategies used by analysts.A variety of approaches were used by the human ana-lysts in filling out the templates.
What follows is a charac-terization of the different strategies used by the fiveJapanese joint venture analysts (referred to as Analysts A,B, C, D and E) in analyzing the documents and filling outthe corresponding templates.The analysts' task can be divided into two parts: astart-up procedure and the actual template filling process,using the on-line tool.
The start-up rocedure includes bothreading the text, and annotating a hard copy of the docu-ment.
The template-filling process addresses the order inwhich the analysts actually filled out the objects and slotsthat represented the various pieces of information to beextracted from the text.For the start-up procedure, three distinct approacheswere identified.
Scheme 1, used by two analysts, is charac-terized by minimal marking of the hard-copy text beforestarting to code the template using the on-line tool.
AnalystB would read the article twice through, then underline andlabel just the tie ups and entities before going to the tool.Analyst D would read and simultaneously underline ntitiesand place check marks by other pertinent data; then hewould begin coding.In Scheme 2, also used by two analysts, a moredetailed annotation of the hard-copy text was made.
AnalystE would read through the hard-copy text and simulta-neously underline and number entities, circle and numbertie ups, and make comments, uch as "El alias," "E2 offi-cial" (for alias or official~associated with a particular entity).Moreover, this analyst would draw links between relatedpieces of information in the text, and would outline in the138margins more complex objects, such as ACTIVITY, OWN-ERSHIP, and REVENUE.
After this process was complete,the coding would begin.
Analyst C's approach was simi-larly detailed, the only difference being that she would labelall pertinent information using color-coded highlighters,e.g., green for ENTITYS, yellow for product/servicestrings, blue for FACILITY and TIME objects.The thkd scheme, used only by Analyst A, involved amixture of initial marking, skimming, initial coding, anno-tating in detail, and then final coding.
This analyst wouldread the beginning of the article, marking potential entitiesuntil a "tie-up verb" was found.
Now certain that he articlehad a valid tie up, she would proceed to skim the remainderof the text, underlining or circling additional pertinentinformation.
At this point, she would use the tool to codethe initial portion of the template, i.e., the TIE-UPs,ENTITYs, and ENTITY-RELATIONSHIPS.
After this keystructure was in place, she would read through the remain-der of the text, annotating indetail all potential product/ser-vice strings and information about FACILITYs,REVENUE, OWNERSHIP, etc.
Finally, the remainder of thetemplate was coded using the tool.In the templateTfilling process, avariety of breadth vs.depth-first trategies were used by the analysts.
Four of theanalysts would completely fill in all information about hefirst tie up before coding any additional tie ups.
Analysts A,B, and E would fill in the TEMPLATE, TIE-UP, ENTITY,and ENT ITY-RELAT IONSHIP  objects first.
Then TIME,REVENUE, OWNERSHIP, PERSON and FACIL ITYobjects were instantiated in no particular order.
TheACTIVITY and INDUSTRY objects were filled in concur-rently, usually last.
This procedure was then repeated foradditional tie ups.
Analyst D followed a complete depth-first strategy for coding each tie up, filling in each slot inturn, so that if a slot pointed to another object, that objectwould then be filled in completely before proceeding to thenext slot in the top level object.
A breadth-first rategy forcoding was used by Analyst C, who would fill in all tie-upobjects and their respective entities first, and then code theremaining information for each tie up.These varying strategies for annotating texts and cod-ing templates did not seem to have a significant effect onthe quality of the templates produced, and seemed to be amatter of personal preference.
However, they give insightinto the different ways in which humans approach a particu-lar analytic task, and suggest hat on-line analytic toolsneed to be sufficiently flexible to accommodate the styles ofdifferent human users.139
