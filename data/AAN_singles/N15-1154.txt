Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1368?1373,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsDiscriminative Phrase Embedding for Paraphrase IdentificationWenpeng Yin and Hinrich Sch?utzeCenter for Information and Language ProcessingUniversity of Munich, Germanywenpeng@cis.lmu.deAbstractThis work, concerning paraphrase identifica-tion task, on one hand contributes to expand-ing deep learning embeddings to include con-tinuous and discontinuous linguistic phrases.On the other hand, it comes up with a newscheme TF-KLD-KNN to learn the discrimi-native weights of words and phrases specificto paraphrase task, so that a weighted sum ofembeddings can represent sentences more ef-fectively.
Based on these two innovations weget competitive state-of-the-art performanceon paraphrase identification.1 IntroductionThis work investigates representation learning viadeep learning in paraphrase identification task,which aims to determine whether two sentenceshave the same meaning.
One main innovation ofdeep learning is that it learns distributed word repre-sentations (also called ?word embeddings?)
to dealwith various Natural Language Processing (NLP)tasks.
Our goal is to use and refine embeddings toget competitive performance.We adopt a supervised classification approach toparaphrase identification like most top performingsystems.
Our focus is representation learning of sen-tences.
Following prior work (e.g., Blacoe and Lap-ata (2012)), we compute the vector of a sentence asthe sum of the vectors of its components.
But unlikeprior work we use single words, continuous phrasesand discontinuous phrases as the components, notjust single words.
Our rationale is that many seman-tic units are formed by multiple words ?
e.g., thecontinuous phrase ?side effects?
and the discontin-uous phrase ?pick .
.
.
off?.
The better we can dis-cover and represent such components, the better thecompositional sentence vector should be.
We usethe term unit to refer to single words, continuousphrases and discontinuous phrases.Ji and Eisenstein (2013) show that not all wordsare equally important for paraphrase identification.They propose TF-KLD, a discriminative weightingscheme to address this problem.
While they do notrepresent sentences as vectors composed of othervectors, TF-KLD is promising for a vector-basedapproach as well since the insight that units are ofdifferent importance still applies.
A shortcoming ofTF-KLD is its failure to define weights for wordsthat do not occur in the training set.
We proposeTF-KLD-KNN, an extension of TF-KLD that com-putes the weight of an unknown unit as the averageof the weights of its k nearest neighbors.
We de-termine nearest neighbors by cosine measure overembedding space.
We then represent a sentence asthe sum of the vectors of its units, weighted by TF-KLD-KNN.We use (Madnani et al, 2012) as our baselinesystem.
They used simple features ?
eight dif-ferent machine translation metrics ?
yet got goodperformance.
Based on above new sentence rep-resentations, we compute three kinds of featuresto describe a pair of sentences ?
cosine similarity,element-wise sum and absolute element-wise differ-ence ?
and show that combining them with the fea-tures from Madnani et al (2012) gets state-of-the-artperformance on the Microsoft Research Paraphrase(MSRP) corpus (Dolan et al, 2004).1368In summary, our first contribution lies in em-bedding learning of continuous and discontinuousphrases.
Our second contribution is the weightingscheme TF-KLD-KNN.This paper is structured as follows.
Section 2 re-views related work.
Section 3 describes our methodfor learning embeddings of units.
Section 4 intro-duces a measure of unit discriminativity that can beused for differential weighting of units.
Section 5presents experimental setup and results.
Section 6concludes.2 Related workThe key for good performance in paraphrase iden-tification is the design of good features.
We nowdiscuss relevant prior work based on the linguisticgranularity of feature learning.The first line is compositional semantics, whichlearns representations for words and then composesthem to representations of sentences.
Blacoe and La-pata (2012) carried out a comparative study of threeword representation methods (the simple distribu-tional semantic space (Mitchell and Lapata, 2010),distributional memory tensor (Baroni and Lenci,2010) and word embedding (Collobert and Weston,2008)), along with three composition methods (ad-dition, point-wise multiplication, and recursive auto-encoder (Socher et al, 2011)).
They showed that ad-dition over word embeddings is competitive, despiteits simplicity.The second category directly seeks sentence-levelfeatures.
Ji and Eisenstein (2013) explored uni-grams, bigrams and dependency pairs as sentencefeatures.
They proposed TF-KLD to weight fea-tures and used non-negative factorization to learn la-tent sentence representations.
Our method TF-KLD-KNN is an extension of their work.The third line directly computes features for sen-tence pairs.
Wan et al (2006) used N-gram overlap,dependency relation overlap, dependency tree-editdistance and difference of sentence lengths.
Finchet al (2005) and Madnani et al (2012) combinedseveral machine translation metrics.
Das and Smith(2009) presented a generative model over two sen-tences?
dependency trees, incorporating syntax, lex-ical semantics, and hidden loose alignments betweenthe trees to model generating a paraphrase of a givensentence.
Socher et al (2011) used recursive autoen-coders to learn representations for words and wordsequences on each layer of the sentence parsing tree,and then proposed dynamic pooling layer to forma fixed-size matrix as the representation of the twosentences.
Other work representative of this line isby Kozareva and Montoyo (2006), Qiu et al (2006),Ul-Qayyum and Altaf (2012).Our work, first learning unit embeddings, thenadding them to form sentence representations, fi-nally calculating pair features (cosine similarity, ab-solute difference and MT metrics) actually is a com-bination of above three lines.3 Embedding learning for unitsAs explained in Section 1, ?units?
in this work in-clude single words, continuous phrases and discon-tinuous phrases.
Phrases have a larger linguisticgranularity than words and thus will in general con-tain more meaning aspects for a sentence.
For ex-ample, successful detection of continuous phrase?side effects?
and discontinuous phrase ?pick ?
?
?off?
is helpful to understand the sentence meaningcorrectly.
This section focuses on how to detectphrases and how to represent them.3.1 Phrase collectionPhrases defined by a lexicon have not been inves-tigated extensively before in deep learning.
Tocollect canonical phrase set, we extract two-wordphrases defined in Wiktionary1and Wordnet (Millerand Fellbaum, 1998) to form a collection of size95,218.
This collection contains continuous phrases?
phrases whose parts always occur next to eachother (e.g., ?side effects?)
?
and discontinuousphrases ?
phrases whose parts more often occur sep-arated from each other (e.g., ?pick .
.
.
off?
).3.2 Identification of phrase continuityWiktionary and WordNet do not categorize phrasesas continuous or discontinuous.
So we need aheuristic to determine this automatically.For each phrase ?A B?, we compute [c1, c2, c3,c4, c5] where ci, 1 ?
i ?
5, indicates there are cioccurrences of A and B in that order with a distance1http://en.wiktionary.org1369of i.
We compute these statistics for a corpus con-sisting of English Gigaword (Graff et al, 2003) andWikipedia.
We set the maximal distance to 5 be-cause discontinuous phrases are rarely separated bymore than 5 tokens.If c1is 10 times higher than (c2+c3+c4+c5)/4,we classify ?A B?
as continuous, otherwise as dis-continuous.
For example, [c1, .
.
.
, c5] is [1121, 632,337, 348, 4052] for ?pick off?, so c1is smaller thanthe average 1342.25 and ?pick off?
is set as ?discon-tinuous?
; [c1, .
.
.
, c5] is [14831, 16, 177, 331, 3471]for ?Cornell University?, c1is 10 times larger thanthe average and this phrase is set to ?continuous?.We found that that this heuristic for distinguish-ing between continuous and discontinuous phrasesworks well and leave the development of a moreprincipled method for future work.3.3 Sentence reformattingSentence ?.
.
.
A .
.
.
B .
.
.
?
is?
reformatted as ?.
.
.
A B .
.
.
?
if A and B form acontinuous phrase and no word intervenes be-tween them and?
reformatted as ?.
.
.
A B .
.
.
A B .
.
.
?
if A andB form a discontinuous phrase and are sepa-rated by 1 to 4 words.
We replace each ofthe two component words with A B to makethe context of both constituents available to thephrase in learning.This method of phrase detection will generatesome false positives, e.g., if ?pick?
and ?off?
occurin a context like ?she picked an island off the coastof Maine?.
However, our experimental results indi-cate that it is robust enough for our purposes.We run word2vec (Mikolov et al, 2013) on thereformatted Wikipedia corpus to learn embeddingsfor all units.
Embedding size is set to 200.4 Measure of unit discriminativityWe will represent a sentence as the sum of the em-beddings of its units.
Building on Ji and Eisenstein(2013)?s TF-KLD, we want to weight units accord-ing to their ability to discriminate two sentences spe-cific to the paraphrase task.TF-KLD assumes a training set of sentence pairsin the form ?ui, vi, ti?, where uiand videnote thebinary unit occurrence vectors for the sentences inthe ith pair and ti?
{0, 1} is the gold tag.
Then, wedefine pkand qkas follows.?
pk= P (uik|vik= 1, ti= 1).
This is the prob-ability that unit wkoccurs in sentence uigiventhat wkoccurs in its counterpart viand they areparaphrases.?
qk= P (uik|vik= 1, ti= 0).
This is the prob-ability that unit wkoccurs in sentence uigiventhat wkoccurs in its counterpart viand they arenot paraphrases.TF-KLD computes the discriminativity of unit wkas the Kullback-Leibler divergence of the Bernoullidistributions (pk, 1-pk) and (qk, 1-qk)TF-KLD has a serious shortcoming for unknownunits.
Unfortunately, the test data of the commonlyused MSPR corpus in paraphrase task has about 6%unknown words and 62.5% of its sentences containunknown words.
It motivates us to design an im-proved scheme TF-KLD-KNN to reweight the fea-tures.TF-KLD-KNN weights are the same as TF-KLDweights for known units.
For a unit that did not oc-cur in training, TF-KLD-KNN computes its weightas the average of the weights of its k nearest neigh-bors in embedding space, where unit similarity iscalculated by cosine measure.2Word2vec learns word embeddings based on theword context.
The intuition of TF-KLD-KNN isthat words with similar context have similar discrim-inativities.
This enables us to transfer the weightsof features in training data to the unknown featuresin test data, greatly helping to address problems ofsparseness.5 Experiments5.1 Data and baselinesWe use the MSRP corpus (Dolan et al, 2004) forevaluation.
It consists of a training set of 2753 trueparaphrase pairs and 1323 false paraphrase pairs anda test set of 1147 true and 578 false pairs.2Unknown words without embeddings (only seven cases inour experiments) are ignored.
This problem can be effectivelyrelieved by training embedding on larger corpora.1370For our new method, it is interesting to measurethe improvement on the subset of those MSRP sen-tences that contain at least one phrase.
In the stan-dard MSRP corpus, 3027 training pairs (2123 true,904 false) and 1273 test pairs (871 true, 402 false)contain phrases; we denote this subset as subset.We carry out experiments on overall (all MSRP sen-tences) as well as subset cases.We compare six methods for paraphrase identifi-cation.?
NOWEIGHT.
Following Blacoe and Lapata(2012), we simply represent a sentence as theunweighted sum of the embeddings of all itsunits.?
MT is the method proposed by Madnani etal.
(2012): the sentence pair is represented asa vector of eight different machine translationmetrics.?
Ji and Eisenstein (2013).
We reimplementedtheir ?inductive?
setup which is based on ma-trix factorization and is the top-performing sys-tem in paraphrasing task.3The following three methods not only use thisvector of eight MT metrics, but use threekinds of additional features given two sentencerepresentations s1and s2: cosine similarity,element-wise sum s1+s2and element-wise ab-solute difference |s1?
s2|.
We now describehow each of the three methods computes thesentence vectors.?
WORD.
The sentence is represented as the sumof all single-word embeddings, weighted byTF-KLD-KNN.?
WORD+PHRASE.
The sentence is repre-sented as the sum of the embeddings of allits units (including phrases), weighted by TF-KLD-KNN.?
WORD+GOOGLE.
Mikolov et al (2013)use a data-driven method to detect statisticalphrases which are mostly continuous bigrams.3They report even better performance in a ?transductive?setup that makes use of test data.
We only address paraphraseidentification for the case that the test data are not available fortraining the model in this paper.We implement their system by first exploitingword2phrase4to reformat Wikipedia, then us-ing word2vec skip-gram model to train phraseembeddings.We use the same weighting scheme TF-KLD-KNN for the three weighted sum approaches:WORD, WORD+PHRASE and WORD+GOOGLE.Note however that there is an interaction be-tween representation space and nearest neighborsearch.
We limit the neighbor range of unknownwords for WORD to single words; in contrast, wesearch the space of all single words and linguistic(resp.
Google) phrases for WORD+PHRASE (resp.WORD+GOOGLE).We use LIBLINEAR (Fan et al, 2008) as our lin-ear SVM implementation.
20% training data is usedas development data.
Parameter k is fine-tuned ondevelopment set and the best value 3 is finally usedin following reported results.5.2 Experimental resultsTable 1 shows performance for the six methods aswell as for the majority baseline.
In the overall (resp.subset) setup, WORD+PHRASE performs best andoutperforms (Ji and Eisenstein, 2013) by .009 (resp..052) on accuracy.
Interestingly, Ji and Eisen-stein (2013)?s method obtains worse performance onsubset.
This can be explained by the effect of ma-trix factorization in their work: it works less wellfor smaller datasets like subset.
This is a short-coming of their approach.
WORD+GOOGLE has aslightly worse performance than WORD+PHRASE;this suggests that linguistic phrases might be moreeffective than statistical phrases in identifying para-phrases.Cases overall and subset both suggest that phraseembeddings improve sentence representations.
Theaccuracy of WORD+PHRASE is lower on overallthan on subset because WORD+PHRASE has no ad-vantage over WORD for sentences without phrases.5.3 Effectiveness of TF-KLD-KNNThe key contribution of TF-KLD-KNN is that itachieves full coverage of feature weights in the faceof data sparseness.
We now compare four weight-ing methods on overall corpus and with the combi-4https://code.google.com/p/word2vec/1371overall subsetmethod acc F1acc F1baseline .665 .799 .684 .812NOWEIGHT .708 .809 .713 .823MT .774 .841 .772 .839Ji and Eisenstein (2013) .778 .843 .749 .827WORD .775 .839 .776 .843WORD+GOOGLE .780 .843 .795 .853WORD+PHRASE .787 .848?.801 .857?Table 1: Results on overall and subset corpus.
Significantimprovements over MT are marked with ?
(approximaterandomization test, Pad?o (2006), p < .05).method acc F1NOWEIGHT .746 .815TF-IDF .752 .821TF-KLD .774 .842TF-KLD-KNN .787 .848Table 2: Effects of different reweighting methods onoverall.nation of MT features: NOWEIGHT, TF-IDF, TF-KLD, TF-KLDTable 2 suggests that task-specific reweighting ap-proaches (including TF-KLD and TF-KLD-KNN)are superior to unspecific schemes (NOWEIGHTand TF-IDF).
Also, it demonstrates the effectivenessof our weight learning solution for unknown units inparaphrase task.5.4 Reweighting schemes for unseen unitsWe compare our reweighting scheme KNN (i.e., TF-KLD-KNN) with three other reweighting schemes.Zero: zero weight, i.e., ignore unseen units; Type-average: take the average of weights of all knownunit types in test set; Context-average: average ofthe weights of the adjacent known units of the un-known unit (two, one or defaulting to Zero, depend-ing on how many there are).
Figure 1 shows thatKNN performs best.6 ConclusionThis work introduced TF-KLD-KNN, a newreweighting scheme that learns the discriminativi-ties of known as well as unknown units effectively.We further improved paraphrase identification per-KNN Zero Type?average Context?average0.780.7810.7820.7830.7840.7850.7860.7870.788Reweighting methods for unseen unitsAccuracyFigure 1: Performance of different reweighting schemesfor unseen units on overall.formance by the utilization of continuous and dis-continuous phrase embeddings.In future, we plan to do experiments in a cross-domain setup and enhance our algorithm for domainadaptation paraphrase identification.AcknowledgmentsWe are grateful to members of CIS for com-ments on earlier versions of this paper.
Thiswork was supported by Baidu (through a Baiduscholarship awarded to Wenpeng Yin) and byDeutsche Forschungsgemeinschaft (grant DFGSCHU 2246/8-2, SPP 1335).ReferencesMarco Baroni and Alessandro Lenci.
2010.
Distribu-tional memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):673?721.William Blacoe and Mirella Lapata.
2012.
A compari-son of vector-based representations for semantic com-position.
In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 546?556.
Association for Computational Lin-guistics.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof the 25th international conference on Machine learn-ing, pages 160?167.
ACM.Dipanjan Das and Noah A Smith.
2009.
Paraphrase iden-tification as probabilistic quasi-synchronous recogni-tion.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural Language Pro-1372cessing of the AFNLP: Volume 1-Volume 1, pages 468?476.
Association for Computational Linguistics.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised construction of large paraphrase corpora:Exploiting massively parallel news sources.
In Pro-ceedings of the 20th international conference on Com-putational Linguistics, pages 350?356.
Association forComputational Linguistics.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
Liblinear: A libraryfor large linear classification.
The Journal of MachineLearning Research, 9:1871?1874.Andrew Finch, Young-Sook Hwang, and EiichiroSumita.
2005.
Using machine translation evalua-tion techniques to determine sentence-level semanticequivalence.
In Proceedings of the Third InternationalWorkshop on Paraphrasing (IWP2005), pages 17?24.David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.2003.
English gigaword.
Linguistic Data Consortium,Philadelphia.Yangfeng Ji and Jacob Eisenstein.
2013.
Discriminativeimprovements to distributional sentence similarity.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP).Zornitsa Kozareva and Andr?es Montoyo.
2006.
Para-phrase identification on the basis of supervised ma-chine learning techniques.
In Advances in natural lan-guage processing, pages 524?533.
Springer.Nitin Madnani, Joel Tetreault, and Martin Chodorow.2012.
Re-examining machine translation metrics forparaphrase identification.
In Proceedings of the 2012Conference of the North American Chapter of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 182?190.
Association forComputational Linguistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InAdvances in Neural Information Processing Systems,pages 3111?3119.George Miller and Christiane Fellbaum.
1998.
Wordnet:An electronic lexical database.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive science,34(8):1388?1429.Sebastian Pad?o, 2006.
User?s guide to sigf: Signifi-cance testing by approximate randomisation.Long Qiu, Min-Yen Kan, and Tat-Seng Chua.
2006.Paraphrase recognition via dissimilarity significanceclassification.
In Proceedings of the 2006 Conferenceon Empirical Methods in Natural Language Process-ing, pages 18?26.
Association for Computational Lin-guistics.Richard Socher, Eric H Huang, Jeffrey Pennington, An-drew Y Ng, and Christopher D Manning.
2011.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In Advances in Neural In-formation Processing Systems, volume 24, pages 801?809.Zia Ul-Qayyum and Wasif Altaf.
2012.
Paraphrase iden-tification using semantic heuristic features.
ResearchJournal of Applied Sciences, Engineering and Tech-nology, 4(22):4894?4904.Stephen Wan, Mark Dras, Robert Dale, and C?ecile Paris.2006.
Using dependency-based features to take thepara-farce out of paraphrase.
In Proceedings of theAustralasian Language Technology Workshop, volume2006, pages 131?138.1373
