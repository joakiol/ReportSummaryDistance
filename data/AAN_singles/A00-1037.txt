Domain-Specific Knowledge Acquisition from TextDan Mo ldovan ,  Roxana  G i r ju  and  Vas i le  RusDepartment  of Computer  Science and Engineer ingUniversity of Southern Methodist  UniversityDallas, Texas, 75275-0122{ moldovan, roxana, rus} @seas.smu.eduAbst rac tIn many knowledge intensive applications, it is nec-essary to have extensive domain-specific knowledgein addition to general-purpose knowledge bases.This paper presents a methodology for discoveringdomain-specific concepts and relationships in an at-tempt o extend WordNet.
The method was testedon five seed concepts elected from the financialdomain: interest rate, stock market, inflation, eco-nomic growth, and employment.1 Des iderata  fo r  AutomatedKnowledge  Acqu is i t ionThe need for knowledgeThe knowledge is infinite and no matter how large aknowledge base is, it is not possible to store all theconcepts and procedures for all domains.
Even ifthat were possible, the knowledge is generative andthere are no guarantees that a system will have thelatest information all the time.
And yet, if we are tobuild common-sense knowledge processing systemsin the future, it is necessary tohave general-purposeand domain-specific knowledge that is up to date.Our inability to build large knowledge bases withoutmuch effort has impeded many ANLP developments.The most successful current Information Extrac-tion systems rely on hand coded linguistic rules rep-resenting lexico-syntactic patterns capable of match-ing natural anguage xpressions of events.
Sincethe rules are hand-coded it is difficult to port sys-tems across domains.
Question answering, inference,summarization, and other applications can benefitfrom large linguistic knowledge bases.The basic ideaA possible solution to the problem of rapid develop-ment of flexible knowledge bases is to design an au-tomatic knowledge acquisition system that extractsknowledge from texts for the purpose of merging itwith a core ontological knowledge base.
The attemptto create a knowledge base manually is time con-suming and error prone, even for small applicationdomains, and we believe that automatic knowledgeacquisition and classification is the only viable solu-tion to large-scale, knowledge intensive applications.This paper presents an interactive method that ac-quires new concepts and connections a sociated withuser-selected seed concepts, and adds them to theWordNet linguistic knowledge structure (Fellbaum1998).
The sources of the new knowledge are textsacquired from the Internet or other corpora.
At thepresent time, our system works in a semi-automaticmode, in the sense that it acquires concepts and re-lations automatically, but their validation isdone bythe user.We believe that domain knowledge should not beacquired in a vacuum; it should expand an existentontology with a skeletal structure built on consistentand acceptable principles.
The method presented inthis paper is applicable to any Machine ReadableDictionary.
However, we chose WordNet because itis freely available and widely used.Re la ted  workThis work was inspired in part by Marti Hearst'spaper (Hearst 1998) where she discovers manuallylexico-syntactic patterns for the HYPERNYMY rela-tion in WordNet.Much of the work in pattern extraction from textswas done for improving the performance of Infor-mation Extraction systems.
Research in this areawas done by (Kim and Moldovan 1995) (Riloff 1996),(Soderland 1997) and others.The MindNet (Richardson 1998) project at Mi-crosoft is an attempt to transform the Longman Dic-tionary of Contemporary English (LDOCE) into aform of knowledge base for text processing.Woods studied knowledge representation a d clas-sification for long time (Woods 1991), and more re-cently is trying to automate the construction oftax-onomies by extracting concepts directly from texts(Woods 1997).The Knowledge Acquisition from Text (KAT) sys-tem is presented next.
It consists of four parts: (1)discovery of new concepts, (2) discovery of new lex-ical patterns, (3) discovery of new relationships re-flected by the lexical patterns, and (4) the classifi-cation and integration of the knowledge discoveredwith a WordNet - like knowledge base.2682 KAT System2.1 D iscover  new conceptsSelect  seed concepts .
New domain knowledge canbe acquired around some seed concepts that a userconsiders important.
In this paper we focus on thefinancial domain, and use: interest rate, stock mar-ket, inflation, economic growth, and employment asseed concepts.
The knowledge we seek to acquire re-lates to one or more of these concepts, and consistsof new concepts not defined in WordNet and new re-lations that link these concepts with other concepts,some of which are in WordNet.For example, from the sentence: When the USeconomy enters a boom, mortgage interest rates rise,the system discovers: (1) the new concept mortgageinterest rate not defined in WordNet but related tothe seed concept interest rate, and (2) the state ofthe US economy and the value of mortgage interestrate are in a DIRECT RELATIONSHIP.In WordNet, a concept is represented as a synsetthat contains words sharing the same meaning.
Inour experiments, we extend the seed words to theircorresponding synset.
For example, stock market issynonym with stock exchange and securities market,and we aim to learn concepts related to all theseterms, not only to stock market.Ext ract  sentences.
Queries are formed with eachseed concept o extract documents from the Internetand other possible sources.
The documents retrievedare further processed such that only the sentencesthat contain the seed concepts are retained.
Thisway, an arbitrarily large corpus .4 is formed of sen-tences containing the seed concepts.
We limit thesize of this corpus to 1000 sentences per seed con-cept.Parse  sentences.
Each sentence in this corpus isfirst part-of-speech (POS) tagged then parsed.
Weuse Brill's POS tagger and our own parser.
The out-put of the POS tagger for the example above is:When/WRB the/DW U.~./NNP economy/NN en-ters/VBZ a/DT boom/NN ,/, mortgage/NN inter-est_rates/NNS rise/vBP ./.The syntactic parser output is:TOP (S (SBAR (WHADVP (WRB When)) (S (NP (DTthe) (NNP U.S.) (NN economy)) (VP (VBZ enters) (NP(DT a) (NN boom) (, ,))))) (NP (NN mortgage) (NNSinterest_rates)) (VP (VI3P rise)))Ext ract  new concepts .
In this paper only nounconcepts are considered.
Since, most likely, one-word nouns are already defined in WordNet, the fo-cus here is on compound nouns and nouns with mod-ifiers that have meaning but are not in WordNet.The new concepts directly related to the seeds areextracted from the noun phrases (NPs) that containthe seeds.
In the example above, we see that theseed belongs to the NP: mortgage interest rate.This way, a list of NPs containing the seeds isassembled automatically from the parsed texts.
Ev-ery such NP is considered a potential new concept.This is only the "raw material" from which actualconcepts are discovered.In some noun phrases the seed is the head noun,i.e.
\[word, word,..see~, where word can be a noun oran adjective.
For example, \[interest rate\] is in Word-Net, but \[short erm nominal interest rate\] is not inWordNet.
Most of the new concepts related to aseed are generated this way.
In other cases the seedis not the head noun i.e.
\[word, word,..seed, word,wor~.
For example \[interest rate peg\], or \[interna-tional interest rate differentia~.The following procedures are used to discover con-cepts, and are applicable in both cases:Procedure 1.1.
WordNet reduction.
Search NP forwords collocations that are defined in WordNet asconcepts.
Thus \[long term interest rate\] becomes\[long_term interest_rate\], \[prime interest rate\] be-comes \[prime_interest_rate\], as all hyphenated con-cepts are in WordNet.Procedure 1.2.
Dictionary reduction.
For eachNP, search further in other on-line dictionaries formore compound concepts, and if found, hyphen-ate the words.
Many domain-specific dictionariesare available on-line.
For example, \[mortgage inter-est_rate\] becomes \[mortgage_interest_rate\], since it isdefined in the on-line dictionary OneLook Dictionar-ies (http://www.onelook.com).Procedure 1.3.
User validation.
Since currently welack a formal definition of a concept, it is not possibleto completely automate the discovery of concepts.The human inspects the list of noun phrases anddecides whether to accept or decline each concept.2.2 D iscover  lex lco -syntact i c  pat ternsTexts represent a rich source of information fromwhich in addition to concepts we can also discoverrelations between concepts.
We are interested in dis-covering semantic relationships that link the con-cepts extracted above with other concepts, some ofwhich may be in WordNet.
The approach is tosearch for lexico-syntactic patterns comprising theconcepts of interest.
The semantic relations fromWordNet are the first we search for, as it is onlynatural to add more of these relations to enhancethe WordNet knowledge base.
However, since thefocus is on the acquisition of domain-specific knowl-edge, there are semantic relations between conceptsother than the WordNet relations that are impor-tant.
These new relations can be discovered auto-matically from the clauses and sentences in whichthe seeds occur.269Pick a semantic relation R. These can be Word-Net semantic relations or any other elations definedby the user.
So far, we have experimented with theWordNet HYPERNYMY (or so-called IS-A) relation,and three other relations.
By inspecting a few sen-tences containing interest rate one can notice thatINFLUENCE is a frequently used relation.
The twoother relations are CAUSE and EQUIVALENT.Pick a pair of concepts Ci, C# among whichR holds.
These may be any noun concepts.
In thecontext of finance domain, some examples of con-cepts linked by the INFLUENCE relation are:interest rate INFLUENCES earnings, orcredit worthiness INFLUENCES interest rate.Extract lexico-syntactic patterns Ci :P Cj.Search any corpus B, different from ,4 for all in-stances where Ci and Cj occur in the same sentence.Extract the lexico-syntactic patterns that link thetwo concepts.
For example~ from the sentence : Thegraph indicates the impact on earnings from severaldifferent interest rate scenarios, the generally appli-cable pattern extracted is:impact on NP2 from NP1This pattern corresponds unambiguously to the re-lation R we started with, namely INFLUENCE.
Thuswe conclude: INFLUENCE(NPI, NP2).Another example is: As the credit worthiness de-creases, the interest rate increases.
From this sen-tence we extract another lexical pattern that ex-presses the INFLUENCE relation:\[as NP1 vbl, NP2 vb$\] & \[vbl and vb2 are antonyms\]This pattern is rather complex since it contains notonly the lexical part but also the verb condition thatneeds to be satisfied.This procedure repeats for all relations R.2.3 Discover new relationships betweenconceptsLet us denote with Cs the seed-related conceptsfound with Procedures 1.1 through 1.3.
We searchnow corpus ,4 for the occurrence of patterns ~ dis-covered above such that one of their two concepts ia concept Cs.Search corpus ,4 for a pattern ~.
Using a lexico-syntactic pattern P, one at a time, search corpus ,4for its occurrence.
If found, search further whetheror not one of the NPs is a seed-related concept Cs.Identify new concepts Cn.
Part of the pattern 7~are two noun phrases, one of which is Cs.
The headnoun from the other noun phrase is a concept Cn weare looking for.
This may be a WordNet concept,and if it is not it will be added to the list of conceptsdiscovered.Form relation R(Cs, Cn).
Since each pattern 7~ isa linguistic expression of its corresponding seman-tic relation R, we conclude R(Cs,Cn) (this is in-terpreted "C8 is relation R Cn)').
These steps arerepeated for all patterns.User intervention to accept or reject relationshipsis necessary mainly due to our system inability ofhandling coreference r solution and other complexlinguistic phenomena.2.4 Knowledge classification andintegrationNext, a taxonomy needs to be created that is con-sistent with WordNet.
In addition to creating ataxonomy, this step is also useful for validating theconcepts acquired above.
The classification is basedon the subsumption principle (Schmolze and Lipkis1983), (Woods 1991).This algorithm provides the overall steps for theclassification ofconcepts within the context of Word-Net.
Figure 1 shows the inputs of the ClassificationAlgorithm and suggests that the classification is aniterative process.
In addition to WordNet, the in-puts consist of the corpus ,4, the sets of concepts Csand Cn, and the relationships 7~.
Let's denote withC = Cs U Cn the union of the seed related conceptswith the new concepts.
All these concepts need tobe classified.Wo,aN=l C?~Tr~ A Co.=i= ~.
C?, V.=~tio.~=a~\[ I R \[ It Knowledge Classification ?--kAlgorithm '1 ..... ~i;\]Figure 1: The knowledge classification diagramStep 1.
From the set of relationships 7"~ discoveredin Part 3, pick all the HYPERNYMY relations.
Fromthe way these relations were developed, there aretwo possibilities:(1) A HYPERNYMY relation links a WordNet conceptCw with another concept from the set C denotedwith CAw , or(2) A HYPERNYMY relation links a concept Cs witha concept Cn.Concepts C~w are immediately linked to Word-Net and added to the knowledge base.
The conceptsfrom case (2) are also added to the knowledge basebut they form at this point only some isolated islandssince are not yet linked to the rest of the knowledgebase.Step 2.
Search corpus `4 for all the patterns asso-ciated with the HYPERNYMY relation that may link270\[Asian_country interest_rate \]IIS-A TIS-A\[Japan discount_rate \]a)\[country interest_rate \]\[Japan discount_rate \] \[Germany prime interest_rate \]b)Figure 2: Relative classification of two conceptsconcepts in the set Cn with any WordNet concepts.Altough concepts C ,  are not seed-based concepts,they are related to at least one Cs concept via a re-lationship (as found in Task 3).
Here we seek to findHYPERNYMY links between them and WordNet con-cepts.
If such C,~ concepts exist, denote them withC~w.
The union Chw = C~w LJ C2w represents allconcepts from the set C that are linked to WordNetwithout any further effort.
We focus now on the restof concepts, Cc -- C N Chw, that are not yet linkedto any WordNet concepts.Step 3.
Classify all concepts in set Ce using Pro-cedures 4.1 through 4.5 below.Step 4.
Repeat Step 3 for all the concepts in setCc several times till no more changes occur.
Thisreclassification is necessary since the insertion of aconcept into the knowledge base may perturb theordering of other surrounding concepts in the hier-archy.Step 5.
Add the rest of relationships 7~ otherthan the HYPERNYMY to the new knowledge base.The HYPERNYMY relations have already been usedin the Classification Algorithm, but the other rela-tions, i.e.
INFLUENCE, CAUSE and EQUIVALENT needto be added to the knowledge base.Concept  classif ication proceduresProcedure 4.1.
Classify a concept of the form \[word,head\] with respect o concept \[head\].It is assumed here that the \[head\] concept existsin WordNet simply because in many instances the"head" is the "seed" concept, and because frequentlythe head is a single word common oun usually de-fined in WordNet.
In this procedure we consider onlythose head nouns that do not have any hyponymssince the other case when the head has other con-cepts under it is more complex and is treated byProcedure 4.4.
Here "word" is a noun or an adjec-tive.The classification is based on the simple ideathat a compound concept \[word, head\] is onto-logically subsumed by concept \[head\].
For exam-ple, mortgage_interest_rate is a kind of interest_rate,thus linked by a relation nYPERNYMY(interest_rate,mortgage_interest_rate).Procedure 4.2.
Classify a concept \[wordx, headx\]with respect o another concept \[words, head2\].For a relative classification of two such concepts, theontological relations between headz and head2 andbetween word1 and words, if exist, are extended tothe two concepts.
We distinguish ere three possi-bilities:1. heady subsumes heads and word1 subsumesword2.
In this case \[wordz, headl\] subsumes\[word2, heads\].
The subsumption may not al-ways be a direct connection; sometimes it mayconsist of a chain of subsumption relations incesubsumption is (usually) a transitive relation(Woods 1991).
An example is shown in Fig-ure 2a; in WordNet, Asian_country subsumesJapan and interest_rate subsumes discount_rate.A particular case of this is when head1 is iden-tical with head2.2.
Another case is when there is no direct sub-sumption relation in WordNet between word1and words, and/or head1 and heads, but thereare a common subsuming concepts, for eachpair.
When such concepts are found, pickthe most specific common subsumer (MSCS)concepts of word1 and words, and of head1and head2, respectively.
Then form a concept\[MSCS(wordz, words), MSCS(headl, head2)\]and place \[word1 headz\] and \[words heads\] un-der it.
This is exemplified in Figure 2b.
InWordNet, country Subsumes Japan and Ger-many, and interest_rate subsumes discount_rateand prime_interest_rate.3.
In all other cases, no subsumption relation is es-tablished between the two concepts.
For exam-ple, we cannot say whether Asian_country dis-count_rate is more or less abstract hen Japaninterest_rate.Procedure 4.3.
Classify concept \[word1 words head\].Several poss!bilities exist:1.
When there is already a concept \[words head\]in the knowledge base under the \[head\], thenplace \[wordl words head\] under concept \[wordshead\].2.
When there is already a concept \[wordz head\]in the knowledge base under the \[head\], thenplace \[wordl word2 head\] under concept \[wordlhead\].3.
When both cases 1 and 2 are true then place\[wordz word2 head\] under both concepts.2714.
When neither \[wordl head\] nor \[words head\] arein the knowledge base, then place \[wordl word~head\] under the \[head\].
The example in Figure3 corresponds to case 3.components ;y/radio components automobile components /automobile radio componentsFigure 3: Classification of a compound concept with respect oits ~ conceptsSince we do not deal here with the sentence seman-tics, it is not possible to completely determine themeaning of \[word1 word2 head\], as it may be either\[((word1 word2) head)\] or \[(word1 (words head))\] of-ten depending on the sentence context.In the example of Figure 3 there is only one mean-ing, i.e.
\[(automobile radio) components\].
However,in the case of ~erformance skiing equipment\] hereare two valid interpretations, namely \[(performanceskiing) equipment\] and ~erformance (skiing equip-ment)\].Procedure 4.4 Classify a concept \[word1, head\] withrespect o a concept h/erarchy under the ~aead\].The task here is to identify the most specific sub-sumer (MSS) from all the concepts under the headthat subsumes \[wordx, head\].
By default, \[wordlhead\] is placed under \[head\], however, since it maybe more specific than other hyponyms of \[head\], amore complex classification analysis needs to be im-plemented.In the previous work on knowledge classificationit was assumed that the concepts were accompaniedby rolesets and values (Schmolze and Lipkis 1983),(Woods 1991), and others.
Knowledge classifiers arepart of almost any knowledge representation system.However, the problem we face here is more diffi-cult.
While in build-by-hand knowledge representa-tion systems, the relations and values defining con-cepts are readily available, here we have to extractthem from text.
Fortunately, one can take advantageof the glossary definitions that are associated withconcepts in WordNet and other dictionaries.
Oneapproach is to identify a set of semantic relationsinto which the verbs used in the gloss definitions aremapped into for the purpose of working with a man-ageable set of relations that may describe the con-cepts restrictions.
In WordNet these basic relationsare already identified and it is easy to map everyverb into such a semantic relation.As far as the newly discovered concepts are con-cerned, their defining relations need to be retrievedfrom texts.
Human assistance is required, at leastfor now, to pinpoint he most characteristic relationsthat define a concept.Below is a two step algorithm that we envision forthe relative classification of two concepts A and B.Let's us denote with ARaCa and BRbCb the rela-tionships that define concepts A and B respectively.These are similar to rolesets and values.1.
Extract relations (denoted by verbs) be-tween concept and other gloss concepts.ARalC~I BRblCblARa2Ca2 BRb2Cb2AR,~Cam B Rbn Cb,,2.
A subsumes B ff and only if(a) Relations Rai subsume Rbl, for 1 < i < m.(b) Col subsumes or is a meronym of Cbi.
(c) Concept B has more relations than conceptA, i.e.
m<n.Example: In Figure 4 it is shown the classificationof concept monetary policy that has been discovered.By default his concept is placed under policy.
How-ever in WordNet there is a hierarchy fiscal policy -IS-A - economic policy - IS-A - policy.
The questionis where exactly to place monetary policy in this hi-erarchy.The gloss of economic policy indicates that it isMADE BY Government, and that it CONTROLS eco-nomic growth- (here we simplified the explanationand used economy instead of economic growth).
Thegloss of fiscal policy leads to relations MADE BY Gov-ernment, CONTROLS budget, and CONTROLS taxa-tion.
The concept money supply was found by Pro-cedure 1.2 in several dictionaries, and its dictionarydefinition leads to relations MADE BY Federal Gov-ernment, and CONTROLS money supply.
In Word-Net Government subsumes Federal Government, andeconomy HAS PART money.
All necessary conditionsare satisfied for economic policy to subsume mone-tary policy.
However, fiscal policy does not subsumemonetary policy since monetary policy does not con-trol budget or taxation, or any of their hyponyms.Procedure 4.5 Merge a structure of concepts withthe rest of the knowledge base.It is possible that structures consisting of severalinter-connected concepts are formed in isolation ofthe main knowledge base as a result of some proce-dures.
The task here is to merge such structures withthe main knowledge base such that the new knowl-edge base will be consistent with both the struc-ture and the main knowledge base.
This is done by272po~cy| IS-Aeconomic policy .~ .
.
.
.
.
.
.
.
.
.
.
>" made by .
.
.
.
.
.
.
.
:" governmentmonetary policy = - - -> madebyfiscal policy : ~ .
.
.
.
.
.
.
.
.
.
> made by .
.
.
.
.
.
.
.
>- government" ' .
"~" controls .
.
.
.
.
.
.
.
~" budget"-k controls .
.
.
.
.
.
.
.
: "  taxationWordNetbefore mergerwork placet lS-AexchangeI IS-Astock marketindustryt lS-AmarketIS-Amoney marketFigure 4: Classification of the new concept monetary policyWordNetThe new structure from text after mergerTaltarke'eapital~market money market\[IS-Astock marketwork placet lS-AexchangeA~ capital marketIS-stock marketind~su'y\[ IS-Ama\[ketIS-Afinancial marketmoney marketFigure 5: Merging a structure of concepts with WordNetbridging whenever possible the structure conceptsand the main knowledge base concepts.
It is possi-ble that as a result of this merging procedure, someHYPERNYMY relations either from the structure orthe main knowledge base will be destroyed to keepthe consistency.
An example is shown in Figure 5.Example : The following HYPERNYMY relation-ships were discovered in Part 3:HYPERNYMY(financial market,capital market)HYPERNYMY(fInancial market,money market)HYPERNYMY(capital market,stock market)The structure obtained from these relationshipsalong with a part of WordNet hierarchy is shownin Figure 5.
An attempt is made to merge the newstructure with WordNet.
To these relations it cor-responds a structure as shown in Figure 5.
An at-tempt is made to merge this structure with Word-Net.
Searching WordNet for all concepts in thestructure we find money market and stock marketin WordNet where as capital market and financialmarket are not.
Figure 5 shows how the structuremerges with WordNet and moreover how conceptsthat were unrelated in WordNet (i.e.
stock marketand money market) become connected through thenew structure.
It is also interesting to notice that theIS-A link in WordNet from money market o marketis interrupted by the insertion of financial marketin-between them.3 Imp lementat ion  and  Resu l tsThe KAT Algorithm has been implemented, andwhen given some seed concepts, it produces new con-cepts, patterns and relationships between conceptsin an interactive mode.
Table 1 shows the numberof concepts extracted from a 5000 sentence corpus,in which each sentence contains at least one of thefive seed concepts.The NPs were automatically searched in Word-Net and other on-line dictionaries.
There were 3745distinct noun phrases of interest extracted; the restcontained only the seeds or repetitions.
Most of the273\[I Re la t ions  I Lex ico -syntact i c  Pat te rns  ExamplesH WordNet  RelationsHYPERNYMY I NP1 \ [<be>\]  a kind of NP2 Thus, LIBOR is a kind of interest rate, as it  is chargedI ::~ HYPERNYMY(NPI,NP2) on deposits between banks in the Eurodolar market.New Re la t ionsCAUSEINFLUENCENPI  \[<be>\] cause NP2=~ CAUSE(NPI,NP2)NP1 impact on NP2INFLU~NCZ(NP1,NP2)As NP1 vb, so <do> NP2=> INFLUENCE(NPI,NP2)NP1  <be> associated with NP2=> INFLUENCE(NP1,NP2)INFLUENCE(NP2,NPI)As/if/when NP1 vbl, NP2  vb2.
-{-vbl, vb2 ---- antonyms / go inopposite directions::~ INFLUENCE(NPI,NP2)the effect(s) of NP1 on/upon NP2::> INFLUENCE(NPI,NP2)inverse relationship betweenNP I  and NP2=> INFLUENCE(NP1,NP2)=~ INFLUENCE(NP2,NP1)NP2  <be> function of NP1=# INFLUENCZ(NP1,NP2)NP1  (and thus NP2):~ INFLUENCE(NPI,NP2)Phi l l ips,  a Brit ish economist, stated in 1958 that  high inflationcauses low unemployment rates.The Bank of Israel governor said that  the t i ;h t  economic policywould have an immediate impact on inflation th is  year.As the economy picks up steam, so does inflation.Higher interest rates are normal ly associated with weaker bond markets.On the other hand, if interest rates go down, bonds go up,and your bond becomes more valuable.The effects of inflation on debtors and creditors varies as theactual  inflation is compared to the expected one.There exists an inverse relat ionship between unemployment ratesand inflation, best i l lustrated by the Phi l l ips  Curve.Irish employment is also largely a funct ion of the pasthigh birth rate.We believe that  the Treasury bonds (and thus interest rates)are in a downward cycle.Table 2: Examples of lexico-syntactic pat terns  and semant ic  relations derived from the 5000 sentence corpusl a  I b l  c Id  I e IIconcepts  (NPs)  773 382 833 921 .Tota l  concepts  ext rac ted  with ProcedurelConcepts foundin WordNet 2 0 1 0 2Concepts Conceptsfound in with seed 6 0 3 0 0on-line headdictionaries, Conceptsbut not in with seed 7 0 1 1 1WordNet not headI C?ncepts accepted Iby human 78 62 58 60 37Table 1: Results showing the number of new concepts learnedfrom the corpus related to (a) interest rate, (b) stock market, (c)inflation, (d) economic 9rowth, a~ld (e) employment.processing in Part 1 is taken by the parser.
The hu-man intervention to accept or decline concepts takesabout 4 min./seed.The next step was to search for lexico-syntacticpatterns.
We considered one WordNet semantic re-lation, HYPERNYMY and three other relations thatwe found relevant for the domain, namely INFLU-ENCE, CAUSE and EQUIVALENT.
For each relation,a pair of related words was selected and searchedfor on the Internet.
The first 500 sentences/relationwere retained.
A human selected and validated semi-automatically the patterns for each sentence.
A sam-ple of the results is shown in Table 2.
A total of 22patterns were obtained and their selection and vali-dation took approximately 35 minutes/relation.Next, the patterns are searched for on the 5000sentence corpus (Part 3).
The procedure provideda total of 43 new concepts and 166 relationshipsin which at least one of the seeds occurred.
Fromthese relationships, by inspection, we have accepted63 and rejected 102, procedure which took about 7minutes.
Table 3 lists some of the 63 relationshipsdiscovered.Relationships.
ExamplesHYPEaNYMY(interest rate, LIBOR)HYPERNYMY(leading stock market ,New York Stock Exchange)HYPERNYMY(market risks, interest  rate risk)HYPERNYMY(Capital markets,  stock markets)CAUSE(inflation, unemployment)CAUSE(labour shortage, wage inflation)CAUSE(excessive demand,  inflationINFLUENCE_DIRECT_PROPORTIONALYI economy, inflation)INFLUENCE_DIRECT_PROPORT1ONALY sett lements,  interest rate)INFLUENCE..DIRECT..PROPORTIONALY~ U.S. interest rates, dollars)INFLUENCE_DIRECT_PROPORTIONALY~ oil prices, inflation)INFLUENCE_DIRECT_PROPORTIONALY' inflation, nominal  interest rates)INFLUENCE..DIRECT_PROPORTIONALY~ deflation, real interest rates)INFLUENCE-DIRECT-PROPORTIONALY currencles, lnf lat ion)INFLUENCE_INVERSE_PROPORTIONALY unemployment  rates, inflation)INFLUENCE_INVERSE-PKOPOKTIONALY monetary  policies, inflation)INFLUENCE_INVERSE_PROPORTIONALY economy, interest rates)INFLUENCE_INVERSE..PROPORTIONALY inflation, unemployment  rates)INFLUENCE.JNVERSE-PROPORTIONALY credit  worthiness, interest rate)INFLUENCE_INVERSE-PROPORTIONALYlinterest rates, bonds)INFLUENCE(Internal Revenue Service, interest rates)INFLUENCE(economic growth, share prices)EQUIVALENT(big mistakes, high inf lation rates of 1970s)EQUIVALENT(fixed interest rate, coupon)Table 3: A part  of the relat ionships derived from the 5000sentence corpus2744 App l i ca t ionsAn application in need of domain-specific knowledgeis Question Answering.
The concepts and the rela-tionships acquired can be useful in answering dif-ficult questions that normally cannot be easily an-swered just by using the information from WordNet.Consider the processing of the following questions af-ter the new domain knowledge has been acquired:QI: What factors have an impact on the interestrate?Q2: What happens with the employment when theeconomic growth rises?Q3: How does deflation influence prices?"'"'"-...
JISA " ~Figure 6: A sample of concepts and relations acquired from the5000 sentence corpus.
Legend: continue lines represent influenceinverse proportionally, dashed lines represent influence directproportionally, and dotted lines represent influence (the directionof the relationship was not specified in the text).Figure 6 shows a portion of the new domainknowledge that is relevant o these questions.
Thefirst question can be easily answered by extractingthe relationships that point to the concept interestrate.
The factors that influence the interest rate areFed, inflation, economic growth, and employment.The last two questions ask for more detailed infor-mation about the complex relationship among theseconcepts.
Following the path from the deflation con-cept up to prices, the system learns that deflation in-fluences direct proportionally real interest rate, andreal interest rate has an inverse proportional impacton prices.
Both these relationships came from thesentence: Thus, the deflation and the real interestrate are positively correlated, and so a higher realinterest rate leads to falling prices.This method may be adapted to acquire infor-mation when the question concepts are not in theknowledge base.
Procedures may be invoked to dis-cover these concepts and the relations in which theymay be used.5 Conclus ionsThe knowledge acquisition technology describedabove is applicable to any domain, by simply select-ing appropriate seed concepts.
We started with fiveconcepts interest rate, stock market, inflation, eco-nomic growth, and employment and from a corpusof 5000 sentences we acquired a total of 362 con-cepts of which 319 contain the seeds and 43 relateto these via selected relations.
There were 22 dis-tinct le:dco-syntactic patterns discovered used in 63instances.
Most importantly, the new concepts canbe integrated with an existing ontology.The method works in an interactive mode wherethe user accepts or declines concepts, patterns andrelationships.
The manual operation took on aver-age 40 minutes per seed for the 5000 sentence corpus.KAT is useful considering that most of the knowl-edge base construction today is done manually.Complex linguistic phenomena such as corefer-ence resolution, word sense disambiguation, and oth-ers have to be dealt with in order to increase theautomation of the knowledge acquisition system.Without a good handling of these problems the re-sults are not always accurate and human interven-tion is necessary.Re ferencesChristiane Fellbaum.
WordNet - An Electronic LezicalDatabase, MIT Press, Cambridge, MA, 1998.Marti Hearst.
Automated Discovery of WordNet Rela-tions.
In WordNet: An Electronic Lezical Databaseand Some of its Applications, editor Fellbaum, C.,MIT Press, Cambridge, MA, 1998.J.
Kim and D. Moldovan.
Acquisition of LinguisticPatterns for knowledge-based information extraction.IEEE Transactions on Knowledge and Data Engineer-ing 7(5): pages 713-724.R.
MacGregor.
A Description Classifier for the PredicateCalculus.
Proceedings of the 12th National Conferenceon Artificial Intelligence (AAAI94), pp.
213-220, 1994.Stephen D. Richardson, William B. Dolan, Lucy Vander-wende.
MindNet: acquiring and structuring seman-tic information from text.
Proceedings of ACL-Coling1998, pages 1098-1102.Ellen Riloff.
Automatically'Generating Extraction Pat-terns from Untagged Text.
In Proceedings of the Thir-teenth National Conference on Artificial Intelligence,1044-1049.
The AAAI Press/MIT Press.J.G.
Schmolze and T. Lipkis.
Classification in the KL-ONE knowledge representation system.
Proceedingsof 8th Int'l Joint Conference on Artificial Intelligence(IJCAI83), 1983.S.
Soderland.
Learning to extract ext-based informa-tion from the world wide web.
In the Proceedings ofthe Third International Conference on Knowledge Dis-cover# and Data Mining (KDD-97).Text REtrieval Conference.
http://trec.nist.gov 1999W.A.
Woods.
Understanding Subsumption and Taxon-omy: A Framework for Progress.
In the Principlesof Semantic Networks: Explorations in the Represen-tation of Knowledge, Morgan Kaufmann, San Mateo,Calif.
1991, pages 45-94.W.A.
Woods.
A Better way to Organize Knowledge.Technical Report of Sun Microsystems Inc., 1997.275
