Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1382?1392,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsProbabilistic Graph-based Dependency Parsingwith Convolutional Neural NetworkZhisong Zhang1,2, Hai Zhao1,2,?, Lianhui Qin1,21Department of Computer Science and Engineering,Shanghai Jiao Tong University, Shanghai, 200240, China2Key Laboratory of Shanghai Education Commission for Intelligent Interactionand Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China{zzs2011,qinlianhui}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cnAbstractThis paper presents neural probabilisticparsing models which explore up to third-order graph-based parsing with maximumlikelihood training criteria.
Two neuralnetwork extensions are exploited for per-formance improvement.
Firstly, a convo-lutional layer that absorbs the influencesof all words in a sentence is used so thatsentence-level information can be effec-tively captured.
Secondly, a linear layeris added to integrate different order neu-ral models and trained with perceptronmethod.
The proposed parsers are evalu-ated on English and Chinese Penn Tree-banks and obtain competitive accuracies.1 IntroductionNeural network methods have shown greatpromise in the field of parsing and other relatednatural language processing tasks, exploiting morecomplex features with distributed representationand non-linear neural network (Wang et al, 2013;Wang et al, 2014; Cai and Zhao, 2016; Wang etal., 2016).
In transition-based dependency pars-ing, neural models that can represent the partial orwhole parsing histories have been explored (Weisset al, 2015; Dyer et al, 2015).
While for graph-based parsing, on which we focus in this work, Peiet al (2015) also show the effectiveness of neuralmethods.
?Corresponding author.
This paper was partially sup-ported by Cai Yuanpei Program (CSC No.
201304490199and No.
201304490171), National Natural Science Founda-tion of China (No.
61170114 and No.
61272248), NationalBasic Research Program of China (No.
2013CB329401),Major Basic Research Program of Shanghai Science andTechnology Committee (No.
15JC1400103), Art and Sci-ence Interdisciplinary Funds of Shanghai Jiao Tong Univer-sity (No.
14JCRZ04), and Key Project of National SocietyScience Foundation of China (No.
15-ZDA041).The graph-based parser generally consists oftwo components: one is the parsing algorithm forinference or searching the most likely parse tree,the other is the parameter estimation approach forthe machine learning models.
For the former, clas-sical dynamic programming algorithms are usu-ally adopted, while for the latter, there are vari-ous solutions.
Like some previous neural methods(Socher et al, 2010; Socher et al, 2013), to tacklethe structure prediction problems, Pei et al (2015)utilize a max-margin training criterion, which doesnot include probabilistic explanations.
Re-visitingthe traditional probabilistic criteria in log-linearmodels, this work utilizes maximum likelihoodfor neural network training.
Durrett and Klein(2015) adopt this method for constituency pars-ing, which scores the anchored rules with neu-ral models and formalizes the probabilities withtree-structured random fields.
Motivated by thiswork, we utilize the probabilistic treatment for de-pendency parsing: scoring the edges or high-ordersub-trees with a neural model and calculating thegradients according to probabilistic criteria.
Al-though scores are computed by a neural network,the existing dynamic programming algorithms forgradient calculation remain the same as those inlog-linear models.Graph-based methods search globally throughthe whole space for trees and get the highest-scored one, however, the scores for the sub-treesare usually locally decided, considering only sur-rounding words within a limited-sized window.Convolutional neural network (CNN) provides anatural way to model a whole sentence.
By in-troducing a distance-aware convolutional layer,sentence-level representation can be exploited forparsing.
We will especially verify the effec-tiveness of such representation incorporated withwindow-based representation.Graph-based parsing has a natural extension1382through raising its order and higher-order parsersusually perform better.
In previous work on high-order graph-parsing, the scores of high-order sub-trees usually include the lower-order parts in theirhigh-order factorizations.
In traditional linearmodels, combining scores can be implemented byincluding low-order features.
However, for neuralmodels, this is not that straightforward because ofnonlinearity.
A straightforward strategy is simplyadding up all the scores, which in fact works well;another way is stacking a linear layer on the topof the representation from various already-trainedneural parsing models of different orders.This paper presents neural probabilistic mod-els for graph-based projective dependency pars-ing, and explores up to third-order models.
Hereare the three highlights of the proposed methods:?
Probabilistic criteria for neural network train-ing.
(Section 2.2)?
Sentence-level representation learned from aconvolutional layer.
(Section 3.2)?
Ensemble models with a stacked linear out-put layer.
(Section 3.3)Our main contribution is exploring sub-tree scor-ing models which combine local features with awindow-based neural network and global featuresfrom a distance-aware convolutional neural net-work.
A free distribution of our implementationis publicly available1.The remainder of the paper is organized as fol-lows: Section 2 explains the probabilistic modelfor graph-based parsing, Section 3 describes ourneural network models, Section 4 presents our ex-periments and Section 5 discusses related work,we summarize this paper in Section 6.2 Probabilistic Graph-based DependencyParsing2.1 Graph-based Dependency ParsingDependency parsing aims to predict a dependencytree, in which all the edges connect head-modifierpairs.
In graph-based methods, a dependency treeis factored into sub-trees, from single edge to mul-tiple edges with different patterns; we will callthese specified sub-trees factors in this paper.
Ac-cording to the sub-tree size of the factors, we can1https://github.com/zzsfornlp/nnpgdparserh m h s m g h s m1st order 2nd order(sibling)3rd order(grand-sibling)Figure 1: The decompositions of factors.define the order of the graph model.
Three differ-ent ordered factorizations considered in this workand their sub-tree patterns are shown in Figure 1.The score for a dependency tree (T ) is definedas the sum of the scores of all its factors (p):Score(T ) =?p?TScore(p)In this way, the dependency parsing task is tofind a max-scoring tree.
For projective depen-dency parsing considered in this work, this search-ing problem is conquered by dynamic program-ming algorithms with the key assumption that thefactors are scored independently.
Previous work(Eisner, 1996; McDonald et al, 2005; McDonaldand Pereira, 2006; Koo and Collins, 2010; Ma andZhao, 2012) explores ingenious algorithms for de-coding ranging from first-order to higher-orders.Our proposed parsers also take these algorithmsas backbones and use them for inference.2.2 Probabilistic ModelWith the graph factorization and inference, the re-maining problems are how to obtain the scoresand how to train the scoring model.
For the scor-ing models, traditional linear methods utilize man-ually specified features and linear scoring mod-els, while we adopt neural network models, whichmay exploit better feature representations.For the training methods, in recent neuralgraph-based parsers, non-probabilistic margin-based methods are usually used.
However, follow-ing the maximum likelihood criteria in traditionallog-linear models, we can treat it in a probabilisticway.
In fact, the probabilistic treatment still uti-lizes the scores of sub-tree factors in graph mod-els.
As in log-linear models like Conditional Ran-dom Field (CRF) (Lafferty et al, 2001), the expo-nentials of scores are taken before re-normalizing,and the probability distribution over trees condi-1383tioned on a sentence X is defined as follows:Pr(T |X, ?)
=1Z(X)exp(Score(T |?
))Z(X) =?T?exp(Score(T?|?
))where ?
represents the parameters andZ(X) is there-normalization partition function.
The intuitionis that the higher the score is, the more potential ormass it will get, leading to higher probability.The training criteria will be log-likelihood inthe classical setting of maximum likelihood esti-mation, and we define the loss for a parse tree asnegative log-likelihood:L(?)
= ?
log Pr(Tg|X, ?
)= ?Score(Tg|?)
+ log(Z(X))where Tgstands for the golden parse tree.
Nowwe need to calculate the gradients of ?
accordingto gradient-based optimization.
Focusing on thesecond term, we have (some conditions are left outfor simplicity):?
log(Z(X))??=?T?Pr(T?)?p?T??Score(p)??=?p?Score(p)???T?
?T (p)Pr(T?
)Here, T (p) is the set of trees that contain the fac-tor p, and the inner summation is defined as themarginal probability m(p):m(p) =?T?
?T (p)Pr(T?
)which can be viewed as the mass of all the treescontaining the specified factor p. The calculationof m(p) (Paskin, 2001; Ma and Zhao, 2015) issolved by a variant of inside-outside algorithm,which is of the same complexity compared withthe corresponding inference algorithms.
Finally,the gradients can be represented as:?L(?)??=?p?Score(p)??(?
[p ?
Tg]+m(p))where [p ?
Tg] is a binary value which indicateswhether p is in tree Tg.Traditional models usually utilize linear func-tions for the Score function, which might needcarefully feature engineering such as (Zhao et al,2009a; Zhao et al, 2009b; Zhao et al, 2009c;Zhao, 2009; Zhao et al, 2013), while we adoptneural models with the probabilistic training crite-ria unchanged.2.3 Training CriteriaWe take a further look between the maximum-likelihood criteria and the max-margin criteria.For the max-margin method, the loss is the differ-ence between the scores of the golden tree and apredicted tree, and its sub-gradient can be writtenin a similar form:?Lm(?)??=?p?Score(p)??(?
[p ?
Tg]+[p ?
Tb])Here, the predicted tree Tbis the best-scored treewith a structured margin loss in the score.Comparing the derivatives, we can see that theone of probabilistic criteria can be viewed as asoft version of the max-margin criteria, and allthe possible factors are considered when calcu-lating gradients for the probabilistic way, whileonly wrongly predicted factors have non-zero sub-gradients for max-margin training.
This observa-tion is not new and Gimpel and Smith (2010) pro-vide a good review of several training criteria.
Itmight be interesting to explore the impacts of dif-ferent training criteria on the parsing performance,and we will leave it for future research.2.4 Labeled ParsingIn a dependency tree, each edge can be given a la-bel indicating the type of the dependency relation,this labeling procedure can be integrated directlyinto the parsing task, instead of a second pass af-ter obtaining the structure.For the probabilistic model, integrating labeledparsing only needs some extensions for the in-ference procedure and marginal probability cal-culations.
For the simplicity, we only consider asingle label for each factor (even for high-orderones) which corresponds to Model 1 in (Ma andHovy, 2015): the label of the edge between headand modifier word, which will only multiply O(l)to the complexity.
We find this direct approachnot only achieves labeled parsing in one pass,but also improves unlabeled attachment accuracies(see Section 4.3), which may benefit from the jointlearning with the labels.3 Neural ModelThe task for the neural models is computing thelabeled scores of the factors.
The inputs are thewords in a factor with contexts, and the outputsare the scores for this factor to be valid in the de-pendency tree.
We propose neural models to in-1384This      is       a           good      game      .
DT             JJ            NNModifier     Sibling       HeadEmbddingHidden2OutputHidden1s = Wsh2 + bsh2 = tanh(W2h1 + b2)h1 = tanh(W1h0 + b1)h0Figure 2: The architecture for the basic model(second order parsing).tegrate features from both local word-neighboringwindows and the entire sentence, and furthermoreexplore ensemble models with different orders.3.1 Basic Local ModelArchitecture The basic model uses a window-based approach, which includes only surround-ing words for the contexts.
Figure 2 illustrates asecond-order sibling model and models of otherorders adopt similar structures.
It is simply a stan-dard feed-forward neural network with two hid-den layers (h1and h2) above the embedding layer(h0), the hidden layers all adopt tanh activationfunction, and the output layer (noted as s) directlyrepresents the scores for different labels.Feature Sets All the features representing theinput factor are atomic and projected to embed-dings, then the embedding layer is formed by con-catenating them.
There are three categories of fea-tures: word forms, POS (part-of-speech) tags anddistances.
For each node in the factor, word formsand POS tags of the surrounding words in a spec-ified window are also considered.
Special tokensfor start or end of sentences, root node and un-known words are added for both word forms andPOS tags.
Distances can be negative or positive torepresent the relative positions between the factornodes in surface string.
Take the situation for thesecond-order model as an example, there are threenodes in a factor: h for head, m for modifier ands for sibling.
When considering three-word win-dows, there will be three word forms and three tagsfor each node and its surrounding context.
m ands both have one distance feature while h does nothave one as its parent does not exist in the factor.Training As stated in Section 2.2, we use themaximum likelihood criteria.
Moreover, we addtwo L2-regularizations: one is for all the weights??
(biases and embeddings not included) to avoidover-fitting and another is for preventing the finaloutput scores from growing too large.
The for-mer is common practice for neural network, whilethe latter is to set soft limits for the norms of thescores.
Although the second term is not usuallyadopted, it directly puts soft constraints on thescores and improves the accuracies (about 0.1%for UAS/LAS overall) according to our primaryexperiments.
So the final loss function will be:L?(?)
=?p(Score(p) ?(?
[p ?
Tg]+m(p))+ ?s?
Score(p)2)+ ?m?
???
?2where ?mand ?srespectively represent regular-ization parameters for model and scores.
Thetraining process utilizes a mini-batched stochasticgradient descent method with momentum.Comparisons Our basic model resembles theone of Pei et al (2015), but with some ma-jor differences: probabilistic training criteria areadopted, the structures of the proposed networksare different and direction information is encodedin distance features.
Moreover, they simply av-erage embeddings in specified regions for phrase-embedding, while we will include sentence-embedding in convolutional model as follows.3.2 Convolutional ModelTo encode sentence-level information and obtainsentence embeddings, a convolutional layer of thewhole sentence followed by a max-pooling layeris adopted.
However, we intend to score a factorin a sentence and the position of the nodes shouldalso be encoded.
The scheme is to use the distanceembedding for the whole convolution window asthe position feature.We will take the second-order model as an ex-ample to introduce the related operations.
Figure3 shows the convolution operation for a convo-lution window, the input atomic features are theword forms and POS tags for each word insidethe window, and the distances of only the centerword (assuming an odd-sized window) to spec-ified nodes in the factor are adopted as positionfeatures.
In the example, ?game-good-a?
is to bescored as a second-order sibling factor, and for a1385This         is          a             good        game      .
DT        VBZ       DTModifier   Sibling     HeadOutputLexical distancev?l = Wlvl + blvlv?d = Wdvd + bdvddh=-3 dm=-1 ds=-2Figure 3: The operations for one convolution win-dow (second order parsing).convolution window of ?This is a?, word formsand corresponding POS tags are projected to em-beddings and concatenated as the lexical vector vl,the distances of the center word ?is?
to all the threenodes in the factor are also projected to embed-dings and concatenated as the distance vector vd,then these two vectors go through difference lineartransformations into the same dimension and arecombined together through element-wise additionor multiplication.In general, assuming after the projection layer,embeddings of the word forms and POS tags ofthe sentence are represented as [w0,w1, ...,wn?1]and [p0,p1, ...,pn?1].
Those embeddings in thebasic model may be reused here by sharing the em-bedding look-up table.
The second-order siblingfactor to be scored has nodes with indexes of m(modifier), h (head) and s (sibling).
The distanceembeddings are denoted by d, which can be eithernegative or positive.
These distance embeddingsare different from the ones in the basic model, be-cause here we measure the distances between theconvolution window (its center word) and factornodes, while the distances between nodes insidethe factors are measured in the basic model.For a specified window [i : j], always assumingan odd number sized window, and the center tokenis indexed to c =i+j2, the vland vdare obtainedthrough simple concatenation:vl= [wi,pi,wi+1,pi+1, ...,wj,pj]vd= [dc?h,dc?m,dc?s]then vland vdgo through difference linear trans-formations into same dimension space: v?l,v?d?Rn, where n is also the dimension of the outputvector vofor the window.
The linear operationscan be expressed as:v?l= Wl?
vl+ blv?d= Wd?
vd+ bdThe final vector vois obtained by element-wiseoperations of v?land v?d.
We consider two strate-gies: (1) add: simple element-wise addition, (2)mul: element-wise multiplication with v?dacti-vated by tanh.
They can be formalized as:vo-add= v?l?
v?dvo-mul= v?ltanh(v?d)All the windows whose center-located word isvalid (exists) in the sentence are considered andwe will get a sequence of convolution outputswhose number is the same as the sentence length.The convolution outputs (all vo) are collapsed intoone global vector vgusing a standard max-poolingoperation.
Finally, for utilizing the sentence-levelrepresentation in the basic model, we can eitherreplace the original first hidden layer h1with vgor concatenate vgto h1for combining local andglobal features.3.3 Ensemble ModelsFor higher-order dependency parsing, it is a stan-dard practice to include the impact of lower-orderparts in the scoring of higher-order factors, whichactually is an ensemble method of different ordermodels for scoring.A simple adding scheme is often used.
For non-linear neural models, we use an explicit addingmethod.
For example, in third-order parsing, thefinal score for the factor (g, h,m, s) will be:sadd(g, h,m, s) = so3(g, h,m, s) + so2(h,m, s)+ so1(h,m)Here, g, h, m and s represent the grandparent,head, modifier and sibling nodes in the grand-sibling third-order factor; so1, so2and so3stand forthe corresponding lower-order scores from first,second and third order models, respectively.We notice that ensemble or stacking methodsfor dependency parsing have explored in previouswork (Nivre and McDonald, 2008; Torres Martinset al, 2008).
Recently, Weiss et al (2015) stack alinear layer for the final scoring in a single model,and we extend this method to combine multiplemodels by stacking a linear layer on their outputand hidden layers.
The simple adding scheme can1386be viewed as adopting a final layer with speciallyfixed weights.For each model to be combined, we concatenatethe output layer and all hidden layers (except em-bedding layer h0):vall= [s,h1,h2]All vallfrom different models are again concate-nated to form the input for the final linear layerand the final scores are obtained through a lineartransformation (no bias adding):vcombine= [vall-o1,vall-o2,vall-o3]scombine= Wcombine?
vcombineWe no longer update weights for the underlyingneural models, and the learning of the final layeris equally training a linear model, for which struc-tured average perceptron (Collins, 2002; Collinsand Roark, 2004) is adopted for simplicity.This ensemble scheme can be extended in sev-eral ways which might be explored in future work:(1) feed-forward network can be stacked ratherthan a single linear layer, (2) traditional sparse fea-tures can also be concatenated to vcombineto com-bine manually specified representations with dis-tributed neural representations as in (Zhang andZhang, 2015).4 ExperimentsThe proposed parsers are evaluated on EnglishPenn Treebank (PTB) and Chinese Penn Tree-bank (CTB).
Unlabeled attachment scores (UAS),labeled attachment scores (LAS) and unlabeledcomplete matches (CM) are the metrics.
Punctu-ations2are ignored as in previous work (Koo andCollins, 2010; Zhang and Clark, 2008).For English, we follow the splitting conven-tion for PTB3: sections 2-21 for training, 22 fordeveloping and 23 for test.
We prepare threedatasets of PTB, using different conversion tools:(1) Penn2Malt3and the head rules of Yamada andMatsumoto (2003), noted as PTB-Y&M; (2) de-pendency converter in Stanford parser v3.3.0 withStanford Basic Dependencies (De Marneffe et al,2006), noted as PTB-SD; (3) LTH Constituent-to-Dependency Conversion Tool4(Johansson and2Tokens whose gold POS tags are one of {?
?
: , .}
forPTB or PU for CTB.3http://stp.lingfil.uu.se/?nivre/research/Penn2Malt.html4http://nlp.cs.lth.se/software/treebank converterNugues, 2007), noted as PTB-LTH.
We use Stan-ford POS tagger (Toutanova et al, 2003) to getpredicted POS tags for development and test sets,and the accuracies for their tags are 97.2% and97.4%, respectively.For Chinese, we adopt the splitting conventionfor CTB5 described in (Zhang and Clark, 2008).The dependencies (noted as CTB), are convertedwith the Penn2Malt converter.
Gold segmentationand POS tags are used as in previous work.4.1 SettingsSettings of our models will be described in thissub-section, including pre-processing and initial-izations, hyper-parameters, and training details.We ignore the words that occur less than 3 timesin the training treebank and use a special tokento replace them.
For English parsing, we initial-ize word embeddings with word vectors trained onWikipedia using word2vec (Mikolov et al, 2013);all other weights and biases are initialized ran-domly with uniform distribution.For the structures of neural models, all the em-beddings (word, POS and distances) have dimen-sions of 50.
For basic local models, h1and h2areset to 200 and 100, and the local window size is setto 7.
For convolutional models, a three-word-sizedwindow for convolution is specified, and convolu-tion output dimension (number of filters) is 100.When concatenating the convolution vector (afterpooling) to h1, it will make the first hidden layer?sdimension 300.For the training of neural network, we set theinitial learning rate to 0.1 and the momentum to0.6.
After each iteration, the parser is tested onthe development set and if the accuracy decreases,the learning rate will be halved.
The learning ratewill also be halved if no decreases of the accuracyfor three epochs.
We train the neural models for12 epochs and select the one that performs best onthe development set.
The regularization parame-ters ?mand ?sare set to 0.0001 and 0.001.
For theperceptron training of the ensemble model, onlyone epoch is enough based on the results of thedevelopment set.The runtime of the model is influenced by thehyper-parameter setting.
According to our ex-periments, using dual-core on 3.0 GHz i7 CPU,the training costs 6 to 15 hours for different-ordermodels and the testing is comparably efficient asrecent neural graph-parsers.
The calculation of the1387Method UAS LAS CMBasic (first-order)Unlabeled 91.53 ?
42.82Labeled 92.13 89.60 45.06Labeled+pre-training 92.19 89.73 45.18Convolutional (first-order)replace-add 92.26 89.83 44.76replace-mul 92.02 89.61 44.24concatenate-add 92.63 90.20 46.18concatenate-mul 92.33 89.83 44.94Higher-orderso2-nope 92.85 90.51 49.65o2-adding 93.47 91.13 51.41o2-perceptron 93.63 91.39 51.53o3-nope 92.47 90.01 49.06o3-adding 93.70 91.37 53.53o3-perceptron 93.51 91.20 51.76Table 1: Effects of the components, on PTB-SDdevelopment set.convolution model approximately takes up 40%of all computations.
The convolution operationindeed costs more, but the lexical parts v?lofthe convolution do not concern the factors andare computed only once for one sentence, whichmakes it less computationally expensive.4.2 PruningFor high-order parsing, the computation cost risesin proportion to the length of the sentence, andit will be too expensive to calculate scores for allthe factors.
Fortunately, many edges are quite un-likely to be valid and can be pruned away usinglow-order models.
We follow the method of Kooand Collins (2010) and directly use the first-orderprobabilistic neural parser for pruning.
We com-pute the marginal probability m(h,m) for eachedge and prune away the edges whose marginalprobability is below ?maxh?m(h?,m).
 meansthe pruning threshold that is set to 0.0001 forsecond-order.
For third-order parsing, consideringthe computational cost, we set it to 0.001.4.3 Model AnalysisThis section presents experiments to verify the ef-fectiveness of the proposed methods and only thePTB-SD development set will be used in these ex-periments, which fall into three groups concerningbasic models, convolutional models and ensembleones, as shown in Table 1.The first group focuses on the basic local mod-els of first order.
The first two, Unlabeled andLabeled, do not use pre-training vectors for ini-tialization, while the third, Labeled+pre-training,utilizes them.
The Unlabeled does not utilize the0.650.70.750.80.850.90.9515  10  15  20  25  30F1Dependency Lengthlabeled+pre-training (no CNN)replace-add (only CNN)concatenate-add (plus CNN)Figure 4: F1 measure of different dependencylengths, on PTB-SD development set.labels in training set and its model only givesone dependency score (we do not train a secondstage labeling model, so the LAS of the unlabeledone is not available) and the Labeled directly pre-dicts the scores for all labels.
We can see thatlabeled parsing not only demonstrates the conve-nience of outputting dependency relations and la-bels for once, but also obtains better parsing per-formances.
Also, we observe that pre-trained wordvectors bring slight improvements.
Pre-trainedinitialization and labeled parsing will be adoptedfor the next two groups and the rest experiments.Next, we explore the effectiveness of the CNNenhancement.
In the four entries of this group,concatenate or replace means whether to concate-nate the sentence-level vector vgto the first hid-den layer h1or just replace it (just throw awaythe representation from basic models), add or mulmeans to use which way for attaching distance in-formation.
Surprisingly, simple adding methodsurpasses the more complex multiplication-with-activation method, which might indicate that thedirect activation operation may not be suitable forencoding distance information.
With no surprises,the concatenating method works better because itcombines both the local window-based and globalsentence-level information.
We also explore theinfluences of the convolution operations on depen-dencies of different lengths, as shown in Figure4, the convolutional methods help the decisions oflong-range dependencies generally.
For the high-order parsing in the rest of this paper, we will alladopt the concatenate-add setting.In the third group, we can see that high-orderparsing brings significant performance improve-ment.
For high-order parsing, three ensembleschemes are examined: no combination, adding1388PTB-Y&M PTB-SD PTB-LTH CTBMethods UAS LAS CM UAS LAS CM UAS LAS CM UAS LAS CMGraph-NN:proposedo3-adding 93.20 92.12 48.92 93.42 91.29 50.37 93.14 90.07 43.38 87.55 86.19 35.65o3-perceptron 93.31 92.23 50.00 93.42 91.26 49.92 93.12 89.53 43.83 87.65 86.17 36.07Graph-NN:othersPei et al (2015) 93.29 92.13 ?
?
?
?
?
?
?
?
?
?Fonseca and Alu?
?sio (2015) ?
?
?
?
?
?
91.6?
88.9?
?
?
?
?Zhang and Zhao (2015) ?
?
?
?
?
?
92.52 ?
41.10 86.01 ?
31.88Graph-LinearKoo and Collins (2010) 93.04 ?
?
?
?
?
?
?
?
?
?
?Martins et al (2013) 93.07 ?
?
92.82 ?
?
?
?
?
?
?
?Ma and Zhao (2015) 93.0?
?
48.8?
?
?
?
?
?
?
87.2?
?
37.0?Transition-NNChen and Manning (2014) ?
?
?
91.8?
89.6?
?
92.0?
90.7?
?
83.9?
82.4?
?Dyer et al (2015) ?
?
?
93.1?
90.9?
?
?
?
?
87.2?
85.7?
?Weiss et al (2015) ?
?
?
93.99 92.05 ?
?
?
?
?
?
?Zhou et al (2015) 93.28 92.35 ?
?
?
?
?
?
?
?
?
?Table 2: Comparisons of results on the test sets.and stacking another linear perceptron layer (withthe suffixes of -nope, -adding and -perceptron re-spectively).
The results show that model ensembleimproves the accuracies quite a few.
For third-order parsing, the no-combination method per-forms quite poorly compared to the others, whichmay be caused by the relative strict setting of thepruning threshold.
Nevertheless, with model en-semble, the third-order models perform better thanthe second-order ones.
Though the perceptronstrategy does not work well for third-order pars-ing in this dataset, it is still more general than thesimple adding method, since the latter can be seenas a special parameter setting of the former.4.4 ResultsWe show the results of two of the best proposedparsers: third-order adding (o3-adding) and third-order perceptron (o3-perceptron) methods, andcompare with the reported results of some previ-ous work in Table 2.
We compare with three cat-egories of models: other Graph-based NN (neu-ral network) models, traditional Graph-based Lin-ear models and Transition-based NN models.
ForPTB, there have been several different dependencyconverters which lead to different sets of depen-dencies and we choose three of the most popularones for more comprehensive comparisons.
Sincenot all work report results on all of these depen-dencies, some of the entries might be not available.From the comparison, we see that the pro-posed parser has output competitive performancefor different dependency conversion conventionsand treebanks.
Compared with traditional graph-based linear models, neural models may benefitfrom better feature representations and more gen-eral non-linear transformations.The results and comparisons in Table 2 demon-strate the proposed models can obtain comparableaccuracies, which show the effectiveness of com-bining local and global features through window-based and convolutional neural networks.5 Related WorkCNN has been explored in recent work of rela-tion classification (Zeng et al, 2014; Chen et al,2015), which resembles the task of deciding de-pendency relations in parsing.
However, relationclassification usually involves labeling for givenarguments and seldom needs to consider the globalstructure.
Parsing is more complex for it needs topredict structures and the use of CNN should beincorporated with the searching algorithms.Neural network methods have been proved ef-fective for graph-based parsing.
Lei et al (2014)explore a tensor scoring method, however, it needsto combine scores from linear models and weare not able to compare with it because of dif-ferent datasets (they take datasets from CoNLLshared task).
Zhang and Zhao (2015) also ex-plore a probabilistic treatment, but its model maygive mass to illegal trees or non-trees.
Fonsecaand Alu?
?sio (2015) utilize CNN for scoring edges,though only explore first-order parsing.
Its modelis based on head selection for each modifier andmight be difficult to be extended to high-orderparsing.
Recently, several neural re-ranking mod-els, like Inside-Outside Recursive Neural Network1389(Le and Zuidema, 2014) and Recursive CNN (Zhuet al, 2015), are utilized for capturing featureswith more contexts.
However, re-ranking mod-els depend on the underlying base parsers, whichmight already miss the correct trees.
Generally,the re-ranking techniques play a role of additionalenhancement for basic parsing models, and there-fore they are not included in our comparisons.The conditional log-likelihood probabilistic cri-terion utilized in this work is actually a (condi-tioned) Markov Random Field for tree structures,and it has been applied to parsing since long timeago.
Johnson et al (1999) utilize the Markov Ran-dom Fields for stochastic grammars and gradientbased methods are adopted for parameter estima-tions, and Geman and Johnson (2002) extend thiswith dynamic programming algorithms for infer-ence and marginal-probability calculation.
Collins(2000) uses the same probabilistic treatment forre-ranking and the denominator only includes thecandidate trees which can be seen as an approx-imation for the whole space of trees.
Finkel etal.
(2008) utilize it for feature-based parsing.
Theprobabilistic training criterion for linear graph-based dependency models have been also exploredin (Li et al, 2014; Ma and Zhao, 2015).
How-ever, these previous methods usually exploit log-linear models utilizing sparse features for inputrepresentations and linear models for score calcu-lations, which are replaced by more sophisticateddistributed representations and neural models, asshown in this work.6 ConclusionsThis work presents neural probabilistic graph-based models for dependency parsing, togetherwith a convolutional part which could capture thesentence-level information.
With distributed vec-tors for representations and complex non-linearneural network for calculations, the model can ef-fectively capture more complex features when de-ciding the scores for sub-tree factors and exper-iments on standard treebanks show that the pro-posed techniques improve parsing accuracies.ReferencesDeng Cai and Hai Zhao.
2016.
Neural word segmen-tation learning for Chinese.
In Proceedings of ACL,Berlin, Germany, August.Danqi Chen and Christopher Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of EMNLP, pages 740?750,Doha, Qatar, October.Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, andJun Zhao.
2015.
Event extraction via dynamicmulti-pooling convolutional neural networks.
InProceedings of ACL, pages 167?176, Beijing, China,July.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Pro-ceedings of the 42nd Meeting of the Association forComputational Linguistics (ACL?04), Main Volume,pages 111?118, Barcelona, Spain, July.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proceedings of theSeventeenth International Conference on MachineLearning, pages 25?70.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing-Volume 10, pages 1?8.Marie-Catherine De Marneffe, Bill MacCartney,Christopher D Manning, et al 2006.
Generat-ing typed dependency parses from phrase structureparses.
In Proceedings of LREC, volume 6, pages449?454.Greg Durrett and Dan Klein.
2015.
Neural crf pars-ing.
In Proceedings of ACL, pages 302?312, Bei-jing, China, July.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-term memory.
In Proceedings of ACL, pages 334?343, Beijing, China, July.Jason M. Eisner.
1996.
Three new probabilistic mod-els for dependency parsing: An exploration.
In Pro-ceedings of the 16th International Conference onComputational Linguistics, pages 340?345, Copen-hagen, August.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, condi-tional random field parsing.
In Proceedings of ACL,pages 959?967, Columbus, Ohio, June.Erick Fonseca and Sandra Alu??sio.
2015.
A deeparchitecture for non-projective dependency parsing.In Proceedings of the 1st Workshop on Vector SpaceModeling for Natural Language Processing, pages56?61, Denver, Colorado, June.Stuart Geman and Mark Johnson.
2002.
Dynamicprogramming for parsing and estimation of stochas-tic unification-based grammars.
In Proceedings of40th Annual Meeting of the Association for Com-putational Linguistics, pages 279?286, Philadelphia,Pennsylvania, USA, July.1390Kevin Gimpel and Noah A. Smith.
2010.
Softmax-margin crfs: Training log-linear models with costfunctions.
In Proceedings of NAACL, pages 733?736, Los Angeles, California, June.Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion for en-glish.
In 16th Nordic Conference of ComputationalLinguistics, pages 105?112.
University of Tartu.Mark Johnson, Stuart Geman, Stephen Canon, ZhiyiChi, and Stefan Riezler.
1999.
Estimators forstochastic ?unification-based?
grammars.
In Pro-ceedings of the 37th Annual Meeting of the Associa-tion for Computational Linguistics, pages 535?541,College Park, Maryland, USA, June.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of ACL,pages 1?11, Uppsala, Sweden, July.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.Phong Le and Willem Zuidema.
2014.
The inside-outside recursive neural network model for depen-dency parsing.
In Proceedings of EMNLP, pages729?739, Doha, Qatar, October.Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, andTommi Jaakkola.
2014.
Low-rank tensors for scor-ing dependency structures.
In Proceedings of ACL,pages 1381?1391, Baltimore, Maryland, June.Zhenghua Li, Min Zhang, and Wenliang Chen.2014.
Ambiguity-aware ensemble training for semi-supervised dependency parsing.
In Proceedings ofACL, pages 457?467, Baltimore, Maryland, June.Xuezhe Ma and Eduard Hovy.
2015.
Efficient inner-to-outer greedy algorithm for higher-order labeleddependency parsing.
In Proceedings of EMNLP,pages 1322?1328, Lisbon, Portugal, September.Xuezhe Ma and Hai Zhao.
2012.
Fourth-order depen-dency parsing.
In Proceedings of COLING, pages785?796, Mumbai, India, December.Xuezhe Ma and Hai Zhao.
2015.
Probabilistic modelsfor high-order projective dependency parsing.
arXivpreprint arXiv:1502.04174.Andre Martins, Miguel Almeida, and Noah A. Smith.2013.
Turning on the turbo: Fast third-order non-projective turbo parsers.
In Proceedings of ACL,pages 617?622, Sofia, Bulgaria, August.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proceedings of EACL, pages 81?88.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL, pages 91?98, Ann Arbor, Michigan, June.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in neural information processingsystems, pages 3111?3119.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proceedings of ACL, pages 950?958,Columbus, Ohio, June.Mark A Paskin.
2001.
Cubic-time parsing and learningalgorithms for grammatical bigram models.
Techni-cal report.Wenzhe Pei, Tao Ge, and Baobao Chang.
2015.
Aneffective neural network model for graph-based de-pendency parsing.
In Proceedings of ACL, pages313?322, Beijing, China, July.Richard Socher, Christopher D. Manning, and An-drew Y. Ng.
2010.
Learning continuous phraserepresentations and syntactic parsing with recursiveneural networks.
In Proceedings of the NIPS-2010Deep Learning and Unsupervised Feature LearningWorkshop.Richard Socher, John Bauer, Christopher D. Manning,and Ng Andrew Y.
2013.
Parsing with compo-sitional vector grammars.
In Proceedings of ACL,pages 455?465, Sofia, Bulgaria, August.Andr?e Filipe Torres Martins, Dipanjan Das, Noah A.Smith, and Eric P. Xing.
2008.
Stacking depen-dency parsers.
In Proceedings of ENNLP, pages157?166, Honolulu, Hawaii, October.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumita,Hai Zhao, and Bao-Liang Lu.
2013.
Convert-ing continuous-space language models into n-gramlanguage models for statistical machine translation.In Proceedings of EMNLP, pages 845?850, Seattle,Washington, USA, October.Rui Wang, Hai Zhao, Bao-Liang Lu, Masao Utiyama,and Eiichiro Sumita.
2014.
Neural network basedbilingual language model growing for statistical ma-chine translation.
In Proceedings of EMNLP, pages189?195, Doha, Qatar, October.Peilu Wang, Yao Qian, Frank Soong, Lei He, and HaiZhao.
2016.
Learning distributed word representa-tions for bidirectional lstm recurrent neural network.In Proceedings of NAACL, June.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural networktransition-based parsing.
In Proceedings of ACL,pages 323?333, Beijing, China, July.1391Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proceedings of IWPT, volume 3, pages195?206.Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,and Jun Zhao.
2014.
Relation classification via con-volutional deep neural network.
In Proceedings ofCOLING, pages 2335?2344, Dublin, Ireland, Au-gust.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: Investigating and combining graph-based and transition-based dependency parsing.
InProceedings of EMNLP, pages 562?571, Honolulu,Hawaii, October.Meishan Zhang and Yue Zhang.
2015.
Combiningdiscrete and continuous features for deterministictransition-based dependency parsing.
In Proceed-ings of EMNLP, pages 1316?1321, Lisbon, Portu-gal, September.Zhisong Zhang and Hai Zhao.
2015.
High-ordergraph-based neural dependency parsing.
In Pro-ceedings of the 29th Pacific Asia Conference on Lan-guage, Information, and Computation, pages 114?123, Shanghai, China, October.Hai Zhao, Wenliang Chen, and Chunyu Kit.
2009a.Semantic dependency parsing of NomBank andPropBank: An efficient integrated approach via alarge-scale feature selection.
In Proceedings ofEMNLP, pages 30?39, Singapore, August.Hai Zhao, Wenliang Chen, Chunyu Kity, and GuodongZhou.
2009b.
Multilingual dependency learning:A huge feature engineering method to semantic de-pendency parsing.
In Proceedings of CoNLL, pages55?60, Boulder, Colorado, June.Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.2009c.
Cross language dependency parsing using abilingual lexicon.
In Proceedings of ACL, pages 55?63, Suntec, Singapore, August.Hai Zhao, Xiaotian Zhang, and Chunyu Kit.
2013.
In-tegrative semantic dependency parsing via efficientlarge-scale feature selection.
Journal of ArtificialIntelligence Research, 46:203?233.Hai Zhao.
2009.
Character-level dependencies in chi-nese: Usefulness and learning.
In Proceedings ofEACL, pages 879?887, Athens, Greece, March.Hao Zhou, Yue Zhang, Shujian Huang, and JiajunChen.
2015.
A neural probabilistic structured-prediction model for transition-based dependencyparsing.
In Proceedings of ACL, pages 1213?1222,Beijing, China, July.Chenxi Zhu, Xipeng Qiu, Xinchi Chen, and XuanjingHuang.
2015.
A re-ranking model for dependencyparser with recursive convolutional neural network.In Proceedings of ACL, pages 1159?1168, Beijing,China, July.1392
