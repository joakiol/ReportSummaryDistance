Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 943?952,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsJoint Generation of Transliterations from Multiple RepresentationsLei Yao and Grzegorz KondrakDepartment of Computing ScienceUniversity of AlbertaEdmonton, AB, Canada{lyao1,gkondrak}@ualberta.caAbstractMachine transliteration is often referred toas phonetic translation.
We show thattransliterations incorporate information fromboth spelling and pronunciation, and pro-pose an effective model for joint transliter-ation generation from both representations.We further generalize this model to includetransliterations from other languages, and en-hance it with reranking and lexicon features.We demonstrate significant improvements intransliteration accuracy on several datasets.1 IntroductionTransliteration is the conversion of a text from onescript to another.
When a new name like Ey-jafjallaj?okull appears in the news, it needs to bepromptly transliterated into dozens of languages.Computer-generated transliterations can be more ac-curate than those created by humans (Sherif andKondrak, 2007).
When the names in question origi-nate from languages that use the same writing scriptas the target language, they are likely to be copiedverbatim; however, their pronunciation may still beambiguous.
Existing transliterations and transcrip-tions can help in establishing the correct pronuncia-tion (Bhargava and Kondrak, 2012).Transliteration is often defined as phonetic trans-lation (Zhang et al, 2012).
In the idealized modelof Knight and Graehl (1997), a bilingual expert pro-nounces a name in the source language, modifies thepronunciation to fit the target language phonology,and writes it down using the orthographic rules ofthe target script.
In practice, however, it may be dif-ficult to guess the correct pronunciation of an unfa-miliar name from the spelling.Phonetic-based models of transliteration tend toachieve suboptimal performance.
Al-Onaizan andKnight (2002) report that a spelling-based modeloutperforms a phonetic-based model even when pro-nunciations are extracted from a pronunciation dic-tionary.
This can be attributed to the importanceof the source orthography in the transliteration pro-cess.
For example, the initial letters of the Russiantransliterations of the names Chicano ([tSIkAno]) andChicago ([SIkAgo]) are identical, but different fromShilo ([SIlo]).
The contrast is likely due to the id-iosyncratic spelling of Chicago.Typical transliteration systems learn direct ortho-graphic mapping between the source and the tar-get languages from parallel training sets of wordpairs (Zhang et al, 2012).
Their accuracy is lim-ited by the fact that the training data is likely to con-tain names originating from different languages thathave different romanization rules.
For example, theRussian transliterations of Jedi, Juan, Jenins, Jelto-qsan, and Jecheon all differ in their initial letters.In addition, because of inconsistent correspondencesbetween letters and phonemes in some languages,the pronunciation of a word may be difficult to de-rive from its orthographic form.We believe that transliteration is not simply pho-netic translation, but rather a process that combinesboth phonetic and orthographic information.
Thisobservation prompted the development of severalhybrid approaches that take advantage of both typesof information, and improvements were reported onsome test corpora (Al-Onaizan and Knight, 2002;Bilac and Tanaka, 2004; Oh and Choi, 2005).
Thesemodels, which we discuss in more detail in Sec-tion 2.1, are well behind the current state of the artin machine transliteration.943In this paper, we conduct experiments that showthe relative importance of spelling and pronuncia-tion.
We propose a new hybrid approach of jointtransliteration generation from both orthography andpronunciation, which is based on a discriminativestring transduction approach.
We demonstrate thatour approach results in significant improvements intransliteration accuracy.
Because phonetic transcrip-tions are rarely available, we propose to capture thephonetic information from supplemental transliter-ations.
We show that the most effective way ofutilizing supplemental transliterations is to directlyinclude their original orthographic representations.We show improvements of up to 30% in word accu-racy when using supplemental transliterations fromseveral languages.The paper is organized as follows.
We discussrelated work in Section 2.
Section 3 describes ourhybrid model and a generalization of this modelthat leverages supplemental transliterations.
Sec-tion 4 and 5 present our experiments of joint genera-tion with supplemental transcriptions and transliter-ations, respectively.
Section 6 presents our conclu-sions and future work.2 Related workIn this section, we focus on hybrid transliterationmodels, and on methods of leveraging supplemen-tal transliterations.2.1 Hybrid modelsAl-Onaizan and Knight (2002) present a hybridmodel for Arabic-to-English transliteration, whichis a linear combination of phoneme-based andgrapheme-based models.
The hybrid model isshown to be superior to the phoneme-based model,but inferior to the grapheme-based model.Bilac and Tanaka (2004) propose a hybrid modelfor Japanese-to-English back-transliteration, whichis also based on linear interpolation, but the interpo-lation is performed during the transliteration genera-tion process, rather than after candidate target wordshave been generated.
They report improvement overthe two component models on some, but not all, oftheir test corpora.Oh and Choi (2005) replace the fixed linear inter-polation approach with a more flexible model thattakes into account the correspondence between thephonemes and graphemes during the transliterationgeneration process.
They report superior perfor-mance of their hybrid model over both componentmodels.
However, their model does not consider thecoherence of the target word during the generationprocess, nor other important features that have beenshown to significantly improve machine translitera-tion (Li et al, 2004; Jiampojamarn et al, 2010).Oh et al (2009) report that their hybrid modelsimprove the accuracy of English-to-Chinese translit-eration.
However, since their focus is on investigat-ing the influence of Chinese phonemes, their hybridmodel is again a simple linear combination of basicmodels.2.2 Leveraging supplemental transliterationsPrevious work that explore the idea of taking advan-tage of data from additional languages tend to em-ploy supplemental transliterations indirectly, ratherthan to incorporate them directly into the generationprocess.Khapra et al (2010) propose a bridge approach oftransliterating low-resource language pair (X,Y ) bypivoting on an high-resource language Z, with theassumption that the pairwise data between (X,Z)and (Y,Z) is relatively large.
Their experimentsshow that pivoting on Z results in lower accuracythan directly transliterating X into Y .
Zhang et al(2010) and Kumaran et al (2010) combine the pivotmodel with a grapheme-based model, which worksbetter than either of the two approaches alone.
How-ever, their model is not able to incorporate more thantwo languages.Bhargava and Kondrak (2011) propose a rerank-ing approach that uses supplemental translitera-tions to improve grapheme-to-phoneme conversionof names.
Bhargava and Kondrak (2012) generalizethis idea to improve transliteration accuracy by uti-lizing either transliterations from other languages,or phonetic transcriptions in the source language.Specifically, they apply an SVM reranker to the top-n outputs of a base spelling-based model.
However,the post-hoc property of reranking is a limiting fac-tor; it can identify the correct transliteration only ifthe base model includes it in its output candidate list.9443 Joint GenerationIn this section, we describe our approach of the jointtransduction of a transliteration T from a source or-thographic string S and a source phonemic string P(Figure 1).
We implement our approach by modi-fying the DIRECTL+ system of Jiampojamarn et al(2010), which we describe in Section 3.1.
In the fol-lowing sections, we discuss other components of ourapproach, namely alignment (3.2), scoring (3.3), andsearch (3.4).
In Section 3.5 we generalize the jointmodel to accept multiple input strings.3.1 DirecTL+DIRECTL+ (Jiampojamarn et al, 2010) is a dis-criminative string transducer which learns to con-vert source strings into target strings from a set ofparallel training data.
It requires pairs of strings tobe aligned at the character level prior to training.M2M-ALIGNER (Jiampojamarn et al, 2007), an un-supervised EM-based aligner, is often used to gener-ate such alignments.
The output is a ranked list ofcandidate target strings with their confidence scores.Below, we briefly describe the scoring model, thetraining process, and the search algorithm.The scoring model assigns a score to an alignedpair of source and target strings (S, T ).
Assum-ing there are m aligned substrings, such that the ithsource substring generates the ith target substring,the score is computed with the following formula:m?i?
?
?
(i, S, T ) (1)where ?
is the weight vector, and ?
is the featurevector.There are four sets of features.
Context featuresare character n-grams within the source word.
Tran-sition features are character n-grams within the tar-get word.
Linear-chain features combine contextfeatures and transition features.
Joint n-gram fea-tures further capture the joint information on bothsides.The feature weights ?
are learned with the Maxi-mum Infused Relaxed Algorithm (MIRA) of Cram-mer and Singer (2003).
MIRA aims to find thesmallest change in current weights so that the newweights separate the correct target strings from in-correct ones by a margin defined by a loss func-Figure 1: Triple alignment between the source phonemes,source graphemes, and the target graphemes ???
(A-RO-N).tion.
Given the training instance (S, T ) and the cur-rent feature weights ?k?1, the update of the featureweights can be described as the following optimiza-tion problem:min?k??k?
?k?1?
s.t.
?
?T ?
Tn:?k?
(?
(S, T )?
?
(S,?T )) ?
loss(T,?T )where?T is a candidate target in the n-best list Tnfound under the current model parameterized by?k?1.
The loss function is the Levenshtein distancebetween T and?T .Given an unsegmented source string, the searchalgorithm finds a target string that achieves the high-est score according to the scoring model.
It searchesthrough all the possible segmentations of the sourcestring and all possible target substrings using the fol-lowing dynamic programming formulation:Q(0, $) = 0Q(j, t) = maxt?,t,j?N?j?<j?
?
?
(Sjj?+1, t?, t) +Q(j?, t?
)Q(J + 1, $) = maxt??
?
?
($, t?, $) +Q(J, t?
)Q(j, t) is defined as the maximum score of the tar-get sequence ending with target substring t, gener-ated by the letter sequence S1...Sj.
?
describes thefeatures extracted from the current generator sub-string Sjj?+1of target substring t, with t?to be thelast generated target substring.
N specifies the max-imum length of the source substring.
The $ sym-bols are used to represent both the start and the endof a string.
Assuming that the source string con-tains J characters, Q(J+1, $) gives the score of thehighest scoring target string, which can be recoveredthrough backtracking.945Figure 2: Three pairwise alignments between the En-glish word abbey, its transcription [abi], and the Japanesetransliteration???
(A-BE-I).3.2 Multi-alignmentM2M-ALIGNER applies the EM algorithm to alignsets of string pairs.
For the purpose of joint gener-ation, we need to align triples S, P and T prior totraining.
The alignment of multiple strings is a chal-lenging problem (Bhargava and Kondrak, 2009).
Ingeneral, there is no obvious way of merging threepairwise alignments.
Figure 2 shows an exampleof three pairwise alignments that are mutually in-consistent: the English letter e is aligned to thephoneme [i] and to the grapheme?
(BE), which arenot aligned to each otherOur solution is to select one of the input stringsas the pivot for aligning the remaining two strings.Specifically, we align the pivot string to each ofthe other two strings through one-to-many align-ments, where the maximum length of aligned sub-strings in the pivot string is set to one.
Then wemerge these two pairwise alignments according tothe pivot string.
Since the source phoneme stringmay or may not be available for a particular train-ing instance, we use the source orthographic stringas the pivot.
The one-to-many pairwise alignmentsbetween the graphemes and phonemes, and betweenthe graphemes and the transliterations are generatedwith M2M-ALIGNER.
Figure 3 provides an exam-ple of this process.An alternative approach is to pivot on the tar-get string.
However, because the target string isnot available at test time, we need to search for thehighest-scoring target string, given an unsegmentedsource string S and the corresponding unsegmentedphoneme string P .
We can generalize the originalsearch algorithm by introducing another dimensioninto the dynamic-programming table for segmentingP , but it substantially increases the time complexityof the decoding process.
Our development experi-ments indicated that pivoting on the target string notFigure 3: Obtaining a triple alignment by pivoting on thesource word.only requires more time, but also results in less ac-curate transliterations.3.3 Scoring ModelThe scoring formula (1) is extended to computea linear combination of features of three alignedstrings (S, P, T ):m?i?
?
[?
(i, S, T ),?
(i, P, T )] (2)The transition features on T are only computedonce, because they are independent of the inputstrings.
We observed no improvement by includingfeatures between S and P in our development exper-iments.3.4 SearchOur search algorithm finds the highest-scoring targetstring, given a source string and a phoneme string.Since we pivot on the source string to achieve mul-tiple alignment, the input to the search algorithmis actually one-to-many aligned pair of the sourcestring and the phoneme string.
The search space istherefore the same as that of DirecTL+, i.e.
the prod-uct of all possible segmentations of the source stringand all possible target substrings.
However, sincewe apply one-to-many alignment, there is only onepossible segmentation of the source string, which isobtained by treating every letter as a substring.
Weapply the same dynamic programming search as Di-recTL+, except that we extend the feature extractionfunction ?
(Sjj?+1, t?, t) in the original formulationto [?
(Sjj?+1, t?, t), ?
(Pkk?+1, t?, t)] so that features be-tween the current phoneme substring Pkk?+1and thetarget substrings are taken into consideration.
Thetime complexity of this search is only double of thecomplexity of DIRECTL+, and is independent of thelength of the phoneme string.9463.5 GeneralizationSince we may need to leverage information fromother sources, e.g., phonemes of supplementaltransliterations, each training instance can be com-posed of a source word, a target word, and a list ofsupplemental strings.
The size of the list is not fixedbecause we may not have access to some of the sup-plemental strings for certain source words.We first align all strings in each training instanceby merging one-to-many pairwise alignments be-tween the source word and every other string in theinstance, as described in Section 3.2.
The general-ization of training is straightforward.
For the scoringmodel, we extract the same set of features as beforeby pairing each supplemental string with the targetword.
Since the alignment is performed beforehand,the time complexity of the generalized search onlyincreases linearly in the number of input strings withrespect to the original complexity.4 Leveraging transcriptionsIn this section, we describe experiments that involvegenerating transliterations jointly from the sourceorthography and pronunciation.
We test our methodon the English-to-Hindi and English-to-Japanesetransliteration data from the NEWS 2010 MachineTransliteration Shared Task (Li et al, 2010).
We ex-tract the corresponding English pronunciations fromthe Combilex Lexicon (Richmond et al, 2009).
Wesplit each transliteration dataset into 80% for train-ing, 10% for development, 10% for testing.
We limitthe datasets to contain only transliterations that havephonetic transcriptions in Combilex, so that each en-try is composed of a source English word, a sourcetranscription, and a target Japanese or Hindi word.The final results are obtained by joining the train-ing and development sets as the final training set.The final training/test sets contain 8,264/916 entriesfor English-to-Japanese, and 3,503/353 entries forEnglish-to-Hindi.4.1 Gold transcriptionsWe compare three approaches that use differ-ent sources of information: (a) graphemes only;(b) phonemes only; and (c) both graphemes andphonemes.
The first two approaches use DI-RECTL+, while the last approach uses our jointModel En?Ja En?HiGraphemes only 58.0 42.6Phonemes only 52.4 39.4Joint 63.6 46.1Table 1: Transliteration word accuracy depending on thesource information.model described in Section 3.
We evaluate each ap-proach by computing the word accuracy.Table 1 presents the transliteration results.
Evenwith gold-standard transcriptions, the phoneme-based model is worse than the grapheme-basedmodel.
This demonstrates that it is incorrect to referto the process of transliteration as phonetic transla-tion.
On the other hand, our joint generation ap-proach outperforms both single-source models onboth test sets, which confirms that transliteration re-quires a joint consideration of orthography and pro-nunciation.It is instructive to look at a couple of exam-ples where outputs of the models differ.
Considerthe name Marlon, pronounced [mArl@n], which istransliterated into Japanese as ???
(MA-RO-N)(correct), and???
(MA-RE-N) (incorrect), by theorthographic and phonetic approaches, respectively.The letter bigram lo is always transliterated into ?in the orthographic training data, while the phonemebigram /l@/ has multiple correspondences in the pho-netic training data.
In this case, the unstressed vowelreduction process in English causes a loss of the or-thographic information, which needs to be preservedin the transliteration.In the joint model, the phonetic informationsometimes helps disambiguate the pronunciation ofthe source word, thus benefiting the transliterationprocess.
For example, the outputs of the threemodels for haddock, pronounced [had@k], are ???
(HA-DA-KU) (phonetic), ?????
(HA-DO-DO-K-KU) (orthographic), and ????
(HA-DO-K-KU) (joint, correct).
The phonetic model is againconfused by the reduced vowel [@], while the ortho-graphic model mistakenly replicates the renderingof the consonant d, which is pronounced as a singlephoneme.947Model En?Ja En?HiGraphemes only 63.1 43.5Joint (gold phon.)
67.4 48.0Joint (generated phon.)
65.8 46.1Table 2: Transliteration accuracy improvement with goldand generated phonetic transcriptions.4.2 Generated TranscriptionsThe training entries that have no corresponding tran-scriptions in our pronunciation lexicon were ex-cluded from the experiment described above.
Whenwe add those entries back to the datasets, we canno longer apply the phonetic approach, but we canstill compare the orthographic approach to our jointapproach, which can handle the lack of a phonetictranscription in some of the training instances.
Thetraining sets are thus larger in the experiments de-scribed in this section: 30,190 entries for English-to-Japanese, and 12,070 for English-to-Hindi.
Thetest sets are the same as in Section 4.1.
The resultsin the first two rows in Table 2 show that the joint ap-proach outperforms the orthographic approach evenwhen most training entries lack the pronunciation in-formation.1Gold transcriptions are not always available, espe-cially for names that originate from other languages.Next, we investigate whether we can replace thegold transcriptions with transcriptions that are au-tomatically generated from the source orthography.We adopt DIRECTL+ as a grapheme-to-phoneme(G2P) converter, train it on the entire Combilex lexi-con, and include the generated transcriptions insteadof the gold transcriptions in the transliteration train-ing and test sets for the joint model.
The test sets areunchanged.The third row in Table 2 shows the result of lever-aging generated transcriptions.
We still see improve-ment over the orthographic approach, albeit smallerthan with the gold transcriptions.
However, we needto be careful when interpreting these results.
Sinceour G2P converter is trained on Combilex, the gen-1The improvement is statistically significant according tothe McNemar test with p < 0.05.
The differences in the base-line results between Table 1 and Table 2 are due to the differ-ences in the training sets.
The matching value of 46.1 acrossboth tables is a coincidence.
The comparison of results withinany given table column is fair.Model En?Ja En?HiGraphemes only 53.3 46.4Phonemes only 19.2 10.4Joint (suppl.
phonemes) 54.8 50.0Table 3: Transliteration accuracy with transcriptions gen-erated from third-language transliterations.erated transcriptions of words in the test set are quiteaccurate.
When we test the joint approach onlyon words that are not found in Combilex, the im-provement over the orthographic approach largelydisappears.
We interpret this result as an indicationthat the generated transcriptions help mostly by cap-turing consistent grapheme-to-phoneme correspon-dences in the pronunciation lexicon.5 Leveraging transliterationsIn the previous section, we have shown that pho-netic transcriptions can improve the accuracy of thetransliteration process by disambiguating the pro-nunciation of the source word.
Unfortunately, pho-netic transcriptions are rarely available, especiallyfor words which originate from other languages, andgenerating them on the fly is less likely to help.However, transliterations from other languages con-stitute another potential source of information thatcould be used to approximate the pronunciation inthe source language.
In this section, we present ex-periments of leveraging such supplemental translit-erations through our joint model.5.1 Third-language transcriptionsAn intuitive way of employing transliterations fromanother language is to convert them into phonetictranscriptions using a G2P model, which are thenprovided to our joint model together with the sourceorthography.
We test this idea on the data fromthe NEWS 2010 shared task.
We select Thai asthe third language, because it has the largest num-ber of the corresponding transliterations.
We re-strict the training and test sets to include only wordsfor which Thai transliterations are available.
Theresulting English-to-Japanese and English-to-Hinditraining/test sets contain 12,889/1,009, and 763/250entries, respectively.
We adopt DIRECTL+ as aG2P converter, and train it on 911 Thai spelling-pronunciation pairs extracted from Wiktionary.
Be-948Language Acc.
Data sizeThai 15.2 911Hindi 25.9 819Hebrew 21.3 475Korean 40.9 3181Table 4: Grapheme-to-phoneme word accuracy on theWiktionary data.cause of the small size of the training data, it canonly achieve about 15% word accuracy in our G2Pdevelopment experiment.Table 3 shows the transliteration results.
The ac-curacy of the model that uses only supplementaltranscriptions (row 2) is very low, but the joint modelobtains an improvement even with such inaccuratethird-language transcriptions.
Note that the Thaipronunciation is often quite different from English.For instance, the phoneme sequence [waj] obtainedfrom the Thai transliteration of Whyte, helps thejoint model correctly transliterate the English nameinto Japanese ????
(HO-WA-I-TO), which isbetter than ???
(HO-I-TO) produced by the or-thographic model.5.2 Multi-lingual transcriptionsTranscriptions obtained from a third language arenot only noisy because of the imperfect G2P con-version, but often also lossy, in the sense of miss-ing some phonetic information present in the sourcepronunciation.
In addition, supplemental transliter-ations are not always available in a given third lan-guage.
In this section, we investigate the idea ofextracting phonetic information from multiple lan-guages, with the goal of reducing the noise of gen-erated transcriptions.We first train G2P converters for several lan-guages on the pronunciation data collected fromWiktionary.
Table 4 shows the sizes of the G2Pdatasets, and the corresponding G2P word accuracynumbers, which are obtained by using 90% of thedata for training, and the rest for testing.2For thehighly-regular Japanese Katakana, we instead cre-ate a rule-based converter.
Then we convert sup-plemental transliterations from those languages into2We use the entire datasets to train G2P converters for thetransliteration experiments, but their accuracy is unlikely to im-prove much due to a small increase in the training data.Model En?Ja En?HiGraphemes only 54.5 46.1Joint (suppl.
phonemes) 58.6 46.4Table 5: Transliteration accuracy with transcriptions gen-erated from multiple transliterations.noisy phonetic transcriptions.
In order to obtain rep-resentative results, we also include transliterationpairs without supplemental transliterations, whichresults in different datasets than in the previous ex-periments.
The sets for English-to-Japanese andEnglish-to-Hindi now contain 30,190/17,557/1,886and 12,070/3,777/380 entries, where the sizes referto (1) the entire training set, (2) the subset of trainingentries that have at least one supplemental transcrip-tion, and (3) the test set (in which all entries havesupplemental transcriptions).An interlingual approach holds the promiseof ultimately replacing n2pairwise grapheme-grapheme transliteration models involving n lan-guages with 2n grapheme-phoneme and phoneme-grapheme models based on a unified phonetic rep-resentation.
In our implementation, we merge dif-ferent phonetic transcriptions of a given word intoa single abstract vector representation.
Specifically,we replace each phoneme with a phonetic featurevector according to a phonological feature chart,which includes features such as labial, voiced, andtense.
After merging the vectors by averaging theirweights, we incorporate them into the joint modeldescribed in Section 3.3 by modifying ?
(i, P, T ).Unfortunately, the results are disappointing.
It ap-pears that the vector merging process compoundsthe information loss, which offsets the advantage ofincorporating multiple transcriptions.Another way of utilizing supplemental transcrip-tions is to provide them directly to our generalizedjoint model described in Section 3.5, which canhandle multiple input strings.
Table 5 presents theresults on leveraging transcriptions generated fromsupplemental transliterations.
We see that the jointgeneration from multiple transcriptions significantlyboosts the accuracy on English-to-Japanese, but theimprovement on English-to-Hindi is minimal.949Model En?Ja Ja?En En?Hi Hi?EnDIRECTL+ 51.5 19.7 43.4 42.6Reranking 56.8 30.3 50.8 48.9Joint 56.4 38.8 51.6 51.1Joint + Reranking 57.0 44.6 53.0 57.2+ Lexicon - 53.1 - 61.7Table 6: Transliteration accuracy with supplemental information.5.3 Multi-lingual transliterationsThe generated transcriptions of supplementaltransliterations discussed in the previous section arequite inaccurate because of small and noisy G2Ptraining data.
In addition, we are prevented fromtaking advantage of supplemental transliterationsfrom other languages by the lack of the G2P train-ing data.
In order to circumvent these limitations,we propose to directly incorporate supplementaltransliterations into the generation process.
Specif-ically, we train our generalized joint model on thegraphemes of the source word, as well as on thegraphemes of supplemental transliterations.The experiments that we have conducted so farsuggest two additional methods of improving thetransliteration accuracy.
We have observed that n-best lists produced by our joint model contain thecorrect transliteration more often than the baselinemodels.
Therefore, we follow the joint genera-tion with a reranking step, in order to boost thetop-1 accuracy.
We apply the reranking algorithmof Bhargava and Kondrak (2011), except that ourjoint model is the base system for reranking.
In or-der to ensure fair comparison, the held-out sets fortraining the rerankers are subtracted from the origi-nal training sets.Another observation that we aim to exploit is thata substantial number of the outputs generated by ourjoint model are very close to gold-standard translit-erations.
In fact, news writers often use slightlydifferent transliterations of the same name, whichmakes the model?s task more difficult.
Therefore,we rerank the model outputs using a target-languagelexicon, which is a list of words together with theirfrequencies collected from a raw corpus.
We fol-low Cherry and Suzuki (2009) in extracting lexiconfeatures for a given word according to coarse bins,i.e., [< 2000], [< 200], [< 20], [< 2], [< 1].
Forexample, a word with the frequency 194 will causethe features [< 2000] and [< 200] to fire.We conduct our final experiment on forward andbackward transliteration.
We utilize supplemen-tal transliterations from all eight languages in theNEWS 2010 dataset.
The English-Japanese andEnglish-Hindi datasets contain 33,540 and 13,483entries, of which 23,613 and 12,131 have at least onesupplemental transliteration, respectively.
Thesesets are split into training/development/test sets.The entries that have no supplemental translitera-tions are removed from the test sets, which resultsin 2,321 and 1,226 test entries.
In addition, weextract an English lexicon comprising 7.5M wordtypes from the English gigaword monolingual cor-pus (LDC2012T21) for the back-transliteration ex-periments.We evaluate the following models: (1) thebaseline DIRECTL+ model trained on sourcegraphemes; (2) the reranking model of Bhargava andKondrak (2011)3, with DIRECTL+ as the base sys-tem; (3) our joint model described in Section 3.5; (4)?combination?, which is a reranking model with ourjoint model as the base system; and (5) a rerankingmodel that uses the English target lexicon and model(4) as the base system.Table 6 present the results.
We see that our jointmodel performs much better by directly incorporat-ing the supplemental transliterations than by usingthe corresponding phonetic transcriptions.
This isconsistent with our experiments in Section 4 thatshow the importance of the orthographic informa-tion.
We also observe that our joint model achievessubstantial improvements over the baseline on theback-transliteration tasks from Japanese and Hindiinto English.
This result suggests the orthographicinformation from the supplemental transliterations is3Code from http://www.cs.toronto.edu/?aditya/g2p-tl-rr/950particularly effective in recovering the informationabout the pronunciation of the original word whichis often obfuscated by the transliteration into a dif-ferent language.Our joint model is more effective in utilizingsupplemental transliterations than the reranking ap-proach of Bhargava and Kondrak (2011), except onEnglish-to-Japanese.
The combination of these twoapproaches works better than either of them, partic-ularly on the back-transliteration tasks.
Finally, theincorporation of a target-lexicon brings additionalgains.Back-transliteration from Japanese to English ismore challenging than in the forward direction,which was already noted by Knight and Graehl(1997).
Most of the names in the dataset origi-nate from English, and Japanese phonotactics re-quire introduction of extra vowels to separate con-sonant clusters.
During back-transliteration, it is of-ten unclear which vowels should be removed andwhich preserved.
Our approach is able to dramati-cally improve the quality of the results by recoveringthe original information from multiple supplementaltransliterations.6 ConclusionWe have investigated the relative importance ofthe orthographic and phonetic information in thetransliteration process.
We have proposed a noveljoint generation model that directly utilizes bothsources of information.
We have shown that a gener-alized joint model is able to achieve substantial im-provements over the baseline represented by a state-of-the-art transliteration tool by directly incorporat-ing multiple supplemental transliterations.
In the fu-ture, we would like to further explore the idea ofusing interlingual representations for transliterationwithout parallel training data.AcknowledgementsWe thank Adam St Arnaud for help in improving thefinal version of this paper.This research was supported by the Natural Sci-ences and Engineering Research Council of Canada(NSERC).ReferencesYaser Al-Onaizan and Kevin Knight.
2002.
Machinetransliteration of names in Arabic texts.
In Proceed-ings of the ACL-02 Workshop on Computational Ap-proaches to Semitic Languages, Philadelphia, Penn-sylvania, USA, July.
Association for ComputationalLinguistics.Aditya Bhargava and Grzegorz Kondrak.
2009.
Multi-ple word alignment with Profile Hidden Markov Mod-els.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, Companion Volume: Student Research Work-shop and Doctoral Consortium, pages 43?48, Boulder,Colorado, June.
Association for Computational Lin-guistics.Aditya Bhargava and Grzegorz Kondrak.
2011.
Howdo you pronounce your name?
Improving G2P withtransliterations.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 399?408,Portland, Oregon, USA, June.
Association for Compu-tational Linguistics.Aditya Bhargava and Grzegorz Kondrak.
2012.
Leverag-ing supplemental representations for sequential trans-duction.
In Proceedings of the 2012 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 396?406, Montr?eal, Canada, June.
Asso-ciation for Computational Linguistics.Slaven Bilac and Hozumi Tanaka.
2004.
A hybrid back-transliteration system for Japanese.
In Proceedingsof Coling 2004, pages 597?603, Geneva, Switzerland,Aug 23?Aug 27.
COLING.Colin Cherry and Hisami Suzuki.
2009.
Discriminativesubstring decoding for transliteration.
In Proceedingsof the 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 1066?1075, Singa-pore, August.
Association for Computational Linguis-tics.Koby Crammer and Yoram Singer.
2003.
Ultraconser-vative online algorithms for multiclass problems.
J.Mach.
Learn.
Res., 3:951?991, March.Sittichai Jiampojamarn, Grzegorz Kondrak, and TarekSherif.
2007.
Applying many-to-many alignmentsand hidden Markov models to letter-to-phoneme con-version.
In Human Language Technologies 2007: TheConference of the North American Chapter of the As-sociation for Computational Linguistics; Proceedingsof the Main Conference, pages 372?379, Rochester,New York, April.
Association for Computational Lin-guistics.951Sittichai Jiampojamarn, Colin Cherry, and GrzegorzKondrak.
2010.
Integrating joint n-gram features intoa discriminative training framework.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 697?700, Los An-geles, California, June.
Association for ComputationalLinguistics.Mitesh M. Khapra, A Kumaran, and Pushpak Bhat-tacharyya.
2010.
Everybody loves a rich cousin: Anempirical study of transliteration through bridge lan-guages.
In Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages420?428, Los Angeles, California, June.
Associationfor Computational Linguistics.Kevin Knight and Jonathan Graehl.
1997.
Machinetransliteration.
In Proceedings of the 35th AnnualMeeting of the Association for Computational Linguis-tics, pages 128?135, Madrid, Spain, July.
Associationfor Computational Linguistics.A.
Kumaran, Mitesh M. Khapra, and Pushpak Bhat-tacharyya.
2010.
Compositional machine translitera-tion.
ACM Transactions on Asian Language Informa-tion Processing (TALIP), 9(4):13:1?13:29, December.Haizhou Li, Min Zhang, and Jian Su.
2004.
A jointsource-channel model for machine transliteration.
InProceedings of the 42Nd Annual Meeting on Associa-tion for Computational Linguistics, ACL ?04, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Haizhou Li, A Kumaran, Min Zhang, and Vladimir Per-vouchine.
2010.
Report of NEWS 2010 transliterationgeneration shared task.
In Proceedings of the 2010Named Entities Workshop, pages 1?11, Uppsala, Swe-den, July.
Association for Computational Linguistics.Jong-Hoon Oh and Key-Sun Choi.
2005.
Machinelearning based English-to-Korean transliteration usinggrapheme and phoneme information.
IEICE - Trans.Inf.
Syst., E88-D(7):1737?1748, July.Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Tori-sawa.
2009.
Can Chinese phonemes improve machinetransliteration?
: A comparative study of English-to-Chinese transliteration models.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing: Volume 2 - Volume 2, EMNLP?09, pages 658?667, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Korin Richmond, Robert Clark, and Sue Fitt.
2009.
Ro-bust LTS rules with the Combilex speech technologylexicon.
In Proceedings of Interspeech, pages 1259?1298, Brighton, UK, September.Tarek Sherif and Grzegorz Kondrak.
2007.
Substring-based transliteration.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics, pages 944?951, Prague, Czech Republic,June.
Association for Computational Linguistics.Min Zhang, Xiangyu Duan, Vladimir Pervouchine, andHaizhou Li.
2010.
Machine transliteration: Leverag-ing on third languages.
In Coling 2010: Posters, pages1444?1452, Beijing, China, August.
Coling 2010 Or-ganizing Committee.Min Zhang, Haizhou Li, A Kumaran, and Ming Liu.2012.
Whitepaper of NEWS 2012 shared task on ma-chine transliteration.
In Proceedings of the 4th NamedEntity Workshop, pages 1?9, Jeju, Korea, July.
Associ-ation for Computational Linguistics.952
