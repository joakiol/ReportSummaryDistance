Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 102?110,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsMonte Carlo inference and maximization for phrase-based translationAbhishek Arun?a.arun@sms.ed.ac.ukPhil Blunsom?pblunsom@inf.ed.ac.ukChris Dyer?redpony@umd.eduAdam Lopez?alopez@inf.ed.ac.ukBarry Haddow?bhaddow@inf.ed.ac.ukPhilipp Koehn?pkoehn@inf.ed.ac.uk?Department of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UK?Department of LinguisticsUniversity of MarylandCollege Park, MD 20742, USAAbstractRecent advances in statistical machinetranslation have used beam search forapproximate NP-complete inference withinprobabilistic translation models.
We presentan alternative approach of sampling from theposterior distribution defined by a translationmodel.
We define a novel Gibbs samplerfor sampling translations given a sourcesentence and show that it effectively exploresthis posterior distribution.
In doing sowe overcome the limitations of heuristicbeam search and obtain theoretically soundsolutions to inference problems such asfinding the maximum probability translationand minimum expected risk training anddecoding.1 IntroductionStatistical machine translation (SMT) poses theproblem: given a foreign sentence f , find thetranslation e?
that maximises the conditionalposterior probability p(e|f).
This probabilisticformulation of translation has driven developmentof state-of-the-art systems which are able to learnfrom parallel corpora which were generated forother purposes ?
a direct result of employing amathematical framework that we can reason aboutindependently of any particular model.For example, we can train SMT models usingmaximum likelihood estimation (Brown et al, 1993;Och and Ney, 2000; Marcu and Wong, 2002).
Alter-natively, we can train to minimise probabilistic con-ceptions of risk (expected loss) with respect to trans-lation metrics, thereby obtaining better results forthose metrics (Kumar and Byrne, 2004; Smith andEisner, 2006; Zens and Ney, 2007).
We can also useBayesian inference techniques to avoid resorting toheuristics that damage the probabilistic interpreta-tion of the models (Zhang et al, 2008; DeNero etal., 2008; Blunsom et al, 2009).Most models define multiple derivations for eachtranslation; the probability of a translation is thusthe sum over all of its derivations.
Unfortunately,finding the maximum probability translation is NP-hard for all but the most trivial of models in thissetting (Sima?an, 1996).
It is thus necessary to resortto approximations for this sum and the search for itsmaximum e?.The most common of these approximations isthe max-derivation approximation, which for manymodels can be computed in polynomial time viadynamic programming (DP).
Though effective forsome problems, it has many serious drawbacks forprobabilistic inference:1.
It typically differs from the true model maxi-mum.2.
It often requires additional approximations insearch, leading to further error.3.
It introduces restrictions on models, such asuse of only local features.4.
It provides no good solution to compute thenormalization factor Z(f) required by many prob-abilistic algorithms.In this work, we solve these problems using aMonte Carlo technique with none of the above draw-backs.
Our technique is based on a novel Gibbssampler that draws samples from the posterior dis-tribution of a phrase-based translation model (Koehnet al, 2003) but operates in linear time with respectto the number of input words (Section 2).
We show102that it is effective for both decoding (Section 3) andminimum risk training (Section 4).2 A Gibbs sampler for phrase-basedtranslation modelsWe begin by assuming a phrase-based translationmodel in which the input sentence, f , is segmentedinto phrases, which are sequences of adjacentwords.1 Each foreign phrase is translated into thetarget language, to produce an output sentence eand an alignment a representing the mapping fromsource to target phrases.
Phrases are allowed to bereordered during translation.The model is defined with a log-linear form,with feature function vector h and parametrised byweight vector ?, as described in Koehn et al (2003).P (e, a|f ;?)
= exp [?
?
h(e, a, f)]??e?,a??
exp [?
?
h(e?, a?, f)](1)The features h of the model are usually few andare themselves typically probabilistic modelsindicating e.g, the relative frequency of a targetphrase translation given a source phrase (translationmodel), the fluency of the target phrase (languagemodel) and how phrases reorder with respectto adjacent phrases (reordering model).
Thereis a further parameter ?
that limits how manysource language words may intervene betweentwo adjacent target language phrases.
For theexperiments in this paper, we use ?
= 6.2.1 Gibbs samplingWe use Markov chain Monte Carlo (MCMC) as analternative to DP search (Geman and Geman, 1984;Metropolis and Ulam, 1949).
MCMC probabilis-tically generates sample derivations from the com-plete search space.
The probability of generatingeach sample is conditioned on the previous sam-ple, forming a Markov chain.
After a long enoughinterval (referred to as the burn-in) this chain returnssamples from the desired distribution.Our MCMC sampler uses Gibbs sampling, whichobtains samples from the joint distribution of a setof random variables X = {X1, .
.
.
, Xn}.
It startswith some initial state (X1 = x10, .
.
.
, Xn = xn0),and generates a Markov chain of samples, where1These phrases are not necessarily linguistically motivated.each sample is the result of applying a set of Gibbsoperators to the previous sample.
Each operator isdefined by specifying a subset of the random vari-ables Y ?
X , which the operator updates by sam-pling from the conditional distribution P (Y |X \Y ).The set X \ Y is referred to as the Markov blanketand is unchanged by the operator.In the case of translation, we require a Gibbs sam-pler that produces a sequence of samples, SN1 =(e1, a1) .
.
.
(eN , aN ), that are drawn from the dis-tribution P (e, a|f).
These samples can thus be usedto estimate the expectation of a function h(e, a, f)under the distribution as follows:EP (a,e|f)[h] = limN?
?1NN?i=1h(ai, ei, f) (2)Taking h to be an indicator functionh = ?
(a, a?)?
(e, e?)
provides an estimate ofP (a?, e?|f), and using h = ?
(e, e?)
marginalises overall derivations a?, yielding an estimate of P (e?|f).2.2 Gibbs operatorsOur sampler consists of three operators.
Examplesof these are depicted in Figure 1.The RETRANS operator varies the translation of asingle source phrase.
Segmentation, alignment, andall other translations are held constant.The MERGE-SPLIT operator varies the sourcesegmentation at a single word boundary.
If theboundary is a split point in the current hypothesis,the adjoining phrases can be merged, providedthat the corresponding target phrases are adjacentand the phrase table contains a translation of themerged phrase.
If the boundary is not a split point,the covering phrase may be split, provided thatthe phrase table contains a translation of both newphrases.
Remaining segmentation points, phrasealignment and phrase translations are held constant.The REORDER operator varies the target phraseorder for a pair of source phrases, provided thatthe new alignment does not violate reordering limit?.
Segmentation, phrase translations, and all otheralignments are held constant.To illustrate the RETRANS operator, we willassume a simplified model with two features: abigram language model Plm and a translation modelPtm.
Both features are assigned a weight of 1.103c?est un re?sultat remarquableit is some result remarkable(a)Initialc?est un re?sultat remarquablebut some result remarkable(b)Retransc?est un re?sultat remarquableit is a result remarkable(c)Mergec?est un re?sultat remarquableit is a remarkable result(d)Reorder1Figure 1: Example evolution of an initial hypothesis viaapplication of several operators, with Markov blanketindicated by shading.We denote the start of the sentence with S and thelanguage model context with C. Assuming theFrench phrase c?est can be translated either as it is orbut, the RETRANS operator at step (b) stochasticallychooses an English phrase, e?
in proportion to thephrases?
conditional probabilities.P (but|c?est,C) = Ptm(but|c?est) ?
Plm(S but some)ZandP (it is|c?est,C) = Ptm(it is|c?est) ?
Plm(S it is some)ZwhereZ = Ptm(but|c?est) ?
Plm(S but some) +Ptm(it is|c?est) ?
Plm(S it is some)Conditional distributions for the MERGE-SPLIT andREORDER operators can be derived in an analogousfashion.A complete iteration of the sampler consists ofapplying each operator at each possible point in thesentence, and a sample is collected after each opera-tor has performed a complete pass.2.3 Algorithmic complexitySince both the RETRANS and MERGE-SPLIT oper-ators are applied by iterating over source side wordpositions, their complexity is linear in the size of theinput.The REORDER operator iterates over the positionsin the input and for the source phrase found at thatposition considers swapping its target phrase withthat of every other source phrase, provided that thereordering limit is not violated.
This means that itcan only consider swaps within a fixed-length win-dow, so complexity is linear in sentence length.2.4 Experimental verificationTo verify that our sampler was behaving as expected,we computed the KL divergence between itsinferred distribution q?
(e|f) and the true distributionover a single sentence (Figure 2).
We computedthe true posterior distribution p(e|f) under anArabic-English phrase-based translation modelwith parameters trained to maximise expectedBLEU (Section 4), summing out the derivations foridentical translations and computing the partitionterm Z(f).
As the number of iterations increases,the KL divergence between the distributionsapproaches zero.3 DecodingThe task of decoding amounts to finding the singletranslation e?
that maximises or minimises some cri-terion given a source sentence f .
In this sectionwe consider three common approaches to decod-ing, maximum translation (MaxTrans), maximumderivation (MaxDeriv), and minimum risk decoding(MinRisk):e?
=??
?argmax(e,a) p(e, a|f) (MaxDeriv)argmaxe p(e|f) (MaxTrans)argmine?e?
`e?
(e)p(e?|f) (MinRisk)In the minimum risk decoder, `e?
(e) is any real-valued loss (error) function that computes the errorof one hypothesis e with respect to some referencee?.
Our loss is a sentence-level approximation of(1 ?
BLEU).As noted in section 2, the Gibbs sampler canbe used to provide an estimate of the probabilitydistribution P (a, e|f) and therefore to determinethe maximum of this distribution, in other wordsthe most likely derivation.
Furthermore, we canmarginalise over the alignments to estimate P (e|f)104IterationsKL divergencel l ll lll llllll l ll l10 100 1000 10000 100000 10000000.0010.010.1KL DivergenceFigure 2: The KL divergence of the true posterior distri-bution and the distribution estimated by the Gibbs sam-pler at different numbers of iterations for the Arabicsource sentence r}ys wzrA?
mAlyzyA yzwr Alflbyn (inEnglish, The prime minister of Malaysia visits the Philip-pines).and so obtain the most likely translation.
The Gibbssampler can therefore be used as a decoder, eitherrunning in max-derivation and max-translationmode.
Using the Gibbs sampler in this way makesmax-translation decoding tractable, and so willhelp determine whether max-translation offers anybenefit over the usual max-derivation.
Using theGibbs sampler as a decoder also allows us to verifythat it is producing valid samples from the desireddistribution.3.1 Training data and preparation.The experiments in this section were performedusing the French-English and German-Englishparallel corpora from the WMT09 shared translationtask (Callison-Burch et al, 2009), as well as 300kparallel Arabic-English sentences from the NISTMT evaluation training data.2 For all languagepairs, we constructed a phrase-based translationmodel as described in Koehn et al (2003), limitingthe phrase length to 5.
The target side of the parallelcorpus was used to train a 3-gram language model.2The Arabic-English training data consists of theeTIRR corpus (LDC2004E72), the Arabic news corpus(LDC2004T17), the Ummah corpus (LDC2004T18), and thesentences with confidence c > 0.995 in the ISI automaticallyextracted web parallel corpus (LDC2006T02).For the German and French systems, the DEV2006set was used for model tuning and the TEST2007(in-domain) and NEWS-DEV2009B (out-of-domain)sets for testing.
For the Arabic system, the MT02set (10 reference translations) was used for tuningand MT03 (4 reference translations) was used forevaluation.
To reduce the size of the phrase table,we used the association-score technique suggestedby Johnson et al (2007a).
Translation quality isreported using case-insensitive BLEU (Papineni etal., 2002).3.2 Translation performanceFor the experiments reported in this section, weused feature weights trained with minimum errorrate training (MERT; Och, 2003) .
Because MERTignores the denominator in Equation 1, it is invari-ant with respect to the scale of the weight vector?
?
the Moses implementation simply normalisesthe weight vector it finds by its `1-norm.
However,when we use these weights in a true probabilisticmodel, the scaling factor affects the behaviour ofthe model since it determines how peaked or flat thedistribution is.
If the scaling factor is too small, thenthe distribution is too flat and the sampler spendstoo much time exploring unimportant probabilityregions.
If it is too large, then the distribution is toopeaked and the sampler may concentrate on a verynarrow probability region.
We optimised the scalingfactor on a 200-sentence portion of the tuning set,finding that a multiplicative factor of 10 worked bestfor fr-en and a multiplicative factor of 6 for de-en.
3The first experiment shows the effect of differentinitialisations and numbers of sampler iterations onmax-derivation decoding performance of the sam-pler.
The Moses decoder (Koehn et al, 2007) wasused to generate the starting hypothesis, either infull DP max-derivation mode, or alternatively withrestrictions on the features and reordering, or withzero weights to simulate a random initialisation, andthe number of iterations varied from 100 to 200,000,with a 100 iteration burn-in in each case.
Figure 3shows the variation of model score with sampler iter-ation, for the different starting points, and for bothlanguage pairs.3We experimented with annealing, where the scale factor isgradually increased to sharpen the distribution while sampling.However, we found no improvements with annealing.105?20.1?20.0?19.9?19.8?19.7?19.6IterationsModel score100 1000 10000French?EnglishInitialisationfullmononolmzero?40.6?40.4?40.2?40.0?39.8IterationsModel score100 1000 10000 100000German?EnglishInitialisationfullmononolmzeroFigure 3: Mean maximum model score, as a function of iteration number and starting point.
The starting point caneither be the full max-derivation translation (full), the monotone translation (mono), the monotone translation with nolanguage model (nolm) or the monotone translation with all weights set to zero (zero).Comparing the best model scores found by thesampler, with those found by the Moses decoderwith its default settings, we found that around50,000 sampling iterations were required forfr-en and 100,000 for de-en, for the sampler togive equivalent model scores to Moses.
FromFigure 3 we can see that the starting point did nothave an appreciable effect on the model score ofthe best derivation, except with low numbers ofiterations.
This indicates that the sampler is ableto move fairly quickly towards the maximum ofthe distribution from any starting point, in otherwords it has good mobility.
Running the samplerfor 100,000 iterations took on average 1670 secondsper sentence on the French-English data set and1552 seconds per sentence on German-English.A further indication of the dependence of sampleraccuracy on the iteration count is provided by Fig-ure 4.
In this graph, we show the mean Spearman?srank correlation between the nbest lists of deriva-tions when ranked by (i) model score and (ii) theposterior probability estimated by the sampler.
Thismeasure of sampler accuracy also shows a logarith-mic dependence on the sample size.3.3 Minimum risk decodingThe sampler also allows us to perform minimumBayes risk (MBR) decoding, a technique introducedby Kumar and Byrne (2004).
In their work, as an0.20.30.40.50.60.70.8IterationsCorrelation100 1000 10000 100000Language Pairsfr?ende?enFigure 4: Mean Spearman?s rank correlation of 1000-bestlist of derivations ranked according to (i) model score and(ii) posterior probability estimated by sampler.
This wasmeasured on a 200 sentence subset of DEV2006.approximation of the model probability distribution,the expected loss of the decoder is calculated bysumming over an n-best list.
With the Gibbs sam-pler, however, we should be able to obtain a muchmore accurate view of the model probability distri-bution.
In order to compare max-translation, max-derivation and MBR decoding with the Gibbs sam-pler, and the Moses baseline, we ran experiments106fr-en de-enin out in outMoses 32.7 19.1 27.4 15.9MaxD 32.6 19.1 27.0 15.5MaxT 32.6 19.1 27.4 16.0MBR 32.6 19.2 27.3 16.0Table 1: Comparison of the BLEU score of the Mosesdecoder with the sampler running in max-derivation(MaxD), max-translation (MaxT) and minumum Bayesrisk (MBR) modes.
The test sets are TEST2007 (in) andNEWS-DEV2009B (out)on both European language pairs, using both the in-domain and out-of-domain test sets.
The samplerwas initialised with the output of Moses with thefeature weights set to zero and restricted to mono-tone, and run for 100,000 iterations with a 100 iter-ation burn-in.
The scale factors were set to the samevalues as in the previous experiment.
The relativetranslation quality (measured according to BLEU) isshown in Table 1.3.4 DiscussionThese results show very little difference between thedecoding methods, indicating that the Gibbs sam-pling decoder can perform as well as a standard DPbased max-derivation decoder with these models,and that there is no gain from doing max-translationor MBR decoding.
However it should be noted thatthe model used for these experiments was optimisedby MERT, for max-derivation decoding, and so theexperiments do not rule out the possibility that max-translation and MBR decoding will offer an advan-tage on an appropriately optimised model.4 Minimum risk trainingIn the previous section, we described how our sam-pler can be used to search for the best translationunder a variety of decoding criteria (max deriva-tion, translation, and minimum risk).
However, thereappeared to be little benefit to marginalizing overthe latent derivations.
This is almost certainly a sideeffect of the MERT training approach that was usedto construct the models so as to maximise the per-formance of the model on its single best derivation,without regard to the shape of the rest of the dis-tribution (Blunsom et al, 2008).
In this section wedescribe a further application of the Gibbs sampler:to do unbiased minimum risk training.While there have been at least two previousattempts to do minimum risk training for MT, bothapproaches relied on biased k-best approximations(Smith and Eisner, 2006; Zens and Ney, 2007).Since we sample from the whole distribution, wewill have a more accurate risk assessment.The risk, or expected loss, of a probabilistic trans-lation model on a corpus D, defined with respect toa particular loss function `e?
(e), where e?
is the refer-ence translation and e is a hypothesis translationL = ??e?,f??D?ep(e|f)`e?
(e) (3)This value can be trivially computed using equa-tion (2).
In this section, we are concerned with find-ing the parameters ?
that minimise (3).
Fortunately,with the log-linear parameterization of p(e|f), L isdifferentiable with respect to ?:?L?
?k= ??e?,f??D?ep(e|f)`e?
(e)(hk ?
Ep(e|f)[hk])(4)Equation (4) is slightly more complicated to com-pute using the sampler since it requires the featureexpectation in order to evaluate the final term.
How-ever, this can be done simply by making two passesover the samples, computing the feature expecta-tions on the first pass and the gradient on the second.We have now shown how to compute ourobjective (3), the expected loss, and a gradientwith respect to the model parameters we wantto optimise, (4), so we can use any standardfirst-order optimization technique.
Since thesampler introduces stochasticity into the gradientand objective, we use stochastic gradient descentmethods which are more robust to noise thanmore sophisticated quasi-Newtonian methodslike L-BFGS (Schraudolph et al, 2007).
For theexperiments below, we updated the learning rateafter each step proportionally to difference insuccessive gradients (Schraudolph, 1999).For the experiments reported in this section, weused sample sizes of 8000 and estimated the gradi-ent on sets of 100 sentences drawn randomly (withreplacement) from the development corpus.
For a107Training Decoder MT03Moses Max Derivation 44.6MERT Moses MBR 44.8Gibbs MBR 44.9Moses Max Derivation 40.6MinRisk MaxTrans 41.8Gibbs MBR 42.9Table 2: Decoding with minimum risk trained systems,compared with decoding with MERT-trained systems onArabic to English MT03 dataloss function we use 4-gram (1 ?
BLEU) computedindividually for each sentence4.
By examining per-formance on held-out data, we find the model con-verges typically in fewer than 20 iterations.4.1 Training experimentsDuring preliminary experiments with training, weobserved on a held-out data set (portions of MT04)that the magnitude of the weights vector increasedsteadily (effectively sharpening the distribution), butwithout any obvious change in the objective.
Sincethis resulted in poor generalization we added a reg-ularization term of ||~?
?
~?||2/2?2 to L. We initiallyset the means to zero, but after further observing thatthe translations under all decoding criteria tended tobe shorter than the reference (causing a significantdrop in performance when evaluated using BLEU),we found that performance could be improved bysetting ?WP = ?0.5, indicating a preference for alower weight on this parameter.Table 2 compares the performance on Arabic toEnglish translation of systems tuned with MERT(maximizing corpus BLEU) with systems tuned tomaximise expected sentence-level BLEU.
Althoughthe performance of the minimum risk model underall decoding criteria is lower than that of the orig-inal MERT model, we note that the positive effectof marginalizing over derivations as well as usingminimum risk decoding for obtaining good resultson this model.
A full exploration of minimum risktraining is beyond the scope of this paper, but theseinitial experiments should help emphasise the versa-tility of the sampler and its utility in solving a varietyof problems.
In the conclusion, we will, however,4The ngram precision counts are smoothed by adding 0.01for n > 1discuss some possible future directions that can betaken to make this style of training more competitivewith standard baseline systems.5 Discussion and future workWe have described an algorithmic technique thatsolves certain problems, but also verifies the utilityof standard approximation techniques.
For exam-ple, we found that on standard test sets the samplerperforms similarly to the DP max-derivation solu-tion and equally well regardless of how it is ini-tialised.
From this we conclude that at least forMERT-trained models, the max-derivation approx-imation is adequate for finding the best translation.Although the training approach presented inSection 4 has a number of theoretical advantages,its performance in a one-best evaluation falls shortwhen compared with a system tuned for optimalone-best performance using MERT.
This contradictsthe results of Zens and Ney (2007), who optimisethe same objective and report improvements over aMERT baseline.
We conjecture that the differenceis due to the biased k-best approximation they used.By considering only the most probable derivations,they optimise a smoothed error surface (as onedoes in minimum risk training), but not one thatis indicative of the true risk.
If our hypothesisis accurate, then the advantage is accidental andultimately a liability.
Our results are in line withthose reported by Smith and Eisner (2006) whofind degradation in performance when minimizingrisk, but compensate by ?sharpening?
the modeldistribution for the final training iterations,effectively maximising one-best performancerather minimising risk over the full distributiondefined by their model.
In future work, we willexplore possibilities for artificially sharpening thedistribution during training so as to better anticipatethe one-best evaluation conditions typical of MT.However, for applications which truly do require adistribution over translations, such as re-ranking,our method for minimising expected risk would bethe objective of choice.Using sampling for model induction has two fur-ther advantages that we intend to explore.
First,although MERT performs quite well on models with108small numbers of features (such as those we consid-ered in this paper), in general the algorithm severelylimits the number of features that can be used sinceit does not use gradient-based updates during opti-mization, instead updating one feature at a time.
Ourtraining method (Section 4) does not have this limi-tation, so it can use many more features.Finally, for the DP-based max-derivation approx-imation to be computationally efficient, the featurescharacterizing the steps in the derivation must beeither computable independently of each other orwith only limited local context (as in the case of thelanguage model or distortion costs).
This has led toa situation where entire classes of potentially use-ful features are not considered because they wouldbe impractical to integrate into a DP based trans-lation system.
With the sampler this restriction ismitigated: any function of h(e, f, a) may partici-pate in the translation model subject only to its owncomputability.
Freed from the rusty manacles ofdynamic programming, we anticipate developmentof many useful features.6 Related workOur sampler is similar to the decoder of Germannet al (2001), which starts with an approximate solu-tion and then incrementally improves it via operatorssuch as RETRANS and MERGE-SPLIT.
It is alsosimilar to the estimator of Marcu and Wong (2002),who employ the same operators to search the align-ment space from a heuristic initialisation.
Althoughthe operators are similar, the use is different.
Theseprevious efforts employed their operators in a greedyhill-climbing search.
In contrast, our operators areapplied probabilistically, making them theoreticallywell-founded for a variety of inference problems.Our use of Gibbs sampling follows from itsincreasing use in Bayesian inference problems inNLP (Finkel et al, 2006; Johnson et al, 2007b).Most closely related is the work of DeNeroet al (2008), who derive a Gibbs sampler forphrase-based alignment, using it to infer phrasetranslation probabilities.
The use of Monte Carlotechniques to calculate posteriors is similar to thatof Chappelier and Rajman (2000) who use thosetechniques to find the best parse under models wherethe derivation and the parse are not isomorphic.To our knowledge, we are the first to apply MonteCarlo methods to maximum translation and mini-mum risk translation.
Approaches to the former(Blunsom et al, 2008; May and Knight, 2006) relyon dynamic programming techniques which do notscale well without heuristic approximations, whileapproaches to the latter (Smith and Eisner, 2006;Zens et al, 2007) use biased k-best approximations.7 ConclusionWe have described a Gibbs sampler for approxi-mating two intractable problems in SMT: maximumtranslation decoding (and its variant, minimum riskdecoding) and minimum risk training.
By usingMonte Carlo techniques we avoid the biases associ-ated with the more commonly used DP based max-derivation (or k-best derivation) approximation.
Indoing so we provide a further tool to the translationcommunity that we envision will allow the devel-opment and analysis of increasing theoretically wellmotivated techniques.AcknowledgmentsThis research was supported in part by the GALEprogram of the Defense Advanced Research ProjectsAgency, Contract No.
HR0011-06-2-001; and by theEuroMatrix project funded by the European Commission(6th Framework Programme).
The project made use ofthe resources provided by the Edinburgh Compute andData Facility (http://www.ecdf.ed.ac.uk/).The ECDF is partially supported by the eDIKT initiative(http://www.edikt.org.uk/).ReferencesP.
Blunsom, T. Cohn, and M. Osborne.
2008.
A discrim-inative latent variable model for statistical machinetranslation.
In Proc.
of ACL-HLT.P.
Blunsom, T. Cohn, and M. Osborne.
2009.
Bayesiansynchronous grammar induction.
In Advances in Neu-ral Information Processing Systems 21, pages 161?168.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
Computa-tional Linguistics, 19(2):263?311.C.
Callison-Burch, P. Koehn, C. Monz, and J. Schroeder,editors.
2009.
Proc.
of Workshop on Machine Trans-lations, Athens.J.-C. Chappelier and M. Rajman.
2000.
Monte-Carlosampling for NP-hard maximization problems in the109framework of weighted parsing.
In Natural LanguageProcessing ?
NLP 2000, number 1835 in Lecture Notesin Artificial Intelligence, pages 106?117.
Springer.J.
DeNero, A. Bouchard, and D. Klein.
2008.
Sam-pling alignment structure under a Bayesian translationmodel.
In Proc.
of EMNLP.J.
R. Finkel, C. D. Manning, and A. Y. Ng.
2006.
Solv-ing the problem of cascading errors: Approximatebayesian inference for linguistic annotation pipelines.In Proc.
of EMNLP.S.
Geman and D. Geman.
1984.
Stochastic relaxation,Gibbs distributions and the Bayesian restoration ofimages.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 6:721?741.U.
Germann, M. Jahr, K. Knight, D. Marcu, andK.
Yamada.
2001.
Fast decoding and optimal decod-ing for machine translation.
In Proceedings of ACL.Association for Computational Linguistics, July.J.
Johnson, J. Martin, G. Foster, and R. Kuhn.
2007a.Improving translation quality by discarding most ofthe phrasetable.
In Proc.
of EMNLP-CoNLL, Prague.M.
Johnson, T. Griffiths, and S. Goldwater.
2007b.Bayesian inference for PCFGs via Markov chainMonte Carlo.
In Proc.
of NAACL-HLT, pages 139?146, Rochester, New York, April.P.
Koehn, F. Och, and D.Marcu.
2003.
Statistical phrase-based translation.
In Proc.
of HLT-NAACL, pages 48?54, Morristown, NJ, USA.P.
Koehn, H. Hoang, A.
B. Mayne, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proc.
of ACLDemonstration Session, pages 177?180, June.S.
Kumar and W. Byrne.
2004.
Minimum Bayes-riskdecoding for statistical machine translation.
In Pro-cessings of HLT-NAACL.D.
Marcu and W. Wong.
2002.
A phrase-based, jointprobability model for statistical machine translation.In Proc.
of EMNLP, pages 133?139.J.
May and K. Knight.
2006.
A better n-best list: Prac-tical determinization of weighted finite tree automata.In Proc.
of NAACL-HLT.N.
Metropolis and S. Ulam.
1949.
The Monte Carlomethod.
Journal of the American Statistical Associa-tion, 44(247):335?341.F.
Och and H. Ney.
2000.
A comparison of alignmentmodels for statistical machine translation.
In Proc.
ofCOLING, Saarbrucken, Germany, July.F.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
of ACL, pages 160?167,Sapporo, Japan, July.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL, pages 311?318.N.
N. Schraudolph, J. Yu, and S. Gu?nter.
2007.
Astochastic quasi-Newton method for online convexoptimization.
In Proc.
of Artificial Intelligence andStatistics.N.
N. Schraudolph.
1999.
Local gain adaptation instochastic gradient descent.
Technical Report IDSIA-09-99, IDSIA.K.
Sima?an.
1996.
Computational complexity of proba-bilistic disambiguation by means of tree grammars.
InProc.
of COLING, Copenhagen.D.
A. Smith and J. Eisner.
2006.
Minimum riskannealing for training log-linear models.
In Proc.
ofCOLING-ACL, pages 787?794.R.
Zens and H. Ney.
2007.
Efficient phrase-table repre-sentation for machine translation with applications toonline MT and speech translation.
In Proc.
of NAACL-HLT, Rochester, New York.R.
Zens, S. Hasan, and H. Ney.
2007.
A systematic com-parison of training criteria for statistical machine trans-lation.
In Proc.
of EMNLP, pages 524?532, Prague,Czech Republic.H.
Zhang, C. Quirk, R. C. Moore, and D. Gildea.
2008.Bayesian learning of non-compositional phrases withsynchronous parsing.
In Proc.
of ACL: HLT, pages97?105, Columbus, Ohio.110
