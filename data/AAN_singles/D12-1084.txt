Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 916?927, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsUsing Discourse Information for Paraphrase ExtractionMichaela RegneriDept.
of Computational LinguisticsSaarland UniversitySaarbr?cken, Germanyregneri@coli.uni-saarland.deRui WangLanguage Technology LabDFKI GmbHSaarbr?cken, Germanyruiwang@dfki.deAbstractPrevious work on paraphrase extraction us-ing parallel or comparable corpora has gener-ally not considered the documents?
discoursestructure as a useful information source.
Wepropose a novel method for collecting para-phrases relying on the sequential event or-der in the discourse, using multiple sequencealignment with a semantic similarity measure.We show that adding discourse informationboosts the performance of sentence-level para-phrase acquisition, which consequently givesa tremendous advantage for extracting phrase-level paraphrase fragments from matched sen-tences.
Our system beats an informed baselineby a margin of 50%.1 IntroductionIt is widely agreed that identifying paraphrases is acore task for natural language processing, includingapplications like document summarization (Barzilayet al1999), Recognizing Textual Entailment (Da-gan et al2005), natural language generation (Zhaoet al2010; Ganitkevitch et al2011), and machinetranslation (Marton et al2009).
As a consequence,many methods have been proposed for generatinglarge paraphrase resources (Lin and Pantel, 2001;Szpektor et al2004; Dolan et al2004).
One ofthe intuitively appropriate data sources for such col-lections are parallel or comparable corpora: if twotexts are translations of the same foreign document,or if they describe the same underlying scenario,they should contain a reasonable number of sentencepairs that convey the same meaning.Most approaches that extract paraphrases fromparallel texts employ some type of pattern match-ing: sentences with the same meaning are assumedto share many n-grams (Barzilay and Lee, 2003;Callison-Burch, 2008, among others), many wordsin their context (Barzilay and McKeown, 2001) orcertain slots in a dependency path (Lin and Pantel,2001; Szpektor et al2004).
Discourse structurehas only marginally been considered for this task:For example, Dolan et al2004) extract the firstsentences from comparable articles and take themas paraphrases.
Another approach (Del?ger andZweigenbaum, 2009) matches similar paragraphs incomparable texts, creating smaller comparable doc-uments for paraphrase extraction.We believe that discourse structure delivers im-portant information for the extraction of para-phrases.
Sentences that play the same role in a cer-tain discourse and have a similar discourse contextcan be paraphrases, even if a semantic similaritymodel does not consider them very similar.
This ex-tends the widely applied distributional hypothesis tothe discourse level: According to the distributionalhypothesis, entities are similar if they share similarcontexts.
In our case, entities are whole sentences,and contexts are discourse units.Based on this assumption, we propose a novelmethod for collecting paraphrases from parallel textsusing discourse information.
We create a new typeof parallel corpus by collecting multiple summariesfor several TV show episodes.
The discourse struc-tures of those summaries are easy to compare: theyall contain the events in the same order as theyhave appeared on the screen.
This allows us totake sentence order as event-based discourse struc-ture, which is highly parallel for recaps of the sameepisode.In its first step, our system uses a sequence align-916ment algorithm combined with a state-of-the-artsimilarity measure.
The approach outperforms in-formed baselines on the task of sentential paraphraseidentification.
The usage of discourse informationeven contributes more to the final performance thanthe sentence similarity measure.As second step, we extract phrase-level para-phrase fragments from the matched sentences.
Thisstep relies on the alignment algorithm?s output, andwe show that discourse information makes a big dif-ference for the precision of the extraction.
We thenadd more discourse-based information by prepro-cessing the text with a coreference resolution sys-tem, which results in additional performance im-provement.The paper is structured as follows: first we sum-marize related work (Sec.
2), and then we give anoverview over our perspective on the task and sketchour system pipeline (Sec.
3).
The following two sec-tions describe the details of the sentence matchingstep (Sec.
4) and the subsequent paraphrase frag-ment extraction (Sec.
5).
We present both automaticand manual evaluation of the two system compo-nents (Sec.
6).
Finally, we conclude the paper andgive some hints for future work (Sec.
7).2 Related WorkPrevious paraphrase extraction approaches can beroughly characterized under two aspects: 1) datasource and 2) granularity of the output.Both parallel corpora and comparable corporahave been quite well studied.
Barzilay and McK-eown (2001) use different English translations ofthe same novels (i.e., monolingual parallel corpora),while others (Quirk et al2004) experiment on mul-tiple sources of the same news/events, i.e., mono-lingual comparable corpora.
Commonly used (can-didate) comparable corpora are news articles writ-ten by different news agencies within a limited timewindow (Wang and Callison-Burch, 2011).
Otherstudies focus on extracting paraphrases from largebilingual parallel corpora, which the machine trans-lation (MT) community provides in many varieties.Bannard and Callison-Burch (2005) as well as Zhaoet al2008) take one language as the pivot andmatch two possible translations in the other lan-guages as paraphrases if they share a common pivotphrase.
As parallel corpora have many alternativeways of expressing the same foreign language con-cept, large quantities of paraphrase pairs can be ex-tracted.The paraphrasing task is also strongly related tocross-document event coreference resolution, whichis tackled by similar techniques used by the availableparaphrasing systems (Bagga and Baldwin, 1999;Tomadaki and Salway, 2005).Most work in paraphrase acquisition has dealtwith sentence-level paraphrases, e.g., (Barzilay andMcKeown, 2001; Barzilay and Lee, 2003; Dolan etal., 2004; Quirk et al2004).
Our approach for sen-tential paraphrase extraction is related to the one in-troduced by Barzilay and Lee (2003), who also em-ploy multiple sequence alignment (MSA).
However,they use MSA at the sentence level rather than at thediscourse level.We take some core ideas from our previous workon mining script information (Regneri et al2010).In this earlier work, we focused on event structuresand their possible realizations in natural language.The corpus used in those experiments were shortcrowd-sourced descriptions of everyday tasks writ-ten in bullet point style.
We aligned them with ahand-crafted similarity measure that was specificallydesigned for this text type.
In this current work,we target the general task of extracting paraphrasesfor events rather than the much more specific script-related task.
The current approach uses a domain-independent similarity measure instead of a specifichand-crafted similarity score and is thus applicableto standard texts.From an applicational point of view, senten-tial paraphrases are difficult to use in other NLPtasks.
At the phrasal level, interchangeable patterns(Shinyama et al2002; Shinyama and Sekine, 2003)or inference rules (Lin and Pantel, 2001) are ex-tracted.
In both cases, each pattern or rule containsone or several slots, which are restricted to certaintype of words, e.g., named entities (NE) or contentwords.
They are quite successful in NE-centeredtasks, like information extraction, but their level ofgeneralization or coverage is insufficient for appli-cations like Recognizing Textual Entailment (Dinuand Wang, 2009).The research on general paraphrase fragment ex-traction at the sub-sentential level is mainly based917on phrase pair extraction techniques from the MTliterature.
Munteanu and Marcu (2006) extract sub-sentential translation pairs from comparable corporausing the log-likelihood-ratio of word translationprobability.
Quirk et al2007) extract fragmentsusing a generative model of noisy translations.
Ourown work (Wang and Callison-Burch, 2011) extendsthe first idea to paraphrase fragment extraction onmonolingual parallel and comparable corpora.
Ourcurrent approach also uses word-word alignment,however, we use syntactic dependency trees to com-pute grammatical fragments.
Our use of dependencytrees is inspired by the constituent-tree-based exper-iments of Callison-Burch (2008).3 Paraphrases and DiscoursePrevious approaches have shown that comparabletexts provide a good basis for paraphrase extrac-tion.
We want to show that discourse structure ishighly useful for precise and high-yield paraphrasecollection from such corpora.
Consider the follow-ing (made-up) example:(1) [House keeps focusing on his aching leg.1.1.
][The psychiatrist suggests him to get a hobby1.2.]
[House joins a cooking class.1.3](2) [He tells him that the Ibuprofen is not helpingwith the pain.2.1.]
[Nolan tells House to take upa hobby.2.2] [Together with Wilson he goes to acookery course.2.3]Read as a whole, it is clear that the two texts de-scribe the same three events, in the same order, andthus, e.g., 1.2 and 2.2 are paraphrases.
However,they share very few n-grams, nor named entities.
Wedetermine three factors that can help to identify suchparaphrases:1.
Consider the sequence of events.
A systemwhich recognizes that the three sentence pairsoccur in the same sequential event order wouldhave a chance of actually matching the sen-tences.2.
Do coreference resolution.
To determinewhich sentence parts actually carry the samemeaning, pronoun resolution is essential (e.g.,to match ?suggest him?
and ?tells House?
).recapsof HouseM.D.parallel corpuswith paralleldiscoursestructuresThe psychiatrist suggestshim to get a hobbyNolan tells House to takeup a hobby.sentence-level paraphrases+ discourse information+ semantic similarity+ word alignments+ coref.
resolution+ dependency treesget a hobbytake up a hobbyparaphrasefragments123Figure 1: System pipeline3.
Try a generic sentence similarity model.
Pat-tern matching or n-gram overlap might not besufficient to solve this problem.Our system pipeline is sketched in Fig.
1:1.
Create a corpus: First, we create a compara-ble corpus of texts with highly comparable dis-course structures.
Complete discourse struc-tures like in the RST Discourse Treebank (Carl-son et al2002) may be very useful for para-phrase computation, however, they are hard toobtain.
Discourse annotation is difficult andwork-intensive, and full-blown automatic dis-course parsers are neither robust nor very pre-cise.
To circumvent this problem, we assembledocuments that have parallel discourse struc-tures by default: We compile multiple plotsummaries of TV show episodes.
The textualorder of those summaries typically mirrors theunderlying event order of the episodes, in thesame sequence they happened on screen.
Wetake sentence sequences of recaps as paralleldiscourse structures.2.
Extract sentence-level paraphrases: Our sys-tem finds sentence pairs that are either para-phrases themselves, or at least contain para-phrase fragments.
This procedure crucially re-lies on discourse knowledge: A Multiple Se-quence Alignment (MSA) algorithm matchessentences if both their inherent semantic sim-ilarities and the overall similarity score of theirdiscourse contexts are high enough.3.
Extract paraphrase fragments: Sentence-level paraphrases may be too specific for fur-ther domain-independent applications, as they918row recap 1 recap 2 recap 3 recap 4 recap 534She gives Fore-man one shot.Cuddy tells Fore-man he has onechance to prove toher he can run theteam.Cuddy agreesto give him onechance to provehimself.Foreman insists he de-serves a chance andCuddy gives in, warn-ing him he gets oneshot.35Foreman, Hadley,and Taub get theconference roomready and Foremanexplains that he?llbe in charge.Foreman gives thenews to Thirteenand Taub and theyunpack the conferenceroom and go with adiagnosis of CRPS.36They decide thatit might be CRPSand Foreman or-ders a spinal stim-ulation.Foreman says totreat him for com-plex regional painsyndrome with aspinal stimulation.Figure 2: Excerpt from an alignment table for 5 exemplary recaps of Episode 2 (Season 6).contain specific NEs (e.g.
?House?)
or time ref-erences.
Thus we take a necessary second stepand extract finer-grained paraphrase fragmentsfrom the sentence pairs matched in step 2.
Theresulting matched phrases should be grammat-ical and interchangeable regardless of context.We propose and compare different fragment ex-traction algorithms.The remainder of the paper shows how both ofthe paraphrasing steps benefit from using a corpuswith highly parallel discourse structures: The sys-tem components employ discourse information ei-ther directly by using MSA (step 1) or coreferenceresolution (step 2), or indirectly, because using MSAin step 1 results in a high precision gain for the sub-sequent second step.4 Sentence Matching with MSAThis section explains how we apply MSA to ex-tract sentence-level paraphrases from a comparablecorpus.
As our input data, we manually collect re-caps for House M.D.
episodes from different sourceson the web1.
House episodes have an intermediatelength (?45 min), which results in recaps of a con-1e.g.
http://house.wikia.com ?
for a detailed list ofURLs, please check the supplementary material or contact theauthors.venient size (40 to 150 sentences).
The result is onecomparable document collection per episode.
Weapplied a sentence splitter (Gillick, 2009) to the doc-uments and treat them as sequences of sentences forfurther processing.Sequence alignment takes as its input two se-quences consisting of elements of some alphabet,and an alphabet-specific score function cm overpairs of sequence elements.
For insertions and dele-tions, the algorithm additionally takes gap costs(cgap).
Multiple Sequence Alignment generalizespairwise alignment to arbitrarily many sequences.MSA has its main application area in bioinformat-ics, where it is used to identify equivalent parts ofDNA (Durbin et al1998).
Our alphabet consists ofsentences, and a sequence is an ordered sentence listconstituting a recap.A Multiple Sequence Alignment results in a tablelike Fig.
2.
Each column contains the sentences ofone recap, possibly intermitted with gaps (?
?
), andeach row contains at least one non-gap.
If two sen-tences end up in the same row, they are aligned; wetake aligned sentence to be paraphrases.
Aligning asentence with a gap can be thought of as an insertionor deletion.
Each alignment has a score which is thesum of all scores for substitutions and all costs forinsertions and deletions.
Informally, the alignment919score is the sum of all scores for each pair of cells(c1, c2), if c1 and c2 are in the same row.
If either c1or c2 is a gap, the pair?s score is cgap.
If both cellscontain sentences, the score is cm(c1, c2).Fern and Stevenson (2009) showed that sophis-ticated similarity measures improve paraphrasing,so we apply a state-of-the-art vector space model(Thater et al2011) as our score function.
The vec-tor space model provides contextualized similaritiesof words, i.e.
the vector of each word is disam-biguated by the context the current instance occursin.
cm(c1, c2) returns the model?s similarity scorefor c1 and c2.We re-implement a standard MSA algorithm(Needleman and Wunsch, 1970) which approxi-mates the best MSA given the input sequences, cmand cgap.
This algorithm recursively aligns two se-quences at a time, treating the resulting alignmentas a new sequence.
This does not necessarily resultin the globally optimal alignment, because the orderin which sequences are aligned can change the finaloutput.
Given this constraint, the algorithm finds thebest alignment, which - in our case - is the alignmentwith the maximal score.
Intuitively, we are lookingfor the alignment where the most similar sentenceswith the most similar preceding and trailing contextsend up as paraphrases.5 Paraphrase Fragment ExtractionTaking the output of the sentence alignment as in-put, we next extract shorter phrase-level paraphrases(paraphrase fragments) from the matched sentencepairs.
We try different algorithms for this step, allrelying on word-word alignments.5.1 PreprocessingBefore extracting paraphrase fragments, we first pre-process all documents as follows:Stanford CoreNLP 2 provides a set of natural lan-guage analysis tools.
We use the part-of-speech (POS) tagger, the named-entity recog-nizer, the parser (Klein and Manning, 2003),and the coreference resolution system (Lee etal., 2011).
In particular, the dependency struc-tures of the parser?s output are used for VP-2http://nlp.stanford.edu/software/corenlp.shtmlfragment extraction (Sec.
5.3).
The output fromthe coreference resolution system is used tocluster all mentions referring to the same en-tity and to select one as the representative men-tion.
If the representative mention is not a pro-noun, we modify the original texts by replac-ing all pronoun mentions in the cluster with thesyntactic head of the representative mention.Note that the coreference resolution system isapplied to each recap as a whole.GIZA++ (Och and Ney, 2003) is a widely usedword aligner for MT systems.
We amend theinput data by copying identical word pairs 10times and adding them as additional ?sentence?pairs (Byrne et al2003), in order to emphasizethe higher alignment probability between iden-tical words.
We run GIZA++ for bi-directionalword alignment and obtain a lexical translationtable.5.2 Fragment ExtractionAs mentioned in Sec.
2, we choose to use alignment-based approaches to this task, which allows us to usemany existing MT techniques and tools.
We mainlyfollow our previous approach (Wang and Callison-Burch, 2011), which is a modified version of an ap-proach by Munteanu and Marcu (2006) on trans-lation fragment extraction.
We briefly review thethree-step procedure here and refer the reader to theoriginal paper for more details:1.
Establish word-word alignment between eachsentence pair using GIZA++;2.
Smooth the alignment based on lexical occur-rence likelihood;3.
Extract fragment pairs using different heuris-tics, e.g., non-overlapping n-grams, chunkboundaries, or dependency trees.After obtaining a lexical translation table by run-ning GIZA++, for each word pair, w1 and w2, weuse both positive and negative lexical associationsfor the alignment, which are defined as the condi-tional probabilities p(w1|w2) and p(w1|?w2), re-spectively.
The resulting alignment can be furtherconstrained by a modified longest common sub-string (LCS) algorithm, which takes sequences of920words instead of letters as input.
Smoothing (step 2)is done for each word by taking the average score ofit and its four neighbor words.
All the word align-ments (excluding stop-words) with positive scoresare selected as candidate fragment elements.Provided with the candidate fragment elements,we previously (Wang and Callison-Burch, 2011)used a chunker3 to finalize the output fragments, inorder to follow the linguistic definition of a (para-)phrase.
We extend this step in the current systemby applying a dependency parser to constrain theboundary of the fragments (Sec.
5.3).
Finally, wefilter out trivial fragment pairs, such as identical orthe original sentence pairs.5.3 VP-fragment ExtractionTo obtain more grammatical output fragments, weadd another layer of linguistic information to ourinput sentences.
Based on the dependency parsesproduced during preprocessing, we extract phrasescontaining verbs and their complements.
More pre-cisely, we match two phrases if their respective sub-trees t1 and t2 satisfy the following conditions:?
The subtrees mirror a complete subset ofthe GIZA++ word alignment, i.e., all wordsaligned to a given word in t1 are contained int2, and vice versa.
For empty alignments, werequire an overlap of at least one lemma (ig-noring stop words).?
The root nodes of t1 and t2 have the sameroles within their trees, e.g., we match clauseswith an xcomp-label only with other xcomp-labelled clauses.?
Both t1 and t2 contain at least one verb withat least one complement.
To enhance recall,we additionally extract complete prepositionalphrases.?
We exclude trivial fragment pairs that are pre-fixes or suffixes of each other (or identical).The main advantage of this approach lies in the out-put?s grammaticality, because the subtrees alwaysmatch complete phrases.
This method also functionsas a filtering mechanism for mistakenly aligned sen-tences: If only the two sentence nodes are returned3We use the same OpenNLP chunker (http://opennlp.sourceforge.net/) for consistency.as possible matching partners, the pair is discardedfrom the results.6 EvaluationWe evaluate both sentential paraphrase matchingand paraphrase fragment extraction using manuallylabelled gold standards (provided in the supplemen-tary material).
We collect recaps for all 20 episodesof season 6 of House M.D., taking 8 summaries perepisode (the supplementary material contains a listof all URLs).
This results in 160 documents con-taining 14735 sentences.
For evaluation, we use allepisodes except no.
2, which is held out for parame-ter optimizations and other development purposes.6.1 Sentential Paraphrase EvaluationTo evaluate sentence matching, we adapt the base-lines from our earlier work (Regneri et al2010) andcreate a new gold standard.
We compute precision,recall and accuracy of our main system and suggestbaselines that separately show the influence of boththe MSA and the semantic scoring function.Gold-StandardWe aim to create an evaluation set that containsa sufficient amount of genuine paraphrases.
Find-ing such sentence pairs with random sampling andmanual annotation is infeasible: There are more than200, 000, 000 possible sentence pairs, and we ex-pect less than 1% of them to be paraphrases.
Wethus sample pairs that either the system or the base-lines recognized as paraphrases and try to create anevaluation set that is not biased towards the actualsystem or any of the baselines.
The evaluation setconsists of 2000 sentence pairs: 400 that the systemrecognized as paraphrases, 400 positively labelledpairs for each of the three baselines (described in thefollowing section) and 400 randomly selected pairs.For the final evaluation, we compute precision, re-call, f-score and accuracy for our main system andeach baseline on this set.Two annotators labelled each sentence pair(S1, S2) with one of the following labels:1. paraphrases: S1 and S2 refer to exactly thesame event(s).2. containment: S1 contains all the event infor-mation mentioned in S2, but refers to at least921one additional event, or vice versa.3.
related: S1 and S2 overlap in at least one eventreference, but both refer to at least one addi-tional event.4.
unrelated: S1 and S2 do not overlap at all.This scheme has a double purpose: The main objec-tive is judging whether two sentences contain para-phrases (1-3) or if they are unrelated (4).
We usethis coarser distinction for system evaluation by col-lapsing the categories 1-3 in one paraphrasecoll cat-egory.
Secondly, the annotation shows how well thesentences fit each other?s content (1 vs. 2&3), andhow much work needs to be done to extract the sen-tence parts with the same meaning (2 vs. 3).The inter-annotator agreement according to Co-hen?s Kappa (Cohen, 1960) is ?
= 0.55 (?mod-erate agreement?).
The distinction between unre-lated cases and elements of paraphrasecoll reaches?
= 0.71 (?substantial agreement?).
For the finalgold standard, a third annotator resolved all conflictcases.Among all gold standard sentence pairs, we find158 paraphrases, 238 containment cases, 194 re-lated ones and 1402 unrelated.
We had to discard 8sentence pairs because one of the items was invalidor empty.
The high proportion of ?unrelated?
casesresults from the 400 random pairs and the low pre-cision of the baselines.
Looking at the paraphrases,27% of the 590 instances in the paraphrasecoll cate-gory are proper paraphrases, and 73% of them con-tain additional information that does not belong tothe paraphrased part.Experimental SetupWe compute precision, recall and f-score with re-spect to the gold standard (paraphrases are membersof paraphrasecoll), taking f-score as follows:f -score =2 ?
precision ?
recallprecision+ recallWe also compute accuracy as the overall fraction ofcorrect labels (negative and positive ones).Our main system uses MSA (denoted by MSA af-terwards) with vector-based similarities (VEC) as ascoring function.
The gap costs are optimized forf-score, resulting in cgap = 0.4To show the contribution of MSA?s structuralcomponent and compare it to the vector model?scontribution, we create a second MSA-based sys-tem that uses MSA with BLEU scores (Papineni etal., 2002) as scoring function (MSA+BLEU).
BLEUestablishes the average 1-to-4-gram overlap of twosentences.
The gap costs for this baseline were opti-mized separately, ending up with cgap = 1.In order to quantify the contribution of the align-ment, we create a discourse-unaware baseline bydropping the MSA and using a state-of-the-art clus-tering algorithm (Noack, 2007) fed with the vec-tor space model scores (CLUSTER+VEC).
The algo-rithm partitions the set of sentences into paraphraseclusters such that the most similar sentences end upin one cluster.
This does not require any parametertuning.We also show a baseline that uses the cluster-ing algorithm with BLEU scores (CLUSTER+BLEU).The comparison of this baseline with the otherclustering-baseline that uses vector similarities helpsto underline the sentence similarities?
advantagecompared to pure word overlap.
Note that the CLUS-TER+BLEU system resembles popular n-gram over-lap measures for paraphrase classification.We also show the results completely random labelassignment, which constitutes a lower bound for thebaselines and the system.ResultsOverall, our system extracts 20379 paraphrasepairs.
Tab.
1 shows the evaluation results on ourgold-standard.The MSA based system variants outperform thetwo clustering baselines significantly (all levels referto p = 0.01 and were tested with a resampling test(Edgington, 1986)).The clustering baselines perform significantlybetter than a random baseline, especially consider-ing recall.
The more elaborated vector-space mea-sure even gives 10% more in precision and accu-racy, and overall 14% more in f-score.
This is al-4Gap costs directly influence precision and recall: ?cheap?gaps lead to a more restrictive system with higher precision, andmore expensive gaps give more recall.
We chose f-score as ourobjective.922System Prec.
Recall F-score Acc.RANDOM 0.30 0.49 0.37 0.51CLUSTER+BLEU 0.35 0.63 0.45 0.54CLUSTER+VEC 0.40 0.68 0.51 0.61MSA+BLEU 0.73 0.74 0.73 0.84MSA+VEC 0.79 0.66 0.72 0.85Table 1: Results for sentence matching.ready a remarkable improvement compared to therandom baseline, and still a significant one com-pared to CLUSTER+BLEU.Adding structural knowledge with MSA im-proves the clustering?s accuracy performance by24% (CLUSTER+VEC vs. MSA+VEC), precisioneven goes up by 39%.Intuitively we expected the MSA-based systemsto end up with a higher recall than the clusteringbaselines, because sentences can be matched evenif their similarity is moderate or low, but their dis-course context is highly similar.
However, this isonly the case for the system using BLEU scores, butnot for the system based on the vector space model.One possible explanation lies in picking f-score asobjective for the optimization of the gap costs forMSA: For the naturally more restrictive word over-lap measure, this leads to a more recall-orientedsystem with a low threshold for aligning sentences,whereas the gap costs for the vector-based systemfavors a more restrictive alignment with more pre-cise results.The comparison of the two MSA-based sys-tems highlights the great benefit of using structuralknowledge: Both MSA+BLEU and MSA+VEC havecomparable f-scores and accuracy.
The advantagefrom using the vector-space model that is still obvi-ous for the clustering baselines is nearly evened outwhen adding discourse knowledge as a backbone.However, the vector model still results in nominallyhigher precision and accuracy.It is hard to do a direct comparison with state-of-the-art paraphrase recognition systems, becausemost are evaluated on different corpora, e.g., theMicrosoft paraphrase corpus (Dolan and Brockett,2005, MSR).
We cannot apply our system to theMSR corpus, because we take complete texts as in-put, while the MSR corpus solely delivers sentencepairs.
While the MSR corpus is larger than ourcollection, the wording variations in its paraphrasepairs are usually lower than for our examples.
Thusthe final numbers of previous approaches might bevaguely comparable with our results: Das and Smith(2009) present two systems reaching f-scores of 0.82and 0.83, with a precision of 0.75 and 0.80.
Bothprecision and f-scores of our msa-based systems liewithin the same range.
Heilman and Smith (2010)introduce a recall-oriented system, which reaches anf-score of 0.81 by a precision of 0.76.
Compared tothis system, our approach results in better precisionvalues.All further computations bases on the system us-ing MSA and the vector space model (MSA+VEC),because it achieves the highest precision and accu-racy values.6.2 Paraphrase Fragment EvaluationWe also manually evaluate precision on paraphrasefragments, and additionally describe the productiv-ity of the different setups, providing some intuitionabout the methods?
recall.Gold-StandardWe randomly collect 150 fragment pairs for eachof the five system configurations (explained in thefollowing section).
Each fragment pair (f1, f2) isannotated with one of the following categories:1. paraphrases: f1 and f2 convey the samemeaning, i.e., they are well-formed and goodmatches on the content level.2.
related: f1 and f2 overlap in their meaning, butone or both phrases have additional unmatchedinformation.3.
irrelevant: f1 and f2 are unrelated.This labeling scheme again assesses precision aswell as paraphrase granularity.
For precision rating,we collapse categories 1&2 into one paraphrasecollcategory.
Each pair is labelled by two annotators,who were shown both the fragments and the wholesentences they originate from.
Overall, the ratershad an agreement of ?
= 0.67 (?substantial agree-ment?
), which suggests that the task was easier thansentence level annotation.
The agreement for the923distinction between the paraphrasecoll categoriesand irrelevant instances reaches a level of ?
= 0.88(also ?substantial agreement?).
All conflicts wereagain adjudicated by a third annotator.
Overall, thegold standard contains 190 paraphrases, 258 relatedpairs and 302 irrelevant instances.
Unlike previ-ous approaches to fragment extraction, we do notevaluate grammaticality, given that the VP-fragmentmethod implicitly constrains the output fragments tobe complete phrases.Configurations & ResultsWe take the output of the sentence matching sys-tem MSA+VEC as input for paraphrase fragment ex-traction.
As detailed in Sec.
5, our core fragmentmodule uses the word-word alignments provided byGIZA++ and uses a chunker for fragment extrac-tion.
We successively enrich this core module withmore information, either by longest common sub-string (LCS) matching or by operating on depen-dency trees (VP).
In addition, we evaluate the in-fluence of coreference resolution by preprocessingthe input to the best performing configuration withpronoun resolution (COREF).We mainly compute precision for this task, as therecall of paraphrase fragments is difficult to define.However, we do include a measure we call produc-tivity to indicate the algorithm?s completeness.
It isdefined as the ratio between the number of result-ing fragment pairs and the number of sentence pairsused as input.Extraction Method Precision ProductivityMSA 0.57 0.76MSA+LCS 0.45 0.30MSA+VP 0.81 0.42MSA+VP+COREF 0.84 0.45Table 2: Results of paraphrase fragment extraction.Tab.
2 shows the evaluation results.
We reachour best precision by using the VP-fragment heuris-tics, which is still more productive than the LCSmethod.
The grammatical filter gives us a higherprecision compared to the purely alignment-basedapproaches.
Enhancing the system with corefer-ence resolution raises the score even further.
Wecannot directly compare this performance to othersystems, as all other approaches have different datasources.
However, precision is usually manuallyevaluated, so the figures are at least indicative fora comparison with previous work: One state-of-the-art system introduced by Zhao et al2008) extractsparaphrase fragments from bilingual parallel cor-pora and reaches a precision of 0.67.
We found thesame number using our previous approach (Wangand Callison-Burch, 2011), which is roughly equiv-alent to our core module.
Our approach outperformsboth by 17% with similar estimated productivity.As a final comparison, we show how the perfor-mance of the sentence matching methods directly af-fects the fragment extraction.
We use the VP-basedfragment extraction system (VP), and compare theperformances by using either the outputs from ourmain system (MSA+VP) or alternatively the base-line that replaces MSA with a clustering algorithm(CLUSTER+VP).
Both variants use the vector-basedsemantic similarity measure.Sentence matching Precision ProductivityCLUSTER+VP 0.31 0.04MSA+VP 0.81 0.42Table 3: Impact of MSA on fragment extractionAs shown in Tab.
3, the precision gain from usingMSA becomes tremendous during further process-ing: We beat the baseline by 50% here, and produc-tivity increases by a factor of 10.
This means that thebaseline produces on average 0.01 good fragmentpairs per matched sentence pair, and the final sys-tem extracts 0.3 of them.
Those numbers show thatfor any application that acquires paraphrases of arbi-trary granularity, sequential event information pro-vides an invaluable source to achieve a lean para-phrasing method with high precision.6.3 Example outputFig.
3 shows exemplary results from our systempipeline, using the VP?FRAGMENTS method withfull coreference resolution on the sentence pairs ex-tracted by MSA.
The results reflect the importanceof discourse information for this task: Sentences arecorrectly matched in spite of not having common de-924Sentence 1 [with fragment 1] Sentence 2 [with fragment 2]1 Taub meets House for dinner and claims [thatRachel had a pottery class].Taub shows up for his dinner with House withoutRachel, explaining [that she?s at a ceramics class].2 House doesn?t want her to go and she doesn?t wantto go either, but [she can?t leave her family.
]Lydia admits that she doesn?t want to leave House but[she has to stay with her family].3 Thirteen is in a cab to the airport when she findsout that [her trip had been canceled].Hadley discovers that [her reservation has been can-celled].4 Nash asks House [for the extra morphine].
The patient is ready [for more morphine].5 House comes in to tell Wilson that Tucker has can-cer and [shows him the test results].House comes in and [informs Wilson that the tests haveproven positive]: Tucker has cancer.6 Foreman tells him [to confide in Cameron].
When Chase points out they can?t move Donny with-out alerting Cameron, Foreman tells Chase [to be honestwith his wife].7 Thirteen breaks [into the old residence] and tellsTaub that she realizes that he?s been with Maya.Taub and Thirteen break [into Ted?s former residence].8 He finds [a darkened patch on his right foot nearthe big toe].House finally finds [a tumorous mole on his toe].Figure 3: Example results; fragments extracted from aligned sentences are bracketed and emphasized.pendency patterns (e.g., Example 4) or sharing manyn-grams (6-8).
Additionally, the coreference resolu-tion allows us to match Rachel (1) and Wilson (5) tothe correct corresponding pronouns.
All examplesshow that this technique of matching sentence couldeven help to make coreference resolution better, be-cause we can easily identify Cameron with his wife,Lydia with the respective pronouns, Nash with ThePatient or the nickname Thirteen with Hadley, thecharacter?s actual name.7 Conclusion and Future WorkWe presented our work on paraphrase extraction us-ing discourse information, on a corpus consistingof recaps of TV show episodes.
Our approach firstuses MSA to extract sentential paraphrases, whichare then further processed to compute finer-grainedparaphrase fragments using dependency trees andpronoun resolution.
The experimental results showgreat advantages from using discourse information,beating informed baselines and performing compet-itively with state-of-the-art systems.For future work, we plan to use MSA to alignsingle clauses rather than whole sentences.
Thiscan also help to define the fragment boundariesmore clearly.
Additionally, we plan to generalizethe method for other parallel texts by preprocessingthem with a temporal classifier.
In a more advancedstep, we will also use the aligned paraphrases to helpresolving discourse structure, e.g.
for coreferenceresolution, which could lead to a high-performancebootstrapping system.
In a long-term view, it wouldbe interesting to see how aligned discourse treescould help to extract paraphrases from arbitrary par-allel text.AcknowledgementsThe first author was funded by the Clusterof Excellence ?Multimodal Computing and In-teraction?
in the German Excellence Initiative.The second Author was funded by the Eu-ropean Community?s Seventh Framework Pro-gramme (FP7/2007-2013) under grant agreementNo.
287923 (EXCITEMENT, http://www.excitement-project.eu/).
?
We want tothank Stefan Thater for supplying the semantic sim-ilarity scores of his algorithm for our data.
We aregrateful to Manfred Pinkal, Alexis Palmer and threeanonymous reviewers for their helpful comments onprevious versions of this paper.925ReferencesAmit Bagga and Breck Baldwin.
1999.
Cross-documentevent coreference: annotations, experiments, and ob-servations.
In Proceedings of the Workshop on Coref-erence and its Applications.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of ACL 2005.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proc.
of HLT-NAACL 2003.Regina Barzilay and Kathleen R. McKeown.
2001.
Ex-tracting paraphrases from a parallel corpus.
In Proc.of ACL 2001.Regina Barzilay, Kathleen McKeown, and Michael El-hadad.
1999.
Information fusion in the context ofmulti-document summarization.
In Proceedings ofACL 1999.W.
Byrne, S. Khudanpur, W. Kim, S. Kumar, P. Pecina,P.
Virga, P. Xu, and D. Yarowsky.
2003.
The JohnsHopkins University 2003 Chinese-English machinetranslation system.
In Proceedings of the MT SummitIX.Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of EMNLP 2008.Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.2002.
RST Discourse Treebank.
LDC.J.
Cohen.
1960.
A Coefficient of Agreement for NominalScales.
Educational and Psychological Measurement,20(1):37.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The pascal recognising textual entailment chal-lenge.
In MLCW, pages 177?190.D.
Das and N. A. Smith.
2009.
Paraphrase identifica-tion as probabilistic quasi-synchronous recognition.
InProceedings of ACL-IJCNLP 2009.Louise Del?ger and Pierre Zweigenbaum.
2009.
Extract-ing lay paraphrases of specialized expressions frommonolingual comparable medical corpora.
In Pro-ceedings of the ACL-IJCNLP BUCC-2009 Workshop.Georgiana Dinu and Rui Wang.
2009.
Inference rulesand their application to recognizing textual entailment.In Proceedings of EACL 2009.W.
B. Dolan and C. Brockett.
2005.
Automatically con-structing a corpus of sentential paraphrases.
In Pro-ceedings of the third International Workshop on Para-phrasing.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised construction of large paraphrase corpora:Exploiting massively parallel news sources.
In Pro-ceedings of COLING 2004.Richard Durbin, Sean Eddy, Anders Krogh, and GraemeMitchison.
1998.
Biological Sequence Analysis.Cambridge University Press.Eugene S Edgington.
1986.
Randomization tests.
Mar-cel Dekker, Inc., New York, NY, USA.Samuel Fern and Mark Stevenson.
2009.
A semanticsimilarity approach to paraphrase detection.
In Pro-ceedings of the Computational Linguistics UK (CLUK2008) 11th Annual Research Colloquium.Juri Ganitkevitch, Chris Callison-Burch, CourtneyNapoles, and Benjamin Van Durme.
2011.
Learningsentential paraphrases from bilingual parallel corporafor text-to-text generation.
In Proceedings of EMNLP2011.Dan Gillick.
2009.
Sentence boundary detection and theproblem with the u.s.
In Proceedings of HLT-NAACL2009: Companion Volume: Short Papers.Michael Heilman and Noah A. Smith.
2010.
Treeedit models for recognizing textual entailments, para-phrases, and answers to questions.
In Proceedings ofNAACL-HLT 2010.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL 2003.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanford?s multi-pass sieve coreference resolution sys-tem at the conll-2011 shared task.
In Proceedings ofthe CoNLL-2011 Shared Task.Dekang Lin and Patrick Pantel.
2001.
DIRT - Discoveryof Inference Rules from Text.
In Proceedings of theACM SIGKDD.Yuval Marton, Chris Callison-Burch, and Philip Resnik.2009.
Improved Statistical Machine Translation UsingMonolingually-Derived Paraphrases.
In Proceedingsof EMNLP 2009.Dragos Stefan Munteanu and Daniel Marcu.
2006.
Ex-tracting Parallel Sub-Sentential Fragments from Non-Parallel Corpora.
In Proceedings of ACL 2006.Saul B. Needleman and Christian D. Wunsch.
1970.
Ageneral method applicable to the search for similaritiesin the amino acid sequence of two proteins.
Journal ofmolecular biology, 48(3), March.Andreas Noack.
2007.
Energy models for graph cluster-ing.
Journal of Graph Algorithms and Applications,11(2):453?480.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of ACL2002.926Chris Quirk, Chris Brockett, and William B. Dolan.2004.
Monolingual machine translation for paraphrasegeneration.
In Proceedings of EMNLP 2004.Chris Quirk, Raghavendra Udupa, and Arul Menezes.2007.
Generative models of noisy translations withapplications to parallel fragment extraction.
In Pro-ceedings of MT Summit XI, Copenhagen, Denmark.Michaela Regneri, Alexander Koller, and ManfredPinkal.
2010.
Learning Script Knowledge with WebExperiments.
In Proceedings of ACL 2010.Yusuke Shinyama and Satoshi Sekine.
2003.
Paraphraseacquisition for information extraction.
In Proceedingsof the ACL PARAPHRASE ?03 Workshop.Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.2002.
Automatic paraphrase acquisition from news ar-ticles.
In Proceedings of HLT 2002.Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-tura Coppola.
2004.
Scaling Web-based Acquisitionof Entailment Relations.
In Proceedings of EMNLP2004.Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.2011.
Word Meaning in Context: A Simple and Effec-tive Vector Model.
In Proceedings of IJCNLP 2011.Eleftheria Tomadaki and Andrew Salway.
2005.
Match-ing verb attributes for cross-document event corefer-ence.
In Proc.
of the Interdisciplinary Workshop onthe Identification and Representation of Verb Featuresand Verb Classes.Rui Wang and Chris Callison-Burch.
2011.
Para-phrase fragment extraction from monolingual compa-rable corpora.
In Proc.
of the ACL BUCC-2011 Work-shop.Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.2008.
Pivot Approach for Extracting Paraphrase Pat-terns from Bilingual Corpora.
In Proceedings of ACL2008.Shiqi Zhao, Haifeng Wang, Xiang Lan, and Ting Liu.2010.
Leveraging Multiple MT Engines for Para-phrase Generation.
In Proceedings of COLING 2010.927
