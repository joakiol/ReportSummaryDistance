Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
986?995, Prague, June 2007. c?2007 Association for Computational LinguisticsAn Empirical Study on Computing Consensus Translationsfrom Multiple Machine Translation SystemsWolfgang MachereyGoogle Inc.1600 Amphitheatre ParkwayMountain View, CA 94043, USAwmach@google.comFranz Josef OchGoogle Inc.1600 Amphitheatre ParkwayMountain View, CA 94043, USAoch@google.comAbstractThis paper presents an empirical study onhow different selections of input translationsystems affect translation quality in systemcombination.
We give empirical evidencethat the systems to be combined should beof similar quality and need to be almostuncorrelated in order to be beneficial for sys-tem combination.
Experimental results arepresented for composite translations com-puted from large numbers of different re-search systems as well as a set of transla-tion systems derived from one of the best-ranked machine translation engines in the2006 NIST machine translation evaluation.1 IntroductionComputing consensus translations from the outputsof multiple machine translation engines has becomea powerful means to improve translation quality inmany machine translation tasks.
Analogous to theROVER approach in automatic speech recognition(Fiscus, 1997), a composite translation is computedby voting on the translation outputs of multiplemachine translation systems.
Depending on howthe translations are combined and how the votingscheme is implemented, the composite translationmay differ from any of the original hypotheses.While elementary approaches simply select for eachsentence one of the original translations, more so-phisticated methods allow for combining transla-tions on a word or a phrase level.Although system combination could be shownto result in substantial improvements in terms oftranslation quality (Matusov et al, 2006; Sim et al,2007), not every possible ensemble of translationoutputs has the potential to outperform the primarytranslation system.
In fact, an adverse combina-tion of translation systems may even deterioratetranslation quality.
This holds to a greater extent,when the ensemble of translation outputs contains asignificant number of translations produced by lowperforming but highly correlated systems.In this paper we present an empirical study onhow different ensembles of translation outputs affectperformance in system combination.
In particular,we will address the following questions:?
To what extent can translation quality benefitfrom combining systems developed by multipleresearch labs?Despite an increasing number of translationengines, most state-of-the-art systems in statis-tical machine translation are nowadays basedon implementations of the same techniques.For instance, word alignment models are oftentrained using the GIZA++ toolkit (Och andNey, 2003); error minimizing training criteriasuch as the Minimum Error Rate Training(Och, 2003) are employed in order to learnfeature function weights for log-linear models;and translation candidates are produced usingphrase-based decoders (Koehn et al, 2003)in combination with n-gram language models(Brants et al, 2007).All these methods are established as de factostandards and form an integral part of moststatistical machine translation systems.
This,however, raises the question as to what ex-tent translation quality can be expected toimprove when similarly designed systems arecombined.?
How can a set of diverse translation systems bebuilt from a single translation engine?Without having access to different translation986engines, it is desirable to build a large numberof diverse translation systems from a singletranslation engine that are useful in systemcombination.
The mere use of N -best listsand word lattices is often not effective, becauseN -best candidates may be highly correlated,thus resulting in small diversity compared tothe first best hypothesis.
Therefore, we need acanonical way to build a large pool of diversetranslation systems from a single translationengine.?
How can an ensemble of translation outputsbe selected from a large pool of translationsystems?Once a large pool of translation systems isavailable, we need an effective means to selecta small ensemble of translation outputs forwhich the combined system outperforms thebest individual system.These questions will be investigated on the basisof three approaches to system combination: (i) anMBR-like candidate selection method based onBLEU correlation matrices, (ii) confusion networksbuilt from word sausages, and (iii) a novel two-pass search algorithm that aims at finding consensustranslations by reordering bags of words constitutingthe consensus hypothesis.Experiments were performed on two Chinese-English text translation corpora under the conditionsof the large data track as defined for the 2006 NISTmachine translation evaluation (MT06).
Resultsare reported for consensus translations built fromsystem outputs provided by MT06 participants aswell as systems derived from one of the best-rankedtranslation engines.The remainder of this paper is organized as fol-lows: in Section 2, we describe three combina-tion methods for computing consensus translations.In Sections 3.1 and 3.2, we present experimentalresults on combining system outputs provided byMT06 participants.
Section 3.3 shows how correla-tion among translation systems affects performancein system combination.
In Section 3.4, we discusshow a single translation engine can be modifiedin order to produce a large number of diversetranslation systems.
First experimental results us-ing a greedy search algorithm to select a smallensemble of translation outputs from a large poolof canonically built translation systems are reported.A summary presented in Section 4 concludes thepaper.2 Methods for System CombinationSystem combination in machine translation aims tobuild a composite translation from system outputsof multiple machine translation engines.
Dependingon how the systems are combined and which votingscheme is implemented, the consensus translationmay differ from any of the original candidate trans-lations.
In this section, we discuss three approachesto system combination.2.1 System Combination via CandidateSelectionThe easiest and most straightforward approach tosystem combination simply returns one of the orig-inal candidate translations.
Typically, this selectionis made based on translation scores, confidence esti-mations, language and other models (Nomoto, 2004;Paul et al, 2005).
For many machine translationsystems, however, the scores are often not normal-ized or may even not be available, which makesit difficult to apply this technique.
We thereforepropose an alternative method based on ?correlationmatrices?
computed from the BLEU performancemeasure (Papineni et al, 2001).Let e1, ..., eM denote the outputs of M translationsystems, each given as a sequence of words inthe target language.
An element of the BLEUcorrelation matrix B  pbijq is defined as thesentence-based BLEU score between a candidatetranslation ei and a pseudo-reference translation ejpi, j  1, ...,Mq:bij  BPpei, ejq  exp$''%144?n1log ?npei, ejq,//-.
(1)Here, BP denotes the brevity penalty factor with ?ndesignating the n-gram precisions.Because the BLEU score is computed on a sen-tence rather than a corpus-level, n-gram precisionsare capped by the maximum over 12|ei| and ?n inorder to avoid singularities, where |ei| is the lengthof the candidate translation 1.Due to the following properties, B can be inter-preted as a correlation matrix, although the termdoes not hold in a strict mathematical sense: (i)bij P r0, 1s; (ii) bij  1.0 ??
ei  ej ; (iii) bij 0.0 ??
eiXej  H, i.e., bij is zero if and only ifnone of the words which constitute ei can be found1 Note that for non-zero n-gram precisions, ?n is alwayslarger than 12|e| .987in ej and vice versa.
The BLEU correlation matrixis in general, however, not symmetric, although inpractice, ||bij  bji|| is typically negligible.Each translation system m is assigned to a systemprior weight ?m P r0, 1s, which reflects the perfor-mance of system m relatively to all other translationsystems.
If no prior knowledge is available, ?m isset to 1{M .Now, let ?
 p?1, ..., ?M qJ denote a vector ofsystem prior weights and let b1, ...,bM denote therow vectors of the matrix B.
Then the translationsystem with the highest consensus is given by:e  em withm  argmaxem!
?J  bm) (2)The candidate selection rule in Eq.
(2) has two usefulproperties:?
The selection does not depend on scored trans-lation outputs; the mere target word sequenceis sufficient.
Hence, this technique is alsoapplicable to rule-based translation systems 2.?
Using the components of the row-vector bmas feature function values for the candidatetranslation em (m  1, ...,M ), the systemprior weights ?
can easily be trained usingthe Minimum Error Rate Training described in(Och, 2003).Note that the candidate selection rule in Eq.
(2)is equivalent to re-ranking candidate translationsaccording to the Minimum Bayes Risk (MBR) deci-sion rule (Kumar and Byrne, 2004), provided thatthe system prior weights are used as estimationsof the posterior probabilities ppe|fq for a sourcesentence f .
Due to the proximity of this methodto the MBR selection rule, we call this combinationscheme MBR-like system combination.2.2 ROVER-Like Combination SchemesROVER-like combination schemes aim at comput-ing a composite translation by voting on confusionnetworks that are built from translation outputsof multiple machine translation engines via an it-erative application of alignments (Fiscus, 1997).To accomplish this, one of the original candidatetranslations, e.g.
em, is chosen as the primarytranslation hypothesis, while all other candidates enpn  mq are aligned with the word sequence of2 This property is not exclusive to this combination schemebut also holds for the methods discussed in Sections 2.2 and 2.3.the primary translation.
To limit the costs whenaligning a permutation of the primary translation,the alignment metric should allow for small shiftsof contiguous word sequences in addition to thestandard edit operations deletions, insertions, andsubstitutions.
These requirements are met by theTranslation Edit Rate (TER) (Snover et al, 2006):TERpei, ejqDel  Ins  Sub  Shift|ej |(3)The outcome of the iterated alignments is a wordtransition network which is also known as wordsausage because of the linear sequence of corre-spondence sets that constitute the network.
Sinceboth the order and the elements of a correspondenceset depend on the choice of the primary transla-tion, each candidate translation is chosen in turnas the primary system.
This results in a total ofM word sausages that are combined into a singlesuper network.
The word sequence along the cost-minimizing path defines the composite translation.To further optimize the word sausages, we replaceeach system prior weight ?m with the lp-norm overthe normalized scalar product between the weightvector ?
and the row vector bm:?1m p?J  bmq`?m?p?J  bm?q`, ` P r0, 8q (4)As ` approaches  8, ?1m  1 if and only ifsystem m has the highest consensus among all inputsystems; otherwise, ?1m  0.
Thus, the wordsausages are able to emulate the candidate selectionrule described in Section 2.1.
Setting `  0 yieldsuniform system prior weights, and setting B tothe unity matrix provides the original prior weightsvector.
Word sausages which take advantage of therefined system prior weights are denoted by wordsausages+.2.3 A Two-Pass Search AlgorithmThe basic idea of the two-pass search algorithm isto compute a consensus translation by reorderingwords that are considered to be constituents of thefinal consensus translation.Initially, the two-pass search is given a repositoryof candidate translations which serve as pseudoreferences together with a vector of system priorweights.
In the first pass, the algorithm usesa greedy strategy to determine a bag of wordswhich minimizes the position-independent word er-ror rate (PER).
These words are considered to be988constituents of the final consensus translation.
Thegreedy strategy implicitly ranks the constituents,i.e., words selected at the beginning of the firstphase reduce the PER the most and are consideredto be more important than constituents selected inthe end.
The first pass finishes when putting furtherconstituents into the bag of words does not improvethe PER.The list of constituents is then passed to a sec-ond search algorithm, which starts with the emptystring and then expands all active hypotheses bysystematically inserting the next unused word fromthe list of constituents at different positions in thecurrent hypothesis.
For instance, a partial consensushypothesis of length l expands into l   1 newhypotheses of length l 1.
The resulting hypothesesare scored with respect to the TER measure based onthe repository of weighted pseudo references.
Low-scoring hypotheses are pruned to keep the space ofactive hypotheses small.
The algorithm will finishif either no constituents are left or if expanding theset of active hypotheses does not further decreasethe TER score.
Optionally, the best consensus hy-pothesis found by the two-pass search is combinedwith all input translation systems via the MBR-likecombination scheme described in Section 2.1.
Thisrefinement is called two-pass+.2.4 Related WorkResearch on multi-engine machine translation goesback to the early nineties.
In (Robert and Nirenburg,1994), a semi-automatic approach is described thatcombines outputs from three translation systems tobuild a consensus translation.
(Nomoto, 2004) and(Paul et al, 2005) used translation scores, languageand other models to select one of the originaltranslations as consensus translation.
(Bangalore etal., 2001) used a multiple string alignment algorithmin order to compute a single confusion network,on which a consensus hypothesis was computedthrough majority voting.
Because the alignmentprocedure was based on the Levenshtein distance,it was unable to align translations with significantlydifferent word orders.
(Jayaraman and Lavie, 2005)tried to overcome this problem by using confi-dence scores and language models in order to ranka collection of synthetic combinations of wordsextracted from the original translation hypotheses.Experimental results were only reported for theMETEOR metric (Banerjee and Lavie, 2005).
In(Matusov et al, 2006), pairwise word alignmentsof the original translation hypotheses were estimatedfor an enhanced statistical alignment model in orderTable 1: Corpus statistics for two Chinese-Englishtext translation sets: ZHEN-05 is a randomselection of test data used in NIST evaluations priorto 2006; ZHEN-06 comprises the NIST portion ofthe Chinese-English evaluation data used in the2006 NIST machine translation evaluation.corpus Chinese EnglishZHEN-05 sentences 2390chars / words 110647 67737ZHEN-06 sentences 1664chars / words 64292 41845to explicitly capture word re-ordering.
Althoughthe proposed method was not compared with otherapproaches to system combination, it resulted insubstantial gains and provided new insights intosystem combination.3 Experimental ResultsExperiments were conducted on two corpora forChinese-English text translations, the first of whichis compiled from a random selected subset of eval-uation data used in the NIST MT evaluations up tothe year 2005.
The second data set consists of theNIST portion of the Chinese-English data used inthe MT06 evaluation and comprises 1664 Chinesesentences collected from broadcast news articles(565 sentences), newswire texts (616 sentences), andnews group texts (483 sentences).
Both corporaprovide 4 reference translations per source sentence.Table 1 summarizes some corpus statistics.For all experiments, system performance wasmeasured in terms of the IBM-BLEU score (Pap-ineni et al, 2001).
Compared to the NIST imple-mentation of the BLEU score, IBM-BLEU followsthe original definition of the brevity penalty (BP)factor: while in the NIST implementation the BP isalways based on the length of the shortest referencetranslation, the BP in the IBM-BLEU score is basedon the length of the reference translation which isclosest to the candidate translation length.
Typically,IBM-BLEU scores tend to be smaller than NIST-BLEU scores.
In the following, BLEU always refersto the IBM-BLEU score.Except for the results reported in Section 3.2, weused uniform system prior weights throughout allexperiments.
This turned out to be more stable whencombining different sets of translation systems andhelped to improve generalization.989Table 2: BLEU scores and brevity penalty (BP) factors determined on the ZHEN-06 test set for primarysystems together with consensus systems for the MBR-like candidate selection method obtained bycombining each three adjacent systems with uniform system prior weights.
Primary systems are sorted indescending order with respect to their BLEU score.
The 95% confidence intervals are computed using thebootstrap re-sampling normal approximation method (Noreen, 1989).combination primary system consensus oracleBLEU CI 95% BP BLEU ?
BP pair-CI 95% BLEU BP01, 02, 03 32.10 (0.88) 0.93 32.97 (+0.87) 0.92 [+0.29, +1.46] 38.54 0.9401, 15, 16 32.10 (0.88) 0.93 23.55 ( -8.54) 0.92 [ -9.29, -7.80] 33.55 0.9502, 03, 04 31.71 (0.90) 0.96 31.55 ( -0.16) 0.92 [ -0.65, +0.29] 37.23 0.9503, 04, 05 29.59 (0.88) 0.87 29.55 ( -0.04) 0.88 [ -0.53, +0.41] 35.55 0.9203, 04, 06 29.59 (0.88) 0.87 29.83 (+0.24) 0.90 [ -0.29, +0.71] 35.69 0.9304, 05, 06 27.70 (0.87) 0.94 28.52 (+0.82) 0.91 [+0.15, +1.49] 34.67 0.9405, 06, 07 27.05 (0.81) 0.88 28.21 (+1.16) 0.92 [+0.63, +1.66] 33.89 0.9405, 06, 08 27.05 (0.81) 0.88 28.47 (+1.42) 0.91 [+0.95, +1.95] 34.18 0.9306, 07, 08 27.02 (0.76) 0.92 28.12 (+1.10) 0.94 [+0.59, +1.59] 33.87 0.9507, 08, 09 26.75 (0.79) 0.97 27.79 (+1.04) 0.94 [+0.52, +1.51] 33.54 0.9508, 09, 10 26.41 (0.81) 0.92 26.78 (+0.37) 0.94 [ -0.07, +0.86] 32.47 0.9609, 10, 11 25.05 (0.84) 0.90 24.96 ( -0.09) 0.94 [ -0.59, +0.46] 30.92 0.9710, 11, 12 23.48 (0.68) 1.00 24.24 (+0.76) 0.94 [+0.27, +1.30] 30.08 0.9611, 12, 13 23.26 (0.74) 0.95 24.05 (+0.79) 0.92 [+0.40, +1.23] 29.56 0.9312, 13, 14 22.38 (0.78) 0.87 22.68 (+0.30) 0.89 [ -0.28, +0.95] 28.58 0.9113, 14, 15 22.13 (0.72) 0.89 21.29 ( -0.84) 0.90 [ -1.33, -0.33] 26.61 0.9214, 15, 16 17.42 (0.66) 0.93 18.45 (+1.03) 0.92 [+0.45, +1.56] 23.30 0.9515 17.20 (0.64) 0.91 ?
?
?
?
?
?16 15.21 (0.63) 0.96 ?
?
?
?
?
?3.1 Combining Multiple Research SystemsIn a first experiment, we investigated the effectof combining translation outputs provided fromdifferent research labs.
Each translation systemcorresponds to a primary system submitted to theNIST MT06 evaluation 3.
Table 2 shows the BLEUscores together with their corresponding BP factorsfor the primary systems of 16 research labs (sitenames were anonymized).
Primary systems aresorted in descending order with respect to theirBLEU score.
Table 2 also shows the consensustranslation results for the MBR-like candidate selec-tion method.
Except where marked with an asterisk,all consensus systems are built from the outputsof three adjacent systems.
While only few com-bined systems show a degradation, the majority ofall consensus translations achieve substantial gainsbetween 0.2% and 1.4% absolute in terms of BLEUscore on top of the best individual (primary) system.The column CI provides 95% confidence intervalsfor BLEU scores with respect to the primary systembaseline using the bootstrap re-sampling normal3 For more information see http://www.nist.gov/speech/tests/mt/mt06eval_official_results.htmlapproximation method (Noreen, 1989).
The column?pair-CI?
shows 95% confidence intervals relativeto the primary system using the paired bootstrapre-sampling method (Koehn, 2004).
The princi-ple of the paired bootstrap method is to create alarge number of corresponding virtual test sets byconsistently selecting candidate translations with re-placement from both the consensus and the primarysystem.
The confidence interval is then estimatedover the differences between the BLEU scores ofcorresponding virtual test sets.
Improvements areconsidered to be significant if the left boundary ofthe confidence interval is larger than zero.Oracle BLEU scores shown in Table 2 are com-puted by selecting the best translation among thethree candidates.
The oracle scores might indicate alarger potential of the MBR-like selection rule, andfurther gains could be expected if the candidate se-lection rule is combined with confidence measures.Table 2 shows that it is important that all trans-lation systems achieve nearly equal quality; com-bining high-performing systems with low-qualitytranslations typically results in clear performancelosses compared to the primary system, which is thecase when combining, e.g., systems 01, 15, and 16.990Table 3: BLEU scores and brevity penalty (BP) factors determined on the ZHEN-06 test set for thecombination of multiple research systems using the MBR-like selection method with uniform and trainedsystem prior weights.
Prior weights are trained using 5-fold cross validation.
The 95% confidence intervalsrealtive to uniform weights are computed using the paired bootstrap re-sampling method (Koehn, 2004).# systems combination uniform ?
opt.
on dev.
?
opt.
on testBLEU BP BLEU BP pair-CI 95% BLEU BP3 01 ?
03 32.98 0.92 33.03 0.93 [ -0.23, +0.34] 33.60 0.934 01 ?
04 33.44 0.93 33.46 0.93 [ -0.26, +0.29] 34.97 0.945 01 ?
05 33.07 0.92 33.14 0.93 [ -0.29, +0.43] 34.33 0.936 01 ?
06 32.86 0.92 33.53 0.93 [+0.26, +1.08] 34.43 0.937 01 ?
07 33.08 0.93 33.51 0.93 [+0.04, +0.82] 34.49 0.938 01 ?
08 33.12 0.93 33.47 0.93 [ -0.06, +0.75] 34.50 0.949 01 ?
09 33.15 0.93 33.22 0.93 [ -0.35, +0.51] 34.68 0.9310 01 ?
10 33.01 0.93 33.59 0.94 [+0.18, +0.96] 34.79 0.9411 01 ?
11 32.84 0.94 33.40 0.94 [+0.13, +0.98] 34.76 0.9412 01 ?
12 32.73 0.93 33.49 0.94 [+0.34, +1.18] 34.83 0.9413 01 ?
13 32.71 0.93 33.54 0.94 [+0.39, +1.26] 34.91 0.9414 01 ?
14 32.66 0.93 33.69 0.94 [+0.58, +1.47] 34.97 0.9415 01 ?
15 32.47 0.93 33.57 0.94 [+0.63, +1.57] 34.99 0.9416 01 ?
16 32.51 0.93 33.62 0.94 [+0.62, +1.59] 35.00 0.943.2 Non-Uniform System Prior WeightsAs pointed out in Section 2.1, a useful propertyof the MBR-like system selection method is thatsystem prior weights can easily be trained usingthe Minimum Error Rate Training (Och, 2003).In this section, we investigate the effect of usingnon-uniform system weights for the combination ofmultiple research systems.
Since for each researchsystem, only the first best translation candidatewas provided, we used a five-fold cross validationscheme in order to train and evaluate the systemprior weights.
For this purpose, all research systemswere consistently split into five random partitions ofalmost equal size.
The partitioning procedure wasdocument preserving, i.e., sentences belonging tothe same document were guaranteed to be assignedto the same partition.
Each of the five partitionsplayed once the role of the evaluation set whilethe other four partitions were used as developmentdata to train the system prior weights.
Consensussystems were computed for each held out set usingthe system prior weights estimated on the respec-tive development sets.
The combination resultsdetermined on all held out sets were then concate-nated and evaluated with respect to the ZHEN-06reference translations.
Table 3 shows the resultsfor the combinations of up to 16 research systemsusing either uniform or trained system prior weights.System 01 achieved the highest BLEU score on allfive constellations of development partitions and istherefore the primary system to which all results inTable 3 compare.
In comparison to uniform weights,consensus translations using trained weights aremore robust toward the integration of low perform-ing systems into the combination scheme.
Thebest combined system obtained with trained systemprior weights (01-14) is, however, not significantlybetter than the best combined system using uniformweights (01-04), for which the 95% confidenceinterval yields r0.17, 0.66s according to the pairedbootstrap re-sampling method.Table 3 also shows the theoretically achievableBLEU scores when optimizing the system priorweights on the held out data.
This provides an upperbound to what extent system combination mightbenefit if an ideal set of system prior weights wereused.3.3 Effect of Correlation on SystemCombinationThe degree of correlation among input translationsystems is a key factor which decides whethertranslation outputs can be combined such a way thatthe overall system performance improves.
Correla-tion can be considered as a reciprocal measure ofdiversity: if the correlation is too large (?
90%),there will be insufficient diversity among the inputsystems and the consensus system will at most beable to only marginally outperform the best indi-991Table 4: BLEU scores obtained on ZHEN-05 with uniform prior weights and a 10-way system combinationusing the MBR-like candidate selection rule, word sausages, and the two-pass search algorithm togetherwith their improved versions ?sausages+?
and ?two-pass+?, respectively for different sample sizes of theFBIS training corpus.sampling primary mbr-like sausages sausages+ two-pass two-pass+r%s BLEU CI 95% BP BLEU BP BLEU BP BLEU BP BLEU BP BLEU BP5 27.82 (0.65) 1.00 29.51 1.00 29.00 0.97 30.25 0.99 29.58 0.94 29.93 0.9610 29.70 (0.69) 1.00 31.42 1.00 30.74 0.98 31.99 0.99 31.30 0.95 31.75 0.9720 31.37 (0.69) 1.00 32.56 1.00 32.64 1.00 33.17 0.99 32.60 0.96 32.76 0.9840 32.66 (0.66) 1.00 33.52 1.00 33.23 0.99 33.98 1.00 33.65 0.97 33.88 0.9980 33.67 (0.66) 1.00 34.17 1.00 33.93 0.99 34.38 1.00 34.20 0.99 34.35 1.00100 33.90 (0.67) 1.00 34.03 1.00 33.98 1.00 34.02 1.00 33.90 1.00 34.08 1.00vidual translation system.
If the correlation is toolow (?
5%), there might be no consensus among theinput systems and the quality of the consensus trans-lations will hardly differ from a random selection ofthe candidates.To study how correlation affects performance insystem combination, we built a large number ofsystems trained on randomly sampled portions of theFBIS 4 training data collection.
Sample sizes rangedbetween 5% and 100% with each larger data set dou-bling the size of the next smaller collection.
For eachsample size, we created 10 data sets, thus resulting ina total of 610 training corpora.
On each data set, anew translation system was trained from scratch and4 LDC catalog number: LDC2003E142728293031323334350  10  20  30  40  50  60  70  80  90  100BLEU[%]sampling [%]consensus system: 109876543primary system:  1Figure 1: Incremental system combination onZHEN-05 using the MBR-like candidate selectionrule and uniform prior weights.
Systems weretrained with different sample sizes of the FBIS data.used for decoding the ZHEN-05 test sentences.
All60 systems applied the MBR decision rule (Kumarand Byrne, 2004), which gave an additional 0.5%gain on average on top of using the maximum a-posteriori (MAP) decision rule.
Systems trained onequally amounts of training data were incrementallycombined.
Figure 1 shows the evolution of theBLEU scores as a function of the number of sys-tems as the sample size is increased from 5?100%.Table 4 shows the BLEU scores obtained with a 10-way system combination using the MBR-like can-didate selection rule, word sausages, and the two-pass search algorithm together with their improvedversions ?sausages+?
and ?two-pass+?, respectively.In order to measure the correlation between the in-dividual translation systems, we computed the inter-system BLEU score matrix as shown exemplary0  10  20  30  40  50  60  70  80  90  1003540455055606570758085correlation [%]sampling [%]consensusFigure 2: Evolution of the correlation on ZHEN-05averaged over 10 systems in the course of the samplesize.992Table 5: Minimum, maximum, and average inter-system BLEU score correlations for (i) the primarysystems of the 2006 NIST machine translation evaluation on the ZHEN-06 test data, (ii) different trainingcorpus sizes (FBIS), and (iii) a greedy strategy which chooses 15 systems out of a pool of 200 translationsystems.ZHEN-6 ZHEN-5 ZHEN-5 ZHEN-616 primary FBIS sampling, 10 systems 15 systems 15 systemssystems 5% 10% 20% 40% 80% 100% greedy selection ZHEN-5 selectionmin 0.08 0.38 0.44 0.47 0.53 0.60 0.72 0.55 0.50mean 0.18 0.40 0.45 0.50 0.56 0.66 0.79 0.65 0.61median 0.19 0.40 0.45 0.49 0.56 0.64 0.78 0.63 0.58max 0.28 0.42 0.47 0.53 0.58 0.70 0.88 0.85 0.83in Table 6 for the 16 MT06 primary submissions.Figure 2 shows the evolution of the correlationaveraged over 10 systems as the sample size isincreased from 5?100%.
Note that all systems wereoptimized using a non-deterministic implementationof the Minimum Error Rate Training described in(Och, 2003).
Hence, using all of the FBIS corpusdata does not necessarily result in fully correlatedsystems, since the training procedure may pick adifferent solution for same training data in orderto increase diversity.
Both Table 4 and Figure 1clearly indicate that increasing the correlation (andthus reducing the diversity) substantially reduces thepotential of a consensus system to outperform theprimary translation system.
Ideally, the correlationshould not be larger than 30%.Especially for low inter-system correlations andreduced translation quality, both the enhanced ver-sions of the word sausage combination methodand the two-pass search outperform the MBR-likecandidate selection scheme.
This advantage, how-ever, diminishes as soon as the correlation increasesand translations produced by the individual systemsbecome more similar.3.4 Toward Automatic System Generation andSelectionSampling the training data is an effective meansto investigate the effect of system correlation onconsensus performance.
However, this is done at theexpense of the overall system quality.
What we needinstead is a method to reduce correlation withoutsacrificing system performance.A simple, though computationally very expensiveway to build an ensemble of low-correlated sta-tistical machine translation systems from a singletranslation engine is to train a large pool of sys-tems, in which each of the systems is trained witha slightly different set of parameters.
Changingonly few parameters at a time typically results inonly small changes in system performance but mayhave a strong impact on system correlation.
Inour experiments we observed that changing pa-rameters which affect the training procedure at avery early stage, are most effective and introducelarger diversity.
For instance, changing the trainingprocedure for word alignment models turned out tobe most beneficial; for details see (Och and Ney,2003).
Other parameters that were changed includethe maximum jump width in word re-ordering, thechoice of feature function weights for the log-lineartranslation models, and the set of language modelsused in decoding.Once a large pool of translation systems hasbeen generated, we need a method to select asmall ensemble of diverse translation outputs thatare beneficial for computing consensus translations.Here, we used a greedy strategy to rank the systemswith respect to their ability to improve systemTable 6: Inter-system BLEU score matrix forprimary systems of the NIST 2006 TIDES machinetranslation evaluation on the ZHEN-06 test data.Id 01 02 03 04 05    14 15 1601 1.00 0.27 0.26 0.23 0.26    0.15 0.15 0.1202 0.27 1.00 0.27 0.22 0.25    0.15 0.15 0.1203 0.26 0.27 1.00 0.21 0.28    0.15 0.15 0.1004 0.23 0.22 0.21 1.00 0.19    0.14 0.12 0.1205 0.26 0.25 0.28 0.19 1.00    0.16 0.17 0.1106 0.27 0.24 0.25 0.21 0.26    0.16 0.18 0.13.........14 0.15 0.15 0.15 0.14 0.16    1.00 0.12 0.0815 0.15 0.15 0.15 0.12 0.17    0.12 1.00 0.0916 0.12 0.12 0.10 0.12 0.11    0.08 0.09 1.0099337.737.837.93838.138.238.338.45  10  15  20  25  30  35  40BLEU[%]number of systemsZHEN-5: consensus systemprimary system5  10  15  20  25  30  35  4031.231.431.631.83232.232.432.6BLEU[%]number of systemsZHEN-6: oracle selectionconsensus systemprimary systemFigure 3: BLEU score of the consensus translation as a function of the number of systems on the ZHEN-05sentences (left) and ZHEN-06 sentences (right).
The middle curve (right) shows the variation of the BLEUscore on the ZHEN-06 data when the greedy selection of the ZHEN-05 is used.combination.
Initially, the greedy strategy selectedthe best individual system and then continued byadding those systems to the ensemble, which gavethe highest gain in terms of BLEU score accordingto the MBR-like system combination method.
Notethat the greedy strategy is not guaranteed to increasethe BLEU score of the combined system when anew system is added to the ensemble of translationsystems.In a first experiment, we trained approximately200 systems using different parameter settings intraining.
Each system was then used to decode boththe ZHEN-05 and the ZHEN-06 test sentences usingthe MBR decision rule.
The upper curve in Figure 3(left) shows the evolution of the BLEU score onthe ZHEN-05 sentences in the course of the numberof selected systems.
The upper curve in Figure 3(right) shows the BLEU score of the consensustranslation as a function of the number of systemswhen the selection is done on the ZHEN-06 set.
Thisserves as an oracle.
The middle curve (right) showsthe function of the BLEU score when the systemselection made on the ZHEN-05 set is used in orderto combine the translation outputs for the ZHEN-06data.
Although system combination gave moderateimprovements on top of the primary system, thegreedy strategy still needs further refinements in or-der to improve generalization.
While the correlationstatistics shown in Table 5 indicate that changing thetraining parameters helps to substantially decreasesystem correlation, there is still need for additionalmethods in order to reduce the level of inter-systemBLEU scores such that they fall within the range ofr0.2, 0.3s.4 ConclusionsIn this paper, we presented an empirical studyon how different selections of translation outputsaffect translation quality in system combination.Composite translations were computed using (i) acandidate selection method based on inter-systemBLEU score matrices, (ii) an enhanced version ofword sausage networks, and (iii) a novel two-passsearch algorithm which determines and re-ordersbags of words that build the constituents of the finalconsensus hypothesis.
All methods gave statisticallysignificant improvements.We showed that both a high diversity among theoriginal translation systems and a similar translationquality among the translation systems are essentialin order to gain substantial improvements on top ofthe best individual translation systems.Experiments were conducted on the NIST portionof the Chinese English text translation corpus usedfor the 2006 NIST machine translation evaluation.Combined systems were built from primary systemsof up to 16 different research labs as well as systemsderived from one of the best-ranked translationengines.We trained a large pool of translation systemsfrom a single translation engine and presented firstexperimental results for a greedy search to select anensemble of translation systems for system combi-nation.994ReferencesS.
Banerjee and A. Lavie.
2005.
METEOR: AnAutomatic Metric for MT Evaluation with ImprovedCorrelation with Human Judgments.
In Proceedingsof Workshop on Intrinsic and Extrinsic EvaluationMeasures for MT and/or Summarization, 43th AnnualMeeting of the Association of Computational Linguis-tics (ACL-2005), Ann Arbor, MI, USA, June.S.
Bangalore, G. Bodel, and G. Riccardi.
2001.
Com-puting Consensus Translation from Multiple MachineTranslation Systems.
In 2001 Automatic SpeechRecognition and Understanding (ASRU) Workshop,Madonna di Campiglio, Trento, Italy, December.T.
Brants, A. Popat, P. Xu, F. Och, and J.
Dean.
2007.Large Language Models in Machine Tranlation.
InProceedings of the 2007 Conference on EmpiricalMethods in Natural Language Processing, Prague,Czech Republic.
Association for Computational Lin-guistics.J.
G. Fiscus.
1997.
A Post-Processing System to YieldReduced Word Error Rates: Recognizer Output VotingError Reduction (ROVER).
In Proceedings 1997IEEE Workshop on Automatic Speech Recognition andUnderstanding, pages 347?352, Santa Barbara, CA,USA, December.S.
Jayaraman and A. Lavie.
2005.
Multi-Engine Ma-chine Translation Guided by Explicit Word Matching.In 10th Conference of the European Association forMachine Translation (EAMT), pages 143?152, Bu-dapest, Hungary.P.
Koehn, F. J. Och, and D. Marcu.
2003.
StatisticalPhrase-Based Translation.
In NAACL ?03: Proceed-ings of the 2003 Conference of the North AmericanChapter of the Association for Computational Linguis-tics on Human Language Technology, pages 48?54,Edmonton, Canada.
Association for ComputationalLinguistics.P.
Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proceedings ofthe 2004 Conference on Empirical Methods in Natu-ral Language Processing, pages 388?395, Barcelona,Spain, August.
Association for Computational Lin-guistics.S.
Kumar and W. Byrne.
2004.
Minimum Bayes-Risk Decoding for Statistical Machine Translation.In Proc.
HLT-NAACL, pages 196?176, Boston, MA,USA, May.E.
Matusov, N. Ueffing, and H. Ney.
2006.
ComputingConsensus Translation from Multiple Machine Trans-lation Systems Using Enhanced Hypotheses Align-ment.
In 11th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL), pages 33?40, Trento, Italy, April.T.
Nomoto.
2004.
Multi-Engine Machine Translationwith Voted Language Model.
In Proceedings of the42nd Meeting of the Association for ComputationalLinguistics (ACL?04), Main Volume, pages 494?501,Barcelona, Spain, July.E.
W. Noreen.
1989.
Computer-Intensive Methods forTesting Hypotheses.
John Wiley & Sons, Canada.F.
J. Och and H. Ney.
2003.
A Systematic Comparison ofVarious Statistical Alignment Models.
ComputationalLinguistics, 29(1):19?51.F.
J. Och.
2003.
Minimum Error Rate Training in Statis-tical Machine Translation.
In 41st Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 160?167, Sapporo, Japan, July.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2001.Bleu: a Method for Automatic Evaluation of MachineTranslation.
Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. WatsonResearch Center, Yorktown Heights, NY, USA.M.
Paul, T. Doi, Y. Hwang, K. Imamura, H. Okuma, andE.
Sumita.
2005.
Nobody is Perfect: ATR?s HybridApproach to Spoken Language Translation.
In Inter-national Workshop on Spoken Language Translation,pages 55?62, Pittsburgh, PA, USA, October.F.
Robert and S. Nirenburg.
1994.
Three Heads areBetter than One.
In Proceedings of the Fourth ACLConference on Applied Natural Language Processing,Stuttgart, Germany, October.K.
C. Sim, W. Byrne, M. Gales, H. Sahbi, and P.C.Woodland.
2007.
Consensus network decoding forstatistical machine translation system combination.
InIEEE Int.
Conf.
on Acoustics, Speech, and SignalProcessing, Honolulu, HI, USA, April.M.
Snover, B. J. Dorr, R. Schwartz, J. Makhoul, L. Micci-ulla, and R. Weischedel.
2006.
A Study of TranslationEdit Rate with Targeted Human Annotation.
InProceedings of Association for Machine Translation inthe Americas.995
