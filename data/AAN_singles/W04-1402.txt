Work-in-Progress  project report : CESTA - Machine Translation EvaluationCampaignWidad Mustafa El HadiIDIST / CERSATESUniversit?
de Lille 3Domaine universitairedu "Pont de Bois"rue du BarreauBP 14959653 Villeneuve d'AscqCedex - Francemustafa@univ-lille3.frMarianne DabbadieIDIST / CERSATESUniversit?
de Lille 3Domaine universitairedu "Pont de Bois"rue du BarreauBP 14959653 Villeneuve d'AscqCedex - Francedabbadie@univ-lille3.frIsma?l TimimiIDIST / CERSATESUniversit?
de Lille 3Domaine universitairedu "Pont de Bois"rue du BarreauBP 14959653 Villeneuve d'AscqCedex - Francetimimi@univ-lille3.frMartin RajmanLIAEcolePolytechniqueF?d?rale de LausanneB?t.
INRCH-1015 LausanneSwitzerlandmartin.rajman@epfl.chPhilippe LanglaisRALI / DIRO -Universit?
de Montr?alC.P.
6128,succursale Centre-villeMontr?al (Qu?bec) -Canada, H3C 3J7felipe@IRO.UMontreal.CAAntony HartleyUniversity of LeedsCentre for TranslationStudiesWoodhouse LaneLEEDS LS2 9JTUKa.hartley@leeds.ac.ukAndrei Popescu BelisUniversity of Geneva 40bvd du Pont d'Arve CH-1211 Geneva 4SwitzerlandAndrei.Popescu-Belis@issco.unige.chAbstractCESTA, the first European Campaigndedicated to MT Evaluation, is a projectlabelled by the French Technolangue action.CESTA provides an evaluation of sixcommercial and academic MT systems using aprotocol set by an international panel ofexperts.
CESTA aims at producing reusableresources and information about reliability ofthe metrics.
Two runs will be carried out: oneusing the system?s basic dictionary, anotherafter terminological adaptation.
Evaluationtask, test material, resources, evaluationmeasures, metrics, will be detailed in the fullpaper.
The protocol is the combination of acontrastive reference to: IBM ?BLEU?protocol (Papineni, K., S. Roukos, T. Wardand Z. Wei-Jing, 2001); ?BLANC?
protocolderived from (Hartley, Rajman, 2002).;?ROUGE?
protocol (Babych, Hartley, Atwell,2003).
The results of the campaign will bepublished in a final report and be the object oftwo intermediary and final workshops.1 Introduction1.1 CESTA and the Technolangue Action inFranceThis article is a collective paper written by theCESTA scientific committee that aims atpresenting the CESTA evaluation campaign, aproject labelled in 2002 by the French Ministry ofResearch and Education within the framework ofthe Technolangue call for projects and integratedto the EVALDA evaluation platform.
It reportswork in progress and therefore is the description ofan on-going campaign for which system results arenot yet available.In France, EVALDA is the new Evaluationplatform, a joint venture between the FrenchMinistry of Research and Technology and ELRA(European Language Resources and EvaluationAssociation, Paris, France).
Within the frameworkof this initiative eight evaluation projets are beingconducted:  ARCADE II: campagne d?
?valuationde l?alignement de corpus multilingues; CESART:campagne d'Evaluation de Syst?mesd?Acquisition de Ressources Terminologiques;CESTA : campagne d'Evaluation de Syst?mes deTraduction automatique; Easy: Evaluation desAnalyseurs Syntaxiques du fran?ais; CampagneEQueR, Evaluation en question-r?ponse;Campagne ESTER, Evaluation de transcriptionsd?
?missions radio; Campagne EvaSY, Evaluationen synth?se vocale; and Campagne MEDIA,Evaluation du dialogue hors et en contexte.Regarding evaluation, the objectives of theAction as Joseph Mariani pointed out in hispresentation at the LREC 2002 conference are to:?
Improve the present evaluationmethodologies?
Identify new (quantitative and qualitative)approaches for already evaluatedtechnologies:  socio-technical and psycho-cognitive aspects?
Identify protocols for new technologiesand applications?
Identification of language resourcesrelevant for evaluation (to promote thedevelopment of new linguistic resourcesfor those languages and domains wherethey do not exist yet, or only exist in aprototype stage, or exist but cannot bemade available to the interested users);The object of the CESTA campaign is twofold.It is on the one hand to provide an evaluation ofcommercial Machine Translation Systems and onthe other hand, to work collectively on the settingof a new reusable Machine Translation Evaluationprotocol that is both user oriented and accounts forthe necessity to use semantic metrics in order tomake available a high quality reusable machinetranslation protocol to system providers.1.2 Object of the campaignThe object of the CESTA campaign is toevaluate technologies together with metrics, i.e.
tocontribute to the setting of a state of the art withinthe field of Machine Translation systemsevaluation.1.3 CESTA user oriented protocolThe campaign will last three years, startingfrom January 2003.
A board of Europeanexperts are members of CESTA Scientificcommittee and have been working together inorder to determine the protocol to use for thecampaign.
Six systems are being evaluated.Five of these systems are commercial MTsystems and one is a prototype developed atthe university of Montreal by the RALIresearch centre.
Evaluation is carried out ontext rather than sentences.
Text approximatewidth will be 400 words.
Two runs will becarried out.
For industrial reasons, systemswill be made anonymous.2 State-of-the-art in the field of MachineTranslation evaluationIn 1966, the ALPAC report draws light on thelimits of Machine Translation systems.
In 1979,the Van Slype report presented a study dedicated toMachine Translation metrics.In 1992, the JEIDA campaign puts the user at thecenter of evaluator?s preoccupation.
JEIDAproposed to draw human measures on the basis ofthree questionnaires:?
One destined to users (containing ahundred questions)?
Other questionnaires are destined tosystem Machine translation systemseditors (three different questionnaires),?
And a set of other questionnaires reservedto Machine Translation systemsdevelopers.Scores are worked out on the background offourteen categories of questions.
From thesescores, graphs are produced according to theanswers obtained.
A comparison of differentgraphs for each systems is used as a basis forsystems classification.The first DARPA Machine Translationevaluation campaign (1992-1994) makes use ofhuman judgments.
It is a very expensive methodbut interesting however, as regards the reliabilityof the evaluation thus produced.
This campaign isbased on tests carried out from French, Spanishand Japanese as source languages and English as atarget language.
The measures used for each of thefollowing criteria are:?
Fidelity ?
a proximity distance is workedout between a source sentence and a targetsentence on a 1 to 5 scale.?
Intelligibility, that corresponds tolinguistic acceptability of a translation ismeasured on a 1 to 5 evaluation scale.?
Informativeness: the test is carried out onreading of the target text alone.
Aquestionnaire on text informative contentis displayed allowing to work out ameasure calculated on the basis of thepercentage of good answers provided insystem translation.In 1995, the OVUM report proposes to comparecommercial Machine Translation systems on thebasis of ten criteria.In 1996, the EAGLES report (EAGLES, 1999)sets new standards for Natural LanguageProcessing software evaluation on the backgroundof ISO 9126.Initiated in 1999, and coordinated by Pr AntonioZampolli, the ISLE project is divided into threeworking groups, one being a Machine Translationgroup.Starting from ISO 9126 standard (King, 1999b),the aim of the project is to produce two taxonomies(c.f.
section 3 of this article) and :?
One defining quality subcriteria with theaim of refining the six criteria defined byISO 9126 (i.e.
functionality, reliability,user-friendliness, efficiency, maintenanceportability)?
The second one specifying use contextsthat define the type of task induced the useof a by Machine Translation system, thetypes of users and input data.
Thistaxonomy uses contextual parameters toselect and order the quality criteria subjectto evaluation.
This taxonomy can beviewed and downloaded on the ISSCOwebsite at the following address :http://www.issco.unige.ch/projects/isle/femti/The second DARPA campaign (Papineni, K., S.Roukos, T. Ward and Z. Wei-Jing, 2001), makinguse of the IBM BLEU metric is mentioned in theCESTA protocol (c.f.
section 8.1 of this article).3 User-oriented evaluationsAn emerging evaluation methodology in NLPtechnology focuses on quality requirementsanalysis.
The needs and consequently thesatisfaction of end-users, and this will depend onthe tasks and expected results requirementdomains, which we have identified as diagnosticquality dimensions.
One of the most suitablemethods in this type of evaluation is the adequacyevaluation that aims at finding out whether asystem or product is adequate to someone?s needs(see Sparck-Jones & Gallier, 1996 and King, 1996among many others for a more detailed discussionof these issues).
This approach encouragescommunication between users and developers.The definition of the CESTA evaluationprotocol took into account the Framework forMT Evaluation in ISLE (FEMTI), availableonline.
FEMTI offers the possibility to defineevaluation requirements, then to select relevant'qualities', and the metrics commonly used toscore them (cf.
ISO/IEC 9126, 14598).
TheCESTA evaluation methodology is founded ona black box approach.CESTA evaluators considered a generic user,which is interested in general-purpose, ready-to-use translations, preferably using an off-the-shelf system.
In addition, CESTA aims atproducing reusable resources, and providinginformation about the reliability of the metrics(validation), while being cost-effective andfast.With these evaluation requirements in mind(FEMTI-1), it appears that the relevantqualities (FEMTI-2) are 'suitability', 'accuracy'and 'well-formedness'.
Automated metrics bestmeet the CESTA needs for reusability, amongwhich BLEU, X-score and D-score (chosen forinternal reasons).
Their validation requires thecomparison of their scores with recognisedhuman scores for the same qualities (e.g.,human assessment of fidelity or fluency).
'Efficiency', measured through post-editingtime, was also discussed.
For the evaluation,first a general-purpose dictionary could beused, then a domain-specific one.3.1 An approach based on use casesISO 14598 directives for evaluators put forth asa prequisite for systems development the detailedidentification of user needs that ought to bespecified through the use case document.Moreover, conducting a full evaluation processinvolves going through the establishment of anevaluation requirements document.
ISO 14598document specifies that quality requirementsshould be identified ?according to user needs,application area and experience, software integrityand experience, regulations, law, requiredstandards, etc.
?.The evaluation specification document is createdusing the Software Requirement Specifications(SRS) and the Use-Case document.
The CESTAprotocol relies on a use case that refers to atranslation need grounded on basic syntacticcorrectness and simple understanding of a text, asrequired by information watch tasks for example,and excludes making a direct use of the text forpost editing purposes.4 Two campaigns4.1 Specificities of the CESTA campaignTwo campaigns are being organised :The first campaign is organised using a system?sdefault dictionary.
After systems terminologicaladaptation a second campaign will be organised.Two studies previously carried out and presentedrespectively at the 2001 MT Summit (Mustafa ElHadi, Dabbadie, Timimi, 2001) and at the 2002LREC conference (Mustafa Mustafa El Hadi,Dabbadie, Timimi, 2002) allowed us to realise thegap in terms in terms of quality between resultsobtained on target text after terminologicalenrichment.4.2 First campaignThe organisation of the campaign implies goingthrough several steps :?
Identification of potential participants?
Original protocol readjustement,?
The setting of a specific test tool that iscurrently being be implemented inconformity with protocol specificationsvalidated by CESTA scientificcommittee.
CESTA protocolspecifications have beencommunicated to participants inparticular as regards data formatting,test schedule, metrics and adaptationphase.
For cost requirements, CESTAwill not include a training phase.
Thefirst run will start during autumn 20044.3 Second campaignThe systems having already been tuned, anadaptation phase will not be carried out for thesecond campaign.
However terminologicaladaptation will be necessary at this stage.
Thesecond series of tests being carried out on athematically homogeneous corpus, the thematicdomain only will be communicated to participantsfor terminological adaptation.
For thematicadaptation, and in order to avoid systemoptimisation after the first series of tests, a newdomain specific 200.000 word hiding corpus willbe used.The terminological domain on which evaluationwill be carried out will then have to be defined.This terminological domain will be communicatedto participants but not the corpus used itself.
Onthe other hand, participants will be asked to sendorganisers a written agreement by which they willcommit themselves to provide organisers with anyrelevant information regarding system tuning andspecific adaptations that have made on each of theparticipating MT systems, in order to allow thescientific committee to understand and analyse theorigin of the potential system ranking changes.
Thesecond run will start during year 2005.Organisers have committed themselves not topublish the results between the two campaigns.After the training phase, the second campaignwill take place.
Participants will be given a fifteendays delay to send the results.
An additional threemonths period will be necessary to carry out resultanalysis and prepare data publication andworkshop organisation.CESTA scientific committee also decided inparallel with the two campaigns, to evaluatesystems capacity to process formatted textsincluding images and HTML tags.
Participantswho do not wish to participate to this additionaltest have informed the scientific committee.
Mostof the time the reason is that their system is onlycapable of processing raw text.
This is the casemainly for academic systems involved in thecampaign, most of the commercial systems beingnowadays able to process formatted text.5 Contrastive evaluationOne of the particularities of the CESTA protocolis to provide a Meta evaluation of the automatedmetrics used for the campaign ?
a kind of state ofthe art of evaluation metrics.
The robustness of themetrics will be tested on minor language pairsthrough a contrastive evaluation against humanjudgement.The scientific committee has decided to useArabic?French as a minor language pair.Evaluation on the minor language pair will becarried directly on two of the participating systemsand using English as a pivotal language on theother systems.
Translation through a pivotallanguage will then be the following :Arabic?English?French.Organiser are, of course, perfectly aware of thepotential loss of quality provoked by the use of apivotal language but recall however that, contrarilyto the major language pair, evaluation carried outon the minor language pair through a pivotalsystem will not be used to evaluate these systemsthemselves, but metric robustness.
Results ofmetric evaluation and systems evaluation will, ofcourse, be obtained and disseminated separately.During the tests of the first campaign, theFrench?English system obtaining the best rankingwill be selected to be used as a pivotal system formetrics robustness Meta evaluation.6 Test materialThe required material is a set of corpora asdetailed in the following section and a test tool thatwill be implemented according to metricsrequirements and under the responsibility ofCESTA organisers.6.1 CorpusThe evaluation corpus is composed of 50 texts,each text length is 400 words to be translatedtwice, considering that a translation already existsin the original corpus.
The different corpora areprovided by ELRA.
The masking corpus has250.000 words and must be thematicallyhomogeneous.For each language pair the following corporawill be used:Adaptation?
This 200.000 ?
250.000 word corpus is abilingual corpus.
It is used to validateexchanges between organisers andparticipants and for system tuning.First Campaign?
One 20.000 word evaluation corpus will beused (50 texts of 400 words each)?
One 200.000 to 250.000 word maskingcorpus that hides the evaluation corpus.Second campaign?
One new 20.000 word corpus will be usedbut it will have to be thematicallyhomogeneous (on a specific domain thatwill be communicated to participants a fewmonths before the run takes place)?
One masking corpus similar to theprevious one.Additional requirementThe BLANC metric requires the use of abilingual aligned corpus at document scale.Three human translations will be used for eachof the evaluation source texts.
Considering that thecorpora used, already provide one officialtranslations, only two additional humantranslations will be necessary.
These translationswill be carried out under the organisersresponsibility.
Within the framework of CESTAuse cases, evaluation is not made in order to obtaina ready to publish target language translation, butrather to provide a foreign user a simple access toinformation within the limits of basic grammaticalcorrectness, as already mentioned in this article.7 The BLEU, BLANC and ROUGE metricsThree types of metrics will be tested on thecorpus, the CESTA protocol being the combinationof a contrastive reference to three differentprotocols:7.1 The IBM ?BLEU?
protocol (Papineni, K.,S.
Roukos, T. Ward and Z. Wei-Jing,2001).The IBM BLEU metric used by the DARPA forits 2001 evaluation campaign, uses co-occurrencemeasures based on N-Grams.
The translation inEnglish of 80 Chinese source documents by sixdifferent commercial Machine Translationsystems, was submitted to evaluation.
From areference corpus of translations made by experts,this metric works out quality measures accordingto a distance calculated between an automaticallyproduced translation and the reference translationcorpus based on shared N-grams (n=1,2,3?).
Theresults of this evaluation are then compared tohuman judgments.?
NIST now offers an online evaluation ofMT systems performance, i.e.
:o A program that can bedownloaded for research aims.The user then provides sourcetexts and reference translations fora determined pair of languages.o An e-mail evaluation service, formore formal evaluations.
Resultscan be obtained in a few minutes.7.2 The ?BLANC?
protocolIt is a metric derived from a study presented atthe LREC 2002 conference (Hartley A., RajmanM., 2002).
We only take into account a part of theprotocol described in the referred paper, i.e.
the Xscore, that corresponds to grammatical correctness.We will not give an exhaustive description ofthis experience and shall only detail the elementsthat are relevant to the CESTA evaluation protocol.The protocol has been tested on the followinglanguages.?
Source language: French?
Target language: English?
Source corpus : 100 texts ?
domain :newspaper articlesHuman judgements for comparison referential:?
12 English monolingual students.?
No human translation reference corpus.?
Three criteria were tested: Fluency,Adequacy, InformativenessSix systems were submitted to evaluation :Candide (CD), Globalink (GL), MetalSystem(MS), Reverso (RV), Systran (SY), XS (XS)?
Each of the systems is due to translate ahundred source texts ranging from 250 to300 words each.
A corpus of 600translations is thus produced.?
For each of the source texts, a corpus of 6translations is produced automatically.These translations are then regrouped byseries of six texts.?
According to the protocol initiated by(White & Forner, 2001) these series arethen ranked by medium adequacy score.?
Every 5 series, a series is extracted fromthe whole.
Packs of twenty series of targettranslations are thus obtained andsubmitted to human evaluators.7.2.1 Evaluators?
tasks?
Each evaluator reads 10 series of 6translations i.e.
60 texts.?
Each of these series is then read by sixdifferent evaluators?
The evaluators must observe a ten minutecompulsory break every two series.?
The evaluators do not know that the textshave been translated automatically.The directive given to them is the following:?
rank these six texts from best to worst.
Ifyou cannot manage to give a different rankingto two texts, regroup them under the sameparenthesis and give them the same score, as inthe following example : 4 [1 2] 6 [3 5].
?The aim of this instruction is to producerankings that are similar to the rankings attributedautomatically.Human judgement that ranks from best to worsecorresponds in reality to a set of the fluency,adequacy and Informativeness criteria that can beattributed to the texts translated automatically.7.2.2 Automatically generated scores?
X-score : syntactic score?
D-score : semantic scoreWithin the framework of the CESTAevaluation campaign the scientific committeedecided to make use of the X-score only, thesemantic D-score having proved to be unstableand that it could be advantageously replacedby the a metric based on (Bogdan, B.; Hartley,A.
; Atwell, 2003), a reformulation of the D-score developed by (Rajman, M. and T.Hartley, 2001), and which we refer to as theROUGE metric in this article.7.2.3 X-score: definition?
This score corresponds to a grammaticalitymetric?
Each of the texts is previously parsed withXELDA Xerox parser.?
22 types of syntactic dependenciesidentified through the corpus of automatictranslations.?
The syntactic profile of each sourcedocument is computed.
This profile is thenused to derive the X-score for eachdocument, making use of the followingformula:?
X-score = (#RELSUBJ+#RELSUBJPASS-#PADJ-#ADVADJ)7.3 The ?ROUGE?
protocolThis protocol, developed by Anthony Hartley in(Bogdan, B.; Hartley, A.; Atwell, 2003), is asemantic score.
It is the result of a reformulation ofthe D-Score, the semantic score initiated throughprevious collaboration with Martin Rajman(Rajman, M. and T. Hartley, 2001), as explained inthe previous section.The original idea on which this protocol is basedrelies on the fact that MT evaluation metrics that?are based on comparing the distribution ofstatistically significant words in corpora of MToutput and in human reference translationcorpora?.The method used to measure MT quality is thefollowing:  a statistical model for MT outputcorpora and for a parallel corpus of humantranslations, each statistically significant wordbeing highlighted in the corpus.
On the other hand,a statistical significance score is given for eachhighlighted word.
Then statistical models for MTtarget texts and human translations are compared,special attention being paid to words that areautomatically marked as significant in MT outputs,whereas they do not appear to be marked assignificant in human translations.
These words areconsidered to be ?over generated?.
The sameoperation is then carried out on ?under generatedwords?.
At this stage, a third operation consists inthe marking of the words equally marked assignificant by the MT systems and the humantranslations.
The overall difference is thencalculated for each pair of texts in the corpora.Three measures specifying differences in statisticalmodels for MT and human translations are thenimplemented : the first one aiming at avoiding?over generation?, the second one aiming atavoiding ?under generation?
and the last one beinga combination of these two measures.
The averagescores for each of the MT systems are thencomputed.As detailed in (Bogdan, B.; Hartley, A.; Atwell,2003):?1.
The score of statistical significance iscomputed for each word (with absolute frequency?
2 in the particular text) for each text in thecorpus, as follows: ( )][][][][][ lncorpallwordfoundnottxtswordcorprestwordtextwordtextword PNPPS????
?
?=where:Sword[text] is the score of statistical significance fora particular word in a particular textPword[text] is the relative frequency of the word inthe text;Pword[rest-corp] is the relative frequency of the sameword in the rest of the corpus, without this text;Nword[txt-not-found] is the proportion of texts in thecorpus, where this word is not found (number oftexts, where it is not found divided by number oftexts in the corpus)Pword[all-corp] is the relative frequency of the wordin the whole corpus, including this particular text2.
In the second stage, the lists of statisticallysignificant words for corresponding texts togetherwith their Sword[text] scores are compared acrossdifferent MT systems.
Comparison is done in thefollowing way:For all words which are present in lists ofstatistically significant words both in the humanreference translation and in the MT output, wecompute the sum of changes of their Sword[text]scores: ( )?
?= ].[].[.
MTtextwordreferencetextworddifftext SSSThe score Stext.diff is added to the scores of all"over-generated" words (words that do not appearin the list of statistically significant words forhuman reference translation, but are present insuch list for MT output).
The resulting scorebecomes the general "over-generation" score forthis particular text:?
??
+=textwordstextgeneratedoverworddifftexttextgenerationover SSS.
][...The opposite "under-generation" score foreach text in the corpus is computed by addingStext.dif and all Sword[text]  scores of "under-generated"words ?
words present in the human referencetranslation, but absent from the MT output.
?+=?textwordstextatedundergenerworddifftexttextgenerationunder SSS.
][...It is more convenient to use inverted scores,which increases as the MT system improves.
Thesescores, So.text and Su.text, could be interpreted asscores for ability to avoid "over-generation" and"under-generation" of statistically significantwords.
The combined (o&u) score is computedsimilarly to the F-measure, where Precision andRecall are equally important:textgenerationovertexto SS..1?= ;textgenerationundertextu SS..1?= ;textutextotextutextotextuo SSSSS.....&2+=The number of statistically significant wordscould be different in each text, so in order to makethe scores compatible across texts we compute theaverage over-generation and under-generationscores per each statistically significant word in agiven text.
For the otext score we divide So.text by thenumber of statistically significant words in the MTtext, for the utext score we divide Su.text by thenumber of statistically significant words in thehuman (reference) translation:rdsInMTstatSignWotextotext nSo .= ;rdsInHTstatSignWotextutext nSu .= ;texttexttexttexttext uouoou +=2&The general performance of an MT system for IEtasks could be characterised by the average o-score, u-score and u&o-score for all texts in thecorpus?.8 Time Schedule and result disseminationThe CESTA evaluation campaign started inJanuary 2003 after having been labeled by theFrench Ministry of Research.
During year 2003CESTA scientific committee went throughprotocol detailed redefinition and specification anda time schedule was agreed upon.2004 first semester is being dedicated to corpusuntagging and the programming of CESTAevaluation tool.
Reference human translations willalso have to be produced and the implementedevaluation tool submitted to trial and validation.After this preliminary work, the first run willstart during autumn 2004.
At the end of the firstcampaign, result analysis will be carried out.
Aworkshop will then be organized for CESTAparticipants.
Then the second campaign will takeplace at the end of Spring 2005, the terminologicaladaptation phase being scheduled on a five monthscale.After carrying out result analysis and final reportredaction, a public workshop will be organized andthe results disseminated and subject to publicationat the end of 2005.9 ConclusionCESTA is the first European Campaigndedicated to MT Evaluation.
The results of thecampaign will be published in a final report and bethe object of an intermediary workshop betweenthe two campaigns and a final workshop at the endof the campaign.It is a noticeable point that the CESTA campaignaims at providing a state of the art of automatedmetrics in order to ensure protocol reusability.
Theoriginality of the CESTA protocol lies in thecombination and contrastive use of three differenttypes of measures carried out in parallel with aMeta evaluation of the metrics.It is also important to note that CESTA aims atproviding a black box evaluation of availableMachine Translation technologies, rather than acomparison of systems and interfaces, that can betuned to match a particular need.
If systems had tobe compared, the fact that these applicationsshould be compared including all software lawyersand ergonomic properties, ought to be taken intoconsideration.Moreover apart from providing a state of the artthrough a Meta evaluation of the metrics used in itsprotocol, thanks to the setting of this originalprotocol that relies on the contrastive use ofcomplementary metrics, CESTA aims at protocolreusability.
One of the outputs of the campaignwill be the creation of a Machine Translationevaluation toolkit that will be put at users andsystem developers?
disposal.AcknowledgementsReferencesBesan?on, R. and Rajman, M., (2002).
Evaluationof aVector Space similarity measure in amultilingual framework.
Procs.
3rdInternational Conference on LanguageResources and Evaluation, LasPalmas,Spain,.1252Bogdan, B.; Hartley, A.; Atwell E.; Statisticalmodelling of MT output corpora forInformation Extraction Proceedings CorpusLinguistics 2003, Lancaster, UK, 28-31March 2003, pp.
62-70Chaudiron, S. Technolangue.
In:http://www.apil.asso.fr/metil.htm, mars 2001Chaudiron, S.
L?
?valuation des syst?mes detraitement de l?information textuelle : vers unchangement de paradigmes, M?moire pourl?habilitation ?
diriger des recherches en sciencesde l?information, pr?sent?
devant l?Universit?
deParis 10, Paris, novembre 2001Dabbadie, M, Mustafa El Hadi, W., Timimi, I.(2001).
Setting a Methodology for MachineTranslation Evaluation.
In: MachineTranslation Summit VIII, ISLE/EMTA,Santiago de Compestela, Spain, 18-23October 2001, pp.
49-54.Dabbadie, M., Mustafa El Hadi, W., Timimi, I.,(2002).
Terminological Enrichment for non-Interactive MT Evaluation.
In: LREC 2002Proceedings ?
Las Palmas de Gran Canaria,Spain ?
29th ?
31st May 2002 ?
vol 6 ?
1878-1884EAGLES-Evaluation-Workgroup.
(1996).EAGLES evaluation of natural languageprocessing systems.
Final report, Center forSprogteknologi, Denmark, October 1996.EAGLES (1999).
EAGLES Reports (ExpertAdvisory Group on Language EngineeringStandards)http://www.issco.unige.ch/projects/eagles/ewg99.ISLE (2001).
MT Evaluation Classification,Expanded Classification.http://www.isi.edu/natural-language/mteval/2b-MT-classification.htm.ISO/IEC-9126.
1991.
ISO/IEC 9126:1991 (E) ?Information Technology ?
SoftwareProduct Evaluation ?
QualityCharacteristics and Guidelines for Their Use.ISO/IEC, Geneva.ISO (1999).
Standard ISO/IEC 9126-1 InformationTechnology ?
Software Engineering ?Quality characteristics and sub-characteristics.
Software QualityCharacteristics and Metrics - Part 1ISO (1999).
Standard ISO/IEC 9126-2 InformationTechnology ?
Software Engineering ?Software products Quality : External Metrics- Part 2ISO/IEC-14598.
1998-2001.
ISO/IEC 14598 ?Information technology ?
Software productevaluation ?
Part 1: General overview(1999), Part 2: Planning and management(2000), Part 3: Process for developers(2000), Part 4: Process for acquirers (1999),Part 5: Process for evaluators (1998), Part 6:Documentation of evaluation modules(2001).
ISO/IEC, Geneva.ISSCO (2001) Machine Translation Evaluation :An Invitation to Get Your Hands Dirty!,ISSCO, University of Geneva, Workshoporganised by M. King (ISSCO) & F. Reed,(Mitre Corporation), April 19-24 2001.King (1999a) EAGLES Evaluation WorkingGroup, report,http://www.issco.unige.ch/projects/eagles.King, M. (1999b).
?ISO Standards as a Point ofDeparture for EAGLES Work in EELSConference (European Evaluation ofLanguage Systems), 12-13 April 1999.Mariani, Joseph.
?Language Technologies :Technolangue Action ?.
Presentation.
In:LREC'2002 International Strategy Panel17, LasPalmas, May 2002.Nomura, H. and J. Isahara.
(1992).
The JEIDAreport on MT.
In Workshop on MTEvaluation: Basis for Future Directions, SanDiego, CA.
Association for MachineTranslation in the Americas (AMTA).Popescu-Belis, A. S. Manzi, and M. King.
(2001).Towards a two-stage taxonomy for MTevaluation.
In Workshop on MT Evaluation?Who did what to whom??
at Mt SummitVIII, pages 1?8, Santiago de Compostela,Spain.Rajman, M. and T. Hartley, (2001).
Automaticallypredicting MT systems rankings compatiblewith Fluency, Adequacy or Informativenessscores.
Procs.
4th ISLE Workshop on MTEvaluation, MT Summit VIII, 29-34.Rajman, M. and T. Hartley, (2002).
Automaticranking of MT systems.
In: LREC 2002Proceedings ?
Las Palmas de Gran Canaria,Spain ?
29th ?
31st May 2002 ?
vol 4 ?
1247-1253Reeder, F., K. Miller, J. Doyon, and J.
White, J.(2001).
The naming of things and theconfusion of tongues: an MT metric.
Procs.4th ISLE Workshop on MT Evaluation, MTSummit VIII, 55-59.Sparck-Jones K., Gallier, J.R. (1996).
EvaluatingNatural Language Processing Systems: AnAnalysis and Review, Springer, Berlin.TREC, NIST Website, last updated, August 1st,2000, visited by the authors, 23-03-2003Vanni, M. and K. Miller (2001).
Scaling the ISLEframework: validating tests of machinetranslation quality for multi-dimensionalmeasurement.
Procs.
4th ISLE Workshop onMT Evaluation, MT Summit VIII, 21-27.VanSlype., G. (1979).
Critical study of methodsfor evaluating the quality of MT.
TechnicalReport BR 19142, European Commission /Directorate for General Scientific andTechnical Information Management (DGXIII).V?ronis, J., Langlais, Ph.
(2000).
ARCADE:?valuation de syst?mes d'alignement de textesmultilingues.
In Chibout, K., Mariani, J.,Masson, N., Neel, F.
?ds., (2000).
Ressources et?valuation en ing?nierie de la langue, Duculot,Coll.
Champs linguistiques, et CollectionUniversit?s Francophones (AUF).
