Sense Tagging in ActionCombining Different Tests with Additive WeightingsAndrew Harley & Dominic GlennonCambridge Language Services Ltd64 Baldock StreetWareHerts SG12 9DTEnglandandrew @ oaldeaf.demon.co.nkAbstractThis paper describes a working sensetagger, which attempts to automaticallylink each word in a text corpus to itscorresponding sense in a machine-readable dictionary.
It uses informationautomatically extracted from the MRD tofind matches between the dictionary andthe Corpus sentences, and combinesdifferent ypes of information by simpleadditive scores with manually setweightings.1.
IntroductionThis paper describes a working sense tagger, whichattempts to automatically ink each word in a textcorpus to its corresponding sub-sense in the CambridgeInternational Dictionary of English (CIDE).
Muchresearch elsewhere has gone into the generation ofprobabilities from corpora and the extraction of textualinformation from printed dictionaries.
Our research ashad the distinct advantage of being done alongside alarge lexicographic team, who have been developingfurther the database used for the creation of CIDE.
Ithas thus been possible to have very usefulcomputational data expertly coded by hand.
We havebeen able to concentrate on defining the specification ofthis lexical resource, encoding it and then making useof it, rather than on trying to extract or refine thedesired information automatically from existingcorpora or printed ictionaries.2.
Methodo logyThe tagger, at present, works on one sentence at a time.Each word in the sentence has a certain number ofpossible senses.
The tagger assigns a score (initially 0)to each possible sense of each word.
A number ofdifferent tagging process could then adjust any of thesescores, increasing them for a positive match (e.g.
acollocation that indicates aparticular sense), decreasingthem for a negative match (e.g.
capitalisationindicating aparticular sense to be unlikely).
At the endof all these processes, each sense of each word willhave a particular score.
For each word, the sense withthe highest score is assumed to be the sense meant inthe context.Simple additive weightings are also commonly usedin the evaluation of chess positions by computers,where for example, a pawn less could score -100 and anopen file for a rook +15.
It is thus possible for a numberof positional factors to outweigh more concrete materialfactors.It would be possible to use multiplicativeprobabilities rather than additive weightings.
Chessprogrammers tend to prefer additive weightingsbecause they ate far simpler to program and also moreefficient.
There are more rigorous rules for combiningprobabilities, but it is not clear how much benefit hisgives if the original probabilities are only roughestimates anyway.
Probabilities can be derived fromtraining corpora, but it is acknowledged that these canvary enormously from corpus to corpus, e.g.
on groundsof register (Biber 1993).
Such methods are far moreappropriate for work in restricted contexts, whererepresentative training corpora can be more easilyderived.3.
P roc J~ureBesides some simple tests for suffixes (for unknownwords), capitalisation, register and frequency, the maintagging processes are the following:3.1 Multi-word unit taggerThe CIDE database contains detailed information onboth single words and multi-word units.
For a wordpair X Y (e.g.
has been), the tagger is thus able toproduce possible scores for X and Y as separate words,and for X Y as a multi-word unit throughout eachnnminunnnuniEiiimi smnR\ [ \ ]I\ [ \ ]munininnuniin74tagging process.
If a multi-word unit is found, it isgiven an initial additional score (a headstart over thewords treated separately) proportional to the number ofwords in the unit minus 1, but this can easily becancelled out by other scores.As a learner dictionary, CIDE contains muchexamples text.
This examples text forms a convenienthand sense tagged corpus, though with only one word(the headword) sense tagged in each example.
Muchresearch has been devoted to using just collocationinformation for sense disambignation, even usingcontexts of as much as 50 words (Gale, Church andYarowsky, 1992).
We instead choose to look more atthe immediate context around a word, by dividingcollocation match weightings by the distance betweenthe pair of collocating words, expecting subject domaintagging (see section 3.2) to deal with more long-rangeeffects.3.2 Subject domain taggerEach entry in CIDE has been subject coded.
A subjectdomain for the sentence is created by looking at thesubject codes of each likely (from the tests so far) senseof every word in the sentence, and at any documentinformation available about he subject domain of thearticle, e.g.
a sports page.
Then the subject codes ofeach sense of each word are compared with the subjectdomain for the sentence and the number of matchesnoted.
The subject codes are arranged in a hierarchy, sofor example, Christmas and Passover would match atsome levels, despite not having exactly the same subjectcode.
Long sentences can distort the results, so theweightings awarded to subject domain matches aredivided by the number of words in the sentence.3.3 Part of speech taggerOur part-of-speech tagger is based on a series of rules,listing valid 'transition pair' sequences of grammaticaltags.
These pairs can be given weightings but theemphasis of the approach is on the list of valid pairsrather than the weightings assigned to each pair.
Thusmost valid pairs are given a standard weighting of 0.Six special intermediate tags have been created toreduce the number of tag pairs that need to be listedand to add 'partial parsing' to the process.
These are:p\[ and p\] around noun phrases acting as subjects (i.e.expecting to be followed by a verb)p< and p> around noun phrases acting as objectsp( and p) around adverbial or prepositional phrases,or sub-clausesThus, for example, a determiner may only bepreceded by p\[ or p< or a pre-determiner.
The p( and p)are a particularly powerful feature which enableintermediate phrases to be ignored.
The tagger does notcheck for p) followed by the next tag, but rather looksback to what came innnediately before the preceding p(and then does the transition pair match on that.
Atwell(1987) has termed these kind of brackets"hyperbrackets" and considers a very similar approachto that we are now adopting, choosing himself insteadto add hyperhrackets toalready tagged text to enhanceit with parsing information, but thereby losing thebenefit hese hyperhrackets can assign to the part-of-speech tagging process itself, One example of thepossible benefit is in trying to make the distinctionbetween a preposition, which is generally followed bywhat we term an object noun phrase as it will not befollowed by a verb, and a subordinating conjunction,which is generally followed by what we term a subjectnoun phrase as it will be followed by a verb.For a valid transition pair between two tags, thescore is simply calculated by adding the maximumscore (from the other tagging processes) for a sense thatcan have each grammatical tag to the transition pairweighting (usually 0).
There are also some specialfeatures to cope with more long-range ffects (e.g.singular nouns being followed by the 3ps form of thepresent simple, conjunctions tending to co-ordinate thesame grammatical tags).
Thus, all valid sequences canbe given a score by adding up the relevant transitionpair scores.Our method is more ambitious but intrinsically lessefficient than Hidden Markov Model .approaches,although certain restrictions are applied to reduce thenumber of sequences to a manageable size (e.g.
a limiton the number of nested brackets).
More time alsoneeds to be spent on rule development.3.4 Selectional preference pattern taggerThe selectional preference pattern tagger checks verbcomplementation a d selectional preferences, and alsoadjective selectional preferences.
Lexicographers havespecifically attached CIDE grammar cedes (which giveverb complementation patterns) to selectionaipreference patterns using a restricted list of about 40selectional classes for nouns.
The tagger translatesthese grammar codes into sequences of grammaticaltags and super-segmental gs representing the possiblesequences that may follow the verb, and then integratesthese with the selectional preference patterns.It is these resulting patterns that the pattern taggeruses to test the syntactic and semantic veracity of thetag sequences produced by the part-of-speech tagger.
If75\[\]Tagger eventPart of speech 'transition pair' not foundVerb complementation pattern failureCapitalisation failureMulti-word unit matchFrequencySelectional preference failure (for each argument)Register failureLexical collocate matchFunctional collocate matchIllustrative 4 collocate matchSubject domain match (for each level)Weightingrejected.801-60+50 times (words in unit - 1)0 to +502-403-30+30 per (distance between words)+20 per (distance between words)+10 per (distance between words)+30 per (words in sentence)the argument pattern (subject and objects) fail to matcha tag sequence, this is considered a verbcomplementation pattern failure.
When an argument isencountered, the class specified in the selectionaipreference pattern is matched against the possibleclasses for the word.
Selectional c asses are hierarchicalin structure like subject domain codes (see section 3.2),so allowance is made for near-matches.
Adjectiveselectional preferences are matched in a similar butmore simple way.
Each adjective is coded with thepossible clnss(es) of the nouns which it may modify.The adjective class is matched against he class of thenoun which it modifies using much the same scoringsystem as for the verbs.Selectionai preference pattern matching has provedone of the most useful of all tests.
A good example isthe sentence:The head asked the pupil a question.Here, the CIDE database gives the possibleselectionai classes for head as body part, state, object,human or device; for pupil as human or body part; forquestion as communication r abstract.The verb asked with two objects can only have thepattern human asked human communication.
Thus, allthe senses can be correctly assigned just by usingselectional preferences.3.5 RefinementThere are three main processes involved in refining thetagger's performance:* Refining the lexicographic data, or indeed addingwhole new categories of lexicographic data (e.g.selectional preference patterns).
* Writing new algorithms ("taggers").
* Analysing the interaction between different ests,and refining the weightings used for each.A hand-tagged corpus is of course very useful forperforming the third of these processes in a rigorousmanner.
The next stage of our research is to use the testcorpus (section 4) as a training corpus to fine-tune theweightings.
The main weightings currently in use,which may be of interest to other researchers trying tocombine different tests, are shown in the table.An example of how different taggers can interact isgiven by the following two sentences:He was fired with enthusiasm by his boss.He was fired by his boss with enthusiasm.The DISMISS sense of fired matches with boss at 3levels of subject domain coding, thus scoring 30*3/8 =11 for both sentences.The EXCITE sense of fired has with as a functionalcollocate and enthusiasm as an illustrative collocate inCIDE, and thus scores 20/1 + 10/2 = 25 for the fu-stsentence and 20/4 + 1015 = 7 for the second sentence.Thus, assuming no other taggers intervene, the sensetagger will make the best possible assignment for thesetwo, admittedly rather ambiguous, examples.4.
ResultsTo test the tagging, we compared the results against apreviously hand sense tagged corpus of 4000 words.1 a successful match scores +10 per argument matched2 certain common senses, like the determiner use of a,were given scores up to +1003 or -10 for each level mismatch in the selectionalpreference hierarchy4 used in a CIDE example but not emboldened aslexicographically significant76Each of the 4000 words was manually assigned withjust one sense tag and the tagging program likewiseassigned precisely one sense tag to each word.
Theresults are thus strictly determined by the number ofmatching taggings, with no ambiguous coding allowed.
(These criteria are somewhat over-strict as in somecases more than one tag could be considered acceptable,e.g.
where there are cross-references in the dictionaryor where there is genuine ambiguity.)
In calculating theresults, prepositions were deliberately ignored becausethey have been heavily "split" in CIDE, far more sothan in other dictionaries (L~ar 1996).
Any attempt atdistinguishing these senses would have to rely heavilyon selectional preferences for prepositions, which areyet to be implemented within the tagging program.At the sense (CIDE guideword) level, with anaverage 5 senses per word, the sense tagger was correct78% of the time.
At the sub-sense level, with anaverage 19 senses per word, the sense tagger wascorrect 73% of the time.The part of speech tagging was also tested on thesame texts to similarly strict criteria (i.e.
no ambiguouscoding allowed) and found to assign the correct part ofspeech 91% of the time.
Three other part of speechtaggers were run on the same texts for comparison.Two taggers developed from work done at CambridgeUniversity under the ACQUILEX programme assigned93% and 87% correctly, while the commercialProspero Parser performed best, assigning 94%correctly.5.
EvaluationThese results clearly need to be improved ramaticallybefore automatic sense tagging can prove practicallyuseful.
Nonetheless, these results, especially at sub-sense level, compare favourably with other research inthe area.Ng and Lee (1996) have found only 57% agreementwhen comparing the same texts tagged according to thesame dictionary senses by different (human!)
researchgroups.
Cowie, Guthrie and Guthrie (1992) havereported 72% correct assignment at the LDOCEhomograph level (and a much lower level for individualsense assignment).
Wilks, Slator and Guthrie (1996)comment hat 62% accuracy can be achieved at thislevel just by assigning the first (therefore mostfrequent) homograph in LDOCE.
Furthermore, Wilksand Stevenson (1996) propose a method which shouldapparently achieve 92% accuracy to that same level justby using grammatical tags.It must be noted however that the LDOCEhomograph level is far more rough-grained than theCIDE guideword level, let alne the sub-sense l vel,and that Wilks and Stevenson's approach on its ownwould, by its very nature, not transfer down to morefine-grained distinctions.
Other research, such asYarowsky's into accent restoration in Spanish andFrench (1994), which reports accuracy levels of 90%-99%, is again at a more rough-grained level, in thiscase that of distinguished unaccented and accentedword forms.While the sense tagging results are fairlyencouraging, the part of speech tagging results arc atpresent relatively poor.
It thus secrns sensible,especially noting Wilks and Stevenson's analysismentioned above, to first run a sentence through atraditional part of speech tagger before trying todisambiguate the senses.
In thcory, we would expectinformation such as subject domain and collocations tohelp part of speech tagging to be more accurate,however slightly, but we have not yet bccn able todemonstrate this in practice.6.
AcknowledgementsThis work was supported by the DTI/SALT ,fundedproject Integrated Language Database, and built onwork funded by the EC funded project ACQUILEX IIand on background material from CambridgeUniversity Press.ReferencesAtwell, E., 1987, Constituent-likelihood grammar, TheComputational Analysis of English, LongmanBiber, D., 1993, Using Register-Diversified Corpora forGeneral Language Studies, ComputationalLinguistics 19:2Cowie, J., L.Guthrie & J.Guthrie, 1992, Lexicaldisambiguation using simulated annealing,Proceedings ofCOLING-92Gale, W.A., K.W.Church & D.Yarowsky, 1992, UsingBilingual Materials to Develop Word SenseDisambiguation MethodsLazar, K.A., 1996, Breaking New Ground, The EvenYearbook 2Ng, H.T.
& H.B.Lee, 1996, Integrating multipleknowledge sources to disambiguate word senses: Anexamplar-based approach, ACL ProceedingsProcter, P., 1995, (ed.)
Cambridge InternationalDictionary of English, CUPWilks, Y.A., B.M.Slator & L.Guthrie, 1996, ElectricWords: Dictionaries, Computers and Meanings,MIT Press77Y.A.Wilks & M.Stevenson, 1996, The Grammar ofSense: Is word-sense tagging much more than part-of-speech tagging?Yarowsky, D., 1994, Decision Lists for LexicalAmbiguity Resolution: Application to AccentRestoration in Spanish and French, ACLProceedings78
