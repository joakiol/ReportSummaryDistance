Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 116?123,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsParameter estimation for agenda-based user simulationSimon Keizer, Milica Gas?ic?, Filip Jurc??
?c?ek, Franc?ois Mairesse,Blaise Thomson, Kai Yu, and Steve Young ?University of Cambridge, Department of Engineering, Cambridge (UK){sk561,mg436,fj228,farm2,brmt2,ky219,sjy}@cam.ac.ukAbstractThis paper presents an agenda-based usersimulator which has been extended to betrainable on real data with the aim of moreclosely modelling the complex rational be-haviour exhibited by real users.
The train-able part is formed by a set of random de-cision points that may be encountered dur-ing the process of receiving a system actand responding with a user act.
A sample-based method is presented for using realuser data to estimate the parameters thatcontrol these decisions.
Evaluation resultsare given both in terms of statistics of gen-erated user behaviour and the quality ofpolicies trained with different simulators.Compared to a handcrafted simulator, thetrained system provides a much better fitto corpus data and evaluations suggest thatthis better fit should result in improved di-alogue performance.1 IntroductionIn spoken dialogue systems research, modellingdialogue as a (Partially Observable) Markov Deci-sion Process ((PO)MDP) and using reinforcementlearning techniques for optimising dialogue poli-cies has proven to be an effective method for de-veloping robust systems (Singh et al, 2000; Levinet al, 2000).
However, since this kind of optimi-sation requires a simulated user to generate a suffi-ciently large number of interactions to learn from,this effectiveness depends largely on the qualityof such a user simulator.
An important require-ment for a simulator is for it to be realistic, i.e., itshould generate behaviour that is similar to that ofreal users.
Trained policies are then more likely toperform better on real users, and evaluation resultson simulated data are more likely to predict resultson real data more accurately.
?This research was partly funded by the UK EPSRC un-der grant agreement EP/F013930/1 and by the EU FP7 Pro-gramme under grant agreement 216594 (CLASSiC project:www.classic-project.org).This is one of the reasons why learning usersimulation models from data on real user be-haviour has become an important direction of re-search (Scheffler and Young, 2001; Cuaya?huitl etal., 2005; Georgila et al, 2006).
However, the datadriven user models developed so far lack the com-plexity required for training high quality policiesin task domains where user behaviour is relativelycomplex.
Handcrafted models are still the mosteffective in those cases.This paper presents an agenda-based user simu-lator which is handcrafted for a large part, but ad-ditionally can be trained with data from real users(Section 2).
As a result, it generates behaviour thatbetter reflects the statistics of real user behaviour,whilst preserving the complexity and rationalityrequired to effectively train dialogue managementpolicies.
The trainable part is formed by a set ofrandom decision points, which, depending on thecontext, may or may not be encountered duringthe process of receiving a system act and decid-ing on a response act.
If such a point is encoun-tered, the simulator makes a random decision be-tween a number of options which may directly orindirectly influence the resulting output.
The op-tions for each random decision point are reason-able in the context in which it is encountered, buta uniform distribution of outcomes might not re-flect real user behaviour.We will describe a sample-based method for es-timating the parameters that define the probabili-ties for each possible decision, using data on realusers from a corpus of human-machine dialogues(Section 3).
Evaluation results will be presentedboth in terms of statistics on generated user be-haviour and the quality of dialogue policies trainedwith different user simulations (Section 4).2 Agenda-based user simulationIn agenda-based user simulation, user acts are gen-erated on the basis of a user goal and an agenda(Schatzmann et al, 2007a).
The simulator pre-sented here is developed and used for a tourist in-116formation application, but is sufficiently generic toaccommodate slot-filling applications in any do-main.1 The user goal consists of the type of venue,for example hotel, bar or restaurant, a listof constraints in the form of slot value pairs, suchas food=Italian or area=east, and a listof slots the user wants to know the value of, suchas the address (addr), phone number (phone),or price information (price) of the venue.
Theuser goals for the simulator are randomly gener-ated from the domain ontology describing whichcombinations of venue types and constraints areallowed and what are the possible values for eachslot.
The agenda is a stack-like structure contain-ing planned user acts.
When the simulator receivesa system act, the status of the user goal is updatedas well as the agenda, typically by pushing newacts onto it.
In a separate step, the response useract is selected by popping one or more items offthe agenda.Although the agenda-based user simulator in-troduced by Schatzmann et al (2007a) was en-tirely handcrafted, it was realistic enough to suc-cessfully test a prototype POMDP dialogue man-ager and train a dialogue policy that outperformeda handcrafted baseline (Young et al, 2009).
Amethod to train an agenda-based user simula-tor from data was proposed by Schatzmann etal.
(2007b).
In this approach, operations onthe agenda are controlled by probabilities learnedfrom data using a variation of the EM algorithm.However, this approach does not readily scale tomore complex interactions in which users can, forexample, change their goal midway through a dia-logue.2.1 Random decision parametersEach time the user simulator receives a system act,a complex, two-fold process takes place involvingseveral decisions, made on the basis of both thenature of the incoming system act and the infor-mation state of the user, i.e., the status of the usergoal and agenda.
The first phase can be seen asan information state update and involves actionslike filling requested slots or checking whether theprovided information is consistent with the usergoal constraints.
In the second phase, the user de-cides which response act to generate, based on theupdated agenda.
Many of the decisions involvedare deterministic, allowing only one possible op-tion given the context.
Other decisions allow forsome degree of variation in the user behaviour andare governed by probability distributions over the1We have to date also implemented systems in appoint-ment scheduling and bus timetable inquiries.options allowed in that context.
For example, ifthe system has offered a venue that matches theuser?s goal, the user can randomly decide to eitherchange his goal or to accept the venue and ask foradditional information such as the phone number.The non-deterministic part of the simulator isformalised in terms of a set of random decisionpoints (RDPs) embedded in the decision process.If an RDP is encountered (depending on the con-text), a random choice between the options de-fined for that point is made by sampling from aprobability distribution.
Most of the RDPs arecontrolled by a multinomial distribution, such asdeciding whether or not to change the goal aftera system offer.
Some RDPs are controlled by ageometric distribution, like in the case where theuser is planning to specify one of his constraints(with an inform act popped from the agenda) andthen repeatedly adds an additional constraint to theact (by combining it with an additional inform actpopped from the agenda) until it randomly decidesnot to add any more constraints (or runs out ofconstraints to specify).
The parameter for this dis-tribution thus controls how cautious the user is inproviding information to the system.Hence, the user simulator can be viewed asa ?decision network?, consisting of deterministicand random decision points.
This is illustrated inFigure 1 for the simplified case of a network withonly four RDPs; the actual simulator has 23 RDPs,with 27 associated parameters in total.
Each timethe simulator receives a system act, it follows apath through the network, which is partly deter-mined by that system act and the user goal andagenda, and partly by random decisions made ac-cording to the probability distributions for eachrandom decision point i given by its parameters?i.3 Training the simulator from dataThe parameterisation of the user simulator as de-scribed in Section 2.1 forms the basis for a methodfor training the simulator with real user data.
Theparameters describing the probability distributionsfor each RDP are estimated in order to generateuser behaviour that fits the user behaviour in thecorpus as closely as possible.
In order to do so,a sample based maximum likelihood approach istaken, in which the simulator is run repeatedlyagainst the system acts in the corpus, and the ran-dom decisions that lead to simulated acts matchingthe true act in the corpus are recorded.
The param-eters are then estimated using the counts for eachof the random decision points.117incomingsystem actoutgoinguser actuser goal + agenda?2?1?3?4Figure 1: User simulator viewed as a ?decision network?
: square nodes indicate deterministic decisionpoints; round nodes indicate random decision points, and have associated parameters ?i; the loop on oneof the nodes indicates it has a geometric distribution associated with it.3.1 Parameter estimationBefore starting the process of matching simulatedacts with true acts and collecting counts for theRDPs, the parameters are initialised to values cor-responding to uniform distributions.
Then, thesimulator is run against all dialogues in the cor-pus in such a way that for each turn in a dialogue(consisting of a system act and a user act), the usersimulator is provided with the system act and isrun repeatedly to generate several simulated userresponse acts for that turn.
For the first turn of a di-alogue, the simulator is initialised with the correctuser state (see Section 3.2).
For each response, thesimulator may make different random decisions,generally leading to different user acts.
The deci-sions that lead to a simulated act that matches thetrue act are recorded as successful.
By generatinga sufficiently large number of simulated acts, allpossible combinations of decisions are explored tofind a matching act.
Given the high complexity ofthe simulator, this sampling approach is preferredover directly enumerating all decision combina-tions to identify the successful ones.
If none ofthe combinations are successful, then either a) theprocessing of the dialogue is ended, or b) the cor-rect context is set for the next turn and processingis continued.
Whereas the former approach aims atmatching sequences of turns, the latter only aimsat matching each user turn separately.
In eithercase, after all data is processed, the parameters areestimated using the resulting counts of successfuldecisions for each of the RDPs.For each RDP i, let DPi represent the decisiontaken, and dij the j?th possible decision.
Then, foreach decision point i that is controlled by a multi-nomial distribution, the corresponding parameterestimates ?ij are obtained as follows from the de-cision frequencies c(DPi = dij):?ij =c(DPi = dij)?j c(DPi = dij)(1)Random decision points that are controlledby geometric distributions involve potentiallymultiple random decisions between two options(Bernoulli trials).
The parameters for such RDPsare estimated as follows:?i =(1nn?k=1bik)?1(2)where bik is the number of Bernoulli trials re-quired at the k?th time decision point i was en-countered.
In terms of the decision network, thisestimate is correlated with the average number oftimes the loop of the node was taken.3.2 User goal inferenceIn order to be able to set the correct user goalstate in any given turn, a set of update rules isused to infer the user?s goals from a dialogue be-forehand, on the basis of the entire sequence ofsystem acts and ?true?
user acts (see Section 4.1)in the corpus.
These update rules are based onthe notion of dialogue act preconditions, whichspecify conditions of the dialogue context thatmust hold for a dialogue agent to perform thatact.
For example, a precondition for the actinform(area=central) is that the speakerwants a venue in the centre.
The user act model118of the HIS dialogue manager is designed accord-ing to this same notion (Keizer et al, 2008).
In thismodel, the probability of a user act in a certain dia-logue context (the last system act and a hypothesisregarding the user goal) is determined by checkingthe consistency of its preconditions with that con-text.
This contributes to updating the system?s be-lief state on the basis of which it determines its re-sponse action.
For the user goal inference model,the user act is given and therefore its precondi-tions can be used to directly infer the user goal.So, for example, in the case of observing the useract inform(area=central), the constraint(area=central) is added to the user goal.In addition to using the inferred user goals, theagenda is corrected in cases where there is a mis-match between real and simulated user acts in theprevious turn.In using this offline goal inference model, ourapproach takes a position between (Schatzmann etal., 2007b), in which the user?s goal is treated ashidden, and (Georgila et al, 2006), in which theuser?s goal is obtained directly from the corpus an-notation.4 EvaluationThe parameter estimation technique for trainingthe user simulator was evaluated in two differ-ent ways.
The first evaluation involved compar-ing the statistics of simulated and real user be-haviour.
The second evaluation involved compar-ing dialogue manager policies trained with differ-ent simulators.4.1 DataThe task of the dialogue systems we are develop-ing is to provide tourist information to users, in-volving venues such as bars, restaurants and hotelsthat the user can search for and ask about.
Thesevenues are described in terms of features such asprice range, area, type of food, phone number,address, and so on.
The kind of dialogues withthese systems are commonly called slot-filling di-alogues.Within the range of slot-filling applications thedomain is relatively complex due to its hierarchi-cal data structure and relatively large number ofslots and their possible values.
Scalability is in-deed one of the primary challenges to be addressedin statistical approaches to dialogue system devel-opment, including user simulation.The dialogue corpus that was used for trainingand evaluating the simulator was obtained fromthe evaluation of a POMDP spoken dialogue sys-tem with real users.
All user utterances in theresulting corpus were transcribed and semanti-cally annotated in terms of dialogue acts.
Dia-logue acts consist of a series of semantic items,including the type (describing the intention ofthe speaker, e.g., inform or request) and alist of slot value pairs (e.g., food=Chinese orarea=south).
An extensive analysis of the an-notations from three different people revealed ahigh level of inter-annotator agreement (rangingfrom 0.81 to 0.94, depending on which pair of an-notations are compared), and a voting scheme forselecting a single annotation for each turn ensuredthe reliability of the ?true?
user acts used for train-ing the simulator.4.2 Corpus statistics resultsA first approach to evaluating user simulations isto look at the statistics of the user behaviour thatis generated by a simulator and compare it withthat of real users as observed in a dialogue cor-pus.
Several metrics for such evaluations havebeen considered in the literature, all of which haveboth strong points and weaknesses.
For the presentevaluation, a selection of metrics believed to givea reasonable first indication of the quality of theuser simulations was considered2 .4.2.1 MetricsThe first corpus-based evaluation metric is the LogLikelihood (LL) of the data, given the user simu-lation model.
This is what is in fact maximised bythe parameter estimation algorithm.
The log like-lihood can be computed by summing the log prob-abilities of each user turn du in the corpus data D:ll(D|{?ij}, {?i}) =?ulog P (du|{?ij}, {?i})(3)The user turn probability is given by the prob-ability of the decision paths (directed paths in thedecision network of maximal length, such as theone indicated in Figure 1 in bold) leading to a sim-ulated user act in that turn that matches the trueuser act.
The probability of a decision path is ob-tained by multiplying the probabilities of the de-cisions made at each decision point i that was en-countered, which are given by the parameters ?ij2Note that not all selected metrics are metrics in the strictsense of the word; the term should therefore be interpreted asa more general one.119and ?i:logP (du|{?ij}, {?i}) =?i?Im(u)log(?j?ij ?
?ij(u))+ (4)?i?Ig(u)log(?k(1 ?
?i)k?1 ?
?i ?
?ik(u))where Im(u) = {i ?
Im|?j ?ij(u) > 0} andIg(u) = {i ?
Ig|?k ?ik(u) > 0} are the subsetsof the multinomial (Im) and geometric (Ig) de-cision points respectively containing those pointsthat were encountered in any combination of deci-sions resulting in the given user act:?ij(u) =????
?1 if decision DPi = dij wastaken in any of thematching combinations0 otherwise(5)?ik(u) =????
?1 if any of the matchingcombinations requiredk > 0 trials0 otherwise(6)It should be noted that the log likelihood onlyrepresents those turns in the corpus for which thesimulated user can produce a matching simulatedact with some probability.
Hence, it is impor-tant to also take into account the corpus cover-age when considering the log likelihood in cor-pus based evaluation.
Dividing by the number ofmatched turns provides a useful normalisation inthis respect.The expected Precision (PRE), Recall (RCL),and F-Score (FS) are obtained by comparing thesimulated user acts with the true user acts in thesame context (Georgila et al, 2006).
These scoresare obtained by pairwise comparison of the simu-lated and true user act for each turn in the corpusat the level of the semantic items:PRE = #(matched items)#(items in simulated act) (7)RCL = #(matched items)#(items in true act) (8)FS = 2 ?
PRE ?
RCLPRE + RCL (9)By sampling a sufficient number of simulatedacts for each turn in the corpus and comparingthem with the corresponding true acts, this resultsin an accurate measure on average.The problem with precision and recall is thatthey are known to heavily penalise unseen data.Any attempt to generalise and therefore increasethe variability of user behaviour results in lowerscores.Another way of evaluating the user simulatoris to look at the global user act distributions itgenerates and compare them to the distributionsfound in the real user data.
A common metricfor comparing such distributions is the Kullback-Leibler (KL) distance.
In (Cuaya?huitl et al,2005) this metric was used to evaluate an HMM-based user simulation approach.
The KL dis-tance is computed by taking the average of thetwo KL divergences3 DKL(simulated||true) andDKL(true||simulated), where:DKL(p||q) =?ipi ?
log2(piqi) (10)KL distances are computed for both full user actdistributions (taking into account both the dia-logue act type and slot value pairs) and user acttype distributions (only regarding the dialogue acttype), denoted by KLF and KLT respectively.4.2.2 ResultsFor the experiments, the corpus data was ran-domly split into a training set, consisting of 4479user turns in 541 dialogues, used for estimat-ing the user simulator parameters, and a test set,consisting of 1457 user turns in 175 dialogues,used for evaluation only.
In the evaluation, thefollowing parameter settings were compared: 1)non-informative, uniform parameters (UNIF); 2)handcrafted parameters (HDC); 3) parameters es-timated from data (TRA); and 4) deterministic pa-rameters (DET), in which for each RDP the prob-ability of the most probable decision according tothe estimated parameters is set to 1, i.e., at alltimes, the most likely decision according to the es-timated parameters is chosen.For both trained and deterministic parameters,a distinction is made between the two approachesto matching user acts during parameter estimation.Recall that in the turn-based approach, in eachturn, the simulator is run with the corrected con-text to find a matching simulated act, whereas inthe sequence-based approach, the matching pro-cess for a dialogue is stopped in case a turnis encountered which cannot be matched by thesimulator.
This results in estimated parametersTRA-T and deterministic parameters DET-T for3Before computing the distances, add-one smoothing wasapplied in order to avoid zero-probabilities.120PAR nLL-T nLL-S PRE RCL FS KLF KLTUNIF ?3.78 ?3.37 16.95 (?0.75) 9.47 (?0.59) 12.15 3.057 2.318HDC ?4.07 ?2.22 44.31 (?0.99) 34.74 (?0.95) 38.94 1.784 0.623TRA-T ?2.97 - 37.60 (?0.97) 28.14 (?0.90) 32.19 1.362 0.336DET-T ??
- 47.70 (?1.00) 40.90 (?0.98) 44.04 2.335 0.838TRA-S - ?2.13 43.19 (?0.99) 35.68 (?0.96) 39.07 1.355 0.155DET-S - ??
49.39 (?1.00) 43.04 (?0.99) 46.00 2.310 0.825Table 1: Results of the sample-based user simulator evaluation on the Mar?09 trainingcorpus (the corpus coverage was 59% for the turn-based and 33% for the sequence-basedmatching approach).PAR nLL-T nLL-S PRE RCL FS KLF KLTUNIF ?3.61 ?3.28 16.59 (?1.29) 9.32 (?1.01) 11.93 2.951 2.180HDC ?3.90 ?2.19 45.35 (?1.72) 36.04 (?1.66) 40.16 1.780 0.561TRA-T ?2.84 - 38.22 (?1.68) 28.74 (?1.57) 32.81 1.405 0.310DET-T ??
- 49.15 (?1.73) 42.17 (?1.71) 45.39 2.478 0.867TRA-S - ?2.12 43.90 (?1.72) 36.52 (?1.67) 39.87 1.424 0.153DET-S - ??
50.73 (?1.73) 44.41 (?1.72) 47.36 2.407 0.841Table 2: Results of the sample-based user simulator evaluation on the Mar?09 test corpus(corpus coverage 59% for the turn-based, and 36% for sequence-based matching).the turn-based approach and analogously TRA-Sand DET-S for the sequence-based approach.
Thecorresponding normalised (see Section 4.2.1) log-likelihoods are indicated by nLL-T and nLL-S.Tables 1 and 2 give the results on the trainingand test data respectively.
The results show that interms of log-likelihood and KL-distances, the es-timated parameters outperform the other settings,regardless of the matching method.
In terms ofprecision/recall (given in percentages with 95%confidence intervals), the estimated parametersare worse than the handcrafted parameters forturn-based matching, but have similar scores forsequence-based matching.The results for the deterministic parameters il-lustrate that much better precision/recall scorescan be obtained, but at the expense of variability aswell as the KL-distances.
It will be easier to traina dialogue policy on such a deterministic simula-tor, but that policy is likely to perform significantlyworse on the more varied behaviour generated bythe trained simulator, as we will see in Section 4.3.Out of the two matching approaches, thesequence-based approach gives the best results:TRA-S outperforms TRA-T on all scores, exceptfor the coverage which is much lower for thesequence-based approach (33% vs. 59%).4.3 Policy evaluation resultsAlthough the corpus-based evaluation results givea useful indication of how realistic the behaviourgenerated by a simulator is, what really should beevaluated is the dialogue management policy thatis trained using that simulator.
Therefore, differ-ent parameter sets for the simulator were used totrain and evaluate different policies for the HiddenInformation State (HIS) dialogue manager (Younget al, 2009).
Four different policies were trained:one policy using handcrafted simulation param-eters (POL-HDC); two policies using simulationparameters estimated (using the sequence-basedmatching approach) from two data sets that wereobtained by randomly splitting the data into twoparts of 358 dialogues each (POL-TRA1 and POL-TRA2); and finally, a policy using a determin-istic simulator (POL-DET) constructed from thetrained parameters as discussed in Section 4.2.2.The policies were then each evaluated on the sim-ulator using the four parameter settings at differentsemantic error rates.The performance of a policy is measured interms of a reward that is given for each dialogue,i.e.
a reward of 20 for a successful dialogue, mi-nus the number of turns.
A dialogue is consid-ered successful if the system has offered a venuematching the predefined user goal constraints andhas given the correct values of all requested slotsfor this venue.
During the policy optimisation, inwhich a reinforcement learning algorithm tries tooptimise the expected long term reward, this dia-logue scoring regime was also used.In Figures 2, 3, and 4, evaluation results aregiven resulting from running 3000 dialogues ateach of 11 different semantic error rates.
Thecurves show average rewards with 95% confidenceintervals.
The error rate is controlled by a hand-121-20246810120  0.1  0.2  0.3  0.4  0.5AveragerewardError ratePOL-HDCPOL-TRA1POL-TRA2POL-DETFigure 2: Average rewards for each policy whenevaluated on UM-HDC.-4-202468100  0.1  0.2  0.3  0.4  0.5AveragerewardError ratePOL-HDCPOL-TRA1POL-TRA2POL-DETFigure 3: Average rewards for each policy whenevaluated on UM-TRA1.2468101214160  0.1  0.2  0.3  0.4  0.5AveragerewardError ratePOL-HDCPOL-TRA1POL-TRA2POL-DETFigure 4: Average rewards for each policy whenevaluated on UM-DET.012345670  0.1  0.2  0.3  0.4  0.5AveragerewardlossError ratePOL-HDCPOL-TRA2POL-DETFigure 5: Average loss in reward for each policy,across three different simulators.crafted error model that converts the user act gen-erated by the simulator into an n-best list of dia-logue act hypotheses.The policy that was trained using the hand-crafted simulator (POL-HDC) outperforms theother policies when evaluated on that same sim-ulator (see Figure 2), and both policies trained us-ing the trained simulators (POL-TRA1 and POL-TRA2) outperform the other policies when evalu-ated on either trained simulator (see Figure 3 forthe evaluation on UM-TRA1; the evaluation onUM-TRA2 is very similar and therefore omitted).There is little difference in performance betweenpolicies POL-TRA1 and POL-TRA2, which canbe explained by the fact that the two trainedparameter settings are quite similar, in contrastto the handcrafted parameters.
The policy thatwas trained on the deterministic parameters (POL-DET) is competitive with the other policies whenevaluated on UM-DET (see Figure 4), but per-forms significantly worse on the other parametersettings which generate the variation in behaviourthat the dialogue manager did not encounter dur-ing training of POL-DET.In addition to comparing the policies when eval-uated on each simulator separately, another com-parison was made in terms of the average perfor-mance across all simulators.
For each policy andeach simulator, we first computed the differencebetween the policy?s performance and the ?maxi-mum?
performance on that simulator as achievedby the policy that was also trained on that simu-lator, and then averaged over all simulators.
Toavoid biased results, only one of the trained simu-lators was included.
The results in Figure 5 showthat the POL-TRA2 policy is more robust thanPOL-DET, and has similar robustness as POL-HDC.
Similar results are obtained when includingUM-TRA1 only.Given that the results of Section 4.2 show thatthe dialogues generated by the trained simulatormore closely match real corpus data, and giventhat the above simulation results show that thePOL-TRA policies are at least as robust as the122other policies, it seems likely that policies trainedusing the trained user simulator will show im-proved performance when evaluated on real users.However, this claim can only be properlydemonstrated in a real user evaluation of the di-alogue system containing different dialogue man-agement policies.
Such a user trial would also beable to confirm whether the results from evalua-tions on the trained simulator can more accuratelypredict the actual performance expected with realusers.5 ConclusionIn this paper, we presented an agenda-based usersimulator extended to be trainable on real userdata whilst preserving the necessary rationalityand complexity for effective training and evalu-ation of dialogue manager policies.
The exten-sion involved the incorporation of random deci-sion points in the process of receiving and re-sponding to a system act in each turn.
The deci-sions made at these points are controlled by prob-ability distributions defined by a set of parameters.A sample-based maximum likelihood approachto estimating these parameters from real user datain a corpus of human-machine dialogues was dis-cussed, and two kinds of evaluations were pre-sented.
When comparing the statistics of real ver-sus simulated user behaviour in terms of a selec-tion of different metrics, overall, the estimated pa-rameters were shown to give better results thanthe handcrafted baselines.
When evaluating dia-logue management policies trained on the simula-tor with different parameter settings, it was shownthat: 1) policies trained on a particular parame-ter setting outperform other policies when evalu-ated on the same parameters, and in particular, 2)a policy trained on the trained simulator outper-forms other policies on a trained simulator.
Withthe general goal of obtaining a dialogue managerthat performs better in practice, these results areencouraging, but need to be confirmed by an eval-uation of the policies on real users.Additionally, there is still room for improvingthe quality of the simulator itself.
For example,the variation in user behaviour can be improved byadding more random decision points, in order toachieve better corpus coverage.
In addition, sincethere is no clear consensus on what is the best met-ric for evaluating user simulations, additional met-rics will be explored in order to get a more bal-anced indication of the quality of the user simu-lator and how the various metrics are affected bymodifications to the simulator.
Perplexity (relatedto the log likelihood, see (Georgila et al, 2005)),accuracy (related to precision/recall, see (Zuker-man and Albrecht, 2001; Georgila et al, 2006)),and Crame?r-von Mises divergence (comparing di-alogue score distributions, see (Williams, 2008))are some of the metrics worth considering.ReferencesH.
Cuaya?huitl, S. Renals, O.
Lemon, and H. Shi-modaira.
2005.
Human-computer dialogue sim-ulation using hidden markov models.
In Proc.ASRU?05, pages 290?295.K.
Georgila, J. Henderson, and O.
Lemon.
2005.Learning user simulations for information state up-date dialogue systems.
In Proc.
Interspeech ?05.K.
Georgila, J. Henderson, and O.
Lemon.
2006.
Usersimulation for spoken dialogue systems: Learningand evaluation.
In Proc.
Interspeech/ICSLP.S.
Keizer, M.
Gas?ic?, F. Mairesse, B. Thomson, K. Yu,and S. Young.
2008.
Modelling user behaviour inthe HIS-POMDP dialogue manager.
In Proc.
SLT,Goa, India.E.
Levin, R. Pieraccini, and W. Eckert.
2000.
Astochastic model of human-machine interaction forlearning dialogue strategies.
IEEE Transactions onSpeech and Audio Processing, 8(1).J.
Schatzmann, B. Thomson, K. Weilhammer, H. Ye,and S. Young.
2007a.
Agenda-based user simula-tion for bootstrapping a POMDP dialogue system.In Proceedings HLT/NAACL, Rochester, NY.J.
Schatzmann, B. Thomson, and S. Young.
2007b.Statistical user simulation with a hidden agenda.
InProc.
SIGDIAL?07, pages 273?282, Antwerp, Bel-gium.K.
Scheffler and S. Young.
2001.
Corpus-based dia-logue simulation for automatic strategy learning andevaluation.
In Proceedings NAACL Workshop onAdaptation in Dialogue.S.
Singh, M. Kearns, D. Litman, and M. Walker.
2000.Reinforcement learning for spoken dialogue sys-tems.
In S. Solla, T. Leen, and K. Mu?ller, editors,Advances in Neural Information Processing Systems(NIPS).
MIT Press.J.
Williams.
2008.
Evaluating user simulations withthe Crame?r-von Mises divergence.
Speech Commu-nication, 50:829?846.S.
Young, M.
Gas?ic?, S. Keizer, F. Mairesse, B. Thom-son, and K. Yu.
2009.
The Hidden InformationState model: a practical framework for POMDPbased spoken dialogue management.
ComputerSpeech and Language, 24(2):150?174.I.
Zukerman and D. Albrecht.
2001.
Predictive statis-tical models for user modeling.
User Modeling andUser-Adapted Interaction, 11:5?18.123
