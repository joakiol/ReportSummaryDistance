Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 54?62,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPUsing Artiially Generated Datato Evaluate Statistial Mahine TranslationManny Rayner, Paula Estrella, Pierrette BouillonUniversity of Geneva, TIM/ISSCO40 bvd du Pont-d'Arve, CH-1211 Geneva 4, SwitzerlandfEmmanuel.Rayner,Paula.Estrella,Pierrette.Bouillongunige.hBeth Ann HokeyMail Stop 19-26, UCSC UARCNASA Ames Researh Center, Moffett Field, CA 94035?1000bahokeyus.eduYukie NakaoLINA, Nantes University, 2, rue de la Houssiniere, BP 92208 44322 Nantes Cedex 03yukie.nakaouniv-nantes.frAbstratAlthough Statistial Mahine Translation(SMT) is now the dominant paradigmwithin Mahine Translation, we argue thatit is far fromlear that itan outperformRule-Based Mahine Translation (RBMT)on small- to medium-voabulary applia-tions where high preision is more impor-tant than reall.
A partiularly importantpratial example is medial speeh trans-lation.
We report the results of exper-iments where weongured the variousgrammars and rule-sets in an Open Souremedium-voabulary multi-lingual medialspeeh translation system to generate largealigned bilingualorpora for English !Frenh and English !
Japanese, whihwere then used to train SMT models basedon theommonombination of Giza++,Moses and SRILM.
The resulting SMTswere unable fully to reprodue the per-formane of the RBMT, with performanetopping out, even for English !
Frenh,with less than 70% of the SMT translationsof previously unseen sentenes agreeingwith RBMT translations.
When the out-puts of the two systems differed, humanjudges reported the SMT result as fre-quently being worse than the RBMT re-sult, and hardly ever better; moreover, theadded robustness of the SMT only yieldeda small improvement in reall, with a largepenalty in preision.1 IntrodutionWhen Statistial Mahine Translation (SMT) wasrst introdued in the early 90s, it enountered ahostile reeption, and many people in the researhommunity were unwilling to believe itould everbe a seriousompetitor to symboliapproahes(f. for example (Arnold et al, 1994)).
The pendu-lum has now swung all the way to the other end ofthe sale; right now, the prevailing wisdom withinthe researhommunity is that SMT is the onlytruly viable arhiteture, and that rule-based ma-hine translation (RBMT) is ultimately doomed tofailure.
In this paper, one of our initialonernswill be to argue for aompromise position.
In ouropinion, the initial septiism about SMT was notgroundless; the arguments presented against it of-ten took the form of examples involving deep lin-guistireasoning, whih, it waslaimed, would behard to address using surfae methods.
Proponentsof RBMT had, however, greatly underestimatedthe extent to whih SMT would be able to taklethe problem of robustness, where it appears to befar more powerful than RBMT.
For most mahinetranslation appliations, robustness is theentralissue, so SMT'surrent preeminene is hardly sur-prising.Even for the large-voabulary tasks where SMTdoes best, the situation is by no means aslear asone might imagine: aording to (Wilks, 2007),purely statistial systems are still unable to out-perform SYSTRAN.
In this paper, we will how-ever be moreonerned with limited-domain MTtasks, where robustness is not the key requirement,and auray is paramount.
An immediate exam-54ple is medial speeh translation, whih is estab-lishing itself as an an appliation area of some sig-niane (Bouillon et al, 2006; Bouillon et al,2008a).
Translation in medial appliations needsto be extremely aurate, sine mistranslationsan have serious or even fatalonsequenes.
Atthe panel disussion at the 2008 COLING work-shop on safety-ritial speeh translation (Rayneret al, 2008), theonsensus opinion, based on in-put from pratising physiians, was that an appro-priate evaluation metrifor medial appliationswould be heavily slanted towards auray, as op-posed to robustness.
If the metriis normalised soas to award 0 points for no translation, and 1 pointfor aorret translation, the estimate was that asuitable sore for an inorret translation wouldbe something between ?25 and ?100 points.
Withthese requirements, it seems unlikely that a robust,broad-overage arhiteture has muhhane ofsuess.
The obvious strategy is to build a limited-domainontrolled-language system, and tune it tothe point where auray reahes the desired level.For systems of this kind, it is at leastoneiv-able that RBMT may be able to outperform SMT.The next question is how to investigate the issuesin a methodologially even-handed way.
A fewstudies, notably (Seneff et al, 2006), suggest thatrule-based translation may in fat be preferable intheseases.
(Another related experiment is de-sribed in (Dugast et al, 2008), though this wasarried out in a large-voabulary system).
Thesestudies, however, have not been widelyited.
Onepossible explanation is suspiion about method-ologial issues.
Seneff and herolleagues trainedtheir SMT system on 20 000 sentene pairs, asmall number by the standards of SMT.
It is a pri-ori not implausible that more training data wouldhave enabled them toreate an SMT system thatwas as good as, or better than, the rule-based sys-tem.In this paper, our primary goal is to take thiskind of objetion seriously, and develop a method-ology designed to enable a tightomparison be-tween rule-based and statistial arhitetures.
Inpartiular, we wish to examine the widely be-lievedlaim that SMT is now inherently betterthan RBMT.
In order to do this, we start with alimited-domain RBMT system; we use it to auto-matially generate a largeorpus of aligned pairs,whih is used to train aorresponding SMT sys-tem.
We thenompare the performane of the twosystems.Our argument will be that this situation essen-tially represents an upper bound for what is possi-ble using the SMT approah in a limited domain.It has been widely remarked that quality, as wellas quantity, of training data is important for goodSMT; in many projets, signiant effort is ex-pended tolean the original training data.
Here,sine the data is automatially generated by a rule-based system, wean be sure that it is alreadyompletelylean (in the sense of being internallyonsistent), and wean generate as large a quan-tity of it as we require.
The appliation, more-over, uses only a smallish voabulary and a fairlyonstrained syntax.
If the derived SMT system isunable to math the original RBMT system's per-formane, it seems reasonable tolaim that thisshows that there are types of appliations whereRBMT arhitetures are superior.The experiments desribed have beenarriedout using MedSLT, an Open Soure interlingua-based limited-domain medial speeh translationsystem.
The rest of the paper is organised as fol-lows.
Setion 2 provides bakground on the Med-SLT system.
Setion 3 desribes the experimen-tal framework, and Setion 4 the results obtained.Setion 5onludes.2 The MedSLT SystemMedSLT (Bouillon et al, 2005; Bouillon et al,2008b) is a medium-voabulary interlingua-basedOpen Soure speeh translation system for dotor-patient medial examination questions, whihprovides any-language-to-any-language transla-tionapabilities for all languages in the set En-glish, Frenh, Japanese, Arabi, Catalan.
Bothspeeh reognition and translation are rule-based.Speeh reognition runs on the Nuane 8.5 reog-nition platform, with grammar-based languagemodels built using the Open Soure Regulusom-piler.
As desribed in (Rayner et al, 2006),eah domain-speilanguage model is extratedfrom a general resoure grammar usingorpus-based methods driven by a seedorpus of domain-speiexamples.
The seedorpus, whih typi-allyontains between 500 and 1500 utteranes,is then used a seond time to add probabilistiweights to the grammar rules; this substantiallyimproves reognition performane (Rayner et al,2006, x11.5).
Voabulary sizes and performanemeasures for speeh reognition in the three lan-55guages where serious evaluations have beenar-ried out are shown in Figure 1.Language Voab WER SemEREnglish 447 6% 11%Frenh 1025 8% 10%Japanese 422 3% 4%Table 1: Reognition performane for English,Frenh and Japanese MedSLT reognisers.
?Vo-ab?
= number of surfae words in soure lan-guage reogniser voabulary; ?WER?
= Word Er-ror Rate for soure language reogniser, on in-overage material; ?SemER?
= semantierror rate(proportion of utteranes failing to produeorretinterlingua) for soure language reogniser, on in-overage material.At run-time, the reogniser produes a soure-langage semantirepresentation.
This is rsttranslated by one set of rules into an interlingualform, and then by a seond set into a target lan-guage representation.
A target-language Regu-lus grammar,ompiled into generation form, turnsthis into one or more possible surfae strings, af-ter whih a set of generation preferenes piksone out.
Finally, the seleted string is realised inspoken form.
Robustness issues are addressed bymeans of a bak-up statistial reogniser, whihdrives a robust embedded help system.
The pur-pose of the help system (Chatzihrisas et al,2006) is to guide the user towards supportedov-erage; it performs approximate mathing of out-put from the statistial reogniser again a libraryof sentenes whih have been marked asorretlyproessed during system development, and thenpresents thelosest mathes to the user.Examples of typial English domain sentenesand their translations into Frenh and Japanese areshown in Figure 2.3 Experimental frameworkIn the literature on language modelling, there isa known tehnique for bootstrapping a statisti-al language model (SLM) from a grammar-basedlanguage model (GLM).
The grammar whihforms the basis of the GLM is sampled randomlyin order toreate an arbitrarily largeorpus of ex-amples; these examples are then used as a train-ingorpus to build the SLM (Jurafsky et al, 1995;Jonson, 2005).
We adapt this proess in a straight-forward way toonstrut an SMT for a givenlanguage pair, using the soure language gram-mar, the soure-to-interlingua translation rules, theinterlingua-to-target-language rules, and the tar-get language generation grammar.
We start in thesame way, using the soure language grammar tobuild a randomly generated soure languageor-pus; as shown in (Hokey et al, 2008), it is im-portant to have a probabilistigrammar.
We thenuse theomposition of the otheromponents toattempt to translate eah soure language senteneinto a target language equivalent, disarding theexamples for whih no translation is produed.The result is an aligned bilingualorpus of ar-bitrary size, whihan be used to train an SMTmodel.We used this method to generate alignedor-pora for the two MedSLT language pairs English!
Frenh and English !
Japanese.
For eah lan-guage pair, we rst generated one million soure-language utteranes; we next ltered them to keeponly examples whih were full sentenes, as op-posed to elliptial phrases, and nally used thetranslation rules and target-language generators toattempt to translate eah sentene.
Thisreatedapproximately 305K aligned sentene-pairs forEnglish !
Frenh (1901K words English, 1993Kwords Frenh), and 311K aligned sentene-pairsfor English !
Japanese (1941K words English,2214K words Japanese).
We held out 2.5% ofeah set as development data, and 2.5% as testdata.
Using Giza++, Moses and SRILM (Oh andNey, 2000; Koehn et al, 2007; Stolke, 2002), wetrained SMT models from inreasingly large sub-sets of the training portion, using the developmentportion in the usual way to optimize parameter val-ues.
Finally, we used the resulting models to trans-late the test portion.Our primary goal was to measure the extent towhih the derived versions of the SMT were ableto approximate the original RBMT on data whihwas within the RBMT'soverage.
There is a sim-ple and natural way to perform this measurement:we apply the BLEU metri(Papineni et al, 2001),with the RBMT's translation taken as the refer-ene.
This means that perfetorrespondene be-tween the two translations would yield a BLEUsore of 1.0.This raises an important point.
The BLEUsores we are using here are non-standard; theymeasure the extent to whih the SMT approxi-mates the RBMT, rather than, as usual, measuring56English Is the pain above your eye?Frenh Avez-vous mal au dessus des yeux?Japanese Itami wa me no ue no atari desu ka?English Have you had the pain for more than a month?Frenh Avez-vous mal depuis plus d'un mois?Japanese Ikkagetsu ijou itami wa tsuzuki mashita ka?English Is the pain assoiated with nausea?Frenh Avez-vous des nause?es quand vous avez la douleur?Japanese Itamu to hakike wa okori masu ka?English Does bright light make the pain worse?Frenh La douleur est-elle aggrave?e par une lumiere forte?Japanese Akarui hikari wo miru to zutsu wa hidoku nari masu ka?Table 2: Examples of English domain sentenes, and the system's translations into Frenh and Japanese.the extent to whih it approximates human trans-lations.
It is important to bring in human judge-ment, to evaluate theases where the SMT andRBMT differ.
If, in theseases, it transpired thathuman judges typially thought that the SMT wasas good as the RBMT, then the differene wouldbe purely aademi.
We need to satisfy ourselvesthat human judges typially asribe differenes be-tween SMT and RBMT to shortomings in theSMT rather than in the RBMT.Conretely, weolleted all the differenthSoure, SMT-translation, RBMT-translationitriples produed during theourse of the ex-periments, and extrated those where the twotranslations were different.
We randomly seleteda set of examples for eah language pair, andasked human judges tolassify them into one ofthe followingategories: RBMT better: The RBMT translation wasbetter, in terms of preserving meaning and/orbeing grammatiallyorret; SMT better: The SMT translation was bet-ter, in terms of preserving meaning and/or be-ing grammatiallyorret; Similar: Both translations were aboutequally good OR the soure sentene wasmeaningless in the domain.In order to show that our metris are intuitivelymeaningful, it is sufient to demonstrate that thefrequeny of ourrene of RBMT better is bothlarge inomparison to that of SMT better, andaounts for a substantial proportion of the totalpopulation.Finally, weonsider the question of whetherthe SMT, whih isapable of translating out-of-grammar sentenes,an add useful robustness tothe base system.
Weolleted, from the set used inthe experiments desribed in (Rayner et al, 2005),all the English sentenes whih failed to be trans-lated into Frenh.
We used the best version ofthe English !
Frenh SMT to translate eah ofthese sentenes, and asked human judges to eval-uate the translations as beinglearly aeptable,learly unaeptable, or borderline.In the next setion, we present the results of thevarious experiments we have just desribed.4 ResultsWe begin with Figure 1, whih shows non-standard BLEU sores for versions of the English!
Frenh SMT system trained on quantities ofdata inreasing from 14 287 to 285 740 pairs.
Asan be seen, translation performane improves upto about 175 000 pairs.
After this, it levels outat around BLEU = 0.90, well below that of theRBMT system with whih it is beingompared.A more diret way to report the result is simply toount the proportion of test sentenes that are notin the training data, whih are translated similarlyby the SMT and the RBMT.
This gure tops out ataround 68%.The results strongly suggest that the SMT isunable to repliate the RBMT's performane atalllosely even in an easy language-pair, irre-spetive of the amount of training data available.Out ofuriosity, and to reassure ourselves that theautomatigeneration proedure was doing some-thing useful, we also tried training the English !Frenh SMT on pairs derived from the 669 ut-57Figure 1: Non-standard BLEU sores againstnumber of pairs of training sentenes for English!
Frenh; training and test data both indepen-dently generated, hene overlapping.terane ?seedorpus?
used to generate the gram-mar (f. Setion 2).
This produed utterly dis-mal performane, with BLEU = 0.52.
The result ismore interesting than it may rst appear, sine, inspeeh reognition, the differene in performanebetween the SLMs trained from seedorpora andlarge generatedorpora is fairly small (Hokey etal., 2008).It seemed possible that the improvement in per-formane with inreased quantities of training datamight, in effet, only be due to the SMT fun-tioning as a translation memory; sine trainingand test data are independently generated by thesame random proess, they overlap, with the de-gree of overlap inreasing as the training set getslarger.
In order to investigate this hypothesis,we repeated the experiments with data whih hadbeen uniqued, so that the training and test setswereompletely disjoint, and neitherontainedany dupliate sentenes1.
In fat, Figure 2 showthat the graph for uniqued English !
Frenh dataare fairly similar to the one for the original non-uniqued data shown in Figures 1.
The main differ-ene is that the non-standard BLEU sore for the1Our opinion is that this is not a realistiway to evaluatethe performane of a small-voabulary system; for example,in MedSLT, one expets that at least some training sentenes,e.g.
?Where is the pain?
?, will also our frequently in testdata.Figure 2: Non-standard BLEU sores againstnumber of pairs of training sentenes for English!
Frenh; training and test data both indepen-dently generated, then uniqued to remove dupli-ates and overlapping items.uniqued data, unsurprisingly, tops out at a lowerlevel, reeting the fat that a ?translation mem-ory?
effet does indeed our to some extent.Results for English !
Japanese showed thesame trends as English !
Frenh, but were morepronouned.
Table 3ompares the performaneof the best versions of the SMTs for the twolanguage-pairs, using both plain and artiiallyuniqued data.
We see that, with plain data, theEnglish !
Japanese SMT falls even further shortof repliating the performane of the RBMT thanwas thease for English !
Frenh; BLEU isonly 0.76.
The differene between the plain anduniqued versions is also more extreme.
BLEU(0.64) isonsiderably lower for the version trainedon uniqued data, suggesting that the SMT for thislanguage pair is nding it harder to generalise,and is in effetloser to funtioning as a trans-lation memory.
This isonrmed byountingthe sentenes in test data and not in training datawhih were translated similarly by the SMT andthe RBMT; we nd that the gure tops out at thevery low value of 26%.As noted in our disussion of the experimentalframework, the non-standard BLEU sores onlyaddress the question of whether the performaneof the SMT and RBMT systems is the same.
It is58Training data Test data BLEUEnglish !
FrenhGenerated Generated 0.90Gen/uniqued Gen/uniqued 0.85English !
JapaneseGenerated Generated 0.76Gen/uniqued Gen/uniqued 0.64Table 3: Translation performane, in terms of non-standard BLEU metri, for differentongura-tions, training on all available data of the spe-ied type.
?Generated?
= data randomly gener-ated; ?Gen/uniqued?
= data randomly generated,then uniqued so that dupliates are removed andtest and training pairs do not overlap.neessary to establish what the differenes meanin terms of human judgements.
Weonsequentlyturn to evaluation of the pairs for whih the SMTand the RBMT systems produed different trans-lation results.Table 4 shows theategorisation, aording totheriteria outlined at the end of Setion 3, for 500English !
Frenh pairs randomly seleted fromthe set of examples where RBMT and SMT gavedifferent results; we asked three judges to evalu-ate them independently, andombined their judg-ments by majority deision where appropriate.
Weobserved a very heavy bias towards the RBMT,with unanimous agreement among the judges thatthe RBMT translation was better in 201/500ases,and 2-1 agreement in a further 127.
Inontrast,there were only 4/500ases where the judgesunanimously thought that the SMT translation waspreferable, with a further 12 supported by a ma-jority deision.
The rest of the table gives theases where the RBMT and SMT translations werejudged the same orases in whih the judges dis-agreed; there were only 41/500ases where nomajority deision was reahed.
Our overallon-lusion is that we are justied in evaluating theSMT by using the BLEU sores with the RBMT asthe referene.
Of theases where the two systemsdiffer, only a tiny fration, at most 16/500, indi-ate a better translation from the SMT, and wellover half are translated better by the RBMT.
Ta-ble 5 presents typial examples of bad SMT trans-lations in the English !
Frenh pair,ontrastedwith the translations produed by the RBMT.
Therst two are grammatial errors (a superuous ex-tra verb in the rst, and agreement errors in theseond).
The third is an badhoie of tense andpreposition; although grammatial, the target lan-guage sentene fails to preserve the meaning, and,rather than referring to a 20 day period endingnow, instead refers to a 20 day period some timein the past.Result Agreement CountRBMT better all judges 201RBMT better majority 127SMT better all judges 4SMT better majority 12Similar all judges 34Similar majority 81Unlear disagree 41Total 500Table 4: Comparison of RBMT and SMT perfor-mane on 500 randomlyhosen English !
Frenhtranslation examples, evaluated independently bythree judges.Table 6 shows a similar evaluation for the En-glish !
Japanese.
Here, the differene betweenthe SMT and RBMT versions was so pronounedthat we felt justied in taking a smaller sample, ofonly 150 sentenes.
This time, 92/150ases wereunanimously judged as having a better RBMTtranslation, and there was not a singlease whereeven a majority found that the SMT was better.Agreement was good here too, with only 8/150ases not yielding at least a majority deision.Result Agreement CountRBMT better all judges 92RBMT better majority 32SMT better all judges 0SMT better majority 0Similar all judges 2Similar majority 16Unlear disagree 8Total 150Table 6: Comparison of RBMT and SMT per-formane on 150 randomlyhosen English !Japanese translation examples, evaluated indepen-dently by three judges.Finally, we look at the performane of the SMTon material whih the RBMT is not able to trans-late.
This would seem to be a situation where59English does a temperaturehangeause the headaheRBMT Frenh vos maux de t?ete sont-ilsause?s par deshangements de tempe?rature(your headahes are-theyaused byhanges of temperature)SMT Frenh avez-vous vos maux de t?ete sont-ilsause?s par deshangements de tempe?rature(have-you your headahes are-theyaused byhanges of temperature)English are headahes relieved in the afternoonRBMT Frenh vos maux de t?ete diminuent-ils l'apres-midi(your headahes (MASC-PLUR) derease-MASC-PLUR the afternoon)SMT Frenh vos maux de t?ete diminue-t-elle l'apres-midi(your headahes (MASC-PLUR) derease-FEM-SING the afternoon)English have you had them for twenty daysRBMT Frenh avez-vous vos maux de t?ete depuis vingt jours(have-you your headahes sine twenty days)SMT Frenh avez-vous eu vos maux de t?ete pendant vingt jours(have-you had your headahes during twenty days)Table 5: Examples of inorret SMT translations from English into Frenh.
Errors are highlighted inbold.the SMTould have an advantage; robustness isgenerally a strength of statistial approahes.
Wereturn to English !
Frenh in Table 7, whihpresents the result of running the best SMT modelon the 357 examples from the test set in (Rayneret al, 2005) whih failed to be translated by theRBMT.
We divide the set intoategories based onthe reason for failure of the RBMT.In the most populous group, translations thatfailed due to out of voabulary items, the SMTwas, more or less byonstrution, also unableto produe a translation.
For the 110 items thatwere out of grammaroverage for the RBMT, theSMT produed 38 good translations, and another 4borderline translations.
There were 50 items thatwere within the soure grammaroverage of theRBMT, but failed somewhere in transfer and gen-eration proessing.
Of those, the majority (32)represented ?bad?
soure sentenes,onsidered asill-formed for the purposes of this experiment.
Outof the remaining items that were within RBMTgrammaroverage, the SMT managed to produe5 good translations and 1 borderline translation.
Intotal, on the most lenient interpretation, the SMTprodued 48 additional translations out of 357.While this improvement in reall is arguably worthhaving, it wouldome at the prie of a substantialdeline in preision.5 Disussion and ConlusionsWe have presented a novel methodology forom-paring RBMT and SMT, and tested it on a spe-Result CountOut of voabularyBad translation 187Out of soure grammaroverageGood translation 38Bad translation 44Borderline translation 4Bad soure sentene 34In soure grammaroverageGood translation 5Bad translation 12Borderline translation 1Bad soure sentene 32Total 357Table 7: English !
Frenh SMT performane onexamples from the test set whih failed to be trans-lated by the RBMT, evaluated by one judge.ipair of RBMT and SMT arhitetures.
Ourlaim is that these results show that the versionof SMT used here is not in fatapable of repro-duing the output of the RBMT system.
Althoughthere has been some interest in attempting to trainSMT systems from RBMT output, the evaluationissues that arise whenomparing SMT and RBMTversions of a high-preision limited-domain sys-tem are different from those arising in most MTtasks, and neessitate aorrespondingly differentmethodology.
It is easy to gain the impression thatit is unsound, and that the experiment has been set60up in suh a way that only one result is possible.This is not, in fat, true.When we have disussed the methodology withpeople who work primarily with SMT, we haveheard two main objetions.
The rst is that theSMT is being trained on RBMT output, and henean only be worse; aommon suggestion is thata system trained on human-produed translationsould yield better results.
It is not at all implau-sible that an SMT trained on this kind of datamight perform better on material whih is outsidetheoverage of the RBMT system.
In this do-main, however, the important issue is preision,not reall; what isritial is the ability to trans-late aurately on material that is within theon-strained language dened by the RBMToverage.The RBMT engine gives very good performaneon in-overage data, as has been shown in otherevaluations of the MedSLT system, e.g.
(Rayner etal., 2005); over 97% of all in-overage sentenesareorretly translated.
Human-generated transla-tions would often, no doubt, be more natural thanthose produed by the RBMT, and there would beslightly fewer outright mistranslations.
But theprimary reason why the SMT is doing badly isnot that the training materialontains bad trans-lations, but rather that the SMT is inapable oforretly reproduing the translations it sees in thetraining data.
Even in the easy English !
Frenhlanguage-pair, the SMT often produes a differenttranslation from the RBMT.
Itould a priori havebeenoneivable that the differenes were unin-teresting, in the sense that SMT outputs differentfrom RBMT outputs were as good, or even better.In fat, Table 4 show that this is not true; when thetwo translations differ, although the SMT transla-tionan oasionally be better, it is usually worse.Table 6 shows that this problem isonsiderablymore aute in English !
Japanese.
Thus theSMT system's inability to model the RBMT sys-tem points to a real limitation.If the SMT had instead been trained on human-generated data, its performane on in-overagematerialould only have improved substantially ifthe SMT for some reason found it easier to learn toreprodue patterns in human-generated data thanin RBMT-generated data.
This seems unlikely.The SMT is being trained from a set of translationpairs whih are guaranteed to beompletelyon-sistent, sine they have been automatially gener-ated by the RBMT; the fat that the RBMT systemonly has a small voabulary should also work inits favour.
If the SMT is unable to reprodue theRBMT's output, it is reasonable to assume it willhave even greater difulty reproduing transla-tions present in normal human-generated trainingdata, whih is always far fromonsistent, and willhave a larger voabulary.The seond objetion we have heard is that thenon-standard BLEU sores whih we have used tomeasure performane use the RBMT translationsas a referene.
People are quik to point out that,if real human translations were sored in this way,they would do less well on the non-standard met-ris than the RBMT translations.
This is, indeed,absolutely true, and explains why it was essentialtoarry out theomparison judging shown in Ta-bles 4 and 6.
If we hadompared human transla-tions with RBMT translations in the same way, wewould have found that human translations whihdiffered from RBMT translations were sometimesbetter, and hardly ever worse.
This would haveshown that the non-standard metris were inap-propriate for the task of evaluating human trans-lations.
In the atualaseonsidered in this paper,we nd aompletely different pattern: the differ-enes are one-sided in the opposite diretion, in-diating that the non-standard metris do in fatagree with human judgements here.A general objetion to all these experiments isthat there may be more powerful SMT arhite-tures.
We used the Giza++/Moses/SRILMom-binination beause it is the de fato standard.
Wehave posted the data we used at http://www.bahr.net/geaf2009; this will allow othergroups to experiment with alternate arhitetures,and determine whether they do in fat yield sig-niant improvements.
For the moment, however,we think it is reasonable tolaim that, in domainswhere high auray is required, it remains to beshown that SMT approahes areapable of ahiev-ing the levels of performane that rule-based sys-temsan deliver.61ReferenesD.
Arnold, L. Balkan, S. Meijer, R.L.
Humphreys, andL.
Sadler.
1994.
Mahine Translation: An Introdu-tory Guide.
Blakwell, Oxford.P.
Bouillon, M. Rayner, N. Chatzihrisas, B.A.Hokey, M. Santaholma, M. Starlander, Y. Nakao,K.
Kanzaki, and H. Isahara.
2005.
A generimulti-lingual open soure platform for limited-domainmedial speeh translation.
In Proeedings of the10th Conferene of the European Assoiation forMahine Translation (EAMT), pages 50?58, Bu-dapest, Hungary.P.
Bouillon, F. Ehsani, R. Frederking, and M. Rayner,editors.
2006.
Proeedings of the HLT-NAACL In-ternational Workshop on Medial Speeh Transla-tion, New York.P.
Bouillon, F. Ehsani, R. Frederking, M. MTear,and M. Rayner, editors.
2008a.
Proeedings ofthe COLING Workshop on Speeh Proessing forSafety Critial Translation and Pervasive Applia-tions, Manhester.P.
Bouillon, G. Flores, M. Georgesul, S. Halimi,B.A.
Hokey, H. Isahara, K. Kanzaki, Y. Nakao,M.
Rayner, M. Santaholma, M. Starlander, andN.
Tsourakis.
2008b.
Many-to-many multilingualmedial speeh translation on a PDA.
In Proeed-ings of The Eighth Conferene of the Assoiationfor Mahine Translation in the Amerias, Waikiki,Hawaii.N.
Chatzihrisas, P. Bouillon, M. Rayner, M. San-taholma, M. Starlander, and B.A.
Hokey.
2006.Evaluating task performane for a unidiretionalontrolled language medial speeh translation sys-tem.
In Proeedings of the HLT-NAACL Interna-tional Workshop on Medial Speeh Translation,pages 9?16, New York.L.
Dugast, J. Senellart, and P. Koehn.
2008.
Can werelearn an RBMT system?
In Proeedings of theThird Workshop on Statistial Mahine Translation,pages 175?178, Columbus, Ohio.B.A.
Hokey, M. Rayner, and G. Christian.
2008.Training statistial language models from grammar-generated data: Aomparativease-study.
In Pro-eedings of the 6th International Conferene on Nat-ural Language Proessing, Gothenburg, Sweden.R.
Jonson.
2005.
Generating statistial language mod-els from interpretation grammars in dialogue sys-tems.
In Proeedings of the 11th EACL, Trento,Italy.A.
Jurafsky, C. Wooters, J. Segal, A. Stolke, E. Fos-ler, G. Tajhman, and N. Morgan.
1995.
Us-ing a stohastiontext-free grammar as a languagemodel for speeh reognition.
In Proeedings ofthe IEEE International Conferene on Aoustis,Speeh and Signal Proessing, pages 189?192.P.
Koehn, H. Hoang, A. Birh, C. Callison-Burh,M.
Federio, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, et al 2007.
Moses: Open souretoolkit for statistial mahine translation.
In AN-NUAL MEETING-ASSOCIATION FOR COMPU-TATIONAL LINGUISTICS, volume 45, page 2.F.J.
Oh and H. Ney.
2000.
Improved statistial align-ment models.
In Proeedings of the 38th AnnualMeeting of the Assoiation for Computational Lin-guistis, Hong Kong.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2001.BLEU: a method for automatievaluation of ma-hine translation.
Researh Report, Computer Si-ene RC22176 (W0109-022), IBM Researh Divi-sion, T.J.Watson Researh Center.M.
Rayner, P. Bouillon, N. Chatzihrisas, B.A.Hokey, M. Santaholma, M. Starlander, H. Isahara,K.
Kanzaki, and Y. Nakao.
2005.
A methodol-ogy foromparing grammar-based and robust ap-proahes to speeh understanding.
In Proeedingsof the 9th International Conferene on Spoken Lan-guage Proessing (ICSLP), pages 1103?1107, Lis-boa, Portugal.M.
Rayner, B.A.
Hokey, and P. Bouillon.
2006.Putting Linguistis into Speeh Reognition: TheRegulus Grammar Compiler.
CSLI Press, Chiago.M.
Rayner, P. Bouillon, G. Flores, F. Ehsani, M. Star-lander, B.
A. Hokey, J. Brotanek, and L. Biewald.2008.
A small-voabulary shared task for medialspeeh translation.
In Proeedings of the COLINGWorkshop on Speeh Proessing for Safety Criti-al Translation and Pervasive Appliations, Manh-ester.S.
Seneff, C. Wang, and J. Lee.
2006.
Combining lin-guistiand statistial methods for bi-diretional En-glish Chinese translation in the ight domain.
InProeedings of AMTA 2006.A.
Stolke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Seventh International Confer-ene on Spoken Language Proessing.
ISCA.Y.
Wilks.
2007.
Stone soup and the Frenh room.
InK.
Ahmad, C. Brewster, and M. Stevenson, editors,Words and Intelligene I: Seleted Papers by YorikWilks, pages 255?265.62
