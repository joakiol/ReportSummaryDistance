Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 48?55,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsResolving Speculation: MaxEnt Cue Classification andDependency-Based Scope Rules?Erik Velldal?
and Lilja ?vrelid??
and Stephan Oepen??
University of Oslo, Department of Informatics (Norway)?Universit?t Potsdam, Institut f?r Linguistik (Germany)erikve@ifi.uio.no and ovrelid@uni-potsdam.de and oe@ifi.uio.noAbstractThis paper describes a hybrid, two-levelapproach for resolving hedge cues, theproblem of the CoNLL-2010 shared task.First, a maximum entropy classifier is ap-plied to identify cue words, using bothsyntactic- and surface-oriented features.Second, a set of manually crafted rules,operating on dependency representationsand the output of the classifier, is appliedto resolve the scope of the hedge cueswithin the sentence.1 IntroductionThe CoNLL-2010 shared task1 comprises twosub-tasks.
Task 1 is described as learning to detectsentences containing uncertainty, while the objectof Task 2 is learning to resolve the in-sentencescope of hedge cues (Farkas et al, 2010).
Parallel-ing this two-fold task definition, the architectureof our system naturally decomposes into two mainsteps.
First, a maximum entropy (MaxEnt) classi-fier is applied to automatically detect cue words.For Task 1, a given sentence is labeled as uncer-tain if it contains a word classified as a cue.
ForTask 2, we then go on to determine the scope ofthe identified cues using a set of manually craftedrules operating on dependency representations.For both Task 1 and Task 2, our system partic-ipates in the stricter category of ?closed?
or ?in-domain?
systems.
This means that we do notuse any additional uncertainty-annotated materialbeyond the supplied training data, consisting of14541 sentences from biomedical abstracts and ar-ticles (see Table 2).
In the official ranking of re-?We are grateful to our colleagues at the University ofOslo and the University of Potsdam, for many useful discus-sions, constructive critique, and encouragment.
We specifi-cally thank Woodley Packard for careful proof-reading.1The CoNLL-2010 shared task website: http://www.inf.u-szeged.hu/rgai/conll2010st/.sults, and considering systems in all categories to-gether (closed/open/cross-domain), our system isranked 4 out of 24 for Task 1 and 3 out of 15 forTask 2, resulting in highest average rank (and F1)overall.
We detail the implementation of the cueclassifier and the syntactic rules in Sections 3 and4, respectively.
Results for the held-out testing areprovided in Section 5.
First, however, the next sec-tion describes the various resources that we usedfor pre-processing the CoNLL data sets, to preparethe input to our hedge analysis systems.2 Architecture and Set-Up2.1 PreprocessingTo ease integration of annotations across systemcomponents, we converted the XML training datato plain-text files, with stand-off annotation linkedto the raw data by virtue of character start and endpositions (dubbed characterization in the follow-ing).
Thus, hedge cues, scope boundaries, tok-enization, Part-of-Speech (PoS) assignments, etc.are all represented in a uniform fashion: as po-tentially overlapping annotations on sub-strings ofthe raw input.The GENIA tagger (Tsuruoka et al, 2005) takesan important role in our pre-processing set-up.However, maybe somewhat surprisingly, we foundthat its tokenization rules are not always opti-mally adapted for the BioScope corpus.
GENIAunconditionally introduces token boundaries forsome punctuation marks that can also occur token-internally.
For example, it wrongly splits tokenslike ?3,926.50?, ?methlycobamide:CoM?,or ?Ca(2+)?.
Conversely, GENIA fails to isolatesome kinds of opening single quotes, because thequoting conventions assumed in BioScope differfrom those used in the GENIA Corpus; furthermore,it mis-tokenizes LATEX-style n- and m-dashes.On average, one in five sentences in the CoNLLtraining data exhibited GENIA tokenization prob-48ID FORM LEMMA POS FEATS HEAD DEPREL XHEAD XDEP1 The the DT _ 4 NMOD 4 SPECDET2 unknown unknown JJ degree:attributive 4 NMOD 4 ADJUNCT3 amino amino JJ degree:attributive 4 NMOD 4 ADJUNCT4 acid acid NN pers:3|case:nom|num:sg|ntype:common 5 SBJ 3 SUBJ5 may may MD mood:ind|subcat:MODAL|tense:pres|clauseType:decl|passive:- 0 ROOT 0 ROOT6 be be VB _ 5 VC 7 PHI7 used use VBN subcat:V-SUBJ-OBJ|vtype:main|passive:+ 6 VC 5 XCOMP8 by by IN _ 7 LGS 9 PHI9 these these DT deixis:proximal 10 NMOD 10 SPECDET10 species specie NNS num:pl|pers:3|case:obl|common:count|ntype:common 8 PMOD 7 OBL-AG11 .
.
.
_ 5 P 0 PUNCTable 1: Enhanced dependency representation of the example sentence The unknown amino acid maybe used by these species with GENIAPoS-tags (POS), Malt parses (HEAD, DEPREL) and XLE parses(XHEAD, XDEP).lems.
Our pre-processing approach therefore de-ploys a home-grown, cascaded finite-state tok-enizer (borrowed and adapted from the open-source English Resource Grammar; Flickinger(2000)), which aims to implement the tokeniza-tion decisions made in the Penn Treebank (Mar-cus et al, 1993) ?
much like GENIA, in principle?
but properly treating corner cases like the onesabove.
Synchronized via characterization, this to-kenization is then enriched with the output of noless than two PoS taggers, as detailed in the nextsection.2.2 PoS Tagging and LemmatizationFor PoS tagging and lemmatization, we combineGENIA (with its built-in, occasionally deviant to-kenizer) and TnT (Brants, 2000), which operateson pre-tokenized inputs but in its default modelis trained on financial news from the Penn Tree-bank.
Our general goal here is to take advantageof the higher PoS accuracy provided by GENIA inthe biomedical domain, while using our improvedtokenization and producing inputs to the parsingstage (see Section 2.3 below) that as much as pos-sible resemble the conventions used in the originaltraining data for the parser ?
the Penn Treebank,once again.To this effect, for the vast majority of tokens wecan align the GENIA tokenization with our own,and in these cases we typically use GENIA PoStags and lemmas (i.e.
base forms).
For better nor-malization, we downcase base forms for all partsof speech except proper nouns.
However, GENIAdoes not make a PoS distinction between properand common nouns, as in the Penn Treebank, andhence we give precedence to TnT outputs for to-kens tagged as nominal by both taggers.
Finally,for the small number of cases where we cannot es-tablish a one-to-one alignment from an element inour own tokenization to a GENIA token, we rely onTnT annotation only.
In the merging of annotationsacross components, and also in downstream pro-cessing we have found it most convenient to op-erate predominantly in terms of characterization,i.e.
sub-strings of the raw input that need not alignperfectly with token boundaries.2.3 Dependency Parsing with LFG FeaturesFor syntactic parsing we employ a data-driven de-pendency parser which incorporates the predic-tions from a large-scale LFG grammar.
A tech-nique of parser stacking is employed, which en-ables a data-driven parser to learn from the out-put of another parser, in addition to gold stan-dard treebank annotations (Nivre and McDonald,2008).
This technique has been shown to pro-vide significant improvements in accuracy for bothEnglish and German (?vrelid et al, 2009), anda similar approach employing an HPSG grammarhas been shown to increase domain independencein data-driven dependency parsing (Zhang andWang, 2009).
For our purposes, we decide to use aparser which incorporates analyses from two quitedifferent parsing approaches ?
data-driven depen-dency parsing and ?deep?
parsing with a hand-crafted grammar ?
providing us with a range ofdifferent types of linguistic features which may beused in hedge detection.We employ the freely available MaltParser(Nivre et al, 2006), which is a language-independent system for data-driven dependencyparsing.2 It is based on a deterministic pars-ing strategy in combination with treebank-inducedclassifiers for predicting parse transitions.
It sup-ports a rich feature representation of the parse his-tory in order to guide parsing and may easily beextended to take into account new features of the2See http://maltparser.org.49Sentences Hedged Cues Multi-Word Tokens Cue TokensSentences CuesAbstracts 11871 2101 2659 364 309634 3056Articles 2670 519 668 84 68579 782Total 14541 2620 3327 448 378213 3838Table 2: Some descriptive figures for the shared task training data.
Token-level counts are based on thetokenization described in Section 2.1.parse history.Parser stacking The procedure to enable thedata-driven parser to learn from the grammar-driven parser is quite simple.
We parse a treebankwith the XLE platform (Crouch et al, 2008) and theEnglish grammar developed within the ParGramproject (Butt et al, 2002).
We then convert theLFG output to dependency structures, so that wehave two parallel versions of the treebank ?
onegold standard and one with LFG-annotation.
Weextend the gold standard treebank with additionalinformation from the corresponding LFG analysisand train the data-driven dependency parser on theenhanced data set.
See ?vrelid et al (2010) fordetails of the conversion and training of the parser.Table 1 shows the enhanced dependency rep-resentation of the English sentence The unknownamino acid may be used by these species, takenfrom the training data.
For each token, the parseddata contains information on the surface form,lemma, and PoS tag, as well as on the head and de-pendency relation in columns 6 and 7.
The depen-dency analysis suggested by XLE is contained incolumns 8 and 9, whereas additional XLE informa-tion, such as morphosyntactic properties like num-ber and voice, as well as more semantic propertiesdetailing, e.g., subcategorization frames, seman-tic conceptual categories such as human, time andlocation, etc., resides in the FEATS column.
Theparser outputs, which in turn form the basis for ourscope resolution rules discussed in Section 4, alsotake this same form.The parser employed in this work is trainedon the Wall Street Journal sections 2 ?
24 of thePenn Treebank, converted to dependency format(Johansson and Nugues, 2007) and extended withXLE features, as described above.
Parsing is per-formed using the arc-eager mode of MaltParser(Nivre, 2003) and an SVM with a polynomial ker-nel.
When tested using 10-fold cross-validation onthis data set, the parser achieves a labeled accuracyscore of 89.8 (?vrelid et al, 2010).3 Identifying Hedge CuesFor the task of identifying hedge cues, we devel-oped a binary maximum entropy (MaxEnt) classi-fier.
The identification of cue words is used for (i)classifying sentences as certain/uncertain (Task 1),and (ii) providing input to the syntactic rules thatwe later apply for resolving the in-sentence scopeof the cues (Task 2).
We also report evaluationscores for the sub-task of cue detection in isola-tion.As annotated in the training data, it is possiblefor a hedge cue to span multiple tokens, e.g.
as inwhether or not.
The majority of the multi-wordcues in the training data are very infrequent, how-ever, most occurring only once, and the classifieritself is not sensitive to the notion of multi-wordcues.
A given word token in the training data issimply considered to be either a cue or a non-cue,depending on whether it falls within the span of acue annotation.
The task of determining whethera cue word forms part of a larger multi-word cue,is performed by a separate post-processing step,further described in Section 3.2.3.1 Maximum Entropy ClassificationIn the MaxEnt framework, each training exam-ple ?
in our case a paired word and label ?wi, yi??
is represented as a feature vector f(wi, yi) =fi ?
<d.
Each dimension or feature function fijcan encode arbitrary properties of the data.
Theparticular feature functions we are using for thecue identification are described under Section 3.4below.
For model estimation we use the TADM3software (Malouf, 2002).
For feature extractionand model tuning, we build on the experimen-tation environment developed by Velldal (2008)(in turn extending earlier work by Oepen et al3Toolkit for Advanced Discriminative Modeling; avail-able from http://tadm.sourceforge.net/.50(2004)).
Among other things, its highly optimizedfeature handling ?
where the potentially expen-sive feature extraction step is performed only onceand then combined with several levels of featurecaching ?
make it computationally feasible to per-form large-scale ?grid searches?
over different con-figurations of features and model parameters whenusing many millions of features.3.2 Multi-Word CuesAfter applying the classifier, a separate post-processing step aims to determine whether tokensidentified as cue words belong to a larger multi-word cue.
For example, when the classifier hasalready identified one or more of the tokens in aphrase such as raises the possibility to be part of ahedge cue, a heuristic rule (viz.
basically lemma-level pattern-matching, targeted at only the mostfrequently occurring multi-word cues in the train-ing data) makes sure that the tokens are treated aspart of one and the same cue.3.3 Model Development, Data Sets andEvaluation MeasuresWhile the training data made available for theshared task consisted of both abstracts and fullarticles from the BioScope corpus (Vincze et al,2008), the test data were pre-announced to consistof biomedical articles only.
In order to make thetesting situation during development as similar aspossible to what could be expected for the held-outtesting, we only tested on sentences taken from thearticles part of the training data.
When developingthe classifiers we performed 10-fold training andtesting over the articles, while always including allsentences from the abstracts in the training set aswell.
Table 2 provides some basic descriptive fig-ures summarizing the training data.As can be seen in Table 3, we will be report-ing precision, recall and F-scores for three dif-ferent levels of evaluation for the cue classifiers:the sentence-level, token-level and cue-level.
Thesentence-level scores correspond to Task 1 of theshared task, i.e.
correctly identifying sentences asbeing certain or uncertain.
A sentence is labeleduncertain if it contains at least one token classi-fied as a hedge cue.
The token-level scores indi-cate how well the classifiers succeed in identify-ing individual cue words (this score does not takeinto account the heuristic post-processing rules forfinding multi-word cues).
Finally, the cue-levelscores are based on the exact-match counts for full707580859010  20  30  40  50  60  70  80  90  100Token level F1Sentence level F1Figure 1: Learning curves showing, for bothtoken- and sentence-level F-scores, the effect ofincrementally including a larger percentage oftraining data into the 10-fold cycles.
(As describedalso for the other development results, while weare training on both the articles and the abstracts,we are testing only on the articles.
)hedge cues (possibly spanning multiple tokens).These latter scores are computed using the officialshared task scorer script.3.4 Feature TypesWe trained cue classifiers using a wide vari-ety of feature types, both syntactic and surface-oriented.
However, to better assess the contri-bution of the different features, we first trainedtwo baseline models using only features definedfor non-normalized surface forms as they occur inthe training data.
The most basic baseline model(Baseline 1) included only unigram features.
Thebehavior of this classifier is similar to what wewould expect from simply compiling a list of cuewords from the training data, based on the major-ity usage of each word as cue or non-cue.
Base-line 2 additionally included 2 words to the left and3 to the right of the focus word (after first perform-ing a search for the optimal spans of n-grams upto 5).
As shown in Table 3, this model achieveda sentence-level F1 of 87.14 and a token-level F1of 81.97.
The corresponding scores for Baseline 1are 79.20 and 69.59.A general goal in our approach to hedge analy-sis is to evaluate the contribution of syntactic in-formation, both in cue detection and scope resolu-tion.
After applying the parser described in Sec-tion 2.3, we extracted a range of classifier featureson the basis of the dependency structures (both as51proposed by the stacked MaltParser and convertedfrom XLE) as well as the deep grammar (XLE).
Ad-ditionally we defined various features on the basisof base forms and PoS information provided by theGENIA pre-processing (see Section 2.2).For a quick overview, the feature types we ex-perimented with include the following:GENIA features n-gram features over the baseforms and PoS tags from the GENIA informationdescribed in Section 2.2.Dependency features A range of features ex-tracted from dependency structures produced byMaltParser and XLE (see Section 2.3), designedto capture the syntactic properties and environ-ment of a token: deprel ?
dependency rela-tion (Malt and XLE), deppath ?
dependencypath to root, deppattern ?
ordered set of co-dependents/siblings, including focus token (Malt),lextriple/postriple ?
lexicalized and unlexicalizeddependency triplet for token (Malt), coord ?
bi-nary feature expressing coordination (XLE), co-ordLevel ?
phrase-structural level of coordination(XLE).Lexical parser features Other features con-structed on the basis of the parser output: subcat?
subcategorization frame for verbs (XLE), adv-Type ?
type of adverbial, e.g.
sentence, VP (XLE),adjType ?
adjectival function, e.g.
attributive vs.predicative (XLE)When added to Baseline 2 in isolation, most ofthese features resulted in a boost in classifier per-formance.
For the dependency-based features, thecontribution was more pronounced for lexicalizedversions of the features.
This also points to thefact that lexical information seems to be the keyfor the task of cue identification, where the modelusing only n-grams over surface forms proved astrong baseline.
As more feature types were addedto the classifier together, we also saw a clear trendof diminishing returns, in that many of the fea-tures seemed to contribute overlapping informa-tion.
After several rounds of grid-search over dif-ferent feature configurations, the best-performingclassifier (as used for the shared task) used onlythe following feature types: n-grams over surfaceforms (including up to 2 tokens to the right), n-grams over base forms (up to 3 tokens left andright), PoS of the target word, ?subcat?, ?coord?,and ?coordLevel?.
The ?subcat?
feature containsinformation taken from XLE regarding the subcat-egorization requirements of a verb in a specificcontext, e.g., whether a verb is modal, takes anexpletive subject etc., whereas the coordinationfeatures signal coordination (?coord?)
and detailthe phrase-structural level of coordination (?co-ordLevel?
), e.g., NP, VP, etc.
This defines the fea-ture set used for the model referred to as final inTable 3.Recall that for Baseline 2, the F-score is 87.14for the sentence-level evaluation and 81.97 for thetoken-level.
For our best and final feature config-uration, the corresponding F-scores are 89.00 and83.42, respectively.
At both the sentence-level andthe token-level, the differences in classifier per-formance were found to be statistically significantat p < 0.005, using a two-tailed sign-test.
Af-ter also applying the heuristic rules for detectingmulti-word cues, the cue-level F-score for our finalmodel is 84.60, compared to 82.83 for Baseline 2.3.5 The Effect of Data SizeIn order to asses the effect of the size of the train-ing set, we computed learning curves showinghow classifier performance changes as more train-ing data is added.
Starting with only 10% of thetraining data included in the 10-fold cycle, Fig-ure 1 shows the effect on both token level andsentence-level F-scores as we incrementally in-clude larger portions of the available training data.Unsurprisingly, we see that the performance ofthe classifier is steadily improving up to the pointwhere 100% of the data is included, and by extrap-olating from the curves shown in Figure 1 it seemsreasonable to assume that this improvement wouldcontinue if more data were available.
We there-fore tried to further increase the size of the trainingset by also using the hedge-annotated clinical re-ports that form part of the BioScope corpus.
Thisprovided us with an additional 855 hedged sen-tences.
However, the classifiers did not seem ableto benefit from the additional training examples,and across several feature configurations perfor-mance was found to be consistently lower (thoughnot significantly so).
The reason is probably thatthe type of text is quite different ?
the clinical re-ports have a high ratio of fragments and also showsother patterns of cue usage, being somewhat morejargon-based.
This seems to underpin the findingsof previous studies that hedge cue learners appearquite sensitive to text type (Morante and Daele-52Sentence Level Token Level Cue LevelModel Prec Rec F1 Prec Rec F1 Prec Rec F1Baseline 1 79.25 79.45 79.20 77.71 63.41 69.59 77.37 71.70 74.43Baseline 2 86.83 87.54 87.14 86.86 77.69 81.97 85.34 80.21 82.69Final 91.39 86.78 89.00 91.20 76.95 83.42 90.18 79.47 84.49Table 3: Averaged 10-fold cross-validation results on the articles in the official shared task training data,always including the abstracts in the training portion.
The model listed as final includes features suchas n-grams over surface forms and base forms (both left and right), PoS, subcategorization frames, andphrase-structural coordination level.
The feature types are further described in Section 3.4.PoS Description SourceCC Coordinations scope over their conjuncts MIN Prepositions scope over their arguments with its descendants MJJattr Attributive adjectives scope over their nominal head and its descendants MJJpred Predicative adjectives scope over referential subjects and clausal arguments, if any M, XMD Modals inherit subj-scope from their lexical verb and scope over their descendants M, XRB Adverbs scope over their heads with its descendants MVBpass Passive verbs scope over referential subjects and the verbal descendants M, XVBrais Raising verbs scope over referential subjects and the verbal descendants M, X* For multi-word cues, the head determines scope for all elements* Back off from final punctuation and parenthesesTable 4: Overview of dependency-based scope rules with information source (MaltParser or XLE), orga-nized by PoS of the cue.mans, 2009).4 Resolving Cue ScopeIn our approach to scope resolution we rely heav-ily on syntactic information, taken from the depen-dency structures proposed by both MaltParser andXLE, as well as various additional features fromthe XLE parses relating to specific syntactic con-structions.4.1 Scope RulesWe construct a small set of heuristic rules whichdefine the scope for each cue detected in Stage1.
In the construction of these rules, we made useof the information provided by the guidelines forscope annotation in the BioScope corpus (Vinczeet al, 2008) as well as manual inspection of thetraining data in order to arrive at reasonable scopehypotheses for various types of cues.The rules take as input a parsed sentence whichhas been tagged with hedge cues and operate overthe dependency structures and additional featuresprovided by the parser.
Default scope is set tostart at the cue word and span to the end of thesentence (not including final puctuation), and thisscope also provides the baseline for the evaluationof our rules.
Table 4 provides an overview of therules employed for scope resolution.In the case of multi-word cues, such as indicatethat, and either ... or, which share scope, we needto determine the head of the multi-word unit.
Wethen set the scope of the whole unit to the scope ofthe head token.As an example, the application of the rules inTable 4 to the sentence with the parsed outputin Table 1 correctly determine the scope of thecue may as shown in example (1), using a varietyof syntactic cues regarding part-of-speech, argu-menthood, voice, etc.
First, the scope of the sen-tence is set to default scope.
Then the MD rule isapplied, which checks the properties of the lexicalverb used, located through a chain of verbal de-pendents from the modal verb.
Since it is passive(passive:+), initial scope is set to include thecue?s subject (SBJ) argument with all its descen-dants (The unknown amino acid).53Task 1 Task 2 Cue DetectionPrec Rec F1 Prec Rec F1 Prec Rec F185.48 84.94 85.21 56.71 54.02 55.33 81.20 76.31 78.68Table 6: Evaluation results for the official held-out testing.Scope Prec Rec F1Default w/gold cues 45.21 45.21 45.21Rules w/gold cues 72.31 72.31 72.31Rules w/classified cues 68.56 61.38 64.77Table 5: Evaluation of the scope resolution ruleson the training articles, using both gold standardcues and predicted cues.
For the row labeled De-fault, the scope for each cue is always taken tospan rightward to the end of the sentence.
In therows labeled Rules, the scopes have been resolvedusing the dependency-based rules.
(1) (The unknown amino acid <may> be usedby these species).4.2 Rule EvaluationTable 5 shows the evaluation of the set of scoperules on the articles section of the data set, usinggold standard cues.4 This gives us an indication ofthe performance of the rules, isolated from errorsin cue detection.First of all, we may note that the baseline isa strong one: choosing to extend the scope of acue to the end of the sentence provides an F-scoreof 45.21.
Given gold standard cue information,the set of scope rules improves on the baseline by27 percentage points on the articles section of thedata set, giving us an F-score of 72.31.
Comparingto the evaluation using classified cues (the bottomrow of Table 5), we find that the use of automati-cally assigned cues causes a drop in performanceof 7.5 percentage points, to a result of 64.77.5 Held-Out TestingTable 6 presents the final results as obtained onthe held-out test data, which constitute the official4This evaluation was carried out using the official scorerscript of the CoNLL shared task.
When cue information iskept constant, as in our case, the values for false positivesand false negatives will be identical, hence the precision andrecall values will always be identical as well.results for our system in the CoNLL-2010 sharedtask.
The held-out test set comprises biomedicalarticles with a total of 5003 sentences (790 of themhedged).For Task 1 we obtain an F-score of 85.21.
Thecorresponding result for the training data, which isreported as ?Sentence Level?
in Table 3, is 89.00.Although we experience a slight drop in perfor-mance (3.8 percentage points), the system seemsto generalize quite well to unseen data when itcomes to the detection of sentence-level uncer-tainty.For Task 2, the result on the held-out data set isan F-score of 55.33, with quite balanced values forprecision and recall, 56.7 and 54.0, respectively.
Ifwe compare this to the end-to-end evaluation onthe training data, provided in the bottom row ofTable 5, we find a somewhat larger drop in perfor-mance (9.5 percentage points), from an F-score of64.77 to the held-out 55.3.
There are several pos-sible reasons for this drop.
First of all, there mightbe a certain degree of overfitting of our system tothe training data.
The held-out data may containhedging constructions that are not covered by ourset of scope rules.
Moreover, the performance ofthe scope rules is also influenced by the cue de-tection, which is reported in the final columns ofTable 6.
The cue-level performance of our systemon the held-out data set is 78.68, whereas the sameevaluation on the training data is 84.49.
We findthat it is the precision, in particular, which suffersin the application to the held-out data set.
A pos-sible strategy for future work is to optimize bothcomponents of the Task 2 system, the cue detec-tion and the scope rules, on the entire training set,instead of just on the articles.6 Conclusions ?
OutlookWe have described a hybrid, two-level approachfor resolving hedging in biomedical text, as sub-mitted for the stricter track of ?closed?
or ?in-domain?
systems in the CoNLL-2010 shared task.For the task of identifying hedge cues, we traina MaxEnt classifier, which, for the held-out test54data, achieves an F-score of 78.68 on the cue-leveland 85.21 on the sentence-level (Task 1).
For thetask of resolving the in-sentence scope of the iden-tified cues (Task 2), we apply a set of manuallycrafted rules operating on dependency representa-tions, resulting in an end-to-end F-score of 55.33(based on exact match of both cues and scopes).
Inthe official shared task ranking of results, and con-sidering systems in all tracks together, our systemis ranked 4 out of 24 for Task 1 and 3 out of 15 forTask 2, resulting in the highest average rank over-all.
For future work we aim to further improve thecue detection, in particular with respect to multi-word cues, and also continue to refine the scoperules.
Instead of defining the scopal rules only atthe level of dependency structure, one could alsohave rules operating on constituent structure ?
per-haps even combining alternative resolution candi-dates using a statistical ranker.ReferencesThorsten Brants.
2000.
TnT.
A statistical Part-of-Speech tagger.
In Proceedings of the Sixth Con-ference on Applied Natural Language Processing,pages 224?231.Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-roshi Masuichi, and Christian Rohrer.
2002.
TheParallel Grammar Project.
In Proceedings of COL-ING Workshop on Grammar Engineering and Eval-uation, pages 1?7.Dick Crouch, Mary Dalrymple, Ron Kaplan, TracyKing, John Maxwell, and Paula Newman.
2008.XLE documentation.
Palo Alto Research Center.Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, J?nosCsirik, and Gy?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to Detect Hedges andtheir Scope in Natural Language Text.
In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning (CoNLL-2010): SharedTask, pages 1?12.Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering, 6 (1):15?28.Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion forEnglish.
In Joakim Nivre, Heiki-Jaan Kaalep,and Mare Koit, editors, Proceedings of NODALIDA2007, pages 105?112.Robert Malouf.
2002.
A comparison of algorithmsfor maximum entropy parameter estimation.
In Pro-ceedings of the 6th Conference on Natural LanguageLearning, pages 49?55.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English.
The Penn Treebank.
Computa-tional Linguistics, 19:313?330.Roser Morante and Walter Daelemans.
2009.
Learn-ing the scope of hedge cues in biomedical texts.
InProceedings of the BioNLP 2009 Workshop, pages28?36.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proceedings of the 46th Meeting of theAssociation for Computational Linguistics, pages950?958.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.MaltParser: A data-driven parser-generator for de-pendency parsing.
In Proceedings of the Fifth In-ternational Conference on Language Resources andEvaluation, pages 2216?2219.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of theEighth International Workshop on Parsing Tech-nologies, pages 149?160.Stephan Oepen, Daniel Flickinger, Kristina Toutanova,and Christopher D. Manning.
2004.
LinGO Red-woods.
A rich and dynamic treebank for HPSG.Journal of Research on Language and Computation,2(4):575?596.Lilja ?vrelid, Jonas Kuhn, and Kathrin Spreyer.
2009.Improving data-driven dependency parsing usinglarge-scale LFG grammars.
In Proceedings of the47th Meeting of the Association for ComputationalLinguistics, pages 37?40.Lilja ?vrelid, Jonas Kuhn, and Kathrin Spreyer.
2010.Cross-framework parser stacking for data-driven de-pendency parsing.
TAL 2010 special issue on Ma-chine Learning for NLP, 50(3).Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,and Jun?ichi Tsujii.
2005.
Developing a robust Part-of-Speech tagger for biomedical text.
In Advancesin Informatics, pages 382?392.
Springer, Berlin,Germany.Erik Velldal.
2008.
Empirical Realization Ranking.Ph.D.
thesis, University of Oslo, Institute of Infor-matics, Oslo.Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas,Gy?rgy M?ra, and J?nos Csirik.
2008.
The Bio-Scope corpus: Annotation for negation, uncertaintyand their scope in biomedical texts.
In Proceedingsof the BioNLP 2008 Workshop.Yi Zhang and Rui Wang.
2009.
Cross-domain depen-dency parsing using a deep linguistic grammar.
InProceedings of the 47th Meeting of the Associationfor Computational Linguistics, Singapore.55
