Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 466?476,Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational LinguisticsAn Empirical Comparison of Features and Tuningfor Phrase-based Machine TranslationSpence Green, Daniel Cer, and Christopher D. ManningComputer Science Department, Stanford University{spenceg,danielcer,manning}@stanford.eduAbstractScalable discriminative training methodsare now broadly available for estimatingphrase-based, feature-rich translation mod-els.
However, the sparse feature sets typi-cally appearing in research evaluations areless attractive than standard dense featuressuch as language and translation modelprobabilities: they often overfit, do not gen-eralize, or require complex and slow fea-ture extractors.
This paper introduces ex-tended features, which are more specificthan dense features yet more general thanlexicalized sparse features.
Large-scale ex-periments show that extended features yieldrobust BLEU gains for both Arabic-English(+1.05) and Chinese-English (+0.67) rel-ative to a strong feature-rich baseline.
Wealso specialize the feature set to specificdata domains, identify an objective functionthat is less prone to overfitting, and releasefast, scalable, and language-independenttools for implementing the features.1 IntroductionScalable discriminative algorithm design for ma-chine translation (MT) has lately been a boomingenterprise.
There are now algorithms for every taste:probabilistic and distribution-free, online and batch,regularized and unregularized.
Technical differ-ences aside, the papers that apply these algorithmsto phrase-based translation often share a curiousempirical characteristic: the algorithms support ex-tra features, but the features do not significantlyimprove translation.
For example, Hopkins andMay (2011) showed that PRO with some simple adhoc features only exceeds the baseline on one ofthree language pairs.
Gimpel and Smith (2012b)observed a similar result for both PRO and theirramp-loss algorithm.
Cherry and Foster (2012)found that, at least in the batch case, many algo-rithms produce similar results, and features onlysignificantly increased quality for one of three lan-guage pairs.
Only recently did Cherry (2013) andGreen et al.
(2013b) identify certain features thatconsistently reduce error.These empirical results suggest that feature de-sign and model fitting, the subjects of this paper,warrant a closer look.
We introduce an effectiveextended feature set for phrase-based MT and iden-tify a loss function that is less prone to overfitting.Extended features share three attractive characteris-tics with the standard Moses dense features (Koehnet al., 2007): ease of implementation, language in-dependence, and independence from ancillary cor-pora like treebanks.
In our experiments, they donot overfit and can be extracted efficiently duringdecoding.
Because all feature weights are tunedon the development set, the new feature templatesare amenable to feature augmentation (Daum?
III,2007), a simple domain adaptation technique thatwe show works surprisingly well for MT.Extended features are designed according to aprinciple rather than a rule: they should fire lessthan standard dense features, which are general, butmore than so-called sparse features, which are veryspecific?they are usually lexicalized?and thusprone to overfitting.
This principle is motivatedby analysis, which shows how expressive modelscan be a mixed blessing in the translation setting.It is obvious that features allow the model to fitthe tuning data more tightly.
For example, sparselexicalized features could reduce tuning error bylearning that the references prefer U.S. over UnitedStates, a minor lexical distinction.
Reference choiceshould matter more than in the dense case, an issuethat we quantify.
We also show that frequency cut-offs, which are a crude but common form of featureselection, are unnecessary and even detrimentalwhen features follow this principle.We report large-scale translation quality experi-ments relative to both dense and feature-rich base-lines.
Our best feature set, which includes domainadaptation features, yields an average+1.05 BLEUimprovement for Arabic-English and +0.67 for466Chinese-English.
In addition to the extended fea-ture set, we show that an online variant of expectederror (Och, 2003) is significantly faster to compute,less prone to overfitting, and nearly as effective as apairwise loss.
We release all software?feature ex-tractors, and fast word clustering and data selectionpackages?used in our experiments.12 Phrase-based Models and LearningThe log-linear approach to phrase-based translation(Och and Ney, 2004) directly models the predictivetranslation distributionp(e|f ;w) =1Z(f)exp[w>?
(e, f)](1)where e is the target string, f is the source string,w ?
Rdis the vector of model parameters, ?(?)
?Rdis a feature map, and Z(f) is an appropriatenormalizing constant.
Assume that there is also afunction ?
(e, f) ?
Rdthat produces a recombina-tion map for the features.
That is, each coordinatein ?
represents the state of the corresponding co-ordinate in ?.
For example, suppose that ?jis thelog probability produced by the n-gram languagemodel (LM).
Then ?jwould be the appropriate LMhistory.
Recall that recombination collapses deriva-tions with equivalent recombination maps duringsearch and thus affects learning.
This issue signifi-cantly influences feature design.To learn w, we follow the online procedure ofGreen et al.
(2013b), who calculate gradient stepswith AdaGrad (Duchi et al., 2011) and perform fea-ture selection via L1regularization in the FOBOS(Duchi and Singer, 2009) framework.
This proce-dure accommodates any loss function for which asubgradient can be computed.
Green et al.
(2013b)used a PRO objective (Hopkins and May, 2011)with a logistic (surrogate) loss function.
However,later results showed overfitting (Green et al., 2013a),and we found that their online variant of PRO tendsto produce short translations like its batch counter-part (Nakov et al., 2013).
Moreover, PRO requiressampling, making it slow to compute.To address these shortcomings, we explore anonline variant of expected error (Och, 2003, Eq.7).Let Et= {ei}ni=1be a scored n-best list of trans-lations at time step t for source input ft. Let G(e)be a gold error metric that evaluates each candi-date translation with respect to a set of one or more1http://nlp.stanford.edu/software/phrasalreferences.
The smooth loss function is`t(wt?1) = Ep(e|ft;wt?1)[G(e)]=1Z?e??Etexp(w>?
(e?, f))?G(e?
)(2)with normalization constant Z =?e??Etexp(w>?
(e?, f)).
The gradient gtfor coordinate j is:gt= E[G(e)?j(e, ft)]?E[G(e)]E[?j(e, ft)] (3)To our knowledge, we are the first to experimentwith the online version of this loss.2When G(e) issentence-level BLEU+1 (Lin and Och, 2004)?thesetting in our experiments?this loss is also knownas expected BLEU (Cherry and Foster, 2012).
How-ever, other metrics are possible.3 Extended Phrase-based FeaturesWe divide our feature templates into five categories,which are well-known sources of error in phrase-based translation.
The features are defined overderivations d = {ri}Di=1, which are ordered se-quences of rules r from the translation model.
De-fine functions f(?)
to be the source string of a ruleor derivation and e(?)
to be the target string.
Localfeatures can be extracted from individual rules anddo not declare any state in the recombination map,thus for all local features i we have ?i= 0.
Non-local features are defined over partial derivationsand declare some state, either a real-valued param-eter or an index indicating a categorical value likean n-gram context.For each language, the extended feature tem-plates require unigram counts and a word-to-classmapping ?
: w 7?
c for word w ?
V and classc ?
C. These can be extracted from any monolin-gual data; our experiments simply use both sides ofthe unaligned parallel training data.The features are language-independent, but wewill use Arabic-English as a running example.3.1 Lexical ChoiceLexical choice features make more specific distinc-tions between target words than the dense transla-tion model features (Koehn et al., 2003).2Gao and He (2013) used stochastic gradient descent andexpected BLEU to learn phrase table feature weights, but notthe full translation model w.467Lexicalized rule indicator (Liang et al., 2006a)Some rules occur frequently enough that we canlearn rule-specific weights that augment the densetranslation model features.
For example, our modellearns the following rule indicator features andweights:H.AJ.?@?
reasons -0.022H.AJ.?@?
reasons for 0.002H.AJ.?@?
the reasons for 0.016These translations are all correct depending on con-text.
When the plural noun H.AJ.?
@ ?reasons?
appearsin a construct state (iDafa) the preposition for isunrealized.
Moreover, depending on the context,the English translation might also require the deter-miner the, which is also unrealized.
The weightsreflect that H.AJ.?
@ ?reasons?
often appears in con-struct and boost insertion of necessary target terms.To prevent overfitting, this template only fires anindicator for rules that occur more than 50 timesin the parallel training data (this is different fromfrequency filtering on the tuning data; see section6.1).
The feature is local.Class-based rule indicator Word classes ab-stract over lexical items.
For each rule r, a pro-totype that abstracts over many rules can be builtby concatenating {?
(w) : w ?
f(r)} with{?
(w) : w ?
e(r)}.
For example, supposethat Arabic class 492 consists primarily of Arabicpresent tense verbs and class 59 contains Englishauxiliaries.
Then the model might penalize a ruleprototype like 492>59_59, which drops the verb.This template fires an indicator for each rule proto-type and is local.Target unigram class (Ammar et al., 2013) Tar-get lexical items with similar syntactic and semanticproperties may have very different frequencies inthe training data.
These frequencies will influencethe dense features.
For example, in one of our En-glish class mappings the following words map tothe same class:word class freq.surface-to-surface 0 269air-to-air 0 98ground-to-air 0 63The classes capture common linguistic attributes ofthese words, which is the motivation for a full class-based LM.
Learning unigram weights directly issurprisingly effective and does not require buildinganother LM.
This template fires a separate indicatorfor each class {?
(w) : w ?
e(r)} and is local.3.2 Word AlignmentsWord alignment features allow the model to recog-nize fine-grained phrase-internal information thatis largely opaque in the dense model.Lexicalized alignments (Liang et al., 2006a)Consider the internal alignments of the rule:sunday ,?
?K1YgB@ 2Alignment 1 ???K?day??
,?
is incorrect and align-ment 2 is correct.
The dense translation modelfeatures might assign this rule high probability ifalignment 1 is a common alignment error.
Lexical-ized alignment features allow the model to compen-sate for these events.
This feature fires an indicatorfor each alignment in a rule?including multiwordcliques?and is local.Class-based alignments Like the class-basedrule indicator, this feature template replaces eachlexical itemwith its word class, resulting in an align-ment prototype.
This feature fires an indicator foreach alignment in a rule after mapping lexical itemsto classes.
It is local.Source class deletion Phrase extraction algo-rithms often use a ?grow?
symmetrization step (Ochand Ney, 2003) to add alignment points.
Sometimesthis procedure can produce a rule that deletes im-portant source content words.
This feature templateallows the model to penalize these rules by firingan indicator for the class of each unaligned sourceword.
The feature is local.Punctuation ratio Languages use different typesand ratios of punctuation (Salton, 1958).
For ex-ample, quotation marks are not commonly used inArabic, but they are conventional in English.
Fur-thermore, spurious alignments often contain punc-tuation.
To control these two phenomena, this fea-ture template returns the ratio of target punctuationtokens to source punctuation tokens for each deriva-tion.
Since the denominator is constant, this featurecan be computed incrementally as a derivation isconstructed.
It is local.Function word ratio Words can also be spuri-ously aligned to non-punctuation, non-digit func-tion words such as determiners and particles.
Fur-thermore, linguistic differences may account for468differences in function word occurrences.
For ex-ample, English has a broad array of modal verbsand auxiliaries not found in Arabic.
This featuretemplate takes the 25 most frequent words in eachlanguage (according to the unigram counts), andcomputes the ratio between target and source func-tion words for each derivation.
As before the de-nominator is constant, so the feature can be com-puted efficiently.
It is local.3.3 Phrase BoundariesThe LM and hierarchical reordering model are theonly dense features that cross phrase boundaries.Target-class bigramboundary Wehave alreadyadded target class unigrams.
We find that both lexi-calized and class-based bigrams cause overfitting,therefore we restrict to bigrams that straddle phraseboundaries.
The feature template fires an indicatorfor the concatenation of the word classes on eitherside of each boundary.
This feature is non-localand its recombination state ?
is the word class atthe right edge of the partial derivation.3.4 Derivation QualityTo satisfy strong features like the LM, or hard con-straints like the distortion limit, the phrase-basedmodel can build derivations from poor translationrules.
For example, a derivation consisting mostlyof unigram rules may miss idiomatic usage thatlarger rules can capture.
All of these feature tem-plates are local.Source dimension (Hopkins and May, 2011) Anindicator feature for the source dimension of therule: |f(r)|.Target dimension (Hopkins and May, 2011) Anindicator for the target dimension: |e(r)|.Rule shape (Hopkins and May, 2011) Theconjunction of source and target dimension:|f(r)|_|e(r)|.3.5 ReorderingLexicalized reordering models score the orientationof a rule in an alignment grid.
We use the samebaseline feature extractor as Moses, which has threeclasses: monotone, swap, and discontinuous.
Wealso add the non-monotone class, which is a con-junction of swap and discontinuous, for a total ofeight orientations.33Each class has ?with-previous?
and ?with-next?
special-izations.Algorithm (implementation) #threads TimeBrown (wcluster) 1 1023.39Clark (cluster_neyessen) 1 890.11Och (mkcls) 1 199.04PredictiveFull (this paper) 8 3.27Predictive (this paper) 8 2.42Table 1: Wallclock time (min.sec) to generate amapping from a vocabulary of 63k English words(3.7M tokens) to 512 classes.
All experiments wererun on the same server, which had eight physicalcores.
Our Java implementation is multi-threaded;the C++ baselines are single-threaded.Lexicalized rule orientation (Liang et al.,2006a) For each rule, the template fires an indi-cator for the concatenation of the orientation class,each element in f(r), and each element in e(r).
Toprevent overfitting, this template only fires for rulesthat occur more than 50 times in the training data.The feature is non-local and its recombination state?
is the rule orientation.Class-based rule orientation For each rule, thetemplate fires an indicator for the concatenationof the orientation class, each element in {?
(w) :w ?
f(r)}, and each element in {?
(w) : w ?e(r)}.
The feature is non-local and its recombina-tion state ?
is the rule orientation.Signed linear distortion The dense feature setincludes a simple reordering cost model.
Assumethat [r] returns the index of the leftmost source indexin f(d) and [[r]] returns the rightmost index.
Thenthe linear distortion is:?
= [r1] +D?i=2|[[ri?1]] + 1?
[ri]| (4)This score does not distinguish between left andright distortion.
To correct this issue, this featuretemplate fires an indicator for each signed com-ponent in the sum, for each positive and negativecomponent.
The feature is non-local and its recom-bination state ?
is the signed distortion.3.6 Feature DependenciesWhile unigram counts are trivial to compute, thesame is not necessarily true of the word-to-classmapping ?.
Standard algorithms run in O(n2),where n = |V |.
Table 1 shows an evaluation ofstandard implementations of several popular algo-rithms: Brown et al.
(1992) implemented by Liang469(2005); Clark (2003) without the morphologicalprior, which increases training time dramatically;and the implementation of Och (1999) that comeswith the GIZA++ word aligner.
The latter hasbeen used recently for MT features (Ammar et al.,2013; Cherry, 2013; Yu et al., 2013).
In a broadsurvey, Christodoulopoulos et al.
(2010) found thatfor several downstream tasks, most word clusteringalgorithms?including Brown and Clark?result insimilar task accuracy.
For our large-scale setting,the primary issue is then the time to estimate ?.For large corpora the existing implementationsmay require days or weeks, making our feature setless practical than the traditional dense MT features.Consequently, we re-implemented the predictiveone-sided class model of Whittaker and Woodland(2001) with the parallelized clustering algorithm ofUszkoreit and Brants (2008) (Predictive), whichwas originally developed for very large scale lan-guage modeling.
Our implementation uses multiplethreads on a single processor instead ofMapReduce.We also added two extensions that are useful fortranslation features.
First, we map all digits to 0.This reduces sparsity while retaining useful patternssuch as 0000 (e.g., years) and 0th (e.g., ordinals).Second, we mapped all words occurring fewer than?
times to an <unk> token.
In our experiment,these two changes reduce the vocabulary size by71.1%.
They also make the mapping ?
more ro-bust to unseen events during translation decoding.For a conservative comparison to the other threealgorithms, we include results without these twoextensions (PredictiveFull).44 Domain Adaptation FeaturesFeature augmentation is a simple yet effective do-main adaptation technique (Daum?
III, 2007).
Sup-pose that the source data comes fromM domains.Then for each original feature ?i, we addM addi-tional features, one for each domain.
The originalfeature ?ican be interpreted as a prior over theMdomains (Finkel and Manning, 2009, fn.2).Most of the extended features are defined overrules, so the critical issue is how to identify in-domain rules.
The trick is to know which trainingsentence pairs are in-domain.
Then we can annotateall rules extracted from these instances with domain4For the baselines the training settings are the suggesteddefaults: Brown, default; Clark, 10 iterations, frequency cutoff?
= 5; Och, 10 iterations.
Our implementation: PredictiveFull,30 iterations, ?
= 0; Predictive, 30 iterations, ?
= 5.labels.
The in-domain rule sets need not be disjointsince some rules might be useful across domains.This paper explores the following approach: wechoose one of theM domains as the default.
Next,we collect some source sentences for each of theM ?
1 remaining domains.
Using these exampleswe then identify in-domain sentence pairs in the bi-text via data selection, in our case the feature decayalgorithm (Bi?ici and Yuret, 2011).
Finally, our ruleextractor adds domain labels to all rules extractedfrom each selected sentence pair.
Crucially, theselabels do not influence which rules are extractedor how they are scored.
The resulting phrase tablecontains the same rules, but with a few additionalannotations.Our method assumes domain labels for eachsource input to be decoded.
Our experiments utilizegold, document-level labels, but accurate sentence-level domain classifiers exist (Wang et al., 2012).4.1 Augmentation of Extended FeaturesIrvine et al.
(2013) showed that lexical selection isthe most quantifiable and perhaps most commonsource of error in phrase-based domain adaptation.Our development experiments seemed to confirmthis hypothesis as augmentation of the class-basedand non-lexical (e.g., Rule shape) features did notreduce error.
Therefore, we only augment the lex-icalized features: rule indicators and orientations,and word alignments.4.2 Domain-Specific Feature TemplatesIn-domain Rule Indicator (Durrani et al., 2013)An indicator for each rule that matches the input do-main.
This template fires a generic in-domain indi-cator and a domain-specific indicator (e.g., the fea-tures might be indomain and indomain-nw).The feature is local.Adjacent Rule Indicator Indicators for adjacentin-domain rules.
This template also fires bothgeneric and domain-specific features.
The featureis non-local and the state is a boolean indicating ifthe last rule in a partial derivation is in-domain.5 ExperimentsWe evaluate and analyze our feature set under a vari-ety of large-scale experimental conditions includingmultiple domains and references.
To our knowl-edge, the only language pairs with sufficient re-search resources to support this protocol are Arabic-English (Ar-En) and Chinese-English (Zh-En).
The470Bilingual Monolingual#Seg.
#Tok.
#Tok.Ar-En 6.6M 375M990MZh-En 9.3M 538MTable 2: Bilingual and monolingual training cor-pora.
The monolingual English data comes fromthe AFP and Xinhua sections of English Gigaword4 (LDC2009T13).training corpora5come from several LinguisticData Consortium (LDC) sources from 2012 andearlier (Table 2).
The test, development, and tuningcorpora6come from the NIST OpenMT andMetric-sMATR evaluations (Table 3).
Extended featuresbenefit from more tuning data, so we concatenatedfive NIST data sets to build one large tuning set.Observe that all test data come from later epochsthan the tuning and development data.From these data we built phrase-based MT sys-tems with Phrasal (Green et al., 2014).7We alignedthe parallel corpora with the Berkeley aligner(Liang et al., 2006b) with standard settings andsymmetrized via the grow-diag heuristic.
We cre-ated separate English LMs for each language pair byconcatenating the monolingual Gigaword data withthe target-side of the respective bitexts.
For eachcorpus we estimated unfiltered 5-gram languagemodels with lmplz (Heafield et al., 2013).For each condition we ran the learning algorithmfor 25 epochs8and selected the model accordingto the maximum uncased, corpus-level BLEU-4(Papineni et al., 2002) score on the dev set.5.1 ResultsWe evaluate the new feature set relative to two base-lines.
Dense is the same baseline as Green et al.5We tokenized the English with Stanford CoreNLP ac-cording to the Penn Treebank standard (Marcus et al., 1993),the Arabic with the Stanford Arabic segmenter (Monroe etal., 2014) according to the Penn Arabic Treebank standard(Maamouri et al., 2008), and the Chinese with the StanfordChinese segmenter (Chang et al., 2008) according to the PennChinese Treebank standard (Xue et al., 2005).6Data sources: tune, MT023568; dev, MT04; dev-dom,domain adaptation dev set is MT04 and all wb and bn datafrom LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En);test2, Progress0809 which was revealed in the OpenMT 2012evaluation; test3, MetricsMATR08-10.7System settings: distortion limit of 5, cube pruning beamsize of 1200, maximum phrase length of 7.8Other learning settings: 16 threads, mini-batch size of 20;L1regularization strength ?
= 0.001; learning rate ?0= 0.02;initialization of LM to 0.5, word penalty to -1.0, and all otherdense features to 0.2; initialization of extended features to 0.0.#Seg.
#Ref.
DomainsAr-En Zh-Entune 5,604 5,900 4 nw,wb,bndev 1,075 1,597 4 nwdev-dom 2,203 2,317 1 nw,wb,bntest1 1,313 820 4 nw,wbtest2 1,378 1,370 4 nw,wbtest3 628 613 1 nw,wb,bnTable 3: Development, test, and tuning data.
Do-main abbreviations: broadcast news (bn), newswire(nw), and web (wb).
(2013b); these dense features are included in all ofthe models that follow.
Sparse is their best feature-rich model, which adds lexicalized rule indicators,alignments, orientations, and source deletions with-out bitext frequency filtering.We do not perform a full ablation study.
Boththe approximate search and the randomization ofthe order of tuning instances make the contribu-tions of each individual template differ from run torun.
Resource constraints prohibit multiple large-scale runs for each incremental feature.
Instead,we divide the extended feature set into two parts,and report large-scale results.
Ext includes all ex-tended features except for the the filtered lexicalizedfeature templates.
Ext+Filt adds those filteredlexicalized templates: rule indicators and orienta-tions, and word alignments (section 3).Table 4 shows translation quality results.
Thenew feature set significantly exceeds the baselineDense model for both language pairs.
An interest-ing result is that the new extended features alonematch the strong Sparse baseline.
The class-basedfeatures, which are more general, should clearlybe preferred to the sparse features when decodingout-of-domain data (so long as word mappings aretrained for that data).
The increased runtime periteration comes not from feature extraction but fromlarger inner products as the model size increases.Next, we add the domain features from section4.2.
We marked in-domain sentence pairs by con-catenating the tuning data with additional bn andwb monolingual in-domain data from several LDCsources.9The FDA selection size was set to 20times the number of in-domain examples for eachgenre.
Newswire was selected as the default domainsince most of the bitext comes from that domain.The bottom rows of Tables 4a and 4b compare9Catalog: LDC2007T24, LDC2008T08, LDC2008T18,LDC2012T16, LDC2013T01, LDC2013T05, LDC2013T14.471Model #features Epochs Min.
/ Epoch tune dev test1 test2 test3Dense (D) 18 24 3 49.52 50.25 47.98 43.41 27.56D+Sparse 48,597 24 8 56.51 52.98 49.55 45.40 29.02D+Ext 62,931 16 11 57.83 54.33 49.66 45.66 29.15D+Ext+Filt 94,606 17 14 59.13 55.35 50.02 46.24 29.59D+Ext+Filt+Dom 123,353 22 18 59.97 29.20?50.45 46.24 30.84(a) Ar-En.Model #features Epochs Min.
/ Epoch tune dev test1 test2 test3Dense (D) 18 17 3 32.82 34.96 26.61 26.72 10.19D+Sparse 55,024 17 8 38.91 36.68 27.86 28.41 10.98D+Ext 67,936 16 13 40.96 37.19 28.27 28.40 10.72D+Ext+Filt 100,275 17 14 41.38 37.36 28.68 28.90 11.24D+Ext+Filt+Dom 126,014 17 14 41.70 17.20?28.71 28.96 11.67(b) Zh-En.Table 4: Translation quality results (uncased BLEU-4%).
Per-epoch times are in minutes (Min.).
Statisticalsignificance relative to D+Sparse, the strongest baseline: bold (p < 0.001) and bold-italic (p < 0.05).Significance is computed by the permutation test of Riezler and Maxwell (2005).
?The dev score ofExt+Filt+Dom is the dev-dom data set from Table 3, so it is not comparable with the other rows.Ext+Filt+Dom to the baselines and other featuresets.
The gains relative to Sparse are statisticallysignificant for all six test sets.A crucial result is that with domain features accu-racy relative to Ext+Filt never decreases: a singledomain-adapted system is effective across domains.Irvine et al.
(2013) showed that when models frommultiple domains are interpolated, scoring errorsaffecting lexical selection?the model could havegenerated the correct target lexical item but didnot?increase significantly.
We do not observe thatbehavior, at least from the perspective of BLEU.Table 5 separates out per-domain results.
Theweb data appears to be the hardest domain.
That issensible given that broadcast news transcripts aremore similar to newswire, the default domain, thanweb data.
Moreover, inspection of the bitext sourcesrevealed very little web data, so our automatic dataselection is probably less effective.
Accuracy onnewswire actually increases slightly.6 Analysis6.1 LearningLoss Function In a now classic empirical com-parison of batch tuning algorithms, Cherry and Fos-ter (2012) showed that PRO and expected BLEUAr-En test1 test2 test3nw wb nw wb bn nw wbEF 59.78 39.55 51.69 38.80 30.39 37.59 20.58EFD 60.21 40.38 51.76 38.77 31.63 38.18 22.37Zh-EnEF 34.56 21.94 17.38 12.07 3.04 17.42 12.83EFD 34.87 21.82 17.96 12.66 3.01 17.74 13.80Table 5: Per-domain results (uncased BLEU-4 %).Here bold simply indicates the maximum in eachcolumn.
Model abbreviations: EF is Ext+Filt andEFD is Ext+Filt+Dom.yielded similar translation quality results.
In con-trast, Table 6a shows significant differences be-tween these loss functions.
First, expected BLEUcan be computed faster since it is linear in the n-best list size, whereas exact computation of the PROobjective is O(n2) (thus sampling is often used).
Italso converges faster.
Second, PRO tends to selectlarger models.10Finally, PRO seems to overfit onthe tuning set, since there are no gains on test1.Feature Selection A common yet crude methodof feature selection is frequency cutoffs on the10PRO L1regularization strength of ?
= 0.01, above whichmodel size decreases but translation quality degrades.472Loss #epochs Min./Epoch #feat.
tune test1EB 17 14 94,606 59.13 50.02PRO 14 25 181,542 61.20 50.09(a) PRO vs. expected BLEU (EB) for Ext+Filt.Feature Selection #features tune test1L194,606 59.13 50.02Freq.
cutoffs 23,617 56.84 49.79(b) Feature selection for Ext+Filt.Model #refs tune test1Dense 4 49.52 47.98Dense 1 49.34 47.78Ext+Filt 4 59.13 50.02Ext+Filt 1 55.39 48.88(c) Single- vs. multiple-reference tuning.Table 6: Ar-En learning comparisons.tuning data.
Only features that fire more thansome threshold are admitted into the feature set.Table 6b shows that for our new feature set, L1regularization?which simply requires setting a reg-ularization strength parameter?is more effectivethan frequency cutoffs.References FewMT data sets supply multiple ref-erences.
Even when they do, those references arebut a sample from a larger pool of possible trans-lations.
This observation has motivated attemptsat generating lattices of translations for evaluation(Dreyer and Marcu, 2012; Bojar et al., 2013).
Butevaluation is only part of the problem.
Table 6cshows that the Dense model, which has only afew features to describe the data, is little affectedby the elimination of references.
In contrast, thefeature-rich model degrades significantly.
This mayaccount for the underperformance of features insingle-reference settings like WMT (Durrani et al.,2013; Green et al., 2013a).
The next section ex-plores the impact of references further.6.2 Reference VarianceWe took the Dense Ar-En output for the devdata, which has four references, and computed thesentence-level BLEU+1 with respect to each refer-ence.
Figure 1a shows a point for each of the 1,075translations.
The horizontal axis is the minimumscore with respect to any reference and the verti-cal axis is the maximum (BLEU has a maximumvalue of 1.0).
Ideally, from the perspective of learn-l llll llllllllllllllllllllllllllllllllllllll ll lllllllllllll lllllllllll llllllllllllllllllllllllll llllllllllllllllll lllll ll lllllllllllllllllllllllllllllllllllllllll lllllllllllll lllllll lllllllllllllllllllllll llllllllllllllll lll llllllllll ll llllllllllllllllll lllllllllll llllll llll llllllll lllllllllllllll lllll lllllllll lllllllllllllllllllllllllllllllllllllllllllllllllllllllll llllllllllllll lllll llllll llllllll llllll ll lllllll lllllllll lllllllllllllllllllllll llllllllllllllllll lllllllllllllllllllllllllllllllllllllll llllllllllllllllllllllllll llllllllllllllllllllllllllllllllllllllllllllll llllllllll lll llllllllllllllllllllllllllllllllllllllllll llllllllllllllllllllllllll lllllllllllllllllllllllllll lllllllllllllllllllllllllllllll llllllllllllllllllllllll lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll llllllllllllllllllllllllll0255075100 0 25 50 75 100MinimumMaximum(a) Maximum vs. minimum BLEU+1 (%)l lllll llll llllllllllllllllllllllllllllllllllllllllllllll llllllllll lllllllllllllllllllll lllllllllllll llllllllllll lll lllllllllll lll ll lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll lllll lllllllllllllll llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll llllllllllllllllllllll lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll lllllllllllll llll lllllllllllllll lllllllllllllllll ll lllllllllll llllllllllllllllllllllllllllll lll lllllllllllllllllllllllllllll lllllll lllll llllllllll llllllllllllllllllllllllllllllllllll llllllllllllllllll lllllll llllllllllllllllllllllll lllllllllllllllllllllllllllllllllllllllllllll llllllllllllll llllllllllllllllllll lllllllllllllllllllllllllllllllllllllllllllllllllllllllll llll lllllllllllll0255075100 0 25 50 75 100MaximumAll References(b) BLEU+1 (%) according to all four references vs.maximumFigure 1: Reference choice analysis for Ar-EnDense output on the dev set.ing, the scores should cluster around the diagonal:the references should yield similar scores.
This ishardly the case.
The mean difference isM = 18.1BLEU, with a standard deviation SD = 11.5.Figure 1b shows the same data set, but with themaximum on the horizontal axis and the multiple-reference score on the vertical axis.
Assuminga constant brevity penalty, the maximum lower-bounds themultiple-reference score since BLEU ag-gregates n-grams across references.
The multiple-reference score is an ?easier?
target since the modelhas more opportunities to match n-grams.Consider again the single-reference conditionand one of the pathological cases at the top of Fig-ure 1a.
Suppose that the low-scoring reference isobserved in the single-reference condition.
Themore expressive feature-rich model has a greatercapacity to fit that reference when, under another473reference, it would have matched the translationexactly and incurred a low loss.Nakov et al.
(2012) suggested extensions toBLEU+1 that were subsequently found to improveaccuracy in the single-reference condition (Gimpeland Smith, 2012a).
Repeating the min/max calcula-tions with the most effective extensions (accordingto Gimpel and Smith (2012a)) we observe lowervariance (M = 17.32, SD = 10.68).
These exten-sions are very simple, so a more sophisticated noisemodel is a promising future direction.7 Related WorkWe review work on phrase-based discriminative fea-ture sets that influence decoder search, and domainadaptation with features.117.1 Feature SetsVariants of some extended features are scatteredthroughout previous work: unfiltered lexicalizedrule indicators and alignments (Liang et al., 2006a);rule shape (Hopkins and May, 2011); rule orien-tation (Liang et al., 2006b; Cherry, 2013); targetunigram class (Ammar et al., 2013).
We foundthat other prior features did not improve translation:higher-order target lexical n-grams (Liang et al.,2006a; Watanabe et al., 2007; Gimpel and Smith,2012b), higher-order target class n-grams (Ammaret al., 2013), target word insertion (Watanabe et al.,2007; Chiang et al., 2009), and many other unpub-lished ideas transmitted through received wisdom.To our knowledge, Yu et al.
(2013) were the firstto experiment with non-local (derivation) featuresfor phrase-based MT.
They added discriminativerule features conditioned on target context.
This isa good idea that we plan to explore.
However, theydo not mention if their non-local features declarerecombination state.
Our empirical experience isthat non-local features are less effective when theydo not influence recombination.Liang et al.
(2006a) proposed replacing lexicalitems with supervised part-of-speech (POS) tags toreduce sparsity.
This is a natural idea that lay dor-mant until recently.
Ammar et al.
(2013) incorpo-rated unigram and bigram target class features.
Yuet al.
(2013) used word classes as backoff features toreduce overfitting.
Wuebker et al.
(2013) replacedall lexical items in the bitext and monolingual datawith classes, and estimated the dense feature set.11Space limitations preclude discussion of re-ranking fea-tures.Then they added these dense class-based featuresto the baseline lexicalized system.
Finally, Cherry(2013) experimented with class-based hierarchicalreordering features.
However, his features used abespoke representation rather than the simple fullrule string that we use.7.2 Domain Adaptation with FeaturesBoth Clark et al.
(2012) and Wang et al.
(2012) aug-mented the baseline dense feature set with domainlabels.
They each showed modest improvementsfor several language pairs.
However, neither incor-porated a notion of a default prior domain.Liu et al.
(2012) investigated local adaption ofthe log-linear scores by selecting comparable bitextexamples for a given source input.
After selectinga small local corpus, their algorithm then performsseveral online update steps?starting from a glob-ally tuned weight vector?prior to decoding theinput.
The resulting model is effectively a locallyweighted, domain-adapted classifier.Su et al.
(2012) proposed domain adaptationvia monolingual source resources much as we usein-domain monolingual corpora for data selection.They labeled each bitext sentence with a topic usinga Hidden Topic Markov Model (HTMM) Gruberet al.
(2007).
Source topic information was thenmixed into the translation model dense feature cal-culations.
This work follows Chiang et al.
(2011),who present a similar technique but using the samegold NIST labels that we use.
Hasler et al.
(2012)extended these ideas to a discriminative sparse fea-ture set by augmenting both rule and unigram align-ment features with HTMM topic information.8 ConclusionThis paper makes four major contributions.
First,we introduced extended features for phrase-basedMT that exceeded both dense and feature-rich base-lines.
Second, we specialized the features to sourcedomains, further extending the gains.
Third, weshowed that online expected BLEU is faster andmore stable than online PRO for extended fea-tures.
Finally, we released fast, scalable, language-independent tools for implementing the feature set.Our work should help practitioners quickly estab-lish higher baselines on the way to more targetedlinguistic features.
However, our analysis showedthat reference choice may restrain otherwise justifi-able enthusiasm for feature-rich MT.474AcknowledgmentsWe thank John DeNero for comments onan earlier version of this work.
The first author is supported bya National Science Foundation Graduate Research Fellowship.This work was supported by the Defense Advanced ResearchProjects Agency (DARPA) Broad Operational Language Trans-lation (BOLT) program through IBM.
Any opinions, findings,and conclusions or recommendations expressed in this mate-rial are those of the author(s) and do not necessarily reflect theview of DARPA or the US government.ReferencesW.
Ammar, V. Chahuneau, M. Denkowski, G. Hanne-man, W. Ling, A. Matthews, et al.
2013.
The CMUmachine translation systems at WMT 2013: Syntax,synthetic translation options, and pseudo-references.In WMT.E.
Bi?ici and D. Yuret.
2011.
Instance selection formachine translation using feature decay algorithms.In WMT.O.
Bojar, M.
Mach?
?ek, A. Tamchyna, and D. Zeman.2013.
Scratching the surface of possible translations.In I. Habernal and V.Matou?ek, editors, Text, Speech,and Dialogue, volume 8082 of Lecture Notes in Com-puter Science, pages 465?474.
Springer Berlin Hei-delberg.P-C. Chang, M. Galley, and C. D. Manning.
2008.Optimizing Chinese word segmentation for machinetranslation performance.
In WMT.C.
Cherry and G. Foster.
2012.
Batch tuning strategiesfor statistical machine translation.
In HLT-NAACL.C.
Cherry.
2013.
Improved reordering for phrase-basedtranslation using sparse features.
In HLT-NAACL.D.
Chiang, K. Knight, and W. Wang.
2009.
11,001new features for statistical machine translation.
InHLT-NAACL.D.
Chiang, S. DeNeefe, and M. Pust.
2011.
Two easyimprovements to lexical weighting.
In ACL.C.
Christodoulopoulos, S. Goldwater, andM.
Steedman.2010.
Two decades of unsupervised POS induction:How far have we come?
In EMNLP.J.
H. Clark, A. Lavie, and C. Dyer.
2012.
One system,many domains: Open-domain statistical machinetranslation via feature augmentation.
In AMTA.H.
Daum?
III.
2007.
Frustratingly easy domain adapta-tion.
In ACL.M.
Dreyer and D. Marcu.
2012.
HyTER: Meaning-equivalent semantics for translation evaluation.
InNAACL.J.
Duchi and Y.
Singer.
2009.
Efficient online and batchlearning using forward backward splitting.
JMLR,10:2899?2934.J.
Duchi, E. Hazan, and Y.
Singer.
2011.
Adaptive sub-gradient methods for online learning and stochasticoptimization.
JMLR, 12:2121?2159.N.
Durrani, B. Haddow, K. Heafield, and P. Koehn.2013.
Edinburgh?s machine translation systems forEuropean language pairs.
In WMT.J.
R. Finkel and C. D. Manning.
2009.
Hierarchicalbayesian domain adaptation.
In HLT-NAACL.J.
Gao and X.
He.
2013.
Training MRF-based phrasetranslation models using gradient ascent.
In NAACL.K.
Gimpel and N. A. Smith.
2012a.
Addendum tostructured ramp loss minimization for machine trans-lation.
Technical report, Language Technologies In-stitute, Carnegie Mellon University.K.
Gimpel and N. A. Smith.
2012b.
Structured ramploss minimization for machine translation.
In HLT-NAACL.S.
Green, D. Cer, K. Reschke, R. Voigt, J. Bauer,S.
Wang, and others.
2013a.
Feature-rich phrase-based translation: Stanford University?s submissionto the WMT 2013 translation task.
In WMT.S.
Green, S. Wang, D. Cer, and C. D. Manning.
2013b.Fast and adaptive online training of feature-rich trans-lation models.
In ACL.S.
Green, D. Cer, and C. D. Manning.
2014.
Phrasal: Atoolkit for new directions in statistical machine trans-lation.
In WMT.A.
Gruber, Y. Weiss, and M. Rosen-Zvi.
2007.
Hiddentopic markov models.
In AISTATS.E.
Hasler, B. Haddow, and P. Koehn.
2012.
Sparselexicalised features and topic adaptation for SMT.
InIWSLT.K.
Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.2013.
Scalable modified Kneser-Ney languagemodel estimation.
In ACL, Short Papers.M.
Hopkins and J.
May.
2011.
Tuning as ranking.
InEMNLP.A.
Irvine, J. Morgan, M. Carpuat, H. Daum?
III, andD.
Munteanu.
2013.
Measuring machine translationerrors in new domains.
TACL, 1.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In NAACL.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, et al.
2007.
Moses: Opensource toolkit for statistical machine translation.
InACL, Demonstration Session.P.
Liang, A.
Bouchard-C?t?, D. Klein, and B. Taskar.2006a.
An end-to-end discriminative approach tomachine translation.
In ACL.P.
Liang, B. Taskar, and D. Klein.
2006b.
Alignmentby agreement.
In NAACL.475P.
Liang.
2005.
Semi-supervised learning for naturallanguage.
Master?s thesis, Massachusetts Institute ofTechnology.C.-Y.
Lin and F. J. Och.
2004.
ORANGE: a method forevaluating automatic evaluation metrics for machinetranslation.
In COLING.L.
Liu, H. Cao, T. Watanabe, T. Zhao, M. Yu, andC.
Zhu.
2012.
Locally training the log-linear modelfor SMT.
In EMNLP-CoNLL.M.
Maamouri, A. Bies, and S. Kulick.
2008.
Enhanc-ing the Arabic Treebank: A collaborative effort to-ward new annotation guidelines.
In LREC.M.
Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19:313?330.W.
Monroe, S. Green, and C. D. Manning.
2014.
Wordsegmentation of informal Arabic with domain adap-tation.
In ACL, Short Papers.P.
Nakov, F. Guzman, and S. Vogel.
2012.
Optimizingfor sentence-level BLEU+1 yields short translations.In COLING.P.
Nakov, F. Guzm?n, and S. Vogel.
2013.
A tale aboutPRO and monsters.
In ACL, Short Papers.F.
J. Och and H. Ney.
2003.
A systematic compari-son of various statistical alignment models.
Compu-tational Linguistics, 29(1):19?51.F.
J. Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Compu-tational Linguistics, 30(4):417?449.F.
J. Och.
2003.
Minimum error rate training for statis-tical machine translation.
In ACL.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In ACL.S.
Riezler and J. T. Maxwell.
2005.
On some pitfalls inautomatic evaluation and significance testing in MT.In ACL Workshop on Intrinsic and Extrinsic Evalua-tion Measures for Machine Translation and/or Sum-marization.G.
Salton.
1958.
The use of punctuation patterns in ma-chine translation.
Mechanical Translation, 5(1):16?24, July.J.
Su, H. Wu, H. Wang, Y. Chen, X. Shi, H. Dong, andQ.
Liu.
2012.
Translation model adaptation for sta-tistical machine translation with monolingual topicinformation.
In ACL.J.
Uszkoreit and T. Brants.
2008.
Distributed word clus-tering for large scale class-based language modelingin machine translation.
In ACL-HLT.W.
Wang, K. Macherey, W. Macherey, F. J. Och, andP.
Xu.
2012.
Improved domain adaptation for statis-tical machine translation.
In AMTA.T.
Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.2007.
Online large-margin training for statistical ma-chine translation.
In EMNLP-CoNLL.E.
W. D. Whittaker and P. C. Woodland.
2001.
Effi-cient class-based language modelling for very largevocabularies.
In ICASSP.J.
Wuebker, S. Peitz, F. Rietig, and H. Ney.
2013.Improving statistical machine translation with wordclass models.
In EMNLP.N.
Xue, F. Xia, F. Chiou, and M. Palmer.
2005.
ThePenn Chinese Treebank: Phrase structure annotationof a large corpus.
Natural Language Engineering,11(2):207?238.H.
Yu, L. Huang, H. Mi, and K. Zhao.
2013.
Max-violation perceptron and forced decoding for scalableMT training.
In EMNLP.476
