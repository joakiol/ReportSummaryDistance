Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 609?618,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPMatching Reviews to Objects using a Language ModelNilesh Dalvi Ravi Kumar Bo Pang Andrew TomkinsYahoo!
Research701 First AveSunnyvale, CA 94089{ndalvi,ravikumar,bopang,atomkins}@yahoo-inc.comAbstractWe develop a general method to match un-structured text reviews to a structured listof objects.
For this, we propose a lan-guage model for generating reviews thatincorporates a description of objects and ageneric review language model.
This mix-ture model gives us a principled method tofind, given a review, the object most likelyto be the topic of the review.
Extensiveexperiments and analysis on reviews fromYelp show that our language model-basedmethod vastly outperforms traditional tf-idf-based methods.1 IntroductionConsider a user searching for reviews of?Casablanca Moroccan Restaurant.?
The searchengine would like to obtain as many reviews ofthis restaurant as possible, both to offer a high-quality result set for even obscure restaurants, andto enable advanced applications such as aggrega-tion/summarization/categorization of reviews andrecommendation of alternate restaurants.
To solvethis problem, it faces two high-level challenges:first, identify the restaurant review pages on theWeb; and second, given a review, identify therestaurant that is being reviewed.
There has beenprevious work addressing the first challenge (Sec-tion 2).
We focus in this paper on the second.The Web is replete with restaurant reviewsavailable on top restaurant verticals such as Yelpand CitySearch, as well as newspaper articles,newsgroup discussions, blog posts, small local re-view aggregators and so forth.
Ideally, the searchengine would like to obtain reviews from all pos-sible sources.
While identifying the subject mat-ter of a given review on the large sites may beamenable to structured extraction through wrapperinduction or related techniques, it is typically notcost-effective to apply such techniques to smaller?tail?
sites, and purely unstructured sources re-quire alternate approaches altogether.
In this pa-per, we explore the setting of matching reviews toobjects using only their textual content.
Note thatmatching reviews to objects is a pervasive prob-lem beyond the restaurant domain.
Shopping ver-ticals like to aggregate camera reviews, entertain-ment verticals wish to collect movie reviews, andso on.
We use restaurant reviews as a running ex-ample, but the techniques are general.More specifically, the problem we consider inthis paper is the following.
Given a list of struc-tured objects (restaurants/cameras/movies) and atext review, identify the object from the list thatis the topic of the review.
Our focus on tex-tual content allows us to expand the universe ofsources from which we can extract reviews to in-clude sources that are purely textual, such as fo-rum posts, blog posts, newsgroup postings, andthe like.
In fact, even among collections of ?struc-tured?
sources like review aggregators, there areno highly accurate unsupervised techniques tomatch a known review page to an object.
Struc-tured (e.g., HTML) cues provide valuable lever-age in attacking this problem, but the types of tex-tual cues we focus on are also a key part of thepuzzle; in such a context, our techniques can stillcontribute to the overall matching problem.It is important to contrast our problem againsttwo settings of related flavor ?
entity matching,whose goal is to find the correspondence betweentwo structured objects and information retrieval(IR), whose goal is to match unstructured shorttext (query) against unstructured text (document).Our problem is considerably harder than entitymatching for the following reasons.
In matchingtwo structured objects there is often a natural cor-respondence between their attributes, whereas nosuch correspondence exists between an object and609its review.
For instance, while trying to match areview to a restaurant object, it is unclear if a spe-cific portion of the review refers to the name of therestaurant, or to its location, or is a statement notconcerning specifics of the restaurant.
Moreover,even if we wish to use entity matching, we mustfirst recognize the entities from a review.
Thereare two methods to do this, namely, wrapper in-duction and information extraction.
Wrapper in-duction methods have serious limitations: they areapplicable only to highly-structured websites andinvolve human labeling effort that is expensive anderror-prone and entails constant maintenance tokeep wrappers up-to-date.
Information extractionmethods (Cardie, 1997; Sarawagi, 2008), on theother hand, often have limited accuracy.Our problem is also not amenable to classicalIR methods such as tf-idf.
For example, supposewe want to find the relevant restaurant for a givenreview.
The standard tf-idf will treat the review asthe query, the set of restaurant as documents andcompute the tf-idf scores.
Now consider a restau-rant called ?Food.
?1Since the term ?food?
is rareas a restaurant name, it will get a very high idfscore and hence will likely be the top match for allreviews containing the word ?food.?
In fact, unlikein traditional IR, a ?query?
(i.e., review) is longand a ?document?
(i.e., restaurant) is short ?
thisdemands adapting established IR concepts such asinverse document frequency and document lengthnormalization to our setting.
If we take the op-posite view by considering reviews as documentsand restaurants as queries, we still deviate from theIR setting, since now we need to rank and find thebest ?query?
for a given ?document.?
In Section3.4, we illustrate the shortcomings of both theseapproaches.In fact, the nature of the object database we con-sider provides several unique opportunities overtraditional IR.
First the ?document?, i.e., the ob-ject to be matched, has more semantics, sinceeach document is associated with one or more se-mantic attribute, such as the name/location of therestaurant.
Second, the ?query?, i.e., the text weare matching is known to be a review of the ob-ject, and hence is rendered in a language that is?review-like?
?
this can be modeled by a genera-tive process that produces reviews from objects.Third, the set of objects we are interested in is11569 Lexington Ave., New York, NY 10029.
(212) 348-0200.given a priori, and we only seek to match reviewswith one of these objects; this makes our problemmore tractable than open-ended entity recognition.Our contributions.
We propose a generalmethod to match reviews to objects.
To this end,we postulate a language model for generating re-views.
The intuition behind our model is simpleand natural: when a review is written about an ob-ject, each word in the review is drawn either from adescription of the object or from a generic reviewlanguage that is independent of the object.
Thismixture model leads to a method to find, given areview, the object most likely to be the topic of thereview.Our method is light-weight and scalable andcan be viewed as obviating the need for highly-expensive information extraction.
Since themethod is text-based and does not rely on anyHTML structural clues, it is especially applicableto reviews present in blogs and the so-called tailweb sites ?
web sites for which it is not feasibleto maintain wrappers to automatically extract theobject of a review.We then report results on over 11K restaurantreviews from Yelp.
The experiments and our ex-tensive analysis show that our language model-based method significantly outperforms traditionaltf-idf based methods, which fail to take full ad-vantage of the properties that are specific to oursetting.2 Related workOpinion topic identification is the work closestto ours.
In a recent paper, Stoyanov and Cardie(2008) approach this problem by treating it as anexercise in topic coreference resolution.
Thoughthey have to deal with topic ambiguities and a lackof explicit topic mentions as in our case, their no-tion of a topic is not driven by a structured list-ing.
There has been some work on fine-grainedopinion extraction from reviews (Kobayashi et al,2004; Yi et al, 2003; Popescu and Etzioni, 2005;Hu and Liu, 2004); see (Pang and Lee, 2008) for acomprehensive survey.
Most of this body of workfocused on identifying product features of the ob-ject under review, rather than identifying the prod-uct itself.
Note that while a dictionary of prod-ucts is often more readily available than a dictio-nary of product features, identifying objects of re-views is non-trivial even with the help of the for-mer.
Indeed, it has been reported that lexicon-610lookup methods have limited success on generalnon-product review texts (Stoyanov and Cardie,2008).
In general, this line of work is more rootedin the information extraction literature, where textspans covering the object (or features of the ob-ject) were extracted as the first step; in contrast,we do not have an explicit extraction phase.
Sincethe (very extensive) list of candidate objects aregiven as input, our task is to rank all matching ob-jects, and in this sense is closer in nature to infor-mation retrieval tasks.
There has been some workon detecting reviews in large-scale collections (Nget al, 2006; Barbosa et al, 2009); this is a logicalstep that precedes the review matching step, thetopic of our paper.Language modeling is becoming a powerfulparadigm in the realm of information retrieval ap-plications (Ponte and Croft, 1998; Hiemstra, 1998;Song and Croft, 1999; Lafferty and Zhai, 2003;Zhai, 2008).
The basic theme behind languagemodeling is to first postulate a model for each doc-ument and for a given query select the documentthat is most likely to have generated the query;smoothing is an important means to manage datasparsity in language models (Zhai and Lafferty,2004).
As noted earlier, language models devel-oped for IR are unsuitable for our setting.
Further-more, there are opportunities, such as the presenceof structure in our data, which we use in this work(Section 3.2).
In fact, in a subsequent paper, weshow how a language model specific to each at-tribute can further improve the accuracy of reviewmatching (Dalvi et al, 2009).Entity matching is a well-studied topic indatabases.
There are several approaches to entitymatching: non-relational approaches, which con-sider pairwise attribute similarities between enti-ties (Newcombe et al, 1959; Fellegi and Sunter,1969), relational approaches, which exploit the re-lationships that exist between entities (Ananthakr-ishna et al, 2002; Kalashnikov et al, 2005), andcollective approaches, which exploit the relation-ship between various matching decisions, (Bhat-tacharya and Getoor, 2007; McCallum and Well-ner, 2004).
The EROCS system (Chakaravarthy etal., 2006), which uses information extraction andentity matching, is closest in spirit to our problem;they, however, employ tf-idf to match, which weshow to be significantly sub-optimal in our set-ting.3 Model and methodIn this section we present the problem formula-tion, the basic generative model for reviews, amethod based on this model to associate an objectwith a review, and the techniques to estimate theparameters of this model.Problem formulation.
Let E denote a set of ob-jects.
Each object e ?
E has a set of attributesand let text(e) denote the union of the textual con-tent of all its attributes.
Suppose we have a col-lection of reviews R, where each review is writ-ten (mainly) about one of the objects in the listingE .
The problem now is to correctly associate eachr ?
R with exactly one of e ?
E .We model each review as a bag of words.Therefore, notation such as ?w ?
r?
for a wordw and a review r makes sense.
For a review r andan object e, let re= r ?
text(e).As a running example, we use E to denote theset of all restaurants and R to denote the set of allrestaurant reviews.3.1 A generative model for reviewsWe first state the intuition behind our generativemodel: when a review r is written about an objecte, some words in r (e.g., the name and the addressof the restaurant) are drawn from text(e) to referto the object under discussion, while some otherwords are drawn from a generic review languageindependent of e.Formally, let ?
?
(0, 1) be a parameter.Let Pe(?)
denote a distribution whose support istext(e); this corresponds to the distribution ofwords specific to the object e, taken from the de-scription text(e).
We use Pe(w) to denote theprobability the word w is chosen according to thisdistribution.
Let P (?)
be an object-independentdistribution whose support is the review language,i.e., all the words that can be used to write a re-view; we use P (w) to denote the probability theword w is chosen according to this distribution.Now, for a given object e, a review r is gener-ated as follows.
Each word in r is generated in-dependently: with probability ?, a word w is cho-sen with probability Pe(w) and with probability1 ?
?, a word w is chosen with probability P (w).Thus, the review generation process is a multino-mial, where the underlying process is a mixture ofobject-specific language and a generic review lan-guage.611Given a review r and an object e, by our inde-pendence assumption,Pr[r | e] = Z(r)?w?rPr[w | e]= Z(r)?w?r((1 ?
?
)P (w) + ?Pe(w)), (1)where Z(r) is a normalizing term that only de-pends on the length of r and the counts of thewords in it.
Recalling re= r ?
text(e), we notethat Pe(w) assigns zero probability to w 6?
re.From (1), we getPr[r | e] = Z(r)?w?r\re(1 ?
?
)P (w)?
?w?re((1 ?
?
)P (w) + ?Pe(w))= Z(r)?w?r(1 ?
?
)P (w) ?
?w?re(1 +?1 ?
?Pe(w)P (w)).
(2)Note that Eq.
(2) appears similar to the formulaobtained in the language model approach for IR(Hiemstra and Kraaij, 1998); the interpretation ofterms, however, is very different.
For instance,P (w) in our case is computed over the ?query?corpus whereas the analogous term (collection fre-quency) in (Hiemstra and Kraaij, 1998) is com-puted over the ?document?
corpus.
As the ?Food?restaurant example in Section 1 suggests, usingthe ?document?
frequency is undesirable.
The useof ?query?
corpus frequency arises naturally fromour generative story and also guides us to a differ-ent way to estimate P (w); see Section 3.3.3.2 Matching a review to an objectGiven the above review language model (RLM),we now state how to match a given review to anobject.
According to our model, the most likelyobject e?to have generated a review r is given bye?= argmaxePr[e | r] = argmaxePr[e]Pr[r]?Pr[r | e].In the absence of any information, we assumea uniform distribution for Pr[e].
(Additionalinformation about objects, such as their rat-ing/popularity, can be used to model Pr[e] moreaccurately.)
From this, we gete?= argmaxePr[r | e],or equivalently,e?= argmaxelog Pr[r | e].Since Z(r)?w?r((1??
)P (w)) is independent ofe, using (2), we havee?= argmaxe?w?relog(1 +?1 ?
?Pe(w)P (w)).
(3)3.3 Estimating the parametersWe now describe how to estimate the parametersof the model, namely, P (?
), Pe(?
), and ?.Recall that P (?)
is the distribution of generic re-view language.
Ideally, for each review r, if weknow the component r(e)that came from the dis-tribution Pe(?)
and the component r(g)that camefrom P (?
), then we can collect the r(g)compo-nents of all the reviews in R, denoted as R(g), andestimate P (?)
by the fraction of occurrences of win R(g).
More specifically, let c(w,R(g)) denotethe number of times w occurs in R(g).
With add-one smoothing, we estimateP (w) =c(w,R(g)) + 1?w?c(w?,R(g)) + |V |,where |V | is the vocabulary size.In reality, we only have access to r and not to thecomponents r(e)and r(g).
If we have an alignedreview corpus R?, where for each review r, weknow the true object e that generated it, we canclosely approximate r(e)with re.2Let no-obj(R?
)be the set of processed reviews where for eachreview-object pair (r, e), words in text(e) are re-moved from r. By treating no-obj(R?)
as an ap-proximation of R(g), we can compute P (w) in theaforementioned manner.
If we only have accessto a review collection R?with no object align-ment, there are other ways to effectively approx-imate R(g); see Section 5.3 for more details.Unlike P (?
), we cannot learn an individual lan-guage model Pe(?)
for each e, since we cannot ex-pect to have training examples of reviews for eachpossible object e in the dataset.
Thus, we needa simpler way to model Pe(w).
The most naiveway would be to assume a uniform distribution,i.e., Pe(w) = 1/|text(e)|.
However, each word2There can be exceptions to this, e.g., review of a restau-rant called ?Tasty Bites?
might use the word ?tasty?
from thereview language, but not to refer to the restaurant.
Nonethe-less, we believe these will be rare exceptions and will nothave significant effect in the estimation of P (?
).612in text(e) may not be generated with equal prob-ability.
In our running example, consider the casewhen text(e) contains the full name of the restau-rant, i.e., ?Casablanca Moroccan Restaurant.?
Areview for this restaurant is more likely to choosethe word ?Casablanca?
than any other word to re-fer to this restaurant since this is arguably more in-formative than ?Moroccan?
or ?Restaurant.?
Thiscan be captured by using the frequency fwof theword w in R or in {text(e) | e ?
E}.
For a suit-able function g(w) that is inversely growing as fw(say, g(w) = log(1/fw)), we letPe(w) =g(w)?w??text(e)g(w?
).Alternatively, it is possible to construct modelswhere Pe(w) is more directly estimated from thedata; in fact, one can also use suitable transla-tion models to estimate Pe(w) for w that may noteven occur in text(e) ?
this will help in caseswhere reviews use an abbreviation such as ?Casa?or ?CMR?
to refer to our running example.
Suchmodels require either fine-grained labeled exam-ples or, as we show in (Dalvi et al, 2009), moresophisticated estimation techniques.It is tempting to assume that common wordssuch as ?Restaurant?
may not contribute towardsmatching a review to an object and hence one canconveniently set Pe(w) = 0 for such words w.(Such a list of words can easily be compiled usinga domain-specific stopword list.)
This may hurt ?in our example, the presence of the word ?Restau-rant?
in a review might help to disambiguate theobject of reference, if the listing were also to con-tain a ?Casablanca Moroccan Cafe?.3.4 Properties of the modelEq.
(3) indicates that our method (denoted asRLM) gives less importance to common wordswith high P (w).
This corresponds to the intuitionbehind the standard tf-idf scheme.
Why, then, dowe expect RLM to be more effective?
Here, wediscuss the salient features of our method, con-trasting it with tf-idf in particular.First, we take a closer look at different ways toapply tf-idf techniques to our setting.
Since thetask is to find the most relevant object given a re-view, a naive way to apply the standard tf-idf (de-noted TFIDF) will treat each review to be the queryand each object to be a document and score docu-ments using the standard tf-idf scoring.
This, how-ever, leads to severe problems since this computesthe inverse document scores over the object corpus?
recall the ?Food?
example in Section 1.A more reasonable way to apply tf-idf is toinstead treat objects as queries and reviews asdocuments for computing tf-idf scores (denotedTFIDF+).
For a word w, let Q(w) =df(w)N,where N is the number of reviews in the corpusand df(w) is the number of reviews containing w.Given a review r and an object e, the score of theobject is given by?w?relog (1/Q(w)), and wewant to pick the object with the maximum score.As we will discuss later, document-length nor-malization (i.e., normalizing by object descriptionlength so that a restaurant with a long name doesnot get an unfair disadvantage) is still non-trivialhere.As noted earlier, Eq.
(3), used by RLM formatching reviews with objects, has a striking re-semblance to the TFIDF+scoring function.
Bothhave the forme?= argmaxe?w?relog f(w),where for RLM,f(w) = fR(w) = 1 +?1 ?
?Pe(w)P (w),and for TFIDF+,f(w) = fB(w) =1Q(w).In both cases, f(w) is monotonically decreas-ing in the frequency of w in the corpus.
How-ever, there are several differences between the twocases.
We highlight some of them here, with theaim of illustrating the power of our review lan-guage model (RLM).Object length normalization.
First note that thePe(w) term in fR(w) acts as an object length nor-malizing term, i.e., it adds up to one for eache and weighs down P (w) for objects with longtext(e).
This also has the effect of penalizing re-views that are missing critical words in the objectdescription.
In contrast, fB(w) is unnormalizedwith respect to the object length.
The standarddocument normalization techniques in IR do notapply well to our setting since our ?documents?
(i.e., object descriptions) are short.
E.g., if the ob-ject description contains only one token, the stan-dard cosine-normalization technique (Salton et al,6131975) will yield a normalized score of 1 irre-spective of the token.
Thus for a review contain-ing the words ?Food?
and ?Casablanca?, the stan-dard normalization will yield the same score for arestaurant named ?Food?
and a restaurant named?Casablanca?, ignoring the fact that ?Food?
ismuch more likely to be an object-independentterm.
Note that this only becomes a problem whenthe entire ?document?
is part of the match, whichrarely happens in an IR setting where the docu-ments are typically much longer than the queries.Indeed, in our experiments, we observe lower per-formance when we apply cosine-normalization tothe tf-idf scores.
On the other hand, in fR(w), theP (w) term can still distinguish the two aforemen-tioned objects even when Pe(w) are equal.Dampening.
With ?
< 1, fR(w) is effectivelya dampened version ofPe(w)P (w).
In other words, dif-ferences between very frequent words and very in-frequent words are somewhat smoothed out.
In-deed, if we modify TFIDF+by introducing a sim-ilar dampening factor into fB(w), we observe im-provement in its performance (Section 5.4).Removingmentions of an object.
Another differ-ence is that in RLM, P (w) is estimated on reviewswith object mentions removed, since the model in-dicate that P (w) accounts for object-independentreview language.
In contrast, TFIDF+computesQ(w) on full reviews.
We illustrate the differ-ence on the following example.
Consider a reviewthat reads ?.
.
.Maggiano?s has great Fondue.?
If?Maggiano?s?
and ?Fondue?
both occur the samenumber of times in the corpus, then they get thesame idf (i.e., Q(w)) score.
In RLM, however,?Maggiano?s?
will get much smaller probabilityin the generic review distribution P (?)
than ?Fon-due?, since ?Maggiano?s?
almost always occurs inreviews as restaurant name mentions, thus is re-moved from the estimation of its P (?)
probabil-ity.
On the other hand, the word ?Fondue?
is morelikely to retain higher probability in P (?)
since ittends to appear as dish names.
As a result, ourmodel will assign higher weight to ?Maggiano?sRestaurant?
than ?Fondue Restaurant?.
As we cansee, RLM evaluates the ability of a word to identifythe review object rather than rely on the absoluterarity of the word, which is done by tf-idf.Using term counts.
One last difference is thatfR(w) uses term counts of words rather than thestandard document counts used by fB(w).
Ourevaluation suggests that at least in practice, thisdoes not have a big impact on the overall accuracy.In the experiments we show that these factorstogether account for the performance differencebetween RLM and tf-idf.
Our model gives a prin-cipled way to introduce these factors, however.4 DataIn this section we describe the dataset constructedfor the task of matching restaurant reviews to thecorresponding restaurant objects.
Our goal is toobtain a large collection of reviews on which toestimate the generic language model, with a sig-nificant portion of them aligned with the objectsfor which the reviews were written; this portionwill serve as the gold-standard test set.To this end, we obtained a set of reviews fromthe Yelp website, yelp.com.
This website con-tains a collection of reviews about various busi-nesses and for each business, has a webpage con-taining the business information and a list of re-views.
We crawled all restaurant pages from Yelp.For each restaurant, we extracted its name andcity location from the business information sec-tion via HTML cues, and a list of no more than40 reviews.
We obtained the textual content of299,762 reviews, each aligned with one of a setof 12,408 unique restaurants hosted on Yelp.
Notethat while our technique is not targeted for headsites like Yelp (where wrapper induction mightbe a more accurate approach), this provides alarge-scale dataset, conveniently labeled with ob-ject information, and simulates the tail-site sce-nario where we rely heavily on the textual contentof reviews to identify objects.Many of the reviews in Yelp do not contain anyidentifying information.
In fact, some of them areas short as ?Great place.
Awesome food!!?.
Weprocessed the dataset to retain only reviews thatmention the name of the restaurant, even if par-tially, and, when the restaurant name is a commonword, also the city of the restaurant.
Each of theremaining reviews is expected to have enough in-formation for a human to identify the restaurantcorresponding to the review.To further increase the difficulty of the match-ing task, we obtained a much more extensive listof restaurant objects in the Yahoo!
Local database,which contains 681,320 restaurants.
Our taskis to match a given Yelp review, using only itsfree-form textual content, with its corresponding614restaurant in the Yahoo!
Local database.
We thenproceeded to generate the gold standard that con-tains the correct restaurant in the Yahoo!
Localdatabase for each review.
We employed geocodingto match addresses across the two databases alongwith approximate name matches.
Note that in thefinal dataset, only half of the restaurants have theexact same name listed in both Yelp and Yahoo!Local; this limits the success of naive dictionary-based methods.The final aligned dataset contained 24,910 Yelpreviews (R), covering 6,010 restaurants.
We setaside half of the reviews (R?)
to estimate the mod-els and the other half (Rtest) to evaluate our tech-nique.
We also set aside 1,000 reviews as devel-opment set, on which we conducted initial exper-iments.
The total size of the test corpus, Rtestwas 11,217.
The splitting of R into R?, Rtest,and the development set was done in such a waythat there are no overlapping restaurants betweenthem.
Also, the reviews that were filtered outbecause of lack of identifying information wereadded back to R?for learning the review languagemodel, expandingR?to a total of 205,447 reviews.5 EvaluationIn this section we evaluate our proposed review-language based matching algorithm RLM.5.1 Experimental considerationsBaseline system.
We use the TFIDF and TFIDF+algorithms described in Section 3.4 as baselinealgorithms.
Since we are comparing objectsthat can have varying lengths, we tried the stan-dard cosine-normalization techniques for docu-ment length normalization.
For reasons describedin Section 3.4, however, the normalization signif-icantly lowered the accuracy.
All the numbers re-ported here are using tf-idf scores without normal-ization.Efficiency.
For both RLM and the baseline algo-rithms, it is impractical to compute the similar-ity of a review with each object in the database.Since all objects that do not intersect with the re-view have a zero score, we built an inverted in-dex to retrieve all objects containing a given word.Even a simple inverted index can be very ineffi-cient since for each review, words such as ?Restau-rant?
or ?Cafe?
retrieve a substantial fraction ofthe whole database.
Hence, we further optimizedthe index by looking at the document frequenciesof the words and considering word bigrams in ob-ject descriptions.
The index only retrieves ob-jects that have a non-trivial overlap with the re-view; e.g., an overlap of ?Casablanca?
is consid-ered non-trivial while an overlap of ?Restaurant?is considered trivial.
Once these candidates are re-trieved, our scoring function takes into account alloverlapping tokens.For the YELP dataset, the index returns an av-erage of 200 restaurants for each review.
Thispoints to the general difficulty of review match-ing over a large corpus of objects, since a simpledictionary-based named-entity recognition will hitat least 200 objects for many reviews.Experiment settings.
For RLM, we conductedinitial experiments and performed parameter esti-mation on the development data.
The experimen-tal settings we used for RLM are as follows: weset g(w) = log(1/fw) for Pe, where fwis esti-mated on the review collection.
P (w) is estimatedon all reviews in R?, where for each review, all to-kens of its corresponding text(e), if present, areremoved, in order to approximate the generic re-view language independent of e, as required byour generative model.
We estimate ?
to be 0.002,tuned on the development set; in our experiments,we observe that the performance is not very sensi-tive to ?.5.2 Main resultsIn this section we present the main comparisonsbetween RLM and the baseline in details.Performance measure.
Our task resembles astandard IR task in that our algorithm ranks can-didate objects for a given review by their ?about-ness?
level.
Unlike a standard IR task, however,we are not interested in retrieving multiple ?rel-evant?
objects, as each review in our dataset hasonly one single correct match from E .
A reviewmatch is correct if the top-1 prediction (i.e., e?)
isaccurate.
In what follows, we report the averageaccuracy for various experimental settings.
Notethat we can take the average accuracy over all re-views (reported as micro-average), regardless ofwhich restaurants they are about; or we can firstcompute the average for reviews about the samerestaurant, and report the average over all restau-rants (macro-average).
When not specified, we re-port the micro-average.Main comparisons.
Table 1(a) summarizes themain comparison.
Our proposed algorithm RLM615Method Micro-avg.
Macro-avg.RLM 0.647 0.576TFIDF+0.518 0.481TFIDF 0.314 0.317(a) Main comparison.Method Micro-avg.
Macro-avg.RLM-UNIFORM 0.634 0.562RLM-UNCUT 0.627 0.546RLM-DECAP 0.640 0.573(b) RLM variants.Method Micro-avg.
Macro-avg.TFIDF+-N 0.586 0.523TFIDF+-D 0.593 0.533TFIDF+-O 0.522 0.488TFIDF+-ND 0.628 0.549TFIDF+-NDO 0.647 0.576(c) TFIDF+variants.Table 1: Average accuracy of the top-1 predictionfor various techniques.
Micro-average computedover 11,217 reviews inRtest; macro-average com-puted over 2,810 unique restaurants in Rtest.clearly outperforms the TFIDF+baseline mea-sured by either micro- or macro-average accuracy.The standard TFIDF, as predicted, performs theworst.Some reviews can be particularly difficult tomatch, which can be reflected in a low matchingscore.
Nonetheless, we predict the most likely ob-ject.
Suppose we impose a threshold and returnthe most likely object only when its score is abovethreshold, we can then compute precision and re-call at different thresholds.
Figure 1 presents theprecision?recall curve (using micro-average) forboth RLM and TFIDF+.
Again, RLM clearly out-performs TFIDF+across the board.We then generalize the definition of accuracyinto accuracy@k: a review is considered as cor-rectly matched if one of the top-k objects returnedis the correct match.
We plot accuracy@k as afunction of k. While the gap between RLM andTFIDF+is smaller as k increases, RLM clearlyoutperforms TFIDF+for all k ?
{1, .
.
.
, 10}.One final comparison is accuracy@1 as a func-tion of the review length.
Given our current set-ting, longer reviews might be more difficult tomatch since they may include more proper nounssuch as dish names and related restaurants, andFigure 1: Precision?recall curve (of top one pre-diction): RLM vs. TFIDF+baseline.Figure 2: Accuracy@k (percentage of reviewswhose correct match is returned in one of its top-kpredictions): RLM vs. TFIDF+baseline.Figure 3: Average accuracy of the top-1 predictionfor reviews with different length (on test set): RLMvs.
TFIDF+baseline.616yield a longer list of highly competitive candi-date objects.
Interestingly, the gap between RLMand TFIDF+is much smaller for shorter reviews.As reviews get longer, the performance of RLMis relatively stable, whereas the performance ofTFIDF+drops down significantly.5.3 Experimental choices for RLMWe now examine the experimental choices wemade for different components of RLM by defin-ing the following variations of RLM.RLM-UNIFORM: rather than setting g(w) =log(1/fw) for Pe, we use the uniform distributionPe(w) = 1/|text(e)|.
From the third line of Table1 (b), there is a slight accuracy drop of ?
1.3%.RLM-UNCUT: suppose we only have access toa review corpus with no alignment to text(e), andthus have to approximate P (w) by estimating iton the set of original ?un-cut?
reviews, how muchdoes that affect our performance?
As indicated inthe fourth row of Table 1 (b), this reduces accuracyby about 2% on our test data.RLM-DECAP: as an alternative way to deal withlack of aligned data, we consider a variation ofthe above algorithm by removing all the capital-ized words from un-annotated reviews.
Clearly,this can result in both ?over-cutting?
and ?under-cutting?
of true restaurant name mentions.
How-ever, as indicated in the fourth row of Table 1 (b),this is very close to the best accuracy achieved.Thus, an effective model can be learned even with-out aligned data.5.4 Revisiting TFIDF+: what?s amiss?In this section we revisit the main differences be-tween our model and the TFIDF+outlined in Sec-tion 3.4, and investigate their empirical impor-tance by introducing these features into TFIDF+and examine their effectiveness in that framework.Object length normalization.
We con-sider a modified TFIDF+measure fM(w) =Pe(w)/Q(w), which we call TFIDF+-N (normal-ized).
As shown in Table 1 (c), this change alonecan increase the average accuracy by nearly 7%.Dampening.
We consider a modified TFIDF+measure fM(w) = 1 + ?
?Ndf(w), which we callTFIDF+-D. Table 1 (c) reports the performance ofusing this measure, with ?
= 0.1 (set on develop-ment data).
Again, this measure alone can induceover 7% increase in accuracy.
Indeed, combin-ing normalization and dampening, (i.e., fM(w) =1+?
?Pe(w) ?Ndf(w)), denoted as TFIDF+-ND, weget comparable performance to RLM-UNCUT.Removing mentions of objects.
Again, we canincorporate this in a heuristic way in TFIDF+,which we denote by TFIDF+-O.
Interestingly,while using the original fB(w) function withdf(w) computed on the object-removed reviewcollection does not yield a big improvement, thisdoes bring the performance of the fully modifiedTFIDF+to the same level of the standard RLM(see line marked TFIDF+-NDO.
)Using term counts.
Our investigation suggeststhat at least in practice, using Q(w) vs. P (w) isnot a critical decision, as a fully modified TFIDF+can achieve the same performance using df(w) toquantify frequency of the word.
Our experimentson this dataset show that each of the other model-ing decisions incorporated in RLM is important.6 ConclusionsWe proposed a generative model for reviewswhere reviews are generated from the mixture ofa distribution involving object terms and a genericreview language model.
The model provides usa principled way to match reviews to objects.Our evaluation on a real-world dataset shows thatour techniques vastly outperforms standard tf-idfbased techniques.AcknowledgmentsWe thank Don Metzler for many discussions andthe anonymous reviewers for their comments.ReferencesR.
Ananthakrishna, S. Chaudhuri, and V. Ganti.
2002.Eliminating fuzzy duplicates in data warehouses.
InProc.
28th VLDB, pages 586?596.L.
Barbosa, R. Kumar, B. Pang, and A. Tomkins.
2009.For a few dollars less: Identifying review pages sanshuman labels.
In Proc.
NAACL.I.
Bhattacharya and L. Getoor.
2007.
Collective entityresolution in relational data.
ACM TKDD, 1(1).C.
Cardie.
1997.
Empirical methods in informationextraction.
AI Magazine, 18(4):65?80.V.
T. Chakaravarthy, H. Gupta, P. Roy, and M. Mo-hania.
2006.
Efficiently linking text documentswith relevant structured information.
In Proc.
32ndVLDB, pages 667?678.617N.
Dalvi, R. Kumar, B. Pang, and A. Tomkins.
2009.A translation model for matching reviews to objects.Manuscript.I.
P. Fellegi and A.
B. Sunter.
1969.
A theory for recordlinkage.
JASIS, 64:1183?1210.D.
Hiemstra and W. Kraaij.
1998.
Twenty-one atTREC7: Ad-hoc and cross-language track.
In Proc.7th TREC, pages 174?185.D.
Hiemstra.
1998.
A linguistically motivated prob-abilistic model of information retrieval.
In Proc.ECDL, pages 569?584.M.
Hu and B. Liu.
2004.
Mining opinion features incustomer reviews.
In Proc.
AAAI, pages 755?760.D.
V. Kalashnikov, S. Mehrotra, and Z. Chen.
2005.Exploiting relationships for domain-independentdata cleaning.
In Proc.
5th SDM.N.
Kobayashi, K. Inui, Y. Matsumoto, K. Tateishi, andT.
Fukushima.
2004.
Collecting evaluative expres-sions for opinion extraction.
In Proc.
1st IJCNLP,pages 596?605.J.
Lafferty and C. Zhai.
2003.
Probabilistic relevancemodels based on document and query generation.
InW.
B. Croft and J. Lafferty, editors, Language Mod-eling and Information Retrieval.
Academic Publish-ers.A.
McCallum and B. Wellner.
2004.
Conditional mod-els of identity uncertainty with application to nouncoreference.
In Proc.
17th NIPS.H.
B. Newcombe, J. M. Kennedy, S. J. Axford, andA.
P. James.
1959.
Automatic linkage of vitalrecords.
Science, 130:954?959.V.
Ng, S. Dasgupta, and S. M. Niaz Arifin.
2006.
Ex-amining the role of linguistic knowledge sources inthe automatic identification and classification of re-views.
In Proc.
21st COLING/44th ACL, pages 611?618.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1-2):1?135.J.
M. Ponte and W. B. Croft.
1998.
A language model-ing approach to information retrieval.
In Proc.
21stSIGIR, pages 275?281.A.-M. Popescu and O. Etzioni.
2005.
Extracting prod-uct features and opinions from reviews.
In Proc.HLT/EMNLP.G.
Salton, A. Wong, and C. S. Yang.
1975.
A vec-tor space model for automatic indexing.
Commun.ACM, 18(11):613?620.S.
Sarawagi.
2008.
Information extraction.
Founda-tions and Trends in Databases, 1(3):261?377.F.
Song and W. B. Croft.
1999.
A general languagemodel for information retrieval.
In Proc.
22nd SI-GIR, pages 279?280.V.
Stoyanov and C. Cardie.
2008.
Topic identificationfor fine-grained opinion analysis.
In Proc.
COLING.J.
Yi, T. Nasukawa, R. Bunescu, and W. Niblack.
2003.Sentiment analyzer: Extrating sentiments about agiven topic.
In Proc.
3rd ICDM, pages 427?434.C.
Zhai and J. Lafferty.
2004.
A study of smoothingmethods for language models applied to informationretrieval.
ACM TOIS, 22(2):179?214.C.
Zhai.
2008.
Statistical language models for infor-mation retrieval a critical review.
Foundations andTrends in Information Retrieval, 2(3):137?213.618
