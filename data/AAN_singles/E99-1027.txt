Proceedings of EACL '99An exper iment on the upper bound of interjudge agreement:the case of taggingAtro VoutilainenResearch Unit for Multilingual Language TechnologyP.O.
Box 4FIN-00014 University of HelsinkiFinlandAtro.Voutilainen@ling.Helsinki.FIAbstractWe investigate the controversial issueabout the upper bound of interjudgeagreement in the use of a low-levelgrammatical representation.
Pessimisticviews suggest that several percent ofwords in running text are undecidable interms of part-of-speech categories.
Ourexperiments with 55kW data give rea-son for optimism: linguists with only 30hours' training apply the EngCG-2 mor-phological tags with almost 100% inter-judge agreement.1 Or ientat ionLinguistic analysers are developed for assign-ing linguistic descriptions to linguistic utterances.Linguistic descriptions are based on a fixed inven-tory of descriptors plus their usage principles: inshort, a grammatical representation specified bylinguists for the specific kind of analysis - e.g.morphological nalysis, tagging, syntax, discoursestructure - that the program should perform.Because automatic linguistic analysis generallyis a very di~cult problem, various methods forevaluating their success have been used.
One suchis based on the degree of correctness ofthe analysisprovided, e.g.
the percentage of linguistic tokensin the text analysed that receives the appropriatedescription relative to analyses provided indepen-dently of the program by competent linguists ide-ally not involved in the development of the anal-yser itself.Now use of benchmark corpora like this turnsout to be problematic because arguments havebeen made to the effect that linguists themselvesmake erroneous and inconsistent analyses.
Unin-tentional mistakes due e.g.
to slips of attentionare obviously unavoidable, but these errors canlargely be identified by the double-blind method:first by having two (or more) linguists analyse thesame text independently b  using the same gram-matical representation, and then identifying dif-ferences of analysis by automatically comparingthe analysed text versions with each other and fi-nally having the linguists discuss the di~erencesand modify the resulting benchmark corpus ac-cordingly.
Clerical errors should be easily (i.e.consensuaUy) identified as such, hut, perhaps ur-prisingly, many attested ifferences do not belongto this category.
Opinions may genuinely differabout which of the competing analyses is the cor-rect one, i.e.
sometimes the grammatical repre-sentation is used inconsistently.
In short, linguis-tic 'truth' seems to be uncertain in many cases.Evaluating - or even developing - linguistic anal-ysers seems to be on uncertain ground if the goalof these analysers cannot be satisfactorily speci-fied.Arguments concerning the magnitude of thisproblem have been made especially in relation totagging, the attempt o automatically assign lex-ically and contextually correct morphological de-scriptors (tags) to words.
A pessimistic view istaken by Church (1992) who argues that even af-ter negotiations of the kind described above, noconsensus can be reached about the correct anal-ysis of several percent of all word tokens in thetext.
A more mixed view on the matter is takenby Marcus et al (1993) who on the one hand notethat in one experiment moderately trained humantext annotators made different analyses even afternegotiations in over 3% of all words, and on theother hand argue that an expert can do much bet-ter.An optimistic view on the matter has been pre-sented by Eyes and Leech (1993).
Empirical ev-idence for a high agreement rate is reported byVoutilainen and J~rvinen (1995).
Their resultssuggest hat at least with one grammatical repre-sentation, namely the ENGCG tag set (cf.
Karls-son et al, eds., 1995), a 100% consistency can be204Proceedings of EACL '99reached after negotiations at the level of parts ofspeech (or morphology in this case).
In short, rea-sonable evidence has been given for the positionthat at least some tag sets can be applied consis-tently, i.e.
earlier observations about potentiallymore problematic tag sets should not be taken aspredictions about all tag sets.1.1 Open quest ionsAdmittedly Voutilainen and J~xvinen's experi-ment provides evidence for the possibility thattwo highly experienced linguists, one of them adeveloper of the ENGCG tag set, can apply thetag set consistently, at least when compared witheach others' performance.
However, the practicalsignificance of their result seems questionable, fortwo reasons.Firstly, large-scale corpus annotation by handis generally a work that is carried out by less ex-perienced linguists, quite typically advanced stu-dents hired as project workers.
Voutilainen andJiirvinen's experiment leaves open the question,how consistently the ENGCG tag set can be ap-plied by a less experienced annotator.Secondly, consider the question of tagger evalu-ation.
Because tagger developers presumably tendto learn, perhaps partly subconsciously, muchabout the behaviour, desired or otherwise, of thetagger, it may well be that if the developers alsoannotate the benchmark corpus used for evaluat-ing the tagger, some of the tagger's misanalysesremain undetected because the tagger developers,due to their subconscious mimicking of their tag-ger, make the same misanalyses when annotatingthe benchmark corpus.
So 100% tagging consis-tency in the benchmark corpus alone does not nec-essarily suffice for getting an objective view of thetagger's performance.
Subconscious 'bad' habitsof this type need to be factored out.
One way to dothis is having the benchmark corpus consistently(i.e.
with approximately 100% consensus aboutthe correct analysis) analysed by people with nofamiliarity with the tagger's behaviour in differ-ent situations - provided this is possible in thefirst place.Another two minor questions left open by Vou-tilainen and Jiirvinen concern the (i) typology ofthe differences and (ii) the reliability of their ex-periment.Concerning the typology of the differences: inVoutilainen and J~irvinen's experiment the lin-guists negotiated about an initial difference, al-most one per cent of all words in the texts.Though they finally agreed about the correct anal-ysis in almost all these differences, with a slightimprovement in the experimental setting a clearcategorisation of the initial differences into un-intentional mistakes and other, more interestingtypes, could have been made.Secondly, the texts used in Voutilainen andJ~vinen's experiment comprised only about 6,000words.
This is probably enough to give a generalindication of the nature of the analysis task withthe ENGCG tag set, but a larger data would in-crease the reliability of the experiment.In this paper, we address all these three clues-'tions.
Two young linguists 1 with no backgroundin ENGCG tagging were hired for making an elab-orated version of the Voutilainen and J~vinen ex-periment with a considerably arger corpus.The rest of this paper is structured as follows.Next, the ENGCG tag set is described in outline.Then the training of the new linguists is described,as well as the test data and experimental setting.Finally, the results are presented.2 ENGCG tag  setDescriptions of the morphological tags used bythe English Constraint Grammar tagger are avail-able in several publications.
Brief descriptions canbe found in several recent ACL conference pro-ceedings by Voutilainen and his colleagues (e.g.EACL93, ANLP94, EACL95, ANLP97, ACL-EACL97).
An in-depth description is given inKarlsson et al, eds., 1995 (chapters 3-6).
Here,only a brief sample is given.ENGCG tagging is a two-phase process.
First,a lexical analyser assigns one or more alternativeanalyses to each word.
The following is a mor-phological analysis of the sentence The raids werecoordinated under a recently expanded federal pro-gram:"<The>""the" <Def> DET CENTRAL ART SG/PL"<raids>""raid" <Count> N NOM PL"raid" <SVO> V PRES SG3"<were>""be" <SVC/A> <SVC/N> V PAST"<coordinated>""coordinate" <SVO> EN"coordinate" <SVO> V PAST"<under>""under" ADV ADVL"under" PREP"under" <Attr> A ABS"<a>""a" ABBR NOM SG"a" <Indef> DET CENTP~L ART SG1Ms.
Pirkko Paljakl~ and Mr. Markku Lappalainen205Proceedings of EACL '99"<re cent ly>""recent" <DER:Iy> ADV"<expanded>""expand" <SV0> <P/on> EN"expand" <SV0> <P/on> V PAST"<f ede ral>""federal" A ABS- <program>.
"program" N N0M SG"program" <SV0> V PRES -SG3"program" <SV0> V INF"program" <SV0> V IMP"program" <SV0> V SUBJUNCTIVE,,<.
>.Each indented line constitutes one morphologi-cal analysis.
Thus program is five-ways ambiguousafter ENGCG morphology.
The disambiguationpart of the ENGCG tagger ~ then removes thosealternative analyses that are contextually illegit-imate according to the tagger's hand-coded con-straint rules (Voutilainen 1995).
The remai-~nganalyses constitute the output of the tagger, inthis case:"<The >""the" <Def> DET CENTRAL ART SG/PL"<raids>""raid" <Count> N N0M PL"<were>""be" <SYC/A> <SVC/N> Y PAST"<coordinated>""coordinate" <SV0> EN"<under>""under" PREP"<a>""a" <Indef> DET CENTRAL ART SG"<recently>""recent" <DER:Iy> ADV"<expanded>""expand" <SV0> <P/on> EN"<federal>""federal" A ABS"<program>""program" N N0M SG..<.
>,,Overall, this tag set represents about 180 differ-ent analyses when certain optional auxiliary tags(e.g.
verb subcategorisation tags) are ignored.3 Preparations for the experiment3.1 Exper imenta l  set t ingThe experiment was conducted as follows.2A new version of the tagger, known as EngCG-2,can be studied and tested at http://www.conexor.fi.1.
The text was morphologically analysed us-ing the ENGCG morphological nalyser.
Forthe analysis of unrecognlsed words, we useda rule-based heuristic omponent that assignsmorphological nalyses, one or more, to eachword not represented in the lexicon of the sys-tem.
Of the analysed text, two identical ver-sions were made, one for each linguist.2.
Two linguists trained to disambiguate theENGCG morphological representation (seethe subsection on training below) indepen-dently marked the correct alternative anal-yses in the ambiguous input, using mainlystructural, but in some structurally unresolv-able cases also higher-level, information.
Thecorpora consisted of continuous text ratherthan isolated sentences; this made the useof textual knowledge possible in the selectionof the correct alternative.
In the rare caseswhere two analyses were regarded as equallylegitimate, both could be marked.
The judgeswere encouraged to consult the documenta-tion of the grammatical representation.
Inaddition, both linguists were provided with achecking program to be used after the textwas analysed.
The program identifies wordsleft without an analysis, in which case thelinguist was to provide the m~.~sing analysis.3.
These analysed versions of the same text werecompared to each other using the Unix sdiffprogram.
For each corpus version, words witha different analysis were marked with a "RE-CONSIDER" symbol.
The "RECONSIDER"symbol was also added to a number of otherambiguous words in the corpus.
These addi-tional words were marked in order to 'force'each linguist to think independently aboutthe correct analysis, i.e.
to prevent he emer-gence of the situation where one linguist con-siders the other to be always right (or wrong)and so 'reconsiders' only in terms of the ex-isting analysis.
The linguists were told thatsome of the words marked with the "RECON-SIDER" symbol were analysed ifferently bythem.4.
Statistics were generated about the num-ber of differing analyses (number of "RE-CONSIDER" symbols) in the corpus versions("diffl" in the following table).5.
The reanalysed versions were automaticallycompared to each other.
To words with adifferent analysis, a "NEGOTIATE" symbolwas added.206Proceedings of EACL '996.
Statistics were generated about the num-ber of differing analyses (number of "NE-GOTIATE" symbols) in the corpus versions("diff2" in the following table).7.
The remaining differences in the analyseswere jointly examined by the linguists in or-der to see whether they were due to (i) inat-tention on the part of one linguist (as a resultof which a correct unique analysis was jointlyagreed upon), (ii) joint uncertainty about hecorrect analysis (both linguists feel unsureabout the correct analysis), or (iii) conflict-ing opinions about the correct analysis (bothlinguists have a strong but different opinionabout the correct analysis).8.
Statistics were generated about the numberof conflicting opinions ("dill3" below) andjoint uncertainty ("unsure" below).This routine was successively applied to eachtext.3.2 TrainingTwo people were hired for the experiment.
Onehad recently completed a Master's degree fromEnglish Philology.
The other was an advanced un-dergraduate student majoring in English Philol-ogy.
Neither of them were familiar with theENGCG tagger.All available documentation about the linguisticrepresentation used by ENGCG was made avail-able to them.
The chief source was chapters 3-6in Karlsson et al (eds., 1995).
Because the lin-guistic solutions in ENGCG are largely based onthe comprehensive d scriptive grammar by Quirket al (1985), also that work was made availableto them, as well as a number of modern Englishdictionaries.The training was based on the disambiguationof ten smallish text extracts.
Each of the extractswas first analysed by the ENGCG morphologicalanalyser, and then each trainee was to indepen-dently perform Step 3 (see the previous subsec-tion) on it.
The disambiguated text was then au-tomatically compared to another version of thesame extract hat was disambiguated byan experton ENGCG.
The ENGCG expert then discussedthe analytic differences with the trainee who hadalso disambiguated the text and explained whythe expert's analysis was correct (almost alwaysby identifying a relevant section in the availableENGCG documentation; i  very rare cases wherethe documentation was underspecific, new docu-mentation was created for future use in the exper-iments).After analysis and subsequent consultation withthe ENGCG expert, the trainee processed the foblowing sample.The training lasted about 30 hours.
It was con-cluded by familiarising the linguists with the rou-tine used in the experiment.3.3 Test corpusFour texts were used in the experiment, to-tailing 55724 words and 102527 morphologi-cal analyses (an average of 1.84 analyses perword).
One was an article about Japaneseculture ('Pop'); one concerned patents ('Pat');one contained excerpts from the law of Cali-fornia; one was a medical text ('Med').
Noneof them had been used in the development ofthe ENGCG grammatical representation r otherparts of the system.
By mid-June 1999, a sam-ple of this data will be available for inspectionat http://www.ling.helsinki.fi/ voutilai/eac199-data.html.4 Resu l ts  and  d i scuss ionThe following table presents the main findings.Figure 1: .Results from a human annotation task.\[ ,oo,,as\[ aifflPop 14861 188/1.3%Pat 13183 92/.7%Law 15495 107/.7%ivied 12185 126/1.0%ALL 55724 513/.9%I diff~ I diff3~'ml \ [11/.1% 2/.0%'u.nsu're4/.0%1/.o%18/.1% 10/.1% 039/.3% 1/.0% 9/.1%112/.2% 13/.0% 14/.0%It is interesting to note how high the agree-ment between the linguists is even before the firstnegotiations (99.80% of all words are analysedidentically).
Of the remaining differences, most,somewhat disappointingly, turned out to be clas-sifted as 'slips of attention'; upon inspection theyseemed to contain little linguistic interest.
Espe-cially one of the linguists admitted that most ofthe job seemed too much of a routine to keep onementally alert enough.
The number of genuineconflicts of opinion were much in line with obser-vations by Voutilainen and J~irvinen.
However,the negotiations were not altogether easy, consid-ering that in all they took almost nine hours.
Pre-sumably uncertain analyses and conflicts of opin-ion were not easily passed by.The main finding of this experiment is thatbasically Voutilainen and J~vinen's observationsabout he high specifiability and consistent usabil-ity of the ENGCG morphological tag set seem tobe extendable to new users of the tag set.
In207Proceedings of EACL '99other words, the reputedly surface-syntactic agset seems to be learnable as well.
Overall, the ex-periment reported here provides evidence for theoptimistic position about the specifiability of atleast certain kinds of linguistic representations.It remains for future research, perhaps as a col-laboration between teams working with differenttag sets, to find out, what exactly are the prop-erties that make some linguistic representationsconsistently learnable and usable, and others lessSO.AcknowledgmentsI am grateful to anonymous EACL99 referees foruseful comments.Re ferencesKenneth W. Church 1992.
Current Practice inPart of Speech Tagging and Suggestions for theFuture.
In Simmons (ed.
), Sbornik praci: InHonor of Henry Kucera, Michigan Slavic Studies.Michigan.
13-48.Elizabeth Eyes and Geoffrey Leech 1993.
Syn-tactic Annotation: Linguistic Aspects of Gram-matical Tagging and Skeleton Parsing.
In EzraBlack, Roger Garside and Geoffrey Leech (eds.)1993.
Statistically-Driven Computer Grammarsof English: The IBM/Lancaster Approach.
Am-sterdam and Atlanta: Rodopi.
36-61.Fred Karlsson, Atro Voutilainen, Juha Heil~kil~iand A.rto Anttila (eds.)
1995.
Constraint Gram-mar.
A Language-Independent System for Pars-ing Unrestricted Tezt.
Berlin and New York:Mouton de Gruyter.Mitchell Marcus, Beatrice Santorini and MaryAnn Marcinkiewicz 1993.
Building a Large An-notated Corpus of English: The Penn Treebank.Computational Linguistics 19:2.
313-330.Randolph Quirk, Sidney Greenbaum, JanSvartvik and Geoffrey Leech 1985.
A Comprehen-sive Grammar of the English Language.
Longman.Atro Voutilainen 1995.
Morphological disam-biguation.
In Karlsson et al, eds.Atro Voutilainen and Timo J~vinen 1995.Specifying a shallow grammatical representationfor parsing purposes.
In Proceedings of the Sev-enth Conference of the European Chapter of theAssociation for Computational Linguistics.
ACL.208
