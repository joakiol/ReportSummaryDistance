Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1571?1580,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPLatent Document Re-RankingDong Zhou1,2                       Vincent Wade11.
University of Dublin, Trinity College, Dublin 2, Ireland2.
School of Computer and Communication, Hunan University, Changsha,Hunan, Chinadongzhou1979@hotmail.com      Vincent.Wade@cs.tcd.ieAbstractThe problem of re-ranking initial retrieval re-sults exploring the intrinsic structure of docu-ments is widely researched in information re-trieval (IR) and has attracted a considerableamount of time and study.
However, one ofthe drawbacks is that those algorithms treatqueries and documents separately.
Further-more, most of the approaches are predomi-nantly built upon graph-based methods, whichmay ignore some hidden information amongthe retrieval set.This paper proposes a novel document re-ranking method based on Latent Dirichlet Al-location (LDA) which exploits the implicitstructure of the documents with respect tooriginal queries.
Rather than relying on graph-based techniques to identify the internal struc-ture, the approach tries to find the latent struc-ture of ?topics?
or ?concepts?
in the initial re-trieval set.
Then we compute the distance be-tween queries and initial retrieval results basedon latent semantic information deduced.
Em-pirical results demonstrate that the method cancomfortably achieve significant improvementover various baseline systems.1 IntroductionConsider a traditional IR problem, where thereexists a set of documents ?
in the collection.
Inresponse to an information need (as expressed ina query ?
), the system determines a best fit be-tween the query and the documents and returns alist of retrieval results, sorted in a decreasing or-der of their relevancy.
In practice, high precisionat the top rankings of the returned results is ofparticular interest.
Generally, there are two waysto automatically assist in achieving this ultimategoal after an initial retrieval process (Baeza-Yates and Ribeiro-Neto, 1999): document re-ranking and query expansion/re-weighting.
Sincethe latter normally need a second round of re-trieval process, our method focuses on the docu-ment re-ranking approach.
We will focus on ad-justing the ranking positions directly over initialretrieval results set ?????
.Recently, there is a trend of exploring the hid-den structure of documents to re-rank results.Some of the approaches represent the documententities as a connected graph ?.
It is usually con-structed by links inferred from the content in-formation as a nearest-neighbor graph.
For ex-ample, Zhang et al (2005) proposed an affinityranking graph to re-rank search results by opti-mizing diversity and information richness.
Kur-land and Lee (2005) introduced a structural re-ranking approach by exploiting asymmetric rela-tionships between documents induced by lan-guage models.
Diaz (2005); Deng et al (2009)use a family of semi-supervised machine learn-ing methods among documents graph con-structed by incorporating different evidences.However in this work we are more interested inadopting an automatic approach.There are two important factors that should betaken into account when designing any re-ranking algorithms: the original queries and ini-tial retrieval scores.
One of issues is that pre-vious structural re-ranking algorithms treat thequery and the content individually when compu-ting re-ranking scores.
Each document is as-signed a score independent of other documentswithout considering of queries.
The problem wewant to address in this paper is how we can leve-rage the interconnections between query anddocuments for the re-ranking purpose.Another problem with such approaches con-cerns the fundamental re-ranking strategy theyadopted.
HITS (Kleinberg, 1999) and PageRank1571(Brin and Page, 1998) style algorithms werewidely used in the past.
However, approachesdepend only on the structure of the global graphor sub-graph may ignore important informationcontent of a document entity.
As pointed out byDeng et al (2009), re-ranking algorithms thatrely only on the structure of the global graph arelikely lead to the problem of topic drift.Instead, we introduce a new document re-ranking method based on Latent Dirichlet Allo-cation (LDA) (Blei et al, 2003) which exploitsimplicit structure of the documents with respectto original queries.
Rather than relying on graph-based techniques to identify the internal struc-ture, the approach tries to directly model the la-tent structure of ?topics?
or ?concepts?
in theinitial retrieval set.
Then we can compute thedistance between queries and initial retrieval re-sults based on latent semantic information in-ferred.
To prevent the problem of topic drift, thegenerative probability of a document is summedover all topics induced.
By combining the initialretrieval scores calculated by language models,we are able to gather important information forre-ranking purposes.
The intuition behind thismethod is the hidden structural informationamong the documents: similar documents arelikely to have the same hidden information withrespect to a query.
In other words, if a group ofdocuments are talking about the same topicwhich shares a strong similarity with a query, inour method they will get alocated similar rank-ing as they are more likely to be relevant to thequery.
In addition, the refined ranking scoresshould be relevant to the initial ranking scores,which, in our method, are combined togetherwith the re-ranking score either using a linearfashion or multiplication process.To illustrate the effectiveness of the proposedmethodology, we apply the framework to ad-hocdocument retrieval and compare it with the initiallanguage model-based method and other threePageRank style re-ranking methods.
Experimen-tal results show that the improvement brought byour method is consistent and promising.The rest of the paper is organized as follows.Related work on re-ranking algorithms and LDAbased methods is briefly summarized in Section2.
Section 3 describes the re-ranking frameworkbased on latent information induced togetherwith details of how to build generative model.
InSection 4 we report on a series of experimentsperformed over three different test collections inEnglish and French as well as results obtained.Finally, Section 5 concludes the paper and specu-lates on future work.2 Related WorkThere exist several groups of related work in theareas of document retrieval and re-ranking.The first category performs re-ranking by us-ing inter-document relationship (Lee et al,2001), evidences obtained from external re-sources (Kamps, 2004), or through local contextanalysis (Xu and Croft, 2000).
In the past, docu-ment distances (Balinski and Daniowicz, 2005),manually built external thesaurus (Qu et al,2001), and structural information (such as docu-ment title) (Luk and Wong, 2004), etc have beenused extensively for this very purpose.A second category of work is related to recentadvances in structural re-ranking paradigm overgraphs.
Kurland and Lee performed re-rankingbased on measures of centrality in the graphformed by generation links induced by languagemodel scores, through a weighted version of Pa-geRank algorithm (Kurland and Lee, 2005) andHITS-style cluster-based approach (Kurland andLee, 2006).
Zhang et al (2005) proposed a simi-lar method to improve web search based on alinear combination of results from text searchand authority ranking.
The graph, which theynamed affinity graph, shares strong similaritieswith Kurland and Lee?s work with the links in-duced by a modified version of cosine similarityusing the vector space model.
Diaz (2005) usedscore regularization to adjust document retrievalrankings from an initial retrieval by a semi-supervised learning method.
Deng et al (2009)further developed this method.
They built a latentspace graph based on content and explicit linksinformation.
Unlike their approach we are tryingto model the latent information directly.This work is also related to a family of me-thods so called latent semantic analysis (LSA)(Landauer et al, 1998), especially topic modelsused for document representation.
Latent Dirich-let Allocation (LDA), after it was first introducedby Blei et al (2003), has quickly become one ofthe most popular probabilistic text modelingtechniques and has inspired research rangingfrom text classification and clustering (Phan etal., 2008), information discovery (Mei et al,2007; Titov and McDonald, 2008) to informationretrieval (Wei and Croft, 2006).
In this model,each topic is represented by a set of words andeach word corresponds with a weight to measureits contribution to the topic.
Wei and Croft1572(2006) described large-scale information retriev-al experiments by using LDA.
In their work,LDA-based document model and language mod-el-based document model were linearly com-bined to rank the entire corpus.
However, unlikethis approach we only apply LDA to a small setof documents.
There are two reasons by doingso.
One is the concern of computational cost.LDA is a very complex model and the complexi-ty will grow linearly with the number of topicsand the number of documents.
Only running itthrough a document set significantly smaller thanthe whole corpus has obvious advantages.
Se-condly, it is well known that LSA-based methodsuffers from an incremental build problem.
Nor-mally adding new documents to the corpus needsto ?be folded in?
to the latent representation.Such incremental addition fails to capture the co-occurrences of the newly added documents (andeven ignores all new terms they contain).
Assuch, the quality of the LSA representation willdegrade as more documents are added and willeventually require a re-computation of the LSArepresentation.
Because our method only requiresrunning LDA once for a small number of docu-ments, this problems could be easily avoided.
Inaddition, we also introduce two new measures tocalculate the distance between a query and adocument.3 Latent Re-Ranking FrameworkIn this section, we describe a novel document re-ranking method based on extracting the latentstructure among the initial retrieval set and mea-suring the distance between queries and docu-ments.3.1 Problem DefinitionLet ?
= {?1,?2,?
,??}
denote the set of docu-ments to be retrieved.
Given a query ?, a set ofinitial results ?????
?
?
of top documents arereturned by a standard information retrievalmodel (initial ranker).
However, the initial rankertends to be imperfect.
The purpose of our re-ranking method is to re-order a set of documents??????
so as to improve retrieval accuracy at thevery top ranks of the final results.3.2 Latent Dirichlet AllocationWe will first introduce Latent Dirichlet Alloca-tion model which forms the basis of the re-ranking framework that will be detailed in thenext subsection.
It was previously shown that co-occurrence structure of terms in text documentscan be used to recover some latent topic struc-tures without any usage of background informa-tion (Landauer et al, 1998).
This means that la-tent-topic representations of text allow modelingof linguistic phenomena such as synonymy andpolysemy.
By doing so, information retrievalsystems can match the information needs withcontent items on a meaning level rather than byjust lexical congruence.The basic generative process of LDA closelyresembles PLSA (Hofmann, 1999).
LDA extendsPLSA method by defining a complete generativemodel of text.
The topic mixture is drawn from aconjugate Dirichlet prior that remains the samefor all documents.
The process of generating adocument corpus is as follows:1) Pick a multinomial distribution ?
?
foreach topic ?
from a Dirichlet distribu-tion with hyperparameter ?
.2) For each document ?
, pick a multi-nomial distribution ?
?
, from a Dirich-let distribution with hyperparameter?
.3) For each word token ?
in document?, pick a topic ?
?
{1??}
from themultinomial distribution ?
?
.4) Pick word ?
from the multinomialdistribution ?
?
.Thus, the likelihood of generating a corpus is:?
?1,?
,??
|?
,?=  ?(?
?
|?
)??=1?
?(??=1?
?|?
)?
?
??
?
?
?(??
|?,?
?)???=1???=1??
???
?Unlike PLSA model, LDA possesses fullyconsistent generative semantics by treating thetopic mixture distribution as a ?-parameter hid-den random variable.
LDA offers a new and in-teresting framework to model a set of documents.The documents and new text sequences (for ex-ample, queries) could be easily connected by?mapping?
them to the topics in the corpus.
Inthe next subsection we will introduce how toachieve this goal and apply it to document re-ranking.LDA is a complex model and cannot be solvedby exact inference.
There are a few approximateinference techniques available in the literature:variational methods (Blei et al, 2003), expecta-tion propagation (Griffiths and Steyvers, 2004)1573and Gibbs sampling (Griffiths and Steyvers,2004).
Gibbs sampling is a special case of Mar-kov-Chain Monte Carlo (MCMC) simulation andoften yields relatively simple algorithms.
For thisreason, we choose to use Gibbs sampling to es-timate LDA.According to Gibbs sampling, we need tocompute the conditional probability ?(??
|?
?
?
,?
),where ?
denotes the vector of all words and ?
?
?denotes the vector of topic assignment except theconsidered word at position ?
.
This probabilitydistribution can be derived as:?
??
?
?
?
,?
=??
,?
??
?
+ ??
?
( ???
+ ??)?
1??=1????
,?
??
+ ??
( ????
+ ??)?
1?
?=1where ??
,?
??
indicates the number of instances ofword ??
assigned to topic ?
= ?
, not includingthe current token and ???
,?
??
denotes the numberof words in document ??
assigned to topic ?
= ?,not including the current token.Then we can obtain the multinomial parametersets:???
,?
=????
+ ??????
+?
?=1 ????
,?
?
=???
?
+ ??
????
+?
?=1 ?
?The Gibbs sampling algorithm runs over threeperiods: initialization, burn-in and sampling.
Wedo not tune to optimize these parameters becausein our experiments the markov chain turns out toconverge very quickly.3.3 LDA-based Re-RankingArmed with this LDA methodology, we nowdescribe the main idea of our re-ranking method.Given a set of initial results ?????
, we are tryingto re-measure the distance between the query anda document.
In the vector space model, this dis-tance is normally the cosine or inner productmeasure between two vectors.
Under the proba-bilistic model framework, this distance can beobtained from a non-commutative measure of thedifference between two probability distributions.The distance used in our approach is the Kull-back-Leibler (KL) divergence (Kullback andLeibler, 1951).
Given two probability mass func-tion  ?
?
and ?(?
), the KL divergence (or rela-tive entropy) between ?
and ?
is defined as:?
(?| ?
= ?
?
?????(?)?(?
)In terms of text sequences (either queries ordocuments), the probability distribution can beregarded as a probabilistic language model ?
?or ??
from each document ?
or each query ?.
Inother words, it assumes that there is an underly-ing language model which ?generates?
a term(sequence) (Ponte and Croft, 1998).
The unigramlanguage model is utilized here.
There are sever-al ways to estimate the probabilities.
Let?(?
?
?)
denotes the number of times the term?
occurs in a document ?
(same idea can beused on a query).
The Maximum-likelihood es-timation (MLE) of ?
with respect to ?
is definedas:?????
??(?
?
?)?(??
?
?)?
?Previous work in language-model-based in-formation retrieval (Zhai and Lafferty, 2004)advocates the use of a Dirichlet-smoothed esti-mation:?????
??
?
?
?
+ ?
?
??????(??
?
?)?
?
+ ?where smoothing parameter ?
controls the de-gree of reliance on relative frequencies in thedocument corpus rather than on the counts in ?.The initial ranker that we choose to use later inthe experiment computes the KL divergence be-tween the ?????
and a modified version of?????
(Zhai and Lafferty, 2001).Both estimations can be easily extended to dis-tributions over text sequences by assuming thatthe terms are independent:????
(?1?2 ???)
?
????(??
)??=1????
(?1?2 ???)
?
????
(??
)?
?=1In the re-ranking setting, we estimate that theprobability of a document ?
generates ?, using amixture model LDA.
It uses a convex combina-tion of a set of component distributions to modelobservations.
In this model, a word  ?
is gener-ated from a convex combination of some hiddentopics ?:????
?
= ?
?
?
?(?|?)?
?=1where each mixture model ?(?|?)
is a multi-nomial distribution over terms that correspond toone of the latent topics ?.
Similar to MLE andDIR estimations, this could be generated to givea distribution on a sequence of text:1574????
(?1?2 ???)
?
????
(??
)?
?=1Then the distance between a query and a doc-ument based on this model can be obtained.
Thefirst method we propose here adopts the KL di-vergence between the query terms and documentterms to compute a Re-Rank score ???????1:??????
?1 = ??(????(?)||????
?
)This method also has the property of length-normalization to ameliorate long document biasproblems (Kurland and Lee, 2005).The second method also measures a KL diver-gence between a query and a document, however,in a different way.
As in the original LDA model,the multinomial parameter  ?
?
indicates the topicdistribution of a document ?
.
Query ?
can beconsidered as topic estimation of a unknowndocument ?
.
Thus by first randomly assigningtopics to words and then performing a number ofloops through the Gibbs sampling update, wehave:?
??
?
?
?
,?
; ?
?
?
,?=??
,?
??
?
+ ?
?
,?
??
?
+ ??
?
( ???
+ ?
??
+ ??)?
1??=1???
?
,?
??
+ ??
( ??
??
+ ??)?
1?
?=1where ?
?
,?
??
?
counts the observations of word ?
?and topic ?
in unseen document.
Then the topicdistribution for the query (just the unseen docu-ment ?
?)
is:?
?
?
,?
=??
??
+ ????
??
+?
?=1 ?
?so that the distance between a query ?
and a doc-ument ?
is defined as the KL divergence be-tween the topic distributions of ?
and ?
.
Thenthe re-ranking score is calculated as:??????
?2 = ??(?
?
||?
?
)Thus we can re-rank the initial retrieved docu-ments according to the scores acquired.
However,as in other topic models, a topic in the LDAmodel represents a combination of words, and itmay not be as precise a representation as wordsin language model.
Hence we need to furtherconsider how to combine initial retrieval scoreswith the re-ranking scores calculated.
Two com-bination methods will be presented in the nextsubsection.3.4 Combining Initial Retrieval ScoresMotivated by the significant improvement ob-tained by (Wei and Croft, 2006) and (Zhang etal., 2005), we formulate our method through alinear combination of the re-ranking scores basedon initial ranker and the latent document re-ranker, shown as follow:?
?1 = (1?
?)
?
??
+ ?
?
???????
?where ??
denotes original scores returned by theinitial ranker and ?
is a parameter that can betuned with ?
= 0 meaning no re-ranking is per-formed.Another scheme considers a multiplicationcombination to incorporate the original score.
Itdoes not need to tune any parameters:?
?2 = ??
?
???????
?This concludes our overview of the proposedlatent re-ranking method.4 EvaluationIn this section, we will empirically study theeffectiveness of the latent document re-rankingmethod over three different data collections.4.1 Experimental SetupData The text corpus used in our experimentwas made up from elements of the CLEF-2007and CLEF-2008 the European Library (TEL)collections1 written in English and French.
Thesecollections are described in greater detail in Ta-ble 1.
All of the documents in the experimentwere indexed using the Lemur toolkit2.
Prior to1 http://www.clef-campaign.org2 http://www.lemurproject.orgCollection Contents Language Num of docs Size QueriesBL(CLEF2008)British LibraryDataEnglish(Main)1,000,100 1.2 GB 50BNF(CLEF2008)Biblioth?que Na-tionale de FranceFrench (Main) 1,000,100 1.3 GB 50LAT(CLEF2007)Los AngelesTimes 2002English 135,153 434 MB 50Table 1.
Statistics of test collections1575indexing, Porter's stemmer and a stopword list3were used for the English documents.
We use aFrench analyzer4 to analyze French documents.It is worth noting that the CLEF-2008 TELdata is actually multilingual: all collections to agreater or lesser extent contain records pointingto documents in other languages.
However this isnot a major problem because the majority ofdocuments in the test collection are written inmain languages of those test collections (BL-English, BNF-French).
Furthermore, documentswritten in different languages tend not to matchthe queries in main languages.
Also the data isvery different from the newspaper articles andnews agency dispatches previously used in theCLEF as well as TREC5.
The data tends to bevery sparse.
Many records contain only title, au-thor and subject heading information; otherrecords provide more detail.
The average docu-ment lengths are 14.66 for BL and 24.19 forBNF collections after pre-processing, respective-ly.
Please refer to (Agirre et al, 2008) for a moredetailed discussion about this data.
The reasonwe choose these data collections is that wewanted to test the scalability of the proposed me-thod in different settings and over differentguages.
In addition we also select a moretional collection (LAT from CLEF2007) as a testbase.We also used the CLEF-2007 and CLEF-2008 query sets.
The query sets consist of 50topics in English for LAT, BL and in French forBNF, all of which were used in the experiment.Each topic is composed of several parts such as:Title, Description, Narrative.
We chose toconduct Title+Description runs as queries.
Thequeries are processed similarly to the treatmentin the test collections.
The relevance judgmentsare taken from the judged pool of top retrieveddocuments by various participating retrievalsystems from previous CLEF workshops.We compare the proposed latent re-rankingmethod with four other approaches: the initialranker, mentioned above, is a KL-divergenceretrieval function using the language models.Three other baseline systems are: Kurland andLee?s structural re-ranking approach (RecursiveWeighted Influx + Language Model), chosen asit demonstrates the best performance in their pa-per (Kurland and Lee, 2005), Zhang et al?s af-finity graph-based approach (Zhang et al, 2005)3 ftp://ftp.cs.cornell.edu/pub/smart/4 http://lucene.apache.org/5 http://trec.nist.gov/and a variant of Kurland and Lee?s work withlinks in the graph calculated by the vector-spacemodel (cosine similarity as mentioned in (Kur-land and Lee, 2005)).
We denote these four sys-tems as InR, RWILM, AFF, and VEC respective-ly.
Furthermore, we denote the permutations ofour methods as follows: LDA1: ?
?2 ????
??????
?1 ,LDA2: ?
?1 ????
??????
?1  , LDA3:?
?2 ????
??????
?2 , LDA4: ?
?1 ????
??????
?2 .Because the inconsistency of the evaluationmetrics employed in the past work, we choose toemploy all of them to measure the effectivenessof various approaches.
These include: mean av-erage precision (MAP), the precision of the top 5documents (Prec@5), the precision of the top 10documents (Prec@10), normalized discountedcumulative gain (NDCG) (Jarvelin and Kekalai-nen, 2002) and Bpref (Buckley and Voorhees,2004).
Statistical-significant differences in per-formance were determined using a paired t-test ata confidence level of 95%.It is worth pointing out that the above meas-urements are not directly comparable with thoseof the CLEF participants because we restrictedour initial pool to a smaller number of documentsand the main purpose in the paper is to comparethe proposed method with different baseline sys-tems.Parameter Two primary parameters need tobe determined in our experiments.
For the re-ranking experiments, the combination parameter?
must be defined.
For the LDA estimation, thenumber of topics ?
must be specified.
We opti-mized settings for these parameters with respectto MAP, not with all other metrics over the BLcollection and apply them to all three collectionsdirectly.The search ranges for these two parameterswere:?
:     0.1, 0.2, ?, 0.9k :     5, 10, 15, ?, 45As it turned out, for many instances, the optim-al value of ?
with respect to MAP was either 0.1or 0.2, suggesting the initial retrieval scores havevaluable information inside them.
In contrast, theoptimal value of ?
was between 20 and 40.
Al-though this demonstrates a relatively large va-riance, the differences in terms of MAP haveremained small and statistically insignificant.
Weset ?????
to 50 in all results reported, as in Kur-land and Lee?s paper (Kurland and Lee, 2005)and we later show that the performance turns outto be very stable when this set enlarged.1576Table 2.
Experimental Results.
For each evaluation setting, improvements over the RWILM baselineare given in italics (because it has highest performance); statistically significant differences between ourmethods and InR, RWILM, AFF, VEC are indicated by o, l, a, v, respectively.
Bold highlights the bestresults over all algorithms.MAP Prec@5 Prec@10 NDCG BprefInR 0.1913 0.52 0.452 0.3489 0.2287RWILM0.21520.532 0.468 0.3663 0.2242AFF 0.1737 0.444 0.434 0.3273 0.22VEC 0.1756 0.448 0.434 0.3258 0.2216LDA1 0.21 o, a, v 0.544 a, v 0.47 0.3679 o, a, v 0.2429 a, vLDA2 0.2148 o, a, v 0.58 o, a, v 0.5 o, a, v 0.3726 o, a, v 0.2491 o, l, a, vLDA3 0.1673 0.452 0.402 0.3297 0.2LDA4 0.2035 o, a, v 0.548 a, v 0.468 a, v 0.3626 o, a, v 0.2326 aMAP Prec@5 Prec@10 NDCG bprefInR 0.1266 0.268 0.216 0.2456 0.1482RWILM 0.1274 0.264 0.218 0.2495 0.1498AFF 0.108 0.248 0.21 0.2221 0.1404VEC 0.1126 0.252 0.214 0.2262 0.1463LDA1 0.1374 a, v 0.292 a 0.242 0.2544 a, v 0.1617LDA2 0.1452 o, a, v 0.292 a, v 0.244 a 0.2608 o, a, v 0.1697 o, l, a, vLDA3 0.1062 0.232 0.202 0.2226 0.1439LDA4 0.1377 a,v 0.28 a 0.246 o, a, v 0.2507 a, v 0.1672 o, a, vMAP Prec@5 Prec@10 NDCG bprefInR 0.3119 0.568 0.48 0.5093 0.3105RWILM 0.3097 0.556 0.478 0.5096 0.3064AFF 0.3065 0.572 0.492 0.5037 0.312VEC 0.301 0.536 0.474 0.4975 0.3087LDA1 0.3253 v 0.584 v 0.502 v 0.5158 v 0.3339 o, l, vLDA2 0.3271 a, v 0.584 o, v 0.496 0.518 o, v 0.3351 o, l, a, vLDA3 0.2848 0.444 0.398 0.486 0.2879LDA4 0.3274 o 0.552 0.478 0.5202 o, v 0.3396 o, l, vBLBNFLAT02Lastly, the parameters in the baseline systemsare set according to the tuning procedures in theiroriginal papers6.6 More specifically, the combination parameter wasset to 0.5 for AFF, the number of links was set to 4 forRWILM.4.2 ResultsPrimary Evaluation The main experimentalresults are presented in Table 2.
The first fourrows in each collection specify reference-comparison data.
The first question we are inter-ested in is how our latent re-ranking methods1577perform (taken as a whole).
It is shown that ourmethods bring improvements upon the variousbaselines in 75% of the 48 relevant comparisons(4 latent re-ranking methods ?
4 corpora ?
4baselines).
Only the algorithm permutationLDA3 performs less well.
Furthermore, our me-thods are able to achieve the highest performanceacross all the evaluation metrics over three testcollections except in one case (MAP in BL col-lection).
An even more exciting observation isthat in many cases, our methods, even thoughtuned for MAP, can outperform various baselinesfor all the evaluation metrics, with statisticallysignificant improvements in many runs.A closer examination of the results in Table 2reveals some interesting properties.
As expected,the RWILM method bought improvements inmany cases in CLEF-2008 test collections.
How-ever, the performance over CLEF-2007 collec-tion was somewhat disappointing.
This seems toindicate that the language model induced graphmethod tends to perform better in sparse datarather than longer documents.
Also LanguageModeling requires large set training data to beeffective, while the complexity of our method isonly linear with number of topics and the numberof documents for each iteration.
The affinity andvector graph based methods demonstrated poorperformance across all the collections.
This maybe due to the fact that the approach Zhang et al(Zhang et al, 2005) developed focuses more ondiversity and information richness and cares lessabout the precision of the retrieval results whileasymmetric graph as constructed by the vectorspace model fails in capturing important relation-ship between the documents.Another observation we can draw from Table2 is that the relative performance tends to be sta-ble during test collections written in differentlanguages.
This shows a promising future forstudying structure of the documents with respectto queries for re-ranking purpose.
At the sametime, efficiency is always an issue in all re-ranking methods.
Although this is not a primaryconcern in the current work, it would definitelyworth thinking in the future.We also conducted some experiments overqueries constructed by using Title field only.This forms some more realistic short queries.The experiments showed very similar resultscompared to longer queries.
This demonstratesthat the query length is a trivial issue in our me-thods (as in other graph-based structural re-ranking).
We examined the best and worse per-formed queries, their performance are generallyconsistent across all the methods.
This phenome-non should be investigated further in the followup evaluation.Comparison of Different Methods In com-parison of performance between four permuta-tions of our methods, LDA2 is the clear winnerover CLEF-2008 test collections.
The results ob-tained by LDA2 and LDA4 over CLEF-2007 testcollection were mixed.
LDA2 performed betterin precision at top ?
documents while LDA4showed promising results in terms of more gen-eral evaluation metrics.
On the other hand, thelinear combination approach performed muchbetter than multiplication based combination.The situation is even worse when we adopted the??????
?2  method, which was inferior in severalcases.
Thus the linear combination should behighly recommended.Scalability We have shown that our latentdocument re-ranking method is successful at ac-complishing the goal of improving the resultsreturned by an initial retrieval engine.
But onemay raise a question of whether it is necessary torestrict our attention to an initial pool  ?????
atsuch a small size.
As it happens, preliminary ex-periments with LDA2 on larger size of the initialpool are presented in Figure 1.
As we can see,our method can bring consistently stable im-provements.Figure 1.
Experiments with larger initial pools5 Conclusion and Future WorkIn this paper we proposed and evaluated a la-tent document re-ranking method for re-orderingthe initial retrieval results.
The key to refine theresults is finding the latent structure of ?topics?or ?concepts?
in the document set, which leve-00.10.20.30.40.50.60.750 100 200 300 400 500Size of the initial retrieval setMAP (InR) MAP (LDA2) Prec@5 (InR) Prec@5 (LDA2)Prec@10 (InR) Prec@10 (LDA2) NDCG (InR) NDCG (LDA2)B ref (InR) Bpref (LDA2)1578rages the latent Dirichlet alocation technique forthe query-dependent ranking problem and resultsin state-of-art performance.There are many research directions we areplanning to investigate.
It has been shown thatLDA-based retrieval is a promising method forranking the whole corpus.
There is a desire tocall for a direct comparison between ranking andre-ranking using the proposed algorithmic varia-tions.
Future work will also include the compari-son between our methods with other related ap-proaches, such as Kurland and Lee?s cluster-based approach (Kurland and Lee, 2006).There exist a sufficient number of latent se-mantic techniques such as singular vector de-composition, non-negative matrix factorization,PLSA, etc.
We are planning to explore these me-thods to compare their performance.
Also directre-ranking can be used to improve automaticquery expansion since better ranking in top re-trieved documents can be expected to improvethe quality of the augmented query.
We believethis is another fruitful line for future research.AcknowledgmentsThe authors would like to thank three anonym-ous reviewers for many constructive comments.This research is supported by the Science Foun-dation Ireland (Grant 07/CE/I1142) as part of theCentre for Next Generation Localisation(www.cngl.ie) at University of Dublin, TrinityCollege.ReferencesEneko Agirre, Giorgio M. Di Nunzio, Nicola Ferro,Thomas Mandl and Carol Peters (2008).
CLEF2008: Ad Hoc Track Overview.
In Working notes ofCLEF2008.Ricardo A. Baeza-Yates and Berthier Ribeiro-Neto(1999).
Modern Information Retrieval, Addison-Wesley Longman Publishing Co., Inc.Jaroslaw Balinski and Czeslaw Daniowicz (2005).
"Re-ranking method based on inter-document dis-tances."
Inf.
Process.
Manage.
41(4): 759-775.David M. Blei, Andrew Y. Ng and Michael I. Jordan(2003).
"Latent dirichlet alocation."
J. Mach.Learn.
Res.
3: 993-1022.Sergey Brin and Lawrence Page (1998).
"The anato-my of a large-scale hypertextual Web search en-gine."
Comput.
Netw.
ISDN Syst.
30(1-7): 107-117.Chris Buckley and Ellen M. Voorhees (2004).
Re-trieval evaluation with incomplete information.
InProceedings of the 27th annual international ACMSIGIR conference on Research and development ininformation retrieval, Sheffield, United Kingdom,ACM.
p. 25-32.Hongbo Deng, Michael R. Lyu and Irwin King(2009).
Effective latent space graph-based re-ranking model with global consistency.
In Proceed-ings of the Second ACM International Conferenceon Web Search and Data Mining, Barcelona, Spain,ACM.
p. 212-221.Fernando Diaz (2005).
Regularizing ad hoc retrievalscores.
In Proceedings of the 14th ACM interna-tional conference on Information and knowledgemanagement, Bremen, Germany, ACM.
p. 672-679.Thomas L. Griffiths and Mark Steyvers (2004).
Find-ing scientific topics.
In Proceeding of the NationalAcademy of Sciences.
p. 5228-5235.Thomas Hofmann (1999).
Probabilistic latent seman-tic indexing.
In Proceedings of the 22nd annual in-ternational ACM SIGIR conference on Researchand development in information retrieval, Berkeley,California, United States, ACM.
p. 50-57.Kalervo Jarvelin and Jaana Kekalainen (2002).
"Cu-mulated gain-based evaluation of IR techniques.
"ACM Trans.
Inf.
Syst.
20(4): 422-446.Jaap Kamps (2004).
Improving Retrieval Effective-ness by Reranking Documents Based on ControlledVocabulary In Proceedings of 26th European Con-ference on IR Research, ECIR 2004, Sunderland,UK.
p. 283-295.Jon M. Kleinberg (1999).
"Authoritative sources in ahyperlinked environment."
J. ACM 46(5): 604-632.S.
Kullback and R. A. Leibler (1951).
"On Informa-tion and Sufficiency."
The Annals of MathematicalStatistics 22(1): 79-86.Oren Kurland and Lillian Lee (2005).
PageRankwithout hyperlinks: structural re-ranking using linksinduced by language models.
In Proceedings of the28th annual international ACM SIGIR conferenceon Research and development in information re-trieval, Salvador, Brazil, ACM.
p. 306-313.Oren Kurland and Lillian Lee (2006).
Respect myauthority!
: HITS without hyperlinks, utilizing clus-ter-based language models.
In Proceedings of the29th annual international ACM SIGIR conferenceon Research and development in information re-trieval, Seattle, Washington, USA, ACM.
p. 83-90.Thomas  K. Landauer, Peter  W. Foltz and DarrellLaham (1998).
"An Introduction to Latent SemanticAnalysis."
Discourse Processes 25: 259-284.Kyung-Soon Lee, Young-Chan Park and Key-SunChoi (2001).
"Re-ranking model based on documentclusters."
Inf.
Process.
Manage.
37(1): 1-14.Robert W. P. Luk and K.F.
Wong (2004).
Pseudo-Relevance Feedback and Title Re-ranking for Chi-nese Information Retrieval.
In Working Notes of theFourth NTCIR Workshop Meeting, Tokyo, Japan,National Institute of Informatics.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Suand ChengXiang Zhai (2007).
Topic sentiment mix-ture: modeling facets and opinions in weblogs.
InProceedings of the 16th international conference onWorld Wide Web, Banff, Alberta, Canada, ACM.
p.171-180.1579Xuan-Hieu Phan, Le-Minh Nguyen and Susumu Ho-riguchi (2008).
Learning to classify short and sparsetext \& web with hidden topics from large-scale datacollections.
In Proceeding of the 17th internationalconference on World Wide Web, Beijing, China,ACM.
p. 91-100.Jay M. Ponte and W. Bruce Croft (1998).
A languagemodeling approach to information retrieval.
In Pro-ceedings of the 21st annual international ACM SI-GIR conference on Research and development in in-formation retrieval, Melbourne, Australia, ACM.
p.275-281.Youli Qu, Guowei Xu and Jun Wang (2001).
RerankMethod Based on Individual Thesaurus.
In Proceed-ings of the Second NTCIR Workshop on Research inChinese & Japanese Text Retrieval and Text Sum-marization, Tokyo, Japan, National Institute of In-formatics.Ivan Titov and Ryan McDonald (2008).
Modelingonline reviews with multi-grain topic models.
InProceeding of the 17th international conference onWorld Wide Web, Beijing, China, ACM.
p. 111-120.Xing Wei and W. Bruce Croft (2006).
LDA-baseddocument models for ad-hoc retrieval.
In Proceed-ings of the 29th annual international ACM SIGIRconference on Research and development in infor-mation retrieval, Seattle, Washington, USA, ACM.p.
178-185.Jinxi Xu and W. Bruce Croft (2000).
"Improving theeffectiveness of information retrieval with local con-text analysis."
ACM Trans.
Inf.
Syst.
18(1): 79-112.Chengxiang Zhai and John Lafferty (2001).
Model-based feedback in the language modeling approachto information retrieval.
In Proceedings of the tenthinternational conference on Information and know-ledge management, Atlanta, Georgia, USA, ACM.p.
403-410.Chengxiang Zhai and John Lafferty (2004).
"A studyof smoothing methods for language models appliedto information retrieval."
ACM Trans.
Inf.
Syst.22(2): 179-214.Benyu Zhang, Hua Li, Yi Liu, Lei Ji, Wensi Xi, Wei-guo Fan, Zheng Chen and Wei-Ying Ma (2005).Improving web search results using affinity graph.In Proceedings of the 28th annual internationalACM SIGIR conference on Research and develop-ment in information retrieval, Salvador, Brazil,ACM.
p. 504-511.1580
