Unsupervised Segmentation of Words Using Prior Distributions of MorphLength and FrequencyMathias CreutzNeural Networks Research Centre, Helsinki University of TechnologyP.O.Box 9800, FIN-02015 HUT, FinlandMathias.Creutz@hut.fiAbstractWe present a language-independent andunsupervised algorithm for the segmenta-tion of words into morphs.
The algorithmis based on a new generative probabilis-tic model, which makes use of relevantprior information on the length and fre-quency distributions of morphs in a lan-guage.
Our algorithm is shown to out-perform two competing algorithms, whenevaluated on data from a language withagglutinative morphology (Finnish), andto perform well also on English data.1 IntroductionIn order to artificially ?understand?
or produce nat-ural language, a system presumably has to know theelementary building blocks, i.e., the lexicon, of thelanguage.
Additionally, the system needs to modelthe relations between these lexical units.
Many ex-isting NLP (natural language processing) applica-tions make use of words as such units.
For in-stance, in statistical language modelling, probabil-ities of word sequences are typically estimated, andbag-of-word models are common in information re-trieval.However, for some languages it is infeasible toconstruct lexicons for NLP applications, if the lexi-cons contain entire words.
In especially agglutina-tive languages,1 such as Finnish and Turkish, the1In agglutinative languages words are formed by the con-catenation of morphemes.number of possible different word forms is simplytoo high.
For example, in Finnish, a single verbmay appear in thousands of different forms (Karls-son, 1987).According to linguistic theory, words are builtfrom smaller units, morphemes.
Morphemes are thesmallest meaning-bearing elements of language andcould be used as lexical units instead of entire words.However, the construction of a comprehensive mor-phological lexicon or analyzer based on linguistictheory requires a considerable amount of work byexperts.
This is both time-consuming and expen-sive and hardly applicable to all languages.
Further-more, as language evolves the lexicon must be up-dated continuously in order to remain up-to-date.Alternatively, an interesting field of research liesopen: Minimally supervised algorithms can be de-signed that automatically discover morphemes ormorpheme-like units from data.
There exist a num-ber of such algorithms, some of which are entirelyunsupervised and others that use some knowledge ofthe language.
In the following, we discuss recent un-supervised algorithms and refer the reader to (Gold-smith, 2001) for a comprehensive survey of previousresearch in the whole field.Many algorithms proceed by segmenting (i.e.,splitting) words into smaller components.
Oftenthe limiting assumption is made that words con-sist of only one stem followed by one (possiblyempty) suffix (De?jean, 1998; Snover and Brent,2001; Snover et al, 2002).
This limitation is reducedin (Goldsmith, 2001) by allowing a recursive struc-ture, where stems can have inner structure, so thatthey in turn consist of a substem and a suffix.
Alsoprefixes are possible.
However, for languages withagglutinative morphology this may not be enough.In Finnish, a word can consist of lengthy sequencesof alternating stems and affixes.Some morphology discovery algorithms learn re-lationships between words by comparing the ortho-graphic or semantic similarity of the words (Schoneand Jurafsky, 2000; Neuvel and Fulop, 2002; Baroniet al, 2002).
Here a small number of componentsper word are assumed, which makes the approachesdifficult to apply as such to agglutinative languages.We previously presented two segmentation algo-rithms suitable for agglutinative languages (Creutzand Lagus, 2002).
The algorithms learn a set ofsegments, which we call morphs, from a corpus.Stems and affixes are not distinguished as sepa-rate categories by the algorithms, and in that sensethey resemble algorithms for text segmentation andword discovery, such as (Deligne and Bimbot, 1997;Brent, 1999; Kit and Wilks, 1999; Yu, 2000).
How-ever, we observed that for the corpus size studied(100 000 words), our two algorithms were somewhatprone to excessive segmentation of words.In this paper, we aim at overcoming the problemof excessive segmentation, particularly when smallcorpora (up to 200 000 words) are used for training.We present a new segmentation algorithm, which islanguage independent and works in an unsupervisedfashion.
Since the results obtained suggest that thealgorithm performs rather well, it could possibly besuitable for languages for which only small amountsof written text are available.The model is formulated in a probabilisticBayesian framework.
It makes use of explicit priorinformation in the form of probability distributionsfor morph length and morph frequency.
The modelis based on the same kind of reasoning as the proba-bilistic model in (Brent, 1999).
While Brent?s modeldisplays a prior probability that exponentially de-creases with word length (with one character as themost common length), our model uses a probabil-ity distribution that more accurately models the reallength distribution.
Also Brent?s frequency distribu-tion differs from ours, which we derive from Man-delbrot?s correction of Zipf?s law (cf.
Section 2.5).Our model requires that the values of two param-eters be set: (i) our prior belief of the most commonmorph length, and (ii) our prior belief of the pro-portion of morph types2 that occur only once in thecorpus.
These morph types are called hapax legom-ena.
While the former is a rather intuitive measure,the latter may not appear as intuitive.
However, theproportion of hapax legomena may be interpreted asa measure of the richness of the text.
Also note thatsince the most common morph length is calculatedfor morph types, not tokens, it is not independent ofthe corpus size.
A larger corpus usually requires ahigher average morph length, a fact that is stated forword lengths in (Baayen, 2001).As an evaluation criterion for the performanceof our method and two reference methods we usea measure that reflects the ability to recognizereal morphemes of the language by examining themorphs found by the algorithm.2 Probabilistic generative modelIn this section we derive the new model.
We fol-low a step-by-step process, during which a morphlexicon and a corpus are generated.
The morphs inthe lexicon are strings that emerge as a result of astochastic process.
The corpus is formed throughanother stochastic process that picks morphs fromthe lexicon and places them in a sequence.
At twopoints of the process, prior knowledge is requiredin the form of two real numbers: the most commonmorph length and the proportion of hapax legomenamorphs.The model can be used for segmentation of wordsby requiring that the corpus created is exactly theinput data.
By selecting the most probable morphlexicon that can produce the input data, we obtain asegmentation of the words in the corpus, since wecan rewrite every word as a sequence of morphs.2.1 Size of the morph lexiconWe start the generation process by deciding the num-ber of morphs in the morph lexicon (type count).This number is denoted by n?
and its probabilityp(n?)
follows the uniform distribution.
This meansthat, a priori, no lexicon size is more probable thananother.32We use standard terminology: Morph types are the set ofdifferent, distinct morphs.
By contrast, morph tokens are theinstances (or occurrences) of morphs in the corpus.3This is an improper prior, but it is of little practical signif-icance for two reasons: (i) This stage of the generation process2.2 Morph lengthsFor each morph in the lexicon, we independentlychoose its length in characters according to thegamma distribution:p(l?i) =1?(?)??
l?i?
?1e?l?i/?, (1)where l?i is the length in characters of the ith morph,and ?
and ?
are constants.
?(?)
is the gamma func-tion:?(?)
=?
?0z??1e?zdz.
(2)The maximum value of the density occurs at l?i =(?
?
1)?, which corresponds to the most commonmorph length in the lexicon.
When ?
is set to one,and ?
to one plus our prior belief of the most com-mon morph length, the pdf (probability density func-tion) is completely defined.We have chosen the gamma distribution formorph lengths, because it corresponds rather well tothe real length distribution observed for word typesin Finnish and English corpora that we have stud-ied.
The distribution also fits the length distributionof the morpheme labels used as a reference (cf.
Sec-tion 3).
A Poisson distribution can be justified andhas been used in order to model the length distri-bution of word and morph tokens [e.g., (Creutz andLagus, 2002)], but for morph types we have chosenthe gamma distribution, which has a thicker tail.2.3 Morph stringsFor each morph ?i, we decide the character string itconsists of: We independently choose l?i charactersat random from the alphabet in use.
The probabil-ity of each character cj is the maximum likelihoodestimate of the occurrence of this character in thecorpus:4p(cj) =ncj?k nck, (3)where ncj is the number of occurrences of the char-acter cj in the corpus, and?k nck is the total num-ber of characters in the corpus.only contributes with one probability value, which will have anegligible effect on the model as a whole.
(ii) A proper prob-ability density function would presumably be very flat, whichwould hardly help guiding the search towards an optimal model.4Alternatively, the maximum likelihood estimate of the oc-currence of the character in the lexicon could be used.2.4 Morph order in the lexiconThe lexicon consists of a set of n?
morphs and itmakes no difference in which order these morphshave emerged.
Regardless of their initial order, themorphs can be sorted into a uniquely defined (e.g.,alphabetical) order.
Since there are n?!
ways to or-der n?
different elements,5 we multiply the proba-bility accumulated so far by n?!
:p(lexicon) = p(n?)n??i=1[p(l?i)l?i?j=1p(cj)]?n?!
(4)2.5 Morph frequenciesThe next step is to generate a corpus using the morphlexicon obtained in the previous steps.
First, we in-dependently choose the number of times each morphoccurs in the corpus.
We pursue the following lineof thought:Zipf has studied the relationship between the fre-quency of a word, f , and its rank, z.6 He suggeststhat the frequency of a word is inversely proportionalto its rank.
Mandelbrot has refined Zipf?s formula,and suggests a more general relationship [see, e.g.,(Baayen, 2001)]:f = C(z + b)?a, (5)where C, a and b are parameters of a text.Let us derive a probability distribution from Man-delbrot?s formula.
The rank of a word as a func-tion of its frequency can be obtained by solving forz from (5):z = C 1a f?
1a ?
b.
(6)Suppose that one wants to know the number ofwords that have a frequency close to f rather thanthe rank of the word with frequency f .
In order toobtain this information, we choose an arbitrary in-terval around f : [(1/?
)f .
.
.
?f [, where ?
> 1, andcompute the rank at the endpoints of the interval.The difference is an estimate of the number of words5Strictly speaking, our probabilistic model is not perfect,since we do not make sure that no morph can appear more thanonce in the lexicon.6The rank of a word is the position of the word in a list,where the words have been sorted according to falling fre-quency.that fall within the interval, i.e., have a frequencyclose to f :nf = z1/?
?
z?
= (?1a ?
??
1a )C 1a f?
1a .
(7)This can be transformed into an exponential pdfby (i) binning the frequency axis so that there areno overlapping intervals.
(This means that the fre-quency axis is divided into non-overlapping inter-vals [(1/?)f?
.
.
.
?f?
[, which is equivalent to havingf?
values that are powers of ?2: f?0 = ?0 = 1, f?1 =?2, f?2 = ?4, .
.
.
All frequencies f are rounded tothe closest f?
.)
Next (ii), we normalize the numberof words with a frequency close to f?
with the to-tal number of words?f?
nf?
.
Furthermore (iii), f?is written as elog f?
, and (iv) C must be chosen sothat the normalization coefficient equals 1/a, whichyields a proper pdf that integrates to one.
Note alsothe factor log ?2.
Like f?
, log f?
is a discrete variable.We approximate the integral of the density functionaround each value log f?
by multiplying with the dif-ference between two successive log f?
values, whichequals log ?2:p(f ?
[(1/?)f?
.
.
.
?f?
[) = ?1a ?
??
1a?f?
nf?C 1a e?
1a log f?= 1ae?
1a log f?
?
log ?2.
(8)Now, if we assume that Zipf?s and Madelbrot?sformulae apply to morphs as well as to words, wecan use formula (8) for every morph frequency f?i ,which is the number of occurrences (or frequency)of the morph ?i in the corpus (token count).
How-ever, values for a and ?2 must be chosen.
We set?2 to 1.59, which is the lowest value for which noempty frequency bins will appear.7 For f?i = 1, (8)reduces to log ?2/a.
We set this value equal to ourprior belief of the proportion of morph types that areto occur only once in the corpus (hapax legomena).2.6 CorpusThe morphs and their frequencies have been set.
Theorder of the morphs in the corpus remains to be de-cided.
The probability of one particular order is theinverse of the multinomial:7Empty bins can appear for small values of f?i due to f?i ?sbeing rounded to the closest f?
?i , which is a power of ?2.p(corpus) =((?n?i=1 f?i)!
?n?i=1 f?i !
)?1 =( N !
?n?i=1 f?i !)?1.
(9)The numerator of the multinomial is the factorial ofthe total number of morph tokens, N , which equalsthe sum of frequencies of every morph type.
The de-nominator is the product of the factorial of the fre-quency of each morph type.2.7 Search for the optimal modelThe search for the optimal model given our inputdata corresponds closely to the recursive segmen-tation algorithm presented in (Creutz and Lagus,2002).
The search takes place in batch mode, butcould as well be done incrementally.
All words inthe data are randomly shuffled, and for each word,every split into two parts is tested.
The most proba-ble split location (or no split) is selected and in caseof a split, the two parts are recursively split in two.All words are iteratively reprocessed until the prob-ability of the model converges.3 EvaluationFrom the point of view of linguistic theory, it is pos-sible to come up with different plausible sugges-tions for the correct location of morpheme bound-aries.
Some of the solutions may be more elegantthan others,8 but it is difficult to say if the most el-egant scheme will work best in practice, when realNLP applications are concerned.We utilize an evaluation method for segmentationof words presented in (Creutz and Lagus, 2002).
Inthis method, segments are not compared to one sin-gle ?correct?
segmentation.
The evaluation criterioncan rather be interpreted from the point of view oflanguage ?understanding?.
A morph discovered bythe segmentation algorithm is considered to be ?un-derstood?, if there is a low-ambiguity mapping fromthe morph to a corresponding morpheme.
Alterna-tively, a morph may correspond to a sequence ofmorphemes, if these morphemes are very likely tooccur together.
The idea is that if an entirely newword form is encountered, the system will ?under-stand?
it by decomposing it into morphs that it ?un-derstands?.
A segmentation algorithm that segments8Cf.
?hop + ed?
vs. ?hope + d?
(past tense of ?to hope?
).words into too small parts will perform poorly due tohigh ambiguity.
At the other extreme, an algorithmthat is reluctant at splitting words will have bad gen-eralization ability to new word forms.Reference morpheme sequences for the words areobtained using existing software for automatic mor-phological analysis based on the two-level morphol-ogy of Koskenniemi (1983).
For each word form,the analyzer outputs the base form of the word to-gether with grammatical tags.
By filtering the out-put, we get a sequence of morpheme labels that ap-pear in the correct order and represent correct mor-phemes rather closely.
Note, however, that the mor-pheme labels are not necessarily orthographicallysimilar to the morphemes they represent.The exact procedure for evaluating the segmenta-tion of a set of words consists of the following steps:(1) Segment the words in the corpus using the au-tomatic segmentation algorithm.
(2) Divide the segmented data into two parts ofequal size.
Collect all segmented word forms fromthe first part into a training vocabulary and collectall segmented word forms from the second part intoa test vocabulary.
(3) Align the segmentation of the words in thetraining vocabulary with the corresponding refer-ence morpheme label sequences.
Each morph mustbe aligned with one or more consecutive morphemelabels and each morpheme label must be alignedwith at least one morph; e.g., for a hypothetical seg-mentation of the English word winners?
:Morpheme labels win -ER PL GENMorph sequence w inn er s?
(4) Estimate conditional probabilities for themorph/morpheme mappings computed over thewhole training vocabulary: p(morpheme |morph).Re-align using the Viterbi algorithm and employ theExpectation-Maximization algorithm iteratively un-til convergence of the probabilities.
(5) The quality of the segmentation is evaluatedon the test vocabulary.
The segmented words in thetest vocabulary are aligned against their referencemorpheme label sequences according to the condi-tional probabilities learned from the training vocab-ulary.
To measure the quality of the segmentationwe compute the expectation of the proportion ofcorrect mappings from morphs to morpheme labels,E{p(morpheme |morph)}:1NN?i=1pi(morpheme |morph), (10)where N is the number of morph/morpheme map-pings, and pi(?)
is the probability associated withthe ith mapping.
Thus, we measure the proportionof morphemes in the test vocabulary that we can ex-pect to recognize correctly by examining the morphsegments.94 ExperimentsWe have conducted experiments involving (i) threedifferent segmentation algorithms, (ii) two corporain different languages (Finnish and English), and(iii) data sizes ranging from 2000 words to 200 000words.4.1 Segmentation algorithmsThe new probabilistic method is compared to twoexisting segmentation methods: the Recursive MDLmethod presented in (Creutz and Lagus, 2002)10and John Goldsmith?s algorithm called Linguistica(Goldsmith, 2001).11 Both methods use MDL (Min-imum Description Length) (Rissanen, 1989) as a cri-terion for model optimization.The effect of using prior information on the dis-tribution of morph length and frequency can be as-sessed by comparing the probabilistic method to Re-cursive MDL, since both methods utilize the samesearch algorithm, but Recursive MDL does not makeuse of explicit prior information.Furthermore, the possible benefit of using thetwo sources of prior information can be comparedagainst the possible benefit of grouping stems andsuffixes into signatures.
The latter technique is em-ployed by Linguistica.4.2 DataThe Finnish data consists of subsets of a news-paper text corpus from CSC,12 from which non-words (numbers and punctuation marks) have been9In (Creutz and Lagus, 2002) the results are reported lessintuitively as the ?alignment distance?, i.e., the negative logprobof the entire test set: ?
log?
pi(morpheme |morph).10Online demo at http://www.cis.hut.fi/projects/morpho/.11The software can be downloaded from http://humanities.uchicago.edu/faculty/goldsmith/Linguistica2000/.12http://www.csc.fi/kielipankki/removed.
The reference morpheme labels have beenfiltered out from a morphosyntactic analysis of thetext produced by the Connexor FDG parser.13The English corpus consists of mainly newspapertext (with non-words removed) from the Brown cor-pus.14 A morphological analysis of the words hasbeen performed using the Lingsoft ENGTWOL an-alyzer.15For both languages data sizes of 2000, 5000,10 000, 50 000, 100 000, and 200 000 have beenused.
A notable difference between the morpholog-ical structure of the languages lies in the fact thatwhereas there are about 17 000 English word typesin the largest data set, the corresponding number ofFinnish word types is 58 000.4.3 ParametersIn order to select good prior values for the prob-abilistic method, we have used separate develop-ment test sets that are independent of the final datasets.
Morph length and morph frequency distribu-tions have been computed for the reference mor-pheme representations of the development test sets.The prior values for most common morph length andproportion of hapax legomena have been adjusted inorder to produce distributions that fit the referenceas well as possible.We thus assume that we can make a good guess ofthe final morph length and frequency distributions.Note, however, that our reference is an approxima-tion of a morpheme representation.
As the segmen-tation algorithms produce morphs, not morphemes,we can expect to obtain a larger number of morphsdue to allomorphy.
Note also that we do not op-timize for segmentation performance on the devel-opment test set; we only choose the best fit for themorph length and frequency distributions.As for the two other segmentation algorithms, Re-cursive MDL has no parameters to adjust.
In Lin-guistica we have used Method A Suffixes + Find pre-fixes from stems with other parameters left at theirdefault values.
We are unaware whether anotherconfiguration could be more advantageous for Lin-guistica.13http://www.connexor.fi/14The Brown corpus is available at the Linguistic Data Con-sortium at http://www.ldc.upenn.edu/.15http://www.lingsoft.fi/2 5 10 50 100 2000102030405060FinnishCorpus size [1000 words]  (log.
scaled axis)Expectation(recognizedmorphemes)[%]ProbabilisticRecursive MDLLinguisticaNo segmentationFigure 1: Expectation of the percentage of recog-nized morphemes for Finnish data.4.4 ResultsThe expected proportion of morphemes recognizedby the three segmentation methods are plotted inFigures 1 and 2 for different sizes of the Finnishand English corpora.
The search algorithm usedin the probabilistic method and Recursive MDL in-volve randomness and therefore every value shownfor these two methods is the average obtained overten runs with different random seeds.
However, thefluctuations due to random behaviour are very smalland paired t-tests show significant differences at thesignificance level of 0.01 for all pair-wise compar-isons of the methods at all corpus sizes.For Finnish, all methods show a curve that mainlyincreases as a function of the corpus size.
The prob-abilistic method is the best with morpheme recogni-tion percentages between 23.5% and 44.2%.
Lin-guistica performs worst with percentages between16.5% and 29.1%.
None of the methods are closeto ideal performance, which, however, is lower than100%.
This is due to the fact that the test vocabu-lary contains a number of morphemes that are notpresent in the training vocabulary, and thus are im-possible to recognize.
The proportion of unrecog-nizable morphemes is highest for the smallest corpussize (32.5%) and decreases to 8.8% for the largestcorpus size.The evaluation measure used unfortunately scores2 5 10 50 100 2000102030405060EnglishCorpus size [1000 words]  (log.
scaled axis)Expectation(recognizedmorphemes)[%]ProbabilisticRecursive MDLLinguisticaNo segmentationFigure 2: Expectation of the percentage of recog-nized morphemes for English data.a baseline of no segmentation fairly high.
The no-segmentation baseline corresponds to a system thatrecognizes the training vocabulary fully, but has noability to generalize to any other word form.The results for English are different.
Linguisticais the best method for corpus sizes below 50 000words, but its performance degrades from the max-imum of 39.6% at 10 000 words to 29.8% for thelargest data set.
The probabilistic method is con-stantly better than Recursive MDL and both methodsoutperform Linguistica beyond 50 000 words.
Therecognition percentages of the probabilistic methodvary between 28.2% and 43.6%.
However, for cor-pus sizes above 10 000 words none of the threemethods outperform the no-segmentation baseline.Overall, the results for English are closer to idealperformance than was the case for Finnish.
Thisis partly due to the fact that the proportion of un-seen morphemes that are impossible to recognize ishigher for English (44.5% at 2000 words, 19.0% at200 000 words).As far as the time consumption of the algorithmsis concerned, the largest Finnish corpus took 20 min-utes to process for the probabilistic method and Re-cursive MDL, and 40 minutes for Linguistica.
Thelargest English corpus was processed in less thanthree minutes by all the algorithms.
The tests wererun on a 900 MHz AMD Duron processor with256 MB RAM.5 DiscussionFor small data sizes, Recursive MDL has a tendencyto split words into too small segments, whereas Lin-guistica is much more reluctant at splitting words,due to its use of signatures.
The extent to which theprobabilistic method splits words lies somewhere inbetween the two other methods.Our evaluation measure favours low ambiguity aslong as the ability to generalize to new word formsdoes not suffer.
This works against all segmentationmethods for English at larger data sizes.
The En-glish language has rather simple morphology, whichmeans that the number of different possible wordforms is limited.
The larger the training vocabu-lary, the broader coverage of the test vocabulary, andtherefore the no-segmentation approach works sur-prisingly well.
Segmentation always increases am-biguity, which especially Linguistica suffers from asit discovers more and more signatures and short suf-fixes as the amount of data increases.
For instance,a final ?s?
stripped off its stem can be either a nounor a verb ending, and a final ?e?
is very ambiguous,as it belongs to orthography rather than morphologyand does not correspond to any morpheme.Finnish morphology is more complex and thereare endless possibilities to construct new wordforms.
As can be seen from Figure 1, the proba-bilistic method and Recursive MDL perform betterthan the no-segmentation baseline for all data sizes.The segmentations could be evaluated using othermeasures, but for language modelling purposes,we believe that the evaluation measure should notfavour shattering of very common strings, eventhough they correspond to more than one morpheme.These strings should rather work as individual vo-cabulary items in the model.
It has been shown thatincreased performance of n-gram models can be ob-tained by adding larger units consisting of commonword sequences to the vocabulary; see e.g., (Deligneand Bimbot, 1995).
Nevertheless, in the near fu-ture we wish to explore possibilities of using com-plementary and more standard evaluation measures,such as precision, recall, and F-measure of the dis-covered morph boundaries.Concerning the length and frequency prior dis-tributions in the probabilistic model, one notes thatthey are very general and do not make far-reachingassumptions about the behaviour of natural lan-guage.
In fact, Zipf?s law has been shown to ap-ply to randomly generated artificial texts (Li, 1992).In our implementation, due to the independence as-sumptions made in the model and due to the searchalgorithm used, the choice of a prior value for themost common morph length is more important thanthe hapax legomena value.
If a very bad prior valuefor the most common morph length is used perfor-mance drops by twelve percentage units, whereasextreme hapax legomena values only reduces per-formance by two percentage units.
But note that thetwo values are dependent: A greater average morphlength means a greater number of hapax legomenaand vice versa.There is always room for improvement.
Our cur-rent model does not represent contextual dependen-cies, such as phonological rules or morphotactic lim-itations on morph order.
Nor does it identify whichmorphs are allomorphs of the same morpheme, e.g.,?city?
and ?citi + es?.
In the future, we expect to ad-dress these problems by using statistical languagemodelling techniques.
We will also study how thealgorithms scale to considerably larger corpora.6 ConclusionsThe results we have obtained suggest that the per-formance of a segmentation algorithm can indeed beincreased by using prior information of general na-ture, when this information is expressed mathemati-cally as part of a probabilistic model.
Furthermore,we have reasons to believe that the morph segmentsobtained can be useful as components of a statisticallanguage model.AcknowledgementsI am most grateful to Krista Lagus, Krister Linde?n,and Anders Ahlba?ck, as well as the anonymous re-viewers for their valuable comments.ReferencesR.
H. Baayen.
2001.
Word Frequency Distributions.Kluwer Academic Publishers.M.
Baroni, J. Matiasek, and H. Trost.
2002.
Unsuper-vised learning of morphologically related words basedon orthographic and semantic similarity.
In Proc.
ACLWorkshop Morphol.
& Phonol.
Learning, pp.
48?57.M.
R. Brent.
1999.
An efficient, probabilistically soundalgorithm for segmentation and word discovery.
Ma-chine Learning, 34:71?105.M.
Creutz and K. Lagus.
2002.
Unsupervised discoveryof morphemes.
In Proc.
ACL Workshop on Morphol.and Phonological Learning, pp.
21?30, Philadelphia.H.
De?jean.
1998.
Morphemes as necessary conceptfor structures discovery from untagged corpora.
InWorkshop on Paradigms and Grounding in Nat.
Lang.Learning, pp.
295?299, Adelaide.S.
Deligne and F. Bimbot.
1995.
Language modelingby variable length sequences: Theoretical formulationand evaluation of multigrams.
In Proc.
ICASSP.S.
Deligne and F. Bimbot.
1997.
Inference of variable-length linguistic and acoustic units by multigrams.Speech Communication, 23:223?241.J.
Goldsmith.
2001.
Unsupervised learning of the mor-phology of a natural language.
Computational Lin-guistics, 27(2):153?198.F.
Karlsson.
1987.
Finnish Grammar.
WSOY, 2nd ed.C.
Kit and Y. Wilks.
1999.
Unsupervised learning ofword boundary with description length gain.
In Proc.CoNLL99 ACL Workshop, Bergen.K.
Koskenniemi.
1983.
Two-level morphology: A gen-eral computational model for word-form recognitionand production.
Ph.D. thesis, University of Helsinki.W.
Li.
1992.
Random texts exhibit Zipf?s-Law-like wordfrequency distribution.
IEEE Transactions on Infor-mation Theory, 38(6):1842?1845.S.
Neuvel and S. A. Fulop.
2002.
Unsupervised learn-ing of morphology without morphemes.
In Proc.
ACLWorkshop on Morphol.
& Phonol.
Learn., pp.
31?40.J.
Rissanen.
1989.
Stochastic Complexity in StatisticalInquiry, vol.
15.
World Scientific Series in ComputerScience, Singapore.P.
Schone and D. Jurafsky.
2000.
Knowledge-free induc-tion of morphology using Latent Semantic Analysis.In Proc.
CoNLL-2000 & LLL-2000, pp.
67?72.M.
G. Snover and M. R. Brent.
2001.
A Bayesian modelfor morpheme and paradigm identification.
In Proc.39th Annual Meeting of the ACL, pp.
482?490.M.
G. Snover, G. E. Jarosz, and M. R. Brent.
2002.
Un-supervised learning of morphology using a novel di-rected search algorithm: Taking the first step.
In Proc.ACL Worksh.
Morphol.
& Phonol.
Learn., pp.
11?20.H.
Yu.
2000.
Unsupervised word induction using MDLcriterion.
In Proc.
ISCSL, Beijing.
