Proceedings of the ACL Interactive Poster and Demonstration Sessions,pages 53?56, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSenseLearner: Word Sense Disambiguationfor All Words in Unrestricted TextRada Mihalcea and Andras CsomaiDepartment of Computer Science and EngineeringUniversity of North Texasrada@cs.unt.edu, ac0225@unt.eduAbstractThis paper describes SENSELEARNER ?
aminimally supervised word sense disam-biguation system that attempts to disam-biguate all content words in a text usingWordNet senses.
We evaluate the accu-racy of SENSELEARNER on several stan-dard sense-annotated data sets, and showthat it compares favorably with the best re-sults reported during the recent SENSEVALevaluations.1 IntroductionThe task of word sense disambiguation consists ofassigning the most appropriate meaning to a polyse-mous word within a given context.
Applications suchas machine translation, knowledge acquisition, com-mon sense reasoning, and others, require knowledgeabout word meanings, and word sense disambiguationis considered essential for all these applications.Most of the efforts in solving this problem wereconcentrated so far toward targeted supervised learn-ing, where each sense tagged occurrence of a particu-lar word is transformed into a feature vector, which isthen used in an automatic learning process.
The appli-cability of such supervised algorithms is however lim-ited only to those few words for which sense taggeddata is available, and their accuracy is strongly con-nected to the amount of labeled data available at hand.Instead, methods that address all words in unre-stricted text have received significantly less attention.While the performance of such methods is usuallyexceeded by their supervised lexical-sample alterna-tives, they have however the advantage of providinglarger coverage.In this paper, we present a method for solving thesemantic ambiguity of all content words in a text.
Thealgorithm can be thought of as a minimally supervisedword sense disambiguation algorithm, in that it usesa relatively small data set for training purposes, andgeneralizes the concepts learned from the training datato disambiguate the words in the test data set.
As aresult, the algorithm does not need a separate classi-fier for each word to be disambiguated, but instead itlearns global models for general word categories.2 BackgroundFor some natural language processing tasks, such aspart of speech tagging or named entity recognition,regardless of the approach considered, there is a con-sensus on what makes a successful algorithm.
Instead,no such consensus has been reached yet for the taskof word sense disambiguation, and previous work hasconsidered a range of knowledge sources, such as lo-cal collocational clues, common membership in se-mantically or topically related word classes, semanticdensity, and others.In recent SENSEVAL-3 evaluations, the most suc-cessful approaches for all words word sense disam-biguation relied on information drawn from annotatedcorpora.
The system developed by (Decadt et al,2004) uses two cascaded memory-based classifiers,combined with the use of a genetic algorithm for jointparameter optimization and feature selection.
A sep-arate ?word expert?
is learned for each ambiguousword, using a concatenated corpus of English sense-53New rawtextFeature vectorconstruction(POS, NE, MWE)PreprocessingSemantic modellearningSense?taggedtextsemantic modelsSenseLearnerdefinitionsWord sensedisambiguationTrained semanticmodelsSense?taggedtextsFigure 1: Semantic model learning in SENSE-LEARNERtagged texts, including SemCor, SENSEVAL data sets,and a corpus built from WordNet examples.
The per-formance of this system on the SENSEVAL-3 Englishall words data set was evaluated at 65.2%.Another top ranked system is the one developed by(Yuret, 2004), which combines two Naive Bayes sta-tistical models, one based on surrounding collocationsand another one based on a bag of words around thetarget word.
The statistical models are built based onSemCor and WordNet, for an overall disambiguationaccuracy of 64.1%.A different version of our own SENSELEARNERsystem (Mihalcea and Faruque, 2004), using three ofthe semantic models described in this paper, combinedwith semantic generalizations based on syntactic de-pendencies, achieved a performance of 64.6%.3 SenseLearnerOur goal is to use as little annotated data as possi-ble, and at the same time make the algorithm generalenough to be able to disambiguate as many contentwords as possible in a text, and efficient enough sothat large amounts of text can be annotated in realtime.
SENSELEARNER is attempting to learn generalsemantic models for various word categories, startingwith a relatively small sense-annotated corpus.
Webase our experiments on SemCor (Miller et al, 1993),a balanced, semantically annotated dataset, with allcontent words manually tagged by trained lexicogra-phers.The input to the disambiguation algorithm consistsof raw text.
The output is a text with word meaningannotations for all open-class words.The algorithm starts with a preprocessing stage,where the text is tokenized and annotated with part-of-speech tags; collocations are identified using a slidingwindow approach, where a collocation is defined asa sequence of words that forms a compound conceptdefined in WordNet (Miller, 1995); named entities arealso identified at this stage1.Next, a semantic model is learned for all predefinedword categories, which are defined as groups of wordsthat share some common syntactic or semantic prop-erties.
Word categories can be of various granulari-ties.
For instance, using the SENSELEARNER learn-ing mechanism, a model can be defined and trained tohandle all the nouns in the test corpus.
Similarly, us-ing the same mechanism, a finer-grained model can bedefined to handle all the verbs for which at least oneof the meanings is of type <move>.
Finally, smallcoverage models that address one word at a time, forexample a model for the adjective small, can be alsodefined within the same framework.
Once defined andtrained, the models are used to annotate the ambigu-ous words in the test corpus with their correspondingmeaning.
Section 4 below provides details on the vari-ous models that are currently implemented in SENSE-LEARNER, and information on how new models canbe added to the SENSELEARNER framework.Note that the semantic models are applicable onlyto: (1) words that are covered by the word categorydefined in the models; and (2) words that appeared atleast once in the training corpus.
The words that arenot covered by these models (typically about 10-15%of the words in the test corpus) are assigned with themost frequent sense in WordNet.An alternative solution to this second step was sug-gested in (Mihalcea and Faruque, 2004), using seman-tic generalizations learned from dependencies identi-fied between nodes in a conceptual network.
Theirapproach however, although slightly more accurate,conflicted with our goal of creating an efficient WSDsystem, and therefore we opted for the simpler back-off method that employs WordNet sense frequencies.1We only identify persons, locations, and groups, which arethe named entities specifically identified in SemCor.544 Semantic ModelsDifferent semantic models can be defined and trainedfor the disambiguation of different word categories.Although more general than models that are built in-dividually for each word in a test corpus (Decadt etal., 2004), the applicability of the semantic modelsbuilt as part of SENSELEARNER is still limited tothose words previously seen in the training corpus,and therefore their overall coverage is not 100%.Starting with an annotated corpus consisting of allannotated files in SemCor, a separate training data setis built for each model.
There are seven models pro-vided with the current SENSELEARNER distribution,implementing the following features:4.1 Noun ModelsmodelNN1: A contextual model that relies on the firstnoun, verb, or adjective before the target noun, andtheir corresponding part-of-speech tags.modelNNColl: A collocation model that implementscollocation-like features based on the first word to theleft and the first word to the right of the target noun.4.2 Verb ModelsmodelVB1 A contextual model that relies on the firstword before and the first word after the target verb,and their part-of-speech tags.modelVBColl A collocation model that implementscollocation-like features based on the first word to theleft and the first word to the right of the target verb.4.3 Adjective ModelsmodelJJ1 A contextual model that relies on the firstnoun after the target adjective.modelJJ2 A contextual model that relies on the firstword before and the first word after the target adjec-tive, and their part-of-speech tags.modelJJColl A collocation model that implementscollocation-like features using the first word to the leftand the first word to the right of the target adjective.4.4 Defining New ModelsNew models can be easily defined and trained fol-lowing the same SENSELEARNER learning method-ology.
In fact, the current distribution of SENSE-LEARNER includes a template for the subroutine re-quired to define a new semantic model, which can beeasily adapted to handle new word categories.4.5 Applying Semantic ModelsIn the training stage, a feature vector is constructedfor each sense-annotated word covered by a semanticmodel.
The features are model-specific, and featurevectors are added to the training set pertaining to thecorresponding model.
The label of each such featurevector consists of the target word and the correspond-ing sense, represented as word#sense.
Table 1 showsthe number of feature vectors constructed in this learn-ing stage for each semantic model.To annotate new text, similar vectors are created forall content-words in the raw text.
Similar to the train-ing stage, feature vectors are created and stored sepa-rately for each semantic model.Next, word sense predictions are made for all testexamples, with a separate learning process run foreach semantic model.
For learning, we are using theTimbl memory based learning algorithm (Daelemanset al, 2001), which was previously found useful forthe task of word sense disambiguation (Hoste et al,2002), (Mihalcea, 2002).Following the learning stage, each vector in the testdata set is labeled with a predicted word and sense.If several models are simultaneously used for a giventest instance, then all models have to agree in the la-bel assigned, for a prediction to be made.
If the wordpredicted by the learning algorithm coincides with thetarget word in the test feature vector, then the pre-dicted sense is used to annotate the test instance.
Oth-erwise, if the predicted word is different than the tar-get word, no annotation is produced, and the word isleft for annotation in a later stage.5 EvaluationThe SENSELEARNER system was evaluated on theSENSEVAL-2 and SENSEVAL-3 English all wordsdata sets, each data set consisting of three texts fromthe Penn Treebank corpus annotated with WordNetsenses.
The SENSEVAL-2 corpus includes a total of2,473 annotated content words, and the SENSEVAL-3 corpus includes annotations for an additional setof 2,081 words.
Table 1 shows precision and recallfigures obtained with each semantic model on thesetwo data sets.
A baseline, computed using the mostfrequent sense in WordNet, is also indicated.
Thebest results reported on these data sets are 69.0% onSENSEVAL-2 data (Mihalcea and Moldovan, 2002),55Training SENSEVAL-2 SENSEVAL-3Model size Precision Recall Precision RecallmodelNN1 88058 0.6910 0.3257 0.6624 0.3027modelNNColl 88058 0.7130 0.3360 0.6813 0.3113modelVB1 48328 0.4629 0.1037 0.5352 0.1931modelVBColl 48328 0.4685 0.1049 0.5472 0.1975modelJJ1 35664 0.6525 0.1215 0.6648 0.1162modelJJ2 35664 0.6503 0.1211 0.6593 0.1153modelJJColl 35664 0.6792 0.1265 0.6703 0.1172model*1/2 207714 0.6481 0.6481 0.6184 0.6184model*Coll 172050 0.6622 0.6622 0.
6328 0.6328Baseline 63.8% 63.8% 60.9% 60.9%Table 1: Precision and recall for the SENSELEARNERsemantic models, measured on the SENSEVAL-2 andSENSEVAL-3 English all words data.
Results for com-binations of contextual (model*1/2) and collocational(model*Coll) models are also included.and 65.2% on SENSEVAL-3 data (Decadt et al, 2004).Note however that both these systems rely on signifi-cantly larger training data sets, and thus the results arenot directly comparable.In addition, we also ran an experiment where a sep-arate model was created for each individual word inthe test data, with a back-off method using the mostfrequent sense in WordNet when no training exam-ples were found in SEMCOR.
This resulted into sig-nificantly higher complexity, with a very large num-ber of models (about 900?1000 models for each ofthe SENSEVAL-2 and SENSEVAL-3 data sets), whilethe performance did not exceed the one obtained withthe more general semantic models.The average disambiguation precision obtainedwith SENSELEARNER improves significantly over thesimple but competitive baseline that selects by de-fault the ?most frequent sense?
from WordNet.
Notsurprisingly, the verbs seem to be the most difficultword class, which is most likely explained by the largenumber of senses defined in WordNet for this part ofspeech.6 ConclusionIn this paper, we described and evaluated an efficientalgorithm for minimally supervised word-sense dis-ambiguation that attempts to disambiguate all contentwords in a text using WordNet senses.
The results ob-tained on both SENSEVAL-2 and SENSEVAL-3 datasets are found to significantly improve over the sim-ple but competitive baseline that chooses by defaultthe most frequent sense, and are proved competitivewith the best published results on the same data sets.SENSELEARNER is publicly available for downloadat http://lit.csci.unt.edu/?senselearner.AcknowledgmentsThis work was partially supported by a National Sci-ence Foundation grant IIS-0336793.ReferencesW.
Daelemans, J. Zavrel, K. van der Sloot, and A. van denBosch.
2001.
Timbl: Tilburg memory based learner,version 4.0, reference guide.
Technical report, Univer-sity of Antwerp.B.
Decadt, V. Hoste, W. Daelemans, and A.
Van denBosch.
2004.
Gambl, genetic algorithm optimization ofmemory-based wsd.
In Senseval-3: Third InternationalWorkshop on the Evaluation of Systems for the SemanticAnalysis of Text, Barcelona, Spain, July.V.
Hoste, W. Daelemans, I. Hendrickx, and A. van denBosch.
2002.
Evaluating the results of a memory-basedword-expert approach to unrestricted word sense dis-ambiguation.
In Proceedings of the ACL Workshop on?Word Sense Disambiguatuion: Recent Successes andFuture Directions?, Philadelphia, July.R.
Mihalcea and E. Faruque.
2004.
SenseLearner: Min-imally supervised word sense disambiguation for allwords in open text.
In Proceedings of ACL/SIGLEXSenseval-3, Barcelona, Spain, July.R.
Mihalcea and D. Moldovan.
2002.
Pattern learningand active feature selection for word sense disambigua-tion.
In Senseval 2001, ACL Workshop, pages 127?130,Toulouse, France, July.R.
Mihalcea.
2002.
Instance based learning with automaticfeature selection applied to Word Sense Disambiguation.In Proceedings of the 19th International Conference onComputational Linguistics (COLING 2002), Taipei, Tai-wan, August.G.
Miller, C. Leacock, T. Randee, and R. Bunker.
1993.A semantic concordance.
In Proceedings of the 3rdDARPA Workshop on Human Language Technology,pages 303?308, Plainsboro, New Jersey.G.
Miller.
1995.
Wordnet: A lexical database.
Communi-cation of the ACM, 38(11):39?41.D.
Yuret.
2004.
Some experiments with a naive bayes wsdsystem.
In Senseval-3: Third International Workshopon the Evaluation of Systems for the Semantic Analysisof Text, Barcelona, Spain, July.56
