Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1172?1182,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsBag-of-Words Forced Decodingfor Cross-Lingual Information RetrievalFelix HieberComputational LinguisticsHeidelberg University69120 Heidelberg, Germanyhieber@cl.uni-heidelberg.deStefan RiezlerComputational LinguisticsHeidelberg University69120 Heidelberg, Germanyriezler@cl.uni-heidelberg.deAbstractCurrent approaches to cross-lingual informa-tion retrieval (CLIR) rely on standard retrievalmodels into which query translations by sta-tistical machine translation (SMT) are inte-grated at varying degree.
In this paper, wepresent an attempt to turn this situation on itshead: Instead of the retrieval aspect, we em-phasize the translation component in CLIR.We perform search by using an SMT decoderin forced decoding mode to produce a bag-of-words representation of the target documentsto be ranked.
The SMT model is extended byretrieval-specific features that are optimizedjointly with standard translation features for aranking objective.
We find significant gainsover the state-of-the-art in a large-scale eval-uation on cross-lingual search in the domainspatents and Wikipedia.1 IntroductionApproaches to CLIR have been plentiful and di-verse.
While simple word translation probabilitiesare easily integrated into term-based retrieval mod-els (Berger and Lafferty, 1999; Xu et al, 2001),state-of-the-art SMT systems (Koehn, 2010; Chi-ang, 2007) are complex statistical models on theirown.
The use of established translation modelsfor context-aware translation of query strings, effec-tively reducing the problem of CLIR to a pipelineof translation and monolingual retrieval, has beenshown to work well in the past (Chin et al, 2008).Only recently, approaches have been presented toinclude (weighted) translation alternatives into thequery structure to allow a more generalized termmatching (Ture et al, 2012a; Ture et al, 2012b).However, this integration of SMT remains agnosticabout its use for CLIR and is instead optimized tomatch fluent, human reference translations.
In con-trast, retrieval systems often use bag-of-word repre-sentations, stopword filtering, and stemming tech-niques during document scoring, and queries arerarely fluent, grammatical natural language queries(Downey et al, 2008).
Thus, most of a translation?sstructural information is lost during retrieval, andlexical choices may not be optimal for the retrievaltask.
Furthermore, the nature of modeling transla-tion and retrieval separately requires that a singlequery translation is selected, which is usually doneby choosing the most probable SMT output.Attempts to inform the SMT system about its usefor retrieval by optimizing its parameters towards aretrieval objective have been presented in the formor re-ranking (Nikoulina et al, 2012) or ranking(Sokolov et al, 2014).
In this paper, we take thisidea a step further and directly integrate the task ofscoring documents with respect to the query intothe process of translation decoding.
We make thefull expressiveness of the translation search spaceavailable to the retrieval model, without enumerat-ing all possible translation alternatives.
This is doneby augmenting the linear model of the SMT systemwith features that relate partial translation hypothe-ses to documents in the retrieval collection.
Theseretrieval-specific features decompose over partialtranslation hypotheses and thus allow efficient de-coding using standard dynamic programming tech-niques.
Furthermore, we apply learning-to-rank tojointly optimize translation and retrieval for the ob-1172jective of retrieving relevant documents, and use de-coding over the weighted translation hypergraph di-rectly to perform cross-lingual search.
Since highweights on retrieval features for words in the bag-of-words (BOW) representation of documents forcethe decoder to prefer relevant documents with highprobability, by a slight abuse of terminology, we callour approach BOW Forced Decoding.One of the key features of our approach is theuse of context-sensitive information such as the lan-guage model and reordering information.
We showthat the use of such a translation-benign search spaceis crucial to outperform state-of-the-art CLIR ap-proaches.
Our experimental evaluation of retrievalperformance is done on Wikipedia cross-lingual arti-cle retrieval (Bai et al, 2010; Schamoni et al, 2014)and patent prior art search (Fujii et al, 2009; Guoand Gomes, 2009; Sokolov et al, 2013; Schamoniet al, 2014).
On both datasets, we show substan-tial improvements over the CLIR baselines of directtranslation (Chin et al, 2008) or Probabilistic Struc-tured Queries (Ture et al, 2012b), with and with-out further parameter tuning using learning-to-ranktechniques and extended feature sets.
From our re-sults we conclude, that, in spite of algorithmic com-plexity, it is central to model translation and retrievaljointly to create more powerful CLIR models.2 Related WorkThe framework of translation-model based retrievalhas been introduced by Berger and Lafferty (1999).An extension to the cross-lingual case using context-free lexical translation tables has been given by Xuet al (2001).
While the industry standard to CLIRis a pipeline of SMT-based query translation feedinginto monolingual retrieval (Chin et al, 2008), recentapproaches include (weighted) SMT translation al-ternatives into the query structure to allow a moregeneralized term matching (Ture et al, 2012a; Tureet al, 2012b).
Less work has been devoted to op-timizing SMT towards a retrieval objective, for ex-ample in a re-ranking framework (Nikoulina et al,2012) or by integrating a decomposable proxy forretrieval quality of query translations into discrimi-native ranking (Sokolov et al, 2014).The idea of forced decoding has been employedrecently to select better perceptron updates from thefull SMT search space for discriminative parametertuning of SMT systems (Yu et al, 2013; Zhao et al,2014).Most similar to our approach is the recent work ofDong et al (2014) who use the Moses translationoption lattices for translation retrieval, i.e., for min-ing comparable data.
Their query lattices given bythe translation options encode exponentially manyqueries and are used to retrieve the most probabletranslation candidate from a set of candidates.
Theapproach is evaluated in the context of a parallel cor-pus mining system.
We present a model that not onlyuses the full search space, including the languagemodel and reordering information, but also evalu-ate the model specifically for the task of retrieval,rather than mate-finding only.
We show that a forceddecoding model using bag-of-word representationsfor documents and retrieval features that are decom-posable over query terms significantly outperformsstate-of-the-art CLIR baselines such as direct trans-lation (Chin et al, 2008) or Probabilistic StructuredQueries obtained from n-best list query translations(Darwish and Oard, 2003; Ture et al, 2012b).
Ad-ditionally we find that the use of context-sensitivetranslation information such as language models orreordering information, greatly improves retrievalquality in these types of models.
We furthermoreshow how to directly optimize the retrieval objectiveusing large-scale retrieval data sets with automati-cally induced relevance judgments.3 A Bag-of-Words Forced Decoding ModelModel Definition.
SMT systems use a Viterbi ap-proximation to find the output hypothesis q?eq?e= arg maxqemaxh?EqfP (h, qe|qf).
(1)over the search space of hypotheses or derivationsh ?
Eqffor a given input qf.
The probability of atranslation output qeunder derivation h given qfisusually modeled in a log-linear modelP (h, qe|qf; wsmt) =eFsmt(h,qe,qf)?qe,heFsmt(h,qe,qf),where F (h, qe, qf) is a learned linear combinationof input-output features, that is, the dot product be-tween parameter column vector wsmtand feature1173column vector given by feature map ?smt,Fsmt(h, qe, qf) = wTsmt?smt(h, qe, qf).
(2)In CLIR, we seek to choose a derivation that isboth an accurate translation of the input accordingto the translation model, and a well-formed discrim-inative query that matches relevant documents withhigh probability.
We combine both objectives by di-rectly modeling the probability of a document deintarget language e given a query qfin source lan-guage f , factorized as follows:P (de|qf) =?h?EqfP (h|qf)?
??
?translation?P (de|h, qf)?
??
?retrieval.Applying the same Viterbi approximation during in-ference as in (1), we choose the retrieval score of deto be the score of the highest scoring hypothesis h,score(qf, de) = maxh?EqfP (h|qf)?
P (de|h, qf), (3)where the product between both models can be in-terpreted as a conjunctive operation similar to aproduct of experts (Hinton, 2002): A high scoreis achieved if both experts, namely translation andretrieval models, assign high scores to a hypothe-sis.
That is, the model attempts to produce a well-formed translation, but at the same time chooses lex-ical items present in the bag-of-words representationof the document.
Similarly, we can interpret the in-clusion of the retrieval component as a constraintto force the decoder to retrieve dewith high prob-ability.
By a slight abuse of terminology, we willhenceforth call our approach Bag-of-Words ForcedDecoding (BOW-FD).1The translation term P (h|qf) is modeled as in (2)for standard hierarchical phrase-based SMT (Chi-ang, 2007) and left unchanged in our joint model.The retrieval term P (de|h, qf) is modeled in a simi-lar formFir(h, de) = wTir?ir(h, de),1Standardly, the term forced decoding is used to describe thesearch for only those derivations that exactly produce the refer-ence translation.
Our use of this terminology deviates from thestandard in two respects: First, we do not require exact reach-ability of the reference, but only a BOW match.
Second, ourconstraint on the decoder is not strict, but only applies with highprobability.where IR features do not depend on qf(thus allow-ing us to drop this term) and decompose over deriva-tion terms.
This allows a bag-of-word vector rep-resentation of documents, and retrieval features arelocal to single edges in the search space for efficientViterbi inference.
The joint scoring model is definedas follows:score(qf, de; w) = maxh?EqfeFsmt(h,qe,qf)+Fir(h,de),where the weight vector is defined by the vector con-catenation w = wsmt?wir, and qerefers to the yieldthat is determined uniquely by derivation h.Following the interpretation of our joint modelas forced or constrained decoding, we can viewpipeline approaches such as the direct translationbaseline as instances of unconstrained decoding.That is, the SMT decoder yields a single transla-tion output for every document and the assignmentof document scores is deferred to a (monolingual)retrieval model given this single output structure.Other CLIR approaches such as probabilistic struc-tured queries (Darwish and Oard, 2003; Ture et al,2012b) try to mitigate this early disambiguation bykeeping enumerated translation alternatives at re-trieval time.
However, they either use context-freeword-based translation tables or select only termsfrom a small n-best fraction of the full search space.Dynamic Programming on Hypergraphs.
De-coding in a hierarchical phrase-based SMT (Chi-ang, 2007) is usually understood as a two-stepprocess: Initially, an input sentence is parsed us-ing a Weighted Synchronous Context-Free Gram-mar (WSCFG) in a bottom-up manner to constructan initial hypergraph H that compactly encodes thefull search space (?translation forest?)
(Gallo et al,1993; Klein and Manning, 2001; Huang and Chi-ang, 2005; Dyer et al, 2010).
An ordered, directedhypergraph H is a tuple ?V,E, g,W?, consisting ofa finite set of nodes V , a finite set of hyperedges E,and weight function W : E 7?
R assigning real-valued weights to e ?
E. Language models aretypically added in a second rescoring phase that iscarried out by approximate solutions, such as cube-pruning (Chiang, 2007; Huang and Chiang, 2007),limiting the number of derivations created at eachnode.
A translation hypothesis h ?
E corresponds1174to a sequence of nodes S ?
V connected via hyper-edges e ending in goal node g. Each edge e is associ-ated with a synchronous grammar rule r(e), and cor-responding feature values ?(r(e)).
The weight ofhyperedge e is defined asW(e; w) = wT?
(r(e)).The quantity in (1) is efficiently computed usingdynamic programming under the proper semiring.
Acommutative semiringK is a tuple ?K,?,?,?0,?1?,of a set K, an associative and commutative additionoperator?, an associative multiplication operator?, and their ?neutral?
elements?0 and?1, respec-tively (Dyer, 2010).
The Inside algorithm over thetopologically sorted, acyclic hypergraph H underthe tropical ?R,max,?,?
?, 0?
semiring (Good-man, 1999; Mohri, 2009) computes the inside score?
of the Viterbi hypothesis, i.e.
the weight of itssequence of nodes ending in goal node g:arg maxh?EqP (h|q) ?
?
(g)=?h?Hq?e?hW(e; wsmt),where W(e; wsmt) = wTsmt?smt(r(e)) assignsweights given parameters and features of the trans-lation model.For Bag-of-Words Forced Decoding, we extendW with another set of parameters wirfor local IRfeatures ?ir:arg maxh?EqP (h|q, d) ?
?(g)=?h?Hq?e?hW?
(e, d; wsmt,wir), (4)with W?
(e, d; wsmt,wir) = wTsmt?smt(r(e)) +wTir?ir(r(e), d).
Note that ?irdepends on bothtranslation rule r(e) and document d, while ?smtsolely depends on source and target side of r(e).Decomposable Retrieval Features.
We usesparse, lexicalized, real-valuead IR features thatrelate derivations h to document d using Okapibm25 term weights (Robertson and Zaragoza,2009):bm25(t, d) = rsj(t, C) ?
tfbm25(t, d),where rsj(t, C) = log(|C|?df(t,C)+0.5df(t,C)+0.5)is a con-stant term weight approximated on document fre-quencies for collection C, and tfbm25(t, d) =tf(t, d)/(k1((1?
b) + bdlavdl) + tf(t, d)) a saturatedterm frequency weight of term t in document d, tak-ing into account (average) document lengths dl andavdl2.
We fire the Okapi bm25 term weight for eachderivation term t ?
h w.r.t.
document d in collec-tion C. The sum of feature values for all deriva-tion terms ti?
h equals the regular BM25 scoreBM25(h, d) =?t?hbm25(t, d).
Weights wirforthis type of features are interpretable as additional,general term weights.Additionally, we report experiments using sparsealignment features that fire an indicator for eachalignment, insertion, or deletion of words in sourceand target.
They allow the model to adapt lexicalchoice and dropping of function words for retrieval.Default Retrieval Weights & Self-Translation.To enforce a ranking over documents, we definean IR default weight v, wir= 1v.
Intuitively, vcontrols the model?s disposition to diverge from theSMT Viterbi path.
If IR features fire in other re-gions of the search space than the SMT Viterbi path,this weight compensates for the loss incurred for notproducing the Viterbi hypothesis.
Furthermore, thedefault weight allows the model to generalize to un-seen data: If an unknown query word, for exam-ple a named entity, causes an IR feature to fire attest time, the decoder will simply pass through thesource word to any derivation, and the IR feature cancontribute to the retrieval score with v > 0.Multi-Sentence Queries.
Specialized retrievaltasks such as patent prior art search may exhibitlong, coherent search queries that contain mul-tiple sentences.
If multiple sentences of queryq = (s1, .
.
.
, sm) are processed independently,we need to combine the sentence-wise rankings toobtain a final ranking.
We model this task from aproduct of experts perspective (Hinton, 2002) andmultiply scores score(?, d) of document d in all msentence rankings, re-sorting the final output.
If d isnot in the top-k ranking of a sentence, we take theminimum score of that top-k ranking as a smoothingvalue to prevent the product to become zero.2bm25 parameters were fixed at k1= 1.2 and b = 0.751175Implementation and Complexity Analysis.3Weimplemented the above model on top of the hier-archical phrase-based decoder cdec (Dyer et al,2010), but there are no limitations for applying thisapproach to phrase-based systems (Koehn et al,2007).
Procedurally, after cdec yields the trans-lation forest, we compute the overlap of IR featureactivations between edges in the forest and the doc-ument candidates.
The Inside algorithm is only car-ried out for documents that activate at least one IRfeature in the search space.
For documents with noactivation we can skip the computation of scores andassign the SMT Viterbi score, which constitutes alower bound on the model score.For a single query q, forced decoding requiresa single pass over the topologically sorted searchspace to find IR feature activations along hyper-edges, yielding a complexity of O(|V | + |E|).
Thedynamic programming procedure that computes ascore for a document requires another pass over theforest evaluating the extended edge weight (4) forevery edge e ?
E, where the dot product for transla-tion features is already precomputed by cdec , andthe retrieval part depends on the number of active IRfeatures, ?
:= |?ir(r(e), d)|.
Overall complexityfor a single query and all documents d ?
C is thusO(|V |+ |E|+(|V |+ |E| ?
?)?
|C|).As noted above, we can reduce the quantity |C| bychecking if a document candidate shares any IR fea-tures with the search space and avoid superfluousexecutions of the Inside algorithm.
In our experi-ments on Wikipedia data, we found that this checkreduces |C| to about 64% of its original size.
Thispre-filtering is similar to the coarse query approachof Dong et al (2014), who score only documentsthat contain at least one term in the query lattice.
Wefurther reduce runtime of the inference procedure byusing approximate decoding.
We experimented withusing a beam search approach to limit the number ofweight evaluations in (4) for incoming edges at eachnode.
The max operation of the tropical semiring isdiscontinued once the number of considered incom-ing edges at a node exceeds the size of the beam.3The complexity of the construction of the translation forestincluding the language model is common to BOW-FD and theother baselines and thus not included in the following analysis.4 Learning to Decode for RetrievalWe now turn to the problem of learning parameterweights for the BOW-FD model.
The objective is toprefer a relevant document d+over an irrelevant oned?by assigning a higher score to d+than to d?,score(q, d+; w) > score(q, d?
; w).We sample a set of preference pairsP = {(d+, d?
)|rl(d+, q) > rl(d?, q)}from relevance-annotated data where rl(d, q) indi-cates the relevance level of a document given query.Furthermore, we require the difference of scores tosatisfy a certain margin:score(q, d+; w) > score(q, d?
; w) + ?,where the margin is defined as?
= rl(d+, q)?
rl(d?, q).Our final objective is a margin-rescaled hinge-lossL(P) =?d+,d?
?P[score(q, d?
; w)?
score(q, d+; w) + ?
]+where [?
]+= max(0, ?
).We use stochastic (sub)gradient descent optimiza-tion using the Adadelta (Zeiler, 2012) update rule.Adadelta does not require manual tuning of a globallearning rate and requires only two hyperparame-ters that have shown to be quite robust to changes:the sliding window decay rate ?
= 0.95 and a con-stant  = 10?6were set to the default parametersgiven in the original paper.
We furthermore use thedistributed learning technique of Iterative Parame-ter Mixing (McDonald et al, 2010), where multi-ple models on several shards of the training data aretrained in parallel and parameters are averaged aftereach epoch.
We perform incremental optimizationusing a cyclic order of the data sequence (Bertsekas,2011), that is, the learner steps through a fixed se-quence of pairs, query by query, and relevant docu-ment by relevant document, without randomizationafter epochs.
This allows us to cache consecutivequery search spaces and feature vectors for relevantdocuments.
Regularization is done by early stop-ping where the best iteration is found on a held-outdevelopment set.1176Wikipedia patentsMAP NDCG PRES MAP NDCG PRESDT .3678 .5691 .7219 .2554 .5397 .5680PSQ .3642 .5671 .7165 .2659 .5508 .5851BOW-FD?.3880?.5911?.7417?.2825?.5721?.6072BOW-FD+LTR?.3913?.5962?.7543?.2870?.5807?.6260BOW-FD+LEX+LTR?.3919?.5963?.7528?.2883?.5819?.6251Table 1: Retrieval results of baseline systems and BOW-FD with default weight v = 1.6 for Wikipedia and v = 0.8 forpatents, respectively.
Baseline and BOW-FD models use the same SMT system.
Significant differences at p = 10?4with respect to baselines are indicated with?.
Significant differences at p = 10?6of learning-to-rank-based models(LTR) with respect to BOW-FD are indicated with?.Wikipedia patentsMAP NDCG PRES MAP NDCG PRESDT .3347(?.03).5368(?.03).6970(?.03).2315(?.02).5105(?.03).5420(?.03)PSQ .3464(?.02).5483(?.02).7006(?.02).2460(?.02).5290(?.02).5672(?.02)BOW-FD .3218(?.07).5315(?.06).7220(?.02).1651(?.12).4185(?.15).4959(?.11)Table 2: SMT-based CLIR models without a language model.
Numbers in superscripts denote the absolute loss withrespect to equivalent systems in Table 1.5 EvaluationData and Systems.
We conducted experi-ments on two large-scale CLIR tasks, namelyGerman-English Wikipedia cross-lingual articleretrieval4(Bai et al, 2010; Schamoni et al, 2014),and patent prior art search with Japanese-Englishpatent abstracts5(Fujii et al, 2009; Guo and Gomes,2009; Sokolov et al, 2013; Schamoni et al, 2014),comparing retrieval performance of BOW-FDagainst the state-of-the-art SMT-based CLIR base-lines of Direct Translation (DT) and cross-lingualProbabilistic Structured Queries (PSQ) (Ture et al,2012a; Ture et al, 2012b).
The SMT models, aswell as baseline evaluation scores were taken from(Schamoni et al, 2014).We present results for BOW-FD using a defaultweight v optimized on the development sets, andfor models with parameters trained using pairwiselearning-to-rank.
We compute MAP, NDCG (Man-ning et al, 2008) and PRES (Magdy and Jones,2010) scores on the top 1,000 returned documents4http://www.cl.uni-heidelberg.de/statnlpgroup/wikiclir/5http://www.cl.uni-heidelberg.de/statnlpgroup/boostclir/to provide an extensive evaluation across precision-,and recall-oriented measures.
Differences in evalua-tion scores between two systems were tested for sta-tistical significance using paired randomization tests(Smucker et al, 2007).
Significance levels are eitherindicated as superscripts, or provided in the captionsof the respective tables.Baseline SMT systems and BOW-FD share thehierarchical phrase-based SMT systems built withcdec (Dyer et al, 2010).
For German-Englishcross-lingual article retrieval on Wikipedia, we builta system analogously to Schamoni et al (2014) fromparallel training data (over 104M words) consist-ing of the Europarl corpus (Koehn, 2005) in version7, the News Commentary corpus, and the CommonCrawl corpus (Smith et al, 2013).
Word alignmentswere created with fast align (Dyer et al, 2013).The 4-gram language model was trained with theKenLM toolkit (Heafield, 2011) on the English sideof the training data and the English Wikipedia arti-cles.
Language model scores are added to the searchspaces using the cube pruning algorithm (Huang andChiang, 2007) with poplimit = 200.
SMT Modelparameters were optimized using MIRA (Chiang etal., 2008) on the WMT2011 news test set (30031177sentences).
The parameters for the baseline PSQmodel were found on a development set consist-ing of 10,000 German queries using 1,000-best lists:interpolation parameter ?
= 0.4, lower thresholdL = 0, and cumulative threshold C = 1.For the task of Japanese-English patent prior-artsearch, we use a system analog to Sokolov et al(2013) and Schamoni et al (2014).
Its SMT featuresare trained on 1.8M parallel sentences of NTCIR-7 data (Fujii et al, 2008) and weights were tunedon the NTCIR-8 test collection (2,000 sentences)using MIRA (Chiang et al, 2008).
A 5-gram lan-guage model on the English side of the trainingdata was trained with the KenLM toolkit (Heafield,2011).
The system uses a cube pruning poplimit of30.
Parameters for the baseline PSQ model werefound on a development set of 2,000 patent abstractqueries and set to n-best list size = 1000, ?
= 1.0,L = 0.005, C = 0.95Experimental Results.
We first find a defaultweight v using grid search within v = [0, 3] andv = [0, 2] on the development sets for Wikipedia andpatents, respectively.
v controls the balance be-tween the retrieval and translation features and withlarger v, the model is more likely to produce queryderivations diverging from the SMT 1-best transla-tion.
For Wikipedia, we sample 1,000 out of 10,000queries to reduce the time of the grid search.
Forpatents we use the full development set of 2,000queries with 8,381 sentences.
We combine rank-ings for single-sentence queries from multi-sentencepatent abstracts using the product method as previ-ously described.
Well performing values were foundat v = 1.6 for Wikipedia, and v = 0.8 for patents,respectively.Table 1 shows test set performance of DT andPSQ baselines versus BOW-FD.
Scores for DT andPSQ are as reported in Schamoni et al (2014).We observe that BOW-FD significantly outperformsboth baselines by over 2 points on Wikipedia andpatents under all three evaluation measures.
Whilethe cube pruning poplimit was set to 200 for theWikipedia experiments, it is set to 30 for patents.This may reduce the diversity of the search spaceconsiderably.
Increasing the poplimit from 30 to200 yielded another significant gain (MAP=0.2893,NDCG=0.5807, PRES=0.6172) on this dataset.1000 900 800 700 600 500 400 300 200 100Nmax0.0150.0200.0250.0300.0350.0400.045differenceinPRESFigure 1: Difference in PRES scores on the Wikipedia de-velopment set as a function of PRES?s Nmaxparameterbetween BOW-FD +LM and -LM systems.Learning-to-rank results.
We learned theweights of the BOW-FD model starting from IRdefault weights optimized by grid search, and fromSMT feature weights ?pre-trained?
on parallel data.We furthermore found improvements over BOW-FDin precision-oriented metrics (MAP and NDCG) byfreezing SMT weights.
Table 1 shows that BOW-FD+(LEX+)LTR models significantly outperformBOW-FD on both data sets, with the largest im-provement for PRES.
Differences between modelswith and without lexical alignment features are notstatistically significant.
We conjecture that LTRmodels mostly optimize recall rather than precision,i.e.
placing more relevant document in the ranking.This is supported by the fact that BOW-FD+LTRretrieves 70.1% of the relevant documents in the testset, compared to 68.0% by BOW-FD, while MeanReciprocal Rank (MRR) hardly differs (0.7344 vs.0.7332).
An experiment with no pre-trained SMTor default IR weights, performed worse, indicatingthe importance of translation-benign search spacesand IR default weights for generalization to unseenterms.Importance of Language Model for Retrieval.Liu et al (2012) and Dong et al (2014) claimthat computationally expensive SMT feature func-tions such as language models have only minorimpact on CLIR performance of SMT-based mod-els.
We found that such context-sensitive informa-tion present in single 1-best query translations (DT),weighted translation alternatives from the n-bestlist (PSQ), and forced decoding in a ?translation-benign?
search space (BOW- FD) is crucial for re-trieval performance in the experiments reported thispaper.
In order to investigate the question of theimportance of context-sensitive information such as1178language model scores for retrieval we conducted anexperiment in which the language model informa-tion is removed from all three SMT-based models.For the PSQ models, we also set the parameter ?to 1.0 to disable interpolation with the context-freelexical translation table (Ture et al, 2012a).
Table 2shows that retrieval performance drops significantlyfor all models.
The drop in performance for the twobaseline models is comparable on both data sets.
Re-moving the language model for BOW-FD hurts per-formance the most (with an average drop of 6 pointsin MAP and NDCG scores for Wikipedia, and over11 points in all measures for patents).
However,scores for recall-oriented PRES on Wikipedia re-mains relatively stable for BOW-FD with and with-out a language model.
A closer analysis on therankings for BOW-FD on Wikipedia shows that the-LM model returns 1,589 (out of 86,994) relevantdocuments less than the +LM model.
However,only 2 documents with relevance level 3, i.e., di-rectly linked cross-lingual ?mates?, were no longerretrieved, suggesting that excluding the languagemodel from the system mostly affects the retrievalof ?non-mates?, i.e.
documents that are linked by,or link to the cross-lingual mate.
We explain thisbehavior as follows: Cross-lingual mates are likelyto contain words that are close to an adequate querytranslation, since they constitute the beginning of aWikipedia article with the same topic as the query.Derivations generated for these documents are suchthat both translation model features (with or with-out the LM) and retrieval features agree on a pathclose to the SMT Viterbi translation.
In contrast,other relevant documents require more non-standardlexical choices that are harder to achieve in a +LMsearch space, since the strong weight on the lan-guage model, plus a language model-driven pruningtechnique, strongly favor lexical choices that agreewith the language model?s concept of fluency.
Ina -LM search space, disfluent derivations are eas-ily reached by IR feature activations whose defaultweight is much larger in relation to the remainingSMT features.
The use of ?glue rules?
allowing left-to-right concatenation of partial translations alongwith loosely extracted synchronous grammar rulesgive hierarchical MT models large degrees of free-dom in producing very disfluent translations in the-LM space.
If a language model is not ensuring amore or less ?translation-benign?
search space, the?reachability?
of terms in irrelevant documents is in-creased causing them to interfere with the rankingof relevant documents that may be closer transla-tions of the query.
This behavior immediately affectsprecision-oriented scores such as MAP and NDCG,while PRES is only affected if its recall cutoff pa-rameter, Nmax, is lowered, as shown in Figure 1.The major drop in performance for patent datamay be explained with the way multiple sentencequeries are evaluated: A language model limits di-versity of translation options for multiple sentences.Without a language model, the sets of documents re-trieved by each sentence are almost disjoint, i.e.
thesentences do not agree on a common set of docu-ments.6 ConclusionIn this paper, we presented an approach to CLIRthat shifts the focus from retrieval to translationby forcing a standard SMT decoder to produce abag-of-words representation of the document repos-itory.
This is done by joint optimization of a linearmodel including both translation and retrieval fea-tures under a ranking objective.
Highly weightedterm-match features are then used to find a decod-ing path that gives highest score to the documentthat is optimal with respect to both relevance andtranslational adequacy.
We showed in a large-scaleevaluation on cross-lingual retrieval tasks in the do-mains of patents and Wikipedia pages that our ap-proach significantly outperforms direct translationand Probabilistic Structured Query approaches un-der a variety of evaluation metrics.
Furthermore, weinvestigated the role of context-sensitive informationsuch as language model scores in retrieval.
In con-trast to previous claims about the minor impact oflanguage models in retrieval performance in SMT-based CLIR, we found significant drops in MAP andNDCG across all models when removing languagemodel information.
This confirms the dual role ofthe language model to ensure fluency and to selectthe proper translation terms in the context of theneighboring target terms.
The latter role of the lan-guage model makes it an indispensable ingredient ofany SMT-based CLIR approach.Open questions in our work regard further im-1179provements in efficiency of retrieval.
So far wecould achieve substantial reductions in retrievalcomplexity by pre-filtering based on coarse termmatches.
The inherent complexity of SMT decod-ing is less of a problem in offline applications suchas translation retrieval (Dong et al, 2014), but itbecomes prohibitive in online applications such ascross-lingual web search.
In future work, we wouldlike to address efficiency, e.g.
by investigating thepossibility of incorporating an inverted index intoonline applications of forced decoding.AcknowledgmentsThis research was supported in part by DFGgrant RI-2221/1-2 ?Weakly Supervised Learning ofCross-Lingual Systems?.ReferencesBing Bai, Jason Weston, David Grangier, RonanCollobert, Kunihiko Sadamasa, Yanjun Qi, OlivierChapelle, and Kilian Weinberger.
2010.
Learning torank with (a lot of) word features.
Information Re-trieval, 13(3):291?314.Adam Berger and John Lafferty.
1999.
Information re-trieval as statistical translation.
In Proceedings of the22Nd Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval(SIGIR?99), Berkeley, CA.Dimitri P. Bertsekas.
2011.
Incremental gradient, sub-gradient, and proximal methods for convex optimiza-tion: A survey.
In Suvrit Sra, Sebastian Nowozin, andStephen J. Wright, editors, Optimization for MachineLearning.
MIT Press.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP?08), Honolulu, Hawaii.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Jeffrey Chin, Maureen Heymans, Alexandre Kojoukhov,Jocelyn Lin, and Hui Tan.
2008.
Cross-languageinformation retrieval.
Patent Application.
US2008/0288474 A1.Kareem Darwish and Douglas W. Oard.
2003.
Proba-bilistic structured query methods.
In Proceedings ofthe 26th annual international ACM SIGIR conferenceon Research and development in informaion retrieval(SIGIR?03), Toronto, Canada.Meiping Dong, Yong Cheng, Yang Liu, Jia Xu, andMaosong Sun.
2014.
Query lattice for translation re-trieval.
In Proceedings of the 25th International Con-ference on Computational Linguistics (COLING?14),Dublin, Ireland.Doug Downey, Susan Dumais, Dan Liebling, and EricHorvitz.
2008.
Understanding the relationship be-tween searchers?
queries and information goals.
InProceedings of the 17th ACM conference on Infor-mation and knowledge management (CIKM?08), NapaValley, California.Christopher Dyer, Adam Lopez, Juri Ganitkevitch,Jonathan Weese, Ferhan Ture, Phil Blunsom, Hen-dra Setiawan, Vladimir Eidelman, and Philip Resnik.2010.
cdec: A decoder, alignment, and learningframework for finite-state and context-free translationmodels.
In Proceedings of the ACL-10 System Demon-strations (ACL?10), Uppsala, Sweden.Christopher Dyer, Victor Chahuneau, and Noah Smith.2013.
A simple, fast, and effective reparameterizationof IBM Model 2.
In Proceedings of the 2013 Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics on Human Lan-guage Technologies (NAACL-HLT?13), Atlanta, Geor-gia.Christopher Dyer.
2010.
A Formal Model of Ambigu-ity and Its Applications in Machine Translation.
Ph.D.thesis, University of Maryland, College Park, Mary-land.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, andTakehito Utsuro.
2008.
Overview of the patent trans-lation task at the NTCIR-7 workshop.
In Proceedingsof the 7th NII Testbeds and Community for Informa-tion access Research Workshop (NTCIR-7?08), Tokyo,Japan.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, andTakehito Utsuro.
2009.
Evaluating effects of machinetranslation accuracy on cross-lingual patent retrieval.In Proceedings of the 32rd international ACM SIGIRconference on Research and development in informa-tion retrieval (SIGIR?09), Boston, Massachusetts.Giorgio Gallo, Giustino Longo, Stefano Pallottino, andSang Nguyen.
1993.
Directed hypergraphs and ap-plications.
Discrete Applied Mathematics ?
Specialissue: combinatorial structures and algorithms, 42(2-3):177?201.Joshua Goodman.
1999.
Semiring parsing.
Computa-tional Linguistics, 25(4):573?605.Yunsong Guo and Carla Gomes.
2009.
Ranking struc-tured documents: A large margin based approach forpatent prior art search.
In Proc.
of the InternationalJoint Conference on Artificial Intelligence (IJCAI?09),Pasadena, CA.1180Kenneth Heafield.
2011.
KenLM: faster and smaller lan-guage model queries.
In Proceedings of the EMNLP2011 Sixth Workshop on Statistical Machine Transla-tion (WMT?11), Edinburgh, UK.Geoffrey E. Hinton.
2002.
Training products of expertsby minimizing contrastive divergence.
Neural Com-putation, 14(8):1771?1800.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technology (IWPT?05), Vancou-ver, Canada.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of the 45th Annual Meeting of the Associ-ation of Computational Linguistics (ACL?07), Prague,Czech Republic.Dan Klein and Christopher D. Manning.
2001.
Pars-ing and hypergraphs.
In Proceedings of the Sev-enth International Workshop on Parsing Technologies(IWPT?01), Beijing, China.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Christopher Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the ACL-07 2007 Demo and Poster Sessions(ACL?07), Prague, Czech Republic.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of Ma-chine Translation Summit X, Phuket, Thailand.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press, New York, New York, 1stedition.Chunyang Liu, Qi Liu, Yang Liu, and Maosong Sun.2012.
Thutr: A translation retrieval system.
InProceedings of COLING?12: Demonstration Papers,Bombay, India.Walid Magdy and Gareth Jones Jones.
2010.
PRES:A score metric for evaluating recall-oriented informa-tion retrieval applications.
In Proceedings of the 33rdInternational ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR?10),Geneva, Switzerland.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Sch?utze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, New York, NewYork.Ryan McDonald, Keith Hall, and Gideon Mann.
2010.Distributed training strategies for the structured per-ceptron.
In Proceedings of the 2010 Annual Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics on Human Lan-guage Technologies (NAACL-HLT?10), Los Angeles,California.Mehryar Mohri.
2009.
Weighted automata algorithms.In Handbook of weighted automata, pages 213?254.Springer Berlin Heidelberg.Vassilina Nikoulina, Bogomil Kovachev, Nikolaos Lagos,and Christof Monz.
2012.
Adaptation of statisticalmachine translation model for cross-lingual informa-tion retrieval in a service context.
In Proceedings ofthe 13th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL?12),Avignon, France.Stephen Robertson and Hugo Zaragoza.
2009.
Theprobabilistic relevance framework: BM25 and be-yond.
Foundations and Trends in Information Re-trieval, 3(4):333?389.Shigehiko Schamoni, Felix Hieber, Artem Sokolov, andStefan Riezler.
2014.
Learning translational andknowledge-based similarities from relevance rankingsfor cross-language retrieval.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (ACL?14), Baltimore, USA.Jason R. Smith, Herve Saint-Amand, Magdalena Pla-mada, Philipp Koehn, Chris Callison-Burch, andAdam Lopez.
2013.
Dirt cheap web-scale parallel textfrom the common crawl.
In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics (ACL?13), Sofia, Bulgaria.Mark D. Smucker, James Allan, and Ben Carterette.2007.
A comparison of statistical significance tests forinformation retrieval evaluation.
In Proceedings of the16th ACM conference on Conference on Informationand Knowledge Management (CIKM?07), New York,New York.Artem Sokolov, Laura Jehl, Felix Hieber, and StefanRiezler.
2013.
Boosting cross-language retrievalby learning bilingual phrase associations from rele-vance rankings.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP?13), Seattle, WA.Artem Sokolov, Felix Hieber, and Stefan Riezler.
2014.Learning to translate queries for clir.
In Proceedings ofthe 37th Annual ACM SIGIR Conference (SIGIR?14),Gold Coast, Australia.Ferhan Ture, Jimmy Lin, and Douglas W. Oard.
2012a.Combining statistical translation techniques for cross-language information retrieval.
In Proceedings of theInternational Conference on Computational Linguis-tics (COLING?12), Mumbai, India.Ferhan Ture, Jimmy Lin, and Douglas W. Oard.
2012b.Looking inside the box: Context-sensitive transla-tion for cross-language information retrieval.
In Pro-ceedings of the ACM SIGIR Conference on Research1181and Development in Information Retrieval (SIGIR?12),Portland, Oregon.Jinxi Xu, Ralph Weischedel, and Chanh Nguyen.
2001.Evaluating a probabilistic model for cross-lingual in-formation retrieval.
In Proceedings of the 24th annualinternational ACM SIGIR conference on Research anddevelopment in information retrieval (SIGIR?01), NewOrleans, Louisiana.Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.2013.
Max-violation perceptron and forced decodingfor scalable mt training.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP?13), Seattle, Washington.Matthew D. Zeiler.
2012.
ADADELTA: An adaptivelearning rate method.
Computing Research Repository(CoRR?2012), abs/1212.5701.Kai Zhao, Liang Huang, Haitao Mi, and Abe Ittycheriah.2014.
Hierarchical mt training using max-violationperceptron.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics(ACL?14), Baltimore, Maryland.1182
