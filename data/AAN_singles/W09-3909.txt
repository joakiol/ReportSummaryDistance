Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 62?70,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsGenre-Based Paragraph Classification for Sentiment AnalysisMaite TaboadaDepartment of LinguisticsSimon Fraser UniversityBurnaby, BC, Canadamtaboada@sfu.caJulian BrookeDepartment of Computer ScienceUniversity of TorontoToronto, ON, Canadajbrooke@cs.toronto.eduManfred StedeInstitute of LinguisticsUniversity of PotsdamPotsdam, Germanystede@ling.uni-potsdam.deAbstractWe present a taxonomy and classificationsystem for distinguishing between differ-ent types of paragraphs in movie reviews:formal vs. functional paragraphs and,within the latter, between description andcomment.
The classification is used forsentiment extraction, achieving im-provement over a baseline without para-graph classification.1 IntroductionMuch of the recent explosion in sentiment-related research has focused on finding low-levelfeatures that will help predict the polarity of aphrase, sentence or text.
Features, widely unders-tood, may be individual words that tend to ex-press sentiment, or other features that indicatenot only sentiment, but also polarity.
The twomain approaches to sentiment extraction, the se-mantic or lexicon-based, and the machine learn-ing or corpus-based approach, both attempt toidentify low-level features that convey opinion.In the semantic approach, the features are lists ofwords and their prior polarity, (e.g., the adjectiveterrible will have a negative polarity, and maybeintensity, represented as -4; the noun masterpiecemay be a 5).
Our approach is lexicon-based, butwe make use of information derived from ma-chine learning classifiers.Beyond the prior polarity of a word, its localcontext obviously plays an important role inconveying sentiment.
Polanyi and Zaenen (2006)use the term ?contextual valence shifters?
to referto expressions in the local context that maychange a word?s polarity, such as intensifiers,modal verbs, connectives, and of course negation.Further beyond the local context, the overallstructure and organization of the text, influencedby its genre, can help the reader determine howthe evaluation is expressed, and where it lies.Polanyi and Zaenen (2006) also cite genre con-straints as relevant factors in calculating senti-ment.Among the many definitions of genre, we takethe view of Systemic Functional Linguistics thatgenres are purposeful activities that develop instages, or parts (Eggins and Martin, 1997), whichcan be identified by lexicogrammatical proper-ties (Eggins and Slade, 1997).
Our proposal isthat, once we have identified different stages in atext, the stages can be factored in the calculationof sentiment, by weighing more heavily thosethat are more likely to contain evaluation, an ap-proach also pursued in automatic summarization(Seki et al, 2006).To test this hypothesis, we created a taxonomyof stages specific to the genre of movie reviews,and annotated a set of texts.
We then trainedvarious classifiers to differentiate the stages.Having identified the stages, we lowered theweight of those that contained mostly description.Our results show that we can achieve improve-ment over a baseline when classifying the polar-ity of texts, even with a classifier that can standto improve (at 71.1% accuracy).
The best per-formance comes from weights derived from theoutput of a linear regression classifier.We first describe our inventory of stages andthe manual annotation (Section 2), and in Sec-tion 3 turn to automatic stage classification.
Afterdescribing our approach to sentiment classifica-tion of texts in Section 4, we describe experi-ments to improve its performance with the in-formation on stages in Section 5.
Section 6 dis-62cusses related work, and Section 7 provides con-clusions.2 Stages in movie reviewsWithin the larger review genre, we focus onmovie reviews.
Movie reviews are particularlydifficult to classify (Turney, 2002), because largeportions of the review contain description of theplot, the characters, actors, director, etc., orbackground information about the film.Our approach is based on the work of Bieler etal.
(2007), who identify formal and functionalzones (stages) within German movie reviews.Formal zones are parts of the text that contributefactual information about the cast and the credits,and also about the review itself (author, date ofpublication and the reviewer?s rating of the mov-ie).
Functional zones contain the main gist of thereview, and can be divided roughly into descrip-tion and comment.
Bieler et al showed that func-tional zones could be identified using 5-gramSVM classifiers built from an annotated Germancorpus.2.1 TaxonomyIn addition to the basic Describe/Comment dis-tinction in Bieler et al, we use a De-scribe+Comment label, as in our data it is oftenthe case that both description and comment arepresent in the same paragraph.
We decided that aparagraph could be labeled as De-scribe+Comment when it contained at least aclause of each, and when the comment part couldbe assigned a polarity (i.e., it was not only sub-jective, but also clearly positive or negative).Each of the three high-level tags has a subtag,a feature also present in Bieler et al?s manualannotation.
The five subtags are: overall, plot,actors/characters, specific and general.
?Specific?refers to one particular aspect of the movie (notplot or characters), whereas ?general?
refers tomultiple topics in the same stage (special effectsand cinematography at the same time).
Outsidethe Comment/Describe scale, we also includetags such as Background (discussion of othermovies or events outside the movie beingreviewed), Interpretation (subjective but notopinionated or polar), and Quotes.
Altogether,the annotation system includes 40 tags, with 22formal and 18 functional zones.
Full lists ofzone/stage labels are provided in Appendix A.2.2 Manual annotationWe collected 100 texts from rottentomatoes.com,trying to include one positive and one negativereview for the same movie.
The reviews are partof the ?Top Critics?
section of the site, all ofthem published in newspapers or on-line maga-zines.
We restricted the texts to ?Top Critics?because we wanted well-structured, polishedtexts, unlike those found in some on-line reviewsites.
Future work will address those more in-formal reviews.The 100 reviews contain 83,275 words and1,542 paragraphs.
The annotation was performedat the paragraph level.
Although stages may spanacross paragraphs, and paragraphs may containmore than one stage, there is a close relationshipbetween paragraphs and stages.
The restrictionalso resulted in a more reliable annotation, per-formed with the PALinkA annotation tool (Ora-san, 2003).The annotation was performed by one of theauthors, and we carried out reliability tests withtwo other annotators, one another one of the au-thors, who helped develop the taxonomy, and thethird one a project member who read the annota-tion guidelines1, and received a few hours?
train-ing in the labels and software.
We used Fleiss?kappa (Fleiss, 1971), which extends easily to thecase of multiple raters (Di Eugenio and Glass,2004).
We all annotated four texts.
The results ofthe reliability tests show a reasonable agreementlevel for the distinction between formal andfunctional zones (.84 for the 3-rater kappa).
Thelowest reliability was for the 3-way distinction inthe functional zones (.68 for the first two raters,and .54 for the three raters).
The full kappa val-ues for all the distinctions are provided in Ap-pendix B.
After the reliability test, one of theauthors performed the full annotation for all 100texts.
Table 1 shows the breakdown of high-levelstages for the 100 texts.Stage CountDescribe 347Comment 237Describe+Comment 237Background 51Interpretation 22Quote 2Formal 646Table 1.
Stages in 100 text RT corpus1Available from http://www.sfu.ca/~mtaboada/nserc-project.html633 Classifying stagesOur first classification task aims at distinguishingthe two main types of functional zones, Com-ment and Describe, vs.
Formal zones.3.1 FeaturesWe test two different sets of features.
The first,following Bieler et al (2007), consists of 5-grams (including unigrams, bigrams, 3-gramsand 4-grams), although we note in our case thatthere was essentially no performance benefitbeyond 3-grams.
We limited the size of our fea-ture set to n-grams that appeared at least 4 timesin our training corpus.
For the 2 class task (noformal zones), this resulted in 8,092 binary fea-tures, and for the 3 and 4 class task there were9,357 binary n-gram features.The second set of features captures differentaspects of genre and evaluation, and can in turnbe divided into four different types, according tosource.
With two exceptions (features indicatingwhether a paragraph was the first or last para-graph in text), the features were numerical (fre-quency) and normalized to the length of the pa-ragraph.The first group of genre features comes fromBiber (1988), who attempted to characterize di-mensions of genre.
The features here include fre-quency of first, second and third person pro-nouns; demonstrative pronouns; place and timeadverbials; intensifiers; and modals, among anumber of others.The second category of genre features in-cludes discourse markers, primarily from Knott(1996), that indicate contrast, comparison, causa-tion, evidence, condition, and similar relations.The third type of genre features was a list of500 adjectives classified in terms of Appraisal(Martin and White, 2005) as indicating Apprec-iation, Judgment or Affect.
Appraisal categorieshave been shown to be useful in improving theperformance of polarity classifiers (Whitelaw etal., 2005).Finally, we also include text statistics as fea-tures, such as average length of words and sen-tences and position of paragraphs in the text.3.2 ClassifiersTo classify paragraphs in the text, we use theWEKA suite (Witten and Frank, 2005), testingthree popular machine learning algorithms:Na?ve Bayes, Support Vector Machine, and Li-near Regression (preliminary testing with Deci-sion Trees suggests that it is not appropriate forthis task).
Training parameters were set to defaultvalues.In order to use Linear Regression, which pro-vides a numerical output based on feature valuesand derived feature weights, we have to conceiveof Comment/Describe/Describe+Comment not asnominal (or ordinal) classes, but rather as corres-ponding to a Comment/Describe ratio, with?pure?
Describe at one end and ?pure?
Commentat the other.
For training, we assign a 0 value (aComment ratio) to all paragraphs tagged De-scribe and a 1 to all Comment paragraphs; forDescribe+Comment, various options (includingomission of this data) were tested.
The time re-quired to train a linear regression classifier on alarge feature set proved to be prohibitive, andperformance with smaller sets of features gener-ally quite poor, so for the linear regression clas-sifier we present results only for our compact setof genre features.3.3 PerformanceTable 2 shows the performance of classifi-er/feature-set combinations for the 2-, 3-, and 4-class tasks on the 100-text training set, with 10-fold cross-validation, in terms of precision (P),recall (R) and F-measure 2 .
SVM and Na?veBayes provide comparable performance, al-though there is considerable variation, particular-ly with respect to the feature set; the SVM is asignificantly (p<0.05) better choice for our genrefeatures 3 , while for the n-gram features theBayes classification is generally preferred.
TheSVM-genre classifier significantly outperformsthe other classifiers in the 2-class task; these ge-nre features, however, are not as useful as 5-grams at identifying Formal zones (the n-gramclassifier, by contrast, can make use of wordssuch as cast).
In general, formal zone classifica-tion is fairly straightforward, whereas identifica-tion of Describe+Comment is quite difficult, andthe SVM-genre classifier, which is more sensi-tive to frequency bias, elects to (essentially) ig-nore this category in order to boost overall accu-racy.To evaluate a linear regression (LR) classifier,we calculate correlation coefficient ?, which re-flects the goodness of fit of the line to the da-ta.
Table 3 shows values for the classifiers builtfrom the corpus, with various Comment ratios2 For the 2- and 3-way classifiers, Describe+Comment pa-ragraphs are treated as Comment.
This balances the num-bers of each class, ultimately improving performance.3 All significance tests use chi-square (?2).64ClassifierComment Describe Formal Desc+Comm OverallAccuracy P R F P R F P R F P R F2-class-5-gram-Bayes .66 .79 .72 .70 .55 .62 - - - - - - 68.02-class-5-gram-SVM .53 .63 .64 .68 .69 .69 - - - - - - 66.82-class-genre-Bayes .66 .75 .70 .67 .57 .61 - - - - - - 66.22-class-genre-SVM .71 .76 .74 .71 .65 .68 - - - - - - 71.13-class-5-gram-Bayes .69 .49 .57 .66 .78 .71 .92 .97 .95 - - - 78.13-class-5-gram-SVM .64 .63 .63 .68 .65 .65 .91 .97 .94 - - - 77.23-class-genre-Bayes .68 .68 .66 .67 .46 .55 .84 .96 .90 - - - 74.03-class-genre-SVM .66 .71 .68 .67 .56 .61 .90 .94 .92 - - - 76.84-class-5-gram-Bayes .46 .35 .38 .69 .47 .56 .92 .97 .95 .42 .64 .51 69.04-class-5-gram-SVM .43 .41 .44 .59 .62 .60 .91 .97 .94 .45 .41 .42 69.64-class-genre-Bayes .38 .31 .34 .66 .30 .41 .86 .97 .90 .33 .60 .42 62.34-class-genre-SVM .46 .32 .38 .53 .82 .65 .87 .94 .90 .26 .03 .06 67.4Table 2.
Stage identification performance of various categorical classifiers(C) assigned to paragraphs with the De-scribe+Comment tag, and with De-scribe+Comment paragraphs removed from con-sideration.Classifier ?LR, Des+Com C = 0 .37LR, Des+Com C = 0.25 .44LR, Des+Com C = 0.5 .47LR, Des+Com C = 0.75 .46LR, Des+Com C = 1 .43LR, No Des+Com .50Table 3.
Correlation coefficients for LRclassifiersThe drop in correlation when more extremevalues are assigned to Describe+Comment sug-gests that Describe+Comment paragraphs do in-deed belong in the middle of the Comment spec-trum.
Since there is a good deal of variation inthe amount of comment across De-scribe+Comment paragraphs, the best correlationcomes with complete removal of these somewhatunreliable paragraphs.
Overall, these numbersindicate that variations in relevant features areable to predict roughly 50% of the variation inComment ratio, which is fairly good consideringthe small number and simplistic nature of thefeatures involved.4 Sentiment detection: SO-CALIn this section, we outline our semantic orienta-tion calculator, SO-CAL.
SO-CAL extractswords from a text, and aggregates their semanticorientation value, which is in turn extracted froma set of dictionaries.
SO-CAL uses five dictionar-ies: four lexical dictionaries with 2,257 adjec-tives, 1,142 nouns, 903 verbs, and 745 adverbs,and a fifth dictionary containing 177 intensifyingexpressions.
Although the majority of the entriesare single words, the calculator also allows formultiword entries written in regular expression-like language.The SO-carrying words in these dictionarieswere taken from a variety of sources, the threelargest a corpus of 400 reviews from Epin-ions.com, first used by Taboada and Grieve(2004), a 100 text subset of the 2,000 movie re-views in the Polarity Dataset (Pang and Lee,2004), and words from the General Inquirer dic-tionary (Stone, 1997).
Each of the open-classwords were given a hand-ranked SO value be-tween 5 and -5 (neutral or zero-value words arenot included in the dictionary) by a native Eng-lish speaker.
The numerical values were chosento reflect both the prior polarity and strength ofthe word, averaged across likely interpretations.For example, the word phenomenal is a 5, nicelya 2, disgust a -3, and monstrosity a -5.
The dic-tionary was later reviewed by a committee ofthree other researchers in order to minimize thesubjectivity of ranking SO by hand.Our calculator moves beyond simple averag-ing of each word?s semantic orientation value,and implements and expands on the insights ofPolanyi and Zaenen (2006) with respect to con-textual valence shifters.
We implement negationby shifting the SO value of a word towards theopposite polarity (not terrible, for instance, iscalculated as -5+4 = -1).
Intensification is mod-eled using percentage modifiers (very engaging:4x125% = 5).
We also ignore words appearingwithin the scope of irrealis markers such as cer-tain verbs, modals, and punctuation, and de-crease the weight of words which appear often inthe text.
In order to counter positive linguistic65bias (Boucher and Osgood, 1969), a problem forlexicon-based sentiment classifiers (Kennedy andInkpen, 2006), we increase the final SO of anynegative expression appearing in the text.The performance of SO-CAL tends to be inthe 76-81% range.
We have tested on informalmovie, book and product reviews and on the Po-larity Dataset (Pang and Lee, 2004).
The perfor-mance on movie reviews tends to be on the lowerend of the scale.
Our baseline for movies, de-scribed in Section 5, is 77.7%.
We believe thatwe have reached a ceiling in terms of word- andphrase-level performance, and most future im-provements need to come from discourse fea-tures.
The stage classification described in thispaper is one of them.5 ResultsThe final goal of a stage classifier is to use theinformation about different stages in sentimentclassification.
Our assumption is that descriptiveparagraphs contain less evaluative content aboutthe movie being reviewed, and they may includenoise, such as evaluative words describing theplot or the characters.
Once the paragraph clas-sifier had assigned labels we used those labels toweigh paragraphs.5.1 Classification with manual tagsBefore moving on to automatic paragraph classi-fication, we used the 100 annotated texts to seethe general effect of weighting paragraphs withthe ?perfect?
human annotated tags on sentimentdetection, in order to show the potential im-provements that can be gained from this ap-proach.Our baseline polarity detection performanceon the 100 annotated texts is 65%, which is verylow, even for movie reviews.
We posit that for-mal movie reviews might be particularly difficultbecause full plot descriptions are more commonand the language used to express opinion lessstraightforward (metaphors are common).
How-ever, if we lower the weight on non-Commentand mixed Comment paragraphs (to 0, except forDescribe+Comment, which is maximized by a0.1 weight), we are able to boost performance to77%, an improvement which is significant at thep<0.05 level.
Most of the improvement (7%) isdue to disregarding Describe paragraphs, but 2%comes from Describe+Comment, and 1% eachfrom Background, Interpretation, and (all) For-mal tags.
There is no performance gain, however,from the use of aspect tags (e.g., by increasingthe weight on Overall paragraphs), justifying ourdecision to ignore subtags for text-level polarityclassification.5.2 Categorical classificationWe evaluated all the classifiers from Table 2, butwe omit discussion of the worst performing.
Theevaluation was performed on the Polarity Dataset(Pang and Lee, 2004), a collection of 2,000 on-line movie reviews, balanced for polarity.
TheSO performance for the categorical classifiers isgiven in Figure 1.
When applicable, we alwaysgave Formal Zones (which Table 2 indicates arefairly easy to identify) a weight of 0, however forDescribe paragraphs we tested at 0.1 intervalsbetween 0 and 1.
Testing all possible values ofDescribe+Comment was not feasible, so we setthe weights of those to a value halfway betweenthe weight of Comment paragraphs (1) and theweight of the Describe paragraph.Most of the classifiers were able to improveperformance beyond the 77.7% (unweighted)baseline.
The best performing model (the 2-class-genre-SVM) reached a polarity identifica-tion accuracy of 79.05%, while the second best(the 3-class 5-gram-SVM) topped out at 78.9%.Many of the classifiers showed a similar patternwith respect to the weight on Describe, increas-ing linearly as weight on Describe was decreasedbefore hitting a maximum in the 0.4-0.1 range,and then dropping afterwards (often precipitous-ly).
Only the classifiers which were more con-servative with respect to Describe, such as the 4-class-5-gram-Bayes, avoided the drop, which canbe attributed to low precision Describe identifi-cation: At some point, the cost associated withdisregarding paragraphs which have been mis-tagged as Describe becomes greater that the ben-efit of disregarding correctly-labeled ones.
In-deed, the best performing classifier for each classoption is exactly the one that has the highest pre-cision for identification of Describe, regardlessof other factors.
This suggests that improvingprecision is key, and, in lieu of that, weighting isa better strategy than simply removing parts ofthe text.In general, increasing the complexity of thetask (increasing the number of classes) decreasesperformance.
One clear problem is that the iden-tification of Formal zones, which are much morecommon in our training corpus than our test cor-pus, does not add important information, sincemost Formal zones have no SO valued words.The delineation of an independent De-scribe+Comment class is mostly ineffective,66Figure 1.
SO Performance with various paragraph tagging classifiers, by weight on Describeprobably because this class is not easily distin-guishable from Describe and Comment (nor infact should it be).We can further confirm that our classifier isproperly distinguishing Describe and Commentby discounting Comment paragraphs rather thanDescribe paragraphs (following Pang and Lee2004).
When Comment paragraphs tagged by thebest performing classifier are ignored, SO-CAL?saccuracy drops to 56.65%, just barely abovechance.5.3 Continuous classificationTable 4 gives the results for the linear regressionclassifier, which assigns a Comment ratio to eachparagraph used for weighting.Model AccuracyLR, Des+Com C = 0 78.75LR, Des+Com C = 0.25 79.35LR, Des+Com C = 0.5 79.00LR, Des+Com C = 0.75 78.90LR, Des+Com C = 1 78.95LR, No Des+Com 79.05Table 4.
SO Performance with linear regressionThe linear regression model trained with a0.25 comment ratio on Describe+Comment para-graphs provides the best performance of all clas-sifiers we tested (an improvement of 1.65% frombaseline).
The correlation coefficients notedin Table 4 are reflected in these results, but thespike at C = 0.25 is most likely related to a gen-eral preference for low (but non-zero) weights onDescribe+Comment paragraphs also noted whenweights were applied using the manual tags;these paragraphs are unreliable (as compared topure Comment), but cannot be completely dis-counted.
There were some texts which had onlyDescribe+Comment paragraphs.Almost a third of the tags assigned by the 2-class genre feature classifier were different thanthe corresponding n-gram classifier, suggestingthe two classifiers might have different strengths.However, initial attempts to integrate the varioushigh performing classifiers?including collaps-ing of feature sets, metaclassifiers, and doubletagging of paragraphs?resulted in similar orworse performance.
We have not tested all poss-ible options (there are simply too many), but wethink it unlikely that additional gains will bemade with these simple, surface feature sets.
Al-though our testing with human annotated textsand the large performance gap between moviereviews and other consumer reviews both sug-gest there is more potential for improvement, itwill probably require more sophisticated andprecise models.6 Related workThe bulk of the work in sentiment analysis hasfocused on classification at either the sentencelevel, e.g., the subjectivity/polarity detection ofWiebe and Riloff (2005), or alternatively at thelevel of the entire text.
With regards to the latter,two major approaches have emerged: the use ofmachine learning classifiers trained on n-grams777879801 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0SO?Calculator?AccuracyWeight?on?Describe?ParagraphNo?tagging?Baseline2?Class?5?gram?SVM2?Class?5?gram?Bayes2?Class?genre?Bayes2?Class?genre?SVM3?Class?5?gram?Bayes3?Class?5?gram?SVM3?Class?genre?Bayes4?Class?5?gram?Bayes4?Class?5?gram?SVM4?Class?genre?Bayes67or similar features (Pang et al, 2002), and theuse of sentiment dictionaries (Esuli and Sebas-tiani, 2006; Taboada et al, 2006).
Support Vec-tor Machine (SVM) classifiers have been shownto out-perform lexicon-based models within asingle domain (Kennedy and Inkpen, 2006);however they have trouble with cross-domaintasks (Aue and Gamon, 2005), and some re-searchers have argued for hybrid classifiers (An-dreevskaia and Bergler, 2008).Pang and Lee (2004) attempted to improve theperformance of an SVM classifier by identifyingand removing objective sentences from the texts.Results were mixed: The improvement was mi-nimal for the SVM classifier (though the perfor-mance of a na?ve Bayes classifier was signifi-cantly boosted), however testing with parts of thetext classified as subjective showed that the elim-inated parts were indeed irrelevant.
In contrast toour findings, they reported a drop in performancewhen paragraphs were taken as the only possibleboundary between subjective and objective textspans.Other research that has dealt with identifyingmore or less relevant parts of the text for the pur-poses of sentiment analysis include Taboada andGrieve (2004), who improved the performance ofa lexicon-based model by weighing words to-wards the end of the text; Nigam and Hurst(2006), who detect polar expressions in topicsentences; and Voll and Taboada (2007), whoused a topic classifier and discourse parser toeliminate potentially off-topic or less importantsentences.7 ConclusionsWe have described a genre-based taxonomy forclassifying paragraphs in movie reviews, withthe main classification being a distinction be-tween formal and functional stages, and, withinthose, between mainly descriptive vs. commentstages.
The taxonomy was used to annotate 100movie reviews, as the basis for building classifi-ers.We tested a number of different classifiers.Our results suggest that a simple, two-way orcontinuous classification using a small set of lin-guistically-motivated features is the best for ourpurposes; a more complex system is feasible, butcomes at the cost of precision, which seems to bethe key variable in improving sentiment analysis.Ultimately, the goal of the classification wasto improve the accuracy of SO-CAL, our seman-tic orientation calculator.
Using the manual an-notations, we manage to boost performance by12% over the baseline.
With the best automaticclassifier, we still show consistent improvementover the baseline.
Given the relatively low accu-racy of the classifiers, the crucial factor involvesusing fine-grained weights on paragraphs, ratherthan simply ignoring Describe-labeled para-graphs, as Pang and Lee (2004) did for objectivesentences.An obvious expansion to this work would in-volve a larger dataset on which to train, to im-prove the performance of the classifier(s).
Wewould also like to focus on the syntactic patternsand verb class properties of narration, aspectsthat are not captured with simply using wordsand POS labels.
Connectives in particular aregood indicators of the difference between narra-tion (temporal connectives) and opinion (contras-tive connectives).
There may also be benefit tocombining paragraph- and sentence-based ap-proaches.
Finally, we would like to identifycommon sequences of stages, such as plot andcharacter descriptions appearing together, andbefore evaluation stages.
This generic structurehas been extensively studied for many genres(Eggins and Slade, 1997).Beyond sentiment extraction, our taxonomyand classifiers can be used for searching and in-formation retrieval.
One could, for instance, ex-tract paragraphs that include mostly comment ordescription.
Using the more fine-grained labels,searches for comment/description on actors, di-rectors, or other aspects of the movie are possible.AcknowledgementsThis work was supported by SSHRC (410-2006-1009) and NSERC (261104-2008) grants toMaite Taboada.ReferencesAndreevskaia, Alina & Sabine Bergler.
2008.
Whenspecialists and generalists work together: Domaindependence in sentiment tagging.
Proceedings of46th Annual Meeting of the Association for Com-putational Linguistics (pp.
290-298).
Columbus,OH.Aue, Anthony & Michael Gamon.
2005.
Customizingsentiment classifiers to new domains: A case study.Proceedings of the International Conference onRecent Advances in Natural Language Processing.Borovets, Bulgaria.Biber, Douglas.
1988.
Variation across Speech andWriting.
Cambridge: Cambridge University Press.68Bieler, Heike, Stefanie Dipper & Manfred Stede.2007.
Identifying formal and functional zones infilm reviews.
Proceedings of the 8th SIGdialWorkshop on Discourse and Dialogue (pp.
75-78).Antwerp, Belgium.Boucher, Jerry D. & Charles E. Osgood.
1969.
ThePollyanna hypothesis.
Journal of Verbal Learningand Verbal Behaviour, 8: 1-8.Di Eugenio, Barbara & Michael Glass.
2004.
Thekappa statistic: A second look.
Computational Lin-guistics, 30(1): 95-101.Eggins, Suzanne & James R. Martin.
1997.
Genresand registers of discourse.
In Teun A. van Dijk(ed.
), Discourse as Structure and Process.
Dis-course Studies: A Multidisciplinary Introduction(pp.
230-256).
London: Sage.Eggins, Suzanne & Diana Slade.
1997.
AnalysingCasual Conversation.
London: Cassell.Esuli, Andrea & Fabrizio Sebastiani.
2006.
Senti-WordNet: A publicly available lexical resource foropinion mining.
Proceedings of 5th InternationalConference on Language Resources and Evaluation(LREC) (pp.
417-422).
Genoa, Italy.Fleiss, Joseph L. 1971.
Measuring nominal scaleagreement among many raters.
Psychological Bul-letin, 76: 378-382.Kennedy, Alistair & Diana Inkpen.
2006.
Sentimentclassification of movie and product reviews usingcontextual valence shifters.
Computational Intelli-gence, 22(2): 110-125.Knott, Alistair.
1996.
A Data-Driven Methodology forMotivating a Set of Coherence Relations.
Edin-burgh, UK: University of EdinburghThesis Type.Martin, James R. & Peter White.
2005.
The Languageof Evaluation.
New York: Palgrave.Nigam, Kamal & Matthew Hurst.
2006.
Towards arobust metric of polarity.
In Janyce Wiebe (ed.
),Computing Attitude and Affect in Text: Theoryand Applications (pp.
265-279).
Dordrecht: Sprin-ger.Orasan, Constantin.
2003.
PALinkA: A highly custo-mizable tool for discourse annotation.
Proceedingsof 4th SIGdial Workshop on Discourse and Dialog(pp.
39 ?
43).
Sapporo, Japan.Pang, Bo & Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
Proceedings of42nd Meeting of the Association for Computation-al Linguistics (pp.
271-278).
Barcelona, Spain.Pang, Bo, Lillian Lee & Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingMachine Learning techniques.
Proceedings of Con-ference on Empirical Methods in NLP (pp.
79-86).Polanyi, Livia & Annie Zaenen.
2006.
Contextualvalence shifters.
In James G. Shanahan, Yan Qu &Janyce Wiebe (eds.
), Computing Attitude and Af-fect in Text: Theory and Applications (pp.
1-10).Dordrecht: Springer.Seki, Yohei, Koji Eguchi & Noriko Kando.
2006.Multi-document viewpoint summarization focusedon facts, opinion and knowledge.
In Janyce Wiebe(ed.
), Computing Attitude and Affect in Text:Theory and Applications (pp.
317-336).
Dordrecht:Springer.Stone, Philip J.
1997.
Thematic text analysis: Newagendas for analyzing text content.
In Carl Roberts(ed.
), Text Analysis for the Social Sciences.
Mah-wah, NJ: Lawrence Erlbaum.Taboada, Maite, Caroline Anthony & Kimberly Voll.2006.
Creating semantic orientation dictionaries.Proceedings of 5th International Conference onLanguage Resources and Evaluation (LREC) (pp.427-432).
Genoa, Italy.Taboada, Maite & Jack Grieve.
2004.
Analyzing ap-praisal automatically.
Proceedings of AAAI SpringSymposium on Exploring Attitude and Affect inText (AAAI Technical Report SS-04-07) (pp.
158-161).
Stanford University, CA.Turney, Peter.
2002.
Thumbs up or thumbs down?Semantic orientation applied to unsupervised clas-sification of reviews.
Proceedings of 40th Meetingof the Association for Computational Linguistics(pp.
417-424).Voll, Kimberly & Maite Taboada.
2007.
Not allwords are created equal: Extracting semantic orien-tation as a function of adjective relevance.
Pro-ceedings of the 20th Australian Joint Conferenceon Artificial Intelligence (pp.
337-346).
GoldCoast, Australia.Whitelaw, Casey, Navendu Garg & Shlomo Arga-mon.
2005.
Using Appraisal groups for sentimentanalysis.
Proceedings of ACM SIGIR Conferenceon Information and Knowledge Management(CIKM 2005) (pp.
625-631).
Bremen, Germany.Wiebe, Janyce & Ellen Riloff.
2005.
Creating subjec-tive and objective sentence classifiers from unan-notated texts.
Proceedings of Sixth InternationalConference on Intelligent Text Processing andComputational Linguistics (CICLing-2005).
Mex-ico City, Mexico.Witten, Ian H. & Eibe Frank.
2005.
Data Mining:Practical Machine Learning Tools and Techniques(2nd edn.).
San Francisco: Morgan Kaufmann.69Appendix A: Full lists of formal and functional zonesFigure A1.
Functional zonesFigure A2.
Formal zonesDescribeCommentPlotCharacterSpecificGeneralContentPlotActors+charactersSpecificGeneralOverallPlotActors+charactersSpecificGeneralContentStructuralelementsInformationabout thefilmTaglineStructureOff-topicTitle, Title+year, Runtime,Country+year, Director,Genre, Audience-restriction,Cast, Credits, Show-Loc+date,Misc-Movie-InfoSource, Author, Author-Bio,Place, Date, Legal-Notice,Misc-Review-Info, RatingAppendix B: Kappa values for annotation taskClasses 2-raterkappa3-raterkappaDescribe/Comment/Describe+Comment/Formal .82 .73Describe/Comment/Formal .92 .84Describe/Comment/Describe+Comment .68 .54Describe/Comment .84 .69Table B1.
Kappa values for stage annotations70
