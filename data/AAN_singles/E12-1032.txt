Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 315?324,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsExtending the Entity-based Coherence Model with Multiple RanksVanessa Wei FengDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canadaweifeng@cs.toronto.eduGraeme HirstDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canadagh@cs.toronto.eduAbstractWe extend the original entity-based coher-ence model (Barzilay and Lapata, 2008)by learning from more fine-grained coher-ence preferences in training data.
We asso-ciate multiple ranks with the set of permuta-tions originating from the same source doc-ument, as opposed to the original pairwiserankings.
We also study the effect of thepermutations used in training, and the effectof the coreference component used in en-tity extraction.
With no additional manualannotations required, our extended modelis able to outperform the original model ontwo tasks: sentence ordering and summarycoherence rating.1 IntroductionCoherence is important in a well-written docu-ment; it helps make the text semantically mean-ingful and interpretable.
Automatic evaluationof coherence is an essential component of vari-ous natural language applications.
Therefore, thestudy of coherence models has recently becomean active research area.
A particularly popularcoherence model is the entity-based local coher-ence model of Barzilay and Lapata (B&L) (2005;2008).
This model represents local coherenceby transitions, from one sentence to the next, inthe grammatical role of references to entities.
Itlearns a pairwise ranking preference between al-ternative renderings of a document based on theprobability distribution of those transitions.
Inparticular, B&L associated a lower rank with au-tomatically created permutations of a source doc-ument, and learned a model to discriminate anoriginal text from its permutations (see Section3.1 below).
However, coherence is matter of de-gree rather than a binary distinction, so a modelbased only on such pairwise rankings is insuffi-ciently fine-grained and cannot capture the sub-tle differences in coherence between the permuteddocuments.Since the first appearance of B&L?s model,several extensions have been proposed (see Sec-tion 2.3 below), primarily focusing on modify-ing or enriching the original feature set by incor-porating other document information.
By con-trast, we wish to refine the learning procedurein a way such that the resulting model will beable to evaluate coherence on a more fine-grainedlevel.
Specifically, we propose a concise exten-sion to the standard entity-based coherence modelby learning not only from the original docu-ment and its corresponding permutations but alsofrom ranking preferences among the permutationsthemselves.We show that this can be done by assigning asuitable objective score for each permutation indi-cating its dissimilarity from the original one.
Wecall this a multiple-rank model since we train ourmodel on a multiple-rank basis, rather than tak-ing the original pairwise ranking approach.
Thisextension can also be easily combined with otherextensions by incorporating their enriched featuresets.
We show that our multiple-rank model out-performs B&L?s basic model on two tasks, sen-tence ordering and summary coherence rating,evaluated on the same datasets as in Barzilay andLapata (2008).In sentence ordering, we experiment withdifferent approaches to assigning dissimilarityscores and ranks (Section 5.1.1).
We also exper-iment with different entity extraction approaches315Manila Miles Island Quake Baco1 ?
?
X X ?2 S ?
O ?
?3 X X X X XTable 1: A fragment of an entity grid for five entitiesacross three sentences.
(Section 5.1.2) and different distributions of per-mutations used in training (Section 5.1.3).
Weshow that these two aspects are crucial, depend-ing on the characteristics of the dataset.2 Entity-based Coherence Model2.1 Document RepresentationThe original entity-based coherence model isbased on the assumption that a document makesrepeated reference to elements of a set of entitiesthat are central to its topic.
For a document d, anentity grid is constructed, in which the columnsrepresent the entities referred to in d, and rowsrepresent the sentences.
Each cell correspondsto the grammatical role of an entity in the corre-sponding sentence: subject (S), object (O), nei-ther (X), or nothing (?).
An example fragmentof an entity grid is shown in Table 1; it showsthe representation of three sentences from a texton a Philippine earthquake.
B&L define a lo-cal transition as a sequence {S ,O, X,?
}n, repre-senting the occurrence and grammatical roles ofan entity in n adjacent sentences.
Such transi-tion sequences can be extracted from the entitygrid as continuous subsequences in each column.For example, the entity ?Manila?
in Table 1 hasa bigram transition {S , X} from sentence 2 to 3.The entity grid is then encoded as a feature vector?
(d) = (p1(d), p2(d), .
.
.
, pm(d)), where pt(d) isthe probability of the transition t in the entity grid,and m is the number of transitions with length nomore than a predefined optimal transition lengthk.
pt(d) is computed as the number of occurrencesof t in the entity grid of document d, divided bythe total number of transitions of the same lengthin the entity grid.For entity extraction, Barzilay and Lapata(2008) had two conditions: Coreference+ andCoreference?.
In Coreference+, entity corefer-ence relations in the document were resolved byan automatic coreference resolution tool (Ng andCardie, 2002), whereas in Coreference?, nounsare simply clustered by string matching.2.2 Evaluation TasksTwo evaluation tasks for Barzilay and Lapata(2008)?s entity-based model are sentence order-ing and summary coherence rating.In sentence ordering, a set of random permu-tations is created for each source document, andthe learning procedure is conducted on this syn-thetic mixture of coherent and incoherent docu-ments.
Barzilay and Lapata (2008) experimentedon two datasets: news articles on the topic ofearthquakes (Earthquakes) and narratives on thetopic of aviation accidents (Accidents).
A train-ing data instance is constructed as a pair con-sisting of a source document and one of its ran-dom permutations, and the permuted documentis always considered to be less coherent than thesource document.
The entity transition featuresare then used to train a support vector machineranker (Joachims, 2002) to rank the source docu-ments higher than the permutations.
The model istested on a different set of source documents andtheir permutations, and the performance is evalu-ated as the fraction of correct pairwise rankings inthe test set.In summary coherence rating, a similar exper-imental framework is adopted.
However, in thistask, rather than training and evaluating on a setof synthetic data, system-generated summariesand human-composed reference summaries fromthe Document Understanding Conference (DUC2003) were used.
Human annotators were askedto give a coherence score on a seven-point scalefor each item.
The pairwise ranking preferencesbetween summaries generated from the same in-put document cluster (excluding the pairs consist-ing of two human-written summaries) are used bya support vector machine ranker to learn a dis-criminant function to rank each pair according totheir coherence scores.2.3 Extended ModelsFilippova and Strube (2007) applied Barzilay andLapata?s model on a German corpus of newspa-per articles with manual syntactic, morphological,and NP coreference annotations provided.
Theyfurther clustered entities by semantic relatednessas computed by the WikiRelated!
API (Strube andPonzetto, 2006).
Though the improvement wasnot significant, interestingly, a short subsection in316their paper described their approach to extendingpairwise rankings to longer rankings, by supply-ing the learner with rankings of all renderings ascomputed by Kendall?s ?, which is one of ourextensions considered in this paper.
AlthoughFilippova and Strube simply discarded this ideabecause it hurt accuracies when tested on theirdata, we found it a promising direction for furtherexploration.
Cheung and Penn (2010) adaptedthe standard entity-based coherence model to thesame German corpus, but replaced the originallinguistic dimension used by Barzilay and Lap-ata (2008) ?
grammatical role ?
with topologi-cal field information, and showed that for Germantext, such a modification improves accuracy.For English text, two extensions have been pro-posed recently.
Elsner and Charniak (2011) aug-mented the original features used in the standardentity-based coherence model with a large num-ber of entity-specific features, and their extensionsignificantly outperformed the standard modelon two tasks: document discrimination (anothername for sentence ordering), and sentence inser-tion.
Lin et al(2011) adapted the entity grid rep-resentation in the standard model into a discourserole matrix, where additional discourse informa-tion about the document was encoded.
Their ex-tended model significantly improved ranking ac-curacies on the same two datasets used by Barzi-lay and Lapata (2008) as well as on the Wall StreetJournal corpus.However, while enriching or modifying theoriginal features used in the standard model is cer-tainly a direction for refinement of the model, itusually requires more training data or a more so-phisticated feature representation.
In this paper,we instead modify the learning approach and pro-pose a concise and highly adaptive extension thatcan be easily combined with other extended fea-tures or applied to different languages.3 Experimental DesignFollowing Barzilay and Lapata (2008), we wishto train a discriminative model to give the cor-rect ranking preference between two documentsin terms of their degree of coherence.
We experi-ment on the same two tasks as in their work: sen-tence ordering and summary coherence rating.3.1 Sentence OrderingIn the standard entity-based model, a discrimina-tive system is trained on the pairwise rankings be-tween source documents and their permutations(see Section 2.2).
However, a model learned fromthese pairwise rankings is not sufficiently fine-grained, since the subtle differences between thepermutations are not learned.
Our major contribu-tion is to further differentiate among the permuta-tions generated from the same source documents,rather than simply treating them all as being of thesame degree of coherence.Our fundamental assumption is that there existsa canonical ordering for the sentences of a doc-ument; therefore we can approximate the degreeof coherence of a document by the similarity be-tween its actual sentence ordering and that canon-ical sentence ordering.
Practically, we automati-cally assign an objective score for each permuta-tion to estimate its dissimilarity from the sourcedocument (see Section 4).
By learning from allthe pairs across a source document and its per-mutations, the effective size of the training datais increased while no further manual annotationis required, which is favorable in real applica-tions when available samples with manually an-notated coherence scores are usually limited.
Forr source documents each with m random permuta-tions, the number of training instances in the stan-dard entity-based model is therefore r ?
m, whilein our multiple-rank model learning process, it isr ?(m+12)?
12 r ?
m2 > r ?
m, when m > 2.3.2 Summary Coherence RatingCompared to the standard entity-based coherencemodel, our major contribution in this task is toshow that by automatically assigning an objectivescore for each machine-generated summary to es-timate its dissimilarity from the human-generatedsummary from the same input document cluster,we are able to achieve performance competitivewith, or even superior to, that of B&L?s modelwithout knowing the true coherence score givenby human judges.Evaluating our multiple-rank model in this taskis crucial, since in summary coherence rating,the coherence violations that the reader might en-counter in real machine-generated texts can bemore precisely approximated, while the sentenceordering task is only partially capable of doing so.3174 Dissimilarity MetricsAs mentioned previously, the subtle differencesamong the permutations of the same source docu-ment can be used to refine the model learning pro-cess.
Considering an original document d and oneof its permutations, we call ?
= (1, 2, .
.
.
,N) thereference ordering, which is the sentence order-ing in d, and pi = (o1, o2, .
.
.
, oN) the test order-ing, which is the sentence ordering in that permu-tation, where N is the number of sentences beingrendered in both documents.In order to approximate different degrees of co-herence among the set of permutations which bearthe same content, we need a suitable metric toquantify the dissimilarity between the test order-ing pi and the reference ordering ?.
Such a metricneeds to satisfy the following criteria: (1) It can beautomatically computed while being highly corre-lated with human judgments of coherence, sinceadditional manual annotation is certainly undesir-able.
(2) It depends on the particular sentenceordering in a permutation while remaining inde-pendent of the entities within the sentences; oth-erwise our multiple-rank model might be trainedto fit particular probability distributions of entitytransitions rather than true coherence preferences.In our work we use three different metrics:Kendall?s ?
distance, average continuity, and editdistance.Kendall?s ?
distance: This metric has beenwidely used in evaluation of sentence ordering(Lapata, 2003; Lapata, 2006; Bollegala et al2006; Madnani et al 2007)1.
It measures thedisagreement between two orderings ?
and pi interms of the number of inversions of adjacent sen-tences necessary to convert one ordering into an-other.
Kendall?s ?
distance is defined as?
=2mN(N ?
1),where m is the number of sentence inversions nec-essary to convert ?
to pi.Average continuity (AC): Following Zhang(2011), we use average continuity as the sec-ond dissimilarity metric.
It was first proposed1Filippova and Strube (2007) found that their perfor-mance dropped when using this metric for longer rankings;but they were using data in a different language and withmanual annotations, so its effect on our datasets is worth try-ing nonetheless.by Bollegala et al(2006).
This metric esti-mates the quality of a particular sentence order-ing by the number of correctly arranged contin-uous sentences, compared to the reference order-ing.
For example, if pi = (.
.
.
, 3, 4, 5, 7, .
.
.
, oN),then {3, 4, 5} is considered as continuous while{3, 4, 5, 7} is not.
Average continuity is calculatedasAC = exp??????
?1n ?
1n?i=2log (Pi + ?)???????
,where n = min(4,N) is the maximum numberof continuous sentences to be considered, and?
= 0.01.
Pi is the proportion of continuous sen-tences of length i in pi that are also continuous inthe reference ordering ?.
To represent the dis-similarity between the two orderings pi and ?, weuse its complement AC?
= 1 ?
AC, such that thelarger AC?
is, the more dissimilar two orderingsare2.Edit distance (ED): Edit distance is a com-monly used metric in information theory to mea-sure the difference between two sequences.
Givena test ordering pi, its edit distance is defined as theminimum number of edits (i.e., insertions, dele-tions, and substitutions) needed to transform itinto the reference ordering ?.
For permutations,the edits are essentially movements, which canbe considered as equal numbers of insertions anddeletions.5 Experiments5.1 Sentence OrderingOur first set of experiments is on sentence order-ing.
Following Barzilay and Lapata (2008), weuse all transitions of length ?
3 for feature extrac-tion.
In addition, we explore three specific aspectsin our experiments: rank assignment, entity ex-traction, and permutation generation.5.1.1 Rank AssignmentIn our multiple-rank model, pairwise rankingsbetween a source document and its permutationsare extended into a longer ranking with multipleranks.
We assign a rank to a particular permuta-tion, based on the result of applying a chosen dis-similarity metric from Section 4 (?, AC, or ED) tothe sentence ordering in that permutation.We experiment with two different approachesto assigning ranks to permutations, while each2We will refer to AC?
as AC from now on.318source document is always assigned a zero (thehighest) rank.In the raw option, we rank the permutations di-rectly by their dissimilarity scores to form a fullranking for the set of permutations generated fromthe same source document.Since a full ranking might be too sensitive tonoise in training, we also experiment with thestratified option, in which C ranks are assigned tothe permutations generated from the same sourcedocument.
The permutation with the smallest dis-similarity score is assigned the same (zero, thehighest) rank as the source document, and the onewith the largest score is assigned the lowest (C?1)rank; then ranks of other permutations are uni-formly distributed in this range according to theirraw dissimilarity scores.
We experiment with 3to 6 ranks (the case where C = 2 reduces to thestandard entity-based model).5.1.2 Entity ExtractionBarzilay and Lapata (2008)?s best results wereachieved by employing an automatic coreferenceresolution tool (Ng and Cardie, 2002) for ex-tracting entities from a source document, and thepermutations were generated only afterwards ?entity extraction from a permuted document de-pends on knowing the correct sentence order andthe oracular entity information from the sourcedocument ?
since resolving coreference relationsin permuted documents is too unreliable for an au-tomatic tool.We implement our multiple-rank model withfull coreference resolution using Ng and Cardie?scoreference resolution system, and entity extrac-tion approach as described above ?
the Coref-erence+ condition.
However, as argued by El-sner and Charniak (2011), to better simulatethe real situations that human readers might en-counter in machine-generated documents, suchoracular information should not be taken into ac-count.
Therefore we also employ two alterna-tive approaches for entity extraction: (1) use thesame automatic coreference resolution tool onpermuted documents ?
we call it the Corefer-ence?
condition; (2) use no coreference reso-lution, i.e., group head noun clusters by simplestring matching ?
B&L?s Coreference?
condi-tion.5.1.3 Permutation GenerationThe quality of the model learned depends onthe set of permutations used in training.
We arenot aware of how B&L?s permutations were gen-erated, but we assume they are generated in a per-fectly random fashion.However, in reality, the probabilities of seeingdocuments with different degrees of coherence arenot equal.
For example, in an essay scoring task,if the target group is (near-) native speakers withsufficient education, we should expect their essaysto be less incoherent ?
most of the essays willbe coherent in most parts, with only a few minorproblems regarding discourse coherence.
In sucha setting, the performance of a model trained frompermutations generated from a uniform distribu-tion may suffer some accuracy loss.Therefore, in addition to the set of permutationsused by Barzilay and Lapata (2008) (PSBL), wecreate another set of permutations for each sourcedocument (PSM) by assigning most of the proba-bility mass to permutations which are mostly sim-ilar to the original source document.
Besides itscapability of better approximating real-life situ-ations, training our model on permutations gen-erated in this way has another benefit: in thestandard entity-based model, all permuted doc-uments are treated as incoherent; thus there aremany more incoherent training instances than co-herent ones (typically the proportion is 20:1).
Incontrast, in our multiple-rank model, permuteddocuments are assigned different ranks to fur-ther differentiate the different degrees of coher-ence within them.
By doing so, our model willbe able to learn the characteristics of a coherentdocument from those near-coherent documents aswell, and therefore the problem of lacking coher-ent instances can be mitigated.Our permutation generation algorithm is shownin Algorithm 1, where ?
= 0.05, ?
= 5.0,MAX NUM = 50, and K and K?
are two normal-ization factors to make p(swap num) and p(i, j)proper probability distributions.
For each sourcedocument, we create the same number of permu-tations as PSBL.5.2 Summary Coherence RatingIn the summary coherence rating task, we aredealing with a mixture of multi-document sum-maries generated by systems and written by hu-mans.
Barzilay and Lapata (2008) did not assume319Algorithm 1 Permutation Generation.Input: S 1, S 2, .
.
.
, S N ; ?
= (1, 2, .
.
.
,N)Choose a number of sentence swapsswap num with probability e??
?swap num/Kfor i = 1?
swap num doSwap a pair of sentence (S i, S j)with probability p(i, j) = e???|i?
j|/K?end forOutput: pi = (o1, o2, .
.
.
, oN)a simple binary distinction among the summariesgenerated from the same input document clus-ter; rather, they had human judges give scores foreach summary based on its degree of coherence(see Section 3.2).
Therefore, it seems that thesubtle differences among incoherent documents(system-generated summaries in this case) havealready been learned by their model.But we wish to see if we can replace hu-man judgments by our computed dissimilarityscores so that the original supervised learning isconverted into unsupervised learning and yet re-tain competitive performance.
However, givena summary, computing its dissimilarity score isa bit involved, due to the fact that we do notknow its correct sentence order.
To tackle thisproblem, we employ a simple sentence align-ment between a system-generated summary anda human-written summary originating from thesame input document cluster.
Given a system-generated summary Ds = (S s1, S s2, .
.
.
, S sn) andits corresponding human-written summary Dh =(S h1, S h2, .
.
.
, S hN) (here it is possible that n ,N), we treat the sentence ordering (1, 2, .
.
.
,N)in Dh as ?
(the original sentence ordering), andcompute pi = (o1, o2, .
.
.
, on) based on Ds.
Tocompute each oi in pi, we find the most similarsentence S h j, j ?
[1,N] in Dh by computing theircosine similarity over all tokens in S h j and S si;if all sentences in Dh have zero cosine similaritywith S si, we assign ?1 to oi.Once pi is known, we can compute its ?dissimi-larity?
from ?
using a chosen metric.
But becausenow pi is not guaranteed to be a permutation of ?
(there may be repetition or missing values, i.e.,?1, in pi), Kendall?s ?
cannot be used, and we useonly average continuity and edit distance as dis-similarity metrics in this experiment.The remaining experimental configuration isthe same as that of Barzilay and Lapata (2008),with the optimal transition length set to ?
2.6 Results6.1 Sentence OrderingIn this task, we use the same two sets of sourcedocuments (Earthquakes and Accidents, see Sec-tion 3.1) as Barzilay and Lapata (2008).
Eachcontains 200 source documents, equally dividedbetween training and test sets, with up to 20 per-mutations per document.
We conduct experi-ments on these two domains separately.
For eachdomain, we accompany each source documentwith two different sets of permutations: the oneused by B&L (PSBL), and the one generated fromour model described in Section 5.1.3 (PSM).
Wetrain our multiple-rank model and B&L?s standardtwo-rank model on each set of permutations usingthe SVMrank package (Joachims, 2006), and eval-uate both systems on their test sets.
Accuracy ismeasured as the fraction of correct pairwise rank-ings for the test set.6.1.1 Full Coreference Resolution withOracular InformationIn this experiment, we implement B&L?s fully-fledged standard entity-based coherence model,and extract entities from permuted documents us-ing oracular information from the source docu-ments (see Section 5.1.2).Results are shown in Table 2.
For each test sit-uation, we list the best accuracy (in Acc columns)for each chosen dissimilarity metric, with the cor-responding rank assignment approach.
C repre-sents the number of ranks used in stratifying rawscores (?N?
if using raw configuration, see Sec-tion 5.1.1 for details).
Baselines are accuraciestrained using the standard entity-based coherencemodel3.Our model outperforms the standard entity-based model on both permutation sets for bothdatasets.
The improvement is not significantwhen trained on the permutation set PSBL, andis achieved only with one of the three metrics;3There are discrepancies between our reported accuraciesand those of Barzilay and Lapata (2008).
The differences aredue to the fact that we use a different parser: the Stanford de-pendency parser (de Marneffe et al 2006), and might haveextracted entities in a slightly different way than theirs, al-though we keep other experimental configurations as closeas possible to theirs.
But when comparing our model withtheirs, we always use the exact same set of features, so theabsolute accuracies do not matter.320Condition: Coreference+Perms MetricEarthquakes AccidentsC Acc C AccPSBL?
3 79.5 3 82.0AC 4 85.2 3 83.3ED 3 86.8 6 82.2Baseline 85.3 83.2PSM?
3 86.8 3 85.2*AC 3 85.6 1 85.4*ED N 87.9* 4 86.3*Baseline 85.3 81.7Table 2: Accuracies (%) of extending the stan-dard entity-based coherence model with multiple-ranklearning in sentence ordering using Coreference+ op-tion.
Accuracies which are significantly better than thebaseline (p < .05) are indicated by *.but when trained on PSM (the set of permutationsgenerated from our biased model), our model?sperformance significantly exceeds B&L?s4 for allthree metrics, especially as their model?s perfor-mance drops for dataset Accidents.From these results, we see that in the ideal sit-uation where we extract entities and resolve theircoreference relations based on the oracular infor-mation from the source document, our model iseffective in terms of improving ranking accura-cies, especially when trained on our more realisticpermutation sets PSM.6.1.2 Full Coreference Resolution withoutOracular InformationIn this experiment, we apply the same auto-matic coreference resolution tool (Ng and Cardie,2002) on not only the source documents but alsotheir permutations.
We want to see how removingthe oracular component in the original model af-fects the performance of our multiple-rank modeland the standard model.
Results are shown in Ta-ble 3.First we can see when trained on PSM, run-ning full coreference resolution significantly hurtsperformance for both models.
This suggests that,in real-life applications, where the distribution oftraining instances with different degrees of co-herence is skewed (as in the set of permutations4Following Elsner and Charniak (2011), we use theWilcoxon Sign-rank test for significance.Condition: Coreference?Perms MetricEarthquakes AccidentsC Acc C AccPSBL?
3 71.0 3 73.3AC 3 *76.8 3 74.5ED 4 *77.4 6 74.4Baseline 71.7 73.8PSM?
3 55.9 3 51.5AC 4 53.9 6 49.0ED 4 53.9 5 52.3Baseline 49.2 53.2Table 3: Accuracies (%) of extending the stan-dard entity-based coherence model with multiple-ranklearning in sentence ordering using Coreference?
op-tion.
Accuracies which are significantly better than thebaseline (p < .05) are indicated by *.generated from our model), running full corefer-ence resolution is not a good option, since it al-most makes the accuracies no better than randomguessing (50%).Moreover, considering training using PSBL,running full coreference resolution has a differentinfluence for the two datasets.
For Earthquakes,our model significantly outperforms B&L?s whilethe improvement is insignificant for Accidents.This is most probably due to the different way thatentities are realized in these two datasets.
As an-alyzed by Barzilay and Lapata (2008), in datasetEarthquakes, entities tend to be referred to by pro-nouns in subsequent mentions, while in datasetAccidents, literal string repetition is more com-mon.Given a balanced permutation distribution aswe assumed in PSBL, switching distant sentencepairs in Accidents may result in very similar en-tity distribution with the situation of switchingcloser sentence pairs, as recognized by the auto-matic tool.
Therefore, compared to Earthquakes,our multiple-rank model may be less powerful inindicating the dissimilarity between the sentenceorderings in a permutation and its source docu-ment, and therefore can improve on the baselineonly by a small margin.6.1.3 No Coreference ResolutionIn this experiment, we do not employ any coref-erence resolution tool, and simply cluster head321Condition: Coreference?Perms MetricEarthquakes AccidentsC Acc C AccPSBL?
4 82.8 N 82.0AC 3 78.0 3 **84.2ED N 78.2 3 *82.7Baseline 83.7 80.1PSM?
3 **86.4 N **85.7AC 4 *84.4 N **86.6ED 5 **86.7 N **84.6Baseline 82.6 77.5Table 4: Accuracies (%) of extending the stan-dard entity-based coherence model with multiple-ranklearning in sentence ordering using Coreference?
op-tion.
Accuracies which are significantly better than thebaseline are indicated by * (p < .05) and ** (p < .01).nouns by string matching.
Results are shown inTable 4.Even with such a coarse approximation ofcoreference resolution, our model is able toachieve around 85% accuracy in most test cases,except for dataset Earthquakes, training on PSBLgives poorer performance than the standard modelby a small margin.
But such inferior perfor-mance should be expected, because as explainedabove, coreference resolution is crucial to thisdataset, since entities tend to be realized throughpronouns; simple string matching introduces toomuch noise into training, especially when ourmodel wants to train a more fine-grained discrim-inative system than B&L?s.
However, we can seefrom the result of training on PSM, if the per-mutations used in training do not involve swap-ping sentences which are too far away, the result-ing noise is reduced, and our model outperformstheirs.
And for dataset Accidents, our modelconsistently outperforms the baseline model by alarge margin (with significance test at p < .01).6.1.4 Conclusions for Sentence OrderingConsidering the particular dissimilarity metricused in training, we find that edit distance usuallystands out from the other two metrics.
Kendall?s ?distance proves to be a fairly weak metric, whichis consistent with the findings of Filippova andStrube (2007) (see Section 2.3).
Figure 1 plotsthe testing accuracies as a function of different68.073.078.083.088.03 4 5 6 NAccuracy (%)CEarthquake ED Coref+Earthquake ED Coref?Accidents ED Coref+Accidents  ED Coref?Accidents ?
Coref-Figure 1: Effect of C on testing accuracies in selectedsentence ordering experimental configurations.choices of C?s with the configurations where ourmodel outperforms the baseline model.
In eachconfiguration, we choose the dissimilarity metricwhich achieves the best accuracy reported in Ta-bles 2 to 4 and the PS BL permutation set.
Wecan see that the dependency of accuracies on theparticular choice of C is not consistent across allexperimental configurations, which suggests thatthis free parameter C needs careful tuning in dif-ferent experimental setups.Combining our multiple-rank model with sim-ple string matching for entity extraction is a ro-bust option for coherence evaluation, regardlessof the particular distribution of permutations usedin training, and it significantly outperforms thebaseline in most conditions.6.2 Summary Coherence RatingAs explained in Section 3.2, we employ a simplesentence alignment between a system-generatedsummary and its corresponding human-writtensummary to construct a test ordering pi and calcu-late its dissimilarity between the reference order-ing ?
from the human-written summary.
In thisway, we convert B&L?s supervised learning modelinto a fully unsupervised model, since human an-notations for coherence scores are not required.We use the same dataset as Barzilay and Lap-ata (2008), which includes multi-document sum-maries from 16 input document clusters generatedby five systems, along with reference summariescomposed by humans.In this experiment, we consider only averagecontinuity (AC) and edit distance (ED) as dissimi-larity metrics, with raw configuration for rank as-signment, and compare our multiple-rank modelwith the standard entity-based model using ei-ther full coreference resolution5 or no resolution5We run the coreference resolution tool on all documents.322Entities Metric Same FullCoreference+AC 82.5 *72.6ED 81.3 **73.0Baseline 78.8 70.9Coreference?AC 76.3 72.0ED 78.8 71.7Baseline 80.0 72.3Table 5: Accuracies (%) of extending the stan-dard entity-based coherence model with multiple-ranklearning in summary rating.
Baselines are results ofstandard entity-based coherence model.
Accuracieswhich are significantly better than the correspondingbaseline are indicated by * (p < .05) and ** (p < .01).for entity extraction.
We train both models onthe ranking preferences (144 in all) among sum-maries originating from the same input documentcluster using the SVMrank package (Joachims,2006), and test on two different test sets: same-cluster test and full test.
Same-cluster test is theone used by Barzilay and Lapata (2008), in whichonly the pairwise rankings (80 in all) betweensummaries originating from the same input doc-ument cluster are tested; we also experiment withfull test, in which pairwise rankings (1520 in all)between all summary pairs excluding two human-written summaries are tested.Results are shown in Table 5.
Coreference+and Coreference?
denote the configuration ofusing full coreference resolution or no resolu-tion separately.
First, clearly for both models,performance on full test is inferior to that onsame-cluster test, but our model is still able toachieve performance competitive with the stan-dard model, even if our fundamental assumptionabout the existence of canonical sentence order-ing in documents with same content may breakdown on those test pairs not originating from thesame input document cluster.
Secondly, for thebaseline model, using the Coreference?
configu-ration yields better accuracy in this task (80.0%vs.
78.8% on same-cluster test, and 72.3% vs.70.9% on full test), which is consistent with thefindings of Barzilay and Lapata (2008).
But ourmultiple-rank model seems to favor the Corefer-ence+ configuration, and our best accuracy evenexceeds B&L?s best when tested on the same set:82.5% vs. 80.0% on same-cluster test, and 73.0%vs.
72.3% on full test.When our model performs poorer than thebaseline (using Coreference?
configuration), thedifference is not significant, which suggests thatour multiple-rank model with unsupervised scoreassignment via simple cosine matching can re-main competitive with the standard model, whichrequires human annotations to obtain a more fine-grained coherence spectrum.
This observation isconsistent with Banko and Vanderwende (2004)?sdiscovery that human-generated summaries lookquite extractive.7 ConclusionsIn this paper, we have extended the popular co-herence model of Barzilay and Lapata (2008) byadopting a multiple-rank learning approach.
Thisis inherently different from other extensions tothis model, in which the focus is on enrichingthe set of features for entity-grid construction,whereas we simply keep their original feature setintact, and manipulate only their learning method-ology.
We show that this concise extension iseffective and able to outperform B&L?s standardmodel in various experimental setups, especiallywhen experimental configurations are most suit-able considering certain dataset properties (seediscussion in Section 6.1.4).We experimented with two tasks: sentence or-dering and summary coherence rating, followingB&L?s original framework.
In sentence ordering,we also explored the influence of removing theoracular component in their original model anddealing with permutations generated from differ-ent distributions, showing that our model is robustfor different experimental situations.
In summarycoherence rating, we further extended their modelsuch that their original supervised learning is con-verted into unsupervised learning with competi-tive or even superior performance.Our multiple-rank learning model can be easilyadapted into other extended entity-based coher-ence models with their enriched feature sets, andfurther improvement in ranking accuracies shouldbe expected.AcknowledgmentsThis work was financially supported by the Nat-ural Sciences and Engineering Research Councilof Canada and by the University of Toronto.323ReferencesMichele Banko and Lucy Vanderwende.
2004.
Us-ing n-grams to understand the nature of summaries.In Proceedings of Human Language Technologiesand North American Association for ComputationalLinguistics 2004: Short Papers, pages 1?4.Regina Barzilay and Mirella Lapata.
2005.
Modelinglocal coherence: An entity-based approach.
In Pro-ceedings of the 42rd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2005),pages 141?148.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: an entity-based approach.
Compu-tational Linguistics, 34(1):1?34.Danushka Bollegala, Naoaki Okazaki, and MitsuruIshizuka.
2006.
A bottom-up approach to sen-tence ordering for multi-document summarization.In Proceedings of the 21st International Confer-ence on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Lin-guistics, pages 385?392.Jackie Chi Kit Cheung and Gerald Penn.
2010.
Entity-based local coherence modelling using topologicalfields.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics (ACL 2010), pages 186?195.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation (LREC 2006).Micha Elsner and Eugene Charniak.
2011.
Extendingthe entity grid with entity-specific features.
In Pro-ceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2011),pages 125?129.Katja Filippova and Michael Strube.
2007.
Extend-ing the entity-grid coherence model to semanticallyrelated entities.
In Proceedings of the Eleventh Eu-ropean Workshop on Natural Language Generation(ENLG 2007), pages 139?142.Thorsten Joachims.
2002.
Optimizing search en-gines using clickthrough data.
In Proceedings ofthe 8th ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining (KDD2002), pages 133?142.Thorsten Joachims.
2006.
Training linear SVMsin linear time.
In Proceedings of the 12th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining (KDD 2006), pages217?226.Mirella Lapata.
2003.
Probabilistic text structuring:Experiments with sentence ordering.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics (ACL 2003), pages545?552.Mirella Lapata.
2006.
Automatic evaluation of in-formation ordering: Kendall?s tau.
ComputationalLinguistics, 32(4):471?484.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.Automatically evaluating text coherence using dis-course relations.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics (ACL 2011), pages 997?1006.Nitin Madnani, Rebecca Passonneau, Necip FazilAyan, John M. Conroy, Bonnie J. Dorr, Ju-dith L. Klavans, Dianne P. O?Leary, and Judith D.Schlesinger.
2007.
Measuring variability in sen-tence ordering for news summarization.
In Pro-ceedings of the Eleventh European Workshop onNatural Language Generation (ENLG 2007), pages81?88.Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the 40th Annual Meeting on Asso-ciation for Computational Linguistics (ACL 2002),pages 104?111.Michael Strube and Simone Paolo Ponzetto.
2006.Wikirelate!
Computing semantic relatedness usingWikipedia.
In Proceedings of the 21st NationalConference on Artificial Intelligence, pages 1219?1224.Renxian Zhang.
2011.
Sentence ordering driven bylocal and global coherence for summary generation.In Proceedings of the ACL 2011 Student Session,pages 6?11.324
