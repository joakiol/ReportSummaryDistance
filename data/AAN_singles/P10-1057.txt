Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 555?564,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsIdentifying Non-explicit Citing Sentences for Citation-basedSummarizationVahed QazvinianDepartment of EECSUniversity of MichiganAnn Arbor, MIvahed@umich.eduDragomir R. RadevDepartment of EECS andSchool of InformationUniversity of MichiganAnn Arbor, MIradev@umich.eduAbstractIdentifying background (context) informa-tion in scientific articles can help schol-ars understand major contributions in theirresearch area more easily.
In this paper,we propose a general framework basedon probabilistic inference to extract suchcontext information from scientific papers.We model the sentences in an article andtheir lexical similarities as a Markov Ran-dom Field tuned to detect the patterns thatcontext data create, and employ a BeliefPropagation mechanism to detect likelycontext sentences.
We also address theproblem of generating surveys of scien-tific papers.
Our experiments show greaterpyramid scores for surveys generated us-ing such context information rather thancitation sentences alone.1 IntroductionIn scientific literature, scholars use citations to re-fer to external sources.
These secondary sourcesare essential in comprehending the new research.Previous work has shown the importance of cita-tions in scientific domains and indicated that ci-tations include survey-worthy information (Sid-dharthan and Teufel, 2007; Elkiss et al, 2008;Qazvinian and Radev, 2008; Mohammad et al,2009; Mei and Zhai, 2008).A citation to a paper in a scientific article maycontain explicit information about the cited re-search.
The following example is an excerpt froma CoNLL paper1 that contains information aboutEisner?s work on bottom-up parsers and the notionof span in parsing:?Another use of bottom-up is due to Eisner(1996), who introduced the notion of a span.
?1Buchholz and Marsi ?CoNLL-X Shared Task On Multi-lingual Dependency Parsing?, CoNLL 2006However, the citation to a paper may not alwaysinclude explicit information about the cited paper:?This approach is one of those described in Eis-ner (1996)?Although this sentence alone does not provide anyinformation about the cited paper, it suggests thatits surrounding sentences describe the proposedapproach in Eisner?s paper:?...
In an all pairs approach, every possiblepair of two tokens in a sentence is consideredand some score is assigned to the possibility ofthis pair having a (directed) dependency rela-tion.
Using that information as building blocks,the parser then searches for the best parse forthe sentence.
This approach is one of those de-scribed in Eisner (1996).
?We refer to such implicit citations that containinformation about a specific secondary source butdo not explicitly cite it, as sentences with con-text information or context sentences for short.We look at the patterns that such sentences cre-ate and observe that context sentences occur with-ing a small neighborhood of explicit citations.
Wealso discuss the problem of extracting context sen-tences for a source-reference article pair.
We pro-pose a general framework that looks at each sen-tence as a random variable whose value deter-mines its state about the target paper.
In summary,our proposed model is based on the probabilisticinference of these random variables using graphi-cal models.
Finally we give evidence on how suchsentences can help us produce better surveys of re-search areas.
The rest of this paper is organized asfollows.
Preceded by a review of prior work inSection 2, we explain the data collection and ourannotation process in Section 3.
Section 4 explainsour methodology and is followed by experimentalsetup in Section 5.555#RefsACL-ID Author Title Year all AAN # SentsP08-2026 McClosky & Charniak Self-Training for Biomedical Parsing 2008 12 8 102N07-1025?
Mihalcea Using Wikipedia for Automatic ... 2007 21 12 153N07-3002 Wang Learning Structured Classifiers ... 2007 22 14 74P06-1101 Snow et, al.
Semantic Taxonomy Induction ... 2006 19 9 138P06-1116 Abdalla & Teufel A Bootstrapping Approach To ... 2006 24 10 231W06-2933 Nivre et, al.
Labeled Pseudo-Projective Dependency ... 2006 27 5 84P05-1044 Smith & Eisner Contrastive Estimation: Training Log-Linear ... 2005 30 13 262P05-1073 Toutanova et, al.
Joint Learning Improves Semantic Role Labeling 2005 14 10 185N03-1003 Barzilay & Lee Learning To Paraphrase: An Unsupervised ... 2003 26 13 203N03-2016?
Kondrak et, al.
Cognates Can Improve Statistical Translation ... 2003 8 5 92Table 1: Papers chosen from AAN as source papers for the evaluation corpus, together with their publi-cation year, number of references (in AAN) and number of sentences.
Papers marked with ?
are used tocalculate inter-judge agreement.2 Prior WorkAnalyzing the structure of scientific articles andtheir relations has received a lot of attention re-cently.
The structure of citation and collaborationnetworks has been studied in (Teufel et al, 2006;Newman, 2001), and summarization of scientificdocuments is discussed in (Teufel and Moens,2002).
In addition, there is some previous workon the importance of citation sentences.
Elkiss etal, (Elkiss et al, 2008) perform a large-scale studyon citations in the free PubMed Central (PMC)and show that they contain information that maynot be present in abstracts.
In other work, Nanbaet al (Nanba and Okumura, 1999; Nanba et al,2004b; Nanba et al, 2004a) analyze citation sen-tences and automatically categorize them in orderto build a tool for survey generation.The text of scientific citations has been used inprevious research.
Bradshaw (Bradshaw, 2002;Bradshaw, 2003) uses citations to determine thecontent of articles.
Similarly, the text of cita-tion sentences has been directly used to producesummaries of scientific papers in (Qazvinian andRadev, 2008; Mei and Zhai, 2008; Mohammadet al, 2009).
Determining the scientific attribu-tion of an article has also been studied before.Siddharthan and Teufel (Siddharthan and Teufel,2007; Teufel, 2005) categorize sentences accord-ing to their role in the author?s argument into pre-defined classes: Own, Other, Background, Tex-tual, Aim, Basis, Contrast.Little work has been done on automatic cita-tion extraction from research papers.
Kaplan etal, (Kaplan et al, 2009) introduces ?citation-site?as a block of text in which the cited text is dis-cussed.
The mentioned work uses a machinelearning method for extracting citations from re-search papers and evaluates the result using 4 an-notated articles.In our work we use graphical models to ex-tract context sentences.
Graphical models havea number of properties and corresponding tech-niques and have been used before on InformationRetrieval tasks.
Romanello et al (Romanello etal., 2009) use Conditional Random Fields (CRF)to extract references from unstructured text in dig-ital libraries of classic texts.
Similar work includeterm dependency extraction (Metzler and Croft,2005), query expansion (Metzler and Croft, 2007),and automatic feature selection (Metzler, 2007).3 DataThe ACL Anthology Network (AAN)2 is a col-lection of papers from the ACL Anthology3 pub-lished in the Computational Linguistics journaland proceedings from ACL conferences and work-shops and includes more than 14, 000 papers overa period of four decades (Radev et al, 2009).AAN includes the citation network of the papersin the ACL Anthology.
The papers in AAN arepublicly available in text format retrieved by anOCR process from the original pdf files, and aresegmented into sentences.To build a corpus for our experiments we picked10 recently published papers from various areasin NLP4, each of which had references for a to-tal of 203 candidate paper-reference pairs.
Table 1lists these papers together with their authors, titles,publication year, number of references, number ofreferences within AAN, and the number of sen-2http://clair.si.umich.edu/clair/anthology/3http://www.aclweb.org/anthology-new/4Regardless of data selection, the methodology in thiswork is applicable to any of the papers in AAN.556L&PS&al Sentence?
?
?C C Jacquemin (1999) and Barzilay and McKeown (2001) identifyphrase level paraphrases, while Lin and Pantel (2001) andShinyama et al (2002) acquire structural paraphrases encodedas templates.1 1 These latter are the most closely related to the sentence-level para-phrases we desire, and so we focus in this section on template-induction approaches.C 0 Lin and Pantel (2001) extract inference rules, which are relatedto paraphrases (for example, X wrote Y implies X is the author ofY), to improve question answering.1 0 They assume that paths in dependency trees that take similar argu-ments (leaves) are close in meaning.1 0 However, only two-argument templates are considered.0 C Shinyama et al (2002) also use dependency-tree information toextract templates of a limited form (in their case, determined bythe underlying information extraction application).1 1 Like us (and unlike Lin and Pantel, who employ a single largecorpus), they use articles written about the same event in differentnewspapers as data.1 1 Our approach shares two characteristics with the two methods justdescribed: pattern comparison by analysis of the patterns respec-tive arguments, and use of nonparallel corpora as a data source.0 0 However, extraction methods are not easily extended to generationmethods.1 1 One problem is that their templates often only match small frag-ments of a sentence.1 1 While this is appropriate for other applications, deciding whetherto use a given template to generate a paraphrase requires informa-tion about the surrounding context provided by the entire sentence.?
?
?Table 2: Part of the annotation for N03-1003 withrespect to two of its references ?Lin and Pan-tel (2001)?
(the first column) ?Shinyama et al(2002)?
(the second column).
Cs indicate explicitcitations, 1s indicate implicit citations and 0s arenone.tences.3.1 Annotation ProcessWe annotated the sentences in each paper from Ta-ble 1.
Each annotation instance in our setting cor-responds to a paper-reference pair, and is a vec-tor in which each dimension corresponds to a sen-tence and is marked with a C if it explicitly citesthe reference, and with a 1 if it implicitly talksabout it.
All other sentences are marked with 0s.Table 2 shows a portion of two separate annota-tion instances of N03-1003 corresponding to twoof its references.
Our annotation has resulted in203 annotation instances each corresponding toone paper-reference pair.
The goal of this workis to automatically identify all context sentences,which are marked as ?1?.3.1.1 Inter-judge AgreementWe also asked a neutral annotator5 to annotatetwo of our datasets that are marked with ?
in Ta-ble 1.
For each paper-reference pair, the annotatorwas provided with a vector in which explicit cita-5Someone not involved with the paper but an expert inNLP.ACL-ID vector size # Annotations ?N07-1025?
153 21 0.889 ?
0.30N03-2016?
92 8 0.853 ?
0.35Table 3: Average ?
coefficient as inter-judgeagreement for annotations of two setstions were already marked with Cs.
The annota-tion guidelines instructed the annotator to look ateach explicit citation sentence, and read up to 15sentences before and after, then mark context sen-tences around that sentence with 1s.
Next, the 29annotation instances done by the external annota-tor were compared with the corresponding anno-tations that we did, and the Kappa coefficient (?
)was calculated.
The ?
statistic is formulated as?
=Pr(a)?
Pr(e)1?
Pr(e)where Pr(a) is the relative observed agreementamong raters, and Pr(e) is the probability that an-notators agree by chance if each annotator is ran-domly assigning categories.
To calculate ?, we ig-nored all explicit citations (since they were pro-vided to the external annotator) and used the bi-nary categories (i.e., 1 for context sentences, and0 otherwise) for all other sentences.
Table 3 showsthe annotation vector size (i.e., number of sen-tences), number of annotation instances (i.e., num-ber of references), and average ?
for each set.
Theaverage ?
is above 0.85 in both cases, suggest-ing that the annotation process has a low degreeof subjectivity and can be considered reliable.3.2 AnalysisIn this section we describe our analysis.
First,we look at the number of explicit citations eachreference has received in a paper.
Figure 1 (a)shows the histogram corresponding to this distri-bution.
It indicates that the majority of referencesget cited in only 1 sentence in a scientific arti-cle, while the maximum being 9 in our collecteddataset with only 1 instance (i.e., there is only 1reference that gets cited 9 times in a paper).
More-over, the data exhibits a highly positive-skeweddistribution.
This is illustrated on a log-log scalein Figure 1 (b).
This highly skewed distributionindicates that the majority of references get citedonly once in a citing paper.
The very small numberof citing sentences can not make a full inventory ofthe contributions of the cited paper, and therefore,extracting explicit citations alone without context557gap size 0 1 2 4 9 10 15 16instance 273 14 2 1 2 1 1 1Table 4: The distribution of gaps in the annotateddatasentences may result in information loss about thecontributions of the cited paper.1 2 3 4 5 6 7 8 9020406080100120140cit100 10110?310?210?1100citp(cit)alpha = 3.13; D=0.02a bFigure 1: (a) Histogram of the number of differ-ent citations to each reference in a paper.
(b) Thedistribution observed for the number of differentcitations on a log-log scale.Next, we investigate the distance between con-text sentences and the closest citations.
For eachcontext sentence, we find its distance to the clos-ets context sentence or explicit citation.
Formally,we define the gap to be the number of sentencesbetween a context sentence (marked with 1) andthe closest context sentence or explicit citation(marked with either C or 1) to it.
For example,the second column of Table 2 shows that there is agap of size 1 in the 9th sentence in the set of con-text and citation sentences about Shinyama et al(2002).
Table 4 shows the distribution of gap sizesin the annotated data.
This observation suggeststhat the majority of context sentences directly oc-cur after or before a citation or another contextsentence.
However, it shows that gaps betweensentences describing a cited paper actually exist,and a proposed method should have the capabilityto capture them.4 Proposed MethodIn this section we propose our methodology thatenables us to identify the context information of acited paper.
Particularly, the task is to assign a bi-nary label XC to each sentence Si from a paper S,where XC = 1 shows a context sentence relatedto a given cited paper, C. To solve this problemwe propose a systematic way to model the net-work level relationship between consecutive sen-tences.
In summary, each sentence is representedwith a node and is given two scores (context, non-context), and we update these scores to be in har-mony with the neighbors?
scores.A particular class of graphical models knownas Markov Random Fields (MRFs) are suited forsolving inference problems with uncertainty in ob-served data.
The data is modeled as an undirectedgraph with two types of nodes: hidden and ob-served.
Observed nodes represent values that areknown from the data.
Each hidden node xu, cor-responding to an observed node yu, represents thetrue state underlying the observed value.
The stateof a hidden node is related to the value of its cor-responding observed node as well as the states ofits neighboring hidden nodes.The local Markov property of an MRF indi-cates that a variable is conditionally independenton all other variables given its neighbors: xv ??
xV \cl(v)|xne(v), where ne(v) is the set of neigh-bors of v, and cl(v) = {v} ?
ne(v) is the closedneighborhood of v. Thus, the state of a node is as-sumed to statistically depend only upon its hiddennode and each of its neighbors, and independentof any other node in the graph given its neighbors.Dependencies in an MRF are represented usingtwo functions: Compatibility function (?)
and Po-tential function (?).
?uv(xc, xd) shows the edgepotential of an edge between two nodes u, v ofclasses xc and xd.
Large values of ?uv wouldindicate a strong association between xc and xdat nodes u, v. The Potential function, ?i(xc, yc),shows the statistical dependency between xc andyc at each node i assumed by the MRF model.In order to find the marginal probabilities ofxis in a MRF we can use Belief Propagation(BP) (Yedidia et al, 2003).
If we assume the yisare fixed and show ?i(xi, yi) by ?i(xi), we canfind the joint probability distribution for unknownvariables xi asp({x}) = 1Z?ij?ij(xi, xj)?i?i(xi)In the BP algorithm a set of new variables m isintroduced where mij(xj) is the message passedfrom i to j about what state xj should be in.
Eachmessage, mij(xj), is a vector with the same di-mensionality of xj in which each dimension showsi?s opinion about j being in the correspondingclass.
Therefore each message could be consid-ered as a probability distribution and its compo-nents should sum up to 1.
The final belief at a558Figure 2: The illustration of the message updatingrule.
Elements that make up the message from anode i to another node j: messages from i?s neigh-bors, local evidence at i, and propagation functionbetween i, j summed over all possible states ofnode i.node i, in the BP algorithm, is also a vector withthe same dimensionality of messages, and is pro-portional to the local evidence as well as all mes-sages from the node?s neighbors:bi(xi)?
k?i(xi)?j?ne(i)mji(xi) (1)where k is the normalization factor of the be-liefs about different classes.
The message passedfrom i to j is proportional to the propagation func-tion between i, j, the local evidence at i, and allmessages sent to i from its neighbors except j:mij(xj)?
?xi?i(xi)?ij(xi, xj)?k?ne(i)\jmki(xi) (2)Figure 2 illustrates the message update rule.Convergence can be determined based on a va-riety of criteria.
It can occur when the maximumchange of any message between iteration steps isless than some threshold.
Convergence is guaran-teed for trees but not for general graphs.
However,it typically occurs in practice (McGlohon et al,2009).
Upon convergence, belief scores are deter-mined by Equation 1.4.1 MRF constructionTo find the sentences from a paper that form thecontext information of a given cited paper, webuild an MRF in which a hidden node xi andan observed node yi correspond to each sentence.The structure of the graph associated with theMRF is dependent upon the validity of a basic as-sumption.
This assumption indicates that the gen-eration of a sentence (in form of its words) only(a) (b)Figure 3: The structure of the MRF constructedbased on the independence of non-adjacent sen-tences; (a) left, each sentence is independent onall other sentences given its immediate neighbors.
(b) right, sentences have dependency relationshipwith each other regardless of their position.depends on its surrounding sentences.
Said dif-ferently, each sentence is written independently ofall other sentences given a number of its neigh-bors.
This local dependence assumption can resultin a number of different MRFs, each built assum-ing a dependency between a sentence and all sen-tences within a particular distance.
Figure 3 showsthe structure of the two MRFs at either extreme ofthe local dependence assumption.
In Figure 3 a,each sentence only depends on one following andone preceding sentence, while Figure 3 b showsan MRF in which sentences are dependent on eachother regardless of their position.
We refer to theformer by BP1, and to the latter by BPn.
Gen-erally, we use BPi to denote an MRF in whicheach sentence is connected to i sentences beforeand after.
?ij(xc, xd) xd = 0 xd = 1xc = 0 0.5 0.5xc = 1 1?
Sij SijTable 5: The compatibility function ?
betweenany two nodes in the MRFs from the sentences inscientific papers4.2 Compatibility FunctionThe compatibility function of an MRF representsthe association between the hidden node classes.A node?s belief to be in class 1 is its probability tobe included in the context.
The belief of a node i,about its neighbor j to be in either classes is as-sumed to be 0.5 if i is in class 0.
In other words, ifa node is not part of the context itself, we assume559it has no effect on its neighbors?
classes.
In con-trast, if i is in class 1 its belief about its neighborj is determined by their mutual lexical similarity.If this similarity is close to 1 it indicates a strongertie between i, j.
However, if i, j are not similar,i?s probability of being in class 1, should not af-fect that of j?s.
To formalize this assumption weuse the sigmoid of the cosine similarity of two sen-tences to build ?.
More formally, we define S tobeSij =11 + e?cosine(i,j)The sigmoid function obtains a value of 0.5 fora cosine of 0 indicating that there is no bias in theassociation of the two sentences.
The matrix in Ta-ble 5 shows the compatibility function built basedon the above arguments.4.3 Potential FunctionThe node potential function of an MRF can incor-porate some other features observable from data.Here, the goal is to find all sentences that are abouta specific cited paper, without having explicit cita-tions.
To build the node potential function of theobserved nodes, we use some sentence level fea-tures.
First, we use the explicit citation as an im-portant feature of a sentence.
This feature can af-fect the belief of the corresponding hidden node,which can in turn affect its neighbors?
beliefs.
Fora given paper-reference pair, we flag (with a 1)each sentence that has an explicit citation to thereference.The second set of features that we are inter-ested in are discourse-based features.
In particu-lar we match each sentence with specific patternsand flag those that match.
The first pattern is a bi-gram in which the first term matches any of ?this;that; those; these; his; her; their; such; previ-ous?, and the second term matches any of ?work;approach; system; method; technique; result; ex-ample?.
The second pattern includes all sentencesthat start with ?this; such?.Finally, the similarity of each sentence to thereference is observable from the data and can beused as a sentence-level feature.
Intuitively, if asentence has higher similarity with the referencepaper, it should have a higher potential of beingin class 1 or C. The flag of each sentence here isa value between 0 and 1 and is determined by itscosine similarity to the reference.
Once the flagsfor each sentence, Si are determined, we calculatenormalized fi as the unweighted linear combina-tion of individual features.
Based on fis, we com-pute the potential function, ?, as shown in Table 6.?i(xc, yc) xc = 0 xc = 11?
fi fiTable 6: The node potential function ?
for eachnode in the MRFs from the sentences in scientificpapers is built using the sentences?
flags computedusing sentence level features.5 ExperimentsThe intrinsic evaluation of our methodologymeans to directly compare the output of ourmethod with the gold standards obtained from theannotated data.
Our methodology finds the sen-tences that cite a reference implicitly.
Thereforethe output of the inference method is a vector, ?,of 1?s and 0?s, whereby a 1 at element i meansthat sentence i in the source document is a con-text sentence about the reference while a 0 meansan explicit citation or neither.
The gold standardfor each paper-reference pair, ?
(obtained from theannotated vectors in Section 3.1 by changing allCs to 0s), is also a vector of the same format anddimensionality.Precision, recall, and F?
for this task can be de-fined asp = ?
?
??
?
1 ; r =?
?
??
?
1 ; F?
=(1 + ?2)p ?
r?2p + r (3)where 1 is a vector of 1?s with the same dimen-sionality and ?
is a non-negative real number.5.1 Baseline MethodsThe first baseline that we use is an IR-basedmethod.
This baseline, B1, takes explicit citationsas an input but use them to find context sentences.Given a paper-reference pair, for each explicit ci-tation sentence, marked with C, B1 picks its pre-ceding and following sentences if their similaritiesto that sentence is greater than a cutoff (the medianof all such similarities), and repeats this for neigh-boring sentences of newly marked sentences.
In-tuitively, B1 tries to find the best chain (window)around citing sentences.As the second baseline, we use the hand-crafteddiscourse based features used in MRF?s potentialfunction.
Particularly, this baseline, B2, marks560paper B1 B2 SVM BP1 BP4 BPnP08-2026 0.441 0.237 0.249 0.470 0.613 0.285N07-1025 0.388 0.102 0.124 0.313 0.466 0.138N07-3002 0.521 0.339 0.232 0.742 0.627 0.315P06-1101 0.125 0.388 0.127 0.649 0.889 0.193P06-1116 0.283 0.104 0.100 0.307 0.341 0.130W06-2933 0.313 0.100 0.176 0.338 0.413 0.160P05-1044 0.225 0.100 0.060 0.172 0.586 0.094P05-1073 0.144 0.100 0.144 0.433 0.518 0.171N03-1003 0.245 0.249 0.126 0.523 0.466 0.125N03-2016 0.100 0.181 0.224 0.439 0.482 0.185Table 7: Average F?=3 for similarity based baseline (B1), discourse-based baseline (B2), a supervisedmethod (SVM) and three MRF-based methods.each sentence that is within a particular distance(4 in our experiments) of an explicit citation andmatches one of the two patterns mentioned in Sec-tion 4.3.
After marking all such sentences, B2also marks all sentences between them and theclosest explicit citation, which is no farther than4 sentences away.
This baseline helps us under-stand how effectively this sentence level featurecan work in the absence of other features and thenetwork structure.Finally, we use a supervised method, SVM,to classify sentences as context/non-context.
Weuse 4 features to train the SVM model.
These4 features comprise the 3 sentence level featuresused in MRF?s potential function (i.e., similar-ity to reference, explicit citation, matching certainregular-expressions) and a network level feature:distance to the closes explicit citation.
For eachsource paper, P , we use all other source papersand their source-reference annotation instances totrain a model.
We then use this model to clas-sify all instances in P .
Although the number ofreferences and thus source-reference pairs are dif-ferent for different papers, this can be consideredsimilar to a 10-fold cross validation scheme, sincefor each source paper the model is built using allsource-reference pairs of all other 9 papers.We compare these baselines with 3 MRF-basedsystems each with a different assumption about in-dependence of sentences.
BP1 denotes an MRFin which each sentence is only connected to 1 sen-tence before and after.
In BP4 locality is morerelaxed and each sentence is connected to 4 sen-tences on each sides.
BPn denotes an MRF inwhich all sentences are connected to each otherregardless of their position in the paper.Table 7 shows F?=3 for our experiments andshows how BP4 outperforms the other methodson average.
The value 4 may suggest the fact thatalthough sentences might be independent of dis-tant sentences, they depend on more than one sen-tence on each side.The final experiment we do to intrinsically eval-uate the MRF-base method is to compare differ-ent sentence-level features.
The first feature usedto build the potential function is explicit citations.This feature does not directly affect context sen-tences (i.e., it affects the marginal probability ofcontext sentences through the MRF network con-nections).
Therefore, we do not alter this fea-ture in comparing different features.
However, welook at the effect of the second and the third fea-tures: hand-crafted regular expression-based fea-tures and similarity to the reference.
For each pa-per, we use BP4 to perform 3 experiments: two inabsence of each feature and one including all fea-tures.
Figure 4 shows the average F?=3 for eachexperiment.
This plot shows that the features leadto better results when used together.6 Impact on Survey GenerationWe also performed an extrinsic evaluation ofour context extraction methodology.
Here weshow how context sentences add important survey-worthy information to explicit citations.
Previouswork that generate surveys of scientific topics usethe text of citation sentences alone (Mohammadet al, 2009; Qazvinian and Radev, 2008).
Here,we show how the surveys generated using citationsand their context sentences are better than thosegenerated using citation sentences alone.We use the data from (Mohammad et al, 2009)561...
Naturally, our current work on question answering for the reading comprehension task is most related to those of(Hirschman et al , 1999; Charniak et al , 2000; Riloffand Thelen, 2000 ; Wang et al , 2000).
In fact, all of thisbody of work as well as ours are evaluated on the same set of test stories, and are developed (or trained) on thesame development set of stories.
The work of (Hirschman et al , 1999) initiated this series of work, and it reportedan accuracy of 36.3% on answering the questions in the test stories.
Subsequently, the work of (Riloffand Thelen ,2000) and (Chaxniak et al , 2000) improved the accuracy further to 39.7% and 41%, respectively.
However, allof these three systems used handcrafted, deterministic rules and algorithms......The cross-model comparison showed that the performance ranking of these models was: U-SVM > PatternM> S-SVM > Retrieval-M.
Compared with retrieval-based [Yang et al 2003], pattern-based [Ravichandran et al 2002and Soubbotin et al 2002], and deep NLP-based [Moldovan et al 2002, Hovy et al 2001; and Pasca et al 2001]answer selection, machine learning techniques are more effective in constructing QA components from scratch.
Thesetechniques suffer, however, from the problem of requiring an adequate number of handtagged question-answertraining pairs.
It is too expensive and labor intensive to collect such training pairs for supervised machinelearning techniques ...... As expected, the definition and person-bio answer types are covered well by these resources.
The web hasbeen employed for pattern acquisition (Ravichandran et al , 2003), document retrieval (Dumais et al , 2002), queryexpansion (Yang et al , 2003), structured information extraction, and answer validation (Magnini et al , 2002).
Someof these approaches enhance existing QA systems, while others simplify the question answering task, allowing aless complex approach to find correct answers ...Table 8: A portion of the QA survey generated by LexRank using the context information.Figure 4: Average F?=3 for BP4 employing dif-ferent features.that contains two sets of cited papers and corre-sponding citing sentences, one on Question An-swering (QA) with 10 papers and the other on De-pendency Parsing (DP) with 16 papers.
The QAset contains two different sets of nuggets extractedby experts respectively from paper abstracts andcitation sentences.
The DP set includes nuggetsextracted only from citation sentences.
We usethese nugget sets, which are provided in form ofregular expressions, to evaluate automatically gen-erated summaries.
To perform this experiment weneeded to build a new corpus that includes con-text sentences.
For each citation sentence, BP4 isused on the citing paper to extract the proper con-text.
Here, we limit the context size to be 4 oneach side.
That is, we attach to a citing sentenceany of its 4 preceding and following sentences ifcitation survey context surveyQACT nuggets 0.416 0.634AB nuggets 0.397 0.594DPCT nuggets 0.324 0.379Table 9: Pyramid F?=3 scores of automaticsurveys of QA and DP data.
The QA surveysare evaluated using nuggets drawn from citationtexts (CT), or abstracts (AB), and DP surveys areevaluated using nuggets from citation texts (CT).BP4 marks them as context sentences.
Therefore,we build a new corpus in which each explicit ci-tation sentence is replaced with the same sentenceattached to at most 4 sentence on each side.After building the context corpus, we useLexRank (Erkan and Radev, 2004) to generate 2QA and 2 DP surveys using the citation sentencesonly, and the new context corpus explained above.LexRank is a multidocument summarization sys-tem, which first builds a cosine similarity graph ofall the candidate sentences.
Once the network isbuilt, the system finds the most central sentencesby performing a random walk on the graph.
Welimit these surveys to be of a maximum length of1000 words.
Table 8 shows a portion of the sur-vey generated from the QA context corpus.
Thisexample shows how context sentences add mean-ingful and survey-worthy information along withcitation sentences.
Table 9 shows the PyramidF?=3 score of automatic surveys of QA and DP562data.
The QA surveys are evaluated using nuggetsdrawn from citation texts (CT), or abstracts (AB),and DP surveys are evaluated using nuggets fromcitation texts (CT).
In all evaluation instances thesurveys generated with the context corpora excelat covering nuggets drawn from abstracts or cita-tion sentences.7 ConclusionIn this paper we proposed a framework based onprobabilistic inference to extract sentences thatappear in the scientific literature, and which areabout a secondary source, but which do not con-tain explicit citations to that secondary source.Our methodology is based on inference in an MRFbuilt using the similarity of sentences and theirlexical features.
We show, by numerical exper-iments, that an MRF in which each sentence isconnected to only a few adjacent sentences prop-erly fits this problem.
We also investigate the use-fulness of such sentences in generating surveys ofscientific literature.
Our experiments on generat-ing surveys for Question Answering and Depen-dency Parsing show how surveys generated usingsuch context information along with citation sen-tences have higher quality than those built usingcitations alone.Generating fluent scientific surveys is difficultin absence of sufficient background information.Our future goal is to combine summarizationand bibliometric techniques towards building au-tomatic surveys that employ context informationas an important part of the generated surveys.8 AcknowledgmentsThe authors would like to thank Arzucan ?Ozgu?rfrom University of Michigan for annotations.This paper is based upon work supported by theNational Science Foundation grant ?iOPENER: AFlexible Framework to Support Rapid Learning inUnfamiliar Research Domains?, jointly awardedto University of Michigan and University of Mary-land as IIS 0705832.
Any opinions, findings, andconclusions or recommendations expressed in thispaper are those of the authors and do not necessar-ily reflect the views of the National Science Foun-dation.ReferencesShannon Bradshaw.
2002.
Reference Directed Index-ing: Indexing Scientific Literature in the Context ofIts Use.
Ph.D. thesis, Northwestern University.Shannon Bradshaw.
2003.
Reference directed index-ing: Redeeming relevance for subject search in ci-tation indexes.
In Proceedings of the 7th EuropeanConference on Research and Advanced Technologyfor Digital Libraries.Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes?Erkan, David States, and Dragomir R. Radev.
2008.Blind men and elephants: What do citation sum-maries tell us about a research article?
Journal ofthe American Society for Information Science andTechnology, 59(1):51?62.Gu?nes?
Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based centrality as salience in text summa-rization.
Journal of Artificial Intelligence Research(JAIR).Dain Kaplan, Ryu Iida, and Takenobu Tokunaga.
2009.Automatic extraction of citation contexts for re-search paper summarization: A coreference-chainbased approach.
In Proceedings of the 2009 Work-shop on Text and Citation Analysis for ScholarlyDigital Libraries, pages 88?95, Suntec City, Sin-gapore, August.
Association for Computational Lin-guistics.Mary McGlohon, Stephen Bay, Markus G. Anderle,David M. Steier, and Christos Faloutsos.
2009.Snare: a link analytic system for graph labeling andrisk detection.
In KDD ?09: Proceedings of the 15thACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 1265?1274.Qiaozhu Mei and ChengXiang Zhai.
2008.
Generatingimpact-based summaries for scientific literature.
InProceedings of ACL ?08, pages 816?824.Donald Metzler and W. Bruce Croft.
2005.
A markovrandom field model for term dependencies.
In SI-GIR ?05: Proceedings of the 28th annual interna-tional ACM SIGIR conference on Research and de-velopment in information retrieval, pages 472?479.Donald Metzler and W. Bruce Croft.
2007.
Latent con-cept expansion using markov random fields.
In SI-GIR ?07: Proceedings of the 30th annual interna-tional ACM SIGIR conference on Research and de-velopment in information retrieval, pages 311?318.Donald A. Metzler.
2007.
Automatic feature selectionin the markov random field model for informationretrieval.
In CIKM ?07: Proceedings of the sixteenthACM conference on Conference on information andknowledge management, pages 253?262.Saif Mohammad, Bonnie Dorr, Melissa Egan, AhmedHassan, Pradeep Muthukrishan, Vahed Qazvinian,Dragomir Radev, and David Zajic.
2009.
Using ci-tations to generate surveys of scientific paradigms.563In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 584?592, Boulder, Colorado, June.Association for Computational Linguistics.Hidetsugu Nanba and Manabu Okumura.
1999.
To-wards multi-paper summarization using referenceinformation.
In IJCAI1999, pages 926?931.Hidetsugu Nanba, Takeshi Abekawa, Manabu Oku-mura, and Suguru Saito.
2004a.
Bilingual presri:Integration of multiple research paper databases.
InProceedings of RIAO 2004, pages 195?211, Avi-gnon, France.Hidetsugu Nanba, Noriko Kando, and Manabu Oku-mura.
2004b.
Classification of research papers us-ing citation links and citation types: Towards au-tomatic review article generation.
In Proceedingsof the 11th SIG Classification Research Workshop,pages 117?134, Chicago, USA.Mark E. J. Newman.
2001.
The structure of scientificcollaboration networks.
PNAS, 98(2):404?409.Vahed Qazvinian and Dragomir R. Radev.
2008.
Sci-entific paper summarization using citation summarynetworks.
In COLING 2008, Manchester, UK.Dragomir R. Radev, Pradeep Muthukrishnan, and Va-hed Qazvinian.
2009.
The ACL anthology networkcorpus.
In ACL workshop on Natural LanguageProcessing and Information Retrieval for Digital Li-braries.Matteo Romanello, Federico Boschetti, and GregoryCrane.
2009.
Citations in the digital library of clas-sics: Extracting canonical references by using con-ditional random fields.
In Proceedings of the 2009Workshop on Text and Citation Analysis for Schol-arly Digital Libraries, pages 80?87, Suntec City,Singapore, August.
Association for ComputationalLinguistics.Advaith Siddharthan and Simone Teufel.
2007.
Whoseidea was this, and why does it matter?
attribut-ing scientific work to citations.
In Proceedings ofNAACL/HLT-07.Simone Teufel and Marc Moens.
2002.
Summarizingscientific articles: experiments with relevance andrhetorical status.
Comput.
Linguist., 28(4):409?445.Simone Teufel, Advaith Siddharthan, and Dan Tidhar.2006.
Automatic classification of citation function.In Proceedings of the EMNLP, Sydney, Australia,July.Simone Teufel.
2005.
Argumentative Zoning for Im-proved Citation Indexing.
Computing Attitude andAffect in Text: Theory and Applications, pages 159?170.Jonathan S. Yedidia, William T. Freeman, and YairWeiss.
2003.
Understanding belief propagation andits generalizations.
pages 239?269.564
