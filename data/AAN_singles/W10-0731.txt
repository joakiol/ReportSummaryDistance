Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 195?203,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAmazon Mechanical Turk for Subjectivity Word Sense DisambiguationCem AkkayaUniversity of Pittsburghcem@cs.pitt.eduAlexander ConradUniversity of Pittsburghconrada@cs.pitt.eduJanyce WiebeUniversity of Pittsburghwiebe@cs.pitt.eduRada MihalceaUniversity of North Texasrada@cs.unt.eduAbstractAmazon Mechanical Turk (MTurk) is a mar-ketplace for so-called ?human intelligencetasks?
(HITs), or tasks that are easy for hu-mans but currently difficult for automated pro-cesses.
Providers upload tasks to MTurkwhich workers then complete.
Natural lan-guage annotation is one such human intelli-gence task.
In this paper, we investigate us-ing MTurk to collect annotations for Subjec-tivity Word Sense Disambiguation (SWSD),a coarse-grained word sense disambiguationtask.
We investigate whether we can useMTurk to acquire good annotations with re-spect to gold-standard data, whether we canfilter out low-quality workers (spammers), andwhether there is a learning effect associatedwith repeatedly completing the same kind oftask.
While our results with respect to spam-mers are inconclusive, we are able to ob-tain high-quality annotations for the SWSDtask.
These results suggest a greater role forMTurk with respect to constructing a largescale SWSD system in the future, promisingsubstantial improvement in subjectivity andsentiment analysis.1 IntroductionMany Natural Language Processing (NLP) systemsrely on large amounts of manually annotated datathat is collected from domain experts.
The anno-tation process to obtain this data is very laboriousand expensive.
This makes supervised NLP systemssubject to a so-called knowledge acquisition bottle-neck.
For example, (Ng, 1997) estimates an effort of16 person years to construct training data for a high-accuracy domain independent Word Sense Disam-biguation (WSD) system.Recently researchers have been investigatingAmazon Mechanical Turk (MTurk) as a source ofnon-expert natural language annotation, which is acheap and quick alternative to expert annotations(Kaisser and Lowe, 2008; Mrozinski et al, 2008).In this paper, we utilize MTurk to obtain trainingdata for Subjectivity Word Sense Disambiguation(SWSD) as described in (Akkaya et al, 2009).
Thegoal of SWSD is to automatically determine whichword instances in a corpus are being used with sub-jective senses, and which are being used with ob-jective senses.
SWSD is a new task which suffersfrom the absence of a substantial amount of anno-tated data and thus can only be applied on a smallscale.
SWSD has strong connections to WSD.
Likesupervised WSD, it requires training data where tar-get word instances ?
words which need to be dis-ambiguated by the system ?
are labeled as havingan objective sense or a subjective sense.
(Akkayaet al, 2009) show that SWSD may bring substantialimprovement in subjectivity and sentiment analysis,if it could be applied on a larger scale.
The goodnews is that training data for 80 selected keywords isenough to make a substantial difference (Akkaya etal., 2009).
Thus, large scale SWSD is feasible.
Wehypothesize that annotations for SWSD can be pro-vided by non-experts reliably if the annotation taskis presented in a simple way.The annotations obtained from MTurk workersare noisy by nature, because MTurk workers arenot trained for the underlying annotation task.
Thatis why previous work explored methods to assessannotation quality and to aggregate multiple noisyannotations for high reliability (Snow et al, 2008;Callison-Burch, 2009).
It is understandable that notevery worker will provide high-quality annotations,195depending on their background and interest.
Un-fortunately, some MTurk workers do not follow theannotation guidelines and carelessly submit annota-tions in order to gain economic benefits with onlyminimal effort.
We define this group of workersas spammers.
We believe it is essential to distin-guish between workers as well-meaning annotatorsand workers as spammers who should be filtered outas a first step when utilizing MTurk.
In this work,we investigate how well the built-in qualifications inMTurk function as such a filter.Another important question about MTurk workersis whether they learn to provide better annotationsover time in the absence of any interaction and feed-back.
The presence of a learning effect may supportworking with the same workers over a long time andcreating private groups of workers.
In this work, wealso examine if there is a learning effect associatedwith MTurk workers.To summarize, in this work we investigate the fol-lowing questions:?
Can MTurk be utilized to collect reliable train-ing data for SWSD ??
Are the built-in methods provided by MTurkenough to avoid spammers ??
Is there a learning effect associated with MTurkworkers ?The remainder of the paper is organized as fol-lows.
In Section 2, we give general background in-formation on the Amazon Mechanical Turk service.In Section 3, we discuss sense subjectivity.
In Sec-tion 4, we describe the subjectivity word sense dis-ambiguation task.
In Section 5, we discuss the de-sign of our experiment and our filtering mechanismsfor workers.
In Section 6, we evaluate MTurk anno-tations and relate results to our questions.
In Section7, we review related work.
In Section 8, we drawconclusions and discuss future work.2 Amazon Mechanical TurkAmazon Mechanical Turk (MTurk)1 is a market-place for so-called ?human intelligence tasks,?
orHITs.
MTurk has two kinds of users: providers and1http://mturk.amazon.comworkers.
Providers create HITs using the Mechan-ical Turk API and, for a small fee, upload them tothe HIT database.
Workers search through the HITdatabase, choosing which to complete in exchangefor monetary compensation.
Anyone can sign up asa provider and/or worker.
Each HIT has an associ-ated monetary value, and after reviewing a worker?ssubmission, a provider may choose whether to ac-cept the submission and pay the worker the promisedsum or to reject it and pay the worker nothing.
HITstypically consist of tasks that are easy for humansbut difficult or impossible for computers to completequickly or effectively, such as annotating images,transcribing speech audio, or writing a summary ofa video.One challenge for requesters using MTurk is thatof filtering out spammers and other workers whoconsistently produce low-quality annotations.
In or-der to allow requesters to restrict the range of work-ers who can complete their tasks, MTurk providesseveral types of built-in statistics, known as quali-fications.
One such qualification is approval rating,a statistic that records a worker?s ratio of acceptedHITs compared to the total number of HITs sub-mitted by that worker.
Providers can require that aworker?s approval rating be above a certain thresholdbefore allowing that worker to submit one of his/herHITs.
Country of residence and lifetime approvednumber of HITs completed also serve as built-inqualifications that providers may check before al-lowing workers to access their HITs.2 Amazon alsoallows providers to define their own qualifications.Typically, provider-defined qualifications are used toensure that HITs which require particular skills areonly completed by qualified workers.
In most cases,workers acquire provider-defined qualifications bycompleting an online test.Amazon also provides a mechanism by whichmultiple unique workers can complete the same HIT.The number of times a HIT is to be completed isknown as the number of assignments for the HIT.By having multiple workers complete the same HIT,2According to the terms of use, workers are prohibited fromhaving more than one account, but to the writer?s knowledgethere is no method in place to enforce this restriction.
Thus,a worker with a poor approval rating could simply create anew account, since all accounts start with an approval ratingof 100%.196Subjective senses:His alarm grew.alarm, dismay, consternation ?
(fear resulting from the aware-ness of danger)=> fear, fearfulness, fright ?
(an emotion experienced in an-ticipation of some specific pain or danger (usually accompa-nied by a desire to flee or fight))What?s the catch?catch ?
(a hidden drawback; ?it sounds good but what?s thecatch??
)=> drawback ?
(the quality of being a hindrance; ?hepointed out all the drawbacks to my plan?
)Objective senses:The alarm went off.alarm, warning device, alarm system ?
(a device that signals theoccurrence of some undesirable event)=> device ?
(an instrumentality invented for a particular pur-pose; ?the device is small enough to wear on your wrist?
; ?adevice intended to conserve water?
)He sold his catch at the market.catch, haul ?
(the quantity that was caught; ?the catch was only10 fish?
)=> indefinite quantity ?
(an estimated quantity)Figure 1: Subjective and objective word sense examples.techniques such as majority voting among the sub-missions can be used to aggregate the results forsome types of HITs, resulting in a higher-qualityfinal answer.
Previous work (Snow et al, 2008)demonstrates that aggregating worker submissionsoften leads to an increase in quality.3 Word Sense Subjectivity(Wiebe and Mihalcea, 2006) define subjective ex-pressions as words and phrases being used to ex-press mental and emotional states, such as specula-tions, evaluations, sentiments, and beliefs.
Many ap-proaches to sentiment and subjectivity analysis relyon lexicons of such words (subjectivity clues).
How-ever, such clues often have both subjective and ob-jective senses, as illustrated by (Wiebe and Mihal-cea, 2006).
Figure 1 provides subjective and objec-tive examples of senses.
(Akkaya et al, 2009) points out that most sub-jectivity lexicons are compiled as lists of keywords,rather than word meanings (senses).
Thus, subjec-tivity clues used with objective senses ?
false hits ?are a significant source of error in subjectivity andsentiment analysis.
SWSD specifically deals withthis source of errors.
(Akkaya et al, 2009) showsthat SWSD helps with various subjectivity and sen-timent analysis systems by ignoring false hits.4 Annotation Task4.1 Subjectivity Word Sense DisambiguationOur target task is Subjectivity Word Sense Disam-biguation (SWSD).
SWSD aims to determine whichword instances in a corpus are being used with sub-jective senses and which are being used with ob-jective senses.
It can be considered to be a coarse-grained application-specific WSD that distinguishesbetween only two senses: (1) the subjective senseand (2) the objective sense.Subjectivity word sense annotation is done in thefollowing way.
We try to keep the annotation taskfor the worker as simple as possible.
Thus, we donot directly ask them if the instance of a target wordhas a subjective or an objective sense (without anysense inventory), because the concept of subjectivityis fairly difficult to explain to someone who does nothave any linguistics background.
Instead we showMTurk workers two sets of senses ?
one subjectiveset and one objective set ?
for a specific target wordand a text passage in which the target word appears.Their job is to select the set that best reflects themeaning of the target word in the text passage.
Thespecific sense set automatically gives us the subjec-tivity label of the instance.
This makes the annota-tion task easier for them as (Snow et al, 2008) showsthat WSD can be done reliably by MTurk workers.This approach presupposes a set of word senses thathave been annotated as subjective or objective.
Theannotation of senses in a dictionary for subjectivityis not difficult for an expert annotator.
Moreover,it needs to be done only once per target word, al-lowing us to collect hundreds of subjectivity labeledinstances for each target word through MTurk.In this annotation task, we do not inform theMTurk workers about the nature of the sets.
Thismeans the MTurk workers have no idea that they areannotating subjectivity of senses; they are just se-lecting the set which contains a sense matching theusage in the sentence or being as similar to it as pos-sible.
This ensures that MTurk workers are not bi-ased by the contextual subjectivity of the sentencewhile tagging the target word instance.197Sense Set1 (Subjective){ look, appear, seem } ?
give a certain impression or have acertain outward aspect; ?She seems to be sleeping?
; ?This ap-pears to be a very difficult problem?
; ?This project looks fishy?
;?They appeared like people who had not eaten or slept for along time?
{ appear, seem } ?
seem to be true, probable, or apparent; ?Itseems that he is very gifted?
; ?It appears that the weather inCalifornia is very bad?Sense Set2 (Objective){ appear } ?
come into sight or view; ?He suddenly appearedat the wedding?
; ?A new star appeared on the horizon?
{ appear, come out } ?
be issued or published, as of news in apaper, a book, or a movie; ?Did your latest book appear yet??
;?The new Woody Allen film hasn?t come out yet?
{ appear, come along } ?
come into being or existence, or ap-pear on the scene; ?Then the computer came along and changedour lives?
; ?Homo sapiens appeared millions of years ago?
{ appear } ?
appear as a character on stage or appear in a play,etc.
; ?Gielgud appears briefly in this movie?
; ?She appeared in?Hamlet?
on the London{ appear } ?
present oneself formally, as before a (judicial) au-thority; ?He had to appear in court last month?
; ?She appearedon several charges of theft?Figure 2: Sense sets for target word ?appear?.Below, we describe a sample annotation problem.An MTurk worker has access to the following twosense sets of the target word ?appear?, as seen inFigure 2.
The information that the first sense set issubjective and second sense set is objective is notavailable to the worker.
The worker is presentedwith the following text passage holding the targetword ?appear?.It?s got so bad that I don?t even know whatto say.
Charles |target| appeared |target|somewhat embarrassed by his own behav-ior.
The hidden speech was coming, Icould tell.In this passage, the MTurk worker should be ableto understand that ?appeared?
refers to the outwardimpression given by ?Charles?.
This use of appear ismost similar to the first entry in sense set one; thus,the correct answer for this problem is Sense Set-1.4.2 Gold StandardThe gold standard dataset, on which we evaluateMTurk worker annotations, is provided by (Akkayaet al, 2009).
This dataset (called subjSENSEVAL)consists of target word instances in a corpus labeledas S or O, indicating whether they are used witha subjective or objective sense.
It is based on thelexical sample corpora from SENSEVAL1 (Kilgar-riff and Palmer, 2000), SENSEVAL2 (Preiss andYarowsky, 2001), and SENSEVAL3 (Mihalcea andEdmonds, 2004).
SubjSENSEVAL consists of in-stances for 39 ambiguous (having both subjectiveand objective meanings) target words.
(Akkaya et al, 2009) also provided us with sub-jectivity labels for word senses which are used in thecreation of subjSENSEVAL.
Sense labels of the tar-get word senses are defined on the sense inventoryof the underlying corpus (Hector for SENSEVAL1;WordNet1.7 for SENSEVAL2; and WordNet1.7.1for SENSEVAL3).
This means the target wordsfrom SENSEVAL1 have their senses annotated inthe Hector dictionary, while the target words fromSENSEVAL2 and SENSEVAL3 have their sensesannotated in WordNet1.7.
We make use of these la-beled sense inventories to build our subjective andobjective sets of senses, which we present to theMTurk worker as Sense Set1 and Sense Set2 re-spectively.
We want to have a uniform sense rep-resentation for the words we ask subjectivity senselabels for.
Thus, we consider only SENSEVAL2 andSENSEVAL3 subsets of subjSENSEVAL, becauseSENSEVAL1 relies on a sense inventory other thanWordNet.5 Experimental DesignWe chose randomly 8 target words that have a distri-bution of subjective and objective instances in sub-jSENSEVAL with less skew than 75%.
That is, nomore than 75% of a word?s senses are subjective orobjective.
Our concern is that using skewed datamight bias the workers to choose from the more fre-quent label without thinking much about the prob-lem.
Another important fact is that these words withlow skew are more ambiguous and responsible formore false hits.
Thus, these target words are the onesfor which we really need subjectivity word sensedisambiguation.
For each of these 8 target words, weselect 40 passages from subjSENSEVAL in whichthe target word appears, to include in our experi-ments.
Table 1 summarizes the selected target words198Word FLP Word FLPappear 55% fine 72.5%judgment 65% solid 55%strike 62.5% difference 67.5%restraint 70% miss 50%Average 62.2%Table 1: Frequent label percentages for target words.and their label distribution.
In this table, frequent la-bel percentage (FLP) represents the skew for eachword.
A word?s FLP is equal to the percent of thesenses that are of the most frequently occurring typeof sense (subjective or objective) for that word.We believe this annotation task is a good candi-date for attracting spammers.
This task requires onlybinary annotations, where the worker just choosesfrom one of the two given sets, which is not a dif-ficult task.
Since it is easy to provide labels, webelieve that there will be a distinct line, with re-spect to quality of annotations, between spammersand mediocre annotators.For our experiments, we created three differentHIT groups each having different qualification re-quirements but sharing the same data.
To be con-crete, each HIT group consists of the same 320 in-stances: 40 instances for each target word listed inTable 1.
Each HIT presents an MTurk worker withfour instances of the same word in a text passage?
this makes 80 HITs for each HIT group ?
andasks him to choose the set to which the activatedsense belongs.
We know for each HIT the mappingbetween sense set numbers and subjectivity.
Thus,we can evaluate each HIT response on our gold-standard data, as discussed in Section 4.2.
We payseven cents per HIT.
We consider this to be generouscompensation for such a simple task.There are many builtin qualifications in MTurk.We concentrated only on three of them: location,HIT approval rate, and approved HITs, as discussedin Section 2.
In our experience, these qualificationsare widely used for quality assurance.
As mentionedbefore, we created three different HIT groups in or-der to see how well different built-in qualificationcombinations do with respect to filtering spammers.These groups ?
starting from the least constrained tothe most constrained ?
are listed in Table 2.Group1 Location: USAGroup2 Location: USAHIT Approval Rate > 96%Group3Location: USAHIT Approval Rate > 96%Approved HITs > 500Table 2: Constraints for each HIT group.Group1 required only that the MTurk workers arelocated in the US.
This group is the least constrainedone.
Group2 additionally required an approval rategreater than 96%.
Group3 is the most constrainedone, requiring a lifetime approved HIT number tobe greater than 500, in addition to the qualificationsin Group1 and Group2.We believe that neither location nor approval rateand location together is enough to avoid spammers.While being a US resident does to some extent guar-antee English proficiency, it does not guarantee well-thought answers.
Since there is no mechanism inplace preventing users from creating new MTurkworker accounts at will and since all worker ac-counts are initialized with a 100% approval rate, wedo not think that approval rate is sufficient to avoidserial spammers and other poor annotators.
We hy-pothesize that the workers with high approval rateand a large number of approved HITs have a reputa-tion to maintain, and thus will probably be careful intheir answers.
We think it is unlikely that spammerswill have both a high approval rate and a large num-ber of completed HITs.
Thus, we anticipated thatGroup3?s annotations will be of higher quality thanthose of the other groups.Note that an MTurk worker who has access to theHITs in one of the HIT groups also has access toHITs in less constrained groups.
For example, anMTurk worker who has access to HITs in Group3also has access to HITs in Group2 and Group1.
Wedid not prevent MTurk workers from working inmultiple HIT groups because we did not want toinfluence worker behavior, but instead simulate themost realistic annotation scenario.In addition to the qualifications described above,we also required each worker to take a qualificationtest in order to prove their competence in the anno-tation task.
The qualification test consists of 10 sim-199Figure 3: Venn diagram illustrating worker distribution.ple annotation questions identical in form to thosepresent in the HITs.
These questions are split evenlybetween two target words, ?appear?
and ?restraint?.There are a total of five subjective and five objectiveusages in the test.
We required an accuracy of 90%in the qualification test, corresponding to a Kappascore of .80, before a worker was allowed to submitany of our HITs.
If a worker failed to achieve a scoreof 90% on an attempt, that worker could try the testagain after a delay of 4 hours.We collected three sets of assignments withineach HIT group.
In other words, each HIT was com-pleted three times by three different workers in eachgroup.
This gives us a total of 960 assignments ineach HIT group.
A total of 26 unique workers par-ticipated in the experiment: 17 in Group1, 17 inGroup2 and 8 in Group3.
As mentioned before, aworker is able to participate in all the groups forwhich he is qualified.
Thus the unique worker num-bers in each group does not sum up to the total num-ber of workers in the experiment, since some work-ers participated in the HITs for more than one group.Figure 3 summarizes how workers are distributedbetween groups.6 EvaluationWe are interested in how accurate the MTurk annota-tions are with respect to gold-standard data.
We arealso interested in how the accuracy of each groupdiffers from the others.
We evaluate each group it-self separately on the gold-standard data.
Addition-ally, we evaluate each worker?s performance on thegold-standard data and inspect their distribution invarious groups.6.1 Group EvaluationAs mentioned in the previous section, we collectthree annotations for each HIT.
They are assigned torespective trials in the order submitted by the work-ers.
The results are summarized in Table 3.
Trialsare labeled as TX and MV is the majority vote an-notation among the three trials.
The final columncontains the baseline agreement where a worker la-bels each instance of a word with the most frequentlabel of that word in the gold-standard data.
It isclear from this table that, since worker accuracyalways exceeds the baseline agreement, subjectiv-ity word sense annotation can be done reliably byMTurk workers.
This is very promising.
Consid-ering the low cost and low time required to obtainMTurk annotations, a large scale SWSD is realis-tic.
For example, (Akkaya et al, 2009) shows thatthe most frequent 80 lexicon keywords are respon-sible for almost half of the false hits in the MPQACorpus3 (Wiebe et al, 2005; Wilson, 2008), a cor-pus annotated for subjective expressions.
UtilizingMTurk to collect training data for these 80 lexiconkeywords will be quick and cheap and most impor-tantly reliable.When we compare groups with each other, wesee that the best trial result is achieved in Group3.However, according to McNemar?s test (Dietterich,1998), there is no statistically significant differencebetween any trial of any group.
On the other hand,the best majority vote annotation is achieved inGroup2, but again there is no statistically significantdifference between any majority vote annotation ofany group.
These results are surprising to us, sincewe do not see any significant difference in the qual-ity of the data throughout different groups.6.2 Worker EvaluationIn this section, we evaluate all 26 workers and groupthem as either spammers or well-meaning workers.All workers who deviate from the gold-standard by a3http://www.cs.pitt.edu/mpqa/200Group3 Group2 Group1 baselineT1 T2 T3 MV T1 T2 T3 MV T1 T2 T3 MVAccuracy 89.7 86.9 86.6 88.4 87.2 86.3 88.1 90.3 84.4 87.5 87.5 88.4 62.2Kappa .79 .74 .73 .77 .74 .73 .76 .81 .69 .75 .75 .77Table 3: Accuracy and kappa scores for each group of workers.Threshold 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75Spammer CountG1 2 2 2 2 2 4 7 9G2 1 2 2 2 2 3 5 8G3 0 0 0 0 0 0 2 2Spammer PercentageG1 12% 12% 12% 12% 12% 24% 41% 53%G2 6% 12% 12% 12% 12% 12% 29% 42%G3 0% 0% 0% 0% 0% 0% 25% 25%Table 4: Spammer representation in groups.large margin beyond a certain threshold will be con-sidered to be spammers.
As discussed in Section 5,we require all participating workers to pass a quali-fication test before answering HITs.
Thus, we knowthat they are competent to do subjectivity sense an-notations, and providing consistently erroneous an-notations means that they are probably spammers.We think a kappa score of 0.6 is a good thresholdto distinguish spammers from well-meaning work-ers.
For this threshold, we had 2 spammers par-ticipating in Group1, 2 spammers in Group2 and0 spammers in Group3.
Table 4 presents spammercount and spammer percentage in each group forvarious threshold values.
We see that Group3 hasconsistently fewer spammers and a smaller spammerpercentage.
The lowest kappa scores for Group1,Group2, and Group3 are .35, .40, and .69, respec-tively.
The mean kappa scores for Group1, Group2,and Group3 are .73, .75, and .77, respectively.These results indicate that Group3 is less proneto spammers, apparently contradicting Section 6.1.We see the reason when we inspect the data moreclosely.
It turns out that spammers contributed inGroup1 and Group2 only minimally.
On the otherhand there are two mediocre workers (Kappa of0.69) who submit around 1/3 of the HITs in Group3.This behavior might be a coincidence.
In the face ofcontradicting results, we think that we need a moreextensive study to derive conclusions about the rela-tion between spammer distribution and built-in qual-ification.6.3 Learning EffectExpert annotators can learn to provide more accu-rate annotations over time.
(Passonneau et al, 2006)reports a learning effect early in the annotation pro-cess.
This might be due to the formal and informalinteraction between annotators.
Another possibilityis that the annotators might get used to the annota-tion task over time.
This is to be expected if there isnot an extensive training process before the annota-tion takes place.On the other hand, the MTurk workers have nointeraction among themselves.
They do not receiveany formal training and do not have access to trueannotations except a few examples if provided bythe requester.
These properties make MTurk work-ers a unique annotation workforce.
We are interestedif the learning effect common to expert annotatorsholds in this unique workforce in the absence of anyinteraction and feedback.
That may justify workingwith the same set of workers over a long time bycreating private groups of workers.We sort annotations of a worker after the submis-sion date.
This way, we get for each worker an or-dered list of annotations.
We split the list into binsof size 40 and we test for an increasing trend inthe proportion of successes over time.
We use theChi-squared Test for binomial proportions (Rosner,2006).
Using this test, we find that all of the p-values201are substantially larger than 0.05.
Thus, there is noincreasing trend in the proportion of successes andno learning effect.
This is true for both mediocreworkers and very reliable workers.
We think that theresults may differ for harder annotation tasks wherethe input is more complex and requires some adjust-ment.7 Related WorkThere has been recently an increasing interest inAmazon Mechanical Turk.
Many researchers haveutilized MTurk as a source of non-expert naturallanguage annotation to create labeled datasets.
In(Mrozinski et al, 2008), MTurk workers are used tocreate a corpus of why-questions and correspondinganswers on which QA systems may be developed.
(Kaisser and Lowe, 2008) work on a similar task.They make use of MTurk workers to identify sen-tences in documents as answers and create a corpusof question-answer sentence pairs.
MTurk is alsoconsidered in other fields than natural language pro-cessing.
For example, (Sorokin and Forsyth, 2008)utilizes MTurk for image labeling.
Our ultimate goalis similar; namely, to build training data (in our casefor SWSD).Several studies have concentrated specifically onthe quality aspect of the MTurk annotations.
Theyinvestigated methods to assess annotation qualityand to aggregate multiple noisy annotations for highreliability.
(Snow et al, 2008) report MTurk an-notation quality on various NLP tasks (e.g.
WSD,Textual Entailment, Word Similarity) and definea bias correction method for non-expert annota-tors.
(Callison-Burch, 2009) uses MTurk workersfor manual evaluation of automatic translation qual-ity and experiments with weighed voting to com-bine multiple annotations.
(Hsueh et al, 2009) de-fine various annotation quality measures and showthat they are useful for selecting annotations leadingto more accurate classifiers.
Our work investigatesthe effect of built-in qualifications on the quality ofMTurk annotations.
(Hsueh et al, 2009) applies MTurk to get senti-ment annotations on political blog snippets.
(Snowet al, 2008) utilizes MTurk for affective text annota-tion task.
In both works, MTurk workers annotatedlarger entities but on a more detailed scale than wedo.
(Snow et al, 2008) also provides a WSD anno-tation task which is similar to our annotation task.The difference is the MTurk workers are choosingan exact sense not a sense set.8 Conclusion and Future WorkIn this paper, we address the question of whetherbuilt-in qualifications are enough to avoid spam-mers.
The investigation of worker performancesindicates that the lesser constrained a group is themore spammers it attracts.
On the other hand, we didnot find any significant difference between the qual-ity of the annotations for each group.
It turns out thatworkers considered as spammers contributed onlyminimally.
We do not know if it is just a coincidenceor if it is correlated to the task definition.
We did notget conclusive results.
We need to do more extensiveexperiments before arriving at conclusions.Another aspect we investigated is the learning ef-fect.
Our results show that there is no improvementin annotator reliability over time.
We should not ex-pect MTurk workers to provide more consistent an-notations over time.
This will probably be the casein similar annotation tasks.
For harder annotationtasks (e.g.
parse tree annotation) things may be dif-ferent.
An interesting follow-up would be whethershowing the answers of other workers on the sameHIT will promote learning.We presented our subjectivity sense annotationtask to the worker in a very simple way.
The an-notation results prove that subjectivity word senseannotation can be done reliably by MTurk workers.This is very promising since the MTurk annotationscan be collected for low costs in a short time pe-riod.
This implies that a large scale general SWSDcomponent, which can help with various subjectivityand sentiment analysis tasks, is feasible.
We plan towork with selected workers to collect new annotateddata for SWSD and use this data to train a SWSDsystem.AcknowledgmentsThis material is based in part upon work sup-ported by National Science Foundation awards IIS-0916046 and IIS-0917170 and by Department ofHomeland Security award N000140710152.
The au-thors are grateful to the three paper reviewers fortheir helpful suggestions.202ReferencesCem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009.Subjectivity word sense disambiguation.
In Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP 2009).Chris Callison-Burch.
2009.
Fast, cheap, and creative:evaluating translation quality using amazon?s mechan-ical turk.
In EMNLP ?09: Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 286?295, Morristown, NJ,USA.
Association for Computational Linguistics.Thomas G. Dietterich.
1998.
Approximate statisticaltests for comparing supervised classification learningalgorithms.
Neural Computation, 10:1895?1923.Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.2009.
Data quality from crowdsourcing: a study ofannotation selection criteria.
In HLT ?09: Proceedingsof the NAACL HLT 2009 Workshop on Active Learningfor Natural Language Processing, pages 27?35, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Michael Kaisser and John Lowe.
2008.
Creat-ing a research collection of question answer sen-tence pairs with amazons mechanical turk.
In Pro-ceedings of the Sixth International Language Re-sources and Evaluation (LREC?08).
http://www.lrec-conf.org/proceedings/lrec2008/.Joanna Mrozinski, Edward Whittaker, and Sadaoki Furui.2008.
Collecting a why-question corpus for develop-ment and evaluation of an automatic QA-system.
InProceedings of ACL-08: HLT, pages 443?451, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Hwee Tou Ng.
1997.
Getting serious about word sensedisambiguation.
In Proceedings of the ACL SIGLEXWorkshop on Tagging Text with Lexical Semantics:Why,What, and How?Rebecca Passonneau, Nizar Habash, and Owen Rambow.2006.
Inter-annotator agreement on a multilingual se-mantic annotation task.
In Proceedings of the FifthInternational Conference on Language Resources andEvaluation (LREC).Bernard Rosner.
2006.
Fundamentals of Biostatistics.Thompson Brooks/Cole.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In EMNLP ?08: Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, pages 254?263, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.A.
Sorokin and D. Forsyth.
2008.
Utility data annotationwith amazon mechanical turk.
pages 1 ?8, june.J.
Wiebe and R. Mihalcea.
2006.
Word sense and subjec-tivity.
In (ACL-06), Sydney, Australia.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation (for-merly Computers and the Humanities), 39(2/3):164?210.Theresa Wilson.
2008.
Fine-grained Subjectivity andSentiment Analysis: Recognizing the Intensity, Polar-ity, and Attitudes of private states.
Ph.D. thesis, Intel-ligent Systems Program, University of Pittsburgh.203
