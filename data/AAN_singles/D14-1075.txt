Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 681?690,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsFear the REAPER: A System for Automatic Multi-DocumentSummarization with Reinforcement LearningCody RiouxUniversity of LethbridgeLethbridge, AB, Canadacody.rioux@uleth.caSadid A. HasanPhilips Research North AmericaBriarcliff Manor, NY, USAsadid.hasan@philips.comYllias ChaliUniversity of LethbridgeLethbridge, AB, Canadachali@cs.uleth.caAbstractThis paper explores alternate algorithms,reward functions and feature sets for per-forming multi-document summarizationusing reinforcement learning with a highfocus on reproducibility.
We show thatROUGE results can be improved usinga unigram and bigram similarity metricwhen training a learner to select sentencesfor summarization.
Learners are trainedto summarize document clusters based onvarious algorithms and reward functionsand then evaluated using ROUGE.
Our ex-periments show a statistically significantimprovement of 1.33%, 1.58%, and 2.25%for ROUGE-1, ROUGE-2 and ROUGE-L scores, respectively, when comparedwith the performance of the state of theart in automatic summarization with re-inforcement learning on the DUC2004dataset.
Furthermore query focused exten-sions of our approach show an improve-ment of 1.37% and 2.31% for ROUGE-2and ROUGE-SU4 respectively over queryfocused extensions of the state of theart with reinforcement learning on theDUC2006 dataset.1 IntroductionThe multi-document summarization problem hasreceived much attention recently (Lyngbaek,2013; Sood, 2013; Qian and Liu, 2013) due toits ability to reduce large quantities of text to ahuman processable amount as well as its appli-cation in other fields such as question answering(Liu et al., 2008; Chali et al., 2009a; Chali et al.,2009b; Chali et al., 2011b).
We expect this trendto further increase as the amount of linguistic dataon the web from sources such as social media,wikipedia, and online newswire increases.
Thispaper focuses specifically on utilizing reinforce-ment learning (Sutton and Barto, 1998; Szepesv,2009) to create a policy for summarizing clustersof multiple documents related to the same topic.The task of extractive automated multi-document summarization (Mani, 2001) is to se-lect a subset of textual units, in this case sentences,from the source document cluster to form a sum-mary of the cluster in question.
This extractiveapproach allows the learner to construct a sum-mary without concern for the linguistic quality ofthe sentences generated, as the source documentsare assumed to be of a certain linguistic quality.This paper aims to expand on the techniques usedin Ryang and Abekawa (2012) which uses a re-inforcement learner, specifically TD(?
), to createsummaries of document clusters.
We achieve thisthrough introducing a new algorithm, varying thefeature space and utilizing alternate reward func-tions.The TD(?)
learner used in Ryang andAbekawa (2012) is a very early reinforcementlearning implementation.
We explore the option ofleveraging more recent research in reinforcementlearning algorithms to improve results.
To this endwe explore the use of SARSA which is a deriva-tive of TD(?)
that models the action space in ad-dition to the state space modelled by TD(?).
Fur-thermore we explore the use of an algorithm notbased on temporal difference methods, but insteadon policy iteration techniques.
Approximate Pol-icy Iteration (Lagoudakis and Parr, 2003) gener-ates a policy, then evaluates and iterates until con-vergence.The reward function in Ryang and Abekawa(2012) is a delayed reward based on tf?idf values.We further explore the reward space by introduc-ing similarity metric calculations used in ROUGE(Lin, 2004) and base our ideas on Saggion et al.(2010).
The difference between immediate re-wards and delayed rewards is that the learner re-681ceives immediate feedback at every action in theformer and feedback only at the end of the episodein the latter.
We explore the performance differ-ence of both reward types.
Finally we developquery focused extensions to both reward functionsand present their results on more recent DocumentUnderstanding Conference (DUC) datasets whichran a query focused task.We first evaluate our systems using theDUC2004 dataset for comparison with the resultsin Ryang and Abekawa (2012).
We then presentthe results of query focused reward functionsagainst the DUC2006 dataset to provide refer-ence with a more recent dataset and a more recenttask, specifically a query-focused summarizationtask.
Evaluations are performed using ROUGEfor ROUGE-1, ROUGE-2 and ROUGE-L valuesfor general summarization, while ROUGE-2 andROUGE-SU4 is used for query-focused summa-rization.
Furthermore we selected a small subsetof query focused summaries to be subjected to hu-man evaluations and present the results.Our implementation is named REAPER(Relatedness-focused Extractive Automaticsummary Preparation Exploiting Reinfocementlearning) thusly for its ability to harvest a docu-ment cluster for ideal sentences for performingthe automatic summarization task.
REAPER isnot just a reward function and feature set, it is afull framework for implementing summarizationtasks using reinforcement learning and is avail-able online for experimentation.1The primarycontributions of our experiments are as follows:?
Exploration of TD(?
), SARSA and Ap-proximate Policy Iteration.?
Alternate REAPER reward function.?
Alternate REAPER feature set.?
Query focused extensions of automatic sum-marization using reinforcement learning.2 Previous Work and MotivationPrevious work using reinforcement learning fornatural language processing tasks (Branavan etal., 2009; Wan, 2007; Ryang and Abekawa,2012; Chali et al., 2011a; Chali et al., 2012)inspired us to use a similar approach in ourexperiments.
Ryang and Abekawa (2012) im-plemented a reinforcement learning approach to1https://github.com/codyrioux/REAPERmulti-document summarization which they namedAutomatic Summarization using ReinforcementLearning (ASRL).
ASRL uses TD(?)
to learn andthen execute a policy for summarizing a cluster ofdocuments.
The algorithm performs N summa-rizations from a blank state to termination, updat-ing a set of state-value predictions as it does so.From these N episodes a policy is created usingthe estimated state-value pairs, this policy greed-ily selects the best action until the summary entersits terminal state.
This summary produced is theoutput of ASRL and is evaluated using ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004).
Theresults segment of the paper indicates that ASRLoutperforms greedy and integer linear program-ming (ILP) techniques for the same task.There are two notable details that provide themotivation for our experiments; TD(?)
is rela-tively old as far as reinforcement learning (RL)algorithms are concerned, and the optimal ILP didnot outperform ASRL using the same reward func-tion.
The intuition gathered from this is that ifthe optimal ILP algorithm did not outperform thesuboptimal ASRL on the ROUGE evaluation, us-ing the same reward function, then there is clearlyroom for improvement in the reward function?sability to accurately model values in the statespace.
Furthermore one may expect to achievea performance boost exploiting more recent re-search by utilizing an algorithm that intends toimprove upon the concepts on which TD(?)
isbased.
These provide the motivation for the re-mainder of the research preformed.Query focused multi-document summarization(Li and Li, 2013; Chali and Hasan, 2012b; Yin etal., 2012; Wang et al., 2013) has recently gainedmuch attention due to increasing amounts of tex-tual data, as well as increasingly specific user de-mands for extracting information from said data.This is reflected in the query focused tasks run inthe Document Understanding Conference (DUC)and Text Analysis Conference (TAC) over the pastdecade.
This has motivated us to design and im-plement query focused extensions to these rein-forcement learning approaches to summarization.There has been some research into the effects ofsentence compression on the output of automaticsummarization systems (Chali and Hasan, 2012a),specifically the evaluation results garnered fromcompressing sentences before evaluation (Qianand Liu, 2013; Lin and Rey, 2003; Ryang and682Abekawa, 2012).
However Ryang and Abekawa(2012) found this technique to be ineffective in im-proving ROUGE metrics using a similar reinforce-ment learning approach to this paper, as a resultwe will not perform any further exploration intothe effects of sentence compression.3 Problem DefinitionWe use an identical problem definition to Ryangand Abekawa (2012).
Assume the given clusterof documents is represented as a set of textualunits D = {x1, x2, ?
?
?
, xn} where |D| = n andxirepresents a single textual unit.
Textual unitsfor the purposes of this experiment are the indi-vidual sentences in the document cluster, that isD = D1?D2?
?
?
?
?Dmwhere m is the numberof documents in the cluster and eachDirepresentsa document.The next necessary component is the scorefunction, which is to be used as the reward for thelearner.
The function score(s) can be applied toany s ?
D. s is a summary of the given documentor cluster of documents.Given these parameters, and a length limitationk we can define an optimal summary s?as:s?= argmax score(s)where s ?
D and length(s) ?
k (1)It is the objective of our learner to create a pol-icy that produces the optimal summary for its pro-vided document cluster D. Henceforth the lengthlimitations used for general summarization will be665 bytes, and query focused summarization willuse 250 words.
These limitations on summarylength match those set by the Document Under-standing Conferences associated with the datasetutilized in the respective experiments.4 AlgorithmsTD(?)
and SARSA (Sutton and Barto, 1998) aretemporal difference methods in which the primarydifference is that TD(?)
models state value pre-dictions, and SARSA models state-action valuepredictions.
Approximate Policy Iteration (API)follows a different paradigm by iteratively improv-ing a policy for a markov decision process until thepolicy converges.4.1 TD(?
)In the ASRL implementation of TD(?)
the learn-ing rate ?kand temperature ?kdecay as learningprogresses with the following equations with k setto the number of learning episodes that had takenplace.
?k= 0.001 ?
101/(100 + k1.1) (2)?k= 1.0 ?
0.987k?1(3)One can infer from the decreasing values of ?kthat as the number of elapsed episodes increasesthe learner adjusts itself at a smaller rate.
Simi-larly as the temperature ?kdecreases the action se-lection policy becomes greedier and thus performsless exploration, this is evident in (5) below.Note that unlike traditional TD(?)
implementa-tions the eligibility trace e resets on every episode.The reasons for this will become evident in theexperiments section of the paper in which ?
=1, ?
= 1 and thus there is no decay during anepisode and complete decay after an episode.
Thesame holds true for SARSA below.The action-value estimation Q(s, a) is approxi-mated as:Q(s, a) = r + ?V (s?)
(4)The policy is implemented as such:policy(a|s; ?
; ?)
=eQ(s,a)/??a?AeQ(s,a)/?
(5)Actions are selected probabilistically using soft-max selection (Sutton and Barto, 1998) from aBoltzmann distribution.
As the value of ?
ap-proaches 0 the distribution becomes greedier.4.2 SARSASARSA is implemented in a very similar mannerand shares ?k, ?k, ?
(s), m, and policy(s) with theTD(?)
implementation above.
SARSA is alsoa temporal difference algorithm and thus behavessimilarly to TD(?)
with the exception that valuesare estimated not only on the state s but a state-action pair [s, a].4.3 Approximate Policy IterationThe third algorithm in our experiment uses Ap-proximate Policy Iteration (Lagoudakis and Parr,2003) to implement a reinforcement learner.
The683novelty introduced by (Lagoudakis and Parr,2003) is that they eschew standard representationsfor a policy and instead use a classifier to representthe current policy pi.
Further details on the algo-rithm can be obtained from Lagoudakis and Parr(2003).5 ExperimentsOur state space S is represented simply as a three-tuple [s a f ] in which s is the set of textual units(sentences) that have been added to the summary,a is a sequence of actions that have been per-formed on the summary and f is a boolean withvalue 0 representing non-terminal states and 1 rep-resenting a summary that has been terminated.The individual units in our action space are de-fined as [:insert xi] where xiis a textual unit asdescribed earlier, let us define Dias the set [:in-sert xi] for all xi?
D where D is the documentset.
We also have one additional action [:finish]and thus we can define our action space.A = Di?
{[: finish]} (6)The actions eligible to be executed on any givenstate s is defined by a function actions(A, s):actions(A, s) ={[: finish] if length(s) > kA?
atotherwise(7)The state-action transitions are defined below:[st, at, 0]a=insertxi????????
[st?
xi, at?
a , 0] (8)[st, at, 0]:finish?????
[st, at, 1] (9)[st, at+1, 1]any???
[st, at, 1] (10)Insertion adds both the content of the textualunit xito the set s as well as the action itself toset a. Conversely finishing does not alter s or abut it flips the f bit to on.
Notice from (10) thatonce a state is terminal any further actions have noeffect.5.1 Feature SpaceWe present an alternate feature set calledREAPER feature set based on the ideas presentedin Ryang and Abekawa (2012).
Our proposed fea-ture set follows a similar format to the previousone but depends on the presence of top bigramsinstead of tf ?
idf words.?
One bit b ?
0, 1 for each of the top n bigrams(Manning and Sch?utze, 1999) present in thesummary.?
Coverage ratio calculated as the sum of thebits in the previous feature divided by n.?
Redundancy Ratio calculated as the numberof redundant times a bit in the first feature isflipped on, divided by n.?
Length Ratio calculated as length(s)/kwhere k is the length limit.?
Longest common subsequence length.?
Length violation bit.
Set to 1 if length(s) >kSummaries which exceed the length limitationk are subject to the same reduction as the ASRLfeature set (Ryang and Abekawa, 2012) to an allzero vector with the final bit set to one.5.2 Reward FunctionOur reward function (termed as REAPER reward)is based on the n-gram concurrence score metric,and the longest-common-subsequence recall met-ric contained within ROUGE (Lin, 2004).reward(s) =?????
?1, if length(s) > kscore(s) if s is terminal0 otherwise(11)Where score is defined identically to ASRL,with the exception that Sim is a new equationbased on ROUGE metrics.score(s) =?xi?S?sRel(xi)??xi,xj?S,i<j(1?
?s)Red(xi, xj)(12)Rel(xi) = Sim(xi, D) + Pos(xi)?1(13)Red(xi, xj) = Sim(xi, xj) (14)684Ryang and Abekawa (2012) experimentally de-termined a value of 0.9 for the ?sparameter.
Thatvalue is used herein unless otherwise specified.Sim has been redefined as:Sim(s) =1 ?
ngco(1, D, s)+4 ?
ngco(2, D, s)+1 ?
ngco(3, D, s)+1 ?
ngco(4, D, s)+1 ?
rlcs(D, s)(15)and ngco is the ngram co-occurence score met-ric as defined by Lin (2004).ngco(n,D, s) =?r?D?ngram?rCountmatch(ngram)?Sr?D?ngram?rCount(ngram)(16)Where n is the n-gram count for example 2 forbigrams, D is the set of documents, and s is thesummary in question.
Countmatchis the maxi-mum number of times the ngram occurred in eitherD or s.The rlcs(R,S) is also a recall oriented mea-sure based on longest common subsequence(Hirschberg, 1977).
Recall was selected asDUC2004 tasks favoured a ?
value for F-Measure(Lin, 2004) high enough that only recall wouldbe considered.
lcs is the longest common sub-sequence, and length(D) is the total number oftokens in the reference set D.rlcs(D, s) =lcs(D, s)length(D)(17)We are measuring similarity between sentencesand our entire reference set, and thusly our D isthe set of documents defined in section 3.
Thisis also a delayed reward as the provided reward iszero until the summary is terminal.5.2.1 Query Focused RewardsWe have proposed an extension to both rewardfunctions to allow for query focused (QF) summa-rization.
We define a function score?which aimsto balance the summarization abilities of the re-ward with a preference for selecting textual unitsrelated to the provided query q.
Both ASRL andREAPER score functions have been extended inthe following manner where Sim is the same sim-ilarity functions used in equation (13) and (15).score?
(q, s) = ?Sim(q, s) + (1?
?
)score(s)(18)The parameter ?
is a balancing factor betweenquery similarity and overall summary score inwhich 0 <= ?
<= 1, we used an arbitrarily cho-sen value of 0.9 in these experiments.
In the caseof ASRL the parameter q is the vectorized versionof the query function with tf ?
idf values, and forSim q is a sequence of tokens which make up thequery, stemmed and stop-words removed.5.2.2 Immediate RewardsFinally we also employ immediate versions of thereward functions which behave similarly to theirdelayed counterparts with the exception that thescore is always provided to the caller regardless ofthe terminal status of state s.reward(s) ={?1, if length(s) > kscore(s) otherwise(19)6 ResultsWe first present results2of our experiments, spec-ifying parameters, and withholding discussion un-til the following section.
We establish a bench-mark using ASRL and other top-scoring sum-marization systems compared with REAPER us-ing ROUGE.
For generic multi-document sum-marization we run experiments on all 50 docu-ment clusters, each containing 10 documents, ofDUC2004 task 2 with parameters for REAPERand ASRL fixed at ?
= 1, ?
= 1, and k = 665.Sentences were stemmed using a Porter Stemmer(Porter, 1980) and had the ROUGE stop word setremoved.
All summaries were processed in thismanner and then projected back into their original(unstemmed, with stop-words) state and output todisk.Config R-1 R-2 R-LREAPER 0.40339 0.11397 0.36574ASRL 0.39013 0.09479 0.33769MCKP 0.39033 0.09613 0.34225PEER65 0.38279 0.09217 0.33099ILP 0.34712 0.07528 0.31241GREEDY 0.30618 0.06400 0.27507Table 1: Experimental results with ROUGE-1,ROUGE-2 and ROUGE-L scores on DUC2004.2ROUGE-1.5.5 run with -m -s -p 0685Table 1 presents results for REAPER, ASRL(Ryang and Abekawa, 2012), MCKP (Takamuraand Okumura, 2009), PEER65 (Conroy et al.,2004) , and GREEDY (Ryang and Abekawa,2012) algorithms on the same task.
This allowsus to make a direct comparison with the results ofRyang and Abekawa (2012).REAPER results are shown using the TD(?
)algorithm, REAPER reward function, and ASRLfeature set.
This is to establish the validity of thereward function holding all other factors constant.REAPER results for ROUGE-1, ROUGE-2 andROUGE-L are statistically significant compared tothe result set presented in Table 2 of Ryang andAbekawa (2012) using p < 0.01.Run R-1 R-2 R-LREAPER 0 0.39536 0.10679 0.35654REAPER 1 0.40176 0.11048 0.36450REAPER 2 0.39272 0.11171 0.35766REAPER 3 0.39505 0.11021 0.35972REAPER 4 0.40259 0.11396 0.36539REAPER 5 0.40184 0.11306 0.36391REAPER 6 0.39311 0.10873 0.35481REAPER 7 0.39814 0.11001 0.35786REAPER 8 0.39443 0.10740 0.35586REAPER 9 0.40233 0.11397 0.36483Average 0.39773 0.11063 0.36018Table 2: REAPER run 10 times on the DUC2004.We present the results of 10 runs of REAPER,with REAPER feature set.. As with ASRL,REAPER does not converge on a stable solutionwhich is attributable to the random elements ofTD(?).
Results in all three metrics are againstatistically significant compared to ASRL resultspresented in the Ryang and Abekawa (2012) pa-per.
All further REAPER experiments use the bi-gram oriented feature space.Reward R-1 R-2 R-LDelayed 0.39773 0.11397 0.36018Immediate 0.32981 0.07709 0.30003Table 3: REAPER with delayed and immediate re-wards on DUC2004.Table 3 shows the performance difference ofREAPER when using a delayed and immediate re-ward.
The immediate version of REAPER pro-vides feedback on every learning step, unlike thedelayed version which only provides score at theend of the episode.Features R-1 R-2 R-LASRL 0.40339 0.11397 0.36574REAPER 0.40259 0.11396 0.36539Table 4: REAPER with alternate feature spaces onDUC2004.We can observe the results of using REAPERwith various feature sets in Table 4.
Experimentswere run using REAPER reward, TD(?
), and thespecified feature set.Algorithm R-1 R-2 R-LTD(?)
0.39773 0.11063 0.36018SARSA 0.28287 0.04858 0.26172API 0.29163 0.06570 0.26542Table 5: REAPER with alternate algorithms onDUC2004.Table 5 displays the performance of REAPERwith alternate algorithms.
TD(?)
and SARSAare run using the delayed reward feature, whileAPI requires an immediate reward and was thusrun with the immediate reward.System R-2 R-SU4REAPER 0.07008 0.11689ASRL 0.05639 0.09379S24 0.09505 0.15464Baseline 0.04947 0.09788Table 6: QF-REAPER on DUC2006.For query-focused multi-document summariza-tion we experimented with the DUC2006 systemtask, which contained 50 document clusters con-sisting of 25 documents each.
Parameters werefixed to ?
= 1, ?
= 1 and k = 250 words.
InTable 6 we can observe the results3of our queryfocused systems against DUC2006?s top scorer(S24) for ROUGE-2, and a baseline.
The baselinewas generated by taking the most recent documentin the cluster and outputting the first 250 words.Human Evaluations: We had three nativeEnglish-speaking human annotators evaluate a setof four randomly chosen summaries produced byREAPER on the DUC2004 dataset.3ROUGE-1.5.5 run with -n 2 -x -m -2 4 -u -c 95 -r 1000-f A -p 0.5 -t 0 -l 250686Metric A1 A2 A3 AVGGrammaticality 3.00 4.00 4.00 3.67Redundancy 4.75 4.25 2.75 3.92Referential Clarity 4.00 4.50 3.50 4.00Focus 4.50 3.50 2.25 3.42Structure 3.50 4.00 3.00 3.50Responsiveness 4.25 3.75 3.00 3.67Table 7: Human evaluation scores on DUC2004.Table 7 shows the evaluation results accord-ing to the DUC2006 human evaluation guidelines.The first five metrics are related entirely to the lin-guistic quality of the summary in question and thefinal metric, Responsiveness, rates the summaryon its relevance to the source documents.
Columnsrepresent the average provided by a given annota-tor over the four summaries, and the AVG columnrepresents the average score for all three annota-tors over all four summaries.
Score values are aninteger between 1 and 5 inclusive.7 DiscussionFirst we present a sample of a summary generatedfrom a randomly selected cluster.
The followingsummary was generated from cluster D30017 ofthe DUC 2004 dataset using REAPER reward withTD(?)
and REAPER feature space.A congressman who visited remote parts ofNorth Korea last week said Saturday that the foodand health situation there was desperate and de-teriorating, and that millions of North Koreansmight have starved to death in the last few years.North Korea is entering its fourth winter of chronicfood shortages with its people malnourished andat risk of dying from normally curable illnesses,senior Red Cross officials said Tuesday.
More thanfive years of severe food shortages and a near-totalbreakdown in the public health system have led todevastating malnutrition in North Korea and prob-ably left an entire generation of children physi-cally and mentally impaired, a new study by in-ternational aid groups has found.
Years of foodshortages have stunted the growth of millions ofNorth Korean children, with two-thirds of childrenunder age seven suffering malnourishment, U.N.experts said Wednesday.
The founder of South Ko-rea?s largest conglomerate plans to visit his nativeNorth Korea again next week with a gift of 501cattle, company officials said Thursday.
?There isenough rice.We can observe that the summary is both syn-tactically sound, and elegantly summarizes thesource documents.Our baseline results table (Table 1) showsREAPER outperforming ASRL in a statisticallysignificant manner on all three ROUGE metricsin question.
However we can see from the abso-lute differences in score that very few additionalimportant words were extracted (ROUGE-1) how-ever REAPER showed a significant improvementin the structuring and ordering of those words(ROUGE-2, and ROUGE-L).The balancing factors used in the REAPER re-ward function are responsible for the behaviour ofthe reward function, and thus largely responsiblefor the behaviour of the reinforcement learner.
Inequation 15 we can see balance numbers of 1, 4, 1,1, 1 for 1-grams, 2-grams, 3-grams, 4-grams, andLCS respectively.
In adjusting these values a usercan express a preference for a single metric or aspecific mixture of these metrics.
Given that themagnitude of scores for n-grams decrease as n in-creases and given that the magnitude of scores for1-grams is generally 3 to 4 times larger, in our ex-perience, we can see that this specific reward func-tion favours bigram similarity over unigram simi-larity.
These balance values can be adjusted to suitthe specific needs of a given situation, however weleave exploration of this concept for future work.We can observe in Figure 1 that ASRL does notconverge on a stable value, and dips towards the300thepisode while in Figure 2 REAPER doesnot take nearly such a dramatic dip.
These fig-ures display average normalized reward for all 50document clusters on a single run.
Furthermorewe can observe that ASRL reaches it?s peak re-ward around episode 225 while REAPER does soaround episode 175 suggesting that REAPER con-verges faster.7.1 Delayed vs.
Immediate RewardsThe delayed vs. immediate rewards results in Ta-ble 3 clearly show that delaying the reward pro-vides a significant improvement in globally opti-mizing the summary for ROUGE score.
This canbe attributed to the ?
= 1 and ?
= 1 parame-ter values being suboptimal for the immediate re-ward situation.
This has the added benefit of be-ing much more performant computationally as farfewer reward calculations need be done.687Figure 1: ASRL normalized reward.7.2 Feature SpaceThe feature space experiments in Table 4 seem toimply that REAPER performs similarly with bothfeature sets.
We are confident that an improve-ment could be made through further experimenta-tion.
Feature engineering, however, is a very broadfield and we plan to pursue this topic in depth inthe future.7.3 AlgorithmsTD(?)
significantly outperformed both SARSAand API in the algorithm comparison.
Ryang andAbekawa (2012) conclude that the feature space islargely responsible for the algorithm performance.This is due to the fact that poor states such as thosethat are too long, or those that contain few impor-tant words will reduce to the same feature set andreceive negative rewards collectively.
SARSAloses this benefit as a result of its modelling ofstate-action pairs.API on the other hand may have suffered a per-formance loss due to its requirements of an imme-diate reward, this is because when using a delayedreward if the trajectory of a rollout does not reacha terminal state the algorithm will not be able tomake any estimations about the value of the statein question.
We propose altering the policy iter-ation algorithm to use a trajectory length of oneepisode instead of a fixed number of actions in or-der to counter the need for an immediate rewardfunction.7.4 Query Focused RewardsFrom the ROUGE results in Table 6 we can inferthat while REAPER outperformed ASRL on thequery focused task, however it is notable that bothFigure 2: REAPER normalized reward.systems under performed when compared to thetop system from the DUC2006 conference.We can gather from these results that it is notenough to simply naively calculate similarity withthe provided query in order to produce a query-focused result.
Given that the results produced bythe generic summarization task is rather accept-able according to our human evaluations we sug-gest that further research be focused on a propersimilarity metric between the query and summaryto improve the reward function?s overall ability toscore summaries in a query-focused setting.8 Conclusion and Future WorkWe have explored alternate reward functions, fea-ture sets, and algorithms for the task of automaticsummarization using reinforcement learning.
Wehave shown that REAPER outperforms ASRL onboth generic summarization and the query focusedtasks.
This suggests the effectiveness of our re-ward function and feature space.
Our results alsoconfirm that TD(?)
performs best for this taskcompared to SARSA and API .Due to the acceptable human evaluation scoreson the general summarization task it is clear thatthe algorithm produces acceptable summaries ofnewswire data.
Given that we have a frameworkfor generating general summaries, and the cur-rent popularity of the query-focused summariza-tion task, we propose that the bulk of future workin this area be focused on the query-focused taskspecifically in assessing the relevance of a sum-mary to a provided query.
Therefore we intend topursue future research in utilizing word-sense dis-ambiguation and synonyms, as well as other tech-niques for furthering REAPER?s query similarity688metrics in order to improve its ROUGE and humanevaluation scores on query-focused tasks.AcknowledgmentsWe would like to thank the anonymous review-ers for their useful comments.
The research re-ported in this paper was supported by the Nat-ural Sciences and Engineering Research Council(NSERC) of Canada - discovery grant and the Uni-versity of Lethbridge.
This work was done whenthe second author was at the University of Leth-bridge.ReferencesS .
Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzi-lay.
2009.
Reinforcement Learning for MappingInstructions to Actions.
In Proceedings of the 47thAnnual Meeting of the ACL and the 4th IJCNLP ofthe AFNLP, pages 82?90.Y.
Chali and S. A. Hasan.
2012a.
On the Effectivenessof Using Sentence Compression Models for Query-Focused Multi-Document Summarization.
In Pro-ceedings of the 24th International Conference onComputational Linguistics (COLING 2012), pages457?474.
Mumbai, India.Y.
Chali and S. A. Hasan.
2012b.
Query-focused Multi-document Summarization: Auto-matic Data Annotations and Supervised LearningApproaches.
Journal of Natural Language Engi-neering, 18(1):109?145.Y.
Chali, S. A. Hasan, and S. R. Joty.
2009a.
Do Auto-matic Annotation Techniques Have Any Impact onSupervised Complex Question Answering?
Pro-ceedings of the Joint conference of the 47th AnnualMeeting of the Association for Computational Lin-guistics (ACL-IJCNLP 2009), pages 329?332.Y.
Chali, S. R. Joty, and S. A. Hasan.
2009b.
Com-plex Question Answering: Unsupervised LearningApproaches and Experiments.
Journal of ArtificialIntelligence Research, 35:1?47.Y.
Chali, S. A. Hasan, and K. Imam.
2011a.
AReinforcement Learning Framework for AnsweringComplex Questions.
In Proceedings of the 16thInternational Conference on Intelligent User Inter-faces, pages 307?310.
ACM, Palo Alto, CA, USA.Y.
Chali, S. A. Hasan, and S. R. Joty.
2011b.
Improv-ing Graph-based Random Walks for Complex Ques-tion Answering Using Syntactic, Shallow Semanticand Extended String Subsequence Kernels.
Infor-mation Processing and Management (IPM), SpecialIssue on Question Answering, 47(6):843?855.Y.
Chali, S. A. Hasan, and K. Imam.
2012.
Improv-ing the Performance of the Reinforcement Learn-ing Model for Answering Complex Questions.
InProceedings of the 21st ACM Conference on Infor-mation and Knowledge Management (CIKM 2012),pages 2499?2502.
ACM, Maui, Hawaii, USA.J.
Conroy, J. Goldstein, and D. Leary.
2004.
Left-Brain/ Right-Brain Multi-Document Summarization.
InProceedings of the Document Un- derstanding Con-ference (DUC 2004).D.
S. Hirschberg.
1977.
Algorithms for the LongestCommon Subsequence Problem.
In Journal of theACM, 24(4):664?675, October.M.
Lagoudakis and R. Parr.
2003.
Reinforcementlearning as classification: Leveraging modern classi-fiers.
In Proceedings of the Twentieth InternationalConference on Machine Learning, 20(1):424.J.
Li and S. Li.
2013.
A Novel Feature-based BayesianModel for Query Focused Multi-document Summa-rization.
In Transactions of the Association forComputational Linguistics, 1:89?98.C.
Lin and M. Rey.
2003.
Improving Summariza-tion Performance by Sentence Compression A Pi-lot Study.
In Proceedings of the Sixth InternationalWorkshop on Information Retrieval with Asian Lan-guages.C.
Lin.
2004.
ROUGE : A Package for AutomaticEvaluation of Summaries.
In Information Sciences,16(1):25?26.Y.
Liu, S. Li, Y. Cao, C. Lin, D. Han, and Y. Yu.2008.
Understanding and Summarizing Answersin Community-Based Question Answering Services.In COLING ?08 Proceedings of the 22nd Inter-national Conference on Computational Linguistics,1(August):497?504.S.
Lyngbaek.
2013.
SPORK: A SummarizationPipeline for Online Repositories of Knowledge.M.sc.
thesis, California Polytechnic State Univer-sity.I.
Mani.
2001.
Automatic Summarization.
John Ben-jamins Publishing.C.
D. Manning and H. Sch?utze.
1999.
Foundations ofStatistical Natural Language Processing, volume 26of .
MIT Press.M.
F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.X.
Qian and Y. Liu.
2013.
Fast Joint Compression andSummarization via Graph Cuts.
In Conference onEmpirical Methods in Natural Language Process-ing.S.
Ryang and T. Abekawa.
2012.
Framework of Au-tomatic Text Summarization Using ReinforcementLearning.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, 1:256?265.689H.
Saggion, C. Iria, T. M. Juan-Manuel, and E. San-Juan.
2010.
Multilingual Summarization Evalua-tion without Human Models.
In COLING ?10 Pro-ceedings of the 23rd International Conference onComputational Linguistics: Posters, 1:1059?1067.A.
Sood.
2013.
Towards Summarization of WrittenText Conversations.
M.sc.
thesis, International Insti-tute of Information Technology, Hyderabad, India.R.
S. Sutton and A. G. Barto.
1998.
ReinforcementLearning: An Introduction.
MIT Press.C.
A. Szepesv.
2009.
Algorithms for ReinforcementLearning.
Morgan & Claypool Publishers.H.
Takamura and M. Okumura.
2009.
Text Summa-rization Model based on Maximum Coverage Prob-lem and its Variant.
In EACL ?09 Proceedingsof the 12th Conference of the European Chapterof the Association for Computational Linguistics,(April):781?789.X Wan.
2007.
Towards an Iterative Reinforcement Ap-proach for Simultaneous Document Summarizationand Keyword Extraction.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics, pages 552?559.L.
Wang, H. Raghavan, V. Castelli, R. Florian, andC.
Cardie.
2013.
A Sentence Compression BasedFramework to Query-Focused.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics, 1:1384?1394.W.
Yin, Y. Pei, F. Zhang, and L. Huang.
2012.
Query-focused multi-document summarization based onquery-sensitive feature space.
In Proceedings of the21st ACM international conference on Informationand knowledge management - CIKM ?12, page 1652.690
