NAACL HLT Demonstration Program, pages 5?6,Rochester, New York, USA, April 2007. c?2007 Association for Computational LinguisticsAdaptive Tutorial Dialogue Systems Using Deep NLP TechniquesMyroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow,Manuel Marques-Pita, Colin Matheson and Johanna D. MooreICCS-HCRC, School of InformaticsUniversity of EdinburghEdinburgh, EH8 9LW, United Kingdom(mdzikovs,ccallawa,efarrow,mmpita,colin,jmoore)@inf.ed.ac.uk ?AbstractWe present tutorial dialogue systems intwo different domains that demonstratethe use of dialogue management and deepnatural language processing techniques.Generation techniques are used to producenatural sounding feedback adapted to stu-dent performance and the dialogue his-tory, and context is used to interpret ten-tative answers phrased as questions.1 IntroductionIntelligent tutoring systems help students improvelearning compared to reading textbooks, though notquite as much as human tutors (Anderson et al,1995).
The specific properties of human-human di-alogue that help students learn are still being stud-ied, but the proposed features important for learn-ing include allowing students to explain their actions(Chi et al, 1994), adapting tutorial feedback to thelearner?s level, and engagement/affect.
Some tuto-rial dialogue systems use NLP techniques to analyzestudent responses to ?why?
questions.
(Aleven et al,2001; Jordan et al, 2006).
However, for remediationthey revert to scripted dialogue, relying on short-answer questions and canned feedback.
The result-ing dialogue may be redundant in ways detrimentalto student understanding (Jordan et al, 2005) andallows for only limited adaptivity (Jordan, 2004).
?This work was supported under the 6th Framework Pro-gramme of the European Commission, Ref.
IST-507826, andby a grant from The Office of Naval Research N000149910165.We demonstrate two tutorial dialogue systemsthat use techniques from task-oriented dialogue sys-tems to improve the interaction.
The systems arebuilt using the Information State Update approach(Larsson and Traum, 2000) for dialogue manage-ment and generic components for deep natural lan-guage understanding and generation.
Tutorial feed-back is generated adaptively based on the studentmodel, and the interpretation is used to processexplanations and to differentiate between studentqueries and hedged answers phrased as questions.The systems are intended for testing hypothesesabout tutoring.
By comparing student learning gainsbetween versions of the same system using differenttutoring strategies, as well as between the systemsand human tutors, we can test hypotheses about therole of factors such as free natural language input,adaptivity and student affect.2 The BEEDIFF TutorThe BEEDIFF tutor helps students solve symbolicdifferentiation problems, a procedural task.
Solu-tion graphs generated by a domain reasoner are usedto interpret student actions and to generate feed-back.1 Student input is relatively limited and con-sists mostly of mathematical formulas, but the sys-tem generates adaptive feedback based on the notionof student performance and on the dialogue history.For example, if an average student asks for a hinton differentiating sin(x2), the first level of feedbackmay be ?Think about which rule to apply?, which1Solution graphs are generated automatically for arbitraryexpressions, with no limit on the complexity of expressions ex-cept for possible efficiency considerations.5can then be specialized to ?Use the chain rule?
andthen to giving away the complete answer.
For stu-dents with low performance, more specific feed-back can be given from the start.
The same strat-egy (based on an initial corpus analysis) is used inproducing feedback after incorrect answers, and weintend to use the system to evaluate its effectiveness.The feedback is generated automatically from asingle diagnosis and generation techniques are usedto produce appropriate discourse cues.
For example,when a student repeats the same mistake, the feed-back may be ?You?ve differentiated the inner layercorrectly, but you?re still missing the minus sign?.The two clauses are joined by a contrast relationship,and the second indicates that an error was repeatedby using the adverbial ?still?.3 The BEETLE TutorThe BEETLE tutor is designed to teach students ba-sic electricity and electronics concepts.
Unlike theBEEDIFF tutor, the BEETLE tutor is built arounda pre-planned course where the students alternatereading with exercises involving answering ?why?questions and interacting with a circuit simulator.Since this is a conceptual domain, for most exer-cises there is no structured sequence of steps that thestudents should follow, but students need to name acorrect set of objects and relationships in their re-sponse.
We model the process of building an answerto an exercise as co-constructing a solution, wherethe student and tutor may contribute parts of the an-swer.
For example, consider the question ?For eachcircuit, which components are in a closed path?.The solution can be built up gradually, with the stu-dent naming different components, and the systemproviding feedback until the list is complete.
Thisgeneric process of gradually building up a solution isalso applied to giving explanations.
For example, inanswer to the question ?What is required for a lightbulb to light?
the student may say ?The bulb must bein a closed path?, which is correct but not complete.The system may then say ?Correct, but is that every-thing??
to prompt the student towards mentioningthe battery as well.
The diagnosis of the student an-swer is represented as a set of correctly given objectsor relationships, incorrect parts, and objects and re-lationships that have yet to be mentioned, and thesystem uses the same dialogue strategy of elicitingthe missing parts for all types of questions.Students often phrase their answers tentatively,for example ?Is the bulb in a closed path??.
In thecontext of a tutor question the interpretation processtreats yes-no questions from the student as poten-tially hedged answers.
The dialogue manager at-tempts to match the objects and relationships in thestudent input with those in the question.
If a closematch can be found, then the student utterance isinterpreted as giving an answer rather than a truequery.
In contrast, if the student said ?Is the bulbconnected to the battery?
?, this would be interpretedas a proper query and the system would attempt toanswer it.Conclusion We demonstrate two tutorial dialoguesystems in different domains built by adapting di-alogue techniques from task-oriented dialogue sys-tems.
Improved interpretation and generation helpsupport adaptivity and a wider range of inputs thanpossible in scripted dialogue.ReferencesV.
Aleven, O. Popescu, and K. R. Koedinger.
2001.Towards tutorial dialog to support self-explanation:Adding natural language understanding to a cognitivetutor.
In Proc.
AI-ED 2001.J.
R. Anderson, A. T. Corbett, K. R. Koedinger, andR.
Pelletier.
1995.
Cognitive tutors: Lessons learned.The Journal of the Learning Sciences, 4(2):167?207.M.
T. H. Chi, N. de Leeuw, M.-H. Chiu, and C. La-Vancher.
1994.
Eliciting self-explanations improvesunderstanding.
Cognitive Science, 18(3):439?477.P.
Jordan, P. Albacete, and K. VanLehn.
2005.
Takingcontrol of redundancy in scripted tutorial dialogue.
InProc.
of AIED2005, pages 314?321.P.
Jordan, M. Makatchev, U. Pappuswamy, K. VanLehn,and P. Albacete.
2006.
A natural language tutorialdialogue system for physics.
In Proc.
of FLAIRS-06.P.
W. Jordan.
2004.
Using student explanations as mod-els for adapting tutorial dialogue.
In V. Barr andZ.
Markov, editors, FLAIRS Conference.
AAAI Press.S.
Larsson and D. Traum.
2000.
Information state anddialogue management in the TRINDI Dialogue MoveEngine Toolkit.
Natural Language Engineering, 6(3-4):323?340.6
