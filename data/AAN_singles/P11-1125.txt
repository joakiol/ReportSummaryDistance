Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1249?1257,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsMachine Translation System Combination by Confusion ForestTaro Watanabe and Eiichiro SumitaNational Institute of Information and Communications Technology3-5 Hikaridai, Keihanna Science City, 619-0289 JAPAN{taro.watanabe,eiichiro.sumita}@nict.go.jpAbstractThe state-of-the-art system combinationmethod for machine translation (MT) isbased on confusion networks constructedby aligning hypotheses with regard to wordsimilarities.
We introduce a novel systemcombination framework in which hypothesesare encoded as a confusion forest, a packedforest representing alternative trees.
Theforest is generated using syntactic consensusamong parsed hypotheses: First, MT outputsare parsed.
Second, a context free grammar islearned by extracting a set of rules that con-stitute the parse trees.
Third, a packed forestis generated starting from the root symbol ofthe extracted grammar through non-terminalrewriting.
The new hypothesis is producedby searching the best derivation in the forest.Experimental results on the WMT10 systemcombination shared task yield comparableperformance to the conventional confusionnetwork based method with smaller space.1 IntroductionSystem combination techniques take the advantagesof consensus among multiple systems and have beenwidely used in fields, such as speech recognition(Fiscus, 1997; Mangu et al, 2000) or parsing (Hen-derson and Brill, 1999).
One of the state-of-the-artsystem combination methods for MT is based onconfusion networks, which are compact graph-basedstructures representing multiple hypotheses (Banga-lore et al, 2001).Confusion networks are constructed based onstring similarity information.
First, one skeleton orbackbone sentence is selected.
Then, other hypothe-ses are aligned against the skeleton, forming a latticewith each arc representing alternative word candi-dates.
The alignment method is either model-based(Matusov et al, 2006; He et al, 2008) in which astatistical word aligner is used to compute hypothe-sis alignment, or edit-based (Jayaraman and Lavie,2005; Sim et al, 2007) in which alignment is mea-sured by an evaluation metric, such as translation er-ror rate (TER) (Snover et al, 2006).
The new trans-lation hypothesis is generated by selecting the bestpath through the network.We present a novel method for system combina-tion which exploits the syntactic similarity of systemoutputs.
Instead of constructing a string-based con-fusion network, we generate a packed forest (Billotand Lang, 1989; Mi et al, 2008) which encodes ex-ponentially many parse trees in a polynomial space.The packed forest, or confusion forest, is constructedby merging the MT outputs with regard to theirsyntactic consensus.
We employ a grammar-basedmethod to generate the confusion forest: First, sys-tem outputs are parsed.
Second, a set of rules areextracted from the parse trees.
Third, a packed for-est is generated using a variant of Earley?s algorithm(Earley, 1970) starting from the unique root symbol.New hypotheses are selected by searching the bestderivation in the forest.
The grammar, a set of rules,is limited to those found in the parse trees.
Spuri-ous ambiguity during the generation step is furtherreduced by encoding the tree local contextual infor-mation in each non-terminal symbol, such as parentand sibling labels, using the state representation inEarley?s algorithm.1249Experiments were carried out for the systemcombination task of the fifth workshop on sta-tistical machine translation (WMT10) in four di-rections, {Czech, French, German, Spanish}-to-English (Callison-Burch et al, 2010), and we foundcomparable performance to the conventional con-fusion network based system combination in twolanguage pairs, and statistically significant improve-ments in the others.First, we will review the state-of-the-art methodwhich is a system combination framework based onconfusion networks (?2).
Then, we will introducea novel system combination method based on con-fusion forest (?3) and present related work in con-sensus translations (?4).
Experiments are presentedin Section 5 followed by discussion and our conclu-sion.2 Combination by Confusion NetworkThe system combination framework based on confu-sion network starts from computing pairwise align-ment between hypotheses by taking one hypothe-sis as a reference.
Matusov et al (2006) employsa model based approach in which a statistical wordaligner, such as GIZA++ (Och and Ney, 2003), isused to align the hypotheses.
Sim et al (2007) in-troduced TER (Snover et al, 2006) to measure theedit-based alignment.Then, one hypothesis is selected, for example byemploying a minimum Bayes risk criterion (Sim etal., 2007), as a skeleton, or a backbone, which servesas a building block for aligning the rest of the hy-potheses.
Other hypotheses are aligned against theskeleton using the pairwise alignment.
Figure 1(b)illustrates an example of a confusion network con-structed from the four hypotheses in Figure 1(a), as-suming the first hypothesis is selected as our skele-ton.
The network consists of several arcs, each ofwhich represents an alternative word at that position,including the empty symbol, ?.This pairwise alignment strategy is prone to spu-rious insertions and repetitions due to alignment er-rors such as in Figure 1(a) in which ?green?
in thethird hypothesis is aligned with ?forest?
in the skele-ton.
Rosti et al (2008) introduces an incrementalmethod so that hypotheses are aligned incremen-tally to the growing confusion network, not only the....* ..I ..saw ..the .
..forest .
..
..I ..walked ..the ..blue ..forest .
..
..I ..saw ..the .
..green ..trees .. .
.
..the .
..forest ..was ..found(a) Pairwise alignment using the first starred hypothesis as askeleton.. .
.
.
.
.
.
..I.?.saw.?.walked.the.blue.?.forest.green.trees.?.was.found.?
(b) Confusion network from (a).
.
.
.
.
.
.
..I.?.saw.?.walked.the.blue.green.forest.trees.was.?.found.?
(c) Incrementally constructed confusion networkFigure 1: An example confusion network construc-tionskeleton hypothesis.
In our example, ?green trees?is aligned with ?blue forest?
in Figure 1(c).The confusion network construction is largely in-fluenced by the skeleton selection, which determinesthe global word reordering of a new hypothesis.
Forexample, the last hypothesis in Figure 1(a) has a pas-sive voice grammatical construction while the othersare active voice.
This large grammatical differencemay produce a longer sentence with spuriously in-serted words, as in ?I saw the blue trees was found?in Figure 1(c).
Rosti et al (2007b) partially re-solved the problem by constructing a large networkin which each hypothesis was treated as a skeletonand the multiple networks were merged into a singlenetwork.3 Combination by Confusion ForestThe confusion network approach to system com-bination encodes multiple hypotheses into a com-pact lattice structure by using word-level consensus.Likewise, we propose to encode multiple hypothe-ses into a confusion forest, which is a packed forestwhich represents multiple parse trees in a polyno-mial space (Billot and Lang, 1989; Mi et al, 2008)Syntactic consensus is realized by sharing tree frag-1250...PRP.
..I ...NP@1..DT..the ...NN..forest...VBD@3.
..was...VP@4..VBN..found...VBD@2.1..walked .
..saw .
..NP@2.2...DT..the ...JJ.. .blue .
..green...NN..forest .
..trees...DT@2.2.1.
..the ...NN@2.2.2.
..forest.
..VP@2.
..S@?Figure 2: An example packed forest representing hy-potheses in Figure 1(a).ments among parse trees.
The forest is representedas a hypergraph which is exploited in parsing (Kleinand Manning, 2001; Huang and Chiang, 2005) andmachine translation (Chiang, 2007; Huang and Chi-ang, 2007).More formally, a hypergraph is a pair ?V,E?where V is the set of nodes and E is the set of hy-peredges.
Each node in V is represented as X@pwhere X ?
N is a non-terminal symbol and pis an address (Shieber et al, 1995) that encapsu-lates each node id relative to its parent.
The rootnode is given the address ?
and the address of thefirst child of node p is given p.1.
Each hyperedgee ?
E is represented as a pair ?head(e), tails(e)?where head(e) ?
V is a head node and tails(e) ?V ?
is a list of tail nodes, corresponding to theleft-hand side and the right-hand side of an in-stance of a rule in a CFG, respectively.
Figure 2presents an example packed forest for the parsedhypotheses in Figure 1(a).
For example, VP@2has two hyperedges, ?VP@2,(VBD@3,VP@4)?
and?VP@2,(VBD@2.1,NP@2.2)?, leading to differentderivations where the former takes the grammaticalconstruction in passive voice while the latter in ac-tive voice.Given system outputs, we employ the followinggrammar based approach for constructing a confu-sion forest: First, MT outputs are parsed.
Second,Initialization:[TOP ?
?S, 0] : 1?Scan:[X ?
?
?
x?, h] : u[X ?
?x ?
?, h] : uPredict:[X ?
?
?
Y?, h][Y ?
?
?, h + 1] : uY u?
?
?
G, h < HComplete:[X ?
?
?
Y?, h] : u [Y ?
?
?, h + 1] : v[X ?
?Y ?
?, h] : u?
vGoal:[TOP ?
S?, 0]Figure 3: The deductive system for Earley?s genera-tion algorithma grammar is learned by treating each hyperedge asan instance of a CFG rule.
Third, a forest is gen-erated from the unique root symbol of the extractedgrammar through non-terminal rewriting.3.1 Forest GenerationGiven the extracted grammar, we apply a variant ofEarley?s algorithm (Earley, 1970) which can gener-ate strings in a left-to-right manner from the uniqueroot symbol, TOP.
Figure 3 presents the deductiveinference rules (Goodman, 1999) for our generationalgorithm.
We use capital letters X ?
N to denotenon-terminals and x ?
T for terminals.
LowercaseGreek letters ?, ?
and ?
are strings of terminals andnon-terminals (T ?
N )?.
u and v are weights asso-ciated with each item.The major difference compared to Earley?s pars-ing algorithm is that we ignore the terminal span in-formation each non-terminal covers and keep trackof the height of derivations by h. The scanningstep will always succeed by moving the dot to theright.
Combined with the prediction and completionsteps, our algorithm may potentially generate a spu-riously deep forest.
Thus, the height of the forest isconstrained in the prediction step not to exceed H ,which is empirically set to 1.5 times the maximum1251height of the parsed system outputs.3.2 Tree AnnotationThe grammar compiled from the parsed trees is lo-cal in that it can represent a finite number of sen-tences translated from a specific input sentence.
Al-though its coverage is limited, our generation algo-rithm may yield a spuriously large forest.
As a wayto reduce spurious ambiguities, we relabel the non-terminal symbols assigned to each parse tree beforeextracting rules.Here, we replace each non-terminal symbol bythe state representation of Earley?s algorithm corre-sponding to the sequence of prediction steps startingfrom TOP.
Figure 4(a) presents an example parsetree with each symbol replaced by the Earley?s statein Figure 4(b).
For example, the label for VBD isreplaced by ?S + NP : ?VP + ?VBD : NP whichcorresponds to the prediction steps of TOP ?
?S,S ?
NP ?
VP and VP ?
?VBD NP.
The contextrepresented in the Earley?s state is further limited bythe vertical and horizontal Markovization (Klein andManning, 2003).
We define the vertical order v inwhich the label is limited to memorize only v pre-vious prediction steps.
For instance, setting v = 1yields NP : ?VP + ?VBD : NP in our example.Likewise, we introduce the horizontal order h whichlimits the number of sibling labels memorized on theleft and the right of the dotted label.
Limiting h = 1implies that each deductive step is encoded with atmost three symbols.No limits in the horizontal and verticalMarkovization orders implies memorizing ofall the deductions and yields a confusion forestrepresenting the union of parse trees through thegrammar collection and the generation processes.More relaxed horizontal orders allow more reorder-ing of subtrees in a confusion forest by discardingthe sibling context in each prediction step.
Like-wise, constraining the vertical order generates adeeper forest by ignoring the sequence of symbolsleading to a particular node.3.3 Forest RescoringFrom the packed forest F , new k-best derivationsare extracted from all possible derivations D byefficient forest-based algorithms for k-best parsing(Huang and Chiang, 2005).
We use a linear combi-...S...NP..PRP.
..I...VP...VBD.
..saw...NP..DT..the ...NN..forest(a) A parse tree for ?I saw the forest?...?S...
?S+ ?
NP : VP..?S+ ?
NP : VP+ ?
PRP...I...
?S+NP : ?VP...?S+NP : ?VP+ ?
VBD : NP...saw...?S+NP : ?VP+VBD : ?NP..?S+NP : ?VP+VBD : ?NP+ ?
DT : NN...the ...?S+NP : ?VP+VBD : ?NP+DT : ?NN...forest(b) Earley?s state annotated tree for (a).
The sub-labels in bold-face indicate the original labels.Figure 4: Label annotation by Earley?s alsogirhtmstatenation of features as our objective function to seekfor the best derivation d?:d?
= argmaxd?Dw?
?
h(d, F ) (1)where h(d, F ) is a set of feature functions scaledby weight vector w. We use cube-pruning (Chiang,2007; Huang and Chiang, 2007) to approximatelyintersect with non-local features, such as n-gramlanguage models.
Then, k-best derivations are ex-tracted from the rescored forest using algorithm 3 ofHuang and Chiang (2005).4 Related WorkConsensus translations have been extensively stud-ied with many granularities.
One of the simplestforms is a sentence-based combination in whichhypotheses are simply reranked without merging(Nomoto, 2004).
Frederking and Nirenburg (1994)1252proposed a phrasal combination by merging hy-potheses in a chart structure, while others dependedon confusion networks, or similar structures, as abuilding block for merging hypotheses at the wordlevel (Bangalore et al, 2001; Matusov et al, 2006;He et al, 2008; Jayaraman and Lavie, 2005; Simet al, 2007).
Our work is the first to explicitly ex-ploit syntactic similarity for system combination bymerging hypotheses into a syntactic packed forest.The confusion forest approach may suffer from pars-ing errors such as the confusion network construc-tion influenced by alignment errors.
Even with pars-ing errors, we can still take a tree fragment-levelconsensus as long as a parser is consistent in thatsimilar syntactic mistakes would be made for simi-lar hypotheses.Rosti et al (2007a) describe a re-generation ap-proach to consensus translation in which a phrasaltranslation table is constructed from the MT outputsaligned with an input source sentence.
New transla-tions are generated by decoding the source sentenceagain using the newly extracted phrase table.
Ourgrammar-based approach can be regarded as a re-generation approach in which an off-the-shelf mono-lingual parser, instead of a word aligner, is used toannotate syntactic information to each hypothesis,then, a new translation is generated from the mergedforest, not from the input source sentence throughdecoding.
In terms of generation, our approach isan instance of statistical generation (Langkilde andKnight, 1998; Langkilde, 2000).
Instead of gener-ating forests from semantic representations (Langk-ilde, 2000), we generate forests from a CFG encod-ing the consensus among parsed hypotheses.Liu et al (2009) present joint decoding in whicha translation forest is constructed from two distinctMT systems, tree-to-string and string-to-string, bymerging forest outputs.
Their merging method is ei-ther translation-level in which no new translation isgenerated, or derivation-level in that the rules shar-ing the same left-hand-side are used in both sys-tems.
While our work is similar in that a new forestis constructed by sharing rules among systems, al-though their work involves no consensus translationand requires structures internal to each system suchas model combinations (DeNero et al, 2010).cz-en de-en es-en fr-en# of systems 6 16 8 14avg.
words tune 10.6K 10.9K 10.9K 11.0Ktest 50.5K 52.1K 52.1K 52.4Ksentences tune 455test 2,034Table 1: WMT10 system combination tuning/testingdata5 Experiments5.1 SetupWe ran our experiments for the WMT10 sys-tem combination task usinge four language pairs,{Czech, French, German, Spanish}-to-English(Callison-Burch et al, 2010).
The data is summa-rized in Table 1.
The system outputs are retok-enized to match the Penn-treebank standard, parsedby the Stanford Parser (Klein and Manning, 2003),and lower-cased.We implemented our confusion forest sys-tem combination using an in-house developedhypergraph-based toolkit cicada which is motivatedby generic weighted logic programming (Lopez,2009), originally developed for a synchronous-CFGbased machine translation system (Chiang, 2007).Input to our system is a collection of hypergraphs,a set of parsed hypotheses, from which rules are ex-tracted and a new forest is generated as describedin Section 3.
Our baseline, also implemented in ci-cada, is a confusion network-based system combi-nation method (?2) which incrementally aligns hy-potheses to the growing network using TER (Rostiet al, 2008) and merges multiple networks into alarge single network.
After performing epsilon re-moval, the network is transformed into a forest byparsing with monotone rules of S ?
X, S ?
S Xand X ?
x. k-best translations are extracted fromthe forest using the forest-based algorithms in Sec-tion 3.3.5.2 FeaturesThe feature weight vector w in Equation 1 is tunedby MERT over hypergraphs (Kumar et al, 2009).We use three lower-cased 5-gram language mod-1253els hilm(d): English Gigaword Fourth edition1, theEnglish side of French-English 109 corpus and thenews commentary English data2.
The count basedfeatures ht(d) and he(d) count the number of ter-minals and the number of hyperedges in d, respec-tively.
We employ M confidence measures hms (d)for M systems, which basically count the number ofrules used in d originally extracted from mth systemhypothesis (Rosti et al, 2007a).Following Macherey and Och (2007), BLEU (Pa-pineni et al, 2002) correlations are also incorporatedin our system combination.
GivenM system outputse1...eM , M BLEU scores are computed for d usingeach of the system outputs em as a referencehmb (d) = BP (e, em) ?
exp(144?n=1log ?n(e, em))where e = yield(d) is a terminal yield of d, BP (?
)and ?n(?)
respectively denote brevity penalty andn-gram precision.
Here, we use approximated un-clipped n-gram counts (Dreyer et al, 2007) for com-puting ?n(?)
with a compact state representation (Liand Khudanpur, 2009).Our baseline confusion network system has an ad-ditional penalty feature, hp(m), which is the totaledits required to construct a confusion network us-ing themth system hypothesis as a skeleton, normal-ized by the number of nodes in the network (Rosti etal., 2007b).5.3 ResultsTable 2 compares our confusion forest approach(CF) with different orders, a confusion network(CN) and max/min systems measured by BLEU (Pa-pineni et al, 2002).
We vary the horizontal orders,h = 1, 2,?
with vertical orders of v = 3, 4,?.Systems without statistically significant differencesfrom the best result (p < 0.05) are indicated by boldface.
Setting v = ?
and h = ?
achieves compa-rable performance to CN.
Our best results in threelanguages come from setting v = ?
and h = 2,which favors little reordering of phrasal structures.In general, lower horizontal and vertical order leadsto lower BLEU.1LDC catalog No.
LDC2009T132Those data are available from http://www.statmt.org/wmt10/.language cz-en de-en es-en fr-ensystem min 14.09 15.62 21.79 16.79max 23.44 24.10 29.97 29.17CN 23.70 24.09 30.45 29.15CFv=?,h=?
24.13 24.18 30.41 29.57CFv=?,h=2 24.14 24.58 30.52 28.84CFv=?,h=1 24.01 23.91 30.46 29.32CFv=4,h=?
23.93 23.57 29.88 28.71CFv=4,h=2 23.82 22.68 29.92 28.83CFv=4,h=1 23.77 21.42 30.10 28.32CFv=3,h=?
23.38 23.34 29.81 27.34CFv=3,h=2 23.30 23.95 30.02 28.19CFv=3,h=1 23.23 21.43 29.27 26.53Table 2: Translation results in lower-case BLEU.CN for confusion network and CF for confusionforest with different vertical (v) and horizontal (h)Markovization order.language cz-en de-en es-en fr-enrerank 29.40 32.32 36.83 36.59CN 38.52 34.97 47.65 46.37CFv=?,h=?
30.51 34.07 38.69 38.94CFv=?,h=2 30.61 34.25 38.87 39.10CFv=?,h=1 31.09 34.65 39.27 39.51CFv=4,h=?
30.86 34.19 39.17 39.39CFv=4,h=2 30.96 34.32 39.35 39.57CFv=4,h=1 31.44 34.62 39.69 39.90CFv=3,h=?
31.03 34.30 39.29 39.57CFv=3,h=2 31.25 34.97 39.61 40.00CFv=3,h=1 31.55 34.60 39.72 39.97Table 3: Oracle lower-case BLEUTable 3 presents oracle BLEU achievable by eachcombination method.
The gains achievable by theCF over simple reranking are small, at most 2-3points, indicating that small variations are encodedin confusion forests.
We also observed that a lowerhorizontal and vertical order leads to better BLEUpotentials.
As briefly pointed out in Section 3.2,the higher horizontal and vertical order implies morefaithfulness to the original parse trees.
Introducingnew tree fragments to confusion forests leads to newphrasal translations with enlarged forests, as pre-sented in Table 4, measured by the average number1254lang cz-en de-en es-en fr-enCN 2,222.68 47,231.20 2,932.24 11,969.40lattice 1,723.91 41,403.90 2,330.04 10,119.10CFv=?
230.08 540.03 262.30 386.79CFv=4 254.45 651.10 302.01 477.51CFv=3 286.01 802.79 349.21 575.17Table 4: Hypegraph size measured by the averagenumber of hyperedges (h = 1 for CF).
?lattice?
isthe average number of edges in the original CN.of hyperedges3.
The larger potentials do not implybetter translations, probably due to the larger searchspace with increased search errors.
We also conjec-ture that syntactic variations were not captured bythe n-gram like string-based features in Section 5.2,therefore resulting in BLEU loss, which will be in-vestigated in future work.In contrast, CN has more potential for generat-ing better translations, with the exception of theGerman-to-English direction, with scores that areusually 10 points better than simple sentence-wisereranking.
The low potential in German should beinterpreted in the light of the extremely large confu-sion network in Table 4.
We postulate that the di-vergence in German hypotheses yields wrong align-ments, and therefore amounts to larger networkswith incorrect hypotheses.
Table 4 also shows thatCN produces a forest that is an order of magnitudelarger than those created by CFs.
Although we can-not directly relate the runtime and the number ofhyperedges in CN and CFs, since the shape of theforests are different, CN requires more space to en-code the hypotheses than those by CFs.Table 5 compares the average length of the min-imum/maximum hypothesis that each method canproduce.
CN may generate shorter hypotheses,whereby CF prefers longer hypotheses as we de-crease the vertical order.
Large divergence is alsoobserved for German, such as for hypergraph size.6 ConclusionWe presented a confusion forest based method forsystem combination in which system outputs aremerged into a packed forest using their syntactic3We measure the hypergraph size before intersecting withnon-local features, like n-gram language models.language cz-en de-en es-en fr-ensystem avg.
24.84 25.62 25.63 25.75CN min 11.09 3.39 12.27 7.94max 33.69 40.65 33.22 36.27CFv=?
min 15.97 10.88 17.67 16.62max 35.20 47.20 35.28 37.94CFv=4 min 15.52 10.58 17.02 15.85max 37.11 53.67 38.56 42.64CFv=3 min 15.15 10.34 16.54 15.30max 39.88 68.45 42.85 49.55Table 5: Average min/max hypothesis length pro-ducible by each method (h = 1 for CF).similarity.
The forest construction is treated as ageneration from a CFG compiled from the parsedoutputs.
Our experiments indicate comparable per-formance to a strong confusion network baselinewith smaller space, and statistically significant gainsin some language pairs.To our knowledge, this is the first work to directlyintroduce syntactic consensus to system combina-tion by encoding multiple system outputs into a sin-gle forest structure.
We believe that the confusionforest based approach to system combination hasfuture exploration potential.
For instance, we didnot employ syntactic features in Section 5.2 whichwould be helpful in discriminating hypotheses inlarger forests.
We would also like to analyze thetrade-offs, if any, between parsing errors and confu-sion forest constructions by controlling the parsingqualities.
As an alternative to the grammar-basedforest generation, we are investigating an edit dis-tance measure for tree alignment, such as tree editdistance (Bille, 2005) which basically computes in-sertion/deletion/replacement of nodes in trees.AcknowledgmentsWe would like to thank anonymous reviewers andour colleagues for helpful comments and discussion.ReferencesSrinivas Bangalore, German Bordel, and Giuseppe Ric-cardi.
2001.
Computing consensus translation frommultiple machine translation systems.
In Proceedingsof Automatic Speech Recognition and Understanding(ASRU), 2001, pages 351 ?
354.1255Philip Bille.
2005.
A survey on tree edit distance andrelated problems.
Theor.
Comput.
Sci., 337:217?239,June.Sylvie Billot and Bernard Lang.
1989.
The structureof shared forests in ambiguous parsing.
In Proceed-ings of the 27th Annual Meeting of the Association forComputational Linguistics, pages 143?151, Vancou-ver, British Columbia, Canada, June.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR,pages 17?53, Uppsala, Sweden, July.
Revised August2010.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.John DeNero, Shankar Kumar, Ciprian Chelba, and FranzOch.
2010.
Model combination for machine trans-lation.
In Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages975?983, Los Angeles, California, June.Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.2007.
Comparing reordering constraints for smt us-ing efficient bleu oracle computation.
In Proceedingsof SSST, NAACL-HLT 2007 / AMTA Workshop on Syn-tax and Structure in Statistical Translation, pages 103?110, Rochester, New York, April.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the Association for Com-puting Machinery, 13:94?102, February.J.G.
Fiscus.
1997.
A post-processing system to yield re-duced word error rates: Recognizer output voting errorreduction (rover).
In Proceedings of Automatic SpeechRecognition and Understanding (ASRU), 1997, pages347 ?354, December.Robert Frederking and Sergei Nirenburg.
1994.
Threeheads are better than one.
In Proceedings of the fourthconference on Applied natural language processing,pages 95?100, Morristown, NJ, USA.Joshua Goodman.
1999.
Semiring parsing.
Computa-tional Linguistics, 25:573?605, December.Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,and Robert Moore.
2008.
Indirect-HMM-based hy-pothesis alignment for combining outputs from ma-chine translation systems.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 98?107, Honolulu, Hawaii,October.John C. Henderson and Eric Brill.
1999.
Exploitingdiversity in natural language processing: Combiningparsers.
In Proceedings of the Fourth Conference onEmpirical Methods in Natural Language Processing,pages 187?194.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technology, pages 53?64, Van-couver, British Columbia, October.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 144?151,Prague, Czech Republic, June.Shyamsundar Jayaraman and Alon Lavie.
2005.
Multi-engine machine translation guided by explicit wordmatching.
In Proceedings of the ACL 2005 on In-teractive poster and demonstration sessions, ACL ?05,pages 101?104, Morristown, NJ, USA.Dan Klein and Christopher D. Manning.
2001.
Parsingand hypergraphs.
In Proceedings of the Seventh In-ternational Workshop on Parsing Technologies (IWPT-2001), pages 123?134.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 423?430, Sapporo, Japan, July.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error rate train-ing and minimum bayes-risk decoding for translationhypergraphs and lattices.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 163?171, Sun-tec, Singapore, August.Irene Langkilde and Kevin Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InProceedings of the 36th Annual Meeting of the As-sociation for Computational Linguistics and 17th In-ternational Conference on Computational Linguistics- Volume 1, ACL-36, pages 704?710, Morristown, NJ,USA.Irene Langkilde.
2000.
Forest-based statistical sentencegeneration.
In Proceedings of the 1st North Americanchapter of the Association for Computational Linguis-tics conference, pages 170?177, San Francisco, CA,USA.Zhifei Li and Sanjeev Khudanpur.
2009.
Efficient extrac-tion of oracle-best translations from hypergraphs.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,Companion Volume: Short Papers, pages 9?12, Boul-der, Colorado, June.Yang Liu, Haitao Mi, Yang Feng, and Qun Liu.
2009.Joint decoding with multiple translation models.
In1256Proceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 576?584, Suntec, Singapore, Au-gust.Adam Lopez.
2009.
Translation as weighted deduction.In Proceedings of the 12th Conference of the Euro-pean Chapter of the ACL (EACL 2009), pages 532?540, Athens, Greece, March.Wolfgang Macherey and Franz J. Och.
2007.
An empir-ical study on computing consensus translations frommultiple machine translation systems.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages986?995, Prague, Czech Republic, June.Lidia Mangu, Eric Brill, and Andreas Stolcke.
2000.Finding consensus in speech recognition: word errorminimization and other applications of confusion net-works.
Computer Speech & Language, 14(4):373 ?400.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation from multiplemachine translation systems using enhanced hypothe-ses alignment.
In Proceedings of the 11th Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 33?40.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL-08: HLT,pages 192?199, Columbus, Ohio, June.Tadashi Nomoto.
2004.
Multi-engine machine transla-tion with voted language model.
In Proceedings of the42nd Meeting of the Association for ComputationalLinguistics (ACL?04), Main Volume, pages 494?501,Barcelona, Spain, July.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-ros Matsoukas, Richard Schwartz, and Bonnie Dorr.2007a.
Combining outputs from multiple machinetranslation systems.
In Human Language Technolo-gies 2007: The Conference of the North AmericanChapter of the Association for Computational Linguis-tics; Proceedings of the Main Conference, pages 228?235, Rochester, New York, April.Antti-Veikko Rosti, Spyros Matsoukas, and RichardSchwartz.
2007b.
Improved word-level system com-bination for machine translation.
In Proceedings ofthe 45th Annual Meeting of the Association of Com-putational Linguistics, pages 312?319, Prague, CzechRepublic, June.Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, andRichard Schwartz.
2008.
Incremental hypothesisalignment for building confusion networks with appli-cation to machine translation system combination.
InProceedings of the Third Workshop on Statistical Ma-chine Translation, pages 183?186, Columbus, Ohio,June.Stuart M. Shieber, Yves Schabes, and Fernando C. N.Pereira.
1995.
Principles and implementation ofdeductive parsing.
Journal of Logic Programming,24(1?2):3?36, July?August.K.C.
Sim, W.J.
Byrne, M.J.F.
Gales, H. Sahbi, and P.C.Woodland.
2007.
Consensus network decoding forstatistical machine translation system combination.
InProceedings of Acoustics, Speech and Signal Process-ing (ICASSP), 2007, volume 4, pages IV?105 ?IV?108, April.Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
In InProceedings of Association for Machine Translation inthe Americas, pages 223?231.1257
