Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1808?1814,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsUsing Soft Constraints in Joint Inference forClinical Concept RecognitionPrateek Jindal and Dan RothDepartment of Computer Science, UIUC201 N. Goodwin Ave, Urbana, IL 61801, USA{jindal2, danr}@illinois.eduAbstractThis paper introduces IQPs (Integer QuadraticPrograms) as a way to model joint inferencefor the task of concept recognition in clinicaldomain.
IQPs make it possible to easily in-corporate soft constraints in the optimizationframework and still support exact global infer-ence.
We show that soft constraints give statis-tically significant performance improvementswhen compared to hard constraints.1 IntroductionIn this paper, we study the problem of conceptrecognition in the clinical domain.
State-of-the-artapproaches (de Bruijn et al 2011; Patrick et al2011; Torii et al 2011; Minard et al 2011; Jianget al 2011; Xu et al 2012; Roberts and Harabagiu,2011; Jindal and Roth, 2013) for concept recogni-tion in clinical domain (Uzuner et al 2011) usesequence-prediction models like CRF (Lafferty etal., 2001), MEMM (McCallum et al 2000) etc.These approaches are limited by the fact that theycan model only local dependencies (most often,first-order models like linear chain CRFs are usedto allow tractable inference).Clinical narratives, unlike newswire data, providea domain with significant knowledge that can be ex-ploited systematically to improve the accuracy ofthe prediction task.
Knowledge in this domain canbe thought of as belonging to two categories: (1)Background Knowledge captured in medical ontolo-gies like UMLS (Url1, 2013), MeSH and SNOMEDCT and (2) Discourse Knowledge driven by thefact that the narratives adhere to a specific writingstyle.
While the former can be used by generatingmore expressive knowledge-rich features, the lat-ter is more interesting from our current perspective,since it provides global constraints on what outputstructures are likely and what are not.
We exploitthis structural knowledge in our global inference for-mulation.Integer Linear Programming (ILP) based ap-proaches have been used for global inference inmany works (Roth and Yih, 2004; Punyakanok etal., 2004; Punyakanok et al 2008; Marciniak andStrube, 2005; Bramsen et al 2006; Barzilay andLapata, 2006; Riedel and Clarke, 2006; Clarke andLapata, 2007; Clarke and Lapata, 2008; Denis et al2007; Chang et al 2011).
However, in most of theseworks, researchers have focussed only on hard con-straints while formulating the inference problem.Formulating all the constraints as hard constraintsis not always desirable because the constraints arenot perfect in many cases.
In this paper, we pro-pose Integer Quadratic Programs (IQPs) as a wayof formulating the inference problem.
IQPs is aricher family of models than ILPs and it enablesus to easily incorporate soft constraints into the in-ference procedure.
Our experimental results showthat soft constraints indeed give much better perfor-mance than hard constraints.2 Identifying Medical ConceptsTask Description Our input consists of clinical re-ports in free-text (unstructured) format.
The task is:(1) to identify the boundaries of medical conceptsand (2) to assign types to such concepts.
Each con-cept can have 3 possible types: (1) Test, (2) Treat-ment, and (3) Problem.
We would refer to thesethree types by TEST, TRE and PROB in the follow-ing discussion.Our Approach In the first step, we identify theconcept boundaries using a CRF (with BIO encod-1808[Chest x-ray] gave positive evidence for [atelectasis] and [sarcoidosis].Test Problem Problem(a) Example 1No [hemoptysis], [hematemesis], [urgency], [abdominal pain], [black or tarry stools], [dysuria].Problem ProblemProblem ProblemProblemProblem(b) Example 2Figure 1: This figure motivates the global inference procedure we used.
For discussion, please refer to ?2.ing).
Features used by the CRF include the con-stituents given by MetaMap (Aronson and Lang,2010; Url2, 2013), shallow parse constituents, sur-face form and part-of-speech (Url3, 2013) of wordsin a window of size 3.
We also use conjunctions ofthe features.After finding concept boundaries, we determinethe probability distribution for each concept over 4possible types (TEST, TRE, PROB or NULL).
Theseprobability distributions are found using a multi-class SVM classifier (Chang and Lin, 2011).
Fea-tures used for training this classifier include con-cept tokens, full text of concept, bi-grams, head-word, suffixes of headword, capitalization pattern,shallow parse constituent, Metamap type of concept,MetaMap type of headword, occurrence of conceptin MeSH (Url4, 2013) and SNOMED CT (Url5,2013), MeSH and SNOMED CT descriptors.Inference Procedure: The final assignment oftypes to concepts is determined by an inference pro-cedure.
The basic principle behind our inferenceprocedure is: ?Types of concepts which appear closeto one another are often closely related.
For someconcepts, type can be determined with more confi-dence.
And relations between concepts?
types guidethe inference procedure to determine the types ofother concepts.?
We will now explain it in more de-tail with the help of examples.
Figure 1 shows twosentences in which the concepts are shown in brack-ets and correct (gold) types of concepts are shownabove them.First, consider first and second concepts in Fig-ure 1a.
These concepts follow the pattern: [Con-cept1] gave positive evidence for [Concept2].
Inclinical narratives, such a pattern strongly suggeststhat Concept1 is of type TEST and Concept2 is oftype PROB.
Table 1 shows additional such patterns.Next, consider different concepts in Figure 1b.
AllPattern1 using [TRE] for [PROB]2 [TEST] showed [PROB]3 Patient presents with [PROB] status post[TRE]4 use [TRE] to correct [PROB]5 [TEST] to rule out [PROB]6 Unfortunately, [TRE] has caused [PROB]Table 1: Some patterns that were used in constraints.these concepts are separated by commas and hence,form a list.
It is highly likely that such conceptsshould have the same type.3 Modeling Global InferenceInference is done at the level of sentences.
Sup-pose there are m concepts in a sentence.
Each ofthe m concepts has to be assigned one of the follow-ing types: TEST, TRE, PROB or NULL.
To representthis as an inference problem, we define the indicatorvariables xi,j where i takes values from 1 to m (cor-responding to concepts) and j takes values from 1 to4 (corresponding to 4 possible types).
pi,j refers tothe probability that the ith concept has type j.We can now write the following optimizationproblem to find the optimal concept types:maxxm?i=14?j=1xi,j ?
pi,j (1)subject to4?j=1xi,j = 1 ?i (2)xi,j ?
{0, 1} ?i, j (3)The objective function in Equation (1) expressesthe fact that we want to maximize the expected num-ber of correct predictions in each sentence.
Equa-tion (2) enforces the constraint that each concept has1809a unique type.
We would refer to these as Type-1constraints.3.1 Constraints UsedIn this subsection, we will describe two addi-tional types of constraints (Type-2 and Type-3)that were added to the optimization procedure de-scribed above.
Whereas Type-1 constraints de-scribed above were formulated as hard constraints,Type-2 and Type-3 constraints are formulated assoft constraints.3.1.1 Type-2 ConstraintsCertain constructs like comma, conjunction, etc.suggest that the 2 concepts appearing in them shouldhave the same type.
Figure 1b shows an example ofsuch a constraint.
Suppose that there are n2 suchconstraints.
Also, assume that the lth constraint saysthat the concepts Rl and Sl should have the sametype.
To model this, we define a variable wl as fol-lows:wl =4?m=1(xRl,m ?
xSl,m)2 (4)Now, if the concepts Rl and Sl have the sametype, then wl would be equal to 0; otherwise, wlwould be equal to 2.
So, the lth constraint can beenforced by subtracting (?2 ?wl2 ) from the objectivefunction given by Equation (1).
Thus, a penalty of?2 would be enforced iff this constraint is violated.3.1.2 Type-3 ConstraintsSome short patterns suggest possible types for theconcepts which appear in them.
Each such pattern,thus, enforces a constraint on the types of corre-sponding concepts.
Figure 1a shows an exampleof such a constraint.
Suppose that there are n3such constraints.
Also, assume that the kth con-straint says that the concept A1,k should have thetype B1,k and that the concept A2,k should havethe type B2,k.
Equivalently, the kth constraint canbe written as follows in boolean algebra notation:(xA1,k,B1,k = 1)?
(xA2,k,B2,k = 1).
For the kth con-straint, we introduce one more variable zk ?
{0, 1}which satisfies the following condition:zk = 1?
xA1,k,B1,k ?
xA2,k,B2,k (5)Using boolean algebra, it is easy to show thatEquation (5) can be reduced to a set of linear in-equalities.
Thus, we can incorporate the kth con-maxxm?i=14?j=1xi,j ?
pi,j ?n3?k=1?3(1?
zk)?n2?l=1(?2 ?
?4m=1(xRl,m ?
xSl,m)22) (6)subject to4?j=1xi,j = 1 ?i (7)xi,j ?
{0, 1} ?i, j (8)zk = 1?
xA1,k,B1,k ?
xA2,k,B2,k?k ?
{1...n3} (9)Figure 2: Final Optimization Problem (an IQP)straint in the optimization problem by adding to itthe constraint given by Equation (5) and by subtract-ing (?3(1 ?
zk)) from the objective function givenby Equation (1).
Thus, a penalty of ?3 is imposed iffkth constraint is not satisfied (zk = 0).3.2 Final Optimization Problem - An IQPAfter incorporating all the constraints mentionedabove, the final optimization problem (an IQP) isshown in Figure 2.
We used Gurobi toolkit (Url6,2013) to solve such IQPs.
In our case, it solves76 IQPs per second on a quad-core server with In-tel Xeon X5650 @ 2.67 GHz processors and 50 GBRAM.4 Experiments and Results4.1 Datasets and Evaluation MetricsFor our experiments, we used the datasets pro-vided by i2b2/VA team as part of 2010 i2b2/VAshared task (Uzuner et al 2011).
The datasetsused for this shared task contained de-identied clin-ical reports from three medical institutions: Part-ners Healthcare (PH), Beth-Israel Deaconess Med-ical Center (BIDMC) and the University of Pitts-burgh Medical Center (UPMC).
UPMC data was di-vided into 2 sections, namely discharge (UPMCD)and progress notes (UPMCP).
A total of 349 train-ing reports and 477 test reports were made availableto the participants.
However, data which came fromUPMC (more than 50% data) was not made avail-able for public use.
As a result, we had only 170clinical reports for training and 256 clinical reportsfor testing.
Table 3 shows the number of clinical re-ports made available by different institutions.
The1810B BK BC BKCP R F1 P R F1 P R F1 P R F1TEST 92.4 79.4 85.4 91.9 80.2 85.7 92.7 79.6 85.7 92.1 80.4 85.8TRE 92.1 73.6 81.8 92.0 79.5 85.3 92.3 76.8 83.8 92.0 80.2 85.7PROB 83.6 83.6 83.6 88.9 83.7 86.3 85.9 83.8 84.8 89.6 83.9 86.7OVERALL 88.4 79.4 83.6 90.7 81.4 85.8 89.6 80.5 84.8 91.0 81.7 86.1Table 2: Our final system, BKC, consistently performed the best among all 4 systems (B, BK, BC and BKC).PH BIDMC UPMCD UPMCPTrain 97 73 98 81Test 133 123 102 119Table 3: Dataset Characteristicsstrikethrough text in this table indicates that the datawas not made available for public use and hence, wecouldn?t use it.
We used about 20% of the trainingdata as a development set.
For evaluation, we reportprecision, recall and F1 scores.4.2 ResultsIn this section, we would refer to following 4systems: (1) Baseline (B), (2) Baseline + Knowl-edge (BK), (3) Baseline + Constraints (BC) and(4) Baseline + Knowledge + Constraints (BKC).Please note that the difference between B andBK is that B does not use the features derivedfrom domain-specific knowledge sources (namelyMetaMap, UMLS, MeSH and SNOMED CT) fortraining the classifiers.
Both B and BK do not usethe inference procedure.
BKC uses all the featuresand also the inference procedure.
In addition tothese 4 systems, we would refer to another system,namely, BKC-HARD.
This is similar to BKC sys-tem.
However, it sets ?2 = ?3 = 1 which effectivelyturns Type-2 and Type-3 constraints into hard con-straints by imposing very high penalty.4.2.1 Importance of Soft ConstraintsFigures 3a and 3b show the effect of varying thepenalties (?2 and ?3) for Type-2 and Type-3 con-straints respectively.
These figures show the F1-score of BKC on the development set.
Penalty of0 means that the constraint is not active.
As we in-crease the penalty, the constraint becomes stronger.As the penalty becomes 1, the constraint becomeshard in the sense that final assignments must respect0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 179.779.879.98080.180.280.380.480.580.680.7Tuning Penalty Parameter for Type?2 ConstraintsPenalty Parameter for Type?2 Constraints ( ?2)F1?score(a) Type-2 Constraints0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 180.380.480.580.680.780.880.98181.181.281.381.481.5Tuning Penalty Parameter for Type?3 ConstraintsPenalty Parameter for Type?3 Constraints ( ?3)F1?score(b) Type-3 ConstraintsFigure 3: These figures show the result of tuning thepenalty parameters (?2 and ?3) for soft constraints.BKC-HARD BKCTEST 84.7 85.8TRE 84.7 85.7PROB 85.6 86.7OVERALL 85.1 86.1Table 4: Soft constraints (BKC) consistently performmuch better than hard constraints (BKC-HARD).the constraint.
We observe from Figures 3a and 3bthat for Type-2 and Type-3 constraints, global max-ima is attained at ?2 = 0.6 and ?3 = 0.3 respec-tively.Hard vs Soft Constraints Table 4 compares theperformance of BKC-HARD with that of BKC.First 3 rows in this table show the performance ofboth systems for the individual categories (TEST,TRE and PROB).
The fourth row shows the overallscore of both systems.
BKC outperformed BKC-HARD on all the categories by statistically signifi-cant differences at p = 0.05 according to BootstrapResampling Test (Koehn, 2004).
For the OVERALLcategory, BKC improved over BKC-HARD by 1.0F1 points.181140 50 60 70 80 90 100 110 120 13080818283848586Training Data Size (# clinical reports)F1?scoreEffect of Training Data Size on PerformanceBKCBKFigure 4: This figure shows the effect of training datasize on performance of concept recognition.4.2.2 Comparing with state-of-the-art baselineIn the 2010 i2b2/VA shared task, majority oftop systems were CRF-based models, motivatingthe use of CRF as our baseline.
Table 2 com-pares the performance of 4 systems: B, BK, BCand BKC.
As pointed out before, our BK systemuses CRF for boundary detection, employs all theknowledge-based features and is very similar to thetop-performing systems in i2b2 challenge.
We seefrom Table 2 that BKC consistently performed thebest for individual as well as overall categories1.This result is statistically significant at p = 0.05according to Bootstrap Resampling Test (Koehn,2004).
It should also be noted that BC performedsignificantly better than B for all the categories.Thus, the constraints are helpful even in the ab-sence of knowledge-based features.
Since we reportresults on publicly available datasets, future workswould be able to compare their results with ours.4.2.3 Effect of training data sizeIn Figure 4, we report the overall F1-score on apart of the development set as we vary the size of thetraining data from 40 documents to 130 documents.We notice that the performance increases steadily asmore and more training data is provided.
This sug-gests that if we could train on full training data aswas made available in the challenge, the final scorescould be much higher.
We also notice from the fig-ure that BKC consistently outperforms the state-of-the-art BK system as we vary the size of the trainingdata, indicating the robustness of the joint inferenceprocedure.1Please note that the results reported in Table 2 can not bedirectly compared with those reported in the challenge becausewe only had a fraction of the original training and testing data.5 Discussion and Related WorkIn this paper, we chose to train a rather simple se-quential model (using CRF), and focused on incor-porating global constraints only at inference time2.While it is possible to jointly train the model withthe global constraints (as illustrated by Chang et al(2007), Mann and McCallum (2007), Mann and Mc-Callum (2008), Ganchev et al(2010) etc.
), this pro-cess will be a lot less efficient, and prior work (Rothand Yih, 2005) has shown that it may not be benefi-cial.Roth and Yih (2004, 2007) suggested the use ofinteger programs to model joint inference in a fullysupervised setting.
Our paper follows their concep-tual approach.
However, they used only hard con-straints in their inference formulation.
Chang etal.
(2012) extended the ILP formulation and usedsoft constraints within the Constrained ConditionalModel formulation (Chang, 2011).
However, theirimplementation performed only approximate infer-ence.
In this paper, we extended the integer lin-ear programming to a quadratic formulation, argu-ing that it simplifies the modeling step3, and showedthat it is possible to do exact inference efficiently.ConclusionThis paper presented a global inference strategy(using IQP) for concept recognition which allowsus to model structural knowledge of the clinical do-main as soft constraints in the optimization frame-work.
Our results showed that soft constraints aremore effective than hard constraints.AcknowledgmentsThis research was supported by Grant HHS90TR0003/01 and by IARPA FUSE program viaDoI/NBC contract #D11PC2015.
Its contents aresolely the responsibility of the authors and do notnecessarily represent the official views, either ex-pressed or implied, of the HHS, IARPA, DoI/NBCor the US government.
The US Government isauthorized to reproduce and distribute reprints forGovernmental purposes notwithstanding any copy-right annotation thereon.2In another experiment, we replaced the CRF with anMEMM.
Surprisingly, MEMM performed as well as CRF.3It should be noted that it is possible to reduce IQPs to ILPsusing variable substitution.
However, the resulting ILPs can beexponentially larger than original IQPs.1812ReferencesA.R.
Aronson and F.M.
Lang.
2010.
An overview ofmetamap: historical perspective and recent advances.Journal of the American Medical Informatics Associa-tion, 17(3):229.R.
Barzilay and M. Lapata.
2006.
Aggregation via setpartitioning for natural language generation.
In Pro-ceedings of the main conference on Human LanguageTechnology Conference of the North American Chap-ter of the Association of Computational Linguistics,pages 359?366.
Association for Computational Lin-guistics.P.
Bramsen, P. Deshpande, Y.K.
Lee, and R. Barzilay.2006.
Inducing temporal graphs.
In Proceedings ofthe 2006 Conference on Empirical Methods in NaturalLanguage Processing, pages 189?198.
Association forComputational Linguistics.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.M.
Chang, L. Ratinov, and D. Roth.
2007.
Guiding semi-supervision with constraint-driven learning.
In Associ-ation for Computational Linguistics, pages 280?287,Prague, Czech Republic, 6.
Association for Computa-tional Linguistics.K.-W. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,M.
Sammons, and D. Roth.
2011.
Inference proto-cols for coreference resolution.
In Proceedings of theFifteenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 40?44, Portland,Oregon, USA.
Association for Computational Linguis-tics.Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2012.Structured learning with constrained conditional mod-els.
Machine learning, pages 1?33.M.
Chang.
2011.
Structured Prediction with IndirectSupervision.
Ph.D. thesis, University of Illinois atUrbana-Champaign.James Clarke and Mirella Lapata.
2007.
Modelling com-pression with discourse constraints.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages1?11.J.
Clarke and M. Lapata.
2008.
Global inference forsentence compression: An integer linear programmingapproach.
Journal of Artificial Intelligence Research,31(1):399?429.B.
de Bruijn, C. Cherry, S. Kiritchenko, J. Martin, andX.
Zhu.
2011.
Machine-learned solutions for threestages of clinical information extraction: the state ofthe art at i2b2 2010.
Journal of the American MedicalInformatics Association, 18(5):557?562.P.
Denis, J. Baldridge, et al2007.
Joint determi-nation of anaphoricity and coreference resolution us-ing integer programming.
In Proceedings of HumanLanguage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 236?243.Kuzman Ganchev, Joao Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
The Journal of MachineLearning Research, 11:2001?2049.M.
Jiang, Y. Chen, M. Liu, S.T.
Rosenbloom, S. Mani,J.C.
Denny, and H. Xu.
2011.
A study of machine-learning-based approaches to extract clinical entitiesand their assertions from discharge summaries.
J AmMed Info Assoc, 18(5):601?606.P.
Jindal and D. Roth.
2013.
End-to-end coreference res-olution for clinical narratives.
In Proceedings of In-ternational Joint Conference on Artificial Intelligence(IJCAI), pages 2106?2112, 8.P.
Koehn.
2004.
Statistical significance tests for machinetranslation evaluation.
In Proceedings of EmpiricalMethods in Natural Language Processing, volume 4,pages 388?395.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the Eighteenth InternationalConference on Machine Learning, ICML ?01, pages282?289, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Gideon S Mann and Andrew McCallum.
2007.
Sim-ple, robust, scalable semi-supervised learning via ex-pectation regularization.
In Proceedings of the 24thinternational conference on Machine learning, pages593?600.
ACM.Gideon Mann and Andrew McCallum.
2008.
General-ized expectation criteria for semi-supervised learningof conditional random fields.
In Proceedings of Asso-ciation for Computational Linguistics, pages 870?878.T.
Marciniak and M. Strube.
2005.
Beyond thepipeline: Discrete optimization in nlp.
In Proceed-ings of the Ninth Conference on Computational Nat-ural Language Learning, pages 136?143.
Associationfor Computational Linguistics.A.
McCallum, D. Freitag, and F. Pereira.
2000.
Maxi-mum entropy markov models for information extrac-tion and segmentation.
In Proceedings of the Seven-teenth International Conference on Machine Learning,pages 591?598.A.L.
Minard, A.L.
Ligozat, A.B.
Abacha, D. Bernhard,B.
Cartoni, L. Dele?ger, B. Grau, S. Rosset, P. Zweigen-baum, and C. Grouin.
2011.
Hybrid methods for1813improving information access in clinical documents:Concept, assertion, and relation identification.
J AmMed Info Assoc, 18(5):588?593.J.D.
Patrick, D.H.M.
Nguyen, Y. Wang, and M. Li.2011.
A knowledge discovery and reuse pipelinefor information extraction in clinical notes.
Jour-nal of the American Medical Informatics Association,18(5):574?579.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2004.Semantic role labeling via integer linear programminginference.
In Proceedings of the 20th internationalconference on Computational Linguistics, page 1346.Association for Computational Linguistics.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2):257?287.S.
Riedel and J. Clarke.
2006.
Incremental integer linearprogramming for non-projective dependency parsing.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages 129?137.
Association for Computational Linguistics.K.
Roberts and S.M.
Harabagiu.
2011.
A flexible frame-work for deriving assertions from electronic medicalrecords.
Journal of the American Medical InformaticsAssociation, 18(5):568?573.D.
Roth and W. Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.
InProceedings of conference on Computational NaturalLanguage Learning (CoNLL), pages 1?8.
Associationfor Computational Linguistics.D.
Roth and W. Yih.
2005.
Integer linear programminginference for conditional random fields.
In Proceed-ings of International Conference on Machine Learning(ICML), pages 737?744.D.
Roth and W. Yih.
2007.
Global inference for en-tity and relation identification via a linear program-ming formulation.
Introduction to Statistical Rela-tional Learning, pages 553?580.M.
Torii, K. Wagholikar, and H. Liu.
2011.
Us-ing machine learning for concept extraction on clin-ical documents from multiple data sources.
Jour-nal of the American Medical Informatics Association,18(5):580?587.Url1.
2013.
Umls: Unified medical languagesystem (http://www.nlm.nih.gov/research/umls/) (ac-cessed july 1, 2013).Url2.
2013.
Metamap (http://metamap.nlm.nih.gov/)(accessed july 1, 2013).Url3.
2013.
Illinois part-of-speech tagger(http://cogcomp.cs.illinois.edu/page/software view/pos) (accessed july 1, 2013).Url4.
2013.
Mesh: Medical subject headings(http://www.nlm.nih.gov/mesh/meshhome.html) (ac-cessed july 1, 2013).Url5.
2013.
Snomed ct: Snomed clinical terms(http://www.ihtsdo.org/snomed-ct/) (accessed july 1,2013).Url6.
2013.
Gurobi optimization toolkit(http://www.gurobi.com/) (accessed july 1, 2013).O.
Uzuner, B.R.
South, S. Shen, and S.L.
DuVall.
2011.2010 i2b2/va challenge on concepts, assertions, andrelations in clinical text.
Journal of American MedicalInformatics Association.Y.
Xu, K. Hong, J. Tsujii, I. Eric, and C. Chang.
2012.Feature engineering combined with machine learningand rule-based methods for structured information ex-traction from narrative clinical discharge summaries.Journal of the American Medical Informatics Associa-tion, 19(5):824?832.1814
