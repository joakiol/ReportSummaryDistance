Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 19?24,Sydney, July 2006. c?2006 Association for Computational LinguisticsAnnotation Schemes and their Influence on Parsing ResultsWolfgang MaierSeminar fu?r Sprachwissenschaft, Universita?t Tu?bingenWilhelmstr.
19, 72074 Tu?bingen, Germanywmaier@sfs.uni-tuebingen.deAbstractMost of the work on treebank-based sta-tistical parsing exclusively uses the Wall-Street-Journal part of the Penn treebankfor evaluation purposes.
Due to the pres-ence of this quasi-standard, the question ofto which degree parsing results depend onthe properties of treebanks was often ig-nored.
In this paper, we use two similarGerman treebanks, Tu?Ba-D/Z and NeGra,and investigate the role that different an-notation decisions play for parsing.
Forthese purposes, we approximate the twotreebanks by gradually taking out or in-serting the corresponding annotation com-ponents and test the performance of a stan-dard PCFG parser on all treebank versions.Our results give an indication of whichstructures are favorable for parsing andwhich ones are not.1 IntroductionThe Wall-Street-Journal part (WSJ) of the PennTreebank (Marcus et al, 1994) plays a central rolein research on statistical treebank-based parsing.It has not only become a standard for parser eval-uation, but also the foundation for the develop-ment of new parsing models.
For the English WSJ,high accuracy parsing models have been created,some of them using extensions to classical PCFGparsing such as lexicalization and markovization(Collins, 1999; Charniak, 2000; Klein and Man-ning, 2003).
However, since most research hasbeen limited to a single language (English) andto a single treebank (WSJ), the question of howportable the parsers and their extensions are acrosslanguages and across treebanks often remainedopen.Only recently, there have been attempts to eval-uate parsing results with respect to the proper-ties and the language of the treebank that is used.Gildea (2001) investigates the effects that cer-tain treebank characteristics have on parsing re-sults, such as the distribution of verb subcatego-rization frames.
He conducts experiments on theWSJ and the Brown Corpus, parsing one of thetreebanks while having trained on the other one.He draws the conclusion that a small amount ofmatched training data is better than a large amountof unmatched training data.
Dubey and Keller(2003) analyze the difficulties that German im-poses on parsing.
They use the NeGra treebankfor their experiments and show that lexicalization,while highly effective for English, has no bene-fit for German.
This result motivates them to cre-ate a parsing model for German based on sister-head-dependencies.
Corazza et al (2004) con-duct experiments with model 2 of Collins?
parser(Collins, 1999) and the Stanford parser (Klein andManning, 2003) on two Italian treebanks.
They re-port disappointing results which they trace back tothe different difficulties of different parsing tasksin Italian and English and to differences in anno-tation styles across treebanks.In the present paper, our goal is to determinethe effects of different annotation decisions onthe results of plain PCFG parsing without exten-sions.
Our motivation is two-fold: first, we wantto present research on a language different fromEnglish, second, we want to investigate the influ-ences of annotation schemes via a realistic com-parison, i.e.
use two different annotation schemes.Therefore, we take advantage of the availabilityof two similar treebanks of German, Tu?Ba-D/Z(Telljohann et al, 2003) and NeGra (Skut et al,1997).
The strategy we adopt extends Ku?bler19(2005).
Treebanks and their annotation schemesrespectively are compared using a stepwise ap-proximation.
Annotation components correspond-ing to certain annotation decisions are taken out orinserted, submitting each time the resulting mod-ified treebank to the parser.
This method allowsus to investigate the role of single annotation deci-sions in two different environments.In section 2, we describe the annotation ofboth treebanks in detail.
Section 3 introduces themethodology used.
In section 4, we describe ourexperimental setup and discuss the results.
Section5 presents a conclusion and plans for future work.2 The Treebanks: Tu?Ba-D/Z and NeGraWith respect to treebanks, German is in a priv-ileged position.
Various treebanks are avail-able, among them are two similar ones: Ne-Gra (Skut et al, 1997), from Saarland Universityat Saarbru?cken and Tu?Ba-D/Z (Telljohann et al,2003), from the University of Tu?bingen.
NeGracontains about 20,000 sentences, Tu?Ba-D/Z about15,000, both consist of newspaper text.
In bothtreebanks, predicate argument structure is anno-tated, the core principle of the annotation being itstheory independence.
Terminal nodes are labeledwith part-of-speech tags and morphological labels,non-terminal nodes with phrase labels.
All edgesare labeled with grammatical functions.
Anno-tation was accomplished semi-automatically withthe same software tools.The main difference between the treebanks isrooted in the partial free word order of Ger-man sentences: the positions of complementsand adjuncts are of great variability.
This leadsto a high number of discontinuous constituents,even in short sentences.
An annotation schemefor German must account for that.
NeGra al-lows for crossing branches, thereby giving up thecontext-free backbone of the annotation.
Withcrossing branches, discontinuous constituents arenot a problem anymore: all children of everyconstituent, discontinuous or not, can always begrouped under the same node.
The inconvenienceof this method is that the crossing branches mustbe resolved before the treebank can be used witha (PCFG) parser.
However, this can be accom-plished easily by reattaching children of discon-tinuous constituents to higher nodes.Tu?Ba-D/Z uses another mechanism to accountfor the free word order.
Above the phrase level,an additional layer of annotation is introduced.
Itconsists of topological fields (Drach, 1937; Ho?hle,1986).
The concept of topological fields is widelyaccepted among German grammarians.
It reflectsthe empirical observation that German has threepossible sentence configurations with respect tothe position of the finite verb.
In its five fields(initial field, left sentence bracket, middle field,right sentence bracket, final field), verbal mate-rial generally resides in the two sentence brackets,while the initial field and the middle field containall other elements.
The final field contains mostlyextraposed material.
Since word order variationsgenerally do not cross field boundaries, with themodel of topological fields, the free word order ofGerman can be accounted for in a natural way.On the phrase level, the treebanks show greatdifferences, too.
NeGra does not allow for any in-termediate (?bar?)
phrasal projections.
Addition-ally, no unary productions are allowed.
This re-sults in very flat phrases: pre- and postmodifiersare attached directly to the phrase, nominal sub-jects are attached directly to the sentence, nominalmaterial within PPs doesn?t project to NPs, com-plex (non-coordinated) NPs remain flat.
Tu?Ba-D/Z, on the contrary, allows for ?deep?
annota-tion.
Intermediate productions and unary produc-tions are allowed and extensively used.To illustrate the annotation principles, the fig-ures 1 and 2 show the annotation of the sentences(1) and (2) respectively.
(1) Daru?berAbout-thatmu?mustnachgedachttoughtwerden.be?This must be tought about.?
(2) SchillenSchillenwiesrejecteddiesthatgesternyesterdayzuru?ck:VPART?Schillen rejected that yesterday.
?0 1 2 3 4500501502Dar?berPROAV??mu?VMFIN3.Sg.Pres.IndnachgedachtVVPP??werdenVAINF??.$.?
?MO HDVPOC HDHDVPOCSFigure 1: A NeGra tree200 1 2 3 4 5500 501 502 503 504505 506 507508SchillenNEnsfwiesVVFIN3sitdiesPDSasngesternADV??zur?ckPTKVZ??:$.?
?HD HD HD HD VPTNXONVXFINHDNXOAADVXV?MODVF?LK?MF?VC?SIMPXFigure 2: A Tu?Ba-D/Z tree3 Treebanks, Parsing, and ComparisonsOur goal is to determine which components ofthe annotation schemes of Tu?Ba-D/Z and NeGrahave which influence on parsing results.
A directcomparison of the parsing results shows that theTu?Ba-D/Z annotation scheme is more appropriatefor PCFG parsing than NeGra?s (see tables 2 and3).
However, this doesn?t tell us anything aboutthe role of the subparts of the annotation schemes.A first idea for a more detailed comparisoncould be to compare the results for different phrasetypes.
The problem is that this would not givemeaningful results.
NeGra noun phrases, e.g.,cover a different set of constituents than Tu?Ba-D/Znoun phrases, due to NeGra?s flat annotation andavoidance of annotation of unary NPs.
Further-more, both annotation schemes contain categoriesnot contained in the other one.
There are, e.g.,no categories in NeGra that correspond to Tu?Ba-D/Z?s field categories, while in Tu?Ba-D/Z, thereare no categories equivalent to NeGra?s categoriesfor coordinated phrases or verb phrases.We therefore pursue another approach.
We usea method introduced by Ku?bler (2005) to investi-gate the usefulness of different annotation compo-nents for parsing.
We gradually modify the tree-bank annotations in order to approximate the an-notation style of the treebanks to one another.
Thisis accomplished by taking out or inserting cer-tain components of the annotation.
For our tree-banks, this generally results in reduced structuresfor Tu?Ba-D/Z and augmented structures for Ne-Gra.
Table 1 presents three measures that cap-ture the changes between each of the modifica-tions.
The average number of child nodes of non-terminal nodes shows the degree of flatness of theannotation on phrase level.
Here, the unmodi-fied NeGra consequently shows the highest values.The average tree height relates directly to the num-ber of annotation hierarchies in the tree.
Here, theunmodified Tu?Ba-D/Z has the highest values.4 Experimental SetupFor our experiments, we use lopar (Schmid,2000), a standard PCFG parser.
We read the gram-mar and the lexicon directly off the trees togetherwith their frequencies.
The parser is given thegold POS tagging to avoid parsing errors that arecaused by wrong POS tags.
Only sentences up to alength of 40 words are considered due to memorylimitations.Traditionally, most of the work on WSJ uses thesame section of the treebank for testing.
How-ever, for our aims, this method has a shortcom-ing: since both treebanks consist of text createdby different authors, linguistic phenomena are notevenly distributed over the treebank.
When usinga whole section as test set, some phenomena mayonly occur there and thus not occur in the gram-mar.
To reduce data sparseness, we use anothertest/training-set split for the treebanks and theirvariations.
Each 10th sentence is put into the testset, all other sentences go into the training set.4.1 Preprocessing the TreebanksSince we want to read the grammars for our parserdirectly off the treebanks, preprocessing of thetreebanks is necessary due to the non-context-freenature of the original annotation.
In both tree-banks, punctuation is not included in the trees,furthermore, sentence splitting in both treebanksdoes not always coincide with the linguistic no-tion of a sentence.
This leads to sentences con-sisting of several unconnected trees.
All nodes ina sentence, i.e.
the roots and the punctation, aregrouped by a virtual root node, which may causecrossing branches.
Furthermore, the NeGra anno-tation scheme allows for crossing branches for lin-guistic reasons, as described in section 2.
All ofthe crossing branches have to be removed beforeparsing.The crossing branches caused by the NeGra an-notation scheme are removed with a small pro-gram by Thorsten Brants.
It attaches some of thechildren of discontinuous constituents to highernodes.
The virtual root node is made continu-ous by attaching all punctuation to the highestpossible location in the tree.
Pairs of parenthe-sis and quotation marks are preferably attached to21NeGra NE fi.
NE NP NE tr.
Tu?Ba Tu?
NF Tu?
NU Tu?
f Tu?
f NU Tu?
f NU NFN/T 0.41 0.70 0.50 0.41 1.21 0.89 0.54 1.00 0.42 0.35?
D/N 2.92 2.22 2.59 2.92 1.61 1.89 2.53 1.83 2.93 3.35?
H(T) 4.86 5.81 5.16 4.68 6.88 5.68 5.45 5.94 4.72 4.15Table 1: Properties of the treebank modifications1the same node, to avoid low-frequent productionsin the grammar that only differ by the position ofparenthesis marks on their right hand side.4.2 Results of the ComparisonWe use the standard parseval measures for theevaluation of parser output.
They measure the per-centage of correctly parsed constituents, in termsof precision, recall, and F-Measure.
The parseroutput of each modified treebank version is evalu-ated against the correspondingly modified test set.Unparsed sentences are fully included in the eval-uation.NeGra.
Along with the unmodified treebank,two modifications of NeGra are tested.
Both ofthem introduce annotation components present inTu?Ba-D/Z but not in NeGra.
In the first one,NE fi, we add an annotation layer of topologi-cal fields2, as existing in Tu?Ba-D/Z.
The precisionvalue benefits the most from this modification.When parsing without grammatical functions, itincreases about 6,5%.
When parsing with gram-matical functions, it increases about 14%.
Thus,the additional rules provided by a topological fieldlevel that groups phrases below the clausal levelare favorable for parsing.
The average number ofcrossing brackets per sentence increases, which isdue to the fact that there are simply more bracketsto create.A detailed evaluation of the results for nodecategories shows that the new field categories areeasy to recognize (e.g.
LF gets 97.79 F-Measure).Nearly all categories have a better precision value.However, the F-Measure for VPs is low (only26.70 while 59.41 in the unmodified treebank),while verb phrases in the unmodified Tu?Ba-D/Z(see below) are recognized with nearly 100 pointsF-Measure.
The problem here is the following.
Inthe original NeGra annotation, a verb and its com-plements are grouped under the same VP.
To pre-1explanation: N/T = node/token ratio, ?
D/N = averagenumber of daughters of non-terminal nodes, ?
H(T) = averagetree height2We are grateful to the DFKI Saarbru?cken for providingus with the topological field annotation.serve as much of the annotation as possible, thetopological fields are inserted below the VP (com-plements are grouped by a middle field node, theverb complex by the right sentence bracket).
Sincethis way, the phrase node VP resides above thefield level, it becomes difficult to recognize.In the second modification, NE NP, we approx-imate NeGra?s PPs to Tu?Ba-D/Z?s by groupingall nominal material below the PPs to separateNPs.
This modification gives us a small bene-fit in terms of precision and recall (about 2-3%).Although there are more brackets to place, thenumber of crossing parents increases only slightly,which can be attributed to the fact that below PPs,there is no room to get brackets wrong.We finally parse a version of NeGra wherefor each node movement during the resolution ofcrossing edges, a trace label was created in thecorresponding edge (NE tr).
Although this bringsthe treebank closer to the format of Tu?Ba-D/Z, theresults get even worse than in the version withouttraces.
However, the high number of unparsed sen-tences indicates that the result is not reliable due todata sparseness.NeGra NE fi.
NE NP NE tr.without grammatical functionscross.
br.
1.10 1.67 1.14 ?lab.
prec.
68.14% 74.96% 70.43% ?lab.
rec.
69.98% 70.37% 72.81% ?lab.
F1 69.05 72.59 71.60 ?not parsed 1.00% 0.10% 0.15% ?with grammatical functionscross.
br.
1.10 1.21 1.27 1.05lab.
prec.
52.67% 67.90% 59.77% 51.81%lab.
rec.
52.17% 65.18% 60.36% 49.19%lab.
F1 52.42 66.51 60.06 50.47not parsed 12.90% 1.66% 9.88% 16.01%Table 2: Parsing NeGra: ResultsTu?Ba-D/Z.
Apart from the original treebank,we test six modifications of Tu?Ba-D/Z.
In eachof the modifications, annotation material is re-moved in order to obtain NeGra-like structures.Since they are equally absent in NeGra, we deletethe annotation of topological fields in the firstmodification, Tu?
NF.
This results in small losses.22Tu?Ba Tu?
NF Tu?
NU Tu?
flat Tu?
f NU Tu?
f NU NFwithout grammatical functionscrossing brackets 2.21 1.82 1.67 1.04 0.80 1.03labeled precision 87.39% 86.31% 79.97% 86.22% 75.18% 63.05%labeled recall 83.57% 83.43% 78.52% 85.41% 76.11% 66.86%labeled F-Measure 85.44 84.85 79.24 85.81 75.64 64.90not parsed 0.07% 0.07% 2.45% 0.07% 2.99% 6.87%with grammatical functionscrossing brackets 1.84 1.82 1.79 0.98 1.01 1.12labeled precision 76.99% 68.55% 63.71% 76.93% 58.91% 45.15%labeled recall 75.30% 68.40% 62.79% 77.21% 58.92% 44.76%labeled F-Measure 76.14 68.47 63.25 77.07 58.92 44.96not parsed 0.07% 0.27% 4.49% 0.07% 7.21% 17.76%Table 3: Parsing Tu?Ba-D/Z: ResultsA closer look at category results shows thatlosses are mainly due to categories on the clausallevel; structures within fields do not deteriorate.Field categories are thus especially helpful for theclausal level.In the second modification of Tu?Ba-D/Z,Tu?
NU, unary nodes are collapsed with the goalto get structures comparable to NeGra?s.
As thefigures show, the unary nodes are very helpful,the F-Measure drops about 6 points without them.The number of crossing brackets also drops, alongwith the total number of nodes.
When parsingwith grammatical functions, taking out unary pro-ductions has a detrimental effect, F-Measure dropsabout 13 points.
A plausible explanation could bedata sparseness.
32.78% of the rules that the parserneeds to produce a correct parse don?t occur in thetraining set.An evaluation of the results for the differentcategories shows that all major phrase categoriesloose both in precision and recall.
Since fieldnodes are mostly unary, many of them disappear,but most of the middle field nodes stay becausethey generally contain more than one element.However, their recall drops about 10%.
Suppos-edly it is more difficult for the parser to annotatethe middle field ?alone?
without the other field cat-egories.We also test a version of Tu?Ba-D/Z with flat-tened phrases that mimic NeGra?s flat phrases,Tu?
flat.
With this treebank version, we get resultsvery similar to those of the unmodified treebank.The F-Measure values are slightly higher and theparser produces less crossing brackets.
A singlecategory benefits the most from this treebank mod-ification: EN-ADD, its F-Measure rising about 45points.
It was originally introduced as a markerfor named entities, which means that it has no spe-cific syntactic function.
In the Tu?Ba-D/Z versionwith flattened phrases, many of the nominal nodesbelow EN-ADD are taken out, bringing EN-ADDcloser to the lexical level.
This way, the categoryhas more meaningful context and therefore pro-duces better results.Furthermore, we test combinations of the mod-ifications.
Apart from the average tree height, thedimensions of Tu?Ba-D/Z with flattened phrasesand without unary productions (Tu?
f NU) re-semble those of the unmodified NeGra treebank,which indicates their similarity.
Nevertheless,parser results are worse on NeGra.
This indicatesthat Tu?Ba-D/Z still benefits from the remainingfield nodes.
The number of crossing branches isthe lowest in this treebank version.In the last modification that combines all mod-ifications made before (T ?U f NU NF), as ex-pected, all values drop dramatically.
F-Measureis about 5 points worse than with the unmodifiedNeGra treebank.POS tagging.
In a second round, we investigatethe benefits that gold POS tags have when makingthem available in the parser input.
We repeat allexperiments without giving the parser the perfecttagging.This leads to higher time and space require-ments during parsing, caused by the additionaltagging step.
With Tu?Ba-D/Z, NeGra, and all theirmodifications, the F-Measure results are about 3-5 points worse when parsing with grammaticalfunctions.
When parsing without them, they drop3-6 points.
We can determine two exceptions:Tu?Ba-D/Z with flattened phrases, where the F-Score drops more than 9 points when parsing withgrammatical functions, and the Tu?Ba-D/Z versionwith all modifications combined, where F-Scoredrops only a little less than 2 points.
The behavior23of the flattened Tu?Ba-D/Z relates directly to thefact that the categories that loose the most with-out gold POS tags are phrase categories (partic-ularly infinite VPs and APs).
They are directlyconditioned on the POS tagging and thus behaveaccordingly to its quality.
For the Tu?Ba-D/Z ver-sion with all modifications combined, one couldargue that the results are not reliable because ofdata sparseness, which is confirmed by the highnumber of unparsed sentences in this treebank ver-sion.
However, in all cases, less crossing bracketsare produced.To sum up, obviously, it is more difficult for theparser to build a parse tree onto an already exist-ing layer of POS-tagging.
This explains the biggernumber of unparsed sentences.
Nevertheless, interms of F-Score, the parsing results profit visiblyfrom the gold POS tagging.5 Conclusions and OutlookWe presented an analysis of the influences of theparticularities of annotation schemes on parsingresults via a comparison of two German tree-banks, NeGra and Tu?Ba-D/Z, based on a step-wise approximation of both treebanks.
The exper-iments show that as treebanks are approximated,the parsing results also get closer.
When annota-tion structure is deleted in Tu?Ba-D/Z, the numberof crossing brackets drops, but F-Measure drops,too.
When annotation structure is added in Ne-Gra, the contrary happens.
We can conclude that,being interested in good F-Measure results, thedeep Tu?Ba-D/Z structures are more appropriatefor parsing than NeGra?s flat structures.
Moreover,we have observed that it is beneficial to providethe parser with the gold POS tags at parsing time.However, we see that especially when parsing withgrammatical functions, data sparseness becomes aserious problem, making the results less reliable.Seen in the context of a parse tree, the expansionprobability of a PCFG rule just covers a subtree ofheight 1.
This is a clear deficiency of PCFGs sincethis way, e.g., the expansion probability of a VP isindependent of the choice of the verb.
Our futurework will start at this point.
We will conduct fur-ther experiments with the Stanford Parser (Kleinand Manning, 2003) which considers broader con-texts in its probability.
It uses markovization to re-duce horizontal context (right hand sides of rulesare broken up) and add vertical context (rule prob-abilities are conditioned on (grand-)parent-nodeinformation).
This way, we expect further insightsin NeGra?s an Tu?Ba-D/Z?s annotation schemes.ReferencesEugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of NAACL 2000.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Anna Corazza, Alberto Lavelli, Giorgio Satta, andRoberto Zanoli.
2004.
Analyzing an Italian tree-bank with state-of-the-art statistical parsers.
In Pro-ceedings of the 3rd Workshop on Treebanks and Lin-guistic Theories (TLT 2004).Erich Drach.
1937.
Grundgedanken der deutschenSatzlehre.
Diesterweg, Frankfurt/Main.Amit Dubey and Frank Keller.
2003.
Probabilisticparsing for German using sisterhead dependencies.In Proceedings of ACL 2003.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proceedings of EMNLP 2001.Tilman Ho?hle.
1986.
Der Begriff ?Mittelfeld?,Anmerkungen ber die Theorie der topologischenFelder.
In Akten des Siebten Internationalen Ger-manistenkongresses 1985, Go?ttingen, Germany.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of ACL2003.Sandra Ku?bler.
2005.
How do treebank annotationschemes influence parsing results?
Or how not tocompare apples and oranges.
In Proceedings ofRANLP 2005.Mitchell P. Marcus, Grace Kim, Marry AnnMarcinkiewicz, Robert MacIntyre, Ann Biew,Mark Freguson, Karen Katz, and Britta Schas-berger.
1994.
The Penn Treebank: Annotatingpredicate argument structure.
In Proceedings of the1994 Human Language Technology Workshop, HLT94, Plainsboro, NJ.Helmut Schmid.
2000.
LoPar: Design and implemen-tation.
Technical report, Universita?t Stuttgart, Ger-many.Wojciech Skut, Brigitte Krenn, Thorsten Brants, andHans Uszkoreit.
1997.
An annotation scheme forfree word order languages.
In Proceedings of ANLP1997.Heike Telljohann, Erhard W. Hinrichs, and SandraKu?bler, 2003.
Stylebook for the Tu?bingen Tree-bank of Written German (Tu?Ba-D/Z).
Seminarfu?r Sprachwissenschaft, Universita?t Tu?bingen, Ger-many.24
