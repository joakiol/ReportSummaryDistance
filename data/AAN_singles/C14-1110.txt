Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1165?1176, Dublin, Ireland, August 23-29 2014.Fast High-Accuracy Part-of-Speech Tagging by Independent ClassifiersRobert C. MooreGoogle Inc.bobmoore@google.comAbstractPart-of-speech (POS) taggers can be quite accurate, but for practical use, accuracy often has tobe sacrificed for speed.
For example, the maintainers of the Stanford tagger (Toutanova et al.,2003; Manning, 2011) recommend tagging with a model whose per tag error rate is 17% higher,relatively, than their most accurate model, to gain a factor of 10 or more in speed.
In this paper,we treat POS tagging as a single-token independent multiclass classification task.
We show thatby using a rich feature set we can obtain high tagging accuracy within this framework, and byemploying some novel feature-weight-combination and hypothesis-pruning techniques we canalso get very fast tagging with this model.
A prototype tagger implemented in Perl is tested andfound to be at least 8 times faster than any publicly available tagger reported to have comparableaccuracy on the standard Penn Treebank Wall Street Journal test set.1 IntroductionPart-of-speech (POS) tagging remains an important basic task in natural-language processing, often beingused as an initial step in addressing more complex problems such as parsing (e.g., McDonald et al.,2005) or named-entity recognition (e.g., Florian et al., 2003).
State-of-the-art-taggers typically employdiscriminatively-trained models with hidden tag-sequence features.
These models include features of theobservable input sequence, plus hidden features consisting of tag sequences up to some fixed length.With a tag-sequence model, the highest scoring tagging for an input sentence can be found by theViterbi algorithm, but exact search can be slow with a large tag set.
If tri-tag features are used, the fullsearch space is O(|T |3n), where |T | is the size of the tag set and n is the length of the sentence.
For theEnglish Penn Treebank (Marcus et al., 1993) , |T | = 45, hence |T |3= 91125.
For efficiency, some formof approximate search is normally used.
For example, both Shen et al.
(2007) and Huang et al.
(2012)use approximate search in both training and tagging.
Shen et al.
use a specialized bi-directional beamsearch in which the search order is learned at training time and applied at tagging time, along with themodel.
Huang et al.
use a more conventional left-to-right beam search, but they explore various specialvariants of the perceptron algorithm to cope with search errors during model training.
These two taggersrepresent the current state of the art on the Penn Treebank Wall Street Journal (WSJ) corpus, for modelstrained using no additional resources, as measured on the standard training/development/test data splitintroduced by Collins (2002a): 2.67% per tag error for Shen et al., and 2.65% for Huang et al.Alternatively, one may omit hidden tag-sequence features, enrich the set of observable features, andtreat tagging each token as an independent multi-class classification problem.
Toutanova et al.
(2003)were the first to note that such models could achieve fairly high accuracy for POS tagging, reportingper-tag error of 3.43% on the standard WSJ development set.
Liang et al.
(2008) report 3.2% error onthe standard WSJ test set (using a slightly smaller than standard training set), which as far as we knowis the current state of the art for WSJ POS tagging by independent classifiers.
The independent classifierapproach has the advantage of a simple model structure with a search space for tagging of O(|T |n).
Onthe other hand, while Liang et al.
?s result would have been state-of-the-art before Collins (2002a), todayThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1165it represents an error rate about 20% higher than Huang et al.
?s best result for tri-tag-based POS tagging,under similar training conditions.In the first part of this paper, we introduce new features for tagging by independent classifiers.
Weintroduce case-insensitive versions of several standard types of features, which enables our models togeneralize over different casings of the same underlying word.
We also cluster the vocabulary of theannotated training set, preserving as much information as possible about the tag probabilities for eachword, and use sequences of the resulting classes to approximate the contextual information providedby hidden tri-tag features.
With the further addition of another set of word-class features based ondistributional similarity over a large corpus of unnanotated data, we obtain a model with a WSJ test seterror of 2.66% (97.34% accuracy).In the remainder of the paper, we show how to perform fast tagging with this model.
Even withthe simple structure of an independent multiclass classifer, tagging can be slow with a rich model anda large tag set, simply because feature extraction and model scoring take so much time.
We addressthis in two ways.
First we effectively reduce the number of features that have to be considered for agiven token by combining the feature weights for more general features into those for more specificfeatures.
For example, if a word is in the training set vocabulary, none of its sublexical features needto be extracted or scored, if the weights of those features have already been combined into the weightsfor the corresponding ?whole word?
feature.
Second, we limit the number of tags considered for eachtoken by a pruning method that refines Ratnaparkhi?s (1996) tag dictionary, employing a Kneser-Ney-smoothed probability distribution over the possible tags for each word, and applying a threshold tunedto reduce the number of tags considered while minimizing loss of accuracy.
We have implemented aprototype tagger in Perl using these methods, which we find to be at least 8 times faster than any of thepublicly available taggers reported to have comparable accuracy on the standard WSJ test set.2 Models for Tagging by Independent ClassifiersWe formulate the POS-tagging task as a linear multiclass classification problem defined by a set of tagsT and a set of indicator features F .
Each training example consists of a set of features f ?
F present inthat example and a correct tag t ?
T .
The feature set f for a particular example consists of observableproperties of the token to be tagged and the tokens surrounding it.
A model is a vector w ?
<|T |?|F|indexed by feature-tag pairs.
We refer to the coordinates w(f,t)of w as feature weights.
A model wmaximizes the sum of relevant feature weights to predict a tag t(f ,w):t(f ,w) = argmaxt?T?f?fw(f,t)(1)In the remainder of this section we explain the feature sets we use and our method of training featureweights, and we evaluate the accuracy of the resulting models on the usual Wall Street Journal corpusfrom Penn Treebank III (Marcus et al., 1993).2.1 Lexical FeaturesAs noted above, the current state of the art for tagging by independent classifiers seems to be the resultspresented by Liang et al.
(2008).
Their best model uses the following set of base features for each word:Whether the first character of the word is a capital letterPrefixes of the word up to three charactersSuffixes of the word up to three charactersTwo ?shape?
features described belowThe full wordFor each base feature, Liang et al.
define three expanded features: whether the token being tagged has thebase feature, whether the preceding token has the base feature, and whether the following token has thebase feature.
The shape features were first introduced by Collins (2002b) for named-entity recognition.What we will call the ?Shape 1?
feature is a generalization of the spelling of the word with all capital1166letters treated as equivalent, all lower-case letters treated as equivalent, and all digits treated as equivalent.All other characters are treated as distinct.
In the ?Shape 2?
feature, all sequences of capital letters, allsequences of lower case letters, and all sequences of digits are treated as equivalent, regardless of thelength of the sequence or the identity of the upper case letters, lower case letters, or digits.With this feature set as our starting point, and partially drawing from the feature sets of Ratnaparkhi(1996) and Collins (2002a), we settled on the following set of base features through experimentation onthe WSJ development set:Whether the word contains a capital letterWhether the word contains a digitWhether the word contains a hyphenLower-cased prefixes of the word up to four charactersLower-cased suffixes of the word up to four charactersThe Shape 1 feature for the wordThe Shape 2 feature for the wordThe full lower-cased wordThe full wordA distributional-similarity-based class for the full wordIn all these features we ignore distinctions among digits (rather than just in the shape features, as Lianget al.
do).
For the last feature, we used 256 word classes derived by unsupervised clustering for the mostfrequent 999996 distinct tokens (ignoring distinctions among digits) in 121.6 billion tokens of English-language newswire, using the method of Uszkoreit and Brants (2008).
A 257th class was added fortokens not found in this set.
We use Liang et al.
?s mapping of all base features into expanded features forthe token being tagged, the preceding token, and the following token.
For the first token of a sentence weinclude a beginning-of-sentence feature in place of the preceding-token features, and for the last tokenof a sentence we include an end-of-sentence feature in place of the following-token features.2.2 Word-Class-Sequence FeaturesIn a hidden tri-tag model, the prediction for a particular tag tiis linked to the predictions for the precedingtag ti?1, the following tag ti+1, the preceding tag pair ?ti?2, ti?1?, the following tag pair ?ti+1, ti+2?,and the surrounding tag pair ?ti?1, ti+1?.
In tagging by independent classifiers, we do not have access toinformation regarding predictions for these nearby tags and tag combinations.To substitute for these missing features, we carry out supervised clustering of the distinct words inthe training set (again ignoring distinctions among digits) into 50 classes, attempting to maximize theinformation carried by each class regarding the tag probabilities for the words in the class.
From theseclasses, we construct the featuresc(wi?1)c(wi+1)?c(wi?2), c(wi?1)?
?c(wi+1), c(wi+2)?
?c(wi?1), c(wi+1)?The type of clustering we use here differs from the unsupervised clustering described previously.
Inassigning each word to a cluster, the unsupervised clustering algorithm looks only at adjacent words inunannoted data, while the supervised clustering algorithm looks only at the tags the word receives inthe annotated data.
The unsupervised clustering tells us what known words a large number of unknownwords are simliar to, but the supervised clustering carries much more information about what tags theknown words are likely to receive.2.2.1 Clustering AlgorithmOur supervised clustering algorithm is based on the method presented by Dhillon et al.
(2003).
Thisis similar to the well-known Lloyd algorithm for k-means clustering, but uses KL-divergence between1167probability distributions, instead of Euclidian distance, to assign items to clusters.
In our application ofthis algorithm, we simply keep moving each word to the cluster that has the most similar probabilitydistribution over tags, and then re-estimating the tag probability distributions for the clusters, until theclustering converges.
At a high-level, our algorithm is:?
For each unique word w in the training set, estimate a smoothed probability distribution p(T |w)over tags given w.?
Select k seed words, and initialize k clusters for clustering 0, with one seed word per cluster.1?
Set i = 0.?
Repeat until the assignment of words to clusters in clustering i is the same as in clustering i ?
1,returning clustering i:?
For each cluster c in clustering i, compute a probability distribution p(T |c) over tags given c,such thatp(t|c) =?w?cp(w|c)p(t|w)?
For each word w, find the cluster c that minimizes the KL-divergence DKL(p(T |w)||p(T |c)),and assign w to cluster c in clustering i+ 1.?
Set i = i+ 1.As indicated, the probability distributions p(T |c) over tags for a given cluster are computed as theword-frequency-weighted mean of probability distributions p(T |w) over tags given the words in thecluster.
The p(T |w) distributions are estimated based on the relative frequencies of each tag for a givenword, smoothed using the interpolated Kneser-Ney method (Chen and Goodman, 1999) widely used instatistical language modeling.
(See Section 3.2 for more discussion of this smoothing method applied toPOS tag prediction.
)2.2.2 Cluster InitializationOur clustering algorithm is identical to that of Dhillon et al., except for the method of initializing theclusters.
Their initialization method would assign all words with the same most likely tag to the sameinitial cluster.
Instead, we initialize the clusters using a set of seed words with the property that conflatingany two of them would result in a large loss of information about tag probabilities.We define the distance between a pair of words (w1, w2) as the total decrease resulting from treatingw1and w2as indistinguishable, in the estimated log probability, based on p(T |W ), of the referencetagging of the training data.
Letting n1be the number of occurrences in the training data of w1, andsimilarly for n2and w2, we compute the distance between w1and w2asn1DKL(p(T |w1)||pw1w2) + n2DKL(p(T |w2)||pw1w2)where pw1w2= p(T |w1?
w2), computed aspw1w2(t) =n1n1+ n2p(t|w1) +n2n1+ n2p(t|w2)We select a set S of k seed words as follows:?
Choose a maximal subset V of the training data vocabulary, such that every word in V has a differentdistribution of observed POS tags.?
Choose a random ordering of V .?
Initialize S to contain the first k words of V .1Note that most words in the training set are not assigned to any initial cluster.1168?
Find the minimum distance d between any two words in S.?
Taking each remaining word w of V in order:?
Find the minimum distance d?between w and any word in S.?
If d?> d,?
Select from S a pair of words (w?, w??)
separated by d.?
Find the minimum distance d?2between w?and any word in S other than w??.?
Find the minimum distance d?
?2between w?
?and any word in S other than w?.?
If d?2< d?
?2, remove w?from S, otherwise remove w?
?from S.?
Add w to S.?
Recompute the minimum distance d between any two words in S.2.2.3 Random restartsThe clustering we find depends on the set of seed words, which in turn depends on the order in whichthe words in V are enumerated to select the seed words.
To ensure that we find a good clustering, wetry multiple runs of the algorithm based on different random enumerations of V , returning the clusteringyielding the lowest entropy for predicting the training set tags from the clusters.We noticed in preliminary experiments that a poor clustering on the first iteration of the algorithmseldom leads to a good final clustering, so we save the training set tag entropy for the first iteration of thebest clustering found so far, and we abandon a run of the algorithm if it results in higher training set tagentropy on its first iteration than the best previously observed final clustering had on its first iteration.
Wecontinue trying different random enumerations until a fixed number of runs has passed since the currentbest clustering was found.2.2.4 Classes for unknown wordsNote that this clustering method assigns classes only to words observed in the training data.
All words(ignoring distinctions among digits) not seen in the training data are assigned to an additional class.
Intraining the tagging model, however, we treat each word that has a single occurrence in the training dataas a member of this unknown-word class, so that features based on that class will be seen in training; butat tagging time, we give all words seen in the training data the class they are assigned by the clusteringalgorithm, and apply the unknown-word class only to words not seen in the training data.2.3 Feature Weight TrainingOur models are trained by optimizing the multiclass SVM hinge loss objective (Crammer and Singer,2001) using stochastic subgradient descent as described by Zhang (2004).
We use a small, constantlearning rate of 2?8, which early in our experiments we found generally to be a good value, given thesize of our training set and the sorts of feature sets we were using.
We did not re-optimize the learningrate as we experimented with different feature sets.
We do not use a numerical regularizer (such as L1or L2), but we avoid over-fitting by using early stopping, and averaging as Collins (2002a) does withthe averaged perceptron.
To determine the stopping point, we evaluate the model on the developmentset after each pass through the training data.
We continue iterating until we have made 10 consecutivepasses through the training data without reducing the development set error, and we return the modelfrom the iteration with the lowest error.2.4 Evaluation of Tagging AccuracyWe evaluate the tagging accuracy of three models: our new model with all the features discussed above,our new model minus the unsupervised distributional clustering features (to give a ?no additional re-sources?
measurement), and the Liang et al.
model that was our starting point.
Our data is the ususalWall Street Journal corpus from Penn Treebank III (Marcus et al., 1993), split into standard training(sections 0?18), development (sections 19?21), and test (sections 22-24) sets.Table 1 shows WSJ development and test set error rates for all tokens and for unknown-word (OOV)tokens for all three models.
Our full model has an overall test set tag error rate of 2.66%, or 97.34%1169Dev Set Dev Set Test Set Test SetTagging Model All Tag OOV Tag All Tag OOV TagError % Error % Error % Error %Our full feature set 2.69 9.40 2.66 8.93Our features minus unsupervised classes 2.83 10.45 2.77 10.14Liang et al.
feature set 3.23 12.47 3.17 11.92Table 1: WSJ development and test set error rates for different feature setsaccuracy.
Omitting unsupervised word-class features results in a relative increase in the error rate of4.1% overall and 13.5% on unknown words.
The model trained on the Liang et al.
feature set givesresults consistent with their reported 3.2% test set error, but the error is 19.2% higher than the modelusing our full feature set, and 14.4% higher than our model without unsupervised word-class features.3 Efficient Tag InferenceAlthough the complexity of tag inference with our model is only O(|T |n), with a rich feature set andmany possible tags, the simple summation of feature weights and comparison of sums implied by Equa-tion 1 can still be slow.
With our full model, a given token occurrence can have up to 53 features present,and on the WSJ development set, we measured the average number of features present with a non-zeroweight for at least one tag to be 38.0.
Given 45 possible tags in the Penn Treebank tag set and our fullmodel, the average number of relevant non-zero feature weights per token on the WSJ development set is1215.0.
We reduce computational costs in two ways.
First, we introduce a method of combining featureweights that effectively reduces the number of features per token by a factor of 8.
Then we introducea refined version of a tag dictionary that reduces the number of tags considered per token by a factorof more than 12 without noticeably affecting tagging accuracy.
The combination of these techniquesreduces the number of non-zero feature weights used per token by a factor of 75, which, in our Perlimplementation, speeds up tagging by a factor of 45.3.1 Combining Feature WeightsThe base lexical feature types in our model form a natural hierarchy as follows:1.
Original case tokens1.1.
Unsupervised distributional word clusters1.2.
Lower-cased tokens1.2.1.
Lower-cased 4-character prefixes1.2.1.1.
Lower-cased 3-character prefixes1.2.1.1.1.
Lower-cased 2-character prefixes1.2.1.1.1.1.
Lower-cased 1-character prefixes1.2.2.
Lower-cased 4-character suffixes1.2.2.1.
Lower-cased 3-character suffixes1.2.2.1.1.
Lower-cased 2-character suffixes1.2.2.1.1.1.
Lower-cased 1-character suffixes1.3.
Shape 1 features1.3.1.
Shape 2 features1.3.1.1.
Contains upper case token1.3.1.2.
Contains digit1.3.1.3.
Contains hyphenThe significance of the hierarchy is that the occurrence of a base feature of any of these types fullydetermines which features of the types below it in the hierarchy also occur.
For example, given a wholetoken with its original casing, the corresponding features of all the other feature types in the hierarchyare completely determined.
Given just the lower-cased version of the token, the lower-cased prefixes1170and suffixes are determined, but the distributional word cluster and the shape features are not completelydetermined, because they depend on capitalization.2We use this hierarchy to perform a simple transformation on the trained tagging model.
For everybase lexical feature f found in the training data, we add to the value of each feature weight associatedwith that base feature, the value of all corresponding feature weights for base features below f in thehierarchy.
For instance, to the feature weight for the 3-character suffix ion, the tag NN, and the position-1 (i.e, the word preceding the word being tagged), we add the value of feature weight for the 2-charactersuffix on, the tag NN, and the position -1, plus the value of the feature weight for the 1-character suffixn, the tag NN, and the position -1.To use this transformed model, we make a corresponding modification to feature extraction in thetagger.
We carry out feature extraction top-down with respect to the base feature hierarchy, and wheneverwe find a base feature f for which there are any corresponding feature weights in the model, we skipthe extraction of all the base features below f in the hierarchy.
We can do that because the model hasbeen transformed to incorporate the weights for all the skipped features into the corresponding featureweights associated with f .
The weights for the skipped features are still kept in the model, so that theycan be used when we encounter an unknown feature of the same type as f , such as an unknown wholeword, or an unknown 4-character suffix, when we have seen the corresponding 3-character suffix.The word-class-sequence features are arranged into a similar hierarchy, which is used in a similar way.1.
?c(wi?2), c(wi?1), c(wi+1), c(wi+2)?1.1.
?c(wi?2), c(wi?1)?1.2.
?c(wi+1), c(wi+2)?1.3.
?c(wi?1), c(wi+1)?1.3.1.
c(wi?1)1.3.2. c(wi+1)Note that in this hierarchy, we have introduced a new feature type that does not actually exist in the trainedmodel, the combination of the word-class bigrams preceding and following the word being tagged.
Theweights for the features of this type are constructed from the sums of the weights of other features lowerin the hierarchy.
To keep the size of the transformed model from exploding, we limit the instances ofthis feature type to those seen at least twice in the training data.
We found this covered about 80%of the tagging decisions for the WSJ development set.
We also included in the transformed model allpossible instances (including those not observed in the training data) of the feature type 1.3 for word-class bigrams surrounding the word being tagged, which allows us to drop the feature weights for thelowest two feature types 1.3.1 and 1.3.2 after their feature weights have been added to the weights forthe word-class-bigram features.Altogether, these transformations increase the size of our full model from 151,174 features with861,111 non-zero feature weights to 392,318 features with 17,047,515 non-zero feature weights.
Whilethis may be a substantial relative increase in size, the resulting model is still not particuarly large inabsolute terms.Feature Weight Features Weights Tokens All Tag OOV TagCombination per Token per Token per Second Error % Error %No 38.0 1215.0 1100 2.69 9.40Yes 4.7 194.0 6400 2.69 9.40Table 2: WSJ development set speeds and error rates without and with feature weight combinationIn Table 2, we show the effect of these transformations on the speed of tagging the WSJ developmentset while considering all possible labels for each token.
As expected, feature weight combination has noeffect on tagging error, since it results in the same tagging decisions as the original model and featureextraction method.
The ?Features per Token?
column shows the average number of features used for2Note that we could have placed the ?contains digit?
or ?contains hyphen?
features under ?lower-cased tokens?
instead of?Shape 2?
; our choice here was arbitrary.1171each tagging decision, without and with the model and feature extraction feature-weight-combinationtransformations.
The transformations reduce this number by a factor of 8.13.
The ?Weights per Token?column is the corresponding number of non-zero feature weights used for each tagging decision.
Featureweight combination reduces this number by a factor of 6.26.The ?Tokens per Second?
measurements are rounded to two significant digits due to the limited preci-sion of our observations.
Time was measured to the nearest second, and for each tagger, the data set wasreplicated enough times for the total tagging time to fall in the range of 100 to 200 seconds.
The timesreported include only reading the sentence-split tokenized text, extracting features, and predicting tags;time to read in model parameters and initialize the corresponding data structures is not inlcuded.
Timesare for a single-threaded implementation in Perl on a Linux workstation equipped with Intel Xeon X55502.67 GHz processors.
In this implementation, feature weight combination increases tagging throughputby a factor of 5.82.3.2 Pruning Possible TagsIt has long been standard practice to prune the set of possible tags considered for each word, in order tospeed up tagging.
Ratnaparkhi (1996) may have been the first to use the common heuristic of defining atag dictionary allowing any tag for unknown words, but restricting each known word to the tags it wasobserved with in the training data.
In addition, the tag dictionary for known words is sometimes furtherpruned (e.g., Banko and Moore, 2004; Gim?enez and M`arquez, 2004, 2012) according to the relativefrequency of tags for each word.
Tags observed in the training data with less than some fixed proportionof the occurrences of a particular word are not considered as possible tags for that word in test data.In our experiments, we find these heuristics produce fast tagging, but lead to a noticable loss of ac-curacy, because known words are never allowed to be labeled with tags they were not observed with inthe training data.
This is similar to the problem of unseen n-grams in statistical language modeling, sowe apply methods developed in that field to the problem of dictionary pruning for POS tagging.
Weconstruct our tag dictionary based on a ?bigram?
model of the probability p(t|w) of a tag t given a wordw, estimated from the annotated training data.
The probabilities for tags that have never been seen witha given word, as well as all the tag probabilities for unknown words, are estimated by interpolation witha ?unigram?
distribution over the tags.To estimate the probabilities of tags given words, we use the same interpolated-Kneser-Ney-smoothed(Chen and Goodman, 1999) model that we used in Section 2.2.1 in our supervised word-clustering pro-cedure.
In this model, we estimate the probabilty p(t|w) by interpolating a discounted relative frequencyestimate with a lower-order estimate of p(t).
The lower-order estimates are based on ?diversity counts?,taking the count of a tag t to be the number of distinct words ever observed with that tag.
This has thedesirable property for POS tagging that closed-class tags receive a very low estimated probabilty of be-ing assigned to a rare or unknown word, even though they occur very frequently with a small number offrequent words.
We use a single value for the discount parameter in the Kneser-Ney formula, chosen tomaximize the estimated probability of the reference tagging of the development set.
These probabilitiesare estimated ignoring distinctions among digit characters, just as in the features of our tagging model.We construct our tag dictionary by setting a threshold on the value of p(t|w).
Whenever p(t|w) isless than or equal to the threshold, the tag t is considered not to be a possible POS tag for the word w.Our preferred threshold (p(t|w) > 0.0005) is set to prune as agressively as possible while maintainingtagging accuracy on the WSJ development set.
This threshold is applied to both known and unknownwords, which produces 24 possible tags for unknown words by applying the threshold to the lower-orderprobability estimate p(t).
Note that the probabilities we use for pruning can be viewed as posteriors ofa very simple POS tagging model, which makes inferring a tag dictionary an instance of coarse-to-fineinference with posterior pruning (Charniak et al., 2006; Weiss and Taskar, 2010).The standard tag dictionary pruning heuristics can be viewed as a application of the same approach,but with the p(t|w) probabilities being unsmoothed relative-frequency estimates for known words anda uniform distribution for unknown words.
The original Ratnaparkhi heuristic amounts to thresholdingthese probabilities at 0, with a higher threshold being applied when using additional pruning.1172Tags Weights Tokens All OOV Seen Unseen OOV Seen UnseenPruning per per per Tag Tag Tag Tag Mean Mean MeanMethod Token Token Second Error % Error % Error % Error % Tags Tags TagsNone 45.0 194.0 6400 2.69 9.40 2.19 52.0 45.0 45.0 45.0Ratnaparkhi 3.7 19.0 47000 2.81 9.40 2.07 100.0 45.0 2.3 1.3Ratnaparkhi+ 2.9 14.3 56000 2.81 9.40 2.07 100.0 45.0 1.4 1.2Kneser-Ney 3.5 16.1 49000 2.69 9.45 2.18 55.5 21.3 2.8 10.2Kneser-Ney+ 1.8 6.1 67000 2.81 9.74 2.14 83.8 10.6 1.4 2.8Table 3: WSJ development set speeds and error rates for different tag dictionary pruning methodsIn Table 3 we compare these methods of tag dictionary pruning on the WSJ development set, whencombined with our feature-weight-combination technique.
The ?Tags per Token?
column shows theaverage number of tags considered for each tagging decision, depending on the tag pruning methodused.
?Weights per Token?, ?Tokens per Second?, ?All Tag Error %?, and ?OOV Tag Error %?
are asin Table 2.
The first line repeats the experiment with no tag dictionary pruning from Table 2.
The nextline gives results for Ratnaparkhi?s dictionary pruning method, and the next line, ?Ratnaparkhi+?, givesresults for the maximum additional pruning by thresholding based on unsmoothed relative frequenciesthat does not increase overall tagging error (p(t|w) > 0.005).
We see that these taggers are much fasterthan the unpruned tagger, but noticeably less accurate.The final two lines of Table 3 are for our tag dictionary pruning method, with different pruning thresh-olds.
The ?Kneser-Ney?
line represents our preferred threshold, set to prune as agressively as possiblewithout noticeably degrading the overall tagging error on the WSJ development set.
This produces alower error rate than either Ratnaparkhi or Ratnaparkhi+ pruning, but Ratnaparkhi+ pruning results infaster tagging.
However, if we increase the pruning threshold until we match the Ratnaparkhi+ errorrate, as shown in the final ?Kneser-Ney+?
line, our method is faster than Ratnaparkhi+.The remaining columns of Table 3 provide some insight as to why Kneser-Ney-smoothed pruningwith our preferred threshold results in lower error than Ratnaparkhi and Ratnaparkhi+ pruning.
Thecolumn labeled ?Seen Tag Error %?
is the error rate for examples with word/tag pairs seen in training.The column labeled ?Unseen Tag Error %?
is the error rate for examples with word/tag pairs not seen intraining, but with a word that was seen in training.
There are 660 of the latter examples in the WSJ de-velopment set, which amounts to 0.5% of that data set.
By construction, the error rate of the Ratnaparkhiand Ratnaparkhi+ pruning methods on this subset of the data is 100%, but both the unpruned tagger andthe tagger with Kneser-Ney-smoothed pruning correctly tag nearly half of these examples.The Ratnaparkhi and Ratnaparkhi+ pruning methods are somewhat more accurate than the Kneser-Ney-smoothed pruning method on the seen word/tag pairs and the unknown words, but not enough toovercome the losses on the unseen word/tag pairs with known words.
In absolute numbers on the WSJdevelopment set, both the Ratnaparkhi and Ratnaparkhi+ pruning methods make 131 fewer errors onthe seen word/tag pairs and 2 fewer errors on the unknown words, but 294 more errors on the unseenword/tag pairs with known words, compared to Kneser-Ney-smoothed pruning method with our pre-ferred threshold.
The final three columns of Table 3 show the mean number of tags allowed by eachdictionary for these three categories of examples.
Compared to Ratnaparkhi and Ratnaparkhi+ pruning,our preferred threshold for Kneser-Ney-smoothed pruning slightly increases the number of tags consid-ered for seen word/tag pairs, substantially reduces the number of tags considered for unknown words,and substantially increases the number of tags considered for unseen word/tag pairs with known words.4 Comparison to Other TaggersWe compared our tagger to several publicly available taggers, on the standard WSJ POS tagging testset.
As far as we know, six taggers have been reported to have an error rate of less than 2.7% (accuracygreater than 97.3%) on this test set.
Three of these are publicly available: the Stanford tagger (Toutanovaet al., 2003; Manning, 2011), the Prague COMPOST tagger (Spoustov?a, et al., 2009), and the UPenn1173bidirectional tagger (Shen et al., 2007).3We tested two versions of the Stanford tagger, one basedon their most accurate model ?wsj-0-18-bidirectional-distsim?, and one based on the much faster, butless accurate model ?english-left3words-distsim?
recommended for practical use on the Stanford taggerwebsite.
The UPenn tagger is run with a beam width of 3, which is the setting that gave their best reportedresults.These taggers were all tested on on the same Linux workstation as our Perl tagger.
To obtain compa-rable speed measurements omitting time for initialization, we performed two runs with each tagger.
oneon the first 1000 sentences of the test set, and another with those 1000 sentences followed by the entiretest set replicated enough times to produce a difference in total time of at least 100 seconds.
The taggingspeed was inferred from the difference in these two times.
The Stanford tagger reports tagging timesdirectly, and these agreed with our measurements to two significant digits, which is the precision limit ofour measurements.We also report on the SVMTool tagger of Gim?enez and M`arquez (2004).
Gim?enez recently providedus with benchmarks, which he obtained with a somewhat faster processor than ours, the Intel XeonX5660 2.80 GHz.
We give results for two versions of this tagger, one in Perl and one in C++, both witha combination of left-to-right and right-to-left tagging, which gives higher accuracy with this tagger thaneither direction by itself.WSJ WSJ WSJ Brown Brown BrownTagger Implementation Tokens All Tag OOV Tag Tokens All Tag OOV TagLanguage per Second Error % Error % per Second Error % Error %This work Perl 51000 2.66 9.02 40000 3.46 10.64Stanford fast Java 80000 3.13 10.31 50000 4.47 12.62Stanford accurate Java 5900 2.67 7.90 1600 3.86 11.21COMPOST C 2600 2.57 10.03 2700 3.36 12.16UPenn Java 270 2.67 10.39 290 3.90 12.96SVMTool Perl 1340 2.86 11.37SVMTool C++ 7700 2.86 11.37Table 4: WSJ test set and Brown corpus speeds and error rates compared to publicly available taggersResults on the WSJ test set are shown in Table 4.
We include a column giving the implementationlanguage of each tagger to help interpret the results.
Generally, we would expect an algorithm imple-mented in Perl to be slower than the same algorithm implemented in Java, which in turn would probablybe slower than the same algorithm implemented in C/C++; although depending on the libraries usedand the degree of optimization in the compilers, Java can sometimes be competitive with C/C++ (See,for example, http://blog.famzah.net/2010/07/01/cpp-vs-python-vs-perl-vs-php-performance-benchmark/).
?This work?
refers to our tagger with feature weight combination and Kneser-Ney-smoothed dictio-nary pruning, with the pruning threshold set to maximize pruning without decreasing overall taggingaccuracy on the WSJ development set.
The fast Stanford tagger is the fastest overall by a wide margin,but it is also the least accurate.
Our tagger is both the second fastest and the second most accurate, hav-ing an error rate relatively 3.9% higher (absolutely 0.09% higher) than the COMPOST tagger.
But ourtagger is almost 20 times faster than COMPOST, and more than 8 times faster than the accurate Stanfordtagger, the second fastest tagger of equivalent or better accuracy.
This is despite the fact that our taggeris written in Perl, while the other high-accuracy taggers are written either in Java or C.As a final, out-of-domain evaluation, we ran the five taggers that we had direct access to on the BrownCorpus subset (3279 sentences, 83769 tokens) from the Penn Treebank.
As might be expected, taggingwas in general both slower and less accurate than on in-domain data.
Our tagger maintained its relativeposition with respect to both speed and accuracy compared to all the other taggers.
The only qualitativechange in position of any tagger is that on the Brown Corpus data, the accurate Stanford tagger is slowerthan COMPOST, which actually runs faster than it does on the WSJ test set.3A fourth tagger, the semi-supervised condensed nearest neighbor tagger of S?gaard (2011), has some released source code,but not a complete tagger nor detailed instructions on how to build the tagger S?gaard evaluates.11745 ConclusionsWe have shown that a feature-rich model for POS tagging by independent classifiers can reach taggingaccuracies comparable to several state-of-the art taggers, and we have introduced implementation strate-gies that result in much faster tagging than any other high-accuracy tagger we are aware of, despite theseother taggers being implemented in faster programming languages.A number of the techniques introduced here may have applications to other tasks.
The sort of word-class-sequence models derived by supervised clustering described in Section 2.2 may be useful for othersequence labeling tasks, such as named-entity recognition.
Our method of pruning the tag dictionarywith smoothed probability distributions could also be used for label pruning for other problems withlarge label sets.
Finally, the feature-weight-combination technique of Section 3.1 can be applied to anyrich feature space in which the features have the kind of hierarchical structure we see in POS tagging.Such feature spaces are common in NLP, since we are almost always dealing with lexical items and theirsublexical features.AcknowledgementsThanks to Chris Manning, John Bauer, and Jenny Liu for help with the Stanford tagger; to JohankaSpoustov?a and Jan Haji?c, for help with the COMPOST tagger; to Jes?us Gim?enez, for benchmarking theSVMTool tagger; and to Kuzman Ganchev and Dan Bikel, for valuable comments on earlier versions ofthis paper.ReferencesMichele Banko and Robert C. Moore.
2004.
Part of speech tagging in context.
In Proceedings of the20th International Conference on Computational Linguistics, August 23?27, Geneva, Switzerland,556?561.Eugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil, David Ellis, Isaac Haxton, CatherineHill, R. Shrivaths, Jeremy Moore, Michael Pozar, and Theresa Vu.
2006.
Multilevel coarse-to-nePCFG parsing.
In Proceedings of the Human Language Technology Conference of the North AmericanChapter of the ACL, June 4?9, New York, New York, USA, 168?175.Stanley F. Chen and Joshua T. Goodman.
1999.
An empirical study of smoothing techniques for languagemodeling.
Computer Speech and Language, 13(4):359?393.Michael Collins.
2002a.
Discriminative training methods for hidden Markov models: theory and experi-ments with perceptron algorithms.
In Proceedings of the Conference on Empirical Methods in NaturalLanguage Processing, July 6?7, Philadelphia, Pennsylvania, USA, 1?8.Michael Collins.
2002b.
Ranking algorithms for named-entity extraction: boosting and the voted per-ceptron.
In Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics,July 7?12, Philadelphia, Pennsylvania, USA, 489?486.Koby Crammer and Yoram Singer.
2001.
On the algorithmic implementation of multiclass kernel-basedvector machines.
Journal of Machine Learning Research, 2:265?292.Radu Florian, Abe Ittycheriah, Honyan Jing, and Tong Zhang.
2003.
Named entity recognition throughclassifier combination.
In Proceedings of CoNLL-2003, May 31?June 1, Edmonton, Alberta, Canada.Inderjit S. Dhillon, Subramanyam Mallela, and Rahul Kumar.
2003.
A divisive information-theoreticfeature clustering algorithm for text classification.
Journal of Machine Learning Research, 3:1265?1287.Jes?us Gim?enez and Llu?
?s M`arquez.
2004.
SVMTool: a general POS tagger generator based on supportvector machines.
In Proceedings of the 4th International Conference on Language Resources andEvaluation, May 26?28, Lisbon, Portugal, 43?46.1175Jes?us Gim?enez and Llu?
?s M`arquez.
2012.
SVMTool Technical Manual v1.4.
http://www.lsi.upc.edu/?nlp/SVMTool/SVMTool.v1.4.pdfLiang Huang, Suphan Fayong, and Yang Guo.
2012.
Structured perceptron with inexact search.
In Pro-ceedings of the 2012 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technology, June 3?8, Montreal, Quebec, Canada, 142?151.Percy Liang, Hal Daum?e III, and Dan Klein.
2008.
Structure compilation: trading structure for fea-tures.
In Proceedings of the 25th International Conference on Machine Learning, July 5?9, Helsinki,Finland, 592?599.Christopher D. Manning.
2011.
Part-of-speech tagging from 97% to 100%: Is it time for some linguis-tics?
In Alexander Gelbukh (ed.
), Computational Linguistics and Intelligent Text Processing, 12thInternational Conference, CICLing 2011, Proceedings, Part I. Lecture Notes in Computer Science6608, Springer, 171?189.Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313-330.Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of the 43rd Annual Meeting of the Association of ComputationalLinguistics, June 25?30, Ann Arbor, Michigan, USA, 91?98.Adwait Ratnaparkhi.
1996.
A maximum entropy model for part-of-speech tagging.
In Proceedings ofthe Conference on Empirical Methods in Natural Language Processing, May 17?18, Philadelphia,Pennsylvania, USA, 133?142.Libin Shen, Giorgio Satta, and Aravind K. Joshi.
2007.
Guided learning for bidirectional sequence clas-sification.
In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,June 23?30, Prague, Czech Republic, 760?767.Drahom?
?ra ?johanka?
Spoustov?a, Jan Haji?c, Jan Raab, and Miroslav Spousta.
2009.
Semi-supervisedtraining for the averaged perceptron POS tagger.
In Proceedings of the 12th Conference of the Euro-pean Chapter of the Association for Computational Linguistics, March 30?April 3, Athens, Greece,763?771.Anders S?gaard.
2011.
Semisupervised condensed nearest neighbor for part-of-speech tagging.
InProceedings of the 49th Annual Meeting of the Association of Computational Linguistics, June 19?24,Portland, Oregon, USA, 48?52.Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Human LanguageTechnology Conference of the North American Chapter of the Association for Computational Linguis-tics, May 27?June 1, Edmonton, Alberta, Canada, 173?180.David Weiss and Ben Taskar.
2010.
Structured prediction cascades.
In Proceedings of the 13th Interna-tional Conference on Artificial Intelligence and Statistics (AISTATS), May 13?15, Chia Laguna Resort,Sardinia, Italy, 916-923.Tong Zhang.
2004.
Solving large scale linear prediction problems using stochastic gradient descentalgorithms.
In Proceedings of the 21st International Conference on Machine Learning, July 4?8,Banff, Alberta, Canada, 919?926.1176
