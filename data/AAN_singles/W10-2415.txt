Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 93?101,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsAssessing the Challenge of Fine-GrainedNamed Entity Recognition and ClassificationAsif Ekbal, Eva Sourjikova, Anette Frank and Simone Paolo PonzettoDepartment of Computational LinguisticsHeidelberg University, Germany{ekbal,sourjikova,frank,ponzetto}@cl.uni-heidelberg.deAbstractNamed Entity Recognition and Classi-fication (NERC) is a well-studied NLPtask typically focused on coarse-grainednamed entity (NE) classes.
NERC formore fine-grained semantic NE classes hasnot been systematically studied.
This pa-per quantifies the difficulty of fine-grainedNERC (FG-NERC) when performed atlarge scale on the people domain.
Weapply unsupervised acquisition methodsto construct a gold standard dataset forFG-NERC.
This dataset is used to bench-mark methods for classifying NEs at var-ious levels of fine-grainedness using clas-sical NERC techniques and global contex-tual information inspired fromWord SenseDisambiguation approaches.
Our resultsindicate high difficulty of the task and pro-vide a ?strong?
baseline for future research.1 IntroductionNamed Entity Recognition and Classification (cf.Nadeau and Sekine (2007)) is a well-establishedNLP task relevant for nearly all semantic process-ing and information access applications.
NERChas been investigated using supervised (McCallumand Li, 2003), unsupervised (Etzioni et al, 2005)and semi-supervised (Pas?ca et al, 2006b) learningmethods.
It has been investigated in multilingualsettings (Tjong Kim Sang, 2002; Tjong Kim Sangand De Meulder, 2003) and special domains, e.g.biomedicine (Ananiadou et al, 2004).The classical NERC task is confined to coarse-grained named entity (NE) classes establishedin the MUC (MUC-7, 1998) or CoNLL (TjongKim Sang, 2002) competitions, typically PERS,LOC, ORG, MISC.
While most recent work con-centrates on feature engineering and robust statis-tical models for various domains, few researchersaddressed the problem of recognizing and catego-rizing fine-grained NE classes (such as biologist,composer, or athlete) in an open-domain setting.Fine-grained NERC is expected to be benefi-cial for a wide spectrum of applications, includ-ing Information Retrieval (Mandl and Womser-Hacker, 2005), Information Extraction (Pas?ca etal., 2006a) or Question-Answering (Pizzato etal., 2006).
However, manually compiling wide-coverage gazetteers for fine-grained NE classes istime-consuming and error-prone.
Also, without anextrinsic evaluation, it is difficult to define a prioriwhich classes are relevant for a particular domainor task.
Finally, prior research in FG-NERC is dif-ficult to evaluate, due to the diversity of NE classesand datasets used.Accordingly, in the interest of a general ap-proach, we address the challenge of capturing abroad range of NE classes at various levels of con-ceptual granularity.
By turning FG-NERC intoa widely applicable task, applications are free tochoose relevant NE categories for specific needs.Also, establishing a gold standard dataset for thistask enables comparative benchmarking of meth-ods.
However, the envisaged task is far from triv-ial, given that the set of possible semantic classesfor a given NE comprises the full space of NEclasses, whereas descriptive nouns may be am-biguous between a fixed set of meanings only.The paper aims to establish a general frame-work for FG-NERC by addressing two goals: (i)we automatically build a gold standard dataset ofNE instances classified in context with fine-grain-ed semantic class labels; (ii) we develop strongbaseline methods, to assess the aptness of standardNLP approaches for this task.
The two efforts arestrongly interleaved: a standardized dataset is notonly essential for (comparative) evaluation, butalso a prerequisite for classification approachesbased on supervised learning, the most successfultechniques for sequential labeling problems.932 Related workAn early approach to FG-NERC is Alfonseca andManandhar (2002), who identify it as a problemrelated to Word Sense Disambiguation (WSD).They jointly address concept hierarchy learningand instance classification using topic signatures,yet the experiments are restricted to a small on-tology of 9 classes.
Similarly, Fleischman andHovy (2002) extend previous work from Fleis-chman (2001) on locations and address the ac-quisition of instances for 8 fine-grained personclasses.
For supervised training they compile aweb corpus which is filtered using high-confidentclassifications from an initial classifier trained onseeds.
Due to the limitations of their method tocreate a good sample of training data, the perfor-mance could not be generalized to held-out data.Recent work takes the task of FG-NERC onestep further by (i) extending the number of classes,(ii) relating them to reference concept hierar-chies and (iii) exploring methods for buildingtraining and evaluation data, or applying weaklyand unsupervised learning based on high-volumedata.
Tanev and Magnini (2006) selected 10 NE-subclasses of person and location using Word-Net as a reference.
Datasets were automaticallyacquired and manually filtered.
They compareword and pattern-based supervised and a semi-supervised approach based on syntactic features.Giuliano & Gliozzo (2007, 2008) perform NEclassification against the People Ontology, an ex-cerpt of the WordNet hierarchy, comprising 21people classes populated with at least 40 instances.Using minimally supervised lexical substitutionmethods, they cast NE classification as an ontol-ogy population task ?
as opposed to recognitionand classification in context.
In a similar setting,Giuliano (2009) explores semi-supervised classifi-cation of the People Ontology classes using latentsemantic kernels, comparing models built fromWikipedia and from a news corpus.
In a differ-ent line of research Pas?ca (2007) and Pas?ca andvan Durme (2008) make use of query logs to ac-quire NEs on a large scale.
While Pas?ca (2007)extracts NEs for 10 target classes, Pas?ca and vanDurme (2008) combine web query logs and webdocuments to acquire both NE-concept pairs andconcept attributes using seeds.But while these more recent approaches all of-fer substantially novel contributions for many NEacquisition subtasks, none of them addresses thefull task of FG-NERC, i.e., recognition and clas-sification of NE tokens in context.
Compared toontology population, focusing on types, classifica-tion in raw texts needs to consider any token andcannot rely on special contexts offering indicativeclues for class membership.Bunescu and Pas?ca (2006) also perform dis-ambiguation and classification of NEs in context,yet in a different setup.
Disambiguation is per-formed into one of the known possible classesfor a NE, as determined from Wikipedia disam-biguation pages.
Contexts for training and testingare acquired from Wikipedia pages, as opposedto general text.
Disambiguation is performed us-ing vectors of co-occurring terms and a taxonomy-based kernel that integrates word-category corre-lations.
Evaluation is performed on the task ofpredicting, for a given NE in a Wikipedia pagecontext, the correct class from among its knownclasses, including one experiment that included10% of out-of-Wikipedia entities.
The categoryspace was confined to People by occupation, with8,202 subclasses.
Classification considered 110broad classes, 540 highly populated classes (w/oout-of-Wikipedia entities), and 2,847 classes in-cluding less populated ones.
This setup is diffi-cult to compare given the sense granularities em-ployed and the special Wikipedia text genre.
Eventhough classification is performed in context, thetask does not evaluate recognition.To summarize, the field has developed robustmethods for acquisition and fine-grained classifi-cation of NEs on a large scale.
But, the full taskof NE recognition and classification in context stillremains to be addressed for a wide-coverage, fine-grained semantic class inventory that can serve asa common benchmark for future research.3 Fine-grained NERC on a large-scaleWe present experiments that assess the difficultyof open-domain FG-NERC pursued at a largescale.
We concentrate on instances and classes re-ferring to people, since it is a well-studied domain(see Section 2) and structured fine-grained infor-mation can be readily applied to a well-definedend-user task such as IR, cf.
the Web PeopleSearch task (Artiles et al, 2008).
Our methodis general in that it requires only a (PoS taggedand chunked) corpus and a reference taxonomyto provide a concept hierarchy.
Given a map-ping between automatically extracted class labels94and concepts in a taxonomic resource, it can befurther extended to other domains, e.g.
locationsor the biomedical domain by leveraging open-domain taxonomies such as Yago (Suchanek etal., 2008) or WikiTaxonomy (Ponzetto and Strube,2007).
The contribution of this work is two-fold:(i) We develop an unsupervised method for ac-quiring a comprehensive dataset for FG-NERC byapplying linguistically motivated patterns to a cor-pus harvested from the Web (Section 4).
Largeamounts of NEs are acquired together with theircontexts of occurrence and with their fine-grainedclass labels which are mapped to synsets in Word-Net.
The controlled sense inventory and the tax-onomic structure offered by WordNet enables anevaluation of FG-NERC performance at differentlevels of concept granularity, as given by the depthat which the concepts are found.
As our extractionpatterns reflect a wide-spread grammatical con-struct, the method can be applied to many lan-guages and extended to other domains.
(ii) Given this automatically acquired dataset,we assess the problem of FG-NERC in a sys-tematic series of experiments, exploring the per-formance of NERC methods on different levelsof granularities.
For recognition and classifica-tion we apply standard sequential labeling tech-niques ?
i.e.
a Maximum Entropy (MaxEnt) tag-ger (Section 5.1) ?
which we adapt to this hier-archical classification problem (Section 5.2).
Totest the hypothesis of whether a sequential la-beler represents a valid choice to perform FG-NERC, we compare the latter to a MaxEnt systemtrained on a more semantically informed featureset, and a gloss-overlap method inspired by WSDapproaches (Section 5.3).4 Acquisition of a FG-NERC datasetWe present an unsupervised method that simul-taneously acquires NEs, their semantic class andcontexts of occurrence from large textual re-sources.
In order to develop a clean resource ofproperly disambiguated NEs, we develop acqui-sition patterns for a grammatical construction thatunambiguously associates proper names with theircorresponding semantic class.Pattern-based extraction of NE-concept pairs.NEs are often introduced by so-called apposi-tional structures as in (1), which overtly ex-press which semantic class (here, painter) the NE(Kandinsky) belongs to.
Appositions involvingproper names can be captured by extraction pat-terns as given in (2).
(1) .
.
.
writings of the abstract painter Kandinskyfrequently explored similarities between .
.
.
(2) a.
[the|The]?
[JJ|NN]* [NN] [NP]the abstract painter Kandinskyb.
[NP] [,]?
[a|an|the]* [JJ|NN]* [NN]W. Kandinsky, a Russian-born painter, ..Contexts like (2.a) provide a less noisy se-quence for extraction, due to the class and instancelabels being adjacent ?
in contrast to (2.b) whereany number of modifiers can intervene betweenthe two.
Accordingly, we apply in our experimentsonly a restricted version of (2.a) ?
with a deter-miner ?
to UKWAC, an English web-based cor-pus (Baroni et al, 2009) that comes in a cleaned,PoS-tagged and lemmatized form.
Due to its size(>2 billion tokens) and mixed genres, the corpusis ideally suited for acquiring large quantities ofNEs pertaining to a broad variety of open-domainsemantic classes.Filtering heuristics.
The apposition patterns aresubject to noise, due to PoS-tagging errors, aswell as special constructions, e.g.
reduced rela-tive clauses.
The former can be controlled by fre-quency filters, the latter can be circumvented byusing chunk boundary information1.
A more chal-lenging problem is to recognize whether an ex-tracted nominal is in fact a valid semantic class forNEs.
Besides, class labels can be ambiguous, sothere is uncertainty as to which class an extractedentity should be assigned to.
We apply two fil-tering strategies: we set a frequency threshold fton the number of extracted NE tokens per class,to remove infrequent class label extractions; wethen filter invalid semantic classes using informa-tion from WordNet: given the WordNet PERSONsupersense, i.e.
the lexicographer file for nouns de-noting people, we check whether the first sense ofthe class label candidate is found in PERSON.Mapping to the WordNet person domain.
Inorder to perform a hierarchical classification ofpeople, we need a taxonomy for the domain athand.
We achieve this by mapping the extractedclass labels to WordNet synsets.
In our setting, wemap against all synsets found under person#n#1,1We use YamCha (Kudo and Matsumoto, 2000) to per-form phrase chunking.95which are direct hypernyms of at least one in-stance in WordNet (CWN pers+Inst).2 Since ourgoal is to map class labels to synsets (i.e.
our fu-ture NE classes), we check each class label candi-date against all synonyms contained in the synset.At this point we have to deal with two cases: twoextracted class label candidates (synonyms suchas doctor, physician) will map to a single synset,while ambiguous class labels (e.g.
director) can bemapped to more than one synset.
In the latter case,we heuristically choose the synset which domi-nates the highest number of instances in WordNet.Mapping evaluation.
We evaluated the cover-age of our mapping for two sets of class labelsextracted for two different frequency thresholds:ft = 40 and ft = 1.
With ft = 40, we cover31.1% of the synsets found under person#n#1 inWordNet, i.e.
the set of classes CWN pers+Inst;conversely, 45.8% of the extracted class labels canbe successfully mapped to CWN pers+Inst.
Forthreshold ft = 1, we are able to map to 87.9%of CWN pers+Inst, with only 20.1% of extractedclasses mapped to CWN pers+Inst.
For the re-maining 79.9% of class labels (e.g.
goalkeeper,chancellor, superstar) that have no instances inWordNet, we manually inspected 20 classes, in 20contexts each, and established that 76% of themare appropriate NE person classes.For threshold ft = 40, we obtain 153 class labelswhich are mapped to 146 synsets.
Ten class labelsare mapped to more than one synset.
Using ourmapping heuristic based on the majority instanceclass, we successfully disambiguate all of them.However, since we only map to CWN pers+Inst,we introduce errors for 5 classes.
E.g.
?manager?incorrectly gets mapped to manager#n#2, sincethe latter is the only synset containing instances.For these cases we manually corrected the auto-matic mapping.A taxonomy for FG-NERC.
We create our goldstandard taxonomy of semantic classes by start-ing with the 146 synsets obtained from the map-ping, including the 5 classes that were manuallycorrected.
Since we concentrate on the peopledomain, we additionally remove 5 classes thatcan refer to other domains as well (e.g.
carrier,guide).
Given the remaining 141 synsets, we se-lect the portion of WordNet rooted at person#n#12We use WordNet version 3.0.
With w#p#i we denote thei-th sense of a wordw with part of speech p.
E.g., person#n#1is defined as { person, individual .
.
.
}).Level #C #C w/inst #inst #inst/C % of inst1 1 0 0 - -2 29 8 2,662 332 5.493 57 37 18,229 493 37.584 63 46 18,422 401 37.945 37 30 6,231 208 12.846 18 13 2,366 182 4.887 6 5 423 85 0.878 2 2 179 90 0.36all 213 141 48,512 344 100Table 1: Level-wise statistics of classes and in-stances across the FG-NERC person taxonomy.which contains them, together with any interven-ing synset found along the WordNet hierarchy.Given this WordNet excerpt, the extracted NE to-kens are then appended to the respective synsets inthe hierarchy.
Statistics of the resulting WordNetfragment augmented with instances are given inTable 1.
The taxonomy has a maximum depth of 8,and contains 213 synsets, i.e.
NE classes (see col-umn 2).
83.5% of the 31,819 extracted instances(type-level) sit in leaf nodes.
The classes automat-ically refer back to the acquired appositional con-texts.
Table 1 gives statistics about the number ofinstances (token-level) acquired for classes at dif-ferent embedding levels.
In total we have at ourdisposal 48,512 instances (token-level) in apposi-tional contexts.
The type-token ratio is 1.52.Gold standard validation.
To create a goldstandard dataset of entities in context labeled withfine-grained classes, we first randomly select 20classes, as well as an additional 18 which arealso found in the People Ontology (Giuliano andGliozzo, 2008).
For each class, we randomly se-lect 40 occurrences of instances in context, i.e.the words co-occurring in a window of 60 tokensbefore and after the instance.
We asked four an-notators to label these extractions for correctness,and to provide the correct label for the incorrectcases, if one was available.
Only 52 contexts outof 1520 were labeled as incorrect, thus giving us96.58% accuracy on our automatically extracteddata.
The manually validated dataset is used toprovide a ground-truth for FG-NERC.
However,the noun (e.g.
hunter) denoting the NE class is re-moved from these contexts for training and testingin all experiments.
This is because, due to the ex-traction method based on POS-patterns denotingappositions, class labels are known a priori to oc-cur in the context of an instance and thus identifythem with high precision.965 Methodology for FG-NERCWe develop methods to perform FG-NERC usingstandard techniques developed for coarse-grainedNERC and WSD.
These are applied to our datasetfrom Section 4, in order to measure performanceat different levels of semantic class granularity, i.e.corresponding to the depth of the semantic classesfound in our WordNet fragment.
We start in Sec-tion 5.1 to present a Maximum Entropy model toperform coarse-grained NERC and we extend itto perform multiclass classification in a hierarchi-cal taxonomy (Section 5.2).
We then present inSection 5.3 an alternative proposal to perform FG-NERC using global context information, as foundin state-of-the-art approaches to supervised andunsupervised WSD.5.1 NERC using a MaxEnt taggerOur baseline system is modeled following a Maxi-mum Entropy approach (Bender et al, 2003, interalia).
The MaxEnt model produces a probabilityfor each class label t (the NE tag) of a classifica-tion instance, conditioned on its context of occur-rence h. This probability is calculated by:P (t|h) =1Z(h)exp?
?n?j=1?jfj(h, t)??
(1)where fj(h, t) is the j-th feature with associatedweight ?j and Z(h) is a normalization constant toensure a proper probability distribution.3 Given aword wi to be classified as Beginning, Inside orOutside (IOB) of a NE, we extract as features:1.
Context words.
The words occurring withinthe context window wi+2i?2 = wi?2 .
.
.
wi+2.2.
Word prefix and suffix.
Word prefix and suffixcharacter sequences of length up to n.3.
Infrequent word.
A feature that fires if wi oc-curs in the training set less frequently than agiven threshold (i.e.
below 10 occurrences).4.
Part-of-Speech (PoS) and chunk informa-tion.
The PoS and chunk labels of wi.5.
Capitalization.
A binary feature that checkswhether wi starts with a capital letter or not.6.
Word length.
A binary feature that fires ifthe length of wi is smaller than a pre-definedthreshold (i.e.
less than 5 characters).3In our implementation, we use the OpenNLP MaxEnt li-brary (http://maxent.sourceforge.net).7.
Digit and symbol features.
Three featurescheck whether wi contains digit strings, non-characters (e.g.
slashes) or number expressions.8.
Dynamic feature.
The tag ti?1 of the wordwi?1 preceding wi in the sequence wn1 .5.2 MaxEnt extension for FG-NERCExtension to hierarchical classification.
Weapply our baseline NERC system to FG-NERC.Given a word in context, the task consists of recog-nizing it as a NE, and classifying it into the appro-priate semantic class from our person taxonomy.We approach this as a hierarchical classificationtask by generating a binary classifier4 with sepa-rate training and test sets for each node in the tree.To perform level-wise classification from coarseto fine-grained classes, we need to adjust the classlabels and their corresponding training and test in-stances for each experiment.
For classification atthe deepest level, each concept contains the in-stances of the original dataset.
For classificationat higher levels we leverage the semantics of theWordNet hyponym relations and expand the setof target classes (i.e.
synsets) of a given level tocontain all instances of hyponym synsets.
Givena set I of classification instances for a given tar-get class c, we add all instances labeled with thehyponyms of c to I .
All other instances (not inthat subtree) are labeled as being Outside (O-) aNE.
This approach ensures that, for each node, thedataset contains two classes (NE and O) only, andimplicitly ?propagates?
the instances up the tree.As a result, non-leaf nodes that did not have anyinstance in the original dataset become populated.Also, the classification of classes at higher levelsis based on larger datasets.Extension to multiclass classification.
Sincewe train a binary classifier for each node of thetree, we apply two methods to infer multiclass de-cisions from these binary classifiers, namely level-wise and global multiclass classification.
In bothparadigms, we combine the single decisions ofthe individual classifiers with the winner-takes-allstrategy, using weighted voting.
The weights arecalculated based on the confidence value for thecorresponding class, i.e., its conditional probabil-ity according to Equation (1).
The output label isselected randomly in case of ties.4The IOB tagging scheme normally assigns three differentlabels, i.e.
Inside (I-), Outside (O-) and Beginning (B-) ofa chunk.
However, our dataset does not have any instancelabeled as B-, since it does not contain any adjacent NEs.97For level-wise classification, we combine onlyclassifiers at the same level of embedding.
Givenn concepts at level l, we have n possible out-put labels for each word.
The output label for aclassification instance is determined by the highestweighted vote among all binary classifiers at levell.
For global classification we combine all binaryclassifiers of the entire tree using weighted votingto determine the winning class label.
The weightsare calculated based on the product of confidencevalue and depth of the corresponding class in thetree.5.3 FG-NERC using global contextsFG-NERC is a more demanding task than ?classi-cal?
NERC, due to the larger amount of classes,the paucity of examples for each class, and theincreasingly subtle semantic differences betweenthese classes.
For such a task contextual informa-tion is expected to be very informative ?
e.g.
if anentity co-occurs in context with ?Nobel prize?, thisprovides evidence that it is likely to be a scien-tist or scholar.
However, the context window usedby our baseline MaxEnt tagger is very local, in-cluding at most the two preceding and succeedingwords.
Hence, the classifier is not able to captureinformative contextual clues in a larger context.Previous work has related FG-NERC to WSDapproaches (Alfonseca and Manandhar, 2002).Accordingly, we investigate two context-sensitiveapproaches inspired from WSD proposals, whichconsider a more global context for classification.We first define a new feature set to induce a newMaxEnt model (MaxEnt-B) which only uses lexi-cal features from a larger context window, as usedin standard supervised WSD (Lee and Ng, 2002):1.
PoS context.
The part-of-speech occur-ring within the context window posi+3i?3 =posi?3 .
.
.
posi+3.2.
Local collocation.
Local collocations Cnm sur-rounding wi.
We use C?2,?1 and C1,2.3.
Content words in surrounding context.
Weconsider all unigrams in contexts wi+3i?3 =wi?3 .
.
.
wi+3 of wi (crossing sentence bound-aries) for the entire training data.
We convert to-kens to lower case, remove stopwords, numbersand punctuation symbols.
We define a featurevector of length 10 using the 10 most frequentcontent words.
Given a classification instance,the feature corresponding to token t is set to 1iff the context wi+3i?3 of wi contains t.In addition, we use a Lesk-like method (Lesk,1986) which labels instances in context with theWordNet synset whose gloss has the maximumoverlap with the glosses of the senses of its wordsin context.
Given the small context provided bytheWordNet glosses, we follow Banerjee and Ped-ersen (2003) and expand these to also include thewords from the glosses of the hypernym and hy-ponym synsets.6 Experiments6.1 Benchmarking on coarse-grained NERCWe benchmark the performance of our baselineMaxEnt classifier using the feature set from Sec-tion 5.1 (MaxEnt-A henceforth) on the CoNLL-2003 shared task dataset (Tjong Kim Sang andDe Meulder, 2003), the de-facto standard for eval-uating coarse-grained NERC systems.In MaxEnt modeling, feature selection is a cru-cial problem and key to improving classificationperformance.
MaxEnt, however, does not providemethods for automatic feature selection.
We there-fore experimented with various combinations offeatures standardly used for NERC (1-8 of Section5.1).
Model parameters are computed with 200iterations without feature frequency cutoff.
Thebest configuration is found by optimizing the F1measure on the development data with various fea-ture representations.
The chosen features are: 1, 2(with n = 3), 4, 5, 6, 7 and 8.
Evaluation on thetest set is performed blindly, using this feature set.The results are presented in Table 2.TheMaxEnt labeler achieves performance com-parable with the CoNLL-2003 task participants,ranking 12th among the 16 systems participatingin the task, with a 2 point margin off the F1 of themost similar system of Bender et al (2003) and7 points below the best-performing system (Flo-rian et al, 2003).
The former used a relativelycomplex set of features and different gazetteersextracted from unannotated data.
The latter com-bined four diverse classifiers, namely a robust lin-ear classifier, maximum entropy, transformation-based learning and a hidden Markov model.
Theyused different feature sets, unannotated data andan additional NE tagger.
In comparison, ourNERC system is simpler and based on a small setof features that can be easily obtained for manylanguages.
Besides, it does not make use of anyexternal resources and still shows state-of-the-artperformance on the overall data.98Recall Precision F?=1PER 83.02% 81.40% 82.21%LOC 88.47% 88.19% 88.23%ORG 77.20% 68.03% 72.23%MISC 81.20% 83.92% 82.54%Overall 83.11% 80.47% 81.77%Table 2: Results on the CoNLL-2003 test data.Set # tokens # NEsTraining 2,431,041 38,810Development 478,871 9,702Test 181,490 1,520Table 3: Statistics for training, dev and test sets.6.2 Evaluating FG-NERCExperimental setting.
For the task of FG-NERC, we compare the performance of MaxEnt-A with the MaxEnt-B model from Section 5.3 andthe Lesk method.
The data is partitioned into train-ing and development sets by randomly selecting80%-20% of the contexts in which the NEs occur.We use the held-out, manually validated gold stan-dard from Section 4 for blind evaluation.
Statisticsfor the dataset are reported in Table 3.We build a MaxEnt model for each FG-NEclass, using the features that performed best onthe CoNLL task, except the digit and dynamicNE features (MaxEnt-A), and context features 1-3 of Section 5.3 (MaxEnt-B).
Model parametersare computed in the same way as for coarse-grained NERC.
Table 3 shows that our training setis highly unbalanced.
The ratio between positive(NEs) and negative examples (i.e.
O classificationinstances) at the topmost level is 63:1.
Also, withincreasing levels of fine-grainedness, the numberof negative (-O) NE classes is increasing for eachbinary classifier.
We observed on the develop-ment set that this skewed distribution heavily bi-ases the classifiers towards the negative category,and accordingly investigated sampling techniquesto make the ratio of positive and negative examplesmore balanced.
We experiment with a samplingstrategy that over-samples the positive examplesand under-samples the negative ones.
We definevarious ratios of over-sampling depending uponthe number of positive examples in the originaltraining set.
Table 4 lists the factors (f ) of over-sampling applied to the original positive samples(P ), with minimum and maximum sizes of the ob-factor f size of P min P ?
max P ?20 ?
P 1 ?
2K 20 40K15 ?
P 2K ?
5K 30K 75K10 ?
P 5K ?
10K 50K 100K5 ?
P 10K ?
20K 50K 100K2 ?
P 20K ?
50K 40K 100KP 50K ?
.
.
.
50K >50KTable 4: Oversampling of positive samples.MaxEnt-A MaxEnt-BLevelR P F1 R P F11 98.7 85.0 91.4 95.1 83.0 88.62 96.0 65.5 77.9 48.1 46.3 47.23 95.3 54.3 69.3 43.3 41.1 42.24 86.8 52.8 65.6 41.1 37.2 39.15 90.4 45.9 60.9 49.2 21.5 29.96 91.6 36.9 52.6 51.7 13.2 21.17 89.5 31.8 46.9 42.2 10.2 16.48 100.0 19.9 66.7 87.1 8.1 14.7global 85.1 43.2 57.3 61.9 26.6 37.2hierarchical 87.7 44.8 59.4 64.5 29.5 40.5Table 6: Level-wise NE recognition & classifica-tion evaluation (in %).tained oversampled sets P ?
for different ranges oforiginal sizes of P .5 Oversampling is done with-out replacement.
The number of negative instan-ces is always downsampled on the basis of P ?
toyield a 1:5 ratio of positive and negative samples,a ratio we estimated from the CoNLL-2003 data.Level-wise evaluation results on the FG-NEclassification-only (NEC) task for the MaxEntclassifiers and Lesk are given in Table 5.
Table6 reports results for the evaluation of the MaxEntmodel performing both classification and recog-nition.
As for coarse-grained NERC, we evaluateusing the standard metrics of recall (R), precision(P) and balanced F-measure (F1).
As baseline, weuse a majority class assignment ?
i.e.
at each level,we label all instances with the most frequent classlabel.
For global FG-NE classification, reported inTable 5, the original fine-grained classes are con-sidered, across the entire class hierarchy.
Globalevaluation is performed by counting exact labelpredictions on the entire hierarchy (global) and us-ing the evaluation metric of Melamed and Resnik(2000, hierarchical).
As baseline we assume themost frequent class label in the training set.Discussion.
All methods perform reasonablywell, indicating the feasibility of the task.
For theMaxEnt models, Table 5 shows a general high re-call and decreasing precision as we move down thehierarchy.
Degradation in the overall F1 score is5Sampling ratios are determined on the development set.99Baseline MaxEnt-A MaxEnt-B LeskLevelR P F1 R P F1 R P F1 R P F11 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.02 28.4 25.9 27.1 85.8 88.6 87.0 79.5 84.9 82.2 16.4 19.7 17.93 27.9 23.1 25.2 83.9 88.1 85.9 75.5 79.8 77.5 16.2 16.2 16.24 18.8 20.4 19.5 74.6 85.0 79.5 65.4 71.3 68.2 11.3 11.3 11.35 25.8 19.0 21.9 78.8 83.4 80.9 78.6 74.1 76.3 13.5 14 13.86 24.7 7.8 11.9 88.5 73.6 80.4 78.7 74.1 75.7 33.2 37.5 35.27 19.1 5.34 8.3 79.2 76.5 77.8 78.1 72.7 75.3 49.4 49.4 49.48 34.2 2.9 5.5 82.8 73.8 78.1 81.1 71.1 75.8 0.1 0.1 0.1global 34.6 18.5 24.1 81.1 84.2 82.6 78.0 74.2 76.6 36.5 38.6 37.5hierarchical 33.0 21.2 25.8 83.5 86.2 84.8 78.2 77.8 78.1 36.6 38.7 37.6Table 5: Level-wise evaluation of fine-grained NE classification techniques (in %).given by the increasingly limited amount of classinstances found towards the low regions of the tree(down to an average of 85 and 90 instances perclass for levels 7 and 8, respectively) (cf.
Table 1).The ?classical?
feature set (MaxEnt-A) yields bet-ter performance compared to the semantic featureset (MaxEnt-B).
However MaxEnt-B still achievesa respectable performance, given that it contains afew semantic features only.The MaxEnt classifiers achieve a far better per-formance than Lesk.
This is in-line with previ-ous findings in WSD, namely unsupervised fine-grained disambiguation methods rarely perform-ing above the baseline, and suggests that Lesk canbe merely used as a ?strong?
baseline.
Error anal-ysis showed that it performs poorly due to the lim-ited context provided by the WordNet glosses, andthe limited impact of gloss expansions derivingfrom the low connectivity between synsets.Comparison of Tables 5 and 6 shows that per-formance decreases considerably for a classifierthat not only assigns fine-grained classes, but alsodetects which tokens actually are NEs.
As forthe classification-only task, the performance de-creases as one moves to lower levels.
This in-dicates that the complexity of the task is propor-tional to the fine-grainedness of the class inven-tory.
MaxEnt-B, lacking ?classical?
NER features,shows dramatic losses, compared to MaxEnt-A.Comparison to other work.
We compared theperformance of our system based on global classi-fication (one vs. rest) against the figures reportedfor individual categories in Giuliano (2009).
TheMaxEnt-A system compares favorably, although itconsiders (i) more classes at each level ?
i.e.
213vs.
21 ?
and (ii) classifies NEs at finer-grained lev-els ?
i.e.
8 vs. 4 maximum depth in the respec-tive WordNet fragments.
We achieve overall mi-cro average R, P and F1 values of 87.5%, 85.7%and 86.6%, respectively, compared to Giuliano?s79.6%, 80.9% and 80.2%.
Due to the different se-tups and data used, these figures do not offer a ba-sis for true comparison.
However, the figures sug-gest that our system achieves respectable perfor-mance on a more complex classification problem.7 ConclusionsWe presented a method to perform FG-NERC ona large scale.
Our contribution lies in the def-inition of a benchmarking setup for this task interms of gold standard datasets and strong base-line methods provided by a MaxEnt classifier.
Weproposed a pattern-based approach for the acqui-sition of fined-grained NE semantic classes andinstances.
This corpus-based method relies onlyon the availability of large text corpora, such asthe WaCky corpora, in contrast to resources diffi-cult to obtain, such as query-logs (Pas?ca and vanDurme, 2008).
It makes use of a very large Webcorpus to extract instances from open-domain con-texts ?
in contrast to standard NERC approaches,which are tailored for newswire data and do notgeneralize well across domains.
Our gold stan-dard training and test datasets are currently basedonly on appositional patterns6.
Therefore, it doesnot include the full spectrum of constructions inwhich instances can be found in context.
Futurework will investigate semi-supervised and heuris-tics (e.g.
?one sense per discourse?)
methods to ex-pand the data with examples from follow-up men-tions, e.g.
co-occurring in the same document.Our MaxEnt models still perform very localclassification decisions, relying on separate mod-els for each semantic class.
We accordingly plan toexplore both global models operating on the over-all hierarchy, and more informative feature sets.6The data are available for research purposes at http://www.cl.uni-heidelberg.de/fgnerc.100ReferencesEnrique Alfonseca and Suresh Manandhar.
2002.An unsupervised method for general named entityrecognition and automated concept discovery.
InProc.
of GWC-02.S.
Ananiadou, C. Friedman, and J.I.
Tsujii.
2004.Special issue on named entity recognition inbiomedicine.
Journal of Biomedical Informatics,37(6).Javier Artiles, Satoshi Sekine, and Julio Gonzalo.2008.
Web people search.
In Proc.
of LREC ?08.Satanjeev Banerjee and Ted Pedersen.
2003.
Extendedgloss overlap as a measure of semantic relatedness.In Proc.
of IJCAI-03, pages 805?810.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The wacky wide web:A collection of very large linguistically processedweb-crawled corpora.
Journal of Language Re-sources and Evaluation, 43(3):209?226.Oliver Bender, Franz Josef Och, and Hermann Ney.2003.
Maximum entropy models for named entityrecognition.
In Proc.
of CoNLL-03, pages 148?151.Razvan Bunescu and Marius Pas?ca.
2006.
Using en-cyclopedic knowledge for named entity disambigua-tion.
In Proc.
of EACL-06, pages 9?16.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Un-supervised named-entity extraction from the web:an experimental study.
Artificial Intelligence,165(1):91?134.Michael Fleischman and Eduard Hovy.
2002.
Finegrained classification of named entities.
In Proc.
ofCOLING-02, pages 1?7.Michael Fleischman.
2001.
Automated subcategoriza-tion of named entities.
In Proc.
of the ACL 2001Student Workshop.Radu Florian, Abe Ittycheriah, Hongyan Jing, andTong Zhang.
2003.
Named entity recognitionthrough classifier combination.
In Proc.
of CoNLL-03, pages 168?171.Claudio Giuliano and Alfio Gliozzo.
2007.
Instancebased lexical entailment for ontology population.
InProc.
of ACL-07, pages 248?256.Claudio Giuliano and Alfio Gliozzo.
2008.
Instance-based ontology population exploiting named-entitysubstitution.
In Proc.
of COLING-ACL-08, pages265?272.Claudio Giuliano.
2009.
Fine-grained classification ofnamed entities exploiting latent semantic kernels.
InProc.
of CoNLL-09, pages 201?209.Taku Kudo and Yuji Matsumoto.
2000.
Use of SupportVector Machines for chunk identification.
In Proc.of CoNLL-00, pages 142?144.Yoong Keok Lee and Hwee Tou Ng.
2002.
An empir-ical evaluation of knowledge sources and learningalgorithms for word sense disambiguation.
In Proc.of EMNLP-02, pages 41?48.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: how to tell apine cone from an ice cream cone.
In Proceedingsof the ACL-SIGDOC Conference, pages 24?26.Thomas Mandl and Christa Womser-Hacker.
2005.The effect of named entities on effectiveness incross-language information retrieval evaluation.
InProc.
of ACM SAC 2005, pages 1059?1064.Andrew McCallum and Andrew Li.
2003.
Early re-sults for named entity recognition with conditionalrandom fields, features induction and web-enhancedlexicons.
In Proc.
of CoNLL-03, pages 188?191.Dan Melamed and Philip Resnik.
2000.
Tagger evalu-ation given hierarchical tag sets.
Computers and theHumanities, pages 79?84.MUC-7.
1998.
Proceedings of the Seventh Mes-sage Understanding Conference (MUC-7).
MorganKaufmann, San Francisco, Cal.David Nadeau and Satoshi Sekine.
2007.
A survey ofnamed entity recognition and classification.
Journalof Linguisticae Investigationes, 30(1).Marius Pas?ca and Benjamin van Durme.
2008.Weakly-supervised acquisition of open-domainclasses and class attributes from web documents andquery logs.
In Proc.
of ACL-08, pages 19?27.M.
Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.2006a.
Names and similarities on the web: Fact ex-traction in the fast lane.
In Proc.
of COLING-ACL-06, pages 809?816.Marius Pas?ca, Dekang Lin, Jeffrey Bigham, AndreiLifchits, and Alpa Jain.
2006b.
Organizing andsearching the world wide web of facts ?
Step one:The one-million fact extraction challenge.
In Proc.of AAAI-06, pages 1400?1405.Marius Pas?ca.
2007.
Weakly-supervised discovery ofnamed entities using web search queries.
In Proc.
ofCIKM-2007, pages 683?690.Luiz Augusto Pizzato, Diego Molla, and Ce?cile Paris.2006.
Pseudo relevance feedback using named enti-ties for question answering.
In Proc.
of ALTW-2006,pages 83?90.Simone Paolo Ponzetto andMichael Strube.
2007.
De-riving a large scale taxonomy from Wikipedia.
InProc.
of AAAI-07, pages 1440?1445.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2008.
YAGO: A Large Ontology fromWikipedia and WordNet.
Elsevier Journal of WebSemantics, 6(3):203?217.Hristo Tanev and Bernardo Magnini.
2006.
Weaklysupervised approaches for ontology population.
InProc.
of EACL-06, pages 17?24.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CoNLL-2003 SharedTask: Language-independent Named Entity Recog-nition.
In Proc.
of CoNLL-03, pages 127?132.Erik F. Tjong Kim Sang.
2002.
Introduction to theCoNLL-2002 Shared Task: Language-independentNamed Entity Recognition.
In Proc.
of CoNLL-02,pages 155?158.101
