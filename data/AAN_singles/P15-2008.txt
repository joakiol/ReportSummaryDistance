Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 45?49,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsDocument Classification by Inversion ofDistributed Language RepresentationsMatt TaddyUniversity of Chicago Booth School of Businesstaddy@chicagobooth.eduAbstractThere have been many recent advancesin the structure and measurement of dis-tributed language models: those that mapfrom words to a vector-space that is rich ininformation about word choice and com-position.
This vector-space is the dis-tributed language representation.The goal of this note is to point outthat any distributed representation can beturned into a classifier through inversionvia Bayes rule.
The approach is simpleand modular, in that it will work withany language representation whose train-ing can be formulated as optimizing aprobability model.
In our application to 2million sentences from Yelp reviews, wealso find that it performs as well as or bet-ter than complex purpose-built algorithms.1 IntroductionDistributed, or vector-space, language representa-tions V consist of a location, or embedding, forevery vocabulary word in RK, where K is the di-mension of the latent representation space.
Theselocations are learned to optimize, perhaps approx-imately, an objective function defined on the origi-nal text such as a likelihood for word occurrences.A popular example is the Word2Vec machineryof Mikolov et al (2013).
This trains the distributedrepresentation to be useful as an input layer forprediction of words from their neighbors in a Skip-gram likelihood.
That is, to maximizet+b?j 6=t, j=t?blog pV(wsj| wst) (1)summed across all words wstin all sentences ws,where b is the skip-gram window (truncated by theends of the sentence) and pV(wsj|wst) is a neuralnetwork classifier that takes vector representationsfor wstand wsjas input (see Section 2).Distributed language representations have beenstudied since the early work on neural networks(Rumelhart et al, 1986) and have long been ap-plied in natural language processing (Morin andBengio, 2005).
The models are generating muchrecent interest due to the large performance gainsfrom the newer systems, including Word2Vec andthe Glove model of Pennington et al (2014), ob-served in, e.g., word prediction, word analogyidentification, and named entity recognition.Given the success of these new models, re-searchers have begun searching for ways to adaptthe representations for use in document classifica-tion tasks such as sentiment prediction or authoridentification.
One naive approach is to use ag-gregated word vectors across a document (e.g., adocument?s average word-vector location) as inputto a standard classifier (e.g., logistic regression).However, a document is actually an ordered pathof locations throughRK, and simple averaging de-stroys much of the available information.More sophisticated aggregation is proposed inSocher et al (2011; 2013), where recursive neu-ral networks are used to combine the word vectorsthrough the estimated parse tree for each sentence.Alternatively, Le and Mikolov?s Doc2Vec (2014)adds document labels to the conditioning set in (1)and has them influence the skip-gram likelihoodthrough a latent input vector location in V .
In eachcase, the end product is a distributed representa-tion for every sentence (or document for Doc2Vec)that can be used as input to a generic classifier.1.1 Bayesian InversionThese approaches all add considerable model andestimation complexity to the original underlyingdistributed representation.
We are proposing asimple alternative that turns fitted distributed lan-guage representations into document classifiers45without any additional modeling or estimation.Write the probability model that the represen-tation V has been trained to optimize (likeli-hood maximize) as pV(d), where document d ={w1, ...wS} is a set of sentences ?
ordered vectorsof word identities.
For example, in Word2Vec theskip-gram likelihood in (1) yieldslog pV(d) =?s?tt+b?j 6=t, j=t?blog pVy(wsj| wst).
(2)Even when such a likelihood is not explicit it willbe implied by the objective function that is opti-mized during training.Now suppose that your training documents aregrouped by class label, y ?
{1 .
.
.
C}.
We cantrain separate distributed language representationsfor each set of documents as partitioned by y;for example, fit Word2Vec independently on eachsub-corpus Dc= {di: yi= c} and obtain thelabeled distributed representation map Vc.
A newdocument d has probability pVc(d) if we treat it asa member of class c, and Bayes rule impliesp(y|d) =pVy(d)piy?cpVc(d)pic(3)where picis our prior probability on class label c.Thus distributed language representationstrained separately for each class label yielddirectly a document classification rule via (3).This approach has a number of attractive qualities.Simplicity: The inversion strategy works for anymodel of language that can (or its training can) beinterpreted as a probabilistic model.
This makesfor easy implementation in systems that are al-ready engineered to fit such language represen-tations, leading to faster deployment and lowerdevelopment costs.
The strategy is also inter-pretable: whatever intuition one has about the dis-tributed language model can be applied directly tothe inversion-based classification rule.
Inversionadds a plausible model for reader understandingon top of any given language representation.Scalability: when working with massive corporait is often useful to split the data into blocks as partof distributed computing strategies.
Our model ofclassification via inversion provides a convenienttop-level partitioning of the data.
An efficient sys-tem could fit separate by-class language represen-tations, which will provide for document classi-fication as in this article as well as class-specificanswers for NLP tasks such as word prediction oranalogy.
When one wishes to treat a document asunlabeled, NLP tasks can be answered through en-semble aggregation of the class-specific answers.Performance: We find that, in our examples, in-version of Word2Vec yields lower misclassifica-tion rates than both Doc2Vec-based classificationand the multinomial inverse regression (MNIR) ofTaddy (2013b).
We did not anticipate such out-right performance gain.
Moreover, we expect thatwith calibration (i.e., through cross-validation)of the many various tuning parameters availablewhen fitting both Word and Doc 2Vec the perfor-mance results will change.
Indeed, we find that allmethods are often outperformed by phrase-countlogistic regression with rare-feature up-weightingand carefully chosen regularization.
However, theout-of-the-box performance of Word2Vec inver-sion argues for its consideration as a simple defaultin document classification.In the remainder, we outline classificationthrough inversion of a specific Word2Vec modeland illustrate the ideas in classification of Yelpreviews.
The implementation requires only asmall extension of the popular gensim pythonlibrary (Rehurek and Sojka, 2010); the ex-tended library as well as code to reproduceall of the results in this paper are availableon github.
In addition, the yelp data ispublicly available as part of the correspond-ing data mining contest at kaggle.com.
Seegithub.com/taddylab/deepir for detail.2 ImplementationWord2Vec trains V to maximize the skip-gramlikelihood based on (1).
We work with the Huff-man softmax specification (Mikolov et al, 2013),which includes a pre-processing step to encodeeach vocabulary word in its representation via abinary Huffman tree (see Figure 1).Each individual probability is thenpV(w|wt) =L(w)?1?j=1?
(ch [?
(w, j + 1)]u>?
(w,j)vwt)(4)where ?
(w, i) is the ithnode in the Huffman treepath, of length L(w), for word w; ?
(x) = 1/(1 +exp[?x]); and ch(?)
?
{?1,+1} translates fromwhether ?
is a left or right child to +/- 1.
Everyword thus has both input and output vector coor-dinates, vwand [u?(w,1)?
?
?u?(w,L(w))].
Typically,46Figure 1: Binary Huffman encoding of a 4 wordvocabulary, based upon 18 total utterances.
Ateach step proceeding from left to right the twonodes with lowest count are combined into a par-ent node.
Binary encodings are read back off ofthe splits moving from right to left.only the input space V = [vw1?
?
?vwp], for a p-word vocabulary, is reported as the language rep-resentation ?
these vectors are used as input forNLP tasks.
However, the full representation V in-cludes mapping from each word to both V and U.We apply the gensim python implementationof Word2Vec, which fits the model via stochasticgradient descent (SGD), under default specifica-tion.
This includes a vector space of dimensionK = 100 and a skip-gram window of size b = 5.2.1 Word2Vec InversionGiven Word2Vec trained on each of C class-specific corpora D1.
.
.
DC, leading to C distinctlanguage representations V1.
.
.VC, classificationfor new documents is straightforward.
Considerthe S-sentence document d: each sentence wsisgiven a probability under each representation Vcby applying the calculations in (1) and (4).
Thisleads to the S?C matrix of sentence probabilities,pVc(ws), and document probabilities are obtainedpVc(d) =1S?spVc(ws).
(5)Finally, class probabilities are calculated viaBayes rule as in (3).
We use priors pic= 1/C, sothat classification proceeds by assigning the classy?
= argmaxcpVc(d).
(6)3 IllustrationWe consider a corpus of reviews provided by Yelpfor a contest on kaggle.com.
The text is tok-enized simply by converting to lowercase beforesplitting on punctuation and white-space.
Thetraining data are 230,000 reviews containing morethan 2 million sentences.
Each review is markedby a number of stars, from 1 to 5, and we fitseparate Word2Vec representations V1.
.
.V5forthe documents at each star rating.
The valida-tion data consist of 23,000 reviews, and we ap-ply the inversion technique of Section 2 to scoreeach validation document d with class probabili-ties q = [q1?
?
?
q5], where qc= p(c|d).The probabilities will be used in three differentclassification tasks; for reviews asa.
negative at 1-2 stars, or positive at 3-5 stars;b. negative 1-2, neutral 3, or positive 4-5 stars;c. corresponding to each of 1 to 5 stars.In each case, classification proceeds by sum-ming across the relevant sub-class probabilities.For example, in task a, p(positive) = q3+q4+ q5.
Note that the same five fitted Word2Vecrepresentations are used for each task.We consider a set of related comparator tech-niques.
In each case, some document repre-sentation (e.g., phrase counts or Doc2Vec vec-tors) is used as input to logistic regression pre-diction of the associated review rating.
The lo-gistic regressions are fit under L1regularizationwith the penalties weighted by feature standarddeviation (which, e.g., up-weights rare phrases)and selected according to the corrected AICc cri-teria (Flynn et al, 2013) via the gamlr R pack-age of Taddy (2014).
For multi-class tasks b-c,we use distributed Multinomial regression (DMR;Taddy 2015) via the distrom R package.
DMRfits multinomial logistic regression in a factorizedrepresentation wherein one estimates independentPoisson linear models for each response category.Document representations and logistic regressionsare always trained using only the training corpus.Doc2Vec is also fit via gensim, using the samelatent space specification as for Word2Vec: K =100 and b = 5.
As recommended in the doc-umentation, we apply repeated SGD over 20 re-orderings of each corpus (for comparability, thiswas also done when fitting Word2Vec).
Le andMikolov provide two alternative Doc2Vec specifi-cations: distributed memory (DM) and distributedbag-of-words (DBOW).
We fit both.
Vector rep-resentations for validation documents are trainedwithout updating the word-vector elements, lead-ing to 100 dimensional vectors for each docu-ment for each of DM and DCBOW.
We input47llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1 2 3 4 50.00.20.40.60.81.0word2vec inversionlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1 2 3 4 50.00.20.40.60.81.0phrase regressionlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1 2 3 4 50.00.20.40.60.81.0doc2vec regressionlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1 2 3 4 50.00.20.40.60.81.0mnirstarsprobability positiveFigure 2: Out-of-Sample fitted probabilities of a review being positive (having greater than 2 stars) as afunction of the true number of review stars.
Box widths are proportional to number of observations ineach class; roughly 10% of reviews have each of 1-3 stars, while 30% have 4 stars and 40% have 5 stars.each, as well as the combined 200 dimensionalDM+DBOW representation, to logistic regression.Phrase regression applies logistic regression of re-sponse classes directly onto counts for short 1-2word ?phrases?.
The phrases are obtained usinggensim?s phrase builder, which simply combineshighly probable pairings; e.g., first date andchicken wing are two pairings in this corpus.MNIR, the multinomial inverse regression ofTaddy (2013a; 2013b; 2015) is applied as im-plemented in the textir package for R. MNIRmaps from text to the class-space of inter-est through a multinomial logistic regression ofphrase counts onto variables relevant to the class-space.
We apply MNIR to the same set of 1-2word phrases used in phrase regression.
Here, weregress phrase counts onto stars expressed numeri-cally and as a 5-dimensional indicator vector, lead-ing to a 6-feature multinomial logistic regression.The MNIR procedure then uses the 6?pmatrix offeature-phrase regression coefficients to map fromphrase-count to feature space, resulting in 6 di-mensional ?sufficient reduction?
statistics for eachdocument.
These are input to logistic regression.Word2Vec aggregation averages fitted word rep-resentations for a single Word2Vec trained on allsentences to obtain a fixed-length feature vectorfor each review (K = 100, as for inversion).
Thisvector is then input to logistic regression.3.1 ResultsMisclassification rates for each task on the valida-tion set are reported in Table 1.
Simple phrase-count regression is consistently the strongest per-former, bested only by Word2Vec inversion ontask b.
This is partially due to the relative strengthsof discriminative (e.g., logistic regression) vs gen-a (NP) b (NNP) c (1-5)W2V inversion .099 .189 .435Phrase regression .084 .200 .410D2V DBOW .144 .282 .496D2V DM .179 .306 .549D2V combined .148 .
284 .500MNIR .095 .254 .480W2V aggregation .118 .248 .461Table 1: Out-of-sample misclassification rates.erative (e.g., all others here) classifiers: givena large amount of training text, asymptotic effi-ciency of logistic regression will start to work inits favor over the finite sample advantages of agenerative classifier (Ng and Jordan, 2002; Taddy,2013c).
However, the comparison is also unfairto Word2Vec and Doc2Vec: both phrase regres-sion and MNIR are optimized exactly under AICcselected penalty, while Word and Doc 2Vec haveonly been approximately optimized under a sin-gle specification.
The distributed representationsshould improve with some careful engineering.Word2Vec inversion outperforms the other doc-ument representation-based alternatives (except,by a narrow margin, MNIR in task a).
Doc2Vecunder DBOW specification and MNIR both doworse, but not by a large margin.
In contrast toLe and Mikolov, we find here that the Doc2VecDM model does much worse than DBOW.
Re-gression onto simple within- document aggrega-tions of Word2Vec perform slightly better than anyDoc2Vec option (but not as well as the Word2Vecinversion).
This again contrasts the results of Leand Mikolov and we suspect that the more com-plex Doc2Vec model would benefit from a careful48tuning of the SGD optimization routine.1Looking at the fitted probabilities in detail wesee that Word2Vec inversion provides a more use-ful document ranking than any comparator (in-cluding phrase regression).
For example, Figure2 shows the probabilities of a review being ?pos-itive?
in task a as a function of the true star rat-ing for each validation review.
Although phraseregression does slightly better in terms of misclas-sification rate, it does so at the cost of classifyingmany terrible (1 star) reviews as positive.
This oc-curs because 1-2 star reviews are more rare than 3-5 star reviews and because words of emphasis (e.g.very, completely, and !!!)
are used bothin very bad and in very good reviews.
Word2Vecinversion is the only method that yields positive-document probabilities that are clearly increasingin distribution with the true star rating.
It is not dif-ficult to envision a misclassification cost structurethat favors such nicely ordered probabilities.4 DiscussionThe goal of this note is to point out inversion as anoption for turning distributed language representa-tions into classification rules.
We are not arguingfor the supremacy of Word2Vec inversion in par-ticular, and the approach should work well with al-ternative representations (e.g., Glove).
Moreover,we are not even arguing that it will always outper-form purpose-built classification tools.
However,it is a simple, scalable, interpretable, and effectiveoption for classification whenever you are workingwith such distributed representations.ReferencesCheryl Flynn, Clifford Hurvich, and Jefferey Simonoff.2013.
Efficiency for Regularization Parameter Se-lection in Penalized Likelihood Estimation of Mis-specified Models.
Journal of the American Statisti-cal Association, 108:1031?1043.Quoc V. Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
In Pro-1Note also that the unsupervised document representa-tions ?
Doc2Vec or the single Word2Vec used in Word2Vecaggregation ?
could be trained on larger unlabeled corpora.
Asimilar option is available for Word2Vec inversion: one couldtake a single Word2Vec model trained on a large unlabeledcorpora as a shared baseline (prior) and update separate mod-els with additional training on each labeled sub-corpora.
Therepresentations will all be shrunk towards a baseline languagemodel, but will differ according to distinctions between thelanguage in each labeled sub-corpora.ceedings of the 31 st International Conference onMachine Learning.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InProceedings of the International Workshop on Arti-ficial Intelligence and Statistics, pages 246?252.Andrew Y. Ng and Michael I. Jordan.
2002.
On Dis-criminative vs Generative Classifiers: A Compar-ison of Logistic Regression and naive Bayes.
InAdvances in Neural Information Processing Systems(NIPS).Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning.
2014.
Glove: Global vectors forword representation.
Proceedings of the EmpiricialMethods in Natural Language Processing (EMNLP2014), 12.Radim Rehurek and Petr Sojka.
2010.
SoftwareFramework for Topic Modelling with Large Cor-pora.
In Proceedings of the LREC 2010 Workshopon New Challenges for NLP Frameworks, pages 45?50.David Rumelhart, Geoffrey Hinton, and RonaldWilliams.
1986.
Learning representations by back-propagating errors.
Nature, 323:533?536.Richard Socher, Cliff C. Lin, Chris Manning, and An-drew Y. Ng.
2011.
Parsing natural scenes and natu-ral language with recursive neural networks.
In Pro-ceedings of the 28th international conference on ma-chine learning (ICML-11), pages 129?136.Richard Socher, Alex Perelygin, Jean Y. Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the conference onempirical methods in natural language processing(EMNLP), volume 1631, page 1642.Matt Taddy.
2013a.
Measuring Political Sentimenton Twitter: Factor Optimal Design for MultinomialInverse Regression.
Technometrics, 55(4):415?425,November.Matt Taddy.
2013b.
Multinomial Inverse Regressionfor Text Analysis.
Journal of the American Statisti-cal Association, 108:755?770.Matt Taddy.
2013c.
Rejoinder: Efficiency and struc-ture in MNIR.
Journal of the American StatisticalAssociation, 108:772?774.Matt Taddy.
2014.
One-step estimator paths for con-cave regularization.
arXiv:1308.5623.Matt Taddy.
2015.
Distributed Multinomial Regres-sion.
Annals of Applied Statistics, To appear.49
