Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
847?857, Prague, June 2007. c?2007 Association for Computational LinguisticsMultiple Alignment of Citation Sentences withConditional Random Fields and Posterior DecodingAriel S. Schwartz?EECS, Computer Science DivisionUC BerkeleyBerkeley, CA 94720-1776sariel@cs.berkeley.eduAnna Divoli, Marti A. HearstSchool of InformationUC BerkeleyBerkeley, CA 94720-4600{hearst,divoli}@ischool.berkeley.eduAbstractIn scientific literature, sentences that cite re-lated work can be a valuable resource forapplications such as summarization, syn-onym identification, and entity extraction.In order to determine which equivalent en-tities are discussed in the various citationsentences, we propose aligning the wordswithin these sentences according to semanticsimilarity.
This problem is partly analogousto the problem of multiple sequence align-ment in the biosciences, and is also closelyrelated to the word alignment problem in sta-tistical machine translation.
In this paperwe address the problem of multiple citationconcept alignment by combining and mod-ifying the CRF based pairwise word align-ment system of Blunsom & Cohn (2006)and a posterior decoding based multiple se-quence alignment algorithm of Schwartz &Pachter (2007).
We evaluate the algorithmon hand-labeled data, achieving results thatimprove on a baseline.1 IntroductionThe scientific literature of biomedicine, genomics,and other biosciences is a rich, complex, and con-tinually growing resource.
With appropriate infor-mation extraction and retrieval tools, bioscience re-searchers can use the contents of the literature tofurther their research goals.
With online full text?Current address: Department of Bioengineering, Univer-sity of California, San Diego, La Jolla, CA 92093-0412.
Email:sariel@ucsd.edu.of journal articles finally becoming the norm, newforms of citation analysis become possible.Nearly every statement in biology articles isbacked up by at least one citation, and, conversely,it is quite common for papers in the bioscience do-main to be cited by 30?100 other papers.
The citedfacts are typically stated in a more concise way in theciting papers than in the original papers.
Since thesame facts are repeatedly stated in different ways indifferent papers, statistical models can be trained onexisting citation sentences to identify similar facts inunseen text.
Citation sentences also have the poten-tial to be useful for text summarization and databasecuration.
Figure 1 shows an example of three differ-ent citation sentences to the same target paper.Most citation analysis work focuses on the cita-tion network structure, to determine which papersare most central, or uses co-citation analysis to de-termine which papers are similar to one another incontent (White, 2004; Liu, 1993; Garfield, 1955;Lipetz, 1965; Giles et al, 1998).
In this paper wefocus instead on analyzing the sentences that sur-round the citations to related work, which we termedcitances in Nakov et al (2004).
In that paper wenote that one subproblem for using citances for au-tomated analysis is to identify the different conceptsmentioned; a given paper may be cited for more thanone fact or relation.Citances often state similar information usingvarying words and phrases.
In order to build con-cise summaries, those entities and relations that areexpressed in different ways should be matched up,or aligned, so that subsequent processing steps willknow what the core concepts are.
In this paper we847Example of unaligned citances?In response to genotoxicstress, Chk1 and Chk2phosphorylateCdc25Aon N-terminalsitesandtarget itrapidly for ubiquitin-dependent degradation (Mailandetal, 2000,2002;Molinariet al, 2000 ; Falcket al,2001; Shimutaet al, 2002; Businoetal, 2003), which isthought to becentral to theS and G2cellcycle checkpoints(BartekandLukas, 2003;DonzelliandDraetta, 2003 ).?
?Given that Chk1promotesCdc25Aturnoverin response to DNAdamage invivo(Falcket al.2001; Sorensen etal.
2003)andthatChk1 isrequiredfor Cdc25Aubiquitinationby SCF?-TRCPin vitro,weexploredtheroleof Cdc25Aphosphorylationinthe ubiquitinationprocess.?
?Since activated phosphorylatedChk2-T68is involved inphosphorylationanddegradation of Cdc25A(Falcket al.,2001 , Falcket al.,2002; BartekandLukas,2003), wealsoexamined thelevels ofCdc25Ain 2fTGH and U3A cells exposed to?-IR.
?Figure 1: Example of three unaligned citances.Alignment after normalizationresponsegenotoxicstressChk1Chk2phosphorylateCdc25ANterminalsites targetrapidly ubiquitindependentdegradationthoughtcentral SG2cellcycle checkpointsGiven Chk1promotesCdc25AturnoverresponseDNAdamagevivoChk1requiredCdc25AubiquitinationSCF beta TRCPvitro exploredrole Cdc25AphosphorylationubiquitinationprocessactivatedphosphorylatedChk2T68involvedphosphorylationdegradationCdc25Aexaminedlevels Cdc25A2fTGHU3Acells exposedgamma IRFigure 2: Example of three normalized aligned ci-tances.
Homologous entities are colored the same.Unaligned entities are black.build on the work of Nakov et al (2004) by tacklingthe entity normalization step.The citance alignment problem is partially anal-ogous to the problem of multiple alignment of bi-ological sequences (Durbin et al, 1998).
In bothcases the goal is to align homologous entities thatare derived from the same ancestral entity.
While inbiology homology is well-defined in the molecularlevel, in the citances case it is defined in the seman-tic level, which is much more subjective.
Given agroup of citances that cite the same target paper, weloosely define semantic homology as a symmetric,transitive, and reflexive relation between two enti-ties (words or phrases) in the same or different ci-tance that have similar semantics in the context ofthe cited paper.Figure 1 shows an example of three citances thatcite the same target paper (Falck et al, 2001).
Amultiple alignment of the entities in the same ci-tances (after removal of stopwords) is shown in Fig-ure 2.
Homologous entities are colored the same.This small example illustrates some of the mainchallenges of multiple citance alignment (MCA).While orthographic similarity can help to identifysemantic homology (e.g., phosphorylate and phos-phorylation), it can also be misleading (e.g., cell cy-cle and U3A cells).
In addition, semantic homologymight not include any orthographic clues (e.g., geno-toxic stress and DNA damage).Unlike global multiple sequence alignment(MSA) in genomics, where each character can bealigned to at most one character in every other se-quence, in multiple citance alignment, each wordcan be aligned to any number of words in other sen-tences.
Another major difference between the twoproblems is the fact that while the sequential order-ing of characters must be maintained in multiple se-quence alignment, this is not the case for multiplecitance alignment.MCA is also related to the problem of word align-ment in statistical machine translation (SMT) (Ochand Ney, 2003).
However, unlike SMT alignment,MCA aligns multiple citances in the same languagerather than a pair of sentences in different languages.In this paper we present an MCA algorithm thatis based on an extension to the posterior decodingalgorithm for MSA called AMAP (Schwartz et al,2006; Schwartz and Pachter, 2007), with an under-lying pairwise alignment model based on the CRFSMT alignment of Blunsom & Cohn (2006).2 Multiple citance alignmentsLet G , {C1, C2, .
.
.
, CK} be a group of K ci-tances that cite the same target paper, where the ithcitance is a sequence of words Ci , Ci1Ci2 ?
?
?Cini ,and ci , {ci1, ci2, .
.
.
, cini} is the set of word indicesof Ci.
A pairwise citance alignment of Ci and Cjis an equivalence (symmetric, reflexive, and transi-tive) relation ?ij on the set ci ?
cj .
The expres-sion cik ?ij cjl means that according to the pairwisealignment?ij word k in citance Ci and word l in ci-tance Cj are aligned.
A multiple citance alignment(MCA) is an equivalence relation ?,(?ij ?ij)+on the set?i ci, which is the transitive closure ofthe union of all pairwise alignments of citance pairsin G. Taking the transitive closure and not onlythe union of all pairwise alignments ensures that theMCA is an equivalence relation.An MCA ?
defines a partition of the set of allword indices c ,?ik {cik}, which is of size n ,848|c| =?i ni.
Therefore, the number of distinctMCAs of G is the number of partitions of a set ofsize n. This number is called the nth Bell number(Rota, 1964)Bn ,1e??k=0knk!.
(1)Asymptotically,Bn grows faster than an exponentialbut slower than a factorial.
For example B100 ?10116.
Obviously, enumerating all possible MCAsis impractical even for small problems.3 Probabilistic model for MCAUnlike biological sequences, for which pair-HMMsare a natural choice for modeling evolutionary pro-cesses between two sequences, there is no simplegenerative model that can be used for modelingpairwise citance alignment.
Most of the work onpairwise alignment of sentences at the word levelhas been done in the statistical machine translation(SMT) community.Och & Ney (2003) present an overview and com-parison of the most common models used for SMTword alignments.
Out of the models they describe,the HMM models are the most expressive mod-els that can compute posterior probabilities usingthe forward-backward algorithm.
However, unlikesequence alignments, there are no ordering con-straints in word alignments, and the alignments aremany-to-many as opposed to one-to-one.
Therefore,the SMT HMM models cannot be based on pair-HMMs, which generate two sentences simultane-ously.
Rather, they are directional models that modelthe probability of generating a target sentence givena source sentence.
In other words they only modelmany-to-one alignments, recovering the many-to-many alignments in a preprocessing step.
Therefore,SMT HMMs can only compute the posterior proba-bilities P (cik ; cjl |Ci, Cj) and P (cjl ; cik|Ci, Cj),where the relation ; represents the (directional)event that a source word is translated into a targetword.
Nevertheless, recently such posterior proba-bilities have been used in SMT word alignment sys-tem as an alternative to Viterbi decoding, and helpedto improve the performance of such systems (Ma-tusov et al, 2004; Liang et al, 2006).Generative models like HMMs have several lim-itations.
First, they require relatively large train-ing data, which is difficult to attain in case of SMTword alignment, and even more so in the case ofMCA.
Second, generative models explicitly modelthe inter-dependence of different features, which re-duces the ability to incorporate multiple arbitraryfeatures into the model.
Since orthographic similar-ity is not a strong enough indication for semantic ho-mology in MCA, we would like to be able to incor-porate multiple inter-dependent features into a singlemodel, including orthographic, contextual, ontolog-ical, and lexical features.Recently, several authors have described dis-criminative SMT alignment models (Moore, 2005;Lacoste-Julien et al, 2006; Blunsom and Cohn,2006).
However, to the best of our knowledge onlythe model of Blunsom & Cohn (2006), which isbased on a Conditional Random Field (CRF) (Laf-ferty et al, 2001), can compute word indices pairs?directional posterior probabilities, like those com-puted by the HMM models.
Therefore, we decidedto adopt the CRF-based model to the MCA problem.3.1 Conditional random fields for wordalignmentThe model of Blunsom & Cohn (2006) is based ona linear chain CRF, which can be viewed as theundirected version of an HMM.
The CRF modelsa many-to-one pairwise alignment, in which everysource word can get algned to zero or one targetwords, but every word in the target sentence can bethe target of multiple source words.
CRFs definea conditional distribution over a latent labeling se-quence given observation sequence(s).
In the caseof CRF for word alignment, the observed sequencesare the source and target sentences (citances), andthe latent labeling sequence is the mapping of sourcewords to target word-indices.
Given a source citanceCi of length ni, and a target citance Cj of length nj ,the many-to-one alignment of Ci to Cj is the rela-tion ;.
Since this is a many-to-one alignment, ;can be represented by a vector a of length ni.
TheCRF models the probability of the alignment a con-ditioned on Ci and Cj as follows:P?
(a|Ci, Cj) =exp(?t?k ?kfk(t, at?1, at, Ci, Cj))Z?
(Ci, Cj), (2)849where f , {fk} are the model?s features, ?
, {?k}are the features?
weights, and Z?
(Ci, Cj) is the par-tition (normalization) function which is defined as:Z?
(Ci, Cj) ,?aexp(?t?k?kfk(t, at?1, at, Ci, Cj)).
(3)Parameters are estimated from fully observed data(manually aligned citances) using a maximum a pos-teriori estimate.
The parameter estimation proce-dure is described in more details in the original pa-per.
Blunsom & Cohn (2006) use Viterbi decodingto find an alignment of two sentences given a trainedCRF model, a?
, argmaxa P?
(a|Ci, Cj).
How-ever, the posterior probabilities of the labels at eachposition can be calculated as well using the forward-backward algorithm:P?
(cil ; cjk|Ci, Cj) = P?
(al = cjk|Ci, Cj) =?l(cjk|Ci, Cj)?l(cjk|Ci, Cj)Z?
(Ci, Cj)(4)where ?l and ?l are the forward and backward vec-tors that are computed with the forward-backwardalgorithm (Lafferty et al, 2001).3.2 The posterior decoding algorithm for MCAUltimately, the success of an MCA algorithm shouldbe judged by its effect on the success of the citanceanalysis systems that use MCAs as their input.
How-ever, measuring this effect directly is difficult, sincehigh-level tasks such as summarization are difficultto evaluate objectively.
More to the point, it is dif-ficult to quantify the contribution of the MCA ac-curacy to the accuracy of the high-level system thatuses it.
A more practical alternative is to measure theaccuracy of MCAs directly using a meaningful ac-curacy measure, under the simplifying assumptionthat there is a strong correlation between the mea-sured MCA accuracy and the performance of thehigh-level application.We argue that a useful utility function should becorrelated (or even identical) to the accuracy mea-sure used to evaluate the performance of an algo-rithm.
In addition, the utility function should beeasily decomposable, to enable direct optimizationusing posterior-decoding.
Although any accuracymeasure that is acceptable as a single performancemeasure can be used to guide the design of the util-ity function, metric-based accuracy measures haveseveral noticeable advantages.
First, a metric for-malizes the intuitive notion of distance.
Hence, anaccuracy measure which is based on a metric fol-lows the intuition that reducing the distance to thecorrect answer should increase the accuracy of thepredicted answer.
Therefore, defining a metric spacefor the objects of a given problem leads to a nat-ural definition of accuracy.
Another advantage ofusing a metric-based accuracy measure is the abil-ity to provide bounds in the search space using thetriangle inequality.
For example, while searching forthe answer with the optimal (metric-based) expectedutility, a step of length x can only change the ex-pected utility as well as the actual utility by at most?x units.
Examples of more complex bounds us-ing metric loss functions are described in (Schlu?teret al, 2005) and (Domingos, 2000).Schwartz et al (2006) define the alignment met-ric accuracy (AMA), which is a utility function forone-to-one MSA.
Intuitively, AMA measures thefraction of characters that are aligned correctly ac-cording to the reference alignment, either to anothercharacter or to a gap (null).
We extend the definitionof AMA to the case of many-to-many MCA.A good utility function for MCA should give par-tial credit to word positions that align to some of thecorrect word positions while penalizing for aligningto wrong word positions.
To help define such a util-ity function we define the following.
Let mij?
(cjl ) ,{cik ?
ci|cik ?
cjl } be the set of all word positionsin citance Ci that align to word position l in citanceCj according to MCA ?.
We can then define thefollowing utility function for the MCA ?p of the ci-tance group G given a reference MCA ?r:UAMA(?r,?p) ,?ijl|i6=j Uset agreement(mij?r(cjl ),mij?p(cjl ))n(K ?
1), (5)where n is the number of word indices in G, K ,|G| is the number of citances in the group, andUset agreement is any utility function for agreementbetween sets that assigns values in the range [0, 1].850Uset agreement can be viewed as a ?score?
assignedto each word position based on the agreement be-tween the two alignments with regards to the otherword positions that align to it.
Using a 0?1 loss asthe set agreement score is equivalent to the originalAMA.
Other utility functions, such as Dice, Jaccardand Hamming can be used as Uset agreement.
How-ever, only metric-based utility functions will resultin a metric-based UAMA utility function.
It is easyto see that 1 ?
UAMA satisfies all the requirementsof a metric, i.e., it is non-negative, equals zero ifand only if ?r=?p, symmetric, and obeys the trian-gle inequality, since if the triangle inequality holdsfor 1 ?
Uset agreement, it must hold for a sum of1 ?
Uset agreement values.
(We refer the reader toSchwartz (2007) for a longer discussion of the prop-erties of the different utility functions.)
We definethe AMA for MCA by setting the Uset agreement tobe the Braun-Blanquet coefficient (Braun-Blanquet,1932), which is defined as:UBraun?Blanquet(mij?r(cjl ),mij?p(cjl )),??????
?1 if mij?r(cjl ) = ?and mij?p(cjl ) = ?|mij?r (cjl )?mij?p (cjl )|max{|mij?r (cjl )|,|mij?p (cjl )|}otherwise.
(6)Caillez & Kuntz (1996) show that the Braun-Blanquet coefficient is based on a metric.As with the MSA case, a family of utility func-tions can be defined to enable control of the re-call/precision trade-off.
Unlike MSA, in the case ofMCA two free parameters are needed, in order tohave better control of the trade-off using posterior-decoding.
In addition to a gap-factor that controlsthe threshold at which unaligned words start to getaligned, a match-factor is added to enable control ofthe number of word-positions each word aligns to.The result is the following utility function:U?,?
(?r,?p) ,1n(K ?
1)?ijl|i6=j(?|mij?p (cjl )||mij?r(cjl ) ?mij?p(cjl )|max{|mij?r(cjl )|, |mij?p(cjl )|, 1}+?1{mij?r(cjl ) = mij?p(cjl ) = ?
}), (7)where ?
?
[0,?)
is a gap-factor, and ?
?
(0,?)
isa match factor.
The neutral value for both parame-ters is 1.
Increasing ?
results in increased utility tosparser MCAs, while reducing ?
increases the util-ity of denser alignments.
However, in the case ofMCA, the gap-factor only affects the first alignedword position, but it cannot affect the number ofword positions each word is aligned to.
The match-factor adds this functionality by rewarding MCAsthat align words to multiple word positions when?
> 1, and penalizing such MCAs when ?
< 1.Given a group of K citances G and a trainedCRF model, the goal of the MCA algorithm is tofind the MCA ?
?, argmax?p E?tU?,?
(?t,?p)that maximizes the expected utility.
Since search-ing the space of possible MCAs exhaustively is in-feasible, we resort to a simple heuristic for predict-ing an MCA.
Instead of searching for a global opti-mum, the predicted MCA is defined as the equiva-lence (symmetric transitive) closure of the union ofmultiple local optima.
For each target word posi-tion cjl and every source citance Ci the combina-tion of source word positions ci?
that maximize theexpected set-agreement score of cjl is added to thepredicted MCA.
Formally, let P(ci) be the power-set of ci, then we define the predicted MCA as?p,(;p ?
(;p)?1)+, where;p is defined as:;p,?ijl|i6=j{cjl } ?
argmaxci?
?P(ci)Emij?t(cjl )??
?|ci?||mij?t(cjl ) ?
ci?|max{|mij?t(cjl )|, |ci?|, 1}+?1{mij?t(cjl ) = ci?
= ?}).
(8)The value of ;p can be computed from the CRFdirectional posterior probabilities as follows:;p=?ijl|i6=j{cjl } ?
argmaxci??P(ci)?ci?
?P(ci)P(mij?t(cjl ) = ci?)(?|ci?||ci?
?
ci?|max {|ci?|, |ci?|, 1}+ ?1{ci?
= ci?
= ?
}),(9)851and using an independence assumption we get:;p?
?ijl|i6=j{cjl } ?
argmaxci??P(ci)?ci??P(ci)???cik(P?
(cik ; cjl |Ci, Cj)1{cik ?
ci?}+(1?
P?
(cik ; cjl |Ci, Cj))1{cik /?
ci?}))(?|ci?||ci?
?
ci?|max {|ci?|, |ci?|, 1}+ ?1{ci?
= ci?
= ?}).
(10)Note that although the directional posterior prob-abilities are used to generate the predicted MCA,the result is a many-to-many alignment, since theunion is done over all pairs of sequences in bothdirections.
The calculation in Equation (10) canbe computationally intensive in practice, as it re-quires |P(ci)|2 = 22nioperations for each wordposition cjl and citance Ci.
This can be over-come by restricting the combinations of source wordpositions (ci?
and ci?)
to include only the the topMAX SOURCES source words with a minimumposterior probability of MIN PROB to align to cjl(P?
(cik ; cjl |Ci, Cj) ?
MIN PROB).
In our im-plementation we set MAX SOURCES to 8 andMIN PROB to 0.01.
Additionally, the probabilitiesof each combination ci?
can be calculated only once,since it is independent of ci?.
This reduces the to-tal computational complexity of calculating ?p toO(216(K2 ?K)maxni{ni}).4 Data setsSince citance alignment is a new task, we had tocreate our own evaluation and training sets.
We re-stricted the domain of the target papers to molec-ular interactions, a domain which is actively re-searched in the biosciences text mining community(Hirschman et al, 2002).
The biologist in our groupannotated citances to 6 target papers.
The trainingset consisted of 40 citances to 4 different target pa-pers (10 citances each; we wanted to have variety inthe training set).
The development set consisted of51 citances to the fifth target paper, and the test setcontained 45 citances to the sixth target paper.For each target paper we downloaded the full textof those papers citing it that were available in HTMLformat.
The link structure of the cited references inthe HTML documents allowed us to automaticallyextract citances to a given target paper.
We defined acitance to be the full sentence that contains a citationto the target paper.
Each citance was then tokenized,and normalized by removing all stopwords from apredefined list.One goal of the annotation was to cover as muchof the content of the citances as possible.
Anothergoal was consistency; our biologist manually fol-lowed a small number of rules to determine seman-tic similarity.
Distinct semantic units (words orphrases) were identified and given an annotation ID.Within each group of citances, words or phrases thatshare semantic similarity were annotated with thesame ID.Using the manually annotated citance groups,pairwise word alignments were generated for everysource-target pair of citances from every group.
Thatresulted in a training, development, and test setsof 180, 1275, and 990 pairwise alignments respec-tively.Alignments that were used for development andtesting were generated as many-to-many alignments.However, many-to-many alignments are not suit-able for the training the many-to-one CRF align-ment model.
When a given source word cik alignsto multiple words in the target citance, the CRFmodel chooses only one target word as a true pos-itive, while incorrectly treating the other true posi-tive target words as true negatives.
To alleviate thisproblem, in such cases we replaced all true-positivetarget words other than the first with ?
*?, thus forcingthem to act as real true negatives for the purpose oftraining.
This adjustment does not solve the inher-ent limitation of the CRF?s many-to-one modeling ofa many-to-many alignment, but it prevents learningincorrect weights for good features.5 Feature engineeringThe CRF alignment model can combine multipleoverlapping features.
We evaluated the effectivenessof different features by training models on the train-ing set and evaluating their performance on the de-velopment set.
We considered variations of features852that were part of the original system of Blunsom &Cohn (2006), and also designed new features that arespecific to the problem of MCA.Orthographic featuresWe used the following orthographic features fromthe original system of Blunsom & Cohn (2006) (be-low all features are either Boolean indicator func-tions (b) or real valued (r)):(b) exact string similarity of source-target words;(b) every possible source-target pair of length 3 prefixes;(b) exact string match of length 3 prefixes;(b) exact string match of length 3 suffixes;(r) absolute difference in word lengths;(b) both words are shorter than 4 characters.In addition, the following orthographic featureswere added: indicator that both words include capi-tal letters, and normalized edit-similarity of the twowords (1?edit distance(cik,cjl )max{|cik|,|cjl |}).Markov featuresWe used the following Markov features from theoriginal system:(r) absolute jump width (abs(at?at?1?1), which measuresthe distance between the target words of adjacent sourcewords;(r) positive jump width (max{at ?
at?1 ?
1, 0});(r) negative jump width (max{at?1 + 1?
at, 0});(b) transition from null aligned source-word to non-nullaligned source-word;(b) transition from non-null aligned source-word to nullaligned source-word;(b) transition from null aligned source-word to null alignedsource-word.In addition we added the following Markov fea-tures in order to model the tendency of certain wordsto be part of longer phrases:(b) source-word aligns to the same target-word as the previ-ous source-word;(b) source-word aligns to the same target-word as the nextsource-word;(b) transition from non-null aligned source-word to non-nullaligned source-word.Sentence position: We included the relative sen-tence position feature from the original system,which is defined as abs( at|cj | ?tci ).
Although it wasnot expected to be relevant for MCA, since the ci-tances are not expected to align along the diagonal,this feature slightly improved the performance of thedevelopment set.Null: An indicator function for leaving a source-word unaligned was retained from the original sys-tem.
This is an essential feature since without it theCRF tends to over-align words, and produces mean-ingless posterior probabilities.Ontological features: Orthographic and posi-tional features alone do not cover all cases of se-mantic homology.
We therefore included featuresthat are based on domain specific ontologies.Using an automated script we mapped specificwords and phrases in every citance to MeSH1 terms,Gene identifiers from Entrez Gene,2 UniProt,3 andOMIM.4 We then added features indicating whenthe source and target words are annotated with thesame MeSH term or the same gene identifier.
Wetried numerous features that compare MeSH termsbased on their distance in the ontology, and otherfeatures that indicate whether a word is part of alonger term.
However, none of these feature wereselected for the final system.In addition to biological ontologies we added afeature for semantic word similarity between thesource and target words, based on the Lin (1998)WordNet similarity measure.6 ResultsWe modified the CRF alignment system of Blun-som & Cohn (2006) to support MCA by incorpo-rating the posterior decoding algorithm from Sec-tion 3.2 into the existing system.
The CRF modelwas trained using the features that were selected us-ing the development set, on a dataset that includedthe training and development MCAs.
All the perfor-mance results in this section are reported on the testset, which includes 990 pairs of citances (45?44/2),with a total of 34188 words (8547 ?
44).
On aver-age, 20% of the source words are aligned to at leastone other target word in a given reference pairwisealignment.
Since the union of all the pairwise align-ments results in only a single test MCA, it is hardto make strong arguments about the performance1http://www.nlm.nih.gov/mesh/2http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=gene3http://www.pir.uniprot.org/4http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=OMIM85300.10.20.30.40.50.60.70.80.91 0.30.40.50.60.70.8RecallPrecisionViterbi_IntersectionViterbi_UnionPosterior decodingFigure 3: Recall/Precision curve of pairwise ci-tance alignments comparing Viterbi to posteriordecoding.of the system in general.
Therefore, we concen-trate our discussion on general trends, and do notclaim that the specific performance numbers we re-port here are statistically significant.
As a point ofcomparison, the SMT community has been evalu-ating performance of word-alignment systems on aneven smaller dataset of 447 pairs of non-overlappingsentences (Mihalcea and Pedersen, 2003).We first analyze the performance of the systemon pairwise citance alignments.
Instead of tak-ing the equivalence closure of ;p we take onlythe symmetric closure.
The result is 990 many-to-many pairwise alignments.
In order to evalu-ate the effectiveness of the posterior-decoding al-gorithm, we generate the Viterbi alignments usingthe same CRF model.
The Viterbi many-to-manypairwise alignments are then generated by combin-ing equivalent pairs of many-to-one alignments us-ing three different standard symmetrization methodsfor word-alignment?union, intersection, and the re-fined method of Och & Ney (2003).Figure 3 shows the recall/precision trade-off ofthe pairwise posterior-decoding and Viterbi align-ments.
The curve for the posterior-decoding align-ments was produced by varying the gap and matchfactors.
For the Viterbi alignments, only three re-sults could be generated (one for each symmetriza-tion method).
However, since the refined methodproduced a very similar result to the union, onlythe union is displayed in the figure.
The impor-tant observation is that while posterior-decoding en-ables refined control over the recall/precision trade-off, the Viterbi decoding generates only three align-ments, which cover only a small fraction of the curveat its high precision range.
The union of Viterbialignments achieves 0.531 recall at 0.913 preci-sion, which is similar result to the 0.540 recall at0.909 precision achieved using posterior-decodingwith gap-factor and match-factor set to 1.
However,unlike Viterbi, posterior-decoding produces align-ments with much higher recall levels, by increas-ing the match-factor and decreasing the gap-factor.For example setting the gap-factor to 0.1 and match-factor to 1.2 results in alignments with 0.636 recallat 0.517 precision, and setting them to 0.05 and 1.5results in 0.742 recall at 0.198 precision.
Generally,the gap and match factor affect the accuracy of thealignments as expected.
In particular, the alignmentswith the best AMA (0.889) and the best F1-measure(0.678) are generated when the gap match factor areset to their natural values (1,1), which theoreticallyshould maximize the expected AMA.The performance of the pairwise alignmentsvalidates the underlying probabilistic model,showing it behaves as theoretically expected.However, the union of all pairwise alignmentsis not a valid MCA.
To evaluate the MCAposterior decoding algorithm, we compared itto baseline MCAs.
The baseline MCAs areconstructed by using only the normalized-edit-distanceedit distance(cik,cjl )max{|cik|,|cjl |}, and defining cik ;?cjl if and only if normalized edit distance(cik, cjl ) ?
?, where ?
is a distance threshold.
The fi-nal baseline MCA is constructed by taking theequivalence closure of all pairwise alignments,??,(;?
?(;?)?1))+.
The ?
parameter canbe used to control the recall/precision trade-off,since increasing it adds more position-pairs to thealignment, thus increasing recall, while decreasingit increases precision.Figures 4 compares the performance of the CRFposterior-decoding MCAs with the baseline MCAs.The different MCAs were produced by varying thegap and match factors in the case of the posterior-decoding, and ?
for the baseline MCAs.
The CRFcurve clearly dominates the baseline curve.
How-ever, they do overlap in range between 0.52 and0.55 recall (0.84 and 0.90 precision).
This is prob-85400.10.20.30.40.50.60.70.80.91 00.10.20.30.40.50.60.70.80.91RecallPrecisionCRFBaselineFigure 4: Recall/Precision curve of MCAscomparing CRF with posterior decoding tonormalized-edit-distance baseline.ably a range in which for this particular MCA theorthographic similarity is the most dominant fea-ture.
While the baseline curve drops sharply afterthat range, the posterior-decoding curve keeps im-proving recall up to 0.636 at 0.748 precision, be-fore a major drop in precision.
The additional recallis due to the ability of the CRF model to incorpo-rate multiple overlapping features.
In particular, thedomain-specific features are important for aligningwords and phrases that have little or no orthographicsimilarity.
At the other end of the overlap range, theposterior-decoding achieves better precision than thebaseline for the same recall levels.
For example, theposterior decoding gets 0.381 recall at 0.982 preci-sion compared with 0.346 at 0.937 for the baseline.Unlike the pairwise alignment case, the neutralsettings of the gap and match factors did not result inthe best AMA score.
This is due to the equivalenceclosure heuristic that results in MCAs that are toodense, since a single link between two equivalenceclasses causes them to merge.
The best AMA score(0.886) is obtained by reducing the gap-factor to 0.5and match-factor to 0.45, in order to compensate forthe effect of the equivalence closure heuristic.
Forcomparison, the best F1-measure (0.690) is achievedby setting the gap and match factors to 0.75.An error analysis on the latter MCA shows thatout of 1400 unique errors, 1194 (85.3%) are falsenegatives (FN) and 206 (14.7%) false positives (FP).Most errors (more than 600) are due to misalign-ment of subtypes (e.g., cdc, cdc6, cdc25A), oppo-sites (e.g., phosphorylated and unphosphorylated)and complex entities (e.g., cell cycle v.s.
cell line).In addition, a large portion of FN errors are due tonot aligning entities that belong to just four equiva-lence classes (e.g., 97 FN errors caused by terms inthe class of motif, site and domain).
Other types oferrors include not aligning plural and singular formsof the same entities, aligning only part of multi-word entities, and incorrectly aligning orthographi-cally similar entities that belong to different classes.7 ConclusionsWe have shown how to derive a posterior-decodingalgorithm that aims at maximizing the expected util-ity for the MCA problem, as a substitute for thesequence-annealing algorithm for MSA.
Adding agap and match factor to the utility function allowscontrol over the recall/precision trade-off when us-ing posterior-decoding.
Another advantage of opti-mizing the expected utility with posterior-decodingmethods is the decoupling from the probabilisticmodel that generated the posterior probabilities.This allows the use of CRFs instead of HMMs witha similar posterior decoding algorithm.Our experiments were limited by the size of thelabeled data.
However, the results support the the-oretical predictions, and demonstrate the advantageof posterior-decoding over Viterbi decoding.Since citances are still a relatively unexplored re-source, it is still unclear whether the formulation wepresented here for citance alignment is the most use-ful for applications that use citances for compara-tive analysis of bioscience text.
Unlike biologicalsequence alignment, citance alignments are muchmore subjective, as they depend on a loose defini-tion of semantic homology between entities.
Eventhe definition of the basic entities can vary, since inmany cases noun-compounds and other multi-wordentities seem to be a more natural choice for basic el-ements of semantic homology and alignment.
How-ever, automatic segmentation and entity recognitionare still difficult tasks in the bioscience text domainand so new methods are worth investigating.Acknowledgements: We thank Phil Blunsom andTrevor Cohn for sharing their CRF-based wordalignment code.
This research was supported in partby NSF DBI 0317510.855ReferencesPhil Blunsom and Trevor Cohn.
2006.
Discrimina-tive word alignment with conditional random fields.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 65?72, Sydney, Australia, July.
Association forComputational Linguistics.Josias Braun-Blanquet.
1932.
Plant sociology: the studyof plant communities.
McGraw-Hill, New York.Francis Caillez and Pascale Kuntz.
1996.
A contributionto the study of the metric and euclidean structures ofdissimilarities.
Psychometrika, 61(2):241?253.Pedros Domingos.
2000.
A unified bias-variance de-composition and its applications.
In Proceedingsof the Seventeenth International Conference on Ma-chine Learning, pages 231?238, Stanford, CA.
Mor-gan Kaufmann.Richard Durbin, Sean R. Eddy, Anders Krogh, andGraeme Mitchison.
1998.
Biological sequence analy-sis.
Probablistic models of proteins and nucleic acids.Cambridge University Press.Jacob Falck, Niels Mailand, Randi G. Syljuasen, JiriBartek, and Jiri Lukas.
2001.
The ATM-Chk2-Cdc25A checkpoint pathway guards against radiore-sistant DNA synthesis.
Nature, 410(6830):842?847.Eugene Garfield.
1955.
Citation indexes for science: Anew dimension in documentation through associationof ideas.
Science, 122(3159):108?111.C.
Lee Giles, Kurt D. Bollacker, and Steve Lawrence.1998.
Citeseer: an automatic citation indexing sys-tem.
In Proceedings of the third ACM conference onDigital libraries, pages 89?98.
ACM Press.Lynette Hirschman, Jong C. Park, Junichi Tsujii, Lim-soon Wong, and Cathy H. Wu.
2002.
Accomplish-ments and challenges in literature data mining for bi-ology.
Bioinformatics, 18(12):1553?1561.Simon Lacoste-Julien, Ben Taskar, Dan Klein, andMichael I. Jordan.
2006.
Word alignment viaquadratic assignment.
In Proceedings of the HumanLanguage Technology Conference of the NAACL, MainConference, pages 112?119, New York City, USA,June.
Association for Computational Linguistics.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proc.18th International Conf.
on Machine Learning, pages282?289.
Morgan Kaufmann, San Francisco, CA.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the HumanLanguage Technology Conference of the NAACL, MainConference, pages 104?111, New York City, USA,June.
Association for Computational Linguistics.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proc.
15th International Conf.
on Ma-chine Learning, pages 296?304.
Morgan Kaufmann,San Francisco, CA.Ben-Ami Lipetz.
1965.
Improvements of the selectiv-ity of citation indexes to science literature through in-clusion of citation relationship indicators.
AmericanDocumentation, 16:81?90.Mengxiong Liu.
1993.
Progress in documentation.
thecomplexities of citation practice: A review of citationstudies.
Journal of Documentation, 49(4):370?408.Evgeny Matusov, Richard Zens, and Hermann Ney.2004.
Symmetric word alignments for statistical ma-chine translation.
In COLING ?04: Proceedings of the20th international conference on Computational Lin-guistics, page 219, Morristown, NJ, USA.
Associationfor Computational Linguistics.Rada Mihalcea and Ted Pedersen.
2003.
An evaluationexercise for word alignment.
In Rada Mihalcea andTed Pedersen, editors, HLT-NAACL 2003 Workshop:Building and Using Parallel Texts: Data Driven Ma-chine Translation and Beyond, pages 1?10, Edmonton,Alberta, Canada, May 31.
Association for Computa-tional Linguistics.Robert C. Moore.
2005.
A discriminative framework forbilingual word alignment.
In HLT/EMNLP, pages 81?88.Preslav I. Nakov, Ariel S. Schwartz, and Marti A. Hearst.2004.
Citances: Citation sentences for semantic anal-ysis of bioscience text.
In SIGIR?04 Workshop onSearch and Discovery in Bioinformatics.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Gian-Carlo Rota.
1964.
The number of partitions of aset.
The American Mathematical Monthly, 71(5):498?504, may.Ralf Schlu?ter, Thomas Scharrenbach, Volker Steinbiss,and Hermann Ney.
2005.
Bayes risk minimizationusing metric loss functions.
In Proceedings of theEuropean Conference on Speech Communication andTechnology, Interspeech, pages 1449?1452, Portugal,September.Ariel S. Schwartz and Lior Pachter.
2007.
Multiplealignment by sequence annealing.
Bioinformatics,23(2):e24?29.856Ariel S. Schwartz, Eugene W. Myers, and Lior Pachter.2006.
Alignment metric accuracy.
arXiv:q-bio.QM/0510052.Ariel S. Schwartz.
2007.
Posterior Decoding Meth-ods for Optimization and Accuracy Control of MultipleAlignments.
Ph.D. thesis, EECS Department, Univer-sity of California, Berkeley.Howard D. White.
2004.
Citation analysis and discourseanalysis revisited.
Applied Linguistics, 25(1):89?116.857
