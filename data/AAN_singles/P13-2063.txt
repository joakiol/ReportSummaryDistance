Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 352?357,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLearning to Prune: Context-Sensitive Pruning for Syntactic MTWenduan XuComputer LaboratoryUniversity of Cambridgewenduan.xu@cl.cam.ac.ukYue ZhangSingapore University ofTechnology and Designyue zhang@sutd.edu.sgPhilip Williams and Philipp KoehnSchool of InformaticsUniversity of Edinburghp.j.williams-2@sms.ed.ac.ukpkoehn@inf.ed.ac.ukAbstractWe present a context-sensitive chart prun-ing method for CKY-style MT decoding.Source phrases that are unlikely to havealigned target constituents are identifiedusing sequence labellers learned from theparallel corpus, and speed-up is obtainedby pruning corresponding chart cells.
Theproposed method is easy to implement, or-thogonal to cube pruning and additive toits pruning power.
On a full-scale English-to-German experiment with a string-to-tree model, we obtain a speed-up of morethan 60% over a strong baseline, with noloss in BLEU.1 IntroductionSyntactic MT models suffer from decoding effi-ciency bottlenecks introduced by online n-gramlanguage model integration and high grammarcomplexity.
Various efforts have been devoted toimproving decoding efficiency, including hyper-graph rescoring (Heafield et al, 2013; Huang andChiang, 2007), coarse-to-fine processing (Petrovet al, 2008; Zhang and Gildea, 2008) and gram-mar transformations (Zhang et al, 2006).
Formore expressive, linguistically-motivated syntac-tic MT models (Galley et al, 2004; Galley etal., 2006), the grammar complexity has grownconsiderably over hierarchical phrase-based mod-els (Chiang, 2007), and decoding still suffers fromefficiency issues (DeNero et al, 2009).In this paper, we study a chart pruning methodfor CKY-style MT decoding that is orthogonal tocube pruning (Chiang, 2007) and additive to itspruning power.
The main intuition of our methodis to find those source phrases (i.e.
any sequenceof consecutive words) that are unlikely to have anyconsistently aligned target counterparts accordingto the source context and grammar constraints.
Weshow that by using highly-efficient sequence la-belling models learned from the bitext used fortranslation model training, such phrases can be ef-fectively identified prior to MT decoding, and cor-responding chart cells can be excluded for decod-ing without affecting translation quality.We call our method context-sensitive pruning(CSP); it can be viewed as a bilingual adap-tation of similar methods in monolingual pars-ing (Roark and Hollingshead, 2008; Zhang et al,2010) which improve parsing efficiency by ?clos-ing?
chart cells using binary classifiers.
Our con-tribution is that we demonstrate such methods canbe applied to synchronous-grammar parsing by la-belling the source-side alone.
This is achievedthrough a novel training scheme where the la-belling models are trained over the word-alignedbitext and gold-standard pruning labels are ob-tained by projecting target-side constituents to thesource words.
To our knowledge, this is the firstwork to apply this technique to MT decoding.The proposed method is easy to implementand effective in practice.
Results on a full-scaleEnglish-to-German experiment show that it givesmore than 60% speed-up over a strong cube prun-ing baseline, with no loss in BLEU.
While we usea string-to-tree model in this paper, the approachcan be adapted to other syntax-based models.352but we need that reform process .TOPS-TOPKONdennNP-OAPDATdiesenNNReformproze?VVFINbrauchenNP-SBPPERwirPUNC..r1 KON ?
?
but, denn ?r2 NP-SB ?
?
we, wir ?r3 NP-OA ?
?
that reform process,diesen Reformproze?
?r4 TOP ?
?
X1 .
, S-TOP1 .
?r5 S-TOP ?
?
but X1 need X2,denn NP-OA2 brauchen NP-SB1 ?Figure 1: A selection of grammar rules extractablefrom an example word-aligned sentence pair.2 The Baseline String-to-Tree ModelOur baseline translation model uses the rule ex-traction algorithm of Chiang (2007) adapted to astring-to-tree grammar.
After extracting phrasalpairs using the standard approach of Koehn et al(2003), all pairs whose target phrases are not ex-haustively dominated by a constituent of the parsetree are removed and each remaining pair, ?f, e?,together with its constituent label, C, forms a lex-ical grammar rule: C ?
?f, e?.
The rules r1, r2,and r3 in Figure 1 are lexical rules.
Non-lexicalrules are generated by eliminating one or morepairs of terminal substrings from an existing ruleand substituting non-terminals.
This process pro-duces the example rules r4 and r5.Our decoding algorithm is a variant of CKYand is similar to other algorithms tailored for spe-cific syntactic translation grammars (DeNero etal., 2009; Hopkins and Langmead, 2010).
By tak-ing the source-side of each rule, projecting onto itthe non-terminal labels from the target-side, andweighting the grammar according to the model?slocal scoring features, decoding is a straightfor-ward extension of monolingual weighted chartparsing.
Non-local features, such as n-gram lan-guage model scores, are incorporated throughcube pruning (Chiang, 2007).3 Chart Pruning3.1 MotivationsThe abstract rules and large non-terminal sets ofmany syntactic MT grammars cause translationproductstheofvalueNP-TOPNP-AGNNProdukteARTderNNWert(a) en-deproductstheofvalueNPNN??NPDEG?NN??
(b) en-jpFigure 2: Two example alignments.
In (a) ?theproducts?
does not have a consistent alignment onthe target side, while it does in (b).overgeneration at the span level and render decod-ing inefficient.
Prior work on monolingual syn-tactic parsing has demonstrated that by exclud-ing chart cells that are likely to violate constituentconstraints, decoding efficiency can be improvedwith no loss in accuracy (Roark and Hollingshead,2008).
We consider a similar mechanism for syn-tactic MT decoding by prohibiting subtranslationgeneration for chart cells violating synchronous-grammar constraints.A motivating example is shown in Figure 2a,where a segment of an English-German sentencepair from the training data, along with its wordalignment and target-side parse tree is depicted.The English phrases ?value of?
and ?the products?do not have corresponding German translations inthis example.
Although the grammar may haverules to translate these two phrases, they can besafely pruned for this particular sentence pair.In contrast to chart pruning for monolingualparsing, our pruning decisions are based on thesource context, its target translation and the map-ping between the two.
This distinction is impor-tant since the syntactic correspondence betweendifferent language pairs is different.
Suppose thatwe were to translate the same English sentenceinto Japanese (Figure 2a); unlike the English toGerman example, the English phrase ?the prod-ucts?
will be a valid phrase that has a Japanesetranslation under a target constituent, since it issyntactically aligned to ????
(Figure 2b).The key question to consider is how to injecttarget syntax and word alignment information intoour labelling models, so that pruning decisions canbe based on the source alone, we address this in thefollowing two sections.3.2 Pruning by LabellingWe use binary tags to indicate whether a sourceword can start or end a multi-word phrase that has35310111(a) b-tags11101(b) e-tagsFigure 3: The pruning effects of two types of bi-nary tags.
The shaded cells are pruned and twotypes of tags are assigned independently.a consistently aligned target constituent.
We callthese two types the b-tag and the e-tag, respec-tively, and use the set of values {0, 1} for both.Under this scheme, a b-tag value of 1 indi-cates that a source word can be the start of asource phrase that has a consistently aligned targetphrase; similarly an e-tag of 0 indicates that a wordcannot end a source phrase.
If either the b-tag orthe e-tag of an input phrase is 0, the correspond-ing chart cells will be pruned.
The pruning effectsof the two types of tags are illustrated in Figure 3.In general, 0-valued b-tags prune a whole columnof chart cells and 0-valued e-tags prune a wholediagonal of cells; and the chart cells on the firstrow and the top-most cell are always kept so thatcomplete translations can always be found.We build a separate labeller for each tag type us-ing gold-standard b- and e-tags, respectively.
Wetrain the labellers with maximum-entropy models(Curran and Clark, 2003; Ratnaparkhi, 1996), us-ing features similar to those used for suppertag-ging for CCG parsing (Clark and Curran, 2004).In each case, features for a pruning tag consistof word and POS uni-grams extracted from the 5-word window with the current word in the middle,POS trigrams ending with the current word, as wellas two previous tags as a bigram and two separateuni-grams.
Our pruning labellers are highly effi-cient, run in linear time and add little overhead todecoding.
During testing, in order to prevent over-pruning, a probability cutoff value ?
is used.
A tagvalue of 0 is assigned to a word only if its marginalprobability is greater than ?.3.3 Gold-standard Pruning TagsGold-standard tags are extracted from the word-aligned bitext used for translation model train-ing, respecting rule extraction constraints, whichis crucial for the success of our method.For each training sentence pair, gold-standardb-tags and e-tags are assigned separately to theAlgorithm 1 Gold-standard Labelling AlgorithmInput forward alignment Ae?f , backward align-ment A?f?e and 1-best parse tree ?
for fOutput Tag sequences b and e for e1: procedure TAG(e, f , ?,A, A?
)2: l?
|e|3: for i?
0 to l ?
1 do4: b[i]?
0, e[i]?
05: for f [i?, j?]
in ?
do6: s?
{A?
[k] | k ?
[i?, j?
]}7: if |s| ?
1 then continue8: i?
min(s), j ?
max(s)9: if CONSISTENT(i, j, i?, j?)
then10: b[i?]?
1, e[j?]?
111: procedure CONSISTENT(i, j, i?, j?
)12: t?
{A[k] | k ?
[i, j]}13: return min(t) ?
i?
and max(t) ?
j?source words.
First, we initialize both tags of eachsource word to 0s.
Then, we iterate through all tar-get constituent spans, and for each span, we findits corresponding source phrase, as determined bythe word alignment.
If a constituent exists for thephrase pair, the b-tag of the first word and the e-tagof the last word in the source phrase are set to 1s,respectively.
Pseudocode is shown in Algorithm 1.Note that our definition of the gold-standard al-lows source-side labels to integrate bilingual in-formation.
On line 6, the target-side syntax isprojected to the source; on line 9, consistency ischecked against word alignment.Consider again the alignment in Figure 2a.
Tak-ing the target constituent span covering ?der Pro-dukte?
as an example, the source phrase under aconsistent word alignment is ?of the products?.Thus, the b-tag of ?of?
and the e-tag of ?prod-ucts?
are set to 1s.
After considering all targetconstituent spans, the complete b- and e-tag se-quences for the source-side phrase in Figure 2aare [1, 1, 0, 0] and [0, 0, 1, 1], respectively.
Notethat, since we never prune single-word spans, weignore source phrases under consistent one-to-oneor one-to-many alignments.From the gold standard data, we found 73.69%of the 54M words do not begin a multi-wordaligned phrase and 77.71% do not end a multi-word aligned phrase; the 1-best accuracies of thetwo labellers tested on a held-out 20K sentencesare 82.50% and 88.78% respectively.3540.1460.14650.1470.14750.1480.14850.1490  2  4  6  8  10  12  14  16  18  20BLEUCPU seconds/sentencecspcube pruning(a) time vs. BLEU0.1460.14650.1470.14750.1480.14850.1490  0.5  1  1.5  2  2.5BLEUHypothesis Count (x106)cspcube pruning(b) hypo count vs. BLEUFigure 4: Translation quality comparison with the cube pruning baseline.4 Experiments4.1 SetupA Moses (Koehn et al, 2007) string-to-tree sys-tem is used as our baseline.
The training cor-pus consists of the English-German sections ofthe Europarl (Koehn, 2005) and the News Com-mentary corpus.
Discarding pairs without target-side parses, the final training data has 2M sen-tence pairs, with 54M and 52M words on theEnglish and German sides, respectively.
Word-alignments are obtained by running GIZA++ (Ochand Ney, 2000) in both directions and refinedwith ?grow-diag-final-and?
(Koehn et al, 2003).For all experiments, a 5-gram language modelwith Kneser-Ney smoothing (Chen and Goodman,1996) built with the SRILM Toolkit (Stolcke andothers, 2002) is used.The development and test sets are the 2008WMT newstest (2,051 sentences) and 2009 WMTnewstest (2,525 sentences) respectively.
Featureweights are tuned with MERT (Och, 2003) onthe development set and output is evaluated us-ing case-sensitive BLEU (Papineni et al, 2002).For both rule extraction and decoding, up to seventerminal/non-terminal symbols on the source-sideare allowed.
For decoding, the maximum span-length is restricted to 15, and the grammar is pre-filtered to match the entire test set for both thebaseline system and the chart pruning decoder.We use two labellers to perform b- and e-tag la-belling independently prior to decoding.
Trainingof the labelling models is able to complete in un-der 2.5 hours and the whole test set is labelled inunder 2 seconds.
A standard perceptron POS tag-ger (Collins, 2002) trained on Wall Street Journalsections 2-21 of the Penn Treebank is used to as-sign POS tags for both our training and test data.4.2 ResultsFigures 4a and 4b compare CSP with the cubepruning baseline in terms of BLEU.
Decodingspeed is measured by the average decoding timeand average number of hypotheses generated persentence.
We first run the baseline decoder un-der various beam settings (b = 100 - 2500) un-til no further increase in BLEU is observed.
Wethen run the CSP decoder with a range of ?
val-ues (?
= 0.91 ?
0.99), at the default beam sizeof 1000 of the baseline decoder.
The CSP de-coder, which considers far fewer chart cells andgenerates significantly fewer subtranslations, con-sistently outperforms the slower baseline.
It ulti-mately achieves a BLEU score of 14.86 at a proba-bility cutoff value of 0.98, slightly higher than thehighest score of the baseline.At all levels of comparable translation quality,our decoder is faster than the baseline.
On aver-age, the speed-up gained is 63.58% as measuredby average decoding time, and comparing on apoint-by-point basis, our decoder always runs over60% faster.
At the ?
value of 0.98, it yields aspeed-up of 57.30%, compared with a beam sizeof 400 for the baseline, where both achieved thehighest BLEU.Figures 5a and 5b demonstrate the pruningpower of CSP (?
= 0.95) in comparison with thebaseline (beam size = 300); across all the cutoffvalues and beam sizes, the CSP decoder considers54.92% fewer translation hypotheses on averageand the minimal reduction achieved is 46.56%.Figure 6 shows the percentage of spans of dif-ferent lengths pruned by CSP (?
= 0.98).
As ex-355012345670  20  40  60  80  100  120  140HypothesisCount(x106 )Sentence Lengthcspcube pruning(a) sentence length vs. hypo count01000200030004000500060007000800090000  20  40  60  80  100  120  140Chart Cell CountSentence Lengthcspcube pruning(b) sentence length vs. cell countFigure 5: Search space comparison with the cube pruning baseline.01020304050600  20  40  60  80  100  120PercentagePruned(%)Span LengthFigure 6: Percentage of spans of different lengths pruned at ?
= 0.98.pected, longer spans are pruned more often, asthey are more likely to be at the intersections ofcells pruned by the two types of pruning labels,thus can be pruned by either type.We also find CSP does not improve search qual-ity and it leads to slightly lower model scores,which shows that some higher scored translationhypotheses are pruned.
This, however, is perfectlydesirable.
Since our pruning decisions are basedon independent labellers using contextual infor-mation, with the objective of eliminating unlikelysubtranslations and rule applications.
It may evenoffset defects of the translation model (i.e.
high-scored bad translations).
The fact that the outputBLEU did not decrease supports this reasoning.Finally, it is worth noting that our string-to-treemodel does not force complete target parses to bebuilt during decoding, which is not required in ourpruning method either.
We do not use any otherheuristics (other than keeping singleton and thetop-most cells) to make complete translation al-ways possible.
The hypothesis here is that goodlabelling models should not affect the derivationof complete target translations.5 ConclusionWe presented a novel sequence labelling based,context-sensitive pruning method for a string-to-tree MT model.
Our method achieves more than60% speed-up over a state-of-the-art baseline ona full-scale translation task.
In future work, weplan to adapt our method to models with differ-ent rule extraction algorithms, such as Hiero andforest-based translation (Mi and Huang, 2008).AcknowledgementsWe thank the anonymous reviewers for comments.The first author is fully supported by the CarnegieTrust and receives additional support from theCambridge Trusts.
Yue Zhang is supported bySUTD under the grant SRG ISTD 2012-038.Philip Williams and Philipp Koehn are supportedunder EU-FP7-287658 (EU BRIDGE).356ReferencesS.F.
Chen and J. Goodman.
1996.
An empirical studyof smoothing techniques for language modeling.
InProc.
ACL, pages 310?318.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Comput.
Linguist., 33(2):201?228.S.
Clark and J.R. Curran.
2004.
The importance of su-pertagging for wide-coverage ccg parsing.
In Proc.COLING, page 282.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proc.
EMNLP,pages 1?8.J.R.
Curran and S. Clark.
2003.
Investigating gis andsmoothing for maximum entropy taggers.
In Proc.EACL, pages 91?98.John DeNero, Mohit Bansal, Adam Pauls, and DanKlein.
2009.
Efficient parsing for transducer gram-mars.
In Proc.
NAACL-HLT, pages 227?235.M.
Galley, M. Hopkins, K. Knight, and D. Marcu.2004.
What?s in a translation rule.
In Proc.
HLT-NAACL, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.COLING and ACL, pages 961?968.Kenneth Heafield, Philipp Koehn, and Alon Lavie.2013.
Grouping language model boundary words tospeed k?best extraction from hypergraphs.
In Proc.NAACL.Mark Hopkins and Greg Langmead.
2010.
SCFGdecoding without binarization.
In Proc.
EMNLP,pages 646?655, October.L.
Huang and D. Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProc.
ACL, volume 45, page 144.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.NAACL-HLT, pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proc.ACL Demo Sessions, pages 177?180.P.
Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In MT Summit, volume 5.H.
Mi and L. Huang.
2008.
Forest-based translationrule extraction.
In Proc.
EMNLP, pages 206?214.F.
J. Och and H. Ney.
2000.
Improved statisticalalignment models.
In Proc.
ACL, pages 440?447,Hongkong, China, October.F.J.
Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proc.
ACL, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proc.
ACL, pages 311?318.S.
Petrov, A. Haghighi, and D. Klein.
2008.
Coarse-to-fine syntactic machine translation using languageprojections.
In Proc.
ACL, pages 108?116.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proc.
EMNLP, volume 1,pages 133?142.Brian Roark and Kristy Hollingshead.
2008.
Classify-ing chart cells for quadratic complexity context-freeinference.
In Proc.
COLING, pages 745?751.Brian Roark and Kristy Hollingshead.
2009.
Linearcomplexity context-free parsing pipelines via chartconstraints.
In Proc.
NAACL, pages 647?655.A.
Stolcke et al 2002.
Srilm-an extensible languagemodeling toolkit.
In Proc.
ICSLP, volume 2, pages901?904.Hao Zhang and Daniel Gildea.
2008.
Efficient multi-pass decoding for synchronous context free gram-mars.
In Proc.
ACL.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for ma-chine translation.
In Proc.
NAACL, pages 256?263.Y.
Zhang, B.G.
Ahn, S. Clark, C. Van Wyk, J.R. Cur-ran, and L. Rimell.
2010.
Chart pruning for fastlexicalised-grammar parsing.
In Proc.
COLING,pages 1471?1479.357
