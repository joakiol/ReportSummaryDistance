The ARC A3 Project:Terminology Acquisition Tools: Evaluation Method and TaskWidad Mustafa El Hadimustafa@univ-lille3.frI s m a ?
l  T i m i m itimimi@univ-lille3.frA n n e t t e  B ?
g u i nbeguin@univ-lille3.frMarcilio De Britomdebrito@noos.frUFR IDIST & CERSATES (CNRS UMR 8529)Universit?
Charles De Gaulle, Lille 3BP 149, F-59 653 Villeneuve D'Ascq, FranceAbstractThis paper describes the work achieved inthe Concerted Research Project ARC A3supported and coordinated by the AUF1,former Aupelf-Uref2.
The project dealswith the evaluation of term and semanticrelation extraction from corpora in French.Eight participants, both from publicinstitutions and industrial corporationswere involved in this project and wereresponsible for producing corpora suitablefor extraction tasks and elaborating aprotocol in order to evaluate objectivelyterminology acquisition tools.
Thisexpression covers respectively, termextractors, classifiers and semantic relationextractors.
The paper also reports on themethodology used for comparing four termextractors, one classifier and threesemantic relation extractors during the2000 evaluation campaign.
There are alsoseveral by-products of this campaign: first,two corpora which can be used for NLPsystem development and evaluation as theAUF recommended; and then terminologyproducts: for each corpus a list of termscharacterizing the field is available.
We arenot giving details about the results butrather an assessment of what the evaluationof Terminology Extraction Tools is: howwas it done, what were the difficulties,which are the advantages anddisadvantages of the adopted protocol,what are the limits and how should weproceed for future testing.1The Association des Universit?s Francophones2AUPELF is the "Association des Universit?s Enti?rementou Partiellement de Langue Fran?aise", an NGO whosemission is to promote the dissemination of French as ascientific medium.1 The ARC A3 ProgramARC A3 is a project of the ILEC3 groupcoordinated and founded by AUF.
It was startedin 1995 in order to promote research in the fieldof terminology acquisition.
The ARC A3,?Term and Semantic Relation Extraction fromCorpora in French?
project aim is to testsoftware capabilities in term and semanticrelation extraction from corpora in French.Systems submitted to this evaluation aredesigned by French and Canadian researchinstitutions (National Scientific Research Centerand Universities) and/or private businesses.These systems have been extensively describedin our previous work (cf.
B?guin, et al, 1997,2000; Jouis et al, 1997; Mustafa El Hadi et al,1996a, 1996b, 1997 & 1998;).
The first phase ofthe project has been directed towards testing thesystems on one corpus4 (trial run) and towardselaborating a workable protocol based on thisexperience.
The first results were presentedduring the first conference of JST5 (cf.
B?guin etal., 1997, 2000).
This article reports on thesecond and final evaluation campaign.2 ARC A3 OrganizationARC A3 brings together four kinds of actors: acoordinator who plays an organizational role(schedule, quality control of corpora, dataproduction, etc.
), corpora providers; participantsof the test and two scientific advisors.
Theaction has been coordinated by the University ofLille 3.
The organizing team in cooperation withthe discussion group made up of representativesof each participating team and two scientific3Ing?nierie de la Langue, Linguistique-informatique etCorpus ?crits.4SPIRALE, a periodical dealing with education andpedagogy issues.
Each periodical sizes around 200 pages.5Journ?es Scientifiques e Techniques de Francil, Avignon,France, 1997.advisors are supposed to co-operate in defining amethodology for testing the systems.2.1 Participating SystemsThe systems are designed by French andCanadian research institutions.
There were tenregistered participants at the beginning of theproject and three withdrew later for a variety ofreasons.
The organizers then launched anothercall for participation in July 1999 and threemore participants joined the project (two privateenterprises (Xerox and Logos) and a publicinstitution (the University of Grenoble).
Logosand the University team later dropped out forreasons unrelated to the program.
When the finalcampaign was launched in 2000 there were eightsystems remaining under evaluation.Fig.
1.
Participating SystemsSoftware AffiliationAcabit IRIN (Nantes)Ana IRIN (Nantes)Conterm LANCI (Montr?al)Iota CLIPS-IMAG (Grenoble)Lexter ERSS (Toulouse)Seek-Java CAMS-LALIC (Paris 4)Loria6 LORIA (Nancy)Xerox-Termfinder XEROX (Grenoble)2.2 Overview of the Tested ToolsTerminology plays a major role in informationprocessing and management and in specializedcommunication.
Its role has been enhanced bythe spread of automation and by the availabilityof electronic corpora.
These two factors havehad a massive impact on many differentapplications: systematic terminology7 building,natural-language interface design, lexical unitsmanagement for specific use in some sub-6This name is used for practical reasons since no finalsoftware name has yet been chosen7A holistic list of terms drawn from a representative corpuscharacterizing and describing a field of knowledge.
Inorder to be of any use this type of list must be subject to astructuring which is an important step towards exploitingextraction results.languages and technical writing, thesaurusconstruction, translation and indexing as well asthe recent growth of cross-language informationretrieval (CLIR).If we focus on the tools, presented in ourevaluation project, from the point of view oftheir functions and of the purposes for whichthey were designed), there are three categories:"Term Extractors", "Classifying Tools", and"Semantic relations extraction tools".
As wealready mentioned, these systems wereextensively described in our previouspublications.2.2.1 Term Extractors (TE)We will briefly describe the basic ideaunderlying TE tools.
Most of the extracting toolsconsider terms as noun phrases.
Systemsidentify terms by using frequency, distributionand category-pattern matching (Daille et al1995; Dagan, 1996; Lauriston, 1994).
All lexicalunits contained in a given text are analyzed andmatched to patterns (typical forms ofterminological units) described in rules.
Moreterm extractors are accounted for elsewhere(L'Homme, 1996; Kageura et al, 1996; Daganet al, 1994).
Some of the systems described bythese authors are tested in the framework of ourvaluation project (Acabit, Lexter, and Ana).2.2.2 Classifiers and Semantic RelationExtractors (SRE)Terminology resources are increasingly seen asstructured data i.e.
as a network of termsorganized by relations.
Pure alphabetical listscan hardly be used except for bilingual referencetools.
The variety of tools, their functions andthe different possible uses offered within theframework of ARC A3 shows this need.Consequently such lists of terms are quitedifficult to evaluate except by specialists in therelevant fields which makes it a ratherconstraining process.Structuring terms by semantic relations or inclasses is useful for the following applications:Index-making for on-line technicaldocumentation; browsing; information accessand retrieval; building thesaurus and ontologiesfor information systems.Many applications and extraction methodsrelevant to these tools have been described inthe literature.
The systems tested in the AUFframework are geared towards a variety ofapplications ranging from rough semanticrelation extraction, through indexing, thesaurusconstruction to knowledge-based systemmodeling (see figure 2).Classifiers and semantic relation extractors aretested within the same framework as the oneused for evaluating term extractors.
The firstcategory is characterized as classifying tools.Their role is to build classes of networks ofterms linked to a major one.
This categoryconsists of statistical and/or connectionistmodels such as Conterm.
It is the only classifiertested within the framework of this campaign.The second category includes semantic relationextractors which focus particularly on semanticrelations (Iota, Loria and Seek-Java).
Acomplete description of all the systems whichwere tested (main characteristics and purposes,description as far as approaches are concerned)is documented in previous work.3 Evaluation paradigmEvaluation activities are a corollary of the quickdevelopment of NLP tools in general and ofterminology extraction in particular.
It thusbecame necessary to evaluate these tools onobjectively based criteria in order to have a clearpicture of the state-of-the-art, assess the needs inthis sector and hence promote research in thisspecific field.
Moreover, the principal aim ofexisting testing methods, as reported in theliterature, is to come across software errors andthen try to adapt them for a particular userenvironment.Evaluation paradigm is basically dependantupon two major steps: (i) Creation of textualdata: raw or tagged corpora and test material.
Acorpus-based research is part of theinfrastructure for the development of advancedlanguage processing applications; (ii) Test andcomparison of systems on a similar data(Cavazza, 1993; Adda et al, 2000).3.1 The ARC A3 Evaluation ApproachThe approach we adopted is a black-boxqualitative approach8 The results are compared8This approach is adopted and validated by the vastmajority of participants to the test in June 1999.
Theorganizers have slightly adapted the protocol because moreparticipants joined the ARC after the validation of theprotocol.with the human performance of a task (eitherexperts examining results or using reference listsor both).
Moreover comparisons are made withother systems performing the same task.
Theresults are finally calculated and translated interms of traditional IR measures9.The conventional distinction between black-boxand glass-box is the following: the formerconsiders only system input-out-put relationswithout regard to the specific mechanisms bywhich the outputs were obtained while the latterexamines the mechanisms linking input andoutput.
(Sparck-Jones, 1996 p. 26; King, 1996;1999, among many others).The qualitative evaluation measures as describedby Sparck-Jones 1996, pp.
61-122, are based onobservation or interviewing and are broadlydesigned to obtain a more holistic, less reductiveor fragmented view of the situation.
It ismoreover more naturalistic.
This type ofevaluation naturally fits an end-free style.
In ourcase the quality of the results is evaluated bydomain experts.
We distinguish two types ofexperts: experts for the three applications tested(systematic terminology, translation andindexing); and experts in the two domains ofcorpora (biotechnology and pedagogy).Both quantitative and qualitative approaches aregoal-oriented, that is focusing on discrepanciesbetween performance results and initial systemrequirements.
Sparck-Jones points out how thetwo types of measures are deeply interwovenalthough different in their nature:- Recall is a quantitative measure of systemperformance while- Declared Satisfaction is a qualitative one (i.e.such a measure is really qualitative even if theresult of applying it to a set of users is apercentage figure).The qualitative approach in the evaluationprocess is the easiest one for end users.
It meansgiving a value judgment on how the systemglobally works (Cavazza, 1993; Chaudiron,2000).
The dominant approach today is towardsquantitative evaluations which are considered asmore objective and reproducible than thequalitative approach (EAGLES-1 1996; ISLE2001).
The main attempt of these approaches is9We chose to accompany the qualitative approach (mainlybased on manual evaluations) by a translation of themanual evaluations into numerical scales of values (seebelow for more details).to translate the concepts of relevance and qualityinto numerical data.
Statistical approaches suchas MUC 2 and TREC 3 are frequently used forthis type of evaluation.
(Chaudiron, 2000).3.1.1 The merits of a black-box evaluationObviously this approach has its pros and cons.But it can be justified on the following basis:- Since most developers cannot provide us (astest organizers) with their systems, the only waywas to send them the text corpora and let themprovide us with the results.
A glass-boxevaluation would have required an examinationof the systems by the organizers which wouldhave been impossible except for Xerox?sTermFinder and Logos System?s KnowledgeDiscovery, two commercialized systems.- Even if this approach may be criticized onaccount of its subjective side, end-users like itbecause of its usefulness when comparing twoor more systems which differ in all theirparameter settings.
(Chaudiron 2000; Cavazza1993).- A black-box evaluation is more orientedtowards system?s end-user when compared to aglass-box evaluation.
For the latter the test willinvolve analyzing the system?s functioning bylooking at its different components.
Eachcomponent is evaluated separately in itself.
Suchan approach allows for spotting andunderstanding the causes of dysfunctionalresults.
It is a long term process which requiresaccess to the internal parts of the system and anunderstanding of the architecture and globalstrategy of the software.
This is obviously adeveloper oriented approach and not an end-user one (Chaudiron 2000; Cavazza 1993).- In spite of its limited scope the evaluationprotocol we adopted is used in morecomplicated NLP tools, such as MT tools.Evaluators examine the systems?
output withoutconsidering the differences between them (cf.L?Homme, 2001).
Last Spring our team tookpart in a workshop organized by ISSCO(University of Geneva) where we and all theother participants adopted this approach.3.2 Elements of the Evaluation Protocol ofthe 2000 Campaign3.2.1 Evaluation TaskThe extraction of terms, of classes and ofsemantic relations was necessary to test the toolsperformance in the three following tasks:Systematic terminology (characterizing thetested corpora); (ii) Translation; (iii) Indexing.This means in practice: what is the relevance ofterms, classes and semantic relations providedby the systems being tested?
Do the terms,classes and semantic relations satisfy minimumrequirements?
Do we need to define a minimumlevel of terms, classes, semantic production?Are discrepancies meaningful?
For example, itcould be that most of the systems being testedare having qualitatively poor outputs, while onlyone or two produce worthwhile results.
Withinthis perspective the idea was to submit theresults to specialists.
We distinguished for thepurpose of this campaign two types of humanexpertise as we mentioned above.3.2.2 Test materialEvaluation data can normally be divided intotwo different categories (i) representativesamples of the tested corpora (ii) test material,which, in our evaluation framework, is made upof both custom-designed lists and real life lists /thesaurus.3.2.2.1 CorpusTwo corpora were tested: Spirale10 and INRA11.We have chosen a sample representing 10% ofeach corpus: for Spirale n?
19 was chosen.
Asfor INRA corpus, the providers of this corpussuggested 8 articles (603, 604, 607, 609, 631,666, 732, 740).3.2.2.2 Reference ListsThese lists are standard human professionalresults which can be used as performanceexemplars or norms for comparison.
This typeof data is considered to be a gold standard (seeSensEval, Kilgarrif 1998; ISLE 2001).For the INRA corpus the following lists havebeen created:For translation two lists were processed (i) a listcreated by a novice translator (ii) another one bya confirmed professional translator.10423 texts, 16 mega bytes1151 texts, 2,2 mega bytes.For indexing: six lists were created both byprofessional and by non professional indexers.We are not developing these lists in this papergiven the limited scope of this type of evaluationfrom an indexing point of view.
Hence thelimited interest of term extraction tools forhuman indexing.
We will however comment onthe terminology lists provided by the two corpusproviders, INRA (Institut National pour laRecherche Agronomique i.e.
National Institutefor Agronomic Research), the Francis list ofINIST12) and the translation lists.As far as INRA corpus is concerned:We think that our evaluation task could havegiven better results if the lists had been morerepresentative of a systematic terminologyactivity.
For the INRA corpora, for example,only 113 terms were chosen by the experts torepresent their terminology.
Our estimation isthat, 113 terms only constitute a poorrepresentation of an activity.
It would have beena good idea to have specialists establish the listsof terms and to compare those to the systems?output.
Even if this work is time consuming itmakes for a better evaluation of the systems?productivity.
As far as indexing is concerned theinterest of these lists is quite limited and wethink that a lot of time has been lost in drawingthem up and even grooming them.
From ageneral point of view the tools we haveconsidered, especially term extraction ones, onlyhave a limited interest for indexing contrary toother tools (semantic relation extractors) theyhave not been conceived for this purpose.
Thispoint of view is shared by their own designers.However, some of the semantic extraction toolsare adapted for indexing among their otherapplications (Iota and Loria, for instance).As for Spirale corpus:Terminology (i) Thesaurus Mobis, (educationalsciences section) (ii) Francis list (of the INIST,covering the complete volume on educationalsciences section).Three lists for indexing: - Dictionnaireencyclop?dique de l'?ducation et de laformation13.
- CRDP list14 de Lille.
- Br?hier list(PRCE in documentation ).12INIST is the National Institute of Scientific andTechnical Information.
The list they provided is used toindex their data-base to complete this part.13P.
Champy et C. Etev?.
Index pp 1059-1097.14Centre R?gional de la Documentation P?dagogique.3.2.2.3 Unified Presentation FormatThe protocol we suggested was based on theprevious evaluation sessions.
The layout ofsome results could at times make the task ofevaluation difficult.
In some cases, good graphicpresentation (conceptual graphs, etc.)
could hidea poor term extraction and hence influence theevaluation.
Conversely a system which has thecapacity to extract relevant terms and semanticrelations but whose layout is poor can influencethe evaluation process.
To prevent this,participants have been asked to adopt a unifiedformat for their presentations for 2000evaluation campaign.3.2.2.4.
Non-unified TaggingGiven the fact that system designers havedifferent processing possibilities, some of thesystems use an independent tagger, others havean integrated one which is part and parcel oftheir system.
The organizers decided to allowthe participants their own choice in terms oftagging methods.3.2.2.5.
Evaluation MeasuresGiven the three tasks to be performed (indexing,systematic terminology and translation), theusual notions of recall and precision can be usedto evaluate the quality of results when matchedwith a manually-produced reference list.Performance failure at this level can beinterpreted in terms of silence and noise (seebelow).3.2.2.6.
Automatic Matching by EvalTermIf the qualitative approach offers the easiestform of systems evaluation it neverthelessretains two major drawbacks: (i) it makes up fora very boring job when there are too manyresults (ii) judgments can easily be slanted bythe subjective approach of the expert.Our protocol being based on the qualitativeblack-box principle where parameters are hardto quantify we chose to apply traditional IRmeasures, recall and precision which normallyaccompany qualitative evaluations:R = number of correct extractions / number ofreference extractions.P = number of correct extractions / number ofproposed extractions15Since the manual matching of lists proved to belong and complicated due to the huge size of the15Or their equivalents in terms of noise and silence:Silence = 1 ?
Recall, Noise = 1 ?
Precisionprocessed data and to a variety of otherinconveniences, we chose to automaticallycalculate these measures.
We then decided toduplicate the manual evaluation with itsconversion into numerical scales of values.For this purpose we developed a program whichmatches the results provided by the softwarewith the reference lists16 The program comparestwo lists: L1 represents the results given by asoftware and list L2 is a reference list proposedby an expert17.
The program output consists oftwo files: file (a)which contains the elements ofL2 found in L1 (the relevant terms which thesoftware was able to find).
And file (b) whichcontains some elements of L2 which have notbeen identified and consequently were notmentioned in L1 (the correct terms not found bythe software).
Through a simple subtraction wecan get a file containing the noisy terms of eachsoftware.In our automatic matching we have not includedany linguistic treatment for fear of introducingnew parameters which would influence theresults.
Right from the beginning we havenoticed that over-productive systems such asAna or Term Finder are difficult to comparewith reference lists because the noise ratebecomes irrelevant.4 An Overview of the Results4.1 Term Extraction on the two CorporaWe will now comment globally on how the termextractors performed when run on the twocorpora for the three different tasks (indexing,systematic terminology and translation):First, automatic matching concurred with humanexperience which notices that the systemsproduce many ?
noisy ?
terms while on thecontrary there are many terms not included inthe reference lists but which the expertsconsidered as relevant for systematicterminology.
Hence the interest of some of these?
noisy ?
terms for enriching and updatingreference lists and terminology data bases.Matching the results of the different systems has16These lists can be: a) existing lists, real-life lists ( thesaurior alphabetical lists, such as Francis List);  b) establishedby the evaluators/indexers (specifically tailored for thethree tasks, indexing, terminology and translation).17They are many lists proposed by our experts.showed a great similarity between Lexter andAcabit.As for indexing, if the systems could generallyprovide relevant and effective help forterminology (systematic terminology, andtranslation) their contribution to indexing is lessobvious.
Indexing supposes other mentaloperations than those needed for terminologyconstruction and simply picking out candidate-descriptors is not enough to supply a reliableform of indexing.The three core criteria of good indexing are:reliability, selectivity and exhaustiveness.
Theindexer must hold a balance betweenexhaustiveness and selectivity.
Having too manyterms leads to noise and too few to silence.
It ison this criteria of selectivity that humanprocessing varies.Softwares based on term extraction offer a largenumber of potential candidate terms, connectingthem with more or less precise criteria ofrelevance, mostly of a statistical nature.
At thislevel of processing the indexer has recourse toauthorized lists and thesauri i.e.
he or she refersto the work of terminologists in structuring thefield and attributing a label to each and everyconcept.
The systems which we tried to assessare not yet likely to provide a very effective helpto indexing since the results are over-productivein view of the needs.5 The Classifier and the Semantic RelationExtraction (SRE) ToolsThe protocol we adopted specifies theevaluation of semantic relation and classvalidity, coherence and comprehensiveness onall of the three tasks (i.e.
semantic relationsexamined from the point of view of systematicterminology, translation and indexing).
Theclasses and semantic relations extracted weresubject to a comparison with the humanperformance of these tasks (experts andreference lists), plus a comparison with othersystems performing the same task.
Thisqualitative evaluation is measured by thetraditional IR performance measures (silence,noise, recall and precision).
The first thing wecan remark on is that it is very difficult to fulfillthe evaluation within our proposed terms ofreference.
We are presenting hereunder thereasons limiting the scope of our protocol whenapplied to SRE results.5.1 An Overview of the Results of SRE onthe Two CorporaWhat we observed is that these tools are toodifferent to allow a useful comparison for thefollowing reasons:- SRE extract different types of relations andhence are incomparable.- This difference is linked to the different formsof semantic model implementation.
Converselysome extractors are based on models that willnot allow the type of relations required for thethree evaluation tasks.- SRE are designed for different functions andhave different objectives or carry out differenttasks.- These differences are reflected in the type ofoutput or results.- Another problem came from the fact thatINRA could not provide us with a structured listcorresponding to the eight selected texts.
Even ifthis list had been available, comparing it to theresults would have been of limited interest only.The remaining solution was to submit the resultsto a field specialist.- Difficulties in interpreting the non-labeledSemantic Relations.
Fig.
Two shows thesedifferences:Fig.
2.
Synthetic comparison table for SREIOTA LORIA SEEK-JAVAObjectivesBuildingindexes ofone or morelevels (layers)for documentretrievalScientific &technicalwatch(identifyingrare or newinformationa) Cognitive textorganizationb) Extraction of LabeledSemantic relations betweenterms in a thesaurus or anetwork of termsc)Constructing/modelingKnowledge-based systemsFunctionsInformationseekingsystems,automaticextraction ofcomplexindexesSemanticrelationsextractionSemantic relationsextraction andrepresentationPresentation in a conceptualgraph fashionBuilding relational databasesOut-putLists ofpotentialcandidateterms rankedby withfrequencyTerminologynetworksnon-labeledLogico-SemanticRelationsbetween termsClasses oftermsA descriptive network/graphof terms linked withsemantic relations betweencomplex or simple terms,on the one hand and a tripletof argument-relation-argument on the otherassembled in a relationaldata-base- Moreover, it is difficult or even impossible tomeasure silence using a protocol based on IRsystems performance measure.?
Without a prior knowledge of the missingpossible relations one cannot account for thesilence measure.?
To account for noise, a thorough knowledgeof both the semantic model and the field ofknowledge is required.?
These observation are also valid for recalland precision measures.We can thus say for the time being that SREcannot be assessed by the protocol since theirresults cannot be matched.The field specialist18 gave the following account:?It is essential to have an interface to manipulateand interpret the relations.
Everything seemedsomewhat inconclusive.
At times the relation?fits well?, at times it does not at all.
Results arenot always relevant and it is difficult to trust thistype of analysis on its own if one is not at thesame time be conversant with the domain, sincesome of the relations can be wrong.For Iota, concept extraction seems generallyquite relevant.
However one has to wonderabout the relevance of a number of extractedconcepts which are not at all relevant to thefield.
How did these non-specific concepts getextracted more easily than others ?As for the table on Conceptual SemanticDependence19 it is hard to draw any conclusionsfrom it since it offers only one semantic label forany relation.The Iota approach is more global than the Seek-Java one since the relations are based on thewhole document and not only at the level of onesentence.
These two softwares are thus difficultto compare since their purpose is not the same?.5.2 Conterm, the Classifier: an ad hocEvaluationGiven the difficulties we listed above and thefact that it was impossible to compare Contermwith other systems performing the same task.The only possible evaluation for Conterm wouldhave been a progress evaluation for this soleclassifier of the campaign20.
This problem showsagain the limits of our Protocol.
The Contermlists were matched to an automatically produced18Patricia Volland-Neil, from INRA-Tours19The evaluator is referring to the tables accompanying theresults provided by the system?s designer.20The protocol is not suitable for its evaluation.
After thewithdrawal of another participant who had also presented aclassifier, only this one remained.untagged list of terms which corresponds to theeight texts of the INRA corpus.
The mostimportant element in its evaluation is not that wematched its results with a tagged list but that theresults had been matched with indexers?
and/orexperts lists and that we could observe thecorrespondence between Conterm?s output andthe lists.
It does not mean that Conterm is goodfor indexing but that the classes suggested bythis tool embody conceptual attributes which areclose to the logic underlying the humanselection of candidate-terms suitable forindexing, namely its rich lexico-semanticnetwork.6 Concluding Remarks- This evaluating action provided us with anawareness of the State-of-the-art in the field ofterminology acquisition tools.
It also allowed usto test evaluation paradigms, demonstrating howdifficult it was to apply a single evaluationprotocol to a variety of systems operating alongdifferent lines.- The discussions among participants aiming atthe creation of a testing protocol resulted in thedefinition of an evaluation procedure and in anassessment of their relative merits.
Thecomparative study of the systems?
out-put alsoenabled a better understanding of theperformances of the wide range of techniquesinvolved.
As by-products of the project twocorpora can be used in further evaluationcampaigns and a set of material tests (real-lifeand constructed or specifically tailored one thatcan be shared during future evaluations).- The evaluation results can be used predictivelyfor system design, development or modificationThe limits of our evaluation approach can besketched in the following manner:- If the adopted protocol based upon referencelist can be applicable to the two tasks(translation and terminology) it is hardlyapplicable to indexing tasks.- It is not adequate to account neither for theclassifiers nor for the SRE.- Several questions remain unanswered:a) first, is it possible to fully automateevaluation procedures?
Then is it possible toabandon test material, such as reference lists orother type of human-made data, which areconsidered as a kind of gold standard reusablefor other evaluation campaigns?
(see our recentexperience in MT evaluation workshop, April200121.b) As far as semantic relation extraction isconcerned, is it possible to automate SREvaluation procedure in the way Grefensttete(1994) does?7 Future Directions1.
Exploiting Results: the Campaign?s SideBenefits:Full treatment of the Spirale corpus will allowthe creation of an index of all the reviews pastnumbers, which fulfills  the moral contract madewith its Editorial Board in exchange for gettingthe corpus free of charge.
In addition, theseresults can help broaden the terminologicalrepository for the education sciences, especiallyin drawing up the Francis Thesaurus whichcovers all education sciences.2.
Towards Trans-Systemic Integration: Theoutput of the systems are divergent but can insome cases be complementary.
In fact thepreliminary results drawn from the firstevaluation in 1997 (cf.
B?guin et al 2000) haveled us to consider the feasibility of trans-systemic integration for strengthening theirautomatic terms identification capabilities.
Theidea is to combine two or three different types ofsystems in order to specify various integratedproduction processes.
Systems could21?Setting a methodology for Machine Translationevaluation?.
The context: evaluation of a translationmade by an MT System on the following source text:INRA corpus text N?604 ?
corpus biotechnologiquesur la reproduction chez l?animal ?
Source language:French - Target language: English.
We carried  outsome manual testing but with the objective of settinga rough methodology that might be irrelevant fortranslating huge size corpora.
The tool we used was anon interactive French / English MT System with abasic French/English dictionary that does not includeany specific terminology.
We had two indexes (aFrench index and an English index of domainspecific expressions, but they are not aligned).
Theyhave been provided by the INRA and considered asgold standard.
We used the indexes to create aspecific dictionary in order to feed the MT systemswith this specific lexical data.
The next step is toassess the impact of specific terminology whenintegrated to an MT system by comparing the resultsof the two translations we get: with and withoutspecific terminology.increasingly be seen as parts of these integratedproduction processes.3.
Towards User-Oriented Evaluations: in thelight of the results obtained in this campaign themost suitable type of evaluation would be auser-oriented one.
Other types of approaches22can be designed, such as adequacy evaluation23which can to some extent be adopted for ourcase but we have to define a more strict userprofile.4.
Towards developing interfaces for validatingthe results: even if we opted for a unifiedpresentation format for the reasons mentioned insection 3.2.2.3, we however think it is essentialfor future campaign organizers to have aninterface to manipulate and interpret the results(validating term, relations and classes).
Thistype of interface can dramatically facilitate theinteraction with the evaluators and the end-userof these tools.5.
Designing tools for generic bi-lingualproduction, allowing ad hoc extractions throughad hoc interfaces.6.
Capability to share resources in the future(test material such as gold standard lists, real-life and/or constructed ones).7.
Developing automatic evaluation tools suchas Evalterm which can be reused in similarfuture evaluations.8.
Hypothesis are still to be tested for semanticrelations extraction: results of the varioussemantic extractors will be of different qualitydepending on the type and nature of corpora(domain and genre) chosen (cf.
also Condamineset al 98; Davidson et al98, among manyothers).8 AcknowledgementsThe authors gratefully acknowledge thefinancial assistance provided by the AUF(Association des Universit?s Francophones) infunding the general research project withinwhich this paper was written.22Spark-Jones et al 1996; King 1999, among many others,identified more three types of evaluation processes: theprogress evaluation,  the adequacy evaluation and thediagnostic evaluation.
The first and second types are usedfor comparative benchmarking.23Adequacy evaluation aims at finding out whether asystem or product is adequate to someone?s needs.
Thistype is typically done when thinking of acquiring a system.9.
ReferencesAdda, G., Lecompte, J., Mariani, J., Paroubek, P.,Rajman, M. (2000).
Les proc?dures de mesureautomatique de l?action GRACE pour l?
?valuationdes assignateurs de partie du discours pour lefran?ais, Chibout, K., Mariani, J., Masson, N.,Neel, F.
?ds., (2000).
Ressources et ?valuation ening?nierie de la langue, Duculot, Coll.
Champslinguistiques, et Collection Universit?sFrancophones (AUF), pp.
645-664.B?guin, A, Jouis, C, Mustafa El Hadi , W, (1997):"Evaluation d?outils d?aide ?
la construction determinologie et de relations s?mantiques entretermes ?
partir de corpus", In JST'97, FRANCIL,AUPELF-UREF, Avignon, avril 1997, pp.
419-426.
This article is published in Chibout et al2001 (eds.)
pp 161-179.B?guin, A., Jouis, Ch., Mustafa Elhadi, W. (2000).Evaluation d?outils d?aide ?
l?extraction et ?
laconstruction automatiques de termes et de relationss?mantiques.
In Chibout, K., Mariani, J., Masson,N., Neel, F.
?ds., (2000).
Ressources et ?valuationen ing?nierie de la langue, Duculot, Coll.
Champslinguistiques, et Collection Universit?sFrancophones (AUF), pp 161-179.Bourigault, D. (1993).
?Analyse syntaxique localepour le rep?rage de termes complexes dans untexte?, Traitement automatique des langues 34(2),pp.
105-117.Bourigault, D., Jacquemin Ch.
&.
L?Homme M-C.?ds.
(1998).
Computerm ?98, First Workshop onComputational Terminology, COLING-ACL?98,15 August 1998.Bruandet, M.F.
(1989).
Outline of a knowledge basemodel for an intelligent Information Retrievalsystem.
In Information Processing andmanagement, Vol 25, N?
3, 1989.Cavazza, M., (1993).
M?thodes d'?valuation deslogiciels incorporant des technologiesd'informatique linguistique, Paris, Rapport MRE-DIST, 1993.Chaudiron, S. (2000).
The Relevance of QualityModel for NLP Applications, in Proceedings ofRIAO, pp 1568-1577, Paris 12-I4 April 2000.Condamines, A. et Rebeyrolle, J.
(1998).
CTKB: Acorpus-based approch to a TerminologicalKnowledge Base, In Bourrigault, D., JacqueminCh.
&.
L?Homme M-C, ?ds.
(1998).Computerm?98, First Workshop on ComputationalTerminology, COLING-ACL?98, 15 August 1998,pp.
29-35.Dagan.
I. and Church, K. (1994).
Termight:Identifying and Translating TechnicalTerminology.
In: Proceedings of 4th Applied NLPConference 1994, 34-40.Daille, B., (1994).
ACABIT: une maquette d?aide ?la construction automatique de banquesterminologiques monolingues ou bilingues.
InClass, A., Thoiron, P, , B?joint (eds)Lexicomatique et Dictionnairiques, pp.
123-136,Beyrouth 1996.Daille, B.
B. Habert, C. Jacquemin et J.
Royaut?(1995).
?Empirical Observation of Term Variationand Principles for their Description?, Terminology3(2), pp.
197-257.Davidson, L., Kavanagh, J., Mackintoch, K., Meyer,I., Skuce, D. (1998).
Semi-Automatic Extraction ofKnowledge-Rich Contents from Corpora, InBourrigault, D., Jacquemin, Ch.
&.
L?Homme M-C, ?ds.
(1998).
Computerm?98, First Workshop onComputational Terminology, COLING-ACL?98,15 August 1998, pp.
51-56.EAGLES (1996)http://www.issco.unige.ch/projects/eagles.Enguehard, C. (1993).
Acquisition de terminologie ?partir de gros corpus.
Informatique et languenaturelle ILN?
93, Nantes, pp.
373-384, d?cembre1993.Grefenstette, G. (1994).
Explorations in AutomaticThesaurus Discovery, Boston: Kluwer Academic-Press.
[Gaussier, E. (1995).
Mod?les statistiques et patronsmorphosyntaxiques pour l?extraction de lexiquesbilingues de termes, Th?se de doctorat, Paris:Universit?
Paris VII.ISLE (2001) MT Evaluation Classification,Expanded Classification (2001).http://www.isi.edu/natural-language/mteval/2b-MT-classification.htm.Jouis, C., (1993).
Contribution ?
la Conceptualisationet ?
la Mod?lisation des connaissances ?
partird'une analyse linguistique de textes.
R?alisationd'un prototype: le syst?me SEEK.
Th?se dedoctorat.
EHESS.
Paris.
1993.Jouis, C., Mustafa El Hadi, W. (1997), AUPELFProject: Term and Semantic Relation ExtractionTools.
Evaluation Paradigms, In Proc.
of theSpeech and Language Technology Club Workshop?
Evaluation in Speech and LanguageTechnology ?, Univ.
of Sheffied, June 17-18,Sheffield, UK, pp.
106-113.Kageura, K. and Umino, B.
(1996).
Methods ofAutomatic Term Recognition: A Review.
In:Terminology Vol.
3(2), 1996, 259-289.Kilgarrif, A., Rosenzweig, J.
(1998).
EnglishSENSVAL: Reports and Results.King (1999) EAGLES Evaluation Working Group,report, http://www.issco.unige.ch/projects/eagles.King M. (1996) EAGLES, Workshop, University ofGeneva http://www.issco.unige.ch/projects/eagles.Le Priol, F, Chavallet, J-P., Bruandet, M-F., Descl?s,J-P. (1998).
Int?gration d?un syst?me statistique(IOTA) et d?un syst?me s?mantique (SEEK) dansune cha?ne de traitement permettant l?extraction determinologies.
Actes Ing?nierie des Connaissances,Pont-?-Mousson, pages 33-40.
1998.L?Homme, Marie-Claude, Benali, Loubna, Bertrand,Claudine and Lauduique, Claudine.
(1996).Definition of an Evaluation Grid for Term-Extraction Software.
In: Terminology Vol.
3(2),1996, 291-312.L?Homme M-C. (2001).
?valuation d?outils d?aide ?la construction automatique de terminologie et derelations s?mantiques entre termes ?
partir decorpus ARC-A3 Rapport final, Montr?al, mars2001.Lauriston, A.
(1994).
Automatic Recognition ofComplex Terms: Problems and the TERMINOSolution.
In: Terminology 1(1), 147-170.Manzi S. (1999) Test Material for Evaluation, SandraManzi, ISSCO - University of Genevahttp://www.issco.unige.ch/projects/eagles/ewg99.Mariani, J. Masson, N., Neel, F., ?ds.
(2000).Ressources et ?valuation en ing?nierie de lalangue, Duculot, Coll.
Champs linguistiques, etCollection Universit?s Francophones (AUF), 2000,pp.
13-24, actes des 1?res Journ?es Francil autourdu th?me: L'ing?nierie de la Langue: de laRecherche au produit, Avignon 15-16 avril 1997.MUC-3, (1991).
Proceedings of the Third MessageUnderstanding Conference.
Morgan Kaufmann.MUC-4, (1992).
Proceedings of the Fourth MessageUnderstanding Conference.
Morgan Kaufmann.Mustafa El Hadi, W. & Jouis, C. (1998),Terminology Extraction and Acquisition fromTextual Data: Criteria for Evaluating Tools andMethod, Proceedings of the First InternationalConference on Language Resources andEvaluation, Grenada, Spain may 1998, pp.
11750-1178.Mustafa El Hadi, W. & Jouis, C. (1997), "Naturallanguage processing techniques and their use indata modeling and information retrieval".
In:Proceedings of the sixth international studyconference on classification research, KnowledgeOrganization for Information Retrieval, UniversityCollege London, London, 16-18 June 1997.
TheHague: FID, 157-161.Mustafa El Hadi, W., Jouis, C. (1996a), EvaluatingNatural Language Processing Systems as a Toolfor Building Terminological Databases, InProceedings of the Fourth International ISKOConference: Knowledge Organization andChange, Washington, Library of Congress, July1996, Advances in Knowledge Organization, Vol.5, INDEX Verlag, Frankfurt/Main, pp.
346-355.Mustafa El Hadi, W., Jouis, C. (1996b), NaturalLanguage Processing-based Systems forTerminological Construction and theirContribution to Information Retrieval.
InProceedings of the Fourth International Congresson Terminology and Knowledge Engineering(TKE'96), Vienna, INDEX Verlag,Frankfurt/Main.
pp.
118-130.Seffah, A. Meunier, J.G.
(1995).
ALADIN :un atelierorient?
objet pour l?analyse et la lecture de textesassist?e par ordinateur.
In : InternationalConference on statistics and textsSparck-Jones K., Gallier, J.R. (1996).
EvaluatingNatural Language Processing Systems: An Analysisand Review, Springer, Berlin.Toussaint, Y. Namer F., Daille, B., Jacquemein, C.,Royaut?, J., Hathou N., (1998).
Une approchelinguistique et statistique pour l?analyse del?information en corpus.
In : TALN?
98, Paris,France, 1998.
