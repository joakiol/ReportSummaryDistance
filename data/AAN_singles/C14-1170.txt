Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1803?1812, Dublin, Ireland, August 23-29 2014.Situated Incremental Natural Language Understanding using aMultimodal, Linguistically-driven Update ModelCasey KenningtonCITEC, Bielefeld Universityckennington1Spyros KousidisBielefeld Universityspyros.kousidis21@cit-ec.uni-bielefeld.de2@uni-bielefeld.deDavid SchlangenBielefeld Universitydavid.schlangen2AbstractA common site of language use is interactive dialogue between two people situated together inshared time and space.
In this paper, we present a statistical model for understanding naturalhuman language that works incrementally (i.e., does not wait until the end of an utterance tobegin processing), and is grounded by linking semantic entities with objects in a shared space.We describe our model, show how a semantic meaning representation is grounded with propertiesof real-world objects, and further show that it can ground with embodied, interactive cues suchas pointing gestures or eye gaze.1 IntroductionDialogue between co-located participants is possibly the most common form of language use (Clark,1996).
It is highly interactive (time is shared between two participants), interlocutors can refer to ob-jects in their visual field (space is also shared), and visual cues such as gaze or pointing gestures oftenplay a role (shared time and space).
Most computational dialogue research focuses only one of theseconstraints.In this paper, we present a model that processes incrementally (i.e., can potentially work interactively),can make use of the visual world by symbolically representing objects in a scene, and incorporate gazeand gestures.
The model can learn from conversational data and can potentially be used in an applicationfor a situated dialogue system, such as an autonomous robot.In the following section we will provide background and present related work.
That will be followedby a description of the task and the model.
In Section 4 we will show how our model performs in twoexperiments, the first uses speech and a visual scene, the second incorporates visual cues.2 Background and Related Work2.1 Background: Incremental Dialogue ProcessingDialogue systems that process incrementally produce behavior that is perceived by human users to bemore natural than systems that use a turn-based approach (Aist et al., 2006; Skantze and Schlangen, 2009;Skantze and Hjalmarsson, 2010).
Incremental dialogue has seen improvements in speech recognition(Baumann et al., 2009), speech synthesis (Buschmeier et al., 2012), and dialogue management (Bu?
etal., 2010; Selfridge et al., 2012).
Futhermore, architectures for incremental dialogue systems have beenproposed (Schlangen and Skantze, 2009; Schlangen and Skantze, 2011) and incremental toolkits are alsoavailable (Baumann and Schlangen, 2012).In this paper, we approach natural language understanding (NLU), which aims to map an utterance toan intention, as a component in the incremental model of dialogue processing as described in (Schlangenand Skantze, 2011; Schlangen and Skantze, 2009), where incremental systems consist of a network ofprocessing modules.
Each module has a left buffer and a right buffer, where a typical module takes inputThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1803Figure 1: Example of an IU network composed of words, parts of speech (POS), a semtic representation(Robust Minimal Recursion Semantics; RMRS), and NLU modules.
Solid arrows represent GRIN linksand the dotted lines represent SLLs.
The utterance take the red cross is represented as word IUs, whichare GRIN by the part of speech tags, phrase-structure parse, semantic representation, and the intention.Note that red and cross are GRIN by the same syntactic IU, which in turn is GRIN by two semantic IUs.Succeeding levels of IUs are shifted slightly to the right, representing a processing delay.
The X14 slotin the bolded NLU frame refers to the cross-shaped object in the game board on the right.from its left buffer, performs some kind of processing on that data, and places the processed result ontoits right buffer.
The data are packaged as the payload of incremental units (IU) which are passed betweenmodules.
The IUs themselves are also interconnected via so-called same level links (SLL) and grounded-in links (GRIN), the former allowing the linking of IUs as a growing sequence, the latter allowing thatsequence to convey what IUs directly affect them.
See Figure 1 for an example; each layer represents amodule in the IU-module network and each node is an IU in the IU network.
The focus of this paper isthe top layer (module), but how it is produced depends on the layers below it.2.2 Related WorkThe work presented in this paper connects and extends recent work in grounded semantics (Roy, 2005;Hsiao et al., 2008; Liu et al., 2012; Chai et al., 2014), which aims to connect language with the world,but typically does not work incrementally; semantic parsing / statistical natural language understandingvia logical forms (Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009), dependency-basedcompositional semantics (Liang et al., 2011), neural networks (Huang and Er, 2010), Markov LogicNetworks (Meurs et al., 2008; Meza-Ruiz et al., 2008), and Dynamic Bayesian Networks (Meurs et al.,2009); see also overviews of NLU in (De Mori et al., 2008; Tur and De Mori, 2011), but typically neitherprovide situated interpretations nor incremental specifications of the representations; incremental NLU(DeVault et al., 2009; DeVault et al., 2011; Aist et al., 2007; Schlangen and Skantze, 2009), whichfocuses on incrementality, but not on situational grounding; as well as integration of gaze into languageunderstanding (Prasov and Chai, 2010).We move beyond this work in that we present a model that is incremental, uses a form of grounded se-mantics, can easily incorporate multi-modal information sources, and which inference can be performedquickly, satisfying the demands of real-time dialogue.3 Task and Model3.1 TaskThe task for our model is as follows: to compute at any moment a distribution over possible intentionswhich the speaker wanted to convey in the utterance, expressed as semantic frames, given the unfoldingutterance and information about the state of the world in which the utterance is happening.
The slots ofthese frames are to be filled with semantic constants, that is, they are uniquely resolved, if appropriate,to objects in the shared environment.
This is illustrated in Figure 1 where the words of the utterance give1804rise to the part-of-speech tags, the incrementally growing syntax, semantic representation, and, finally,the intention.
Note how x14 in the bolded NLU frame resolves to an object identifier for a real object inthe shared scene (red cross in the bottom-left of the game board shown on the right in the figure).3.2 ModelKennington et al., (2013) presented a simple, incremental model of NLU, which is an update model(i.e., increments build on previous ones) and which can potentially work in real time and in situatedenvironments.
The goal of the model is to recover I , the intention of the speaker behind the utterance,word by word.
We observe U , the current word (or in this paper, a semantic meaning representation,see below) and an unobserved mediating variable R which represents visual or abstract properties of theobject of the intention.
Formally, we are interested in P (I|U), the probability of a certain intention Iunderlying utterance U .
We assume a latent variable R (pRoperties of entities in the world), and builda generative model (that is, model the joint P (I,R, U)).
Going from P (I,R|U) and making certainindependence assumptions, we arrive atP (I|U) =P (I)P (U)?r?RP (U |R = r)P (R = r|I) (1)That is, we assume that R is only conditional on I , and U is only conditional on R, and we can moveP (I) and P (U) out of the summation, as they do not depend on R. This is an update model in the usualsense that the posterior (P (I|U)) at one step becomes the prior (P (I)) at the next.
P (R|I) provides thelink between the intentions and the properties.Another variant of the model which we will use in this paper is as follows: we rewrite P (U |R) usingBayes?
rule, which cancels P (U) and introduces P (R) into the summation, but P (R) can be droppedsince (in this work) it can be approximated with a uniform distribution, yielding:P (I|U) = P (I)?r?RP (R = r|U)P (R = r|I) (2)There are, however, three important differences between the realisation of our model and the onepresented in Kennington et al., (2013), all of which are a direct result of replacing, as we do here, the n-gram model represented by P (U |R) with output from a parser that produces a Robust Minimal RecursionSemantics (RMRS) semantic representation (Copestake, 2007).
Such a representation provides our modelwith a structured way to abstract over the surface forms.
We will first give a brief explanation of theRMRS framework, then describe each of the three differences between our model and that of Kenningtonet al., (2013), namely (1) how the language grounds with the world, (2) how the frame is built, and (3)when to consider evidence for the slots in the frame.RMRS RMRS is a framework for representing semantics that factors a logical form into elementarypredicates (EP).
For example in Table 1, the first row represents the first word of an utterance, take, andthe corresponding RMRS representation; the EPs take and addressee are produced.
The EPs in this exam-ple have anchor variables and in most cases, an EP has an argument entity.
Relations between EPs can beexpressed via argument relations, e.g., for take in the table, there is an ARG1 relation, denoting addresseeas the first argument of the predicate take.
Other relations include ARG2 and BV (relating determiners tothe words they modify).
A full example of an utterance and corresponding RMRS representation can befound in Table 1, where each row in the word column makes up the words of the example utterance.In this paper we are interested in processing utterances incrementally.
As argued in Peldzsus et al.,(2012), RMRS is amenable to incremental processing by allowing for underspecification in how relationsare represented (RMRS can also underspecify scope, but we don?t consider that here).
Table 1 has anexample of an underspecified relation: when the second word the is uttered, the RMRS segment predictsthat the entity represented by x14 will be the ARG2 relation of the EP for take, but the actual word that1805word RMRS segmenttake a7 : addressee(x8), a1 : take(e2), ARG1(a1, x8)the a13 : def(), ARG2(a1, x14), BV (a13, x14)red a33 : red(e34), ARG1(a33, x14)cross a19 : cross(x14)next to a49 : next(e50), ARG1(a49, x14), ARG2(a49, x53)the a52 : def(), BV (a52, x53)blue a72 : blue(e73), ARG1(a72, x53)piece a58 : piece(x53)Table 1: Example RMRS representation for the utterance take the red cross next to the blue piece.
Eachrow represents an increment of the utterance.produces the EP that has x14 as an argument has not yet been uttered.
Each row in the table representswhat we would want an RMRS parser to produce for our model at each word increment.A more detailed explanation of RMRS can be found in Copestake (2007).
We will now discuss thethree key differences of our model with that of previous work.
(1) Grounding Semantics with the Visual World In Kennington et al., (2013), the utterance wasrepresented via n-grams, which was used to ground with the world.
Here, we ground RMRS structureswith the world.
For example, Figure 1 shows which words produced which RMRS increments; our modellearns the co-occurances between those increments and properties of objects (real properties such ascolors, shapes, and spatial placements, or abstract properties; e.g., take is a property of the action take).
(2) Building the Frame In this paper, intentions are represented as frames.
However, unlike Kenning-ton et al., (2013), we don?t assume beforehand that we know the slots of the frame.
To determine theslots, we turn again to RMRS and build a slot for each entity that is produced (more on this below).
Thiskind of frame, coupled with the RMRS representation, shows not just a meaning representation, but alsointerpretation of the representation in the current model (the real situation / visual domain of discourse),outputted incrementally making our model fully incremental in the sense of Heintze et al., (2010).
Thefinal, bolded NLU frame in Figure 1 shows the addressee (in this case, the dialogue system) as the recip-ient of the request, the request itself is a take request, where the object to be taken is obj5, as indexedby the real world, and that object happens to be red (i.e., e12 represents the notion of redness).
(3) Driven by Sematics Another important difference is when to consider the semantic evidence andwhen to ignore it, in terms of when to apply the model for interpretation of the slots.
In Kennington etal., (2013), each slot in the frame was processed at each increment in the entire utterance, regardless ofwhether n-grams in that segment contributed to the interpretation of that slot.
In our approach, again,we turn to RMRS.
At each word increment, RMRS produces a corresponding, underspecified semanticmeaning represenation which is added to at the next increment.
Our model takes the new informationand only attempts to process the interpretation for those ?active?
entities.
For example, by the time red isuttered in Figure 1, the processing for entities x8, e2, and e12 is complete, but the processing for x14is under way, and active as long as x14 is referenced as an entity in the RMRS increment.With these important extensions, our model of NLU is highly driven by the semantic meaning repre-sentation that is being built incrementally for the utterance.
We will now show through two experimentshow our approach improves upon previous work.4 ExperimentsSimilar to Kennington et al., (2013), we use the model represented formally in Equation 2, whereP (R|U) is realised using a maximum entropy classifier (ME) that predicts properties from RMRS evi-dence.1We use the German RMRS parser described in Peldszus et al (2012), Peldszus and Schlangen(2012) which is a top-down PCFG parser that builds RMRS structure incrementally with the parse.We train an individual model for each RMRS entity type (e.g., e and x), where the features are theentity type, relations, and predicates of an RMRS increment and the class label are the visual properties.1http://opennlp.apache.org/1806The RMRS representations are not checked for accuracy (i.e., they do not represent ground truth); we usethe top-predicted output of the RMRS parser explained in Peldszus et al (2012).4.1 Pento Puzzle with SpeechFigure 2: Example Pen-tomino Board??
?ACTION rotateOBJECT obj4RESULT clockwise??
?Figure 3: Pento gold frame ex-ample???
?X8 addrE2 rotateX14 obj4E21 clockwise???
?Figure 4: Pento frame examplefrom our modelData and Task The Pentomino domain (Fern?andez et al., 2007) contains task-oriented conversationaldata which has been used in several situated dialogue studies (Heintze et al., 2010; Peldszus et al., 2012;Kennington and Schlangen, 2012; Kennington et al., 2013).
This corpus was collected in a Wizard-of-Ozstudy, where the user goal was to instruct the computer to pick up, delete, rotate or mirror puzzle tiles ona rectangular board (as in Figure 2), and place them onto another board.
For each utterance, the corpusrecords the state of the game board before the utterance, the immediately preceding system action, andthe intended interpretation of the utterance (as understood by the Wizard) in the form of a semantic framespecifying action-type and arguments, where those arguments are objects occurring in the description ofthe state of the board.
The language of the corpus is German.
See Figure 2 for a sample source board,and Figure 3 for an annotated frame.The task that we want our model to perform is as follows: given information about the state of theworld (i.e., game board), previous system action, and the ongoing utterance, incrementally build theframe by providing the interpretation of each RMRS entity, represented as a distribution over all possibleinterpretations for that entity (i.e., domain of discourse).Procedure To make our work comparable to previous work, results were obtained by averaging theresults of a 10-fold validation on 1489 Pento boards (i.e., utterances+context, as in (Kennington andSchlangen, 2012)).
We used a separate set of 168 boards for small-scale, held-out experiments.
Forincremental processing, we used INPROTK.2We calculate accuracies by comparing against a gold frame,with assumptions.
We check to see if the slot values (3 slots in total) exist in the frame our modelproduces.
If a gold slot value exists in any slot produced by our model, it is counted as correct (it isdifficult to tell which slot from our model?s frame maps to which slot in the gold frame, we leave this forfuture work).
A fully correct frame would contain all three values.
For example, each of the values for thegold slots in Figure 3 exist in the example frame our model would produce in Figure 4, marking each goldslot as correct, and the entire frame as correct since all three were correct together.
To directly comparewith previous work, we will use the gold slot names action, object, and result in the Resultssection.
We perform training and evaluation on hand-transcribed data and on automatically transcribeddata, using the incremental speech recogniser (Sphinx4) in InproTK.
We report results on sentence-leveland incremental evaluations.On the incremental level, we followed previously used metrics for evaluation:first correct: how deep into the utterance do we make the first correct guess?first final: how deep into the utterance do we make the correct guess, without subsequent changes?edit overhead: what is the ratio of unnecessary edits / sentence length, where the only necessary edit isthe first prediction for an entity?Results Figure 5 shows the results of our evaluation in graph and table form.
As expected, our modeldramatically improved the result value, which generally is verbally represented towards the end of2https://bitbucket.org/inpro/inprotk1807ME+RMRS ME+NGRAMS MLN Pframe 78.75 74.08 74.76(63.0) (67.2) (61.2)action 92.11 93.62 92.62object 90.44 90.79 84.71 64.3result 94.0 82.34 86.65Figure 5: Comparison of accuracies in Pento using the model presented here ME+RMRS, (Kenningtonet al., 2013) ME+NGRAMS, (Kennington and Schlangen, 2012) MLN, (Peldszus et al., 2012) P; paren-theses denote results from automatically transcribed speech.
Bolded values represent the highest valuesfor that row.
Note that the column chart begins at 60%.
The chart and table show the same information.an utterance.
This resulted in a dramatic increase in frame accuracy (a somewhat strict metric).
Ourmodel fares better than previous work using speech (in parentheses in the figure), but is outperformed bythe n-gram approach.
These results are encouraging, however we leave improvements on automaticallytranscribed speech to future work.Incremental Table 2 shows the incremental results of Kennington et al.,(2013), and Table 3 showsour results.
Utterances are binned into short, normal, and long utterance lengths (1-6, 7-8, 9-17 words,respectively; 7-8 word utterances were the most represented).
Previous work processed all three slotsthroughout the ongoing utterance, whereas the model presented here only processed entities (that couldgive rise to these slots) as dictated by the RMRS.
This causes a later overall first correct, but an overallearlier first final, with a much narrower window between them.
This represents an ideal system that waitsfor processing a slot until it needs to, but comes to a final decision quickly, without changing its mindlater.
This is further evidenced by the edit overhead which is lower here than previous work.
This hasimplications in real-time systems that need to define operating points; i.e., a dialogue system would needto wait for specific information before making a decision.action 1-6 7-8 9-14first correct (% into utt.)
5.78 2.56 3.64first final (% into utt.)
38.26 36.10 30.84edit overhead 2.37object 1-6 7-8 9-14first correct (% into utt.)
7.39 7.5 10.11first final (% into utt.)
44.7 44.18 35.55edit overhead 4.6result 1-6 7-8 9-14first correct (% into utt.)
15.16 23.23 20.88first final (% into utt.)
42.55 40.57 35.21edit overhead 10.19Table 2: Incremental Results for Pento slots withvarying sentence lengths, Kennington et al.,(2013),Edit overhead represents all lengths of utterances.action 1-6 7-8 9-14first correct (% into utt.)
12.03 7.8 12.59first final (% into utt.)
37.84 26.02 24.11edit overhead 1.57object 1-6 7-8 9-14first correct (% into utt.)
30.64 17.66 14.46first final (% into utt.)
32.27 19.20 15.79edit overhead 3.1result 1-6 7-8 9-14first correct (% into utt.)
59.72 54.50 48.94first final (% into utt.)
62.80 64.13 60.72edit overhead 7.71Table 3: Incremental Results for Pento slots withvarying sentence lengths, current work.
Edit over-head represents all lengths of utterances.4.2 Pento Puzzle with Speech, Gaze, and DeixisData and Task The second experiment uses data also from the Pentomino domain, as described in(Kousidis et al., 2013; Kennington et al., 2013), also a Wizard-of-Oz study consisting of 7 participants,example in Figure 1.
The user was to select a puzzle tile (out of a possible 15) on a game board shownon a large monitor, and then describe this piece to the ?system?
(wizard).
Speech, eye gaze (tracked bySeeingmachines FaceLab) and pointing gestures (tracked by Microsoft Kinect) were recorded.
After theparticipant uttered a confirmation, the wizard began a new episode, generating a new random board and1808the process repeated.The task for the NLU in this experiment was reference resolution.
The information available to ourmodel for these data included the utterance (hand-transcribed) the visual context (game board), gazeinformation, and deixis (pointing) information, where a rule-based classifier predicted from the motioncapture data the quadrant of the screen at which the participant was pointing.
These data were very noisy(and hence, realistic) despite the constrained conditions of the task; the participants were not required tosay things a certain way (as long as it was understood by the wizard), their hand movements potentiallycovered their faces which interfered with the eye tracker, and each participant had a different way ofpointing (e.g., different gesture space, handedness, distance of hand from body when pointing, alignmentof hand with face, etc.
).Procedure Removing the utterances which were flagged by the wizard (i.e., when the wizard mis-understood the participant) and the utterances of one of the participants (who had misunderstood thetask) left a total of 1051 utterances.
We used 951 for development and training the model, and 100 forevaluation.
We give results as resolution accuracy.
All models were trained on hand-transcribed data,but two evaluations were performed: one with hand-transcribed data, and one with speech automaticallytranscribed by the Google Web Speech API.3Gaze and deixis are incorporated by incrementally com-puting properties to be provided to our NLU model; i.e., a tile has a property in R of being gazed atif it is gazed at for some interval of time, or tiles in a quadrant of the screen have the property of beingpointed at.
Figure 6 shows an example utterance, gaze, and gesture activity over time and how theyare reflected in the model.
Our baseline model is the NLU without using gaze or deixis information;random accuracy is 7%.
We will compare our model with that of an NGRAM (up to trigram) model in theevaluations, for each of the conditions (baseline, deixis, gaze, deixis and gaze).We also include the percentage of the time the gold tile is in the top 2 and top 4 rankings (out of 15);situations in which a dialogue system could at least provide alternatives in a clarification request (if itcould detect that it should have low confidence in the best prediction; which we didn?t investigate here).For gaze, we also make the naive assumption that over the utterance the participant (who in this case isthe speaker) will gaze at his chosen intended tile most of the time.Figure 6: Human activity (top) aligned with how modalities are reflected in the model for Gaze and Point(bottom) over time for example utterance: take the yellow t from this group here.
The intervals of theproperties are denoted by square brackets.Results Table 4 shows the results of our evaluation.
Overall, the model that uses RMRS outperformsthe model that uses NGRAMS under all conditions using hand-transcribed data.
The results for speech tella different story; speech with NGRAMS is generally better ?
an effect of the model here relying on parseroutput.
Overall, both model types increase performance when using hand-transcribed or automatically-transcribed speech when incorporating other modalities, particularly pointing.
Furthermore, the Top 2and Top 4 columns show that this model has an overall good distribution, especially in the case of RMRSand pointing, where the target object is in the top four ranks 90% of the time.
This would allow a real-time system to ask a specific clarification request to the human, with a high confidence that the object isamong the top four ranking objects.Incremental For further incremental results, Figure 7 shows the rank of each object on an exampleboard using our baseline model for the utterance nimm das rote untere kreuz (take the red below cross /3The Web Speech API Specificiation: https://dvcs.w3.org/hg/speech-api/raw-file/tip/speechapi.html1809NLU Acc Top 2 Top 4NGRAMS 68% 83% 87%(speech) NGRAMS 44% 57% 69%RMRS 73% 82% 88%(speech) RMRS 36% 54% 66%NLU + Pointing Acc Top 2 Top 4NGRAMS 70% 83% 88%(speech) NGRAMS 46% 60% 72%RMRS 78% 85% 90%(speech) RMRS 40% 56% 73%NLU + Gaze Acc Top 2 Top 4NGRAMS 68% 84% 88%(speech) NGRAMS 43% 59% 71%RMRS 74% 81% 88%(speech) RMRS 39% 54% 67%NLU + Gaze + Point Acc Top TopNGRAMS 70% 84% 87%(speech) NGRAMS 45% 61% 65%RMRS 77% 85% 89%(speech) RMRS 41% 56% 74%Table 4: Results for Experiment 2.
The highest scores for each column are in bold.
Four evaluations arecompared under four different settings; Acc denotes accuracy (referent in top position), Top 2 and Top4 respectively show the percentage of time the referent was between those ranks and the top.take the red cross below).
Once das (the) is uttered, RMRS makes an X entity and the model begins tointerpret.
The initial distribution appears to be quite random as das does not have high co-occurence withany particular object property.
Once rote (red) is uttered, all non-red objects fall to the lowest ranks inthe distribution.
Once untere (under / below) is uttered, all of the red pieces in the bottom two quadrantsincrease overall in rank.
Finally, as kreuz (cross) is uttered, the two crosses receive the highest ranks,the bottom one being the highest rank and intended object.
Note the rank of the cross in the top leftquadrant over time; it began with a fairly high rank, which moved lower once untere was uttered, thenmoved into second rank once kreuz was uttered.
As the utterance progresses the rank of the intendedobject decreases, showing that our model predicted the correct piece at the appropriate word.... das rote untere kreuzFigure 7: Example of reference resolution for the utterance: nimm das rote untere kreuz / take the redbelow cross; objects are annotated with their rank in the distribution as outputed by the NLU model ateach increment.
The board size has been adjusted for formatting purposes.5 Discussion and ConclusionsWe have presented a model of NLU that uses a semantic representation to recover the intention of aspeaker utterance.
Our model is general in that it doesn?t fit a template or ontology like other NLU ap-proaches (though we would need to determine how a dialogue manager would make use of such a frame),and grounds the semantic representation with a symbolic representation of the visual world.
It works in-crementally and can incorporate other modalities incrementally.
It improves overall upon previous workthat used a similar model, but relied on n-grams.
Our model implicitely handles complex utterances thatuse spatial language.
However, we leave important aspects, such as negation in an utterance, to futurework (they were not very common in our data).The experiments in this paper were done off-line, but we have a real-time system currently working.Our model incorporates in real-time the gesture and gaze information as it is picked up by the sensors,as well as the speech of the user.
We leave a full evaluation using this interactive setup with humanparticipants for future work.Acknowledgements Thanks to the anonymous reviewers for their useful comments.1810ReferencesGregory Aist, James Allen, Ellen Campana, Lucian Galescu, Carlos A Gomez Gallo, Scott Stoness, Mary Swift,and Michael Tanenhaus.
2006.
Software architectures for incremental understanding of human speech.
InProceedings of InterspeechICSLP.Gregory Aist, James Allen, Ellen Campana, Carlos Gomez Gallo, Scott Stoness, Mary Swift, and Michael KTanenhaus.
2007.
Incremental understanding in human-computer dialogue and experimental evidence foradvantages over nonincremental methods.
In Proceedings of Decalog (Semdial 2007), Trento, Italy.Timo Baumann and David Schlangen.
2012.
The InproTK 2012 Release.
In NAACL.Timo Baumann, Michaela Atterer, and David Schlangen.
2009.
Assessing and Improving the Performance ofSpeech Recognition for Incremental Systems.
In Proceedings of NAACL-HLT 2009, Boulder, USA, June.Hendrik Buschmeier, Timo Baumann, Benjamin Dosch, Stefan Kopp, and David Schlangen.
2012.
CombiningIncremental Language Generation and Incremental Speech Synthesis for Adaptive Information Presentation.In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages295?303, Seoul, South Korea, July.
Association for Computational Linguistics.Okko Bu?
Timo Baumann, and David Schlangen.
2010.
Collaborating on Utterances with a Spoken DialogueSystem Using an ISU-based Approach to Incremental Dialogue Management.
In Proceedings of the SIGdial2010 Conference, pages 233?236, Tokyo, Japan, September.Joyce Y Chai, Lanbo She, Rui Fang, Spencer Ottarson, Cody Littley, Changsong Liu, and Kenneth Hanson.
2014.Collaborative Effort towards Common Ground in Situated Human-Robot Dialogue.
In HRI?14, pages 33?40,Bielefeld, Germany.Herbert H Clark.
1996.
Using Language.
Cambridge University Press.Ann Copestake.
2007.
Semantic composition with (robust) minimal recursion semantics.
In Proceedings ofthe Workshop on Deep Linguistic Processing - DeepLP ?07, page 73, Morristown, NJ, USA.
Association forComputational Linguistics.Renato De Mori, Frederic B?echet, Dilek Hakkani-t?ur, Michael Mctear, Giuseppe Riccardi, and Gokhan Tur.
2008.Spoken Language Understanding.
IEEE Signal Processing Magazine, (May):50?58, May.David DeVault, Kenji Sagae, and David Traum.
2009.
Can I finish?
: learning when to respond to incrementalinterpretation results in interactive dialogue.
In Proceedings of the 10th SIGdial, number September, pages11?20.
Association for Computational Linguistics.David DeVault, Kenji Sagae, and David Traum.
2011.
Incremental Interpretation and Prediction of UtteranceMeaning for Interactive Dialogue.
Dialogue & Discourse, 2(1):143?170.Raquel Fern?andez, Tatjana Lucht, and David Schlangen.
2007.
Referring under restricted interactivity conditions.In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue, pages 136?139.Silvan Heintze, Timo Baumann, and David Schlangen.
2010.
Comparing local and sequential models for statisticalincremental natural language understanding.
In Proceedings of the 11th Annual Meeting of the Special InterestGroup on Discourse and Dialogue, pages 9?16.
Association for Computational Linguistics.Kai-yuh Hsiao, Soroush Vosoughi, Stefanie Tellex, Rony Kubat, and Deb Roy.
2008.
Object schemas for ground-ing language in a responsive robot.
Connection Science2, 20(4):253?276.Guangpu Huang and Meng Joo Er.
2010.
A Hybrid Computational Model for Spoken Language Understanding.In 11th International Conference on Control, Automation, Robotics, and Vision, number December, pages 7?10,Singapore.
IEEE.Casey Kennington and David Schlangen.
2012.
Markov Logic Networks for Situated Incremental Natural Lan-guage Understanding.
In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourseand Dialogue, pages 314?322, Seoul, South Korea.
Association for Computational Linguistics.Casey Kennington, Spyros Kousidis, and David Schlangen.
2013.
Interpreting Situated Dialogue Utterances: anUpdate Model that Uses Speech, Gaze, and Gesture Information.
In SIGdial 2013.Spyros Kousidis, Casey Kennington, and David Schlangen.
2013.
Investigating speaker gaze and pointing be-haviour in human-computer interaction with the mint.tools collection.
In SIGdial 2013.1811Percy Liang, Michael Jordan, and Dan Klein.
2011.
Learning Dependency-Based Compositional Semantics.
InProceedings of the 49th ACLHLT, pages 590?599, Portland, Oregon.
Association for Computational Linguistics.Changsong Liu, Rui Fang, and Joyce Chai.
2012.
Towards Mediating Shared Perceptual Basis in Situated Dia-logue.
In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,pages 140?149, Seoul, South Korea, July.
Association for Computational Linguistics.Marie-Jean Meurs, Frederic Duvert, Fabrice Lefevre, and Renato De Mori.
2008.
Markov Logic Networks forSpoken Language Interpretation.
Information Systems Journal, (1978):535?544.Marie-Jean Meurs, Fabrice Lef`evre, and Renato De Mori.
2009.
Spoken Language Interpretation: On the Useof Dynamic Bayesian Networks for Semantic Composition.
In IEEE International Conference on Acoustics,Speech, and Signal Processing, pages 4773?4776.Ivan Meza-Ruiz, Sebastian Riedel, and Oliver Lemon.
2008.
Accurate Statistical Spoken Language Understandingfrom Limited Development Resources.
In IEEE International Conference on Acoustics, Speech, and SignalProcessing, pages 5021?5024.
IEEE.Andreas Peldszus and David Schlangen.
2012.
Incremental Construction of Robust but Deep Semantic Represen-tations for Use in Responsive Dialogue Systems.
In Proceedings of the Workshop on Advances in DiscourseAnalysis and its Computational Aspects, pages 59?76, Mumbai, India, December.
The COLING 2012 Organiz-ing Committee.Andreas Peldszus, Okko Bu?, Timo Baumann, and David Schlangen.
2012.
Joint Satisfaction of Syntactic andPragmatic Constraints Improves Incremental Spoken Language Understanding.
In Proceedings of the 13thEACL, pages 514?523, Avignon, France, April.
Association for Computational Linguistics.Zahar Prasov and Joyce Y Chai.
2010.
Fusing Eye Gaze with Speech Recognition Hypotheses to Resolve Ex-ophoric References in Situated Dialogue.
In EMNLP 2010, number October, pages 471?481.Deb Roy.
2005.
Grounding words in perception and action: computational insights.
Trends in Cognitive Sciences,9(8):389?396, August.David Schlangen and Gabriel Skantze.
2009.
A General, Abstract Model of Incremental Dialogue Processing.
InProceedings of the 10th EACL, number April, pages 710?718, Athens, Greece.
Association for ComputationalLinguistics.David Schlangen and Gabriel Skantze.
2011.
A General, Abstract Model of Incremental Dialogue Processing.Dialoge & Discourse, 2(1):83?111.Ethan O Selfridge, Iker Arizmendi, Peter A Heeman, and Jason D Williams.
2012.
Integrating IncrementalSpeech Recognition and POMDP-Based Dialogue Systems.
In Proceedings of the 13th Annual Meeting of theSpecial Interest Group on Discourse and Dialogue, pages 275?279, Seoul, South Korea, July.
Association forComputational Linguistics.Gabriel Skantze and Anna Hjalmarsson.
2010.
Towards Incremental Speech Generation in Dialogue Systems.
InProceedings of SigDial 2010, pages 1?8, Tokyo, Japan, September.Gabriel Skantze and David Schlangen.
2009.
Incremental dialogue processing in a micro-domain.
Proceedingsof the 12th Conference of the European Chapter of the Association for Computational Linguistics on EACL 09,(April):745?753.Gokhan Tur and Renato De Mori.
2011.
Spoken Language Understanding: Systems for Extracting SemanticInformation from Speech.
Wiley.Luke S Zettlemoyer and Michael Collins.
2007.
Online Learning of Relaxed CCG Grammars for Parsing toLogical Form.
Computational Linguistics, (June):678?687.Luke S Zettlemoyer and Michael Collins.
2009.
Learning context-dependent mappings from sentences to logicalform.
Proceedings of the Joint Conference of the 47th ACL and the 4th AFNLP: Volume 2 - ACL-IJCNLP ?09,2:976.1812
