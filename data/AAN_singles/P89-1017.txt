How to  cover  a grammarRen6 LeermakersPhil ips Research Laboratories,  P.O.
Box 80.0005600 JA  Eindhoven, The NetherlandsABSTRACTA novel formalism is presented for Earley-like parsers.It accommodates the simulation of non-deterministicpushdown automata.
In particular, the theory is appliedto non-deterministlc LRoparsers for RTN grammars.1 In t roduct ionA major problem of computational linguistics is theinefficiency of parsing natural language.
The mostpopular parsing method for context-free natural lan-guage grammars, is the genera/ context-free parsingmethod of Earley \[1\].
It was noted by Lang \[2\], thatEarley-like methods can be used for simulating a classof non-determlnistic pushdown antomata(NPDA).
Re-cently, Tondta \[3\] presented an algorithm that simulatesnon-determlnistic LRoparsers, and claimed it to be a fastMgorithm for practical natural anguage processing sys-tems.
The purpose of the present paper is threefold:1 A novel formalism is presented for Earley-like parsers.A key rSle herein is played by the concept of bi-linear grammaxs.
These are defined as context-freegrammars, that satisfy the constraint that the righthand side of each grammar ule have at most twonon-terminals.
The construction of parse matrices?
for bilinear grammars can be accomplished in cubictime, by an algorithm called C-paxser.
It includesan elegant way to represent the (possibly infinite)set of parse trees.
A case in point is the use ofpredict functions, which impose restrictions on theparse matrix, if part of it is known.
The exact formand effectiveness of predict functions depend on thebilineax grammar at hand.
In order to parse a gen-era\] context-free grammar G, a possible strategyis to define a cover for G that satisfies the bilin-ear grammar constraint, and subsequently parse itwith C-parser using appropriate predict functions.The resulting parsers axe named Earley-like, anddiffer only in the precise description for derivingcovers, and predict functions.2 We present the Lang algorithm by giving a bilin-ear grammar corresponding to an NPDA.
Em-ploying the correct predict functions, the parserfor this grammar is equivalent o Lang's algo-rithm, although it works for a slightly differentclass of NPDA's.
We show that simulation of non-deterministic LR-parsers can be performed in ourversion of the Lang framework.
It follows thatEarley-like Tomita parsers can handle all context-free grammars, including cyclic ones, althoughTomita suggested ifferently\[3\].3 The formalism is illustrated by applying it to Recur-sire Transition Networka(RTN)\[S\]: Applying thetechniques of deterministic LR-parsing to gram-mars written as RTN's has been the subject of re-cent studies \[9,10\].
Using this research, we showhow to construct efficient non-deterministic LR-parsers for RTN's.2 C-ParserThe simplest parser that is applicable to all context-freelanguages, is the well-known Cocke-Younger-Kasa~i(CYK) parser.
It requires the grammar to be cast inChomsky normal form.
The CYK parser constructs,for the sentence zl ..zn, a parse matrix T. To each partzi+1 ..zj of the input corresponds the matrix element T.j,the value of which is a set of non-terminals from whichone can derive zi+1..zj.
The algorithm can easily begeneralized to work for any grammar, but its complexitythen increases with the number of non-terminals at theright hand side of grammar ules.
Bilinear grammarshave the lowest complexity, disregarding linear gram-mars which do not have the generative power of generalcontext-free grammars.
Below we list the recursion re-lation T must satisfy for general bilinear grammars.
Wewrite the grammar as as a four-tuple (N, E, P, S), whereN is the set of non-terminals, E the set of terminals, Pthe set of production rules, and S 6 N the start sym-bol.
We use variables I , J ,K ,L  E N,  ~1,~2,~z E E*,and i , j ,  kl..k4 as indices of the matrix T 1 .I E ~ij -~ 3J, KEN,i<kt<k2~ks~ka<j(J ~ Tktk~^K E Tkak4 A I "* 81JI~2KI~ A ~a = zi+l ..zktAB2 = Zk3?1..Zk3 A B3 ~-" 2~k4-~1..Zj)^Bt = zi+t..zk~ a ~2 = Zk~..zi)The relation can be solved for the diagonal elements T, ,independently of the input sentence.
They are equal tothe set of non-terminals that derive e in one or more1 Throughout the paper we identify a gr~ummar rule \[ --*with the boolean expression ' l directly derives ~'.135steps.
Algorithms that construct T for given input, willbe referred to as C-paxsers.
The time needed for con-structing T is at most a cubic function of the inputlength ~, while it takes an amount of space that is aquadratic function of n. The sentence is successfullyparsed, if S E Ton.
From T, one can simply deduce anoutput grammar O, which represents the set of parsetrees.
Its non-termlnals axe triples < I , i , j  >, where Iis a non-termlnal of the original bilineax grammar, andi , j  are integers between 0 and n.< l , i ,# >--.
#~ < 3,h,I~2 > fl~ < K,h , /~,  > #s =I E T,i A I  --.
#13\[~Kfl3 ^ J G Th~h2 ^K  G Tk~k,Afll = z~+l .
.
z~ ^fl~ ---- z~+~..z~ Afls = z~+~.
.z#< I, i, j >--  fl~ < 3, h ,  k~ > ~ - I ~ T~j ^  I - -  fl~ 3#2^J  E Tk~ka A fll ---- zi+~..zk~ A/@2 ---- :gk3.1.1 .Z i< I, i , j  >--* fla _= I ~ T~# A I --* fl~ ^  & = zi+~..zjThe grammar rules of O axe such that they generate onlythe sentence that was parsed.
The parse trees accordingto the output grammar are isomorphic to the parse treesgenerated by the original grammar.
The latter parsetrees can be obtained from the former by replacing thetriple non-terminals by their first element.Matrix elements of T are such that their memberscover part of the input.
This does not imply that allmembers axe useful for constructing a possible parse ofthe input as a whole.
In fact, many are useless for thispurpose.
Depending on the grammar, knowledge of partof T may give restrictions on the possibly useful contentsof the rest of T. Making use of these restrictions, onemay get more efficient parsers, with the same function-ality.
As an example, one has the generalized E~rleyprediction.
It involves functions predlct~ : 2 ~ --* 2N(Nis the set of non-terminais), such that one can provethat the useful contents of the Tj~ axe contained in theelements of a matrix @ related to T bySoo = S~,O,~ ffi p red ic t j _ , (~.o  O~,) m T,~, if j > O,where O c, called the initial prediction, in some constantset of non-termln~ls that derive (.
It follows that T~$can be calculated from the matrix elements O~t with i <k, l ~ j,  i.e.
the occurrences of T at the right hand sideof the recurrence relation may be replaced by O. Hence0~j, j > 0, can be calculated from the matrix elementsO~t, with !
< j:O~j = predict~_~(~ Os~)~{II~J, xe~t,,<~<~<~<~o<_~(3 ~ 0~^K ~.
O~s, , A I  -- fl~Jfl~Kfls Afl~ = z,+~..z~Afl~ = z~+z.
.z~ Aria = z~,+z..z~)V3aeN, i<k~<_k~<j( 3 ~ Okxk~ A I "-~ fll 3~Aflx = z~+~.
.zk~ A fl~ ---- Zk~.
.z~)V(!
- -  ~ ^ ~ = ~,+~..z,))The algorithm that creates the matrix @ in this way,scanning the input from left to right, is called a re-stricted C-paxser.
The above relation does not deter-mine the diagonal elements of ~ uniquely, and a re-stricted C-paxser is to find the smal\]est olution.
Con-cerning the gain of efficiency, it should be noted thatthis is very grammax-dependent.
For some grammars,restriction of the paxser educes its complexity, while forothers predict functions may even be counter-productive\[4\].3 B i l inear  coversA grammar G is said to be covered by a grammar C(G),if the language generated by both grammars i identical,and if for each sentence the set of parse trees generatedby G can be recovered from the set of parse trees gen-erated by C(G).
The grammar C(G) is called a coverfor G, and we will be interested in covers that axe hi-linear, and can thus be parsed by C-paxser.
It is rathersurprising that at the heart of most parsing algorithmsfor context-free languages lies a method for deriving abilineax cover.3 .1  Ear ley ' s  methodEaxley's construction ofitems is a clear example of a con-struction of a biHneax cover CE(G) for each context-freegrammar G. The terminals of CE(G) and G axe iden-ticai, the non-terminals of Cz(G) axe the items (dottedrnies\[1\]) I~, defined as follows.
Let the non-terminal de-fined by rule i of grammar G be given by N~, then I~ isN~ - -  a .
fl, with lilt + 1 = k (~, # axe used for sequencesof terminals and non-terminais).
We assume that onlyone rule, rule O, of G rewrites the start symbol S. Thelength of the right-hand side of rule i is given by M~ - 1.The rules of C~(G) are derived as follows.?
Let I~ be an item of the form A --* ~ ?
B~, andhence I~ - l  be A --, aB .
~.
Then if B is a terminal,I~ - I  ...* I~B, and if B is non-terminal then I~ - I  - -I~ ,  for all j such that Nj = B.?
Initial items of the form N~ --- .or rewrite to e:?
For each i one has the final rule/~ - -  I~.In \[4\] a similar construction was given, leading to agrammar in canonical two-form for each context-freegrammar.
Among other things it differs from the abovein the appearance of the final rules, which axe indeedsuperfluous.
We have introduced them to make the ex-tension to RTN's, in section 4, more immediate.The description just given, yields a set of productionrules consisting of sections P~, that have the followingstructure:Pi  --- ~-,iI211M' ,'fI#-li - -  I~ z'~/} t .
, l{ I~ ( - -  f lu  {I ?
-* I!
},where z~/ E U, {/~i) u E. Note that the start symbol ofthe cover is/~0.
The construction of parse matrices T byC-paxser yields the Eaxley algorithm, without its pre-diction part.
By restricting the parser by the predictofunction satisfyingv,edicto(  W) - ( X, - ^ x, t ) ,the initial prediction 0?
being the smallest solution ofs ?
= v, dicto(S u },136one obtains a conventional Earley parser (predict~ -~U~.
{I~ } for k > 0).
The cover is such that usually theJpredict action speeds up the parser considerably.There are many ways to define covers with dottedrules as non-terminals.
For example, from recent workby Kruseman Aretz \[6\], we learn a prescription for abilinear cover for G, which is smaller in size compared toC~(G) ,  at the cost of rules with longer ight hand sides.The prescription is as follows (c~, ~, 7, s are sequencesof terminals and non-termlnaJs, ~ stands for sequencesof terminals only, and A, B, C are non-terminals):?
Let I be an item of the form A --* or.
Bs ,  and K isan item B --* */-, then J .--, IK~,  where eitherJ is item A --* c~B~.
C~ and ~: = ~C~, orJ is item A --* ~B~.
and s --- 6.?
Let I be an item of the form A ---, 6 .Bc~ or A -* 6.,then I --* 6.3.2 Lang grammarIn a similar fashion the items used by Lang \[2\] inhis algorithm for non-deterministic pushdown automata(NPDA) may be interpreted as non-terminals of a hi-linear grammar, which we will call the Lang grammar.We adopt restrictions on NPDA's similarly to \[2\], themain one being that one or two symbols be pushed onthe stack in a singie move, and each stark symbol is re-moved when it is read.
If two symbols &re pushed onthe sta~k, the bottom one must be identical to the sym-bol that is removed in the same transition.
Formally wewrite an NPDA as & 7-tuple (Q, E, r ,  6, q0, Co, F), whereQ is the set of state symbols, E the input alphabet, rthe pnshdown symbols, 6 : Q x (I" tJ {e}) ?
(E U {?})
--* 2 Qx((~}uru(rxr)) the transition function, qo E Q theinitial state, ?0 E 1  `the start symbol, and F C_ Q is theset of final states.
If the automaton is in state p, and ?~is the top of the stack, and the current symbol on theinput tape is It, then it may make the following eighttypes of moves:if (r, e) E 6(p, e, e): gO to state rif (r, e) E 6(p, or, e): pop ~, go to state rif (r, 3") ~ 6(p, a, e): pop ~, push 3', go to state rif (r, e) ~ 6(p, e, It): shift input tape, go to state rif (r, 3') E 6(p, e, It): push 7, shift tape, go to rif (r, e) ~ 6(p, c~, It): pop ~, shift tape, go to rif (r, 3") ~ 6(p, ?~, It): pop c~, push % shift tape, go to rif (r, 3"or) ~ 6(p, ~, y): push % shift tape, go to rWe do not allow transitions such that (r, ~r) ~ 6(p, e, e),or (r, "yo~) ~ 6(p, ~, e), and assume that the initial statecan not be reached from other states.The non-terminals of the Lang grammar are the startsymbol 3 and four-tuple ntities (Lang's 'items') of theform < q, c~,p, ~ >, where p and q axe states, and cr andstack symbols.
The idea is that i f f  there exists a com-putation that consumes input symbols zi..zj, startingat state p with a stack ~0 (the leftmost symbol is thetop), and ending in state q with stack ~0,  and if thestack fl(o does not re-occur in intermediate configura~tions, then < q ,a ,p ,~ >---" z~..zj.
The rewrite rulesof the La~g grammar are defined as follows (universalquantification over p, q, r, s E Q; ~, ~, 7 E 1`; z E ~, t.J e,It E E is understood):S -*< p,a, qo,?0 >-p  E F (final rules)< r ,~,s ,  7 >--,< q,~,s ,  7 >< p,c~,q,/3 > z ----(,', ~) ~ 6(p, ~, ~)< r, 7, q, ~ >--"< P, ct, q, ~ > z((,', ~) ~ 6(,,,,, ~, z))V ((,', '0 E 5(p, e, ,~) ^  (~ = 7))< r, 7 ,P ,a  >---, It((,, ~) ~ 6(p, ~, It))v ((,, ~)  ~ ~(p, ~, It))< q0, ~0, g0, ?0 >--* e (initial rule)From each NPDA one may deduce context-free gram-mars that generate the same language \[5\].
The aboveconstruction yields such a grammar in bilinear form.It only works for automata, that have transitions likewe use above.
Lang grammars are rather big, in therough form given above.
Many of the non-terminals donot occur, however, in the derivation of any sentence.They can be removed by a standard procedure \[5\].
Inaddition, during parsing, predict functions can be usedto limit the number of possible contents of parse ma-trix elements.
The following initial prediction and pre-dict functions render the restricted C-parser functionallyequivalent to Lang's original algorithm, albeit that Langconsidered &class of NPDA's which is slightly differentfrom the class we alluded to above:s ?
= {< q0,?0,q0,?0 >}pred ic tk (L )  = ~ if  k = 0 elsepredic~h(L)  --  {< s ,~,q,~ > 13,,~ < ?,~, r ,  3" >~ L}u{Slk ffi n} (n is sentence l ngth).The Tomita parser \[3\] simulates an NPDA, con-structed from a context-free grammar via LR-parsing twhies.
Within our formalism we can implement this idea,and arrive at an Earley-like version of the Tomita parser,which is able to handle general context-free grammars,including cyclic ones.4 Extens ion  to  RTN'sIn the preceding section we discussed various ways ofderiving bilinear covers.
Reversely, one may try to dis-cover what kinds of grammars are covered by certainbllinear grammars.A billnear grammar C~(G), generated from a context-free grammar by the Earley prescription, has peculiarproperties.
In general, the sections P~ defined above con-stitute regular subgrammars, with the ~ as terminals.Alternatively, P~ may be seen as a finite state automa-ton with states I~.
Each rule I~ - l  --.//Jz~ correspondsto a transition from I~ to I~ - l  labeled by z~.
This cot-respondence b tween regular grammars and finite state137automata is in fact a special instance of the correspon-dence between Lang bilinear grammars and NPDA's.The Pi of the above kind are very restricted finitestate automata, generating only one string.
It is a natu-ral step to remove this restriction and study covers thatare the union of general regular subgrammars.
Such agrammar will cover a grammar, consisting of rules ofthe form N~ - .
~, where ~ is a regular expression ofterminals and non-terminals.
Such grammars go underthe names of RTN grammars \[8\], or extended context-free grammars \[9\], or regular ight part grammars \[10\].Without loss of generality we may restrict the formatof the fufite state automata, and stipulate that it haveone initial ?tale I~ '  and one final state/~, and only thefollowing type of rules:?
final rules P, - .
I~?
rules I I - -  .\[~z, where z ~ Um{J?m} U ~, k <> 0and j <> M~.?
the initial rule I/M~ - -  (.For future reference we define define the set I of non-terminals as I = U,${I~}, and its subset/o = U,{/~i }.A covering prescription that turns an RTN into a setof such subgrammars, educes to C~ if applied to normalcontext-free grammars, and will be referred to by thesame name, although in general the above format doesnot determine the cover uniquely.
For some exampledefinitions of items for RTN's (i.e.
the I~), see \[1,9\].5 The  CNLR CoverA different cover for RTN grammars may be derivedfrom the one discussed in the previous section.
Soour starting point is that we have a biline&r grammarC?
(G), consisting of regular subgrammars.
We (approx-imately) follow the idea of Tomita, and construct anNPDA from an LR(O)-antomaton, whose states are setsof items.
In our case, the items are the non-terminalsof C~(G).
The full specification of the automaton is ex-tracted from \[9\] in a straightforward way.
Subsequently,the general prescription of chapter 3 yields a bilineargrammar.
In this way we arrive at what we would like tocall the canonical non-deterministic LR-parser (CNLRparser, for short).5.1  LR(0)  s ta tesIn order to derive the set Q of LR(0) states, which aresubset?
of I, we first need a few definitions.
Let ?
be anelement of 2 I, then closure(s) is the smMlest element of2 x, such thats c closure(s)^ ((~!
~ ~osure(s)^ (xp - x l~) )x= ~-  ~ aos.re(s))Similarly, the sets gotot(s, z), and goto.j(s, z), where z E/o U E, are defined asgoto~(s, ffi) = closu,e({~'lII ~ s ^ (I,* --.
I !~)  ^  j <> M,})goto~(s, ~) = closure({I?lI, ~'  ~ ?
^ (Ip - I~'ffi)})The set Q then is the smallest one that satisfiesaosnre({&~?})
~ q^ (~ ~ q *(gaot(s, =) = O V gotot(s, z) ~ q)^Oao2(,, z) = O v go,o2(s, ~) ~ q))The automaton we look for can be constructed in termsof the LR(0) states.
In addition to the goto function?,we will need the predicate reduce, defined by, 'edna(s,_ : )  -- 3,,((~ -- X~') ^ Xl' ~ s).A point of interest is the possible xistence of ?tackingconflicts\[9\].
These arise if for some s, z both gotol (s, z)and goto2(a, x) are not empty.
Stacking conflict?
causean increase of non-determinism that can always beavoided by removing the conflicts.
One method for do-ing this has been detailed in \[9\], and consist?
of the split-ting in parts of the right hand side of grammar rule?
thatcause conflicts.
Here we need not and will not assumeanything about the occurrence of stacking conflict?.Grammars, of which Earley cover?
do not give riseto stacking conflicts, form a proper subset of the setof extended context-free grammars.
It could very wellbe that natural language grammar?, written as RTN's inorder to produce 'natural' syntax trees, generally belongto this subset.
For an example, see section 6.5.2  The  automatonTo determine the automaton we specify, in addition tothe set of states Q, the set of stack symbols F ---- QUI?u{Co}, the initial state q0 = closure({IoM?
}), the finalstates F ffi {slrednce(s, ~)}~ and the transition function&6(s, -f, y) = {(t, q'f)l "f ~ /?A(0 = goto~(s, y) ^  q ffi s) v(~ = gotol(s, y) ^  q = +))}6(8,-r, ?)
-- {(t, q)l~ E /?h((t = gotot (s, "f) Aq = ?
)V ((t = goto2 (s, 7) A q = s))}u{(~, ~)l ' f  ~ q ^ reduce(s, ~)}5.3  The  grammarFrom the automaton, which is of the type discussed insection 3.2, we deduce the bilinear grammarS - -< s,~,q0,?0 >= reduce(s,~)< t,r,q,~ >---~< s,r,q,/~ > y = t = gotoz(s,y)< t, s, s, r >--.
y -- t = goto2(?, y)< t ,#,p ,~ >- .< q,~,p,~ > < s,/?, ,q,~ >- t = goto l ( s , l  ?
)< t , s ,q ,~ >- .< s, I2 ,q ,~ >=- t = goto~(?,l'~,)< p,l~,q,~ >--*< s,p,q,# > - -  reduce(s,I ?
)< qo, Co, qo,?o >"* ~,where $,t,q,p E Q, r E QU{C0}, ~,/~ E r, y E E.A?
was mentioned in section 3.2, this grammar can bereduced by a standard algorithm to contain only usefulnon-terminals.1385.3.1 A reduced  fo rmIf the reduction algorithm of \[5\] is performed, it turnsout that the structure of the above grammar issuch thatuseful non-terminals < p, ?~, q, ~ > satisfya ~Q=~.ot fq~f~Q=~p=qFurthermore, two non-terminals that differ only in theirfourth tuple-element always derive the same strings ofterminals.
Hence, the fourth element can safely be dis-carded, as can the second if it is in Q and the first ifthe second is not in Q.
The non-termlnals then becomepairs < ~, s >, with ~ ~ I' and s ~ Q.
For such non-terminals, the predict functions, mentioned in section 2,must be changed:0 ?
= {< ~o,~o >}pcedia~(L) = 0 if k = 0 elsepredicts(L) = {< ~, ~ > 13~ < s, q >E L} U {Sit = n}The grammar gets the general formS --*< s, qo >---- reduce(s, /~o)< t,q >--*< ~,q > / /=  t = gotot(s, 9)< t, s >--* y ---- t = gotoa(s, y)< ~,0 >-< , ,~ >< P, , ,  > - ~ = ~oto:(,,~)< ~,, >-< ~,  s >= ~ = ~o~o~(,, ~)< ~,  q >-<.
,  ~ > __.
reau~(s,  ~)Note that the terminal < q0, q0 > does not appear inthis grammar, but will appear in the parse matrix be-cause of the initial prediction 0c.
Of course, when theautomaton is fully specified for a particular language,the corresponding CNLR grammar can be reduced stillfurther, see section 6.4.5.3.2 F ina l  formEven the grammar in reduced form contains many non-terminals that derive the same set of strings.
In partic-ular, all non-terminals that only differ in their secondcomponent generate the same language.
Thus, the sec-ond component only encodes information for the predictfunctions.
The redundancy can be removed by the fol-lowing means.
Define the function ?
: I' - .
2 Q, suchthat~(~r) ---- {s{ < or, s > is a useful non-terminal of theabove grammar}.Then we may simply parse with the 'bare' grammar, thenon-terminals of which are the automaton stack symbolsF:S --* S ~ reduce(s, ~0)t --.
sy - - t  =.gotoz(s,y)* - -  P, - ~,(~ = goto20, ~,))I~, - .
~ - reduce(s, I?
),using the predict functions0 ?
= {qo}predicth(L) = ~ if k = 0 elseprea iah(Z , )  = {~1~,(" ~ L^,  ~ ~(~))} u {Slk = .
}.The function ?
can also be deduced irectly from thebare grammar, see section 7.5.4  Parse  t reesEach parse tree r according to the original grammar canbe obtained from a corresponding parse tree t accordingto the cover.
Each subset of the set of nodes of t is par-tially ordered by the relation 'is descendant of'.
Nowconsider the set of nodes of t that correspond to non-terminals/~.
The 'is descendant of' ordering defines aprojected tree that contains, apart from the terminals,only these nodes.
The desired parse tree r is now ob-tained by replacing in the projected tree, each node 1 ?by a node labeled by N~, the left hand side of grammarrule i of the original grammar.6 ExampleThe foregoing was rather technical and we will try to re-pair this by showing, very explicitly, how the formalismworks for a small example grammar.
In particular, wewill for a small RTN grammar, derive the Earley coverof section 4, and the two covers of sections 5.3.1 and5.3.2.6.1  The  grammarThe following is a simple grammar for finite subordinateclauses in Dutch.$ -* conj NP  VPVP  --* \[NP\] {PP} verb \[S\]PP  --* prep NPNP  --* det noun {PP}So we have four regular expressions defining No = S,N1 ffi V P, N2 = P P, N3 -- N P.6.2  The  Ear ley  coverThe above grammar is covered by four regular subgrarn-m aA's"~0 - z~;I~ - I0~z,?
; Zo ~ - I~ ;  Ig - Io`co.j;  Io' -- x~;g - I~;II - I~Ig;x ~, - I~erb;X~ -x?~;x~ - I , *~;P ,  - ~?,,erb;??
- z~z?
;z~ -x~ &; P, - Xb, erb; x~ -z** .o .
;  ~ - It ae*; xt -Note that the Mi in this case turn out as M0 = 4, Mz =5, M~ = 3, M3 = 4.1396.3  The  automatonThe construction of section 5.1 yields the following setof states:qo = {I~}; ql = {I~,I~}; q2 = {~, I \ [ , I~ ,~};qa = {I~}; q, -- {IoI }; qs ffi {I~,I$}; q* = {I~,I~};q, = {Xo~, x,=}; qs = {P,,xD;qo = {zL xD;qlO = {R};q -  = {R}; ?12 = {xLR}The transitions axe grouped into two parts.
First we listthe function goto~:goto2(?0, ~o,=~) = ~;  goto=(?l, det) ffi ?~;go=o.
(q~, P.) ffi qs; OO=O~(q2, ~)  ffi ~s;goto2(,2, verb) ffi q.; goto~(~2, prep) = ~;go?o2(q2, de0 = q~; got~(~,  prep) = qs;goto~(qs,prep) = qs; goto~(qr, conj)  - -  ql;goto~(qs, det) = qa; goto~(qs,prep) = qs;goto2(ql=, prep) "J-- qsLikewise, we have the gotot function, which gives thenon-stacking transitions for our grammar:gotol (ql , ~)  = q'a; gotol (q,, I~ ) = q,;gotol (q~, noun) = q~; gotol (qs, g) ---- qs;gotol(qs, verb) = ~,; goto~(qs, ~=) = qs;goto, (~, , Po ) = elo; goto, (es, ~)  = q .
;go,o, (e., ~)  = el=; go,o, (q,=, g )  = e,,The predicate reduce holds for six pairs of states andnon-terminals:redu~O,,  Po); redu=O,o, ~) ;  redffi~(q,, ~);reduce(q,l , \]~=); reduce(q,, g) ;  reduce(ql=, l~a )6.4  CNLR parserGiven the automaton, the CNLR grammar follows ac-cording to section 5.3.
After removal of the useless non-terminals we arrive at the following grammar, which isof the format of section 5.3.1.S ..--,< q4,qo >< q~,q >- .< q~,q > noun, where q E \[ql,q~,qs\]< qT, q~ >--*< qs,q~ > verb< q~,q >-* conj, where q E \[qo, qT\]< q~,q >--* det, where q E \[qt,q~,qs\]< q?, q2 >--* verb< qs,q >"* prep, where q E \[q~,q~,qs,qe,q~\]< q~,q >-*< ql,q > </~,q~ >, where q ~ \[qo,qT\]< qt,q >--*< q~,q > </~t,q~ >, where q E \[qo, qT\]< qs, q2 >'-*< qs,q~ >< I~, qs >< qs, q~ >'-'*< qs,q~ ></~,qs  >< qlo, q2 >"'*< ql', q2 >< ~0, q?
>< q~,q  >-'*< qs,q > < /~,qs >, where q E\[q~, ~s, qs, w,  q~2\]< ql2, q >-"*< qs, q > < ~,q9  >, where q E \[ql,q2,qs\]< q12, q >"*< ql2, q > < /~2,q12 >, where q E\[~,,q2,qd< qs,~ >-*< ~,q2 >, < qs,q2 >- .< ~,q2 >< I~o,qv >--.< q4,q7 >, < I~l,q2 >- .< qlo,q2 >"</~x,q2 >-'*< qT, q2 >< \]~2,q >"~< ql l ,q >, where q E \[q2,qs,qe,qo, q12\]< I?
,q  >-*< qs, q >, where q E \[qx,q2,qs\]</~3,q >'-'~< q12,q >, where q E \[ql,q2,qs\]From this grammar, the function ?
can be deduced.
Itis given by~(?1) ffi ~(q2 ffi ~(q.)
= \[?0, q,\]~r(q3) -- ~(qg) --- a(q12) ---- ~(I ?)
= \[ql, q2, qs\].
(q~) = ~(?s) = #(q,) = ~(q~0) = ~( : )  = \[q2\]~0s)  = ~(q- )  = ~(~)  = \[q2, q~, q~, q~, q12\]~(g)  = \[q,lEither by stripping the above cover, or by directly de-ducing it ~om the automaton, the bare cover can beobtained.
We list it here for completeness.S -* q4, q9 -* q3noun, q?
"* qsverbql -* conj, q3 --* det, q7 "* verbqs "* prep, q2 "* qlI~3, q4 "* q2\]~zqn "* qs~, g12 "-* qs~, q12 --* q12~- qlo, ~ - q,, ~ - qll~-  q,, ~-  q,2,Together with the predict functions defined in section5.3.2, this grammar should provide an efficient parserfor our example grammar.7 Tadpo le  GrammarsThe function ~ has been defined, in section 5, via agrammar reduction algorithm.
In this section we wish toshow that an alternative method exists, and, moreover,that it can be applied to the class of bilinear tadpolegrammars.
This class consists of all bilineax grammarswithout epsilon rules, and with no useless ymbols, withnon-termlnals (the head) preceding terminals (the tail)at the right hand side of rules.Thus, rules are of the formA -* a6,where we use the symbol 6 as a variable over possiblyempty sequences of terminals, and a denotes a possiblyempty sequence of at most two non-terminals.
Capitalromu letters are used for non-terminals.
Note that aCNLR cover is a member of this class of grammars, asare all grammars that are in Chomsky normal form.First we change the grammar a little bit by addingq0 to the set of non-terminals of the grammar, assum-ing that it was not there yet.
Next, we create a new140grammar, inspired by the grammar of 5.3.1, with pairs< A, C > as non=terminals.
The rules of the new gram-mar are such that (with implicit universal quantificationover all variables, as before)< A, C >-..~ 6 -- A -.~ 6< A,C  >.--~< B ,C  > 6 m__A..-~ B6< A,C  >-~< B ,C  >< D,B  > 8 =_ A - .
BD8The start symbol of the new grammar, which can beseen as a parametrized version of the tadpole grammar,is defined to be < S, qo >.
A non-terminal < B, C > is auseful one, whence C E ~(B) according to the definitionof ~, if it occurs in a derivation of the parametrizedgrammar:< S, qo >---" ~ < B, C > A,where i?
is an arbitrary sequence of non-terminals, andA is a sequence of terminals and non-terminals.
Then,we conclude thatq0 E ~(B) -<  S, q0 >- .
'<  B, q0 > AC E ~r(B) ^  C <> q0 --- 3A,~(< A,C  >-- '<  B,C  >/,^ < S, qo >--* " s < C,D >< A ,C  > A)This definition may be rephrased without referenceto the parametrized grammar.
Define, for each non-terminal A a set f irstnonts(A), such thatf irstnonts(A) --.. {BIA --" BA}.The predict set o(A) then is obtainabh as?
( s )  = {Cl3.~,v, , (a ~.
firstnonts(A)AD - -  CA6)}  u {qolS E f irstnonts(S)},where S is the start symbol.
As in section 5.3.2, theinitial prediction is given by 0= = {q0}.8 An LL/LR-automatonIn order to illustrate the amount of freedom that ex-ists for the construction of automata nd associatedparsers, we shall construct a non-deterministic LL/LR-automaton and the associated cover, along the lines ofsection 5.8.1  The  automatonWe change the goto functions, such that they yield setsof states rather that just one state, as follows:go=o,(s, z) ---- {dosure({I,~})lZl ~ s ^ (Z~ - -  ZI=) A j <> M,}goto~O, =) = {ao.ure({z~}) lZ ,  ~ '  e s A (Z, ~ - -  Z,~'=)}The set Q is changed accordingly to be the smallest onethat satisfiesctos,,re({Xo"?})
E Q^ (s E q =~(go=o,(s, =) = 0 v goto,(s, =) c q)^(goto2(s, z) m ~ V gotoa(s ,  z) C q))Every state in this automaton is defined as a setclos~re({I~ }) and is, as a consequence, completely char-acterized by the one non-terminal I~.
The reason forcalling the above an LL/LR-automaton lies in the factthat the states of LR(0) automata for LL(1) grammarshave exactly this property.
The predicate reduce is de-fined as in section 5.1.8.2 The LL/LR-coverThe cover associated with the LL/LR-automaton justdefined, is a simple variant of the cover of section 5.3.2:S - -  s -ffi reduce(s, I ?
)t -* 8y =-- t E gotox(s,g)t - .
y - 3 ,0  ~ ao~oz(s, y))t - sP,, - ~ ~ goto, O ,z  ?
)t - -  I ?
= 3,(t E goto2(s, I?
))- .
s - reduce(s, I?
),As it is of the tadpole type, the predict mechanism worksas explained in section 7.We just mentioned that each LL/LR-state, and henceeach non-terminal of the LL/LR-cover, is completelycharacterized by one non-terminal, or 'item', of theEarley cover.
This correspondence b tween their non-terminals leads to a tight connection between the twocovers.
Indeed, the cover we obtained from the LL/LR-automaton can be obtained from the cover of section4, by eliminating the e-rules-I~  --~ e. Of course, thepredict functions associated to both covers differ consid-erably, as it axe the non-terminals deriving e, the itemsbeginning with a dot, that axe the object of predictionin the Earley algorithm, and they axe no longer presentin the LL/LR-cover.9 E f f i c iencyWe have discussed a number of bilinear covers now, andwe could add many more.
In fact, the space of bilinearcovers for each context-free grammar, or RTN grammar,is huge.
The optimal one would be the one that makesC-parser spend the least time on the average sentence.In general, the least time will be more or less equivalentto the smallest content of the parse matrix.
Naively,this content would be proportional to the size of thecover.
Under this assumption, the smallest cover wouldbe optimal.
Note that the number of non-terminals ofthe CNLR cover is equal to the number of states of theLR-antomaton plus the number of non-terminals of theoriginal grammar.
The size of the Earley cover is givenby the number of items.
In worst case situations the sizeof the CNLR cover is an exponential function of the sizeof the original grammar, whereas the size of the Ea~leycover dearly grows linearly with the size of the originalgrammar.
For many grammars, however, the numberof LR(0)-states, may be considerably smaller than thenumber of items.
This seems to be the case for the nat-ural language grammaxs considered by Tomita\[3\].
His141data even suggest hat the number of LR(0) states is asub-linear function of the original grammar size.
Note,however, that predict functions may influence the re-lation between grammar size and average parse matrixcontent, as some grammars may allow more restrictivepredict functions then others.
Summarizing, it seemsunlikely, that a single parsing approach would be opti-mal for all grammars.
A viable goal of research wouldbe to find methods for determining the optimal coverfor a given grammar.
Such research should have a solidexperimental back-bone.The matter gets still more complicated when the orig-inal grammar is an attribute grammar.
Attribute evalu-ation may lead to the rejection of certain parse trees thatare correct for the grammar without attributes.
Thenthe ease and efficiency of on-the-fly attribute evalua-tion becomes important, in order to stop wrong parsesas soon as possible.
In the Rosetta machine transla-tion system \[11,12\], we use an attributed RTN duringthe analysis of sentences.
The attribute evaluation isbottom-up only, and designed in such a way that thegrammar is covered by an attributed Earley cover.Other points concerning efficiency that we would liketo discuss, are issues of precomputation.
In the con-ventional Earley parser, the calculation of the cover isdone dynamically, while parsing a sentence.
However, itcould just as well be done statically, i.e.
before parsing,in order to increase parsing performance.
For instance,set operations can be implemented more efficiently if theset elements are known non-terminals, rather than un-known items, although this would depend on the choiceof programming language.
The procedure of generatingbilinear covers from LR-antomata should always be per-formed statically, because of the amount of computationinvolved.
Tomita has reported \[3\], that for a number ofgrammars, his parsing method turns out to be more efli-cient than the Earley ~gorithm.
It is not clear, whetherhis results would still hold if the creation of the coverfor the Earley parser were being done statically.Onedmight be inclined to think that if use is madeof precomputed sets of items, as in LR-parsers, one isbound to have a parser that is significantly different fromand probably faster than Earley's algorithm, which com-putes these sets at parse time.
The question is muchmore subtle as we showed in this paper.
On the onehand, non-deterministic LR-parsing comes down to theuse of certain covers for the grammar at hand, just likethe Earley algorithm.
Reversely, we showed that theEarley cover can, with minor modifications, be obtainedfrom the LL/LR-automaton, which also uses precom-puted sets of items.10 ConclusionsWe studied parsing of general context-free languages, bysplitting the process into two parts.
Firstly, the gram-mar is turned into bilinear grammar format, and sub-sequently a general parser for bilinear grammars is ap-plied.
Our view on the relation between parsers andcovers is similar to the work on covers of Nijholt \[7\] forgrammars that are deterministically parsable.We established that the Lung algorithm for simulat-ing pushdown automata, hides a prescription for deriv-ing bilinear covers from automata that satisfy certainconstraints.
Reversely, the LR-parser construction tech-nique has been presented as a way to derive automatafrom certain bilinear grammars.We found that the Earley algorithm is intimately re-lated to an automaton that simulates non-deterministicLL-parsing and, furthermore, that non-deterministicLR-automata provide general parsers for context-freegrammars, with the same complexity as the Earley al-gorithm.
It should be noted, however, that there are asmany parsers with this property, as there are ways toobtain bilinear covers for a given grammar.References1 Earley, J.
1970.
An Efficient Context-Free ParsingAlgorithm, Communication8 ACM 13(2):94-102.2 Lang, B.
1974.
Deterministic Techniques for EfficientNon-deterministic Parsers, Springer Lecture Notesin Computer Science 14:255-269.3 Tomita, M. 1986.
Efficient Parsing for Natural Lan-guage, Kluwer Academic Publishers.4 Graham, S.L., M.A.
Harrison and W.L.
Ruzzo 1980.An improved context-free recognizer, ACM trans.actions on Progr.
Languages and Systems 2:415-462.5 Aho, A.V.
and J.D.
Ullman 1972.
The theory of pars-ing, translation, and compiling, Prentice Hall Inc.Englewood Cliffs N.J.6 Kruseman Aretz, F.E.J.
1989.
A new approach toEarley's parsing algorithm, Science of ComputerProgramming volume 12..T Nijholt, A.
1980.
Context-free Grammars: Cov-ers, Normal Forms, and Parsing, Springer LectureNotes in Computer Science 93.8 Woods, W.A.
1970.
Transition etwork grammars fornatural anguage analysis, Commun.
ACM 13:591-602.9 Purdom, P.W.
and C.A.
Brown 1981.
Parsing ex-tended LR(k) grammars, Acta \[n\]ormatica 15:115-127.10 Nagata, I and M. Sama 1986.
Generation of EfficientLALR Parsers for Regular Right Part Grammars,Acta In\]ormatica 23:149-162.11 Leermakers, R. and J. Rons 1986.
The Transla-tion Method of Rosetta, Computers and Transla-tion 1:169-183.12 Appelo L., C Fellinger and J. Landsbergen 1987.Subgrammars, Rule Classes and Control in theRosetta Translation System, Proceedings o/ 3rdConference ACL, European Chapter, Copenhagen118-133.142
