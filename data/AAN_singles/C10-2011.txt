Coling 2010: Poster Volume, pages 90?98,Beijing, August 2010Automatic Acquisition of Lexical FormalityJulian Brooke, Tong Wang, and Graeme HirstDepartment of Computer ScienceUniversity of Toronto{jbrooke,tong,gh}@cs.toronto.eduAbstractThere has been relatively little work fo-cused on determining the formality levelof individual lexical items.
This studyapplies information from large mixed-genre corpora, demonstrating that signif-icant improvement is possible over simpleword-length metrics, particularly whenmultiple sources of information, i.e.
wordlength, word counts, and word associ-ation, are integrated.
Our best hybridsystem reaches 86% accuracy on an En-glish near-synonym formality identifica-tion task, and near perfect accuracy whencomparing words with extreme formalitydifferences.
We also test our word as-sociation method in Chinese, a languagewhere word length is not an appropriatemetric for formality.1 IntroductionThe derivation of lexical resources for use incomputational applications has been focused pri-marily on the denotational relationships amongwords, e.g.
the synonym and hyponym relation-ships encapsulated in WordNet (Fellbaum, 1998).Largely missing from popular lexical resourcessuch as WordNet and the General Inquirer (Stoneet al, 1966) is stylistic information; there are,for instance, no resources which provide com-prehensive information about the formality levelof words, which relates to the appropriatenessof a word in a given context.
Consider, forexample, the problem of choice among near-synonyms: there are only minor denotational dif-ferences among synonyms such as get, acquire,obtain, and snag, but it is difficult to construct asituation where any choice would be equally suit-able.
The key difference between these words istheir formality, with acquire the most formal andsnag the most informal.In this work, we conceive of formality asa continuous property.
This approach is in-spired by resources such as Choose The RightWord (Hayakawa, 1994), in which differences be-tween synonyms are generally described in rela-tive rather than absolute terms, as well as linguis-tic literature in which the quantification of stylis-tic differences among genres is framed in terms ofdimensions rather than discrete properties (Biber,1995).
We begin by defining the formality scorefor a word as a real number value in the range 1to ?1, with 1 representing an extremely formalword, and ?1 an extremely informal word.
Aformality lexicon, then, gives a FS score to everyword within its coverage.The core of our approach to the problem ofclassifying lexical formality is the automated cre-ation of formality lexicons from large corpora.
Inthis paper, we focus on the somewhat low-leveltask of identifying the relative formality of wordpairs; we believe, however, that a better under-standing of lexical formality is relevant to a num-ber of problems in computational linguistics, in-cluding sub-fields such as text generation, errorcorrection of (ESL) writing, machine translation,text classification, text simplification, word-sensedisambiguation, and sentiment analysis.
One con-clusion of our research is that formality variationis omnipresent in natural corpora, but it does notfollow that the identification of these differenceson the lexical level is a trivial one; nevertheless,90we are able to make significant progress using themethods presented here, in particular the applica-tion of latent semantic analysis to blog corpora.2 Related WorkAs far as we are aware, there are only a fewlines of research explicitly focused on the ques-tion of linguistic formality.
In linguistics proper,the study of register and genre usually involvesa number of dimensions or clines, sometimesexplicitly identified as formality (Leckie-Tarry,1995; Carter, 1998), or decomposed into notionssuch as informational versus interpersonal con-tent (Biber, 1995).
Heyligen and Dewaele (1998)provide a part-of-speech based quantification oftextual contextuality (which they argue is funda-mental to the notion of formality); their metrichas been used, for instance, in a computationalinvestigation of the formality of online encyclo-pedias (Emigh and Herring, 2005).
In this kindof quantification, however, there is little, if any,focus on individual elements of the lexicon.
Incomputational linguistics, formality has receivedattention in the context of text generation (Hovy,1990); of particular note relevant to our researchis the work of Inkpen and Hirst (2006), who de-rive boolean formality tags from Choose the RightWord (Hayakawa, 1994).
Like us, their focus wasimproved word choice, though the approach wasmuch broader, also including dimensions such aspolarity.
An intriguing example of formality rel-evant to text classification is the use of infor-mal language (slang) to help distinguish true newsfrom satire (Burfoot and Baldwin, 2009).Our approach to this task is inspired and in-formed by automatic lexical acquisition researchwithin the field of sentiment analysis (Turneyand Littman, 2003; Esuli and Sebastiani, 2006;Taboada and Voll, 2006; Rao and Ravichandra,2009).
Turney and Littman (2003) apply latentsemantic analysis (LSA) (Landauer and Dumais,1997) and pointwise mutual information (PMI) toderive semantic orientation ratings for words us-ing large corpora; like us, they found that LSAwas a powerful technique for deriving this lexicalinformation.
The lexical database SentiWordNet(Esuli and Sebastiani, 2006) provides 0?1 rank-ings for positive, negative, and neutral polarity,derived automatically using relationships betweenwords in WordNet (Fellbaum, 1998).
Unfortu-nately, WordNet synsets tend to cut across the for-mal/informal distinction, and so the resource isnot obviously useful for our task.The work presented here builds directly on a pi-lot study (Brooke et al, 2010), the focus of whichwas the construction of formality score (FS) lex-icons.
In that work, we employed less sophis-ticated forms of some of the methods used herein a relatively small dataset (the Brown Corpus),providing a proof of concept, but with poor cov-erage, and with no attempt to combine the meth-ods to maximize performance.
However, the smalldataset alowed us to do a thorough test of certainoptions associated with our task.
In particular wefound that using a similarity metric based on LSAgave good performance across our test sets, es-pecially when the term-document matrix was bi-nary (unweighted), the k-value used for LSA wassmall, and the method used to derive a formalityscore was cosine similarity to our seed terms.
Ametric using total word counts in corpora with di-vergent formality also showed promise, with bothmethods performing above our word-length base-line for words within their coverage.
PMI, bycomparison, proved less effective, and we do notpursue it further here.3 Data and Resources3.1 Word ListsAll the word lists discussed here are publiclyavailable.1 We begin with two, one formal andone informal, that we use both as seeds for ourlexicon construction methods and as test sets forevaluation (our gold standard).
We assume thatall slang terms are by their very nature informaland so our 138 informal seeds were taken primar-ily from an online slang dictionary2 (e.g.
wuss,grubby) and also include some contractions andinterjections (e.g.
cuz, yikes).
The 105 formalseeds were selected from a list of discourse mark-ers (e.g.
moreover, hence) and adverbs from a sen-timent lexicon (e.g.
preposterously, inscrutably);these sources were chosen to avoid words with1 http://www.cs.toronto.edu/?jbrooke/FormalityLists.zip2 http://onlineslangdictionary.com/91overt topic, and to ensure that there was somebalance of sentiment across formal and informalseed sets.
Part of speech, however, is not balancedacross our seed sets.Another test set we use to evaluate our methodsis a collection of 399 pairs of near-synonyms fromChoose the Right Word (CTRW), a manual for as-sisting writers with synonym word choice; eachpair was either explicitly or implicitly comparedfor formality in the book.
Implicit comparison in-cluded statements such as this is the most formalof these words; in those cases, and more gener-ally, we avoided words appearing in more thanone comparison (there are no duplicate words inour CTRW set), as well as multiword expressionsand words whose formality is strongly ambigu-ous (i.e.
word-sense dependent).
An example ofthis last phenomenon is the word cool, which isused colloquially in the sense of good but moreformally as in the sense of cold.
Partly as a re-sult of this polysemy, which is clearly more com-mon among informal words, our pairs are biasedtoward the formal end of the spectrum; althoughthere are some informal comparisons, e.g.
belly-ache/whine, wisecrack/joke, more typical pairsinclude determine/ascertain and hefty/ponderous.Despite this imbalance, one obvious advantageof using near-synonyms in our evaluation is thatfactors other than linguistic formality (e.g.
topic,opinion) are less likely to influence performance.In general, the CTRW allows for a more objective,fine-grained evaluation of our methods, and is ori-ented towards our primary interest, near-synonymword choice.To test the performance of our unsupervisedmethod beyond English, one of the authors (a na-tive speaker of Mandarin Chinese) created twosets of Chinese two-character words, one formal,one informal, based on but not limited to thewords in the English sets.
The Chinese seeds in-clude 49 formal seeds and 43 informal seeds.3.2 CorporaOur corpora fall generally into three categories:formal (written) copora, informal (spoken) cor-pora, and mixed corpora.
The Brown Corpus(Francis and Kuc?era, 1982), our development cor-pus, is used here both as a formal and mixed cor-pus.
Although extremely small by modern cor-pus standards (only 1 million words), the BrownCorpus has the advantage of being compiled ex-plicitly to represent a range of American English,though it is all of the published, written variety.The Switchboard (SW) Corpus is a collection ofAmerican telephone conversations (Godfrey et al,1992), which contains roughly 2400 conversationswith over 2.6 million word tokens; we use it as aninformal counterpart to the Brown Corpus.
Likethe Brown Corpus, The British National Corpus(Burnard, 2000) is a manually-constructed mixed-genre corpus; it is, however, much larger (roughly100 million words).
It contains a written portion(90%), which we use as a formal corpus, and aspontaneous spoken portion (4.3%), which we useas an informal corpus.
Our other mixed corporaare two blog collections available to us: the first,which we call our development blog corpus (Dev-Blog) contains a total of over 900,000 Englishblogs, with 216 million tokens.3 The second is the?first tier?
English blogs included in the publiclyavailable ICSWM 2009 Spinn3r Dataset (Burtonet al, 2009), a total of about 1.3 billion word to-kens in 7.5 million documents.
For our investiga-tions in Chinese, we use the Chinese portion of theICSWM blogs, approximately 25.4 million char-acter tokens in 86,000 documents.4 Methods4.1 Simple Formality MeasuresThe simplest kind of formality measure is basedon word length, which is often used directly asan indicator of formality for applications such asgenre classification (Karlgren and Cutting, 1994).Here, we use logarithmic scaling to derive a FSscore based on word length.
Given a maximumword length L4 and a word w of length l, the for-mality score function, FS(w), is given by:FS(w) =?1+2 log llogL3These blogs were gathered by the University of TorontoBlogscope project (www.blogscope.net) over a week in May2008.4We use an upper bound of 28 characters, which isthe length of antidisestablishmentarianism, the prototypicallongest word in English; this value of L provides an appropri-ate formality/informality threshold, between 5- and 6-letterwords92For hyphenated terms, the length of each compo-nent is averaged.
Though this metric works rela-tively well for English, we note that it is problem-atic in a language with significant word aggluti-nation (e.g.
German) or without an alphabet (e.g.Chinese, see below).Another straightforward method is the assump-tion that Latinate prefixes and suffixes are indica-tors of formality in English (Kessler et al, 1997),i.e.
informal words will not have Latinate affixessuch as -ation and intra-.
Here, we simply assignwords that appear to have such a prefix or suffixan FS of 1, and all other words an FS of ?1.Our frequency methods derive FS from wordcounts in corpora.
Our first, naive approach as-sumes a single corpus, where either formal wordsare common and informal words are rare, or viceversa.
To smooth out the Zipfian distribution, weuse the frequency rank of words as exponentials;for a corpus with R frequency ranks, the FS for aword of rank r under the formal is rare assumptionis given by:FS(w) =?1+2 e(r?1)e(R?1)Under the informal is rare assumption:FS(w) = 1?2 e(r?1)e(R?1)We have previously shown that these methods arenot particularly effective on their own (Brooke etal., 2010), but we note that they provide usefulinformation for a hybrid system.A more sophisticated method is to use two cor-pora that are known to vary with respect to for-mality and use the relative appearance of words ineach corpus as the metric.
If word appears n timesin a (relatively) formal corpus and m times in aninformal corpus (and one of m, n is not zero), wederive:FS(w) =?1+2 nm?N +nHere, N is the ratio of the size (in tokens) of theinformal corpus (IC) to the formal corpus (FC).We need the constant N so that an imbalance inthe size of the corpora does not result in an equiv-alently skewed distribution of FS.4.2 Latent Semantic AnalysisNext, we turn to LSA, a technique for extractinginformation from a large corpus of texts by (dras-tically) reducing the dimensionality of a term?document matrix, i.e.
a matrix where the row vec-tors correspond to the appearance or (weighted)frequency of words in a set of texts.
In essence,LSA simplifies the variation of words across a col-lection of texts, exploiting document?documentcorrelation to produce information about the kmost important dimensions of variation (k < to-tal number of documents), which are generallythought to represent semantic concepts, i.e.
topic.The mathematical basis for this transformation issingular value decomposition5; for the details ofthe matrix transformations, we refer the reader tothe discussion of Turney and Littman (2003).
Thefactor k, the number of columns in the compactedmatrix, is an important variable in any applicationof LSA, one is generally determined by trial anderror (Turney and Littman, 2003).LSA is computationally intensive; in order toapply it to extremely large blog corpora, we needto filter the documents and terms before build-ing our term?document matrix.
We adopt thefollowing strategy: to limit the number of docu-ments in our term?document matrix, we first re-move documents less than 100 tokens in length,with the rationale that these documents provideless co-occurrence information.
Second, we re-move documents that either do not contain anytarget words (i.e.
one of our seeds or CTRW testwords), or contain only target words which areamong the most common 20 in the corpus; thesedocuments are less likely to provide us with use-ful information, and the very common target termswill be well represented regardless.
We furthershrink the set of terms by removing all hapaxlegomena; a single appearance in a corpus is notenough to provide reliable co-occurrence informa-tion, and roughly half the words in our blog cor-pora appear only once.
Finally, we remove sym-bols and all words which are not entirely lower5We use the implementation included in Matlab; we takethe rows of the decomposed U matrix weighted by the sin-gular values in ?
for our word vectors.
Using no weightsor ?
?1 generally resulted in worse performance, particularlywith the CTRW sets.93case; we are not interested, for instance, in num-bers, acronyms, and proper nouns.
We can esti-mate the effect this filtering has on performanceby testing it both ways in a development corpus.Once a k-dimensional vector for each relevantword is derived using LSA, a standard method isto use the cosine of the angle between a word vec-tor and the vectors of seed words to identify howsimilar the distribution of the word is to the distri-bution of the seeds.
To begin, each formal seed isassigned a FS value of 1, each informal seed a FSvalue of ?1, and then a raw seed similarity score(FS?)
is calculated for each word w:FS?
(w) = ?s?S,s6=wWs?FS(s)?
cos(?
(w,s))S is the set of all seeds.
Note that seed terms areexcluded from their own FS calculation, this isequivalent to leave-one-out cross-validation.
Wsis a weight that depends on whether s is a formalor informal seed, Wi (for informal seeds) is calcu-lated as:Wi = ?
f?F FS( f )|?i?I FS(i)|+?
f?F FS( f )and Wf (for formal seeds) is:Wf = |?i?I FS(i)||?i?I FS(i)|+?
f?F FS( f )Here, I is the set of all informal seeds, and F is theset of all formal seeds.
These weights have the ef-fect of countering any imbalance in the seed set,as formal and informal seeds ultimately have thesame (potential) influence on each word, regard-less of their count.
This weighting is necessary forthe iterative extension of this method discussed inthe next section.We calculate the final FS score as follows:FS(w) = FS?(w)?FS?
(r)NwThe word r is a reference term, a common func-tion word that has no formality.6 This has the ef-fect of countering any (moderate) bias that might6The particular choice of this word is relatively unimpor-tant; common function words all have essentially the sameLSA vectors because they appear at least once in nearly ev-ery document of any size.
For English, we chose r = and,and for Chinese, r = yinwei (because); there does not seemto be an obvious two-character, formality-neutral equivalentto and in Chinese.exist in the corpus; in the Brown Corpus, for in-stance, function words have positive formality be-fore this step, simply because formal words oc-curred more often in the corpus.
Nw is a normal-ization factor, eitherNw = maxwi?I?
|FS?(wi)?FS?
(r)|for all wi ?
I?
orNw = maxw f?F ?
|FS?
(w f )?FS?
(r)|for all w f ?
F ?.
I?
contains all words w such thatFS?(w)?FS?
(r) < 0, and F ?
contains all words wsuch that FS?(w)?FS?
(r) > 0.
This ensures thatthe resulting lexicon has terms exactly in the range1 to?1, with the reference word r at the midpoint.We also tested the LSA method in Chinese.The only major relevant difference between Chi-nese and English is word segmentation: Chinesedoes not have spaces between words.
To sidestepthis problem, we simply included all character bi-grams found in our corpus.
The drawback of thisapproach in the inclusion of a huge number ofnonsense ?words?
(1.3 million terms in just 86,000documents), however we are at least certain toidentify all instances of our seeds.4.3 Hybrid MethodsThere are a number of ways to leverage the infor-mation we derive from our basic methods.
Oneintriguing option is to use the basic FS measuresas the starting point for an iterative process usingthe LSA cosine similarity.
Under this paradigm,all words in the starting FS lexicon are potentialseed words; we choose a cutoff value for inclu-sion in the seed word set (e.g.
words which haveat least .5 or ?.5 FS), and then carry out the co-sine calculations, as above, to derive new FS val-ues (a new FS lexicon).
We can repeat this processas many times as required, with the idea that theconnections between various words (as reflectedin their LSA-derived vectors) will cause the sys-tem to converge towards the true FS values.A simple hybrid method that combines the twoword count models uses the ratio of word countsin two corpora to define the center of the FS spec-trum, but single corpus methods to define the ex-tremes.
Formally, if m and n (word counts for the94informal corpus IC and formal corpus FC, respec-tively) are both non-zero, then FS is given by:FS(w) =?0.5+ nm?N +nHowever, if n is zero, FS is given by:FS(w) =?1+0.5 e?rIC?1e?RIC?1where rIC is the frequency rank of the word in IC,and RIC is the total number of ranks in IC.
If m iszero, FS is given by:FS(w) = 1?0.5 e?rFC?1e?RFC?1where i is the rank of the word in IC, and RIC is thetotal number of frequency ranks in IC).
This func-tion is undefined in the case where m and n areboth zero.
Intuitively, this is a kind of backoff, re-lying on the idea that words of extreme formalityare rare even in a corpus of corresponding formal-ity, whereas words in the core vocabulary (Carter,1998), which are only moderately formal, will ap-pear in all kinds of corpora, and thus are amenableto the ratio method.Finally, we explore a number of ways to com-bine lexicons directly.
The motivation for thisis that the lexicons have different strengths andweaknesses, representing partially independentinformation.
An obvious method is an averag-ing or other linear combination of the scores, butwe also investigate vote-based methods (requiringagreement among n dictionaries).
Beyond thesesimple options, we test support vector machinesand naive Bayes classification using the WEKAsoftware suite (Witten and Frank, 2005), applying10-fold cross-validation using default WEKA set-tings for each classifier.
The features here are taskdependent (see Section 5); for the pairwise task,we use the difference between the FS value of thewords in each lexicon, rather than their individ-ual scores.
Finally, we can use the weights fromthe SVM model of the CTRW (pairwise) task tointerpolate an optimal formality lexicon.5 EvaluationWe evaluate our methods using the gold standardjudgments from the seed sets and CTRW wordpairs.
To differentiate the two, we continue to usethe term seed for the former; in this context, how-ever, these ?seed sets?
are being viewed as a testset (recall that our LSA method is equivalent toleave-one-out cross-validation).We derive the following measures: first, thecoverage (Cov.)
is the percentage of words in theset that are covered under the method.
The class-based accuracy (C-Acc.)
of our seed sets is thepercentage of covered words which are correctlyclassified as formal (FS > 0) or informal (FS <0).
The pair-based accuracy (P-Acc.)
is the resultof exhaustively pairing words in the two seed setsand testing their relative formality; that is, for allwi ?
I and w f ?
F , the percentage of wi/w f pairswhere FS(wi) < FS(w f ).
For the CTRW pairsthere are only two metrics, the coverage and thepair-based accuracy; since the CTRW pairs repre-sent relative formality of varying degrees, it is notpossible to calculate a class-based accuracy.The first section of Table 1 provides the re-sults for the basic methods in various corpora.The word length (1) and morphology-based (2)methods provide good coverage, but poor accu-racy, while the word count ratio methods (3?4) arefairly accurate, but suffer from low coverage.
TheLSA results in Table 1 are the best for each corpusacross the k values we tested.
When both cover-age and accuracy are considered, there is a clearbenefit associated with increasing the amount ofdata, though the difference between the Dev-Blogand ICWSM suggests diminishing returns.
Theperformance of the filtered Dev-Blog is actuallyslightly better than the unfiltered versions (thoughthere is a drop in coverage), suggesting that filter-ing is a good strategy.In our previous work (Brooke et al, 2010), wenoted that CTRW set performance in the Browndropped for k > 3, while performance on the seedset was mostly steady as k increased.
Figure 1shows the pairwise performance of each test setfor various corpora across various k. The resultshere are similar; all three corpora reach a CTRWmaximum at a relatively low k values (thoughhigher than Brown Corpus); however the seed setperformance in each corpus continues to improve(though marginally) as k increases, while CTRWperformance drops.
An explanation for this is that95Table 1: Seed coverage, class-based accuracy, pairwise accuracy, CTRW coverage, and pairwise accu-racy for various FS lexicons and hybrid methods (%).Seed set CTRW setMethod Cov.
C-Acc.
P-Acc.
Cov.
P-Acc.Simple(1) Word length 100 86.4 91.8 100 63.7(2) Latinate affix 100 74.5 46.3 100 32.6(3) Word count ratio, Brown and Switchboard 38.0 81.5 85.7 36.0 78.2(4) Word count ratio, BNC Written vs.
Spoken 60.9 89.2 97.3 38.8 74.3(5) LSA (k=3), Brown 51.0 87.1 94.2 59.6 73.9(6) LSA (k=10), BNC 94.7 83.0 98.3 96.5 69.4(7) LSA (k=20), Dev-Blog 100 91.4 96.8 99.0 80.5(8) LSA (k=20), Dev-Blog, filtered 99.0 92.1 97.0 97.7 80.5(9) LSA (k=20), ICWSM, filtered 100 93.0 98.4 99.7 81.9Hybrid(10) BNC ratio with backoff (4) 97.1 78.8 75.7 97.0 78.8(11) Combined ratio with backoff (3 + 4) 97.1 79.2 79.9 97.5 79.9(12) BNC weighted average (10,6), ratio 2:1 97.1 83.5 90.0 97.0 83.2(13) Blog weighted average (9,7), ratio 4:1 100 93.8 98.5 99.7 83.4(14) Voting, 3 agree (1, 6, 7, 9, 11) 92.6 99.1 99.9 87.0 91.6(15) Voting, 2 agree (1, 11, 13) 86.8 99.1 100 81.5 96.9(16) Voting, 2 agree (1, 12, 13) 87.7 98.6 100 82.7 97.3(17) SVM classifier (1, 2, 6, 7, 9, 11) 100 97.9 99.9 100 84.2(18) Naive Bayes classifier (1, 2, 6, 7, 9, 11) 100 97.5 99.8 100 83.9(19) SVM (Seed, class) weighted (1, 2, 6, 7, 9, 11) 100 98.4 99.8 100 80.5(20) SVM (CTRW) weighted (1, 6, 7, 9, 11) 100 93.0 99.0 100 86.0(21) Average (1, 6, 7, 9, 11) 100 95.9 99.5 100 84.5Figure 1: Seed and CTRW pairwise accuracy,LSA method for large corpora k, 10?
k ?
200.the seed terms represent extreme examples of for-mality; thus there are numerous semantic dimen-sions to distinguish them.
However, the CTRWset includes near-synonyms, many with only rel-atively subtle differences in formality; for thesepairs, it is important to focus on the core di-mensions relevant to formality, which are amongthe first discovered in a factor analysis of mixed-register texts (Biber, 1995).With regards to hybrid methods, we first brieflysummarize our testing with the iterative model,which included extensive experiments using ba-sic lexicons and the LSA vectors derived fromthe Brown Corpus, and some targeted testing withthe blog corpora (iteration on these corpora isextraordinarily time-consuming).
In general, wefound only that there were only small, inconsis-tent benefits to be gained from the iterative ap-96proach.
More generally, the intuition behind theiterative method, i.e.
that performance would in-crease with an drastic increase in the number ofseeds, was found to be flawed: in other testing,we found that we could randomly remove mostof the seeds without negatively affecting perfor-mance.
Even at relatively high k values, it seemsthat a few seeds are enough to calibrate the model.The ratio (with backoff) hybrid built from theBNC (10) provides CTRW performance that iscomparable the best LSA models, though perfor-mance in the seed sets is somewhat poor; supple-menting with word counts from the Brown Cor-pus and Switchboard Corpus provides a small im-provement (11).
The weighed hybrid dictionar-ies in (12,13) demonstrate that it is possible to ef-fectively combine lexicons built using two differ-ent methods on the same corpus (12) or the samemethod on different corpora (13); the former, inparticular, provides an impressive boost to CTRWaccuracy, indicating that word count and word as-sociation methods are partially independent.The remainder of Table 1 shows the best re-sults using voting, averaging, and weighting.
Thevoting results (14?16) indicate that it is possibleto sacrifice some coverage for very high accu-racy in both sets, including a near-perfect scorein the seed sets and significant gains in CTRWperformance.
In general, the best accuracy with-out a significant loss of coverage came from 2of 3 voting (15?16), using dictionaries that rep-resented our three basic sources of information(word length, word count, and word associa-tion).
The machine learning hybrids (17?18) alsodemonstrate a marked improvement over any sin-gle lexicon, though it is important to note thateach accuracy score here reflects a different task-specific model.
Hybrid FS lexicons built with theweights learned by the SVM models (19?20) pro-vide superior performance on the task correspond-ing to the model used, though the simple averag-ing of the best dictionaries (21) also provides goodperformance across all evaluation metrics.Finally, the LSA results for Chinese are mod-est but promising, given the relatively small scaleof our experiments: we saw a pairwise accuracy of82.2%, with 79.3% class-based accuracy (k = 10).We believe that the main reason for the generallylower performance in Chinese (as compared toEnglish) is the modest size of the corpus, thoughour simplistic character bigram term extractiontechnique may also play a role.
As mentioned,smaller seed sets do not seem to be an issue.
Inter-estingly, the class-based accuracy is 10.8% lowerif no reference word is used to calibrate the dividebetween formal and informal, suggesting a ratherbiased corpus (towards informality); in English,by comparison, the reference-word normalizationhad a slightly negative effect on the LSA results,though the effect mostly disappeared after hy-bridization.
The obvious next step is to integrate aChinese word segmenter, and use a larger corpus.We could also try word count methods, thoughfinding appropriate (balanced) resouces similar tothe BNC might be a challenge; (mixed) blog cor-pora, on the other hand, are easily collected.6 ConclusionIn this work, we have experimented with a numberof different methods and source corpora for deter-mining the formality level of lexical items, withthe implicit goal of distinguishing the formality ofnear-synonym pairs.
Our methods show markedimprovement over simple word-length metrics;when multiple sources of information, i.e.
wordlength, word counts, and word association, are in-tegrated, we are able to reach over 85% perfor-mance on the near-synonym task, and close to100% accuracy when comparing words with ex-treme formality differences; our voting methodsshow that even higher precision is possible.
Wehave also demonstrated that our LSA word associ-ation method can be applied to a language whereword length is not an appropriate metric of for-mality, though the results here are preliminary.Other potential future work includes addressing awider range of phenomena, for instance assign-ing formality scores to morphological elements,syntactic cues, and multi-word expressions, anddemonstrating that a formality lexicon can be use-fully applied to other NLP tasks.AcknowledgementsThis work was supported by Natural Sciences andEngineering Research Council of Canada.
Thanksto Paul Cook for his ICWSM corpus API.97ReferencesBiber, Douglas.
1995.
Dimensions of Register Vari-ation: A cross-linguistic comparison.
CambridgeUniversity Press.Brooke, Julian, Tong Wang, and Graeme Hirst.
2010.Inducing lexicons of formality from corpora.
InProceedings of the Language Resources and Eval-uation Conference (LREC ?10), Workshop on Meth-ods for the automatic acquisition of Language Re-sources and their evaluation methods.Burfoot, Clint and Timothy Baldwin.
2009.
Auto-matic satire detection: Are you having a laugh?
InProceedings of the Joint Conference of the 47th An-nual Meeting of the Association for ComputationslLinguistics and the 4th International Joint Confer-ence on Nautral Language Processing of the AsianFederation of Natural Language Processing (ACL-IJCNLP ?09), Short Papers, Singapore.Burnard, Lou.
2000.
User reference guide for BritishNational Corpus.
Technical report, Oxford Univer-sity.Burton, Kevin, Akshay Java, and Ian Soboroff.
2009.The ICWSM 2009 Spinn3r Dataset.
In Proceedingsof the Third Annual Conference on Weblogs and So-cial Media (ICWSM 2009), San Jose, CA.Carter, Ronald.
1998.
Vocabulary: applied linguisticperspectives.
Routledge, London.Emigh, William and Susan C. Herring.
2005.
Col-laborative authoring on the web: A genre analysisof online encyclopedias.
In Proceedings of the 38thAnnual Hawaii International Conference on SystemSciences (HICSS ?05).Esuli, Andrea and Fabrizio Sebastiani.
2006.
Senti-WordNet: A publicly available lexical resource foropinion mining.
In Proceedings of the 5th Interna-tion Conference on Language Resources and Eval-uation(LREC), Genova, Italy.Fellbaum, Christiane, editor.
1998.
WordNet: AnElectronic Lexical Database.
The MIT Press.Francis, Nelson and Henry Kuc?era.
1982.
FrequencyAnalysis of English Usage: Lexicon and Grammar.Houghton Mifflin, Boston.Godfrey, J.J., E.C.
Holliman, and J. McDaniel.
1992.Switchboard: telephone speech corpus for researchand development.
IEEE International Confer-ence on Acoustics, Speech, and Signal Processing,1:517?520.Hayakawa, S.I., editor.
1994.
Choose the Right Word.HarperCollins Publishers, second edition.
Revisedby Eugene Ehrlich.Heylighen, Francis and Jean-Marc Dewaele.
2002.Variation in the contextuality of language: An em-pirical measure.
Foundations of Science, 7(3):293?340.Hovy, Eduard H. 1990.
Pragmatics and natural lan-guage generation.
Artificial Intelligence, 43:153?197.Inkpen, Diana and Graeme Hirst.
2006.
Building andusing a lexical knowledge base of near-synonymdifferences.
Computational Linguistics, 32(2):223?262.Karlgren, Jussi and Douglas Cutting.
1994.
Recog-nizing text genres with simple metrics using dis-criminant analysis.
In Proceedings of the 15th Con-ference on Computational Linguistics, pages 1071?1075.Kessler, Brett, Geoffrey Nunberg, and HinrichSchu?tze.
1997.
Automatic detection of text genre.In Proceedings of the 35th Annual Meeting of theAssociation for Computational Linguistics, pages32?38.Landauer, Thomas K. and Susan Dumais.
1997.
A so-lution to Plato?s problem: The latent semantic anal-ysis theory of the acquisition, induction, and rep-resentation of knowledge.
Psychological Review,104:211?240.Leckie-Tarry, Helen.
1995.
Language Context: afunctional linguistic theory of register.
Pinter.Rao, Delip and Deepak Ravichandra.
2009.
Semi-supervised polarity lexicon induction.
In Pro-ceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Lin-gusitics, Athens, Greece.Stone, Philip J., Dexter C. Dunphy, Marshall S. Smith,and Daniel M. Ogilivie.
1966.
The General In-quirer: A Computer Approach to Content Analysis.MIT Press.Taboada, Maite and Kimberly Voll.
2006.
Methodsfor creating semantic orientation dictionaries.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation (LREC), Gen-ova, Italy.Turney, Peter and Michael Littman.
2003.
Measuringpraise and criticism: Inference of semantic orienta-tion from association.
ACM Transactions on Infor-mation Systems, 21:315?346.Witten, Ian H. and Eibe Frank.
2005.
Data Mining:Practical Machine Learning Tools and Techniques.Morgan Kaufmann, San Francisco.98
