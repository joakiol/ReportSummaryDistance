Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 146?154,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsThe Role of Implicit Argumentation in Nominal SRLMatt GerberDept.
of Computer ScienceMichigan State Universitygerberm2@msu.eduJoyce Y. ChaiDept.
of Computer ScienceMichigan State Universityjchai@cse.msu.eduAdam MeyersDept.
of Computer ScienceNew York Universitymeyers@cs.nyu.eduAbstractNominals frequently surface without overtlyexpressed arguments.
In order to measure thepotential benefit of nominal SRL for down-stream processes, such nominals must be ac-counted for.
In this paper, we show that astate-of-the-art nominal SRL system with anoverall argument F1 of 0.76 suffers a perfor-mance loss of more than 9% when nominalswith implicit arguments are included in theevaluation.
We then develop a system thattakes implicit argumentation into account, im-proving overall performance by nearly 5%.Our results indicate that the degree of implicitargumentation varies widely across nominals,making automated detection of implicit argu-mentation an important step for nominal SRL.1 IntroductionIn the past few years, a number of studies havefocused on verbal semantic role labeling (SRL).Driven by annotation resources such as FrameNet(Baker et al, 1998) and PropBank (Palmer et al,2005), many systems developed in these studieshave achieved argument F1 scores near 80% inlarge-scale evaluations such as the one reported byCarreras and Ma`rquez (2005).More recently, the automatic identification ofnominal argument structure has received increasedattention due to the release of the NomBank cor-pus (Meyers, 2007a).
NomBank annotates predicat-ing nouns in the same way that PropBank annotatespredicating verbs.
Consider the following exampleof the verbal predicate distribute from the PropBankcorpus:(1) Freeport-McMoRan Energy Partners will beliquidated and [Arg1 shares of the newcompany] [Predicate distributed] [Arg2 to thepartnership?s unitholders].The NomBank corpus contains a similar instance ofthe deverbal nominalization distribution:(2) Searle will give [Arg0 pharmacists] [Arg1brochures] [Arg1 on the use of prescriptiondrugs] for [Predicate distribution] [Location intheir stores].This instance demonstrates the annotation of split ar-guments (Arg1) and modifying adjuncts (Location),which are also annotated in PropBank.
In caseswhere a nominal has a verbal counterpart, the inter-pretation of argument positions Arg0-Arg5 is con-sistent between the two corpora.In addition to deverbal (i.e., event-based) nomi-nalizations, NomBank annotates a wide variety ofnouns that are not derived from verbs and do not de-note events.
An example is given below of the parti-tive noun percent:(3) Hallwood owns about 11 [Predicate %] [Arg1 ofIntegra].In this case, the noun phrase headed by the predicate% (i.e., ?about 11% of Integra?)
denotes a fractionalpart of the argument in position Arg1.Since NomBank?s release, a number of studieshave applied verbal SRL techniques to the task ofnominal SRL.
For example, Liu and Ng (2007) re-ported an argument F1 of 0.7283.
Although thisresult is encouraging, it does not take into accountnominals that surface without overt arguments.
Con-sider the following example:(4) The [Predicate distribution] represents [NPavailable cash flow] [PP from the partnership][PP between Aug. 1 and Oct. 31].146As in (2), distribution in (4) has a noun phrase andmultiple prepositional phrases in its environment,but not one of these constituents is an argument todistribution in (4); rather, any arguments are implic-itly supplied by the surrounding discourse.
As de-scribed by Meyers (2007a), instances such as (2) arecalled ?markable?
because they contain overt argu-ments, and instances such as (4) are called ?unmark-able?
because they do not.
In the NomBank corpus,only markable instances have been annotated.Previous evaluations (e.g., those by Jiang andNg (2006) and Liu and Ng (2007)) have been basedon markable instances, which constitute 57% of allinstances of nominals from the NomBank lexicon.In order to use nominal SRL systems for down-stream processing, it is important to develop andevaluate techniques that can handle markable as wellas unmarkable nominal instances.
To address thisissue, we investigate the role of implicit argumenta-tion for nominal SRL.
This is, in part, inspired by therecent CoNLL Shared Task (Surdeanu et al, 2008),which was the first evaluation of syntactic and se-mantic dependency parsing to include unmarkablenominals.
In this paper, we extend this task to con-stituent parsing with techniques and evaluations thatfocus specifically on implicit argumentation in nom-inals.We first present our NomBank SRL system,which improves the best reported argument F1 scorein the markable-only evaluation from 0.7283 to0.7630 using a single-stage classification approach.We show that this system, when applied to all nomi-nal instances, achieves an argument F1 score of only0.6895, a loss of more than 9%.
We then presenta model of implicit argumentation that reduces thisloss by 46%, resulting in an F1 score of 0.7235 onthe more complete evaluation task.
In our analyses,we find that SRL performance varies widely amongspecific classes of nominals, suggesting interestingdirections for future work.2 Related workNominal SRL is related to nominal relation interpre-tation as evaluated in SemEval (Girju et al, 2007).Both tasks identify semantic relations between ahead noun and other constituents; however, the tasksfocus on different relations.
Nominal SRL focusesprimarily on relations that hold between nominaliza-tions and their arguments, whereas the SemEval taskfocuses on a range of semantic relations, many ofwhich are not applicable to nominal argument struc-ture.Early work in identifying the argument struc-ture of deverbal nominalizations was primarily rule-based, using rule sets to associate syntactic con-stituents with semantic roles (Dahl et al, 1987;Hull and Gomez, 1996; Meyers et al, 1998).
La-pata (2000) developed a statistical model to classifymodifiers of deverbal nouns as underlying subjectsor underlying objects, where subject and object de-note the grammatical position of the modifier whenlinked to a verb.FrameNet and NomBank have facilitated machinelearning approaches to nominal argument struc-ture.
Gildea and Jurafsky (2002) presented an earlyFrameNet-based SRL system that targeted both ver-bal and nominal predicates.
Jiang and Ng (2006)and Liu and Ng (2007) have tested the hypothe-sis that methodologies and representations used inPropBank SRL (Pradhan et al, 2005) can be portedto the task of NomBank SRL.
These studies reportargument F1 scores of 0.6914 and 0.7283, respec-tively.
Both studies also investigated the use of fea-tures specific to the task of NomBank SRL, but ob-served only marginal performance gains.NomBank argument structure has also been usedin the recent CoNLL Shared Task on Joint Parsingof Syntactic and Semantic Dependencies (Surdeanuet al, 2008).
In this task, systems were required toidentify syntactic dependencies, verbal and nominalpredicates, and semantic dependencies (i.e., argu-ments) for the predicates.
For nominals, the best se-mantic F1 score was 0.7664 (Surdeanu et al, 2008);however this score is not directly comparable to theNomBank SRL results of Liu and Ng (2007) or theresults in this paper due to a focus on different as-pects of the problem (see the end of section 5.2 fordetails).3 NomBank SRLGiven a nominal predicate, an SRL system attemptsto assign surrounding spans of text to one of 23classes representing core arguments, adjunct argu-ments, and the null or non-argument.
Similarly to147verbal SRL, this task is traditionally formulated asa two-stage classification problem over nodes in thesyntactic parse tree of the sentence containing thepredicate.1 In the first stage, each parse tree node isassigned a binary label indicating whether or not itis an argument.
In the second stage, argument nodesare assigned one of the 22 non-null argument types.Spans of text subsumed by labeled parse tree nodesconstitute arguments of the predication.3.1 An improved NomBank SRL baselineTo investigate the effects of implicit argumenta-tion, we first developed a system based on previ-ous markable-only approaches.
Our system followsmany of the traditions above, but differs in the fol-lowing ways.
First, we replace the standard two-stage pipeline with a single-stage logistic regressionmodel2 that predicts arguments directly.
Second,we model incorporated arguments (i.e., predicatesthat are also arguments) with a simple maximumlikelihood model that predicts the most likely argu-ment label for a predicate based on counts from thetraining data.
Third, we use the following heuris-tics to resolve argument conflicts: (1) If two argu-ments overlap, the one with the higher probability iskept.
(2) If two non-overlapping arguments are ofthe same type, the one with the higher probabilityis kept unless the two nodes are siblings, in whichcase both are kept.
Heuristic (2) accounts for splitargument constructions.Our NomBank SRL system uses features that areselected with a greedy forward search strategy sim-ilar to the one used by Jiang and Ng (2006).
Thetop half of Table 2 (next page) lists the selected ar-gument features.3 We extracted training nodes fromsections 2-21 of NomBank, used section 24 for de-velopment and section 23 for testing.
All parsetrees were generated by Charniak?s re-ranking syn-tactic parser (Charniak and Johnson, 2005).
Follow-ing the evaluation methodology used by Jiang andNg (2006) and Liu and Ng (2007), we obtained sig-1The syntactic parse can be based on ground-truth annota-tion or derived automatically, depending on the evaluation.2We use LibLinear (Fan et al, 2008).3For features requiring the identification of support verbs,we use the annotations provided in NomBank.
Preliminary ex-periments show a small loss when using automatic support verbidentification.Dev.
F1 Testing F1Jiang and Ng (2006) 0.6677 0.6914Liu and Ng (2007) - 0.7283This paper 0.7454 0.7630Table 1: Markable-only NomBank SRL results for ar-gument prediction using automatically generated parsetrees.
The f-measure statistics were calculated by ag-gregating predictions across all classes.
?-?
indicatesthat the result was not reported.Markable-only All-token % lossP 0.7955 0.6577 -17.32R 0.7330 0.7247 -1.13F1 0.7630 0.6895 -9.63Table 3: Comparison of the markable-only and all-token evaluations of the baseline argument model.nificantly better results, as shown in Table 1 above.43.2 The effect of implicit nominal argumentsThe presence of implicit nominal argumentspresents challenges that are not taken into accountby the evaluation described above.
To assess the im-pact of implicit arguments, we evaluated our Nom-Bank SRL system over each token in the testingsection.
The system attempts argument identifica-tion for all singular and plural nouns that have atleast one annotated instance in the training portionof the NomBank corpus (morphological variationsincluded).Table 3 gives a comparison of the results from themarkable-only and all-token evaluations.
As can beseen, assuming that all known nouns take overt argu-ments results in a significant performance loss.
Thisloss is due primarily to a drop in precision caused byfalse positive argument predictions made for nomi-nals with implicit arguments.4 Accounting for implicit arguments innominal SRLA natural solution to the problem described aboveis to first distinguish nominals that bear overtarguments from those that do not.
We treat this4As noted by Carreras and Ma`rquez (2005), the discrepancybetween the development and testing results is likely due topoorer syntactic parsing performance on the development sec-tion.148Argumentfeatures# Description N S1 12 & parse tree path from n to pred2 Position of n relative to pred & parse tree path from n to pred *3 First word subsumed by n4 12 & position of n relative to pred5 12 & 146 Head word of n?s parent *7 Last word subsumed n8 n?s syntactic category & length of parse tree path from n to pred9 First word of n?s right sibling * *10 Production rule that expands the parent of pred11 Head word of the right-most NP in n if n is a PP *12 Stem of pred13 Parse tree path from n to the lowest common ancestor of n and pred14 Head word of n15 12 & n?s syntactic category16 Production rule that expands n?s parent * *17 Parse tree path from n to the nearest support verb *18 Last part of speech (POS) subsumed by n *19 Production rule that expands n?s left sibling *20 Head word of n, if the parent of n is a PP21 The POS of the head word of the right-most NP under n if n is a PP...
Features 22-31 are available upon request 0 3Nominalfeatures1 n?s ancestor subcategorization frames (ASF) (see section 4) *2 n?s word3 Syntactic category of n?s right sibling4 Parse tree paths from n to each support verb *5 Last word of n?s left sibling * *6 Parse tree path from n to previous nominal, with lexicalized source (see section 4) *7 Last word of n?s right sibling *8 Production rule that expands n?s left sibling * *9 Syntactic category of n *10 PropBank markability score (see section 4) *11 Parse tree path from n to previous nominal, with lexicalized source and destination *12 Whether or not n is followed by PP *13 Parse tree path from n to previous nominal, with lexicalized destination *14 Head word of n?s parent *15 Whether or not n surfaces before a passive verb * *16 First word of n?s left sibling *17 Parse tree path from n to closest support verb, with lexicalized destination *18 Whether or not n is a head *19 Head word of n?s right sibling20 Production rule that expands n?s parent * *21 Parse tree paths from n to all support verbs, with lexicalized destinations *22 First word of n?s right sibling * *23 Head word of n?s left sibling *24 If n is followed by a PP, the head of that PP?s object *25 Parse tree path from n to previous nominal *26 Token distance from n to previous nominal *27 Production rule that expands n?s grandparent *Table 2: Features, sorted by gain in selection algorithm.
& denotes concatenation.
The last two columns indicate(N)ew features (not used in Liu and Ng (2007)) and features (S)hared by the argument and nominal models.149as a binary classification task over token nodes.Once a nominal has been identified as bearingovert arguments, it is processed with the argumentidentification model developed in the previoussection.
To classify nominals, we use the featuresshown in the bottom half of Table 2, which wereselected with the same algorithm used for theargument classification model.
As shown by Table2, the sets of features selected for argument andnominal classification are quite different, and manyof the features used for nominal classification havenot been previously used.
Below, we briefly explaina few of these features.Ancestor subcategorization frames (ASF)As shown in Table 2, the most informative featureis ASF.
For a given token t, ASF is actually a setof sub-features, one for each parse tree node abovet.
Each sub-feature is indexed (i.e., named) by itsdistance from t. The value of an ASF sub-featureis the production rule that expands the correspond-ing node in the tree.
An ASF feature with twosub-features is depicted below for the token ?sale?
:VP: ASF2 = V P ?
V,NPV (made) NP: ASF1 = NP ?
Det,NDet (a) N (sale)Parse tree path lexicalization A lexicalized parsetree path is one in which surface tokens from thebeginning or end of the path are included in the path.This is a finer-grained version of the traditionalparse tree path that captures the joint behavior ofthe path and the tokens it connects.
For example,in the tree above, the path from ?sale?
to ?made?with a lexicalized source and destination would besale : N ?
NP ?
V P ?
V : made.
Lexicalizationincreases sparsity; however, it is often preferredby the feature selection algorithm, as shown in thebottom half of Table 2.PropBank markability score This feature isthe probability that the context (?
5 words) of a de-verbal nominal is generated by a unigram languagemodel trained over the PropBank argument wordsfor the corresponding verb.
Entities are normalizedPrecision Recall F1Baseline 0.5555 0.9784 0.7086MLE 0.6902 0.8903 0.7776LibLinear 0.8989 0.8927 0.8958Table 4: Evaluation results for identifying nominalswith explicit arguments.to their entity type using BBN?s IdentiFinder, andadverbs are normalized to their related adjective us-ing the ADJADV dictionary provided by NomBank.The normalization of adverbs is motivated by thefact that adverbial modifiers of verbs typically havea corresponding adjectival modifier for deverbalnominals.5 Evaluation resultsOur evaluation methodology reflects a practical sce-nario in which the nominal SRL system must pro-cess each token in a sentence.
The system can-not safely assume that each token bears overt argu-ments; rather, this decision must be made automat-ically.
In section 5.1, we present results for the au-tomatic identification of nominals with overt argu-ments.
Then, in section 5.2, we present results forthe combined task in which nominal classification isfollowed by argument identification.5.1 Nominal classificationFollowing standard practice, we train the nomi-nal classifier over NomBank sections 2-21 usingLibLinear and automatically generated syntacticparse trees.
The prediction threshold is set to thevalue that maximizes the nominal F1 score ondevelopment section (24), and the resulting modelis tested over section 23.
For comparison, weimplemented the following simple classifiers.Baseline nominal classifier Classifies a tokenas overtly bearing arguments if it is a singular orplural noun that is markable in the training data.As shown in Table 4, this classifier achieves nearlyperfect recall.5MLE nominal classifier Operates similarly to5Recall is less than 100% due to (1) part-of-speech errorsfrom the syntactic parser and (2) nominals that were not anno-tated in the training data but exist in the testing data.15000.010.020.030.040.050.060.070.080.090.10.05 0.1 0.15 0.2 0.25 0.3(0.25) 0.35 0.4 0.45 0.5 0.55 0.6(0.5) 0.65 0.7 0.75(0.75) 0.80.85 0.9 0.95 1Observed markable probability%of nominalinstances(a) Distribution of nominals.
Each interval on the x-axis denotes a set of nominals that are markable between (x?5)%and x% of the time in the training data.
The y-axis denotes the percentage of all nominal instances in TreeBank thatis occupied by nominals in the interval.
Quartiles are marked below the intervals.
For example, quartile 0.25 indicatesthat one quarter of all nominal instances are markable 35% of the time or less.00.10.20.30.40.50.60.70.80.910.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1Observed markable probabilityPredicatenominalF1BaselineLibLinear(b) Nominal classification performance with respect to thedistribution in Figure 1a.
The y-axis denotes the combinedF1 for nominals in the interval.00.10.20.30.40.50.60.70.80.910.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1Observed markable probabilityArgument F1BaselineMLELibLinear(c) All-token argument classification performance with re-spect to the distribution in Figure 1a.
The y-axis denotes thecombined F1 for nominals in the interval.Figure 1: Evaluation results with respect to the distribution of nominals in TreeBank.the baseline classifier, but also produces a scorefor the classification.
The value of the score isequal to the probability that the nominal bears overtarguments, as observed in the training data.
Aprediction threshold is imposed on this score asdetermined by the development data (t = 0.23).As shown by Table 4, this exchanges recall forprecision and leads to a significant increase in theoverall F1 score.The last row in Table 4 shows the results forthe LibLinear nominal classifier, which significantlyoutperforms the others, achieving balanced preci-sion and recall scores near 0.9.
In addition, it isable to recover from part-of-speech errors becauseit does not filter out non-noun instances; rather, itcombines part-of-speech information with other lex-ical and syntactic features to classify nominals.Interesting observations can be made by groupingnominals according to the probability with whichthey are markable in the corpus.
Figure 1a givesthe overall distribution of markable nominals in thetraining data.
As shown, 50% of nominal instancesare markable only 65% of the time or less, makingnominal classification an important first step.
Usingthis view of the data, Figure 1b presents the over-all F1 scores for the baseline and LibLinear nominal151classifiers.6 As expected, gains in nominal classi-fication diminish as nominals become more overtlyassociated with arguments.
Furthermore, nominalsthat are rarely markable (i.e., those in interval 0.05)remain problematic due to a lack of positive traininginstances and the unbalanced nature of the classifi-cation task.5.2 Combined nominal-argument classificationWe now turn to the task of combined nominal-argument classification.
In this task, systems mustfirst identify nominals that bear overt arguments.
Weevaluated three configurations based on the nominalclassifiers from the previous section.
Each config-uration uses the argument classification model fromsection 3.As shown in Table 3, overall argument classifi-cation F1 suffers a loss of more than 9% under theassumption that all known nouns bear overt argu-ments.
This corresponds precisely to using the base-line nominal classifier in the combined nominal-argument task.
The MLE nominal classifier is ableto reduce this loss by 25% to an F1 of 0.7080.
TheLibLinear nominal classifier reduces this loss by46%, resulting in an overall argument classificationF1 of 0.7235.
This improvement is the direct resultof filtering out nominal instances that do not bearovert arguments.Similarly to the nominal evaluation, we can viewargument classification performance with respect tothe probability that a nominal bears overt arguments.This is shown in Figure 1c for the three configura-tions.
The configuration using the MLE nominalclassifier obtains an argument F1 of zero for nom-inals below its prediction threshold.
Compared tothe baseline nominal classifier, the LibLinear clas-sifier achieves argument classification gains as largeas 150.94% (interval 0.05), with an average gain of52.87% for intervals 0.05 to 0.4.
As with nomi-nal classification, argument classification gains di-minish for nominals that express arguments moreovertly - we observe an average gain of only 2.15%for intervals 0.45 to 1.00.
One possible explana-tion for this is that the argument prediction modelhas substantially more training data for the nomi-nals in intervals 0.45 to 1.00.
Thus, even if the nom-6Baseline and MLE are identical above the MLE threshold.NominalsDeverbal Deverbal-like OtherBaseline 0.7975 0.6789 0.6757MLE 0.8298 0.7332 0.7486LibLinear 0.9261 0.8826 0.8905ArgumentsBaseline 0.7059 0.6738 0.7454MLE 0.7206 0.6641 0.7675LibLinear 0.7282 0.7178 0.7847Table 5: Nominal and argument F1 scores for dever-bal, deverbal-like, and other nominals in the all-tokenevaluation.inal classifier makes a false positive prediction in the0.45 to 1.00 interval range, the argument model maycorrectly avoid labeling any arguments.As noted in section 2, these results are not di-rectly comparable to the results of the recent CoNLLShared Task (Surdeanu et al, 2008).
This is due tothe fact that the semantic labeled F1 in the SharedTask combines predicate and argument predictionsinto a single score.
The same combined F1 score forour best two-stage nominal SRL system (logistic re-gression nominal and argument models) is 0.7806;however, this result is not precisely comparable be-cause we do not identify the predicate role set as re-quired by the CoNLL Shared Task.5.3 NomLex-based analysis of resultsAs demonstrated in section 1, NomBank annotatesmany classes of deverbal and non-deverbal nomi-nals, which have been categorized on syntactic andsemantic bases in NomLex-PLUS (Meyers, 2007b).To help understand what types of nominals are par-ticularly affected by implicit argumentation, we fur-ther analyzed performance with respect to theseclasses.Figure 2a shows the distribution of nominalsacross classes defined by the NomLex resource.
Asshown in Figure 2b, many of the most frequentclasses exhibit significant gains.
For example, theclassification of partitive nominals (13% of all nom-inal instances) with the LibLinear classifier resultsin gains of 55.45% and 33.72% over the baselineand MLE classifiers, respectively.
For the 5 mostcommon classes, which constitute 82% of all nomi-nals instances, we observe average gains of 27.47%and 19.30% over the baseline and MLE classifiers,15200.050.10.150.20.250.30.350.40.450.5nompartitivenomlikerelationalnomingattributeenvironmentabilitynomadjwork-of-artgroupnomadjlike jobshareeventtypeversionhallmarkable-nom fieldNomLex class%of nominalinstances(a) Distribution of nominals across the NomLex classes.
They-axis denotes the percentage of all nominal instances that isoccupied by nominals in the class.00.10.20.30.40.50.60.70.80.91nompartitivenomlikerelationalnomingattributeenvironmentabilitynomadjwork-of-artgroupnomadjlike jobshareeventtypeversionhallmarkable-nom fieldNomLex classPredicatenominalF1BaselineMLELibLinear(b) Nominal classification performance with respect to theNomLex classes in Figure 2a.
The y-axis denotes the com-bined F1 for nominals in the class.Figure 2: Evaluation results with respect to NomLex classes.respectively.Table 5 separates nominal and argument classifi-cation results into sets of deverbal (NomLex classnom), deverbal-like (NomLex class nom-like), andall other nominalizations.
A deverbal-like nominalis closely related to some verb, although not mor-phologically.
For example, the noun accolade sharesargument interpretation with award, but the two arenot morphologically related.
As shown by Table 5,nominal classification tends to be easier - and ar-gument classification harder - for deverbals whencompared to other types of nominals.
The differ-ence in argument F1 between deverbal/deverbal-likenominals and the others is due primarily to relationalnominals, which are relatively easy to classify (Fig-ure 2b); additionally, relational nominals exhibit ahigh rate of argument incorporation, which is eas-ily handled by the maximum-likelihood model de-scribed in section 3.1.6 Conclusions and future workThe application of nominal SRL to practical NLPproblems requires a system that is able to accuratelyprocess each token it encounters.
Previously, it wasunclear whether the models proposed by Jiang andNg (2006) and Liu and Ng (2007) would operate ef-fectively in such an environment.
The systems de-scribed by Surdeanu et al (2008) are designed withthis environment in mind, but their evaluation didnot focus on the issue of implicit argumentation.These two problems motivate the work presented inthis paper.Our contribution is three-fold.
First, we improveupon previous nominal SRL results using a single-stage classifier with additional new features.
Sec-ond, we show that this model suffers a substantialperformance degradation when evaluated over nom-inals with implicit arguments.
Finally, we identify aset of features - many of them new - that can be usedto reliably detect nominals with explicit arguments,thus significantly increasing the performance of thenominal SRL system.Our results also suggest interesting directions forfuture work.
As described in section 5.2, many nom-inals do not have enough labeled training data toproduce accurate argument models.
The general-ization procedures developed by Gordon and Swan-son (2007) for PropBank SRL and Pado?
et al (2008)for NomBank SRL might alleviate this problem.Additionally, instead of ignoring nominals with im-plicit arguments, we would prefer to identify the im-plicit arguments using information contained in thesurrounding discourse.
Such inferences would helpconnect entities and events across sentences, provid-ing a fuller interpretation of the text.AcknowledgmentsThe authors would like to thank the anonymous re-viewers for their helpful suggestions.
The first twoauthors were supported by NSF grants IIS-0535112and IIS-0347548, and the third author was supportedby NSF grant IIS-0534700.153ReferencesCollin Baker, Charles Fillmore, and John Lowe.
1998.The Berkeley FrameNet project.
In Christian Boitetand Pete Whitelock, editors, Proceedings of the Thirty-Sixth Annual Meeting of the Association for Computa-tional Linguistics and Seventeenth International Con-ference on Computational Linguistics, pages 86?90,San Francisco, California.
Morgan Kaufmann Publish-ers.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introductionto the conll-2005 shared task: Semantic role labeling.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28:245?288.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.Semeval-2007 task 04: Classification of semantic re-lations between nominals.
In Proceedings of the 4thInternational Workshop on Semantic Evaluations.A.
Gordon and R. Swanson.
2007.
Generalizing seman-tic role annotations across syntactically similar verbs.In Proceedings of ACL, pages 192?199.Z.
Jiang and H. Ng.
2006.
Semantic role labeling ofnombank: A maximum entropy approach.
In Proceed-ings of the 2006 Conference on Empirical Methods inNatural Language Processing.Maria Lapata.
2000.
The automatic interpretationof nominalizations.
In Proceedings of the Seven-teenth National Conference on Artificial Intelligenceand Twelfth Conference on Innovative Applications ofArtificial Intelligence, pages 716?721.
AAAI Press /The MIT Press.Chang Liu and Hwee Ng.
2007.
Learning predictivestructures for semantic role labeling of nombank.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 208?215,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Adam Meyers.
2007a.
Annotation guidelines for nom-bank - noun argument structure for propbank.
Techni-cal report, New York University.Adam Meyers.
2007b.
Those other nombank dictionar-ies.
Technical report, New York University.Sebastian Pado?, Marco Pennacchiotti, and CarolineSporleder.
2008.
Semantic role assignment for eventnominalisations by leveraging verbal data.
In Pro-ceedings of the 22nd International Conference onComputational Linguistics (Coling 2008), pages 665?672, Manchester, UK, August.
Coling 2008 Organiz-ing Committee.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106.Sameer Pradhan, Wayne Ward, and James H. Martin.2005.
Towards robust semantic role labeling.
In Asso-ciation for Computational Linguistics.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The CoNLL2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In CoNLL 2008: Proceedingsof the Twelfth Conference on Computational Natu-ral Language Learning, pages 159?177, Manchester,England, August.
Coling 2008 Organizing Committee.154
