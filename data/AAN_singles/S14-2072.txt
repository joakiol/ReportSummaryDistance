Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 416?423,Dublin, Ireland, August 23-24, 2014.Meerkat Mafia: Multilingual and Cross-LevelSemantic Textual Similarity SystemsAbhay Kashyap, Lushan Han, Roberto Yus, Jennifer Sleeman,Taneeya Satyapanich, Sunil Gandhi and Tim FininUniversity of Maryland, Baltimore CountyBaltimore, MD 21250 USA{abhay1,lushan1,ryus,jsleem1,taneeya1,sunilga1,finin}@umbc.eduAbstractWe describe UMBC?s systems developedfor the SemEval 2014 tasks on Multi-lingual Semantic Textual Similarity (Task10) and Cross-Level Semantic Similarity(Task 3).
Our best submission in theMultilingual task ranked second in bothEnglish and Spanish subtasks using anunsupervised approach.
Our best sys-tems for Cross-Level task ranked secondin Paragraph-Sentence and first in bothSentence-Phrase and Word-Sense subtask.The system ranked first for the Phrase-Word subtask but was not included in theofficial results due to a late submission.1 IntroductionWe describe the semantic text similarity systemswe developed for two of the SemEval tasks for the2014 International Workshop on Semantic Evalu-ation.
We developed systems for task 3, Cross-Level Semantic Similarity (Jurgens et al., 2014),and task 10, Multilingual Semantic Textual Simi-larity (Agirre et al., 2014).
A key component inall the systems was an enhanced version of theword similarity system used in our entry (Han etal., 2013b) in the 2013 SemEval Semantic TextualSimilarity task.Our best system in the Multilingual SemanticTextual Similarity task used an unsupervised ap-proach and ranked second in both the English andSpanish subtasks.
In the Cross-Level SemanticSimilarity task we developed a number of new al-gorithms and used new linguistic data resources.In this task, our best systems ranked second inthe Paragraph-Sentence task, first in the Sentence-Phrase task and first in the Word-Sense task.
TheThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence de-tails:http://creativecommons.org/licenses/by/4.0/system ranked first for the Phrase-Word task butwas not included in the official results due to a latesubmission.The remainder of the paper proceeds as follows.Section 2 describes our word similarity model andit?s wrapper to deal with named entities and outof vocabulary words.
Sections 3 and 4 describehow we extended the word similarity model forthe specific tasks.
Section 5 presents the resultswe achieved on these tasks along with instanceswhere the system failed.
Section 6 highlights ourfuture plans for improving the system.2 Semantic Word Similarity Model2.1 LSA Word Similarity ModelOur word similarity model is a revised version ofthe one we used in the 2013 *SEM semantic textsimilarity task.
This was in turn derived froma system developed for the Graph of Relationsproject (UMBC, 2013b).
For SemEval, we wanteda measure that considered a word?s semantics butnot its lexical category, e.g., the verb ?marry?should be semantically similar to the noun ?wife?.An online demonstration of a similar model de-veloped for the GOR project is available (UMBC,2013a), but it lacks some of this version?s features.LSA-based word similarity.
LSA Word Simi-larity relies on the distributional hypothesis thatwords occurring in the same context tend to havesimilar meanings (Harris, 1968).
LSA relies on thefact that words that are semantically similar (e.g.,cat and feline or nurse and doctor) are more likelyto occur near one another in text.
Thus evidencefor word similarity can be computed from a statis-tical analysis of a large text corpus.We extracted raw word co-occurrence statis-tics from a portion of the 2007 crawl of the Webcorpus from the Stanford WebBase project (Stan-ford, 2001).
We processed the collection to re-move some undesirable elements (text duplica-416Word pair ?4 model ?1 model1 doctor NN, physician NN 0.775 0.7262 car NN, vehicle NN 0.748 0.8023 person NN, car NN 0.038 0.0244 car NN, country NN 0.000 0.0165 person NN, country NN 0.031 0.0696 child NN, marry VB 0.098 0.0007 wife NN, marry VB 0.548 0.2748 author NN, write VB 0.364 0.1289 doctor NN, hospital NN 0.473 0.34710 car NN, driver NN 0.497 0.281Table 1: Examples from the LSA similarity model.tions, truncated text, non-English text and strangecharacters) and produced a three billion word cor-pus of high quality English, which is available on-line (Han and Finin, 2013).We performed POS tagging and lemmatiza-tion on the corpus using the Stanford POS tag-ger (Toutanova et al., 2000).
Word/term co-occurrences were counted in a moving windowof a fixed size that scans the entire corpus.
Wegenerated two co-occurrence models using win-dow sizes ?1 and ?4 because we observed differ-ent natures of the models.
?1 window producesa context similar to the dependency context usedin (Lin, 1998).
It provides a more precise con-text but is only good for comparing words withinthe same POS.
This is because words of differentPOS are typically surrounded by words in differ-ent syntactic forms.
In contrast, a context windowof ?4 words allows us to compute semantic simi-larity between words with different POS.Examples from our LSA similarity model aregiven in Table 1.
Pairs 1 to 6 illustrate that themeasure has a good property of differentiatingsimilar words from non-similar words.
Examples7 and 8 show that the ?4 model can detect se-mantically similar words even with different POSwhile the ?1 model yields poor results.
The pairsin 9 and 10 show that highly related, but not sub-stitutable, words may have a strong similarity andthat the ?1 model is better at detecting them.Our word co-occurrence models were based ona predefined vocabulary of more than 22,000 com-mon English words and noun phrases.
We alsoadded to it more than 2,000 verb phrases extractedfrom WordNet.
The final dimensions of our wordco-occurrence matrices are 29,000?
29,000 whenwords are POS tagged.
Our vocabulary includesonly open-class words, i.e., nouns, verbs, adjec-tives and adverbs.
There are no proper nouns inthe vocabulary with the only exception of countrynames.Singular Value Decomposition (SVD) has beenfound to be effective in improving word similar-ity measures (Landauer and Dumais, 1997).
SVDis typically applied to a word by document matrix,yielding the familiar LSA technique.
In our case,we apply it to our word by word matrix (Burgess etal., 1998).
Before performing SVD, we transformthe raw word co-occurrence count fijto its log fre-quency log(fij+1).
We select the 300 largest sin-gular values and reduce the 29K word vectors to300 dimensions.
The LSA similarity between twowords is defined as the cosine similarity of theircorresponding word vectors after the SVD trans-formation.
See (Han et al., 2013b; Lushan Han,2014) for examples and more information on theLSA model.Statistical word similarity measures have limi-tations.
Related words can have similarity scoresas high as what similar words get, e.g., ?doctor?and ?hospital?.
Word similarity is typically lowfor synonyms that have many word senses sinceinformation about different senses are mashed to-gether (Han et al., 2013a).
To address these issues,we augment the similarity between two words us-ing knowledge from WordNet, for example, in-creasing the score if they are in the same WordNetsynset or if one is a direct or two link hypernymof the other.
See (Han et al., 2013b) for furtherdetails.2.2 Word Similarity WrapperOur word similarity model is restricted to the vo-cabulary size which only comprises open classwords.
For words outside of the vocabulary, wecan only rely on their lexical features and deter-mine equivalence (which we score as 0 or 1, sincea continuous scale makes little sense in this sce-nario).
An analysis of the previous STS datasetsshow that out-of-vocabulary words account forabout 25 ?
45% of the total words.
Datasets likeMSRpar and headlines lie on the higher end of thisspectrum due to the high volume of proper nouns.In the previous version, we computed a charac-ter bigram overlap score given bycharacterBigramScore =|A ?B||A ?B|where A and B are the set of bigrams from the firstand second word respectively.
We compare this417against a preset threshold (0.8) to determine equiv-alence.
While this is reasonable for named enti-ties, it is not the best approach for other classes.Named Entities.
The wrapper is extended tohandle all classes of named entities that are in-cluded in Stanford CoreNLP (Finkel et al., 2005).We use heuristic rules to compute the similaritybetween two numbers or two dates.
To handlenamed entity mentions of people, locations and or-ganizations, we supplement our character bigramoverlap method with the DBpedia Lookup service(Mendes et al., 2011).
For each entity mention, weselect the DBpedia entity with the most inlinks,which serves as a good estimate of popularity orsignificance (Syed et al., 2010).
If the two namedentity mentions map to identical DBpedia entities,we lower our character bigram overlap thresholdto 0.6.OOV words.
As mentioned earlier, when deal-ing with out-of-vocabulary words, we only haveits lexical features.
A straightforward approach isto simply get more context for the word.
Sinceour vocabulary is limited, we need to use externaldictionaries to find the word.
For our system, weuse Wordnik (Davidson, 2013), which is a compi-lation of several dictionaries including The Amer-ican Heritage Dictionary, Wikitionary and Word-Net.
Wordnik provides a REST API to access sev-eral attributes for a given word such as it?s defini-tions, examples, related words etc.
For out of vo-cabulary words, we simply retrieve the word pair?stop definitions and supply it to our existing STSsystem (UMBC, 2013a) to compute its similarity.As a fallback, in case the word is absent even inWordnik, we resort to our character bigram over-lap measure.3 Multilingual Semantic Text Similarity3.1 English STSFor the 2014 STS-English subtask we submittedthree runs.
They all used a simple term alignmentstrategy to compute sentence similarities.
The firstrun was an unsupervised approach that used thebasic word-similarity model for term alignment.The next two used a supervised approach to com-bine the scores from the first run with alignmentscores using the enhanced word-similarity wrap-per.
The two runs differed in their training.Align and Penalize Approach.
The pairingWordrun was produced by the same Align-and-Penalizesystem (Han et al., 2013b) that we used in the2013 STS task with only minor changes.
Thebiggest change is that we included a small listof disjoint concepts (Han et al., 2013b) that areused in the penalization phase, such as {piano, vi-olin} and {dog, cat}.
The disjoint concepts weremanually collected from the MSRvid dataset pro-vided by the 2012 STS task because we still lack areliable general method to automatically producethem.
The list only contains 23 pairs, which canbe downloaded at (dis, 2014).We also slightly adjusted our stopword list.We removed a few words that appear in the trialdatasets of 2013 STS task (e.g., frame) but we didnot add any new stopwords for this year?s task.
Allthe changes are small and we made them only inthe hope that they can slightly improve our system.Unlike machine learning methods that requiremanually selecting an appropriate trained modelfor a particular test dataset, our unsupervisedAlign-and-Penalize system is applied uniformlyto all six test datasets in 2014 STS task, namely,deft-forum, deft-news, headlines, images, OnWNand tweet-news.
It achieves the second best rankamong all submitted runs.Supervised Machine Learning.
Our second andthird runs used machine learning approaches sim-ilar to those we developed for the 2013 STS taskbut with significant changes in both pre-processingand the features extracted.The most significant pre-processing change wasthe use of Stanford coreNLP (Finkel et al., 2005)tool for tokenization, part-of-speech tagging andidentifying named entity mentions.
For the tweet-news dataset we also removed the hashtag symbol(?#?)
prior to applying the Stanford tools.
We useonly open class words and named entity mentionsand remove all other tokens.We align tokens between two sentences basedon the updated word similarity wrapper that wasdescribed in Section 2.2.
We use informationcontent from Google word frequencies for wordweights similar to our approach last year.
Thealignment process is a many-to-one mapping sim-ilar to the Align and Penalize approach and twotokens are only aligned if their similarity is greaterthan 0.1.
The sentence similarity score is thencomputed as the average of the scores of theiraligned tokens.
This score, along with the Alignand Penalize approach score, are used as featuresto train support vector regression (SVR) models.418We use an epsilon SVR with a radial basis kernelfunction and use a grid search to get the optimalparameter values for cost, gamma and epsilon.
Weuse datasets from the previous STS tasks as train-ing data and the two submitted runs differ in thechoice of their training data.The first approach, named Hulk, is an attemptto use a generic model trained on a large data set.The SVR model uses a total of 3750 sentence pairs(1500 from MSRvid, 1500 from MSRpar and 750from headlines) for training.
Datasets like SMTwere excluded due to poor quality.The second approach, named Super Saiyan,is an attempt at domain specific training.
ForOnWN, we used 1361 sentence pairs from previ-ous OnWN dataset.
For Images, we used 1500sentence pairs from MSRvid dataset.
The otherslacked any domain specific training data so weused a generic training dataset comprising 5111sentence pairs from MSRvid, MSRpar, headlinesand OnWN datasets.3.2 Spanish STSAs a base-line for this task we first consideredtranslating the Spanish sentences to English andrunning the same systems explained for the En-glish Subtask (i.e., pairingWord and Hulk).
Theresults obtained applying this approach to the pro-vided training data gave a correlation of 0.777 so,we selected this approach (with some improve-ments) for the competition.Translating the sentences.
For the automatictranslation of the sentences from Spanish to En-glish we used the Google Translate API1, afree, multilingual machine-translation product byGoogle.
Google Translate presents very accuratetranslations for European languages by using sta-tistical machine translation (Brown et al., 1990)where the translations are generated on the basis ofstatistical models derived from bilingual text cor-pora.
In fact, Google used as part of this corpora200 billion words from United Nations documentsthat are typically published in all six official UNlanguages, including English and Spanish.In the experiments performed with the trial datawe manually evaluated the quality of the trans-lations (one of the authors is a native Spanishspeaker).
The overall translation was very accu-rate but some statistical anomalies, incorrect trans-lations due to the abundance of a specific sense of1http://translate.google.comI1: Las costas o costa de un mar, lago o extenso r?
?o es latierra a lo largo del borde de estos.T11: Costs or the cost of a sea, lake or wide river is theland along the edge of these.T12: Coasts or the coast of a sea, lake or wide river is theland along the edge of these.T13: Coasts or the coast of a sea, lake or wide river is theland along the border of these....Figure 1: Three of the English translations for theSpanish sentence I1.a word in the training set, appeared.On one hand, some homonym words arewrongly translated.
For example, the Spanish sen-tence ?Las costas o costa de un mar [...]?
wastranslated to ?Costs or the cost of a sea [...]?.The Spanish word costa has two different senses:?coast?
(the shore of a sea or ocean) and ??cost?
(the property of having material worth).
On theother hand, some words are translated preservingtheir semantics but with a slightly different mean-ing.
For example, the Spanish sentence ?Un coj?
?nes una funda de tela [...]?
was correctly translatedto ?A cushion is a fabric cover [...]?.
However,the Spanish sentence ?Una almohada es un coj?
?nen forma rectangular [...]?
was translated to ?Apillow is a rectangular pad [...]?2.Dealing with statistical anomalies.
The afore-mentioned problem of statistical machine transla-tion caused a slightly adverse effect when comput-ing the similarity of two English (translated fromSpanish) sentences with the systems explained inSection 3.1.
Therefore, we improved the directtranslation approach by taking into account thedifferent possible translations for each word in aSpanish sentence.
For that, our system used the in-formation provided by the Google Translate API,that is, all the possible translations for every wordof the sentence along with a popularity value.
Foreach Spanish sentence the system generates all itspossible translations by combining the differentpossible translations of each word.
For example,Figure 1 shows three of the English sentences gen-erated for a given Spanish sentence from the trialdata.As a way of controlling the combinatorial ex-plosion of this step, especially for long sentences,we limited the maximum number of generated2Notice that both Spanish sentences used the term coj?
?nthat should be translated as cushion (the Spanish word forpad is almohadilla).419sentences for each Spanish sentence to 20 andwe only selected words with a popularity greaterthan 65.
We arrived at the popularity thresholdthrough experimentation on every sentence in thetrial data set.
After this filtering, our input forthe ?news?
and ?wikipedia?
tests went from 480and 324 pairs of sentences to 5756 and 1776 pairs,respectively.Given a pair of Spanish sentences, I1and I2, and the set of possible translationsgenerated by our system for each sentence,TI1= {T11, T12, T13, .
.
.
, T1n} and TI2={T21, T22, .
.
.
, T2m}, we compute the similaritybetween them by using the following formula:SimSPA(I1, I2) =n?i=1m?j=1SimENG(T1i, T2j)n ?mwhere SimENG(x, y) computes the similarity oftwo English sentences using our existing STS sys-tem (Han et al., 2013b).For the final competition we submitted threeruns.
The first (Pairing in Table 3) used thepairingWord system with the direct translation ofthe Spanish sentences to English.
The secondrun (PairingAvg in Table 3) used the formula forSimSPA(x, y) based on SimENG(x, y) withthe pairingWord system.
Finally, the third one(Hulk in Table 3) used the Hulk system with thedirect translation.4 Cross Level Similarity4.1 Sentence to Paragraph/PhraseWe used the three systems developed for the En-glish sentence similarity subtask and described inSection 3.1 for both the sentence to paragraph andsentence to phrase subtasks, producing three runs.The model for Hulk remained the same (trainedon 3750 sentence pairs from MSRvid, MSRparand headlines dataset) but the SuperSaiyan sys-tem, which is the domain specific approach, usedthe given train and trial text pairs (about 530) forthe respective subtasks as training to generate taskspecific models.4.2 Phrase to WordIn our initial experiments, we directly computedthe phrase-word pair similarity using our EnglishSTS.
This yielded a very low correlation of 0.239for the training set, primarily due to the absence ofthese phrases and words in our vocabulary.
To ad-dress this issue, we used external sources to obtainmore contextual information and extracted severalfeatures.Dictionary features.
We used Wordnik as a dic-tionary resource and retrieved definitions and us-age examples for the word.
We then used ourEnglish STS system to measure the similarity be-tween these and the given phrase to extract twofeatures.Web search features.
These features were basedon the hypothesis that if a word and phrase havesimilar meanings, then a web search that combinesthe word and phrase should return similar docu-ments when compared to a web search for eachindividually.We implemented this idea by comparing resultsof three search queries: the word alone, the phrasealone, and the word and phrase together.Using the Bing Search API (BIN, 2014), we re-trieved the top five results for each search, indexedthem with Lucene (Hatcher et al., 2004), and ex-tracted term frequency vectors for each of the threesearch result document sets.
For the phrase ?spillthe beans?
and word ?confess?, for example, webuilt a Lucene index for the set of documents re-trieved by a Bing search for ?spill the beans?, ?con-fess?, and ?spill the beans confess?.
We calculatedthe similarity of pairs of search result sets usingthe cosine similarity (1) of their term frequencyvectors.CosineSimilarity =n?i=1V 1i?
V 2i?n?i=1(V 1i)2?
?n?i=1(V 2i)2(1)We calculated the mean and minimum sim-ilarity of pairs of results for the phrase andphrase+word searches.
These features were ex-tracted from the provided training set and used inconjunction with the dictionary features to trainan SVM regression model to predict similarityscores.We observed this method can be problematicwhen a word or phrase has multiple meanings.For example, ?spill the beans?
relates to ?confess-ing?
but it is also the name of a coffee shop anda soup shop.
A mix of these pages do get re-turned by Bing and reduces the accuracy of our re-sults.
However, we found that this technique oftenstrengthens evidence of similarity enough that itimproves our overall accuracy when used in com-bination with our dictionary features.420Dante#n#1: an Italian poet famous for writing theDivine Comedy that describes a journey through Hell andpurgatory and paradise guided by Virgil and his idealizedBeatricewriter#n#1: writes books or stories or articles or the likeprofessionally for paygenerator#n#3: someone who originates or causes orinitiates something, ?he was the generator of severalcomplaints?author#v#1: be the author of, ?She authored this play?Figure 2: The WordNet sense for Dante#n#1 andthe three author#n senses.4.3 Word to SenseFor this subtask, we used external resources to re-trieve more contextual information.
For a givenword, we retrieved its synonym set from WordNetalong with their corresponding definitions.
We re-trieved the WordNet definition for the word senseas well.
For example, given a word-sense pair(author#n, Dante#n#1), we retrieved the synset ofauthor#n (writer.n.01, generator.n.03, author.v.01)along with their WordNet definitions and the sensedefinition of Dante#n#1.
Figure 2 shows theWordNet data for this example.By pairing every combination of the word?ssynset and their corresponding definitions with thesense?s surface form and definition, we createdfour features.
For each feature, we used our En-glish STS system to compare their semantic sim-ilarity and kept the maximum score as feature?svalue.We found that about 10% of the trainingdataset?s words fell outside of WordNet?s vocab-ulary.
Examples of missing words included manyinformal or ?slang?
words like kegger, crackberryand post-season.
To address this, we used Word-nik to retrieve the word?s top definition and com-puted its similarity with the sense.
This reducedthe out-of-vocabulary words to about 2% for thetraining data.
Wordnik thus gave us two addi-tional features: the maximum semantic similarityscore of word-sense using Wordnik?s additionaldefinitions for all words and for just the out-of-vocabulary words.
We used these features to trainan SVM regression model with the provided train-ing set to predict similarity scores.Dataset Pairing Hulk SuperSaiyandeft-forum 0.4711 (9) 0.4495 (15) 0.4918 (4)deft-news 0.7628 (8) 0.7850 (1) 0.7712 (3)headlines 0.7597 (8) 0.7571 (9) 0.7666 (2)images 0.8013 (7) 0.7896 (10) 0.7676 (18)OnWN 0.8745 (1) 0.7872 (18) 0.8022 (12)tweet-news 0.7793 (2) 0.7571 (7) 0.7651 (4)Weighted Mean 0.7605 (2) 0.7349 (6) 0.7410 (5)Table 2: Performance of our three systems on thesix English test sets.Dataset Pairing PairingAvg HulkWikipedia 0.6682 (12) 0.7431 (6) 0.7382 (8)News 0.7852 (12) 0.8454 (1) 0.8225 (6)Weighted Mean 0.7380 (13) 0.8042 (2) 0.7885 (5)Table 3: Performance of our three systems on thetwo Spanish test sets.5 ResultsMultilingual Semantic Text Similarity.
Table2 shows the system performance for the EnglishSTS task.
Our best performing system rankedsecond3, behind first place by only 0.0005.It employs an unsupervised approach with notraining data required.
The supervised systemsthat handled named entity recognition and out-of-vocabulary words performed slightly better ondatasets in the news domain but still suffered fromnoise due to diverse training datasets.Table 3 shows the performance for the Spanishsubtask.
The best run achieved a weighted correla-tion of 0.804, behind first place by only 0.003.
TheHulk system was similar to the Pairing run andused only one translation per sentence.
The per-formance boost could be attributed to large num-ber of named entities in the News and Wikipediadatasets.Cross Level Similarity.
Table 4 shows our per-formance in the Cross Level Similarity tasks.
TheParagraph-Sentence and Sentence-Phrase yieldedgood results (ranked second and first respectively)with our English STS system because of sufficientamount of textual information.
The correlationscores dropped as the granularity level of the textgot finer.The Phrase-Word run achieved a correlation of0.457, the highest for the subtask.
However, anincorrect file was submitted prior to the deadline3An incorrect file for ?deft-forum?
dataset was submitted.The correct version had a correlation of 0.4896 instead of0.4710.
This would have placed it at rank 1 overall.421Wordnik BingSim ScoreID S1 S2 Baseline Definitions Example Sim Avg Min SVM GS ErrorIdiomatic-212 spill the beans confess 0 0 0 0.0282 0.1516 0.1266 0.5998 4.0 3.4002Idiomatic-292 screw the pooch mess up 0 0.04553 0.0176 0.0873 0.4238 0.0687 0.7185 4.0 3.2815Idiomatic-273 on a shoogly peg insecure 0 0.0793 0 0.0846 0.3115 0.1412 0.8830 4.0 3.1170Slang-115 wacky tabaccy cannabis 0 0 0 0.0639 0.4960 0.1201 0.5490 4.0 3.4510Slang-26 pray to the porcelain god vomiting 0 0 0 0.0934 0.5275 0.0999 0.6452 4.0 3.3548Slang-79 rock and roll commence 0 0.2068 0.0720 0.0467 0.5106 0.0560 0.8820 4.0 3.1180Newswire-160 exercising rights under canon law lawyer 0.0044 0.6864 0.0046 0.3642 0.4990 0.2402 3.5562 0.5 3.0562Table 5: Examples where our algorithm performed poorly and the scores for individual features.Dataset Pairing Hulk SuperSaiyan WordExpandPara.-Sent.
0.794 (10) 0.826 (4) 0.834 (2)Sent.-Phrase 0.704 (14) 0.705 (13) 0.777 (1)Phrase-Word 0.457 (1)Word-Sense 0.389 (1)Table 4: Performance of our systems on the fourCross-Level Subtasks.Figure 3: Average error with respect to category.which meant that this was not included in the of-ficial results.
Figure 3 shows the average error(measured as the average deviation from the goldstandard) across different categories for phrase toword subtask.
Our performance is slightly worsefor slang and idiomatic categories when comparedto others which is due to two reasons: (i) the se-mantics of idioms is not compositional, reducingthe effectiveness of a distributional similarity mea-sure and (ii) dictionary-based features often failedto find definitions and/or examples of idioms.
Ta-ble 5 shows some of the words where our algo-rithm performed poorly and their scores for indi-vidual features.The Word-Sense run ranked first in the sub-task with a correlation score of 0.389.
Table 6shows some of the word-sense pairs where thesystem performed poorly.
Our system only usedWordnik?s top definition which was not always theright one to use to detect the similarity.
For ex-ample, the first definition of cheese#n is ?a solidfood prepared from the pressed curd of milk?
butthere is a latter, less prominent one, which isID word sense key sense number predicted gold80 cheese#n moolah%1:21:00:: moolah#n#1 0.78 4377 bone#n chalk%1:07:00:: chalk#n#2 1.52 4441 wasteoid#n drug user%1:18:00:: drug user#n#1 0.78 3Table 6: Examples where our system performedpoorly.?money?.
A second problem is that some words,like wasteoid#n, were absent even in Wordnik.Using additional online lexical resources to in-clude more slangs and idioms, like the Urban Dic-tionary (Urb, 2014), could address these issues.However, care must be taken since the quality ofsome content is questionable.
For example, theUrban Dictionary?s first definition of ?program-mer?
is ?An organism capable of converting caf-feine into code?.6 ConclusionWe described our submissions to the MultilingualSemantic Textual Similarity (Task 10) and Cross-Level Semantic Similarity (Task 3) tasks for the2014 International Workshop on Semantic Eval-uation.
Our best runs ranked second in both En-glish and Spanish subtasks for Task 10 while rank-ing first in Sentence-Phrase, Phrase-Word, Word-Sense tasks and second in Paragraph-Sentencesubtasks for Task 3.
Our success is attributed toa powerful word similarity model based on LSAword similarity and WordNet knowledge.
Weused new linguistic resources like Wordnik to im-prove our existing system for the Phrase-Word andWord-Sense tasks and plan to include other re-sources like ?Urban dictionary?
in the future.AcknowledgementsThis research was supported by awards 1228198,1250627 and 0910838 from the U.S. National Sci-ence Foundation.422ReferencesEneko Agirre, Carmen Banea, Claire Cardie, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, WeiweiGuo, Rada Mihalcea, German Rigau, and JanyceWiebe.
2014.
SemEval-2014 Task 10: Multilingualsemantic textual similarity.
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland.2014.
BING search API.
http://bing.com/developers-/s/APIBasics.html.Peter F Brown, John Cocke, Stephen A Della Pietra,Vincent J Della Pietra, Fredrick Jelinek, John D Laf-ferty, Robert L Mercer, and Paul S Roossin.
1990.A statistical approach to machine translation.
Com-putational linguistics, 16(2):79?85.Curt Burgess, Kay Livesay, and Kevin Lund.
1998.Explorations in context space: Words, sentences,discourse.
Discourse Processes, 25(2-3):211?257.Sara Davidson.
2013.
Wordnik.
The Charleston Advi-sor, 15(2):54?58.2014.
Disjoint concept pairs.
http://semanticweb-archive.cs.umbc.edu/disjointConcepts.txt.Jenny Rose Finkel, Trond Grenager, and Christo-pher D. Manning.
2005.
Incorporating non-localinformation into information extraction systems bygibbs sampling.
In 43rd Annual Meeting of the ACL,pages 363?370.Lushan Han and Tim Finin.
2013.
UMBC webbasecorpus.
http://ebiq.org/r/351.Lushan Han, Tim Finin, Paul McNamee, AnupamJoshi, and Yelena Yesha.
2013a.
Improving WordSimilarity by Augmenting PMI with Estimates ofWord Polysemy.
IEEE Trans.
on Knowledge andData Engineering, 25(6):1307?1322.Lushan Han, Abhay L. Kashyap, Tim Finin,James Mayfield, and Johnathan Weese.
2013b.UMBC EBIQUITY-CORE: Semantic TextualSimilarity Systems.
In 2nd Joint Conf.
on Lexicaland Computational Semantics.
ACL, June.Zellig Harris.
1968.
Mathematical Structures of Lan-guage.
Wiley, New York, USA.Erik Hatcher, Otis Gospodnetic, and Michael McCand-less.
2004.
Lucene in action.
Manning PublicationsGreenwich, CT.David Jurgens, Mohammad Taher Pilehvar, andRoberto Navigli.
2014.
SemEval-2014 Task 3:Cross-Level Semantic Similarity.
In Proceedings ofthe 8th International Workshop on Semantic Evalu-ation (SemEval-2014), Dublin, Ireland.Thomas K Landauer and Susan T Dumais.
1997.
Asolution to plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological review,104(2):211.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proc.
17th Int.
Conf.
on Compu-tational Linguistics, pages 768?774, Montreal, CN.Lushan Han.
2014.
Schema Free Querying of Seman-tic Data.
Ph.D. thesis, University of Maryland, Bal-timore County.Pablo N Mendes, Max Jakob, Andr?es Garc?
?a-Silva, andChristian Bizer.
2011.
Dbpedia spotlight: sheddinglight on the web of documents.
In 7th Int.
Conf.
onSemantic Systems, pages 1?8.
ACM.Stanford.
2001.
Stanford WebBase project.http://bit.ly/WebBase.Zareen Syed, Tim Finin, Varish Mulwad, and AnupamJoshi.
2010.
Exploiting a Web of Semantic Data forInterpreting Tables.
In Proceedings of the SecondWeb Science Conference, April.Kristina Toutanova, Dan Klein, Christopher Manning,William Morgan, Anna Rafferty, and Michel Gal-ley.
2000.
Stanford log-linear part-of-speech tagger.http://nlp.stanford.edu/software/tagger.shtml.UMBC.
2013a.
Semantic similarity demonstration.http://swoogle.umbc.edu/SimService/.UMBC.
2013b.
Umbc graph of relations project.http://ebiq.org/j/95.2014.
Urban dictionary.
http://urbandictionary.com/.423
