Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 181?189,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsAuthorship Attribution with Latent Dirichlet AllocationYanir Seroussi Ingrid ZukermanFaculty of Information Technology, Monash UniversityClayton, Victoria 3800, Australiafirstname.lastname@monash.eduFabian BohnertAbstractThe problem of authorship attribution ?
at-tributing texts to their original authors ?
hasbeen an active research area since the end ofthe 19th century, attracting increased interestin the last decade.
Most of the work on au-thorship attribution focuses on scenarios withonly a few candidate authors, but recently con-sidered cases with tens to thousands of can-didate authors were found to be much morechallenging.
In this paper, we propose waysof employing Latent Dirichlet Allocation inauthorship attribution.
We show that our ap-proach yields state-of-the-art performance forboth a few and many candidate authors, incases where these authors wrote enough textsto be modelled effectively.1 IntroductionThe problem of authorship attribution ?
attributingtexts to their original authors ?
has received con-siderable attention in the last decade (Juola, 2006;Stamatatos, 2009).
Most of the work in this field fo-cuses on cases where texts must be attributed to oneof a few candidate authors, e.g., (Mosteller and Wal-lace, 1964; Gamon, 2004).
Recently, researchershave turned their attention to scenarios with tens tothousands of candidate authors (Koppel et al, 2011).In this paper, we study authorship attribution withfew to many candidate authors, and introduce a newmethod that achieves state-of-the-art performance inthe latter case.Our approach to authorship attribution consists ofbuilding models of authors and their texts using La-tent Dirichlet Allocation (LDA) (Blei et al, 2003).We compare these models to models built from textswith unknown authors to find the most likely authorsof these texts (Section 3.2).
Our evaluation showsthat our approach yields a higher accuracy than themethod recently introduced by Koppel et al (2011)in several cases where prolific authors are consid-ered, while requiring less runtime (Section 4).This paper is structured as follows.
Related workis surveyed in Section 2.
Our LDA-based approachto authorship attribution is described in Section 3,together with the baselines we considered in ourevaluation.
Section 4 presents and discusses the re-sults of our evaluation, and Section 5 discusses ourconclusions and plans for future work.2 Related WorkThe field of authorship attribution predates moderncomputing.
For example, in the late 19th century,Mendenhall (1887) suggested that word length canbe used to distinguish works by different authors.
Inrecent years, increased interest in authorship attribu-tion was fuelled by advances in machine learning,information retrieval, and natural language process-ing (Juola, 2006; Stamatatos, 2009).Commonly used features in authorship attribu-tion range from ?shallow?
features, such as tokenand character n-gram frequencies, to features thatrequire deeper analysis, such as part-of-speech andrewrite rule frequencies (Stamatatos, 2009).
As inother text classification tasks, Support Vector Ma-chines (SVMs) have delivered high accuracy, asthey are designed to handle feature vectors of highdimensionality (Juola, 2006).
For example, one-vs.-all (OVA) is an effective approach to using bi-nary SVMs for multi-class (i.e., multi-author) prob-lems (Rifkin and Klautau, 2004).
Given A authors,181OVA trains A binary classifiers, where each classi-fier is trained on texts by one author as positive ex-amples and all the other texts as negative examples.However, ifA is large, each classifier has many morenegative than positive examples, often yielding poorresults due to class imbalance (Raskutti and Kowal-czyk, 2004).
Other setups, such as one-vs.-one ordirected acyclic graph, require training O(A2) clas-sifiers, making them impractical where thousands ofauthors exist.
Multi-class SVMs have also been sug-gested, but they generally perform comparably toOVA while taking longer to train (Rifkin and Klau-tau, 2004).
Hence, using SVMs for scenarios withmany candidate authors is problematic (Koppel etal., 2011).
Recent approaches to employing binarySVMs consider class similarity to improve perfor-mance (Bickerstaffe and Zukerman, 2010; Chenget al, 2007).
We leave experiments with such ap-proaches for future work (Section 5).In this paper, we focus on authorship attributionwith many candidate authors.
This problem was pre-viously addressed by Madigan et al (2005) and Luy-ckx and Daelemans (2008), who worked on datasetswith texts by 114 and 145 authors respectively.
Inboth cases, the reported results were much poorerthan those reported in the binary case.
More re-cently, Koppel et al (2011) considered author sim-ilarity to handle cases with thousands of candidateauthors.
Their method, which we use as our base-line, is described in Section 3.1.Our approach to authorship attribution utilises La-tent Dirichlet Allocation (LDA) (Blei et al, 2003)to build models of authors from their texts.
LDAis a generative probabilistic model that is tradition-ally used to find topics in textual data.
The mainidea behind LDA is that each document in a cor-pus is generated from a distribution of topics, andeach word in the document is generated accordingto the per-topic word distribution.
Blei et al (2003)showed that using LDA for dimensionality reductioncan improve performance for supervised text clas-sification.
We know of only one case where LDAwas used in authorship attribution: Rajkumar et al(2009) reported preliminary results on using LDAtopic distributions as feature vectors for SVMs, butthey did not compare the results obtained with LDA-based SVMs to those obtained with SVMs trainedon tokens directly.
Our comparison shows that bothmethods perform comparably (Section 4.3).Nonetheless, the main focus of our work ison authorship attribution with many candidate au-thors, where it is problematic to use SVMs.
OurLDA+Hellinger approach employs LDA withoutSVM training (Section 3.2), yielding state-of-the-artperformance in several scenarios (Section 4).3 Authorship Attribution MethodsThis section describes the authorship attributionmethods considered in this paper.
While all thesemethods can employ various representations of doc-uments, e.g., token frequencies or part-of-speech n-gram frequencies, we only experimented with tokenfrequencies.1 This is because they are simple to ex-tract, and can achieve good performance (Section 4).Further, the focus of this paper is on comparing theperformance of our methods to that of the baselinemethods.
Thus, we leave experiments on other fea-ture types for future work (Section 5).3.1 BaselinesWe consider two baseline methods, depending onwhether there are two or many candidate authors.If there are only two, we use Support Vector Ma-chines (SVMs), which have been shown to de-liver state-of-the-art performance on this task (Juola,2006).
If there are many, we follow Koppel etal.
?s (2011) approach, which we denote KOP.The main idea behind KOP is that different pairsof authors may be distinguished by different sub-sets of the feature space.
Hence, KOP randomlychooses k1 subsets of size k2F (k2 < 1) from a setof F features; for each of the k1 subsets, it calcu-lates the cosine similarity between a test documentand all the documents by one author (each author isrepresented by one feature vector); it then outputsthe author who had most of the top matches.
KOPalso includes a threshold ??
to handle cases wherea higher level of precision is required, at the costof lower recall.
If the top-matching author was thetop match less than ??
times, then KOP outputs ?un-known author?.
In our experiments we set ??
= 0 toobtain full coverage, as this makes it easier to inter-pret the results using a single measure of accuracy.1Token frequency is the token count divided by the totalnumber of tokens.1823.2 Authorship Attribution with LDAIn this work, we follow the extended LDA model de-fined by Griffiths and Steyvers (2004).
Under the as-sumptions of the extended model, given a corpus ofM documents, a document iwithN tokens is gener-ated by choosing a document topic distribution ?i ?Dir(?
), where Dir(?)
is a T -dimensional symmet-ric Dirichlet distribution, and ?
and T are parame-ters of the model.
Then, each token in the documentwij is generated by choosing a topic from the docu-ment topic distribution zij ?
Multinomial(?i), andchoosing a token from the token topic distributionwij ?
Multinomial(?zij ), where ?zij ?
Dir(?
), and?
is a parameter of the model.
The model can beinferred from the data using Gibbs sampling, as out-lined in (Griffiths and Steyvers, 2004) ?
an approachwe follow in our experiments.Note that the topics obtained by LDA do not haveto correspond to actual, human-interpretable topics.A more appropriate name may be ?latent factors?,but we adopt the convention of calling these fac-tors ?topics?
throughout this paper.
The meaning ofthe factors depends on the type of tokens that areused as input to the LDA inference process.
Forexample, if stopwords are removed from the cor-pus, the resulting factors often, but not necessarily,correspond to topics.
However, if only stopwordsare retained, as is commonly done in authorship at-tribution studies, the resulting factors lose their in-terpretability as topics; rather, they can be seen asstylistic markers.
Note that even if stopwords arediscarded, nothing forces the factors to stand for ac-tual topics.
Indeed, in a preliminary experiment on acorpus of movie reviews and message board posts,we found that some factors correspond to topics,with words such as ?noir?
and ?detective?
consid-ered to be highly probable for one topic.
However,other factors seemed to correspond to authorshipstyle as reflected by authors?
vocabulary, with net-speak words such as ?wanna?, ?alot?
and ?haha?
as-signed to one topic, and words such as ?compelling?and ?beautifully?
assigned to a different topic.We consider two ways of using LDA in authorshipattribution: (1) Topic SVM, and (2) LDA+Hellinger.The LDA part of both approaches consists of apply-ing a frequency filter to the features in the trainingdocuments,2 and then using LDA to reduce the di-mensionality of each document to a topic distribu-tion of dimensionality T .Topic SVM.
The topic distributions are used asfeatures for a binary SVM classifier that discrimi-nates between authors.
This approach has been em-ployed in the past for document classification, e.g.,in (Blei et al, 2003), but it has been applied to au-thorship attribution only in a limited study that con-sidered just stopwords (Rajkumar et al, 2009).
InSection 4.3, we present the results of more thoroughexperiments in applying this approach to binary au-thorship attribution.
Our results show that the per-formance of this approach is comparable to that ob-tained without using LDA.
This indicates that wedo not lose authorship-related information when em-ploying LDA, even though the dimensionality of thedocument representations is greatly reduced.LDA+Hellinger.
This method is our main contri-bution, as it achieves state-of-the-art performance inauthorship attribution with many candidate authors,where it is problematic to use SVMs (Section 2).The main idea of our approach is to use theHellinger distance between document topic distribu-tions to find the most likely author of a document:3D(?1, ?2) =?12?Tt=1(?
?1,t ??
?2,t)2where ?iis a T -dimensional multinomial topic distribution,and ?i,t is the probability of the t-th topic.We propose two representations of an author?sdocuments: multi-document and single-document.?
Multi-document (LDAH-M).
The LDA modelis built based on all the training documents.Given a test document, we measure theHellinger distance between its topic distribu-tion and the topic distributions of the trainingdocuments.
The author with the lowest meandistance for all of his/her documents is returnedas the most likely author of the test document.2We employed frequency filtering because it has been shownto be a scalable and effective feature selection method for au-thorship attribution tasks (Stamatatos, 2009).
We leave experi-ments with other feature selection methods for future work.3We considered other measures for comparing topic dis-tributions, including Kullback-Leibler divergence and Bhat-tacharyya distance.
From these measures, only Hellinger dis-tance satisfies all required properties of a distance metric.Hence, we used Hellinger distance.183?
Single-document (LDAH-S).
Each author?sdocuments are concatenated into a single doc-ument (the profile document), and the LDAmodel is learned from the profile documents.4Given a test document, the Hellinger distancebetween the topic distributions of the test docu-ment and all the profile documents is measured,and the author of the profile document with theshortest distance is returned.The time it takes to learn the LDA model de-pends on the number of Gibbs samples S, the num-ber of tokens in the training corpusW , and the num-ber of topics T .
For each Gibbs sample, the al-gorithm iterates through all the tokens in the cor-pus, and for each token it iterates through all thetopics.
Thus, the time complexity of learning themodel is O(SWT ).
Once the model is learned, in-ferring the topic distribution of a test document oflength N takes O(SNT ).
Therefore, the time ittakes to classify a document when using LDAH-SisO(SNT+AT ), whereA is the number of authors,and O(T ) is the time complexity of calculating theHellinger distance between two T -dimensional dis-tributions.
The time it takes to classify a docu-ment when using LDAH-M is O(SNT + MT ),where M is the total number of training documents,and M ?
A, because every candidate author haswritten at least one document.An advantage of LDAH-S over LDAH-M is thatLDAH-S requires much less time to classify a testdocument when many documents per author areavailable.
However, this improvement in runtimemay come at the price of accuracy, as authorshipmarkers that are present only in a few short doc-uments by one author may lose their prominenceif these documents are concatenated to longer doc-uments.
In our evaluation we found that LDAH-M outperforms LDAH-S when applied to one ofthe datasets (Section 4.3), while LDAH-S yieldsa higher accuracy when applied to the other twodatasets (Sections 4.4 and 4.5).
Hence, we presentthe results obtained with both variants.4Concatenating all the author documents into one documenthas been named the profile-based approach in previous studies,in contrast to the instance-based approach, where each docu-ment is considered separately (Stamatatos, 2009).4 EvaluationIn this section, we describe the experimental setupand datasets used in our experiments, followedby the evaluation of our methods.
We evaluateTopic SVM for binary authorship attribution, andLDA+Hellinger on a binary dataset, a dataset withtens of authors, and a dataset with thousands of au-thors.
Our results show that LDA+Hellinger yieldsa higher accuracy than Koppel et al?s (2011) base-line method in several cases where prolific authorsare considered, while requiring less runtime.4.1 Experimental SetupIn all the experiments, we perform ten-fold crossvalidation, employing stratified sampling where pos-sible.
The results are evaluated using classificationaccuracy, i.e., the percentage of test documents thatwere correctly assigned to their author.
Note thatwe use different accuracy ranges in the figures thatpresent our results for clarity of presentation.
Sta-tistically significant differences are reported whenp < 0.05 according to a paired two-tailed t-test.We used the LDA implementation from Ling-Pipe (alias-i.com/lingpipe) and the SVM im-plementation from Weka (www.cs.waikato.ac.nz/ml/weka).
Since our focus is on testing theimpact of LDA, we used a linear SVM kernel andthe default SVM settings.
For the LDA param-eters, we followed Griffiths and Steyvers (2004)and the recommendations in LingPipe?s documenta-tion, and set the Dirichlet hyperparameters to ?
=min(0.1, 50/T ) and ?
= 0.01, varying only thenumber of topics T .
We ran the Gibbs samplingprocess for S = 1000 iterations, and based the doc-ument representations on the last sample.
Whiletaking more than one sample is generally consid-ered good practice (Steyvers and Griffiths, 2007),we found that the impact of taking several sampleson accuracy is minimal, but it substantially increasesthe runtime.
Hence, we decided to use only one sam-ple in our experiments.4.2 DatasetsWe considered three datasets that cover differentwriting styles and settings: Judgement, IMDb62 andBlog.
Table 1 shows a summary of these datasets.The Judgement dataset contains judgements bythree judges who served on the Australian High184Judgement IMDb62 BlogAuthors 3 62 19,320Texts 1,342 62,000 678,161Texts perAuthorDixon: 902McTiernan: 253Rich: 1871,000Mean: 35.10Stddev.
: 104.99Table 1: Dataset StatisticsCourt from 1913 to 1975: Dixon, McTiernan andRich (available for download from www.csse.monash.edu.au/research/umnl/data).
Inthis paper, we considered the Dixon/McTiernan andthe Dixon/Rich binary classification cases, usingjudgements from non-overlapping periods (Dixon?s1929?1964 judgements, McTiernan?s 1965?1975,and Rich?s 1913?1928).
We removed numbers fromthe texts to ensure that dates could not be used to dis-criminate between judges.
We also removed quotesto ensure that the classifiers take into account onlythe actual author?s language use.5 Employing thisdataset in our experiments allows us to test our meth-ods on formal texts with a minimal amount of noise.The IMDb62 dataset contains 62,000 movie re-views by 62 prolific users of the Internet Moviedatabase (IMDb, www.imdb.com, available uponrequest from the authors of (Seroussi et al, 2010)).Each user wrote 1,000 reviews.
This dataset is nois-ier than the Judgement dataset, since it may con-tain spelling and grammatical errors, and the reviewsare not as professionally edited as judgements.
Thisdataset alows us to test our approach in a settingwhere all the texts have similar themes, and the num-ber of authors is relatively small, but is already muchlarger than the number of authors considered in tra-ditional authorship attribution settings.The Blog dataset is the largest dataset we consid-ered, containing 678,161 blog posts by 19,320 au-thors (Schler et al, 2006) (available for downloadfrom u.cs.biu.ac.il/?koppel).
In contrast toIMDb reviews, blog posts can be about any topic,but the large number of authors ensures that everytopic is likely to interest at least some authors.
Kop-pel et al (2011) used a different blog dataset con-sisting of 10,240 authors in their work on authorship5We removed numbers and quotes by matching regular ex-pressions for numbers and text in quotation marks, respectively.attribution with many candidate authors.
Unfortu-nately, their dataset is not publicly available.
How-ever, authorship attribution is more challenging onthe dataset we used, because they imposed some re-strictions on their dataset, such as setting a minimalnumber of words per author, and truncating the train-ing and testing texts so that they all have the samelength.
The dataset we use has no such restrictions.4.3 LDA in Binary Authorship AttributionIn this section, we present the results of our experi-ments with the Judgement dataset (Section 4.2), test-ing the use of LDA in producing feature vectors forSVMs and the performance of our LDA+Hellingermethods (Section 3.2).In all the experiments, we employed a classifierensemble to address the class imbalance problempresent in the Judgement dataset, which contains 5times more texts by Dixon than by Rich, and over 3times more texts by Dixon than by McTiernan (Ta-ble 1).
Dixon?s texts are randomly split into 5 or3 subsets, depending on the other author (Rich orMcTiernan respectively), and the base classifiers aretrained on each subset of Dixon?s texts together withall the texts by the other judge.
Given a text by anunknown author, the classifier outputs are combinedusing majority voting.
We found that the accuraciesobtained with an ensemble are higher than those ob-tained with a single classifier.
We did not requirethe vote to be unanimous, even though this increasesprecision, because we wanted to ensure full cover-age of the test dataset.
This enables us to comparedifferent methods using only an accuracy measure.6Experiment 1.
Figure 1 shows the results of anexperiment that compares the accuracy obtained us-ing SVMs with token frequencies as features (TokenSVMs) with that obtained using LDA topic distribu-tions as features (Topic SVMs).
We experimentedwith several filters on token frequency, and differ-ent numbers of LDA topics (5, 10, 25, 50, .
.
., 250).The x-axis labels describe the frequency filters: theminimum and maximum token frequencies, and theapproximate number of unique tokens left after fil-tering (in thousands).
We present only the resultsobtained with 10, 25, 100 and 200 topics, as the re-6For all our experiments, the results for the Dixon/McTier-nan case are comparable to those for Dixon/Rich.
Therefore,we omit the Dixon/McTiernan results to conserve space.185455055606570758085909510001E-59.205E-512.201E-41305E-413.801141E-55E-531E-51E-43.81E-55E-44.61E-514.85E-51E-40.75E-55E-41.55E-511.71E-45E-40.81E-4115E-410.2AccuracyToken Frequency FilterMinMaxTokens (K)Token SVMMajority Baseline10 Topic SVM25 Topic SVM100 Topic SVM200 Topic SVMFigure 1: LDA Features for SVMs in Binary AuthorshipAttribution (Judgement dataset, Dixon/Rich)sults obtained with other topic numbers are consis-tent with the presented results, and the results ob-tained with 225 and 250 topics are comparable tothe results obtained with 200 topics.Our results show that setting a maximum boundon token frequency filters out important authorshipmarkers, regardless of whether LDA is used ornot (performance drops).
This shows that it is un-likely that discriminative LDA topics correspond toactual topics, as the most frequent tokens are mostlynon-topical (e.g., punctuation and function words).An additional conclusion is that using LDA forfeature reduction yields results that are comparableto those obtained using tokens directly.
While TopicSVMs seem to perform slightly better than TokenSVMs, the differences between the best results ob-tained with the two approaches are not statisticallysignificant.
However, the number of features thatthe SVMs consider when topics are used is usuallymuch smaller than when tokens are used directly, es-pecially when no token filters are used (i.e., whenthe minimum frequency is 0 and the maximum fre-quency is 1).
This makes it easy to apply LDA to dif-ferent datasets, since the token filtering parametersmay be domain-dependent, and LDA yields good re-sults without filtering tokens.Experiment 2.
Figure 2 shows the results ofan experiment that compares the performance ofthe single profile document (LDAH-S) and multi-ple author documents (LDAH-M) variants of ourLDA+Hellinger approach to the results obtainedwith Token SVMs and Topic SVMs.
As in Exper-iment 1, we employ classifier ensembles, where the45505560657075808590951000  25  50  75  100  125  150  175  200AccuracyNumber of TopicsToken SVMMajority BaselineTopic SVMLDAH-SLDAH-MFigure 2: LDA+Hellinger in Binary Authorship Attribu-tion (Judgement dataset, Dixon/Rich)base classifiers are either SVMs or LDA+Hellingerclassifiers.
We did not filter tokens, since Experi-ment 1 indicates that filtering has no advantage overnot filtering tokens.
Instead, Figure 2 presents theaccuracy as a function of the number of topics.Note that we did not expect LDA+Hellinger tooutperform SVMs, since LDA+Hellinger does notconsider inter-class relationships.
Indeed, Figure 2shows that this is the case (the differences betweenthe best Topic SVM results and the best LDAH-M results are statistically significant).
However,LDA+Hellinger still delivers results that are muchbetter than the majority baseline (the differences be-tween LDA+Hellinger and the majority baseline arestatistically significant).
This leads us to hypothe-sise that LDA+Hellinger will perform well in caseswhere it is problematic to use SVMs due to the largenumber of candidate authors.
We verify this hypoth-esis in the following sections.One notable result is that LDAH-S delivers highaccuracy even when only a few topics are used,while LDAH-M requires about 50 topics to outper-form LDAH-S (all the differences between LDAH-Sand LDAH-M are statistically significant).
This maybe because there are only two authors, so LDAH-S builds the LDA model based only on two profiledocuments.
Hence, even 5 topics are enough to ob-tain two topic distributions that are sufficiently dif-ferent to discriminate the authors?
test documents.The reason LDAH-M outperforms LDAH-S whenmore topics are considered may be that some impor-tant authorship markers lose their prominence in theprofile documents created by LDAH-S.18601020304050607080901000  50  100  150  200  250  300  350  400AccuracyNumber of TopicsKOP: k1 = 400, k2 = 0.2LDAH-SLDAH-MFigure 3: LDA+Hellinger with Tens of Authors (IMDb62dataset)4.4 LDA+Hellinger with Tens of AuthorsIn this section, we apply our LDA+Hellinger ap-proaches to the IMDb62 dataset (Section 4.2), andcompare the obtained results to those obtained withKoppel et al?s (2011) method (KOP).
To this effect,we first established a KOP best-performance base-line by performing parameter tuning experiments forKOP.
Figure 3 shows the results of the comparisonof the accuracies obtained with our LDA+Hellingermethods to the best accuracy yielded by KOP (ob-tained in the parameter tuning experiment).For this experiment, we ran our LDA+Hellingervariants with 5, 10, 25, 50, .
.
., 300, 350 and 400topics.
The highest LDAH-M accuracy was ob-tained with 300 topics (Figure 3).
However, LDAH-S yielded a much higher accuracy than LDAH-M.This may be because the large number of trainingtexts per author (900) may be too noisy for LDAH-M. That is, the differences between individual textsby each author may be too large to yield a meaning-ful representation of the author if they are consideredseparately.
Finally, LDAH-S requires only 50 topicsto outperform KOP, and outperforms KOP by about15% for 150 topics.
All the differences between themethods are statistically significant.This experiment shows that LDAH-S models theauthors in IMDb62 more accurately than KOP.
Thelarge improvement in accuracy shows that the com-pact author representation employed by LDAH-S,which requires only 150 topics to obtain the highestaccuracy, has more power to discriminate betweenauthors than KOP?s much heavier representation, of400 subsets with more than 30,000 features each.
Inaddition, the per-fold runtime of the KOP baselinewas 93 hours, while LDAH-S required only 15 hoursper fold to obtain the highest accuracy.4.5 LDA+Hellinger with Thousands of AuthorsIn this section, we compare the performance of ourLDA+Hellinger variants to the performance of KOPon several subsets of the Blog dataset (Section 4.2).For this purpose, we split the dataset according tothe prolificness of the authors, i.e., we ordered theauthors by the number of blog posts, and consideredsubsets that contain all the posts by the 1000, 2000,5000 and 19320 most prolific authors.7 Due to thelarge number of posts, we could not run KOP formore than k1 = 10 iterations on the smallest subsetof the dataset and 5 iterations on the other subsets,as the runtime was prohibitive for more iterations.For example, 10 iterations on the smallest subset re-quired about 90 hours per fold (the LDA+Hellingerruntimes were substantially shorter, with maximumruntimes of 56 hours for LDAH-S and 77 hours forLDAH-M, when 200 topics were considered).
Inter-estingly, running KOP for 5 iterations on the largersubsets decreased performance compared to runningit for 1 iteration.
Thus, on the larger subsets, themost accurate KOP results took less time to obtainthan those of our LDA+Hellinger variants.Figure 4 shows the results of this experiment.For each author subset, it compares the results ob-tained by LDAH-S and LDAH-M to the best re-sult obtained by KOP.
All the differences betweenthe methods are statistically significant.
For up to2000 prolific authors (Figures 4(a), 4(b)), LDAH-Soutperforms KOP by up to 50%.
For 5000 prolificusers (figure omitted due to space limitations), themethods perform comparably, and KOP outperformsLDAH-S by a small margin.
However, with all theauthors (Figure 4(c)), KOP yields a higher accuracythan both LDA+Hellinger variants.
This may bebecause considering non-prolific authors introducesnoise that results in an LDA model that does not cap-ture the differences between authors.
However, it isencouraging that LDAH-S outperforms KOP whenless than 5000 prolific authors are considered.7These authors make up about 5%, 10%, 25% and exactly100% of the authors, but they wrote about 50%, 65%, 80% andexactly 100% of the texts, respectively.187024681012141618200  25  50  75  100  125  150  175  200AccuracyNumber of TopicsKOP: k1 = 10, k2 = 0.6LDAH-SLDAH-M(a) 1,000 Prolific Authors024681012141618200  25  50  75  100  125  150  175  200AccuracyNumber of TopicsKOP: k1 = 1, k2 = 1.0LDAH-SLDAH-M(b) 2,000 Prolific Authors024681012141618200  25  50  75  100  125  150  175  200AccuracyNumber of TopicsKOP: k1 = 1, k2 = 1.0LDAH-SLDAH-M(c) 19,320 (all) AuthorsFigure 4: LDA+Hellinger with Thousands of Authors (Blog dataset)The accuracies obtained in this section are ratherlow compared to those obtained in the previoussections.
This is not surprising, since the author-ship attribution problem is much more challengingwith thousands of candidate authors.
This chal-lenge motivated the introduction of the ??
thresh-old in KOP (Section 3.1).
Our LDA+Hellinger vari-ants can also be extended to include a threshold: ifthe Hellinger distance of the best-matching author isgreater than the threshold, the LDA+Hellinger algo-rithm would return ?unknown author?.
We leave ex-periments with this extension to future work, as ourfocus in this paper is on comparing LDA+Hellingerto KOP, and we believe that this comparison isclearer when no thresholds are used.5 Conclusions and Future WorkIn this paper, we introduced an approach to author-ship attribution that models texts and authors usingLatent Dirichlet Allocation (LDA), and considersthe distance between the LDA-based representationsof the training and test texts when classifying testtexts.
We showed that our approach yields state-of-the-art performance in terms of classification accu-racy when tens or a few thousand authors are consid-ered, and prolific authors exist in the training data.This accuracy improvement was achieved togetherwith a substantial reduction in runtime compared toKoppel et al?s (2011) baseline method.While we found that our approach performs wellon texts by prolific authors, there is still room forimprovement on authors who have not written manytexts ?
an issue that we will address in the future.One approach that may improve performance onsuch authors involves considering other types of fea-tures than tokens, such as parts of speech and char-acter n-grams.
Since our approach is based on LDA,it can easily employ different feature types, whichmakes this a straightforward extension to the workpresented in this paper.In the future, we also plan to explore ways of ex-tending LDA to model authors directly, rather thanusing it as a black box.
Authors were considered byRosen-Zvi et al (2004; 2010), who extended LDAto form an author-topic model.
However, this modelwas not used for authorship attribution, and wasmostly aimed at topic modelling of multi-authoredtexts, such as research papers.Another possible research direction is to improvethe scalability of our methods.
Our approach, likeKoppel et al?s (2011) baseline, requires linear timein the number of possible authors to classify a singledocument.
One possible way of reducing the timeneeded for prediction is by employing a hierarchi-cal approach that builds a tree of classifiers based onclass similarity, as done by Bickerstaffe and Zuker-man (2010) for the sentiment analysis task.
Underthis framework, class similarity (in our case, authorsimilarity) can be measured using LDA, while smallgroups of classes can be discriminated using SVMs.In addition to authorship attribution, we plan toemploy text-based author models in user modellingtasks such as rating prediction ?
a direction that wealready started working on by successfully applyingour LDA-based approach to model users for the rat-ing prediction task (Seroussi et al, 2011).AcknowledgementsThis research was supported in part by grantLP0883416 from the Australian Research Council.The authors thank Russell Smyth for the collabora-tion on initial results on the judgement dataset.188ReferencesAdrian Bickerstaffe and Ingrid Zukerman.
2010.
A hier-archical classifier applied to multi-way sentiment de-tection.
In COLING 2010: Proceedings of the 23rdInternational Conference on Computational Linguis-tics, pages 62?70, Beijing, China.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3(Jan):993?1022.Haibin Cheng, Pang-Ning Tan, and Rong Jin.
2007.
Lo-calized support vector machine and its efficient algo-rithm.
In SDM 2007: Proceedings of the 7th SIAMInternational Conference on Data Mining, pages 461?466, Minneapolis, MN, USA.Michael Gamon.
2004.
Linguistic correlates of style:Authorship classification with deep linguistic analysisfeatures.
In COLING 2004: Proceedings of the 20thInternational Conference on Computational Linguis-tics, pages 611?617, Geneva, Switzerland.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101(Suppl.
1):5228?5235.Patrick Juola.
2006.
Authorship attribution.
Founda-tions and Trends in Information Retrieval, 1(3):233?334.Moshe Koppel, Jonathan Schler, and Shlomo Argamon.2011.
Authorship attribution in the wild.
LanguageResources and Evaluation, 45(1):83?94.Kim Luyckx and Walter Daelemans.
2008.
Authorshipattribution and verification with many authors and lim-ited data.
In COLING 2008: Proceedings of the 22ndInternational Conference on Computational Linguis-tics, pages 513?520, Manchester, UK.David Madigan, Alexander Genkin, David D. Lewis,Shlomo Argamon, Dmitriy Fradkin, and Li Ye.
2005.Author identification on the large scale.
In Proceed-ings of the Joint Annual Meeting of the Interface andthe Classification Society of North America, St. Louis,MO, USA.Thomas C. Mendenhall.
1887.
The characteristic curvesof composition.
Science, 9(214S):237?246.Frederick Mosteller and David L. Wallace.
1964.
In-ference and Disputed Authorship: The Federalist.Addison-Wesley.Arun Rajkumar, Saradha Ravi, VenkatasubramanianSuresh, M. Narasimha Murthy, and C. E. Veni Mad-havan.
2009.
Stopwords and stylometry: A latentDirichlet alocation approach.
In Proceedings of theNIPS 2009 Workshop on Applications for Topic Mod-els: Text and Beyond (Poster Session), Whistler, BC,Canada.Bhavani Raskutti and Adam Kowalczyk.
2004.
Extremere-balancing for SVMs: A case study.
ACM SIGKDDExplorations Newsletter, 6(1):60?69.Ryan Rifkin and Aldebaro Klautau.
2004.
In defense ofone-vs-all classification.
Journal of Machine LearningResearch, 5(Jan):101?141.Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, andPadhraic Smyth.
2004.
The author-topic model forauthors and documents.
In UAI 2004: Proceedings ofthe 20th Conference on Uncertainty in Artificial Intel-ligence, pages 487?494, Banff, AB, Canada.Michal Rosen-Zvi, Chaitanya Chemudugunta, ThomasGriffiths, Padhraic Smyth, and Mark Steyvers.
2010.Learning author-topic models from text corpora.
ACMTransactions on Information Systems, 28(1):1?38.Jonathan Schler, Moshe Koppel, Shlomo Argamon, andJames W. Pennebaker.
2006.
Effects of age and gen-der on blogging.
In Proceedings of AAAI Spring Sym-posium on Computational Approaches for AnalyzingWeblogs, pages 199?205, Stanford, CA, USA.Yanir Seroussi, Ingrid Zukerman, and Fabian Bohnert.2010.
Collaborative inference of sentiments fromtexts.
In UMAP 2010: Proceedings of the 18th In-ternational Conference on User Modeling, Adaptationand Personalization, pages 195?206, Waikoloa, HI,USA.Yanir Seroussi, Fabian Bohnert, and Ingrid Zukerman.2011.
Personalised rating prediction for new users us-ing latent factor models.
In Hypertext 2011: Proceed-ings of the 22nd ACM Conference on Hypertext andHypermedia, Eindhoven, The Netherlands.Efstathios Stamatatos.
2009.
A survey of modern au-thorship attribution methods.
Journal of the Ameri-can Society for Information Science and Technology,60(3):538?556.Mark Steyvers and Tom Griffiths.
2007.
Probabilistictopic models.
In Thomas K. Landauer, Danielle S.McNamara, Simon Dennis, and Walter Kintsch, ed-itors, Handbook of Latent Semantic Analysis, pages427?448.
Lawrence Erlbaum Associates.189
