Proceedings of the ACL 2007 Demo and Poster Sessions, pages 149?152,Prague, June 2007. c?2007 Association for Computational LinguisticsEmpirical Measurements of Lexical Similarity in Noun Phrase ConjunctsDeirdre Hogan?Department of Computer ScienceTrinity College DublinDublin 2, Irelanddhogan@computing.dcu.ieAbstractThe ability to detect similarity in conjunctheads is potentially a useful tool in help-ing to disambiguate coordination structures- a difficult task for parsers.
We propose adistributional measure of similarity designedfor such a task.
We then compare several dif-ferent measures of word similarity by testingwhether they can empirically detect similar-ity in the head nouns of noun phrase con-juncts in the Wall Street Journal (WSJ) tree-bank.
We demonstrate that several measuresof word similarity can successfully detectconjunct head similarity and suggest that themeasure proposed in this paper is the mostappropriate for this task.1 IntroductionSome noun pairs are more likely to be conjoinedthan others.
Take the follow two alternate brack-etings: 1. busloads of ((executives) and (theirspouses)) and 2.
((busloads of executives) and(their spouses)).
The two head nouns coordinatedin 1 are executives and spouses, and (incorrectly)in 2: busloads and spouses.
Clearly, the formerpair of head nouns is more likely and, for the pur-pose of discrimination, a parsing model would ben-efit if it could learn that executives and spousesis a more likely combination than busloads andspouses.
If nouns co-occurring in coordination pat-terns are often semantically similar, and if a simi-?
Now at the National Centre for Language Technology,Dublin City University, Ireland.larity measure could be defined so that, for exam-ple: sim(executives, spouses) > sim(busloads, spouses)then it is potentially useful for coordination disam-biguation.The idea that nouns co-occurring in conjunc-tions tend to be semantically related has been notedin (Riloff and Shepherd, 1997) and used effec-tively to automatically cluster semantically similarwords (Roark and Charniak, 1998; Caraballo, 1999;Widdows and Dorow, 2002).
The tendency for con-joined nouns to be semantically similar has alsobeen exploited for coordinate noun phrase disam-biguation by Resnik (1999) who employed a mea-sure of similarity based on WordNet to measurewhich were the head nouns being conjoined in cer-tain types of coordinate noun phrase.In this paper we look at different measures ofword similarity in order to discover whether they candetect empirically a tendency for conjoined nouns tobe more similar than nouns which co-occur but arenot conjoined.
In Section 2 we introduce a measureof word similarity based on word vectors and in Sec-tion 3 we briefly describe some WordNet similaritymeasures which, in addition to our word vector mea-sure, will be tested in the experiments of Section 4.2 Similarity based on CoordinationCo-occurrencesThe potential usefulness of a similarity measure de-pends on the particular application.
An obviousplace to start, when looking at similarity functionsfor measuring the type of semantic similarity com-mon for coordinate nouns, is a similarity functionbased on distributional similarity with context de-149fined in terms of coordination patterns.
Our mea-sure of similarity is based on noun co-occurrenceinformation, extracted from conjunctions and lists.We collected co-occurrence data on 82, 579 distinctword types from the BNC and the WSJ treebank.We extracted all noun pairs from the BNC whichoccurred in a pattern of the form: noun cc noun1,as well as lists of any number of nouns separated bycommas and ending in cc noun.
Each noun in the listis linked with every other noun in the list.
Thus fora list: n1, n2, and n3, there will be co-occurrencesbetween words n1 and n2, between n1 and n3 andbetween n2 and n3.
To the BNC data we added allhead noun pairs from the WSJ (sections 02 to 21)that occurred together in a coordinate noun phrase.2From the co-occurrence data we constructed wordvectors.
Every dimension of a word vector repre-sents another word type and the values of the com-ponents of the vector, the term weights, are derivedfrom the coordinate word co-occurrence counts.
Weused dampened co-occurrence counts, of the form:1 + log(count), as the term weights for the wordvectors.
To measure the similarity of two words, w1and w2, we calculate the cosine of the angle betweenthe two word vectors, ~w1 and ~w2.3 WordNet-Based Similarity MeasuresWe also examine the following measures of seman-tic similarity which are WordNet-based.3 Wu andPalmer (1994) propose a measure of similarity oftwo concepts c1 and c2 based on the depth of con-cepts in the WordNet hierarchy.
Similarity is mea-sured from the depth of the most specific node dom-inating both c1 and c2, (their lowest common sub-sumer), and normalised by the depths of c1 andc2.
In (Resnik, 1995) concepts in WordNet areaugmented by corpus statistics and an information-theoretic measure of semantic similarity is calcu-lated.
Similarity of two concepts is measured1It would be preferable to ensure that the pairs extracted areunambiguously conjoined heads.
We leave this to future work.2We did not include coordinate head nouns from base nounphrases (NPB) (i.e.
noun phrases that do not dominate othernoun phrases) because the underspecified annotation of NPBsin the WSJ means that the conjoined head nouns can not alwaysbe easily identified.3All of the WordNet-based similarity measure ex-periments, as well as a random similarity measure,were carried out with the WordNet::Similarity package,http://search.cpan.org/dist/WordNet-Similarity.by the information content of their lowest com-mon subsumer in the is-a hierarchy of WordNet.Both Jiang and Conrath (1997) and Lin (1998) pro-pose extentions of Resnik?s measure.
Leacock andChodorow (1998)?s measure takes into account thepath length between two concepts, which is scaledby the depth of the hierarchy in which they re-side.
In (Hirst and St-Onge, 1998) similarity isbased on path length as well as the number ofchanges in the direction in the path.
In (Banerjee andPedersen, 2003) semantic relatedness between twoconcepts is based on the number of shared wordsin their WordNet definitions (glosses).
The glossof a particular concept is extended to include theglosses of other concepts to which it is related in theWordNet hierarchy.
Finally, Patwardhan and Peder-son (2006) build on previous work on second-orderco-occurrence vectors (Schu?tze, 1998) by construct-ing second-order co-occurrence vectors from Word-Net glosses, where, as in (Banerjee and Pedersen,2003), the gloss of a concept is extended so that itincludes the gloss of concepts to which it is directlyrelated in WordNet.4 ExperimentsWe selected two sets of data from sections 00, 01,22 and 24 of the WSJ treebank.
The first consistsof all nouns pairs which make up the head wordsof two conjuncts in coordinate noun phrases (againnot including coordinate NPBs).
We found 601 suchcoordinate noun pairs.
The second data set consistsof 601 word pairs which were selected at randomfrom all head-modifier pairs where both head andmodifier words are nouns and are not coordinated.We tested the 9 different measures of word similar-ity just described on each data set in order to see ifa significant difference could be detected betweenthe similarity scores for the coordinate words sam-ple and non-coordinate words sample.Initially both the coordinate and non-coordinatepair samples each contained 601 word pairs.
How-ever, before running the experiments we removedall pairs where the words in the pair were identical.This is because identical words occur more often incoordinate head words than in other lexical depen-dencies (there were 43 pairs with identical words inthe coordination set, compared to 3 such pairs in the150SimTest ncoord xcoord SDcoord nnonCoord xnonCoord SDnonCoord 95% CI p-valuecoordDistrib 503 0.11 0.13 485 0.06 0.09 [0.04 0.07] 0.000(Resnik, 1995) 444 3.19 2.33 396 2.43 2.10 [0.46 1.06] 0.000(Lin, 1998) 444 0.27 0.26 396 0.19 0.22 [0.04 0.11] 0.000(Jiang and Conrath, 1997) 444 0.13 0.65 395 0.07 0.08 [-0.01 0.11] 0.083(Wu and Palmer, 1994) 444 0.63 0.19 396 0.55 0.19 [0.06 0.11] 0.000(Leacock and Chodorow, 1998) 444 1.72 0.51 396 1.52 0.47 [0.13 0.27] 0.000(Hirst and St-Onge, 1998) 459 1.599 2.03 447 1.09 1.87 [0.25 0.76] 0.000(Banerjee and Pedersen, 2003) 451 114.12 317.18 436 82.20 168.21 [-1.08 64.92] 0.058(Patwardhan and Pedersen, 2006) 459 0.67 0.18 447 0.66 0.2 [-0.02 0.03] 0.545random 483 0.89 0.17 447 0.88 0.18 [-0.02 0.02] 0.859Table 1: Summary statistics for 9 different word similarity measures (plus one random measure):ncoordand nnonCoord are the sample sizes for the coordinate and non-coordinate noun pairs samples, respectively;xcoord, SDcoord and xnonCoord, SDnonCoord are the sample means and standard deviations for the two sets.The 95% CI column shows the 95% confidence interval for the difference between the two sample means.The p-value is for a Welch two sample two-sided t-test.
coordDistrib is the measure introduced in Section 2.non-coordination set).
If we had not removed them,a statistically significant difference between the sim-ilarity scores of the pairs in the two sets could befound simply by using a measure which, say, gaveone score for identical words and another (lower)score for all non-identical word pairs.Results for all similarity measure tests on the datasets described above are displayed in Table 1.
In onefinal experiment we used a random measure of sim-ilarity.
For each experiment we produced two sam-ples, one consisting of the similarity scores given bythe similarity measure for the coordinate noun pairs,and another set of similarity scores generated for thenon-coordinate pairs.
The sample sizes, means, andstandard deviations for each experiment are shownin the table.
Note that the variation in the samplesize is due to coverage: the different measures didnot produce a score for all word pairs.
Also dis-played in Table 1 are the results of statistical signif-icance tests based on the Welsh two sample t-test.A 95% confidence interval for the difference of thesample means is shown along with the p-value.5 DiscussionFor all but three of the experiments (excluding therandom measure), the difference between the meansimilarity measures is statistically significant.
Inter-estingly, the three tests where no significant differ-ence was measured between the scores on the co-ordination set and the non-coordination set (Jiangand Conrath, 1997; Banerjee and Pedersen, 2003;Patwardhan and Pedersen, 2006) were the threetop scoring measures in (Patwardhan and Pedersen,2006), where a subset of six of the above WordNet-based experiments were compared and the measuresevaluated against human relatedness judgements andin a word sense disambiguation task.
In anothercomparative study (Budanitsky and Hirst, 2002) offive of the above WordNet-based measures, evalu-ated as part of a real-word spelling correction sys-tem, Jiang and Conrath (1997)?s similarity score per-formed best.
Although performing relatively wellunder other evaluation criteria, these three measuresseem less suited to measuring the kind of similar-ity occurring in coordinate noun pairs.
One possi-ble explanation for the unsuitability of the measuresof (Patwardhan and Pedersen, 2006) for the coordi-nate similarity task could be based on how contextis defined when building context vectors.
Contextfor an instance of the the word w is taken to be thewords that surround w in the corpus within a givennumber of positions, where the corpus is taken as allthe glosses in WordNet.
Words that form part of col-locations such as disk drives or task force would thentend to have very similar contexts, and thus suchword pairs, from non-coordinate modifier-head re-lations, could be given too high a similarity score.Although the difference between the mean simi-larity scores seems rather slight in all experiments,it is worth noting that not all coordinate headwords are semantically related.
To take a cou-ple of examples from the coordinate word pair set:work/harmony extracted from hard work and har-mony, and power/clause extracted from executivepower and the appropriations clause.
We wouldnot expect these word pairs to get a high similar-ity score.
On the other hand, it is also possible that151some of the examples of non-coordinate dependen-cies involve semantically similar words.
For exam-ple, nouns in lists are often semantically similar, andwe did not exclude nouns extracted from lists fromthe non-coordinate test set.Although not all coordinate noun pairs are se-mantically similar, it seems clear, on inspection ofthe two sets of data, that they are more likely to besemantically similar than modifier-head word pairs,and the tests carried out for most of the measuresof semantic similarity detect a significant differencebetween the similarity scores assigned to coordinatepairs and those assigned to non-coordinate pairs.It is not possible to judge, based on the signifi-cance tests alone, which might be the most usefulmeasure for the purpose of disambiguation.
How-ever, in terms of coverage, the distributional mea-sure introduced in Section 2 clearly performs best4.This measure of distributional similarity is perhapsmore suited to the task of coordination disambigua-tion because it directly measures the type of simi-larity that occurs between coordinate nouns.
Thatis, the distributional similarity measure presented inSection 2 defines two words as similar if they occurin coordination patterns with a similar set of wordsand with similar distributions.
Whether the wordsare semantically similar becomes irrelevant.
A mea-sure of semantic similarity, on the other hand, mightfind words similar which are quite unlikely to ap-pear in coordination patterns.
For example, Ceder-berg and Widdows (2003) note that words appearingin coordination patterns tend to be on the same onto-logical level: ?fruit and vegetables?
is quite likely tooccur, whereas ?fruit and apples?
is an unlikely co-occurrence.
A WordNet-based measure of semanticsimilarity, however, might give a high score to bothof the noun pairs.In the future we intend to use the similarity mea-sure outlined in Section 2 in a lexicalised parser tohelp resolve coordinate noun phrase ambiguities.Acknowledgements Thanks to the TCD BroadCurriculum Fellowship and to the SFI ResearchGrant 04/BR/CS370 for funding this research.Thanks also to Pa?draig Cunningham, Saturnino Luzand Jennifer Foster for helpful discussions.4Somewhat unsurprisingly given it is part trained on datafrom the same domain.ReferencesSatanjeev Banerjee and Ted Pedersen.
2003 Extended GlossOverlaps as a Measure of Semantic Relatedness.
In Pro-ceeding of the 18th IJCAI.Alexander Budanitsky and Graeme Hirst.
2002 Semantic Dis-tance in WordNet: An experimental, application-orientedEvaluation of Five Measures In Proceedings of the 3rd CI-CLING.Sharon Caraballo.
1999 Automatic construction of ahypernym-labeled noun hierarchy from text In Proceedingsof the 37th ACL.Scott Cederberg and Dominic Widdows.
2003.
Using LSAand Noun Coordination Information to Improve the Preci-sion and Recall of Automatic Hyponymy Extraction.
In Pro-ceedings of the 7th CoNLL.G.
Hirst and D. St-Onge 1998.
Lexical Chains as repre-sentations of context for the detection and correction ofmalapropisms.
WordNet: An electronic lexical database.MIT Press.J.
Jiang and D. Conrath.
1997.
Semantic similarity based oncorpus statistics and lexical taxonomy.
In Proceedings ofthe ROCLING.C.
Leacock and M. Chodorow.
1998.
Combining local contextand WordNet similarity for word sense identification.
Word-Net: An electronic lexical database.
MIT Press.D.
Lin.
1998.
An information-theoretic definition of similarity.In Proceedings of the 15th ICML.Siddharth Patwardhan and Ted Pedersen.
2006.
UsingWordNet-based Context Vectors to Estimate the SemanticRelatedness of Concepts.
In Proceedings of Making Sense ofSense - Bringing Computational Linguistics and Psycholin-guistics Together, EACL.Philip Resnik.
1995.
Using Information Content to EvaluateSemantic Similarity.
In Proceedings of IJCAI.Philip Resnik.
1999.
Semantic Similarity in a Taxonomy: AnInformation-Based Measure and its Application to Problemsof Ambiguity in Natural Language.
In Journal of ArtificialIntelligence Research, 11:95-130.Ellen Riloff and Jessica Shepherd 1997.
A Corpus-based Ap-proach for Building Semantic Lexicon.
In Proceedings ofthe 2nd EMNLP.Brian Roark and Eugene Charniak 1998.
Noun-phrase Co-occurrence Statistics for Semi-automatic semantic lexiconconstruction.
In Proceedings of the COLING-ACL.Hinrich Schu?tze.
1998.
Automatic Word Sense Discrimination.Computational Linguistics, 24(1):97-123.Dominic Widdows and Beate Dorow.
2002.
A Graph Modelfor Unsupervised Lexical Acquisition.
In Proceedings of the19th COLING.Zhibiao Wu and Martha Palmer.
1994.
Verb Semantics andLexical Selection.
In Proceedings of the ACL.152
