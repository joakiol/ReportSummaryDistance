First Joint Conference on Lexical and Computational Semantics (*SEM), pages 365?373,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSemEval-2012 Task 3: Spatial Role LabelingParisa KordjamshidiKatholieke Universiteit Leuvenparisa.kordjamshidi@cs.kuleuven.beSteven BethardUniversity of Coloradosteven.bethard@colorado.eduMarie-Francine MoensKatholieke Universiteit Leuvensien.moens@cs.kuleuven.beAbstractThis SemEval2012 shared task is based on arecently introduced spatial annotation schemecalled Spatial Role Labeling.
The Spatial RoleLabeling task concerns the extraction of maincomponents of the spatial semantics from nat-ural language: trajectors, landmarks and spa-tial indicators.
In addition to these majorcomponents, the links between them and thegeneral-type of spatial relationships includ-ing region, direction and distance are targeted.The annotated dataset contains about 1213sentences which describe 612 images of theCLEF IAPR TC-12 Image Benchmark.
Wehave one participant system with two runs.The participant?s runs are compared to the sys-tem in (Kordjamshidi et al, 2011c) which isprovided by task organizers.1 IntroductionOne of the essential functions of natural language isto talk about spatial relationships between objects.The sentence ?Give me the book on AI on the bigtable behind the wall.?
expresses information aboutthe spatial configuration of the objects (book, table,wall) in some space.
Particularly, it explains the re-gion occupied by the book with respect to the tableand the direction (orientation) of the table with re-spect to the wall.
Understanding such spatial utter-ances is a problem in many areas, including robotics,navigation, traffic management, and query answer-ing systems (Tappan, 2004).Linguistic constructs can express highly complex,relational structures of objects, spatial relations be-tween them, and patterns of motion through spacerelative to some reference point.
Compared to nat-ural language, formal spatial models focus on oneparticular spatial aspect such as orientation, topol-ogy or distance and specify its underlying spatiallogic in detail (Hois and Kutz, 2008).
These for-mal models enable spatial reasoning that is difficultto perform on natural language expressions.Learning how to map natural language spatial in-formation onto a formal representation is a challeng-ing problem.
The complexity of spatial semanticsfrom the cognitive-linguistic point of view on theone hand, the diversity of formal spatial represen-tation models in different applications on the otherhand and the gap between the specification level ofthe two sides has led to the present situation that nowell-defined framework for automatic spatial infor-mation extraction exists that can handle all of theseaspects.In a previous paper (Kordjamshidi et al, 2010b),we introduced the task of spatial role labeling(SpRL) and proposed an annotation scheme that islanguage-independent and practically facilitates theapplication of machine learning techniques.
Ourframework consists of a set of spatial roles basedon the theory of holistic spatial semantics (Zlat-evl, 2007) with the intent of covering the main as-pects of spatial concepts at a course level, includ-ing both static and dynamic spatial semantics.
Thisshared task is defined on the basis of that annota-tion scheme.
Since this is the first shared task on thespatial information and this particular data, we pro-posed a simplified version of the original scheme.The intention of this simplification was to make thispractice feasible in the given timeframe.
However,365the current task is very challenging particularly forlearning the spatial links and relations.The core problem of SpRL is: i) the identificationof the words that play a role in describing spatialconcepts, and ii) the classification of the relationalrole that these words play in the spatial configura-tion.For example, consider again the sentence ?Giveme the book on AI on the big table behind the wall.
?.The phrase headed by the token book is referringto a trajector object.
The trajector (TR) is an en-tity whose location is described in the sentence.
Thephrase headed by the token table is referring to therole of a landmark (LM).
The landmark is a refer-ence object for describing the location of a trajector.These two spatial entities are related by the spatialexpression on denoted as spatial indicator (SP).
Thespatial indicator (often a preposition in English, butsometimes a verb, noun, adjective, or adverb) indi-cates the existence of spatial information in the sen-tence and establishes the type of a spatial relation.The spatial relations that can be extracted from thewhole sentence are <onSP bookTR tableLM> and<behindSP tableTR wallLM>.
One could also usespatial reasoning to infer that the statement <behindbook wall> holds, however, such inferred relationsare not considered in this task.
Although the spa-tial indicators are mostly prepositions, the reversemay not hold- for example, the first prepositionon only states the topic of the book, so <on bookAI> is not a spatial relation.
For each of the truespatial relations, a general type is assigned.
The<onSP bookTR tableLM> relation expresses a kindof topological relationship between the two objectsand we assign it a general type named region.
The<behindSP tableTR wallLM> relation expresses di-rectional information and we assign it a general typenamed direction.In general we assume two main abstraction layersfor the extraction of spatial information (Bateman,2010; Kordjamshidi et al, 2010a; Kordjamshidi etal., 2011a): (a) a linguistic layer, corresponding tothe annotation scheme described above, which startswith unrestricted natural language and predicts theexistence of spatial information at the sentence levelby identifying the words that play a particular spa-tial role as well as their spatial relationship; (b) aformal layer, in which the spatial roles are mappedonto a spatial calculus model (Galton, 2009).
Forexample, the linguistic layer recognizes that the spa-tial relation (on) holds between book and table, andthe formal layer maps this to a specific, formal spa-tial representation, e.g., a logical representation likeAboveExternallyConnected(book, table) or aformal qualitative spatial representation like EC (ex-ternally connected) in the RCC model (RegionalConnection Calculus) (Cohn and Renz, 2008).In this shared task we focus on the first (linguistic)level which is a necessary step for mapping naturallanguage to any formal spatial calculus.
The mainroles that are considered here are trajector, land-mark, spatial indicator, their links and the generaltype of their spatial relation.
The general type of arelation can be direction, region or distance.2 Motivation and related workSpatial role labeling is a key task for applicationsthat are required to answer questions or reason aboutspatial relationships between entities.
Examples in-clude systems that perform text-to-scene conversion,generation of textual descriptions from visual data,robot navigation tasks, giving directional instruc-tions, and geographical information systems (GIS).Recent research trends (Ross et al, 2010; Hois etal., 2011; Tellex et al, 2011) indicate an increasinginterest in the area of extracting spatial informationfrom language and mapping it to a formal spatialrepresentation.
Although cognitive-linguistic stud-ies have investigated this problem extensively, thecomputational aspect of making this bridge betweenlanguage and formal spatial representation (Hoisand Kutz, 2008) is still in its elementary stages.
Thepossession of a practical and appropriate annotationscheme along with data is the first requirement.
Toobtain this one has to investigate and schematizeboth linguistic and spatial ontologies.
This processneeds to cover the necessary information and seman-tics on the one hand, and to maintain the practicalfeasibility of the automatic annotation of unobserveddata on the other hand.In recent research on spatial information and nat-ural language, several annotation schemes have beenproposed such as ACE, GUM, GML, KML, TRMLwhich are briefly described and compared to Spa-tialML scheme in (MITRE Corporation, 2010).
But366to our knowledge, the main obstacles for employingmachine learning in this context and the very limitedusage of this effective approach have been (a) thelack of an agreement on a unique semantic modelfor spatial information; (b) the diversity of formalspatial relations; and consequently (c) the lack ofannotated data on which machine learning can beemployed to learn and extract the spatial relations.The most systematic work in this area includes theSpatialML (Mani et al, 2008) scheme which fo-cuses on geographical information, and the work of(Pustejovsky and Moszkowicz, 2009) in which thepivot of the spatial information is the spatial verb.The most recent and active work is the ISO-Spacescheme (Pustejovsky et al, 2011) which is basedon the above two schemes.
The ideas behind ISO-Space are closely related to our annotation schemein (Kordjamshidi et al, 2010b), however it consid-ers more detailed and fine-grained spatial and lin-guistic elements which makes the preparation of thedata for machine learning more difficult.Spatial information is directly related to the partof the language that can be visualized.
Thus, theextraction of spatial information is useful for mul-timodal environments.
One advantage of our pro-posed scheme is that it considers this dimension.
Be-cause it abstracts the spatial elements that could bealigned with the objects in images/videos and usedfor annotation of audio-visual descriptions (Butko etal., 2011).
This is useful in the multimodal environ-ments where, for example, natural language instruc-tions are given to a robot for finding the way or ob-jects.Not much work exists on using annotations forlearning models to extract spatial information.
Ourprevious work (Kordjamshidi et al, 2011c) is a firststep in this direction and provides a domain indepen-dent linguistic and spatial analysis to this problem.This shared task invites interested research groupsfor a similar effort.
The idea behind this task isfirstly to motivate the application of different ma-chine learning approaches, secondly to investigateeffective features for this task, and thirdly to revealthe practical problems in the annotation schemes andthe annotated concepts.
This will help to enrich thedata and the annotation in parallel with the machinelearning practice.3 Annotation schemeAs mentioned in the introduction, the annotation ofthe data set is according to the general spatial rolelabeling scheme (Kordjamshidi et al, 2010b).
Thebelow example presents the annotated elements inthis scheme.A womanTR and a childTR arewalkingMOTION overSP the squareLM .General-type: regionSpecific type: RCCSpatial value: PP (proper part)DynamicPath: middleFrame of reference: ?According to this scheme the main spatial roles are,Trajector (TR).
The entity, i.e., person, object orevent whose location is described, which canbe static or dynamic; (also called: local/figureobject, locatum).
In the above example womanand child are two trajectors.Landmark (LM).
The reference entity in relationto which the location or the motion of the tra-jector is specified.
(also called: reference ob-ject or relatum).
square is the landmark in theabove example.Spatial indicator (SP).
The element that definesconstraints on spatial properties such as the lo-cation of the trajector with respect to the land-mark.
The spatial indicator determines the typeof spatial relation.
The preposition over is an-notated as the spatial indicator in the currentexample.Moreover, the links between the three roles are an-notated as a spatial Relation.
Since each spatialrelation is defined with three arguments we callit a spatial triplet.
Each triplet indicates a re-lation between the three above mentioned spatialroles.
The sentence contains two spatial relationsof <overSP womanTR squareLM> and <overSPchildTR squareLM>, with the same spatial at-tributes listed below the example.
In spatial infor-mation theory the relations and properties are usu-ally grouped into the domains of topological, direc-tional, and distance relations and also shape (Stock,3671997).
Accordingly, we propose a mapping betweenthe extracted spatial triplets to the coarse-grainedtype of spatial relationships including region, direc-tion or distance.
We call these types as general-type of the spatial relations and briefly describethese below:Region.
refers to a region of space which is alwaysdefined in relation to a landmark, e.g.
the inte-rior or exterior, e.g.
?the flower is in the vase?.Direction.
denotes a direction along the axes pro-vided by the different frames of reference, incase the trajector of motion is not characterizedin terms of its relation to the region of a land-mark, e.g.
?the vase is on the left?.Distance.
states information about the spatial dis-tance of the objects and could be a qualitativeexpression such as close, far or quantitativesuch as 12 km, e.g.
?the kids are close to theblackboard?.The general-type of the relation in the example isannotated as region.After extraction of these relations a next fine-grained step will be to map each general spatial re-lationship to an appropriate spatial calculi represen-tation.
This step is not intended for this task andthe additional tags in the scheme will be consid-ered in the future shared tasks.
For example Re-gion Connection Calculus RCC-8 (Cohn and Renz,2008) representation reflects region-based topolog-ical relations.
Topological or region-based spatialinformation has been researched in depth in the areaof qualitative spatial representation and reasoning.We assume that the trajectors and landmarks can of-ten be interpreted as spatial regions and, as a conse-quence, their relation can be annotated with a spe-cific RCC-8 relation.
The RCC type in the aboveexample is specifically annotated as the PP (properpart).
Similarly, the direction and distance relationsare mapped to more specific formal representations.Two additional annotations are about motionverbs and dynamism.
Dynamic spatial informationare associated with spatial movements and spatialchanges.
In dynamic spatial relations mostly mo-tion verbs are involved.
Motion verbs carry spatialinformation and influence the spatial semantics.
Inthe above example the spatial indicator over is re-lated to a motion verb walking.
Hence the spatialrelation is dynamic and walking is annotated as themotion.
In contrast to the dynamic spatial relations,the static ones explain a static spatial configurationsuch as the example of the previous section <onSPbookTR tableLM> .In the case of dynamic spatial information a pathis associated with the location of the trajector.
In ourscheme the path is characterized by the three valuesof beginning, middle, end and zero.
The frame ofreference can be intrinsic, relative or absolute and istypically relevant for directional relations.
For moredetails about the scheme, see (Kordjamshidi et al,2010b).4 TasksThe SemEval-2012 shared task is defined in threeparts.?
The first part considers labeling the spatialindicators and trajector(s) / landmark(s).
Inother words at this step we consider theextraction of the individual roles that aretagged with TRAJECTOR, LANDMARK andSPATIAL INDICATOR.?
The second part is a kind of relation predictiontask and the goal is to extract triples contain-ing (spatial-indicator, trajector, landmark).
Theprediction of the tag of RELATION with its threearguments of SP, TR, LM at the same time isconsidered.?
The third part concerns the classification of thetype of the spatial relation.
At the most coarse-grained level this includes labeling the spatialrelations i.e.
the triplets of (spatial indicator,trajector, landmark) with region, direction, anddistance labels.
This means the general-typeof the RELATION should be predicted.
Thegeneral-type is an attribute of the RELATIONtag, see the example represented in XML for-mat in section 5.1.5 Preparation of the datasetThe annotated corpus that we used for this sharedtask is a subset of IAPR TC-12 image Bench-mark (Grubinger et al, 2006).
It contains 613 text368files that include 1213 sentences in total.
This is anextension of the dataset used in (Kordjamshidi etal., 2011c).
The original corpus was available freeof charge and without copyright restrictions.
Thecorpus contains images taken by tourists with de-scriptions in different languages.
The texts describeobjects, and their absolute and relative positions inthe image.
This makes the corpus a rich resource forspatial information.
However the descriptions arenot always limited to spatial information.
Thereforethey are less domain-specific and contain free expla-nations about the images.
Table 1 shows the detailedstatistics of this data.
The average length of the sen-tences in this data is about 15 words including punc-tuation marks with a standard deviation of 8.The spatial roles are assigned both to phrases andtheir headwords, but only the headwords are eval-uated for this task.
The spatial relations indicate atriplet of these roles.
The general-type is assigned toeach triplet of spatial indicator, trajector and land-mark.At the starting point two annotators including onetask-organizer and another non-expert annotator, an-notated 325 sentences for the spatial roles and rela-tions.
The purpose was to realize the disagreementpoints and prepare a set of instructions in a way toachieve highest-possible agreement.
From the firsteffort an inter-annotator agreement (Carletta, 1996)of 0.89 for Cohen?s kappa was obtained.
We contin-ued with the a third annotator for the remaining 888sentences.
The annotator had an explanatory sessionand received a set of instructions and annotated ex-amples to decrease the ambiguity in the annotations.To avoid complexity only the relations that are di-rectly expressed in the sentence are annotated andspatial reasoning was avoided during the annota-tions.
Sometimes the trajectors and landmarks orboth are implicit, meaning that there is no word inthe sentence to represent them.
For example in thesentence Come over here, the trajector you is onlyimplicitly present.
To be consistent with the numberof arguments in spatial relations, in these cases weuse the term undefined for the implicit roles.
There-fore, the spatial relation in the above example is<overSP undefinedTR hereLM>.5.1 Data formatThe data is released in XML format.
The originaltextual files are split into sentences.
Each sentenceis placed in a <SENTENCE/> tag and assigned anidentifier.
This tag contains all the other tags whichdescribe the content and spatial relations of one sen-tence.The content of the sentence is placed in the<CONTENT/> tag.
The words in each sentenceare assigned identifiers depending on their specificroles.
Trajectors, landmarks and spatial indicatorsare identified by <TRAJECTOR/>, <LANDMARK/>and <SPATIAL INDICATOR/> tags, respectively.Each of these XML elements has an ?ID?
attributethat identifies a related word by its index.
The ?ID?prefixed by either ?TW?, ?LW?
or ?SW?, respec-tively for the mentioned roles.
For example, a tra-jector with ID=?TW2?
corresponds to the word atindex 2 in the sentence.
Indexes start at 0.
Com-mas, parentheses and apostrophes are also countedas tokens.Spatial relations are assigned identifiers too, andrelate the role-playing words to each other.
Spa-tial relations are identified by the <RELATION/>tag.
The spatial indicator, trajector and land-mark for the relation are identified by the ?SP?,?TR?
and ?LM?
attributes, respectively.
The val-ues of these attributes correspond to the ?ID?
at-tributes in the <TRAJECTOR/>, <LANDMARK/>and <SPATIAL INDICATOR/> elements.
If a tra-jector or landmark is implicit, then the index of?TR?
or ?LM?
attribute will be set to a dummyindex.
This dummy index is equal to the in-dex of the last word in the sentence plus one.In this case, the value of TRAJECTOR or LAND-MARK is set to ?undefined?.
The coarse-grainedspatial type of the relation is indicated by the?GENERAL TYPE?
attribute and gets one valuein {REGION, DIRECTION, DISTANCE}.
In theoriginal data set there are cases annotated withmultiple spatial types.
This is due to the ambi-guity and/or under-specificity of natural languagecompared to formal spatial representations (Kord-jamshidi et al, 2010a).
In this task the general-type with a higher priority by the annotator is pro-vided.
Here, by the high priority type, we mean thegeneral type which has been the most informative369Spatial Roles Relations General TypesSentences TR LM SP Spatial triplets Region Direction Distance1213 1593 1408 1464 1715 1036 644 35Table 1: Number of annotated components in the data set.and relevant type for a relation, from the annotator?spoint of view.
This task considers labeling wordsrather than phrases for all spatial roles.
However, inthe XML file for spatial indicators often the wholephrase is tagged.
In these cases, the index of theindicator refers to one word which is typically thespatial preposition of the phrase.
For evaluation onlythe indexed words are compared and should be pre-dicted correctly.Below is one example copied from the data.
Formore examples and details about the general anno-tation scheme see (Kordjamshidi et al, 2010b).<SENTENCE ID=?S11?><CONTENT >there are red umbrellas in a park on the right .</CONTENT><TRAJECTOR ID=?TW3?>umbrellas</TRAJECTOR><LANDMARK ID=?LW6?>park</LANDMARK><SPATIAL INDICATOR ID=?SW4?>in</SPATIAL INDICATOR><RELATION ID=?R0?
SP=?SW4?
TR=?TW3?LM=?LW6?
GENERAL TYPE=?REGION?/><SPATIAL INDICATOR ID=?SW7?>on the right</SPATIAL INDICATOR><RELATION ID=?R1?
SP=?SW7?
TR=?TW3?LM=?LW6?
GENERAL TYPE=?DIRECTION?/></SENTENCE>The dataset, both train and test, also the 10-foldsplits are made available in the LIIR research groupwebpage of KU Leuven.16 Evaluation methodologyAccording to the usual setting of the shared tasksour evaluation setting was based on splitting the dataset into a training and a testing set.
Each set con-tained about 50% of the whole data.
The test set re-1http://www.cs.kuleuven.be/groups/liir/software/SpRL Data/leased without the ground-truth labels.
However, af-ter the systems submission deadline the ground-truthtest was released.
Hence the participant group per-formed an additional 10-fold cross validation eval-uation too.
We report the results of both evaluationsettings.Prediction of each component including TRAJEC-TORs, LANDMARKs and SPATIAL-INDICATORs isevaluated on the test set using their individual spatialelement XML tags.
The evaluation metrics of pre-cision, recall and F1-measure are used, which aredefined as:recall = TPTP+FN (1)precision = TPTP+FP (2)F1 = 2?recall?precision(recall+precision) , (3)where:TP = the number of system-producedXML tags that match an annotated XMLtag,FP = the number of system-producedXML tags that do not match an annotatedtag,FN = the number of annotated XML tagsthat do not match a system-produced tag.For the roles evaluation two XML tags matchwhen they have exactly same identifier.
In fact,when the identifiers are the same then the role andthe word index are the same.
In addition, systemsare evaluated on how well they are able to retrievetriplets of (trajector, spatial-indicator, landmark), interms of precision, recall and F1-measure.
The TP,FP, FN are counted in a similar way but two RELA-TION tags match if the combination of their TR, LMand SP is exactly the same.
In other words a true pre-diction requires all the three elements are correctlypredicted at the same time.The last evaluation is on how well the systems areable to retrieve the relations and their general type370i.e {region, direction, distance} at the same time.To evaluate the GENERAL-TYPE similarly the RELA-TION tag is checked.
For a true prediction, an exactmatch between the ground-truth and all the elementsof the predicted RELATION tag including TR, LM,SPand GENERAL-TYPE is required.7 Systems and resultsOne system with two runs was submitted from theUniversity of Texas Dallas.
The two runs (Robertsand Harabagiu, 2012), UTDSPRL-SUPERVISED1and UTDSPRL-SUPERVISED2 are based on thejoint classification of the spatial triplets in a bi-nary classification setting.
To produce the candi-date (indicator, trajector, landmark) triples, in thefirst stage heuristic rules targeting a high recall areused.
Then a binary support vector machine clas-sifier is employed to predict whether a triple is aspatial relation or not.
Both runs start with a largenumber of manually engineered features, and usefloating forward feature selection to select the mostimportant ones.
The difference between the tworuns of UTDSPRL-SUPERVISED1 and UTDSPRL-SUPERVISED2 is their feature set.
Particularly, inUTDSPRL-SUPERVISED1 a joint feature based onthe conjunctions (e.g.
and, but) is considered beforerunning feature selection but this feature is removedin UTDSPRL-SUPERVISED2.The submitted runs are compared to a previoussystem from the task organizers (Kordjamshidi etal., 2011c) which is evaluated on the current datawith the same settings.
This system, KUL-SKIP-CHAIN-CRF, uses a skip chain conditional randomfield (CRF) model (Sutton and MacCallum, 2006)to annotate the sentence as a sequence.
It considersthe long distance dependencies between the prepo-sitions and nouns in the sentence.The type and structure of the features used in theUTD and KUL systems are different.
In the UTDsystem, the classifier works on triples and the fea-tures are of two main types: (a) argument-specificfeatures about the trajector, landmark, or indicatore.g., the landmark?s hypernyms, or the indicator?sfirst token; and (b) joint features that consider twoor more of the arguments, e.g.
the dependency pathbetween indicator and landmark.
For more detail,see (Roberts and Harabagiu, 2012).
In the KUL sys-Label Precsion Recall F1TRAJECTOR 0.731 0.621 0.672LANDMARK 0.871 0.645 0.741SPATIAL-INDICATOR 0.928 0.712 0.806RELATION 0.567 0.500 0.531GENERAL-TYPE 0.561 0.494 0.526Table 2: UTDSPRL-SUPERVISED1: The Universityof Texas-Dallas system with a larger number of fea-tures,test/train one split.Label Precsion Recall F1TRAJECTOR 0.782 0.646 0.707LANDMARK 0.894 0.680 0.772SPATIAL-INDICATOR 0.940 0.732 0.823RELATION 0.610 0.540 0.573GENERAL-TYPE 0.603 0.534 0.566Table 3: UTDSPRL-SUPERVISED2: The University ofTexas-Dallas system with a smaller number of features,test/train one split.Label Precsion Recall F1TRAJECTOR 0.697 0.603 0.646LANDMARK 0.773 0.740 0.756SPATIAL-INDICATOR 0.913 0.887 0.900RELATION 0.487 0.512 0.500Table 4: KUL-SKIP-CHAIN-CRF: The organizers?
sys-tem (Kordjamshidi et al, 2011c)- test/train one split.tem, the classifier works on all tokens in a sentence,and a number of linguistically motivated local andpairwise features over candidate words and preposi-tions are used.
To consider long distance dependen-cies a template, called a preposition template, is usedin the general CRF framework.
Loopy belief prop-agation is used for inference.
Mallet2 and GRMM:3implementations are employed there.Tables 2, 3 and 4 show the results of the threeruns in the standard setting of the shared task us-ing the train/test split.
In this evaluation setting theUTDSPRL-SUPERVISED2 run achieves the highestperformance on the test set, with F1 of 0.573 forthe full triplet identification task, and an F1 of 0.566for additionally classifying the triplet?s general-type2http://mallet.cs.umass.edu/download.php3http://mallet.cs.umass.edu/grmm/index.php371System Precsion Recall F1KUL-SKIP-CHAIN-CRF 0.745 0.773 0.758UTDSPRL-SUPERVISED2 0.773 0.679 0.723Table 5: The RELATION extraction of KUL-SKIP-CHAIN-CRF (Kordjamshidi et al, 2011c) vs. UTDSPRL-SUPERVISED2 evaluated with 10-fold cross validationcorrectly.
It also consistently outperforms both theUTDSPRL-SUPERVISED1 run and the KUL-SKIP-CHAIN-CRF system on each of the individual trajec-tor, landmark and spatial-indicator extraction.The dataset was relatively small, so we releasedthe test data and the two systems were addition-ally evaluated using 10-fold cross validation.
Theresults of this cross-validation are shown in Ta-ble 5.
The UTDSPRL-SUPERVISED2 run achievesa higher precision, while the KUL-SKIP-CHAIN-CRF system achieves a higher recall.
It should bementioned the 10-fold splits used by KUL and UTDare not the same.
This implies that the results withexactly the same cross-folds may vary slightly fromthese reported in Table 5.Using 10-fold cross validation, we also evaluatedthe classification of the general-type of a relationgiven the manually annotated positive triplets.
TheUTDSPRL-SUPERVISED2 system achieved F1=0.974, and similar experiments using SMO-SVM in(Kordjamshidi et al, 2011b; Kordjamshidi et al,2011a) achieved F1= 0.973.
Thus it appears thatidentifying the general-type of a relation is a rela-tively easy task on this data.Discussion.
Since the feature sets of the two sys-tems are different and given the evaluation resultsin the two evaluation settings, it is difficult to assertwhich model is better in general.
Obviously usingjoint features potentially inputs richer information tothe model.
However, it can increase the sparsity inone hand and overfitting on the training data on theother hand.
Another problem is that finding heuris-tics for high recall that are sufficiently general to beused in every domain is not an easy task.
By increas-ing the number of candidates the dataset imbalancewill increase dramatically.
This can cause a lowerperformance of a joint model based on a binary clas-sification setting when applied on different data sets.It seems that this task might require a more elabo-rated structured output prediction model which canconsider the joint features and alleviate the problemof huge negatives in that framework while consider-ing the correlations between the output components.8 ConclusionThe SemEval-2012 spatial role labeling task is astarting point to formally consider the extraction ofspatial semantics from the language.
The aim isto consider this task as a standalone linguistic taskwhich is important for many applications.
Our firstpractice on this task and the current submitted sys-tem to SemEval 2012 clarify the type of the featuresand the machine learning approaches appropriate forit.
The proposed features and models help to per-form this task automatically in a reasonable accu-racy.
Although the spatial scheme is domain inde-pendent, the achieved accuracy is dependent on thedomain of the used data for training a model.
Ourfuture plan is to extend the data for the next work-shops and to cover more semantic aspects of spatialinformation particularly for mapping to formal spa-tial representation models and spatial calculus.AcknowledgmentsSpecial thanks to Martijn Van Otterlo for his greatcooperation from the initiation phase and in thegrowth of this task.
Many thanks to SabineDrebusch for her kind and open cooperation in an-notating the very first dataset.
Thanks to Tigist Kas-sahun for her help in annotating the current dataset.Thanks the participant team of the University ofTexas Dallas and their useful feedback on the an-notated data.ReferencesJ.
A. Bateman.
2010.
Language and space: a two-levelsemantic approach based on principles of ontologicalengineering.
International Journal of Speech Technol-ogy, 13(1):29?48.372T.
Butko, C. Nadeu, and A. Moreno.
2011.
A multi-lingual corpus for rich audio-visual scenedescriptionin a meeting-room environment.
In ICMI workshopon multimodal corpora for machine learning: TakingStock and Roadmapping the Future.J.
Carletta.
1996.
Assessing agreement on classificationtasks: the kappa statistic.
Computational Linguistics,22(2):249?254.A.
G. Cohn and J. Renz.
2008.
Qualitative spatial repre-sentation and reasoning.
In Handbook of KnowledgeRepresentation, volume 3 of Foundations of ArtificialIntelligence, pages 551 ?
596.
Elsevier.A.
Galton.
2009.
Spatial and temporal knowledge rep-resentation.
Journal of Earth Science Informatics,2(3):169?187.M.
Grubinger, P. Clough, Henning Mu?ller, and ThomasDeselaers.
2006.
The IAPR benchmark: A new evalu-ation resource for visual information systems.
In In-ternational Conference on Language Resources andEvaluation (LREC).J.
Hois and O. Kutz.
2008.
Natural language meets spa-tial calculi.
In Christian Freksa, Nora S. Newcombe,Peter Ga?rdenfors, and Stefan Wo?lfl, editors, SpatialCognition, volume 5248 of Lecture Notes in ComputerScience, pages 266?282.
Springer.J.
Hois, R. J. Ross, J. D. Kelleher, and J.
A. Bateman.2011.
Computational models of spatial language in-terpretation and generation.
In COSLI-2011.P.
Kordjamshidi, M. van Otterlo, and M. F. Moens.2010a.
From language towards formal spatial calculi.In Workshop on Computational Models of Spatial Lan-guage Interpretation (CoSLI 2010, at Spatial Cogni-tion 2010).P.
Kordjamshidi, M. van Otterlo, and M. F. Moens.2010b.
Spatial role labeling: Task definition and anno-tation scheme.
In Proceedings of the Seventh confer-ence on International Language Resources and Eval-uation (LREC?10).P.
Kordjamshidi, J. Hois, M. van Otterlo, and M.-F.Moens.
2011a.
Machine learning for interpretation ofspatial natural language in terms of qsr.
Poster Presen-tation at the 10th International Conference on SpatialInformation Theory (COSIT?11).P.
Kordjamshidi, J. Hois, M. van Otterlo, and M.F.Moens.
2011b.
Learning to interpret spatial naturallanguage in terms of qualitative spatial relations.
Rep-resenting space in cognition: Interrelations of behav-ior, language, and formal models.
Series Explorationsin Language and Space, Oxford University Press, sub-mitted.P.
Kordjamshidi, M. Van Otterlo, and M.F.
Moens.2011c.
Spatial role labeling: Towards extraction ofspatial relations from natural language.
ACM Trans.Speech Lang.
Process., 8:1?36, December.I.
Mani, J. Hitzeman, J.
Richer, D. Harris, R. Quimby, andB.
Wellner.
2008.
SpatialML: Annotation scheme,corpora, and tools.
In Nicoletta Calzolari, KhalidChoukri, Bente Maegaard, Joseph Mariani, Jan Odjik,Stelios Piperidis, and Daniel Tapias, editors, Proceed-ings of the Sixth International Language Resourcesand Evaluation (LREC?08).
European Language Re-sources Association (ELRA).MITRE Corporation.
2010.
SpatialML: Annotationscheme for marking spatial expression in natural lan-guage.
Technical Report Version 3.0.1, The MITRECorporation.J.
Pustejovsky and J.L.
Moszkowicz.
2009.
Integrat-ing motion predicate classes with spatial and tempo-ral annotations.
In CoLing 2008: Companion volumePosters and Demonstrations, pages 95?98.J.
Pustejovsky, J. Moszkowicz, and M. Verhagen.
2011.Iso-space: The annotation of spatial information inlanguage.
In Proceedings of ISA-6: ACL-ISO Inter-national Workshop on Semantic Annotation.K.
Roberts and S.M.
Harabagiu.
2012.
Utd-sprl: A jointapproach to spatial role labeling.
In Submitted to thisworkshop of SemEval-2012.R.
Ross, J. Hois, and J. Kelleher.
2010.
Computationalmodels of spatial language interpretation.
In COSLI-2010.O.
Stock, editor.
1997.
Spatial and Temporal Reasoning.Kluwer.C.
Sutton and A. MacCallum.
2006.
Introduction to con-ditional random fields for relational learning.
In LiseGetoor and Ben Taskar, editors, Introduction to Statis-tical Relational Learning.
MIT Press.D.
A. Tappan.
2004.
Knowledge-Based Spatial Rea-soning for Automated Scene Generation from Text De-scriptions.
Ph.D. thesis.S.
Tellex, T. Kollar, S. Dickerson, M. R. Walter, andN.
Roy A. G. Banerjee, S. Teller.
2011.
Understand-ing natural language commands for robotic naviga-tion and mobile manipulation.
In Proceedings of theNational Conference on Artificial Intelligence (AAAI),San Francisco, CA.J.
Zlatevl.
2007.
Spatial semantics.
In Hubert Cuyck-ens and Dirk Geeraerts (eds.)
The Oxford Handbookof Cognitive Linguistics, Chapter 13, pages 318?350.373
