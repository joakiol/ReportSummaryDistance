Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1213?1222,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Neural Probabilistic Structured-Prediction Model for Transition-BasedDependency ParsingHao Zhou?
?Yue Zhang?
Shujian Huang?
Jiajun Chen?
?State Key Laboratory for Novel Software Technology, Nanjing University, China?Singapore University of Technology and Design, Singapore{zhouh, huangsj, chenjj}@nlp.nju.edu.cn, yue zhang@sutd.edu.sgAbstractNeural probabilistic parsers are attrac-tive for their capability of automatic fea-ture combination and small data sizes.A transition-based greedy neural parserhas given better accuracies over its lin-ear counterpart.
We propose a neuralprobabilistic structured-prediction modelfor transition-based dependency parsing,which integrates search and learning.Beam search is used for decoding, andcontrastive learning is performed for max-imizing the sentence-level log-likelihood.In standard Penn Treebank experiments,the structured neural parser achieves a1.8% accuracy improvement upon a com-petitive greedy neural parser baseline, giv-ing performance comparable to the bestlinear parser.1 IntroductionTransition-based methods have given competitiveaccuracies and efficiencies for dependency pars-ing (Yamada and Matsumoto, 2003; Nivre andScholz, 2004; Zhang and Clark, 2008; Huang andSagae, 2010; Zhang and Nivre, 2011; Goldbergand Nivre, 2013).
These parsers construct depen-dency trees by using a sequence of transition ac-tions, such as SHIFT and REDUCE, over input sen-tences.
High accuracies are achieved by using alinear model and millions of binary indicator fea-tures.
Recently, Chen and Manning (2014) pro-pose an alternative dependency parser using a neu-ral network, which represents atomic features asdense vectors, and obtains feature combination au-tomatically other than devising high-order featuresmanually.The greedy neural parser of Chen and Man-ning (2014) gives higher accuracies compared to?Work done while the first author was visiting SUTD.the greedy linear MaltParser (Nivre and Scholz,2004), but lags behind state-of-the-art linear sys-tems with sparse features (Zhang and Nivre,2011), which adopt global learning and beamsearch decoding (Zhang and Nivre, 2012).
Thekey difference is that Chen and Manning (2014) isa local classifier that greedily optimizes each ac-tion.
In contrast, Zhang and Nivre (2011) leveragea structured-prediction model to optimize wholesequences of actions, which correspond to treestructures.In this paper, we propose a novel framework forstructured neural probabilistic dependency pars-ing, which maximizes the likelihood of action se-quences instead of individual actions.
Follow-ing Zhang and Clark (2011), beam search is ap-plied to decoding, and global structured learn-ing is integrated with beam search using early-update (Collins and Roark, 2004).
Designing sucha framework is challenging for two main reasons:First, applying global structured learning totransition-based neural parsing is non-trivial.
Adirect adaptation of the framework of Zhang andClark (2011) under the neural probabilistic modelsetting does not yield good results.
The main rea-son is that the parameter space of a neural networkis much denser compared to that of a linear modelsuch as the structured perceptron (Collins, 2002).Due to the dense parameter space, for neural mod-els, the scores of actions in a sequence are rela-tively more dependent than that in the linear mod-els.
As a result, the log probability of an action se-quence can not be modeled just as the sum of logprobabilities of each action in the sequence, whichis the case of structured linear model.
We addressthe challenge by using a softmax function to di-rectly model the distribution of action sequences.Second, for the structured model above,maximum-likelihood training is computationallyintractable, requiring summing over all possibleaction sequences, which is difficult for transition-1213based parsing.
To address this challenge, we takea contrastive learning approach (Hinton, 2002; Le-Cun and Huang, 2005; Liang and Jordan, 2008;Vickrey et al, 2010; Liu and Sun, 2014).
Usingthe sum of log probabilities over the action se-quences in the beam to approximate that over allpossible action sequences.In standard PennTreebank (Marcus et al, 1993)evaluations, our parser achieves a significant accu-racy improvement (+1.8%) over the greedy neu-ral parser of Chen and Manning (2014), and givesthe best reported accuracy by shift-reduce parsers.The incremental neural probabilistic frameworkwith global contrastive learning and beam searchcould be used in other structured prediction tasks.2 Background2.1 Arc-standard ParsingTransition-based dependency parsers scan an in-put sentence from left to right, and perform a se-quence of transition actions to predict its parsetree (Nivre, 2008).
In this paper, we employthe arc-standard system (Nivre et al, 2007),which maintains partially-constructed outputs us-ing a stack, and orders the incoming words in theinput sentence in a queue.
Parsing starts with anempty stack and a queue consisting of the wholeinput sentence.
At each step, a transition actionis taken to consume the input and construct theoutput.
The process repeats until the input queueis empty and stack contains only one dependencytree.Formally, a parsing state is denoted as ?j, S, L?,where S is a stack of subtrees [.
.
.
s2, s1, s0], jis the head of the queue (i.e.
[ q0= wj, q1=wj+1?
?
?
]), and L is a set of dependency arcs.
Ateach step, the parser chooses one of the followingactions:?
SHIFT: move the front word wjfrom thequeue onto the stacks.?
LEFT-ARC(l): add an arc with label l betweenthe top two trees on the stack (s1?
s0), andremove s1from the stack.?
RIGHT-ARC(l): add an arc with label l be-tween the top two trees on the stack (s1?s0), and remove s0from the stack.The arc-standard parser can be summarized asthe deductive system in Figure 1, where k denotesinput : w0.
.
.
wn?1axiom : 0 : ?0, ?, ?, 0?goal : 2n?
1 : ?n, s0, L?SHIFTk : ?j, S, L?k + 1 : ?j + 1, S|wj, L?LEFT-ARC(l)k : ?j, S|s1|s0, L?k + 1 : ?j, S|s0, L ?
{s1l??
s0}?RIGHT-ARC(l)k : ?j, S|s1|s0, L?k + 1 : ?j, S|s1, L ?
{s1l??
s0}?Figure 1: The deductive system for arc-standarddependency parsing.the current parsing step.
For a sentence with sizen, parsing stops after performing exactly 2n ?
1actions.MaltParser uses an SVM classifier for deter-ministic arc-standard parsing.
At each step, Malt-Parser generates a set of successor states accordingto the current state, and deterministically selectsthe highest-scored one as the next state.2.2 Global Learning and Beam SearchThe drawback of deterministic parsing is errorpropagation.
An incorrect action will have a nega-tive influence to its subsequent actions, leading toan incorrect output parse tree.To address this issue, global learning and beamsearch (Zhang and Clark, 2011; Bohnet and Nivre,2012; Choi and McCallum, 2013) are used.
Givenan input x, the goal of decoding is to find thehighest-scored action sequence globally.y = arg maxy??GEN(x)score(y?)
(1)Where GEN(x) denotes all possible action se-quences on x, which correspond to all possibleparse trees.
The score of an action sequence y is:score(y) =?a?y?
?
?
(a) (2)Here a is an action in the action sequence y, ?is a feature function for a, and ?
is the parametervector of the linear model.
The score of an actionsequence is the linear sum of the scores of eachaction.
During training, action sequence scores areglobally learned.1214The parser of Zhang and Nivre (2011) is devel-oped using this framework.
The structured percep-tron (Collins, 2002) with early update (Collins andRoark, 2004) is applied for training.
By utilizingrich manual features, it gives state-of-the-art accu-racies in standard Penn Treebank evaluation.
Wetake this method as one baseline.2.3 Greedy Neural Network ModelChen and Manning (2014) build a greedy neuralarc-standard parser.
The model can be regarded asan alternative implementation of MaltParser, usinga feedforward neural network to replace the SVMclassifier for deterministic parsing.2.3.1 ModelThe greedy neural model extracts n atomic fea-tures from a parsing state, which consists ofwords, POS-tags and dependency labels from thestack ans queue.
Embeddings are used to rep-resent word, POS and dependency label atomicfeatures.
Each embedding is represented as a d-dimensional vector ei?
R. Therefore, the fullembedding matrix is E ?
Rd?V, where V is thenumber of distinct features.
A projection layer isused to concatenate the n input embeddings into avector x = [e1; e2.
.
.
en], where x ?
Rd?n.
Thepurpose of this layer is to fine-tune the embeddingfeatures.
Then x is mapped to a dh-dimensionalhidden layer by a mapping matrix W1?
Rdh?d?nand a cube activation function:h = (W1x + b1)3(3)Finally, h is mapped into a softmax output layerfor modeling the probabilistic distribution of can-didate shift-reduce actions:p =softmax(o) (4)whereo = W2h (5)W2?
Rdo?dhand dois the number of shift-reduceactions.2.3.2 FeaturesOne advantage of Chen and Manning (2014) isthat the neural network parser achieves featurecombination automatically.
Their atomic featuresare defined by following Zhang and Nivre (2011).As shown in Table 1, the features are categorizedTemplatesFws0w, s2w, q0w, q1w, q2w, lc1(s0)w, lc2(s0)ws1w, rc2(s0)w, lc1(s1)w, lc2(s1)w, rc2(s1)wrc1(s0)w, rc1(s1)w, lc1(lc1(s0))w, lc1(lc1(s1))wrc1(rc1(s1))w, rc1(rc1(s0))wFts0t, q0t, q1t, q2t, rc1(s0)t, lc1(s0)t, lc2(s0)ts1t, s2t, lc1(s1)t, lc2(s1)t, rc1(s1)t, rc2(s0)trc2(s1)t, lc1(lc1(s0))t, lc1(lc1(s1))trc1(rc1(s0))t, rc1(rc1(s1))tFlrc1(s0)l, lc1(s0)l, lc2(s0)l, lc1(s1)l, lc2(s1)lrc1(s1)l, rc2(s0)l, rc2(s1)l, lc1(lc1(s0))llc1(lc1(s1))l, rc1(rc1(s0))l, rc1(rc1(s1))lTable 1: Feature templates.into three types: Fw, Ft, Fl, which representsword features, POS-tag features and dependencylabel features, respectively.For example, s0w and q0w represent thefirst word on the stack and queue, respectively;lc1(s0)w and rc1(s0)w represent the leftmost andrightmost child of s0, respectively.
Similarly,lc1(s0)t and lc1(s0)l represent the POS-tag anddependency label of the leftmost child of s0, re-spectively.Chen and Manning (2014) find that the cubeactivation function in Equation (3) is highly ef-fective in capturing feature interaction, which isa novel contribution of their work.
The cube func-tion achieves linear combination between atomicword, POS and label features via the product ofthree element combinations.
Empirically, it worksbetter compared to a sigmoid activation function.2.4 TrainingGiven a set of training examples, the training ob-jective of the greedy neural parser is to minimizethe cross-entropy loss, plus a l2-regularizationterm:L(?)
= ?
?i?Alog pi+?2?
?
?2(6)?
is the set of all parameters (i.e.
W1, W2, b,E), and A is the set of all gold actions in the train-ing data.
AdaGrad (Duchi et al, 2011) with mini-batch is adopted for optimization.
We take thegreedy neural parser of Chen and Manning (2014)as a second baseline.3 Structured Neural Network ModelWe propose a neural structured-prediction modelthat scores whole sequences of transition actions,rather than individual actions.
As shown in Ta-ble 2, the model can be seen as a neural prob-abilistic alternative of Zhang and Nivre (2011),1215localclassifierstructuredpredictionlinearsparseSection 2.1(Nivreet al, 2007)Section 2.2(Zhang andNivre, 2011)neuraldenseSection 2.3(Chen andManning, 2014)this workTable 2: Correlation between different parsers.or a structured-prediction alternative of Chen andManning (2014).
It combines the advantages ofboth Zhang and Nivre (2011) and Chen and Man-ning (2014) over the greedy linear MaltParser.3.1 Neural Probabilistic RankingGiven the baseline system in Section 2.2, the mostintuitive structured neural dependency parser isto replace the linear scoring model with a neu-ral probabilistic model.
Following Equation 1, thescore of an action sequence y, which correspondsto its log probability, is sum of log probabilityscores of each action in the sequence.s(y) =?a?ylog pa(7)where pais defined by the baseline neural modelof Section 2.3 (Equation 4).
The training objec-tive is to maximize the score margin between thegold action sequences (yg) and these of incorrectlypredicated action sequences (yp):L(?)
= max(0, ??s(yg)+s(yp))+?2?
?
?2(8)With this ranking model, beam search andearly-update are used.
Given a training instance,the negative example is the incorrectly predictedoutput with largest score (Zhang and Nivre, 2011).However, we find that the ranking model workspoorly.
One explanation is that the actions in asequence is probabilistically dependent on eachother, and therefore using the total log probabil-ities of each action to compute the log probabil-ity of an action sequence (Equation 7) is inaccu-rate.
Linear models do not suffer from this prob-lem, because the parameter space of linear modelsis much more sparse than that of neural models.For neural networks, the dense parameter space isshared by all the actions in a sequence.
Increasingthe likelihood of a gold action may also change thelikelihood of incorrect actions through the sharedparameters.
As a result, increasing the scores of agold action sequence and simultaneously reducingthe scores of an incorrect action sequence does notwork well for neural models.3.2 Sentence-Level Log-LikelihoodTo overcome the above limitation, we try to di-rectly model the probabilistic distribution of wholeaction sequences.
Given a sentence x and neuralnetworks parameter ?, the probability of the actionsequence yiis given by the softmax function:p(yi| x, ?)
=ef(x, ?
)i?yj?GEN(x)ef(x, ?
)j(9)wheref(x, ?
)i=?ak?yio(x, yi, k, ak) (10)Here GEN(s) is the set of all possible valid ac-tion sequences for a sentence x; o(x, yi, k, ak)denotes the neural network score for the actionakgiven x and yi.
We use the same sub net-work as Chen and Manning (2014) to calculateo(x, yi, k, ak) (Equation 5).
The same featuresin Table 1 are used.Given the training data as (X , Y ), our train-ing objective is to minimize the negative log-likelihood:L(?)
= ??
(xi, yi)?
(X,Y )log p(yi| xi, ?)
(11)= ??
(xi, yi)?
(X,Y )logef(xi,?
)iZ(xi, ?)(12)=?
(xi, yi)?
(X,Y )logZ(xi, ?)?
f(xi, ?
)i(13)whereZ(x, ?)
=?yj?GEN(x)ef(x, ?
)j(14)Here, Z(x, ?)
is called the partition function.Following Chen and Manning(2014), we apply l2-regularization for training.For optimization, we need to compute gradientsfor L(?
), which includes gradients of exponential1216numbers of negative examples in partition func-tion Z(x, ?).
However, beam search is used fortransition-based parsing, and no efficient optimaldynamic program is available to estimate Z(x, ?)accurately.
We adopt a novel contrastive learningapproach to approximately compute Z(x, ?
).3.3 Contrastive LearningAs an alternative to maximize the likelihood onsome observed data, contrastive learning (Hinton,2002; LeCun and Huang, 2005; Liang and Jordan,2008; Vickrey et al, 2010; Liu and Sun, 2014) isan approach that assigns higher probabilities to ob-served data and lower probabilities to noisy data.We adopt the contrastive learning approach, as-signing higher probabilities to the gold action se-quence compared to incorrect action sequences inthe beam.
Intuitively, this method only penalizesincorrect action sequences with high probabilities.Our new training objective is approximated as:L?(?)
= ??
(xi, yi)?
(X,Y )log p?
(yi| xi, ?)
(15)= ??
(xi, yi)?
(X,Y )logef(xi,?)iZ?
(xi, ?)(16)=?
(xi, yi)?
(X,Y )logZ?
(xi, ?)?
f(xi, ?)i(17)whereZ?
(x, ?)
=?yj?BEAM(x)ef(x, ?)j(18)p?
(yi| x, ?)
is the relative probability of the ac-tion sequence yi, computed over only the actionsequences in the beam.
Z?
(x, ?)
is the contrastiveapproximation of Z(x, ?).
BEAM(x) returns thepredicated action sequences in the beam and thegold action sequence.We assume that the probability mass concen-trates on a relatively small number of action se-quences, which allows the use of a limited num-ber of probable sequences to approximate the fullset of action sequences.
The concentration may beenlarged dramatically with an exponential activa-tion function of the neural network (i.e.
a > b ?ea?
eb).3.4 The Neural ProbabilisticStructured-Prediction FrameworkWe follow Zhang and Clark (2011) to integratesearch and learning.
Our search and learningAlgorithm 1: Training Algorithm for Struc-tured Neural ParsingInput: training examples (X, Y)Output: ???
pretrained embeddingfor i?
1 to N dox, y = RANDOMSAMPLE(X, Y)?
= 0foreach xj, yj?
x, y dobeam = ?goldState = nullterminate = falsebeamGold = truewhile beamGold and not terminatedobeam = DECODE(beam, xj, yj)goldState =GOLDMOVE(goldState, xj, yj)if not ISGOLD(beam) thenbeamGold = falseif ITEMSCOMPLETE(beam) thenterminate = true;?
= ?
+ UPDATE(goldState, beam)?
= ?
+ deltaframework for dependency parsing is shown asAlgorithm 1.
In every training iteration i, werandomly sample the training instances, and per-form online learning with early update (Collinsand Roark, 2004).
In particular, given a trainingexample, we use beam-search to decode the sen-tence.
At any step, if the gold action sequence fallsout of the beam, we take all the incorrect actionsequences in the beam as negative examples, andthe current gold sequence as a positive examplefor parameter update, using the training algorithmof Section 3.3.
AdaGrad algorithm (Duchi et al,2011) with mini-batch is adopted for optimization.In this way, the distribution of ot only full ac-tion sequences (i.e.
complete parse trees), but alsopartial action sequences (i.e.
partial outputs) aremodeled, which makes training more challenging.The advantage of early update is that training isused to guide search, minimizing search errors.12174 Experiments4.1 Set-upOur experiments are performed using the EnglishPenn Treebank (PTB; Marcus et al, (1993)).
Wefollow the standard splits of PTB3, using sections2-21 for training, section 22 for development test-ing and section 23 for final testing.
For compar-ison with previous work, we use Penn2Malt1toconvert constituent trees to dependency trees.
Weuse the POS-tagger of Collins (2002) to assignPOS automatically.
10-fold jackknifing is per-formed for tagging the training data.We follow Chen and Manning (2014), and usethe set of pre-trained word embeddings2fromCollobert et al (2011) with a dictionary size of13,000.
The word embeddings were trained on theentire English Wikipedia, which contains about631 million words.4.2 WSJ Experiments4.2.1 Development experimentsWe set the following hyper-parameters accordingto the baseline greedy neural parser (Chen andManning, 2014): embedding size d = 50, hiddenlayer size dh= 200, regularization parameter ?
=10?8, initial learning rate of Adagrad ?
= 0.01.For the structured neural parser, beam size andmini-batch size are important to the parsing per-formance.
We tune them on the development set.Beam size.
Beam search enlarges the searchspace.
More importantly, the larger the beam is,the more accurate our training algorithm is.
theContrastive learning approximates the exact prob-abilities over exponential many action sequencesby computing the relative probabilities over actionsequences in the beam (Equation 18).
Therefore,the larger the beam is, the more accurate the rela-tive probability is.The first column of Table 3 shows the accura-cies of the structured neural parser on the devel-opment set with different beam sizes, which im-proves as the beam size increases.
We set the finalbeam size as 100 according to the accuracies ondevelopment set.The effect of integrating search and learning.We also conduct experiments on the parser of1http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html2http://ronan.collobert.com/senna/Description UASBaseline 91.63structured greedybeam = 1 74.90 91.63beam = 4 84.64 91.92beam = 16 91.53 91.90beam = 64 93.12 91.84beam = 100 93.23 91.81Table 3: Accuracies of structured neural parsingand local neural classification parsing with differ-ent beam sizes.Description UASgreedy neural parser 91.47ranking model 89.08beam contrastive learning 93.28Table 4: Comparison between sentence-level log-likelihood and ranking model.Chen and Manning (2014) with beam search de-coding.
The score of a whole action sequenceis computed by the sum of log action probabili-ties (Equation 7).
As shown in the second col-umn of Table 3, beam search can improve pars-ing slightly.
When the beam size increases beyond16, however, accuracy improvements stop.
In con-trast, by integrating beam search and global learn-ing, our parsing performance benefits from largebeam sizes much more significantly.
With a beamsize as 16, the structured neural parser gives anaccuracy close to that of baseline greedy parser3.When the beam size is 100, the structured neuralparser outperforms baseline by 1.6%.Zhang and Nivre (2012) find that global learn-ing and beam search should be used jointly forimproving parsing using a linear transition-basedmodel.
In particular, increasing the beam size,the accuracy of ZPar (Zhang and Nivre, 2011) in-creases significantly, but that of MaltParser doesnot.
For structured neural parsing, our finding issimilar: integrating search and learning is muchmore effective than using beam search only in de-coding.Our results in Table 3 are obtained by usingthe same beam sizes for both training and testing.Zhang and Nivre (2012) also find that for their lin-3Our baseline accuracy is a little lower than accuracy re-ported in baseline paper (Chen and Manning, 2014), becausewe use Penn2Malt to convert the Penn Treebank, and they useLTH Conversion.12180 1000 2000 3000 4000 5000102030405060708090100iterationUASbatch size = 1batch size = 10batch size = 1000batch size = 2000batch size = 5000Figure 2: Parsing performance with differenttraining batch sizes.ear model, the best results are achieved by usingthe same beam sizes during training and testing.We find that this observation does not apply to ourneural parser.
In our case, a large training beam al-ways leads to better results.
This is likely becausea large beam improves contrastive learning.
As aresult, our training beam size is set to 100 for thefinal test.Batch size.
Parsing performance using neuralnetworks is highly sensitive to the batch size oftraining.
In greedy neural parsing (Chen and Man-ning, 2014), the accuracy on the development dataimproves from 85% to 91% by setting the batchsize to 10 and 100000, respectively.
In structuredneural parsing, we fix the beam size as 100 anddraw the accuracies on the development set by thetraining iteration.As shown in Figure 2, in 5000 training itera-tions, the parsing accuracies improve as the itera-tion grows, yet different batch sizes result in dif-ferent convergence accuracies.
With a batch sizeof 5000, the parsing accuracy is about 25% higherthan with a batch size of 1 (i.e.
SGD).
For the re-maining experiments, we set batch size to 5000,which achieves the best accuracies on develop-ment testing.4.2.2 Sentence-level maximum likelihood vs.ranking modelWe compare parsing accuracies of the sentence-level log-likelihood + beam contrastive learning(Section 3.2), and the structured neural parser withprobabilistic ranking (Section 3.1).
As shownin Table 4, performance of global learning withranking model is weaker than the baseline greedySystem UAS LAS Speedbaseline greedy parser 91.47 90.43 0.001Huang and Sagae (2010) 92.10 0.04Zhang and Nivre (2011) 92.90 91.80 0.03Choi and McCallum (2013) 92.96 91.93 0.009Ma et al (2014) 93.06Bohnet and Nivre (2012)??
93.67 92.68 0.4Suzuki et al (2009)?
93.79Koo et al (2008)?
93.16Chen et al (2014)?
93.77beam sizetraining decoding100 100 93.28 92.35 0.07100 64 93.20 92.27 0.04100 16 92.40 91.95 0.01Table 5: Results on WSJ.
Speed: sentences persecond.
?
: semi-supervised learning.
?
: jointPOS-tagging and dependency parsing models.parser.
In contrast, structured neural parsingwith sentence-level log-likelihood and contrastivelearning gives a 1.8% accuracy improvement uponthe baseline greedy parser.As mentioned in Section 3.1, a likely reasonfor the poor performance of the structured neu-ral ranking model may be that, the likelihoods ofaction sequences are highly influenced by eachother, due to the dense parameter space of neuralnetworks.
To maximize likelihood of gold actionsequence, we need to decrease the likelihoods ofmore than one incorrect action sequences.4.2.3 Final ResultsTable 5 shows the results of our final parser anda line of transition-based parsers on the test set.Our structured neural parser achieves an accu-racy of 93.28%, 0.38% higher than Zhang andNivre (2011), which employees millions of high-order binary indicator features in parsing.
Themodel size of ZPar (Zhang and Nivre, 2011) isover 250 MB on disk.
In contrast, the model sizeof our structured neural parser is only 25 MB.
Toour knowledge, the result is the best reported re-sult achieved by shift-reduce parsers on this dataset.Bohnet and Nivre (2012) obtain an accuracy of93.67%, which is higher than our parser.
How-ever, their parser is a joint model of parsing andPOS-tagging, and they use external data in pars-ing.
We also list the result of Chen et al (2014),Koo et al (2008) and Suzuki et al (2009) inTable 5, which make use of large-scale unanno-tated text to improve parsing accuracies.
Theinput embeddings of our parser are also trained1219over large raw text, and in this perspective ourmodel is correlated with the semi-supervised mod-els.
However, because we fine-tune the word em-beddings in supervised training, the embeddingsof in-vocabulary words become systematically dif-ferent from these of out-of-vocabulary words af-ter training, and the effect of pre-trained out-of-vocabulary embeddings become uncertain.
In thissense, our model can also be regarded as an al-most fully supervised model.
The same applies tothe models of Chen and Manning (2014).We also compare the speed of the structuredneural parser on an Intel Core i7 3.40GHz CPUwith 16GB RAM.
The structured neural parserruns about as fast as Zhang and Nivre (Zhang andNivre, 2011) and Huang and Sagae (Huang andSagae, 2010).
The results show that our parsercombines the benefits of structured models andneural probabilistic models, offering high accura-cies, fast speed and slim model size.5 Related WorkParsing with neural networks.
A line of workhas been proposed to explore the effect of neu-ral network models for constituent parsing (Hen-derson, 2004; Mayberry III and Miikkulainen,2005; Collobert, 2011; Socher et al, 2013;Legrand and Collobert, 2014).
Performances ofmost of these methods are still well below thestate-of-the-art, except for Socher et al(2013),who propose a neural reranker based on a PCFGparser.
For transition-based dependency parsing,Stenetorp (2013) applies a compositional vectormethod (Socher et al, 2013), and Chen and Man-ning (2014) propose a feed-forward neural parser.The performances of these neural parsers lag be-hind the state-of-the-art.More recently, Dyer et al (2015) propose agreedy transition-based dependency parser, usingthree stack LSTMs to represent the input, the stackof partial syntactic trees and the history of parseactions, respectively.
By modeling more history,the parser gives significant better accuracies com-pared to the greedy neural parser of Chen andManning (2014).Structured neural models.
Collobert etal.
(2011) presents a unified neural networkarchitecture for various natural language pro-cessing (NLP) tasks.
They propose to usesentence-level log-likelihood to enhance a neuralprobabilistic model, which inspires our model.Sequence labeling is used for graph-based de-coding.
Using the Viterbi algorithm, they cancompute the exponential partition function inlinear time without approximation.
However, witha dynamic programming decoder, their sequencelabeling model can only extract local features.
Incontrast, our integrated approximated search andlearning framework allows rich global features.Weiss et al (2015) also propose a structuredneural transition-based parser by adopting beamsearch and early updates.
Their model is closein spirit to ours in performing structured predic-tion using a neural network.
The main differenceis that their structured neural parser uses a greedyparsing process for pre-training, and fine-tunesan additional perceptron layer consisting of thepre-trained hidden and output layers using struc-tured perceptron updates.
Their structured neuralparser achieves an accuracy of 93.36% on Stan-ford conversion of the PTB, which is significanthigher than the baseline parser of Chen and Man-ning (2014).
Their results are not directly compa-rable with ours due to different dependency con-versions.6 ConclusionWe built a structured neural dependency parsingmodel.
Compared to the greedy neural parser ofChen and Manning (2014), our parser integratesbeam search and global contrastive learning.
Instandard PTB evaluation, our parser achieved a1.8% accuracy improvement over the parser ofChen and Manning (2014), which shows the effectof combining search and learning.
To our knowl-edge, the structured neural parser is the first neuralparser that outperforms the best linear shift-reducedependency parsers.
The structured neural proba-bilistic framework can be used in other incremen-tal structured prediction tasks.7 AcknowledgmentsWe would like to thank the anonymous review-ers for their insightful comments.
This workwas partially founded by the Natural ScienceFoundation of China (61170181, 61300158), theJiangsu Provincial Research Foundation for Ba-sic Research (BK20130580), Singapore Ministra-try of Education Tier 2 Grant T2MOE201301 andSRGISTD2012038 from Singapore University ofTechnology and Design.1220ReferencesBernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1455?1465.
Association for Computational Linguis-tics.Danqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Empirical Methods in Natural LanguageProcessing (EMNLP).Wenliang Chen, Yue Zhang, and Min Zhang.
2014.Feature embedding for dependency parsing.
In Pro-ceedings of the international conference on Compu-tational linguistics.
Association for ComputationalLinguistics.Jinho D Choi and Andrew McCallum.
2013.Transition-based dependency parsing with selec-tional branching.
In ACL (1), pages 1052?1062.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Pro-ceedings of the 42nd Annual Meeting on Associationfor Computational Linguistics, page 111.
Associa-tion for Computational Linguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing-Volume 10, pages 1?8.Association for Computational Linguistics.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.R.
Collobert.
2011.
Deep learning for efficient dis-criminative parsing.
In AISTATS.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-term memory.
In Proceedings of the ACL confer-ence.
Association for Computational Linguistics.Yoav Goldberg and Joakim Nivre.
2013.
Trainingdeterministic parsers with non-deterministic oracles.Transactions of the Association for ComputationalLinguistics, 1:403?414.James Henderson.
2004.
Discriminative training of aneural network statistical parser.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, page 95.
Association for Com-putational Linguistics.Geoffrey Hinton.
2002.
Training products of expertsby minimizing contrastive divergence.
Neural com-putation, 14(8):1771?1800.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InProceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1077?1086.
Association for Computational Linguistics.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.Yann LeCun and F Huang.
2005.
Loss functionsfor discriminative training of energybased models.AIStats.J.
Legrand and R. Collobert.
2014.
Recurrent greedyparsing with neural networks.
In Proceedings ofthe European Conference on Machine Learning andPrinciples and Practice of Knowledge Discovery inDatabases (ECML-PKDD).Percy Liang and Michael I Jordan.
2008.
Anasymptotic analysis of generative, discriminative,and pseudolikelihood estimators.
In Proceedings ofthe 25th international conference on Machine learn-ing, pages 584?591.
ACM.Yang Liu and Maosong Sun.
2014.
Contrastive un-supervised word alignment with non-local features.arXiv preprint arXiv:1410.2082.Ji Ma, Yue Zhang, and Jingbo Zhu.
2014.
Punctu-ation processing for projective dependency parsing.In Proc.
of ACL.Mitchell P Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Compu-tational linguistics, 19(2):313?330.Marshall R Mayberry III and Risto Miikkulainen.2005.
Broad-coverage parsing with neural net-works.
Neural Processing Letters, 21(2):121?132.Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of English text.
In Proceedingsof the 20th International Conference on Computa-tional Linguistics (COLING), pages 64?70.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
Computational Lin-guistics, 34(4):513?553.1221Richard Socher, John Bauer, Christopher D Manning,and Andrew Y Ng.
2013.
Parsing with composi-tional vector grammars.
In In Proceedings of theACL conference.
Citeseer.Pontus Stenetorp.
2013.
Transition-based dependencyparsing using recursive neural networks.
In NIPSWorkshop on Deep Learning.Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichael Collins.
2009.
An empirical study of semi-supervised structured conditional models for depen-dency parsing.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 2-Volume 2, pages 551?560.Association for Computational Linguistics.David Vickrey, Cliff C Lin, and Daphne Koller.
2010.Non-local contrastive objectives.
In Proceedingsof the 27th International Conference on MachineLearning (ICML-10), pages 1103?1110.DavidWeiss, Christopher Alberti, Michael Collins, andSlav Petrov.
2015.
Structured training for neuralnetwork transition-based parsing.
In Proceedings ofthe ACL conference.
Association for ComputationalLinguistics.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proceedings of IWPT, volume 3.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-based and transition-based dependency parsing us-ing beam-search.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 562?571.
Association for Computa-tional Linguistics.Yue Zhang and Stephen Clark.
2011.
Syntactic pro-cessing using the generalized perceptron and beamsearch.
Computational Linguistics, 37(1):105?151.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: short papers-Volume 2, pages188?193.
Association for Computational Linguis-tics.Yue Zhang and Joakim Nivre.
2012.
Analyzingthe effect of global learning and beam-search ontransition-based dependency parsing.
In COLING(Posters), pages 1391?1400.1222
