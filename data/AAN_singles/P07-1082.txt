Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 648?655,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsCollapsed Consonant and Vowel Models: New Approaches forEnglish-Persian Transliteration and Back-TransliterationSarvnaz Karimi Falk Scholer Andrew TurpinSchool of Computer Science and Information TechnologyRMIT University, GPO Box 2476V, Melbourne 3001, Australia{sarvnaz,fscholer,aht}@cs.rmit.edu.auAbstractWe propose a novel algorithm for Englishto Persian transliteration.
Previous meth-ods proposed for this language pair applya word alignment tool for training.
Bycontrast, we introduce an alignment algo-rithm particularly designed for translitera-tion.
Our new model improves the Englishto Persian transliteration accuracy by 14%over an n-gram baseline.
We also proposea novel back-transliteration method for thislanguage pair, a previously unstudied prob-lem.
Experimental results demonstrate thatour algorithm leads to an absolute improve-ment of 25% over standard transliterationapproaches.1 IntroductionTranslation of a text from a source language toa target language requires dealing with technicalterms and proper names.
These occur in almostany text, but rarely appear in bilingual dictionar-ies.
The solution is the transliteration of such out-of-dictionary terms: a word from the source languageis transformed to a word in the target language, pre-serving its pronunciation.
Recovering the originalword from the transliterated target is called back-transliteration.
Automatic transliteration is impor-tant for many different applications, including ma-chine translation, cross-lingual information retrievaland cross-lingual question answering.Transliteration methods can be categorized intographeme-based (AbdulJaleel and Larkey, 2003; Liet al, 2004), phoneme-based (Knight and Graehl,1998; Jung et al, 2000), and combined (Bilac andTanaka, 2005) approaches.
Grapheme-based meth-ods perform a direct orthographical mapping be-tween source and target words, while phoneme-based approaches use an intermediate phonetic rep-resentation.
Both grapheme- or phoneme-basedmethods usually begin by breaking the source wordinto segments, and then use a source segment to tar-get segment mapping to generate the target word.The rules of this mapping are obtained by aligningalready available transliterated word pairs (trainingdata); alternatively, such rules can be handcrafted.From this perspective, past work is roughly dividedinto those methods which apply a word alignmenttool such as GIZA++ (Och and Ney, 2003), and ap-proaches that combine the alignment step into theirmain transliteration process.Transliteration is language dependent, and meth-ods that are effective for one language pair maynot work as well for another.
In this paper, weinvestigate the English-Persian transliteration prob-lem.
Persian (Farsi) is an Indo-European language,written in Arabic script from right to left, but withan extended alphabet and different pronunciationfrom Arabic.
Our previous approach to English-Persian transliteration introduced the grapheme-based collapsed-vowel method, employing GIZA++for source to target algnment (Karimi et al, 2006).We propose a new transliteration approach that ex-tends the collapsed-vowel method.
To meet Per-sian language transliteration requirements, we alsopropose a novel alignment algorithm in our trainingstage, which makes use of statistical information of648the corpus, transliteration specifications, and simplelanguage properties.
This approach handles possi-ble consequences of elision (omission of sounds tomake the word easier to read) and epenthesis (addingextra sounds to a word to make it fluent) in writtentarget words that happen due to the change of lan-guage.
Our method shows an absolute accuracy im-provement of 14.2% over an n-gram baseline.In addition, we investigate the problem of back-transliteration from Persian to English.
To ourknowledge, this is the first report of such a study.There are two challenges in Persian to Englishtransliteration that makes it particularly difficult.First, written Persian omits short vowels, while onlylong vowels appear in texts.
Second, monophthon-gization (changing diphthongs to monophthongs) ispopular among Persian speakers when adapting for-eign words into their language.
To take these intoaccount, we propose a novel method to form trans-formation rules by changing the normal segmenta-tion algorithm.
We find that this method signifi-cantly improves the Persian to English translitera-tion effectiveness, demonstrating an absolute perfor-mance gain of 25.1% over standard transliterationapproaches.2 BackgroundIn general, transliteration consists of a training stage(running on a bilingual training corpus), and a gen-eration ?
also called testing ?
stage.The training step of a transliteration developstransformation rules mapping characters in thesource to characters in the target language usingknowledge of corresponding characters in translit-erated pairs provided by an alignment.
For example,for the source-target word pair (pat,HH), an align-ment may map ?p?
to ?H?
and ?a?
to ?
?, and thetraining stage may develop the rule pa ?
, with ?
?as the transliteration of ?a?
in the context of ?pa?.The generation stage applies these rules on a seg-mented source word, transforming it to a word inthe target language.Previous work on transliteration either employs aword alignment tool (usually GIZA++), or developsspecific alignment strategies.
Transliteration meth-ods that use GIZA++ as their word pair aligner (Ab-dulJaleel and Larkey, 2003; Virga and Khudanpur,2003; Karimi et al, 2006) have based their work onthe assumption that the provided alignments are re-liable.
Gao et al (2004) argue that precise align-ment can improve transliteration effectiveness, ex-perimenting on English-Chinese data and compar-ing IBM models (Brown et al, 1993) with phoneme-based alignments using direct probabilities.Other transliteration systems focus on alignmentfor transliteration, for example the joint source-channel model suggested by Li et al (2004).
Theirmethod outperforms the noisy channel model indirect orthographical mapping for English-Chinesetransliteration.
Li et al also find that grapheme-based methods that use the joint source-channelmodel are more effective than phoneme-based meth-ods due to removing the intermediate phonetictransformation step.
Alignment has also been in-vestigated for transliteration by adopting Coving-ton?s algorithm on cognate identification (Coving-ton, 1996); this is a character alignment algorithmbased on matching or skipping of characters, witha manually assigned cost of association.
Coving-ton considers consonant to consonant and vowel tovowel correspondence more valid than consonant tovowel.
Kang and Choi (2000) revise this method fortransliteration where a skip is defined as inserting anull in the target string when two characters do notmatch based on their phonetic similarities or theirconsonant and vowel nature.
Oh and Choi (2002)revise this method by introducing binding, in whichmany to many correspondences are allowed.
How-ever, all of these approaches rely on the manuallyassigned penalties that need to be defined for eachpossible matching.In addition, some recent studies investigate dis-criminative transliteration methods (Klementiev andRoth, 2006; Zelenko and Aone, 2006) in which eachsegment of the source can be aligned to each seg-ment of the target, where some restrictive conditionsbased on the distance of the segments and phoneticsimilarities are applied.3 The Proposed Alignment ApproachWe propose an alignment method based on segmentoccurrence frequencies, thereby avoiding predefinedmatching patterns and penalty assignments.
We alsoapply the observed tendency of aligning consonants649to consonants, and vowels to vowels, as a substi-tute for phonetic similarities.
Many to many, one tomany, one to null and many to one alignments canbe generated.3.1 FormulationOur alignment approach consists of two steps: thefirst is based on the consonant and vowel natureof the word?s letters, while the second uses afrequency-based sequential search.Definition 1 A bilingual corpus B is the set{(S, T )}, where S = s1..s?, T = t1..tm, si is aletter in the source language alphabet, and tj is aletter in the target language alphabet.Definition 2 Given some word, w, the consonant-vowel sequence p = (C|V )+ for w is obtainedby replacing each consonant with C and each vowelwith V .Definition 3 Given some consonant-vowel se-quence, p, a reduced consonant-vowel sequence qreplaces all runs of C?s with C, and all runs of V ?swith V; hence q = q?|q?
?, q?
= V(CV)?(C|?
)and q??
= C(VC)?(V|?
).For each natural language word, we can determinethe consonant-vowel sequence (p) from which thereduced consonant-vowel sequence (q) can be de-rived, giving a common notation between two dif-ferent languages, no matter which script either ofthem use.
To simplify, semi-vowels and approxi-mants (sounds intermediate between consonants andvowels, such as ?w?
and ?y?
in English) are treatedaccording to their target language counterparts.In general, for all the word pairs (S, T ) in a corpusB, an alignment can be achieved using the functionf : B ?
A; (S, T ) 7?
(S?, T?
, r).The function f maps the word pair (S, T ) ?
B tothe triple (S?, T?
, r) ?
A where S?
and T?
are sub-strings of S and T respectively.
The frequency ofthis correspondence is denoted by r. A represents aset of substring alignments, and we use a per wordalignment notation of ae2p when aligning English toPersian and ap2e for Persian to English.3.2 Algorithm DetailsOur algorithm consists of two steps.Step 1 (Consonant-Vowel based)For any word pair (S, T ) ?
B, the correspondingreduced consonant-vowel sequences, qS and qT , aregenerated.
If the sequences match, then the alignedconsonant clusters and vowel sequences are addedto the alignment set A.
If qS does not match withqT , the word pair remains unaligned in Step 1.The assumption in this step is that transliterationof each vowel sequence of the source is a vowel se-quence in the target language, and similarly for con-sonants.
However, consonants do not always map toconsonants, or vowels to vowels (for example, theEnglish letter ?s?
may be written as ???
in Persianwhich consists of one vowel and one consonant).
Al-ternatively, they might be omitted altogether, whichcan be specified as the null string, ?.
We thereforerequire a second step.Step 2 (Frequency based)For most natural languages, the maximum lengthof corresponding phonemes of each grapheme is adigraph (two letters) or at most a trigraph.
Hence,alignment can be defined as a search problem thatseeks for units with a maximum length of two orthree in both strings that need to be aligned.
In ourapproach, we search based on statistical occurrencedata available from Step 1.In Step 2, only those words that remain unalignedat the end of Step 1 need to be considered.
For eachpair of words (S, T ), matching proceeds from left toright, examining one of the three possible options oftransliteration: single letter to single letter, digraphto single letter and single letter to digraph.
Trigraphsare unnecessary in alignment as they can be effec-tively captured during transliteration generation, aswe explain below.We define four different valid alignments for thesource (S = s1s2 .
.
.
si .
.
.
sl) and target (T =t1t2 .
.
.
tj .
.
.
tm) strings: (si , tj , r), (sisi+1, tj , r),(si , tj tj+1, r) and (si , ?, r).
These four options areconsidered as the only possible valid alignments,and the most frequently occurring alignment (high-est r) is chosen.
These frequencies are dynamicallyupdated after successfully aligning a pair.
For ex-ceptional situations, where there is no character inthe target string to match with the source charactersi , it is aligned with the empty string.It is possible that none of the four valid alignment650options have occurred previously (that is, r = 0for each).
This situation can arise in two ways:first, such a tuple may simply not have occurred inthe training data; and, second, the previous align-ment in the current string pair may have been incor-rect.
To account for this second possibility, a par-tial backtracking is considered.
Most misalignmentsare derived from the simultaneous comparison ofalignment possibilities, giving the highest priority tothe most frequent.
For example if S=bbc, T=H.
?and A = {(b,H.,100),(bb,H.,40),(c,?,60)}, startingfrom the initial position s1 and t1 , the first alignmentchoice is (b,H.,101).
However immediately after, weface the problem of aligning the second ?b?.
Thereare two solutions: inserting ?
and adding the triple(b,?,1), or backtracking the previous alignment andsubstituting that with the less frequent but possiblealignment of (bb,H.,41).
The second solution is abetter choice as it adds less ambiguous alignmentscontaining ?.
At the end, the alignment set is up-dated as A = {(b,H.,100),(bb,H.,41),(c,?,61)}.In case of equal frequencies, we check possiblesubsequent alignments to decide on which align-ment should be chosen.
For example, if (b,H.,100)and (bb,H.,100) both exist as possible options, weconsider if choosing the former leads to a subse-quent ?
insertion.
If so, we opt for the latter.At the end of a string, if just one character in thetarget string remains unaligned while the last align-ment is a ?
insertion, that final alignment will be sub-stituted for ?.
This usually happens when the align-ment of final characters is not yet registered in thealignment set, mainly because Persian speakers tendto transliterate the final vowels to consonants to pre-serve their existence in the word.
For example, inthe word ?Jose?
the final ?e?
might be transliteratedto ?
??
which is a consonant (?h?)
and therefore is notcaptured in Step 1.BackparsingThe process of aligning words explained abovecan handle words with already known componentsin the alignment set A (the frequency of occurrenceis greater than zero).
However, when this is not thecase, the system may repeatedly insert ?
while partor all of the target characters are left intact (unsuc-cessful alignment).
In such cases, processing thesource and target backwards helps to find the prob-lematic substrings: backparsing.The poorly aligned substrings of the source andtarget are taken as new pairs of strings, which arethen reintroduced into the system as new entries.Note that they themselves are not subject to back-parsing.
Most strings of repeating nulls can be bro-ken up this way, and in the worst case will remain asone tuple in the alignment set.To clarify, consider the example given in Figure 1.For the word pair (patricia,HHP??
?), where anassociation between ?c?
and ?
??
is not yet regis-tered.
Forward parsing, as shown in the figure, doesnot resolve all target characters; after the incorrectalignment of ?c?
with ??
?, subsequent characters arealso aligned with null, and the substring ?
???
re-mains intact.
Backward parsing, shown in the nextline of the figure, is also not successful.
It is able tocorrectly align the last two characters of the string,before generating repeated null alignments.
There-fore, the central region ?
substrings of the sourceand target which remained unaligned plus one extraaligned segment to the left and right ?
is enteredas a new pair to the system (ici,?
??
), as shownin the line labelled Input 2 in the figure.
This newinput meets Step 1 requirements, and is aligned suc-cessfully.
The resulting tuples are then merged withthe alignment set A.An advantage of our backparsing strategy is thatit takes care of casual transliterations happening dueto elision and epenthesis (adding or removing ex-tra sounds).
It is not only in translation that peoplemay add extra words to make fluent target text; fortransliteration also, it is possible that spurious char-acters are introduced for fluency.
However, this of-ten follows patterns, such as adding vowels to thetarget form.
These irregularities are consistentlycovered in the backparsing strategy, where they re-main connected to their previous character.4 Transliteration MethodTransliteration algorithms use aligned data (the out-put from the alignment process, ae2p or ap2e align-ment tuples) for training to derive transformationrules.
These rules are then used to generate a tar-get word T given a new input source word S.651Initial alignment set:A = {(p,H,42),(a, ,320),(a,?,99),(a, ?,10),(a,?,35),(r,P,200),(i,?,60),(i,?,5),(c,?,80),(c,h,25),(t, H,51)}Input: (patricia,HHP??
?) qS = CVCVCV qT = CVCVStep 1: qS 6= qTForward alignment: (p,H,43), (a,?,100), (t, H,52), (r,P,201), (i,?,61), (c,?,1), (i,?,6), (a,?,100)Backward alignment: (a, ,321), (i,?,61), (c,?,1), (i,?,6), (r,?,1), (t,?,1), (a,?,100), (p,?,1)Input 2: (ici,?
??)
qS = VCV qT = VCVStep 1: (i,?,61),(c, ?,1), (i,?,61)Final Alignment: ae2p = ((p,H ),(a,?
),(t, H),((r,P),(i,?
),(c, ?),(i,?
),(a, ))Updated alignment set:A = {(p,H,43),(a, ,321),(a,?,100),(a, ?,10),(a,?,35),(r,P,201),(i,?,62),(i,?,5),(c,?,80),(c,h,25),(c, ?,1),(t, H,52)}Figure 1: A backparsing example.
Note middle tuples in forward and backward parsings are not merged inA till the alignment is successfully completed.Method Intermediate Sequence Segment(Pattern) BackoffBigram N/A #s, sh, he, el, ll, le, ey s,h,e,l,e,yCV-MODEL1 CCVCCV sh(CC), hel(CVC), ll(CC), lley(CV) s(C), h(C), e(V), l(C), e(V), y(V)CV-MODEL2 CCVCCV sh(CC), e(CVC), ll(CC), ey(CV) As Above.CV-MODEL3 CVCV #sh(C), e(CVC), ll(C), ey(CV) sh(C), s(C), h(C), e(V), l(C), e(V), y(V)Figure 2: An example of transliteration for the word pair (shelley, ???).
Underlined characters are actuallytransliterated for each segment.4.1 BaselineMost transliteration methods reported in the litera-ture ?
either grapheme- or phoneme-based ?
usen-grams (AbdulJaleel and Larkey, 2003; Jung et al,2000).
The n-gram-based methods differ mainly inthe way that words are segmented, both for train-ing and transliteration generation.
A simple n-gram based method works only on single charac-ters (unigram) and transformation rules are definedas si ?
tj , while an advanced method may takethe surrounding context into account (Jung et al,2000).
We found that using one past symbol (bigrammodel) works better than other n-gram based meth-ods for English to Persian transliteration (Karimi etal., 2006).Our collapsed-vowel methods consider languageknowledge to improve the string segmentation ofn-gram techniques (Karimi et al, 2006).
The pro-cess begins by generating the consonant-vowel se-quence (Definition 2) of a source word.
For ex-ample, the word ?shelley?
is represented by the se-quence p = CCV CCV V .
Then, following the col-lapsed vowel concept (Definition 3), this sequencebecomes ?CCVCCV?.
These approaches, whichwe refer to as CV-MODEL1 and CV-MODEL2 re-spectively, partition these sequences using basic pat-terns (C and V) and main patterns (CC , CVC , VCand CV).
In the training phase, transliteration rulesare formed according to the boundaries of the de-fined patterns and their aligned counterparts (basedon ae2p or ap2e) in the target language word T .
Simi-lar segmentation is applied during the transliterationgeneration stage.4.2 The Proposed Transliteration ApproachThe restriction on the context length of consonantsimposed by CV-MODEL1 and CV-MODEL2 makesthe transliteration of consecutive consonants map-ping to a particular character in the target languagedifficult.
For example, ?ght?
in English maps toonly one character in Persian: ?
H?.
Dealing withlanguages which have different alphabets, and forwhich the number of characters in their alphabetsalso differs (such as 26 and 32 for English and Per-sian), increases the possibility of facing these cases,especially when moving from the language withsmaller alphabet size to the one with a larger size.To more effectively address this, we propose a col-lapsed consonant and vowel method (CV-MODEL3)which uses the full reduced sequence (Definition 3),rather than simply reduced vowel sequences.
Al-though recognition of consonant segments is basedon the vowel positions, consonants are considered asindependent blocks in each string.
Conversely, vow-els are transliterated in the context of surrounding652consonants, as demonstrated in the example below.A special symbol is used to indicate the startand/or end of each word if the beginning and endof the word is a consonant respectively.
Therefore,for the words starting or ending with consonants, thesymbol ?#?
is added, which is treated as a consonantand therefore grouped in the consonant segment.An example of applying this technique is shown inFigure 2 for the string ?shelley?.
In this example,?sh?
and ?ll?
are treated as two consonant segments,where the transliteration of individual characters in-side a segment is dependent on the other membersbut not the surrounding segments.
However, this isnot the case for vowel sequences which incorporatea level of knowledge about any segment neighbours.Therefore, for the example ?shelley?, the first seg-ment is ?sh?
which belongs to C pattern.
Duringtransliteration, if ?#sh?
does not appear in any ex-isting rules, a backoff splits the segment to smallersegments: ?#?
and ?sh?, or ?s?and ?h?.
The secondsegment contains the vowel ?e?.
Since this vowelis surrounded by consonants, the segment pattern isCVC.
In this case, backoff only applies for vowels asconsonants are supposed to be part of their own in-dependent segments.
That is, if search in the rules ofpattern CVC was unsuccessful, it looks for ?e?
in Vpattern.
Similarly, segmentation for this word con-tinues with ?ll?
in C pattern and ?ey?
in CV pattern(?y?
is an approximant, and therefore considered asa vowel when transliterating English to Persian).4.3 Rules for Back-TransliterationWritten Persian ignores short vowels, and only longvowels appear in text.
This causes most Englishvowels to disappear when transliterating from En-glish to Persian; hence, these vowels must be re-stored during back-transliteration.When the initial transliteration happens from En-glish to Persian, the transliterator (whether hu-man or machine) uses the rules of transliterat-ing from English as the source language.
There-fore, transliterating back to the original languageshould consider the original process, to avoid los-ing essential information.
In terms of segmenta-tion in collapsed-vowel models, different patternsdefine segment boundaries in which vowels arenecessary clues.
Although we do not have mostof these vowels in the transliteration generationphase, it is possible to benefit from their existencein the training phase.
For example, using CV-MODEL3, the pair (P?
?,merkel) with qS =C andap2e=((,me),(P,r),(?,ke),(?,l)), produces just onetransformation rule ?
P??
?
merkel?
based on aC pattern.
That is, the Persian string contains novowel characters.
If, during the transliteration gen-eration phase, a source word ???Q??
(S=P??)
isentered, there would be one and only one outputof ?merkel?, while an alternative such as ?mercle?might be required instead.
To avoid overfitting thesystem by long consonant clusters, we perform seg-mentation based on the English q sequence, but cate-gorise the rules based on their Persian segment coun-terparts.
That is, for the pair (P?
?,merkel) withae2p=((m,),(e,?),(r,P),(k,?),(e,?),(l,?
)), these rulesare generated (with category patterns given in paren-thesis):  ?
m (C), P?
?
rk (C), ?
?
l (C), P??
merk (C), P??
?
rkel (C).
We call the suggestedtraining approach reverse segmentation.Reverse segmentation avoids clustering all theconsonants in one rule, since many English wordsmight be transliterated to all-consonant Persianwords.4.4 Transliteration Generation and RankingIn the transliteration generation stage, the sourceword is segmented following the same process ofsegmenting words in training stage, and a probabil-ity is computed for each generated target word:P (T |S) =|K|Yk=1P (T?k|S?k),where |K| is the number of distinct source seg-ments.
P (T?k|S?k) is the probability of the S?k?T?ktransformation rule, as obtained from the trainingstage:P (T?k|S?k) =frequency of S?k ?
T?kfrequency of S?k,where frequency of S?k is the number of its oc-currence in the transformation rules.
We apply atree structure, following Dijkstra?s ?-shortest path,to generate the ?
highest scoring (most probable)transliterations, ranked based on their probabilities.653Corpus Baseline CV-MODEL3Bigram CV-MODEL1 CV-MODEL2 GIZA++ New AlignmentSmall CorpusTOP-1 58.0 (2.2) 61.7 (3.0) 60.0 (3.9) 67.4 (5.5) 72.2 (2.2)TOP-5 85.6 (3.4) 80.9 (2.2) 86.0 (2.8) 90.9 (2.1) 92.9 (1.6)TOP-10 89.4 (2.9) 82.0 (2.1) 91.2 (2.5) 93.8 (2.1) 93.5 (1.7)Large CorpusTOP-1 47.2 (1.0) 50.6 (2.5) 47.4 (1.0) 55.3 (0.8) 59.8 (1.1)TOP-5 77.6 (1.4) 79.8 (3.4) 79.2 (1.0) 84.5 (0.7) 85.4 (0.8)TOP-10 83.3 (1.5) 84.9 (3.1) 87.0 (0.9) 89.5 (0.4) 92.6 (0.7)Table 1: Mean (standard deviation) word accuracy (%) for English to Persian transliteration.5 ExperimentsTo investigate the effectiveness of CV-MODEL3 andthe new alignment approach on transliteration, wefirst compare CV-MODEL3 with baseline systems,employing GIZA++ for alignment generation duringsystem training.
We then evaluate the same sys-tems, using our new alignment approach.
Back-transliteration is also investigated, applying bothalignment systems and reverse segmentation.
In allour experiments, we used ten-fold cross-validation.The statistical significance of different performancelevels are evaluated using a paired t-test.
The no-tation TOP-X indicates the first X transliterationsprodcued by the automatic methods.We used two corpora of word pairs in Englishand Persian: the first, called Large, contains 16,670word pairs; the second, Small, contains 1,857 wordpairs, and are described fully in our previous paper(Karimi et al, 2006).The results of transliteration experiments are eval-uated using word accuracy (Kang and Choi, 2000)which measures the proportion of transliterationsthat are correct out of the test corpus.5.1 Accuracy of Transliteration ApproachesThe results of our experiments for transliterating En-glish to Persian, using GIZA++ for alignment gen-eration, are shown in Table 1.
CV-MODEL3 out-performs all three baseline systems significantly inTOP-1 and TOP-5 results, for both Persian corpora.TOP-1 results were improved by 9.2% to 16.2%(p<0.0001, paired t-test) relative to the baseline sys-tems for the Small corpus.
For the Large corpus,CV-MODEL3 was 9.3% to 17.2% (p<0.0001) moreaccurate relative to the baseline systems.The results of applying our new alignment al-gorithm are presented in the last column of Ta-ble 1, comparing word accuracy of CV-MODEL3 us-ing GIZA++ and the new alignment for English toPersian transliteration.
Transliteration accuracy in-creases in TOP-1 for both corpora (a relative increaseof 7.1% (p=0.002) for the Small corpus and 8.1%(p<0.0001) for the Large corpus).
The TOP-10 re-sults of the Large corpus again show a relative in-crease of 3.5% (p=0.004).
Although the new align-ment also increases the performance for TOP-5 andTOP-10 of the Small corpus, these increases are notstatistically significant.5.2 Accuracy of Back-TransliterationThe results of back-transliteration are shown in Ta-ble 2.
We first consider performance improvementsgained from using CV-MODEL3: CV-MODEL3 usingGIZA++ outperforms Bigram, CV-MODEL1 and CV-MODEL2 by 12.8% to 40.7% (p<0.0001) in TOP-1 for the Small corpus.
The corresponding im-provement for the Large corpus is 12.8% to 74.2%(p<0.0001).The fifth column of the table shows the perfor-mance increase when using CV-MODEL3 with thenew alignment algorithm: for the Large corpus, thenew alignment approach gives a relative increase inaccuracy of 15.5% for TOP-5 (p<0.0001) and 10%for TOP-10 (p=0.005).
The new alignment methoddoes not show a significant difference using CV-MODEL3 for the Small corpus.The final column of Table 2 shows the perfor-mance of the CV-MODEL3 with the new reverse seg-mentation approach.
Reverse segmentation leads toa significant improvement over the new alignmentapproach in TOP-1 results for the Small corpus by40.1% (p<0.0001), and 49.4% (p<0.0001) for theLarge corpus.654Corpus Bigram CV-MODEL1 CV-MODEL2 CV-MODEL3GIZA++ New Alignment ReverseSmall CorpusTOP-1 23.1 (2.0) 28.8 (4.6) 24.9 (2.8) 32.5 (3.6) 34.4 (3.8) 48.2 (2.9)TOP-5 40.8 (3.1) 51.0 (4.8) 52.9 (3.4) 56.0 (3.5) 54.8 (3.7) 68.1 (4.9)TOP-10 50.1 (4.1) 58.2 (5.3) 63.2 (3.1) 64.2 (3.2) 63.8 (3.6) 75.7 (4.2)Large CorpusTOP-1 10.1 (0.6) 15.6 (1.0) 12.0 (1.0) 17.6 (0.8) 18.0 (1.2) 26.9 (0.7)TOP-5 20.6 (1.2) 31.7 (0.9) 28.0 (0.7) 36.2 (0.5) 41.8 (1.2) 41.3 (1.7)TOP-10 27.2 (1.0) 40.1 (1.1) 37.4 (0.8) 46.0 (0.8) 50.6 (1.1) 49.3 (1.6)Table 2: Comparison of mean (standard deviation) word accuracy (%) for Persian to English transliteration.6 ConclusionsWe have presented a new algorithm for English toPersian transliteration, and a novel alignment al-gorithm applicable for transliteration.
Our newtransliteration method (CV-MODEL3) outperformsthe previous approaches for English to Persian, in-creasing word accuracy by a relative 9.2% to 17.2%(TOP-1), when using GIZA++ for alignment in train-ing.
This method shows further 7.1% to 8.1% in-crease in word accuracy (TOP-1) with our new align-ment algorithm.Persian to English back-transliteration is also in-vestigated, with CV-MODEL3 significantly outper-forming other methods.
Enriching this model witha new reverse segmentation algorithm gives rise tofurther accuracy gains in comparison to directly ap-plying English to Persian methods.In future work we will investigate whether pho-netic information can help refine our CV-MODEL3,and experiment with manually constructed rules asa baseline system.AcknowledgmentsThis work was supported in part by the Australiangovernment IPRS program (SK) and an ARC Dis-covery Project Grant (AT).ReferencesNasreen AbdulJaleel and Leah S. Larkey.
2003.
Statisticaltransliteration for English-Arabic cross language informa-tion retrieval.
In Conference on Information and KnowledgeManagement, pages 139?146.Slaven Bilac and Hozumi Tanaka.
2005.
Direct combinationof spelling and pronunciation information for robust back-transliteration.
In Conferences on Computational Linguis-tics and Intelligent Text Processing, pages 413?424.Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra,and Robert L. Mercer.
1993.
The mathematics of statisti-cal machine translation: Parameter estimation.
ComputionalLinguistics, 19(2):263?311.Michael A. Covington.
1996.
An algorithm to alignwords for historical comparison.
Computational Linguistics,22(4):481?496.Wei Gao, Kam-Fai Wong, and Wai Lam.
2004.
Improvingtransliteration with precise alignment of phoneme chunksand using contextual features.
In Asia Information RetrievalSymposium, pages 106?117.Sung Young Jung, Sung Lim Hong, and Eunok Paek.
2000.
AnEnglish to Korean transliteration model of extended Markovwindow.
In Conference on Computational Linguistics, pages383?389.Byung-Ju Kang and Key-Sun Choi.
2000.
Automatic translit-eration and back-transliteration by decision tree learning.
InConference on Language Resources and Evaluation, pages1135?1411.Sarvnaz Karimi, Andrew Turpin, and Falk Scholer.
2006.
En-glish to Persian transliteration.
In String Processing and In-formation Retrieval, pages 255?266.Alexandre Klementiev and Dan Roth.
2006.
Weakly super-vised named entity transliteration and discovery from mul-tilingual comparable corpora.
In Association for Computa-tional Linguistics, pages 817?824.Kevin Knight and Jonathan Graehl.
1998.
Machine translitera-tion.
Computational Linguistics, 24(4):599?612.Haizhou Li, Min Zhang, and Jian Su.
2004.
A joint source-channel model for machine transliteration.
In Associationfor Computational Linguistics, pages 159?166.Franz Josef Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.Jong-Hoon Oh and Key-Sun Choi.
2002.
An English-Koreantransliteration model using pronunciation and contextualrules.
In Conference on Computational Linguistics.Paola Virga and Sanjeev Khudanpur.
2003.
Transliteration ofproper names in cross-language applications.
In ACM SIGIRConference on Research and Development on InformationRetrieval, pages 365?366.Dmitry Zelenko and Chinatsu Aone.
2006.
Discriminativemethods for transliteration.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural Language Process-ing., pages 612?617.655
