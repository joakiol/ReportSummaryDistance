Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156?164,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAutomatic Selection of High Quality Parses Created By a FullyUnsupervised ParserRoi ReichartICNCThe Hebrew Universityroiri@cs.huji.ac.ilAri RappoportInstitute of computer scienceThe Hebrew Universityarir@cs.huji.ac.ilAbstractThe average results obtained by unsupervisedstatistical parsers have greatly improved in thelast few years, but on many specific sentencesthey are of rather low quality.
The output ofsuch parsers is becoming valuable for vari-ous applications, and it is radically less expen-sive to create than manually annotated trainingdata.
Hence, automatic selection of high qual-ity parses created by unsupervised parsers isan important problem.In this paper we present PUPA, a POS-basedUnsupervised Parse Assessment algorithm.The algorithm assesses the quality of a parsetree using POS sequence statistics collectedfrom a batch of parsed sentences.
We eval-uate the algorithm by using an unsupervisedPOS tagger and an unsupervised parser, se-lecting high quality parsed sentences from En-glish (WSJ) and German (NEGRA) corpora.We show that PUPA outperforms the leadingprevious parse assessment algorithm for su-pervised parsers, as well as a strong unsuper-vised baseline.
Consequently, PUPA allowsobtaining high quality parses without any hu-man involvement.1 IntroductionIn unsupervised parsing an algorithm should un-cover the syntactic structure of an input sentencewithout using any manually created structural train-ing data.
The last decade has seen significantprogress in this field of research (Klein and Man-ning, 2002; Klein and Manning, 2004; Bod, 2006a;Bod, 2006b; Smith and Eisner, 2006; Seginer,2007).Many NLP systems use the output of supervisedparsers (e.g., (Kwok et al, 2001) for QA, (Moldovanet al, 2003) for IE, (Punyakanok et al, 2008) forSRL, (Srikumar et al, 2008) for Textual Inferenceand (Avramidis and Koehn, 2008) for MT).
Toachieve good performance, these parsers should betrained on large amounts of manually created train-ing data from a domain similar to that of the sen-tences they parse (Lease and Charniak, 2005; Mc-Closky and Charniak, 2008).
In the highly variableWeb, where many of these systems are used, it isvery difficult to create a representative corpus formanual annotation.
The high cost of manual annota-tion of training data for supervised parsers imposesa significant burden on their usage.A possible answer to this problem can be pro-vided by high quality parses produced by unsuper-vised parsers that require little to no manual effortsfor their training.
These parses can be used eitheras input for applications, or as training material formodern supervised parsers whose output will in turnbe used by applications.Although unsupervised parser results improve,the quality of many of the parses they produce is stilltoo low for such goals.
For example, the Seginer(2007) parser achieves an F-score of 75.9% on theWSJ10 corpus and 59% on the NEGRA10 corpus,but the percentage of individual sentences with anF-score of 100% is 21.5% for WSJ10 and 11% forNEGRA10.
When requirements are relaxed, onlyasking for an F-score higher than 85%, percentageis still low, 42% for WSJ10 and 15% for NEGRA10.In this paper we address the task of a fully un-supervised assessment of high quality parses cre-156ated by an unsupervised parser.
The assessmentshould be unsupervised in order to avoid the prob-lems mentioned above with manually trained super-vised parsers.
Assessing the quality of a learning al-gorithm?s output and selecting high quality instanceshas been addressed for supervised algorithms (Caru-ana and Niculescu-Mizil, 2006) and specifically forsupervised parsers (Yates et al, 2006; Reichart andRappoport, 2007; Kawahara and Uchimoto, 2008;Ravi et al, 2008).
Moreover, it has been shownto be valuable for supervised parser adaptation be-tween domains (Sagae and Tsujii, 2007; Kawaharaand Uchimoto, 2008; Chen et al, 2008).
However,as far as we know the present paper is the first toaddress the task of unsupervised assessment of thequality of parses created by unsupervised parsers.Our POS-based Unsupervised Parse Assessment(PUPA) algorithm uses statistics about POS tag se-quences in a batch of parsed sentences1.
The con-stituents in the batch are represented using the POSsequences of their yield and of the yields of neigh-boring constituents.
Constituents whose representa-tion is frequent in the output of the parser are con-sidered to be of a high quality.
A score for eachrange of constituent length is calculated, reflectingthe robustness of statistics used for the creation ofthe constituents of that length.
The final sentencescore is a weighted average of the scores calculatedfor each constituent length.
The score thus integratesthe quality of short and long constituents into onescore reflecting the quality of the whole parse tree.PUPA provides a quality score for every sentencein a parsed sentences set.
An NLP application canthen decide if to use a parse or not, according toits own definition of a high quality parse.
For ex-ample, it can select every sentence whose score isabove some threshold, or the k top scored sentences.The selection strategy is application dependent andis beyond the scope of this paper.The unsupervised parser we use is the Seginer(2007) incremental parser2, which achieves state-of-1The algorithm can be used with supervised POS taggersand parsers, but we focus here on the fully unsupervised sce-nario, which is novel and more useful.
For completeness ofanalysis, we experimented with PUPA using a supervised POStagger (see Section 5).
Using PUPA with supervised parsers isleft for future work.2www.seggu.net/ccl.the-art results without using manually created POStags.
The POS tags we use are induced by the un-supervised tagger of (Clark, 2003)3.
Since both tag-ger and parser do not require any manual annotation,PUPA identifies high quality parses without any hu-man involvement.The incremental parser of (Seginer, 2007) doesnot give any prediction of its output quality, andextracting such a prediction from its internal datastructures is not straightforward.
Such a predic-tion can be given by supervised parsers in termsof the parse likelihood, but this was shown to beof medium quality (Reichart and Rappoport, 2007).While the algorithms of Yates et al (2006), Kawa-hara and Uchimoto (2008) and Ravi et al (2008) aresupervised (Section 3), the ensemble based SEPA al-gorithm (Reichart and Rappoport, 2007) can be ap-plied to unsupervised parsers in a way that preservesthe unsupervised nature of the selection task.To compare between two algorithms, we use eachof them to assess the quality of the sentences in En-glish and German corpora (WSJ and NEGRA)4.
Weshow that for every sentence length (up to 20) thequality of the top scored k sentences according toPUPA is higher than the quality of SEPA?s list (forevery k).
As in (Reichart and Rappoport, 2007), thequality of a set selected from the parser?s output isevaluated using two measures: constituent F-score5and average sentence F-score.Section 2 describes the PUPA algorithm, Sec-tion 3 discusses previous work, and Sections 4 and5 present the evaluation setup and results.2 The POS-based Unsupervised ParseAssessment (PUPA) AlgorithmIn this section we detail our parse assessment algo-rithm.
Its input consists of a set I of parsed sen-tences, which in our evaluation scenario are pro-duced by an unsupervised parser.
The algorithmassigns each parsed sentence a score reflecting itsquality.3www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html,the neyessenmorph model.4This is in contrast to algorithms for selection from the re-sults of supervised constituency parsers, which were evaluatedonly for English (Yates et al, 2006; Reichart and Rappoport,2007; Ravi et al, 2008).5This is the traditional parsing F-score.157The algorithm has three steps.
First, the words inI are POS tagged (in our case, using the fully unsu-pervised POS induction algorithm of Clark (2003)).Second, POS statistics about the constituents in Iare collected.
Finally, a quality score is calculatedfor each parsed sentence in I using the POS statis-tics.
In the following we detail the last two steps.Collecting POS statistics.
In its second step, thealgorithm collects statistics about the constituents inthe input set I .
Recall that the yield of a constituentis the set of words covered by it.
The PUPA con-stituent representation (PCR) consists of three fea-tures: (1) the ordered POS tag sequence of the con-stituent?s yield, (2) the constituents?
right context,and (3) the constituents?
left context.We define context to be the leftmost and rightmostPOS tags in the yield of the neighbor of the con-stituent (if there is only one POS tag in the neigh-bor?s yield, this POS tag is the context).
For theright and left contexts we consider the right and leftneighbors respectively.
A constituent C1 is the rightneighbor of a constituent C2 if C1 is the highest levelconstituent such that the first word in the yield of C1comes immediately after the last word in the yield ofC2.
A constituent C1 is the left neighbor of a con-stituent C2 if C1 is the highest level constituent suchthat the first word in the yield of C2 comes immedi-ately after the last word in the yield of C1.Figure 1 shows an example, an unlabeled tree forthe sentence ?I will give you the ball?.
The tree has6 constituents (C0-C5).
C3 and C4 have both rightand left neighbors.
For C3, the POS sequence of itsyield is POS2, POS3 , the left neighbor is C1 and thusthe left context is POS1, and the right neighbor is C4and thus the right context is POS4.
Note that theleft and right neighbors of C3 have only one POStag in their yield and therefore this POS tag is thecontext.
For C4 the yield is POS4, the left neighboris C3 (and thus the left context is POS2,POS3), andthe right neighbor is C5 (and thus the right contextis POS5,POS6).
C1, whose yield is POS1, has onlya right neighbor, C2, and thus its right context isPOS2,POS6 and its left context is NULL.
C2 and C5(whose yields are POS2, POS3, POS4, POS5, POS6 forC2 and POS5, POS6 for C5) have only a left neigh-bor.
For C2, this is C1 (and the context is POS1)while for C5 this is C4 (with the context POS4).01POS1I23POS2willPOS3give4POS4you5POS5thePOS6ballFigure 1: An example parse tree for contexts and neigh-bors (see text).The right context of both constituents is NULL.
Asall sentence level constituents, C0 has no neighbors,and thus both its left and right contexts are NULL.We have also explored other representations ofleft and right contexts based on the POS tags of theiryields.
In these, we represented the left/right neigh-bor using only the leftmost/rightmost POS tags ofits yield or other subsets of the yield?s POS tags.These variations produced lower quality results thanthe main variant above in our experiments, whichwere for English and German.
Exploring the suit-ability of our representation for other languages isleft for future research.Score computation.
The third and last step of thealgorithm is a second pass over I for computing aquality score for each parse tree.Short constituents tend to be more frequent thanlong ones.
In order not to distort our score due toparsing errors in short constituents, PUPA computesthe grade using a division into lengths, in three steps.First, constituents are assigned to bins according totheir length, each bin containing the constituents ofa certain range of lengths.
Denote this range byW (for width), and the number of bins by N(W ).For example, in our experiments the longest possibleconstituent is of length 20, so we can take W = 5,resulting in N(W ) = 4: bin 1 for constituents oflength 1-5, bin 2 for constituents of length 6-10, andso on for bins 3, 4.The score of bini is given by(1) BinScore(bini) =?t=Xt=2 (X ?
t + 2) ?
|Cit ||Ci|Where X is the maximal number of occurrencesof constituents in the bin that we consider as impor-tant for the score (see below for its selection), |Cit |is the number of constituents in bin i occurring at158least t times in the batch of parsed sentences, and|Ci| is the number of constituents in bin i.
In words,the score is a weighted average: the fraction of theconstituents in the bin occuring at least 2 times (withweight X), plus the fraction of the constituents in thebin occuring at least 3 times (with weight X ?
1),etc, until the fraction of the constituents in the binoccuring at least X times (with weight 2).A score for the division into N bins is given by(2) Score(N(W )) =?N(W )i=1 BinScore(bini)Z?MWhere Z is the maximum bin score (according to(1)) and M is the number of bins containing at leastone constituent.
If, for example, N(W ) = 4 andthere is no constituent whose length is between 11and 15 then bin number 3 is empty.
If every otherbin contains at least one constituent, M = 3.To get a final score for the parse tree of sentenceS that is independent of a specific bin division, wesum the scores of the various bin division:(3) PupaScore(S) =?W=YW=1 Score(N(W ))Ywhere Y is the length of S (which is also its max-imum bin width).
PupaScore thus takes values inthe [0, 1] range.In equation (1), if, for example, X = 20 thenthe weight of the fraction of the bin?s constituentsoccurring at least 2 times is 20 while the weight ofthe fraction of the constituents occurring at least 10times is 12 and of the fraction of constituents occur-ring at least 20 times is 2.
We consider the numberof times a constituent appears in a batch to be an in-dication of its correctness.
The difference between 3and 2 occurrences is therefore more indicative thanthe difference between 20 and 19 occurrences.
Moregenerally, the more times a constituent occurs, theless indicative any additional appearance is.In equation (2) we give all bins the same weight.Short constituents are more frequent and are gener-ally more likely to be correct.
However, the cor-rectness of long constituents is an indication that theparser has a correct interpretation of the tree struc-ture and that it is likely to create a high quality tree.The usage of equal bin weights was done to balancethe tendency of parse trees to have more short con-stituents.Parameters.
PUPA has two parameters: X , themaximal number of occurrences considered in equa-tion (1), and P , the number of POS tags induced bythe unsupervised POS tagger.
In the following wepresent the unsupervised technique we used to tunethese parameters.Figure 2 shows nc(t), the number of constituentsappearing at least t times in WSJ20 (left) and NE-GRA20 (right).
For both corpora, the pattern isshown when using 5 POS tags (P = 5, solid line)and 50 POS tags (P = 50, dashed line).
The distri-bution obeys Zipf?s law: many constituents appear asmall number of times while a few constituents ap-pear a large number of times.
We denote the t valuewhere the slope changes from steep to moderate bytelbow.
Practically, we approximate the ?real?
elbowvalue and define telbow to be the smallest t for whichnc(t + 1) ?
nc(t) = 1.
When P = 5, telbow is 32for WSJ and 19 for NEGRA.
When P = 50, telbow is15 for WSJ and 9 for NEGRA.The number of constituents appearing more thantelbow times is considerably smaller than the numberof constituents appearing telbow times or less.
There-fore, the fact that a constituent appears telbow + Stimes (for a positive integer S) is not a better indica-tion of its quality than the fact that it appears telbowtimes.
We thus select X to be telbow.The graphs also demonstrate that for both cor-pora, telbow for P = 50 is smaller than telbow forP = 5.
Generally, telbow is a monotonically decreas-ing function of P .
Lower telbow values imply thatPUPA would be less distinctive between constituentsquality (see equation (1); recall that X = telbow).We thus want to select the P value that maximizestelbow.
We therefore minimize P .
telbow values forP ?
{3, .
.
.
, 10} are very similar.
Indeed, PUPAachieves its best performance for P ?
{3, .
.
.
, 10}and it is insensitive to the selection of P in thisrange.
In Section 5 we report results with P = 5.3 Related WorkUnsupervised parsing has been explored for severaldecades (see (Klein, 2005) for a recent review).
Re-cently, unsupervised parsing algorithms have for thefirst time outperformed the right branching heuristicbaseline for English.
These include CCM (Klein andManning, 2002), the DMV and DMV+CCM models(Klein and Manning, 2004), (U)DOP based mod-1590 50 100050001000015000t# of constituentsappearingatleast ttimes0 50 100010002000300040005000600070008000tP = 5P = 50P = 5P = 50Figure 2: Number of constituents appearing at least ttimes (nc(t)) as a function of t. Shown are WSJ (left)and NEGRA (right), where constituents are representedaccording to PUPA?s PCR with 5 POS tags (P = 5, solidline) or 50 POS tags (P = 50, dashed line).els (Bod, 2006a; Bod, 2006b), an exemplar basedapproach (Dennis, 2005), guiding EM using con-trastive estimation (Smith and Eisner, 2006), and theincremental parser of Seginer (2007) that we use inthis work.
To obtain good results, manually createdPOS tags are used as input in all of these algorithmsexcept Seginer?s, which uses plain text.Quality assessment of a learning algorithm?s out-put and selection of high quality instances have beenaddressed for supervised algorithms (see (Caruanaand Niculescu-Mizil, 2006) for a survey) and specif-ically for supervised constituency parsers (Yates etal., 2006; Reichart and Rappoport, 2007; Ravi et al,2008).
For dependency parsing in a corpus adapta-tion scenario, (Kawahara and Uchimoto, 2008) builta binary classifier that classifies each parse in theparser?s output as reliable or not.
To do that, theyselected 2500 sentences from the parser?s output,compared them to their manually created gold stan-dard, and used accurate (inaccurate) parses as posi-tive (negative) examples for the classifier.
Their ap-proach is supervised and the features used by theclassifier are dependency motivated .As far as we know, the present paper is the first toaddress the task of selecting high quality parses fromthe output of unsupervised parsers.
The algorithmsof Yates et al (2006), Kawahara and Uchimoto(2008) and Ravi et al (2008) are supervised, per-forming semantic analysis of the parse tree and goldstandard-based calssification, respectively.
How-ever, the SEPA algorithm of Reichart and Rappoport(2007), an algorithm for supervised constituencyparsers, can be applied to unsupervised parsers ina way that preserves the unsupervised nature of theselection task.
In Section 5 we provide a detailedcomparison between PUPA and SEPA showing thefirst to be superior.
Below is a brief description ofthe SEPA algorithm.The input of the SEPA algorithm consists of aparsing algorithm A, a training set, and a test set(which in the unsupervised case might be the sameset).
The algorithm provides, for each of the testset?s parses generated by A when trained on the fulltraining set, a grade assessing the parse quality, ona continuous scale between 0 to 100.
The qual-ity grade is calculated in the following way: N ran-dom samples of size S are sampled from the train-ing data and used for training the parsing algorithmA.
In that way N committee members are created.Then, each of the test sentences is parsed by each ofthe N committee members and an agreement scoreranging from 0 to 100 between the committee mem-bers is calculated.
All unsupervised parsers men-tioned above (including the Seginer parser), have atraining phase where parameter values are estimatedfrom unlabeled data.
SEPA can thus be applied to theunsupervised case.Automatic selection of high quality parses hasbeen shown to improve parser adaptation.
Sagae andTsujii (2007) and Kawahara and Uchimoto (2008)applied a self-training protocol to a parser adaptationscenario but used only high quality parses to retrainthe parser.
In the first work, high quality parses wereselected using an ensemble method, while in the sec-ond a binary classifier was used (see above).
Thefirst system achieved the highest score in the CoNLL2007 shared task on domain adaptation of depen-dency parsers, and the second system improved overthe basic self-training protocol.
Chen et al (2008)parsed target domain sentences and used short de-pendencies information, which is often accurate, toadapt a dependency parser to the Chinese language.Automatic quality assessment has been exten-sively explored for machine translation (Ueffing andNey, 2007) and speech recognition (Koo et al,2001).
Other NLP tasks where it has been exploredinclude semi-supervised relation extraction (Rosen-feld and Feldman, 2007), IE (Culotta and McCal-lum, 2004), QA (Chu-Carroll et al, 2003), and dia-log systems (Lin and Weng, 2008).The idea of representing a constituent by its yield160and (a different definition of) context is used by theCCM unsupervised parsing model (Klein and Man-ning, 2002).
As far as we know the current work isthe first to use unsupervised POS tags for the selec-tion of high quality parses.4 Evaluation SetupWe experiment with sentences of up to 20 wordsfrom the English WSJ Penn Treebank (WSJ20,25236 sentences, 225126 constituents) and the Ger-man NEGRA corpus (Brants, 1997) (NEGRA20,15610 sentences, 108540 constiteunts), both con-taining newspaper texts.The unsupervised parsers of the kind addressedin this paper output unlabeled parse trees.
To eval-uate the quality of a single parse tree with respectto another, we use the unlabeled F-score (UF =2?UR?UPUR+UP ), where UR and UP are unlabeled recalland unlabeled precision respectively.Following the unsupervised parsing literature,multiple brackets and brackets covering a singleword are not counted, but the sentence level bracketis.
We exclude punctuation and null elements ac-cording to the scheme of (Klein, 2005).The performance of unsupervised parsersmarkedly degrades as sentence length increases.For example, the Average sentence F?score for WSJsentences of length 10 is 71.4% compared to 58.5for sentences of length 20 (the numbers for NEGRAare 48.2% and 36.9%).
We therefore evaluate PUPA(and the baseline) for sentences of a given length.We do this for every sentence of length 2-20 inWSJ20 and NEGRA20.For every sentence length L, we use PUPA and thebaseline algorithm (SEPA) to give a quality score toeach of the sentences of that length in the experi-mental corpus.
We then compare the quality of thetop k parsed sentences according to each algorithm.We do this for every k from 1 to the number of sen-tences of length L.Following Reichart and Rappoport (2007), we usetwo measures to evaluate the quality of a set ofparses: the constituent F-score (the traditional F-score used in the parsing literature), and the averageF-score of the parses in the set.
In the first mea-sure we treat the whole set as a bag of constituents.Each constituent is marked as correct (if it appearsin the gold standard parses of the set) or erroneous(if it does not).
Then, recall, precision and F-scoreare calculated over these constituents.
In the sec-ond measure, the constituent F-score of each of theparses in the set is computed, and then results areaveraged.There are applications that use individual con-stituents from the output of a parser while othersneed the whole parse tree.
For example, if the se-lected set is used for training supervised parsers suchas the Collins parser (Collins, 1999), which collectsconstituent statistics, the constituent F-score of theselected set is the important measure.
In applica-tions such as the syntax based machine translationmodel of (Yamada and Knight, 2001), a low qual-ity tree might lead to errorenous translation of thesentence.
For such applications the average F-scoreis more indicative.
These measures thus representcomplementary aspects of a set quality and we con-sider both of them.The parser we use is the incremental parser of(Seginer, 2007), POS tags are induced using the un-supervised POS tagger of ((Clark, 2003), neyessen-morph model).
In each experiment, the tagger wastrained with the raw sentences of the experiment cor-pus, and then the corpus words were POS tagged.The output of the unsupervised POS tagger de-pends on a random initialization.
We ran the tagger5 times, each time with a different random initializa-tion, and then ran PUPA with its output.
The resultswe report for PUPA are the average over these 5 runs.Random selection results (given for reference) werealso averages over 5 samples.PUPA ?s parameter estimation is completely unsu-pervised (see Section 2).
No development data wasused to tune its parameters.A 200 sentences development set from each cor-pus was used for calibrating the parameters of theSEPA algorithm.
Based on the analysis of SEPA per-formance with different assignments of its param-eters given by Reichart and Rappoport (2007) (seeSection 3), we ran the SEPA algorithm with sam-ple size (SEPA parameter S) of 30% and 80%, andwith 2 ?
10 committee members (N )6.
The optimalparameters were N = 10,S = 80 for WSJ20, and6We tried higherN values but observed no improvements inSEPA?s performance.1610 200 400 600707580859095100Number of SentencesAverageFScore(a) WSJ, length 50 500 100060657075808590Number of SentencesAverageFScore(b) WSJ, length 100 500 1000 150050556065707580Number of SentencesAverageFScore(c) WSJ, length 150 500 1000 1500 2000485052545658606264Number of SentencesAverageFScore(d) WSJ, length 200 200 400 600 80050556065707580Number of SentnecesAverageFScore(e) NEGRA, length50 200 400 600 800 1000404550556065Number of SentencesAverageFScore(f) NEGRA, length 100 200 400 600 80030354045505560Number of SentencesAverageFScore(g) NEGRA, length 150 200 400 600 80050556065707580Number of SentnecesAverageFScore(h) NEGRA, length 200 500 1000 1500 2000707580859095Number of ConstituentsConstituentsFScore(i) WSJ, length 50 2000 4000 6000 8000 10000606570758085Number of ConstituentsConstituentsFScore(j) WSJ, length 100 0.5 1 1.5 2x 10455606570Number of ConstituentsConstituentsFScore(k) WSJ, length 150 0.5 1 1.5 2 2.5x 104545658606264Number of ConstituentsConstituentsFScore(l) WSJ, length 200 500 1000 1500 2000 2500 30006263646566676869Number of ConstituentsConstituentsFScore(m) NEGRA, length50 1000 2000 3000 4000 50004446485052545658Number of ConstituentsConstituent FScore(n) NEGRA, length 100 2000 4000 6000 8000404244464850Number of ConstituentsConstituentsFScore(o) NEGRA, length 150 500 1000 1500 2000 2500 30006263646566676869Number of ConstituentsConstituentsFScore(p) NEGRA, length 20Figure 3: In all graphs: PUPA: solid line.
SEPA: line with triangles.
MC: line with circles.
Random selection ispresented for reference as a dotted line.
Top two rows: Average F-score for PUPA, SEPA and MC for sentences fromWSJ (top row) and NEGRA (bottom row).
Bottom two rows: Constituents F-score for PUPA, SEPA and MC forsentences from WSJ (top row) and NEGRA (bottom row).
Results are presented for sentence lengths of 5,10,15 and20 (patterns for other sentence lengths between 2 and 20 are very similar).
PUPA is superior in all cases.
The graphsfor PUPA and SEPA show a downward trend because parsed sentences were sorted according to score, which correlatespositively with F-score (unlike MC).
The graphs converge because on the extreme right all test sentences were selected.N = 10, S = 30 for NEGRA20.We also compare PUPA to a baseline selecting thesentences with the lowest number of constituents.Since the number of constituents is an indication ofthe complexity of the syntactic structure of a sen-tence, it is reasonable to assume that selecting thesentences with the lowest number of constituents isa good selection strategy.
We denote this baseline byMC (for minimum constituents).The incremental parser does not give any predic-tion of its output quality as supervised generativeparsers do.
We are thus not able to compare to sucha score.5 ResultsFigure 3 shows Average F-score and Constituents F-score results for PUPA SEPA and MC, for sentences162of lengths 5,10,15 and 20 in WSJ20 and NEGRA20.The top two rows are for Average F-score (top row:WSJ, bottom row: NEGRA), while the bottom tworows are for Constituents F-score (top row: WSJ,bottom row: NEGRA).PUPA and SEPA are both better than random selec-tion for both corpora for every sentence length.
TheMC baseline is better than random selection only forNEGRA (in which case it outperforms SEPA).
ForWSJ, however, random selection is a better strategythan MC.It is clear from the graphs that PUPA outperformsSEPA and MC in all experimental conditions.
Weobserved very similar patterns in all other sentencelengths in WSJ20 and NEGRA20 for both AverageF-score and Constituent F-score.
In other words, forevery sentence length in both corpora, PUPA outper-forms SEPA and MC in terms of both measures.
wepresent our results per sentence length to deprive thepossibility that PUPA is useful only for short sen-tences or that it prefers sentences whose syntacticstructure is not complex (i.e.
with a small number ofconstituents, like MC).Table 1 shows that the same pattern of resultsholds when evaluating on the whole corpus (WSJ20or NEGRA20) without any sentence length restric-tion.Note that while PUPA is a fully unsupervised al-gorithm, SEPA requires a few hundreds of sentencesfor its parameters tuning.The main result of this paper is for sentenceswhose length is up to 20 words (note that most un-supervised parser literature reports numbers for sen-tences up to length 10).
We have also ran the exper-iments for the remaining length range, 20-40.
ForNEGRA, PUPA is superior over MC up to length 36,and both are much better than SEPA.
For WSJ, PUPAand SEPA both outperform MC, but SEPA is a bit bet-ter than PUPA.
When evaluating on the whole corpus(i.e.
without sentence length restriction, like in Ta-ble 1) PUPA is superior over both SEPA and MC forWSJ40 and NEGRA40.For completeness of analysis we also experi-mented in the condition where PUPA uses gold stan-dard POS tags as input.
The number of these tags is35 for WSJ and 57 for NEGRA.
Interestingly, PUPAachieves in this condition the same performance aswhen using the same number of POS tags inducedby an unsupervised POS tagger.
Since PUPA?s per-formance for a smaller number of POS tags is better(see our parameter tuning discussion above), the bot-tom line is that PUPA pefers using induced POS tagsover gold POS tags.5% 10% 20% 30% 40% 50%WSJ20PUPA 82.75 79.34 75.77 73.46 71.68 70.3SEPA 78.68 75.7 72.64 70.72 69.54 68.58MC 76.75 74.6 72.1 70.35 68.97 67.77NEGRA20PUPA 70.66 67.06 61.89 58.75 56.6 54.73SEPA 66.19 62.75 59.41 57.16 55.23 53.7MC 69.41 65.79 60.87 58.08 55.9 54.36Table 1: Average F?score for the top k% of constituentsselected from WSJ20 (up) and NEGRA20 (down).
No sen-tence length restriction is imposed.
Results presented forPUPA , SEPA and MC.
Average F?score of random se-lection is 66.55 (WSJ20) and 47.05 (NEGRA20).
PUPA issuperior over all methods.6 ConclusionsWe introduced PUPA, an algorithm for unsupervisedparse assessment that utilizes POS sequence statis-tics.
PUPA is a fully unsupervised algorithm whoseparameters can be tuned in an unsupervised man-ner.
Experimenting with the Seginer unsupervisedparser and Clark?s unsupervised POS tagger on En-glish and German corpora, PUPA was shown to out-perform both the leading parse assessment algorithmfor supervised parsers (SEPA, even when its param-eters are tuned on manually annotated developmentdata) and a strong baseline (MC).Using PUPA, we extracted high quality parsesfrom the output of a parser which requires raw textas input, using POS tags induced by an unsupervisedtagger.
PUPA thus provides a way of obtaining highquality parses without any human involvement.For future work, we intend to use parses selectedby PUPA from the output of unsupervised parsersas training data for supervised parsers, and in NLPapplications that use parse trees.
A challenge forthe first direction is the fact that state of the art su-pervised parsers require labeled parse trees, whilemodern unsupervised parsers create unlabeled trees.Combining PUPA with algorithms for labeled parsetrees induction (Haghighi and Klein, 2006; Reichartand Rappoport, 2008) is a one direction to overcomethis challenge.
We also intend to use PUPA to assessthe quality of parses created by supervised parsers.163ReferencesEleftherios Avramidis and Philipp Koehn, 2008.
En-riching Morphologically Poor Languages for Statisti-cal Machine Translation.
ACL ?08.Rens Bod, 2006a.
An All-Subtrees Approach to Unsu-pervised Parsing.
ACL ?06.Rens Bod, 2006b.
Unsupervised Parsing with U-DOP.CoNLL X.Thorsten Brants, 1997.
The NEGRA Export Format.CLAUS Report, Saarland University.Rich Caruana and Alexandru Niculescu-Mizil, 2006.
AnEmpirical Comparison of Supervised Learning Algo-rithms.
ICML ?06.Jennifer Chu-Carroll, Krzysztof Czuba, John Prager andAbraham Ittycheriah, 2003.
In Question Answering,Two Heads Are Better Than One.
HLT-NAACL ?03.Wenliang Chen, Youzheng Wu and Hitoshi Isahara,2008.
Learning Reliable Information for DependencyParsing Adaptation.
Coling ?08.Alexander Clark, 2003.
Combining Distributional andMorphological Information for Part of Speech Induc-tion.
EACL ?03.Michael Collins, 1999.
Head-driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Aron Culotta and Andrew McCallum, 2004.
ConfidenceEstimation for Information Extraction.
HLT-NAACL?04.Simon Dennis, 2005.
An Exemplar-based Approach toUnsupervised Parsing.
Proceedings of the 27th Con-ference of the Cognitive Science Society.Aria Haghighi and Dan Klein, 2006.
Prototype-drivenGrammar Induction.
ACL ?06.Daisuke Kawahara and Kiyotaka Uchimoto 2008.Learning Reliability of Parses for Domain Adaptationof Dependency Parsing.
IJCNLP ?08.Dan Klein and Christopher Manning, 2002.
A Gener-ative Constituent-Context Model for Improved Gram-mar Induction.
ACL ?02.Dan Klein and Christopher Manning, 2004.
Corpus-based Induction of Syntactic Structure: Models of De-pendency and Constituency.
ACL ?04.Dan Klein, 2005.
The Unsupervised Learning of NaturalLanguage Structure.
Ph.D. thesis, Stanford University.Myoung?Wan Koo, Chin-Hui Lee and Biing?HwangJuang 2001.
Speech Recognition and Utterance Ver-ification Based on a Generalized Confidence Score.IEEE Transactions on Speech and Audio Processing,9(8):821?832.Cody Kwok, Oren Etzioni and Daniel S. Weld, 2001.Scaling Question Answering to the Web.
WWW ?01.Matthew Lease and Eugene Charniak, 2005.
Towards aSyntactic Account of Punctuation.
IJCNLP ?05.Feng Lin and Fuliang Weng, 2008.
Computing Confi-dence Scores for All Sub Parse Trees.
ACL ?08, shortpaper.David McClosky and Eugene Charniak, 2008.
Self-Training for Biomedical Parsing.
ACL ?08, short pa-per.Dan Moldovan, Christine Clark, Sanda Harabagiu andSteve Maiorano, 2003.
Cogex: A Logic Prover forQuestion Answering.
HLT-NAACL ?03.Vasin Punyakanok and Dan Roth and Wen-tau Yih, 2008.The Importance of Syntactic Parsing and Inference inSemantic Role Labeling.
Computational Linguistics,34(2):257-287.Sujith Ravi, Kevin Knight and Radu Soricut, 2008.
Au-tomatic Prediction of Parser Accuracy.
EMNLP ?08.Roi Reichart and Ari Rappoport, 2007.
An EnsembleMethod for Selection of High Quality Parses.
ACL?07.Roi Reichart and Ari Rappoport, 2008.
UnsupervisedInduction of Labeled Parse Trees by Clustering withSyntactic Features.
COLING ?08.Benjamin Rosenfeld and Ronen Feldman, 2007.
Us-ing Corpus Statistics on Entities to Improve Semi?Supervised Relation Extraction From The WEB.
ACL?07.Kenji Sagae and Junichi Tsujii, 2007.
DependencyParsing and Domain Adaptation with LR Models andParser Ensemble.
EMNLP-CoNLL ?07.Yoav Seginer, 2007.
Fast Unsupervised IncrementalParsing.
ACL ?07.Vivek Srikumar, Roi Reichart, Mark Sammons, Ari Rap-poport and Dan Roth, 2008.
Extraction of EntailedSemantic Relations Through Syntax-based CommaResolution.
ACL ?08.Noah A. Smith and Jason Eisner, 2006.
Annealing Struc-tural Bias in Multilingual Weighted Grammar Induc-tion.
ACL ?06.Nicola Ueffing and Hermann Ney, 2007.
Word-Level Confidence Estimation for Machine Translation.Computational Linguistics, 33(1):9?40.Kenji Yamada and Kevin Knight, 2001.
A Syntax-BasedStatistical Translation Model.
ACL ?01.Alexander Yates, Stefan Schoenmackers and Oren Et-zioni, 2006.
Detecting Parser Errors Using Web-based Semantic Filters .
EMNLP ?06.164
