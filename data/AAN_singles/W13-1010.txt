Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 64?72,Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational LinguisticsAutomatic Identification of Bengali Noun-NounCompounds Using Random ForestVivekananda Gayen Kamal SarkarDepartment of Computer Science andTechnologyDepartment of Computer Science andEngineeringCentral Calcutta Polytechnic Jadavpur UniversityKolkata-700014, India Kolkata, Indiavivek3gayen@gmail.com jukamal2001@yahoo.comAbstractThis paper presents a supervised machinelearning approach that uses a machine learn-ing algorithm called Random Forest for rec-ognition of Bengali noun-noun compounds asmultiword expression (MWE) from Bengalicorpus.
Our proposed approach to MWE rec-ognition has two steps: (1) extraction of can-didate multi-word expressions using Chunkinformation and various heuristic rules and (2)training the machine learning algorithm torecognize a candidate multi-word expressionas Multi-word expression or not.
A variety ofassociation measures, syntactic and linguisticclues are used as features for identifyingMWEs.
The proposed system is tested on aBengali corpus for identifying noun-nouncompound MWEs from the corpus.1 IntroductionAutomatic identification of multiword expression(MWE) from a text document can be useful formany NLP (natural language processing) applica-tions such as information retrieval, machine trans-lation, word sense disambiguation.
According toFrank Samadja (1993), MWEs are defined as ?re-current combinations of words that co-occur moreoften than expected by chance?.
Timothy Baldwinet al(2010) defined multiword expressions(MWEs) as lexical items that: (a) can be decom-posed into multiple lexemes; and (b) display lexi-cal, syntactic, semantic, pragmatic and/or statisticalidiomaticity.
Most real world NLP applicationstend to ignore MWE, or handle them simply bylisting, but successful applications will need toidentify and treat them appropriately.As Jackendoff (1997) stated, the magnitude ofthis problem is far greater than has traditionallybeen realized within linguistics.
He estimates thatthe number of MWEs in a native speakers?s lex-icon is of the same order of magnitude as the num-ber of single words.
In WordNet 1.7 (Fellbaum,1999), for example, 41% of the entries are multi-word.MWEs can be broadly classified into lexicalizedphrases and institutionalized phrases (Ivan A. saget al 2002).
In terms of the semantics, composi-tionality is an important property of MWEs.
Com-positionality is the degree to which the features ofthe parts of a MWE combine to predict the featuresof the whole.
According to the compositionalityproperty,  the MWEs  can take a variety of forms:complete compositionality (also known as institu-tionalized phrases, e.g.
many thanks, ?????
??????
(Rajya Sarkar, state government)), partial composi-tionality (e.g.
light house, ?????
???
(shopping mall),???
?????
(aam admi, common people)), idiosyn-cratically compositionality (e.g.
spill the beans (toreveal)) and finally complete non-compositionality(e.g.
hot dog, green card, ?u??
????
(ubhoy sang-kat, on the horns of a dilemma)).Compound noun is a lexical unit.
It is a class ofMWE which is rapidly expanding due to the conti-nuous addition of new terms for introducing newideas.
Compound nouns fall into both groups: lexi-calized and institutionalized.
A noun-noun com-pound in English characteristically occursfrequently with high lexical and semantic variabili-ty.
A summary examination of the 90 million-64word written component of the British NationalCorpus (BNC) uncover the fact that there are over400,000 NN (Noun-Noun) compound types, with acombined token frequency of 1.3 million, that is,over 1% of words in the BNC are NN compounds(Timothy Baldwin et al 2003).
Since compoundnouns are rather productive and new compoundnouns are created from day to day, it is impossibleto exhaustively store all compound nouns in a dic-tionaryIt is also common practice in Bengali literatureto use compound nouns as MWEs.
Bengali newterms directly coined from English terms are alsocommonly used as MWEs in Bengali (e.g., ????
???
(dengue three), ???????
????
(nano sim), ??????
?????????
(village tourism), ?a??????
??????
(alert message)).The main focus of our work is to develop a ma-chine learning approach based on a set of statistic-al, syntactic and linguistic features for identifyingBengali noun-noun compounds.To date, not much comprehensive work hasbeen done on Bengali multiword expression identi-fication.Different types of compound nouns in Bengaliare discussed in section 2.
Related works are pre-sented in section 3.
The proposed noun-nounMWE identification method has been detailed insection 4.
The evaluation and results are presentedin section 5 and conclusions and feature work aredrawn in section 6.2 Classification of Bengali CompoundNounsIn Bengali, MWEs are quite varied and many ofthese are of types that are not encountered in Eng-lish.
The primary types of compound nouns inBengali are discussed below.Named-Entities (NE): Names of people (?????????
(Tirtha Das), ????
????
(Nayan Roy)).
Name ofthe location (??????
?s???
(Hooghly Station), ?a??????????
(Ashok Bihar)).
Names of the Organization(??i?????
?
?b a???????
a????????????
(Ideal cableoperators association), ?????
i?n???
(Reebok India)).Here inflection can be added to the last word.Idiomatic Compound Nouns: These are cha-racteristically idiomatic and unproductive.
For ex-ample, ???
?????
(maa baba, father mother), ???????????
(kaal karkhana, mills and workshops) areMWEs of this kind.Idioms: These are the expressions whose mean-ings can not be recovered from their componentwords.
For example, ??????
???
(taser ghar, anyconstruction that may tumble down easily at anytime), ??????
?????
(pakhir chokh, target), ?????
??p??
(sabuj biplab, green revolution) are the idioms inBengali.Numbers: These are productive in nature andlittle inflection like syntactic variation is also seenin number expression.
For example,  ??????
???
?n??
(soya teen ghanta, three hours and fifteen minutes),???
?i ????
(arawi guun, two and a half times), ???????????
(sharre teenta, three hours and thirtyminutes), ????
????
(der bachar, one and a half year)are MWEs of this kind.Relational Noun Compounds: These are gen-erally consists of two words, no word can be in-serted in between.
Some examples are:  ???????????i?
(pistuto bhai, cousin), ????
?????
(majo meyya,second daughter).Conventionalized Phrases (or Institutiona-lized phrases):Institutionalized?
phrases?
are?
conventionalized?phrases,?
such?
as?
(??????
???????
(bibaha barshiki,marriage anniversary, ???k?
?????
(chakka jam,standstill), ??????
??????
(share bazar, share market)).?They?
are?
semantically?
and?
syntactically?
composi?tional,?but?statistically?idiosyncratic.
?Simile terms: It is analogy term in Bengali andsemi-productive (??????
?????
(hater panch, lastresort), ?????
????
(kather katha, a word for word?ssake)).Reduplicated terms: Reduplicated terms arenon-productive and tagged as noun phrase.
NamelyOnomatopoeic expression (???
???
(khhat khhat,knock knock), ???
???
(hu hu, the noise made by astrong wind)), Complete reduplication (?????
?????
(bari bari, door to door), ?b??
b???
(blocke blocke,block block)), Partial reduplication (??n?
?n??
(jantar mantar)), Semantic reduplication (?????
?n???
(matha mundu, head or tail)), Correlative redupli-cation (??????????
(maramari, fighting)).Administrative terms: These are institutiona-lized as administrative terms and are non-productive in nature.
Here inflection can be addedwith the last word (?s???
?nt??
(sarastra montrak,home ministry)), ?s?s?
?????
(sastha sachib, healthsecretary)).65One of the component of MWE from Englishliterature: Some examples of Bengali MWEs ofthis kind are ???d???
?????
?
(madrasha board), ??????
????
(metro sahar, metro city).Both of the component of MWE from Englishliterature: Some examples of Bengali MWEs ofthis kind are ???????
????
?
(roaming charge), ??k???
????
?
(credit card).3 Related WorkThe earliest works on Multiword expression ex-traction can be classified as:  Association measurebased methods, deep linguistic based methods, ma-chine learning based methods and hybrid methods.Many previous works have used statisticalmeasures for multiword expression extraction.
Oneof the important advantages of using statisticalmeasures for extracting multiword expression isthat these measures are language independent.Frank Smadja (1993) developed a system, Xtractthat uses positional distribution and part-of-speechinformation of surrounding words of a word in asentence to identify interesting word pairs.
Clas-sical statistical hypothesis test like Chi-square test,t-test, z-test, log-likelihood ratio (Ted Dunning,1993) have also been employed to extract colloca-tions.
Gerlof Bouma (2009) has presented a me-thod for collocation extraction that uses someinformation theory based association measuressuch as mutual information and pointwise mutualinformation.Wen Zhang et al2009) highlights the deficien-cies of mutual information and suggested an en-hanced mutual information based associationmeasures to overcome the deficiencies.
The majordeficiencies of the classical mutual information, asthey mention, are its poor capacity to measure as-sociation of words with unsymmetrical co-occurrence and adjustment of threshold value.Anoop et al2008) also used various statisticalmeasures such as point-wise mutual information(K. Church et al 1990), log-likelihood, frequencyof occurrence, closed form (e.g., blackboard)count, hyphenated count (e.g., black-board) forextraction of Hindi compound noun multiwordextraction.
Aswhini et al2004) has used co-occurrence and significance function to extractMWE automatically in Bengali, focusing mainlyon Noun-verb MWE.
Sandipan et al2006) hasused association measures namely salience (AdamKilgarrif et al 2000), mutual information and loglikelihood for finding N-V collocation.
Tanmoy(2010) has used a linear combination of some ofthe association measures namely co-occurrence,Phi, significance function to obtain a  linear rank-ing function for ranking Bengali noun-noun collo-cation candidates and MWEness is measured bythe rank score assigned by the ranking function.The statistical tool (e.g., log likelihood ratio)may miss many commonly used MWEs that occurin low frequencies.
To overcome this problem,some linguistic clues are also useful for multiwordexpression extraction.
Scott Songlin Paul et al(2005) focuses on a symbolic approach to multi-word extraction that uses large-scale semanticallyclassified multiword expression template databaseand semantic field information assigned to MWEsby the USAS semantic tagger (Paul Rayson etal.,2004 ).
R. Mahesh et al2011) has used a step-wise methodology that exploits linguistic know-ledge such as replicating words (ruk ruk e.g.
stopstop), pair of words (din-raat e.g.
day night), sa-maas (N+N, A+N) and Sandhi (joining or fusion ofwords), Vaalaa morpheme (jaane vaalaa e.g.
aboutto go) constructs for mining Hindi MWEs.
A Rule-Based approach for identifying only reduplicationfrom Bengali corpus has been presented in Tan-moy et al2010).
A semantic clustering based ap-proach for indentifying bigram noun-noun MWEsfrom a medium-size Bengali corpus has been pre-sented in Tanmoy et al2011).
The authors of thispaper hypothesize that the more the similarity be-tween two components in a bigram, the less theprobability to be a MWE.
The similarity betweentwo components is measured based on the syn-onymous sets of the component words.Pavel Pecina (2008) used linear logistic regres-sion, linear discriminant analysis (LDA) and Neur-al Networks separately on feature vector consistingof 55 association measures for extracting MWEs.M.C.
Diaz-Galiano et al(2004) has applied Koho-nen?s linear vector quantization (LVQ) to integrateseveral statistical estimators in order to recognizeMWEs.
Sriram Venkatapathy et al(2005) has pre-sented an approach to measure relative composi-tionality of Hindi noun-verb MWEs usingMaximum entropy model (MaxEnt).
Kishorjit et al(2011) has presented a conditional random field(CRF) based method for extraction and translitera-tion of Manipuri MWEs.66Hybrid methods combine statistical, linguisticand/or machine learning methods.
Maynard andAnaniadou (2000) combined both linguistics andstatistical information in their system, TRUCK, forextracting multi-word terms.
Dias (2003) has de-veloped a hybrid system for MWE extraction,which integrates word statistics and linguistic in-formation.
Carlos Ramisch et al(2010) presents ahybrid approach to multiword expression extrac-tion that combines the strengths of differentsources of information using a machine learningalgorithm.
Ivan A.
Sag et al2002) argued in favorof maintaining the right balance between symbolicand statistical approaches while developing a hybr-id MWE extraction system.4 Proposed Noun-Noun compound Identi-fication MethodOur proposed noun-noun MWE identification me-thod has several steps: preprocessing, candidatenoun-noun MWE extraction and MWE identifica-tion by classifying the candidates MWEs into twocategories: positive (MWE) and negative (non-MWE).4.1 PreprocessingAt this step, unformatted documents are segmentedinto a collection of sentences automatically accord-ing to Dari (in English, full stop), Question mark(?)
and Exclamation sign (!).
Typographic or pho-netic errors are not corrected automatically.
Thenthe sentences are submitted to the chunker 1 one byone for processing.
The chunked output is thenprocessed to delete the information which is notrequired for MWE identification task.
A Sampleinput sentence and the corresponding chunked sen-tence after processing are shown in figure 1.Figure 1: A Sample input sentence and processed outputfrom the chunker.1 http//ltrc.iiit.ac.in/analyzer/bengali4.2 Candidate Noun-Noun MWE ExtractionThe chunked sentences are processed to identifythe noun-noun multi-word expression candidates.The multiword expression candidates are primarilyextracted using the following rule:Bigram consecutive noun-noun token sequencewithin same NP chunk is extracted from thechunked sentences if the Tag of the token is NN orNNP or XC (NN: Noun, NNP: Proper Noun, XC:compounds) (Akshar Bharati et al 2006).We observed that some potential noun-nounmulti-word expressions are missed due to thechunker?s error.
For example, the chunked versionof the sentence is ((NP ??????
NN)) ((NP ?
?e?e NN)) ((NP ??i???
NN, SYM )).
Here we find that thepotential noun-noun multi-word expression candi-date ??
?e?e ??i????
(BSA Cycle) cannot be detectedusing the first rule since ???e?e?
(BSA) and ??i???
(Cycle) belong to the different chunk.To identify more number of potential noun-nounMWE candidates, we use some heuristic rules asfollows:Bigram noun-noun compounds which are hy-phenated or occur within single quote or withinfirst brackets or whose words are out of vocabulary(OOV) are also considered as the potential candi-dates for MWE.4.3 Features4.3.1 Statistical features: We use the associationmeasures namely phi, point-wise mutual informa-tion (pmi), salience, log likelihood, poisson stirl-ing, chi and t-score to calculate the scores of eachnoun-noun candidate MWE.
These associationmeasures use various types of frequency statisticsassociated with the bigram.
Since Bengali is highlyinflectional language, the candidate noun-nouncompounds are stemmed while computing theirfrequencies.The frequency statistics used in computing asso-ciation measures are represented using a typicalcontingency table format (Satanjeev Banerjee etal., 2003).
Table 1 shows a typical contingencytable showing various types of frequencies asso-ciated with the noun-noun bigram <word, word2>(e.g., ????
?????).
The meanings of the entries in thecontingency table are given below:n11 = number of times the bigram occurs, joint fre-quency.Sample input sentence:??????
e??
a???????
?
?l ?
(paribhan ekti attyabo-shak shilpo, Communication is a essentialindustry.
)Processed output from the chunker:((NP ??????
NN )) (( NP e??
QC a???????
JJ?
?l NN SYM ))67n12 = number of times word1 occurs in the firstposition of a bigram when word2 does not occur inthe second position.?????(government)??????
(~ govern-ment)????
(state)n11 n12 n1p?????
(~state)n21 n22 n2pnp1 np2 nppTable 1: Contingency tablen21 = number of times word2 occurs in the secondposition of a bigram when word1 does not occur inthe first position.n22 = number of bigrams where word1 is not in thefirst position and word2 is not in the second posi-tion.n1p = the number of bigrams where the first wordis word, that is, n1p =n11+ n12.np1 = the number of bigrams where the secondword is word2, that is np1=n11+n21.n2p = the number of bigrams where the first wordis not word1, that is n2p=n21+n22.np2 = the number of bigrams where the secondword is not word2, that is np2=n12+n22.npp is the total number of bigram in the entire cor-pus.Using the frequency statistics given in the contin-gency table, expected frequencies, m11, m12, m21and m22 are calculated as follows:m11 = (n1p*np1/npp)m12 =  (n1p*np2/npp)m21 = (np1*n2p/npp)m22 =  (n2p*np2/npp)where:m11: Expected number of times both words inthe bigram occur together if they are independent.m12: Expected number of times word1 in the bi-gram will occur in the first position when word2does not occur in the second position given that thewords are independent.m21: Expected number of times word2 in the bi-gram will occur in the second position when word1does not occur in the first position given that thewords are independent.m22: Expected number of times word1 will notoccur in the first position and word2 will not occurin the second position given that the words are in-dependent.The following association measures that use theabove mentioned frequency statistics are used inour experiment.Phi, Chi and T-score: The Phi, Chi and T-scoreare calculated using the following equations:1 1 2 2 1 2 2 1(( * ) ( * ))( 1 * 1* 2* 2 )n n n nn p n p np n pp h i?=11 11 12 12 21 21 22 2211 12 21 222 2 2 2( ) ( ) ( ) ( )2*(( ) ( ) ( ) ( ) )n m n m n m n mm m m mchi ?
?
?
?= + + +11 1111( )n mnT Score??
=Log likelihood, Pmi, Salience and PoissonStirling:  Log likelihood is calculated as:11 11 11 12 12 12 21 21 21 22 22 222*( *log( * ) *log( * ) *log( * ) *log( * ))LL n n m n n m n n m n n m= + + +Pointwise Mutual Information (pmi) is calculatedas:1111log( )n mpmi =The salience is defined as:1111 11(log( ))*log( )nmsalience n=The Poisson Stirling measure is calculated usingthe formula:111111 *((log( ) 1)nmPoisson Stirling n?
= ?Co-occurrence: Co-occurrence is calculated us-ing the following formula (Agarwal et al 2004):( , 1, 2)( 1, 2)( 1, 2) d s w ws S w wco w w e?
?= ?Where co(w1,w2)=co-occurrence between thewords (after stemming).S(w1,w2)= set of all sentences where both w1 andw2 occurs.d(s,w1,w2)= distance between w1 and w2 in a sen-tence in terms of words.Significance Function: The significance func-tion (Aswhini Agarwal et al 2004) is defined as:1 1( 2) ( 2)1 ( 1)( 2) [ 1(1 ( 1, 2).
)].
[ 2.
1]w wf w f ww f wsig w k co w w k ??
?= ?
?11( 2)1 max( ( 2))( 1, 2) ( 2).exp[ 1]wwf ww f wsig w w sig w= ?Where:sigw1(w2) = significance of w2 with respect to w1.fw1(w2) = number of w1 with which w2 has oc-curred.Sig(w1,w2)= general significance of w1 and w2,lies between 0 and 1.?
(x)= sigmoid function =exp(-x)/(1+exp(-x))]68k1 and k2 define the stiffness of the sigmoid curve(for simplicity they are set to 5.0)?
is defined as the average number of noun-nounco-occurrences.4.3.2 Syntactic and linguistic features: Otherthan the statistical features discussed in the abovesection, we also use some syntactic and linguisticfeatures which are listed in the table 2.Featurenamefeature descrip-tionFeature valueAvgWor-dLengthaverage length ofthe componentsof a candidateMWEAveragelength of thewords in acandidateMWEWhether-HyphenatedWhether a can-didate MWE ishyphenatedBinaryWhether-Within-QuoteWhether a can-didate MWE iswithin singlequoteBinaryWhether-Within-BracketWhether a can-didate MWE iswithin firstbracketsBinaryOOV Whether candi-date MWE is outof vocabularyBinaryFirst-Word-InflectionWhether the firstword is inflectedBinarySecond-Word-InflectionWhether secondword is inflectedBinaryTagOf-FirstWordLexical categoryof the first wordof a candidate.XC (com-pound),NN (noun),NNP (propernoun)TagOfSe-condWordLexical categoryof the secondword of a candi-dateXC (com-pound),NN (noun),NNP (propernoun)Table2.
Syntactic and linguistic features4.4 Noun-noun MWE identification usingrandom forestRandom forest (Leo Breiman, 2000) is an ensem-ble classifier that combines the predictions ofmany decision trees using majority voting to out-put the class for an input vector.
Each decisiontree participated in ensembling chooses a subset offeatures randomly to find the best split at eachnode of the decision tree.
The method combinesthe idea of "bagging" (Leo Breiman, 1996) and therandom selection of features.
We use this algo-rithm for our multiword identification task for sev-eral reasons:  (1) For many data sets, it produces ahighly accurate classifier (Rich Caruana et al2008), (2) It runs efficiently on large databases andperforms well consistently across all dimensionsand (3) It generates an internal unbiased estimateof the generalization error as the forest buildingprogresses.The outline of the algorithm is given in the fig-ure 2.Training Random Forests for noun-noun MWEidentification requires candidate noun-noun MWEsto be represented as the feature vectors.
For thispurpose, we write a computer program for auto-matically extracting values for the features charac-terizing the noun-noun MWE candidates in thedocuments.
For each noun-noun candidate MWEin a document in our corpus, we extract the valuesof the features of the candidate using the measuresdiscussed in subsection 4.3.
If the noun-noun can-didate MWE is found in the list of manually identi-fied noun-noun MWEs, we label the MWE as a?Positive?
example and if it is not found we label itas a ?negative?
example.
Thus the feature vectorfor each candidate looks like {<a1 a2 a3 ?..
an>,<label>} which becomes a training instance (ex-ample) for the random forest, where a1, a2 .
.
.an,indicate feature values for a candidate.
A trainingset consisting of a set of instances of the aboveform is built up by running a computer program onthe documents in our corpus.For our experiment, we use Weka(www.cs.waikato.ac.nz/ml/weka) machine learningtools.
The random forest is included under thepanel Classifier/ trees of WEKA workbench.. Forour work, the random forest classifier of theWEKA suite has been run with the default valuesof its parameters.
One of the important parameters69is number of trees in the forest.
We set this parame-ter to its default value of 10.Figure 2.
Random forest learning algorithm5 Evaluation and resultsFor evaluating the performance of our system thetraditional precision, recall and F-measure arecomputed by comparing machine assigned labelsto the human assigned labels for the noun-nouncandidate MWEs extracted from our corpus of 274Bengali documents.5.1 Experimental datasetOur corpus is created by collecting the news ar-ticles from the online version of well known Ben-gali newspaper ANANDABAZAR PATRIKAduring the period spanning from 20.09.2012 to19.10.2012.
The news articles published onlineunder the section Rajya and Desh on the topicsbandh-dharmoghat, crime, disaster, jongi, mishap,political and miscellaneous are included in the cor-pus.
It consists of total 274 documents and allthose documents contain 18769 lines of Unicodetexts and 233430 tokens.
We have manually identi-fied all the noun-noun compound MWEs in thecollection and labeled the training data by assign-ing positive labels to the noun-noun compoundsand negative labels to the expressions which arenot noun-noun compounds.
It consists of 4641noun-noun compound MWEs.
Total 8210 noun-noun compound MWE candidates are automatical-ly extracted employing chunker and using heuristicrules as described in subsection 4.2.5.2 ResultsTo estimate overall accuracy of our proposednoun-noun MWE identification system, 10-foldcross validation is done.
The dataset is randomlyreordered and then split into n parts of equal size.For each of 10 iterations, one part is used for test-ing and the other n-1 parts are used for training theclassifier.
The test results are collected and aver-aged over all folds.
This gives the cross-validationestimate of the accuracy of the proposed system.J48 which is basically a decision tree included inWEKA is used as a single decision tree for com-paring our system.
The table 2 shows the estimatedaccuracy of our system.
The comparison of theperformance of the proposed random forest basedsystem to that of a single decision tree is alsoshown in table 2.
Our proposed random forestbased system gives average F-measure of 0.852which is higher than F-measure obtained by a sin-gle decision tree for bigram noun-noun compoundrecognition task.Systems Precision Recall F-measureRandomForest0.852 0.852 0.852SingleDecisionTree0.831 0.83 0.831Table 2: Comparisons of the performances of the pro-posed random forest based system and a single decisiontree based system for bigram noun-noun compound rec-ognition task.6 Conclusion and Future WorkThis paper presents a machine learning based ap-proach for identifying noun-noun compoundMWEs from a Bengali corpus.
We have used anumber of association measures, syntactic and lin-guistic information as features which are combinedRandom forest learning algorithmTraining phrase:For each of N decision trees to be built?
Select a new bootstrap samplefrom training set?
Grow an un-pruned decision treeon this bootstrap.?
While growing a decision tree, ateach internal node, randomly se-lect mtry predictors (features) anddetermine the best split using onlythese predictors.?
Do not perform pruning.
Save thedecision tree.Testing phase:For an input vector, output the class that isthe mode of the classes produced by the allindividually trained decision trees.70by a random forest learning algorithm for recog-nizing noun-noun compounds.As a future work, we have planned to improvethe noun-noun candidate MWE extraction step ofthe proposed system and/or   introduce new fea-tures such as lexical features and semantic featuresfor improving the system performance.ReferencesAdam Kilgarrif and Joseph Rosenzweig.
2000.
Frame-work and Results for English Senseval.
Computerand the Humanities, 34(1): pp 15-48.Akshar Bharati, Dipti Misra Sharma, Lakshmi Bai, Ra-jeev Sangal.
2006.
AnnCorra : Annotating CorporaGuidelines For POS And Chunk Annotation For In-dian Languages.Anoop Kunchukuttan and Om P. Damani.
2008.
A Sys-tem for Compound Noun Multiword Expression Ex-traction for Hindi.
In proceeding of 6th InternationalConference on National Language Processing(ICON).
pp.
20-29.Aswhini Agarwal, Biswajit Ray, Monojit Choudhury,Sudeshna Sarkar and Anupam Basu.
2004.
Automat-ic Extraction of Multiword Expressions in Bengali:An Approach for Miserly Resource Scenario.
In Pro-ceedings of International Conference on NaturalLanguage Processing (ICON), pp.
165-174Carlos Ramisch, Helena de Medeiros Caseli, Aline Vil-lavicencio, Andr?
Machado, Maria Jos?
Finatto: AHybrid Approach for Multiword Expression Identifi-cation.
PROPOR 2010: 65-74Fellbaum, Christine, ed.
: 1998, WordNet: An ElectronicLexical Database, Cambridge,MA: MIT Press.Frank Smadja 1993.
?Retrieving Collocation from Text:Xtract.?
Computational Linguistics.
19.1(1993):143-177.Gerlof Bouma.
2009.
"Normalized (pointwise) mutualinformation in collocation extraction."
Proceedings ofGSCL  (2009): 31-40.Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann Co-pestake and Dan Flickinger.2002.
Multi-worh ex-pression: A Pain in the neck for NLP.
CICLing,2002.Jackendoff, Ray: 1997, The Architecture of the Lan-guage Faculty, Cambridge, MA: MIT Press.Kishorjit Nongmeikapam, Ningombam Herojit Singh,Bishworjit Salam and Sivaji Bandyopadhyay.
2011.Transliteration of CRF Based Multiword Expression(MWE) in Manipuri: From Bengali Script Manipurito Meitei Mayek (Script) Manipuri.
InternationalJournal of Computer Science and Information Tech-nology, vol.2(4) .
pp.
1441-1447K.
Church and P. Hanks.
1990.
Word associationnorms, mutual information, and lexicography.
Com-putational Linguistics.
16(1).
1990.Leo Breiman .
1996.
"Bagging predictors".
MachineLearning 24 (2): 123?140.Leo Breiman .
2001.
"Random Forests".
MachineLearning 45 (1): 5?32.M.C.
Diaz-Galiano, M.T.
Martin-Valdivia, F. Martinez-Santiago, L.A. Urea-Lopez.
2004.
Multiword Ex-pressions Recognition with the LVQ Algorithm.Workshop on methodologies and evaluation of Mul-tiword Units in Real-word Applications associatedwith the 4th International Conference on LanguagesResources and Evaluation, Lisbon, Portugal.
pp.12-17Paul Rayson, Dawn Archer, Scott Piao and Tony McE-nery.
2004.
The UCREL semantic analysis system.
InProceedings of the LREC-04 Workshop, beyondNamed Entity Recognition Semantic Labelling forNLP Tasks, Lisbon, Portugal, pp.7-12.Pavel Pecina.
2008.
Reference data for czech colloca-tion extraction.
In Proc.
Of the LREC Workshop To-wards a Shared Task for MWEs (MWE 2008).
pp.
11-14, Marrakech, Morocco, Jun.Rich Caruana, Nikos Karampatziakis and Ainur Yesse-nalina (2008).
"An empirical evaluation of super-vised learning in high dimensions".
Proceedings ofthe 25th International Conference on MachineLearning (ICML).R.
Mahesh and K. Sinha.
2011.
Stepwise Mining ofMulti-Word Expressions in Hindi.
Proceedings of theWorkshop on Multiword Expressions: from Parsingand Generation to the Real World (MWE 2011) pp.110-115Sandipan Dandapat, Pabitra Mitra and Sudeshna Sarkar.2006.
Statistical Investigation of Bengali Noun-Verb(N-V) Collocations as Multi-word expressions.
Inthe Proceedings of MSPIL, Mumbai, pp 230-233.Santanjeev Banerjee and Ted Pedersen.
2003.
?The De-sign, Implementation and Use of the Ngram StaisticsPackage.?
Proceedings of the Fourth InternationalConference on Intelligent Text Processing and Com-putational Linguistics.
Pp.
370-381Scott Songlin Piao, Paul Rayson, Dawn Archer, TonyMcEnery.
2005.
Comparing and combining a seman-tic tagger and a statistical tool for MWE extraction.Computer Speech and Language (ELSEVIER) 19(2005) pp.
378-397Sriram Venkatapathy, Preeti Agrawal and Aravind K.Joshi.
Relative Compositionality of Noun+Verb Mul-ti-word Expressions in Hindi.
In Proceedings ofICON-2005, Kanpur.Takaaki Tanaka, Timothy Baldwin.
2003.
?Noun-NounCompound Machine Translation: a Feasibility Studyon  Shallow Processing.?
Proceeings of the ACL2003 workshop on Multiword expressions.
pp.
17-2471Tanmoy Chakraborty.
2010.
Identification of Noun-Noun(N-N) Collocations as Multi-Word Expressionsin Bengali Corpus.
8th International Conference onNatural Language Processing (ICON 2010).Tanmoy Chakraborty and Sivaji Bandyopadhyay.
2010.Identification of Reduplication in Bengali Corpusand their Semantic Analysis: A Rule-Based Ap-proach.
Proceedings of Workshop on Multiword Ex-pressions: from Theory to Applications (MWE 2010)pp.
72-75Tanmoy Chakraborty, Dipankar Das and Sivaji Ban-dyopadhyay.
2011.
Semantic Clustering: an Attemptto Identify Multiword Expressions in Bengali.
Pro-ceedings of Workshop on Multiword Expressions:from Parsing and Generation to the RealWorld(MWE 2011).
Association for ComputationalLinguistics.
Portland, Oregon, USA, 23 June 2011.Ted Dunning.
1993.
Accurate Method for the Statisticof Surprise and Coincidence.
In Computational Lin-guistics, pp.
61-74Timothy Baldwin and Su Nam Kim (2010), in NitinIndurkhya and Fred J. Damerau (eds .)
Handbook ofNatural Language Processing, Second Edition, CRCPress, Boca Raton, USA, pp.
267-292.72
