A Hybrid Approach to Chinese Word Segmentation around CRFsZHOU Jun-sheng1, 2  DAI Xin-yu1  NI Rui-yu1  CHEN Jia-jun11Department of Computer Science and Technology, Nanjing University, Nanjing, 210093 CHINA2Deptartment of Computer Science, Nanjing Normal University, Nanjing, 210097 CHINA{Zhoujs, dxy, niry, chenjj}@nlp.nju.edu.cnAbstractIn this paper, we present a Chinese wordsegmentation system which is consisted offour components, i.e.
basic segmentation,named entity recognition, error-drivenlearner and new word detector.
The basicsegmentation and named entity recognition,implemented based on conditional randomfields, are used to generate initialsegmentation results.
The other twocomponents are used to refine the results.Our system participated in the tests onopen and closed tracks of BeijingUniversity (PKU) and Microsoft Research(MSR).
The actual evaluation results showthat our system performs very well in MSRopen track, MSR closed track and PKUopen track.1 IntroductionWord segmentation is the first step in ChineseNLP, but segmentation of the Chinese text intowords is a nontrivial task.
Three difficult tasks,i.e.
ambiguities resolution, named entityrecognition and new word identification, arethe key problems to word segmentation inChinese.In this paper, we report a Chinese wordsegmentation system using a hybrid strategy.
Inour system, texts are segmented in four steps:basic segmentation, named entity recognition,error-driven learning and new word detection.The implementations of basic segmentationcomponent and named entity recognitioncomponent are both based on conditionalrandom fields (CRFs) (Lafferty et al, 2001),while the Error-Driven learning component andnew word detection component use statisticaland rule methods.
We will describe each ofthese steps in more details below.2 System Description2.1 Basic segmentationWe implemented the basic segmentationcomponent with linear chain structure CRFs.CRFs are undirected graphical models thatencode a conditional probability distributionusing a given set of features.
In the special casein which the designated output nodes of thegraphical model are linked by edges in a linearchain, CRFs make a first-order Markovindependence assumption among output nodes,and thus correspond to finite state machines(FSMs).
CRFs define the conditional probabilityof a state sequence given an input sequence as??????
?
?$TtKkttkkotossfZosP1 11 ),,,(exp1)|( OWhere  is an arbitraryfeature function over its arguments, and?Nis a learned weight for each feature function.
),,,( 1 tossf ttk Based on CRFs model, we cast thesegmentation problem as a sequence taggingproblem.
Different from (Peng et al, 2004), werepresent the positions of a hanzi (Chinesecharacter) with four different tags: B for a hanzi196that starts a word, I for a hanzi that continues theword, F for a hanzi that ends the word, S for ahanzi that occurs as a single-character word.
Thebasic segmentation is a process of labeling eachhanzi with a tag given the features derived fromits surrounding context.
The features used in ourexperiment can be broken into two categories:character features and word features.
Thecharacter features are instantiations of thefollowing templates, similar to those described in(Ng and Jin, 2004), C refers to a Chinese hanzi.
(a) Cn (n ?2,?1,0,1,2 )(b) CnCn+1( n ?2,?1,0,1) (c) C?1C1(d) Pu(C0 )In addition to the character features, we cameup with another type word context feature whichwas found very useful in our experiments.
Thefeature captures the relationship between thehanzi and the word which contains the hanzi.
Fora two-hanzi word, for example, the first hanzi??
?
within the word ????
will have thefeature WC0=TWO_F set to 1, the second hanzi???
within the same word ????
will have thefeature WC0=TWO_L set to 1.
For thethree-hanzi word, for example, the first hanzi???
within a word ?????
will have thefeature WC0=TRI_F set to 1, the second hanzi???
within the same word ?????
will havethe feature WC0=TRI_M set to 1, and the lasthanzi ???
within the same word ?????
willhave the feature WC0=TRI_L set to 1.
Similarly,the feature can be extended to a four-hanzi word.2.2 Named Entity recognitionAfter basic segmentation, a great number ofnamed entities in the text, such as personalnames, location names and organization names,are not yet segmented and recognized properly.So the second step we take is named entityrecognition based on CRFs.
In contrast toChinese personal names and location name, therecognition of Chinese organization names is adifficult task.
Especially in Microsoft Researchcorpus, the whole organization name, such as???????
?, ???????????????????????
and so on, is regardedas a single word.
In this section, we onlypresent our approach for organization namerecognition.The important factor in applying CRFs modelto organization name recognition is how to selectthe proper features set.
The constitution ofChinese organization is very complicated, andmost organization names do not have anycommon structural characteristics except forcontaining some feature words, such as ????
and so on.
But as a proper noun, theoccurrence of an organization name has thespecific context.
The context information oforganization name mainly includes the boundarywords and some title words (e.g.
??????
).By analyzing a large amount of organizationname corpus, we find that the indicative intensityof different boundary words vary greatly.
So wedivide the left and right boundary words into twoclasses according to the indicative intensity.Accordingly we construct the four boundarywords lexicons.
To solve the problem of theselection and classification of boundary words,we make use of mutual Information I(x, y).
Ifthere is a genuine association between x and y,then I(x,y) >> 0.
If there is no interestingrelationship between x and y, then I(x,y)?0.
If xand y are in complementary distribution, thenI(x,y) << 0.
By using mutual information, wecompute the association between boundary wordand the type of organization name, then selectand classify the boundary words.In order to increase the precision oforganization name recognition, we stillintroduce the ?forbidden word?
feature thatwould prevent some words from beingrecognized as component of organization name.197For we know that some words, such as ???
??
???
?, are impossible to occur inorganization name, we collected these words toformed a ?forbidden words?
lexicon.
Based onthe consideration given in preceding section,we constructed a set of atomic feature patterns,listed in table 2.
Additionally, we defined a setof conjunctive feature patterns, which couldform effective feature conjunctions to expresscomplicated contextual information.2.3 Error-driven learningAs a method based on statistics, no matter howwell a CRFs model is constructed, someobviously errors always occurred because ofthe sparseness of training data.
For this reson,error-driven learning method (Brill, 1995) isadopted to refine the segmentation result in thisbakeoff in three steps:1) Based on CRFs model, we segment thetraining data which has been removed all thespace between words.
Based on the comparisonof the segmentation result with the originaltraining data, the difference between them willbe extracted.
If a difference occurs more thanone time, an error-driven rule will beconstructed.
The rule is described as: ?
?ZZZZZZis the segmentation of ?intraining data.
We named this rule setconstructed by this step CRF-Ruleset.2) Based on FMM&BMM, we segment thetraining data which has been removed all thespace between words.
As we know,overlapping ambiguity strings can be foundthrough FMM&BMM, and the truesegmentation of such OASs can be found intraining data.
If an OAS string has uniquesegmentation, a rule ?
?
 ZZZ wasconstructed.
We called the rule set constructedin this step OAS-Ruleset.3) In the testing data, if there is the samestring as ?in CRFs-Ruleset or OAS-Ruleset,it will be segmented as ZZZ according tothe rule?
?ZZZ.For example, in the PKU testing data,through error-driven learning, we can segmentthe string ???????
as ??????
?while this string is always segmented wrong as???????
segmented by CRFs model.In other words, error-driven learning canalways can be seen as a consistency check.
Itassures the consistency of the segmentation ofthe training data and testing data when somestrings such as ???????
occur in both.2.4 New word detectionCRFs segmentation model can gives goodperformance on OOV words identification.
Butthere are still some new words that have notbeen recognized.
So an additive new wordsrecognizer is adopted (Chen, 2003).In-word probability of each character is usedfor new word detection.
The in-wordprobability of a character is a probability thatthe character occurs as a part of a word of twoor more characters.
And the in-wordprobability of a character is trained from thetraining data and is calculated as follows:( )in wordNumber of C Occurrence in wordsP CNumber of C Occurrence.The consecutive single characters arecombined into a new word if the in-wordprobability of each single character is over athreshold.
Obviously, the value of the thresholdis the key to the performance of this new wordsrecognizer.
Same as (Chen, 2003), we dividedthe training data as training data anddeveloping data to find an exactly value of thethreshold.
For this bakeoff, we set the thresholdof PKU data as 0.86 and that of MSR data as0.88.
Some new words such as ?????
????
????
were recognized by this recognizer.3 Experimental results198We participated in the four GB tracks in thesecond international Chinese wordsegmentation bakeoff: PKU-open, PKU-closed,MSR-open, MSR-closed.
In the closed tracks,we used the dictionary with the wordsappearing in the training corpus and didn?tconduct the process of named entityrecognition.
In the open tracks, we employed adictionary of 134,458 entries.
The size oftraining data used in the open tracks is same asthe closed tracks.
Except for a dictionary withmore vocabulary, we have not employed anyother special resources in the open tracks.Table 1 shows the performance of our systemin the bakeoff.PKU(open)PKU(closed)MSR(open)MSR(closed)Precision 0.970 0.950 0.971 0.956Recall 0.964 0.941 0.959 0.959F 0.967 0.946 0.965 0.957OOV 0.058 0.058 0.026 0.026Recallon OOV 0.864 0.813 0.785 0.496Table 1: Official Bakeoff OutcomeIt?s a pity that we make a careless mistake (aprogram bug) which led to 752 left quotationmarks concatenated to the words following it inthe closed and open tracks on Microsoftresearch corpus.
With the problem fixed, theactual results of the official test data are betterthan any other system, as shown in Table 2.MSR (open) MSR (closed)Precision 0.978 0.957Recall 0.976 0.976F 0.977 0.966OOV 0.026 0.026Recall onOOV 0.772 0.387Recall onIn-Voc 0.982 0.992Table 2  Actual evaluation on MSR corpus4 ConclusionOur open and closed GB track experimentsshow that its performance is competitive.
Themost important advantage of our system is theability to cope with the unknown words.Especially in the open track on the Microsoftresearch corpus, the recall on OOV of oursystem achieves 77.2%, higher than any othersystem.
In future work, we would attempt togeneralize the ideas of large-margin to CRFsmodel, leading to new optimal trainingalgorithms with stronger guarantees againstoverfitting.ReferencesEric Brill , 1995.
Transformation Based Error DrivenLearning and Natural Language Processing : ACase Study in Part of Speech Tagging ,Computational Linguistics , V21.
No.
4 ,J. Lafferty, A. McCallum, and F. Pereira.
2001.Conditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In Proc.ICML 01.Aitao Chen.
2003.
Chinese Word Segmentation UsingMinimal Linguistic Knowledge.
In Proceedings ofthe Second SIGHAN Workshop on ChineseLanguage Processing.Ng, Hwee Tou and Jin Kiat Low.
2004.
ChinesePart-of-Speech Taging: One-at-a-Time or All atOnce?
Word-based or Character based?
InProceedings of the Conference on EmpiricalMethods in Natural Language Processing, Spain.Peng, Fuchun, Fangfang Feng, and AndrewMcCallum.
2004.
Chinese Segmentation and NewWord Detection using Conditional Random Fields .In Proceedings of the Twentith InternationalConference on Computaional Linguistics, pages562?568.199
