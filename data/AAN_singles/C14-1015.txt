Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 141?150, Dublin, Ireland, August 23-29 2014.Co-learning of Word Representations and Morpheme RepresentationsSiyu QiuNankai UniversityTianjin, 300071, Chinappqq2356@gmail.comQing CuiTsinghua UniversityBeijing, 100084, Chinacuiq12@mails.tsinghua.edu.cnJiang BianMicrosoft ResearchBeijing, 100080, Chinajibian@microsoft.comBin GaoMicrosoft ResearchBeijing, 100080, Chinabingao@microsoft.comTie-Yan LiuMicrosoft ResearchBeijing, 100080, Chinatyliu@microsoft.comAbstractThe techniques of using neural networks to learn distributed word representations (i.e., wordembeddings) have been used to solve a variety of natural language processing tasks.
The re-cently proposed methods, such as CBOW and Skip-gram, have demonstrated their effectivenessin learning word embeddings based on context information such that the obtained word embed-dings can capture both semantic and syntactic relationships between words.
However, it is quitechallenging to produce high-quality word representations for rare or unknown words due to theirinsufficient context information.
In this paper, we propose to leverage morphological knowledgeto address this problem.
Particularly, we introduce the morphological knowledge as both ad-ditional input representation and auxiliary supervision to the neural network framework.
As aresult, beyond word representations, the proposed neural network model will produce morphemerepresentations, which can be further employed to infer the representations of rare or unknownwords based on their morphological structure.
Experiments on an analogical reasoning task andseveral word similarity tasks have demonstrated the effectiveness of our method in producinghigh-quality words embeddings compared with the state-of-the-art methods.1 IntroductionWord representation is a key factor for many natural language processing (NLP) applications.
In theconventional solutions to the NLP tasks, discrete word representations are often adopted, such as the1-of-v representations, where v is the size of the entire vocabulary and each word in the vocabularyis represented as a long vector with only one non-zero element.
However, using discrete word vectorscannot indicate any relationships between different words, even though they may yield high semanticor syntactic correlations.
For example, while careful and carefully have quite similar semantics, theircorresponding 1-of-v representations trigger different indexes to be the hot values, and it is not explicitthat careful is much closer to carefully than other words using 1-of-v representations.To deal with the problem, neural network models have been widely applied to obtain word repre-sentations.
In particular, they usually take the 1-of-v representations as the word input vectors in theneural networks, and learn new distributed word representations in a low-dimensional continuous em-bedding space.
The principle of these models is that words that are highly correlated in terms of eithersemantics or syntactics should be close to each other in the embedding space.
Representative works inthis field include feed-forward neural network language model (NNLM) (Bengio et al., 2003), recurrentneural network language model (RNNLM) (Mikolov et al., 2010), and the recently proposed continuesbag-of-words (CBOW) model and continues skip-gram (Skip-gram) model (Mikolov et al., 2013a).However, there are still challenges for using neural network models to achieve high-quality wordembeddings.
First, it is difficult to obtain word embeddings for emerging words as they are not includedin the vocabulary of the training data.
Some previous studies (Mikolov, 2012) used one or more defaultindexes to represent all the unknown words, but such solution will lose information for the new words.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/141Second, the embeddings for rare words are often of low quality due to the insufficient context informationin the training data.Fortunately, semantically or syntactically similar words often share some common morphemes suchas roots, affixes, and syllables.
For example, probably and probability share the same root, i.e., probab,as well as the same syllables, i.e., pro and ba.
Therefore, morphological information can provide valu-able knowledge to bridge the gap between rare or unknown words and well-known words in learningword representations.
In this paper, we propose a novel neural network architecture that can leveragemorphological knowledge to obtaining high-quality word embeddings.
Specifically, we first segment thewords in the training data into morphemes, and then employ the 1-of-v representations of both the wordsand their morphemes as the input to the neural network models.
In addition, we propose to use mor-phological information as auxiliary supervision.
Particularly, in the output layer of the neural networkarchitecture, we predict both the words and their corresponding morphemes simultaneously.
Moreover,we introduce extra coefficients into the network to balance the weights between word embeddings andmorpheme embeddings.
Therefore, in the back propagation stage, we will update the word embeddings,the morpheme embeddings, and the balancing coefficients simultaneously.Our proposed neural network model yields two major advantages: on one hand, it can leverage threetypes of co-occurrence information, including co-occurrence between word and word (conventional),co-occurrence between word and morpheme (newly added), and co-occurrence between morpheme andmorpheme (newly added); on the other hand, this new model allows to learn word embeddings andmorpheme embeddings simultaneously, so that it is convenient to build the representations for unknownwords from morpheme embeddings and enhance the representations for rare words.
Experiments onlarge-scale public datasets demonstrate that our proposed approach can help produce improved wordrepresentations on an analogical reasoning task and several word similarity tasks compared with thestate-of-the-art methods.The rest of the paper is organized as follows.
We briefly review the related work on word embeddingusing neural networks in Section 2.
In Section 3, we describe the proposed methods to leverage mor-phological knowledge in word embedding using neural network models.
The experimental results arereported in Section 4.
The paper is concluded in Section 5.2 Related WorkNeural Language Models (NLMs) (Bengio et al., 2003) have been applied in a number of NLP tasks (Col-lobert and Weston, 2008) (Glorot et al., 2011) (Mikolov et al., 2013a) (Mikolov et al., 2013b) (Socheret al., 2011) (Turney, 2013) (Turney and Pantel, 2010) (Weston et al., ) (Deng et al., 2013) (Collobertet al., 2011) (Mnih and Hinton, 2008) (Turian et al., 2010).
In general, they learn distributed word rep-resentations in a continuous embedding space.
For example, Mikolov et al.
proposed the continuousbag-of-words model (CBOW) and the continuous skip-gram model (Skip-gram) (Mikolov et al., 2013a).Both of them assume that words co-occurring with the same context should be similar.
Collobert etal.
(Collobert et al., 2011) fed their neural networks with extra features such as the capital letter featureand the part-of-speech (POS) feature, but they still met the challenge of producing high-quality wordembeddings for rare words.Besides using neural network, many different types of models were proposed for estimating continuousrepresentations of words, such as the well-known Latent Semantic Analysis (LSA) and Latent DirichletAllocation (LDA).
However, Mikolov et al.
(Mikolov et al., 2013c) have shown that words learned byneural networks are signicantly better than LSA for preserving linear regularities while LDA becomescomputationally expensive on large datasets.There were a lot of previous attempts to include morphology in continuous models, especially inthe speech recognition field.
Represent works include Letter n-gram (Sperr et al., 2013) and feature-rich DNN-LMs (Mousa et al., 2013).
The first work improves the letter-based word representation byreplacing the 1-of-v word input of restricted Boltzman machine with a vector indicating all n-grams oforder n and smaller that occur in the word.
Additional information such as capitalization is added as well.In the model of feature-rich DNN-LMs, the authors expand the inputs of the network to be a mixture of142selected full words and morphemes together with their features such as morphological tags.
Both ofthese works intend to capture more morphological information so as to better generalize to unknown orrare words and to lower the out-of-vocabulary rate.There are some other related works that consider morphological knowledge when learning the wordembeddings, such as factored NLMs (Alexandrescu and Kirchhoff, 2006) and csmRNN (Luong et al.,2013), both of which are designed to handle rare words.
In factored NLMs, each word is viewed as avector of shape features (e.g., affixed, capitalization, hyphenation, and classes) and a word is predictedbased on several previous vectors of factors.
Although they made use of the co-occurrence of morphemesand words, the context information is lost after chopping the words and feeding the neural network withmorphemes.
In our model, we also utilize the co-occurrence information between morphemes, which hasnot been investigated before.
In csmRNN, Luong et al proposed a hierarchical model considering theknowledge of both morphological constitutionality and context.
The hierarchical structure looks moresophisticated, but the relatedness of words with morphological similarity are weaken by layers whencombining morphemes into words.
In addition, the noise accumulated in the hierarchical structure inbuilding a word might be propagated to the context layer.
In our model, the morphological and contextualknowledge are combined in parallel, and their contributions to the input vector are decided by a pair oflearned tradeoff coefficients.3 The Morpheme powered CBOW ModelsIn this section, we introduce the architecture of our proposed neural network model based on the CBOWmodel.
In CBOW (see Figure 1), a sliding window is employed on the train text stream to obtain the train-ing samples.
In each sliding window, the model aims to predict the central word using the surroundingwords as the input.
Specifically, the input words are represented in the 1-of-v format.
In the feed-forwardprocess, these input words are first mapped into the embedding space by the same weight matrix M , andthen the embedding vectors are summed up to a combined embedding vector.
After that, the combinedembedding vector is mapped back to the 1-of-v space by another weight matrix M?, and the resultingvector is used to predict the central word after conducting softmax on it.
In the back-propagation process,the prediction errors are propagated back to the network to update the two weight matrices.
After thetraining process converges, the weight matrix M is regarded as the learned word representations.SUM?1?????1???????
??
?0Embedding Matrix1-of-?
representationEmbedding Space(?-dimension)Vocabulary Space(?-dimension)Vocabulary Space(?-dimension)Projection MatrixFigure 1: The CBOW model.143In our proposed model, we address the challenge of producing high-quality word embeddings for rarewords and unknown words by leveraging the three types of co-occurrence information between wordsand morphemes.On the input side, we segment the words into morphemes and put both the words and the morphemesas input.
That is, the vocabulary for the 1-of-v representation contains both words and morphemes.As shown in Figure 2, the surrounding words in the sliding window are w?s, ?
?
?
, w?1, w1, ?
?
?
, wsandtheir corresponding morphemes arem?s,1,m?s,2, ?
?
?
,m?s,t?s; ?
?
?
;m?1,1,m?1,2, ?
?
?
,m?1,t?1;m1,1,m1,2, ?
?
?
, m1,t1; ?
?
?
;ms,1, ms,2, ?
?
?
, ms,ts, where 2s is the number of the surrounding words and tiisthe number of morphemes for wi(i = ?s, ?
?
?
,?1, 1, ?
?
?
, s).
Note that tidepends on the formation ofwiso that it may vary from word to word.
If a word is also a morpheme, there will be two embeddingvectors which are tagged differently.
We use vwiand vmi,jto represent the 1-of-v vectors of word wiandmorpheme mi,jrespectively.
On the input side, both the words and their morphemes are mapped intothe embedding space by the same weight matrix M , and then the weighted sum vIof the combination ofword embeddings and the combination of morpheme embeddings is calculate as below,vI= ?w?s?i=?si 6=0vwi+ ?m?s?i=?si 6=0ti?j=1vmi,j,where ?wand ?mare the tradeoff coefficients between the combination of word embeddings and thecombination of morpheme embeddings.On the output side, we map the combined embedding vector vIback to the 1-of-v space by anotherweight matrix M?to do the prediction.
We have four settings of the structure.
In the first setting, we onlypredict the central wordw0, and we name the model under this setting as MorphemeCBOW.
In the secondsetting, we predict both the central word w0and its morphemes m0,1,m0,2, ?
?
?
,m0,t0, and we name thissetting as MorphemeCBOW+.
In the above two settings, the tradeoff weights ?wand ?mare fixed.
Ifwe update the two weights in the learning process of MorphemeCBOW, we will get the third setting andwe name it as MorphemeCBOW*, while updating the two weights in MorphemeCBOW+ yields the forthsetting named MorphemeCBOW++ .Take MorphemeCBOW+ as example, the objective is to maximize the following conditional co-occurrence probability,log(P (w0| {wi}, {mi,j})) + log(t0?j=1P (m0,j| {wi}, {mi,j})), (1)where {wi}, {mi,j} represent the bag of words and bag of morphemes separately.
The conditional prob-ability in the above formula is defined using the softmax function,P (w0| {wi}, {mi,j}) =exp(v?Tw0?
vI)?v??VOexp(v?T?
vI), P (m0,j| {wi}, {mi,j}) =exp(v?Tm0,j?
vI)?v??VOexp(v?T?
vI), (2)where VOis the set of the output representations for the whole vocabulary; v?is used to differentiate withinput representations; and v?w0, v?m0,jrepresent the output embedding vectors ofw0andm0,jrespectively.Usually, the computation cost for Formula (2) is expensive since it is proportional to the vocabularysize.
In our model, we use negative sampling discussed in (Mikolov et al., 2013b) to speed up thecomputation.
Particularly, we random select k negative samples u1, u2, ?
?
?
, ukfor each prediction target(word or morpheme).
By using this technique, Formula (1) can be equally written as,G(vI) ?
log ?(v?Tw0?
vI) +t0?j=1log ?(v?Tm0,j?
vI) +k?i=1ui6=w0ui6=?m0,jEui?Pn(u)[log ?(?v?Tui?
vI)],144where ?
denotes the logistic function, and Pn(u) is the vocabulary distribution used to select the negativesamples.
Pn(u) is set as the 3/4rd power of the unigram distribution U(u)1.
The negative samples shouldnot be the same as any of the prediction targetsw0andm0,j(j = 1, ?
?
?
, t0).
By using negative sampling,the training time spent on summing up the whole vocabulary in Formula (2) is greatly reduced so that itbecomes linear with the number of the negative samples.
Thus, we can calculate the gradient of G(vI)as below,?G(vI)?vI=(1?
?(v?Tw0?
vI)) ??(v?Tw0?
vI)?vI+t0?j=1(1?
?(v?Tm0,j?
vI)) ??(v?Tm0,j?
vI)?vI?k?i=1ui6=w0ui6=?m0,j[?(v?Tui?
vI) ??(v?Tui?
vI)?vI].In the back-propagation process, the weights in the matricesM andM?are updated.
When the trainingprocess converges, we take the matrix M as the learned word embeddings and morpheme embeddings.??????
??????????????
???????????????????????????????1???????????????1??????????
????1???
????
?0 0??1???
0??
?Projection MatrixEmbedding MatrixEmbedding Space(?-dimension)Vocabulary Space(?-dimension)1-of-?
representationWord + MorphemesVocabulary Space(?-dimension)Bag of MorphemesBag of WordsFigure 2: The proposed neural network model.4 Experimental EvaluationIn this section we test the effectiveness of our model in generating high-quality word embeddings.
Wefirst introduce the experimental settings, and then we report the results on one analogical reasoning taskand several word similarity tasks.4.1 DatasetsWe used two datasets for training: enwiki92and wiki20103.1http://www.cs.bgu.ac.il/?yoavg/publications/negative-sampling.pdf2http://mattmahoney.ent/dc/enwik9.zip3http://www.psych.ualberta.ca/?westburylab/downloads/westburylab.wikicorp.download.html145?
The enwiki9 dataset contains about 123.4 million words.
We used Matt Mahoney?s text pre-processing script4to process the corpus.
Thus, we removed all non-Roman characters and mappedall digits to English words.
In addition, words occurred less than 5 times in the training corpus werediscarded.
We used the learned word embeddings from enwiki9 to test an analogical reasoning taskdescribed in (Mikolov et al., 2013a).?
The wiki2010 dataset contains about 990 million words.
The learned embeddings from this datasetwere used on word similarity tasks as it was convenient to compare with the csmRNN model (Luonget al., 2013).
We did the same data pre-processing as csmRNN did.
That is, we removed all non-Roman characters and mapped all digits to zero.4.2 SettingsIn the analogical reasoning task, we used the CBOW model as the baseline.
In both CBOW and ourproposed model, we set the context window size to be 5, and generated three dimension sizes (100, 200,and 300) of word embeddings.
We used negative sampling (Mikolov et al., 2013b) in the output layerand the number of negative samples is chosen as 3.In the word similarity tasks, we used the csmRNN model as the baseline.
The context window size ofour model was set to be 5.
To make a fair comparison with the csmRNN model, we conducted the samesettings in our experiments as csmRNN.
First, as csmRNN used the Morfessor (Creutz and Lagus, 2007)method to segment words into morphemes, we also used Morfessor as one of our word segmentationmethods to avoid the influence caused by the segmentation methods.
Second, as csmRNN used twoexisting embeddings C&W5(Collobert et al., 2011) and HSMN6(Huang et al., 2012) to initialize thetraining process, we also used the two embeddings as the initial weights of M in our experiments.
Third,we set the dimension of the embedding space to 50 as csmRNN did.In our model, we employed three methods to segment a word into morphemes.
The first method iscalled Morfessor, which is a public tool implemented based on the minimum descriptions length algo-rithm (Creutz and Lagus, 2007).
The second method is called Root, which segments a word into rootsand affixes according to a predefined list in Longman Dictionaries.
The third method is called Syllable,which is implemented based on the hyphenation tool proposed by Liang (Liang, 1983).
Besides, the ar-chitecture of the proposed model can be specified into four types: MorphemeCBOW, MorphemeCBOW*,MorphemeCBOW+, and MorphemeCBOW++.
For the model MorphemeCBOW and MorphemeCBOW+with fixed tradeoff coefficients, we set the weights ?wand ?mto be 0.8 and 0.2 respectively; while forthe other two models with updated tradeoff weights, the weights ?wand ?mare initialized as 1.
Theseweight settings are chosen empirically.4.3 Evaluation Tasks4.3.1 Analogical reasoning taskThe analogical reasoning task was introduced by Mikolov et al (Mikolov et al., 2013a).
All the questionsare in the form ?a is to b is as c is to ?
?, denoted as a : b?
c : ?.
The task consists of 19,544 questionsinvolving semantic analogies (e.g., England: London ?
China: Beijing) and syntactic analogies (e.g.,amazing: amazingly?
unfortunate: unfortunately).
Suppose that the corresponding vectors are?
?a ,?
?b ,and?
?c , we will answer the question by finding the word with the representation having the maximumcosine similarity to vector?
?b ??
?a +?
?c , i.e,maxx?V,x 6=b,x 6=c(?
?b ??
?a +?
?c )T?
?xwhere V is the vocabulary.
Only when the computed word is exactly the answer word in evaluation setcan the question be regarded as answered correctly.4http://mattmahoney.net/dc/textdata.html5http://ronan.collobert.com/senna/6http://ai.stanford.edu/?ehhuang/1464.3.2 Word similarity taskThe word similarity task was tested on five evaluation sets: WS353 (Finkelstein et al., 2002),SCWS* (Huang et al., 2012), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965)and RW (Luong et al., 2013), which contain 353, 1,762, 30, 65 and 2,034 pairs of words respectively.Table 1 shows some statistics about the datasets.
Furthermore, the words in WS353, MC, RG are mostlyfrequent words, while SCWS* and RW have much more rare words and unknown words (i.e., unseenwords in the training corpus) than the first three sets.
The word distributions of these datasets are shownin Figure 3, from which we can see that RW contains the largest number of rare and unknown words.For the unknown words, we segmented them into morphemes, and calculated their word embeddings bysumming up their corresponding morpheme embeddings.
Each word pair in these datasets is associatedwith several human judgments on similarity and relatedness on a scale from 0 to 10 or 0 to 4.
For ex-ample, (cup, drink) received an average score of 7.25, while (cup, substance) received an average scoreof 1.92.
To evaluate the quality of the learned word embeddings, we computed Spearman?s ?
correlationbetween the similarity scores calculated on the learned word embeddings and the human judgments.Figure 3: Word distribution by frequency.
Distinct words in each test dataset are grouped accordingto frequencies.
The figure shows the percentage of words in each bin.Table 1: Statistics on the word similarity evaluation sets.Dataset Number of pairs Number of words Percentage of multi-segments words by MorfessorWS353 353 437 28.15%SCWS* 1726 1703 34.00%RW 2034 2951 69.06%4.4 Experimental Results4.4.1 Results on analogical reasoning taskThe experimental results on the analogical reasoning task are shown in Table 2, including semanticaccuracy, syntactic accuracy, and total accuracy of all competition settings.
Semantic/syntactic accuracyrefers to the number of correct answers over the total number of all semantic/syntactic questions.
Fromthe results, we have the following observations:?
In MorphemeCBOW, we used the surrounding words and their morphemes to predict the centralword.
The total accuracies are all improved compared with baseline using the three word segmen-tation methods across three different dimensions of the embedding space.
Generally, the improve-ments on semantic accuracies are less than those on syntactic accuracies.
The reason is that themorphological information favors more for the syntactic tasks than the semantic tasks.
Further-147more, the Root method achieved the best among the three segmentation methods, showing that theroots and affixes from the dictionary can help produce a high-quality morpheme segmentation tool.?
In MorphemeCBOW*, we predicted the central word, and updated the tradeoff coefficients inthe learning process.
We can see that the results are comparable or slightly better than Morphe-meCBOW using the three word segmentation methods across three different dimensions of theembedding space, showing that updating the tradeoff coefficients may further boost the model per-formance under some specific settings.?
In MorphemeCBOW+, we predicted both the central word and its morphemes.
MorphemeCBOW+can provide slightly better results compared with MorphemeCBOW and MorphemeCBOW*, indi-cating that putting morphemes (especially roots) in the output layer can do extra help in generatinghigh-quality word embeddings.?
In MorphemeCBOW++, we predicted the central word and its morphemes, and updated the trade-off coefficients in the learning process.
The performance under all of the three word segmentationmethods got further improved compared with MorphemeCBOW+.
It tells that the contributionsfrom words and morphemes are different to the analogical reasoning task.
According to our obser-vations, the weight for words is usually higher than that for morphemes.?
By comparing MorphemeCBOW with MorphemeCBOW* as well as MorphemeCBOW+ with Mor-phemeCBOW++, we can observe that updating the weights of tradeoff coefficients seem to essen-tially boost syntactic accuracy by trading off a bit of semantic accuracy.
As introduced in Section4.2, in the fixed weight model the ratio of weight of morphemes to the weight of word is 0.25; whileour experiment records show that the averaged ratio are 0.43 if the two weights are updated, mean-ing that the weight of the combination of morphemes increases and the contribution of the originalword to the final combined embedding decreased.
As a result, the syntactic accuracy which largelyreflected in the morphological structure of a word increased, but the semantic accuracy hurts a little.4.4.2 Results on word similarity taskExperimental results on the word similarity tasks are shown in Table 37,where the labels of C&W + csm-RNN and HSMN + csmRNN mean that using C&W and HSMN to initialize csmRNN model as what hadbeen introduced in the paper of Luong et al.
In our experiments, the architecture of MorphemeCBOW*performs the best, so we only show the results related to MorphemeCBOW* in the table.
We have thefollowing observations from the results:?
On WS353, MC, RG, and SCWS*, MorphemeCBOW* performs consistently better than the csm-RNN model, showing that our model can achieve better representations for common words.7csmRNN embeddings are available on http://www-nlp.stanford.edu/?lmthang/morphoNLM/, Perfor-mances are tested based on the two embeddings.Table 2: Performance of leveraging morphological information on the analogical reasoning task.
(a) BaselineDimension (%) CBOW100 Total 26.49Semantic 17.51Syntactic 33.96200 Total 30.50Semantic 19.71Syntactic 39.46300 Total 29.04Semantic 17.58Syntactic 38.56(b) MorphemeCBOWMorfessor Syllable Root31.99 31.28 32.4919.44 18.76 21.7742.42 41.68 41.4034.04 34.71 36.2919.10 19.13 22.4546.45 47.65 47.7931.27 32.45 36.1215.45 15.63 20.7944.41 46.44 48.86(c) MorphemeCBOW*Morfessor Syllable Root33.07 31.16 34.0415.20 15.68 17.8747.92 44.02 47.4834.69 33.13 36.5011.53 15.91 18.9253.92 47.44 51.1031.21 32.16 35.638.85 12.54 15.7549.79 48.47 52.14(d) MorphemeCBOW+Morfessor Syllable Root33.26 31.12 32.7722.82 20.80 22.7941.93 39.70 41.0738.28 39.32 39.5325.94 27.99 28.2948.52 48.74 48.8638.01 39.56 39.7025.11 26.94 27.8048.72 50.05 49.58(e) MorphemeCBOW++Morfessor Syllable Root38.86 34.42 35.7821.12 22.58 22.4353.59 44.26 46.8740.32 41.79 43.2924.20 24.05 25.0453.72 56.53 58.4537.65 41.64 41.9613.97 26.64 25.8257.32 54.10 55.36148Table 3: Performance of leveraging morphological information on the word similarity task.Model WS353 (%) SCWS* (%) MC(%) RG(%) RW(%)C&W 49.73 48.45 57.33 48.22 21.93C&W + csmRNN 58.27 49.09 60.22 58.92 31.77C&W + MorphemeCBOW* 63.81 53.30 74.33 61.22 31.14HSMN 62.58 32.09 66.18 64.51 1.97HSMN + csmRNN 64.58 44.08 71.88 65.15 22.31HSMN + MorphemeCBOW* 65.19 53.40 81.62 67.41 32.13MorphemeCBOW* 63.45 53.40 77.40 63.78 32.88?
On RW, MorphemeCBOW* performs better than the csmRNN model when using the HSMN em-beddings as the initialization.
When using the C&W embeddings as the initialization, the perfor-mance of MorphemeCBOW* is also comparable with that of csmRNN.
In particular, if we do notuse any pre-trained embeddings to initialize our mode, it performed the best (32.88%), and it evenbeats the best performance of csmRNN with initializations (31.77%)8.
The initialization is very im-portant to a neural network.
Suitable initialization will help increase the embedding quality whichworks like training with multi-epochs.
However, as there are two matrix M and M?in our networkstructure, the initialization of both of them are more sensible.
Furthermore, considering that therecursive structure of csmRNN will bring higher computation complexity, we can conclude that ourmodel has excellent ability in learning the embeddings of rare words from pure scratch.?
The improvement on RW is more significant than those on the other four datasets.
Considering thatRW contains more rare and unknown words (See Figure 3), we verified our idea that leveragingmorphological information will especially benefit the embedding of low-frequency words.
Morespecifically, without sufficient context information for the rare words in the training data, buildingconnections between words using morphemes will provide additional evidence for the model togenerate effective embeddings for these rare words; and, by combining the high-quality morphemeembeddings to obtain the representations of the unknown words, the model does a good job indealing with the new emerging words.5 Conclusions and Future WorkWe proposed a novel neural network model to learn word representations from text.
The model can lever-age several types of morphological information to produce high-quality word embeddings, especially forrare words and unknown words.
Empirical experiments on an analogical reasoning task and several wordsimilarity tasks have shown that the proposed model can generate better word representations comparedwith several state-of-the-art approaches.For the future work, we plan to separate words and morphemes into several buckets according to theirfrequencies.
Different buckets will be associated with different coefficients, so that we can tune thecoefficients to approach even better word embeddings.
We also plan to run our model on more trainingcorpus to obtain the embedding vectors for rare words, especially those new words invented out recently.These emerging new words usually do not exist in standard training corpus such as Wikipedia, but existsin some noisy data such as news articles and web pages.
How well our model performs on these newtraining corpus is an interesting question to explore.ReferencesAndrei Alexandrescu and Katrin Kirchhoff.
2006.
Factored neural language models.
In Proceedings of the HumanLanguage Technology Conference of the NAACL, Companion Volume: Short Papers, pages 1?4, New York City,USA, June.
Association for Computational Linguistics.834.36% in the paper of Luong et al; 32.06% in their project website, see note7149Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin.
2003.
A neural probabilistic languagemodel.
J. Mach.
Learn.
Res., 3:1137?1155, March.R.
Collobert and J. Weston.
2008.
A unified architecture for natural language processing: Deep neural networkswith multitask learning.
In ICML.R.
Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa.
2011.
Natural language processing(almost) from scratch.
JMLR, 12.M.
Creutz and K. Lagus.
2007.
Unsupervised models for morpheme segmentation and morphology learning.TSLP.L.
Deng, X.
He, and J. Gao.
2013.
Deep stacking networks for information retrieval.
In ICASSP, pages 3153?3157.L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.
2002.
Placing searchin context: The concept revisited.
In ACM Transactions on Information Systems.X.
Glorot, A. Bordes, and Y. Bengio.
2011.
Domain adaptation for large-scale sentiment classification: A deeplearning approach.
In ICML.Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng.
2012.
Improving Word Represen-tations via Global Context and Multiple Word Prototypes.
In Annual Meeting of the Association for Computa-tional Linguistics (ACL).F.
M. Liang.
1983.
Word hy-phen-a-tion by com-put-er.
Technical report.M.-T. Luong, R. Socher, and C. D. Manning.
2013.
Better word representations with recursive neural networksfor morphology.
CoNLL-2013, 104.Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan Cernock?y, and Sanjeev Khudanpur.
2010.
Recurrent neuralnetwork based language model.
In INTERSPEECH, pages 1045?1048.T.
Mikolov, K. Chen, G. Corrado, and J.
Dean.
2013a.
Efficient estimation of word representations in vector space.ICLR ?13.T.
Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J.
Dean.
2013b.
Distributed representations of words andphrases and their compositionality.
In NIPS, pages 3111?3119.T.
Mikolov, W.-T. Yih, and G. Zweig.
2013c.
Linguistic regularities in continuous space word representations.
InIn NAACL-HLT, pages 746?751.T.
Mikolov.
2012.
Statistical Language Models Based on Neural Networks.
Ph.D. thesis, Brno University ofTechnology.G.A.
Miller and W.G.
Charles.
1991.
Contextual correlates of semantic similarity.
6(1):1?28.A.
Mnih and G. E. Hinton.
2008.
A scalable hierarchical distributed language model.
In NIPS, pages 1081?1088.Amr El-Desoky Mousa, Hong-Kwang Jeff Kuo, Lidia Mangu, and Hagen Soltau.
2013.
Morpheme-based feature-rich language models using deep neural networks for lvcsr of egyptian arabic.
In ICASSP, pages 8435?8439.Herbert Rubenstein and John B. Goodenough.
1965.
Contextual correlates of synonymy.
Commun.
ACM,8(10):627?633, October.R.
Socher, C. C. Lin, A. Y. Ng, and C. D. Manning.
2011.
Parsing natural scenes and natural language withrecursive neural networks.
In ICML.Henning Sperr, Jan Niehues, and Alex Waibel.
2013.
Letter n-gram-based input encoding for continuous spacelanguage models.
In Proceedings of the Workshop on Continuous Vector Space Models and their Composition-ality, pages 30?39, Sofia, Bulgaria, August.
Association for Computational Linguistics.J.
P. Turian, L.-A.
Ratinov, and Y. Bengio.
2010.
Word representations: A simple and general method for semi-supervised learning.
In ACL, pages 384?394.P.
D. Turney and P. Pantel.
2010.
From frequency to meaning: Vector space models of semantics.
Journal ofArtificial Intelligence Research, 37:141?188.P.
D. Turney.
2013.
Distributional semantics beyond words: Supervised learning of analogy and paraphrase.TACL, pages 353?366.J.
Weston, S. Bengio, and N. Usunier.
Wsabie: Scaling up to large vocabulary image annotation.
In IJCAI.150
