Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 320?325,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsFinding Optimists and Pessimists on TwitterXianzhi Ruan, Steven R. Wilson, and Rada MihalceaUniversity of MichiganAnn Arbor, MI{rxianzhi, steverw, mihalcea}@umich.eduAbstractOptimism is linked to various personal-ity factors as well as both psychologicaland physical health, but how does it re-late to the way a person tweets?
We an-alyze the online activity of a set of Twit-ter users in order to determine how wellmachine learning algorithms can detect aperson?s outlook on life by reading theirtweets.
A sample of tweets from each useris manually annotated in order to estab-lish ground truth labels, and classifiers aretrained to distinguish between optimisticand pessimistic users.
Our results sug-gest that the words in people?s tweets pro-vide ample evidence to identify them asoptimists, pessimists, or somewhere in be-tween.
Additionally, several applicationsof these trained models are explored.1 IntroductionOptimists believe that future events are going towork out for the best; pessimists expect the worst(Carver et al, 2010).
Research has shown that op-timism is correlated with many positive life out-comes including improvements in life expectancy(Diener and Chan, 2011), physical health (Peter-son and Bossio, 2001), and mental health (Achat etal., 2000).
Previously, it was found that optimismand pessimism are differentiable but related: pes-simism was principally associated with neuroti-cism and negative affect while optimism was pri-marily associated with extraversion and positiveaffect (Marshall et al, 1992).
Another study foundthat optimism was correlated with personality fac-tors, including extraversion, emotional stability,conscientiousness, and agreeableness (Sharpe etal., 2011).It is clear that optimism relates to a wide vari-ety of psychological and social variables, but howmight optimism influence the way a person uti-lizes a social media platform?
What features dis-tinguish optimistic users from pessimistic ones?In order to answer these questions, we must firstestablish a means by which we can measure peo-ple?s levels of optimism and pessimism.
The LifeOrientation Test (LOT) is commonly used to as-sess the degree to which a person is an optimist(Scheier and Carver, 1985).
This short surveyasks respondents to evaluate their own agreementwith a number of short statements using a five-point scale.
However, distributing such a sur-vey over a large population requires both timeand a form of incentive.
Recent work has shownthat open-ended text samples can be computation-ally analyzed to provide a more comprehensiveview of a person?s personal values than can beachieved using a more constrained, forced choicesurvey (Boyd et al, 2015).
Furthermore, we knowthat language use is an independent and meaning-ful way of exploring personality (Pennebaker andKing, 1999), and personality is correlated with op-timism (Sharpe et al, 2011).
Given a large enoughtext corpus, it may therefore be possible to buildcomputational models that can automatically rec-ognize optimism itself by looking at the wordspeople use.
The vast amount of publicly availablesocial media data provides an excellent source ofdata that can be used to build models of users?
psy-chological traits, as was done in previous studiesthat trained machines to predict aspects of person-ality from tweets (Golbeck et al, 2011; Sumner etal., 2012).A tool that could identify optimists and pes-simists by analyzing their text would aid in largescale studies of optimism among social media orother web users by providing a large number ofsubjects to analyze.
This would open the door to320massive studies of the relationships between opti-mism, pessimism, and a range of online behaviors.On the other hand, an optimism classification sys-tem could help improve the social platform itself.For example, by learning more about the psycho-logical traits of its users, Twitter could improve its?who to follow?
suggestions so that they reflectpeople who have a similar outlook on life.2 Data and MethodAs a data source, we chose Twitter because itis widely used and is ripe with short pieces oftext (i.e., tweets) containing people?s everydaythoughts, observations, and conversations.
Weused Twitter?s basic search function to look forusers whose tweets include words and phrases thatindicate that they might identify as optimistic orpessimistic.
Looking for phrases such as ?I am op-timistic?
within a user?s tweets to find potentiallyoptimistic users, we identified 714 candidates.Finding pessimistic Twitter users proved more dif-ficult because users would not usually tweet some-thing negative such as ?I am pessimistic?
andpresent themselves in an unflattering way.
We in-stead searched for keywords such as ?hate,?
?un-fair,?
and ?disgust,?
which may indicate a pes-simistic nature.
This led to 640 potential pes-simists.
For each user, we crawled their 2,000most recent tweets (or all their tweets if the userhad less than 2,000).
In order to verify that the ac-counts identified were owned mostly by individ-ual users (as opposed to organizations), we man-ually inspected a random sample of 50 accountsand found only one that appeared to be related toan organization.Using the collected data set, which we expectedwould be more representative of optimistic or pes-simistic nature than the norm based on the con-tent of their tweets, we selected a fraction of theusers to create a ground truth set for our task.
Weused Amazon Mechanical Turk (MTurk)1to ob-tain human annotatations for a subset of our cor-pus.
We randomly selected 500 users who wereretrieved by the optimistic queries and 500 usersfound when searching for pessimists.
For eachuser, we randomly selected 15 tweets for a total of15,000 tweets to be labeled on a scale of ?3 (verypessimistic) to 3 (very optimistic) by five indepen-dent annotators.
Before labeling began, we pro-vided clear definitions of optimism and pessimism1http://www.mturk.comto the annotators.In order to pick the tweets from each user thathad a stronger emotional signal, we took advan-tage of the ?positive emotions?
and ?negative emo-tions?
word categories included in the LinguisticInquiry and Word Count Tool (Pennebaker et al,2001).2If any of the original 15 tweets did notcontain at least one word from either category, thetweet was removed and a new tweet was chosen atrandom to replace it.
This process was repeateduntil we had a set of 15 tweets per user with-out skewing that user?s true distribution of positiveand negative tweets.During the MTurk annotation, to identify work-ers who were quickly selecting options withouteven reading the tweets, we added a ?check?
ques-tion that asked the workers to choose a specificvalue for that question.
All the workers who didnot correctly answer this ?check?
question wereremoved from the annotation.
When a worker?sannotations had to be thrown out, the tweets wereput back onto MTurk for reannotation.
Addition-ally, we compared the scores of each annotatorwith the average score and removed workers whodeviated significantly from the others.
The finalagreement (Krippendorf?s alpha) between the fiveannotators was measured at 0.731, assuming an in-terval scale.For each individual tweet, we assigned a la-bel of ?optimistic,?
?pessimistic,?
or ?neutral?.Any tweet with an average score greater than one(slightly optimistic or higher in the annotationtask) was considered an ?optimistic?
tweet, andthose with an average score less than one (slightlypessimistic or lower) were given the ?pessimistic?class label.
The tweets with average MTurk anno-tation scores between -1 and 1 were considered tobe ?neutral.
?We also assigned a class label to each user.
Toaccomplish this, we calculated the average of theassigned scores, sorted the Twitter users by theirlevel of optimism, and considered the top 25% ofusers as optimists, the bottom 25% as pessimists,and the remaining ones as neutral.Before moving on, we decided to investigate theonline behaviors and attributes of the optimisticand pessimistic users in our new data set.
A sum-mary of some of the differences between the twogroups is shown in Table 1.
Interestingly the opti-mists have more followers and are following more2The 2007 version of LIWC was used321Optimistic Pessimisticmean followers 6238 1840mean following 1898 1156mean tweets 12156 28190median followers 972 572median following 718 443median tweets 5687 17451std.
dev of tweets 17952 37851min number of tweets 108 28max number of tweets 99344 231220tweet rate 10.24 19.184favorite count 4314.34 8761.2listed count 112.62 10.23Table 1: Statistics for the most extreme 100 opti-mistic & 100 pessimistic users.other users than the more pessimistic users.
Onthe other hand, the pessimists tend to tweet muchmore frequently, with a mean and median num-ber of tweets both more than twice as large as theoptimist group.
This is not just a factor of thepessimists having been around longer to build upa history of tweets- we also compute the ?tweetrate?
for each user by dividing their total numberof tweets by the total number of days since the ac-tivation of their Twitter account.
Looking at thisvariable, we see that the average number of tweetsper day is much higher for the pessimists.
Opti-mists are also included in more lists, while pes-simists choose to label things as a ?favorite?
moreoften.In order to build computational models to dif-ferentiate between the optimistic and pessimisticusers, we use five different methods from thescikit-learn python library3: Naive Bayes (NB),Nearest Neighbor (NN), Decision Tree (DT), Ran-dom Forest Classifier (RFC), Gradient BoostingClassifier (GBC) and Stochastic Gradient Descent(SGD).
The default parameters are used for each.The preprocessing method was the same for alldifferent classifiers: the text was preprocessed byremoving mentions (@), web links, and the phraseRT.
We also used the Emoji unicode tables to re-place all Emoji unicodes to their correspondingmeanings (e.g., ?<smiling-face>?).
We tried per-forming classification both with and without re-moving stopwords to see what the effect was.
Forall different classifiers, we tested with differentsettings: with and without stopwords; and addinga user?s profile information as additional featuresor not.3http://scikit-learn.org/stable/NN DTRFC GBC SGDNB50607080modelaccuracy(%)two class three classFigure 1: Tweet level classification accuracy forthe two-way and three-way classification prob-lems.3 ResultsWe first evaluate the ability of our classifiersto distinguish between optimistic and pessimistictweets (two-way classification) or among opti-mistic, pessimistic, and neutral tweets (three-way classification).
We randomly selected 1,000tweets from each class.
Figure 1 shows the ten-fold cross validation results obtained using the sixclassifiers.
During each classification, we madesure that tweets from the same user were notshared between the training and testing folds.
Inboth cases, the best setting was using the NaiveBayes classifier and not including profile informa-tion as features.
Stopword removal had no notice-able effect.
Note that the majority baseline is ascore of 50% in the two-class case, while it is 33%in the three-class case.For additional insight, Table 2 shows some ofthe top features for the optimistic and pessimisticclass, sorted by the probability of the feature giventhe class.
We can see that, as one might ex-pect, the useful words for detecting optimists aregenerally very positive, while the pessimistic fea-tures are negative and sprinkled with profanity.Since we formulated the problem as a three-wayclassification, it is reasonable that some wordsmay have high scores for both optimistic and pes-simistic classes.
These words distinguish opti-mism/pessimism from the neutral class.We perform our next evaluation at the user level,which means that we consider all tweets from322Optimism Pessimismlove, so, that, be fuck, that, not, sohave, good, am, this like, am, do, hateon, your, not, day have, be, this, justlike, just, do, will up, life, on, shitcan, get, what, at no, people, can, whatgreat, make, up, much feel, your, about, I?mbest, we, if, go go, know, get, evenwas, from, thing, out want, at, was, offlook, thank, know,he out, kill, if, doneTable 2: Most discriminating features collectedby the Naive Bayes classifier for the three-classtweet-level prediction setting.100 150 200 250 300 350 40045505560657075number of usersmodelaccuracy(%)NB NN DTRFC GBC SGDFigure 2: Accuracy on the user-level predictiontask for different data sizes.a user as a single document.
The classificationis performed using a randomly selected set of100, 200, 300, and 400 users from the annotatedset (each set adds 100 new users to the previousgroup).
In each case, the 25% users with high-est annotation score are considered the optimisticgroup, 25% users with lowest annotation score aspessimist group, and the other 50% of users is theneutral group.
The results of the ten-fold cross val-idation are shown in Figure 2.
In this setting, theGradient Boosting Classifier usually outperformsthe others and achieves an accuracy of 73.33% onthe 400 user data set.We also sought to discover how accurate theclassifiers would be if the objective was simplyto identify the top N optimists or pessimists.
Forexample, if we wanted to find the 10 users outof a group with the greatest number of optimistictweets, how accurately could this be done?
To0 25 50 7560708090100Top N users predicted by NBCumulativeaccuracy(%)PessimisticOptimisticFigure 3: Cumulative classification accuracy onthe top N users sorted by Naive Bayes predictedprobabilitiesCity Neutral Users Optimists PessimistsChicago 20.65% 39.27% 40.08%Los Angeles 14.55% 31.87% 53.58%New York 20.74% 40.63% 38.63%Table 3: Predicted optimism & pessimism in threemajor citiescarry out this analysis, we sorted the users by theprobabilities that they belonged to either the op-timistic class or the pessimistic class as predictedby a Naive Bayes classifier (Figure 3).
Then, wecompute the accuracy for the top N optimists andpessimists.
As we can see, it is possible to predictthe most pessimistic 14 users with perfect accu-racy.
On the other hand, some of the most likelyoptimistic users actually belonged to another classbased on the ground truth labels.
With a largernumber of users to classify, it becomes easier tocorrectly label optimists than pessimists.4 ApplicationsWhat kinds of things can we learn with a tool forclassifying optimists and pessimists?
First, welook at groups of users from three major cities inthe United States: Chicago, Los Angeles, and NewYork.
We found users who listed their location asone of these three cities (494 users from Chicago,433 from Los Angeles, 480 from New York), thencollected 2,000 tweets from each user.
Using ourbest models from the user-level experiments, weobtain predictions for the optimism/pessimism ofthe users.
The breakdown of predicted optimists,pessimists, and neutral users is listed in Table 3.323Chicago and New York are fairly balanced withroughly 40% of people falling into each category(leaving 20% as neutral).
However, pessimistswere predicted much more often than optimists inLos Angeles.For a second sample application, we wentto the official twitter accounts of six presiden-tial candidates: Hillary Clinton, Donald Trump,Marco Rubio, Martin O?Malley, Bernie Sandersand Ben Carson.
We randomly picked approxi-mately 500 followers of each of the candidates andpredicted the optimism/pessimism of them (Table4).4While these scores are only estimates, wesee that O?Malley?s followers tend to be the userswho posted a greater number of optimistic tweets,while the users who tweeted lots of pessimistictweets are those keeping up-to-date with Rubio?scampaign.
Overall, we see that most of the fol-lowers of these candidates are optimistic.Candidate Neutral Users Optimists PessimistsClinton 31.52% 49.22% 19.26%Trump 36.20% 39.46% 24.32%Rubio 30.00% 49.41% 20.59%O?Malley 29.22% 64.51% 6.26%Sanders 44.62% 44.42% 10.96%Carson 31.54% 49.90% 18.56%Table 4: Predicted optimism & pessimism of thosefollowing some of the candidates for the 2016Presidential election.5 Conclusions and Future WorkWe have shown that we can use Twitter to collecta data set5of optimistic and pessimistic users, andpredict the most (top 25%) optimistic/pessimisticusers with greater than 70% accuracy.
The opti-mistic users on Twitter tended to have more so-cial connections, but tweet less often than the pes-simists.
In the future, we hope to explore the socialeffects of optimism, such as the degree to whichoptimistic users follow one another and whether ornot optimistic comments receive more ?favorites?and retweets.
Finally, we would like to comparethe optimism and pessimism scores that our modelpredicts with those received when taking the LOTin order to compare the text-based analysis with awidely used tool for measuring optimism.4data collected late December, 20155The data set introduced in this paper is avail-able at http://lit.eecs.umich.edu/research/downloads.AcknowledgementsWe would like to thank Seong Ju Park, Tian Bao,and Yihan Li for their assistance in the initialproject that led to this work.
This material isbased in part upon work supported by the Na-tional Science Foundation award #1344257 andby grant #48503 from the John Templeton Foun-dation.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthe views of the National Science Foundation orthe John Templeton Foundation.ReferencesHelen Achat, Ichiro Kawachi, Avron Spiro, Deborah ADeMolles, and David Sparrow.
2000.
Optimismand depression as predictors of physical and mentalhealth functioning: the normative aging study.
An-nals of Behavioral Medicine, 22(2):127?130.Ryan L Boyd, Steven R Wilson, James W Pennebaker,Michal Kosinski, David J Stillwell, and Rada Mi-halcea.
2015.
Values in words: Using language toevaluate and understand personal values.
In NinthInternational AAAI Conference on Web and SocialMedia.Charles S Carver, Michael F Scheier, and Suzanne CSegerstrom.
2010.
Optimism.
Clinical psychologyreview, 30(7):879?889.Ed Diener and Micaela Y Chan.
2011.
Happy peo-ple live longer: Subjective well-being contributes tohealth and longevity.
Applied Psychology: Healthand Well-Being, 3(1):1?43.Jennifer Golbeck, Cristina Robles, Michon Edmond-son, and Karen Turner.
2011.
Predicting per-sonality from twitter.
In Privacy, Security, Riskand Trust (PASSAT) and 2011 IEEE Third Iner-national Conference on Social Computing (Social-Com), 2011 IEEE Third International Conferenceon, pages 149?156.
IEEE.Grant N Marshall, Camille B Wortman, Jeffrey WKusulas, Linda K Hervig, and Ross R Vickers Jr.1992.
Distinguishing optimism from pessimism:Relations to fundamental dimensions of mood andpersonality.
Journal of personality and social psy-chology, 62(6):1067.James W Pennebaker and Laura A King.
1999.
Lin-guistic styles: language use as an individual differ-ence.
Journal of personality and social psychology,77(6):1296.James W Pennebaker, Martha E Francis, and Roger JBooth.
2001.
Linguistic inquiry and word count:Liwc 2001.
Mahway: Lawrence Erlbaum Asso-ciates, 71:2001.324Christopher Peterson and Lisa M. Bossio.
2001.
Opti-mism and physical well-being.
In Optimism & pes-simism: Implications for theory, research, and prac-tice., pages 127?145.
American Psychological As-sociation (APA).Michael F Scheier and Charles S Carver.
1985.
Opti-mism, coping, and health: assessment and implica-tions of generalized outcome expectancies.
Healthpsychology, 4(3):219.J Patrick Sharpe, Nicholas R Martin, and Kelly A Roth.2011.
Optimism and the big five factors of personal-ity: Beyond neuroticism and extraversion.
Person-ality and Individual Differences, 51(8):946?951.Chris Sumner, Alison Byers, Rachel Boochever, andGregory J Park.
2012.
Predicting dark triad person-ality traits from twitter usage and a linguistic anal-ysis of tweets.
In Machine Learning and Applica-tions (ICMLA), 2012 11th International Conferenceon, volume 2, pages 386?393.
IEEE.325
