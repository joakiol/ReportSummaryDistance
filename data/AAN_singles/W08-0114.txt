Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 88?91,Columbus, June 2008. c?2008 Association for Computational LinguisticsA Framework for Building Conversational Agents Based on a Multi-ExpertModelMikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, Hiroshi TsujinoHonda Research Institute Japan Co., Ltd.8-1 Honcho, Wako, Saitama 351-0188, Japan{nakano, funakoshi, yuji.hasegawa, tsujino}@jp.honda-ri.comAbstractThis paper presents a novel framework forbuilding symbol-level control modules of an-imated agents and robots having a spoken di-alogue interface.
It features distributed mod-ules called experts each of which is special-ized to perform certain kinds of tasks.
A com-mon interface that all experts must support isspecified, and any kind of expert can be incor-porated if it has the interface.
Several modulesrunning in parallel coordinate the experts byaccessing them through the interface, so thatthe whole system can achieve flexible control,such as interruption handling and parallel taskexecution.1 IntroductionAs much attention is recently paid to autonomousagents such as robots and animated agents, spokendialogue is expected to be a natural interface be-tween users and such agents.
Our objective is to es-tablish a framework for developing the intelligencemodule of such agents.In establishing such a framework, we focus onachieving the following features.
(1) Multi-domaindialogue: Since agents are usually expected to per-form multiple kinds of tasks, they need to work inmultiple domains and switch domains according touser utterances.
(2) Interruption handling: It is cru-cial for human-agent interaction to be able to handleusers?
interrupting utterances while speaking or per-forming tasks.
(3) Parallel task execution: Agents,especially robots that perform physical actions, areexpected to be able to execute multiple tasks in par-allel when possible.
For example, robots should beable to engage in a dialogue while moving.
(4) Ex-tensibility: Since the agents can be used for a vari-ety of tasks, various strategies for dialogue and taskplanning should be able to be incorporated.Although a number of models for conversationalagents have been proposed, no model has all of theabove properties.
Several multi-domain dialoguesystem models have been proposed and they are ex-tensible, but it is not clear how they handle interrup-tions to system utterances and actions (e.g., O?Neillet al (2004), Lin et al (1999), and Hartikainen et al(2004)).
There are several spoken dialogue agentsand robots that can handle interruptions thanks totheir asynchronous control (Asoh et al, 1999; Boyeet al, 2000; Blaylock et al, 2002; Lemon et al,2002), they do not focus on making it easy to addnew dialogue domains with a variety of dialoguestrategies.This paper presents a framework called RIME(Robot Intelligence based on Multiple Experts),which employs modules called experts.1 Each ex-pert is specialized for achieving certain kinds oftasks by performing physical actions and engagingin dialogues.
It corresponds to the symbol-level con-trol module of a system that can engage in tasks ina single small domain, and it employs fixed con-trol strategies.
Only some of the experts take chargein understanding user utterances and decide actions.The basic idea behind RIME is to specify a com-mon interface of experts for coordinating them andto achieve flexible control.
In RIME, several mod-1RIME is an improved version of our previous model(Nakano et al, 2005), whose interruption handling was too sim-ple and which could not achieve parallel task execution.88ules run in parallel for coordinating experts.
Theyare understander, which is responsible for speechunderstanding, action selector, which is responsiblefor selecting actions, and task planner, which is re-sponsible for deciding which expert should work toachieve tasks.RIME achieves the above mentioned features.Multi-domain dialogues are possible by selecting anappropriate expert which is specialized to dialoguesin a certain domain.
Interruption handling is possi-ble because each expert must have methods to de-tect interruptions and decide actions to handle in-terruptions, and coordinating modules can use thesemethods.
Parallel task execution is possible becauseexperts have methods for providing information todecide which experts can take charge at the sametime, and the task planner utilizes that information.Extensibility is achieved because any kind of expertcan be incorporated if it supports the common inter-face.
This makes it possible for agent developers tobuild a variety of conversational agents.2 Multi-Expert ModelThis section explains RIME in detail.
Fig.
1 depictsits module architecture.2.1 ExpertsEach expert is a kind of object in the object-orientedprogramming framework.
In this paper, we calltasks performed by one expert primitive tasks.
Ex-perts should be prepared for each primitive task type.For example, if there is an expert for a primitive tasktype ?telling someone?s extension number?, ?tellingperson A?s extension number?
is a primitive task.By performing a series of primitive tasks, a com-plicated task can be performed.
For example, a mu-seum guide robot can perform ?explaining object B?by executing ?moving to B?
and ?giving an explana-tion on B?.
Among the experts, a small number ofexperts can perform tasks at one time.
Such expertsare called being in charge.Each expert holds information on the progress ofthe primitive task.
It includes task-type-independentinformation, such as which action in this primitivetask is being performed and whether the previousrobot action finished, and task-type-dependent in-formation such as the user intention understandingunderstanderexpert 1expert 2expert 3expert nactionselectortaskplannerglobalcontextinputprocessoractionexecutorspeechrecognitionresultscoreexpertselectioninformationspeechrecognitionresultaction(from expertsin charge)actionexecutionreportexec.
report (to the expertthat selected the action)charge/discharge new task informationacross tasks.....microphone etc.
agent & speech synthesizerFigure 1: Architecture for RIME-Based Systemsresults and dialogue history.
The contents and thedata structure for the task-type-dependent informa-tion for each expert can be designed by the systemdeveloper.Experts are classified into system-initiative taskexperts and user-initiative task experts.
In this pa-per, the initiative of a task means who can initiatethe task.
For example, the task ?understanding arequest for weather information?
is a user-initiativetask, and the task ?providing weather information?is a system-initiative task.In RIME, executing multiple tasks in parallel be-comes possible by making multiple experts takecharge.
To check whether two experts can takecharge simultaneously, we currently use two fea-tures verbal and physical.
Two experts having thesame feature cannot take charge simultaneously.The interface of experts consists of methods foraccessing its internal state.
Below are some of thetask-type-dependent methods, which need to be im-plemented by system developers.The understand method updates the internal statebased on the user speech recognition results, us-ing domain-dependent sentence patterns for utter-ance understanding.
This method returns a scorewhich indicates the plausibility the user utteranceshould be dealt with by the expert.
Domain selectiontechniques in multi-domain spoken dialogue sys-tems (Komatani et al, 2006) can be applied to obtainthe score.
The select-action method outputs one ac-tion based on the content of the internal state.
Here,an action is a multimodal command which includesa text to speak and/or a physical action command.89The action can be an empty action, which means do-ing nothing.
The detect-interruption method returnsa Boolean value that indicates whether the previoususer utterance is an interruption to the action beingperformed when this expert is being in charge.
Thehandle-interruption method returns the action to beperformed after an interruption is detected.
For ex-ample, an instruction to stop the utterance can bereturned.In the definition of these methods, experts canaccess a common database called global context tostore and utilize information across domains, suchas information on humans, information on the envi-ronment, and past dialogue topics.2.2 Modules Coordinating ExpertsTo exploit experts, three processes, namely the un-derstander, the action selector, and the task planner,work in parallel.The understander receives output of an input pro-cessor, which typically performs speech recogni-tion.
Each time the understander receives a userspeech recognition result from the input processor,it performs the following process.
First it dispatchesthe speech recognition result to the experts in chargeand the user-initiative experts with their understandmethods, which then returns the scores mentionedabove.
The expert that returns the highest score isselected as the expert to take charge.
If the selectedexpert is not in charge, it tells the task planner thatthe expert is selected as the user-initiative expert totake charge.
If the selected expert is in charge, itcalls the detect-interruption method of the expert.
Iftrue is returned, it tells the action selector that aninterruption utterance is detected.The action selector repeats the following processfor each expert being in charge in a short cycle.When an interruption for the expert is detected, itcalls the expert?s handle-interruption method, andit then sends the returned action to the action ex-ecutor, which is assumed to execute multimodal ac-tions by controlling agents, speech synthesizers, andother modules.
Otherwise, unless it is not waitingfor a user utterance, it calls the expert?s select-actionmethods, and then sends the returned action to theaction executor.
The returned action can be an emptyaction.
Note that it is assumed that the action execu-tor can perform two or more actions in parallel whenverbalagentexplaining placesGphysicalagentmoving to show the way Fverbaluserunderstanding requests for guiding to placesEverbalagentproviding extension numbersDverbaluserunderstanding extension number requestsCverbalagentproviding weather informationBverbaluserunderstanding weather information requestsAfeatureinitiativetask typeIDTable 1: Experts in the Example Robotic SystemHuman: "Where is the meetingroom?
"Robot: "Would you like to knowwhere the meeting room is?
"Human: "yes.
"Human: "Tell me A's extensionnumber.
"Robot: "Please come this way.
"(start moving)Robot: "A's extension number is1234.
"Robot: (stop moving)Expert EExpert GExpert CExpert Dunderstand requestto show the wayshow the waytell A's ext.numberunderstandrequest for A'sext.
numberRobot: "The meeting room is overthere.
"Utterances and physical actions Experts in charge and tasksmove toshow thewayExpert FFigure 2: Expert Selection in a Parallel Task ExecutionExamplepossible.The task planner is responsible for deciding whichexperts take charge and which experts do not.
Itsometimes makes an expert take charge by settinga primitive task, and sometimes it discharges an ex-pert to cancel the execution of its primitive task.
Tomake such decisions, it receives several pieces of in-formation from other modules.
First it receives fromthe understander information on which expert is se-lected to understand a new utterance.
It also receivesinformation on the finish of the primitive task froman expert being in charge.
In addition, it receivesnew tasks from the experts that understand humanrequests.
The task planner also consults the globalcontext to access the information shared by the ex-perts and the task planner.
In this paper we do notdiscuss the details of task planning algorithms, butwe have implemented a task planner with a simplehierarchical planning mechanism.There can be other processes whose output iswritten in the global context.
For example, a robotand human localization process using image pro-cessing and other sensor information processing canbe used.903 Implementation as a ToolkitThe flexibility of designing experts increases theamount of effort for programming in building ex-perts.
We therefore developed RIME-TK (RIME-ToolKit), which provides libraries that facilitatebuilding systems based on RIME.
It is implementedin Java, and contains an abstract expert class hier-archy.
The system developers can create new ex-perts by extending those abstract classes.
Those ab-stract classes have frequently used functions suchas WFST-based language understanding, template-based language generation, and frame-based dia-logue management.
RIME-TK also contains the im-plementations of the understander and the action se-lector.
In addition, it specifies the interfaces for theinput processor, the action executor, and the taskplanner.
Example implementations of these mod-ules are also included in RIME-TK.
Using RIME-TK, conversational agents can be built by creatingexperts, an input processor, an action executor, anda task planner.As an example, we have built a robotic system,which is supposed to work at a reception, and canperform several small tasks such as providing ex-tension numbers of office members and guiding toseveral places near the reception such as a meetingroom and a restroom.
Some experts in the systemare listed in Table 1.
Fig.
2 shows an example inter-action between a human and the robotic system thatincludes parallel task execution and how experts arecharged.
The detailed explanation is omitted for thelack of the space.By developing several other robotic systems andspoken dialogue systems (e.g., Komatani et al(2006), Nakano et al (2006), and Nishimura et al(2007)), we have confirmed that RIME and RIME-TK are viable.4 Concluding RemarksThis paper presented RIME, a framework for build-ing conversational agents.
It is different from pre-vious frameworks in that it makes it possible tobuild agents that can handle interruptions and exe-cute multiple tasks in parallel by employing expertswhich have a common interface.
Although the cur-rent implementation is useful for building variouskinds of systems, we believe that preparing morekinds of expert templates and improving expert se-lection for understanding utterances facilitate build-ing a wider variety of systems.Acknowledgments We would like to thank allpeople who helped us to build RIME-TK and its ap-plications.ReferencesH.
Asoh, T. Matsui, J. Fry, F. Asano, and S. Hayamizu.1999.
A spoken dialog system for a mobile officerobot.
In Proc.
Eurospeech-99, pages 1139?1142.N.
Blaylock, J. Allen, and G. Ferguson.
2002.
Synchro-nization in an asynchronous agent-based architecturefor dialogue systems.
In Proc.
Third SIGdial Work-shop, pages 1?10.J.
Boye, B.
A. Hockey, and M. Rayner.
2000.
Asyn-chronous dialogue management: Two case-studies.
InProc.
Go?talog-2000.M.
Hartikainen, M. Turunen, J. Hakulinen, E.-P. Salo-nen, and J.
A. Funk.
2004.
Flexible dialogue manage-ment using distributed and dynamic dialogue control.In Proc.
Interspeech-2004, pages 197?200.K.
Komatani, N. Kanda, M. Nakano, K. Nakadai, H. Tsu-jino, T. Ogata, and H. G. Okuno.
2006.
Multi-domainspoken dialogue system with extensibility and robust-ness against speech recognition errors.
In Proc.
7thSIGdial Workshop, pages 9?17.O.
Lemon, A. Gruenstein, A.
Battle, and S. Peters.
2002.Multi-tasking and collaborative activities in dialoguesystems.
In Proc.
Third SIGdial Workshop, pages113?124.B.
Lin, H. Wang, and L. Lee.
1999.
Consistent dialogueacross concurrent topics based on an expert systemmodel.
In Proc.
Eurospeech-99, pages 1427?1430.M.
Nakano, Y. Hasegawa, K. Nakadai, T. Nakamura,J.
Takeuchi, T. Torii, H. Tsujino, N. Kanda, and H. G.Okuno.
2005.
A two-layer model for behavior anddialogue planning in conversational service robots.
InProc.
2005 IEEE/RSJ IROS, pages 1542?1547.M.
Nakano, A. Hoshino, J. Takeuchi, Y. Hasegawa,T.
Torii, K. Nakadai, K. Kato, and H. Tsujino.
2006.A robot that can engage in both task-oriented and non-task-oriented dialogues.
In Proc.
2006 IEEE/RAS Hu-manoids, pages 404?411.Y.
Nishimura, S. Minotsu, H. Dohi, M. Ishizuka,M.
Nakano, K. Funakoshi, J. Takeuchi, Y. Hasegawa,and H. Tsujino.
2007.
A markup language for describ-ing interactive humanoid robot presentations.
In Proc.IUI-07.I.
O?Neill, P. Hanna, X. Liu, and M. McTear.
2004.Cross domain dialogue modelling: an object-based ap-proach.
In Proc.
Interspeech-2004, pages 205?208.91
