MPLUS:  A Probabilistic Medical Language Understanding  SystemLee M. Christensen, Peter J. Haug, and Marcelo FiszmanDepartment of Medical Informatics, LDS Hospital/University of Utah, Salt Lake City, UTE-mail: ldlchris@ihc.com, ldphaug@ihc.com, ldmfiszm@ihc.comAbstractThis paper describes the basic philosophyand implementation of MPLUS (M+), arobust medical text analysis tool that uses asemantic model based on BayesianNetworks (BNs).
BNs provide a conciseand useful formalism for representingsemantic patterns in medical text, and forrecognizing and reasoning over thosepatterns.
BNs are noise-tolerant, andfacilitate the training of M+.12IntroductionIn the field of medical informatics,computerized tools are being developed thatdepend on databases of clinical information.These include alerting systems for improvedpatient care, data mining systems for qualityassurance and research, and diagnostic systemsfor more complex medical decision support.These systems require data that is appropriatelystructured and coded.
Since a large portion ofthe information stored in patient databases is inthe form of free text, manually coding thisinformation in a format accessible to these toolscan be time consuming and expensive.
In recentyears, natural language processing (NLP)methodologies have been studied as a means ofautomating this task.
There have been manyprojects involving automated medical languageanalysis, including deciphering pathologyreports (Smart and Roux, 1995), physical examfindings (Lin et al, 1991), and radiology reports(Friedman et al, 1994; Ranum, 1989; Koehler,1998).M+ is the latest in a line of NLP toolsdeveloped at LDS Hospital in Salt Lake City,Utah.
Its predecessors include SPRUS (Ranum,1989) and SymText (Koehler, 1998).
Thesetools have been used in the realm of radiologyreports, admitting diagnoses (Haug et al, 1997),radiology utilization review (Fiszman, 2002)and syndromic detection (Chapman et al,2002).
Some of the character of these toolsderives from common characteristics ofradiology reports, their initial target domain.Because of the off-the-cuff nature ofradiology dictation, a report will frequentlycontain text that is telegraphic or otherwise notwell formed grammatically.
Our desire was notonly to take advantage of phrasal structure todiscover semantic patterns in text, but also to beable to infer those patterns from lexical andcontextual cues when necessary.Most NLP systems capable of semanticanalysis employ representational formalismswith ties to classical logic, including semanticgrammars (Friedman et al, 1994), unification-based semantics (Moore, 1989), and descriptionlogics (Romacker and Hahn, 2000).
M+ and itspredecessors employ Bayesian Networks (Pearl,1988), a methodology outside this tradition.This study discusses the philosophy andimplementation of M+, and attempts to showhow Bayesian Networks can be useful inmedical text analysis.The M+ Semantic Model2.1 Semantic Bayesian NetworksM+ uses Bayesian Networks (BNs) to representthe basic semantic types and relations within amedical domain such as chest radiology reports.M+  BNs are structurally similar to semanticnetworks, in that they are implemented asdirected acyclic graphs, with  nodesrepresenting word and concept types, and linksrepresenting relations between those types.
BNsalso have a character as frames or slot-fillerrepresentations (Minsky, 1975).
Each node istreated as a variable, with an associated  list ofpossible values.
For instance a noderepresenting "disease severity" might includethe possible values {"severe", "moderate","mild"}.
Each value  has a probability, eitherassigned or inferred, of being the true value ofthat node.In addition to providing a frameworkfor representation, a BN is also a probabilisticinference engine.
The probability of eachpossible value of a node is conditioned on theprobabilities of the values of neighboring nodes,Association for Computational Linguistics.the Biomedical Domain, Philadelphia, July 2002, pp.
29-36.Proceedings of the Workshop on Natural Language Processing inthrough a training process that learns a Bayesianjoint probability function from a set of trainingcases.
After a BN is trained, a node can beassigned a value by setting the probability ofthat value to 1, and the probabilities of thealternate values to 0.
This results in a cascadingupdate of the value probabilities in allunassigned nodes, in effect predicting what thevalues of the unassigned nodes should be, giventhe initial assignments.
The sum of theprobabilities for the values of a given node isconstrained to equal 1, making the valuesmutually exclusive, and reflecting uncertainty ifmore than one value has a nonzero probability.Please note that in this paper, "BN instance"refers to the state of a BN after assignmentshave been made.A training case for a BN is a list of node/ value assignments.
For instance, consider asimple BN for chest anatomy phrases, as shownin Figure 1.Figure 1.
BN for simple chest anatomy phrases.A training case for this BN applied tothe phrase "right upper lobe" could be:side=rightverticality=upperlocation=lobeinterpretation= *right-upper-lobeIn the context of the Bayesian learning,this case has an effect similar to a productionrule which states "If  you find the words 'right','upper' and 'lobe' together in a phrase, infer themeaning *right-upper-lobe".
After training onthis case, assigning one or more values from thiscase would increase the probabilities of theother values; for instance assigning side="right" would increase the probability of thevalue interpretation= *right-upper-lobe.Interpretive concepts such as *right-upper-lobe are atomic symbols which are eitherinvented by the human trainer, or else obtainedfrom a medical knowledge database such as theUMLS metathesaurus.
By convention, conceptnames in M+ are preceded with an asterisk.A medical domain is represented in M+as a network of BNs, with word-level and lowerconcept-level BNs providing input to higherconcept-level BNs.
Figure 2 shows a partialview of the network of BNs used to model theM+ Head CT (Computerized Tomography)domain, instantiated with the phrase "temporalsubdural hemorrhage".
Each BN instance isshown with a list of nodes and most probablevalues.
Note that input nodes of higher BNs inthis model have the same name as, and takeinput from, the summary nodes of lower BNs.Word level BNs have input nodes named"head", "mod1" and "mod2", corresponding tothe syntactic head and modifiers of a phrase.Each node in a BN has a distinguished "null"value, whose meaning is that no informationrelevant to that node, explicit or inferable, ispresent in the represented phrase.Figure 2.
Network of M+ BNs, applied to"temporal subdural hemorrhage".One way in which M+ differs from itspredecessor SymText (Koehler, 1998) is in thesize and modularity of its semantic BNs.
TheSymText BNs group observation and diseaseconcepts together with state ("present","absent"), change-of-state ("old", "chronic"),anatomic location and other concept types.
M+trades the inferential advantages of suchmonolithic BNs for the modularity andcomposability of smaller BNs such as thoseshown in figure 2.
Figure 3 shows a singleinstance of the SymText Chest RadiologyFindings BN, instantiated with the sentence"There is dense infiltrative opacity in the rightupper lobe".
*observations :  *localized upper lobe infiltrate (0.888)*state :  *present (0.989)state term :  null (0.966)*topic concept :  *poorly-marginated opacity (0.877)topic term :  opacity  (1.0)topic modifier :  infiltrative (1.0)*measurement concept :  *null (0.999)measurement term :  null (0.990)first value :  null (0.998)second value :  null (0.999)values link :  null (0.999)size descriptor :  null (0.999)*tissue concept :  *lung parenchyma (0.906)tissue term : alveolar (1.0)*severity concept :  *high severity (0.893)severity term :  dense (1.0)*anatomic concept :  *right upper lobe (0.999)*anatomic link concept :  *involving (1.0)anatomic link term :  in (1.0)anatomic location term :  lobe (1.0)anatomic location modifier :  null (0.999)anatomic modifier side :  right (1.0)anatomic modifier superior/inferior : upper (1.0)anatomic modifier lateral/medial : null (0.999)anatomic modifier anterior/posterior : null (0.999)anatomic modifier central/peripheral : null (0.955)*change concept :  *null (0.569)change with time :  null (0.567)change degree :  null (0.904)change quality :  null (0.923)Figure 3.
SymText BN instantiation.2.2 Parse-Driven BN InstantiationM+ BNs are instantiated as part of thesyntactic parse process.
M+ syntactic andsemantic analyses are interleaved, in contrastwith NLP systems that perform semanticanalysis after the parse has finished.M+ uses a bottom-up chart parser, witha context free grammar (CFG).
As a word suchas "right" is recognized by the parser, a word-level phrase object is created and a BN instancecontaining the assignment side= "right" isattached to that phrase.
As larger grammaticalpatterns are recognized, the BN instancesattached to subphrases within those patterns areunified and attached to the new phrases, asdescribed in section 3.
The result of thisprocess is a set of completed BN instances, asillustrated in figure 2.
Each BN instance is atemplate containing word and concept-levelvalue assignments, and the interpretive conceptsinferred from those assignments.
The templatesthemselves are nested in a symbolic expression,as described in section 2.3, to facilitatecomposing multiple BN instances inrepresentations of arbitrary complexity.Each phrase recognized by the parser isassigned a probability, based on a weighted sumof the joint probabilities of its associated BNinstances, and adjusted for various syntactic andsemantic constraint violations.
Phrases areprocessed in order of probability; thus the parseinvolves a semantically-guided best-first search.Syntactic and semantic analysis in M+are mutually constraining.
If a grammaticallypossible phrase is uninterpretable, i.e.
if itssubphrase interpretations cannot be unified, it isrejected.
If the interpretation has a lowprobability, the phrase is less likely to appear inthe final parse tree.
On the other hand,interpretations are constructed as phrases arerecognized.
The exception to this rule is whenan ungrammatical fragment of text isencountered.
M+ then uses a semantically-guided phrase repair procedure not described inthis paper.2.3 The M+ Abstract Semantic LanguageThe probabilistic reasoning afforded by BNs issuperior to classical logic in important ways(Pearl, 1988).
However, BNs are limited inexpressive power relative to first-order logics(Koller and Pfeffer, 1997), and commerciallyavailable implementations lack the flexibility ofsymbolic languages.
Friedman et alhave madeconsiderable headway in giving BNs manyuseful characteristics of first order languages, inwhat they call probabilistic relational models, orPRMs (e.g.
Friedman at al.
1999).While we are waiting for industry-standard PRMs, we have tried to make oursemantic BNs more useful by combining themwith a first-order language, called the M+Abstract Semantic Language (ASL),implemented within M+.
Specifically, BNs aretreated as object types within the ASL.
There isa "chest anatomy" type, for instance, and a"chest radiology findings" type, correspondingto BNs of those same names.
The interpretationof a phrase is an expression in the ASL,containing predicates that state the relation ofBN instances to one another, and to the phrasethey describe.
For instance, the interpretation of"hazy right lower lobe opacity" could be theexpression(and (head-of #phrase1 #find1)(located-at #find1 #loc1))where #phrase1 identifies a syntactic phraseobject, and #find1 and #loc1 are tokensrepresenting instances of the findings BN(instanced with the words "hazy" and "opacity")and the anatomic BN (instanced with "right","lower" and "lobe"), respectively.
The relation'head-of' denotes that the findings BN is themain or "head" BN for that phrase.
Conversely,"hazy right lower lobe opacity" can be thoughtof as a findings-type phrase, with an anatomic-type modifier.This expression captures the abstract or"skeletal" structure of the interpretation, whilethe BN instances contain the details and specificinferences.
One can think of the meaning of anexpression like (located-at #find1 #loc1) inabstract terms, e.g.
"some-finding located-atsome-location".
Alternatively, the meaning of aBN token might be thought of as the mostprobable interpretive concept within that BNinstance.
In this case, (located-at #find1 #loc1)could mean "*localized-infiltrate located-at*left-lower-lobe".Because the object types in the ASL arethe abstract concept types represented by theBNs, semantic rules formulated in this languageconstitute an "abstract semantic grammar"(ASG).
The ASG recognizes patterns ofsemantic relations among the BNs, and supportsanalysis and inference based on those patterns.It also permits rule-based control over thecreation, instantiation, and use of the BNs,including defining pathways for informationsharing among BNs using virtual evidence(Pearl, 1988).One use of the ASG is in post-parseprocessing of interpretations.
After the M+parser has constructed an interpretation, post-parse ASG productions may augment or alterthis interpretation.
One rule instructs "If twopathological conditions exist in a 'consistent-with' relation, and the first condition has a statemodifier (i.e.
*present or *absent), and thesecond condition does not, apply the firstcondition's state to the second condition".For instance, in the ambiguous sentence"There is no opacity consistent withpneumonia", if the parser doesn't correctlydetermine the scope of "no", it may produce thean interpretation in which *pneumonia lacks astate modifier, and is therefore inferred (bydefault) to be present.
This rule correctlyattaches (state-of *pneumonia *absent) to thisinterpretation.One important consequence of themodularity of the M+ BNs, and of the ability tonest them within the ASL, is that M+ cancompose BN instances in expressions ofarbitrary complexity.
For instance, it isstraightforward to represent the multipleanatomic concepts in the phrase "opacity in theinferior segment of the left upper lobe, adjacentto the heart":(and (head-of #phrase1 #find1)(located-at #find1 #anat1)(qualified-by #anat1 #anat2)(adjacent-to #anat1 #anat3))where the interpretive concepts of #anat1,#anat2 and #anat3 are *left-upper-lobe,*inferior-segment, and *heart, respectively.The set of binary predicates thatconstitutes a phrase interpretation in M+ forms adirected acyclic graph; thus we can refer to theinterpretation as an interpretation graph.
Theinterpretation graph of a new phrase is formedby unifying the graphs of its subphrases, asdescribed in section 3.2.4 Advantages of Bayesian NetworksAs mentioned, a BN training case bears asimilarity to a production rule.
It would bestraightforward to implement the training casesas a set of rules, and apply them to text analysisusing a deductive reasoning engine.
However,Bayesian reasoning has important advantagesover first order logic, including:1- BNs are able to respond gracefully toinput "noise".
A semantic BN may producereasonable inferences from phrasal patterns thatonly partially match any given training case, orthat overlap different cases, or that containwords in an unexpected order.
For instance,having trained on multi-word phrases containing"opacity", the single word "opacity" could raisethe probabilities of several interpretations suchas *localized-infiltrate and *parenchymal-abnormality, both of which are reasonablehypotheses for the underlying cause of opacityon a chest x-ray film.2- Bayesian inference works bi-directionally; i.e.
it is abductive as well asdeductive.
If instead of assigning word-levelnodes, one assigns the value of the summarynode, the probability of word values having ahigh correlation with that summary willincrease.
For instance, assigning the value*localized-infiltrate will raise the probabilitythat the topic word is "opacity".Bi-directional inference provides ameans for modeling the effects of lexicalcontext.
A value assignment made to one wordnode can alter value probabilities at unassignedword nodes, in a path of inference that passesthrough the connecting concept nodes.
Forinstance, if a BN were trained on "right upperlobe" and "left upper lobe", but had never seenthe term "bilateral", applying the BN to thephrase "bilateral upper lobes" would increasethe  probabilities of both "left" and "right",suggesting that "bilateral" is semanticallysimilar to "left" and "right".
This is oneapproach to guessing the node assignments ofunknown words, a step in the direction ofautomated learning of new training cases.Similarly, if the system encounters aphrase with a misspelling such as "rght upperlobe", by noting the orthographic similarity of"rght" to "right" and the fact that "right" ishighly predicted from surrounding words, it candetermine that "rght" is a misspelling of "right".The spell checker currently used by M+employs this technique.3 Generating  InterpretationGraphsAs mentioned, in M+ the interpretation graph ofa phrase is created by unifying the graphs of itschild phrases.
High joint probabilities in theresulting BN instances are one source ofevidence that the words thus brought togetherexist in the expected semantic pattern.However, corroborating evidence must besought in the syntax of the text.
Words whichappear together in a training phrase may not bein that same relation in a given text.
Forinstance, "no" and "pneumonia" supportdifferent conclusions in "no evidence ofpneumonia" and "patient has pneumonia with noapparent complicating factors".
M+ thereforeonly attempts to unify sub-interpretations thatappear, on syntactic grounds, to be talking aboutthe same things.
This is less constraining thanproduction rules that look for words in aspecific order, but more constraining thansimply pulling key words out of a string of text.The following are examples of rulesused to guide the unification of ASLinterpretation graphs.
For convenience, severalshorthand functional notations are used:  If Prepresents a phrase on the parse chart, root-bn(P) represents the root or head BN instance inP's interpretation graph, and type-of(root-bn(P))is the BN type of root-bn(P).
If A and B aresibling child phrases of parent phrase C, then C= parent-phrase(A,B).
Note that forconvenience, BN instances in the interpretationgraphs in Figures 4 - 6 are representedalternately as the words slotted in thoseinstances, and as the most probable interpretiveconcepts inferred by those instances.3.1 Same-type UnificationIf phrase A syntactically modifies phrase B,then M+ assumes that some semantic relationexists between A and B.
The nature of thatrelation is partly determinable from type-of(root-bn(A)) and type-of(root-bn(B)).
If type-of(root-bn(A)) = type-of(root-bn(B)), thatrelation is simply one where root-bn(A) androot-bn(B) are partial descriptions of a singleconcept.
If root-bn(A) and root-bn(B) areunifiable, M+ composes their input to formroot-bn(parent-phrase(A,B)).If in addition there are two unifiablesame-type BN instances X and Y linked to root-bn(A) and root-bn(B) respectively, via arcs ofthe same name, then X and Y also describe asingle concept, and the arcs describe a singlerelationship.
For instance, if X and Y describethe anatomic locations of  root-bn(A) and root-bn(B), and if root-bn(A) and root-bn(B) arepartial descriptions of a single "finding", then Xand Y are partial descriptions of a singleanatomic location, and ought to be unified.Figure 4:  Same-type unificationIn figure 4, in the Chest X-ray domain,the phrase "bilateral hazy lower lobe opacity" isinterpreted by unifying the interpretations of itssubphrases "bilateral hazy" and "lower lobeopacity".
Note that without any correspondingsyntactic transformation, this rule brings about a"virtual transformation", whereby words aregrouped together within BN instances in amanner that reflects the conceptual structure ofthe text.
In this example "bilateral hazy lowerlobe opacity" is treated as ("bilateral lowerlobe") ("hazy opacity").Figure 6: Grammar rule - based unification.3.2 Different-type UnificationIf phrase A syntactically modifies phrase B, andtype-of(root-bn(A)) <> type-of(root-bn(B)),then root-bn(A) and root-bn(B) representdifferent concepts within some semanticrelation.
M+ uses the ASG to identify thatrelation and to add it to the interpretation graphin the form of a path of named arcs connectingroot-bn(A) and root-bn(B).
This path mayinclude implicit connecting BN instances.M+ Implementation 45M+ is written in Common Lisp, with some Croutines for BN access.
The M+ architectureconsists of six basic components:  The parser,concept space, rule base, lexicon, ASL inferenceengine, and Bayesian network component.For instance, to interpret "subduralhemorrhage" in the Head CT domain, M+attempts to unify the graphs for the subphrases"subdural" and "hemorrhage", where type-of(root-bn("subdural")) = location, and type-of(root-bn("hemorrhage")) = topic.
M+identifies the connecting path for these twotypes as shown in figure 2, and adds that path tothe interpretation as shown in figure 5.
Notethat this path contains instances of the"observation" and "anatomy" BN types.As mentioned, the parser is animplementation of a bottom up chart parser withcontext free grammar.The concept space is a table of symbolsrepresenting types, objects and relations withinthe ASL.
These include BN names, BN nodevalue names, inter-BN relation names, and asmall ontology of useful concepts such as thoserelated to time.Figure 5.
Different-type unification.The rule base contains rules, whichcomprise the syntactic grammar and ASG.The lexicon is a table of Lisp-readableword information entries, obtained in part fromthe UMLS Specialist Lexicon.The ASL inference engine combinessymbolic unification with backward-chaininginference.
It can be used to match an ASGpattern against an interpretation graph, and toperform tests associated with grammar rules.3.3 Grammar Rule Based Unification The Bayesian network component utilizesthe Norsys Netica(TM) API, and includes a set ofLisp and C language routines for instantiatingand retrieving probabilities from BNs.Individual grammar rules in M+ can recognizesemantic relations, and add connecting arcs tothe interpretation graph.
For instance, M+ has arule which recognizes findings-type phrasesconnected with strings of the "suggesting"variety, and connects their graphs with a'consistent-with' arc.
This is used to interpret"opacity suggesting possible infarct" in theHead CT domain, as shown in figure 6.Training M+Porting M+ to a new medical domain involvesgathering a corpus of training sentences for thedomain, using the Netica(TM) graphical interfaceto create domain-specific BNs, and generatingtraining cases for the new BNs.The most time-consuming task is thecreation of training cases.
We have developed aprototype version of a Web-based tool whichlargely automates this task.
The basic idea is toenable M+ to guess the BN value assignmentsof unknown words, then use it to parse phrasessimilar to phrases already seen.
For instance,having been trained on the phrase "right upperlobe", the parser is able to produce reasonableparses, with some "guessed" value assignments,for "left upper lobe", "right middle lobe","bilateral lungs", etc.
The BN assignmentsproduced by the parse are output as tentativenew cases to be reviewed and corrected by thehuman trainer.The training process begins with aninitial set of interpreted "seed" phrases.
Fromthis set, the tool can apply the parser to phrasessimilar to this set, and so semi-automaticallytraverse ever widening semantically contiguousareas within the space of corpus phrases.
As thetraining proceeds, the role of the human trainerincreasingly becomes one of providingcorrection and interpretations for semanticpatterns the system is increasingly able todiscover on its own.To parse phrases containing unknownwords, M+ uses a technique based on a variationof the vector space model of lexical semanticsimilarity (Manning and Schutze, 1999).
AsM+ encounters an unknown word, it gathers alist of training corpus words judged similar tothat word, as predicted by the vector spacemeasure.
It then identifies BN nodes whoseknown values significantly overlap with this list,and provisionally assigns the unknown word asa new value for those nodes.
The assignmentresulting in the best parsetree is selected for thenew provisional training case.6 EvaluationM+ was evaluated for the extraction ofAmerican College of Radiology (ACR)utilization review codes from Head CT reports(Fiszman, 2002).
The ACR codes compare theoutcome in a report with the suspected diagnosisprovided by emergency department physicians.If the outcome relates to the suspected diagnosisthen the report should be encoded as positive(P).
If the outcome is negative and does notrelate to the suspected diagnosis then the reportshould be encoded as negative  (N).
In order toextract those ACR codes we trained M+ toextract eleven broad disease concepts, theninferred the ACR codes based on the applicationof a rule to the M+ output:  If any of theconcepts was present, the report was consideredpositive, else the report was considerednegative.Twenty six hundred head CT scanreports were used for this evaluation.
Sixhundred reports were randomly selected fortesting, and the rest were used to train M+ inthis domain.
The performance of M+ on thistask was measured against that of four boardcertified physicians, using a gold standard basedon majority vote, as described in (Fiszman,2002).
For each subject we calculated recall,precision and specificity with their respective 95% confidence intervals for the capture of ACRutilization codes.From 600 head CT reports, 67 werejudged to be positive (P) by the gold standardphysicians and 534 were judged to be negative(N).
Therefore the positive rate for head CT inthis sample was 11%.
Recall, precision andspecificity for every subject are presented withtheir respective 95% confidence intervals inTable 1.
The physicians had an average recall of88% (CI, 84% to 92.%), an average precision of86% (CI, 81% to 90%), and average specificityof 98% (CI, 97% to 99%).
M+  had recall of87% (CI, 78% to 95%), precision of 85% (CI,77% to 94%) and specificity of 98% (CI, 97%to 99).Table 1.
Results of  ACR utilization code study.Subject Recall Specificity PrecisionPhysician1 0.83(0.74-0.92)0.99(0.98-1.00)0.91(0.84-0.99)Physician2 0.88(0.81-0.97)0.98(0.97-0.99)0.84(0.75-0.93)Physician3 0.93(0.87-1.00)0.98(0.97-0.99)0.86(0.78-0.95)Physician4 0.88(0.96-0.99)0.97(0.96-0.99)0.81(0.71-0.90)M+ 0.87(0.78-0.95)0.98(0.97-0.99)0.85(0.77-0.94)The results on Head CT reports areencouraging, but there are limitations.
We onlyevaluated 600 reports, because it's very hard toget physicians to produce gold standard data formedical reports.
The prevalence of positivereports is only 11% and reflects the fact that theindividual brain conditions  have very lowprevalence.78ConclusionsM+ and its predecessors have demonstrated thatBNs provide a useful semantic model formedical text processing.
In practice, a medicalNLP system will frequently encounter missingand unknown words,  unknown andungrammatical phrase structures, andtelegraphic usages.
Knowledge databases willbe imperfect and incomplete.
Using BNs forsemantic representation brings a noise-tolerant,partial match-tolerant, context-sensitivecharacter to the recognition of semanticpatterns, and to relevant inferences based onthose patterns.
In addition, BNs can be used toguess the semantic types of unknown words,providing a basis for bootstrapping the system'ssemantic knowledge.AcknowledgementsMany thanks to Wendy W. Chapman for heradvice and input in this paper, and her efforts tomake M+ a useful addition to the RODS projectat the University of Pittsburgh.ReferencesChapman W., Christensen L. M., Wagner M., HaugP.
J., Ivanov O., Dowling J. N., Olszewski R. T.2002.
Syndromic Detection from Free-textTriage Diagnoses: Evaluation of a MedicalLanguage Processing System before Deploymentin the Winter Olympics.
Proc AMIA Symp.
(submitted).Chomsky, Noam.
1965.
Aspects of the theory ofsyntax.
Special technical report (MassachusettsInstitute of Technology, Research Laboratory ofElectronics); no.
11.
Cambridge, MA: MIT Press.Fiszman M., Blatter D.D., Christensen L.M., OderichG., Macedo T., Eidelwein A.P., Haug P.J.
2002.Utilization review of head CT scans: value of amedical language processing system.
AmericanJournal of Roentgenology (AJR).
(submitted)Friedman C, Alderson PO, Austin JH, Cimino JJ,Johnson SB.
1994,  A general natural-languagetext processor for clinical radiology.
J Am MedInform Assoc.
Mar-Apr;1(2) pp.
161-74.Friedman N., Getoor L., Koller D. and Pfeffer A.1999.
Learning Probabilistic Relational Models.Proceedings of the 16th International JointConference on Artificial Intelligence (IJCAI):pp.
1300-1307.Haug P. J., Christensen L., Gundersen M., ClemonsB., Koehler S., Bauer K. 1997.
A naturallanguage parsing system for encoding admittingdiagnoses.
Proc AMIA Symp.
81: pp.
4-8.Koehler, S. B.
1998.
SymText: A natural languageunderstanding system for encoding free textmedical data.
Ph.D. Dissertation, University ofUtah.Koller D., and Pfeffer A.
1997.
Object-OrientedBayesian Networks.
Proceedings of the 13thAnnual Conference on Uncertainty in AI:  pp.302-313.Lin R, Lenert L, Middleton B, Shiffman S. A free-text processing system to capture physicalfindings: Canonical Phrase Identification System(CAPIS).
Proc Annu Symp Comput Appl MedCare.
pp.
843-7.Manning C. D. and Schutze H. 1999.
Foundations ofStatistical Natural Language Processing.
MITPress.Minsky, M. 1975.
A framework for representingknowledge.
In The Psychology of Human Vision,ed.
P. H. Winston, pp.
211-277.
McGraw Hill.Moore, R. C. 1989.
Unification-based SemanticInterpretation.
Proceedings of the 27th AnnualMeeting of the Association for ComputationalLinguistics, pp33-41.Pearl, Judea.
1988.
Probabilistic inference inintelligent systems.
Networks of plausibleinference:  Morgan Kaufmann.Ranum D.L.
1989.
Knowledge-based understandingof radiology text.
Comput Methods ProgramsBiomed.
Oct-Nov;30(2-3) pp.209-215.Romacker, Martin and Hahn, Udo.
2000.
Anempirical assessment of semantic interpretation.ANLP/NAACL 2000 -- Proceedings of the 6thApplied Natural Language ProcessingConference & the 1st Conference of the NorthAmerican Chapter of the Association forComputational Linguistics.
pp.
327-334.Schank, R.C.
and R. Abelson.
1997.
Scripts, Plans,Goals, and Understanding.
Hillsdale, NJ:Lawrence Erlbaum.Smart, J. F. and M. Roux.
1995.
A  model formedical knowledge representation application tothe analysis of descriptive pathology reports.Methods Inf Med.
Sep;34(4) pp.
352-60.
