Proceedings of the 12th Conference of the European Chapter of the ACL, pages 737?744,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsUsing Non-lexical Features to Identify Effective Indexing Terms forBiomedical IllustrationsMatthew Simpson, Dina Demner-Fushman, Charles Sneiderman,Sameer K. Antani, George R. ThomaLister Hill National Center for Biomedical CommunicationsNational Library of Medicine, NIH, Bethesda, MD, USA{simpsonmatt, ddemner, csneiderman, santani, gthoma}@mail.nih.govAbstractAutomatic image annotation is an attrac-tive approach for enabling convenient ac-cess to images found in a variety of docu-ments.
Since image captions and relevantdiscussions found in the text can be usefulfor summarizing the content of images, itis also possible that this text can be used togenerate salient indexing terms.
Unfortu-nately, this problem is generally domain-specific because indexing terms that areuseful in one domain can be ineffectivein others.
Thus, we present a supervisedmachine learning approach to image an-notation utilizing non-lexical features1 ex-tracted from image-related text to selectuseful terms.
We apply this approach toseveral subdomains of the biomedical sci-ences and show that we are able to reducethe number of ineffective indexing terms.1 IntroductionAuthors of biomedical publications often utilizeimages and other illustrations to convey informa-tion essential to the article and to support and re-inforce textual content.
These images are usefulin support of clinical decisions, in rich documentsummaries, and for instructional purposes.
Thetask of delivering these images, and the publica-tions in which they are contained, to biomedicalclinicians and researchers in an accessible way isan information retrieval problem.Current research in the biomedical domain (e.g.,Antani et al, 2008; Florea et al, 2007), has in-vestigated hybrid approaches to image retrieval,combining elements of content-based image re-trieval (CBIR) and annotation-based image re-trieval (ABIR).
ABIR, compared to the image-1Non-lexical features describe attributes of image-relatedtext but not the text itself, e.g., unlike a bag-of-words model.only approach of CBIR, offers a practical advan-tage in that queries can be more naturally specifiedby a human user (Inoue, 2004).
However, manu-ally annotating biomedical images is a laboriousand subjective task that often leads to noisy results.Automatic image annotation is a more robustapproach to ABIR than manual annotation.
Un-fortunately, automatically selecting the most ap-propriate indexing terms is an especially challeng-ing problem for biomedical images because ofthe domain-specific nature of these images andthe many vocabularies used in the biomedical sci-ences.
For example, the term ?sweat gland adeno-carcinoma?
could be a useful indexing term for animage found in a dermatology publication, but it isless likely to have much relevance in describing animage from a cardiology publication.
On the otherhand, the term ?mitral annular calcification?
maybe of great relevance for cardiology images, but oflittle relevance for dermatology ones.Our problem may be summarized as follows:Given an image, its caption, its discussion in thearticle text (henceforth the image mention), and alist of potential indexing terms, select the termsthat are most effective at describing the content ofthe image.
For example, assume the image shownin Figure 1, obtained from the article ?MetastaticHidradenocarcinoma: Efficacy of Capecitabine?by Thomas et al (2006) in Archives of Dermatol-ogy, has the following potential indexing terms,?
Histopathology finding?
Reviewed?
Confirmation?
Diagnosis aspect?
Diagnosis?
Eccrine?
Sweat gland adenocarcinoma?
Lesionwhich have been extracted from the image men-tion.
While most of these do not uniquely identify737Caption: Figure 1.
On recurrence, histologic featuresof porocarcinoma with an intraepidermal spread ofneoplastic clusters (hematoxylin-eosin, original magni-fication x100).Mention: Histopathologic findings were reviewedand confirmed a diagnosis of eccrine hidradenocarci-noma for all lesions excised (Figure 1).Figure 1: Example Image.
We index an imagewith concepts generated from its caption and dis-cussion in the document text (mention).
This im-age is from ?Metastatic Hidradenocarcinoma: Ef-ficacy of Capecitabine?
by Thomas et al (2006)and is reprinted with permission from the authors.the image, we would like to automatically select?sweat gland adenocarcinoma?
and ?eccrine?
forindexing because they clearly describe the contentand purpose of the image?supporting a diagno-sis of hidradenocarinoma, an invasive cancer ofsweat glands.
Note that effective indexing termsneed not be exact lexical matches of the text.
Eventhough ?diagnosis?
is an exact match, its meaningis too broad in this context to be a useful term.In a machine learning approach to image anno-tation, training data based on lexical features aloneis not sufficient for finding salient indexing terms.Indeed, we must classify terms that are not en-countered while training.
Therefore, we hypoth-esize that non-lexical features, which have beensuccessfully used for speech and genre classifica-tion tasks, among others (see Section 5 for relatedwork), may be useful in classifying text associatedwith images.
While this approach is broad enoughto apply to any retrieval task, given the goals of ourongoing research, we restrict ourselves to studyingits feasibility in the biomedical domain.In order to achieve this, we make use of thepreviously developed MetaMap (Aronson, 2001)tool, which maps text to concepts contained inthe Unified Medical Language System R?
(UMLS)Metathesaurus R?
(Lindberg et al, 1993).
TheUMLS is a compendium of several controlled vo-cabularies in the biomedical sciences that providesa semantic mapping relating concepts from thevarious vocabularies (Section 2).
We then use a su-pervised machine learning approach, described inSection 3, to classify the UMLS concepts as usefulindexing terms based on their non-lexical features,gleaned from the article text and MetaMap output.Experimental results, presented in Section 4, in-dicate that ineffective indexing terms can be re-duced using this classification technique.
We con-clude that ABIR approaches to biomedical im-age retrieval as well as hybrid CBIR/ABIR ap-proaches, which rely on both image content andannotations, can benefit from an automatic anno-tation process utilizing non-lexical features to aidin the selection of useful indexing terms.2 Image Retrieval: Recent WorkAutomatic image annotation is a broad topic, andthe automatic annotation of biomedical images,specifically, has been a frequent component ofthe ImageCLEF2 cross-language image retrievalworkshop.
In this section, we describe previouswork in biomedical image retrieval that forms thebasis of our approach.
Refer to Section 5 for workrelated to our method in general.Demner-Fushman et al (2007) developed a ma-chine learning approach to identify images frombiomedical publications that are relevant to clin-ical decision support.
In this work, the authorsutilized both image and textual features to clas-sify images based on their usefulness in evidence-based medicine.
In contrast, our work is focusedon selecting useful biomedical image indexingterms; however, we utilize the methods developedin their work to extract images and their relatedcaptions and mentions.Authors of biomedical publications often as-semble multiple images into a single multi-panelfigure.
Antani et al (2008) developed a uniquetwo-phase approach for detecting and segmentingthese figures.
The authors rely on cues from cap-tions to inform an image analysis algorithm thatdetermines panel edge information.
We make useof this approach to uniquely associate caption andmention text with a single image.2http://imageclef.org/738Our current work most directly stems from theresults of a term extraction and image annota-tion evaluation performed by Demner-Fushmanet al (2008).
In this study, the authors uti-lized MetaMap to extract potential indexing terms(UMLS concepts) from image captions and men-tions.
They then asked a group of five physiciansand one medical imaging specialist (four of whomare trained in medical informatics) to manuallyclassify each concept as being ?useful for index-ing?
its associated images or ineffective for thispurpose.
The reviewers also had the opportunityto identify additional indexing terms that were notautomatically extracted by MetaMap.In total, the reviewers evaluated 4006 concepts(3,281 of which were unique), associated with186 images from 109 different biomedical articles.Each reviewer was given 50 randomly chosen im-ages from the 2006?2007 issues of Archives of Fa-cial Plastic Surgery3 and Cardiovascular Ultra-sound4.
Since MetaMap did not automatically ex-tract all of the useful indexing terms, this selectionprocess exhibited high recall averaging 0.64 buta low precision of 0.11.
Indeed, assuming all theextracted terms were selected for indexing, this re-sults in an average F1-score of only 0.182 for theclassification problem.
Our work is aimed at im-proving this baseline classification by reducing thenumber of ineffective terms selected for indexing.3 Term Selection MethodA pictorial representation of our term extractionand selection process is shown in Figure 2.
Werely on the previously described methods to ex-tract images and their corresponding captions andmentions, and the MetaMap tool to map this textto UMLS concepts.
These concepts are potentialindexing terms for the associated image.We derive term features from various textualitems, such as the preferred name of the UMLSconcept, the MetaMap output for the concept, thetext that generated the concept, the article contain-ing the image, and the document collection con-taining the article.
These are all described in moredetail in Section 3.2.
Once the feature vectors arebuilt, we automatically classify the term as eitherbeing useful for indexing the image or not.To select useful indexing terms, we trained abinary classifier, described in Section 3.3, in a3http://archfaci.ama-assn.org/4http://www.cardiovascularultrasound.com/Figure 2: Term Extraction and Selection.
Wegather features for the extracted terms and usethem to train a classifier that selects the terms thatare useful for indexing the associated images.supervised learning scenario with data obtainedfrom the previous study by Demner-Fushman et al(2008).
We obtained our evaluation data from the2006 Archives of Dermatology5 journal.
Note thatour training and evaluation data represent distinctsubdomains of the biomedical sciences.In order to reduce noise in the classification ofour evaluation data, we asked two of the review-ers who participated in the initial study to man-ually classify our extracted terms as they did forour training data.
In doing so, they each eval-uated an identical set of 1539 potential indexingterms relating to 50 randomly chosen images from31 different articles.
We measured the perfor-mance of our classifier in terms of how well it per-formed against this manual evaluation.
These re-sults, as well as a discussion pertaining to the inter-annotator agreement of the two reviewers, are pre-sented in Section 4.Since our general approach is not specific to thebiomedical domain, it could equally be applied in5http://archderm.ama-assn.org/739any domain with an existing ontology.
For exam-ple, the UMLS and MetaMap can be replaced bythe Art and Architecture Thesaurus6 and an equiv-alent mapping tool to annotate images related toart and art history (Klavans et al, 2008).3.1 TerminologyTo describe our features, we adopt the followingterminology.?
A collection contains all the articles from agiven publication for a specified number ofyears.
For example, the 2006?2007 issues ofCardiovascular Ultrasound represent a sin-gle collection.?
A document is a specific biomedical articlefrom a particular collection and contains im-ages and their captions and mentions.?
A phrase is the portion of text that MetaMapmaps to UMLS concepts.
For example, fromthe caption in Figure 1, the noun phrase ?his-tologic features?
maps to four UMLS con-cepts: ?Histologic,?
?Characteristics,?
?Pro-tein Domain?
and ?Array Feature.??
A mapping is an assignment of a phrase toa particular set of UMLS concepts.
Eachphrase can have more than one mapping.3.2 FeaturesUsing this terminology, we define the followingfeatures used to classify potential indexing terms.We refer to these as non-lexical features becausethey generally characterize UMLS concepts, go-ing beyond the surface representation of wordsand lexemes appearing in the article text.F.1 CUI (nominal): The Concept Unique Iden-tifier (CUI) assigned to the concept in theUMLS Metathesaurus.
We choose the con-cept identifier as a feature because some fre-quently mapped concepts are consistentlyineffective for indexing the images in ourtraining and evaluation data.
For exam-ple, the CUI for ?Original,?
another termmapped from the caption shown in Figure1, is ?C0205313.?
Our results indicate that?C0205313,?
which occurs 19 times in ourevaluation data, never identifies a useful in-dexing term.6http://www.getty.edu/research/conducting research/vocabularies/aat/F.2 Semantic Type (nominal): The concept?s se-mantic categorization.
There are currently132 different semantic types7 in the UMLSMetathesaurus.
For example, The semantictype of ?Original?
is ?Idea or Concept.
?F.3 Presence in Caption (nominal): true if thephrase that generated the concept is locatedin the image caption; false if the phrase islocated in the image mention.F.4 MeSH Ratio (real): The ratio of words ci inthe concept c that are also contained in theMedical Subject Headings (MeSH terms)8M assigned to the document to the totalnumber of words in the concept.R(m) =|{ci : ci ?M}||c|(1)MeSH is a controlled vocabulary created bythe US National Library of Medicine (NLM)to index biomedical articles.
For example,?Adenoma, Sweat?
is one MeSH term as-signed to ?Metastatic Hidradenocarcinoma:Efficacy of Capecitabine?
(Thomas et al,2006), the article containing the image fromFigure 1.F.5 Abstract Ratio (real): The ratio of wordsci in the concept c that are also in the doc-ument?s abstract A to the total number ofwords in the concept.R(a) =|{ci : ci ?
A}||c|(2)F.6 Title Ratio (real): The ratio of words ci inthe concept c that are also in the document?stitle T to the total number of words in theconcept.R(t) =|{ci : ci ?
T }||c|(3)F.7 Parts-of-Speech Ratio (real): The ratio ofwords pi in the phrase p that have beentagged as having part of speech s to the totalnumber of words in the phrase.R(s) =|{pi : TAG(pi) = s}||p|(4)This feature is computed for noun, verb, ad-jective and adverb part-of-speech tags.
We7http://www.nlm.nih.gov/research/umls/META3 currentsemantic types.html8http://www.nlm.nih.gov/mesh/740obtain tagging information from the outputof MetaMap.F.8 Concept Ambiguity (real): The ratio of thenumber of mappingsmi of phrase p that con-tain concept c to the total number of map-pings for the phrase:A =|{mpi : c ?
mpi }||mp|(5)F.9 Tf-idf (real): The frequency of term ti (i.e.,the phrase that generated the concept) timesits inverse document frequency:tfidfi,j = tfi,j ?
idfi (6)The term frequency tfi,j of term ti in docu-ment dj is given bytfi,j =ni,j?|D|k=1 nk,j(7)where ni,j is the number of occurrences of tiin dj , and the denominator is the number ofoccurrences of all terms in dj .
The inversedocument frequency idfi of ti is given byidfi = log|D||{dj : ti ?
dj}|(8)where |D| is the total number of documentsin the collection, and the denominator is thetotal number of documents that contain ti(see Salton and Buckley, 1988).F.10 Document Location (real): The location inthe document of the phrase that generatedthe concept.
This feature is continuous on[0, 1] with 0 representing the beginning ofthe document and 1 representing the end.F.11 Concept Length (real): The length of theconcept, measured in number of characters.For the purpose of computing F.9 and F.10, we in-dexed each collection with the Terrier9 informa-tion retrieval platform.
Terrier was configured touse a block indexing scheme with a Tf-idf weight-ing model.
Computation of all other features isstraightforward.3.3 ClassifierWe explored these feature vectors using variousclassification approaches available in the Rapid-Miner10 tool.
Unlike many similar text and image9http://ir.dcs.gla.ac.uk/terrier/10http://rapid-i.com/classification problems, we were unable to achieveresults with a Support Vector Machine (SVM)learner (libSVMLearner) using the Radial BaseFunction (RBF).
Common cost and width parame-ters were used, yet the SVM classified all terms asineffective.
Identical results were observed usinga Na?
?ve Bayes (NB) learner.For these reasons, we chose to use the Aver-aged One-Dependence Estimator (AODE) learner(Webb et al, 2005) available in RapidMiner.AODE is capable of achieving highly accurateclassification results with the quick training timeusually associated with NB.
Because this learnerdoes not handle continuous attributes, we pre-processed our features with equal frequency dis-cretization.
The AODE learner was trained in aten-fold cross validation of our training data.4 ResultsResults relating to specific aspects of our work(annotation, features and classification) are pre-sented below.4.1 Inter-Annotator AgreementTwo independent reviewers manually classifiedthe extracted terms from our evaluation data asuseful for indexing their associated images or not.The inter-annotator agreement between reviewersA and B is shown in the first row of Table 1.
Al-though both reviewers are physicians trained inmedical informatics, their initial agreement is onlymoderate, with ?
= 0.519.
This illustrates thesubjective nature of manual ABIR and, in general,the difficultly in reliably classifying potential in-dexing terms for biomedical images.Annotator Pr(a) Pr(e) ?A/B 0.847 0.682 0.519A/Standard 0.975 0.601 0.938B/Standard 0.872 0.690 0.586Table 1: Inter-annotator Agreement.
The prob-ability of agreement Pr(a), expected probability ofchance agreement Pr(e), and the associated Co-hen?s kappa coefficient ?
are given for each re-viewer combination.After their initial classification, the two review-ers were instructed to collaboratively reevaluatethe subset of extracted terms upon which they dis-agreed (roughly 15% of the terms) and create a741Feature Gain ?2F.1 CUI 0.003 13.331F.2 Semantic Type 0.015 68.232F.3 Presence in Caption 0.008 35.303F.4 MeSH Ratio 0.043 285.701F.5 Abstract Ratio 0.023 114.373F.6 Title Ratio 0.021 132.651F.7 Noun Ratio 0.053 287.494Verb Ratio 0.009 26.723Adjective Ratio 0.021 96.572Adverb Ratio 0.002 5.271F.8 Concept Ambiguity 0.008 33.824F.9 Tf-idf 0.004 21.489F.10 Document Location 0.002 12.245F.11 Phrase Length 0.021 102.759Table 2: Feature Comparison.
The informationgain and chi-square statistic is shown for each fea-ture.
A higher score indicates greater influence onterm effectiveness.gold standard evaluation.
The second and thirdrows of Table 1 suggest the resulting evaluationstrongly favors reviewer A?s initial classificationcompared to that of reviewer B.Since the reviewers of the training data eachclassified terms from different sets of randomlyselected images, it is impossible to calculate theirinter-annotator agreement.4.2 Effectiveness of FeaturesThe effectiveness of individual features in describ-ing the potential indexing terms is shown in Ta-ble 2.
We used two measures, both of which in-dicate a similar trend, to calculate feature effec-tiveness: Information gain (Kullback-Leibler di-vergence) and the chi-square statistic.Under both measures, the MeSH ratio (F.4) isone of the most effective features.
This makesintuitive sense because MeSH terms are assignedto articles by specially trained NLM profession-als.
Given the large size of the MeSH vocabu-lary, it is not unreasonable to assume that an arti-cle?s MeSH terms could be descriptive, at a coarsegranularity, of the images it contains.
Also, thesubjectivity of the reviewers?
initial data calls intoquestion the usefulness of our training data.
Itmay be that MeSH terms, consistently assignedto all documents in a particular collection, are amore reliable determiner of the usefulness of po-tential indexing terms.
Furthermore, the study byDemner-Fushman et al (2008) found that, on aver-age, roughly 25% of the additional (useful) termsthe reviewers added to the set of extracted termswere also found in the MeSH terms assigned tothe document containing the particular image.The abstract and title ratios (F.6 and F.5) alsohad a significant effect on the classification out-come.
Similar to the argument for MeSH terms, asthese constructs are a coarse summary of the con-tents of an article, it is not unreasonable to assumethey summarize the images contained therein.Finally, the noun ratio (F.7) was a particularlyeffective feature, and the length of the UMLS con-cept (F.11) was moderately effective.
Interest-ingly, tf-idf and document location (F.9 and F.10),both features computed using standard informa-tion retrieval techniques, are among the least ef-fective features.4.3 ClassificationWhile the AODE learner performed reasonablywell for this task, the difficulty encountered whentraining the SVM learner may be explained asfollows.
The initial inter-annotator agreementof the evaluation data suggests that it is likelythat our training data contained contradictory ormislabeled observations, preventing the construc-tion of a maximal-margin hyperplane required bythe SVM.
An SVM implementation utilizing softmargins (Cortes and Vapnik, 1995) would likelyachieve better results on our data, although at theexpense of greater training time.
The success ofthe AODE learner in this case is probably due toits resilience to mislabeled observations.Annotator Precision Recall F1-scoreA 0.258 0.442 0.326B 0.200 0.225 0.212Combined 0.326 0.224 0.266Standard 0.453 0.229 0.304Standarda 0.492 0.231 0.314Training 0.502 0.332 0.400Table 3: Classification Results.
The classifier?sprecision and recall, as well as the correspondingF1-score, are given for the responses of each re-viewer.aFor comparison, the classifier was also trained using thesubset of training data containing responses from reviewersA and B only.742Classification results are shown in Table 3.
Theprecision and recall of the classification scheme isshown for the manual classification by reviewersA and B in the first and second rows.
The thirdrow contains the results obtained from combiningthe results of the two reviewers, and the fourth rowshows the classification results compared to thegold standard obtained after discovering the initialinter-annotator agreement.We hypothesized that the training data labelsmay have been highly sensitive to the subjectiv-ity of the reviewers.
Therefore, we retrained thelearner with only those observations made by re-viewers A and B (of the five total reviewers) andagain compared the classification results with thegold standard.
Not surprisingly, the F1-score ofthis classification (shown in the fifth row) is some-what improved compared to that obtained whenutilizing the full training set.The last row in Table 3 shows the results of clas-sifying the training data.
That is, it shows the re-sults of classifying one tenth of the data after a ten-fold cross validation and can be considered an up-per bound for the performance of this classifier onour evaluation data.
Notice that the associated F1-score for this experiment is only marginally bet-ter than that of the unseen data.
This implies thatit is possible to use training data from particularsubdomains of the biomedical sciences (cardiol-ogy and plastic surgery) to classify potential in-dexing terms in other subdomains (dermatology).Overall, the classifier performed best when ver-ified with reviewer A, with an F1-score of 0.326.Although this is relatively low for a classificationtask, these results improve upon the baseline clas-sification scheme (all extracted terms are usefulfor indexing) with an F1-score of 0.182 (Demner-Fushman et al, 2008).
Thus, non-lexical featurescan be leveraged, albeit to a small degree withour current features and classifier, in automaticallyselecting useful image indexing terms.
In futurework, we intend to explore additional features andalternative tools for mapping text to the UMLS.5 Related WorkNon-lexical features have been successful in manycontexts, particularly in the areas of genre classifi-cation and text and speech summarization.Genre classification, unlike text classification,discriminates between document style instead oftopic.
Dewdney et al (2001) show that non-lexicalfeatures, such as parts of speech and line-spacing,can be successfully used to classify genres, andFerizis and Bailey (2006) demonstrate that accu-rate classification of Internet documents is possi-ble even without the expensive part-of-speech tag-ging of similar methods.
Recall that the noun ratio(F.7) was among the most effective of our features.Finn and Kushmerick (2006) describe a studyin which they classified documents from variousdomains as ?subjective?
or ?objective.?
They, too,found that part-of-speech statistics as well as gen-eral text statistics (e.g., average sentence length)are more effective than the traditional bag-of-words representation when classifying documentsfrom multiple domains.
This supports the notionthat we can use non-lexical features to classify po-tential indexing terms in one biomedical subdo-main using training data from another.Maskey and Hirschberg (2005) found thatprosodic features (see Ward, 2004) combined withstructural features are sufficient to summarize spo-ken news broadcasts.
Prosodic features relate tointonational variation and are associated with par-ticularly important items, whereas structural fea-tures are associated with the organization of a typ-ical broadcast: headlines, followed by a descrip-tion of the stories, etc.Finally, Schilder and Kondadadi (2008) de-scribe non-lexical word-frequency features, sim-ilar to our ratio features (F.4?F.7), which areused with a regression SVM to efficiently gener-ate query-based multi-document summaries.6 ConclusionImages convey essential information in biomedi-cal publications.
However, automatically extract-ing and selecting useful indexing terms from thearticle text is a difficult task given the domain-specific nature of biomedical images and vocab-ularies.
In this work, we use the manual classifi-cation results of a previous study to train a binaryclassifier to automatically decide whether a poten-tial indexing term is useful for this purpose or not.We use non-lexical features generated for eachterm with the most effective including whether theterm appears in the MeSH terms assigned to thearticle and whether it is found in the article?s ti-tle and caption.
While our specific retrieval taskrelates to the biomedical domain, our results in-dicate that ABIR approaches to image retrieval inany domain can benefit from an automatic annota-743tion process utilizing non-lexical features to aid inthe selection of indexing terms or the reduction ofineffective terms from a set of potential ones.ReferencesSameer Antani, Dina Demner-Fushman, Jiang Li,Balaji V. Srinivasan, and George R. Thoma.2008.
Exploring use of images in clinical ar-ticles for decision support in evidence-basedmedicine.
In Proc.
of SPIE-IS&T ElectronicImaging, pages 1?10.Alan R. Aronson.
2001.
Effective mapping ofbiomedical text to the UMLS metathesaurus:The MetaMap program.
In Proc.
of the AnnualSymp.
of the American Medical Informatics As-sociation (AMIA), pages 17?21.Corinna Cortes and Vladimir Vapnik.
1995.Support-vector networks.
Machine Learning,20(3):273?297.Dina Demner-Fushman, Sameer Antani, MatthewSimpson, and George Thoma.
2008.
Combin-ing medical domain ontological knowledge andlow-level image features for multimedia index-ing.
In Proc.
of the Language Resources forContent-Based Image Retrieval Workshop (On-toImage), pages 18?23.Dina Demner-Fushman, Sameer K. Antani, andGeorge R. Thoma.
2007.
Automatically findingimages for clinical decision support.
In Proc.
ofthe Intl.
Workshop on Data Mining in Medicine(DM-Med), pages 139?144.Nigel Dewdney, Carol VanEss-Dykema, andRichard MacMillan.
2001.
The form is the sub-stance: Classification of genres in text.
In Proc.of the Workshop on Human Language Technol-ogy and Knowledge Management, pages 1?8.George Ferizis and Peter Bailey.
2006.
Towardspractical genre classification of web documents.In Proc.
of the Intl.
Conference on the WorldWide Web (WWW), pages 1013?1014.Aidan Finn and Nicholas Kushmerick.
2006.Learning to classify documents according togenre.
Journal of the American Society forInformation Science and Technology (JASIST),57(11):1506?1518.F.
Florea, V. Buzuloiu, A. Rogozan, A. Bensrhair,and S. Darmoni.
2007.
Automatic image an-notation: Combining the content and context ofmedical images.
In Intl.
Symp.
on Signals, Cir-cuits and Systems (ISSCS), pages 1?4.Masashi Inoue.
2004.
On the need for annotation-based image retrieval.
In Proc.
of the Workshopon Information Retrieval in Context (IRiX),pages 44?46.Judith Klavans, Carolyn Sheffield, Eileen Abels,Joan Beaudoin, Laura Jenemann, Tom Lipin-cott, Jimmy Lin, Rebecca Passonneau, TandeepSidhu, Dagobert Soergel, and Tae Yano.
2008.Computational linguistics for metadata build-ing: Aggregating text processing technologiesfor enhanced image access.
In Proc.
of the Lan-guage Resources for Content-Based Image Re-trieval Workshop (OntoImage), pages 42?47.D.A.
Lindberg, B.L.
Humphreys, and A.T. Mc-Cray.
1993.
The unified medical languagesystem.
Methods of Information in Medicine,32(4):281?291.Sameer Maskey and Julia Hirschberg.
2005.Comparing lexical, acoustic/prosodic, struc-tural and discourse features for speech sum-marization.
In Proc.
of the European Confer-ence on Speech Communication and Technol-ogy (EUROSPEECH), pages 621?624.Gerard Salton and Christopher Buckley.
1988.Term-weighting approaches in automatic textretrieval.
Information Processing & Manage-ment, 24(5):513?523.Frank Schilder and Ravikumar Kondadadi.
2008.FastSum: Fast and accurate query-based multi-document summarization.
In Proc.
of theWorkshop on Human Language Technology andKnowledge Management, pages 205?208.Jouary Thomas, Kaiafa Anastasia, LipinskiPhilippe, Vergier Be?atrice, Lepreux Se?bastien,Delaunay Miche`le, and Ta??ebAlain.
2006.Metastatic hidradenocarcinoma: Efficacy ofcapecitabine.
Archives of Dermatology,142(10):1366?1367.Nigel Ward.
2004.
Pragmatic functions ofprosodic features in non-lexical utterances.In Proc.
of the Intl.
Conference on SpeechProsody, pages 325?328.Geoffrey I. Webb, Janice R. Boughton, and ZhihaiWang.
2005.
Not so na?
?ve bayes: Aggregatingone-dependence estimators.
Machine Learning,58(1):5?24.744
