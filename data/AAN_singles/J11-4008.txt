Half-Context Language ModelsHinrich Schu?tze?University of StuttgartMichael Walsh?
?University of StuttgartThis article investigates the effects of different degrees of contextual granularity on languagemodel performance.
It presents a new language model that combines clustering and half-contextualization, a novel representation of contexts.
Half-contextualization is based on the half-context hypothesis that states that the distributional characteristics of a word or bigram are bestrepresented by treating its context distribution to the left and right separately and that only direc-tionally relevant distributional information should be used.
Clustering is achieved using a newclustering algorithm for class-based language models that compares favorably to the exchangealgorithm.
When interpolated with a Kneser-Ney model, half-context models are shown to havebetter perplexity than commonly used interpolated n-gram models and traditional class-basedapproaches.
A novel, fine-grained, context-specific analysis highlights those contexts in whichthe model performs well and those which are better treated by existing non-class-based models.1.
IntroductionStochastic language models are a crucial component of many speech and languagetechnology applications.
The key problem encountered by these models is that sparsedata make the accurate estimation of the probability of novel and rare word sequencesdifficult.In order to address this, language model researchers have developed a number ofstrategies.
Of particular interest in this article are the following four:1.
Context length.
Careful selection of the length of the history or contextthat is the basis for predicting the next word.2.
Interpolation.
Models typically interpolate several predictions, forexample, predictions that are based on several different context lengths.3.
Classes.
In a class-based model, prediction is (partially) based on classesthat the n-grams involved are members of.4.
Similarity.
Similarity models smooth predictions with predictions forsimilar entities.?
Institute for Natural Language Processing.
E-mail: hinrichcl11@ifnlp.org.??
Universita?t Stuttgart, Institut fu?r Maschinelle Sprachverarbei-Azenbergstrasse 12, D-70174 Stuttgart,Germany.
E-mail: michael.walsh@ims.uni-stuttgart.de.Submission received: 3 August 2010; revised submission received: 17 February 2011; accepted for publication:30 March 2011?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 4The language models that are most commonly used today, in particular the modifiedKneser-Ney (KN) model (Chen and Goodman 1998), are based on the first two strate-gies, context length and interpolation?that is, they interpolate distributions of differenthistory lengths.
We will call such models history-length interpolated models.
The setof contexts that history-length-interpolated models base their prediction on is limitedto those whose history is identical for the history length considered.
For example, thelength-2 component of the model will compute the probability of P(w3|w1w2) based oncontexts in the training corpus with identical history w1w2.Class-based and similarity-based models consider a wider range of contexts.
Theirestimates rely on contexts in the training data that are similar to (or in the same classas) the new sequence whose probability is to be estimated.
Thus, for example, inattempting to estimate a probability for the bigram black cloud, unseen in training, thetransition probability associated with the class to which black belongs being followedby the class to which cloud belongs can be used.
The intuition is that although black cloudmight not have been seen in training, the class sequence containing related bigramslike gray cloud, or black mist, or gray mist, that is, combinations of other members of thetwo classes seen in training, can offer a reasonable estimate.
In principle, this type ofgeneralization is more powerful than history-length interpolation and has been, andcontinues to be, used to good effect in a variety of domains.
However, the model mustbe a good model of the distribution of sequences of strings; if its assumptions are toounrealistic or approximate, then class-based generalization will perform worse thanhistory-length interpolation.Although there has been much work on class-based and similarity-based languagemodels in recent years, no such model has been widely adopted as superior to history-length-interpolated models.
We believe one reason for this is that the granularity ofcontext that is optimal for generalization has not been investigated sufficiently.
Conse-quently, in this article, we present the following contributions: We demonstrate that class-based models can be made more effective.In particular, we put forward the half-context hypothesis as ageneral principle on the basis of which to construct class-basedlanguage models. We argue for the novel, beneficial use of a mixed n-gram class of bothbigrams and unigrams instead of a class of unigrams alone. We specify a discounting method which facilitates better treatment ofrare events. We deploy a new clustering algorithm for class-based language modelsthat is more efficient than the exchange algorithm. We perform a systematic investigation, including significance testing,of half-context versus whole-context class-based models whichdemonstrates the utility of a half-context approach. We carry out a novel fine-grained context-specific experimental validationof a half-context model that performs better than a traditional class-basedmodel, and, when interpolated, improves on a modified KN trigrammodel.
This new fine-grained analysis distinguishes those contexts bestsuited to history-length interpolation and those most appropriate forclass-based generalization.844Schu?tze and Walsh Half-Context Language ModelsThese contributions, particularly our analyses, offer a richer understanding of the rel-ative characteristics of history-length interpolation and class-based generalization andshould lead to more powerful language models that combine class-based and history-length generalization mechanisms.The remainder of the article is organized as follows.
Section 2 defines half-contextrepresentation and puts forward the half-context hypothesis.
Section 3 develops a half-context language model in the context of a specific subset of related prior work onlanguage modeling.
Additional related work is discussed in a subsequent subsection.Parameter estimation is described in Section 4.
A variety of models and interpolationsare evaluated, and fine-grained results, significance tests, and context-specific analysesare discussed in Section 5.
Conclusions and opportunities for future work are presentedin Section 6.2.
Half ContextsThe representation employed in this article builds on a specification used in our earlierwork (Schu?tze 1993, 1995; Schu?tze and Walsh 2008), motivated by Exemplar Theory(Hintzman 1986; Nosofsky 1986; Pierrehumbert 2001), where rich exemplar represen-tations facilitated the acquisition of local grammatical knowledge and outperformeda categorical representation in the same task.
Specifically, each word was representedin terms of its immediate left and right neighborhood context.
These neighborhoodswere treated separately for two reasons: (1) separate treatment of left-neighbor infor-mation and right-neighbor information resulted in reduced complexity in the modeland better generalization, and (2) right and left context behavior can differ consider-ably, for example, him and her would have very similar left contexts but could havesignificantly differing right contexts (e.g., compare the life in her garden vs. the life in himgarden).These representations of left and right context distributions of a given word wereknown as half-words but can in fact be viewed as a word-level instantiation of abroader representational formalism which we term half-contextualization.
Accord-ing to this schema a given unit (word, n-gram, class, etc.)
is represented in termsof half-context (HC) distributions over its immediate left and right neighborhoods.Hence, for example, at the bigram level, each bigram type is specified by two distri-butions, namely a left HC distribution Pl and right HC distribution Pr that capturethe bigram?s behavior to the immediate left/right.
For example, given walk home earlytwice, and drive home early once, then the left HC distribution of the bigram homeearly, denoted Plhome early, is Plhome early(walk) = 2/3 and Plhome early(drive) = 1/3, and 0 for allother words.
These HC distributions underpin the HC language models presented inSection 3.In order to determine the extent of the particular merits of considering wordsas possessing two separate directional behaviors, in the experiments that follow wecompare our HC language model against a whole-context (WC) model where agiven word?s WC distribution is a single distribution which combines the word?sleft and right HC distributions.
For a clear statement of the contrast HC vs. WC,we define inward and outward distributions.
For the estimation of P(wn+1|w1,n)based on a training set S, the inward distributions IWwn+1|w1,n consist of the setof right contexts of w1,n and left contexts of wn+1 in S; the outward distributionsOWwn+1|w1,n consist of the set of left contexts of w1,n and right contexts of wn+1in S.845Computational Linguistics Volume 37, Number 4We can then state our underlying hypothesis that HC-based classes are better forlanguage modeling than WC-based classes as follows.Half-Context Hypothesis.
A distributional language model should base its estimate ofP(wn+1|w1,n) on contexts v1,nvn+1 whose inward distributions IWvn+1|v1,n are similar toIWwn+1|w1,n .
Similarity of the outward distributions OWwn+1|w1,n and OWvn+1|v1,n shouldnot be employed as a criterion for using or not using training set contexts v1,nvn+1 forthe estimation of P(wn+1|w1,n).An example for the intuition behind the HC hypothesis is the him/her example givenearlier.
When estimating P(him|Mary helped), a context like Mary helped her should alsobe considered as evidence because even though the right contexts of him and her inthe corpus are dissimilar, their left contexts are similar.
The HC hypothesis states thatwe should only worry about similarity of the ?relevant side?
of the n-grams involved,that is we should only consider inward distributional information.
Most clustering algo-rithms used for class-based language models, notably the exchange algorithm (Brownet al 1992; Kneser and Ney 1993; Martin, Liermann, and Ney 1998), are ?whole-context?clustering algorithms that violate the hypothesis.The HC hypothesis provides an alternative basis for designing class-based languagemodels.
In general, in designing a language model only information sources that arerelevant for the task to be solved should be included.
Adding additional complexity ornonrelevant additional features increases the variance of predictions without improvingtheir accuracy.
We can view this as a type of bias?variance tradeoff.
Half-context modelsare simpler and have less variance because they only use one half of the availablecontext information, the half that is actually useful for prediction.
The experimentalresults that we report later in this article confirm this by demonstrating that half-contextmodels perform significantly better than whole-context models.A consequence of only using inward distributions in accordance with the half-context hypothesis is that we need two different types of classes: one set of classes forthe predictors and another set of classes for the predictees.
The reason is that we use twodistinct and unrelated representations, left-context distributions to induce classes of pre-dictees and right-context distributions to induce classes of predictors.
In other words,half-context models are inherently asymmetric, reflecting the fact that language modelsare inherently asymmetric: The role of the predictor and the predicted are different.This asymmetry shows up in word-based models to a limited extent: In most modelsthe unit of prediction is a word; predictors include n-grams of any size in principle,not just words.
However, in a class-based model the asymmetry between predictorand predicted is more important: There is no justification for the premise (made, forexample, in the Brown model) that the classes that are optimal for predictors are alsothe classes that are optimal for predictees.
This observation has also been made by Gaoet al (2002), albeit without explicit reference to half contexts.
We view our approachas better motivated since the asymmetry of our model is not posited, but follows froman analysis of the information sources needed for probabilistic inference in languagemodeling.Linguistic theory also provides evidence for half-context models.
In many theories,there is a single formal concept that can be instantiated either by arguments ofprepositions or by arguments of transitive verbs.
For example, there are few if anysyntactic differences between the arguments that can appear after a prepositionlike by and after a transitive verb like brought.
Thus, the predicting histories by andbrought should be treated alike in a class-based model.
But that is not possible in a846Schu?tze and Walsh Half-Context Language Modelswhole-context model.
We can interpret this as a syntactic justification for half-contextmodels.
Referring back to our earlier allusion to the bias?variance tradeoff, if we hadunlimited data, then estimating separate distributions for by and brought would be un-problematic; but because training sets are not unlimited, we can improve generalizationby assigning the two linguistically identical contexts to the same right-context class.Finally, efficiency is also a strong argument for half-context models.
Time of clus-tering and storage requirements are cut in half by omitting those parts of the contextthat are nonrelevant.
The time complexity of many clustering algorithms depends onthe number of different types of features that occur in a particular cluster as opposedto the number of tokens.
The number of feature types occurring in a cluster is reducedsubstantially in half-context models.It is of course possible to find cases where the outward distributions are helpful foraccurate estimation.
Consider estimating P(is|strilp), where strilp is a word that occurredonce in the training set.
Suppose for the sake of argument that is after nouns is morelikely than is after adjectives (because phrases like yellow is the new black are infrequent).If strilp occurred in the context a very strilp car, then it is likely to be an adjective andP(is|strilp) is low.
If strilp occurred in the context the strilp car, then it could also be anoun (as in the bakery car or the wedding ring) and P(is|strilp) should be estimated to behigher.
In this case, it is the outward distribution of strilp that helps us to arrive at anaccurate estimate.
However, our hypothesis is not that there are no such cases; rather,we believe that as a generalization mechanism, only inward distributional informationis useful in improving performance.
This is borne out by the experiments reportedherein.In the future, there may be non-distributional models that use more complex infer-ences for language modeling.
Parsing-based language models (e.g., Hall and Johnson2003) are a first step in this direction.
The hypothesis would probably not apply to suchnon-distributional models.3.
Half-Context Language Model3.1 Half-ContextualizationOur starting point is the model by Brown et al (1992).
It models the probability of classc2 following class c1 where c2 emits (e in the diagram) the member word w2 and w1belongs to (?, a deterministic process) class c1:w1c1 c2w2?seqeThis model has been frequently investigated and discussed.
Recent examples includeits successful application in word co-occurrence and sentence retrieval investigations(Momtazi and Klakow 2009; Momtazi, Khudanpur, and Klakow 2010), and polarityclassification of movie reviews (Wiegand and Klakow 2008).847Computational Linguistics Volume 37, Number 4The key concept introduced in this article is that of a half-context class.
Class-basedlanguage models can be half-contextualized by replacing classes that model right and leftcontexts simultaneously by right half-context classes cr1 and left half-context classes cl2.Words are assigned to HC classes and these HC classes then generate words:w1cr1 cl2w2?seqe3.2 Mixed n-gram ClassesA second modification of the Brown model we propose is motivated by the fact thattrigram models perform better than bigram models because a sequence of two wordssignificantly limits the possible ways of continuing.
For this reason, we condition thesequential continuation on a mixed n-gram class of both bigrams and unigrams insteadof on a class of unigrams alone.
The resulting model, the HC model, is depicted inFigure 1.
We show the bigram w1w2 as the member of the class cr12, but cr12 can also bethe class of w2 if w1w2 was not frequent enough to be included in the clustering (criteriafor inclusion are discussed in Section 4).To summarize, the generative process shown in Figure 1 is that the right-contextclass cr12 to which the bigram w1w2 belongs generates a unigram left-context class cl3which generates w3.
As will become apparent from the description of parameter esti-mation and the clustering algorithm in Section 4, the HC classes in the model are basedFigure 1The half-context language model.848Schu?tze and Walsh Half-Context Language Modelson inward (IW) distributions only.
The corresponding outward (OW) distributions arenot taken into account in accordance with the HC hypothesis.In addition to the novel use of half-contexts it is important to note that the right HCclasses employed are mixed classes of unigrams and bigrams rather than of unigramsonly.
To our knowledge this represents the first such usage and can be motivated by thefact that frequent bigrams in language often behave similarly whereas the constituentunigrams do not.
For example, the right HCs of the bigrams University of and based inare similar because both are often followed by locations; but the right HCs of of and inare much more diffuse and there are many prepositional objects that occur more oftenwith of than with in (e.g., names of people) and others that occur more often with inthan with of (e.g., response).3.3 DiscountingIn initial experiments, we found that it was difficult to achieve an improvement usingclass-based generalization because for many contexts history-length interpolation is thebetter strategy for estimation.
For a high-frequency event, it can be best to base estimateson instances of this event with identical history only?instead of smoothing them withother contexts that are in the same class.Consider the unigram Hong.
In 3,998 out of 4,045 cases in the training set part ofour corpus of Wall Street Journal (WSJ) articles (consisting of 40 million words), it isfollowed by Kong.
In this case, redistributing probability mass to other members of theclass that Kong is a member of will decrease the estimate for P(Kong|Hong) (an estimatethat should be close to 3,998/4,045) and decrease the model?s performance.
On the otherhand, we have H(P(w|Mr.))
?
11.9 in our WSJ training set.
Any of a large number offirst and last names can occur after Mr. and a language model should reallocate someprobability mass from names that did occur in this environment in the training set tothose names that did not.To treat these two different cases correctly, we use a variant of absolute discounting(Ney, Essen, and Kneser 1994).
Following the notation of Chen and Goodman (1998),we first define the number N1+(w1,n?)
of distinct words that can occur after an n-gramin the training set:N1+(w1,n?)
= |{w|C(w1,nw) > 0}|where C(w1,n) is the frequency of w1,n in the training set.We then define the exemplar-theoretic (ET) language model as follows:PET(wn+1|w1,n) = DN1+(w1,n?
)C(w1,n)PHC(wn+1|w1,n)+max(0, C(w1,nwn+1) ?
D)C(w1,n)(1)The discount D is a parameter of the model that controls how much of each count isredistributed to the class-based model.
In a way that is similar to other discountingmethods, this formalization satisfies the two desiderata stated earlier: The estimatesof high-frequency events are, in relative terms, much less affected than those of low-frequency events.849Computational Linguistics Volume 37, Number 4PET is the exemplar-theoretic model we will evaluate in the experiments describedbelow.
We use an analogous model for WC distributions.
In that case, PHC is simplyreplaced by PWC in Equation (1).To summarize, the innovations of our exemplar-theoretic model are (1) the use ofHC classes instead of WC classes, (2) the use of mixed classes of unigrams and bigrams(instead of classes of unigrams), and (3) the use of absolute discounting to concentratethe effect of class-based generalization on rare hard-to-estimate events while leavingrobust estimates based on frequent events largely unchanged.3.4 Additional Related WorkIn addition to the motivating articles discussed earlier, other relevant work includes therandomization techniques applied by Emami and Jelinek (2005) to class-based n-gramlanguage models.
Half-context clusters are not at odds with a randomized approach asthey could easily be implemented in such a fashion.Other related research includes the ?mixed?
model employed by Uszkoreit andBrants (2008), in which a word bigram (as opposed to a class of bigrams) proba-bilistically generates a class.
They use, in our terminology, whole-context classes.
Theexperiments reported subsequently suggest that HC classes are preferable to WC classesin the Brown-type set-up (classes generating classes); we plan to investigate whetherthis is also true in a mixed model in future work.Bassiou and Kotropoulos (2011) investigate two word-clustering techniques thatoperate on long-distance bigram probabilities (of varying distances) within a contextand on interpolated long-distance bigram probabilities, both with a view to captur-ing long-distance dependencies.
Evaluation of both clustering techniques?hierarchicalclustering exploiting Mahalanobis distances to form compact clusters and ProbabilisticLatent Semantic Analysis?demonstrates that the use of long distance bigrams or theirinterpolated varieties yield more compact and meaningful (in the case of interpolatedlong distance bigrams) word clusters than the use of the traditional bigram (and bi-grams which employ trigger pairs over various histories).
This research demonstratesan interesting avenue for contemporary models of word clustering and it would be nodoubt interesting to see how such clustering strategies might contribute to half-contextclustering, how their clusters would compare to those produced via bisecting k-means(though we cluster bigrams also), as proffered here, and indeed how long distancebigrams could be half-contextualized; these questions, however, are beyond the scopeof the current article which seeks primarily to investigate the potential merits of half-contextualization.Related work by Justo and Torres (2009) explores the use of language models thatemploy classes containing phrases.
They describe their models as two-level becausespecific language models act within the classes.
Their first approach takes into accountthe probabilities between words which constitute the different phrases of a given class,that is, phrases are sequences of unconnected words and words are considered thebasic lexical unit, and their second approach considers phrases to be indivisible lexicalunits.
The first model is also interpolated with a standard word-based language model,and the second model is interpolated with a standard phrase-based model.
Word-error-rate analyses in an ASR system indicate that these models are better than theirtraditional counterparts.
These results provide useful motivation for extending class-based language models from classes of isolated words to classes of longer sequences,such as the classes of bigrams employed in the half-context model.
Their research850Schu?tze and Walsh Half-Context Language Modelsdoes not, however, consider the different directional behaviors of words or bigramsas we do.Zitouni and Zhou (2007, 2008) propose linearly interpolated hierarchical languagemodels (and a back-off variety [Zitouni 2007]) where each vocabulary item constitutesa leaf node in a word-tree, words are clustered into classes, and, in a recursive process,classes are clustered into more general classes until the root is reached.
The tree rootis a class containing all vocabulary items.
In attempting to estimate the likelihood ofan n-gram event they linearly interpolate over different language models, each oneof which is trained on one level of the tree.
In this way they seek to strike a balancebetween specificity and generalization.
In constructing the class hierarchy, words arerepresented by their probability given their left and right neighboring words over avocabulary (equivalent to the whole-context representation discussed in this article) andsimilarity between words is established using the Kullback-Leibler distortion measure.Words occurring frequently in similar contexts should be clustered together with a viewto finding a set of clusters that minimizes global discriminative information (see alsoBai et al 1998).
The clustering algorithm is based on k-means.
The use of a hierarchicaltree, and interpolating over it, represents an interesting approach not at odds with ourresearch (i.e., that is, half-context classes could form nodes in the tree), although ourapproach differs in the separate treatment of word contexts, the use of bigrams as classmembers, and in the clustering methodology.Additional related work includes research by Bahrani et al (2008), who buildclass-based models using the k-means algorithm and words represented in terms ofvectors where each vector element corresponds to the number of times the wordhad a particular part of speech tag given a tagged corpus.
This approach wouldtypically yield much shorter feature vectors than approaches (including our own)which have vectors matching the vocabulary size, thus leading to lower time com-plexity.
They do not, however, avail of classes of bigrams as we do, nor look at di-rectional behavior of words (though half-contextualization using part of speech tagswould be an interesting extension of both our model and theirs).
Abdoos and Naeini(2008) use a clustering ensemble approach to categorize words, although it is unclearfrom their evaluation how such an approach compares, in terms of performance, toothers in the literature.
Gao et al (2002) propose an asymmetric clustering model(ACM) grounded upon the apt observation that different clusters for predicted andconditional words should be employed, a view shared here.
Their research does notpresent an explicit treatment of half-contextualization, however, nor considers half-contextualization and the significance of inward distributional information as insightswhich meet language modeling needs.
Furthermore, our evaluation also differs inthat it involves comparison against a whole-context model and a modified KN tri-gram model, rather than a simple word trigram model.
Our use of a mixed n-gramclass of both bigrams and unigrams also represents a marked difference betweenapproaches.With regard to context direction Essen and Steinbiss (1992) also look at left and rightcontexts similarly to our approach.
However, they do not compare half-context withwhole-context approaches and they pursue a less efficient similarity-based approach incontrast to the class-based approach proposed here.Finally, Dagan, Lee, and Pereira?s (1999) similarity-based language model uses asimilar word to the observed word as the conditioning context used to generate the nextword in the sequence.
Again, no comparison to whole-context approaches is made.
Asimilarity-based approach is also difficult to use for large corpora as it would necessitatethe calculation of similarity of every word to every other word in the corpus.
Similarities851Computational Linguistics Volume 37, Number 4can be computed more efficiently for a subset of words on a smaller corpus, but thenmany of the rare events that class and similarity based methods are most beneficialfor will not be covered.
Our analyses in Section 5.2 and Section 5.3 demonstrate thathalf-context modeling is most beneficial for rare events.
Similar concerns apply to othersimilarity-based models, such as those proposed by Bengio et al (2003) and Schwenkand Koehn (2008).4.
Parameter EstimationIn this section we describe how the parameters of the model in Figure 1 are estimated.These parameters belong to two broad categories, namely, those which model the HCdistributions (Pr and Pl) and are used in the construction of clusters, and those whichcapture emission probabilities Pe and sequence probabilities Pseq that are used when themodel is applied.
Estimates were calculated on the basis of the training set part of acorpus of WSJ articles, 1987?1989, consisting of almost 50 million words, which will bedescribed in more detail subsequently.4.1 Clustering of HC DistributionsIn the clustering, n-grams are represented as HC distributions.
These distributions areestimated using maximum likelihood as follows:Prw1w2 (w3) =C(w1w2w3)?w C(w1w2w)Prw2 (w3) =C(w2w3)?w C(w2w)PrUNK(w3) =C(w3)?w C(w)Plw3 (w2) =C(w2w3)?w C(ww3)PlUNK(w2) =C(w2)?w C(w)Only a subset of items is clustered.
When clustering unigrams we include all 54,243unigrams that occur more than 10 times in the corpus as well as the unknown wordUNK.
For mixed clusterings of unigrams and bigrams we include all 378,109 unigramsand bigrams that occur more than 10 times and the unknown word UNK (thus, theunigram set is a subset of the mixed set).
We call these sets Suni and Smixed and they areused for all HC and WC models herein, including the Brown model.We employ bisecting k-means (Steinbach, Karypis, and Kumar 2000) to cluster HCdistributions.
The distance measure employed is Euclidean distance because the formalproperties of k-means, including convergence, only apply to Euclidean spaces.
Bisectingk-means is applied to a small random sample of the set of items: k-means first splits thisrandom sample in two, then the largest existing cluster is split and so on until k = 512(or k = 1,024, depending on the experiment) clusters have been found.
The size of therandom sample is then doubled, items in the enlarged sample are assigned to cluster852Schu?tze and Walsh Half-Context Language Modelscentroids, and centroids are recomputed.
The size of the sample is doubled again andso on until all items have been assigned.Incremental doubling of the sample has the advantage that several iterations ofreassignment and recomputation of centroids are performed (thus producing centroidsthat are good representatives of the overall distribution of items); and that at the sametime the total number of assignments that needs to be computed is bounded by 2Mwhere M is the number of items.
Computing the assignments is responsible for almostall the computation time of k-means and more than 90% of the time needed to estimatethe parameters of the exemplar-theoretic model.We do not investigate the effect of the number of clusters k on the performance ofclass models in this article.
As the default we chose k = 1,024, similar to Brown et al?sexperiments.
Note that we have 512 left HC clusters and 512 right HC clusters, a totalof 1,024 in the experiments with k = 512.
We also experiment with 2 ?
1,024 clustersbecause one could also argue that this is the setting that is most comparable to Brownet al We choose the powers of 2, k = 512 and k = 1,024 (instead of 500 and 1,000), foroptimal compression and compact storage.Two examples of half-context clusters (one left HC cluster and one right HC cluster)and their sizes are given in Table 1.
The three most frequent words in the left HC clusterhave similar left contexts (dominated by forms of to be) and different right contexts(large variety of part of speech forms).
The three most frequent bigrams in the right HCcluster have similar right contexts (dominated by gerunds) and dissimilar left contexts(again a large variety of possibilities).
In traditional whole-context clusters one wouldneed to split the two clusters at least in two.
For example, the right HC cluster in Table 1would have to be split into one subcluster containing without-first/of-improperly andone subcluster containing pain-and.
However, this presents two distinct disadvantages:(1) The extra clusters would require the estimation of more parameters, each basedon fewer data points and hence less reliable, and (2) The left-context generalization,whereby of-improperly and pain-and have similar right contexts, would be lost.Once clusters and cluster memberships have been computed, we need to determinethe relevant right HC cluster cr12 and left HC cluster cl3 when computing the probabilityP(w3|w1w2) according to the model in Figure 1.
We do this as follows:1.
If w1w2 ?
Smixed, we use the right HC cluster that Prw1w2 was assigned to.2.
Otherwise, if w2 ?
Smixed, we use the right HC cluster that Prw2 wasassigned to.3.
Otherwise, we use the right HC cluster that PrUNK was assigned to.4.
If w3 ?
Suni, we use the left HC cluster that Plw3 was assigned to.5.
Otherwise, we use the left HC cluster that PlUNK was assigned to.Table 1Examples of half-context clusters.most frequent n-grams in cluster sizeleft HC cluster unlikely, unclear, happening 753right HC cluster pain-and, without-first, of-improperly 248853Computational Linguistics Volume 37, Number 44.2 Emission and Sequence ProbabilitiesEmission probabilities need only be estimated for left HC clusters in the exemplar-theoretic model.
They are estimated by maximum likelihood:Pe(w|c) =C(w)?w?
?c C(w?
)Cluster sequence probabilities are additively smoothed:Pseq(cl|cr) = C(crcl) + ?C(cr) + B?where ?
= 0.1, B ?
{512, 1,024} is the number of HC clusters, C(crcl) is the number oftrigrams w1w2w3 occurring in the training set, where w1w2 was assigned to cr and w3 tocl, and C(cr) is the number of bigrams w?1w?2 occurring in the training set, where w?1w?2was assigned to cr.WC clusters are generated by representing an n-gram as the concatenation oftwo HC distributions, its left HC distribution and its right HC distribution.
Clus-tering, membership assignment, and probability estimation are the same in all otherrespects.5.
Experiments and AnalysisA corpus of WSJ articles, 1987?1989, consisting of almost 50 million words, was ran-domly split into training set (80%), validation set (10%), and test set (10%).Unigrams, bigrams, and trigrams and their counts were extracted from training,validation, and test sets.
A modified KN model (Chen and Goodman 1998), termedP(KN), was estimated on the training set count files and applied to the test set usingsrilm, the SRI language modeling toolkit (Stolcke 2002).
The same count files were theinput to the HC and exemplar-theoretic model estimation and application procedure.Vocabulary size was the same for both KN and exemplar-theoretic models: 256,874 (the256,873 words occurring in the training set and the unknown word).
A total of 70.8%of tokens w3 in the test set occur in a context w1w2w3 occurring in the training set; for22.2% of tokens only w2w3 occurs in the training set; and for 6.7% only w3 occurs inthe training set.
The out-of-vocabulary rate is 0.27%.
All validation and test set wordsthat do not occur in the training set are mapped to the special unknown token UNK.In all interpolation experiments, the weight of the P(KN) model is 1 ?
?
and the weightof the model with which P(KN) is interpolated is ?.
The validation set was employed todetermine the optimum interpolation weight ?
and discount D for each case.Total processing time for estimating the HC clusters for Suni and Smixed (lines 13 and15 in Table 4, subsequently) was less than 3.5 hours on an Opteron 8214 processor.In evaluating our model it seems appropriate to compare its performance againstother class-based models.
Consequently, the SRI toolkit was also used to constructa class bigram language model, following the incremental version of the algorithmproposed by Brown et al (1992), which we simply term the P(Brown) model.
A total of854Schu?tze and Walsh Half-Context Language ModelsTable 2Perplexity results for interpolation of P(Brown) with a bigram model P(KN).
?
= 0 corresponds toP(KN) alone, ?
= 1 corresponds to P(Brown) alone.perplexity?
validation test0.000 164.52 164.800.025 164.080.050 164.03 164.330.075 164.150.100 164.400.200 166.251.000 245.14 245.45Table 3Models used in our experiments.P(KN) modified Kneser-Ney modelP(ET-Brown) exemplar-theoretic Brown modelP(Half) exemplar-theoretic Half-Context model (Equation 1)P(Whole) whole-context analogue of Equation 1P(KN-Brown) interpolation of P(KN) with P(ET-Brown)P(KN-Half) interpolation of P(KN) with P(Half)P(KN-Whole) interpolation of P(KN) with P(Whole)1,024 classes (the same number of classes as the combined left and right context clustersin the 2 ?
512 HC model) were derived from the training data.1Table 2 presents results for the interpolation of P(Brown) with a bigram model P(KN)when applied to the validation set over a number of interpolation weights, followed byresults from the test data using the optimum weight for the P(Brown) model (?
= 0.05)found during the validation phase.It is clear from Table 2 that although interpolating a traditional class-based modelwith a KN bigram model does offer some benefit, this benefit is slight (perplexity =164.80 for P(KN) alone, versus 164.33 using the optimum interpolation weight on the testset).
It is also clear that the traditional class-based model operating by itself (?
= 1.0,perplexity = 245.45) performs poorly relative to P(KN).Of course the SRI class-based model employs whole-context classes, not half-context distributions which consider behavior to the left and right separately.The following models, detailed in Table 3, were used in our experiments: modifiedKneser-Ney (P(KN)); exemplar-theoretic half-context (P(Half)); exemplar-theoretic whole-context (P(Whole)); exemplar-theoretic Brown (P(ET-Brown)); and P(KN-Half), P(KN-Whole), andP(KN-Brown), the interpolations of Kneser-Ney with exemplar-theoretic half-context,whole-context, and Brown, respectively.2 Perplexity results, for each of these models,from the validation and test sets, are presented in Table 4.
Order-2 in Table 4 implies1 Here we approximate Brown et al (1992) who used 1,000 classes.
As our classes are also being used toinvestigate language model compression in other work, we prefer to use powers of 2.2 Test set perplexities for Kneser-Ney in Table 2 (164.80) and Table 4 (165.13, line 1) differ slightly becauseof different handling of beginning and end of sentence symbols in the two experiments.855Computational Linguistics Volume 37, Number 4Table 4Perplexity results for Kneser-Ney, exemplar-theoretic Brown, exemplar-theoretic half-context,exemplar-theoretic whole-context, and interpolations.perplexityD ?
validation test order/model number of classes1 164.83 165.13 2 P(KN)2 .8 1.0 191.71 192.24 2 P(ET-Brown) 5123 1.0 1.0 171.17 171.58 2 P(Half) 5124 1.0 1.0 170.81 171.21 2 P(Whole) 5125 .8 1.0 171.16 171.57 2 P(Half) 1,0246 .8 1.0 170.75 171.19 2 P(Whole) 1,0247 .4 .2 163.97 164.28 2 P(KN-Brown) 5128 .9 .5 161.51 161.82 2 P(KN-Half) 5129 .7 .4 161.83 162.13 2 P(KN-Whole) 51210 .6 .5 161.37 161.67 2 P(KN-Half) 1,02411 .6 .5 161.53 161.83 2 P(KN-Whole) 1,02412 94.67 94.94 3 P(KN)13 .8 1.0 105.31 105.65 3 P(Half) 51214 .8 1.0 107.99 108.33 3 P(Whole) 51215 .5 .4 88.91 89.15 3 P(KN-Half) 51216 .5 .4 89.39 89.63 3 P(KN-Whole) 512that only unigrams are clustered in the exemplar-theoretic models and the Kneser-Neymodel is a bigram model.
As for order-3, this implies that both unigrams and bigramsare clustered together in the exemplar-theoretic models and that the Kneser-Ney modelis a trigram model.For lines 7?11 and 15?16, the parameters ?
and D that were optimal on the valida-tion set are given.
For lines 2?6 and 13?14, the optimal value of D on the validation setfor ?
= 1 (that is, 0 weight for the P(KN) model) was chosen.The ?
parameter on lines 8?11 and 15?16 indicates that half- and whole-contextmodels are as valuable, or nearly so, as the KN models: The interpolation weight ofhalf/whole-context models is either 0.4 or 0.5.
In contrast, the Brown class model (line 7)receives a lower weight of 0.2, indicating that it is less valuable in the interpolationwith KN.The discount parameter D determines the influence of class-based generalizationin the overall model.
Again, the Brown model receives the smallest weight (line 7).
Forboth D and ?, the lowest half/whole-context model values are those for the KN order-3interpolations on lines 15?16: D = .5,?
= .4 (value of ?
tied with KN order-2 interpo-lation on line 9).
This may be a reflection of the fact that class-based generalization iscontributing more to better performance in order-2 models because order-2 models havea much lower baseline performance.For order-2 the differences between HC and WC models are small (lines 3 vs. 4, 5vs.
6, 8 vs. 9, 10 vs. 11).
For order-3, exemplar-theoretic half-context is clearly better thanexemplar-theoretic whole-context (lines 13 vs. 14), although that difference is reduced inthe two interpolated models P(KN-Half) and P(KN-Whole) (lines 15 vs. 16).
On this evidenceit would appear that the combination of left and right context information into a singlecontext distribution (i.e., a whole-context approach) is redundant, if not harmful.
This856Schu?tze and Walsh Half-Context Language Modelsis evidence for the half-context hypothesis put forward at the beginning of the article:Outward distributions, present in WC representations but absent in HC representations,do not seem to be helpful in class-based generalization, and are perhaps even harmfulin order-3.One reason why HC models perform better for order-3 than WC models could bethat unigrams and bigrams are clustered together for the order-3 models.
Althoughit makes sense to treat, say, the right contexts of from Mark and Martin as similar, thedistributional patterns of the two n-grams are very different if the left context is alsotaken into account, which is the case for WC models.We could attempt to extend the exchange algorithm that has most often been usedfor class-based language modeling to half-context clustering.
This is beyond the scopeof this article, however.
Instead, we compare the order-2 WC experiments directly withthe Brown classes.
We do this to make sure that our good results for HC models are notdue to the fact that we use a weak WC baseline.
As we will argue now, our WC baselineis at least as good or even better than Brown clustering.There are two set-ups that can be argued to be directly comparable to the Brownexperiments reported here: either 512 left HC classes and 512 right HC classes (lines2?4, 7?9, and 13?16); or 1,024 left HC classes and 1,024 right HC classes (lines 5?6 and10?11).
In the Brown experiments (lines 2 and 7), Equation (1) is used in the same wayas in the HC/WC experiments except that class membership is based on the classesinduced by srilm (corresponding to the experiments in Table 2).
The comparisons onlines 2 vs. 4 and 6 and 7 vs. 9 and 11 clearly show that the quality of bisecting k-meanswhole-context clustering is comparable to, if not better than, Brown-type whole-contextclustering.
That is, keeping the representation constant in both cases (i.e., whole-context)enables us to see the algorithmic benefits of bisecting k-means as it appears to offer moreuseful clusters than those produced by the exchange algorithm.Finally, although the exemplar-theoretic models are clearly outperformed by theP(KN) model (lines 1 vs. 3?6, 12 vs. 13?14), it is important to note that the combination ofthe P(KN) model and the exemplar-theoretic models outperforms the stand-alone P(KN)model (lines 1 vs. 8?11, 12 vs. 15 and 16).
This is strong evidence that a combined class-based and history-length-interpolated model is superior to history-length interpolationby itself.5.1 Establishing SignificanceAlthough the perplexity results documented here provide tangible support in favorof the half-context hypothesis, it would nevertheless be desirable to establish if theperplexity scores are indicative of improvements that are statistically significant.
To thisend, the following significance test was performed.
The test set has a length of 2,800,613words.
These 2,800,613 positions are divided into 47 bins, corresponding to the part-of-speech of the word at that position that is most frequent in the training set.3 This wasbased on a tagging of the training set with TreeTagger (Schmid 1994).
One additional bin3 We initially performed this test by assigning word types randomly to bins.
We found that this ?random?version of the test was unrealistically sensitive (all differences were highly significant) becausedifferences in perplexity were highly correlated across bins.
For example, if a model has a beneficial effecton names only and names are randomly distributed across bins, then perplexity will be better for everybin, which we would then interpret as significance.
To reduce correlation across bins, we then definedbins on the basis of part of speech.
As a result all names (or rather words whose dominant part of speechis a proper noun) will be in one bin that is not correlated with all other bins.857Computational Linguistics Volume 37, Number 4contains all positions with a number of rare tags (e.g., FW, ?foreign word?)
and unknownwords.
Two models are then compared by computing perplexity separately for each bin,counting the number of bins where the first model performs better than the second, andtesting the significance of this count using the exact binomial test.
This significance testis not very sensitive in some cases because the positive effect of class generalization canbe concentrated on a few parts of speech.
To the extent that half-context and whole-context classes approximate part-of-speech information, this makes it more difficult toshow significance because a number of bins may not be affected by the model.
However,as we will see subsequently the test is sufficiently sensitive for the key results of thearticle.A number of noteworthy points can be made on the basis of the results of thesesignificance tests.
As all models of order-3 significantly (and unsurprisingly) outper-form those of order-2, both order types are considered separately in the discussion thatfollows.
All significant results are with respect to p = 0.05.With regard to the uninterpolated order-2 models (Table 4 models 1?6), the exper-iments indicate no significant improvement between models, with the exception thatall models (P(KN), P(Half), and P(Whole) 512 classes, and P(Half) and P(Whole) 1,024 classes)are significantly better than P(ET-Brown).
Although such a result sheds no light on theveracity of the half-context hypothesis, it nevertheless demonstrates that our exemplar-theoretic models are competitive at order-2 and are superior to P(ET-Brown).
Concern-ing the interpolated order-2 models (Table 4 models 7?11), a similar story presentsitself, that is, there is no significant difference between the interpolated models withthe exception that all interpolated models (P(KN-Half) and P(KN-Whole) 512 classes, andP(KN-Half) and P(KN-Whole) 1,024 classes) improve significantly on P(KN-Brown).
It shouldbe noted, however, that the better performance of the five order-2 class-based models(lines 7?11), including P(KN-Brown), compared to P(KN), is statistically significant; this isin keeping with previous findings in the literature and demonstrates that class-basedgeneralization can complement history-length modeling.As for the order-3 models (Table 4 models 12?16), here the significance resultsdemonstrate that half-contextualization yields statistically significant improvementsover whole-context models.
Specifically, P(Half) significantly outperforms P(Whole), andP(KN-Half) significantly outperforms P(KN-Whole).
In addition, although P(KN) demonstratessuperior performance to P(Whole) and P(Half), interpolation with either of our exemplar-theoretic models yields significantly better performance over P(KN) alone.
That is, bothP(KN-Whole) and P(KN-Half) significantly improve on P(KN).Overall, these significance results indicate the potential merits of our models.
Allfour exemplar-theoretic models outperform the Brown varieties and the models offersignificant improvements versus P(KN) when interpolated at orders 2 and 3.
Indeed,P(KN-Half) significantly beats every other model.
Crucially, however, in our view, are theresults of order-3 which demonstrate the significant benefits of half-contextualization asthese lend considerable corroborative weight to our half-context hypothesis.5.2 Context-Specific AnalysisIn order to better understand the relative strengths and weaknesses of the P(KN-Half),P(KN-Whole), and P(KN) models, Table 5 illustrates their performance in fine-grainedcontext-specific detail.
P(KN-Half), P(KN-Whole), and P(KN) in Table 5 correspond to lines 15(order-3 P(KN-Half)), 16 (order-3 P(KN-Whole)), and 12 (order-3 P(KN)) in Table 4, respectively.The table is a stratification into 17 strata of the positions w3, occurring in contextw1w2w3, in the validation set according to length |h| of history used by the half-context858Schu?tze and Walsh Half-Context Language ModelsTable 5Context-specific analysis of model performance.
|h| is the length of the history used by P(KN-Half)and P(KN-Whole) for prediction.
f3 is the training set frequency of w3.
f1,3 is the training setfrequency of w1w2w3.
The number of tokens and types of w1w2w3 for the validation set are alsoprovided.
KN-Half corresponds to line 15 in Table 4 (the interpolation of Kneser-Ney andexemplar-theoretic half-context); KN-Whole to line 16 (the interpolation of Kneser-Ney andexemplar-theoretic whole-context); and KN corresponds to line 12 (the order-3 P(KN) model); .L(KN-Half) is the log likelihood of P(KN-Half) on the validation set.
?
(KN-Whole) and ?
(KN) are thedifferences in log likelihood of these models from L(KN-Half) on the validation set.
The l valuegives average per-position log likelihood on the validation set and ?
values per positiondifferences.
The three largest absolute differences in each delta column are in bold.tokenstypesL(KN-Half)?(KN-Whole)?(KN)l(KN-Half)?(KN-Whole)?
(KN)|h| f3 f1,31 0 ?0 ?0 62,810 61,453 ?151,604 ?4 1,545 ?2.41 ?0.00 0.022 1 0?9 0 10,529 10,482 ?54,333 ?17 ?1,770 ?5.16 ?0.00 ?0.173 1 1?9 ?1 1,249 1,132 ?2,837 0 ?114 ?2.27 0.00 ?0.094 1 ?10 0 673,816 666,307 ?1,626,791 ?114 24,439 ?2.41 ?0.00 0.045 1 ?10 1 115,562 109,492 ?170,263 139 10,378 ?1.47 0.00 0.096 1 ?10 2 48,165 43,296 ?34,540 31 ?2,385 ?0.72 0.00 ?0.057 1 ?10 3?4 42,903 35,839 ?21,703 18 ?3,474 ?0.51 0.00 ?0.088 1 ?10 5?9 33,199 23,429 ?9,045 8 ?3,123 ?0.27 0.00 ?0.099 1 ?10 ?10 1,967 1,153 ?282 0 ?158 ?0.14 0.00 ?0.0810 2 0?9 0 33,410 33,076 ?191,114 816 ?5,866 ?5.72 0.02 ?0.1811 2 1?9 ?1 9,278 8,310 ?42,757 1 4,479 ?4.61 0.00 0.4812 2 ?10 0 718,269 702,377 ?2,891,127 5266 ?72,624 ?4.03 0.01 ?0.1013 2 ?10 1 259,856 241,428 ?724,464 1324 109,446 ?2.79 0.01 0.4214 2 ?10 2 161,628 141,797 ?383,855 717 22,986 ?2.37 0.00 0.1415 2 ?10 3?4 214,073 173,784 ?447,205 693 19,603 ?2.09 0.00 0.0916 2 ?10 5?9 308,716 211,119 ?553,419 745 13,821 ?1.79 0.00 0.0417 2 ?10 ?10 2,407,403 337,259 ?2,639,923 2179 21,844 ?1.10 0.00 0.01and whole-context models (0, 1, or 2), frequency f3 of w3 in the training set, and fre-quency f1,3 of w1w2w3 in the training set.
Each line gives statistics for one stratum.
Weexplain the statistics for the example of stratum 13.
This stratum contains all positionsw3 in the validation set that satisfy the following three conditions: w1w2 is a bigram thathalf- and whole-context models use for class-based prediction (|h| = 2); w3?s frequencyin the training set is at least 10; and the trigram w1w2w3 occurred exactly once in thetraining set.
There are 259,856 validation set positions in this stratum, correspondingto 241,428 different trigram types w1w2w3.
The log likelihood of this subset of thevalidation set for P(KN-Half) is ?724,464.
This log likelihood of ?724,464 is better by 1,324than that of P(KN-Whole) and better by 109,446 that that of P(KN).
The per-position (average)log likelihood of P(KN-Half) is?2.79.
This per-position log likelihood is better by 0.01 thanthat of P(KN-Whole) and better by 0.42 that that of P(KN).The 17 strata were chosen so as to get good resolution on the contexts that distin-guish the models.
These are the contexts that contain a history that is used by the classmodels (lines 4?9 and 12?17).
The other five strata (1, 2, 3, 10, 11) are comparativelysmall and have a small impact on overall difference in log likelihood.
The KN modelinterpolates predictions for histories of different lengths.
In general, this will include the859Computational Linguistics Volume 37, Number 4history that the class-based models use, but also include other lengths.
For example, incases where the class-based model is using a length-2 history, the KN model interpolateslength-2, length-1, and length-0 histories.We first compare P(KN-Half) and P(KN).
In general, P(KN-Half) performs better than P(KN)if a history of length 2 is available for prediction and the predictee w3 occurred at leastonce in the identical context in the training set (lines 11, 13?17, ?
(KN) is positive for thesestrata).
Stratum 11 contains a small number of positions and consequently contributeslittle to ?
(KN), but the per position difference is the largest of any stratum (0.48).
Incontrast, the per-position difference for stratum 17 is small, but the overall contributionto ?
(KN) is still noticeable since this stratum is the largest.
The overall contribution of alllength-2 history strata to ?
(KN) is 113,689 (the sum of rows 10?17 of the correspondingcolumn) and is thus responsible for the majority of the perplexity improvement due tohalf-context modeling.The two strata 10 and 12 are exceptions.
Per-position decrease in performance islarge for half-context modeling and the overall impact is also large for stratum 12.
Inthese two cases, P(KN) backs off to context length 1 for prediction.
Recall that, in contrast,P(KN-Half) uses only one context length for prediction, the longest that is available.
So online 12, P(KN-Half) underpredicts the next word w3 using a bigram w1w2 because w3 didnot occur in that position in the training set ( f1,3 = 0).
P(KN) can also use the unigramw2 for prediction and computes better estimates in cases where w2w3 occurred in thetraining set.
We are planning to address this problem in future work on the half-contextmodel.A similar problem for P(KN-Half) can be observed on lines 6?9.
In these cases, P(KN) canuse both the unigram w2 and the bigram w1w2 for prediction.
Note that the values for f1,3indicate that the trigram w1w2w3 occurred at least twice in the training set.
P(KN-Half) onlyuses the unigram w2 because the bigram w1w2 was not frequent enough to be includedin the model as a bigram.
The results on lines 6?9 suggest that further improvements ofP(KN-Half) are possible by interpolating predictions of histories of different lengths.Large improvements are realized by P(KN-Half) in strata 4 and 5.
For these two strata,the trigram w1w2w3 has frequency 0 or 1 and therefore the length-2 component of P(KN)does not predict w3 well.
P(KN-Half) achieves good predictions because the single wordw2 used for prediction occurred frequently ( f3 ?
10) and its class therefore is likely toreflect the distributional properties of w2 well.In summary, P(KN-Half) is the overall superior model because it successfully employsclass-based generalization for rare events.
However, for a number of strata (6?9, 10, 12)P(KN-Half) only uses one context for prediction, which in many cases is an inferior choicecompared to the predicting contexts used by P(KN).
As a result, the averages in thesestrata are negative.
We plan to address this problem in future work (see Section 6).Turning to the differences between P(KN-Half) and P(KN-Whole), we see that these dif-ferences are quite small?some positive, some negative?for length-1 histories (strata 2?9).
Only for length-2 histories do we find larger differences: strata 12, 13, and 17 fortotal differences on the validation set and strata 10, 12, and 13 for average differences.Length-2 differences are consistently positive for P(KN-Half) although some of the differ-ences are small.The better relative performance of P(KN-Half) for length-2 histories can be explainedby the fact that the difference between half-context and whole-context models increasesas the size of n-grams being clustered grows.
For unigrams, there is significant cor-relation between left and right half-contexts: If two words have the same type ofright context, then they often also have the same type of left context.
For bigrams,this correlation is smaller.
As an illustration consider the bigram underwriter was.860Schu?tze and Walsh Half-Context Language ModelsIt occurs four times in the validation set, followed by the words Dillon, Hambrecht,Merrill, and Nesbitt, none of which occur in this context in the training set.
For all fourwords log[P(KN-Half)(w3|underwriter was)/P(KN-Whole)(w3|underwriter was)] ?
2, that is,P(KN-Half) has a large advantage compared to P(KN-Whole).
The reason is that underwriterwas is in the same right half-context bigram class as many bigrams that are followedby Dillon, Hambrecht, Merrill, or Nesbitt in the training set.
Examples of such bigramsinclude banking at, bonds via, firm of, sold through, and strategist at.
These bigrams havesimilar right contexts, but dissimilar left contexts.
As a result, the whole-context modelgroups underwriter was with other bigrams that do not support good generalization.The pattern of consistent improvements of P(KN-Half) compared to P(KN-Whole) for bigrams(lines 11?17) indicates that half-context clustering is able to capture useful generaliza-tion for language modeling that whole-context clustering cannot capture.For unigrams, there are many cases that show the same effect.
For example, theunigram persuades is followed by them in the validation set, again a context unseen inthe training set.
The half-context model groups persuades with n-grams like They told,and prevented, and both of?dissimilar on the left, but similar on the right?that supporta high estimate for P(KN-Half)(them|persuade).
The class of persuade of the whole-contextmodel is more diffuse, generally containing n-grams that are followed by a noun phrase,but in contexts like act until, admit that, clearance by that make a following them less likely.However, there are also unigrams where class membership in the whole-contextmodel leads to better generalization than class memberships in the half-context modelbecause the whole-context model can exploit the correlation of left and right contexts.For example, the whole-context model assigns the unigram 367,000 to a class that con-sists almost exclusively of numbers whereas the half-context model assigns it to a classthat is more mixed.
Because 367,000 occurs in only 11 distinct contexts in the trainingset, its syntactic behavior can be better characterized if both left and right contexts areexploited.In summary, half-context and whole-context models perform similarly on averagefor length-1 histories although there are large differences between the two models forindividual 1-word histories.
For length-2 histories, the half-context model is superiordue to its ability to group histories according to the relevant half-context only?the righthalf-context?in accordance with the half-context hypothesis.5.3 Objective Function of n-gram ClusteringAs we argued in Section 3.3 when introducing the exemplar-theoretic model, class-based generalization is most useful for unseen and for infrequent events.
This basicinsight motivates two differences between our class-based model and previous work.First, the discounting mechanism defined in Equation (1) varies the weight thatclass-based generalization is given: Weights for unseen and infrequent events are higherthan weights for frequent events.
As a result, the model?s estimates are close to maxi-mum likelihood estimates for frequent events because the maximum likelihood estima-tor is appropriate in these cases.
In contrast, the model?s estimate of the probability ofa word occurring in an unattested context is closer to the estimate of the class-basedmodel.Using class-based generalization only for rare events also has implications for theobjective function of clustering.
Most previous work has employed objective functionsthat optimize a quantity on the entire training set.
For example, Brown et al (1992)maximize mutual information and Gao et al (2002) minimize perplexity on the entire861Computational Linguistics Volume 37, Number 4training set.
In contrast, the objective function of our clustering is similarity of half-contexts to cluster centroids or, more precisely, minimizing the residual sum of squaresof differences between half-context vectors and cluster centroids.
This criterion is muchless sensitive to frequency than previously used criteria.
In the extreme case, it may beoptimal on the global criteria to put a very frequent idiosyncratic word in its own class.This is so because even slightly better improvements of the class model for a frequentword will affect many positions in the training set and have a large cumulative effect.Our objective function is not influenced by frequency because vectors are normal-ized.
In principle, the gain from finding an appropriate cluster for a rare word is aslarge as the gain from finding an appropriate cluster for a frequent word.
In particular,frequent idiosyncratic words have no advantage compared to infrequent idiosyncraticwords and very frequent idiosyncratic words are less likely to be assigned to singletonclusters or small clusters dominated by them.
This clustering set-up may not be optimalfor achieving good results with a cluster-only language model, that is, a model that doesnot contain a ?lexical?
component similar to the maximum likelihood estimates in ourexemplar-theoretic model.
But if we acknowledge that class-based generalization is notuseful or is even harmful for frequent events, then this should not be our goal.In summary, we attribute part of the success of our half-context models to the factthat both the design of the discounting mechanism and the k-means objective functiontarget a different subspace of the space of all events: those that are unseen or infrequent.5.4 Efficient ClusteringThe focus of this article is the comparison of HC and WC classes and our investigationsinto the context-specific characteristics of history-length interpolation and class-basedgeneralization.
However, we also want to point out that the clustering algorithm weare using is very efficient, thus removing a potential obstacle to the widespread useof class-based language models.
In total, the clustering algorithm requires fewer thantwo assignments per item on average (see Section 4.1).
A single assignment requirescomputing the distance between an HC distribution and each of k centroids.
The timenecessary for computing one distance is a function of the number of nonzero entriesin the distribution.4 The total number of nonzero entries for any given bigram is thenumber of distinct trigrams in which it occurs.
Thus, the total number of operationsfor performing all assignments necessary for the clustering of the bigrams is less thanb times the number of distinct trigrams in the corpus where b is a small constant.
Thisnumber scales linearly with the number of distinct trigrams, which in turn scales sub-linearly with the length of the training corpus.
Thus, although the estimation procedureis expensive compared to standard trigram models like KN, it has desirable propertiescompared to other clustering algorithms, in particular the exchange algorithm.
Eventhough there exist fast implementations for the exchange algorithm (Martin, Liermann,and Ney 1998; Uszkoreit and Brants 2008), it has worse than linear complexity.6.
Conclusions and Future WorkIn this article we introduced a new representational formalism for language modelingknown as half-contextualization.
Half-contextualization employs only inward contextual4 This only holds for the dot product, not for the Euclidean distance, but the latter can be computed fromthe former as?
(xi ?
yi )2 =?x2i +?y2i ?
2?xiyi if sums of squares are precomputed and cached.862Schu?tze and Walsh Half-Context Language Modelsinformation in estimation and prediction?where we defined the inward distributionsas the conditioning context?s right-context distribution and the predicted word?s left-context distribution.
Our hypothesis was that only inward context is helpful for accurateprediction.The experimental results and statistical analyses herein indicate that this hypothesisis correct and that the use of outward directed information is not only redundant butalso, in the case of order-3, damaging.
We believe this is a particularly noteworthydiscovery as it is essentially tantamount to requiring only half of the available dis-tributional information in order to achieve an equivalent, and often better, result.Furthermore, from the outset we argued that the lack of adoption of class- andsimilarity-based approaches was, in part, because the granularity of contexts best suitedfor generalization and history-length interpolation have yet to be established; the novelcontext-specific analysis we presented here, which goes beyond traditional perplex-ity comparisons between models, illustrates the specific context scenarios where half-contextualization is particularly beneficial.The HC hypothesis is at first counter-intuitive: Standard language models treatwords as atomic units that are best characterized by taking into account all informationavailable about them in the training set, including what we call outward context.
Themodel we have proposed uses different parts of the available contextual information fordifferent inference tasks.
While it may seem surprising that contextual information canbe redundant or harmful for class-based generalization, we have argued that direction-ally nonrelevant information for a particular inference task can be noisy and misleading.In addition to half-contextualization, we introduced three other innovations forclass-based language models.
First, we defined classes as mixed classes of bigrams andunigrams and argued that this flexible granularity gives rise to better classes.
Second,we successfully employed a discounting method which focuses the impact of general-ization onto rare events while leaving frequent events to better-suited history-length in-terpolation.
This addresses the problem that class-based generalization is often harmfulfor high frequency events that are best estimated by maximum likelihood on identicalcontexts.
Third, we presented a new clustering algorithm for class-based language mod-els that has linear time complexity and is more efficient than the exchange algorithm.With regard to the future development of our exemplar-theoretic model, one obvi-ous avenue, given our analyses, is to incorporate the ability to interpolate distributionsof different-length conditioning contexts into the model.
By incorporating such an inter-polation mechanism, we anticipate an amelioration in performance further supportingthe use of half-context in language models.
However, crucially, the primary endeavor inthis article is not simply to promote the merits of half-contextualization, nor to establishhow to build a better exemplar-theoretic model, but rather to develop and promote adeeper understanding of the relationship between history-length interpolation, class-based generalization, and context, in order to construct and combine language models,of varying varieties, in a more targeted fashion.AcknowledgmentsThe research presented in this article wasfunded by DFG grant SFB 732.
We wouldlike to thank Helmut Schmid and audiencesat Google Mountain View research, theBerkeley International Computer ScienceInstitute, the 2010 Google EMEA facultysummit, and the reviewers for theirconstructive comments.ReferencesAbdoos, Monireh and SeyedGholamreza Jalali Naeini.
2008.Word categorization usingclustering ensemble.
In Proceedingsof the 2008 International Conferenceon Advanced Computer Theory andEngineering, pages 662?666,Washington, DC.863Computational Linguistics Volume 37, Number 4Bahrani, Mohammad, Hossein Sameti,Nazila Hafezi, and Saeedeh Momtazi.2008.
A new word clustering method forbuilding n-gram language models incontinuous speech recognition systems.In Proceedings of the 21st InternationalConference on Industrial, Engineering andOther Applications of Applied IntelligentSystems: New Frontiers in Applied ArtificialIntelligence, IEA/AIE ?08, pages 286?293,Berlin.Bai, Shuanghu, Haizhou Li, Zhiwei Lin, andBaosheng Yuan.
1998.
Building class-basedlanguage models with contextual statistics.In Acoustics, Speech and Signal Processing,1998.
Proceedings of the 1998 IEEEInternational Conference on, volume 1,pages 173?176, Seattle, WA.Bassiou, Nikoletta and ConstantineKotropoulos.
2011.
Long distance bigrammodels applied to word clustering.Pattern Recognition, 44:145?158.Bengio, Yoshua, Re?jean Ducharme, PascalVincent, and Christian Jauvin.
2003.A neural probabilistic language model.Journal of Machine Learning Research,3:1137?1155.Brown, Peter F., Vincent J. Della Pietra,Peter V. de Souza, Jennifer C. Lai, andRobert L. Mercer.
1992.
Class-basedn-gram models of natural language.Computational Linguistics, 18(4):467?479.Chen, Stanley F. and Joshua Goodman.
1998.An empirical study of smoothingtechniques for language modeling.Technical report TR-10-98, HarvardUniversity, Cambridge, MA.Dagan, Ido, Lillian Lee, and FernandoPereira.
1999.
Similarity-based modelsof cooccurrence probabilities.
MachineLearning, 34(1-3):43?69.Emami, Ahmad and Fred Jelinek.
2005.Random clustering for languagemodeling.
In Proceedings of the InternationalConference on Acoustics, Speech, and SignalProcessing (ICASSP), pages I:581?584,Philadelphia, PA.Essen, Ute and Volker Steinbiss.
1992.Cooccurrence smoothing for stochasticlanguage modeling.
In Proceedings of theInternational Conference on Acoustics,Speech and Signal Processing (ICASSP),pages I:161?164, San Francisco, CA.Gao, Jianfeng, Joshua T. Goodman,Guihong Cao, and Hang Li.
2002.Exploring asymmetric clustering forstatistical language modeling.
InProceedings of the 40th Annual Meetingof the Association for ComputationalLinguistics, ACL ?02, pages 183?190,Morristown, NJ.Hall, Keith and Mark Johnson.
2003.Language modelling using efficientbest-first bottom-up parsing.
In AutomaticSpeech Recognition and UnderstandingWorkshop (ASRU), pages 507?512,St.
Thomas.Hintzman, Douglas L. 1986.
?Schemaabstraction?
in a multiple-trace memorymodel.
Psychological Review, 93:328?338.Justo, Raquel and M. Ine?s Torres.
2009.Phrase classes in two-level languagemodels for ASR.
Pattern Analysis &Applications, 12(4):427?437.Kneser, Reinhard and Hermann Ney.
1993.Improved clustering techniques forclass-based statistical language modelling.In Proceedings of the 3rd European Conferenceon Speech Communication and Technology,pages 973?976, Berlin.Martin, Sven, Jo?rg Liermann, and HermannNey.
1998.
Algorithms for bigram andtrigram word clustering.
SpeechCommunication, 24:19?37.Momtazi, Saeedeh, Sanjeev Khudanpur, andDietrich Klakow.
2010.
A comparativestudy of word co-occurrence for termclustering in language model-basedsentence retrieval.
In Human LanguageTechnologies: The 2010 Annual Conference ofthe North American Chapter of the Associationfor Computational Linguistics, HLT ?10,pages 325?328, Morristown, NJ.Momtazi, Saeedeh and Dietrich Klakow.2009.
A word clustering approach forlanguage model-based sentenceretrieval in question answering systems.In Proceedings of the 18th ACM Conferenceon Information and Knowledge Management(CIKM), pages 1911?1914, Hong Kong.Ney, Hermann, Ute Essen, and ReinhardKneser.
1994.
On structuring probabilisticdependencies in stochastic languagemodelling.
Computer Speech and Language,8:1?28.Nosofsky, Robert M. 1986.
Attention,similarity, and the identification-categorization relationship.
Journalof Experimental Psychology: General,115(1):39?57.Pierrehumbert, Janet B.
2001.
Exemplardynamics: Word frequency, lenitionand contrast.
In J. Bybee and P. Hopper,editors, Frequency Effects and the Emergenceof Lexical Structure.
John Benjamins,Amsterdam, pages 137?157.Schmid, Helmut.
1994.
Probabilisticpart-of-speech tagging using decision864Schu?tze and Walsh Half-Context Language Modelstrees.
In Proceedings of Conference onNew Methods in Language Processing,pages 44?49, Manchester.Schu?tze, Hinrich.
1993.
Distributed syntacticrepresentations with an application topart-of-speech tagging.
In Proceedingsof the IEEE International Conference onNeural Networks, pages 1504?1509,San Francisco, CA.Schu?tze, Hinrich.
1995.
Distributionalpart-of-speech tagging.
In Proceedingsof the 7th Conference of the European Chapterof the Association for ComputationalLinguistics, pages 141?148, Belfield.Schu?tze, Hinrich and Michael Walsh.
2008.A graph-theoretic model of lexicalsyntactic acquisition.
In Proceedings of the2008 Conference on Empirical Methods inNatural Language Processing (EMNLP),pages 917?926, Honolulu, HI.Schwenk, Holger and Philipp Koehn.2008.
Large and diverse language modelsfor statistical machine translation.In Proceedings of the International JointConference on Natural Language Processing,pages 661?666, Hyderabad.Steinbach, Michael, George Karypis, andVipin Kumar.
2000.
A comparison ofdocument clustering techniques.
Paperpresented at the Workshop onText Mining, Knowledge Discoveryand Data Mining, Boston, MA.Stolcke, Andreas.
2002.
Srilm?an extensiblelanguage modeling toolkit.
In Proceedingsof the International Conference on SpokenLanguage Processing (ICSLP),pages 901?904, Denver, CO.Uszkoreit, Jakob and Thorsten Brants.2008.
Distributed word clustering forlarge scale class-based languagemodeling in machine translation.In Proceedings of the 46th Annual Meetingof the Association for ComputationalLinguistics (ACL), pages 755?762,Columbus, OH.Wiegand, Michael and Dietrich Klakow.2008.
Optimizing language models forpolarity classification.
In Proceedings of theIR Research, 30th European Conference onAdvances in Information Retrieval, ECIR?08,pages 612?616, Berlin.Zitouni, Imed.
2007.
Backoff hierarchicalclass n-gram language models:Effectiveness to model unseen events inspeech recognition.
Journal of ComputerSpeech and Language, 21(1):88?104.Zitouni, Imed and Qiru Zhou.
2007.
Linearlyinterpolated hierarchical n-gram languagemodels for speech recognition engines.In Michael Grimm and Kristian Kroschel,editors, Robust Speech Recognition andUnderstanding.
InTech, Vienna,pages 301?318.Zitouni, Imed and Qiru Zhou.
2008.Hierarchical linear discounting classn-gram language models: A multilevelclass hierarchy approach.
In Proceedingsof the International Conference on Acoustics,Speech and Signal Processing (ICASSP),pages 4917?4920, Las Vegas, NV.865
