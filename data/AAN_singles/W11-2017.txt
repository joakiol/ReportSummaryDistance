Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 142?151,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics?The day after the day after tomorrow??
A machine learning approach toadaptive temporal expression generation:training and evaluation with real usersSrinivasan Janarthanam, Helen Hastie, Oliver Lemon, Xingkun LiuInteraction LabSchool of Mathematical and Computer Sciences (MACS)Heriot-Watt University{sc445, h.hastie, o.lemon, x.liu}@hw.ac.ukAbstractGenerating Temporal Expressions (TE) thatare easy to understand, unambiguous, and rea-sonably short is a challenge for humans andSpoken Dialogue Systems.
Rather than devel-oping hand-written decision rules, we adopt adata-driven approach by collecting user feed-back on a variety of possible TEs in termsof task success, ambiguity, and user prefer-ence.
The data collected in this work is freelyavailable to the research community.
Thesedata were then used to train a simulated userand a reinforcement learning policy that learnsan adaptive Temporal Expression generationstrategy for a variety of contexts.
We evalu-ate our learned policy both in simulation andwith real users and show that this data-drivenadaptive policy is a significant improvementover a rule-based adaptive policy, leading toa 24% increase in perceived task completion,while showing a small increase in actual taskcompletion, and a 16% decrease in call dura-tion.
This means that dialogues are more ef-ficient and that users are also more confidentabout the appointment that they have agreedwith the system.1 IntroductionTemporal Expressions are linguistic expressions thatare used to refer to a date and are often a source ofconfusion in human-human, human-computer andtext interactions such as emails and instant messag-ing.
For example, ?Let?s meet next Sunday??
?doyou mean Sunday this week or a week on Sunday??.
(Mccoy and Strube, 1999) state that changes in tem-poral structure in text are often indicated by eithercue words and phrases (e.g.
?next Thursday?, ?thisweek?, ?tomorrow?
), a change in grammatical timeof the verb (e.g.
present tense versus future tense),or changes in aspect (e.g.
atomic versus extendedevents versus states as defined by (Moens and Steed-man, 1988)).
In this study, we will concentrate onthe first of these phenomena, generating TEs withthe optimal content and lexical choice.Much work in the field of Natural Language Pro-cessing concerns understanding and resolving thesetemporal expressions in text (Gerber et al, 2002;Pustejovsky et al, 2003; Ahn et al, 2007; Mazurand Dale, 2007; Han et al, 2006), however, littlework has looked at how best to plan and realise tem-poral expressions in order to minimize ambiguityand confusion in a Spoken Dialogue System (SDS).
(Reiter et al, 2005) presented a data driven ap-proach to generating TEs to refer to time in weatherforecast information where appropriate expressionswere identified using contextual features using su-pervised learning.
We adopt an adaptive, data-drivenreinforcement learning approach instead.
Similardata-driven approaches have been applied to infor-mation presentation (Rieser et al, 2010; Walker etal., 2007) where each Natural Language Generation(NLG) action is a sequential decision point, based onthe current dialogue context and expected long-termreward of that action.
A data-driven approach hasalso been applied to the problem of referring expres-sion generation in dialogue for expert and novice-users of a SDS (Janarthanam and Lemon, 2010).However, to date, there has been no previous workon adaptive data-driven approaches for temporal re-ferring expression generation, where uncertainty in142the stochastic environment is explicitly modelled.The data-driven approach to temporal expressiongeneration presented here is in the context of ap-pointment scheduling dialogues.
The fact that thereare multiple ways that a time slot can be referred toleads to an interesting NLG problem of how best torealise a TE for a particular individual in a particularcontext for certain domains.
For example, the fol-lowing expressions all vary in terms of length, ambi-guity, redundant information and users?
preference:?next Friday afternoon?
or ?Friday next week at thesame time?, or ?in the afternoon, a week on Friday?.Temporal Expressions contain two types of refer-ences: absolute references such as ?Tuesday?
and?12th January?, and relative references such as ?to-morrow?
and ?this Tuesday?.
Generating TEs there-fore, involves both in selecting appropriate pieces ofinformation (date, day, time, month, and week) topresent and deciding how to present them (absoluteor relative reference).Our objective here is to convey a target appoint-ment slot to users using an expression that is optimalin terms of the trade-off between understandability,length and user preference.2 MethodologyWe address the issue of generating TEs by adoptinga data-driven approach that has four stages.
Firstly,we define Temporal Expression Units (TEU) as de-scribed in Section 2.1.
Secondly, we design and im-plement a web-based data collection, gathering met-rics on the TEUs in various contexts for a varietyof date types (Section 3).
Thirdly, we train a usersimulation and use it to learn a policy using rein-forcement learning techniques that generates the op-timal combination of TEUs for each context (Sec-tion 4).
Finally, we deploy and evaluate this pol-icy in a Spoken Dialogue System for appointmentscheduling and show that our learned policy per-forms better than a hand-written, adaptive one (re-sults presented in Section 5).2.1 Temporal Expression UnitsFor this study, TEs are broken down into 5 cate-gories or units (TEUs) presented in a fixed order:DAY, DATE, MONTH, WEEK and TIME.
Each ofthese units can be expressed relative to the currentTEU ChoicesDAY abs, rel, rc, nnDATE abs, nnMONTH abs, nnWEEK abs, rel, nnTIME abs, rcTable 1: TEU choices where abs is absolute, rel is rela-tive, rc is relative to context and nn is noneday and to the current context (i.e.
previously men-tioned dates).
Specifically, there are 3 unit attributes:absolute (e.g.
DAY=abs ?Tuesday?
); relative to cur-rent day (e.g.
DAY=rel ?tomorrow?
); and relative tocontext (e.g.
DAY=rc ?the following day?
).Certain restrictions on possible TEU combina-tions were imposed, for example, DATE=rc andDAY=rel were combined to be just DAY=rel, andsome combinations were omitted on the basis thatit is highly unlikely that they would be utteredin natural speech, for example WEEK=rel andMONTH=abs would result in ?this week in Septem-ber?.
Finally, every TE has to contain a time (am orpm for this application).
The possible combinationsare summarised in Table 1.3 Data CollectionThe data collection experiment was in two parts(Task 1 and Task 2) and was designed using the We-bexp experimental software1.
Webexp is a client-server set up where a server application hosts the ex-periment and stores the experimental files, logs andresults.
The client side runs an applet on the user?sweb-browser.In Task 1, participants listened to an audio filecontaining a TE generated from absolute and rela-tive TEUs (see Figure 1).
No relative-context (rc)TEUs were used in Task 1 since the dialogue ex-cerpt presented was in isolation and therefore hadno context.
Each participant was asked to listen to10 different audio files in a sequence correspondingto a variety of dates randomly chosen from 8 pos-sible dates.
The participant then had to identify thecorrect appointment slot that the system is referringto.
There is scope for the participant to add multi-ple answers in order to capture potential ambiguity1http://www.webexp.info143Figure 1: Screen shot of Task 1 in the on-line data collection experimentof a TE, and we report on this below.
The 8 datesthat were used to generate the TEs fell into a twoweek period in a single month which is in-line withthe evaluation set-up of the appointment schedulingSDS discussed in Section 5.3.For each date, the TE was randomly picked from aset of 30 possible combinations of TEUs.
Each TEUwas generated by a rule-based realiser and synthe-sized using the Baratinoo synthesizer (France Tele-com, 2011).
This realiser generates text from a can-didate list for each TEU based on the given date.For example, if the slot currently being discussedis Tuesday 7th, the realiser would generate ?tomor-row?
for DAY=rel; if the date in discussion wasWednesday 8th then DAY=rel would be realised as?the day after tomorrow?.
There was potential foroverlap of stimuli, as any given TE for any givendate may be assessed by more than one participant.Task 2 of the experiment was in two stages.
In thefirst stage (Task 2A), the participants are given to-day?s date and the following dialogue excerpt; Op-erator: ?We need to send out an engineer to yourhome.
The first available appointment is .
.
.?
(seeFigure 2).
They are then asked to listen to 5 audiofiles of the system saying different TEs for the samedate and asked to rate preference on a scale of 1-6(where 1 is bad and 6 is great.)
For the second stage(Task 2B), the dialogue is as follows; Operator: ?soyou can?t do Wednesday 8th September in the morn-ing.?
and then the participants are asked to listento 5 more audio files that are generated TEs includ-ing relative context such as ?how about Thursday atthe same time??.
This two-stage process is then re-peated 4 times for each participant.Table 2 summarizes the metrics collected in thedifferent parts of the experiment.
The metric Dis-tance is calculated in terms of the number of slotsfrom the current date to the target date (TD).
In-stances were grouped into four distance groups: G1:TD is 1-2 slots away; G2: TD is 3-6 slots away; G3:TD is 7-11 slots away and G4: TD more than 11slots away.
P replay is calcuated by the total num-ber of replays divided by the total number of playsfor that temporal expression, i.e.
the probability thatthe temporal expression played is requested to be re-played.
P ambiguous is calculated by the number oftimes a given temporal expression is given more than1 interpretation divided by the total number of timesthat the same given referring expression is answered.In total there were 73 participants for Task 1 and144Figure 2: Screen shot of Task 2 in the on-line data collection experiment730 TE samples collected.
Although Task 2 directlyfollowed on from Task 1, there was a significantdrop out rate as only 48 participants completed thesecond task resulting in 1,920 TE samples.
Partici-pants who completed both tasks were rewarded by achance to win an Amazon voucher.3.1 Data AnalysisFigure 3 shows various metrics with respect to TEabsoluteness and relativeness is the number of ab-solute and relative TEUs respectively.
These twographs represent the state space that the genera-tion policy described in Section 4 is exploring, trad-ing off between various features such as Length,taskSuccess and userPref.As we can see, there is a tendency for averagetaskSuccess to increase as absoluteness increaseswhereas, for relativeness the distribution is moreeven.
The TE with the greatest taskSuccess has anabsoluteness of 4 and zero relativeness: DATE=abs,MONTH=abs, WEEK=abs, TIME=abs (e.g.
?11thSeptember, the week starting the 10th, between 8amand 10am?)
and the TE with the least taskSuccesshas an absoluteness of only 2, again with no rela-tiveness: DATE=abs, TIME=abs, (e.g.
?8th between8am and 10am?
).Average userPref stays level and then decreasesif absoluteness is 5.
We infer from this that al-though long utterances that are completely explicitare more clear in terms of taskSuccess, they are notnecessarily preferred by users.
This is likely dueto TE length increasing.
On average, the inclusionof one relative expression is preferred over none atall or two.
The most preferred TE has an abso-luteness of 3 with a relativeness of 2: DAY=rel,DATE=abs, MONTH=abs, WEEK=rel, TIME=abs(e.g.
?Tomorrow the 7th of September, this week,between 8am and 10am?
).145Figure 3: Graph showing the trade-offs between various metrics with respect to absoluteness and relativeness (numberof absolute/relative TEUs) in terms of probabilities or normalised values.Metric Description TaskP ambiguous Probability that the expres-sion is ambiguous to theuser1taskSuccess Correct slot identified 1P replay Probability of replay (mea-sure of understandability)1 & 2Length Expression length in termsof number of TEUs thatare non null divided by thetotal number of possibleTEUs (5)1 & 2wordLength Expression length in wordsnormalised over max numof words (15)1 & 2userPref Preference rating of audiofrom 1-62Distance Distance from target date(TD) to current date interms of number of slots1 & 2Table 2: Metrics collected in various parts of the experi-mentThe probability of ambiguity and replay does notseem to be affected by absoluteness.
The most am-biguous TE has an absoluteness of 3 and zero rela-tiveness: DAY=abs MONTH=abs TIME=abs, (e.g.
?Tuesday September between 8am and 10am?)
in-dicating that a date is needed for precision.
TheTEs that the participants were most likely to replaytended to be short e.g.
?Tomorrow at the same time?.This may be due to the clarity of the speech synthe-siser.4 Learning a TE generation policyReinforcement learning is a machine learning ap-proach based on trial and error learning, in whicha learning agent learns to map sequences of ?opti-mal?
actions to environment or task states (Suttonand Barto, 1998).
In this framework the problemof generating temporal expressions is presented asa Markov Decision Process.
The goal of the learn-ing agent is to learn to choose those actions that ob-tain maximum expected reward in the long run.
Inthis section, we present the reinforcement learningsetup for learning temporal expression generationpolicies.4.1 Actions and StatesIn this learning setup, we focus only on generatingthe formal specification and treat the set of TEUchoices as the sequential actions of the learningagent.
Table 1 presents the choices that are availablefor each TEU.The actions are taken based on two factors: the146distance (in terms of time slots: morning or after-noon appointments) between (1) the current dateand the target slot and (2) the current date and theslot in context.
Based on the distance, the targetslot was classified to belong to one of the four dis-tance groups (G1-G4).
The slot in context repre-sents whether there was any other slot already men-tioned in the conversation so far, so that the systemhas an option to use ?relative context?
expressionsto present day and time information.
Informationconcerning the target slot?s group and the slot in con-text make up the state space of the Markov DecisionProcess (MDP).4.2 User SimulationWe built a user simulation to simulate the dialoguebehaviour of a user in appointment scheduling con-versations based on the data from real users de-scribed in Section 3.
It responds to the TE usedby the system to refer to an appointment slot.
Itresponds by either accepting, rejecting, or clarify-ing the offered slot based on the user?s own calen-dar of available slots.
For instance, the simulateduser rejects an offered slot if the user is not avail-able at that time.
If they accept or reject an offeredslot, the user is assumed to understand the TE unam-biguously.
However, if the user is unable to resolvethe appointment slot from the TE, it responds with aclarification request.
The simulation responded witha dialogue action (Au,t) to TEs based on the sys-tem?s dialogue act (As,t), system?s TE (TEs,t).
Thefollowing probabilistic model was used to generateuser dialogue actions:P (Au,t|As,t, TEs,t, G,C,Cal)In addition to TEs,t and As,t, other factors such asdistance between the target slot and the current slot(G), the previous slot in context (C), and the user?scalendar (Cal) were also taken into account.
G is ei-ther G1, G2, G3 or G4 as explained in Section 3.
TheUser?s dialogue action (Au,t) is one of the three: Ac-cept slot, Reject slot or Request Clarification.
Theprobability of clarification request was calculated asthe average of the ambiguity and replay probabilitiesseen in real user data.4.3 Reward functionThe learning agent was rewarded for each TE that itgenerated.
The reward given to the agent was basedon trade-offs between three variables: User prefer-ence (UP), Length of the temporal expression (L),and Clarification request probability (CR).
UP foreach TE is obtained from Task 2 of the data collec-tion.
In the following reward function, UP is nor-malised to be between 0 and 1.
L is based on numberof TEUs used.
The maximum number of TEUs thatcan be used is 5 (i.e.
DAY, DATE, WEEK, MONTH,TIME).
L is calculated as follows:Length of TE (L) = No.
of used TEUsMax.
no.
of TEUsThe clarification request (CR) is set to be 1 if theuser responds to the TE with a Request Clarificationand 0 otherwise.
Reward is therefore calculated ona turn-by-turn basis using the following formula:Reward = UP ?
10.0 ?
L ?
10.0 ?
CR ?
10.0In short, we chose a reward function that penalisesTEs that are long and ambiguous, and which rewardsTEs that users prefer.
It also indirectly rewards tasksuccess by penalising ambiguous TEs resulting inclarification requests.
This trade-off structure is evi-dent from the data collection where TEs that are toolong are dispreferred by the users (see Figure 3).
Themaximum possible reward is 6 (i.e.
UP=1, CR=0,L=2/5) and the minimum is -20 (i.e.
UP=0, CR=1,L=1).
Note that other reward functions could be ex-plored in future work, for example maximising onlyfor user preference or length.4.4 TrainingWe trained a TE generation policy using the aboveuser simulation model for 10,000 runs using theSARSA reinforcement learning algorithm (Suttonand Barto, 1998).
During the training phase, thelearning agent generated and presented TEs to theuser simulation.
When a dialogue begins, there is noappointment slot in context (i.e.
C = 0).
However,if the user rejects the first slot, the dialogue systemsets C to 1 and presents the next slot.
This is againreset at the beginning of the next dialogue.
Theagent was rewarded at the end of every turn basedon the user?s response, length of the TE, and userpreference scores as shown above.
It gradually ex-plored all possible combinations of TEUs and identi-fied those TEUs in different contexts that maximize147Figure 4: Learning curvethe long-term reward.
Figure 4 shows the learningcurve of the agent.Table 3 presents the TE generation policy learnedby the agent.
As one can observe, it used a mini-mum number of TEUs to avoid length penalties inthe reward.
In all cases, MONTH and WEEK in-formation have not been presented at all.
For targetslots that were closest (in group G1) and the farthest(in group G4), it used relative forms of day (e.g.
?to-morrow?, ?next Tuesday?, etc.).
This is probablybecause users dispreferred day information for in-between slots (e.g.
?the day after the day after to-morrow?).
Also, MONTH information may havebeen considered to be irrelevant due to the fact thatthe two week window over which the data has beencollected do not span over two different months.5 EvaluationIn this section, we present the baseline policies thatwere evaluated along with the learned policy.
Wethen present the results of evaluation.Slots Specification learned1-2 DAY=rel;DATE=abs;MONTH=nn;> 11 WEEK=nn;TIME=abs3-11 DAY=nn;DATE=abs;MONTH=nn;WEEK=nn;TIME=absTable 3: Learned policy5.1 Baseline policiesThe following are the baseline TEG policies:1.
Absolute policy: always use absolute for-mats for all TEUs (i.e.
DAY=abs; DATE=abs;MONTH=abs; WEEK=abs; TIME=abs)2.
Minimal policy: always use a minimal formatwith only date, month and time information intheir absolute forms (i.e.
DAY=nn; DATE=abs;MONTH=abs; WEEK=nn; TIME=abs)3.
Random policy: select possible formats ran-domly for each TEU.148TEG Policy Average rewardLearned -0.071* (?3.75)Absolute -4.084 (?4.36)Minimal -1.340 (?4.2)Random -8.21 (?7.72)Table 4: Evaluation with simulated users (* p < 0.05,two-tailed independent samples t-test)5.2 ResultsWe evaluated the learned policy and the three otherhand-coded baseline TE generation policies with ouruser simulation model.
Each policy generated 1,000TEs in different states.
Table 4 present the resultsof evaluation with simulated users.
On average, thelearned policy scores higher than all the baselinepolicies and the differences between the average re-ward of the learned policy and the other baselinesare statistically significant.
This shows that targetslots can be presented using different TEs dependingon how far they are from the current date and suchadaptation can produce less ambiguous, shorter anduser preferred expressions.5.3 Evaluation with real usersThe policy was also integrated into an NLG com-ponent of a deployed Appointment Scheduling spo-ken dialogue system.
Please note that this is differ-ent from the web environment in which the trainingdata was collected.
Our data-driven policy was acti-vated when the system informs the user of an avail-able time slot.
This system was compared to theexact same system but with a rule-based adaptivebaseline system.
In the rule-based policy MONTH,DATE and TIME were always absolute, DAY wasrelative if the target date was less than three daysaway (i.e.
?today, tomorrow, day after tomorrow?
),and WEEK was always relative (i.e.
?this week, nextweek?).
All 5 information units were included in therealisation (e.g.
?Thursday the 15th July in the after-noon, next week?)
although the order was slightlydifferent (DAY-DATE-MONTH-TIME-WEEK).In this domain, the user tries to make an appoint-ment for an engineer to visit their home.
Each useris given a set of 2-week calendars which shows theiravailability and the goal is to arrange an appoint-ment when both they and the engineer are available.There were 12 possible scenarios that were evenlyrotated across participants and systems.
Each sce-nario is categorised in terms of scheduling difficulty(Hard/Medium/Easy).
Scheduling difficulty is cal-culated for User Difficulty (UD) and System Diffi-culty (SD) separately to assess the system?s mixedinitiative ability.
Scheduling difficulty is calculatedas the ordinal of the first session that is free for boththe User and the System.
Hard scenarios are with anordinal of 3 or 4; Medium with an ordinal of 2, andEasy with an ordinal of 1.
There are 4 scenarios ineach of these difficulty categories for both the userand system.
To give an example, in Scenario 10,the user can schedule an appointment on Wednes-day afternoon but he/she also has one free sessionon the previous Tuesday afternoon when the engi-neer is busy therefore UD = 2.
For the system, inthis scenario, the first free session it has is on theWednesday afternoon therefore SD=1.
In this case,the scenario is easier for the system than the user be-cause the system could just offer the first session thatit has free.605 dialogues were collected and analysed.
Thesystem was evaluated by employees at France Tele-com and students of partner universities who havenever used the appointment scheduling system be-fore.
After each scenario, participants were thenasked to fill out a questionnaire on perceived tasksuccess and 5 user satisfaction questions on a 6-point Likert Scale (Walker et al, 2000).
Resultsfrom the real user study are summarised in Table 5.The data-driven policy showed significant improve-ment in Perceived Task Success (+23.7%) althoughno significant difference was observed between thetwo systems in terms of Actual Task Success (Chi-square test, df=1).
Perceived Task Success is users?perception of whether they completed the task suc-cessfully or not.
Overall user satisfaction (the aver-age score of all the questions) was also significantlyhigher (+5%)2.
Dialogues with the learned policywere significantly shorter with lower Call Durationin terms of time (-15.7%)2 and fewer average wordsper system turn (-23.93%)2.
Figure 5 shows thelength results in time for systems of varying UD andSD.
We can see that the data-driven adaptive policyconsistently results in a shorter dialogue across alllevels of difficulty.
In summary, these results showthat using a policy trained on the data collected here149Parameters Learned BaselineTEG TEGActual Task Success 80.05% 78.57%Perceived Task Success 74.86%* 60.50%User satisfaction 4.51* 4.30No.
system turns 22.8 23.2Words per system turn 13.16* 17.3Call duration 88.60 sec * 105.11 secTable 5: Results with real users (* statistically significantdifference at p<0.05)results in shorter dialogues and greater confidencein the user that they have had a successful dialogue.Although the learned policy was trained to generateoptimal TEs within a two week window and there-fore is not general policy for all TE generation prob-lems, we believe that the data-driven approach thatwe have followed can generalise to other TE gener-ation tasks.Figure 5: Graph comparing length of dialogues for user(UD) and system difficulty (SD)6 ConclusionWe have presented a principled statistical learningmethod for generating Temporal Expressions (TEs)that refer to appointment slots in natural languageutterances.
We presented a method for gatheringdata on TEs with an on-line experiment and showedhow we can use these data to generate TEs us-ing a Markov Decision Process which can be opti-mised using reinforcement learning techniques.
Weshowed that a TEG policy learned using our frame-2independent two-tailed t-test p < 0.05work performs signifcantly better than hand-codedadaptive policies with real users as well as with sim-ulated users.The data collected in this work has been freelyreleased to the research community in 20113.AcknowledgementsThe research leading to these results has receivedfunding from the EC?s 7th Framework Programme(FP7/2007-2013) under grant agreement no.
216594(CLASSiC project www.classic-project.org), (FP7/2011-2014) under grant agreement no.248765 (Help4Mood project), (FP7/2011-2014) un-der grant agreement no.
270435 (JAMES project),(FP7/2011-2014) under grant agreement no.
270019(SpaceBook project), and from the EPSRC, projectno.
EP/G069840/1.
We would also like to thank ourCLASSiC project colleagues at Cambridge Univer-sity and France Telecom / Orange Labs.ReferencesD.
Ahn, J. van Rantwijk, and M. de Rijke.
2007.
ACascaded Machine Learning Approach to Interpret-ing Temporal Expressions.
In Proceedings of NAACL-HLT 2007.France Telecom.
2011.
Baratinoo expressive speech syn-thesiser.
http://tts.elibel.tm.fr.L.
Gerber, L. Ferro, I. Mani, B. Sundheim, G. Wilson,and R. Kozierok.
2002.
Annotating Temporal Infor-mation: From Theory to Practice.
In Proceedings ofHLT.B.
Han, D. Gates, and L. Levin.
2006.
Understandingtemporal expressions in emails.
In HLT-NAACL 2006.Srinivasan Janarthanam and Oliver Lemon.
2010.
Learn-ing to adapt to unknown users: referring expressiongeneration in spoken dialogue systems.
In ACL ?10.P.
Mazur and R. Dale.
2007.
The DANTE Temporal Ex-pression Tagger.
In Proceedings of the 3rd Languageand Technology Conference, Poznan, Poland.Kathleen F. Mccoy and Michael Strube.
1999.
Takingtime to structure discourse: Pronoun generation be-yond accessibility.
In Proc.
of the 21th Annual Con-ference of the Cognitive Science Society.M.
Moens and M. Steedman.
1988.
Temporal ontologyand temporal reference.
In Computational Linguistics,volume 14(2), pages 15?28.3Sec 2.6 at http://www.macs.hw.ac.uk/ilabarchive/classicproject/data/150J.
Pustejovsky, J. Castano, R. Ingria, R. Sauri,R.
Gaizauskas, A. Setzer, G. Katz, and D. Radev.2003.
TimeML: Robust specification of event andtemporal expressions in text.
In AAAI Spring Sympo-sium on New Directions in Question-Answering, Stan-ford, CA.E.
Reiter, S. Sripada, J.
Hunter, and J. Yu.
2005.
Choos-ing words in computer-generated weather forecasts.Artificial Intelligence, 167:137169.Verena Rieser, Oliver Lemon, and Xingkun Liu.
2010.Optimising information presentation for spoken dia-logue systems.
In Proc.
ACL 2010.R.
Sutton and A. Barto.
1998.
Reinforcement Learning.MIT Press.Marilyn A. Walker, Candace A. Kamm, and Diane J. Lit-man.
2000.
Towards Developing General Models ofUsability with PARADISE.
Natural Language Engi-neering, 6(3).Marilyn Walker, Amanda Stent, Franc?ois Mairesse, andRashmi Prasad.
2007.
Individual and domain adap-tation in sentence planning for dialogue.
Journal ofArtificial Intelligence Research (JAIR), 30:413?456.151
