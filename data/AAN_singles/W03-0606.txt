Learning Word Meaning and Grammatical Constructionsfrom Narrated Video EventsPeter Ford DomineyInstitut des Sciences Cognitives,CNRS67 Blvd.
Pinel,69675 Bron Cedex, Francedominey@ isc.cnrs.frThomas VoegtlinInstitute for Theoretical BiologyHumboldt-Universit?tInvalidenstra?e 43D-10115 Berlin, GermanyVoegtlin@isc.cnrs.frAbstractThe objective of this research is to develop a system forminiature language learning based on a minimum of pre-wired language-specific functionality, that is compatiblewith observations of perceptual and language capabilities inhuman development.
In the proposed system, meaning isextracted from video images based on detection of physicalcontact and its parameters.
Mapping of sentence form tomeaning is performed by learning grammaticalconstructions that are retrieved from a constructioninventory based on the constellation of closed class itemsuniquely identifying the target sentence structure.
Theresulting system displays robust acquisition behavior thatreproduces certain observations from developmentalstudies, with very modest ?innate?
language specificity.1.
IntroductionFeldman et al (1990) posed the problem of"miniature" language acquisition based on <sentence,image> pairs as a "touchstone" for cognitive science.
Inthis task, an artificial system is confronted with a reducedversion of the problem of language acquisition faced bythe child, that involves both the extraction of meaningfrom the image, and the mapping of the paired sentenceonto this meaning.Extraction of MeaningIn this developmental context, Mandler (1999)suggested that the infant begins to construct meaning fromthe scene based on the extraction  of perceptualprimitives.
From simple representations such as contact,support, attachment (Talmy 1988) the infant couldconstruct progressively more elaborate representations ofvisuospatial meaning.
Thus, the physical event "collision"is a form of the perceptual primitive ?contact?.
Kotovsky& Baillargeon (1998) observed that at 6 months, infantsdemonstrate sensitivity to the parameters of objectsinvolved in a collision, and the resulting effect on thecollision, suggesting indeed that infants can representcontact as an event predicate with agent and patientarguments.Siskind (2001) has demonstrated that force dynamicprimitives of  contact, support, attachment can beextracted from video event sequences and used torecognize events including pick-up, put-down, and stackbased on their characterization in an event logic.
The useof these intermediate representations renders the systemrobust to variability in motion and view parameters.
Mostimportantly, Siskind demonstrated that the lexicalsemantics for a number of verbs could be established byau omatic image processing.Sentence to meaning mapping:Once meaning is extracted from the scene, thesignificant problem of mapping sentences to meaningsremains.
The nativist  perspective on this problem holdsthat the <sentence, meaning> data to which the child isexpos  is highly indeterminate, and underspecifies themapping to be learned.
This ?poverty of the stimulus?
is ac ntral argument for the existence of a geneticallypecified universal grammar, such that languageacquisition consists of configuring the UG for theappropriate target language (Chomsky 1995).
In thisframework, once a given parameter is set, its use shouldapply to new constructions in a generalized, generativemanner.An alternative functionalist perspective holds thatlearning plays a much more central role in languageacq isition.
The infant develops an inventory ofgrammatical constructions as mappings from form tomeaning (Goldberg 1995).
These constructions areinitially rather fixed and specific, and later becomege eralized into a more abstract compositional formemployed by the adult (Tomasello 1999).
In this context,construction of the relation between perceptual andcognitive representations and grammatical form plays acentral role in learning language (e.g.
Feldman et al 1990,1996; Langacker 1991; Mandler 1999; Talmy 1998).These issues of learnability and innateness haveprovided a rich motivation for simulation studies that havetaken a number of different forms.
Elman (1990)demonstrated that recurrent networks are sensitive topredictable structure in grammatical sequences.Subsequent studies of grammar induction demonstratehow syntactic structure can be recovered from sentences(e.g.
Stolcke & Omohundro 1994).
From the ?groundingof language in meaning?
perspective (e.g.
Feldman et al1990, 1996; Langacker 1991; Goldberg 1995)  Chang &Maia (2001) exploited the relations between actionrepresentation and simple verb frames in a constructiongrammar approach.
In effort to consider more complexgrammatical forms, Miikkulainen (1996) demonstrated asystem that learned the mapping between relative phraseconstructions and multiple event representations, based onthe use of a stack for maintaining state information duringthe processing of the next embedded clause in a recursivemanner.In a more generalized approach, Dominey (2000)exploited the regularity that sentence to meaning mappingis encoded in all languages by word order andgrammatical marking (bound or free) (Bates et al 1982).That model was based on the functional neurophysiologyof cognitive sequence and language processing and anassociated neural network model that has beendemonstrated to simulate interesting aspects of infant(Dominey & Ramus 2000) and adult language processing(Dominey et al 2003).ObjectivesThe goals of the current study are fourfold: First to testthe hypothesis that meaning can be extracted from visualscenes based on the detection of contact and itsparameters in an approach similar to but significantlysimplified from Siskind (2001); Second to determinewhether the model of Dominey (2000) can be extended tohandle embedded relative clauses; Third to demonstratethat these two systems can be combined to performminiature language acquisition; and finally to demonstratethat the combined system can provide insight into thedevelopmental progression in human language acquisitionwithout the necessity of a pre-wired parameterizedgrammar system (Chomsky 1995).The Training DataThe human experimenter enacts and simultaneouslynarrates visual scenes made up of events that occurbetween a red cylinder, a green block and a bluesemicircle or ?moon?
on a black matte table surface.
Avideo camera above the surface provides a video imagethat is processed by a color-based recognition andtracking system (Smart ?
Panlab, Barcelona Spain) thatgenerates a time ordered sequence of the contacts thatoccur between objects that is subsequently processed forevent analysis (below).
The simultaneous narration of theongoing events is processed by a commercial speech-to-text system (IBM ViaVoiceTM).
Speech and vision datawere acquired and then processed off-line yielding a dataset of matched sentence ?
scene pairs that were providedas input to the structure mapping model.
A total of  ~300<sentence, scene> pairs were tested in the followingexperiments.2.
Visual Scenes and analysisFor a given video sequence the visual scene analysisgenerates the corresponding event description in theformat event(agent, object, recipient).Single Event LabelingEvents are defined in terms of contacts betweenelements.
A contact is defined in terms of the time atwhich it occurred, the agent, object, and duration of thecontact.
The agent is determined as the element that had alarger relative velocity towards the other element involvedin the contact.
Based on these parameters of contact, sceneevents are recognized as follows:Touch(agent, object): A single contact, in which (a)the duration of the contact is inferior to touch_duration(1.5 seconds), and (b) the objectis not displaced duringthe duration of the contact.Push(agent, object): A single contact in which (a) theduration of the contact is superior or equal totouch_duration and inferior toake_duration (5 sec), (b)the object is displaced during the duration of the contact,and (c) the agent and object are not in contact at the end ofthe event.Take(agent, object): A single contact in which (a) theduration of contact is superior or equal to take_duration,(b) the bject is displaced during the contact, and (c) theagent and object remain in contact.Take(agent, object, source): Multiple contacts, as theagent takes the object from the source.
For the firstcontact between the agent and the object (a) the durationof contact is superior or equal to t ke_duration, (b) theobject is displaced during the contact, and (c) the agentand object remain in contact.
For the  optional secondcontact between the agent and the source (a) the durationof the contact is inferior to take_duration, and (b) theagent and source do not remain in contact.
Finally,contact between the object and source is broken during theevent.Give(agent, object, recipient):  In this multiplecontact event, the agent first takes the object, and thengives the object to the recipient.
For the first contactbetween the agent and the object (a) the duration ofcontact is inferior to take_duration, (b) the object isdisplaced during the contact, and (c) the agent and objectdo not remain in contact.
For the second contact betweenthe object and the recipient (a) the duration of the contactis superior to take_duration, and (b) the object andrecipient remain in contact.
For the third (optional)contact between the agent and the recipient (a) theduration of the contact is inferior to take_duration andthus the elements do not remain in contact.These event labeling templates form the basis for atemplate matching algorithm that labels events based onthe contact list, similar to the spanning interval and eventlogic of Siskind (2001).Complex ?Hierarchical?
EventsThe events described above are simple in the sensethat there have no hierarchical structure.
This imposesserious limitations on the syntactic complexity of thecorresponding sentences (Feldman et al 1996,Miikkulainen 1996).
The sentence ?The block thatpushed the moon was touched by the triangle?
illustrates acomplex event that exemplifies this issue.
Thecorresponding compound event will be recognized andrepresented as a pair of temporally successive simpleevent descriptions, in this case: push(block, moon), andtouch(triangle, block).
The ?block?
serves as the linkthat connects these two simple events in order to form acomplex hierarchical event.3.
Structure mapping for language learningThe mapping of sentence form onto meaning(Goldberg 1995) takes place at two distinct levels:  Wordsare associated with individual components of eventdescriptions, and grammatical structure is associated withfunctional roles within scene events.
The first level hasbeen addressed  by Siskind (1996), Roy & Pentland(2000) and Steels (2001) and we treat it here in arelatively simple but effective manner.
Our principleinterest lies more in the second level of mapping betweenscene and sentence structure.Word MeaningIn the initial learning phases there is no influence ofsyntactic knowledge and the word-referent associationsare stored in the WordToReferent matrix (Eqn 1) byassociating every word with every referent in the currentscene (a = 0), exploiting the cross-situational regularity(Siskind 1996) that a given word will have a highercoincidence with referent to which it refers than withother r ferents.
This initial word learning contributes tolearning the mapping between sentence and scenes ructure (Eqn.
4, 5 & 6 below).
Then, knowledge of thesyntactic structure, encoded in FormToMeaning can beused to identify the appropriate referent (in the SEA) for agiven word (in the OCA), corresponding to a non-zerovalue of a in Eqn.
1.
In this ?syntactic bootstrapping?
forthe new word ?gugle,?
for example, syntactic knowledgeof Agent-Event-Object structure of the sentence ?Johnpushed the gugle?
can be used to assign ?gugle?
to theobject of push.WordToReferent(i,j) = WordToReferent(i,j) +OCA(k,i) * SEA(m,j) *aFormToMeaning(m,k) (1)Figure 1.
Structure-Mapping Architecture.
Open classwords in OCA are translated to Predicted Referents in thePRA via the WorldToReferent   mapping.
PRA elementsare mapped onto their roles in the SEA by theFormToMeaning mapping, specific to each sentence type.This mapping is retrieved from Construction Inventory,via the ConstructionIndex that encodes the closed classwords that characterize each sentence type.Open vs Closed Class Word CategoriesOur approach is based on the cross-linguisticobservation that open class words (e.g.
nouns, verbs,adjectives and adverbs) are assigned to their thematicrol s based on word order and/or grammatical functionwords or morphemes (Bates et al 1982).
Newborn infantsare sensitive to the perceptual properties that distinguishthese two categories (Shi et al 1999), and in adults, thesecategories are processed by dissociableneurophysiological systems (Brown et al 1999).Visual SceneAnalysisActionAgentObjectRecipientConstruction I ventoryClosedclasswordsConstructionIndexWordToReferentOpen ClassArray (OCA)FormToMeaningPredictedReferentsArray (PRA)Speech InputProcessingScene  Event Array(SEA)Similarly, artificial neural networks can also learn to makethis function/content distinction (Morgan et al 1996).Thus, for the speech input that is provided to the learningmodel open and closed class words are directed toseparate processing streams that preserve their order andidentity, as indicated in Figure 1.Note that by making this dissociation between openand closed class elements, the grammar learning problemis substantially simplified.
Again, it is thus of interest thatnewborn infants can perform this lexical categorization(Shi et al 1999), and we have recently demonstrated thata recurrent network of leaky integrator neurons cancategorize open and closed class words based on thestructure of the F0 component of the speech signal inFrench and English (Blanc, Dodane & Dominey 2003).Mapping Sentence to MeaningIn terms of the architecture in Figure 2, this mappingcan be characterized in the following successive steps.First, words in the Open Class Array are decoded intotheir corresponding scene referents (via theWordToReferent mapping) to yield the PredictedReferents Array that contains the translated words whilepreserving their original order from the OCA (Eqn 2).ni 1PRA(k,j) = OCA(k,i) * WordToReferent(i,j)=?
(2)Next, each sentence type will correspond to a specificform to meaning mapping between the PRA and the SEA.encoded in the FormToMeaning array.
The problem willbe to retrieve for each sentence type, the appropriatecorresponding FormToMeaning mapping.
To solve thisproblem, we recall that each sentence type will have aunique constellation of closed class words and/or boundmorphemes (Bates et al 1982) that can be coded in aConstructionIndex (Eqn.3) that forms a unique identifierfor each sentence type.
Thus, the appropriateFormToMeaning mapping for each sentence type can beindexed in ConstructionInventory by its correspondingConstructionIndex.ConstructionIndex = fcircularShift(ConstructionIndex,FunctionWord) (3)The link between the ConstructionIndex and thecorresponding FormToMeaning mapping is established asfollows.
As each new sentence is processed, we firstreconstruct the specific FormToMeaning mapping for thatsentence (Eqn 4), by mapping words to referents (in PRA)and referents to scene elements (in SEA).
The resulting,FormToMeaningCurrent encodes the correspondencebetween word order (that is preserved in the PRA Eqn 2)and thematic roles in the SEA.
Note that the quality ofFormToMeaningCurrent will depend on the quality ofacquir d word meanings in WordToReferent.
Thus,syntactic learning requires a minimum baseline ofs mantic knowledge.ni=1FormToMeaningCurrent(m,k) =PRA(k,i)*SEA(m,i)?
(4)G ven the FormToMeaningCurrent mapping for thecurrent sentence, we can now associate it in theConstructionInventory with the corresponding functionword configuration or ConstructionIndex for thatsentence, expressed in (Eqn 5).ConstructionInventory(i,j) = ConstructionInventory(i,j)+ ConstructionIndex(i)* FormToMeaning-Current(j) (5)Finally, once this learning has occurred, for newsentences we can now extract the FormToMeaningmapping from the learned ConstructionInventory by usingthe ConstructionIndex as an index into this associativememory, illustrated in Eqn.
6.ni=1FormToMeaning(i) =ConstructionInventory(i,j) * ConstructinIndex(j)?
(6)To accommodate the dual scenes for complex eventsEqns.
4-7 are instantiated twice each, to represent the twocomponents of the dual scene.
In the case of simplescenes, the second component of the dual scenerepresentation is null.We evaluate performance by using theWordToReferent and FormToMeaning knowledge toconstruct for a given input sentence the ?predicted scene?.That is, the model will construct an internal representationof the scene that should correspond to the input sentence.This is achieved by first converting the Open-Class-Arrayinto its corresponding scene items in the Predicted-Referents-Array as specified in Eqn.
2.
The referents arethen re-ordered into the proper scene representation viaapplication of the FormToMeaning transformation asdescribed in Eqn.
7.PSA(m,i) = PRA(k,i) * FormToMeaning(m,k) (7)When learning has proceeded correctly, the predictedscene array (PSA) contents should match those of thescene event array (SEA) that is directly derived from inputto the model.
We then quantify performance error interms of the number of mismatches between PSA andSEA.4.
Experimental resultsHirsh-Pasek & Golinkof (1996) indicate that childrencan use knowledge of word meaning to acquire a fixedSVO template around 18 months, and then expand this tonon-canonical sentence forms around 24+ months.Tomasello (1999) similarly indicates that fixedgrammatical constructions will be used initially, and thatthese will then provide the basis for the development ofmore generalized constructions (Goldberg 1995).
Thefollowing experiments attempt to follow this type ofdevelopmental progression.A.
Learning of Active Forms for Simple Events1.
Active:  The block pushed the triangle.2.
Dative:  The block gave the triangle to the moon.For this experiment, 17 scene/sentence pairs weregenerated that employed the 5 different events, andnarrations in the active voice, corresponding to thegrammatical forms 1 and 2.
The model was trained for 32passes through the 17 scene/sentence pairs for a total of544 scene/sentence pairs.
During the first 200scene/sentence pair trials, a in Eqn.
1 was 0 (i.e.
nosyntactic bootstrapping before syntax is adquired), andthereafter it was 1.
This was necessary in order to avoidthe random effect of syntactic knowledge on semanticlearning in the initial learning stages.
The trained systemdisplayed error free performance for all 17 sentences, andgeneralization to new sentences that had not previouslybeen tested.B.
Passive formsThis experiment examined learning active and passivegrammatical forms, employing grammatical forms 1-4.Word meanings were used from Experiment A, so onlythe structural FormToMeaning mappings were learned.3.
Passive:  The triangle was pushed by the block.4.
Dative Passive:  The moon was given to the triangleby the block.Seventeen new scene/sentence pairs were generatedwith active and passive grammatical forms for thenarration.
Within 3 training passes through the 17sentences (51 scene/sentence pairs), error freeperformance was achieved, with confirmation of error freegeneralization to new untrained sentences of these types.The rapid learning indicates the importance of lexicon inestablishing the form to meaning mapping for thegrammatical constructions.C.
Relative forms for Complex EventsHere we consider complex scenes narrated by sentenceswith relative clauses.
Eleven complex scene/sentencepairs were generated with narration corresponding to thegrammatical forms indicated in 5 ?
10:5.
T e block that pushed the triangle touched the moon.6.
The block pushed the triangle that touched the moon.7.
T e block that pushed the triangle was touched by themoon.8.
block pushed the triangle that was touched themoon.9.
The block that was pushed by the triangle touched themoon.10.
The block was pushed by the triangle that touched themoon.After presentation of 88 scene/sentence pairs, the modelperformed without error for these 6 grammatical forms,and displayed error-free generalization to new sentencesthat had not been used during the training for all sixgrammatical forms.D.
Combined Test with and Without LexiconA total of 27 scene/sentence pairs, used in ExperimentsB and C, were employed that exercised the ensemble ofgrammatical forms 1 ?
10 using the learnedWordToReferent mappings.
After exposure to 162scene/sentence pairs the model performed and generalizedwithout error.
When this combined test was performedwithout the pre-learned lexical mappings inWordToReferent, the system failed to converge,illustrating the advantage of following the developmentalprogression from lexicon to simple to complexgrammatical structure.
This also illustrates theimportance of interaction between syntactic and semanticknowledge that is treated in more detail in Dominey(2000).E.
Some Scaling IssuesA small lexicon and construction inventory are used toillustrate the system behavior.
Based on the independantrepresentation formats, the architecture should scale well.The has now been tested with a larger lexicon, and haslearned over 35 grammatical constructions.
The systemshould extend to all languages in which sentence tomeaning mapping is encoded by word order and/orgrammatical marking (Bates et al 1982).
In the currentstudy, deliberate human event production yieldedessentially perfect recognition, though the learning modelis relatively robust  (Dominey 2000) to elevated scenee ror rates.F.
Representing Hierarchical StructureThe knowledge of the system is expressed in theWorldToReferent and FormToMeaning matrices.
In orderto deal with complex sentences with embedded clauses, itis necessary to use this same knowledge at different levelsof the hierarchy.
For this, a "branching mechanism" isnecessary, that ordinates the input and output vectorscorresponding to meaning and word events.
An effectivesolution to that problem is to learn the branching for eachconstruction as we have done.
However, a real account ofthe human faculty of recursion should be both general (i.e.it should apply to any reasonably complex structure) andplausible (i.e.
the branching mechanism should beconnectionist).
In order to provide this level of generality,neural models need to include a logical "stack" (cfMiikkulainen 1996), in order to process the context ofembedded sentences.
Complex structures themselves maybe represented in a connectionist way, using the RecursiveAuto-Associative Memory (Pollack, 1990).
In (Voegtlinand Dominey 2003), we proposed a representation systemfor complex events, that is both generative (it can handleany structure) and systematic (it can generalize, and itdoes so in a compositional way).
This system could beused here, as its representation readily provides a case-role system.
The advantages are twofold.
First, thebranching mechanism is implemented in a neurallyrealistic way.
Second, the recursion capability of thesystem will allow it to apply its knowledge to anysentence form, whether known or new.
Future researchwill address this issue.ConclusionThe current study demonstrates (1) that the perceptualprimitive of contact (available to infants at 5 months), canbe used to perform event description in a manner that issimilar to but significantly simpler than Siskind (2001),(2) that a novel implementation of principles fromconstruction grammar can be used to map sentence formto these meanings together in an integrated system, (3)that relative clauses can be processed in a manner that issimilar to, but requires less specific machinery (e.g.
nostack) than that in Miikkalanian (1996), and finally (4)that the resulting system displays robust acquisitionbehavior that reproduces certain observations fromdevelopmental studies with very modest ?innate?language specificity.Note that one could have taken the same approach byintegrating Siskind?s (2001) full event system, andMiikkulainen?s (1996) embedded case-role system.
Eachof these however required significant architecturalcomplexity to accomplish the full job.
The current goalwas to identify minimal event recognition and form-to-meaning mapping capabilities that could be integrated intoa coherent system that performs at the level of a humaninfant in the first years of  development when theconstruction inventory is being built up.
This forms thebasis for the infant?s subsequent ability to de- and re-compose these constructions in a truly compositionalmanner, a topic of future research.AcknowledgmentsSupported by the EuroCores OMLL project, theFrench ACI Integrative and Computational NeuroscienceProject, and the HFSP MCILA Project.Appendix: Sentence and scene descriptionsThe <sentence, meaning> pairs for training and testing areconstructed from the following templates.
The lexiconconsists of 5 nouns (cylinder, moon, block, cat, dog), 5verbs (touch, push, take, give, say), and 8 function words(to, by, from, was, that, it, itself, and)1.A.1 Single event scenes1.
Agent verb object.
(Active)Verb(agent, object)2.
Object was verbed by agent.
(Passive)Verb(agent, object).3.
Agent verbed object to recipient.
(Dative)Verb(agent, object, recipient)4.
Object was verbed to recipient by agent.
(Dativepassive)Action1(agent1, object2, recipient3).5.
Agent1 action1 recipient3 object2.Verb(agent, object, recipient).A.2 Double event relatives6.
Agent1 that verb1ed object2 verb2ed object3.
(Relativeagent).Action1(agent1,object2), Action2(agent1,object3)7.
Object3 was action2ed by  agent1 that action1edobject2.
(Relative object).Action1(agent1,object2), Action2(agent1,object3)8.
Agent1 that action21ed object2 was action22ed byagent3Action1(agent1,object2), Action2(agent3,object1)9.
Agent3 action2ed object1 that action1ed object2Action1(agent1,object2), Action2(agent3,object1)10.
Obj2 that was action1ed by agent1 action2ed obj3Action1(agent1,object2), Action2(agent2,object3)11.
Obj3 was act2d by agent2 that was act1d by agent1Action1(agent1,object2), Action2(agent2,object3)12.
Obj2 that was action1ed by agent1 was action2ed byg3Action1(agent1,object2), Action2(agent3,object2)13.  ag3 act22ed obj2 that was act21ed by ag1Action21(agent1,object2), Action22(agent3,object2)1 Possible scaling issues for WordToWorld mappings are not of concernhere.
If WordToWorld is well specified, then lexicon size has noinfluence on SentenceToWorld mapping.14.
Ag1 that act1ed obj2 act2ed obj3 to recip4Action1(agent1,object2),Action2(agent1,object3,recipient4)15.
Obj3 was act32ed to recip4 by ag1 that act21ed obj2Action1(agent1,object2),Action2(agent1,object3,recipient4)16.
Agent1 that action1ed object2 was action2ed to recip4by ag3Action1(agent1,object2),Action2(agent3,object1,recipient4)17.
Ag3 act2ed obj4 to recip1 that act1ed obj2Action1(agent1,object2),Action2(agent3,object4,recipient1)18.
Obj4 was act2ed from ag3 to recip1 that act1ed obj2Action1(agent1,object2),Action2(agent3,object4,recipient1)19.
Obj2 that was act1ed by ag1 act2ed obj3 to recip4Action1(agent1,object2),Action2(agent2,object3,recipient4)20.
Ag3 act2ed ob4 to rec2 that was act1ed by ag1Action1(agent1,object2),Action2(agent3,object4,recipient2)21.
Ag1 that act1ed obj2 to rec3 act2ed obj4Action1(agent1,object2,recipient3),Action2(agent1,object4)22.
Obj4 was act2ed by ag1 that act1ed ob2 to rec3Action1(agent1,object2,recipient3),Action2(agent1,object4)23.
Ag4 act2ed ob1 that act1ed ob2 to rec3Action1(agent1,object2,recipient3),Action2(agent4,object1)24.
Ob1 that act1ed ob2 to rec3 was act2ed by ag4Action1(agent1,object2,recipient3),Action2(agent4,object1)25.
Ag2 that was act1ed by ag1 to rec3 act2ed ob4Action1(agent1,object2,recipient3),Action2(agent2,object4)26.
Ag4 act2ed obj2 that was act1ed by ag1 to rec3Action1(agent1,object2,recipient3),Action2(agent4,object2)A.3  Dual event Conjoined27.
Agent1 action1 object1 and object2.
(Activeconjoined object)Action1(agent1, object1),Action1(agent1, object2)28.
Agent1 and agent3 action1ed object2.
(Activeconjoined agent)Action1(agent1, object2),Action1(agent3, object2)29.
Agent1 action1ed object2 and action2 object3.
(Conjoined)Action1(agent1, object2),Action2(agent1, object3)A.4 Dual Event Reflexive30.
Agent1 action1r that agent2 action2ed object3.
(Simple reflexive)Action1r2(agent1),Action2(agent2, object3).31.
Agent1 action1ed itself.
(Simple active reflexive)Action1(agent1, agent1).32.
Agent1 action1r that agent2 action2ed itself.
(Reflexive simple noun phrase).Action1r(agent1),Action2(agent2, agent2).33.
Agent1 action1r that agent2 action2ed it.
(Pronoun simple noun phrase).Action1r(agent1),Action2(agent2, agent1).34.
Agent1 action1r that it action1ed object2.Action1r(agent1),Action2(agent1, object2).35.
Agent1 action1r that object3 was action2ed by agent2.Action1r(agent1),Action2(agent2, object3).36.
Agent1 action1r that agent2 action2ed object3 torecipient4.Action1r(agent1),Action2(agent2, object3, recipient4).37.
Agent1 action1r agent2 action2ed object3 torecipient4.Action1r(agent1),Action2(agent2, object3, recipient4).38.
Object2 object3 were action1ed to recipient4 byagent1.Action1(agent1, object2, recipient4), Action1(agent1,object3, recipient4)ReferencesBates E, McNew S, MacWhinney B, Devescovi A, SmithS (1982) Functional constraints on sentence processing:A cross linguistic study, Cognition (11) 245-299.Blanc J-M, Dodane C, Dominey PF (2003)  TemporalProcessing for Syntax Acquisition: A simulation study.Ms submitted to C gSci 2003.Brown CM, Hagoort P, ter Keurs M (1999)Electrophysiological signatures of visual lexicalprocessing :  Open- and closed-class words.
Journal ofCognitive Neuroscience.
11 :3, 261-281Chomsky N. (1995) The Minimalist Program.
MITChang NC, Maia TV (2001) Grounded learning ofgrammatical constructions, AAAI Spring Symp.
OnLearning Grounded Representations, Sta ford CA.Dominey PF, Ramus F (2000) Neural network processingof natural lanugage: I.
Sensitivity to serial, temporal andabstract structure of language in the infant.
Lang.
andCognitive Processes,  15(1) 87-1272 Corresponds to reflexive verbs such as ?said,?
or ?believed.
?Dominey PF (2000) Conceptual Grounding in SimulationStudies of Language Acquisition, Evolution ofCommunication, 4(1), 57-85.Dominey PF, Hoen M, Lelekov T, Blanc JM  (2003)Neurological basis of language in sequential cognition:Evidence from simulation, aphasia and ERP studies, (inpress) Brain and LanguageElman J (1990) Finding structure in time.
CognitiveScience, 14:179-211.Feldman JA, Lakoff G, Stolcke A, Weber SH (1990)Miniature language acquisition: A touchstone forcognitive science.
In Proceedings of the 12th Ann Conf.Cog.
Sci.
Soc.
686-693, MIT, Cambridge MAFeldman J., G. Lakoff, D. Bailey, S. Narayanan, T.Regier, A. Stolcke (1996).
L0: The First Five Years.Artificial Intelligence Review, v10 103-129.Goldberg A (1995) Constructions.
U Chicago Press,Chicago and London.Hirsh-Pasek K, Golinkof RM (1996) The origins ofgrammar: evidence from early languagecomprehension.
MIT Press, Boston.Kotovsky L, Baillargeon R,  The development ofcalibration-based reasoning about collision events inyoung infants.
1998, Cognition, 67, 311-351Langacker, R. (1991).
Foundations of CognitiveGrammar.
Practical Applications, Volume 2.
StanfordUniversity Press, Stanford.Mandler J (1999) Preverbal representations and language,in P. Bloom, MA Peterson, L Nadel and MF Garrett(Eds) Language and Space, MIT Press,  365-384Miikkulainen R (1996) Subsymbolic case-role analysis ofsentences with embedded clauses.
Cognitive Science,20:47-73.Morgan JL, Shi R, Allopenna P (1996) Perceptual basesof rudimentary grammatical categories: Toward abroader conceptualization of bootstrapping, pp 263-286,in Morgan JL, Demuth K (Eds) Signal to syntax,Lawrence Erlbaum, Mahwah NJ, USA.Pollack JB (1990) Recursive distributed representations.Artificial Intelligence, 46:77-105.Roy D, Pentland A (2002).
Learning Words from Sightsand Sounds: A Computational Model.
CognitiveScience, 26(1), 113-146.Shi R., Werker J.F., Morgan J.L.
(1999) Newborn infants'sensitivity to perceptual cues to lexical and grammaticalwords, Cognition, Volume 72, Issue 2, B11-B21.Siskind JM (1996) A computational study of cross-situational techniques for learning word-to-meaningmappings, Cognition (61) 39-91.Siskind JM (2001) Grounding the lexical semantics ofverbs in visual perception using force dynamics andevent logic.
Journal of AI Research (15) 31-90Steels, L. (2001) Language Games for AutonomousRobots.
IEEE Intelligent Systems, vol.
16, nr.
5, pp.
16-22, New York: IEEE Press.Stolcke A, Omohundro SM (1994) Inducing probablisticgrammars by Bayseian model merging/ In GrammaticalInference and Applications: Proc.
2nd Intl.
Colloq.
OnGrammatical Inference, Springer Verlag.Talmy L (1988) Force dynamics in language andcognition.
Cognitive Science, 10(2) 117-149.Tomasello M (1999) The item-based nature of children'searly syntactic development, Trends in CognitiveScience, 4(4):156-163Voegtlin T, Dominey PF (2003) Linear recursivedistributed representations (submitted).
