Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 26?31,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA Domain Adaptation Regularization for Denoising AutoencodersSt?ephane Clinchant, Gabriela Csurka and Boris ChidlovskiiXerox Research Centre Europe6 chemin Maupertuis, Meylan, FranceFirstname.Lastname@xrce.xerox.comAbstractFinding domain invariant features is crit-ical for successful domain adaptation andtransfer learning.
However, in the case ofunsupervised adaptation, there is a signif-icant risk of overfitting on source trainingdata.
Recently, a regularization for domainadaptation was proposed for deep modelsby (Ganin and Lempitsky, 2015).
We buildon their work by suggesting a more appro-priate regularization for denoising autoen-coders.
Our model remains unsupervisedand can be computed in a closed form.On standard text classification adaptationtasks, our approach yields the state of theart results, with an important reduction ofthe learning cost.1 IntroductionDomain Adaptation problem arises each timewhen we need to leverage labeled data in one ormore related source domains, to learn a classifierfor unseen data in a target domain.
It has beenstudied for more than a decade, with applicationsin statistical machine translation, opinion mining,part of speech tagging, named entity recognitionand document ranking (Daum?e and Marcu, 2006;Pan and Yang, 2010; Zhou and Chang, 2014).The idea of finding domain invariant featuresunderpins numerous works in domain adapta-tion.
A shared representation eases predictiontasks, and theoretical analyses uphold such hy-potheses (Ben-David et al, 2007).
For instance,(Daum?e and Marcu, 2006; Daum?e, 2009) haveshown that replicating features in three main sub-spaces (source, common and target) yields im-proved accuracy as the classifier can subsequentlypick the most relevant common features.
Withthe pivoting technique (Blitzer et al, 2006; Panet al, 2010), the bag of words features are pro-jected on a subspace that captures the relationsbetween some central pivot features and the re-maining words.
Similarly, there are several ex-tensions of topic models and matrix factorizationtechniques where the latent factors are shared bysource and target collections (Chen and Liu, 2014;Chen et al, 2013).More recently, deep learning has been pro-posed as a generic solution to domain adaptationand transfer learning problems by demonstratingtheir ability to learn invariant features.
On onehand, unsupervised models such as denoising au-toencoders (Glorot et al, 2011) or models builton word embeddings (Bollegala et al, 2015) areshown to be effective for domain adaptation.
Onthe other hand, supervised deep models (Long etal., 2015) can be designed to select an appropri-ate feature space for classification.
Adaptation toa new domain can also be performed by fine tun-ing the neural network on the target task (Chopraet al, 2013).
While such solutions perform rel-atively well, the refinement may require a signif-icant amount of new labeled data.
Recent workby (Ganin and Lempitsky, 2015) has proposed abetter strategy; they proposed to regularize inter-mediate layers with a domain prediction task, i.e.deciding whether an object comes from the sourceor target domain.This paper proposes to combine the domain pre-diction regularization idea of (Ganin and Lempit-sky, 2015) with the denoising autoencoders.
Moreprecisely, we build on stacked Marginalized De-noising Autoencoders (sMDA) (Chen et al, 2012),which can be learned efficiently with a closed formsolution.
We show that such domain adaptationregularization keeps the benefits of the sMDA andyields results competitive to the state of the art re-sults of (Ganin and Lempitsky, 2015).262 Target Regularized MDAStacked Denoising Autoencoders (sDA) (Vincentet al, 2008) are multi-layer neural networkstrained to reconstruct input data from partial ran-dom corruption.
The random corruption, calledblank-out noise or dropout, consists in randomlysetting to zero some input nodes with probabilityp; it has been shown to act as a regularizer (Wa-ger et al, 2013).
The sDA is composed of aset of stacked one-layer linear denoising autoen-coder components, which consider a set of N in-put documents (represented by d-dimensional fea-tures xn) to be corrupted M times by random fea-ture dropout and then reconstructed with a linearmapping W ?
Rd?dby minimizing the squaredreconstruction loss:L(W) =N?n=1M?m=1||xn?
x?nmW||2.
(1)As explicit corruption comes at a high com-putational cost, (Chen et al, 2012) propose tomarginalize the loss (1) by considering the limit-ing case when M ??
and reducing de facto thelearning cost.
The main advantage of this methodis a closed form solution for W, which dependsonly on the uncorrupted inputs (xn) and the drop-out probability.
Several Marginalized DenoisingAutoencoders (MDA) can be then stacked togetherto create a deep architecture where the representa-tions of the (l ?
1)thlayer serves as inputs to thelthlayer1.In the case of domain adaptation, the idea is toapply MDA (or sMDA) to the union of unlabeledsource Xsand target Xtexamples.
Then, a stan-dard learning algorithm such as SVM or LogisticRegression is trained on the labeled source data us-ing the new feature representations (xsnW) whichcaptures better the correlation between the sourceand target data.In Figure 1, we illustrate the effect of the MDA;it shows the relation between the word log docu-ment frequency (x-axes) and the expansion massdefined as the total mass of words transformedinto word i by MDA and represented by?jWji.We can see that the mapping W learned by MDAis heavily influenced by frequent words.
In fact,MDA behaves similarly to document expansionon text documents: it adds new words with a1Between layers, in general, a non linear function such astanh or ReLU is applied.Figure 1: Relation between log document fre-quency and expansion mass.
One dot representsone word.very small frequency and sometimes words with asmall negative weight.
As the figure shows, MDApromotes common words (despite the use of tf-idfweighting scheme) that are frequent both in sourceand target domains and hence aims to be domaininvariant.This is in line with the work of (Ganin et al,2015).
To strengthen the invariance effect, theysuggested a deep neural architecture which em-beds a domain prediction task in intermediate lay-ers, in order to capture domain invariant features.In this paper we go a step further and refine thisargument by claiming that we want to be domaininvariant but also to be as close as possible to thetarget domain distribution.
We want to match thetarget feature distribution because it is where theclassification takes place.We therefore propose a regularization for thedenoising autoencoders, in particular for MDA,with the aim to make source data resemble the tar-get data and hence to ease the adaptation.We describe here the case of two domains, butit can be easily generalized to multiple domains.Let D be the vector of size N indicating for eachdocument its domain, e.g.
taking values of ?
?1?for source and ?+1?
for target examples.
Let c bea linear classifier represented as a d dimensionalvector trained to distinguish between source andtarget, e.g.
a ridge classifier that minimizes the lossR(c, ?)
= ||D?Xc>||2+ ?||c||2.We guide the mapping W in such a way thatthe denoised data points xW go towards the targetside, i.e.
xWc>= 1 for both source and target27samples.
Hence, we can extend each term of theloss (1) as follows:||xn?
?xnmW||2+ ?||1??xnmWc>||2.
(2)The first term here represents the reconstructionloss of the original input, like in MDA.
In the sec-ond term,?xmnWc>is the domain classifier pre-diction for the denoised objects forced to be closeto 1, the target domain indicator, and ?
> 0.Let?X be the concatenation ofM replicated ver-sion of the original data X, and?X be the matrixrepresentation of the M corrupted versions.
Tak-ing into account the domain prediction term, theloss can be written as:LR(W) = ||?X?
?XW||2+ ?||?R?
?XWc>||2,(3)where R is a vector of sizeN , indicating a desiredregularization objective, and?R its M -replicatedversion.
Loss (3) represents a generic form to cap-ture three different ideas:?
If R = 1, the model incites the reconstructedfeatures moving towards target specific fea-tures.?
If R = ?D, the model aims to promote do-main invariant features as in (Ganin et al,2015).?
If R = [0;1], where 0 values are used forsource data, the model penalizes the sourcespecific features.Learning the mapping W. (Chen et al, 2012)observed that the random corruption from equa-tion (1) could be marginalized out from the re-construction loss, yielding a unique and optimalsolution.
Furthermore, the mapping W can be ex-pressed in closed form as W = PQ?1, with:Qij=[Sijqiqj, if i 6= j,Sijqi, if i = j,Pij= Sijqj, (4)where2q = [1 ?
p, .
.
.
, 1 ?
p] ?
Rd, p is thedropout probability, and S = XXTis the covari-ance matrix of the uncorrupted data X.The domain regularization term in (3) isquadratic in W, the random corruption can still be2In contrast to (Chen et al, 2012), we do not add a biasfeature so that the domain and MDA have the same dimen-sionality.
Experiments shown no impact on the performance.marginalized out and the expectations obtained inclosed form.
Indeed, the mapping W which mini-mizes the expectation of1MLR(W) is the solutionof the following linear system3:(P + ?(1?
p)X>Rc>)(I + ?cc>)?1= QW.
(5)In (5), parameter ?
controls the effect of theproposed target regularization in the MDA andthe regularization on c is controlled by parame-ter ?.
This approach preserves the good propertiesof MDA, i.e.
the model is unsupervised and canbe computed in closed form.
In addition, we caneasily stack several layers together and add non-linearities between layers.3 ExperimentsWe conduct unsupervised domain adaptation ex-periments on two standard collections: the Ama-zon reviews (Blitzer et al, 2011) and the 20News-group (Pan and Yang, 2010) datasets.From the Amazon dataset we consider the fourmost used domains: dvd (D), books (B), electron-ics (E) and kitchen (K), and adopt the settings of(Ganin et al, 2015) with the 5000 most frequentcommon features selected for each adaptation taskand a tf-idf weighting.
We then use the LogisticRegression (LR) to classify the reviews.Our previous experiments with MDA revealedthat the MDA noise probability p needs to be setto high values (e.g.
0.9).
A possible explanation isthat document representations are already sparseand adding low noise has no effect on the featuresalready equal to zero.
Figure 2 shows the averageaccuracy for the twelve Amazon tasks, when wevary the noise probability p.In addition, we observed that a single layerwith a tanh activation function is sufficient toachieve top performance; stacking several layersand/or concatenating the outputs with the originalfeatures yields no improvement but increases thecomputational time.The dropout probability p is fixed to 0.9 in allexperiments, for both the MDA baseline and ourmodel; we test the performance with a single layerand a tanh activation function.
Stacking severallayers is left for future experiments.
Parameters?
and ?
are tuned on a grid of values4by crossvalidation on the source data.
In other words, we3The derivation is not included due to space limitation.4?
?
[.1, 1, 50, 100, 150, 200, 300], ?
?
[.01, .1, 1, 10].280.2 0.4 0.6 0.8noise p0.7750.7800.7850.7900.7950.8000.8050.8100.815AccuracyMDAMDA+TRFigure 2: Impact of the noise parameter p on theaverage accuracy for the 12 Amazon adaptationtasks.
Both MDA and its extension with the reg-ularization (MDA+TR) perform better with a highdropout-out noise.
Here MDA+TR is run withfixed parameters ?
= 100 and ?
= 1.select the LR parameters and the parameters ?, ?by cross validating the classification results usingonly the ?reconstructed?
source data; for estimat-ing W we used the source with an unlabeled tar-get set (excluded at test time).
This correspondsto the setting used in (Ganin et al, 2015), with thedifference that they use SVM and reverse cross-validation5.Table 3 shows the results for twelve adapta-tion tasks on the Amazon review dataset for thefour following methods.
Columns 1 and 2 showthe LR classification results on the target set forthe single layer MDA and the proposed targetregularized MDA (MDA+TR).
Column 3 reportsthe SVM result on the target from (Ganin et al,2015).
They used a 5 layers sMDA where the5 outputs are concatenated with input to generate30,000 features, on which the SVM is then trainedand tested (G-sMDA).
Finally, column 4 showsthe current state of the art results obtained withDomain-Adversarial Training of Neural Networks(DA NN) instead of SVM (Ganin et al, 2015).Despite a single layer and LR trained on thesource only, the MDA baseline (80.15% on aver-age) is very close to the G-sMDA results obtainedwith 5 layer sMDA and 6 times larger feature set(80.18%).
Furthermore, adding the target regular-ization allows to significantly outperform in many5It consists in using self training on the target validationset and calibrating parameters on a validation set from thesource labeled data.S T MDA MDA+TR G-sMDA DA NND B 81.1 81.4 82.6 82.5D K 84.1 85.3 84.2 84.9D E 76.0 81.1 73.9 80.9B D 82.7 81.7 83.0 82.9B K 79.8 81.8 82.1 84.3B E 75.9 79.3 76.6 80.4K D 78.5 79.0 78.8 78.9K B 77.0 77.0 76.9 71.8K E 87.2 87.4 86.1 85.6E D 78.5 78.3 77.0 78.1E B 73.3 75.1 76.2 77.4E K 87.7 88.2 84.7 88.1Avg 80.15 81.27 80.18 81.32Table 1: Accuracies of MDA, MDA+TR, G-sMDA and DA NN on the Amazon review dataset.Underline indicates improvement over the base-line MDA, bold indicates the highest value.cases the baseline and the state of the art DA NN.We note that our method has a much lower cost,as it uses the closed form solution for the recon-struction and a simple LR on the reconstructedsource data, instead of domain-adversarial train-ing of deep neural networks.We also look at the difference between the pre-viously introduced expansion mass for the MDAand MDA+TR.
In the adaptation task from dvd (D)to electronics (E), the words for which the masschanged the most are the following6: worked,to use, speakers, i have, work, mouse, bought, ca-ble, works, quality, unit, ipod, price, number ,sound, card, phone, use, product, my.
These wordsare mostly target specific and the results confirmthat they get promoted by the new model.Our model favors features which are more likelyto appear in target examples, while DA NN seeksdomain invariant features.
Despite this difference,the two approaches achieve similar results.
It issurprising, and we argue that eventually both ap-proaches penalize source specific features.
To testthis hypothesis, we use MDA with R = [0;1](case 3) that penalizes source specific features andwe obtain again similar performances.Finally, we test our approach on the 20News-group adaptation tasks described in (Pan andYang, 2010).
We first filter out rare words andkeep at most 10,000 features.
Then, we apply bothMDA and MDA+TR as above.
Table 3 shows re-sults for ten adaptation tasks.
As we can see, in allcases the target regularization (MDA+TR) helpsimprove the classification accuracy.6In ascending order of the differences.29Task MDA MDA+TRcomp vs sci 73.69 73.38sci vs comp 69.39 69.92rec vs talk 72.54 85.10talk vs rec 72.30 76.22rec vs sci 77.25 82.70sci vs rec 79.95 80.00sci vs talk 78.94 79.26talk vs sci 77.17 77.91comp vs rec 89.84 89.66rec vs comp 89.92 90.29Avg 78.1 80.40Table 2: Accuracies of MDA and MDA+TR on20Newsgroup adaptation tasks.4 ConclusionThis paper proposes a domain adaptation regu-larization for denoising autoencoders, in particu-lar for marginalized ones.
One limitation of ourmodel is the linearity assumption for the domainclassifier, but for textual data, linear classifiers arethe state of the art technique.
As new words andexpressions become more frequent in a new do-main, the idea of using the dropout regularizationthat forces the reconstruction of initial objects toresemble target domain objects is rewarding.
Themain advantage of the new model is in the closedform solution.
It is also unsupervised, as it doesnot require labeled target examples and yields per-formance results comparable with the current stateof the art.ReferencesShai Ben-David, John Blitzer, Koby Crammer, andFernando Pereira.
2007.
Analysis of representa-tions for domain adaptation.
In Advances in NeuralInformation Processing Systems, NIPS ConferenceProceedings, Vancouver, British Columbia, Canada,December 4-7, 2006., volume 19.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of Conference onEmpirical Methods in Natural Language Process-ing, EMNLP, 22-23 July 2006, Sydney, Australia.John Blitzer, Sham Kakade, and Dean P. Foster.
2011.Domain adaptation with coupled subspaces.
In Pro-ceedings of the Fourteenth International Conferenceon Artificial Intelligence and Statistics, AISTATS,Fort Lauderdale, USA, April 11-13, 2011.Danushka Bollegala, Takanori Maehara, and Ken-ichiKawarabayashi.
2015.
Unsupervised cross-domainword representation learning.
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics, ACL, July 26-31, 2015, Beijing,China, volume 1.Zhiyuan Chen and Bing Liu.
2014.
Topic modelingusing topics from many domains, lifelong learningand big data.
In Proceedings of the 31st Interna-tional Conference on Machine Learning, ICML Be-jing, 21-16 June 2014.M.
Chen, Z. Xu, K. Q. Weinberger, and F. Sha.
2012.Marginalized denoising autoencoders for domainadaptation.
ICML, arXiv:1206.4683.Zhiyuan Chen, Arjun Mukherjee, Bing Liu, MeichunHsu, Malu Castellanos, and Riddhiman Ghosh.2013.
Leveraging multi-domain prior knowledge intopic models.
In Proceedings of the Twenty-ThirdInternational Joint Conference on Artificial Intelli-gence, IJCAI ?13, pages 2071?2077.
AAAI Press.S.
Chopra, S. Balakrishnan, and R. Gopalan.
2013.DLID: Deep learning for domain adaptation by in-terpolating between domains.
In Proceedings of the30th International Conference on Machine Learn-ing, ICML, Atlanta, USA, 16-21 June 2013.H.
Daum?e and D. Marcu.
2006.
Domain adaptationfor statistical classifiers.
JAIR, 26:101?126.H.
Daum?e.
2009.
Frustratingly easy domain adapta-tion.
CoRR, arXiv:0907.1815.Yaroslav Ganin and Victor S. Lempitsky.
2015.
Un-supervised domain adaptation by backpropagation.In Proceedings of the 32nd International Confer-ence on Machine Learning, ICML, Lille, France, 6-11 July 2015, pages 1180?1189.Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,Pascal Germain, Hugo Larochelle, Franc?ois Lavi-olette, Mario Marchand, and Victor S. Lempitsky.2015.
Domain-adversarial training of neural net-works.
CoRR, abs/1505.07818.X.
Glorot, A. Bordes, and Y. Bengio.
2011.
Domainadaptation for large-scale sentiment classification:A deep learning approach.
In Proceedings of the28th International Conference on Machine Learn-ing, ICML, Bellevue, Washington, USA, June 28-July 2, 2011.M.
Long, Y. Cao, , J. Wang, and M. Jordan.
2015.Learning transferable features with deep adapta-tion networks.
In Proceedings of the 32nd Inter-national Conference on Machine Learning, ICML2015, Lille, France, 6-11 July 2015.S.
J. Pan and Q. Yang.
2010.
A survey on transferlearning.
Knowledge and Data Engineering, IEEETransactions on, 22(10):1345?1359.30Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, QiangYang, and Zheng Chen.
2010.
Cross-domain sen-timent classification via spectral feature alignment.In Proceedings of the 19th International Conferenceon World Wide Web, WWW, New York, NY, USA.ACM.P.
Vincent, H. Larochelle, Y. Bengio, and P.-A.
Man-zagol.
2008.
Extracting and composing robustfeatures with denoising autoencoders.
In Proceed-ings of the 25nd International Conference on Ma-chine Learning, ICML, Helsinki, Finland on July 5-9, 2008.Stefan Wager, Sida I. Wang, and Percy Liang.
2013.Dropout training as adaptive regularization.
In 26,editor, Advances in Neural Information Process-ing Systems, NIPS Conference Proceedings, LakeTahoe, Nevada, United States, December 5-8, 2013.Mianwei Zhou and Kevin C. Chang.
2014.
Unify-ing learning to rank and domain adaptation: En-abling cross-task document scoring.
In Proceedingsof the 20th ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining, KDD?14, pages 781?790, New York, NY, USA.
ACM.31
