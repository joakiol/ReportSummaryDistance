Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 905?916, Dublin, Ireland, August 23-29 2014.Automatic Prediction of Text Aesthetics and InterestingnessDebasis GangulyCNGL,School of Computing,Dublin City University,Dublin 9, Irelanddganguly@computing.dcu.ieJohannes LevelingCNGL,School of Computing,Dublin City University,Dublin 9, Irelandjleveling@computing.dcu.ieGareth J.F.
JonesCNGL,School of Computing,Dublin City University,Dublin 9, Irelandgjones@computing.dcu.ieAbstractThis paper investigates the problem of automated text aesthetics prediction.
The avail-ability of user generated content and ratings, e.g.
Flickr, has induced research in aesthet-ics prediction for non-text domains, particularly for photographic images.
This problem,however, has yet not been explored for the text domain.
Due to the very subjectivenature of text aesthetics, it is difficult to compile human annotated data by methodssuch as crowd sourcing with a fair degree of inter-annotator agreement.
The availabilityof the Kindle ?popular highlights?
data has motivated us to compile a dataset com-prised of human annotated aesthetically pleasing and interesting text passages.
We thenundertake a supervised classification approach to predict text aesthetics by constructingreal-valued feature vectors from each text passage.
In particular, the features that we usefor this classification task are word length, repetitions, polarity, part-of-speech, semanticdistances; and topic generality and diversity.
A traditional binary classification approachis not effective in this case because non-highlighted passages surrounding the highlightedones do not necessarily represent the other extreme of unpleasant quality text.
Due to theabsence of real negative class samples, we employ the MC algorithm, in which trainingcan be initiated with instances only from the positive class.
On each successive iterationthe algorithm selects new strong negative samples from the unlabeled class and retrainsitself.
The results show that the mapping convergence (MC) algorithm with a Gaussianand a linear kernel used for the mapping and convergence phases, respectively, yields thebest results, achieving satisfactory accuracy, precision and recall values of about 74%,42% and 54% respectively.1 IntroductionSince their inception, Amazon Kindle device1and Apps for other general purpose hand-helddevices, have led to a massive increase in the trend of reading e-books over paper printed ones.The Amazon Kindle and the Kindle Apps provide a very simple mechanism for highlighting apiece of text and sharing it on social media.
The most popular highlighted pieces of text areshown in the Kindle device with an intention to help readers focus on passages that are pleasing orinteresting to the greatest number of people.
Every month, Kindle customers highlight millionsof book passages that are meaningful to them2.
The general trend among Kindle readers, whilereading the classic English literary works, is to highlight text passages that are associated witha high aesthetic quality.
An example highlighted passage is shown in Figure 1.With the availability of such highlighted text, which may be considered as text passages whichmost readers find pleasing to read, an interesting research problem is to attempt automaticprediction of highlighted pieces of text.
In other words, given a text passage, the objective is toThis work is licensed under Creative Commons Attribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1https://kindle.amazon.com/2https://kindle.amazon.com/most_popular905It was the best of times, it was the worst of times, it was the age of wisdom, it wasthe age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it wasthe season of Light, it was the season of Darkness, it was the spring of hope, it was thewinter of despair.Figure 1: Passage from A tale of two cities (Charles Dickens), highlighted by 6843 Kindlereaders.determine the likelihood of it being aesthetically pleasing and interesting.
Such an automatedapproach of identifying aesthetically pleasing text passages may potentially be used to endorsea newly released book on e-commerce websites with an aim to increase its sales.
Moreover, suchan approach may also, in principle, be used as a tool by an author to determine how likely it isfor readers to appreciate a newly written text passage.The key challenge in solving this problem is to determine the characteristic attributes of apopular highlighted text passage.
An intuitive assumption is that the popularity of a high-lighted passage depends on its aesthetic quality.
Generally speaking, passages inclined towardsexpressing an author?s view on a subject, which may often be philosophical in nature, withconsiderable application of atypical figures of speech, e.g.
anaphora, alliteration, antithesis,metaphor, simile, personification etc., are more likely to be highlighted than a straight-forwardstory narrative passage.
For example, the highlighted passage in Figure 1 is rich in anaphora(repetition of the same word or group of words in a paragraph, e.g.
?times?, ?age?, ?epoch?etc.)
and antithesis (juxtaposition of opposing or contrasting ideas, e.g.
?best of times?, ?worstof times?
; ?wisdom?, ?foolishness?
etc).
An automated approach of aesthetic quality predictionthus has to take into account these different features of a text passage.
The idea of using thesefeatures for text aesthetics prediction, in fact, forms a core part of our work.It is particularly interesting to see that this problem of automatically predicting text aes-thetics is largely different from the standard well researched problem of document text classi-fication (Sebastiani, 2002).
The reason is as follows.
The problem of text categorization caneffectively be solved by the application of discrete categorical features, such as character n-gramfrequencies and word frequencies.
In other words, the presence of characteristic words from aparticular domain is a good indicator of the class of a document, e.g.
the presence of the words?soccer?, ?goal?
etc.
in a document is a good indicator that the document is of the sportsgenre, whereas the presence of words such as ?money?, ?bank?
etc.
would indicate that thegenre is finance.
Consequently, the generative framework of a multinomial Naive Bayes (NB)model with character n-gram and word n-grams based features works effectively for this class ofproblems (McCallum and Nigam, 1998).In the case of aesthetic quality prediction, however, the mere presence of a particular word orcharacter n-gram can hardly be a good indicator of the inherent literary quality of the text.
Theoutput classes of this classification problem, namely aesthetic or not aesthetic, do not comprisea small vocabulary of domain-specific representative terms such as in the case of the sports orfinance domains.
The vocabularies of the respective classes in this classification problem arelargely unrestricted and mutually indistinguishable.The rest of the paper is organized as follows.
Section 2 presents related research.
In Section 3,we present our proposed approach to solve the text aesthetics problem.
Section 4 describes ourexperimental settings, following which Section 5 presents the results.
In Section 6, we investigatethe contribution from individual features and then the relative importance of the features whenused in combination.
Finally, Section 7 concludes the paper.2 Related WorkA computational viewpoint of aesthetic quality, in general, takes into account the subjectivity ofan observer and postulates that among several observations, the aesthetically most pleasing one906is the one with the shortest description, given the observer?s previous knowledge (Schmidhuber,2010).
An agent driven reinforcement based learning algorithm can then be used in principle toproduce creative (novel and interesting) outputs (Schmidhuber, 2010).
Our work in this paperis largely different from the general reinforcement learning paradigm, because we focus on theparticular problem of text aesthetics viewing the problem as a supervised classification task.Moreover, the proposition of minimum description length as an attribute of aesthetic quality(Schmidhuber, 2010) is counter-intuitive for literary works.There has been considerable research interest in automatically predicting visual aestheticquality of images (Dhar et al., 2011) and layout of web pages (Reinecke et al., 2013).
Mostempirically successful approaches to image aesthetics prediction first transform an image into afeature vector of characteristic attributes that play a pivotal role in differentiating an interestingimage from a non-interesting one.
Generally speaking, some of these attributes which determinewhether an image is aesthetically pleasing are the presence of salient objects (indicated by a lowdepth of field), compositional attributes (e.g.
the rule of thirds), the effect of light in naturallandscapes, etc.
The next step is to apply a supervised learning algorithm, e.g.
support vectormachine (SVM), to learn a two-class prediction model.
Useful features, extracted from imagesfor this classification task include: i) colourfulness, contrast, symmetry, vanishing point andfacial features (Jiang et al., 2010); ii) face poses, between-face distances, and the consistencyof expressions on multiple faces (Li et al., 2010); iii) high level describable attributes, such ascompositional attributes (e.g.
rule of thirds image layout), content attributes related to thepresence of people, animals, sky illumination attributes etc.
(Dhar et al., 2011).Our proposed method of text aesthetics prediction is similarly based on extracting character-istic features from the text passages.
However, in the case of literature, it is worth mentioningthat in contrast to image aesthetics it is more difficult to describe the subtle attributes whichdifferentiate an aesthetically pleasing text from its counterpart.Although the authors are not aware of any reported research on text aesthetics, there hasbeen a considerable amount of research in the somewhat closely related problem of detect-ing metaphors in text.
Automated approaches to metaphor detection involve both supervisedand unsupervised approaches, some of which include: i) supervised classification on extractedverbal target feature vectors of sentences (Gedigian et al., 2006); ii) expectation maximization(EM) based unsupervised approach to non-literal word sense detection (Birke and Sarkar, 2006);iii) unsupervised approach using hierarchical graph factorization clustering (Shutova and Sun,2013).In general, it is intuitive to assume that metaphorical or figurative parts of text are aestheti-cally pleasing and interesting, which makes the problem of text aesthetics prediction somewhatsimilar to that of metaphor detection.
Unfortunately, this assumption is not often true, andthis is particularly the case for literary works due to the availability of a large number of figuresof speech at an author?s disposal (metaphor just being one of them).
For example, the sampleKindle highlighted passage shown in Section 1 has an obvious aesthetic appeal to a large numberof readers, in spite of it being not metaphorical.3 Our Approach to the Text Aesthetics Prediction ProblemIn this section, we describe the details of our approach to text aesthetics prediction.
We hy-pothesize that a NB classifier with word or character n-gram based features is not suitable forthis particular problem due to the mutual overlap and lack of domain specific restriction in thevocabulary of the output classes (i.e.
aesthetic and non-aesthetic).
One thus needs to extract aset of characteristic features from the text passages which may be useful to solve the classifica-tion problem.
We describe the features used in our approach in Section 3.1.
In Section 3.2, wepropose to use the mapping convergence (MC) algorithm for the text aesthetics problem, wherethe intention is to learn a classifier only from positive samples.907The truth is rarely pure and never simple.
Modern life would be very tedious if it wereeither, and modern literature a complete impossibility!Figure 2: Passage from The Importance of Being Earnest (Oscar Wilde).3.1 Feature Vector Encoding of Text PassagesIn this section, we introduce the various features used for the text aesthetics classification task.Each feature is a function which maps a passage of text P = {w1.
.
.
wN} comprising N wordsinto a real number.3.1.1 Word-based FeaturesIn Section 1, we illustrated that that an anaphora is a rheoteric device used by authors toemphasize a text passage, which in turn indicates that such a passage is likely to attract theattention of readers and hence are likely to be highlighted by them.
Moreover, the closer therepetitions are, the stronger is the emphasis.On the basis of this reasoning, we employ an average positional difference weighted count ofword repetitions in a passage.
To be more precise, for each word in a passage we compute thenumber of times a word wiis repeated, divide this count by the difference between the repeatingposition (say at position j), and average the sum of counts for all repeating words over thepassage length, as shown in Equation 1.
In Equation 1, 1(wi= wj) is the indicator functionwhich is 1 if and only if wi= wjand 0 otherwise.The second word level feature which we use, is the average length of words in a passage.The reasoning behind using this feature is that authors tend to use relatively longer words (e.g.superlatives) to emphasize a passage.
Equation 2 shows how this is computed.W1(P ) =2N(N ?
1)N?i=1N?j=i+11(wi= wj)j ?
i(1)W2(P ) =1NN?i=1len(wi) (2)3.1.2 Topic-based FeaturesAn attribute which can be considered responsible for the aesthetic quality of a text passage isthe diversity of topics it expresses.
It is reasonable to assume that a text passage expressing abroad idea or opinion of an author, often philosophical in nature, is likely to be appealing toreaders.
Such general themed text passages typically cover a broad range of topics, as a resultof which the constituent words of such text passages involve collocation of seemingly unrelatedterms.
For example, in the text passage shown in Figure 2, the word pairs (truth, tedious), and(literature, impossibility) would typically appear in different topic classes, where by a topic wemean a set of words with high co-ocurrence likelihood estimated from a collection of documentsby standard topic modelling techniques such as the Latent Dirichlet allocation (LDA) (Blei etal., 2003).
To encode this diversity of topics as a real valued feature function, we use Equation 3.T1(P ) =2N(N ?
1)N?i=1N?j=i+11[z(wi) 6= z(wj)](j ?
i)(3)In Equation 3, z(w) denotes the topic class of the word w obtained with the help of LDA.
Amismatch in the topic class is divided by the distance between the mismatches to assign moreweight to the close mismatches.
As an example, the mismatch between (literature, impossibility)bears more importance than the mismatch between (modern, impossibility).The second topic-based feature which we use pertains to predicting the abstractness of thecontent of a passage.
It has been reported that words highly representative of topics are generally908not metaphorical.
We apply a similar reasoning to hypothesize that since an interesting piece oftext is more likely to be philosophical or abstract in nature in comparison to a story narrative,the constituent words are less likely to be the representatives of their topic classes.
Formallyspeaking in terms of LDA, these words are expected to have smaller values of maxk?k(w).
Recallthat a topic representative word in LDA exhibits a skewed distribution with a peak for one topicclass (with a high value of maxk?k(w)), whereas a less representative word exhibits a moreuniform distribution of ?k(w) values over the topic classes (thus a low value of maxk?k(w)).
Weuse Equation 4 to compute the average topic concreteness of a text passage.T2(P ) =1NN?i=1maxk?k(wi) (4)3.1.3 Part of Speech FeatureWe hypothesize that another attribute of an aesthetic passage is that it is likely to contain arich usage of adjectives (mostly of superlative type for the sake of emphasis) and adverbs.
Wetherefore employ the part of speech tag (POS) information of the constituent words of a textpassage as one of our features.
To be more specific, we use the average number of adjectivesand adverbs of a text passage as the feature value.
This is shown in Equation 5.POS(P ) =1NN?i=1(#adjectives+ #adverbs) (5)3.1.4 Sentiment FeatureWe pointed out in Section 1 that authors often use the antithesis figure of speech to express con-trasting concepts.
Thus, another feature which we can use is the aggregated absolute differencevalues between the sentiment polarities of words in a text paragraph.
This again is weightedby the difference in position between a positive sentiment word and its negative counterpart toassign more importance to closely occurring opposite sentiment concepts.To obtain the sentiment values of the constituent words, we used the SentiWordNet3.
Toillustrate with an example, consider the closely occurring opposite sentiment word pairs (best(0.75), worst (-0.75)), (wisdom (0.375), foolishness (-0.375)) etc.
of Figure 1 and the word pairs(complete (0.625), impossibility (-0.25)) of Figure 2, where the numbers in the parentheses showthe positive or the negative sentiment value (a normalized number between 0 and 1).
Equation 6shows the real-valued function derived from the sentiment information of word pairs, where thefunction s(w) denotes the sentiment value associated with the word w.SENT (P ) =2N(N ?
1)N?i=1N?j=i+1|s(wi)?
s(wj)|(j ?
i)(6)3.1.5 Inter-word Semantic Distance FeatureAn alternative way to represent the topic diversity is to capture the likelihood of the eventof occurrence of two words in close vicinity.
The higher this likelihood is, the better is thesemantic relation or coherence between the words.
We make use of the DISCO4tool to computethe semantic relation between two words in a word pair.
In DISCO, these semantic relationsbetween the words are precomputed on the basis of co-occurrence likelihoods from a large corpus,e.g.
the Wikipedia (Kolb, 2008).
DISCO provides two similarity measurements (named the firstorder and the second order similarities) between two input words.
While the first order similaritybetween two input words is computed based on their collocation sets, the second order similarityis computed based on their sets of distributionally similar words (Kolb, 2008).
We denote the3http://sentiwordnet.isti.cnr.it/4http://www.linguatools.de/disco/disco_en.html909first order and the second order similarities between words wiand wjrespectively as ds1(wi, wj)and ds2(wi, wj) respectively.In relation to text aesthetics, we expect a small value of average first order and second ordersimilarity values between word pairs in a highlighted piece of text in comparison to a non-highlighted one.
Similar to our earlier features, we divide these similarity values by the positionaldifference between the words in order to put more emphasis on semantic diversity between closelyoccurring words.
Equation 7 shows the two features extracted making use of these similarityvalues.SDk(P ) =2N(N ?
1)N?i=1N?j=i+1|dsk(wi)?
dsk(wj)|(j ?
i), k = {1, 2} (7)3.2 Learning from Positive Examples: The MC AlgorithmBinary classifiers, such as SVMs, work particularly well with a sufficient number of both positiveand negative class instances for training.
In the case of text aesthetics prediction problem, thepassages highlighted by Kindle readers serve as the positive class samples.
Although it mightbe intuitive to use the non-highlighted passages as instances of the negative type, there can beproblems associated with this approach.Firstly, the non-highlighted passages are not essentially instances of the negative class becausethe non-highlighted passages are not necessarily aesthetically unpleasing.
Secondly, there is anelement of cognitive bias associated with the highlighting process because a reader, who canalready see popular highlights while reading a page, may be biased to highlight the same passagehimself, and may not in fact highlight some other passage which he himself found interesting.Note that this observation in fact makes our problem more challenging to solve in comparisonto aesthetics prediction in other domains, such as images, where information such as Flickr5photo ratings can be used as strong positive or negative indicators of an image interestingnessor aesthetic quality, leading to effective classification results using a standard binary classificationapproach (Dhar et al., 2011).Due to the presence of incompletely labeled examples, we apply the mapping convergence(MC) algorithm (Yu et al., 2003) for this task.
The objective of the MC algorithm is to predictthe positive samples from a test data, given a mixture of positive and unlabeled samples.
Theseunlabeled samples in the MC algorithm can be treated as instances of either the positive or thenegative class in order to obtain maximum classification effectiveness.The two stages of the MC algorithm are summarized as follows.1.
The mapping stage identifies from the unlabeled samples the strong negative ones, i.e.
thepoints distinctly different from the positive samples.2.
The convergence stage is an iterative step to learn a binary classification model, e.g.
SVM,using the positive and the strong negative samples.
Each iterative step of convergenceclassifies the remaining unlabeled samples to collect more strong negative samples.
Theconvergence step is repeated until no more strong negative samples are found.The objective of the convergence step of the MC algorithm is to maximize margin to makeprogressively better approximation of the negative data.
At the end of the iteration, the classboundary eventually converges to the boundary around the positive data set in the featurespace (Yu et al., 2003).In our approach to the text aesthetics prediction task, we implement the mapping stage ofthe MC algorithm with the help of standard one-class classifiers, namely the one class SVM(OSVM) (Scho?lkopf et al., 1999) and the support vector data descriptor (SVDD) (Tax andDuin, 2004).
The OSVM separates all the data points in the feature space from the origin, withthe help of a separating hyperplane with maximum distance from the origin.
The OSVM is thus5https://www.flickr.com/910able to separate out regions in the input space with high probability densities (Scho?lkopf et al.,1999).
SVDD, on the other hand, instead of a planar, takes a spherical approach to the oneclass problem.
The algorithm obtains a spherical boundary in feature space around the data.The volume of this hypersphere is minimized to minimize the effect of incorporating outliers inthe solution (Tax and Duin, 2004).It is worth mentioning here that although the OSVM and the SVDD can be trained withpositive samples only, these models are prone to over-fitting or under-fitting due to a smallnumber of support vectors modeled from a small number of positive samples (Yu et al., 2003).In contrast, a binary SVM can model data more robustly due to the presence of the additionalnegative samples.
Hence, OSVM and the SVDD are typically used as a weak classifier to obtaina set of initial strong negative samples in order to initiate the convergence step of the MCalgorithm.4 Experiment SettingsIn this section, we describe the dataset and the tools used for our experiments.4.1 Dataset ConstructionThe standard practice to evaluate the metaphor detection problem, which is somewhat similarto the text aesthetics prediction, is to make extensive use of manually annotated data typicallyobtained under controlled user-based studies, where the users or the participants are instructedto perform some given objectives, such as manually label metaphors in a collection of documents,e.g.
(Hovy et al., 2013).
The main difficulties with this approach are that: i) it takes aconsiderable amount of time to collect data; ii) the quality of the data depends largely oncontrolled experimental settings, e.g.
the data quality may be susceptible to errors caused bytargeted, malicious work efforts, since there is often a financial incentive to complete tasksquickly rather than effectively (Ipeirotis et al., 2010); and iii) it is very difficult to compare theeffectiveness of two methods on two different datasets obtained under different controlled userstudy settings.The availability of fairly large amounts of highlighted text on the Amazon website has ensureda reliable and fast way to construct the dataset for carrying out the text aesthetics experiments.The advantages are as follows.
Firstly, it is not necessary to conduct crowd sourcing experimentsfor data collection.
Secondly, since the data is not generated by controlled crowd sourcing, thequality of the data is more reliable because there is no financial incentive to complete tasksquickly.
Thirdly, since the data is publicly available, it is possible to achieve a fair comparisonbetween different problem solving approaches.The Amazon ?Popular Highlights?6web page presents a ranked list of the most highlightedpassages, sorted in descending order by the number of highlights.
However, at the time of writingthis paper, Amazon has neither made the data publicly downloadable nor provided an API toaccess it.
For conducting our experiments with this data, we therefore had to automaticallycrawl data from the Popular Highlights web page.In addition to the highlighted passages (serving as the positive class samples in our dataset),we also need the non-highlighted ones (meant to serve as the unlabeled samples).
The text fromthe non-highlighted passages, however, are not available in the Popular Highlights web page.This data was thus extracted from those books, the passages of which are popularly highlighted.In order to ensure free access to book content, we had to restrict our dataset to the 50 mostpopular highlighted classic English fictions.More precisely speaking, for every highlighted passage found while crawling the AmazonPopular Highlights page, our crawler checks if the book is available on project Gutenberg7.
Ifnot, then we examine the next highlighted passage, otherwise we craw the full text of the book,6https://kindle.amazon.com/most_popular/highlights_all_time/7http://www.gutenberg.org/911in which the current highlighted passages belongs, from project Gutenberg website.
The crawlercontinued to run until we had collected highlighted passages from 50 different literature classics.The dataset for the prediction task is then constructed as follows.
First, we add the text ofall highlighted passages as instances of the positive class.
Next, for each highlighted passage,we add the paragraph preceding and succeeding it into the dataset as the unlabeled samples.Note that selecting the unlabeled samples this way is better than random selection of non-highlighted passages from full text, because this way of choosing negative samples ensures ameaningful representation of reader judgments to highlight a particular passage of text fromwithin a surrounding context.We then partition the dataset comprised of the positive and unlabeled samples into equalsized training and test sets.
In Table 1, we outline the characteristics of the dataset.Dataset # Books Vocab.
# PassagesSize Highlighted Unhighlighted TotalTrain 25 9560 168 305 473Test 25 7883 169 319 488Total 50 13496 337 624 961Table 1: Dataset characteristics4.2 Implementation DetailsFor each passage in the dataset, we extract the features described in Section 3.1.
To computethe topic modeling based features we used Mallet8.
The number of topics (K) in LDA was setto 100.
The POS tag feature was extracted with the help of the Stanford POS tagger9.
Forextracting the sentiment feature, we made use of the Java API of the SentiWordNet10.
For thesemantic word distance feature, we used the DISCO Java API11.For the naive Bayes experiment, we used the Stanford classifier12.
The SVM experiments(binary SVM, one-class SVM, SVDD) were conducted with the libSVM software13.4.3 Evaluation MetricsFor all the experiments reported in this paper, the classification effectiveness mainly focuses onprecision and recall with respect to the positive class.
Consequently, precision, recall and theF-score measures, shown in Tables 2 and 3, are measured with respect to the positive class only.Ideally, for this problem one would want to obtain a high recall, i.e.
identify as many high-lighted passages correctly as possible.
In this situation, recall is thus more important thanprecision.
Achieving a good precision is desirable, nonetheless, to minimize the false positives.Although we report accuracy, we emphasize that accuracy alone is not a good measure of clas-sification effectiveness in this case, because correct identification of negative instances is notimportant for this problem.5 ResultsBefore conducting experiments with the MC algorithm, we obtained baseline results by classify-ing the dataset using NB and SVMs.
In the case of NB, instead of using the real valued featuresfrom the text passages (as proposed in Section 3.1), we simply used the character n-gram andword n-gram features (maximum value of n was set to 5) from the text, automatically extracted8http://mallet.cs.umass.edu/9http://nlp.stanford.edu/software/tagger.shtml10http://sentiwordnet.isti.cnr.it/code/SentiWordNetDemoCode.java11http://www.linguatools.de/disco/disco_en.html12http://nlp.stanford.edu/software/classifier.shtml13http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/912Classifier Kernel Accuracy Precision Recall F-scoreNB N/A 67.40 54.40 36.70 43.80BSVM Linear 66.19 35.71 5.92 10.15BSVM Gaussian 67.00 39.39 15.38 22.13OSVM Linear 38.32 32.46 51.48 39.82OSVM Gaussian 53.68 41.87 50.29 45.70SVDD Linear 35.04 34.77 100.00 51.60SVDD Gaussian 37.91 35.56 97.63 52.13Table 2: Text aesthetics prediction results with Naive Bayes and SVM.Classifier Kernel Accuracy Precision Recall F-scoreMapping Convergence Mapping ConvergenceOSVM BSVM Linear Linear 66.18 35.71 5.92 10.15OSVM BSVM Linear Gaussian 64.96 40.26 36.69 38.39OSVM BSVM Gaussian Linear 66.80 44.44 11.83 18.69OSVM BSVM Gaussian Gaussian 64.34 36.87 39.05 37.93SVDD BSVM Linear Linear 40.98 35.76 92.90 51.64SVDD BSVM Linear Gaussian 43.44 36.17 90.53 51.69SVDD BSVM Gaussian Linear 56.76 42.90 74.64 54.42SVDD BSVM Gaussian Gaussian 47.34 38.60 88.17 53.69Table 3: Text aesthetics prediction results by the MC algorithm with different settings.by the Stanford classifier.
The result of this experiment (see Table 2) shows that the recall valueis very low, which in turn indicates that word vocabulary based features, typically used for textcategorization, are not effective for this task.The next classification method that we employ is standard binary class SVM (denoted asBSVM).
The training phase of the BSVM used the non-highlighted passages as negative classinstances.
We experimented with both linear and Gaussian kernels.
For all reported resultswhich use the Gaussian kernel, the parameter ?
was set to the default value of 1/(#features)as per the libSVM implementation.
Although the accuracy achieved is comparable to NB, therecall achieved is worse, which shows that treating non-highlighted passages as negative classinstances is not reasonable for this problem (see Section 6.2 for an illustration).The recall value is significantly increased with the help of one-class SVM (OSVM).
SVDDperforms even better in terms of recall.
However, SVDD significantly underfits the data becauseit classifies almost every test data point as an instance of the positive class, thus achieving lowaccuracy and precision due to the presence of too many false positives.Our next set of experiments involves the MC algorithm for classification.
Since, the mappingphase makes use of only the positive data, we employed both the one-class classifiers used in theexperiments of Table 2, i.e.
OSVM and SVDD, for this purpose.
Mapping with OSVM resultsin an improvement in the accuracy at the cost of sacrificing recall, which is not desirable forthis problem.
However, note that the negative samples obtained with the OSVM mapping (withGaussian kernel) improves the classification effectiveness of the BSVM (compare the fourth rowof Table 3 with the second row of Table 2), which indicates that the MC algorithm does improvethe classification effectiveness, confirming our hypothesis that it is reasonable not to considerevery non-highlighted passage as negative samples.The problem of SVDD underfitting (as evident from the SVDD results of Table 2) is alleviatedby the MC approach.
The most effective MC approach uses Gaussian/linear kernels for map-ping/convergence (see the seventh row of Table 3).
Accuracy is increased to around 56% witha satisfactory recall of around 74%.
The use of Gaussian kernel during both the mapping andconvergence steps yields a higher recall but at the cost of more false positives (lower accuracy,precision and F-score).913Feature combination vector Evaluation MetricsWord Topics POS/Polarity Semantic Accuracy Precision Recall F-score1 0 0 0 36.06 34.88 97.63 51.400 1 0 0 37.91 35.74 99.40 52.580 0 1 0 36.05 35.01 98.81 51.700 0 0 1 42.41 37.03 94.67 53.241 1 1 1 56.76 42.90 74.64 54.42Table 4: Individual feature contributions for identifying text aesthetics.Feature igainTopic diversity (T1) 0.3684Sentiment (SENT ) 0.2685Word repetition (W1) 0.2509First-order semantic distance (SD1) 0.1543Part-of-speech (POS) 0.1448Second-order semantic distance (SD2) 0.1141Word length (W2) 0.0732Topic abstractness (T2) 0.0526Table 5: Ranking features by their igain values.6 Posthoc AnalysisIn this section, we comment on the importance of the features used for classification, and alsoillustrate how the MC algorithm helps in increasing the separability between the classes.6.1 Feature ImportanceFirst, we investigate the importance of the different features by a selective choice of only onegroup of features at at time for the classification.
The classifier we use for this experiment isMC with a Gaussian SVDD kernel for mapping and a linear SVM kernel for convergence (as perthe best settings of Table 3).
The results are shown in Table 4 from which it can be seen thatthe best accuracy is obtained with the use of the semantic distance features.It can be observed that the accuracy values obtained with a single category of features, suchas word-based (length and repetition), topic-based (generality and diversity) and so on, areconsiderably lower than the accuracy value obtained with a combination of all the features (thelast row of Table 4.
The precision values achieved with these individual feature groups are alsoconsiderably lower than the precision of 42.90% of the overall combination.Next, we find out the relative importance of each feature in their overall combination byranking the features with the help of a standard feature quality estimator, called informationgain (igain) (Quinlan, 1986).
The results are presented in Table 5.
It can be seen that the topicdiversity is the most discriminative feature having an igain value significantly higher than thesecond most important one in the list.
This observation verifies our hypothesis that aestheticallyappealing passages are those constituting terms from diverse topics.The sentiment and the word repetition features, having close igain values, are second andthird respectively in the list.
The usefulness of the sentiment feature suggests that contrastingconcepts packed in close vicinity of a sentence are likely to be aesthetically pleasing to read.The word repetition feature, on the other hand, suggests that the anaphora figure of speech islikely to be be associated with aesthetically pleasing text.6.2 Illustration of the usefulness of the MC AlgorithmThis section investigates the usefulness of the MC algorithm for the text aesthetics classification.In particular, we show that for this one class classification problem, the MC algorithm canselectively refine the set of unlabeled samples and retrain the model for better separability9141-1 00.94-1-0.031(a) Before MC.1-1 00.94-1-0.031(b) After MC convergence (5 iterations).Figure 3: Visualization of the training set in the two most discriminating dimensions, i.e.
topicdiversity (Y-axis) and sentiment (X-axis).between the positive and the unlabeled classes.To illustrate our claim, we first plot the initial training set in two dimensional subspacebefore the application of MC, i.e.
when all the unlabelled instances are treated as negative classsamples; this is shown in Figure 3a.
The two dimensions that we use for plotting this figure,are the two features having the highest igain values, i.e.
the topic diversity (T1) and sentiment(SENT ) features.
Figure 3a shows that the highlighted text passages (shown in blue) are notwell separated from the non-highlighted ones (shown in red).Next, in Figure 3b, we plot the training set with a reduced number of samples from the negative(non-aesthetic) class obtained after running the MC algorithm.
Figure 3b clearly shows thatafter convergence the MC algorithm has retained only the strong negative samples for training,as is evident from a better visual separation between the classes.
A binary classifier, trained onthe dataset of Figure 3b, is thus likely to be more effective than that trained with Figure 3a.7 ConclusionsThis paper investigated the problem of automated text aesthetics prediction.
As distinguishingfeatures for text aesthetics identification, we applied different statistical features such as wordrepetitions, topic diversity, part-of-speech, word polarity etc.
We collected aesthetically pleasingtext passages from the Kindle ?popular highlights?
website for conducting our experiments.
Dueto the presence of only positive class samples, i.e.
the highlighted passages, in this dataset, weapply the MC algorithm to iteratively train a binary classifier with the strongly negative samples.The results of our experiments show that the MC algorithm with a Gaussian and a linear ker-nel applied for the mapping and convergence phases respectively, yields the best results achievingsatisfactory recall, precision and F-score values of about 74%, 42% and 54% respectively.
More-over, the results also demonstrate that the topic diversity, word polarity and word repetition arethe three most distinguishing features for text aesthetics identification.
Furthermore, our resultsare comparable to those of a somewhat similar problem of figurative text detection where thebest reported F-score values achieved are about 54% (Birke and Sarkar, 2006) and 64% (Shutovaand Sun, 2013).AcknowledgmentsThis research is supported by Science Foundation Ireland (SFI) as a part of the CNGL Centrefor Global Intelligent Content at DCU (Grant No: 12/CE/I2267).915ReferencesJulia Birke and Anoop Sarkar.
2006.
A clustering approach for nearly unsupervised recognition ofnonliteral language.
In EACL 2006, 11st Conference of the European Chapter of the Associationfor Computational Linguistics, Proceedings of the Conference, April 3-7, 2006, Trento, Italy.
TheAssociation for Computer Linguistics.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003.
Latent Dirichlet Allocation.
Journal ofMachine Learning Research, 3:993?1022, March.Sagnik Dhar, Vicente Ordonez, and Tamara L. Berg.
2011.
High level describable attributes for predict-ing aesthetics and interestingness.
In The 24th IEEE Conference on Computer Vision and PatternRecognition, CVPR 2011, Colorado Springs, CO, USA, 20-25 June 2011, pages 1657?1664.Matt Gedigian, John Bryant, Srini Narayanan, and Branimir Ciric.
2006.
Catching metaphors.
InProceedings of the Third Workshop on Scalable Natural Language Understanding, ScaNaLU ?06, pages41?48, Stroudsburg, PA, USA.
Association for Computational Linguistics.Dirk Hovy, Shashank Shrivastava, Sujay Jauhar, Mrinmaya Sachan, Kartik Goyal, Huying Li, Whit-ney Sanders, and Eduard Hovy.
2013.
Identifying metaphorical expressions with tree kernels.
InProceedings of NAACL-HLT Meta4NLP Workshop.Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.
2010.
Quality management on amazon mechani-cal turk.
In Proceedings of the ACM SIGKDD Workshop on Human Computation, HCOMP ?10, pages64?67, New York, NY, USA.
ACM.Wei Jiang, Alexander C. Loui, and Cathleen Daniels Cerosaletti.
2010.
Automatic aesthetic valueassessment in photographic images.
In Proceedings of the 2010 IEEE International Conference onMultimedia and Expo, ICME 2010, 19-23 July 2010, Singapore, pages 920?925.Peter Kolb.
2008.
DISCO: A Multilingual Database of Distributionally Similar Words.
In KONVENS2008 ?
Erga?nzungsband: Textressourcen und lexikalisches Wissen, pages 37?44.Congcong Li, Alexander C. Loui, and Tsuhan Chen.
2010.
Towards aesthetics: A photo quality assess-ment and photo selection system.
In Proceedings of the International Conference on Multimedia, MM?10, pages 827?830, New York, NY, USA.
ACM.Andrew McCallum and Kamal Nigam.
1998.
A comparison of event models for naive bayes text classifi-cation.
In AAAI/ICML Workshop on Learning for Text Categorization, pages 41?48.J.
R. Quinlan.
1986.
Induction of decision trees.
Mach.
Learn., 1(1):81?106, March.Katharina Reinecke, Tom Yeh, Luke Miratrix, Rahmatri Mardiko, Yuechen Zhao, Jenny Liu, andKrzysztof Z. Gajos.
2013.
Predicting users?
first impressions of website aesthetics with a quantifi-cation of perceived visual complexity and colorfulness.
In Proceedings of the SIGCHI Conference onHuman Factors in Computing Systems, CHI ?13, pages 2049?2058, New York, NY, USA.
ACM.Ju?rgen Schmidhuber.
2010.
Formal theory of creativity, fun, and intrinsic motivation (1990-2010).
IEEET.
Autonomous Mental Development, 2(3):230?247.Bernhard Scho?lkopf, Robert C. Williamson, Alex J. Smola, John Shawe-Taylor, and John C. Platt.
1999.Support vector method for novelty detection.
In Advances in Neural Information Processing Systems12, [NIPS Conference, Denver, Colorado, USA, November 29 - December 4, 1999, pages 582?588.
TheMIT Press.Fabrizio Sebastiani.
2002.
Machine learning in automated text categorization.
ACM Comput.
Surv.,34(1):1?47, March.Ekaterina Shutova and Lin Sun.
2013.
Unsupervised metaphor identification using hierarchical graphfactorization clustering.
In Human Language Technologies: Conference of the North American Chapterof the Association of Computational Linguistics, Proceedings, June 9-14, 2013, Westin Peachtree PlazaHotel, Atlanta, Georgia, USA, pages 978?988.
The Association for Computational Linguistics.David M. J.
Tax and Robert P. W. Duin.
2004.
Support vector data description.
Mach.
Learn., 54(1):45?66, January.Hwanjo Yu, ChengXiang Zhai, and Jiawei Han.
2003.
Text classification from positive and unlabeleddocuments.
In Proceedings of the 2003 ACM CIKM International Conference on Information andKnowledge Management, New Orleans, Louisiana, USA, November 2-8, 2003, pages 232?239.
ACM.916
