Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264?271,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsInstance Weighting for Domain Adaptation in NLPJing Jiang and ChengXiang ZhaiDepartment of Computer ScienceUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801, USA{jiang4,czhai}@cs.uiuc.eduAbstractDomain adaptation is an important problemin natural language processing (NLP) due tothe lack of labeled data in novel domains.
Inthis paper, we study the domain adaptationproblem from the instance weighting per-spective.
We formally analyze and charac-terize the domain adaptation problem froma distributional view, and show that thereare two distinct needs for adaptation, cor-responding to the different distributions ofinstances and classification functions in thesource and the target domains.
We thenpropose a general instance weighting frame-work for domain adaptation.
Our empir-ical results on three NLP tasks show thatincorporating and exploiting more informa-tion from the target domain through instanceweighting is effective.1 IntroductionMany natural language processing (NLP) problemssuch as part-of-speech (POS) tagging, named entity(NE) recognition, relation extraction, and seman-tic role labeling, are currently solved by supervisedlearning from manually labeled data.
A bottleneckproblem with this supervised learning approach isthe lack of annotated data.
As a special case, weoften face the situation where we have a sufficientamount of labeled data in one domain, but have littleor no labeled data in another related domain whichwe are interested in.
We thus face the domain adap-tation problem.
Following (Blitzer et al, 2006), wecall the first the source domain, and the second thetarget domain.The domain adaptation problem is commonly en-countered in NLP.
For example, in POS tagging, thesource domain may be tagged WSJ articles, and thetarget domain may be scientific literature that con-tains scientific terminology.
In NE recognition, thesource domain may be annotated news articles, andthe target domain may be personal blogs.
Anotherexample is personalized spam filtering, where wemay have many labeled spam and ham emails frompublicly available sources, but we need to adapt thelearned spam filter to an individual user?s inbox be-cause the user has her own, and presumably very dif-ferent, distribution of emails and notion of spams.Despite the importance of domain adaptation inNLP, currently there are no standard methods forsolving this problem.
An immediate possible solu-tion is semi-supervised learning, where we simplytreat the target instances as unlabeled data but donot distinguish the two domains.
However, giventhat the source data and the target data are from dif-ferent distributions, we should expect to do betterby exploiting the domain difference.
Recently therehave been some studies addressing domain adapta-tion from different perspectives (Roark and Bacchi-ani, 2003; Chelba and Acero, 2004; Florian et al,2004; Daume?
III and Marcu, 2006; Blitzer et al,2006).
However, there have not been many studiesthat focus on the difference between the instance dis-tributions in the two domains.
A detailed discussionon related work is given in Section 5.In this paper, we study the domain adaptationproblem from the instance weighting perspective.264In general, the domain adaptation problem ariseswhen the source instances and the target instancesare from two different, but related distributions.We formally analyze and characterize the domainadaptation problem from this distributional view.Such an analysis reveals that there are two distinctneeds for adaptation, corresponding to the differ-ent distributions of instances and the different clas-sification functions in the source and the target do-mains.
Based on this analysis, we propose a gen-eral instance weighting method for domain adapta-tion, which can be regarded as a generalization ofan existing approach to semi-supervised learning.The proposed method implements several adapta-tion heuristics with a unified objective function: (1)removing misleading training instances in the sourcedomain; (2) assigning more weights to labeled tar-get instances than labeled source instances; (3) aug-menting training instances with target instances withpredicted labels.
We evaluated the proposed methodwith three adaptation problems in NLP, includingPOS tagging, NE type classification, and spam filter-ing.
The results show that regular semi-supervisedand supervised learning methods do not perform aswell as our new method, which explicitly capturesdomain difference.
Our results also show that in-corporating and exploiting more information fromthe target domain is much more useful for improv-ing performance than excluding misleading trainingexamples from the source domain.The rest of the paper is organized as follows.
InSection 2, we formally analyze the domain adapta-tion problem and distinguish two types of adapta-tion.
In Section 3, we then propose a general in-stance weighting framework for domain adaptation.In Section 4, we present the experiment results.
Fi-nally, we compare our framework with related workin Section 5 before we conclude in Section 6.2 Domain AdaptationIn this section, we define and analyze domain adap-tation from a theoretical point of view.
We show thatthe need for domain adaptation arises from two fac-tors, and the solutions are different for each factor.We restrict our attention to those NLP tasks that canbe cast into multiclass classification problems, andwe only consider discriminative models for classifi-cation.
Since both are common practice in NLP, ouranalysis is applicable to many NLP tasks.Let X be a feature space we choose to representthe observed instances, and let Y be the set of classlabels.
In the standard supervised learning setting,we are given a set of labeled instances {(xi, yi)}Ni=1,where xi ?
X , yi ?
Y , and (xi, yi) are drawn froman unknown joint distribution p(x, y).
Our goal is torecover this unknown distribution so that we can pre-dict unlabeled instances drawn from the same distri-bution.
In discriminative models, we are only con-cerned with p(y|x).
Following the maximum likeli-hood estimation framework, we start with a parame-terized model family p(y|x; ?
), and then find the bestmodel parameter ??
that maximizes the expected loglikelihood of the data:??
= argmax?
?X?y?Yp(x, y) log p(y|x; ?
)dx.Since we do not know the distribution p(x, y), wemaximize the empirical log likelihood instead:??
?
argmax??X?y?Yp?
(x, y) log p(y|x; ?
)dx= argmax?1NN?i=1log p(yi|xi; ?
).Note that since we use the empirical distributionp?
(x, y) to approximate p(x, y), the estimated ??
isdependent on p?
(x, y).
In general, as long as we havesufficient labeled data, this approximation is fine be-cause the unlabeled instances we want to classify arefrom the same p(x, y).2.1 Two Factors for Domain AdaptationLet us now turn to the case of domain adaptationwhere the unlabeled instances we want to classifyare from a different distribution than the labeled in-stances.
Let ps(x, y) and pt(x, y) be the true un-derlying distributions for the source and the targetdomains, respectively.
Our general idea is to useps(x, y) to approximate pt(x, y) so that we can ex-ploit the labeled examples in the source domain.If we factor p(x, y) into p(x, y) = p(y|x)p(x),we can see that pt(x, y) can deviate from ps(x, y) intwo different ways, corresponding to two differentkinds of domain adaptation:265Case 1 (Labeling Adaptation): pt(y|x) deviatesfrom ps(y|x) to a certain extent.
In this case, it isclear that our estimation of ps(y|x) from the labeledsource domain instances will not be a good estima-tion of pt(y|x), and therefore domain adaptation isneeded.
We refer to this kind of adaptation as func-tion/labeling adaptation.Case 2 (Instance Adaptation): pt(y|x) is mostlysimilar to ps(y|x), but pt(x) deviates from ps(x).
Inthis case, it may appear that our estimated ps(y|x)can still be used in the target domain.
However, aswe have pointed out, the estimation of ps(y|x) de-pends on the empirical distribution p?s(x, y), whichdeviates from pt(x, y) due to the deviation of ps(x)from pt(x).
In general, the estimation of ps(y|x)would be more influenced by the instances with highp?s(x, y) (i.e., high p?s(x)).
If pt(x) is very differ-ent from ps(x), then we should expect pt(x, y) to bevery different from ps(x, y), and therefore differentfrom p?s(x, y).
We thus cannot expect the estimatedps(y|x) to work well on the regions where pt(x, y)is high, but ps(x, y) is low.
Therefore, in this case,we still need domain adaptation, which we refer toas instance adaptation.Because the need for domain adaptation arisesfrom two different factors, we need different solu-tions for each factor.2.2 Solutions for Labeling AdaptationIf pt(y|x) deviates from ps(y|x) to some extent, wehave one of the following choices:Change of representation:It may be the case that if we change the rep-resentation of the instances, i.e., if we choose afeature space X ?
different from X , we can bridgethe gap between the two distributions ps(y|x) andpt(y|x).
For example, consider domain adaptiveNE recognition where the source domain containsclean newswire data, while the target domain con-tains broadcast news data that has been transcribedby automatic speech recognition and lacks capital-ization.
Suppose we use a naive NE tagger thatonly looks at the word itself.
If we consider capi-talization, then the instance Bush is represented dif-ferently from the instance bush.
In the source do-main, ps(y = Person|x = Bush) is high whileps(y = Person|x = bush) is low, but in the targetdomain, pt(y = Person|x = bush) is high.
If weignore the capitalization information, then in bothdomains p(y = Person|x = bush) will be high pro-vided that the source domain contains much fewerinstances of bush than Bush.Adaptation through prior:When we use a parameterized model p(y|x; ?
)to approximate p(y|x) and estimate ?
based on thesource domain data, we can place some prior on themodel parameter ?
so that the estimated distributionp(y|x; ??)
will be closer to pt(y|x).
Consider againthe NE tagging example.
If we use capitalization asa feature, in the source domain where capitalizationinformation is available, this feature will be given alarge weight in the learned model because it is veryuseful.
If we place a prior on the weight for this fea-ture so that a large weight will be penalized, thenwe can prevent the learned model from relying toomuch on this domain specific feature.Instance pruning:If we know the instances x for which pt(y|x) isdifferent from ps(y|x), we can actively remove theseinstances from the training data because they are?misleading?.For all the three solutions given above, we needeither some prior knowledge about the target do-main, or some labeled target domain instances;from only the unlabeled target domain instances, wewould not know where and why pt(y|x) differs fromps(y|x).2.3 Solutions for Instance AdaptationIn the case where pt(y|x) is similar to ps(y|x), butpt(x) deviates from ps(x), we may use the (unla-beled) target domain instances to bias the estimateof ps(x) toward a better approximation of pt(x), andthus achieve domain adaptation.
We explain the ideabelow.Our goal is to obtain a good estimate of ?
?t that isoptimized according to the target domain distribu-tion pt(x, y).
The exact objective function is thus?
?t = argmax?
?X?y?Ypt(x, y) log p(y|x; ?
)dx= argmax?
?Xpt(x)?y?Ypt(y|x) log p(y|x; ?
)dx.266Our idea of domain adaptation is to exploit the la-beled instances in the source domain to help obtain?
?t .Let Ds = {(xsi , ysi )}Nsi=1 denote the set of la-beled instances we have from the source domain.Assume that we have a (small) set of labeled anda (large) set of unlabeled instances from the tar-get domain, denoted by Dt,l = {(xt,lj , yt,lj )}Nt,lj=1 andDt,u = {xt,uk }Nt,uk=1 , respectively.
We now show threeways to approximate the objective function above,corresponding to using three different sets of in-stances to approximate the instance space X .Using Ds:Using ps(y|x) to approximate pt(y|x), we obtain?
?t ?
argmax?
?Xpt(x)ps(x)ps(x)?y?Yps(y|x) log p(y|x; ?)dx?
argmax?
?Xpt(x)ps(x) p?s(x)?y?Yp?s(y|x) log p(y|x; ?
)dx= argmax?1NsNs?i=1pt(xsi )ps(xsi )log p(ysi |xsi ; ?
).Here we use only the labeled instances in Ds butwe adjust the weight of each instance by pt(x)ps(x) .
Themajor difficulty is how to accurately estimate pt(x)ps(x) .Using Dt,l:?
?t ?
argmax?
?Xp?t,l(x)?y?Yp?t,l(y|x) log p(y|x; ?
)dx= argmax?1Nt,lNt,l?j=1log p(yt,lj |xt,lj ; ?
)Note that this is the standard supervised learningmethod using only the small amount of labeled tar-get instances.
The major weakness of this approxi-mation is that when Nt,l is very small, the estimationis not accurate.Using Dt,u:?
?t ?
argmax?
?Xp?t,u(x)?y?Ypt(y|x) log p(y|x; ?
)dx= argmax?1Nt,uNt,u?k=1?y?Ypt(y|xt,uk ) log p(y|xt,uk ; ?
),The challenge here is that pt(y|xt,uk ; ?)
is unknownto us, thus we need to estimate it.
One possibilityis to approximate it with a model ??
learned fromDs and Dt,l.
For example, we can set pt(y|x, ?)
=p(y|x; ??).
Alternatively, we can also set pt(y|x, ?
)to 1 if y = argmaxy?
p(y?|x; ??)
and 0 otherwise.3 A Framework of Instance Weighting forDomain AdaptationThe theoretical analysis we give in Section 2 sug-gests that one way to solve the domain adaptationproblem is through instance weighting.
We proposea framework that incorporates instance pruning inSection 2.2 and the three approximations in Sec-tion 2.3.
Before we show the formal framework, wefirst introduce some weighting parameters and ex-plain the intuitions behind these parameters.First, for each (xsi , ysi ) ?
Ds, we introduce a pa-rameter ?i to indicate how likely pt(ysi |xsi ) is closeto ps(ysi |xsi ).
Large ?i means the two probabilitiesare close, and therefore we can trust the labeled in-stance (xsi , ysi ) for the purpose of learning a clas-sifier for the target domain.
Small ?i means thesetwo probabilities are very different, and therefore weshould probably discard the instance (xsi , ysi ) in thelearning process.Second, again for each (xsi , ysi ) ?
Ds, we intro-duce another parameter ?i that ideally is equal topt(xsi )ps(xsi ) .
From the approximation in Section 2.3 thatuses only Ds, it is clear that such a parameter is use-ful.Next, for each xt,ui ?
Dt,u, and for each possiblelabel y ?
Y , we introduce a parameter ?i(y) thatindicates how likely we would like to assign y as atentative label to xt,ui and include (xt,ui , y) as a train-ing example.Finally, we introduce three global parameters ?s,?t,l and ?t,u that are not instance-specific but are as-sociated with Ds, Dt,l and Dt,u, respectively.
Thesethree parameters allow us to control the contributionof each of the three approximation methods in Sec-tion 2.3 when we linearly combine them together.We now formally define our instance weightingframework.
Given Ds, Dt,l and Dt,u, to learn a clas-sifier for the target domain, we find a parameter ?
?that optimizes the following objective function:267??
= argmax?
[?s ?
1CsNs?i=1?i?i log p(ysi |xsi ; ?
)+?t,l ?
1Ct,lNt,l?j=1log p(yt,lj |xt,lj ; ?
)+?t,u ?
1Ct,uNt,u?k=1?y?Y?k(y) log p(y|xt,uk ; ?
)+ log p(?
)],where Cs =?Nsi=1 ?i?i, Ct,l = Nt,l, Ct,u =?Nt,uk=1?y?Y ?k(y), and ?s + ?t,l + ?t,u = 1.
Thelast term, log p(?
), is the log of a Gaussian prior dis-tribution of ?, commonly used to regularize the com-plexity of the model.In general, we do not know the optimal values ofthese parameters for the target domain.
Neverthe-less, the intuitions behind these parameters serve asguidelines for us to design heuristics to set these pa-rameters.
In the rest of this section, we introduceseveral heuristics that we used in our experiments toset these parameters.3.1 Setting ?Following the intuition that if pt(y|x) differs muchfrom ps(y|x), then (x, y) should be discarded fromthe training set, we use the following heuristic toset ?s.
First, with standard supervised learning, wetrain a model ?
?t,l from Dt,l.
We consider p(y|x; ?
?t,l)to be a crude approximation of pt(y|x).
Then, weclassify {xsi}Nsi=1 using ??t,l.
The top k instancesthat are incorrectly predicted by ?
?t,l (ranked by theirprediction confidence) are discarded.
In anotherword, ?si of the top k instances for which ysi 6=argmaxy p(y|xsi ; ?
?t,l) are set to 0, and ?i of all theother source instances are set to 1.3.2 Setting ?Accurately setting ?
involves accurately estimatingps(x) and pt(x) from the empirical distributions.For many NLP classification tasks, we do not have agood parametric model for p(x).
We thus need to re-sort to non-parametric density estimation methods.However, for many NLP tasks, x resides in a highdimensional space, which makes it hard to applystandard non-parametric density estimation meth-ods.
We have not explored this direction, and in ourexperiments, we set ?
to 1 for all source instances.3.3 Setting ?Setting ?
is closely related to some semi-supervisedlearning methods.
One option is to set ?k(y) =p(y|xt,uk ; ?).
In this case, ?
is no longer a constantbut is a function of ?.
This way of setting ?
corre-sponds to the entropy minimization semi-supervisedlearning method (Grandvalet and Bengio, 2005).Another way to set ?
corresponds to bootstrappingsemi-supervised learning.
First, let ??
(n) be a modellearned from the previous round of training.
We thenselect the top k instances from Dt,u that have thehighest prediction confidence.
For these instances,we set ?k(y) = 1 for y = argmaxy?
p(y?|xt,uk ; ??
(n)),and ?k(y) = 0 for all other y.
In another word, weselect the top k confidently predicted instances, andinclude these instances together with their predictedlabels in the training set.
All other instances in Dt,uare not considered.
In our experiments, we only con-sidered this bootstrapping way of setting ?.3.4 Setting ?
?s, ?t,l and ?t,u control the balance among the threesets of instances.
Using standard supervised learn-ing, ?s and ?t,l are set proportionally to Cs and Ct,l,that is, each instance is weighted the same whetherit is in Ds or in Dt,l, and ?t,u is set to 0.
Similarly,using standard bootstrapping, ?t,u is set proportion-ally to Ct,u, that is, each target instance added to thetraining set is also weighted the same as a sourceinstance.
In neither case are the target instances em-phasize more than source instances.
However, fordomain adaptation, we want to focus more on thetarget domain instances.
So intuitively, we want tomake ?t,l and ?t,u somehow larger relative to ?s.
Aswe will show in Section 4, this is indeed beneficial.In general, the framework provides great flexibil-ity for implementing different adaptation strategiesthrough these instance weighting parameters.4 Experiments4.1 Tasks and Data SetsWe chose three different NLP tasks to evaluate ourinstance weighting method for domain adaptation.The first task is POS tagging, for which we used2686166 WSJ sentences from Sections 00 and 01 ofPenn Treebank as the source domain data, and 2730PubMed sentences from the Oncology section of thePennBioIE corpus as the target domain data.
Thesecond task is entity type classification.
The setup isvery similar to Daume?
III and Marcu (2006).
Weassume that the entity boundaries have been cor-rectly identified, and we want to classify the typesof the entities.
We used ACE 2005 training datafor this task.
For the source domain, we used thenewswire collection, which contains 11256 exam-ples, and for the target domains, we used the we-blog (WL) collection (5164 examples) and the con-versational telephone speech (CTS) collection (4868examples).
The third task is personalized spam fil-tering.
We used the ECML/PKDD 2006 discov-ery challenge data set.
The source domain contains4000 spam and ham emails from publicly availablesources, and the target domains are three individualusers?
inboxes, each containing 2500 emails.For each task, we consider two experiment set-tings.
In the first setting, we assume there are a smallnumber of labeled target instances available.
ForPOS tagging, we used an additional 300 Oncologysentences as labeled target instances.
For NE typ-ing, we used 500 labeled target instances and 2000unlabeled target instances for each target domain.For spam filtering, we used 200 labeled target in-stances and 1800 unlabeled target instances.
In thesecond setting, we assume there is no labeled targetinstance.
We thus used all available target instancesfor testing in all three tasks.We used logistic regression as our model ofp(y|x; ?)
because it is a robust learning algorithmand widely used.We now describe three sets of experiments, cor-responding to three heuristic ways of setting ?, ?t,land ?t,u.4.2 Removing ?Misleading?
Source DomainInstancesIn the first set of experiments, we gradually remove?misleading?
labeled instances from the source do-main, using the small number of labeled target in-stances we have.
We follow the heuristic we de-scribed in Section 3.1, which sets the ?
for the topk misclassified source instances to 0, and the ?
forall the other source instances to 1.
We also set ?t,land ?t,l to 0 in order to focus only on the effect ofremoving ?misleading?
instances.
We compare witha baseline method which uses all source instanceswith equal weight but no target instances.
The re-sults are shown in Table 1.From the table, we can see that in most exper-iments, removing these predicted ?misleading?
ex-amples improved the performance over the baseline.In some experiments (Oncology, CTS, u00, u01), thelargest improvement was achieved when all misclas-sified source instances were removed.
In the case ofweblog NE type classification, however, removingthe source instances hurt the performance.
A pos-sible reason for this is that the set of labeled targetinstances we use is a biased sample from the targetdomain, and therefore the model trained on these in-stances is not always a good predictor of ?mislead-ing?
source instances.4.3 Adding Labeled Target Domain Instanceswith Higher WeightsThe second set of experiments is to add the labeledtarget domain instances into the training set.
Thiscorresponds to setting ?t,l to some non-zero value,but still keeping ?t,u as 0.
If we ignore the do-main difference, then each labeled target instanceis weighted the same as a labeled source instance(?u,l?s =Cu,lCs ), which is what happens in regular su-pervised learning.
However, based on our theoret-ical analysis, we can expect the labeled target in-stances to be more representative of the target do-main than the source instances.
We can thereforeassign higher weights for the target instances, by ad-justing the ratio between ?t,l and ?s.
In our experi-ments, we set ?t,l?s = aCt,lCs , where a ranges from 2 to20.
The results are shown in Table 2.As shown from the table, adding some labeled tar-get instances can greatly improve the performancefor all tasks.
And in almost all cases, weighting thetarget instances more than the source instances per-formed better than weighting them equally.We also tested another setting where we firstremoved the ?misleading?
source examples as weshowed in Section 4.2, and then added the labeledtarget instances.
The results are shown in the lastrow of Table 2.
However, although both removing?misleading?
source instances and adding labeled269POS NE Type Spamk Oncology k CTS k WL k u00 u01 u020 0.8630 0 0.7815 0 0.7045 0 0.6306 0.6950 0.76444000 0.8675 800 0.8245 600 0.7070 150 0.6417 0.7078 0.79508000 0.8709 1600 0.8640 1200 0.6975 300 0.6611 0.7228 0.822212000 0.8713 2400 0.8825 1800 0.6830 450 0.7106 0.7806 0.823916000 0.8714 3000 0.8825 2400 0.6795 600 0.7911 0.8322 0.8328all 0.8720 all 0.8830 all 0.6600 all 0.8106 0.8517 0.8067Table 1: Accuracy on the target domain after removing ?misleading?
source domain instances.POS NE Type Spammethod Oncology method CTS WL method u00 u01 u02Ds only 0.8630 Ds only 0.7815 0.7045 Ds only 0.6306 0.6950 0.7644Ds + Dt,l 0.9349 Ds + Dt,l 0.9340 0.7735 Ds + Dt,l 0.9572 0.9572 0.9461Ds + 5Dt,l 0.9411 Ds + 2Dt,l 0.9355 0.7810 Ds + 2Dt,l 0.9606 0.9600 0.9533Ds + 10Dt,l 0.9429 Ds + 5Dt,l 0.9360 0.7820 Ds + 5Dt,l 0.9628 09611 0.9601Ds + 20Dt,l 0.9443 Ds + 10Dt,l 0.9355 0.7840 Ds + 10Dt,l 0.9639 0.9628 0.9633D?s + 20Dt,l 0.9422 D?s + 10Dt,l 0.8950 0.6670 D?s + 10Dt,l 0.9717 0.9478 0.9494Table 2: Accuracy on the unlabeled target instances after adding the labeled target instances.target instances work well individually, when com-bined, the performance in most cases is not as goodas when no source instances are removed.
We hy-pothesize that this is because after we added somelabeled target instances with large weights, we al-ready gained a good balance between the source dataand the target data.
Further removing source in-stances would push the emphasis more on the setof labeled target instances, which is only a biasedsample of the whole target domain.The POS data set and the CTS data set have pre-viously been used for testing other adaptation meth-ods (Daume?
III and Marcu, 2006; Blitzer et al,2006), though the setup there is different from ours.Our performance using instance weighting is com-parable to their best performance (slightly worse forPOS and better for CTS).4.4 Bootstrapping with Higher WeightsIn the third set of experiments, we assume that wedo not have any labeled target instances.
We triedtwo bootstrapping methods.
The first is a standardbootstrapping method, in which we gradually addedthe most confidently predicted unlabeled target in-stances with their predicted labels to the trainingset.
Since we believe that the target instances shouldin general be given more weight because they bet-ter represent the target domain than the source in-stances, in the second method, we gave the addedtarget instances more weight in the objective func-tion.
In particular, we set ?t,u = ?s such that thetotal contribution of the added target instances isequal to that of all the labeled source instances.
Wecall this second method the balanced bootstrappingmethod.
Table 3 shows the results.As we can see, while bootstrapping can generallyimprove the performance over the baseline whereno unlabeled data is used, the balanced bootstrap-ping method performed slightly better than the stan-dard bootstrapping method.
This again shows thatweighting the target instances more is a right direc-tion to go for domain adaptation.5 Related WorkThere have been several studies in NLP that addressdomain adaptation, and most of them need labeleddata from both the source domain and the target do-main.
Here we highlight a few representative ones.For generative syntactic parsing, Roark and Bac-chiani (2003) have used the source domain datato construct a Dirichlet prior for MAP estimationof the PCFG for the target domain.
Chelba andAcero (2004) use the parameters of the maximumentropy model learned from the source domain asthe means of a Gaussian prior when training a newmodel on the target data.
Florian et al (2004) firsttrain a NE tagger on the source domain, and then usethe tagger?s predictions as features for training andtesting on the target domain.The only work we are aware of that directly mod-270POS NE Type Spammethod Oncology CTS WL u00 u01 u02supervised 0.8630 0.7781 0.7351 0.6476 0.6976 0.8068standard bootstrap 0.8728 0.8917 0.7498 0.8720 0.9212 0.9760balanced bootstrap 0.8750 0.8923 0.7523 0.8816 0.9256 0.9772Table 3: Accuracy on the target domain without using labeled target instances.
In balanced bootstrapping,more weights are put on the target instances in the objective function than in standard bootstrapping.els the different distributions in the source and thetarget domains is by Daume?
III and Marcu (2006).They assume a ?truly source domain?
distribution,a ?truly target domain?
distribution, and a ?generaldomain?
distribution.
The source (target) domaindata is generated from a mixture of the ?truly source(target) domain?
distribution and the ?general do-main?
distribution.
In contrast, we do not assumesuch a mixture model.None of the above methods would work if therewere no labeled target instances.
Indeed, all theabove methods do not make use of the unlabeledinstances in the target domain.
In contrast, our in-stance weighting framework allows unlabeled targetinstances to contribute to the model estimation.Blitzer et al (2006) propose a domain adaptationmethod that uses the unlabeled target instances toinfer a good feature representation, which can be re-garded as weighting the features.
In contrast, weweight the instances.
The idea of using pt(x)ps(x) toweight instances has been studied in statistics (Shi-modaira, 2000), but has not been applied to NLPtasks.6 Conclusions and Future WorkDomain adaptation is a very important problem withapplications to many NLP tasks.
In this paper,we formally analyze the domain adaptation problemand propose a general instance weighting frameworkfor domain adaptation.
The framework is flexible tosupport many different strategies for adaptation.
Inparticular, it can support adaptation with some targetdomain labeled instances as well as that without anylabeled target instances.
Experiment results on threeNLP tasks show that while regular semi-supervisedlearning methods and supervised learning methodscan be applied to domain adaptation without con-sidering domain difference, they do not perform aswell as our new method, which explicitly capturesdomain difference.
Our results also show that incor-porating and exploiting more information from thetarget domain is much more useful than excludingmisleading training examples from the source do-main.
The framework opens up many interestingfuture research directions, especially those related tohow to more accurately set/estimate those weightingparameters.AcknowledgmentsThis work was in part supported by the National Sci-ence Foundation under award numbers 0425852 and0428472.
We thank the anonymous reviewers fortheir valuable comments.ReferencesJohn Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proc.
of EMNLP, pages 120?128.Ciprian Chelba and Alex Acero.
2004.
Adaptation ofmaximum entropy capitalizer: Little data can help alot.
In Proc.
of EMNLP, pages 285?292.Hal Daume?
III and Daniel Marcu.
2006.
Domain adapta-tion for statistical classifiers.
J.
Artificial IntelligenceRes., 26:101?126.R.
Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-hatla, X. Luo, N. Nicolov, and S. Roukos.
2004.
Astatistical model for multilingual entity detection andtracking.
In Proc.
of HLT-NAACL, pages 1?8.Y.
Grandvalet and Y. Bengio.
2005.
Semi-supervisedlearning by entropy minimization.
In NIPS.Brian Roark and Michiel Bacchiani.
2003.
Supervisedand unsupervised PCFG adaptatin to novel domains.In Proc.
of HLT-NAACL, pages 126?133.Hidetoshi Shimodaira.
2000.
Improving predictive in-ference under covariate shift by weighting the log-likelihood function.
Journal of Statistical Planningand Inference, 90:227?244.271
