Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 7?12,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSolving the ?Who?s Mark Johnson?
Puzzle:Information Extraction Based Cross Document CoreferenceJian Huang??
Sarah M. Taylor?
Jonathan L. Smith?
Konstantinos A. Fotiadis?
C. Lee Giles?
?Information Sciences and TechnologyPennsylvania State University, University Park, PA 16802, USA?
?Advanced Technology Office, Lockheed Martin IS&GS?4350 N. Fairfax Drive, Suite 470, Arlington, VA 22203, USA?230 Mall Blvd, King of Prussia, PA 19406, USAAbstractCross Document Coreference (CDC) is theproblem of resolving the underlying identityof entities across multiple documents and is amajor step for document understanding.We develop a framework to efficientlydetermine the identity of a person based onextracted information, which includes unaryproperties such as gender and title, as well asbinary relationships with other named entitiessuch as co-occurrence and geo-locations.At the heart of our approach is a suite ofsimilarity functions (specialists) for matchingrelationships and a relational density-basedclustering algorithm that delineates nameclusters based on pairwise similarity.
Wedemonstrate the effectiveness of our methodson the WePS benchmark datasets and pointout future research directions.1 IntroductionThe explosive growth of web data offers users boththe opportunity and the challenge to discover andintegrate information from disparate sources.
Asalluded to in the title, a search query of the commonname ?Mark Johnson?
refers to as many as 70namesakes in the top 100 search results from theYahoo!
search engine, only one of whom is theBrown University professor and co-author of anACL 2006 paper (see experiments).
Cross documentcoreference (CDC) (Bagga and Baldwin, 1998) is adistinct technology that consolidates named entitiesacross documents according to their real referents.Despite the variety of styles and content in differenttext, CDC can break the boundaries of documentsand cluster those mentions referring to the same?Contact author: jhuang@ist.psu.eduMark Johnson.
As unambiguous person referencesare key to many tasks, e.g.
social network analysis,this work focuses on person named entities.
Themethod can be later extended to organizations.We highlight the key differences between ourproposed CDC system with past person namesearch systems.
First, we seek to transcend thesimple bag of words approaches in earlier CDCwork by leveraging state-of-the-art informationextraction (IE) tools for disambiguation.
Themain advantage is that our IE based approach hasaccess to accurate information such as a person?swork titles, geo-locations, relationships and otherattributes.
Traditional IR approaches, on the otherhand, may naively use the terms in a documentwhich can significantly hamper accuracy.
Forinstance, an article about Hillary Clinton maycontain references to journalists, politicians whomake comments about her.
Even with careful wordselection, such textual features may still confuse thedisambiguation system about the true identity of theperson.
The information extraction process in ourwork can thus be regarded as an intelligent featureselection step for disambiguation.
Second, aftercoreferencing, our system not only yields clustersof documents, but also structured informationwhich is highly useful for automated documentunderstanding and data mining.We review related work on CDC next anddescribe our approach in Section 3.
The methodsare evaluated on benchmark datasets in Section 4.We discuss directions for future improvement inSection 5 and conclude in Section 6.2 Related WorkThere is a long tradition of work on the withindocument coreference (WDC) problem in NLP,7which links named entities with the same referentwithin a document into a WDC chain.
State-of-the-art WDC systems, e.g.
(Ng and Cardie, 2001),leverage rich lexical features and use supervisedand unsupervised machine learning methods.Research on cross document coreference beganmore recently.
(Bagga and Baldwin, 1998) proposeda CDC system to merge the WDC chains using theVector Space Model on the summary sentences.
(Gooi and Allan, 2004) simplified this approach byeliminating the WDC module without significantdeterioration in performance.
Clustering approaches(e.g.
hierarchical agglomerative clustering (Mannand Yarowsky, 2003)) have been commonly usedfor CDC due to the variety of data distributionsof different names.
Our work goes beyond thesimple co-occurrence features (Bagga and Baldwin,1998) and the limited extracted information (e.g.biographical information in (Mann and Yarowsky,2003) that is relatively scarce in web data) usingthe broad range of relational information with thesupport of information extraction tools.
Thereare also other related research problems.
(Li etal., 2004) solved the robust reading problem byadopting a probabilistic view on how documents aregenerated and how names are sprinkled into them.Our previous work (Huang et al, 2006) resolvedthe author name ambiguity problem based on themetadata records extracted from academic papers.3 MethodsThe overall framework of our CDC system worksas follows.
Given a document, the informationextraction tool first extracts named entities andconstructs WDC chains.
It also creates linkages(relationships) between entities.
The similaritybetween a pair of relationships in WDC chainsis measured by an awakened similarity specialistand the similarity between two WDC chains isdetermined by the mixture of awakened specialists?predictions.
Finally, a density-based clusteringmethod generates clusters corresponding to realworld entities.
We describe these steps in detail.3.1 Entity and Relationship ExtractionGiven a document, an information extractiontool is first used to extract named entities andperform within document coreference.
Hence,named entities in each document are divided intoa set of WDC chains, each chain correspondingto one real world entity.
In addition, state-of-the-art IE tools are capable of creating relationalinformation between named entities.
We use anIE tool AeroText1 (Taylor, 2004) for this purpose.Besides the attribute information about the personnamed entity (first/middle/last names, gender,mention, etc), AeroText also extracts relationshipinformation between named entities, such asFamily, List, Employment, Ownership, Citizen-Resident-Religion-Ethnicity, etc, as specified in theAutomatic Content Extraction (ACE) evaluation.The input to the CDC system is a set of WDC chains(with relationship information stored in them) andthe CDC task is to merge these WDC chains2.3.2 Similarity FeaturesWe design a suite of similarity functions todetermine whether the relationships in a pair ofWDC chains match, divided into three groups:Text similarity.
To decide whether two namesin the co-occurrence or family relationship match,we use SoftTFIDF (Cohen et al, 2003), which hasshown best performance among various similarityschemes tested for name matching.
SoftTFIDF isa hybrid matching scheme that combines the token-based TFIDF with the Jaro-Winkler string distancemetric.
This permits inexact matching of namedentities due to name variations, spelling errors, etc.Semantic similarity.
Text or syntactic similarity isnot always sufficient for matching relationships.
Forinstance, although the mentions ?U.S.
President?and ?Commander-in-chief?
have no textual overlap,they are semantically highly related as they can besynonyms.
We use WordNet and the informationtheory based JC semantic distance (Jiang andConrath, 1997) to measure the semantic similaritybetween concepts in relationships such as mention,employment, ownership and so on.1AeroText is a text mining application for contentanalysis, with main focus on information extractionincluding entity extraction and intrasource link analysis(see http://en.wikipedia.org/wiki/AeroText).2We make no distinctions whether WDC chains areextracted from the same document.
Indeed, the CDC systemcan correct the WDC errors due to lack of information formerging named entities within a document.8Other rule-based similarity.
Several othercases require special treatment.
For example, theemployment relationships of Senator and D-N.Y.should match based on domain knowledge.
Also,we design rule-based similarity functions to handlenicknames (Bill and William), acronyms (COLINGfor International Conference on ComputationalLinguistics), and geographical locations3.3.3 Learning a Similarity MatrixAfter the similarity features between a pair ofWDC chains are computed, we need to computethe pairwise distance metric for clustering.
(Cohenet al, 2003) trained a binary SVM model andinterpreted its confidence in predicting the negativeclass as the distance metric.
In our case of usinginformation extraction results for disambiguation,however, only some of the similarity features arepresent based on the availability of relationshipsin two WDC chains.
Therefore, we treat eachsimilarity function as a subordinate predictingalgorithm (called specialist) and utilize thespecialist learning framework (Freund et al, 1997)to combine the predictions.
Here, a specialist isawake only when the same relationships are presentin two WDC chains.
Also, a specialist can refrainfrom making a prediction for an instance if it isnot confident enough.
In addition to the similarityscores, specialists have different weights, e.g.
amatch in a family relationship is considered moreimportant than in a co-occurrence relationship.The Specialist Exponentiated Gradient (SEG)(Freund et al, 1997) algorithm is adopted to learnto mix the specialists?
prediction.
Given a setof T training instances {xt} (xt,i denotes thei-th specialist?s prediction), the SEG algorithmminimizes the square loss of the outcome y?
in anonline manner (Algorithm 1).
In each learningiteration, SEG first predict y?t using the set of awakeexperts Et with respect to instance xt.
The trueoutcome yt (1 for coreference and 0 otherwise) isthen revealed and square loss L is incurred.
SEGthen updates the weight distribution p accordingly.To sum up, the similarity between a pair of3Though a rich set of similarity features has been built formatching the relationships, they may not encompass all possiblecases in real world documents.
The goal of this work, however,is to focus on the algorithms instead of knowledge engineering.Algorithm 1 SEG (Freund et al, 1997)Input: Initial weight distribution p1;learning rate ?
> 0; training set {xt}1: for t=1 to T do2: Predict using:y?t =?i?Et ptixt,i?i?Et pti(1)3: Observe true label yt and incur square lossL(y?t, yt) = (y?t ?
yt)24: Update weight distribution: for i ?
Etpt+1i = ptie?2?xt,i(y?t?yt)?j?Et ptj?j?Et ptje?2?xt,i(y?t?yt)pt+1i = pti, otherwise5: end forOutput: Model pWDC chains wi and wj can be represented in asimilarity matrix R, with ri,j computed by the SEGprediction step using the learned weight distributionp (Equation 1).
A relational clustering algorithmthen clusters entities using R, as we introduce next.3.4 Relational ClusteringThe set of WDC chains to be clustered arerepresented by a relational similarity matrix.
Mostof the work in clustering, however, is only capableof clustering numerical object data (e.g.
K-means).Relational clustering algorithms, on the other hand,cluster objects based on the less direct measurementof similarity between object pairs.
We choose touse a density based clustering algorithm DBSCAN(Ester et al, 1996) mainly for two reasons.First, most clustering algorithm require thenumber of clusters K as an input parameter.
Theoptimal K can apparently vary greatly for nameswith different frequency and thus is a sensitiveparameter.
Even if a cluster validity index is usedto determine K, it usually requires running theunderlying clustering algorithm multiple timesand hence is inefficient for large scale data.DBSCAN, as a density based clustering method,only requires density parameters such as theradius of the neighborhood ?
that are universal fordifferent datasets.
As we show in the experiment,9density parameters are relatively insensitive fordisambiguation performance.Second, the distance metric in relational spacemay be non-Euclidean, rendering many clusteringalgorithms ineffective (e.g.
single linkage clusteringalgorithm is known to generate chain-shapedclusters).
Density-based clustering, on the otherhand, can generate clusters of arbitrary shapes sinceonly objects in dense areas are placed in a cluster.DBSCAN induces a density-based cluster bythe core objects, i.e.
objects having more thana specified number of other data objects in theirneighborhood of size ?.
In each clustering step, aseed object is checked to determine whether it?s acore object and if so, it induces other points of thesame cluster using breadth first search (otherwiseit?s considered as a noise point).
In interest ofspace, we refer readers to (Ester et al, 1996) foralgorithmic details of DBSCAN and now turnour attention to evaluating the disambiguationperformance of our methods.4 ExperimentsWe first formally define the evaluation metrics,followed by the introduction to the benchmark testsets and the system?s performance.4.1 Evaluation MeasuresWe evaluate the performance of our method usingthe standard purity and inverse purity clusteringmetrics.
Let a set of clusters C = {C1, ..., Cs}denote the system?s output and a set of categoriesD = {D1, ..., Dt} be the gold standard.
Both C andD are partitions of the WDC chains {w1, ..., wn}(n = ?i |Ci| = ?j |Dj |).
First, the precision ofa cluster Ci w.r.t.
a category Dj is defined as,Precision(Ci, Dj) = |Ci ?Dj ||Ci|Purity is defined as the weighted average of themaximum precision achieved by the clusters on oneof the categories,Purity(C,D) =s?i=1|Ci|n maxj Precision(Ci, Dj)Hence purity penalizes putting noise WDC chains ina cluster.
Trivially, the maximum purity (i.e.
1) canbe achieved by making one cluster per WDC chain(referred to as the one-in-one baseline).Reversing the role of clusters and categories,Inverse purity(C,D) def= Purity(D, C).
InversePurity penalizes splitting WDC chains belongingto the same category into different clusters.
Themaximum inverse purity can be achieved by puttingall chain in one cluster (all-in-one baseline).Purity and inverse purity are similar to theprecision and recall measures commonly usedin information retrieval.
There is a tradeoffrelationship between the two and their harmonicmean F0.5 is used for performance evaluation.4.2 DatasetsWe evaluate our methods using the benchmarktest collection from the ACL SemEval-2007 webperson search task (WePS hereafter) (Artiles et al,2007).
The test collection consists of three sets ofdocuments for 10 different names, sampled fromthe English Wikipedia (famous people), participantsof the ACL 2006 conference (computer scientists)and common names from the US Census data,respectively.
For each ambiguous name, the top 100documents retrieved from the Yahoo!
Search APIwere annotated by human annotators according tothe actual entity of the name.
This yields on average45 different real world entities per set and about 3kdocuments in total.We note that the annotation in WePS makes thesimplifying assumption that each document refers toonly one real world person among the namesakesin question.
The CDC task in the perspective ofthis paper, however, is to merge the WDC chainsrather than documents.
Hence in our evaluation,we adopt the document label to annotate the WDCchain from the document that corresponds to theperson name search query.
Despite the difference,the results of the one-in-one and all-in-one baselinesare almost identical to those reported in the WePSevaluation (F0.5 = 0.61, 0.40 respectively).
Hencethe performance reported here is comparable to theofficial evaluation results (Artiles et al, 2007).4.3 Experiment ResultsWe computed the similarity features from the WDCchains extracted from the WePS training data andsubsampled the non-coreferent pairs to generate a10Table 1: Cross document coreference performance(macro-averaged scores, I-Pur denotes inverse purity).Test set Method Purity I-Pur F0.5Wikipedia AT-CDC 0.684 0.725 0.687ACL-06 AT-CDC 0.792 0.657 0.712US Census AT-CDC 0.772 0.700 0.722GlobalAverageAT-CDC 0.749 0.695 0.708One-in-one 1.000 0.482 0.618All-in-one 0.279 1.000 0.389training set of around 32k pairwise instances.
Wethen used the SEG algorithm to learn the weightdistribution model.
The macro-averaged crossdocument coreference results on the WePS testsets are reported in Table 1.
The F0.5 score of ourCDC system (AT-CDC) is 0.708, comparable to thetest results of the first tier systems in the officialevaluation.
The two baselines are also included.Because the test set is very ambiguous (on averageonly two documents per real world entity), theone-in-one baseline has relatively high F0.5 score.The Wikipedia, ACL06 and US Census setshave on average 56, 31 and 50 entities per namerespectively.
We notice that as the data set becomesmore ambiguous, purity decreases implyingit?s harder for the system to discard irrelevantdocuments from a cluster.
The other case is truefor inverse purity.
In particular, we are interested inhow the coreference performance changes with thenumber of entities per name (which can be viewedas the ambiguity level of a data set).
This is shownin Figure 1.
We observe that in general the harmonicmean of the purity is fairly stable across differentnumber of entities per dataset (generally withinthe band between 0.6 and 0.8).
This is importantbecause the system?s performance does not varygreatly with the underlying data characteristics.There is a particular name (with only one underlyingreferent) that appears to be an outlier in performancein Figure 1.
After examining the extraction results,we notice that the extracted relationships refer tothe same person?s employment, coauthors and geo-locations.
The generated CDC clusters correctlyreflect the different aspects of the person but thesystem is unable to link them together due to thelack of information for merging.
This motivates usto further improve performance in future work.Figure 2 shows how the coreference performance00.20.40.60.810  10  20  30  40  50  60  70  80  90  100F(0.5)Number of entities per nameFigure 1: Coreference performance for names withdifferent number of real world entities.changes with different density parameter ?.
Weobserve that as we increase the size of the ?neighborhood, inverse purity increases indicatingthat more correct coreference decisions are made.On the other hand, purity decreases as more noiseWDC chains appear in clusters.
Due to this tradeoffrelationship, the F score is fairly stable with a widerange of ?
values and hence the density parameter israther insensitive (compared to, say, the number ofclusters K).5 Future WorkWe see several opportunities to improve thecoreference performance of the proposed methods.First, though the system?s performance comparesfavorably with the WePS submissions, we observethat purity is higher than inverse purity, indicatingthat the system finds it more difficult to linkcoreferent documents than to discard noise from0.3 0.35 0.4 0.45 0.5 0.5500.20.40.60.81?DBSCAN with different parameter settingsPurityInverse PurityF(0.5)Figure 2: Coreference performance with different ?.11clusters.
Thus coreferencing based solely on theinformation generated by an information extractiontool may not always be sufficient.
For one, itremains a huge challenge to develop a generalpurpose information extraction tool capable ofapplying to web documents with widely differentformats, styles, content, etc.
Also, even if theextraction results are perfect, relationships extractedfrom different documents may be of differenttypes (family memberships vs. geo-locations) andcannot be directly matched against one another.
Weare exploring several methods to complement theextracted relationships using other information:?
Context-aided CDC.
The context where an namedentity is extracted can be leveraged for coreference.The bag of words in the context tend to be less noisythan that from the entire document.
Moreover, wecan use noun phrase chunkers to extract base nounphrases from the context.
These word or phrase levelfeatures can serve as a safenet when the IE tool fails.?
Topic-based CDC.
Similar to (Li et al, 2004),document topics can be used to ameliorate thesparsity problem.
For example, the topics Sportand Education are important cues for differentiatingmentions of ?Michael Jordan?, which may refer to abasketball player, a computer science professor, etc.Second, as noted in the top WePS run (Chen andMartin, 2007), feature development is important inachieving good coreference performance.
We aimto improve the set of similarity specialists in oursystem by leveraging large knowledge bases.Moreover, although the CDC system is developedin the web person search context, the methods arealso applicable to other scenarios.
For instance,there is tremendous interest in building structureddatabases from unstructured text such as enterprisedocuments and news articles for data mining, whereCDC is a key step for ?understanding?
documentsfrom disparate sources.
We plan to continue ourinvestigations along these lines.6 ConclusionsWe have presented and implemented an informationextraction based Cross Document Coreference(CDC) system that employs supervised andunsupervised learning methods.
We evaluatedthe proposed methods with experiments on alarge benchmark disambiguation collection, whichdemonstrate that the proposed methods comparefavorably with the top runs in the SemEvalevaluation.
We believe that by incorporatinginformation such as context and topic, besides theextracted relationships, the performance of the CDCcan be further improved.
We have outlined researchplans to address this and several other issues.ReferencesJavier Artiles, Julio Gonzalo, and Satoshi Sekine.
2007.The SemEval-2007 WePS evaluation: Establishing abenchmark for the web people search task.
In Proc 4thInt?l Workshop on Semantic Evaluations (SemEval).Amit Bagga and Breck Baldwin.
1998.
Entity-basedcross-document coreferencing using the vector spacemodel.
In Proc.
of 36th ACL and 17th COLING.Ying Chen and James Martin.
2007.
Towards robustunsupervised personal name disambiguation.
InProceedings of EMNLP and CoNLL, pages 190?198.William W. Cohen, Pradeep Ravikumar, and Stephen E.Fienberg.
2003.
A comparison of string distancemetrics for name-matching tasks.
In Proc.
of IJCAIWorkshop on Information Integration on the Web.Martin Ester, Hans-Peter Kriegel, Jorg Sander, andXiaowei Xu.
1996.
A density-based algorithm fordiscovering clusters in large spatial databases withnoise.
In Proceedings of 2nd KDD, pages 226 ?
231.Yoav Freund, Robert E. Schapire, Yoram Singer, andManfred K. Warmuth.
1997.
Using and combiningpredictors that specialize.
In Proceedings of 29th ACMsymposium on Theory of computing (STOC).Chung H. Gooi and James Allan.
2004.
Cross-documentcoreference on a large scale corpus.
In HLT-NAACL.Jian Huang, Seyda Ertekin, and C. Lee Giles.2006.
Efficient name disambiguation for large scaledatabases.
In Proc.
of 10th PKDD, pages 536 ?
544.Jay J. Jiang and David W. Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexicaltaxonomy.
In Proceedings of ROCLING X.Xin Li, Paul Morie, and Dan Roth.
2004.
Robustreading: Identification and tracing of ambiguousnames.
In Proceedings of HLT-NAACL, pages 17?24.Gideon S. Mann and David Yarowsky.
2003.Unsupervised personal name disambiguation.
InProceedings of HLT-NAACL, pages 33?40.Vincent Ng and Claire Cardie.
2001.
Improving machinelearning approaches to coreference resolution.
InProceedings of the 40th ACL, pages 104?111.Sarah M. Taylor.
2004.
Information extractiontools: Deciphering human language.
IT Professional,6(6):28 ?
34.12
