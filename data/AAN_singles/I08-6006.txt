Statistical Transliteration for Cross Langauge Information Retrieval usingHMM alignment and CRFSurya Ganesh, Sree HarshaLTRC, IIITHyderabad, Indiasuryag,sreeharshay@students.iiit.netPrasad Pingali, Vasudeva VarmaLTRC, IIITHyderabad, Indiapvvpr,vv@iiit.netAbstractIn this paper we present a statistical translit-eration technique that is language indepen-dent.
This technique uses Hidden MarkovModel (HMM) alignment and ConditionalRandom Fields (CRF), a discriminativemodel.
HMM alignment maximizes theprobability of the observed (source, target)word pairs using the expectation maximiza-tion algorithm and then the character levelalignments (n-gram) are set to maximumposterior predictions of the model.
CRFhas efficient training and decoding processeswhich is conditioned on both source andtarget languages and produces globally op-timal solutions.
We apply this techniquefor Hindi-English transliteration task.
Theresults show that our technique perfomsbetter than the existing transliteration sys-tem which uses HMM alignment and con-ditional probabilities derived from countingthe alignments.1 IntroductionIn cross language information retrieval (CLIR)a user issues a query in one language to searcha document collection in a different language.Out of Vocabulary (OOV) words are problematicin CLIR.
These words are a common source oferrors in CLIR.
Most of the query terms are OOVwords like named entities, numbers, acronyms andtechnical terms.
These words are seldom found inBilingual dictionaries used for translation.
Thesewords can be the most important words in the query.These words need to be transcribed into documentlanguage when query and document languagesdo not share common alphabet.
The practice oftranscribing a word or text written in one languageinto another language is called transliteration.A source language word can have more thanone valid transliteration in target language.
Forexample for the Hindi word below four differenttransliterations are possible .- gautam, gautham, gowtam, gowthamTherefore, in a CLIR context, it becomes im-portant to generate all possible transliterations toretrieve documents containing any of the givenforms.Most current transliteration systems use a gen-erative model for transliteration such as freelyavailable GIZA++1 (Och and Ney , 2000),an im-plementation of the IBM alignment models (Brownet al, 1993).
These systems use GIZA++ (whichuses HMM alignment) to get character levelalignments (n-gram) from word aligned data.
Thetransliteration system was built by counting up thealignments and converting the counts to conditionalprobabilities.
The readers are strongly encouragedto refer to (Nasreen and Larkey , 2003) to have adetailed understanding of this technique.In this paper, we present a simple statisticaltechnique for transliteration.
This techniqueuses HMM alignment and Conditional RandomFields (Hanna , 2004) a discriminative model.Based on this technique desired number of translit-erations are generated for a given source languageword.
We also describe the Hindi-English transliter-ation system built by us.
However there is nothingparticular to both these languages in the system.We evaluate the transliteration system on a testset of proper names from Hindi-English paralleltransliterated word lists.
We compare the efficiencyof this system with the system that was developedusing HMMs (Hidden Markov Models) only.1http://www.fjoch.com/GIZA++.html2 Previous workEarlier work in the field of Hindi CLIR was doneby Jaleel and Larkey (Larkey et al, 2003).
They didthis based on their work in English-Arabic transliter-ation for cross language Information retrieval (Nas-reen and Larkey , 2003).
Their approach wasbased on HMM using GIZA++ (Och and Ney ,2000).
Prior work in Arabic-English translitera-tion for machine translation purpose was done byArababi (Arbabi et al, 1994).
They developed a hy-brid neural network and knowledge-based system togenerate multiple English spellings for Arabic per-son names.
Knight and Graehl (Knight and Graehl, 1997) developed a five stage statistical model todo back transliteration, that is, recover the originalEnglish name from its transliteration into JapaneseKatakana.
Stalls and Knight (Stalls and Knight ,1998) adapted this approach for back translitera-tion from Arabic to English of English names.
Al-Onaizan and Knight (Onaizan and Knight , 2002)have produced a simpler Arabic/English translitera-tor and evaluates how well their system can match asource spelling.
Their work includes an evaluationof the transliterations in terms of their reasonable-ness according to human judges.
None of these stud-ies measures their performance on a retrieval task oron other NLP tasks.
Fujii and Ishikawa (Fujii andIshikawa , 2001) describe a transliteration systemfor English-Japanese cross language IR that requiressome linguistic knowledge.
They evaluate the ef-fectiveness of their system on an English-Japanesecross language IR task.3 Problem DescriptionThe problem can be stated formally as a se-quence labelling problem from one language al-phabet to other.
Consider a source language wordx1x2..xi..xN where each xi is treated as a wordin the observation sequence.
Let the equivalenttarget language orthography of the same word bey1y2..yi..yN where each yi is treated as a label inthe label sequence.
The task here is to generate avalid target language word (label suquence) for thesource language word (observation sequence).x1 ??????
y1x2 ??????
y2.
?????
?- ..
?????
?- ..
?????
?- .xN ??????
yNHere the valid target language alphabet(yi) for asource language alphabet(xi) in the input sourcelanguage word may depend on various factors like1.
The source language alphabet in the inputword.2.
The context(alphabets) surrounding source lan-guage alphabet(xi) in the input word.3.
The context(alphabets) surrounding target lan-guage alphabet(yi) in the desired output word.4 Transliteration using HMM alignmentand CRFOur approach for transliteration is divided intotwo phases.
The first phase induces characteralignments over a word-aligned bilingual corpus,and the second phase uses some statistics over thealignments to transliterate the source language wordand generate the desired number of target languagewords.The selected statistical model for translitera-tion is based on HMM alignment and CRF.
HMMalignment maximizes the probability of the observed(source, target) word pairs using the expectationmaximization algorithm.
After the maximizationprocess is complete, the character level alignments(n-gram) are set to maximum posterior predictionsof the model.
This alignment is used to get char-acter level alignment (n-gram) of source and targetlanguage words.
From the character level alignmentobtained we compare each source language charac-ter (n-gram) to a word and its corresponding targetlanguage character (n-gram) to a label.
Conditionalrandom fields (CRFs) are a probabilistic frameworkfor labeling and segmenting sequential data.
We useCRFs to generate target language word (similar tolabel sequence) from source language word (similarto observation sequence).CRFs are undirected graphical models whichdefine a conditional distribution over a labelsequence given an observation sequence.
Wedefine CRFs as conditional probability distributionsP (Y |X) of target language words given sourcelanguage words.
The probability of a particulartarget language word Y given source language wordX is the normalized product of potential functionseach of the forme(?j?jtj(Yi?1,Yi,X,i))+(?k?ksk(Yi,X,i))where tj(Yi?1, Yi, X, i) is a transition featurefunction of the entire source language word and thetarget language characters (n-gram) at positions iand i?
1 in the target language word; sk(Yi, X, i) isa state feature function of the target language wordat position i and the source language word; and ?jand ?k are parameters to be estimated from trainingdata.Fj(Y,X) =n?i=1fj(Yi?1, Yi, X, i)where each fj(Yi?1, Yi, X, i) is either a statefunction s(Yi?1, Yi, X, i) or a transition functiont(Yi?1, Yi, X, i).
This allows the probability of a tar-get language word Y given a source language wordX to be written asP (Y |X,?)
= (1Z(X))e(?
?jFj(Y,X))Z(X) is a normalization factor.The parameters of the CRF are usually estimatedfrom a fully observed training data {(x(k), y(k))}.The product of the above equation over all trainingwords, as a function of the parameters ?, is knownas the likelihood, denoted by p({y(k)}|{x(k)}, ?
).Maximum likelihood training chooses parametervalues such that the logarithm of the likelihood,known as the log-likelihood, is maximized.
For aCRF, the log-likelihood is given byL(?)
=?k[log1Z(x(k))+?j?jFj(y(k), x(k))]This function is concave, guaranteeing con-vergence to the global maximum.
Maximumlikelihood parameters must be identified usingan iterative technique such as iterative scal-ing (Berger , 1997) (Darroch and Ratcliff, 1972)or gradient-based methods (Wallach , 2002).Finally after training the model using CRF we gen-erate desired number of transliterations for a givensource language word.5 Hindi - English Transliteration systemThe whole model has three important phases.
Twoof them are off-line processes and the other is a runtime process.
The two off-line phases are prepro-cessing the parallel corpora and training the modelusing CRF++2.
CRF++ is a simple, customizable,and open source implementation of ConditionalRandom Fields (CRFs) for segmenting/labeling se-quential data.
The on-line phase involves generat-ing desired number of transliterations for the givenHindi word (UTF-8 encoded).5.1 PreprocessingThe training file is converted into a format requiredby CRF++.
The sequence of steps in preprocessingare1.
Both Hindi and English words were prefixedwith a begin symbol B and suffixed with an endsymbol E which correspond to start and endstates.
English words were converted to lowercase.2.
The training words were segmented in to uni-grams and the English-Hindi word pairs werealigned using GIZA++, with English as thesource language and Hindi as target language.3.
The instances in which GIZA++ aligned a se-quence of English characters to a single Hindiunicode character were counted.
The 50 mostfrequent of these character sequences wereadded to English symbol inventory.
There werehardly any instances in which a sequence ofHindi unicode characters were aligned to a sin-gle English character.
So, in our model we con-sider Hindi unicode characters, NULL, En-glish unigrams and English n-grams.4.
The English training words were re segmentedbased on the new symbol inventory, i.e., if2http://crfpp.sourceforge.net/a character was a part of an n-gram, it wasgrouped with the other characters in the n-gram.
If not, it was rendered separately.GIZA++ was used to align the above Hindiand English training word pairs, with Hindias source language and English as target lan-guage.These four steps are performed to get the char-acter level alignment (n-grams) for each sourceand target language training words.5.
The alignment file from the GIZA++ outputis used to generate training file as required byCRF++ to work.
In the training file a Hindi uni-code character aligned to a English uni-gram orn-gram is called a token.
Each token must berepresented in one line, with the columns sepa-rated by white space (spaces or tabular charac-ters).Each token should have equal number ofcolumns.5.2 Training PhaseThe preprocessing phase converts the corpus intoCRF++ input file format.
This file is used totrain the CRF model.
The training requires a tem-plate file which specifies the features to be selectedby the model.
The training is done using Lim-ited memory Broyden-Fletcher-Goldfarb-Shannonmethod(LBFGS) (Liu and Nocedal, 1989) whichuses quasi-newton algorithm for large scale numer-ical optimization problem.
We used Hindi unicodecharacters as features for our model and a windowsize of 5.5.3 TransliterationThe list of Hindi words that need to be translit-erated is taken.
These words are converted intoCRF++ test file format and transliterated using thetrained model which gives the top n probable En-glish words.
CRF++ uses forward Viterbi and back-ward A* search whose combination produce the ex-act n-best results.6 EvaluationWe evaluate the two transliteration systems forHindi - English that use HMM alignment and CRFwith the system that uses HMM only in two ways.
Infirst evaluation method we compare transliterationaccuracies of the two systems using in-corpus (train-ing data) and out of corpus words.
In second methodwe compare CLIR performance of the two systemsusing Cross Language Evaluation Forum (CLEF)2007 ad-hoc bilingual track (Hindi-English) docu-ments in English language and 50 topics in HindiLanguage.
The evaluation document set consists ofnews articles and reports from Los Angeles Timesof 2002.
A set of 50 topics representing the informa-tion need were given in Hindi.
A set of human rele-vance judgements for these topics were generated byassessors at CLEF.
These relevance judgements arebinary relevance judgements and are decided by ahuman assessor after reviewing a set of pooled doc-uments using the relevant document pooling tech-nique.
The system evaluation framework is similarto the Craneld style system evaluations and the mea-sures are similar to those used in TREC3.6.1 Transliteration accuracyWe trained the model on 30,000 words containingIndian city names, Indian family names, Male firstnames and last names, Female first names and lastnames.
We compare this model with the HMMmodel trained on same training data.
We tested boththe models using in-corpus (training data) and outof corpus words.
The out of corpus words consist ofboth Indian and foreign place names, person names.We evaluate both the models by considering top 5,10, 15 and 20 transliterations.
Accuracy was calcu-lated using the following equation belowAccuracy =CN?
100C - Number of test words with the correct transliter-ation appeared in the desired number (5, 10, 15, 20,25) of transliterations.N - Total number of test words.The results for 30,000 in-corpus words and 1,000out of corpus words are shown in the table 1and table 2 respectively.
In below tables 1 & 2HMM model refers to the system developed usingHMM alignment and conditional probabilities de-rived from counting the alignments, HMM & CRFmodel refers to the system developed using HMM3Text Retrieval Conferences, http://trec.nist.govModel Top 5 Top 10 Top 15 Top 20 Top 25HMM 74.2 78.7 81.1 82.1 83.0HMM & CRF 76.5 83.6 86.5 88.9 89.7Table 1: Transliteration accuracy of the two systems for in-corpus words.Model Top 5 Top 10 Top 15 Top 20 Top 25HMM 69.3 74.3 77.8 80.5 81.3HMM & CRF 72.1 79.9 83.5 85.6 86.5Table 2: Transliteration accuracy of the two systems for out of corpus words.alignment and CRF for generating top n translitera-tions.CRF models for Named entity recognition, POStagging etc.
have efficiency in high nineties whentested on training data.
Here the efficiency (Table 1)is low due to the use of HMM alignment in GIZA++.We observe that there is a good improvement inthe efficiency of the system with the increase in thenumber of transliterations up to some extent(20) andafter that there is no significant improvement in theefficiency with the increase in the number of translit-erations.During testing, the efficiency was calculated by con-sidering only one of the correct transliterations pos-sible for a given Hindi word.
If we consider all thecorrect transliterations the efficiency will be muchmore.The results clearly show that CRF model per-forms better than HMM model for Hindi to Englishtransliteration.6.2 CLIR EvaluationIn this section we evaluate the transliterations pro-duced by the two systems in CLIR task, the task forwhich these transliteration systems were developed.We tested the systems on the CLEF 2007 documentsand 50 topics.
The topics which contain named enti-ties are few in number; there were around 15 topicswith them.
These topics were used for evaluation ofboth the systems.We developed a basic CLIR system which per-forms the following steps1.
Tokenizes the Hindi query and removes stopwords.2.
Performs query translation; each Hindi word islooked up in a Hindi - English dictionary andall the English meanings for the Hindi wordwere added to the translated query and for thewords which were not found in the dictionary,top 20 transliterations generated by one of thesystems are added to the query.3.
Retrieves relevant documents by giving trans-lated query to CLEF documents.We present standard IR evaluation metrics such asprecision, mean average precision(MAP) etc.. in thetable 3 below for the two systems.The above results show a small improvement indifferent IR metrics for the system developed usingHMM alignment and CRF when compared to theother system.
The difference in metrics between thesystems is low because the number of topics testedand the number of named entities in the tested topicsis low.7 Future WorkThe selected statistical model for transliteration isbased on HMM alignment and CRF.
This alignmentmodel is used to get character level alignment (n-gram) of source and target language words.
Thealignment model uses IBM models, such as Model4, that resort to heuristic search techniques to ap-proximate forward-backward and Viterbi inference,which sacrifice optimality for tractability.
So, weplan to use discriminative model CRF for characterlevel alignment (Phil and Trevor , 2006) of sourceand target language words.
The behaviour of theother discrminative models such as Maximum En-tropy models etc., towards the transliteration taskModel P10 tot rel tot rel ret MAP bprefHMM 0.3308 13000 3493 0.1347 0.2687HMM & CRF 0.4154 13000 3687 0.1499 0.2836Table 3: IR Evaluation of the two systems.also needs to be verified.8 ConclusionWe demonstrated a statistical transliteration sys-tem using HMM alignment and CRF for CLIR thatworks better than using HMMs alone.
The followingare our important observations.1.
With the increase in number of output targetlanguage words for a given source languageword the efficiency of the system increases.2.
The difference between efficiencies for top nand n-5 where n > 5; is decreasing on increas-ing the n value.ReferencesA.
L. Berger.
1997.
The improved iterative scaling algo-rithm: A gentle introduction.Al-Onaizan Y, Knight K. 2002.
Machine transla-tion of names in Arabic text.
Proceedings of the ACLconference workshop on computational approaches toSemitic languages.Arababi Mansur, Scott M. Fischthal, Vincent C. Cheng,and Elizabeth Bar.
1994.
Algorithms for Arabic nametransliteration.
IBM Journal of research and Develop-ment.D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory BFGS method for large-scale optimization, Math.Programming 45 (1989), pp.
503?528.Fujii Atsushi and Tetsuya Ishikawa.
2001.Japanese/English Cross-Language InformationRetrieval: Exploration of Query Translation andTransliteration.
Computers and the Humanities,Vol.35, No.4, pp.389-420.H.
M. Wallach.
2002.
Efficient training of condi-tional random fields.
Masters thesis, University of Ed-inburgh.Hanna M. Wallach.
2004.
Conditional Random Fields:An Introduction.J.
Darroch and D. Ratcliff.
1972.
Generalized iterativescaling for log-linear models.
The Annals of Mathe-matical Statistics, 43:14701480.Knight Kevin and Graehl Jonathan.
1997.
Machinetransliteration.
In Proceedings of the 35th AnnualMeeting of the Association for Computational Linguis-tics, pp.
128-135.
Morgan Kaufmann.Larkey, Connell,AbdulJaleel.
2003.
Hindi CLIR inThirty Days.Nasreen Abdul Jaleel and Leah S. Larkey.
2003.
Sta-tistical Transliteration for English-Arabic Cross Lan-guage Information Retrieval.Och Franz Josef and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
Proc.
of the 38th AnnualMeeting of the Association for Computational Linguis-tics, pp.
440-447, Hong Kong, China.P.
F. Brown, S. A. Della Pietra, and R. L. Mercer.1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational Linguis-tics, 19(2):263-311.Phil Blunsom and Trevor Cohn.
2006.
DiscriminativeWord Alignment with Conditional Random Fields.Stalls Bonnie Glover and Kevin Knight.
1998.
Translat-ing names and technical terms in Arabic text.
