THE T IPSTEPdSHOGUN PROJECT *Paul S. Jacobs, George Krupka, Lisa Rau,Michael L. Mauldin, Teruko Mitamura, Tsuyoshi Kitani,Ira Sider and Lois ChildsABSTRACTThis paper presents an overview of the TIPSTER/SHOGUN project,the major esults, and the SHOGUN data extraction system.
TIP-STER/SHOGUN was a joint effort of GE Corporate Research andDevelopment, Carnegie Mellon University, and Martin MariettaManagement and Data Systems (formerly GE Aerospace), part ofthe ARPA TIPSTER Text program.
Two of the main technicalthrusts of the project were: (1) the development of a model of finite-state approximation, i  which the accuracy of more detailed modelsof language interpretation could be realized in a simple, efficientframework, and (2) (3) experiments in automated knowledge acqui-sition, to minimize customization a d ease the tuning and extensionof the system.
Innovations in each of these areas allowed the projectto meet its goal of achieving advances in coverage and accuracywhile showing consistently good performance across languages anddomains.1.
SHOGUN SYSTEM DESCRIPT IONThe GE-CMU TIPSTER/SHOGUN system is the result of atwo-year esearch effort, part of the ARPA-sponsored TIP-STER data extraction program.
The project's main goalswere: (1) to develop algorithms that would advance the stateof the art in coverage and accuracy in data extraction, and(2) to demonstrate high performance across languages anddomains and to develop methods for easing the adaptation ofthe system to new languages and domains.The system as used in MUC-5 (the final TIPSTER bench-mark) represents a considerable shift from those used in earlierstages of the program and in previous MUC's.
The originalSHOGUN design integrated several different approaches bycombining different knowledge sources, such as syntax, se-mantics, phrasal rules, and domain knowledge, at run-time.This allowed the system to achieve a good level of perfor-mance very quickly, and made it easy to test different mod-ules and methods; however, it proved very difficult to makeall the changes necessary to improve the system, especiallyacross languages, when system knowledge was so distributedat run-time.As a result, the team adopted anew approach, relying heavilyonfinite-state pproximation.
This method combines everal*For information contact Paul Jacobs, Information Technology Labora-tory, GE Research and Development, Schenectady, NY 12301earlier previous of work, including Pereira's research on gram-mar approximation \[7\], some of the original ideas on parsercompilation from Tomita \[ 12\], and GE's representation f thedynamic lexicon \[5, 3\].
Like Pereira's model, the systemuses a finite-state grammar as a loose version of a contextfree grammar, under the assumption that the finite state gram-mar will cover all the inputs that the general grammar wouldrecognize but perhaps be more tolerant.
However, the sys-tem also includes methods for compiling different knowledgesources into the finite state model, particularly emphasizinglexical knowledge and domain knowledge as reflected in acorpus.This model, in which knowledge iscombined at developmenttime to be used by a finite-state pattern matching engine atrun-time, makes it easier to tune the system to a new languageor domain without sacrificing the benefit of having generallinguistic and conceptual knowledge in the system.While the GE systems, and more recently, the GE-CMU sys-tems, have done well in all the MUC evaluations, our rate ofprogress has never been so great as it has been in the periodbefore MUC-5.
This is in spite of the fact that the team'sdiagnostic and debugging efforts had to be divided acrosslanguages and domains (handling Japanese, for example, pre-sented a significant overhead in simply being able to followthe rules and analyze the results).
We attribute this progress tothe current focus on facilitating and automating the knowledgeacquisition process, especially on the use of a corpus.The TIPSTER/SHOGUN system as configured for the 24-month/MUC-5 benchmark has roughly the same componentsas earlier versions of the system, but the system now performslinguistic analysis entirely using a finite-state pattern matcher,instead of LR parsing or chart-style parsing, both of whichwere part of the configuration i MUC-4.Figure 1 shows the basic components ofthe SHOGUN system,using our own names for modules, where applicable, alongwith the labels used in Jerry Hobbs' paper "The Generic In-formation Extraction System" from the MUC-5 proceedings\[11\].
The core components of SHOGUN are a subset of themodules that Hobbs describes.
However, the system differsfrom other current extraction systems in the use of the finite-209state analyzer and the way that corpus-based knowledge isintegrated into the lexico-syntactic rules.Because many of the MUC-5 systems now perform muchthe same type of pre-processing, name recognition, and postprocessing that SHOGUN has, we will concentrate here onlinguistic analysis, including parsing and lexical disambigua-tion, which were the main research areas of our work onSHOGUN.About half of the MUC-5 systems till use linguistic analysisdriven by "traditional" phrase structure rules, traditional inthe sense that here is a clearly separable syntactic componentwhose knowledge consists mainly of rules for recognizinggrammatical constituents based on word categories (like noun,verb) and word order.
SHOGUN differs from all these systemsin that it no longer has any purely syntactic omponent, anduses finite state rules in place of phrase structure rules.The remaining systems divide roughly into those that em-phasize pattern matching and those that emphasize fragmentparsing.
The fragment parsing systems, notably BBN's, workfairly close to the way our MUC-4 system did, taking ad-vantage of partial parses by using a combination of syn-tactic and domain knowledge to guide the combination ofsyntactic hunks.
The difference between this approach andSHOGUN's current processing isthat fragment parsing is stilla largely syntax-first method, while pattern matching tendsto introduce specialized omain and corpus knowledge bycombining this knowledge with syntactic knowledge in thesystem's declarative r presentation.By this coarse characterization, the "pattern matching" groupof systems includes, for example, SRI and Unisys as well asGE-CMU.
We also consider UMass to be in this category,because their linguistic analysis emphasizes lexical and con-ceptual knowledge rather than constituent s ructure.Among these approaches, we believe the main differentia-tor is not in the basic processing algorithms but in the waythat knowledge nds up getting assigned to various systemcomponents.
If there is one noteworthy trend among theMUC systems as they have evolved over time, it is that theyhave become more knowledge-based, specially emphasizingmore corpus-based and lexical knowledge as well as auto-mated knowledge acquisition methods.
Within the emerging"generic" model, the main difference among systems is thusin the content of their knowledge bases.
Here, the distin-guishing characteristic of SHOGUN is probably the degreeto which the system still includes entence-level knowledge,assigning linguistic and conceptual roles much the way theTRUMP/TRUMPET combination did but using more de-tailed, lexically-driven knowledge.
Many of the sentence-level rules, for example, include groupings like start a faci l i tyand organization oun phrase, which combine traditional syn-tactic phrases with lexical or domain knowledge.As systems continue to become still broader in scope andmore accurate, it is likely that the way knowledge is acquiredwill become the main differentiator.The SHOGUN system is written in Common Lisp and runson SUN workstations.
Typical processing time is about 1000words per minute.
Components of the system have beenported to other languages and hardware and software nvi-ronments; the core pattern matcher of the system has been re-engineered in "C" and runs on many platforms.
In these otherenvironments, hroughput tends to be considerably higher.The rest of this paper will discuss the overall results ofSHOGUN on MUC-5 and describe how the system handlessome of the system walkthrough examples.
The analysis ofthe examples will highlight some of these characteristics anddemonstrate the system's actions in various tages of process-ing.2.
PROJECT GOALS AND SYSTEMEVOLUTIONWe have described the general architecture of SHOGUN aswell as the experiments our project did with different con-trol strategies.
The end result of the project represented ashift from our original plan, and also somewhat of a sur-prise.
The "new control strategy" we proposed began asrelation-driven control \[4\]ma method of integrating differentsources of knowledge at run-time--and ended up as finite-state approximation--a simpler method in which knowledge-sources are combined at development time.
Although themajor goals of the new control architecture--integratingknowledge sources, enabling corpus-based knowledge acqui-sition, and introducing domain and corpus knowledge arlyin processingmwere all carried out, the final implementedmethod iffered from the original plan significantly in thatit combined knowledge sources mainly at development timerather than at run time.This shift came from the results of two experiments, alongwith one important program goal.
The experimental resultswere (1) the effect of different parsing strategies in MUC-4\[10, 8\] and (2) the surprising success of Boolean retrievalstrategies in text retrieval, as illustrated in the first Text Re-trieval Conference (TREC) \[2, 6\].
Both of these experimentswere carried out near the mid-point of our TIPSTER project.The MUC-4 results combined several important achieve-ments.
First, in order to accommodate our objective of usingthe CMU generalized LR parser in the SHOGUN system, weintegrated CMU's parser with GE's semantic interpreter, ac-complishing what we believe is the first-ever successful com-bination of modules of this scope.
Although the LR parserrecovery strategies were not nearly as well developed, and210Finite-state Sentence Analysis(MUC .5 System)("parser")("lexical disambiguation'9text structure ("zoner")NLlex ("preprocessor")PM1 ("filter") (English)statistical f i l te r  (ME)PM2 ("preparser")TRUMPLR ParserTRUMPET("fragment combiner")"semantic interpreter""discourse processing""template generator"Core lexicons and grammarsFigure 1: SHOGUN configuration i MUC-5the LR parser was slower than the GE TRUMP parser, theaccuracy of SHOGUN with the LR parser on MUC-4 wasreasonably close to that of the GE system on MUC-4.
Atthe same time, we were able to experiment with a variety ofdifferent relation-driven control strategies using both parsers,and the net result was the differences in control seemed tohave very little effect on the system.
We became certain thatwe would not achieve any significant advance with parsercontrol as the central component of our method.
Finally, aspart of our MUC-4 system we had significantly improved thecapabilities of our finite-state driven pre-processor, sowe be-gan to ask what more complex parsing strategies had to offerover finite-state driven strategies, and to take the major stepof bypassing traditional parsing altogether.The TREC results also presented a surprise.
GE's TREC-1 experiment, carried on outside of the TIPSTER program,showed that finite-state pattern matching could apply on abroad scale to large volumes of text, but that the power of thefinite state engine contributed very little beyond the contentof the words and combinations of the words in the patterns.In other words, a Boolean engine recognizing complex com-binations of words would do as well in a routing and retrievaltask as a mechanism using the same combinations along withlinguistic constraints such as part of speech, proximity, andword order.The other important influence on the selection of the the newcontrol strategy was the program goal of getting Japaneseprocessing up to the level of English processing.
At the12-month point of the project, SHOGUN's Japanese perfor-mance was not only behind English, but it was behind some ofthe other contractors.
Our original strategy had assumed theavailability of resources such as broad-coverage lexicons andgrammars, and while we had allowed for the development ofthese resources under our project, it looked like the Japaneseresources would remain well behind the English resources.
Inaddition, there was a problem with relying on the independentgrammars and lexicons in each language: Although the lex-icons tied together through a common ontology, there weremany common aspects of the task across languages that werevery hard to exploit with the emphasis on grammar-driveninterpretation--and theleverage across languages could be-come greater with a new approach.On the surface, these three influences--the r latively smalldifference among parsing strategies, the power of Booleanmethods in classifying texts, and the need for synergy acrosslanguages--may seem to have little in common, but there is acommon thread: They all suggest giving knowledge aboutthe content of texts greater weight than knowledge aboutstructure.
For example, in selecting among parsing strate-gies in our MUC-4 system, the reason that different choiceshad relatively little impact was that he bulk of the informationextracted epended on the labeling of key content elements--names of people, organizations, and event descriptions--atpre-processing time.
If a terrorist organization appeared inconnection with a terrorist event, it seemed to make little dif-ference what parsing strategy was used, so long as terrorist or-ganization ended up somehow in the final template.
In fact, the211more restrictive parsing strategies tended to do slightly worsethan looser strategies, and the loosest approach--basicallyusing the content elements of each sentence to "guess" thelinguistic structure of the sentence--was good as any.The success of Booleans in TREC teaches the same les-son.
The Booleans identify the key content elements of textsbecause they recognize words and combinations of words.If those words appear, enforcing structural constraints con-tributes very little (in general, it improves precision at theexpense of recall), even where one might expect that thestructure is critical.The same observation applies to synergy across languages.
Itis easy to find commonalities between English and Japanesejoint venture texts by emphasizing content--for example, ajoint venture description will usually include a joint ventureverb, the description of two or more partners, and, optionally,a joint venture company.
Structurally, the texts have little incommon.
The same applies to the lexicon: words like theJapanese kaihatsu (~)  and the corresponding English verbdevelop differ in terms of how they are used syntactically andin general, but in the domain of TIPSTER joint ventures thekey discriminations--whether it is land, business, or productsthat are being developed--are the same for the two verbs.In retrospect, hen, we believe the major lesson of our earlyexperiments, punctuated by the success of finite state approx-imation in the final benchmark, is that a content focus inprocessing and acquisition does much better than a structurefocus.
This does not mean that structural analysis of textscan't make a difference.
However, the relative impact ofstructural analysis is very small because the major challenge,especially in improving coverage, is acquiring and refiningcontent knowledge (i.e.
words, combinations of words, re-lationships between combinations of words and the "core"templates, and so forth).The second major area of technical progress in SHOGUNwas in knowledge acquisition.
The ease with which we addedknowledge to the system improved coverage, or recall.
Evenmore importantly, it led to portability.
It would be hard toattribute SHOGUN's consistently good performance acrosslanguages and domains to pre-existing resources--many ofwhich we did not actually use in the final system--or to thetime or effort spent working on each configuration-- whichwas relatively small, especially given our shift in mid-streamfrom one strategy to another.
We attribute this to the easewith which knowledge can be acquired, and to the degree ofknowledge sharing across languages and domains.The relationship between representation, i.e., the finite-statepatterns in our system, and acquisition, i.e., the method bywhich new knowledge is added, is critical.
Knowledge ac-quisition methods emphasize adding to knowledge resourcesthat a system uses.
If the main knowledge resource of a sys-tem is a lexicon, work focuses on lexical acquisition; if themain resource is domain knowledge, work tends to focus ondomain knowledge acquisition.
In our system, we chose toemphasize the finite-state patterns in part because they helpto take advantage ofthe most critical source of knowledge wehave available--the corpus.Knowledge acquisition was one area in which we experi-mented heavily in TIPSTER, and found many methods thatdidn't work as well as some that did.
As part of our initialexperiments at improving control, we tried a variety of sta-tistical approaches to part-of-speech labeling and grammartuning, which are generally very popular because they givethe appearance of improving parsing in the absence of otherknowledge.
In MUC-4, these techniques not only didn't helpperformance: they hurt overall performance.
In fact, theonly area where we got a benefit from acquiring knowledgefor parser tuning was where the statistical method turned upan error in parsing (for example, the word evening was some-times treated as a verb, but it was always a noun in the MUC-4corpus).
These bugs were always easily fixed.The second acquisition method that didn't help much waslearning from answer keys.
There were some exceptions--places where the keys did help--but, in most cases, therewas simply not enough data in the keys to train a systemautomatically.
The timing was bad, also, because many ofthe keys were not available until late in the project, so thesewasn't much we could do to experiment with them.Thus, the main acquisition strategy we chose was to relyvery heavily on the corpus.
Before manually adding anythingto the system's knowledge base, we used word frequencyinformation and keyword-in-context lists to select commonwords and examine how they appeared in the corpus.
Withthe help of a "collated" keyword-in-context browser in bothlanguages, we tried to find common groupings that reflectedthe context in which words were used and helped to resoh;eambiguities.
For common slots that required large amountsof knowledge, particularly the product-service slot in the jointventure texts, we relied heavily on statistical methods to findsets of related words and phrases.
We believe this accounts forthe tremendous differences in coverage between SHOGUNand other systems on these slots.In addition to helping coverage, the corpus-based acquisi-tion strategy greatly eased portability, particularly across lan-guages.
Much of the work done in Japanese (almost all ofthe work for micro-electronics) was done by non-speakers ofJapanese.
In most cases, we did each English component first,then used the English as a way of bootstrapping the Japanese.For example, we would take each important "pivot" word inEnglish, try to identify the corresponding "pivot" in Japanese,then use the corpus to identify the relevant contexts in which212that word occurred in Japanese.
This effectively matched thecontent of the Japanese patterns to that of the English patterns,making it very easy to acquire knowledge in Japanese.
Webelieve this result is shown most by SHOGUN's performanceon some of the "harder" slots in JJV, which was generallynear the level of English performance, although most sites didmuch worse on these slots in Japanese (although we must ex-clude the time and revenue objects, which few sites attemptedin either language).We have concluded that coverage and portability are syn-ergistic goals in natural anguage interpretation, and that arepresentation that emphasizes content rather than structure,in conjunction with an emphasis on corpus-based knowledge,is the secret of success in both.3.
ACCOMPL ISHMENTSThe major accomplishment of SHOGUN, in terms of results,was to show significant advances in coverage of data extractedwhile also demonstrating good performance across languagesand domains.
However, we believe that the results of theproject extend well beyond the benchmarks, including, forexample, the proven value of the finite state method, the suc-cess of the knowledge acquisition methodology, and a host ofexperiments hat showed what worked and what didn't.
Inaddition to demonstrating high coverage across languages inTIPSTER, we believe that these innovations set the stage forfuture research.3.1.
Methodology - Correcting MistakesOur approach is experimentally-oriented.
We do not see anycompetition between experimentalism and theory; however,there is often a competition between experimentalism andhistory.
Experiments often prove that the way we have beendoing things is wrong.
This only rarely shows that our the-ories are wrong (because theories usually are quite dilutedby the time they are implemented in broadly useful or func-tional systems).
But, because there is a general reluctance toacknowledge or report negative results, historical methods--those which have never been shown to work but have becomeaccepted as practice--often appear again and again even whenexperimental evidence points to different approaches.The advances in TIPSTER, like some of our previous ad-vances, have come from trying many different hings, oftendeparting from our intuitions and frequently from "conven-tional wisdom".
However, the results are generally supportedby theory.
Furthermore, in the context of community efforts,the goal must be to expand the polytheoretical spects andminimize the components of each system that are tied to aparticular theory, because those will be the hardest o shareand reuse.SHOGUN has shown the power of focusing on the content oftexts rather than the structure of sentences.
Sentence structurehas been the focus of most linguistic research, including com-putational linguistics.
Within computational linguistics, therehas been a great deal of work on grammatical formalisms,most of it aimed at developing notations that make it easyto represent linguistic generalizations, as well as to coverparticular constructs.
In theory, natural anguages fall intoa class of languages that are context-sensitive (or worse),thus requiring very powerful (and computationally expen-sive) methods.
However, the phenomena that fall outside thescope of context-free languages are sufficiently unusual that ithas become generally accepted that one should handle naturallanguages using a generally context-free notation, with someadditions to deal with the special cases.
This has come outwith a recent maxim, language is context-free plus epsilon,meaning only a small portion of linguistic phenomena requireanything more than a context free grammar.Now, within the class of context free grammars, the choice ofnotation and parsing strategy is largely a matter of personalpreference.
Some popular systems have used "textbook" con-text free parsing algorithms \[1 \], but others have relied on evenmore restrictive methods like shift-reduce parsing \[9\] and LRparsing \[ 12\].
These methods use the same grammars as themore general context free parsers, but rely on simpler compu-tational strategies for resolving ambiguity.The difference between finite state approximation, which canrecognize the class of regular languages, and the least power-ful context-free methods (LR and shift-reduce) is a very smallstep on the theoretical ladder of languages (the "Chomskyhierarchy").
While the difference between LR languages andgeneral context free languages boils down to the manner inwhich ambiguity istreated, the difference between regular lan-guages and LR languages, is, in essence, one phenomenon:recursion, or center embedding.
Regular grammars cannotmatch arbitrary levels of parentheses, for example, or dealwith embedded sentences of arbitrary complexity.
However,people cannot cope with such constructs, either, at least notwithout working for a while with pencil and paper.
So, intheory, it is quite reasonable to state that language is regularplus epsilon.
The choice o f  whether to use a context free orfinite state parsing model is simply a matter of convenience.In speech recognition work, the finite state model has almostalways been the convenient choice.
As a result, an indepen-dent line of research on finite-state approximation \[7\], stem-ming from the speech community, showed that context freerepresentations can be formally converted into finite state rec-ognizers, thus guaranteeing that the finite state method wouldrecognize very input that he context free system would.
Thefinite state model may also admit some inputs that the con-text free model would reject, but the benefit is that the finitestate method can use a simple linear-time algorithm, which isalso compatible with those used in real time speech recogni-213tion.
This method of mapping from context free to finite statemodels tends to be hard to use for large grammars because itcreates avery large number of states, but it bears out, theoret-ically, the close relationship between finite state and contextfree models of parsing.Therefore, we see finite state approximation asbeing experi-mentally motivated and theoretically sound.
The main barrierto the acceptance of the method in computational linguisticsis simply that it differs from the way linguists have grownaccustomed toviewing language.Generally speaking, finite state approximation departs fromthe traditional context free style of representation a d parsing,but there is also one claim in the "conventional" wisdom thathas been shown to be false.
This claim is that because morepowerful parsing models are able to handle a broader range oflinguistic onstructs and enforce more linguistic constraints,they will begin to overtake simpler models as data extractiontasks become broader and more complex.
In fact, exactly theopposite has happened.
In the four years between MUCK-IIand MUC-5, the sites using powerful parsing models haveslipped in their relative standings, and the sites that havemoved from stricter to looser models (particularly, GE andSRI, but also Paramax and LSI) have improved significantly.The most significant data point here is that SRI improvedvery significantly between MUC-3 and MUC-4 while aban-doning their traditional model in favor of a finite state strategy.This shows that even theoretically-minded groups will adopt asuccessful experimental strategy when faced with measurableobjectives.We must also note that, in the scope of the current emphasis oncommunity-wide efforts, in which the aim is to create reusableresources and algorithms, the move to simpler models of pars-ing, as well as the focus on acquisition, are big steps forward.Because finite-state pattern matching is relatively easy to im-plement (in fact, these methods can run on most any platformand even have available hardware support), research can con-centrate on how to develop the right patterns, how to compileknowledge resources, and how to use a corpus to help acquirefinite-state rules.3.2.
Overall Coverage and AccuracyImproved coverage was our strongest area.
Recall, the mainindicator of coverage, was extremely high relative to othersystems in all four configurations.
In addition, recall advanced37% on average between the TIPSTER 18-month evaluationand the 24-month evaluation and was 10% higher in the TIP-STER final test han in SHOGUN's results on MUC-4 (a mucheasier test of coverage).Accuracy, as measured by precision, was somewhat lowerthan some other systems.
We attribute this mostly to the vastlylarger amount of data that SHOGUN produced on much harderslots, at least in English joint ventures.
In micro-electronics,we cannot characterize the results across objects and slots,but the differences between systems, in general, were muchsmaller in micro-electronics.
SHOGUN's accuracy, in gen-eral, was higher than systems with comparable coverage onthe components of the task where there were other systemswith comparable coverage.?
The main contributor to these advances, as discussed above,was the combination of finite-state l vel interpretation withcorpus-driven knowledge acquisition methods.
These meth-ods effectively improved coverage, particularly in portionsof the task requiring larger amounts of knowledge, without asacrifice in accuracy.What didn't work in advancing coverage and accuracy, gen-erally speaking, were (1) methods of "fine tuning" systemcomponents, uch as parser tuning and tagging, and (2) mostautomated acquisition strategies, especially those based ontraining on answer keys.
The acquisition strategies that didwork were those that relied mainly on raw corpus data or onmanual intervention or bootstrapping.We believe that he focus of future work in extending coverageand accuracy must be in acquiring knowledge from corpora.Ideally, a good portion of this knowledge, and certainly agood portion of the acquisition methodology, can be usedacross languages and domains.
At the very least, corpus-based methods help to adapt and extend systems, but webelieve we have only begun to explore the means by whichthe corpus can contribute to application development.3.3.
Portability to New LanguagesWe have emphasized SHOGUN's consistently high perfor-mance across languages.
We can't argue that SHOGUN isthe most consistent system across languages because one sys-tem, SRI's, had identical error rates in Japanese and English.IV, and another system, BBN's, had error rates within a 6-point range (from 66 to 72) across the four configurations.SHOGUN's error rates were within an 11-point range (from54 to 65), and SHOGUN's error rates were consistently owerin Japanese than in English.
However, we believe that theJapanese configurations were somewhat easier than the En-glish; thus SHOGUN's performance was consistently good.in order to equalize resources and ease general portabilityacross languages, CMU built built a 17,854 entry lexicon(with 15,984 different words) of Japanese words from theTipster Corpus, supplemented by other sources to allow thelexicon to be used for domains other than Joint Ventures andMicro-electronics.Sources for lists of Japanese words came from the BBN com-pany name list, N'IT Data's MAJESTY program run overthe Japanese Joint Venture and Japanese Micro-electronics214corpora, plus previous Japanese dictionaries from the CMT-SEMSYN project.The next figure shows two Japanese lexicon entries, a nounand a verbal nominal.
Each entry is headed by the Kana orKanji as segmented by the MAJESTY segmenter, and then allsenses with that particular Kana/Kanji string are stored as alist under the :SENSES field.
( ~ ' - -  #, r~" ,1 #: POS n:TOKENS ( ):G -DERIVS  (): SENSES( ( de - tabanku: EXAMPLES ( q~\ [ \ ]~%~?- -  # r?
~ ~Y ): TYPE  *pr imary*:PAR (c - in fo rmat ion): SYNONYMS (data-bank):NOTE (i occur rences:n t td -kana  (,,'~--~C~'/~ < ,,): j v -dom :me-dom):S -DERIVS  ())))(: POS nsa:TOKENS ( ):G -DERIVS  (): SENSES( ( busshoku:EXAMPLES: TYPE  *pr imary*:PAR (c -search ing):SYNONYMS ( look - fo r  search- fo r ):NOTE (i occur rences:n t td -kana  ("4~<~ b~ < ,,): jv -dom):S -DERIVS  ())))The Japanese lexicon development effort shows that it is rea-sonable and useful to develop core lexicons in new languagesusing a common framework and ontology.
We believe thatthe Japanese lexicon is a useful resource for the community.However, the core lexicon, as we have explained, providesonly a small part of the coverage necessary to do the dataextraction task.3.4.
Portability to New DomainsThe amount of effort required to adapt SHOGUN to newdomains was quite small.
Figure 2 shows about how muchtime went into each configuration, who spent the time, andwhat kinds of things were done.Because development in the joint venture domain coincidedwith a large number of experiments, it was very hard to mea-sure the degree of effort dedicated to joint ventures in particu-lar.
The micro-electronics task presented a more accurate testof porting to a new domain (as well as to a new language),although we believe that the amount of effort to achieve cred-itable performance in micro-electronics is much less than inthe joint venture domain.The Japanese micro-electronics task is our best data point interms of ease of portability of the system.
The main knowl-edge base development effort in micro-electronics was carriedout by a single programmer, who did not know Japanese (butcould read Chinese characters) and was a relative novice inthe system (TIPSTER was his first experience with any AIproject).
He got a low level of help from two sources--nativeJapanese speakers, who helped somewhat with the vocabularyand to identify lists, phrases, and errors; and the main devel-oper of the Japanese joint venture knowledge base (who alsodoes not know Japanese).
He got a somewhat higher level ofassistance from the developer of the English micro-electronicsknowledge base.In some sense, this may show that it is easier to port the systemto a new language than to a new domain, because the Japanesemicro-electronics system required less effort than the Englishand clearly shared more with the English micro-electronicseffort than from the Japanese joint venture ffort.
But it alsoshows the ease of porting in general, as both micro-electronicsefforts for reasonably small.Many factors influence the degree of portability of a system,and we should point out that portability from one domainto another is extremely sensitive to the degree of overlapbetween the two domains, as well as the difficulty of thetarget domain and the level of performance to be obtained.Portability across languages actually seems less variable, inthat the apparent differences between English and Japaneseseemed to have relatively less impact on the developmenteffort han the differences among domains.The following are some of the important factors influencingportability across domains:Sharing of knowledge base components based on task orcorpus similarity, including, for example, name recog-nition, headline and dateline processing, special verbs(like manufacture, develop).Re-use of knowledge base components from a "library"of generic ontent elements that apply across a range ofdomains--for example, dates, locations, numbers, andmonetary units.?
Tools for reducing the amount of manual effort in port-ing (including word in context access, statistical analysis,215Domain~Language Effort~Skill Level Other NotesEnglish joint ventures I person-year, system developers,native English speakersSome effort not reflectedin resultsDifficult o measure becauseof many experimentsJapanese joint ventures 1.5 person-years, mostly Japanesecollege students with non-nativedevelopersLeast efficient, but mostinteresting effortBest overall resultsEnglish micro- electronics 3 person-months, system developers,native speakers, no knowledge of MELowest overall results (butexplained by samplevariation)Japanese micro- electronics 2 person-months, non-developers,non-native speakers (with some helpfrom natives, developers)Last configuration done, leastwork, good results (but notrefined)Figure 2: Level of effort required to port SHOGUNword frequency information) and for automatically aug-menting the knowledge base.?
Generality of linguistic components, for example, lex-icon coverage, morphology, Japanese segmentation,and ability of components to produce useful domain-independent information.All of these factors contributed to SHOGUN's portability.
Interms of what worked, we were especially pleased with theability to re-use knowledge base components across tasks, andwith the use of tools for reducing the manual effort in port-ing.
The automatic acquisition effort, as we have discussed,was somewhat less successful.
The use of "libraries" of rec-ognizers that help to configure data extraction systems is apotentially very helpful resource, one which we have begunto exploit in other projects as well.In terms of what didn't work, we were surprised and disap-pointed with the degree to which general inguistic compo-nents contributed (or didn't contribute) to portability.
In pre-vious work, we relied more heavily on lexical and grammarresources, and successfully ported to new domains (althoughnot as complex as TIPSTER) with roughly the same level ofeffort.
In this more grammar-oriented approach, we wouldgenerally start by making sure the "core" parser and semanticinterpreter could cover most of the text in each new domain,then attach domain knowledge to the output of the semanticinterpreter.
Although this did not represent a very large effort,there were two major problems: (1) the first step, checkingthe linguistic omponents and tuning them to a new domain,usually required substantial expertise and involvement by sys-tem developers, and (2) there seemed to be very little hope ofreducing the level of effort required to tie domain knowledgeto semantic representations.We are thus very optimistic about he prospects for improvingcorpus analysis and acquisition tools, and about the use oflibraries to help to configure xtraction systems to new do-mains.
We also believe that considering available resourcesduring the task design in an application can greatly reducethe effort in porting a system; sharing a "core" structure that216applies to a related domain can make porting much easier.On the other hand, we are pessimistic about he prospects forreducing customization time by improving "core" linguisticresources; in our view, we have already tapped lexicons andgrammars for much of what they have to offer in data extrac-tion, and the best path we see in this respect is to use theseresources to improve the results of automated corpus analysis.4.
EVALUATION SUMMARYSHOGUN ran the official MUC-5 (TIPSTER 24-month)benchmark in all four configurations; in addition, the teamran an "optional" official system called TEXTRACT, devel-oped mostly independently of SHOGUN, on the two Japaneseconfigurations.
After the benchmarks, the team did a numberof experiments comparing the results of the two systems, andshowing how these results could be combined to produce venbetter performance.4.1.
SHOGUN's ResultsFigure 3 is a summary of SHOGUN's performance on all theofficial metrics.
We put error rate first and F-measure last inthis table because these are the only ones that can be used foroverall system comparison (the goal being low error rate andhigh F-measure).The overall results here are better, on average, thanSHOGUN's scores on the MUC-4 benchmark.
While it isvery difficult to compare results across domains across lan-guages, it is clear that this shows substantial progress, as theMUC-5 tasks are certainly much harder and more detailedthan MUC-4.
In addition, the average improvement betweenthe TIPSTER 18-month benchmark and the current point wasover 20%, and there is certainly more room for further im-provement.
Thus, we are confident that our current methodsand algorithms support continued progress toward high accu-racy.While it seems that there is substantial variation among thescores on the different language-domain pairs, this variationis reasonable given the differences among the task and thevariations on the test samples.
The EME result is worse thanthe others, but the EME MUC-5 test set seemed to be a verydifficult one for our system.
In fact, the system on a blind testusing the same configuration scored 9 error rate points betterin EME than on the test reported above.
We are not sure whataccounts for this variability in EME, which is much greaterthan on the other domain-language pairs.With respect to achieving human performance, it is not clearwhere good human perform falls on these scales, but we are- close.
At the TIPSTER 12-month test, a study of trainedhuman analysts placed individual analysts between 70 and80 in F-measure.
However, this testused a somewhat moregenerous coring algorithm than the current one (there havebeen a number of important changes to the scoring since the12-month point), and did not separate the analysts work fromthe preparation of the "ideal" answers--it is important in ablind test hat he human subject have no impact on the answerkey, because there are many texts that involve fine-grainedinterpretation.The results on Japanese are, on average, somewhat higherthan the English results.
This is consistent with all our tests.We attribute this to the fact that the Japanese tests are con-siderably easier than the English (a factor that is somewhatdifficult to weight, given that none of our system develop-ers know Japanese).
Some of the influences that make theJapanese asier are greater homogeneity in the text sources(for example, EME includes very different sources from EJV,while JJV and JME are quite consistent in style), shorter sto-ries with fewer distinct events in Japanese, far fewer new jointventure companies in Japanese, and an emphasis in Japaneseon research and sales rather than production (production ac-tivities are more difficult to assign to codes in the templatedesign).4.2.
TEXTRACT and Combining SystemsIn addition to the SHOGUN system, the GE-CMU team ranthe Japanese benchmarks only using a system called TEX-TRACT, which was developed in parallel to SHOGUN byTsuyoshi Kitani, a visiting researcher at CMU from N'ITData.
TEXTRACT, like SHOGUN, emphasizes lexically-driven pattern matching, and the two systems share aJapanese tagging/segmentation program from NIT  Data,called MAJESTY, While there is little else that is directlyshared between the two system's, additions to TEXTRACT'sknowledge base were incrementally adapted, in functionality,to SHOGUN's knowledge base in JJV, thus it it not surprisingthat the systems had similar performance on this set.
TEX-TRACT generally had a better performance on company namerecognition than SHOGUN, and a somewhat more effectivemethod of splitting events.
SHOGUN had better coverage ofindustry types and products (based, we think, on the heavyuse of statistically-based training), and had higher recall (butlower precision) in JME.For the Japanese Micro-electronics domain, the SHOGUNsystem scored the highest recall, while the TEXTRACT sys-tem scored the highest precision.
The F-measure and errorscores were almost exactly the same.
We developed a statis-tical technique to combine these systems in a way to improvethe F-measure, and as a by-product we determined the theo-retical imits of combining the output of the two systems.The combining algorithm works as follows: both SHOGUNand TEXTRACT are run on an input text, and the outputtemplates are given as input to the combiner.
The followingmethods were examined:217EJVJJVEMEJMEError61546558UND OVG SUB Min-err Max-err30 39 19 0.8784 0.902636 27 12 0.6624 0.679437 41 19 0.8354 0.872430 38 14 0.7756 0.8152Text Rec Pre F-meas96/92 57 49 52.899/98 57 64 60.195/81 50 48 49.297/861 60 53 56.3Figure 3: SHOGUN Scores for MUC-5SHOGUN this row just shows the scores for the SHOGUNsystem.TEXTRACT this row shows the scores for the TEX-TRACT system.Theoretical max this row shows the scores for a systemwhich chooses perfectly whether SHOGUN orTEXTRACT has the better answer for a particulartext.Entity weight D=T this row shows the results of using totalentity weight o select he output emplate, usingTEXTRACT output in case of ties.Entity weight D=S same as above, but uses SHOGUN out-put to break ties.Most names D=S this method chooses the output emplatewith the most entity names.Avg Entity weight D=T similar to entity weight, but theaverage is used instead of the total weight.SHO + TEX this method uses SHOGUN's output unless itis empty, in which case TEXTRACT's output isused.TEX + SHO this method uses TEXTRACT's output unlessit is empty, it which case SHOGUN's ou.tput isused.Avg Entity weight D=S average en-tity weight with SHOGUN output used in caseof a tie.Single capability D=T this method chooses the output withthe number of capabilities closest to one, andchooses TEXTRACT's output in case of a tie.Figure 5 gives the numeric values for the various combiningmethods, and Figure 6 shows the recall-precision performanceof each method graphically.Note that the best performing method was the total entityweight, which used statistics from the development corpusfor the entity-name slot to determine which output emplatehad more commonly found company names.
Intuitively, ifthe output emplate had more companies that were associatedwith correct keys from the development corpus, that templateis more likely to be correct.
Note also that no knowledge-freecombining method gave a better F-measure than either of thetwo systems alone.4.3.
Analysis of Benchmark ResultsIt is hard to consider the TIPSTER program without seri-ously analyzing the benchmark results.
The extraction part ofTIPSTER included four official benchmarks during a periodof about 15 months, covering the MUC-4 evaluation and theTIPSTER 12-month, 18-month, and 24-month (MUC-5) tests.In addition to funding of individual contractors' efforts, thegovernment devoted significant resources to the preparationof materials for these evaluations.Benchmarks, above all else, are effective as a way of cultivat-ing and comparing technologies.
The evaluations are meantto show coverage and accuracy; in terms of system perfor-mance, these can translate into different characteristics.
Forexample, coverage includes robustness (e.g.
ability to handlemisspellings and other errors in the input, unknown or unusualwords, varieties of news style, etc.
), vocabulary, grammaticalcoverage, and the extent of domain knowledge.
Accuracyincludes the ability to distinguish different senses of words,preciseness of interpretation, adherence to template fill rules,elimination of program bugs, and getting the overall construalof a text correct (i.e., putting the right information i  the rightplace).SHOGUN, on average, extracted 37% more information cor-rectly (37% higher recall) than the second-best ystems ineach configuration on the final benchmarks (which were, ofcourse, different systems in different configurations).
Onaverage, SHOGUN's precision in the information extractedwas 13% lower than the next best system.
Comparing withindividual systems from other groups that did all four con-figurations, SHOGUN had 49% higher ecall (with 5% lowerl:secision) than one system, and 71% higher recall (with 8%..lower precision) than the other, on average.What is the source of these very significant differences incov-erage?
While there are many places where systems differ, the218JJVJMEError UND OVG SUB Min-err Max-err Text Rec Pre F-meas49.99i 32 23 12 0.5877 0.6028 99/99 60 68 63.8458.64 i 43 28 12 0.6728 0.7072 96/85 51 63 56.35Figure 4: Official TEXTRACT Scores for MUC-5most obvious differences are on the more difficult portions ofthe task.
In fact, in one configuration there was one class ofinformation, the recognition of entities in micro-electronics,where SHOGUN did worse than the next best system.
Webelieve that this is because the other system was doing verygood name recognition, and had tested the name recognitioncomponent more carefully with the micro-electronics corpus.However, name recognition and top-level object recognitionare certainly the easiest, least error-prone portions of the TIP-STER task.In the harder extraction sub-tasks, SHOGUN had much higherrecall, and generally much lower precision.
For example, inEJV, SHOGUN correctly extracted 3 times as much industryinformation, 4.5 times as much facility information, and 3.3times as much activity information as the next best system,while making many more errors than the next best system.
It isvery hard to compare systems when their performance is aver-aged across ub-tasks with very different degrees of difficulty,but when we look at performance on individual objects, slots,and messages, it would appear that the difference betweenSHOGUN and other systems tems largely from SHOGUN'shigher coverage on harder parts of the task.
As a result ofdoing more of these harder components, SHOGUN has lowerprecision and higher overgeneration than some other systems,but we do not see this as a real tradeoff, because the loweraccuracy usually comes from extracting difficult informationthat other systems did not extract at all.We attribute these substantive differences between SHOGUNand other systems to the combination of the finite-state ap-proximation method with the corpus-based knowledge acqui-sition strategies.
The finite-state method can be tuned forhigher precision or higher ecall; SHOGUN shows higher e-call.
The difference in recall was most pronounced on objectslike the industry object, where word content far outweighssentence structure.
On these sorts of sub-tasks, the corpus-based knowledge acquisition strategy helped SHOGUN toobtain much better coverage of the task.This analysis has tried to identify the salient differences amongsystems and approaches as shown in the MUC-5/TIPSTER24-month benchmark.
As we have discussed, SHOGUN hadvery good performance across languages and domains, andhad a consistently ower error rate, mostly due to much higherrecall than other systems.
In addition, SHOGUN's advantageseemed to come more from "harder" slots in general, althoughthere were only a few slots in any configuration where othersystems did better, and these were concentrated in Englishmicro-electronics.
It is a safe conclusion that our projectsuccessfully produced a good, high coverage, portable system.5.
SUMMARY AND CONCLUSIONSThe TIPSTER/SHOGUN project advanced the state of theart in data extraction by developing a simplified model oftext processing emphasizing finite state approximation, andby defining new methods for corpus-based knowledge acqui-sition.
While demonstrating high coverage across languagesin TIPSTER, these innovations open the way for future ad-vances by focusing on corpus-based knowledge and contentprocessing of texts.6.
REFERENCES\[l\]\[2\]\[3\]\[4\]\[5\]\[6\]Jay Earley.
An efficient context-free parsing algorithm.Communications of the Association for Computing Ma-chinery, 13(2):94-102, February 1970.Donna Harman, editor.
Proceedings of the Text RetrievalConference (TREC).
Morgan Kaufmann Publishers, SanMateo, CA, November 1992.E S. Jacobs and L. F. Rau.
Innovations in text interpre-tation.
Artificial Intelligence, 63:143-191, 1993.Paul S. Jacobs.
Parsing run amok: Relation-driven con-trol for text analysis.
In Proceedings of the Tenth Na-tional Conference on Artificial Intelligence, San Jose,CA, 1992.Susan McRoy.
Using multiple knowledge sources forword sense discrimination.
Computational Linguistics,18(1), March 1992.George R. Krupka Paul S. Jacobs and Lisa E Rau.A Boolean approximation method for query construc-tion and topic assignment in TREC.
In Second AnnualSymposium on Document Analysis and Information Re-trieval, May 1993.219Method Recall Precision F-MeasureSHOGUNTEXTRACTTheoretical maxEntity weight D=TEntity weight D=SMost names D=SAvg Entity weight D=TSHO + TEXTEX + SHOAvg Entity weight D=SSingle capability D=T60.0306 53.025450.6498 63.498861.0330 63.437156.0208 58.976860.1467 53.103161.7665 51.582453.8203 58.778460.7034 51.478252.3476 58.600755.2294 55.094653.0257 57.072456.311056.351162.211857.460856.405856.217056.190255.711555.297955.161954.9747Figure 5: Combining Two MUC-5 Systems: Table\[7\] Fernando Pereira.
Finite-state approximations of gram-mars.
In DARPA Speech and Natural Language Work-shop, pages 20-25, Hidden Valley, PA, 1990.\[8\] Lisa F. Rau, George R. Krupka, and Paul S. Jacobs.
GENLToolset: MUC-4 test results and analysis.
In Pro-ceedings of the Fourth Message Understanding Con-ference (MUC-4), San Mateo, CA, June 1992.
MorganKaufmann Publishers.\[9\] S. Shieber.
Sentence disambiguation bya shift-reduceparsing technique.
In Proceedings of the Eighth In-ternational Joint Conference on Artificial Intelligence,Karlsruhe, Germany, 1983.\[10\] Beth Sundheim, editor.
Proceedings of the FourthMessage Understanding Conference (MUC-4).
MorganKaufmann Publishers, San Mateo, Ca, June 1992.\[ 11\] Beth Sundheim, editor.
Proceedings of the Fifth Mes-sage Understanding Conference (MUC-5).
MorganKaufmann Publishers, San Mateo, Ca, August 1993.\[12\] M. Tomita.
Efficient Parsing for Natural Language.Kluwer Academic Publishers, Hingham, Massachusetts,1986.22065CO?
~E~?L50.TEXTRACTEntity weight D=T?
Avg entity weight D=TTEX + SHO?
Single capability D=T?
Avg entity weight D=S?
Theoretical max,SHOGUNMost names D=SSHO + TEX50Recall65Figure 6: Combining Two MUC-5 Systems: Graph221
