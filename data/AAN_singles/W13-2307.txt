Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 51?60,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsA Framework for (Under)specifying Dependency Syntaxwithout Overloading AnnotatorsNathan Schneider??
Brendan O?Connor?
Naomi Saphra?
David Bamman?Manaal Faruqui?
Noah A. Smith?
Chris Dyer?
Jason Baldridge?
?School of Computer Science, Carnegie Mellon University?Department of Linguistics, The University of Texas at AustinAbstractWe introduce a framework for lightweightdependency syntax annotation.
Our for-malism builds upon the typical represen-tation for unlabeled dependencies, per-mitting a simple notation and annotationworkflow.
Moreover, the formalism en-courages annotators to underspecify partsof the syntax if doing so would streamlinethe annotation process.
We demonstratethe efficacy of this annotation on three lan-guages and develop algorithms to evaluateand compare underspecified annotations.1 IntroductionComputational representations for natural lan-guage syntax are borne of competing design con-siderations.
When designing such representations,there may be a tradeoff between parsimony andexpressiveness.
A range of linguistic theories at-tract support due to differing purposes and aes-thetic principles (Chomsky, 1957; Tesni?re, 1959;Hudson, 1984; Sgall et al 1986; Mel?c?uk, 1988,inter alia).
Formalisms concerned with tractablecomputation may care chiefly about learnabil-ity or parsing efficiency (Shieber, 1992; Sleatorand Temperly, 1993; Kuhlmann and Nivre, 2006).Further considerations may include psychologi-cal and evolutionary plausibility (Croft, 2001;Tomasello, 2003; Steels et al 2011; Fossum andLevy, 2012), integration with other representa-tions such as semantics (Steedman, 2000; Bergenand Chang, 2005), or suitability for particular ap-plications (e.g., translation).Here we elevate ease of annotation as a pri-mary design concern for a syntactic annotationformalism.
Currently, a lack of annotated datais a huge bottleneck for robust NLP, standing inthe way of parsers for social media text (Fosteret al 2011) and many low-resourced languages(to name two examples).
Traditional syntactic an-notation projects like the Penn Treebank (Marcus?Corresponding author: nschneid@cs.cmu.eduet al 1993) or Prague Dependency Treebank (Ha-jic?, 1998) require highly trained annotators andhuge amounts of effort.
Lowering the cost of an-notation, by making it easier and more accessi-ble, could greatly facilitate robust NLP in new lan-guages and genres.To that end, we design and test new, lightweightmethodologies for syntactic annotation.
We pro-pose a formalism, Fragmentary Unlabeled De-pendency Grammar (FUDG) for unlabeled de-pendency syntax that addresses some of the mostglaring deficiencies of basic unlabeled dependen-cies (?2), with little added burden on annotators.FUDG requires minimal theoretical commitments,and can be supplemented with a project-specificstyle guide (we provide a brief one for English).We contribute a simple ASCII markup language?Graph Fragment Language (GFL; ?3)?that al-lows annotations to be authored using any text ed-itor, along with tools for validating, normalizing,and visualizing GFL annotations.1An important characteristic of our framework isannotator flexibility.
The formalism supports thisby allowing underspecification of structural por-tions that are unclear or unnecessary for the pur-poses of a project.
Fully leveraging this power re-quires new algorithms for evaluation, e.g., of inter-annotator agreement, where annotations are par-tial; such algorithms are presented in ?4.2Finally, small-scale case studies (?5) apply ourframework (formalism, notation, and evaluations)to syntactically annotate web text in English, newsin Malagasy, and dialogues in Kinyarwanda.2 A Dependency Grammar forAnnotationAlthough dependency-based approaches to syntaxplay a major role in computational linguistics, thenature of dependency representations is far fromuniform.
Exemplifying one end of the spectrumis the Prague Dependency Treebank, which articu-lates an elaborate dependency-based syntactic the-1https://github.com/brendano/gfl_syntax/2Parsing algorithms are left for future work.51Found the scarriest mystery door in my school .
I?M SO CURIOUS D:Found** < (the scarriest mystery door*)Found < in < (my > school)I?M** < (SO > CURIOUS)D:**my = I?Mthers still like 1 1/2 hours till Biebs bday here :Pthers** < stillthers < ((1 1/2) > hours < till < (Biebs > bday))(thers like 1 1/2 hours)thers < here:P**Figure 1: Two tweets with example GFL annotations.
(The formalism and notation are described in ?3.
)ory in a rich, multi-tiered formalism (Hajic?, 1998;B?hmov?
et al 2003).
On the opposite end ofthe spectrum are the structures used in dependencyparsing research which organize all the tokens ofa sentence into a tree, sometimes with category la-bels on the edges (K?bler et al 2009).
Insofar asthey reflect a theory of syntax, these vanilla de-pendency grammars provide a highly reduction-ist view of structure?indeed, parses used to trainand evaluate dependency parses are often simpli-fications of Prague-style parses, or else convertedfrom constituent treebanks.In addition to the binary dependency links ofvanilla dependency representations, we offer threedevices to capture certain linguistic phenomenamore straightforwardly:31.
We make explicit the meaningful lexical unitsover which syntactic structure is represented.
Ourapproach (a) allows punctuation and other extrane-ous tokens to be excluded so as not to distract fromthe essential structure; and (b) permits tokens to begrouped into shallow multiword lexical units.42.
Coordination is problematic to represent withunlabeled dependencies due to its non-binary na-ture.
A coordinating conjunction typically joinsmultiple expressions (conjuncts) with equal sta-tus, and other expressions may relate to the com-pound structure as a unit.
There are several differ-ent conventions for forcing coordinate structuresinto a head-modifier straightjacket (Nivre, 2005;de Marneffe and Manning, 2008; Marec?ek et al2013).
Conjuncts, coordinators, and shared de-pendents can be distinguished with edge labels;we equivalently use a special notation, permittingthe coordinate structure to be automatically trans-formed with any of the existing conventions.53Some of this is inspired by the conventions of Reed-Kellogg sentence diagramming, a graphical dependency an-notation system for English pedagogy (Reed and Kellogg,1877; Kolln and Funk, 1994; Florey, 2006).4The Stanford representation supports a limited notion ofmultiword expressions (de Marneffe and Manning, 2008).For simplicity, our formalism treats multiwords as unana-lyzed (syntactically opaque) wholes, though some multiwordexpressions may have syntactic descriptions (Baldwin andKim, 2010).5Tesni?re (1959) and Hudson (1984) similarly usespecial structures for coordination (Schneider, 1998;3.
Following Tesni?re (1959), our formalismoffers a simple facility to express anaphora-antecedent relations (a subset of semantic relation-ships) that are salient in particular syntactic phe-nomena such as relative clauses, appositives, andwh-expressions.Underspecification.
Our desire to facilitatelightweight annotation scenarios requires us toabandon the expectation that syntactic informantsprovide a complete parse for every sentence.
Onone hand, an annotator may be uncertain about theappropriate parse due to lack of expertise, insuf-ficiently mature annotation conventions, or actualambiguity in the sentence.
On the other hand, an-notators may be indifferent to certain phenomena.This can happen for a variety of reasons:?
Some projects may only need annotations ofspecific constructions.
For example, building asemantic resource for events may require anno-tation of syntactic verb-argument relations, butnot internal noun phrase structure.?
As a project matures, it may be more useful toannotate only infrequent lexical items.?
Semisupervised learning from partial annota-tions may be sufficient to learn complete parsers(Hwa, 1999; Clark and Curran, 2006).?
Beginning annotators may wish to focus on eas-ily understood syntactic phenomena.?
Different members of a project may wish to spe-cialize in different syntactic phenomena, reduc-ing training cost and cognitive load.Rather than treating annotations as invalid unlessand until they are complete trees, we formally rep-resent and reason about partial parse structures.Annotators produce annotations, which encodeconstraints on the (inferred) analysis, the parsestructure, of a sentence.
We say that a valid anno-tation supports (is compatible with) one or moreanalyses.
Both annotations and analyses are rep-resented as graphs (the graph representation is de-scribed below in ?3.2).
We require that the di-rected edges in an analysis graph must form a treeover all the lexical items in the sentence.6 LessSangati and Mazza, 2009).6While some linguistic phenomena (e.g., relative clauses,control constructions) can be represented using non-tree52stringent well-formedness constraints on the an-notation graph leave room for underspecification.Briefly, an annotation can be underspecified intwo ways: (a) an expression may not be attached toany parent, indicating it might depend on any non-descendant in a full analysis?this is useful for an-notating sentences piece by piece; and (b) multipleexpressions may be grouped together in a fudgeexpression (?3.3), a constraint that the elementsform a connected subgraph in the full analysiswhile leaving the precise nature of that subgraphindeterminate?this is useful for marking relation-ships between chunks (possibly constituents).A formalism, not a theory.
Our framework fordependency grammar annotation is a syntacticformalism, but it is not sufficiently comprehen-sive to constitute a theory of syntax.
Thoughit standardizes the basic treatment of a few ba-sic phenomena, simplicity of the formalism re-quires us to be conservative about making suchextensions.
Therefore, just as with simpler for-malisms, language- and project-specific conven-tions will have to be developed for specific linguis-tic phenomena.
By embracing underspecified an-notation, however, our formalism aims to encour-age efficient corpus coverage in a nascent anno-tation project, without forcing annotators to makepremature decisions.3 Syntactic Formalism and GFLIn our framework, a syntactic annotation of a sen-tence follows an extended dependency formalismbased on the desiderata enumerated in the previ-ous section.
We call our formalism FragmentaryUnlabeled Dependency Grammar (FUDG).To make it simple to create FUDG annotationswith a text editor, we provide a plain-text de-pendency notation called Graph Fragment Lan-guage (GFL).
Fragments of the FUDG graph?nodes and dependencies linking them?are en-coded in this language; taken together, these frag-ments describe the annotation in its entirety.
Theordering of GFL fragments, and of tokens withineach fragment, is of no formal consequence.
Sincethe underlying FUDG representation is transpar-ently related to GFL constructions, GFL notationwill be introduced alongside the discussion of eachkind of FUDG node.7structures, we find that being able to alert annotators whenthey inadvertently violate the tree constraint is more usefulthan the expressive flexibility.7In principle, FUDG annotations could be created with3.1 TokensWe expect a tokenized string, such as a sentenceor short message.
The provided tokenization is re-spected in the annotation.
For human readability,GFL fragments refer to tokens as strings (ratherthan offsets), so all tokens that participate in anannotation must be unambiguous in the input.8 Atoken may be referenced multiple times in the an-notation.3.2 Graph EncodingDirected arcs.
As in other dependencyformalisms, dependency arcs are directedlinks indicating the syntactic headednessrelationship between pairs of nodes.
InGFL, directed arcs are indicated with an-gle brackets pointing from the dependent toits head, as in black > cat or (equivalently)cat < black.
Multiple arcs can be chained to-gether: the > cat < black < jet describes threearcs.
Parentheses help group portions of a chain:(the > cat < black < jet) > likes < fish (thestructure black < jet > likes, in which jetappears to have two heads, is disallowed).
Notethat another encoding for this structure would beto place the contents of the parentheses and thechain cat > likes < fish on separate lines.
Curlybraces can be used to list multiple dependents ofthe same head: {cat fish} > likes.Anaphoric links.
These undirected links joincoreferent anaphora to each other and to their an-tecedent(s).
In English this includes personal pro-nouns, relative pronouns (who, which, that), andanaphoric do and so (Leo loves Ulla and so doesMax).
This introduces a bit of semantics into ourannotation, though at present we do not attempt tomark non-anaphoric coreference.
It also allows amore satisfying treatment of appositives and rel-ative clauses than would be possible from just thedirected tree (the third example in figures 2 and 3).Lexical nodes.
Whereas in vanilla dependencygrammar syntactic links are between pairs of to-ken nodes, FUDG abstracts away from the indi-vidual tokens in the input.
The lowest level of aFUDG annotation consists of lexical nodes, i.e.,an alternative mechanism such as a GUI, as in Hajic?
et al(2001).8If a word is repeated within the sentence, it must be in-dexed in the input string in order to be referred to from afragment.
In our notation, successive instances of the sameword are suffixed with ~1, ~2, ~3, etc.
Punctuation and othertokens omitted from an annotation do not need to be indexed.53'llIf'sI wake_uprestin' it~1it~2weaponsOur threeare$afear surprise efficiencyruthlessand~1 and~2areWe knightsthewhosayNiFigure 2: FUDG graphs corresponding to the examples in figure 3.
The two special kinds of directed edges are for attachingconjuncts (bolded) and their coordinators (dotted) in a coordinate structure.
Anaphoric links are undirected.
The root node ofeach sentence is omitted.If it~1 's restin' I 'll wake it~2 up .If < (it~1 > 's < restin')I > 'll < [wake up] < it~2If > 'll**it~1 = it~2Our three weapons are fear and~1 surprise and~2ruthless efficiency ...{Our three} > weapons > are < $a$a :: {fear surprise efficiency} :: {and~1 and~2}ruthless > efficiencyWe are the knights who say ... Ni !We > are < knights < theknights < (who > say < Ni)who = knightsFigure 3: GFL for the FUDG graphs in figure 2.lexical item occurrences.
Every token node mapsto 0 or 1 lexical nodes (punctuation, for instance,can be ignored).A multiword is a lexical node incorporatingmore than one input token and is atomic (doesnot contain internal structure).
A multiword nodemay group any subset of input tokens; this allowsfor multiword expressions which are not neces-sarily contiguous in the sentence (e.g., the verb-particle construction make up in make the storyup).
GFL notates multiwords with square brack-ets, e.g., [break a leg].Coordination nodes.
Coordinate structures re-quire at least two kinds of dependents: co-ordinators (i.e., lexical nodes for coordinat-ing conjunctions?at least one per coordina-tion node) and conjuncts (heads of the con-joined subgraphs?at least one per coordinationnode).
The GFL annotation has three parts:a variable representing the node, a set of con-juncts, and a set of coordinator nodes.
For in-stance, $a :: {[peanut butter] honey} :: {and}(peanut butter and honey) can be embeddedwithin a phrase via the coordination nodevariable $a; a [fresh [[peanut butter] andhoney] sandwich] snack would be formed with{fresh $a} > sandwich > snack < a.
A graphicalexample of coordination can be seen in figure 2?note the bolded conjunct edges and the dotted co-ordinator edges.
If the conjoined phrase as a wholetakes modifiers, these are attached to the coordina-tion node with regular directed arcs.
For example,in Sam really adores kittens and abhors puppies.,the shared subject Sam and adverb really attach tothe entire conjoined phrase.
In GFL:$a :: {adores abhors} :: {and}Sam > $a < reallyadores < kittens abhors < puppiesRoot node.
This is a special top-level node usedto indicate that a graph fragment constitutes a stan-dalone utterance or a discourse connective.
For aninput with multiple utterances, the head of eachshould be designated with ** to indicate that it at-taches to the root.3.3 Means of UnderspecificationAs discussed in ?2, our framework distinguishesannotations from full syntactic analyses.
With re-spect to dependency structure (directed edges), theformer may underspecify the latter, allowing theannotator to commit only to a partial analysis.For an annotationA, we define support(A) to bethe set of full analyses compatible with that anno-tation.
A full analysis is required to be a directedrooted tree over all lexical nodes in the annotation.An annotation is valid if its support is non-empty.The 2 mechanisms for dependency underspeci-fication are unattached nodes and fudge nodes.Unattached nodes.
For any node in an annota-tion, the annotator is free to simply leave it notattached to any head.
This is interpreted as al-lowing its head to be any other node (includingthe root node), subject to the tree constraint.
Wecall a node?s possible heads its supported par-ents.
Formally, for an unattached node v in an-notation A, suppParentsA(v) = nodes(A) \ ({v} ?descendants(v)).Fudge nodes.
Sometimes, however, it is desir-able to represent a sort of skeletal structure with-out filling in all the details.
A fudge expres-sion (FE) asserts that a group of nodes (the ex-pression?s members) belong together in a con-nected subgraph, while leaving the internal struc-ture of that subgraph unspecified.9 The notation9This underspecification semantics is, to the best of ourknowledge, novel, though it has been proposed that con-nected dependency subgraphs (known as catenae) are of the-oretical importance in syntax (Osborne et al 2012).54FN2abfFN1c d efbfbb cbb aadac adadc db e fe c e fead dcfe e fcFigure 4: Left: An annotation graph with 2 fudge nodes and 6 lexical nodes; it can be encoded with GFL fragments((a b)* c d) < e and b < f. Right: All of its supported analyses: prom(A) = 6. com(A) = 1 ?log 6log 75= .816.for this is a list of two or more nodes withinparentheses: an annotation for Few if any witchesare friends with Maria.
might contain the FE(Few if any) so as to be compatible with thestructures Few < if < any, Few > if > any, etc.
?but not, for instance, Few > witches < any.
Inthe FUDG graph, this is represented with a fudgenode to which members are attached by specialmember arcs.
Fudge nodes may be linked to othernodes: the GFL fragment (Few if any) > witchesis compatible with (Few < if < any) > witches,(Few < (if > any)) > witches, and so forth.Properties.
Let f be a fudge expression.
Fromthe connected subgraph definition and the treeconstraint on analyses, it follows that:?
Exactly 1 member of f must, in any compatibleanalysis, have a parent that is not a member of f.Call this node the top of the fudge expression,denoted f ?.
f ?
dominates all other members off; it can be considered f?s ?internal head.??
f does not necessarily form a full subtree.
Anyof its members may have dependents that arenot themselves members of the fudge expres-sion.
(Such dependencies can be specified inadditional GFL fragments.
)Top designation.
A single member of a fudgeexpression may optionally be designated as its top(internal head).
This is specified with an asterisk:(Few* if any) > witches indicates that Few mustattach to witches and also dominate both if andany.
In the FUDG graph, this is represented witha special top arc as depicted in bold in figure 4.Nesting.
One fudge expression may nestwithin another, e.g.
(Few (if any)) > witches;the word analyzed as attaching to witches mightbe Few or whichever of (if any) heads the other.A nested fudge expression can be designated astop: (Vanishingly few (if any)*).Modifiers.
An arc attaching a node to afudge expression as a whole asserts that theexternal node should modify the top of the fudgeexpression (whether or not that top is designatedin the annotation).
For instance, two of theinterpretations of British left waffles on Falklandswould be preserved by specifying British > leftand (left waffles) < on < Falklands.
AnalysesBritish > left < waffles < on < Falklands and(British > left < on < Falklands) > waffleswould be excluded because the preposition doesnot attach to the head of (left waffles).10Multiple membership.
A node may be a mem-ber of multiple fudge expressions, or a memberof an FE while attached to some other node viaan explicit arc.
Each connected component ofthe FUDG graph is therefore a polytree (not nec-essarily a tree).
The annotation graph minus allmember edges of fudge nodes and all (undirected)anaphoric links must be a directed tree or forest.Enumerating supported parents.
Fudge ex-pressions complicate the procedure for listing anode?s supported parents (see above).
Consider anFE f having some member v. v might be the topof f (unless some other node is so designated), inwhich case anything the fudge node can attach tois a potential parent of v. If some node other thanv might be the top of f, then v?s head could be anymember of f. Below (?4.1) we develop an algo-rithm for enumerating supported parents for anyannotation graph node.4 Annotation Evaluation MeasuresFor an annotation task which allows for a greatdeal of latitude?as in our case, where a syntac-tic annotation may be full or partial?quantitativeevaluation of data quality becomes a challenge.
Inthe context of our formalism, we propose mea-sures that address:?
Annotation efficiency, quantified in terms ofannotator productivity (tokens per hour).?
The amount of information in an underspeci-fied annotation.
Intuitively, an annotation thatflirts with many full analyses conveys less syn-tactic information than one which supports fewanalyses.
We define an annotation?s promiscu-ity to be the number of full analyses it supports,and develop an algorithm to compute it (?4.1).10Not all attachment ambiguities can be precisely encodedin FUDG.
For instance, there is no way to forbid an attach-ment to a word that lies along the path between the pos-sible heads.
The best that can be done given a sentencelike They conspired to defenestrate themselves on Tuesday.
isThey > conspired < to < defenestrate < themselves and(conspired* to defenestrate (on < Tuesday)).55?
Inter-annotator agreement between two par-tial annotations.
Our measures for dependencystructure agreement (?4.2) incorporate the no-tion of promiscuity.We test these evaluations on our pilot annotationdata in the case studies (?5).4.1 Promiscuity vs. CommitmentGiven a FUDG annotation of a sentence, we quan-tify the extent to which it underspecifies the fullstructure by counting the number of analyses thatare compatible with the constraints in the annota-tion.
We call this number the promiscuity of theannotation.
Each analysis tree is rooted with theroot node and must span all lexical nodes.11A na?ve algorithm for computing promiscuitywould be to enumerate all directed spanning treesover the lexical nodes, and then check each ofthem for compatibility with the annotation.
Butthis quickly becomes intractable: for n nodes,one of which is designated as the root, there arenn?2 spanning trees.
However, we can filter outedges that are known to be incompatible withthe annotation before searching for spanningtrees.
Our ?upward-downward?
method forconstructing a graph of supported edges firstenumerates a set of candidate top nodes for everyfudge expression, then uses that informationto infer a set of supported parents for everynode.12 The supported edge graph then consistsof vertices lexnodes(A) ?
{root} and edges?v?lexnodes(A) {(v?
v?)
?
v?
?
suppParentsA(v)}.From this graph we can count all directed span-ning trees in cubic time using Kirchhoff?s matrixtree theorem (Chaiken and Kleitman, 1978; Smithand Smith, 2007; Margoliash, 2010).13 If somelexical node has no supported parents, this reflectsconflicting constraints in the annotation, and nospanning tree will be found.Promiscuity will tend to be higher for longersentences.
To control for this, we define a secondquantity, the annotation?s commitment quotient(commitment being the opposite of promiscuity),11This measure assumes a fixed lexical analysis (set of lex-ical nodes) and does not consider anaphoric links.
Coordinatestructures are simplified into ordinary dependencies, with co-ordinate phrases headed by the coordinator?s lexical node.
Ifa coordination node has multiple coordinators, one is arbi-trarily chosen as the head and the others as its dependents.12Python code for these algorithms appears in Schneideret al(2013) and the accompanying software release.13Due to a technicality with non-member attachments tofudge nodes, for some annotations this is only an upper boundon promiscuity; see Schneider et al(2013).which normalizes for the number of possible span-ning trees given the sentence length.
The commit-ment quotient for an annotation of a sentence withn?1 lexical nodes and one root node is given by:com(A) = 1 ?log prom(A)log nn?2(the logs are to attenuate the dominance of the ex-ponential term).
This will be 1 if only a singletree is supported by the annotation, and 0 if theannotation does not constrain the structure at all.
(If the constraints in the annotation are internallyinconsistent, then promiscuity will be 0 and com-mitment undefined.)
In practice, there is a trade-off between efficiency and commitment: more de-tailed annotations require more time.
The value ofminimizing promiscuity will therefore depend onthe resources and goals of the annotation project.4.2 Inter-Annotator AgreementFUDG can encode flat groupings and coreferenceat the lexical level, as well as syntactic structureover lexical items.
Inter-annotator agreement canbe measured separately for each of these facets.Pilot annotator feedback indicated that our initiallexical-level guidelines were inadequate, so we fo-cus here on measuring structural agreement pend-ing further clarification of the lexical conventions.Attachment accuracy, a standard measure forevaluating dependency parsers, cannot be com-puted between two FUDG annotations if either ofthem underspecifies any part of the dependencystructure.
One solution is to consider the inter-section of supported full trees, in the spirit ofour promiscuity measure.
For annotations A1 andA2 of sentence s, one annotation?s supported an-alyses can be enumerated and then filtered sub-ject to the constraints of the other annotation.The tradeoff between inter-annotator compatibil-ity and commitment can be accounted for by tak-ing their product, i.e.
comPrec(A1 | A2) =com(A1)|supp(A1)?supp(A2)||supp(A1)|.A limitation of this support-intersection ap-proach is that if the two annotations are notcompatible, the intersection will be empty.
Amore fine-grained approach is to decomposethe comparison by lexical node: we general-ize attachment accuracy with softComPrec(A1 |A2) = com(A1)?`?s?i?
{1,2} suppParentsAi (`)?`?s suppParentsA1 (`), comput-ing com(?)
and suppParents(?)
as in the previoussection.
As lexical nodes may differ between thetwo annotations, a reconciliation step is required56Language Tokens Rate (tokens/hr)English Tweets (partial) 667 430English Tweets (full) 388 250Malagasy 4,184 47Kinyarwanda 8,036 80Table 1: Productivity estimates from pilot annotation project.All annotators were native speakers of English.to compare the structures: multiwords proposed inonly one of the two annotations are converted tofudge expressions.
Tokens annotated by neitherannotator are ignored.
Like with the promiscuitymeasure, we simplify coordinate structures to or-dinary dependencies (see footnote 11).5 Case Studies5.1 Annotation TimeTo estimate annotation efficiency, we performeda pilot annotation project consisting of annotatingseveral hundred English tweets, about 1,000 sen-tences in Malagasy, and a further 1,000 sentencesin Kinyarwanda.14 Table 1 summarizes the num-ber of tokens annotated and the effort required.
Forthe two Twitter cases, the same annotator was firstpermitted to do partial annotation of 100 tweets,and then spend the same amount of time doing acomplete annotation of all tokens.
Although this isa very small study, the results clearly suggest shewas able to make much more rapid progress whenpartial annotation was an option.15This pilot study helped us to identify linguisticphenomena warranting specific conventions: theseinclude wh-expressions, comparatives, vocatives,discourse connectives, null copula constructions,and many others.
We documented these cases in a20-page style guide for English,16 which informedthe subsequent pilot studies discussed below.5.2 Underspecification and AgreementWe annotated 2 small English data samples inorder to study annotators?
use of underspecifica-tion.
The first is drawn from Owoputi et als 2013Twitter part-of-speech corpus; the second is fromthe Reviews portion of the English Web Treebank14Malagasy is a VOS Austronesian language spoken by 15million people, mostly in Madagascar.
Kinyarwanda is anSVO Bantu language spoken by 12 million people mostly inRwanda.
All annotations were done by native speakers of En-glish.
The Kinyarwanda and Malagasy annotators had basicproficiency in these languages.15As a point of comparison, during the Penn Treebankproject, annotators corrected the syntactic bracketings pro-duced by a high-quality hand-written parser (Fidditch) andachieved a rate of only 375 tokens/hour using a specializedGUI interface (Marcus et al 1993).16Included with the data and software release (footnote 1).Omit.
prom Hist.
Mean1Ws MWs Tkns FEs 1 >1 ?10 ?102 comTweets 60 messages, 957 tokensA 597 56 304 23 43 17 11 5 .96B 644 47 266 28 37 23 12 6 .95Reviews 55 sentences, 778 tokensA 609 33 136 2 53 2 2 1 1.00C ?
D 643 19 116 114 11 44 38 21 .82T 704 ?
74 ?
55 0 0 0 1Table 2: Measures of our annotation samples.
Note thatannotator ?D?
specialized in noun phrase?internal structure,while annotator ?C?
specialized in verb phrase/clausal phe-nomena; C ?
D denotes the combination of their annotationfragments.
?T?
denotes our dependency conversion of theEnglish Web Treebank parses.
(The value 1.00 was roundedup from .9994.
)(EWTB) (Bies et al 2012).
(Our annotators onlysaw the tokenized text.)
Both datasets are infor-mal and conversational in nature, and are dom-inated by short messages/sentences.
In spite oftheir brevity, many of the items were deemed tocontain multiple ?utterances,?
which we define toinclude discourse connectives and emoticons (atbest marginal parts of the syntax); utterance headsare marked with ** in figure 1.Table 2 indicates the sizes of the two data sam-ples, and gives statistics over the output of eachannotator: total counts of single-word and mul-tiword lexical nodes, tokens not represented byany lexical node, and fudge nodes; as well asa histogram of promiscuity counts and the aver-age of commitment quotients (see ?4.1).
For in-stance, the two sets of annotations obtained for theTweets sample used underspecification in 17/60and 23/60 tweets, respectively, though the promis-cuity rarely exceeded 100 compatible trees per an-notation.
Examples can be seen in figure 1, whereannotator ?A?
marked only the noun phrase headfor the scarriest mystery door, opted not to choosea head within the quantity 1 1/2, and left ambigu-ous the attachment of the hedge like.
The strongbut not utter commitment to the dependency struc-ture is reflected in the mean commitment quotientsfor this dataset, both of which exceed 0.95.Inter-annotator agreement (IAA) is quantified intable 3.
The row marked A ?
B, for instance,considers the agreement between annotator ?A?and annotator ?B?.
Measuring IAA on the depen-dency structure requires a common set of lexicalnodes, so a lexical reconciliation step ensures that(a) any token used by either annotation is presentin both, and (b) no multiword node is presentin only one annotation?solved by relaxing in-compatible multiwords to FEs (which increasespromiscuity).
For Tweets, lexical reconciliation57thus reduces the commitment averages for eachannotation?to a greater extent for annotator ?A?
(.96 in table 2 vs. .82 in table 3) because ?A?marked more multiwords.
An analysis fully com-patible with both annotations exists for only 27/60sentences; the finer-grained softComPrec measure(?4.2), however, offers insight into the balance be-tween commitment and agreement.Qualitatively, we observe three leading causesof incompatibilities (disagreements): obvious an-notator mistakes (such as the marked as a head);inconsistent handling of verbal auxiliaries; and un-certainty whether to attach expressions to a verbor the root node, as with here in figure 1.17 An-notators noticed occasional ambiguous cases andattempted to encode the ambiguity with fudge ex-pressions: again in the tweet maybe put it off un-til you feel like ~ talking again ?
is one example.More often, fudge expressions proved useful forsyntactically difficult constructions, such as thoseshown in figure 1 as well as: 2 shy of breaking it,asked what tribe I was from, a $ 13 / day charge,you two, and the most awkward thing ever.5.3 Annotator SpecializationAs an experiment in using underspecification forlabor division, two of the annotators of Reviewsdata were assigned specific linguistic phenomenato focus on.
Annotator ?D?
was tasked with the in-ternal structure of base noun phrases, including re-solving the antecedents of personal pronouns.
?C?was asked to mark the remaining phenomena?i.e., utterance/clause/verb phrase structure?but tomark base noun phrases as fudge expressions,leaving their internal structure unspecified.
Bothannotators provided a full lexical analysis.
Forcomparison, a third individual, ?A,?
annotated thesame data in full.
The three annotators workedcompletely independently.Of the results in tables 2 and 3, the most notabledifference between full and specialized annotationis that the combination of independent specializedannotations (C ?
D) produces somewhat higherpromiscuity/lower commitment.
This is unsurpris-ing because annotators sometimes overlook rela-tionships that fall under their specialty.18 Still, an-notators reported that specialization made the task17Another example: Some uses of conjunctions like andand so can be interpreted as either phrasal coordinators or dis-course connectives (cf.
The PDTB Research Group, 2007).18A more practical and less error-prone approach might befor specialists to work sequentially or collaboratively (ratherthan independently) on each sentence.com softComPrecIAA 1 2 N|?|>0 1|2 2|1 F1Tweets (N=60)A ?
B .82 .91 27 .57 .72 .63Reviews (N=55)A ?
(C ?
D) .95 .76 30 .64 .40 .50A ?
T .92 1 26 .48 .91 .63(C ?
D) ?
T .73 1.00 28 .33 .93 .49Table 3: Measures of inter-annotator agreement.
Annotatorlabels are as in table 2.
Per-annotator com (with lexical rec-onciliation) and inter-annotator softComPrec are aggregatedover sentences by arithmetic mean.less burdensome, and the specialized annotationsdid prove complementary to each other.195.4 Treebank ComparisonThough the annotators in our study were nativespeakers well acquainted with representations ofEnglish syntax, we sought to quantify their agree-ment with the expert treebankers who created theEWTB (the source of the Reviews sentences).
Weconverted the EWTB?s constituent parses to de-pendencies via the PennConverter tool (Johanssonand Nugues, 2007),20 then removed punctuation.Agreement with the converted treebank parsesappears in the bottom two rows of table 3.
Be-cause the EWTB commits to a single analysis,precision scores are quite lopsided.
Most of itsattachments are consistent with our annotations(softComPrec > 0.9), but these allow many ad-ditional analyses (hence the scores below 0.5).6 ConclusionWe have presented a framework for simple depen-dency annotation that overcomes some of the rep-resentational limitations of unlabeled dependencygrammar and embraces the practical realities ofresource-building efforts.
Pilot studies (in multiplelanguages and domains, supported by a human-readable notation and a suite of open-source tools)showed this approach lends itself to rapid annota-tion with minimal training.The next step will be to develop algorithms ex-ploiting these representations for learning parsers.Other future extensions might include additionalexpressive mechanisms (e.g., multi-headedness,labels), crowdsourcing of FUDG annotations(Snow et al 2008), or even a semantic counter-part to the syntactic representation.19In fact, for only 2 sentences did ?C?
and ?D?
have in-compatible annotations, and both were due to simple mis-takes that were then fixed in the combination.20We ran PennConverter with options chosen to emulateour annotation conventions; see Schneider et al(2013).58AcknowledgmentsWe thank Lukas Biewald, Yoav Goldberg, Kyle Jerro, Vi-jay John, Lori Levin, Andr?
Martins, and several anony-mous reviewers for their insights.
This research was sup-ported in part by the U. S. Army Research Laboratory andthe U. S. Army Research Office under contract/grant numberW911NF-10-1-0533 and by NSF grant IIS-1054319.ReferencesTimothy Baldwin and Su Nam Kim.
2010.
Multi-word expressions.
In Nitin Indurkhya and Fred J.Damerau, editors, Handbook of Natural LanguageProcessing, Second Edition.
CRC Press, Taylor andFrancis Group, Boca Raton, FL.Benjamin K. Bergen and Nancy Chang.
2005.
Embod-ied Construction Grammar in simulation-based lan-guage understanding.
In Jan-Ola ?stman and Mir-jam Fried, editors, Construction grammars: cog-nitive grounding and theoretical extensions, pages147?190.
John Benjamins, Amsterdam.Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.2012.
English Web Treebank.
Technical Re-port LDC2012T13, Linguistic Data Consortium,Philadelphia, PA.Alena B?hmov?, Jan Hajic?, Eva Hajic?ov?, BarboraHladk?, and Anne Abeill?.
2003.
The Prague De-pendency Treebank: a three-level annotation sce-nario.
In Treebanks: building and using parsed cor-pora, pages 103?127.
Springer.Seth Chaiken and Daniel J. Kleitman.
1978.
MatrixTree Theorems.
Journal of Combinatorial Theory,Series A, 24(3):377?381.Noam Chomsky.
1957.
Syntactic Structures.
Mouton,La Haye.Stephen Clark and James Curran.
2006.
Partial trainingfor a lexicalized-grammar parser.
In Proceedings ofthe Human Language Technology Conference of theNAACL (HLT-NAACL 2006), pages 144?151.
As-sociation for Computational Linguistics, New YorkCity, USA.William Croft.
2001.
Radical Construction Grammar:Syntactic Theory in Typological Perspective.
OxfordUniversity Press, Oxford.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
Stanford typed dependencies man-ual.
http://nlp.stanford.edu/downloads/dependencies_manual.pdf.Kitty Burns Florey.
2006.
Sister Bernadette?s BarkingDog: The quirky history and lost art of diagrammingsentences.
Melville House, New York.Victoria Fossum and Roger Levy.
2012.
Sequentialvs.
hierarchical syntactic models of human incre-mental sentence processing.
In Proceedings of the3rd Workshop on Cognitive Modeling and Computa-tional Linguistics (CMCL 2012), pages 61?69.
As-sociation for Computational Linguistics, Montr?al,Canada.Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,Joseph Le Roux, Stephen Hogan, Joakim Nivre,Deirdre Hogan, and Josef van Genabith.
2011.#hardtoparse: POS Tagging and Parsing the Twitter-verse.
In Proceedings of the 2011 AAAI Workshopon Analyzing Microtext, pages 20?25.
AAAI Press,San Francisco, CA.The PDTB Research Group.
2007.
The Penn DiscourseTreebank 2.0 annotation manual.
Technical ReportIRCS-08-01, Institute for Research in Cognitive Sci-ence, University of Pennsylvania, Philadelphia, PA.Jan Hajic?.
1998.
Building a syntactically annotatedcorpus: the Prague Dependency Treebank.
In EvaHajic?ov?, editor, Issues of Valency and Meaning.Studies in Honor of Jarmila Panevov?, pages 12?19.
Prague Karolinum, Charles University Press,Prague.Jan Hajic?, Barbora Vidov?
Hladk?, and Petr Pajas.2001.
The Prague Dependency Treebank: anno-tation structure and support.
In Proceedings ofthe IRCS Workshop on Linguistic Databases, pages105?114.
University of Pennsylvania, Philadelphia,USA.Richard A. Hudson.
1984.
Word Grammar.
Blackwell,Oxford.Rebecca Hwa.
1999.
Supervised grammar inductionusing training data with limited constituent infor-mation.
In Proceedings of the 37th Annual Meet-ing of the Association for Computational Linguistics(ACL-99), pages 73?79.
Association for Computa-tional Linguistics, College Park, Maryland, USA.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.In Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muis-chnek, and Mare Koit, editors, Proceedings of the16th Nordic Conference of Computational Linguis-tics (NODALIDA-2007), pages 105?112.
Tartu, Es-tonia.Martha Kolln and Robert Funk.
1994.
UnderstandingEnglish Grammar.
Macmillan, New York.Sandra K?bler, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Number 2 in SynthesisLectures on Human Language Technologies.
Mor-gan & Claypool, San Rafael, CA.Marco Kuhlmann and Joakim Nivre.
2006.
Mildly non-projective dependency structures.
In Proceedingsof the COLING/ACL 2006 Main Conference PosterSessions, pages 507?514.
Association for Computa-tional Linguistics, Sydney, Australia.59Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.David Marec?ek, Martin Popel, Loganathan Ramasamy,Jan ?te?p?nek, Daniel Zeman, Zdene?k ?abokrtsk?,and Jan Hajic?.
2013.
Cross-language study on in-fluence of coordination style on dependency parsingperformance.
Technial Report 49, ?FAL MFF UK.Jonathan Margoliash.
2010.
Matrix-Tree Theorem fordirected graphs.
http://www.math.uchicago.edu/~may/VIGRE/VIGRE2010/REUPapers/Margoliash.pdf.Igor Aleksandrovic?
Mel?c?uk.
1988.
Dependency Syn-tax: Theory and Practice.
SUNY Press, Albany,NY.Joakim Nivre.
2005.
Dependency grammar and depen-dency parsing.
Technical Report MSI report 05133,V?xj?
University School of Mathematics and Sys-tems Engineering, V?xj?, Sweden.Timothy Osborne, Michael Putnam, and Thomas Gro?.2012.
Catenae: introducing a novel unit of syntacticanalysis.
Syntax, 15(4):354?396.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah A.Smith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 380?390.
Association for Computational Lin-guistics, Atlanta, Georgia, USA.Alonzo Reed and Brainerd Kellogg.
1877.
Work onEnglish grammar & composition.
Clark & Maynard.Federico Sangati and Chiara Mazza.
2009.
An Englishdependency treebank ?
la Tesni?re.
In Marco Pas-sarotti, Adam Przepi?rkowski, Savina Raynaud, andFrank Van Eynde, editors, Proceedings of the EigthInternational Workshop on Treebanks and LinguisticTheories, pages 173?184.
EDUCatt, Milan, Italy.Gerold Schneider.
1998.
A linguistic comparison ofconstituency, dependency and link grammar.
Mas-ter?s thesis, University of Zurich.Nathan Schneider, Brendan O?Connor, Naomi Saphra,David Bamman, Manaal Faruqui, Noah A. Smith,Chris Dyer, and Jason Baldridge.
2013.
A frame-work for (under)specifying dependency syntax with-out overloading annotators.
arXiv:1306.2091[cs.CL].
arxiv.org/pdf/1306.2091.Petr Sgall, Eva Hajic?ov?, and Jarmila Panevov?.1986.
The Meaning of the Sentence in its Seman-tic and Pragmatic Aspects.
Reidel, Dordrecht andAcademia, Prague.Stuart M. Shieber.
1992.
Constraint-Based GrammarFormalisms.
MIT Press, Cambridge, MA.Daniel Sleator and Davy Temperly.
1993.
Parsing En-glish with a link grammar.
In Proceedings of theThird International Workshop on Parsing Technol-ogy (IWPT?93), pages 277?292.
Tilburg, Nether-lands.David A. Smith and Noah A. Smith.
2007.
Proba-bilistic models of nonprojective dependency trees.In Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL 2007), pages 132?140.
Associa-tion for Computational Linguistics, Prague, CzechRepublic.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Ng.
2008.
Cheap and fast ?
but is it good?Evaluating non-expert annotations for natural lan-guage tasks.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP 2008), pages 254?263.
As-sociation for Computational Linguistics, Honolulu,Hawaii.Mark Steedman.
2000.
The Syntatic Process.
MITPress, Cambridge, MA.Luc Steels, Jan-Ola ?stman, and Kyoko Ohara, editors.2011.
Design patterns in Fluid Construction Gram-mar.
Number 11 in Constructional Approaches toLanguage.
John Benjamins, Amsterdam.Lucien Tesni?re.
1959.
El?ments de Syntaxe Struc-turale.
Klincksieck, Paris.Michael Tomasello.
2003.
Constructing a Language: AUsage-Based Theory of Language Acquisition.
Har-vard University Press, Cambridge, MA.60
