Class-based Collocations for Word-Sense DisambiguationTom O?HaraDepartment of Computer ScienceNew Mexico State UniversityLas Cruces, NM 88003-8001tomohara@cs.nmsu.eduRebecca BruceDepartment of Computer ScienceUniversity of North Carolina at AshevilleAsheville, NC 28804-3299bruce@cs.unca.eduJeff DonnerDepartment of Computer ScienceNew Mexico State UniversityLas Cruces, NM 88003-8001jdonner@cs.nmsu.eduJanyce WiebeDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260-4034wiebe@cs.pitt.eduAbstractThis paper describes the NMSU-Pitt-UNCAword-sense disambiguation system participat-ing in the Senseval-3 English lexical sampletask.
The focus of the work is on using seman-tic class-based collocations to augment tradi-tional word-based collocations.
Three separatesources of word relatedness are used for thesecollocations: 1) WordNet hypernym relations;2) cluster-based word similarity classes; and 3)dictionary definition analysis.1 IntroductionSupervised systems for word-sense disambigua-tion (WSD) often rely upon word collocations(i.e., sense-specific keywords) to provide clueson the most likely sense for a word given thecontext.
In the second Senseval competition,these features figured predominantly among thefeature sets for the leading systems (Mihalcea,2002; Yarowsky et al, 2001; Seo et al, 2001).A limitation of such features is that the wordsselected must occur in the test data in order forthe features to apply.
To alleviate this problem,class-based approaches augment word-level fea-tures with category-level ones (Ide and Ve?ronis,1998; Jurafsky and Martin, 2000).
When ap-plied to collocational features, this approach ef-fectively uses class labels rather than wordformsin deriving the collocational features.This research focuses on the determinationof class-based collocations to improve word-sense disambiguation.
We do not address refine-ment of existing algorithms for machine learn-ing.
Therefore, a commonly used decision treealgorithm is employed to combine the variousfeatures when performing classification.This paper describes the NMSU-Pitt-UNCA system we developed for the thirdSenseval competition.
Section 2 presents anoverview of the feature set used in the system.Section 3 describes how the class-based colloca-tions are derived.
Section 4 shows the resultsover the Senseval-3 data and includes detailedanalysis of the performance of the various col-locational features.2 System OverviewWe use a decision tree algorithm for word-sensedisambiguation that combines features from thelocal context of the target word with other lex-ical features representing the broader context.Figure 1 presents the features that are usedin this application.
In the first Senseval com-petition, we used the first two groups of fea-tures, Local-context features and Collocationalfeatures, with competitive results (O?Hara et al,2000).Five of the local-context features representthe part of speech (POS) of words immediatelysurrounding the target word.
These five fea-tures are POS?i for i from -2 to +2 ), wherePOS+1, for example, represents the POS of theword immediately following the target word.Five other local-context features representthe word tokens immediately surrounding thetarget word (Word?i for i from ?2 to +2).Each Word?i feature is multi-valued; its valuescorrespond to all possible word tokens.There is a collocation feature WordCollsde-fined for each sense s of the target word.
Itis a binary feature, representing the absence orpresence of any word in a set specifically chosenfor s. A word w that occurs more than once inthe training data is included in the collocationset for sense s if the relative percent gain in theconditional probability over the prior probabil-Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of SystemsLocal-context featuresPOS: part-of-speech of target wordPOS?i: part-of-speech of word at offset iWordForm: target wordformWord?i: stem of word at offset iCollocational featuresWordColls: word collocation for sense sWordColl?wordform of non-sense-specificcollocation (enumerated)Class-based collocational featuresHyperColls: hypernym collocation for sHyperColl?,i: non-sense-specific hypernym collo-cationSimilarColls: similarity collocation for sDictColls: dictionary collocation for sFigure 1: Features for word-sense disambigua-tion.
All collocational features are binary indi-cators for sense s, except for WordColl?.ity is 20% or higher:(P (s|w) ?
P (s))P (s) ?
0.20.This threshold was determined to be effectivevia an optimization search over the Senseval-2data.
WordColl?represents a set of non-sense-specific collocations (i.e., not necessarily indica-tive of any one sense), chosen via the G2 criteria(Wiebe et al, 1998).
In contrast to WordColls,each of which is a separate binary feature, thewords contained in the set WordColl?serve asvalues in a single enumerated feature.These features are augmented with class-based collocational features that represent in-formation about word relationships derivedfrom three separate sources: 1) WordNet(Miller, 1990) hypernym relations (HyperColl);2) cluster-based word similarity classes (Simi-larColl); and 3) relatedness inferred from dictio-nary definition analysis (DictColl).
The infor-mation inherent in the sources from which theseclass-based features are derived allows wordsthat do not occur in the training data contextto be considered as collocations during classifi-cation.3 Class-based CollocationsThe HyperColl features are intended to capturea portion of the information in the WordNet hy-pernyms links (i.e., is-a relations).
Hypernym-based collocations are formulated by replacingeach word in the context of the target word (e.g.,in the same sentence as the target word) withits complete hypernym ancestry from WordNet.Since context words are not sense-tagged, eachsynset representing a different sense of a contextword is included in the set of hypernyms replac-ing that word.
Likewise, in the case of multipleinheritance, each parent synset is included.The collocation variable HyperCollsfor eachsense s is binary, corresponding to the absenceor presence of any hypernym in the set chosenfor s. This set of hypernyms is chosen using theratio of conditional probability to prior prob-ability as described for the WordCollsfeatureabove.
In contrast, HyperColl?,iselects non-sense-specific hypernym collocations: 10 sepa-rate binary features are used based on the G2selection criteria.
(More of these features couldbe used, but they are limited for tractability.
)For more details on hypernym collocations, see(O?Hara, forthcoming).Word-similarity classes (Lin, 1998) derivedfrom clustering are also used to expand thepool of potential collocations; this type of se-mantic relatedness among words is expressed inthe SimilarColl feature.
For the DictColl fea-tures, definition analysis (O?Hara, forthcoming)is used to determine the semantic relatedness ofthe defining words.
Differences between thesetwo sources of word relations are illustrated bylooking at the information they provide for ?bal-lerina?
:word-clusters:dancer:0.115 baryshnikov:0.072pianist:0.056 choreographer:0.049... [18 other words]nicole:0.041 wrestler:0.040tibetans:0.040 clown:0.040definition words:dancer:0.0013 female:0.0013 ballet:0.0004This shows that word clusters capture a widerrange of relatedness than the dictionary def-initions at the expense of incidental associa-tions (e.g., ?nicole?).
Again, because contextwords are not disambiguated, the relations forall senses of a context word are conflated.
Fordetails on the extraction of word clusters, see(Lin, 1998); and, for details on the definitionanalysis, see (O?Hara, forthcoming).When formulating the features SimilarColland DictColl, the words related to each con-text word are considered as potential colloca-tions (Wiebe et al, 1998).
Co-occurrence fre-Sense Distinctions Precision RecallFine-grained .566 .565Course-grained .660 .658Table 1: Results for Senseval-3 test data.99.72% of the answers were attempted.
All fea-tures from Figure 1 were used.quencies f(s,w) are used in estimating the con-ditional probability P (s|w) required by the rel-ative conditional probability selection schemenoted earlier.
However, instead of using a unitweight for each co-occurrence, the relatednessweight is used (e.g., 0.056 for ?pianist?
); and,because a given related-word might occur withmore than one context word for the same target-word sense, the relatedness weights are added.The conditional probability of the sense giventhe relatedness collocation is estimated by di-viding the weighted frequency by the sum of allsuch weighted co-occurrence frequencies for theword:P (s|w)?
wf (s,w)?s?wf (s?, w)Here wf(s, w) stands for the weighted co-occurrence frequency of the related-word collo-cation w and target sense s.The relatedness collocations are less reliablethan word collocations given the level of indi-rection involved in their extraction.
Therefore,tighter constraints are used in order to filter outextraneous potential collocations.
In particular,the relative percent gain in the conditional ver-sus prior probability must be 80% or higher, athreshold again determined via an optimizationsearch over the Senseval-2 data.
In addition,the context words that they are related to mustoccur more than four times in the training data.4 Results and DiscussionDisambiguation is performed via a decision treeformulated using Weka?s J4.8 classifier (Wittenand Frank, 1999).
For the system used in thecompetition, the decision tree was learned overthe entire Senseval-3 training data and then ap-plied to the test data.
Table 1 shows the resultsof our system in the Senseval-3 competition.Table 2 shows the results of 10-fold cross-validation just over the Senseval-3 training data(using Naive Bayes rather than decision trees.
)To illustrate the contribution of the three typesExperiment Precision?Local +LocalLocal - .593WordColl .490 .599HyperColl .525 .590DictColl .532 .570SimilarColl .534 .586HyperColl+WordColl .525 .611DictColl+WordColl .501 .606SimilarColl+WordColl .518 .596All Collocations .543 .608#Words: 57 Avg.
Entropy: 1.641Avg.
#Senses: 5.3 Baseline: 0.544Table 2: Results for Senseval-3 training data.All values are averages, except #Words, whichis the number of distinct word types classified.Baseline always uses the most-frequent sense.of class-based collocations, the table shows re-sults separately for systems developed using asingle feature type, as well as for all features incombination.
In addition, the performance ofthese systems are shown with and without theuse of the local features (Local), as well as withand without the use of standard word colloca-tions (WordColl).
As can be seen, the related-word and definition collocations perform betterthan hypernym collocations when used alone.However, hypernym collocations perform bet-ter when combined with other features.
Fu-ture work will investigate ways of ameliorat-ing such interactions.
The best overall system(HyperColl+WordColl+Local) uses the com-bination of local-context features, word colloca-tions, and hypernym collocations.
The perfor-mance of this system compared to a more typi-cal system for WSD (WordColl+Local) is sta-tistically significant at p < .05, using a pairedt-test.We analyzed the contributions of the variouscollocation types to determine their effective-ness.
Table 3 shows performance statistics foreach collocation type taken individually over thetraining data.
Precision is based on the num-ber of correct positive indicators versus the to-tal number of positive indicators, whereas recallis the number correct over the total number oftraining instances (7706).
This shows that hy-pernym collocations are nearly as effective asword collocations.
We also analyzed the occur-rence of unique positive indicators provided bythe collocation types over the training data.
Ta-Total TotalFeature #Corr.
#Pos.
Recall Prec.DictColl 273 592 .035 .461HyperColl 2932 6479 .380 .453SimilarColl 528 1535 .069 .344WordColl 3707 7718 .481 .480Table 3: Collocation performance statistics.Total #Pos.
is number of positive indicators forthe collocation in the training data, and Total#Corr.
is the number of these that are correct.Unique UniqueFeature #Corr.
#Pos.
Prec.DictColl 110 181 .608HyperColl 992 1795 .553SimilarColl 198 464 .427DictColl 1244 2085 .597Table 4: Analysis of unique positive indicators.Unique #Pos.
is number of training instanceswith the feature as the only positive indicator,and Unique #Corr.
is number of these correct.ble 4 shows how often each feature type is pos-itive for a particular sense when all other fea-tures for the sense are negative.
This occursfairly often, suggesting that the different typesof collocations are complementary and thus gen-erally useful when combined for word-sense dis-ambiguation.
Both tables illustrate coverageproblems for the definition and related wordcollocations, which will be addressed in futurework.ReferencesNancy Ide and Jean Ve?ronis.
1998.
Introduc-tion to the special issue on word sense dis-ambiguation: the state of the art.
Computa-tional Linguistics, 24(1):1?40.Daniel Jurafsky and James H. Martin.
2000.Speech and Language Processing.
PrenticeHall, Upper Saddle River, New Jersey.Dekang Lin.
1998.
Automatic retrievaland clustering of similar words.
In Proc.COLING-ACL 98, pages 768?764, Montreal.August 10-14.Rada Mihalcea.
2002.
Instance based learningwith automatic feature selection applied toword sense disambiguation.
In Proceedings ofthe 19th International Conference on Com-putational Linguistics (COLING 2002), Tai-wan.
August 26-30.George Miller.
1990.
Introduction.
Interna-tional Journal of Lexicography, 3(4): SpecialIssue on WordNet.Tom O?Hara, Janyce Wiebe, and Rebecca F.Bruce.
2000.
Selecting decomposable modelsfor word-sense disambiguation: The grling-sdm system.
Computers and the Humanities,34(1-2):159?164.Thomas P. O?Hara.
forthcoming.
Empirical ac-quisition of conceptual distinctions via dictio-nary definitions.
Ph.D. thesis, Department ofComputer Science, New Mexico State Univer-sity.Hee-Cheol Seo, Sang-Zoo Lee, Hae-ChangRim, and Ho Lee.
2001.
KUNLP sys-tem using classification information model atSENSEVAL-2.
In Proceedings of the SecondInternational Workshop on Evaluating WordSense Disambiguation Systems (SENSEVAL-2), pages 147?150, Toulouse.
July 5-6.Janyce Wiebe, Kenneth McKeever, and Re-becca F. Bruce.
1998.
Mapping collocationalproperties into machine learning features.
InProc.
6th Workshop on Very Large Corpora(WVLC-98), pages 225?233, Montreal, Que-bec, Canada.
Association for ComputationalLinguistics.
SIGDAT.Ian H. Witten and Eibe Frank.
1999.
DataMining: Practical Machine Learning Toolsand Techniques with Java Implementations.Morgan Kaufmann, San Francisco, CA.David Yarowsky, Silviu Cucerzan, Radu Flo-rian, Charles Schafer, and Richard Wicen-towski.
2001.
The Johns Hopkins SENSE-VAL2 system descriptions.
In Proceedings ofthe Second International Workshop on Eval-uating Word Sense Disambiguation Systems(SENSEVAL-2), pages 163?166, Toulouse.July 5-6.
