Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1443?1452,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsLearning to Translate with Source and Target SyntaxDavid ChiangUSC Information Sciences Institute4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292 USAchiang@isi.eduAbstractStatistical  translation  models  that  try  tocapture the recursive structure of languagehave been widely adopted over the last fewyears.
These  models  make  use  of  vary-ing amounts of information from linguis-tic theory: some use none at all, some useinformation about the grammar of the tar-get language, some use information aboutthe grammar of the source language.
Butprogress  has  been  slower  on  translationmodels  that  are  able  to  learn  the  rela-tionship  between  the  grammars  of  boththe source and target  language.
We dis-cuss the reasons why this has been a chal-lenge, review existing attempts to meet thischallenge, and show how some old andnew ideas can be combined into a  sim-ple approach that uses both source and tar-get syntax for significant improvements intranslation accuracy.1 IntroductionStatistical translation models that use synchronouscontext-free  grammars  (SCFGs)  or  related  for-malisms to try to capture the recursive structure oflanguage have been widely adopted over the lastfew years.
The simplest of these (Chiang, 2005)make no use of information from syntactic theo-ries or syntactic annotations, whereas others havesuccessfully incorporated syntactic information onthe target side (Galley et al, 2004; Galley et al,2006) or the source side (Liu et al, 2006; Huanget al, 2006).
The next obvious step is toward mod-els that make full use of syntactic information onboth sides.
But the natural generalization to thissetting has been found to underperform phrase-based models (Liu et al, 2009; Ambati and Lavie,2008), and researchers have begun to explore so-lutions (Zhang et al, 2008; Liu et al, 2009).In this paper, we explore the reasons why tree-to-tree translation has been challenging, and howsource syntax and target syntax might be used to-gether.
Drawing on previous successful attempts torelax syntactic constraints during grammar extrac-tion in various ways (Zhang et al, 2008; Liu et al,2009; Zollmann and Venugopal, 2006), we com-pare several methods for extracting a synchronousgrammar from tree-to-tree data.
One confoundingfactor in such a comparison is that some methodsgenerate many new syntactic categories, making itmore difficult to satisfy syntactic constraints at de-coding time.
We therefore propose to move theseconstraints from the formalism into the model, im-plemented as features in the hierarchical phrase-based  model  Hiero  (Chiang, 2005).
This  aug-mented model is able to learn from data whetherto rely on syntax or not, or to revert back to mono-tone phrase-based translation.In experiments on Chinese-English and Arabic-English translation, we find that when both sourceand target syntax are made available to the modelin an unobtrusive way, the model chooses to buildstructures that are more syntactically well-formedand yield significantly better translations than anonsyntactic hierarchical phrase-based model.2 Grammar extractionA synchronous tree-substitution grammar (STSG)is a set of rules or elementary tree pairs (?, ?),where:?
?
is a tree whose interior labels are source-language  nonterminal  symbols  and  whosefrontier labels are source-language nontermi-nal symbols or terminal symbols (words).
Thenonterminal-labeled frontier nodes are calledsubstitution  nodes, conventionally  markedwith an arrow (?).?
?
is  a  tree  of  the  same  form except  with1443....PP....LCP....LC..
?zh?ng..NP?..P..?z?i...PP.....NP?..IN...in....NP....NP..NN..??m?oy?.NP....NP..NN...??n.QP..CD..?li?ng...NP.....PP.....NP.....NNS...shores...CD...two.DT...the.IN...between.NP...NN...trade....PP....LCP....LC..
?zh?ng.NP....NP..NN..??m?oy?.NP.
..NP..NN...??n.QP..CD..?li?ng..P..
?z?i...PP.....NP.....PP.....NP.....NNS...shores...CD...two.DT...the.IN...between.NP...NN...trade.IN...in(?1, ?1) (?2, ?2) (?3, ?3)Figure 1: Synchronous tree substitution.
Rule (?2, ?2) is substituted into rule (?1, ?1) to yield (?3, ?3).target-language  instead  of  source-languagesymbols.?
The substitution nodes of ?
are aligned bijec-tively with those of ?.?
The terminal-labeled frontier nodes of ?
arealigned (many-to-many) with those of ?.In the substitution operation, an aligned pair  ofsubstitution nodes is rewritten with an elementarytree pair.
The labels of the substitution nodes mustmatch the root labels of the elementary trees withwhich they are rewritten (but we will relax thisconstraint below).
See Figure 1 for examples of el-ementary tree pairs and substitution.2.1 Exact tree-to-tree extractionThe use of STSGs for translation was proposedin the Data-Oriented Parsing literature (Poutsma,2000; Hearne  and  Way, 2003)  and  by  Eis-ner (2003).
Both of these proposals are more am-bitious  about  handling  spurious  ambiguity  thanapproaches derived from phrase-based translationusually have been (the former uses random sam-pling to sum over equivalent derivations during de-coding, and the latter uses dynamic programminghuman automaticstring-to-string 198,445 142,820max nested 78,361 64,578tree-to-string 60,939 (78%) 48,235 (75%)string-to-tree 59,274 (76%) 46,548 (72%)tree-to-tree 53,084 (68%) 39,049 (60%)Table 1: Analysis  of  phrases  extracted  fromChinese-English newswire data with human andautomatic  word  alignments  and  parses.
As  treeconstraints are added, the number of phrase pairsdrops.
Errors  in  automatic  annotations  also  de-crease the number of phrase pairs.
Percentages arerelative to the maximum number of nested phrasepairs.to sum over equivalent derivations during train-ing).
If we take a more typical approach, whichgeneralizes that of Galley et al (2004; 2006) andis similar to Stat-XFER (Lavie et al, 2008), weobtain the following grammar extraction method,which we call exact tree-to-tree extraction.Given  a  pair  of  source-  and  target-languageparse trees with a word alignment between theirleaves, identify  all  the phrase pairs ( f?
, e?
), i.e.,those substring pairs that respect the word align-1444...IP....VP..??????y?b?is?sh?q???m?iyu?n...NP..NN..
??sh?nch?...PP....LCP....LC..
?zh?ng.NP....NP..NN..??m?oy?.NP....NP..NN...??n.QP..CD..?li?ng..P..?z?i..NP..NR..?
?T?iw?n...S.....VP...is 14.7 billion US dollars.NP.....PP.....NP.....PP.....NP.....NNS...shores...CD...two.DT...the.IN...between.NP...NN...trade.IN...in.NP.....NN...surplus.NP.....POS...?s.NNP...TaiwanFigure 2: Example Chinese-English sentence pair with human-annotated parse trees and word alignments.ment in the sense that at least one word in f?
isaligned to a word in e?, and no word in f?
is alignedto a word outside of e?, or vice versa.
Then the ex-tracted grammar is the smallest STSGG satisfying:?
If (?, ?)
is a pair of subtrees of a training ex-ample and the frontiers of ?
and ?
form aphrase pair, then (?, ?)
is a rule in G.?
If (?2, ?2) ?
G, (?3, ?3) ?
G, and (?1, ?1) isan elementary tree pair such that substituting(?2, ?2) into (?1, ?1) results in (?3, ?3), then(?1, ?1) is a rule in G.For example, consider the training example in Fig-ure 2, from which the elementary tree pairs shownin Figure 1 can be extracted.
The elementary treepairs (?2, ?2) and (?3, ?3) are rules in G becausetheir yields are phrase pairs, and (?1, ?1) resultsfrom subtracting (?2, ?2) from (?3, ?3).2.2 Fuzzy tree-to-tree extractionExact tree-to-tree translation requires that transla-tion rules deal with syntactic constituents on boththe source and target side, which reduces the num-ber of eligible phrases.
Table 1 shows an analy-sis of phrases extracted from human word-alignedand parsed data and automatically word-alignedand parsed data.1The first line shows the num-ber of phrase-pair occurrences that are extractedin the absence of syntactic constraints,2and thesecond line shows the maximum number of nestedphrase-pair occurrences, which is the most that ex-act syntax-based extraction can achieve.
Whereastree-to-string extraction and string-to-tree extrac-tion  permit  70?80%  of  the  maximum  possiblenumber of phrase pairs, tree-to-tree extraction onlypermits 60?70%.Why does this happen?
We can see that movingfrom human annotations to automatic annotationsdecreases not only the absolute number of phrasepairs, but the percentage of phrases that pass thesyntactic filters.
Wellington et al (2006), in a moresystematic study, find that, of sentences where thetree-to-tree constraint blocks rule extraction, themajority are due to parser errors.
To address thisproblem, Liu et al (2009) extract rules from pairs1The  first  2000  sentences  from  the  GALE Phase  4Chinese  Parallel  Word  Alignment  and  Tagging  Part 1(LDC2009E83) and the Chinese News Translation Text Part 1(LDC2005T06), respectively.2Only counting phrases that have no unaligned words attheir endpoints.1445of packed forests instead of pairs of trees.
Since apacked forest is much more likely to include thecorrect tree, it is less likely that parser errors willcause good rules to be filtered out.However, even on human-annotated data, tree-to-tree extraction misses many rules, and manysuch  rules  would  seem  to  be  useful.
For  ex-ample, in  Figure 2, the  whole  English  phrase?Taiwan?s.
.
.shores?
is  an  NP,  but  its  Chinesecounterpart is not a constituent.
Furthermore, nei-ther ?surplus.
.
.shores?
nor its Chinese counterpartare constituents.
But both rules are arguably use-ful for translation.
Wellington et al therefore ar-gue that in order to extract as many rules as possi-ble, a more powerful formalism than synchronousCFG/TSG is required: for  example, generalizedmultitext grammar (Melamed et al, 2004), whichis equivalent to synchronous set-local multicom-ponent CFG/TSG (Weir, 1988).But  the  problem illustrated  in  Figure 2 doesnot reflect a very deep fact about syntax or cross-lingual divergences, but rather choices in annota-tion style that interact badly with the exact tree-to-tree extraction heuristic.
On the Chinese side,the IP is too flat (because ?
?/T?iw?n has beenanalyzed as a topic), whereas the more articulatedstructure(1) [NPT?iw?n [NP[PPza?
.
.
.]
sh?nch?
]]would also be quite reasonable.
On the Englishside, the high attachment of the PP disagrees withthe corresponding Chinese structure, but low at-tachment also seems reasonable:(2) [NP[NPTaiwan?s] [NPsurplus in trade.
.
.
]]Thus even in the gold-standard parse trees, phrasestructure  can be underspecified (like the flat  IPabove) or uncertain (like the PP attachment above).For this reason, some approaches work with amore flexible notion of constituency.
Synchronoustree-sequence?substitution grammar (STSSG) al-lows either side of a rule to comprise a sequence oftrees instead of a single tree (Zhang et al, 2008).
Inthe substitution operation, a sequence of sister sub-stitution nodes is rewritten with a tree sequence ofequal length (see Figure 3a).
This extra flexibilityeffectively makes the analysis (1) available to us.Any STSSG can be converted into an equivalentSTSG via the creation of virtual nodes (see Fig-ure 3b): for every elementary tree sequence withroots X1, .
.
.
,Xn, create a new root node with a...NP....NNP?...NNP?...NN..Minister.NN..Prime.???.
...NNP..Ariel.
...NNP..Sharon.???(a)...NP....NNP?NNP?...NN..Minister.NN..Prime.
...NNP?NNP....NNP..Sharon.NNP..Ariel(b)Figure 3: (a) Example tree-sequence substitutiongrammar and (b) its equivalent SAMT-style tree-substitution grammar.complex label X1?
?
?
?
?Xnimmediately dominat-ing the old roots, and replace every sequence ofsubstitution sites X1, .
.
.
,Xnwith a single substi-tution site X1?
?
?
?
?Xn.
This is essentially whatsyntax-augmented MT (SAMT) does, in the string-to-tree setting (Zollmann and Venugopal, 2006).
Inaddition, SAMT drops the requirement that the Xiare sisters, and uses categories X / Y (an Xmissinga Y on the right) and Y \X (an Xmissing a Y on theleft) in the style of categorial grammar (Bar-Hillel,1953).
Under this flexible notion of constituency,both (1) and (2) become available, albeit with morecomplicated categories.Both STSSG and SAMT are examples of whatwe might call fuzzy tree-to-tree extraction.
We fol-low this approach here as well: as in STSSG, wework on tree-to-tree data, and we use the com-plex categories of SAMT.
Moreover, we allow theproduct categoriesX1?
?
?
?
?Xnto be of any lengthn, and we allow the slash categories to take anynumber of arguments on either side.
Thus everyphrase can be assigned a (possibly very complex)syntactic category, so that fuzzy tree-to-tree ex-traction does not lose any rules relative to string-to-string extraction.On the other hand, if several rules are extracted1446that differ only in their nonterminal labels, only themost-frequent rule is kept, and its count is the to-tal count of all the rules.
This means that there is aone-to-one correspondence between the rules ex-tracted by fuzzy tree-to-tree extraction and hierar-chical string-to-string extraction.2.3 Nesting phrasesFuzzy tree-to-tree extraction (like string-to-stringextraction) generates many times more rules thanexact tree-to-tree extraction does.
In Figure 2, weobserved that the flat structure of the Chinese IPprevented  exact  tree-to-tree  extraction  from ex-tracting a rule containing just part of the IP, forexample:(3) [PPza?
.
.
.]
[NPsh?nch?
](4) [NPT?iw?n] [PPza?
.
.
.]
[NPsh?nch?
](5) [PPza?
.
.
.]
[NPsh?nch?]
[VP.
.
.
m?iyu?n]Fuzzy tree-to-tree extraction allows any of theseto be the source side of a rule.
We might think ofit as effectively restructuring the trees by insert-ing nodes with complex labels.
However, it is notpossible to represent this restructuring with a sin-gle tree (see Figure 4).
More formally, let us saythat two phrases wi?
?
?wj?1 and wi?
?
?
?wj?
?1 nestif i ?
i?
< j?
?
j or i?
?
i < j < j?
; otherwise,they cross.
The two Chinese phrases (4) and (5)cross, and therefore cannot both be constituents inthe same tree.
In other words, exact tree-to-tree ex-traction commits to a single structural analysis butfuzzy tree-to-tree extraction pursues many restruc-tured analyses at once.We can strike a compromise by continuing to al-low SAMT-style complex categories, but commit-ting to a single analysis by requiring all phrases tonest.
To do this, we use a simple heuristic.
Iteratethrough all the phrase pairs ( f?
, e?)
in the followingorder:1. sort by whether f?
and e?
can be assigned a sim-ple syntactic category (both, then one, thenneither); if there is a tie,2.
sort by how many syntactic constituents f?
ande?
cross (low to high); if there is a tie,3.
give priority to ( f?
, e?)
if neither f?
nor e?
be-gins or ends with punctuation; if there is a tie,finally4.
sort by the position of f?
in the source-sidestring (right to left).For each phrase pair, accept it if it does not crossany previously accepted phrase pair; otherwise, re-ject it.Because this heuristic produces a set of nestingphrases, we can represent them all in a single re-structured tree.
In Figure 4, this heuristic choosesstructure (a) because the English-side counterpartof IP/VP has the simple category NP.3 DecodingIn  decoding, the  rules  extracted during trainingmust be reassembled to form a derivation whosesource side matches the input sentence.
In the ex-act  tree-to-tree  approach, whenever  substitutionis  performed, the  root  labels  of  the  substitutedtrees  must  match  the  labels  of  the  substitutionnodes?call this the matching constraint.
Becausethis constraint must be satisfied on both the sourceand target side, it can become difficult to general-ize well from training examples to new input sen-tences.Venugopal et al (2009), in the string-to-tree set-ting, attempt to soften the data-fragmentation ef-fect of the matching constraint: instead of tryingto find the single derivation with the highest prob-ability, they sum over derivations that differ onlyin their nonterminal labels and try to find the sin-gle derivation-class with the highest probability.Still, only  derivations  that  satisfy  the  matchingconstraint are included in the summation.But in some cases we may want to soften thematching  constraint  itself.
Some syntactic  cate-gories are similar enough to be considered com-patible: for example, if a rule rooted in VBD (past-tense verb) could substitute into a site labeled VBZ(present-tense verb), it might still generate correctoutput.
This is all the more true with the additionof SAMT-style categories: for example, if a rulerooted in ADVP?VP could substitute into a sitelabeled VP, it would very likely generate correctoutput.Since we want syntactic information to help themodel make good translation choices, not to ruleout potentially correct choices, we can change theway the information is used during decoding: weallow any rule to substitute into any site, but letthe model learn which substitutions are better thanothers.
To do this, we add the following features tothe model:1447...IP....VP..??????y?b?is?sh?q???m?iyu?n.IP/VP....PP?NP....NP..NN..
??sh?nch?..PP..?z?i?li?ng??n??m?oy??zh?ng..NP..NR..??T?iw?n...IP....IP\NP....VP..??????y?b?is?sh?q???m?iyu?n.PP?NP....NP..NN..
??sh?nch?..PP..?z?i?li?ng??n??m?oy??zh?ng..NP..NR..?
?T?iw?n(a) (b)Figure 4: Fuzzy tree-to-tree extraction effectively restructures the Chinese tree from Figure 2 in two waysbut does not commit to either one.?
match f counts  the  number  of  substitutionswhere the label of the source side of the sub-stitution  site  matches  the  root  label  of  thesource side of the rule, and ?match f countsthose where the labels do not match.?
subst fX?Y counts the number of substitutionswhere the label of the source side of the sub-stitution site is X and the root label of thesource side of the rule is Y.?
matche, ?matche, and substeX?Y do the samefor the target side.?
rootX,X?
counts  the  number  of  rules  whoseroot label on the source side is X and whoseroot label on the target side is X?.3For example, in the derivation of Figure 1, the fol-lowing features would fire:matchf = 1substfNP?NP = 1matche = 1substeNP?NP = 1rootNP,NP = 1The decoding algorithm then operates as in hier-archical phrase-based translation.
The decoder hasto store in each hypothesis the source and targetroot labels of the partial derivation, but these la-bels are used for calculating feature vectors onlyand not for checking well-formedness of deriva-tions.
This additional state does increase the searchspace of the decoder, but we did not change anypruning settings.3Thanks to Adam Pauls for suggesting this feature class.4 ExperimentsTo compare the methods described above with hi-erarchical string-to-string translation, we ran ex-periments  on both Chinese-English and Arabic-English translation.4.1 SetupThe sizes of the parallel texts used are shown in Ta-ble 2.
We word-aligned the Chinese-English par-allel  text using GIZA++ followed by link dele-tion (Fossum et al, 2008), and the Arabic-Englishparallel text using a combination of GIZA++ andLEAF (Fraser and Marcu, 2007).
We parsed thesource sides of both parallel texts using the Berke-ley parser (Petrov et al, 2006), trained on the Chi-nese Treebank 6 and Arabic Treebank parts 1?3,and the English sides using a reimplementation ofthe Collins parser (Collins, 1997).For string-to-string extraction, we used the sameconstraints as in previous work (Chiang, 2007),with differences shown in Table 2.
Rules with non-terminals were extracted from a subset of the data(labeled ?Core?
in Table 2), and rules without non-terminals were extracted from the full parallel text.Fuzzy tree-to-tree extraction was performed usinganalogous constraints.
For exact tree-to-tree ex-traction, we used simpler settings: no limit on ini-tial phrase size or unaligned words, and a maxi-mum of 7 frontier nodes on the source side.All systems used the glue rule (Chiang, 2005),which allows the decoder, working bottom-up, tostop  building  hierarchical  structure  and  insteadconcatenate  partial  translations  without  any  re-ordering.
The model attaches a weight to the gluerule so that it can learn from data whether to buildshallow or rich structures, but for efficiency?s sakethe decoder has a hard limit, called the distortion1448Chi-Eng Ara-EngCore training words 32+38M 28+34Minitial phrase size 10 15final rule size 6 6nonterminals 2 2loose source 0 ?loose target 0 2Full training words 240+260M 190+220Mfinal rule size 6 6nonterminals 0 0loose source ?
?loose target 1 2Table 2: Rule extraction settings used for exper-iments.
?Loose  source/target?
is  the  maximumnumber of  unaligned source/target  words at  theendpoints of a phrase.limit, above which the glue rule must be used.We trained two 5-gram language models: oneon the combined English halves of the bitexts, andone on two billion words of English.
These weresmoothed using modified Kneser-Ney (Chen andGoodman, 1998) and stored using randomized datastructures similar  to those of Talbot and Brants(2008).The base feature set for all systems was similarto the expanded set recently used for Hiero (Chianget al, 2009), but with bigram features (source andtarget word) instead of trigram features (source andtarget word and neighboring source word).
For allsystems but the baselines, the features describedin Section 3 were added.
The systems were trainedusing MIRA (Crammer and Singer, 2003; Chianget al, 2009) on a tuning set of about 3000 sentencesof newswire from NIST MT evaluation data andGALE development data, disjoint from the train-ing data.
We optimized feature weights on 90% ofthis and held out the other 10% to determine whento stop.4.2 ResultsTable 3 shows the scores on our development setsand  test  sets, which  are  about  3000  and  2000sentences, respectively, of newswire drawn fromNISTMT evaluation data and GALE developmentdata and disjoint from the tuning data.For Chinese, we first tried increasing the distor-tion limit from 10 words to 20.
This limit controlshow deeply nested the tree structures built by thedecoder are, and we want to see whether addingsyntactic information leads to more complex struc-tures.
This change by itself led to an increase inthe BLEU score.
We then compared against twosystems using  tree-to-tree  grammars.
Using  ex-act tree-to-tree extraction, we got a much smallergrammar, but decreased accuracy on all  but theChinese-English test set, where there was no sig-nificant change.
But with fuzzy tree-to-tree extrac-tion, we obtained an improvement of +0.6 on bothChinese-English sets, and +0.7/+0.8 on the Arabic-English sets.Applying the heuristic for nesting phrases re-duced the grammar sizes dramatically (by a factorof 2.4 for Chinese and 4.2 for Arabic) but, interest-ingly, had almost no effect on translation quality: aslight decrease in BLEU on the Arabic-English de-velopment set and no significant difference on theother sets.
This suggests that the strength of fuzzytree-to-tree extraction lies in its ability to break upflat structures and to reconcile the source and targettrees with each other, rather than multiple restruc-turings of the training trees.4.3 Rule usageWe then  took  a  closer  look  at  the  behavior  ofthe  string-to-string  and fuzzy tree-to-tree  gram-mars (without the nesting heuristic).
Because therules of these grammars are in one-to-one corre-spondence, we can analyze the string-to-string sys-tem?s derivations as though they had syntactic cat-egories.
First, Table 4 shows that the system usingthe tree-to-tree grammar used the glue rule muchless and performed more matching substitutions.That is, in order to minimize errors on the tuningset, the model learned to build syntactically richerand more well-formed derivations.Tables 5 and 6 show how the new syntax fea-tures affected particular substitutions.
In generalwe see a shift  towards more matching substitu-tions; correct placement of punctuation is particu-larly emphasized.
Several changes appear to haveto  do  with  definiteness  of  NPs: on the  Englishside, adding the syntax features encourages match-ing substitutions of type DT \NP-C (anarthrousNP),  but  discourages  DT \NP-C and  NN fromsubstituting  into  NP-C and  vice  versa.
For  ex-ample, a translation with the rewriting NP-C ?DT \NP-C begins  with  ?24th  meeting  of  theStanding Committee.
.
.,?
but the system using thefuzzy tree-to-tree grammar changes this to ?The24th meeting of the Standing Committee.
.
.
.
?The root features had a less noticeable effect on1449BLEUtask extraction dist.
lim.
rules features dev testChi-Eng string-to-string 10 440M 1k 32.7 23.4string-to-string 20 440M 1k 33.3 23.7]tree-to-tree exact 20 50M 5k 32.8 23.9tree-to-tree fuzzy 20 440M 160k 33.9]24.3]+ nesting 20 180M 79k 33.9 24.3Ara-Eng string-to-string 10 790M 1k 48.7 48.9tree-to-tree exact 10 38M 5k 46.6 47.5tree-to-tree fuzzy 10 790M 130k 49.4 49.7]+ nesting 10 190M 66k 49.2 49.8Table 3: On both the Chinese-English and Arabic-English translation tasks, fuzzy tree-to-tree extractionoutperforms exact tree-to-tree extraction and string-to-string extraction.
Brackets indicate statisticallyinsignificant differences (p ?
0.05).rule choice; one interesting change was that the fre-quency of rules with Chinese root VP / IP and En-glish root VP / S-C increased from 0.2% to 0.7%:apparently the model learned that it is good to userules that pair Chinese and English verbs that sub-categorize for sentential complements.5 ConclusionThough exact tree-to-tree translation tends to ham-per translation quality by imposing too many con-straints during both grammar extraction and de-coding, we have shown that using both source andtarget syntax improves translation accuracy whenthe model is given the opportunity to learn fromdata how strongly to apply syntactic constraints.Indeed, we have found that the model learns on itsown to choose syntactically richer and more well-formed structures, demonstrating that source- andtarget-side syntax can be used together profitablyas long as they are not allowed to overconstrain thetranslation model.AcknowledgementsThanks to Steve DeNeefe, Adam Lopez, JonathanMay, Miles  Osborne, Adam  Pauls, RichardSchwartz, and the anonymous reviewers for theirvaluable help.
This research was supported in partby  DARPA contract  HR0011-06-C-0022  undersubcontract  to  BBN Technologies  and  DARPAcontract HR0011-09-1-0028.
S. D. G.frequency (%)task side kind s-to-s t-to-tChi-Eng source glue 25 18match 17 30mismatch 58 52target glue 25 18match 9 23mismatch 66 58Ara-Eng source glue 36 19match 17 34mismatch 48 47target glue 36 19match 11 29mismatch 53 52Table 4: Moving from string-to-string (s-to-s) ex-traction to fuzzy tree-to-tree (t-to-t) extraction de-creases glue rule usage and increases the frequencyof matching substitutions.1450frequency (%)kind s-to-s t-to-tNP ?
NP 16.0 20.7VP ?
VP 3.3 5.9NN ?
NP 3.1 1.3NP ?
VP 2.5 0.8NP ?
NN 2.0 1.4NP ?
entity 1.4 1.6NN ?
NN 1.1 1.0QP ?
entity 1.0 1.3VV ?
VP 1.0 0.7PU ?
NP 0.8 1.1VV ?
VP ?
PU 0.2 1.2PU ?
PU 0.1 3.8Table 5: Comparison of frequency of source-siderewrites  in  Chinese-English  translation  betweenstring-to-string (s-to-s) and fuzzy tree-to-tree (t-to-t) grammars.
All rewrites occurring more than 1%of the time in either system are shown.
The label?entity?
stands for handwritten rules for named en-tities and numbers.frequency (%)kind s-to-s t-to-tNP-C ?
NP-C 5.3 8.7NN ?
NN 1.7 3.0NP-C ?
entity 1.1 1.4DT \NP-C ?
DT \NP-C 1.1 2.6NN ?
NP-C 0.8 0.4NP-C ?
VP 0.8 1.1DT \NP-C ?
NP-C 0.8 0.5NP-C ?
DT \NP-C 0.6 0.4JJ ?
JJ 0.5 1.8NP-C ?
NN 0.5 0.3PP ?
PP 0.4 1.7VP-C ?
VP-C 0.4 1.2VP ?
VP 0.4 1.4IN ?
IN 0.1 1.8, ?
, 0.1 1.7Table 6: Comparison of frequency of target-siderewrites  in  Chinese-English  translation  betweenstring-to-string (s-to-s) and fuzzy tree-to-tree (t-to-t) grammars.
All rewrites occurring more than1% of the time in either system are shown, plus afew more of interest.
The label ?entity?
stands forhandwritten rules for named entities and numbers.ReferencesVamshi Ambati  and Alon Lavie.
2008.
Improvingsyntax driven translation models by re-structuringdivergent and non-isomorphic parse tree structures.In Proc.
AMTA-2008 Student Research Workshop,pages 235?244.Yehoshua  Bar-Hillel.
1953.
A quasi-arithmeticalnotation  for  syntactic  description.
Language,29(1):47?58.Stanley F. Chen and Joshua Goodman.
1998.
Anempirical  study  of  smoothing  techniques  for  lan-guage modeling.
Technical Report TR-10-98, Har-vard University Center for Research in ComputingTechnology.David Chiang, Wei Wang, and Kevin Knight.
2009.11,001 new features for statistical machine transla-tion.
In Proc.
NAACL HLT 2009, pages 218?226.David  Chiang.
2005.
A hierarchical  phrase-based model for statistical machine translation.
InProc.
ACL 2005, pages 263?270.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Michael Collins.
1997.
Three generative lexicalisedmodels for statistical parsing.
In Proc.
ACL-EACL,pages 16?23.Koby Crammer and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.Journal of Machine Learning Research, 3:951?991.Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In Proc.
ACL2003 Companion Volume, pages 205?208.Victoria  Fossum, Kevin  Knight, and  Steven  Abney.2008.
Using syntax  to  improve word alignmentfor syntax-based statistical machine translation.
InProc.
Third Workshop on Statistical Machine Trans-lation, pages 44?52.Alexander Fraser and Daniel Marcu.
2007.
Gettingthe structure right for word alignment: LEAF.
InProc.
EMNLP 2007, pages 51?60.Michel  Galley, Mark  Hopkins, Kevin  Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proc.
HLT-NAACL 2004, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve  DeNeefe, Wei  Wang, and  IgnacioThayer.
2006.
Scalable  inference  and  trainingof  context-rich  syntactic  translation  models.
InProc.
COLING-ACL 2006, pages 961?968.Mary Hearne and Andy Way.
2003.
Seeing the woodfor the trees: Data-Oriented Translation.
InProc.MTSummit IX, pages 165?172.1451Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of  locality.
In Proc.
AMTA 2006, pages65?73.Alon Lavie, Alok Parlikar, and Vamshi Ambati.
2008.Syntax-driven learning of sub-sentential translationequivalents and translation rules from parsed parallelcorpora.
In Proc.
SSST-2, pages 87?95.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
COLING-ACL 2006, pages609?616.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
Improv-ing tree-to-tree translation with packed forests.
InProc.
ACL 2009, pages 558?566.I.
Dan  Melamed, Giorgio  Satta, and  Ben  Welling-ton.
2004.
Generalized multitext grammars.
InProc.
ACL 2004, pages 661?668.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proc.
COLING-ACL2006, pages 433?440.Arjen Poutsma.
2000.
Data-Oriented Translation.
InProc.
COLING 2000, pages 635?641.David Talbot and Thorsten Brants.
2008.
Random-ized language models via perfect hash functions.
InProc.
ACL-08: HLT, pages 505?513.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2009.
Preference grammars:Softening syntactic constraints to improve statisti-cal machine translation.
In Proc.
NAACL HLT 2009,pages 236?244.David J. Weir.
1988.
Characterizing Mildly Context-Sensitive Grammar Formalisms.
Ph.D. thesis, Uni-versity of Pennsylvania.Benjamin Wellington, Sonjia Waxmonsky, and I. DanMelamed.
2006.
Empirical  lower  bounds  onthe  complexity  of  translational  equivalence.
InProc.
COLING-ACL 2006, pages 977?984.Min  Zhang, Hongfei  Jiang, Aiti  Aw, Haizhou  Li,ChewLim Tan, and Sheng Li.
2008.
A tree sequencealignment-based tree-to-tree translation model.
InProc.
ACL-08: HLT, pages 559?567.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart parsing.In Proc.
Workshop on Statistical Machine Transla-tion, pages 138?141.1452
