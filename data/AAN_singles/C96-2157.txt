A Self-Learning Universal Concept SpotterTomek Strzalkowski and J in Wang(\]E Cort)or~*l;(', Resear(:h and Dev(',lopmentP.O.
Box 8Schealect, a(ly, NY 12301USA{strzalkowski, wangj }@crd.
ge.
comAbst ractWe describe the Universal Spotter, asystem for identifying in-text referencesto entities of an arbitrary, user-sl)ecitiedtype, such its people, organizations,equipment, products, materials, etc.Starting with some initial seed examples,and a training text eortms , I;he systemgenerates rules that will find fllrther con-cepts of the stone type.
The initial se, edinformation is t)rovided by the user inthe form of a typical lexical context inwhich the enl, ities to be spotted occur,e.g., "the name ends with Co.", or %othe right of produced or made", and soforth, or by simt)ly supplying examplesof the concept itself, e.g., Ford Tau'r'as,gas turbine, Bi 9 Mac.
In addition, nega-tive exalnples can t)e supplied, if known.Given a suf\[ieiently arge training corpus,an unsupervise(t learning process is ini-tiated in which the system will: (1) tindiilstanees of the sought-after concept us-ing the seed-eolltext inforInation whilemaxiinizing recall and precision; (2) find,~dditional contexts in which these en-tities occur; and (3) expand the initialseed-context with selected new com;extst;o find even lllOre entities.
Preliminaryresults of creating spotters for organiza-tions and products are discussed.1 In t roduct ionhlentifying concepts in natural language text isan important intbrmation extraction task.
De-pending upon the current information needs onemay be interested in finding all references to peo-ple, locations, dates, organizations, companies,products, equipment, and so on.
These concepts,along with their classification, can be used to in-dex any given text for search or categorizationpurposes, to generate suimnaries, or to popu-late database records.
However, automating theprocess of concept identification in untbrmattedtext has not been an easy task.
Various single-Imrpose spotters have been developed for specifictypes of conce.pts, including people mm~es, com'-pa.ny n&ines, location names, dates, etc.
})lit; thosewere usually either hand crafted for particularapplications or domains, or were heavily relyingon apriori lexical clues, such as keywords (e.g.,'Co.
'), case (e.g., ' John K. Big'), predicatable for-mat; (e.g., 123 Maple Street), or a combinationof thereof.
This makes treat, ion and extensionof stleh spotters an arduous mamml job.
Other,less s;tlient entities, such as products, equipnmilt,foodstuff', or generic refcrenc.es of any kind (e.g.,'a ,lapanese automaker') could only be i(lenti-fled if a sut\[iciently detailed domain model wasavailable.
Domain-model driven extraction witsused in ARPA-sponsored Message UnderstandingColltc1'eilc(!s (MUC); a detailed overview of cur-rent research can be found in the procecdil~gs ot7MUC-5 (nmcS, 1993) and the recently concludedMUC-6, as well as Tipster Project meetings, orARPA's Human Language q>chnology workshops(tipsterl, 1993), (hltw, 1994).We take a somewh~t different approach to iden-tify various types of text entities, both generic andspecific, without a (let, ailed underst, anding of thetext domain, and relying instead on a comlfinationof shallow linguistic processing (to identi(y candi-date lexical entities), statistical knowledge acqui-sition, unsupervised learning techniques, and t)os-sibly broa(1 (mfiversal but often shallow) knowl-edge, sources, such as on-line dictionaries (e.g.,WordNet, Comlex, ()ALl), etc.).
Our methodIllOVeS t)eytmd the traditional name si)otters andtowards a universal spotter where, the require-ments on what to spot can be specified as in-put paraineters, and a specific-purpose spotterc.ouht be generated automatically.
In this pa-per, we describe a method of creating spotters forentities of a specified category given only initialseed examples, and using an unsupervised learn-ing t)rocess to discover rules for finding more in-stances of the eoncet)t. At this time we placeno limit on what kind of things one may wantto build a spotter for, al@lough our extmrimentsthus far concentrated on entities customarily re-931ferred to with noun phrases, e.g., equipment (e.g.,"gas turbine assembly"), tools (e.g., "adjustablewrench"), products (e.g., "canned soup", "Arm& I lammer baking soda"), orgmfizations (e.g.,American Medical Association), locations (e.g.,Albany County Airport), people (e.g., Bill Clin-ton), and so on.
We view the semantic cate-gorization problem as a case of disambiguation,where for each lexical entity considered (words,phrases, N-grams), a binary decision has to bemade whether or not it is an instance of the se-mantic type we are interested in.
The problem ofsemantic tagging is thus reduced to the problem ofpartitioning the space of lexical entities into thosethat are used in the desired sense, and those thatare not.
We should note here that it is acceptablefor homonym entities to have different classifica-tion depending upon the context in which they areused.
Just as the word "bank" can be assigned if-ferent senses in different contexts, so can "Boeing777 jet" be once a product, and another time anequipment and not a product, depending upon thecontext.
Other entities may be less context depen-dent (e.g., company nan'ms) if their definitions arebased on internal context (e.g., "ends with Co.")as opposed to external context (e.g., "followed bymauufactures"), or if they lack negative contexts.The user provides the initial information (seed)about what kind of things he wishes to identifyin text.
This infortnation should be in a form ofa typical lexical context in which tile entities tobe spotted occur, e.g., "the name ends with Co.",or "to the right of produced or made", or "to theright of maker of', and so forth, or simply by list-ing or highlighting a number of examples in text.In addition, negative examples can be given, ifknown, to eliminate certain 'obvious' exceptions,e.g., "not to the right of made foal', "not tooth-brushes".
Given a sufficiently large training cor-pus, an unsupervised learning process is initiatedin which the system will: (1) generate initial con-text rules from the seed examples; (2) find furtherinstances of tile sought-after concept using the ini-tial context while maximizing recall and precision;(3) find additional contexts in which these entitiesoccur; and (4) expand the current context rulesbased on selected new contexts to find even moreentities.In the rest of tlle paper we discuss the specifiesof our system.
We present and evaluate prelimi-nary results of creating spotters for organizationsand products.2 What  do you  want  to f ind: seedse lec t ionIf we want to identify some things in a streamof text, we first need to learn how to distinguishthem from other items.
For example, companynames are usually capitalized and often end with'Co.
', 'Corp.
', ' Inc.' and so forth.
Place names,such as cities, are nonmflly capitalized, sometimesare followed by a state abbreviation (as in Albauy,NY), and may be preceded by locative preposi-tions (e.g., in, at, from, to).
Products may haveno distinctive lexical appearance, but they tend tobe associated with verbs such as 'produce', 'man-ufacture', 'make', 'sell', etc., which in turn mayinvolve a company name.
Other concepl;s, such asequipment or materials, have R~'w if any ot)viousassociati(ms with the surrounding text, and on(;may prefer just to iioint them out directly to thelearning prograin.
There are texts, e.g., techni-cal manuals, where such specialized entities occurmore often than elsewhere, and it may be adwm-tagous to use these texts to derive spotters.The seed can be obtained either by hand tag-ging some text or using a naive spotter that hashigh precision but presumably low recall.
A naivespotter may contain simple contextual rules suchas those mentioned above, e.g., for organizations:a noun phrases ending with "Co." or "Inc."; forproducts: a noun phrase following "manufacturerof", "producer of", or "retailer of".
When suchnaive spotter is ditlicult to come by, one may re-sort to hand tagging.3 F rom seeds  to  spot tersThe seed should identit~y the sought-after enti-ties with a high precision (thougil not; necessarily100%), however its recall is assumed to be low, orelse we would already have a good spotter.
Ourtask is now to iucrease tile recall while maintain-ing (or ('.veil increase if possible) the precision.We proceed by examining the lexical context inwhich tlle seed entities occur.
In the silnplest in-stance of this process we consider a context o coil-sist of N words to the left of the seed and N wordsto the right of tile seed, as well as the words ill theseed itself.
Each piece of significant contextual ev-idence is then weighted against its distribution inthe balance of the training corpus.
This in turnleads to selection of some contexts to serve as in-dicators of relevant entities, in other words, theybecome the initial rules of the emerging spotter.As an exami)le, let's consider building a spotterfor company names, starting with seeds as illus-trated in the tbllowing fragments (with seed con-t, exts highlighted):... HENRY KAUFMAN is presidentof Henry Kaufmau C~ Co., a ... Gabelli,chairman of Gabelli l%nds Inc.; ClaudeN.
Rosenberg ... is named president ofSlmndinaviska Enskilda Banken ... be-come viee chairman of the state-ownedelectronics giant Thomson S.A .... bank-ing group, said the formal merger ofSl~anska Banken into ... water makerSource Perrier S.A., according to Frenchstock ...932l taving "Co." "htc."
to pick out "Henry Kaufmmn & Co." rand "Gabelli IAmds Inc." as seeds,we proceed to find new evidence in the trainingcorlms , using an unsul)ervised lemrning process,mnd discover thmt "chmirman of" rand "t)residcntof" rare very likely to precede, cOral)any nalnes.
Weexpand our initial set of rules, which tallows us tospot more COml)anies:... ltENI{Y KAUFMAN is pres -ident  o f  lh;nry Kaufm.an ~'4 Co.,  a... Gabclli, cha i rman o f  Gabclli \[,}mdsInc.
;  Clmude N. \]{osenl)erg ... is nmmedpres ident  o f  Skandi'naviska EnskildaBankcn ... be, come vice ( 'hairntan o flhe.
state-o'wncd electronics giant Thom-son S.A .
.
.
.
banldng groul) , said dw, for-real merger of Skansl~ l{anken into ...winter inaker Sotnce Perrier S.A., accord-ing to French stock ...This evidence discovery (:an be relmated in mbool;strmpl)ing process l)y ret)la(:ing the initiml set;of seeds with the new set; of entities obtained frolnthe lmst itermtion.
In t|~e mbove examt)le, we nowhave.
"Slamdinaviskm Fmskihla Bank(m" and "l;hcstmte-owned electronics giant '\]'homson S.A." inmddition to the initiml two names.
A flu'ther it(w-ation ma,y mdd "S.A." rand "Bmnken" {;o l;hc set ofcontcxtuml rules, and so forth, in generml, (ml;itiescan 1)e both added mnd deh;ted from the evolvings(;t of examples, det)ending on how uxmctly the cv-id(;n(:e is weighted and combin(;d. The details areexl)lained in the following sections.4 Text  p reparat ionIn ill()S~, (;asc, s l ;he text needs to t)e preprocessed toisolmte 1)asic lexi(:al tok(',ns (words, ml)l)r(!viations,symbols, mnnol;a|;ions, el;(:), and sl;ru(:turml units(sections, pmragrat)hs , entences) wh(mever api)li-cmt)le.
In addition, t)mrt-of-speech tmgging ix usu-ml\]y desirmble, in which case tim tagger mmy needl;o be re-trained on a text saml)le 1;o ol)l;ilnize itsperformance (Brill, 1993), (Mercer, Schwartz &W(;ischedcl, 1{)91).
Finmlly, a limited amount oflexicml normalization, or stemming, Inay be f)er-lormed.The entities we rare looking for inay be exl)ressed|)y certain tyt)es of phrases.
For example, peo-ple nmmes m'e usually sequences of i)rot)er nouns,while equipment nmmes rare contained within nounphrmses, e.g., 'forwmrd looking int>m'ed radar'.
Weuse 1)art of speech information to delinemte thosese(lllelt(;es of lexicml l;okens t;hat arc likely to (:on-t;mill (Olll "~ enl;itics.
\]~'l'()in l;h(',ll Oil we restrict tonyfurther t)rocessing on these sequences, and theircontexts.These preparatory steps are desirable since theyreduce the amount of noise through which thelemrning process needs to plow, but they mre not,strictly st)eaking, ne(:essary.
Further experimentsrare required to deterlnint~ the level of preprocess-ing required I;o optinfize the t)erforlnanee of the\[hfiversal Sl)otl;er.5 Ev idence  i temsThe smnmnl;i(: categorization problem describedhere displmys ome pmrmllcls to the word sense disambigumdon problem where hoInonylll words ileedto be mssigned to one of several possible senses,(Yarowsky, 19!
)5), (Gale, Chm'ch & Yarowsky,lt)92), (Brown, Pictra, Pietra & Mercer, \]991).
'Fhcre mrc two itnportant difl'erenc(',s, however.First, in the semantic cat, cgorizal;ion l)ro|)lem,t, here is al; lemsl, one Olmn-ended catc, gory servingas m grml) 1)rag for roll things non-relevant;.
This c, mt-e, gory Inay be hard, if not impossible, to describeby any finit(; set of rules.
Second, unlike the wordsense disambigumtion where the it;eros 1;o be clmssi-tied arc known apriori, we attempt to acconqflishtwo things at the smnm time:1. discover l;he items Lo be (:onsidcred for c, mte-gorization;2. acl;ually decide if an item 1)elongs to a givencategory, or falls outside of il;.
'\]'hc mtcgorization of a lexical token its belong-ing l,o m p;ivell selnalltic, clmss is based llpOtt t,}l(':information provided by the words occurriug in1,he token itself, ms well as the words thmL l)re -cede mM follow it; in t(~xl;.
Ill addition, i)ositionmlrelal;ionshil)s among l;hes(; words mmy be of im-portaalce.
~lb capture l;his informal;ion, we definethe notion of an e.'videncc set lbr a lexicml unil;W,//V2...IA<,,.
(m phrase, or an N-gram) its follows.Let .
.
.
.
W.. ,  .... W .I W~.
.
.W, ,W,   W+.2 .
.
.W,  , .... be mstring of subsequellt, tokens (e.g., words) in text,such Lhat W~ W~....I/Km is a unit of interesl, (e.g.,a noun phrase) rand n is the maximum size of thecontext window on either side of the unit.
The mt:-\[;ual window size, mmy l)e limited by boundaries ofstrllcturml mfit, s sm;h its sentences or parmgraphs.For each unit W1 Wu...l/g,,~, a se~ of evidence, i lcmsis colh;cted as a set union of the following foursel;s:1.
Pmirs of (word, posit ion),  where posit ion{p,s,  f} indicates whethex word is fount\[ ill thecontext preceding (p) the central refit, following(t) it,, or whe|;her il; come, s f lom I;he centra.1 unil;itself is).
El =(w  ..... p) ...... (w.~,,p) (w_ , ,p )  \](w~,,~), (w,~, ~) ...... (w,,~, ~)(Wtt , f ) ,  (Wv2,f) ...... (W4,~,f)2.
Pairs of (bi-gram, position) to capture wordse, quence informmtion.
E2 ={(W ..... W--(,~- l)), p) ... ((1/V._:~, W__ t), p) }((w,, w~), .~) ... ((w,,~ _,, w,,& ~)((w+l, w+~),f) ... ((w+/,~_l), w+,o, f)9333.
3-tuples (word, position, distance), wheredistance indicates how far word is located rela-tive to W1 or I/V,~.
Ea ={4.
{(W .... p, n)(Wl, s, m)(W+I, f, 1)... (W_l ,p,  1) }... (w , , ,  ~, 1)... (W+,~,f,n)3-tuples (hi-gram, position, distancc).E4 =((W .... W (n_D) ,p ,n -1) .
.
.
(W-2 ,  W-t ) ,p ,  1) \]((Wl, W2), s, 7D, - 1) ...... ((W .... 1, Win), s, 1)((w+l, w+~), f, 0...((w+(,~_ ~), w+~), f, n - 1)For example, ill the fl'agment below, tile centralphrase the door has the context window of size 2:... boys kicked the door with rage ...The set of evidence items generated for this fl'ag-inent, i.e., E1 UE2 UEaUE4, contains the followingelements:(boys, p), (kicked, p), (the, s),(door, s), (with, f), (rage ,f),((boys, kicked), p), ((the, door)), s),((with, 'age), f), (boys, p, 2),(ki&ed, p, 1), (the, s, 2), (door, s, 1),(with, f, 1), (rage, f, 2),((boys, kicked), p, 1), ((the, door)), s, 1),((with, ,'age), f, 1)Items in evidence sets are assigned significanceweights (SW) to indicate how strongly they pointtowards or against the hyphothesis that the cen-tral unit belongs to the semantic ategory of in-terest to the spotter.
The significance weights areacquired through corpus-based training.6 TrainingEvidence items for all candidate phrases in thetraining corpus, for those selected by tile initialused-supplied seed, as well as for those added bya training iteration, are divided into two groups.Group A items are collected from the candidatephrases that are accepted by tile spotter; groupR items come from the candidate phrases that arerejected.
Note that A and 1% may contain repeatedelements.For each evidence item t, its significance weightis computed as:f(t,A)-f(t,R) f ( t ,A)  + f(t ,R)  > s f(t,A)+y(t,R)SW (t) = 0 otherwise(~)where f(t, X) is the fl'equency of t in group X,and s is a constant used to filter the noise of verylow frequency items.As defined SW(t) takes values from -1 to 1interval.
SW(t) close to 1.0 means that t ap-pears imarly exclusively with the candidates thathave been accepted by tile spotter, and thus pro-vides the strongest positive evidence.
Conversely,SW(t) close to -1.0 means that t is a strong neg-ative indicator since it occurs nearly always withthe rejected candidates.
SW(t) close to 0 indi-cates neutral evidence, which is of little or noconsequeuce to the spotter.
In general, we takeSW(t) > e > 0 as a piece of positive evidence,and SW(t) < -e  as a piece of negative evidence,as provided by item t. Weights of evidence itemswithin an evidence set are then combined to arriveat the compound context weight which is used toaccept or reject candidate phrase.At this time, we make no claim as to whether(1) is an optimal fornmla for cah:ulating evidenceweights.
An alternative method we consideredwas to estimate certain conditional probabilities,similarly to the formula used in (Yarowsky, 1995):SW(t) log P(p C A/t) f(t, A)f(A) = ~ log (2)P(p C R/t) f(t, .R)f(.l~)Here f(A) is (an estimate of) the probabilitythat any given candidate phrase will be acceptedby the spotter, and f(R) is the probability thatthis phrase is rejected, i.e., f(R) = l - f  (A).
Thusfin' our experinmnts show that (1) produces betterresults than (2).
We continue investigating otherweighting schemes as well.7 Combining evidence weights toclassify phrasesIn order to classify a candidate phrase, all ev-idence items need to be collected from its coil-text and their SW weights are combined.
Whenthe combined weight exceeds a threshold value,the candidate is accepted and the i)hrase becomesavailable for tagging by the spotter.
Otherwise,the ('andidate is reje(:te(l, although it may  bereevaluated in a fllture iteration.There are many ways to combine evidenceweights.
In our experiments we tried the followingtwo options:x+y-xy  i f x>Oandy>Ox O y = x + y + xy i f x<Oandy<O (3)x + y otherwiseand(Dy~ ~ x i fabs(x) > abs(y)y otherwise (4) kIn (3), x (I) y is greater than either x or y whenboth x and y are positive, and it is less than bothx and y for negative x and y.
In all cases, x 0) yremains within \[-1, +1\] interval.In (4) only the dominating evidence is consid-ered.
This formula is more noise resistant han(3), but produces generally less recall.9348 Boots t rapp ingThe eviden{:e, training and candidate sele{:tion (:y-cle forms a l)ootstrapI}ing t}rocess, as folh)ws:Procedure  BootstrappingCollect seedsl oopTraining l)haseTagging t)haseunt i l  Satisfied.The bootstrapping t)rocess allows fin' colle{:t-ing more and new ('.oni;exl, ual eviden{:e and in-crease recall of the spotter.
This is possible thanksto overall redundancy and rep(;titiveness of infor-mation, part icularly local {:ontext information, inlarge bodies of text.
For exanq}le,, in our three,-sectional contexl, ret)resent, ation (t}re(:eding, self,following), if one section contains strong evidencethat the candidate t)hrase is selectat}le, eviden(:ef(mnd in other se,{:tions will t}e considere, d in tilenext training cy{:le, in order to sele(:t additionalcandidates.An imi}ortmlt consideration he, re is to main-lain all overall precision level throughout he ell-tire process.
AMmugh, it; may t)e possible torec(}ver fl'om some miselassiti{:ation errors (e.g.,(Ym'owsky, 1995)), (:a.re shouhl 1)e taken when ad-justing the process l}arameters o that 1)r{;eisiondoes not deteriorate too rapidly.
For insl;ance, a(:-(;el}tan(;e thresholds of evide, nce weights, initiallyset, higll, can be gradually decreased to allow morerecall while keeping l}recision at a reasonable level.In additioil, (Yarowsky, 1995), (Gale, Church &;Yarowsky, 1992) point ou{; that there is a st, renttenden(:y for words 1;O occur in (}Ile sense withinany given dis{:ourse ("one sense pe, r dis{:ourse").Th(; same seems to at)ply to (:oncel)t sele(:l;ion,thai, is, Inultil}le o(:(:m'ren(:es of a (:an{lidate 1}hrasewithin ~t disc{}urse should all 1}e eithe\]' a(:eel)te{l orreje,(:t;(;{t \[)y the Sl}Ol,te\]'.
This in turn allows f{}rt}ootstrat}t)ing pr(}cess to gather more contextualevideal{:c more quickly, and thus to (:onwuge fastert)rodu{:ing, better results.9 Exper iments  and  Resu l tsWe, used the Universal St)ot;ter to find organiza-tions an{1 products in a 7 MBytes cortms consist-ing of al'ti(:les fl'om i;ll(', Wall Street Journal.
l,'irst,we l}re-t)rocess{~d the l;ext with a l}arl;-of-sl}{~echtagger and |dent|tied all simple noun groups tol)e used as {:and|date 1}hrases.
10 artMes wereset, aside and ha.rid l,agged as key for evalual;ion.Subsequently, seeds were construct, ed ma.nuallyin forln of contextual rule, s. l~i)r orgmfizati{}ns,these |nit|a.1 rules hall a 98% i)\]'e{;ision and 4{}%)recall; for products, the corresl}onding numberswere 97% and 42%}.
(4) is used to combine evi-dences.
No lexi{:on veriti{:ation (see later) has beenused in order l;o show m()re clearly the behaviorthe learning nmthod itself ( the l}erformance canprecision\] 0{}80604020u l l l l l l I l l l l l l l l~ l  .
."Seeds?
1st loo 1}?
4th loop20 40.......... " .......
i?"L.
.
.
.
.
.
.
.
.
.
.
.
.
.
,.,ilLi,ilLiii.
.
.
.
.
.
.
~ J~ recall60 80 100Figure 1: Organization spotter results.be enhanced by lexicon verification).
Also notethat  the quality of the ,~eeds affects the per'for-malice of the final sI)otl;er since they define whattype of {;()I1(',(;1)1; the system is supt)osed to lookfor.
The seeds that we used in our exlmrimenl;sare quit(; simple, perhaps too simple, l letter seedsmay be neede.d (possibly developed through all in-l;era?
'tion with the user) t;o obtain str(mg r{~stlltsfor some (:~l, cgories of concci}l;s.For orgmdzation tagging, the recall and preci-sion results obtained after the tirst mid the follrtht}ootstrat)t)ing eyt'.le are given in Figm'e 1.The poinl; with the inaximmn precision*recallin the ftmrth rllll is 950/{) pre(:ision and 90% re-call.
Examples of extracted organizations in-{:lude: "l,h,e State Statistical btstit, ntc, lst,,,t,","We.rl, heim Sch, roder #4 Co", "Skandi'naviska En-skilda Ha'nken", "Statistics Canada".The results for products tagging are given inFigure 2 on the next page.
Examph~s of ex-tracted products include: "the Mercury GrandMarquis and Ford Crown Victoria cars", "(_~tevro-let Prizm", "Pump shoe", 'MS/doe".The efl'ect of bootstrapping is clearly visible inboth charts: it improves the recall while, main-raining or even i inproving the pre,(:ision.
We mayalso nol;ice that some misclassifications due to alliml)ext'e,t:t seed (e.g., see the first dip in t)re(:ision()11 the 1}tOdllt;l;s chart) (:all ill t'aet t)e corrected infurther t}ootstrapping loops.
The generally lowerperformance levels for the product; spotl;er is prol)-ably due to the.
fact t;hat the (;oncel)t of produ(;t,is harder to eirt'.mnscril)e.10 Fur ther  opt ions10.1 Lex icon  ver i f i ca t ionThe itenlS identified in the second step can be fur-ther wflidated fl)r their broad semantic classifica-tion using on-line lexical (lat~J)asc8 such as Corn-935precision10080604020....... //~ ...... :::.....::" ::7:!
:..i :'!i ""'1'l.i !?
1st loop?
4th loop '" "~lJ20 40 60 80 100recallFigure 2: Product spotter esults.lex or Longman Dictionary, or Princeton's Word-Net; (Miller, 1990) For example, "gas turbine" isan acceptable quipment/machinery name since'turbine' is listed as "machine" or "device" inWordNet hierarchy.
More complex validation mayinvolve other words in the phrase (e.g., "circuitbreaker") or words in the immediate context.10.2 Con junct ionsThe current program cannot deal with conjunc-tion.
The difficulty with conjunction is not withclassification of the conjoined noun phrases (it iseasier, as a matter of fact, because they carry moreevidences) but with identification of the phrase it-self because of the structural ambiguities it typi-cally involves that cannot be dealt with easily onlexical or even syntactic level.11 Conc lus ionsIn this paper we presented the Universal Spotter,a system that learns to spot in-text references toinstances of a given semantic lass: people, organi-zations, products, equipment, ools, to nmne justa few.
A specific class spotter is created throughan unsupervised learning process on a text corpusgiven only an initial nser-supplied seed: either anumber of examples of the concept, or a typicalcontext in which they can be found.
The exper-iment shows that this method indeed can pro-duce useflfl spotters based on easy-to-constructseeds.
Tile results shown here are promising, canbe further improved by using lexicon verification.Different methods of computing SWs, combiningSWs, and parameter adjustmenting for the boot-strapping process need to be explored as we be-lieve there is still room for improvement.
Themethod is being continuously refined as we gainmore feedback from empirical tests across severaldifferent applications.We believe that tile Universal Spotter can re-place much of the need to create hand-craftedconcept spotters commonly used in text extrac-tion operations.
In can also be applied to build-ing other than the most common spotters suchas those for people names, place names, or com-pany names.
In fact, is can be used to createmore-or-less on-demand spotters, depending uponthe applications and its subject domain.
In par-ticular, we believe such spotters will be requiredto gain further advance in intelligent ext index-ing and retrieval applications, text summariza-tion, and database apI)lications, e.g., (Harman,1995), (Strzalkowski, 1995).Referenceshltw.
1994.
Proceedings of the Human Lan-guage Technology Workshop, Princeton.
SanFrancisco, CA:Morgan Kaufman Publishers.nine5.
1993.
Proceedings of 5th Message Under-standing Conference, Baltimore.
San Francisco,CA:Morgan Kaufman Publishers.tipsterl.
1993.
Tipster Text Phase 1.: 24 monthConference, Fredericksburg, Virginia.Brill, E. 1992.
A Simple Rule-based Part ofSpeech Tagger.
Proceedings of 3rd AppliedNatural Language Processing , San Francisco,CA:Morgan Kaufman Publishers.Brown,P, S. Pietra, V. Pietra and R. Mercer.1991.
Word Sense Disambiguation Using Statis-tical Methods.
Proceedings of the 29h AnnualMeeting of the Association for ComputationalLinguistics, pp.
264-270.Gale, W., K. Church and D. Yarowsky.
1992.
AMethod for l)isambiguating Word Senses in aLarge Corpus.
Computers and the Humanities,26, pp.
415 439.Harman, D. 1995.
Overview of the Third TextREtrieval Conference.
Overview of the l'hirdText REtrieval Conference (TREC-3), pp.1-20.Meteer, M., R. Schwartz, and I{.
Weischedel.1991.
Studies in Part of Speech Labeling.
Pro-ceedings of the ~th DARPA Speech and Natu-ral Language Workshop, Morgan-Kaufman, SanMateo, CA.
pp.
331-336.Miller, G. 1990.
WordNet: An ()n-line LexicalDatabase.
International Journal of Lexicogra-phy, 3, 4.Strzalkowski, T. 1995.
Natural Language Infor-mation Retrieval.
Information Processing andManagement, vol.
31, no.
3, pp.
397-417.Yarowsky, D. 1995.
Unsupervised Word SenseDisambiguation Rivaling Supervised Methods.Proceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics, pp.189-196.936
