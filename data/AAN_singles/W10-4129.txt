An Double Hidden HMM and an CRF for Segmentation Tasks withPinyin?s FinalsHuixing Jiang Zhe DongCenter for Intelligence Science and TechnologyBeijing University of Posts and TelecommunicationsBeijing, Chinajhx0129@163.com jimmybupt@gmail.comAbstractWe have participated in the open tracksand closed tracks on four corpora of Chi-nese word segmentation tasks in CIPS-SIGHAN-2010 Bake-offs.
In our experi-ments, we used the Chinese inner phonol-ogy information in all tracks.
For opentracks, we proposed a double hidden lay-ers?
HMM (DHHMM) in which Chineseinner phonology information was used asone hidden layer and the BIO tags as an-other hidden layer.
N-best results werefirstly generated by using DHHMM, thenthe best one was selected by using a newlexical statistic measure.
For close tracks,we used CRF model in which the Chineseinner phonology information was used asfeatures.1 IntroductionChinese language has many characteristics notpossessed by other languages.
One obvious isthat the written Chinese text does not have explicitword boundaries like western languages.
So wordsegmentation became very significative for Chi-nese information processing, and is usually con-sidered as the first step of any further processing.Identifying words has been a basic task for manyresearchers who have devoted themselves on Chi-nese text processing.The biggest characteristic of Chinese languageis its trinity of sound, form and meaning (Pan,2002).
Hanyu Pinyin is the form of sound forChinese text and the Chinese phonology informa-tion is explicit expressed by Pinyin which is theinner features of Chinese Characters.
And it nat-urally contributes to the identification of Out-Of-Vacabulary words (OOV).In our work, Chinese phonology information isused as basic features of Chinese characters in allmodels.
For open tracks, we propose a new dou-ble hidden layers HMM in which a new phonol-ogy information is built in as a hidden layer, anew lexical association is proposed to deal withthe OOV questions and domains?
adaptation ques-tions.
And for closed tracks, CRF model has beenused , combined with Chinese inner phonology in-formation.
We used the CRF++ package Version0.43 by Taku Kudo1.In the rest sections of this paper, we firstly in-troduce the Chinese phonology in Section 2.
Thenin the Section 3, the models used in our tasks arepresented.
And the experiments and results aredescribed in Section 4.
Finally, we give the con-clusions and make prospect on future work.2 Chinese PhonologyHanyu Pinyin is the form of sound for Chi-nese text and the Chinese phonology informa-tion is explicit expressed by Pinyin.
It is cur-rently the most commonly used romanization sys-tem for Standard Mandarin.
Hanyu means theChinese language, and Pinyin means ?phonetics?,or more literally, ?spelling sound?
or ?spelledsound?
(wikipedia, 2010).
The system has beenemployed to teach Mandarin as home languageor as second language by China, Malaysia, Sin-gapore et.al.
Pinyin has been the most Chinesecharacter?s input method for computers and otherdevices.1http://crfpp.sourceforge.net/The romanization system was developed by agovernment committee in the People?s Repub-lic of China, and approved by the Chinese gov-ernment on February 11, 1958.
The Interna-tional Organization for Standardization adoptedpinyin as the international standard in 1982, andsince then it has been adopted by many otherorganizations(wikipedia, 2010).
In this system,pinyin is composed by initials(pinyin: shengmu),finals(pinyin: yunmu) and tones(pinyin: sheng-diao) instead of consonants and vowels used inEuropean language.
For example, the Pinyin of???
is ?zhong1?
composed by ?zh?, ?ong?
and?1?.
In which ?zh?
is initial, ?ong?
is final and?1?
is the tone.Every language has its rhythm and rhyme, soChinese is no exception.
The rhythm system arethe driving force from the unconscious habit oflanguage(Edward, 1921).
And the Pinyin?s finalscontribute the Chinese rhythm system, Which isthe basic assumption our research based on.3 AlgorithmsGenerally the task of segmentation can be viewedas a sequence labeling problem.
We first define atag set as TS = {B, I, E, S}, shown in Table 1.Table 1: The tag set used in this paper.Label ExplanationB beginning character of a wordI inner character of a wordE end character of a wordS a single character as a wordFor the piece ???????????
ofthe example described in the experiments section,firstly, the TS tags are labeled to it.
And its re-sult is ??/S?/B?/E?/S?/B?/E?/B?/I?/E?.
Then the tags are combined sequentially toget the finally result ??
??
?
??
???
?.In this section, A novel HMM solution is pre-sented firstly for open tracks.
Then the CRF solu-tion for closed tracks is introduced.3.1 Double hidden layers?
HMMFor a given piece of Chinese sentence, X =x1x2 .
.
.
xT , where xi, i = 1, .
.
.
, T is a Chinesecharacter.
Suppose that we can give each Chinesecharacter xi a Pinyin?s final yi.
And suppose thelabel sequence of X is S = s1s2 .
.
.
sT , wheresi ?
TS is the tag of xi.
Then what we want tofind is an optimal tag sequence S?
which is de-fined in (1).S?
= argmaxSP (S, Y |X)= argmaxSP (X|S, Y )P (S, Y ) (1)The model is described in Fig.
1.
For a givenpiece of Chinese character strings, One hiddenlayer is label sequence S. Another hidden layer isPinyin?s finals sequence Y .
The observation layeris the given piece of Chinese characters X .Figure 1: Double Hidden Markov ModelFor transition probability, second-order Markovmodel is used to estimate probability of the doublehidden sequences as described in (2).P (S, Y ) =?tp(st, yt|st?1, yt?1) (2)For emission probability, we keep the first-order Markov assumption as shown in (5).P (X|S, Y ) =?tp(xt|st, yt) (3)3.1.1 Nbest resultsBased on the work of (Jiang, 2010), a word lat-tice is also built firstly, then in the second step, thebackward A?
algorithm is used to find the top Nresults instead of using the backward viterbi al-gorithm to find the top one.
The backward A?search algorithm is described as follow (Wang,2002; Och, 2001).3.1.2 Reranking with a new lexical statisticmeasureGiven two random Chinese charactersX and Yand assume that they appears in an aligned regionof the corpus.
The distribution of the two randomChinese characters could be depicted by a 2 by 2contingency table shown in Fig.
2(Chang, 2002).Figure 2: A 2 by 2 contingency tableIn Fig.
2, a is the counts ofX and Y co-occur; bis the counts of the cases thatX occurs but Y doesnot; c is the counts of the cases that X does notoccur but Y does; d is the counts of the cases thatboth X and Y do not occur.
The Log-likelihoodrate is calculated by (4).LLR(x, y) = 2(a ?
log a ?
N(a + b) ?
(a + c)+ b ?
log b ?
N(a + b) ?
(b + d)+ c ?
log c ?
N(c + d) ?
(a + c)+ d ?
log d ?
N(c + d) ?
(b + d)) (4)For the N-best result described in sec.
3.1.1,they can be re-ranked by (5).S?
= argminS(scoreh(S)+?KK?k=1LLR(xk, yk))(5)where scoreh is the negative log value ofP (S, Y |X).
K is the number of breaks in X andxk is the left Chinese character of the k breakand yk is the right Chinese character of the kbreak.
?
is the regulatory factor(in our experi-ments ?
= 0.45).Bigger value of LLR(xk, yk) means strongerability in combining of the two characters xk andyk, then they should not be segmented.3.2 CRF model for closed tracksConditional random field, as statistical sequencelabeling model, has been used widely in segmen-tation(Lafferty, 2001; Zhao, 2006).
In the closedtracks of the paper, we also use it.3.2.1 Feature templatesWe adopted two main kinds of features: n-gramfeatures and Pinyin?s finals features.
The n-gramfeature set is quite orthodox, they are, namely, C-2, C-1, C0, C1, C2, C-2C-1, C-1C0, C0C1, C1C2.The Pinyin?s finals feature set is the same as n-gram feature set.
They are described in Table.
2.Table 2: Feature templatesTemplates CategoryC-2, C-1, C0, C1, C2 N-gram: UnigramC-2C-1, C-1C0, C0C1, C1C2 N-gram: BigramP-2, P-1, P0, P1, P2 Phonetic: UnigramP-2P-1, P-1P0, P0P1, P1P2 Phonetic: Bigram4 Experiments and Results4.1 DatasetWe build a basic words dictionary for DHHMMand a Pinyin?s finals dictionary for both DHHMMand CRF from The Grammatical Knowledge-baseof Contemporary Chinese(Yu, 2001).
For the fi-nals dictionary, we give each Chinese character afinal extracted from its Pinyin.
When it comes toa polyphone, we just combine its all finals simplyto one.
For example, ??
{ong}?, ??
{a&ai&i}?.The training corpus (5,769 KB) we used is theLabeled Corpus provided by the organizer.
Wefirstly add the Pinyin?s finals to each Chinesecharacter of it, then we train the parameters ofDHHMM and CRF model on it.And the test corpus contains four domains: Lit-erature (A), Computer (B), Medicine (C) and Fi-nance(D).The LLR function?s parameters{a, b, c, d} arecounted from the current test corpus A, B, C, orD.
It?s means that for segmenting A, the LLR pa-rameters are counted from A, so the same for seg-menting B, C and D.4.2 PreprocessingThe date, time, numbers and symbols informationare easily identified by rules.
We propose fourregular expressions?
processes, in which the reg-ular expressions?
processes are handled one afteranother in order of date, time, numbers and sym-bols.
By now, a rough segmentation can be done.For a character stream, the date, time, numbersand symbols are firstly identified, then the wholestream can be divided by these units to somepieces of character strings which will be segmentby the models described in sec.
3.
For example,a character stream ?2009??8?31???????????12??????
will be dividedto ?2009?
?
8?
31?
?
?????????
12 ????
??.
Then the pieces ??
?, ??????????
?, ??????
will be seg-mented sequentially by the models described inSection 3.4.3 Results on DHHMMWe evaluate our system by Precision Rate(6), Re-call Rate(7), F1 measure(8) and OOV(Out-Of-Vocabulary) Recall rate(9).P = C(correct words in segmented result)C(words in segmented result)(6)R = C(correct words in segmented result)C(words in standard result)(7)F1 = 2 ?
P ?
RP + R(8)OR = C(correct OOV in segmented result)C(OOV in standard result)(9)In (6-9), C(?
?
?)
is the count of (?
?
?
).Table 3 are the results of the DHHMM on opentracks.In Table 3, OOV RR is the recall rate of OOV,IV RR is the recall rate of IV(In Vocabulary).4.4 Postprocessing for CRF and Results on ItSince the CRF segmenter will not always returna valid tag sequence that can be translated intosegmentation result, some corrections should bemade if such error occurs.
We devised a dynamicprogramming routine to tackle this problem: firstwe compute the valid tag sequence that closest toTable 3: Results of open tracks using DHHMM:Literature (A), Computer (B), Medicine (C) andFinance(D)A B C DR 0.893 0.918 0.917 0.928P 0.918 0.896 0.907 0.934F1 0.905 0.907 0.912 0.931OOV RR 0.803 0.771 0.704 0.808IV RR 0.899 0.945 0.943 0.939the output of CRF segmenter (by term closest, wemean least hamming distance), if there is a tie, wechoose the one has the least ?S?
tags, if the tie stillexists, we choose the one that comes lexicograph-ically earlier (B < I < E < S, described inTable.
1).
Table 4 are the results of the CRF onclosed tracks.Table 4: Results of closed tracks using CRF: Lit-erature (A), Computer (B), Medicine (C) and Fi-nance(D)A B C DR 0.945 0.946 0.94 0.956P 0.946 0.914 0.928 0.952F1 0.946 0.93 0.934 0.954OOV RR 0.816 0.808 0.761 0.849IV RR 0.954 0.971 0.962 0.966From the results of Table 3 and Table 4, wecan observe that the CRF model outperforms theDHHMM by average 2.72% in F1 measure.
In theother hand, from Table 5, we can see that the com-putation cost in DHHMM is less than half of thetime cost and lower one-fifth memory cost thanCRF model.Table 5: The computation cost in DHHMM andCRFTime cost(ms) Memory cost(MB)DHHMM 34398 16.3CRF 43415 355 Conclusions and Future worksThis paper has presented a double hidden lawyersHMM for Chinese word segmentation task inSIGHAN bakeoff 2010.
It firstly created N topresults and then select the best one from it by anew lexical association.Chinese phonology (specially by Pinyin?s finalin text) is very useful inner information of Chineselanguage, which is the first time used in our mod-els.
We have used it in both DHHMM and CRFmodel.In future work, there are lots of improvementscan be done.
Firstly, which polyphone?s finalsshould be used in a given context is a visible ques-tion.
And the strategy to train the parameter ?
de-scribed in 3.1.2 can also be improved.AcknowledgmentsThis research has been partially supported bythe National Science Foundation of China (NO.NSFC90920006).
We also thank Caixia Yuan forleading our discuss, Li Sun, Peng Zhang, YaojingChen, Zhixu Lin, Gan Lin, Guannan Fang for theiruseful helps in this work.ReferencesWenguo Pan.
2002. zibenwei yu hanyu yanjiu:120?141.
East China Normal University Press.Sapir Edward 1921.
Language: An introduction to thestudy of speech:230.
New York: Harcourt, Braceand company.wikipedia.
2010.
Pinyin.
http://en.wikipedia.org/wiki/Pinyin#cite note-6.Baobao Chang, Pernilla Danielsson, and WolfgangTeubert.
2002.
Extraction of translation unit fromchinese-english parallel corpora, Proceedings ofthe first SIGHAN workshop on Chinese languageprocessing:1?5.Huixing Jiang, Xiaojie Wang, Jilei Tian.
2010.Second-order HMM for Event Extraction from ShortMessage, 15th International Conference on Ap-plications of Natural Language to Information Sys-tems, Cardiff, Wales, UK.Franz Josef Och, Nicola Ueffing, Hermann Ney.
2001.An EfficientA?
Search Algorithm for Statistical Ma-chine Translation, Proceedings of the ACL Work-shop on Data-Driven methods in Machine Transla-tion 14(Toulouse, France): 1-8.Ye-Yi Wang, Alex Waibel.
2002.
Decoding Algo-rithm in Statistical Machine Translation, Proceed-ings of the 35th Annual Meeting of the Associationfor Computational Linguistics and Eighth Confer-ence of the European Chapter of the Association forComputational Linguistics: 366-372.Yu Shiwen, Zhu Xuefeng, Wang Hui.
2001.
NewProgress of the Grammatical Knowledge-base ofContemporary Chinese, ZHONGWEN XINXIXUEBAO, 2001Vol.
01.John Lafferty, A.Mccallum, F.Pereira.
2001.
Condi-tional Random Field: Probabilitic Models for Seg-menting and Labeling Sequence Data., Proceedingsof the Eighteenth International Conference on Ma-chine Learning: 282?289.Hai Zhao, Changning Huang, Mu Li.
2006.
AnImproved Chinese Word Segmentation System withConditional Random Field, Proceedings of the FifthSIGHAN Workshop on Chinese Language Process-ing (SIGHAN-5)(Sydney, Australia):162-165.
