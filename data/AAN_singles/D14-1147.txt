Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1405?1414,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsPredicting Chinese Abbreviations with Minimum Semantic Unit andGlobal ConstraintsLongkai Zhang Li Li Houfeng Wang Xu SunKey Laboratory of Computational Linguistics (Peking University)Ministry of Education, Chinazhlongk@qq.com, {li.l,wanghf,xusun}@pku.edu.cnAbstractWe propose a new Chinese abbreviationprediction method which can incorporaterich local information while generating theabbreviation globally.
Different to previ-ous character tagging methods, we intro-duce the minimum semantic unit, which ismore fine-grained than character but morecoarse-grained than word, to capture wordlevel information in the sequence labelingframework.
To solve the ?character dupli-cation?
problem in Chinese abbreviationprediction, we also use a substring taggingstrategy to generate local substring taggingcandidates.
We use an integer linear pro-gramming (ILP) formulation with variousconstraints to globally decode the final ab-breviation from the generated candidates.Experiments show that our method outper-forms the state-of-the-art systems, withoutusing any extra resource.1 IntroductionAbbreviation is defined as a shortened descriptionof the original fully expanded form.
For example,?NLP?
is the abbreviation for the correspondingfull form ?Natural Language Processing?.
The ex-istence of abbreviations makes it difficult to iden-tify the terms conveying the same concept in theinformation retrieval (IR) systems and machinetranslation (MT) systems.
Therefore, it is impor-tant to maintain a dictionary of the prevalent orig-inal full forms and the corresponding abbrevia-tions.Previous works on Chinese abbreviation gen-eration focus on the sequence labeling method,which give each character in the full form an extralabel to indicate whether it is kept in the abbre-viation.
One drawback of the character taggingstrategy is that Chinese characters only containlimited amount of information.
Using character-based method alone is not enough for Chinese ab-breviation generation.
Intuitively we can think of aword as the basic tagging unit to incorporate moreinformation.
However, if the basic tagging unitis word, we need to design lots of tags to repre-sent which characters are kept for each unit.
For aword with n characters, we should design at least2nlabels to cover all possible situations.
This re-duces the generalization ability of the proposedmodel.
Besides, the Chinese word segmentationerrors may also hurt the performance.
Thereforewe propose the idea of ?Minimum Semantic Unit?
(MSU) which is the minimum semantic unit inChinese language.
Some of the MSUs are words,while others are more fine-grained than words.The task of selecting representative characters inthe full form can be further broken down into se-lecting representative characters in the MSUs.
Wemodel this using the MSU-based tagging method,which can both utilize semantic information whilekeeping the tag set small.Meanwhile, the sequence labeling method per-forms badly when the ?character duplication?
phe-nomenon exists.
Many Chinese long phrases con-tain duplicated characters, which we refer to asthe ?character duplication?
phenomenon.
There isno sound criterion for the character tagging mod-els to decide which of the duplicated charactershould be kept in the abbreviation and which oneto be skipped.
An example is ??????????
(Beijing University of Aeronautics and Astro-nautics) whose abbreviation is ????.
The char-acter ???
appears twice in the full form and onlyone is kept in the abbreviation.
In these cases, wecan break the long phase into local substrings.
Wecan find the representative characters in the sub-strings instead of the long full form and let the de-coding phase to integrate useful information glob-ally.
We utilize this sub-string based approach andobtain this local tagging information by labeling1405on the sub-string of the full character sequence.Given the MSU-based and substring-basedmethods mentioned above, we can get a list ofpotential abbreviation candidates.
Some of thesecandidates may not agree on keeping or skippingof some specific characters.
To integrate their ad-vantages while considering the consistency, wefurther propose a global decoding strategy usingInteger Linear Programming(ILP).
The constraintsin ILP can naturally incorporate ?non-local?
infor-mation in contrast to probabilistic constraints thatare estimated from training examples.
We can alsouse linguistic constraints like ?adjacent identicalcharacters is not allowed?
to decode the correctabbreviation in examples like the previous ???
?example.Experiments show that our Chinese abbrevia-tion prediction system outperforms the state-of-the-art systems.
In order to reduce the size ofthe search space, we further propose pruning con-straints that are learnt from the training corpus.Experiment shows that the average number of con-straints is reduced by about 30%, while the top-1accuracy is not affected.The paper is structured as follows.
Section 1gives the introduction.
In section 2 we describeour method, including the MSUs, the substring-based tagging strategy and the ILP decoding pro-cess.
Experiments are described in section 3.
Wealso give a detailed analysis of the results in sec-tion 3.
In section 4 related works are introduced,and the paper is concluded in the last section.2 System Architecture2.1 Chinese Abbreviation PredictionChinese abbreviations are generated by selectingrepresentative characters from the full forms.
Forexample, the abbreviation of ??????
(PekingUniversity) is ????
which is generated by se-lecting the first and third characters, see TABLE1.
This can be tackled from the sequence labelingpoint of view.Full form ?
?
?
?Status Keep Skip Keep SkipResult ?
?Table 1: The abbreviation ????
of the full form??????
(Peking University)From TABLE 1 we can see that Chinese abbre-viation prediction is a problem of selecting repre-sentative characters from the original full form1.Based on this assumption, previous works mainlyfocus on this character tagging schema.
In thesemethods, the basic tagging unit is the Chinesecharacter.
Each character in the full form is la-beled as ?K?
or ?S?, where ?K?
means the currentcharacter should be kept in abbreviation and ?S?means the current character should be skipped.However, a Chinese character can only containlimited amount of information.
Using character-based method alone is not enough for Chineseabbreviation generation.
We introduce an MSU-based method, which models the process of se-lecting representative characters given local MSUinformation.2.2 MSU Based Tagging2.2.1 Minimum Semantic UnitBecause using the character-based method is notenough for Chinese abbreviation generation, wemay think of word as the basic tagging unit to in-corporate more information intuitively.
In English,the abbreviations (similar to acronyms) are usuallyformed by concatenating initial letters or parts of aseries of words.
In other words, English abbrevia-tion generation is based on words in the full form.However, in Chinese, word is not the most suit-able abbreviating unit.
Firstly, there is no naturalboundary between Chinese words.
Errors from theChinese word segmentation tools will accumulateto harm the performance of abbreviation predic-tion.
Second, it is hard to design a reasonable tagset when the length of a possible Chinese word isvery long.
The second column of TABLE 2 showsdifferent ways of selecting representative charac-ters of Chinese words with length 3.
For a Chi-nese compound word with 3 characters, there are 6possible ways to select characters.
In this case weshould have at least 6 kinds of tags to cover all pos-sible situations.
The case is even worse for wordswith more complicated structures.
A suitable ab-breviating unit should be smaller than word.We propose the ?Minimum Semantic Unit(MSU)?
as the basic tagging unit.
We define MSUas follows:1.
A word whose length is less or equal to 2 isan MSU.1A small portion of Chinese abbreviations are not gener-ated from the full form.
For example, the abbreviation of ????
(Shan Dong Province) is ???.
However, we can use alook-up table to get this kind of abbreviations.1406Full form SK Label MSUs???
(nursery) ?/K?/S?/S ??+????
(allowance) ?/S?/K?/S ??+????
(Credit card) ?/S?/S?/K ??+????
(Hydropower Station) ?/K?/K?/S ?+?+????
(Senate) ?/K?/S?/K ?+?+????
(Music group) ?/S?/K?/K ?
?+?Table 2: Representing characters of Chinese words with length 3 (K for keep and S for skip) and thecorresponding MSUs2.
A word whose length is larger than 2, butdoes not contain any MSUs with length equalto 2.
For example, ?????
(Railway Sta-tion) is not an MSU because the first twocharacters ????
(Train) can form an MSU.By this definition, all 6 strings in TABLE 2 areoften thought as a word, but they are not MSUsin our view.
Their corresponding MSU forms areshown in TABLE 2.We collect all the MSUs from the benchmarkdatasets provided by the second International Chi-nese Word Segmentation Bakeoff2.
We choose thePeking University (PKU) data because it is morefine-grained than all other corpora.
Suppose werepresent the segmented data as L (In our case Lis the PKU word segmentation data), the MSU se-lecting algorithm is shown in TABLE 3.For a given full form, we first segment it us-ing a standard word segmenter to get a coarse-grained segmentation result.
Here we use the Stan-ford Chinese Word Segmenter3.
Then we use theMSU set to segment each word using the strategyof ?Maximum Forward Matching?4to get the fine-grained MSU segmentation result.2.2.2 Labeling strategyFor MSU-based tagging, we use a labeling methodwhich uses four tags, ?KSFL?.
?K?
stands for?Keep the whole unit?, ?S?
stands for ?Skip thewhole unit?, ?F?
stands for ?keep the First charac-ter of the unit?, and Label ?L?
stands for ?keep theLast character of the unit?.
An example is shownin TABLE 4.The ?KSFL?
tag set is also applicable for MSUswhose length is greater than 2 (an example is ????/chocolate?).
By examining the corpus wefind that such MSUs are either kept of skipped in2http://www.sighan.org/bakeoff2005/3http://nlp.stanford.edu/software/segmenter.shtml4In Chinese, ?Forward?
means from left to right.?????????????
(The ab-breviation is ??????
)KSFL ?
?/K ?
?/F ?
?/S ?
?/S ?
?/F?/STable 4: The abbreviation ??????
of ???????????
(National Linguistics Work Com-mittee) based on MSU tagging.the final abbreviations.
Therefore, the labels ofthese long MSUs are either ?K?
or ?S?.
Empirically,this assumption holds for MSUs, but does not holdfor words5.2.2.3 Feature templatesThe feature templates we use are as follows.
SeeTABLE 5.1.
Word Xi(?2 ?
i ?
2)2.
POS tag of word Xi(?2 ?
i ?
2)3.
Word Bigrams (Xi, Xi+1) (?2 ?
i ?
1)4.
Type of word Xi(?2 ?
i ?
2)5.
Length of word Xi(?2 ?
i ?
2)Table 5: Feature templates for unit tagging.
Xrepresents the MSU sequence of the full form.
Xirepresents the ith MSU in the sequence.Templates 1, 2 and 3 express word uni-gramsand bi-grams.
In MSU-based tagging, we can uti-lize the POS information, which we get from theStanford Chinese POS Tagger6.
In template 4, thetype of word refers to whether it is a number, anEnglish word or a Chinese word.
Because the ba-sic tagging unit is MSU, which carries word infor-mation, we can use many features that are infeasi-ble in character-based tagging.5In table 2, all examples are partly kept.6http://nlp.stanford.edu/software/tagger.shtml1407Init:Let MSUSet = empty setFor each word w in L:If Length(w) ?
2Add w to MSUSetEnd ifEnd forFor each word w in L:If Length(w) > 2 and no word x in MSUSet is a substring of wAdd w to MSUSetEnd ifEnd forReturn MSUSetTable 3: Algorithm for collecting MSUs from the PKU corpus2.2.4 Sequence Labeling ModelThe MSU-based method gives each MSU an ex-tra indicative label.
Therefore any sequence label-ing model is appropriate for the method.
Previousworks showed that Conditional Random Fields(CRFs) can outperform other sequence labelingmodels like MEMMs in abbreviation generationtasks (Sun et al., 2009; Tsuruoka et al., 2005).
Forthis reason we choose CRFs model in our system.For a given full form?s MSU list, many can-didate abbreviations are generated by choosingthe k-best results of the CRFs.
We can use theforward-backward algorithm to calculate the prob-ability of a specified tagging result.
To reduce thesearching complexity in the ILP decoding process,we delete those candidate tagged sequences withlow probability.2.3 Substring Based TaggingAs mentioned in the introduction, the sequencelabeling method, no matter character-based orMSU-based, perform badly when the ?characterduplication?
phenomenon exists.
When the fullform contains duplicated characters, there is nosound criterion for the sequence tagging strategyto decide which of the duplicated character shouldbe kept in the abbreviation and which one to beskipped.
On the other hand, we can tag the sub-strings of the full form to find the local represen-tative characters in the substrings of the long fullform.
Therefore, we propose the sub-string basedapproach to given labeling results on sub-strings.These results can be integrated into a more accu-rate result using ILP constraints, which we will de-scribe in the next section.Another reason for using the sub-string basedmethods is that long full forms contain more char-acters and are much easier to make mistakes dur-ing the sequence labeling phase.
Zhang et al.
(2012) shows that if the full form contains lessthan 5 characters, a simple tagger can reach an ac-curacy of 70%.
Zhang et al.
(2012) also shows thatif the full form is longer than 10 characters, theaverage accuracy is less than 30%.
The numerouspotential candidates make it hard for the tagger tochoose the correct one.
For the long full forms,although the whole sequence is not correctly la-beled, we find that if we only consider its shortsubstrings, we may find the correct representativecharacters.
This information can be integrated intothe decoding model to adjust the final result.We use the MSU-based tagging method in thesub-string tagging.
The labeling strategy and fea-ture templates are the same to the MSU-based tag-ging method.
In practice, enumerating all sub-sequences of a given full form is infeasible if thefull form is very long.
For a given full form,we use the boundary MSUs to reduce the pos-sible sub-sequence set.
For example, ???????
(Chinese Academy of Science) has 5 sub-sequences: ???
?, ?????
?, ???
?, ?????
and ??
?.2.4 ILP Formulation of DecodingGiven the MSU-based and sub-sequence-basedmethods mentioned above as well as the preva-lent character-based methods, we can get a listof potential abbreviation candidates and abbrevi-ated substrings.
We should integrate their advan-tages while keeping the consistency between each1408candidate.
Therefore we further propose a globaldecoding strategy using Integer Linear Program-ming(ILP).
The constraints in ILP can naturallyincorporate ?non-local?
information in contrast toprobabilistic constraints that are estimated fromtraining examples.
We can also use linguistic con-straints like ?adjacent identical characters is notallowed?
to decode the correct abbreviation in ex-amples like the ????
example in section 1.Formally, given the character sequence of thefull form c = c1...cl, we keep Q top-rankedMSU-based tagging results T=(T1, ..., TQ) and Mtagged substrings S=(S1, ..., SM) using the meth-ods described in previous sections.
We alsouse N top-ranked character-based tagging resultsR=(R1, ..., RN) based on the previous character-based works.
We also define the setU = S?R?Tas the union of all candidate sequences.
Our goalis to find an optimal binary variable vector solution~v = ~x~y~z = (x1, ..., xM, y1, ..., yN, z1, ..., zQ) thatmaximizes the object function:?1M?i=1score(Si) ?
xi+ ?2N?i=1score(Ri) ?
yi+?3Q?i=1score(Ti) ?
zisubject to constrains in TABLE 6.
The parame-ters ?1, ?2, ?3controls the preference of the threeparts, and can be decided using cross-validation.Constraint 1 indicates that xi, yi, ziare allboolean variables.
They are used as indicator vari-ables to show whether the corresponding taggedsequence is in accordance with the final result.Constraint 2 is used to guarantee that at mostone candidate from the character-based tagging ispreserved.
We relax the constraint to allow thesum to be zero in case that none of the top-rankedcandidate is suitable to be the final result.
If thesum equals zero, then the sub-sequence based tag-ging method will generate a more suitable result.Constrain 3 has the same utility for the MSU-based tagging.Constraint 4, 5, 6 are inter-method constraints.We use them to guarantee that the labels of thepreserved sequences of different tagging methodsdo not conflict with each other.
Constraint 7 isused to guarantee that the labels of the preservedsub-strings do not conflict with each other.Constraint 8 is used to solve the ?character du-plicate?
problem.
When two identical charactersare kept adjacently, only one of them will be kept.Which one will be kept depends on the global de-coding score.
This is the advantage of ILP againsttraditional sequence labeling methods.2.5 Pruning ConstraintsThe efficiency of solving the ILP decoding prob-lem depends on the number of candidate taggingsequences N and Q, as well as the number of sub-sequences M. Usually, N and Q is less than 10 inour experiment.
Therefore, M influences the timecomplexity the most.
Because we use the bound-ary of MSUs instead of enumerating all possiblesubsequences, the value of M can be largely re-duced.Some characters are always labeled as ?S?
or?K?
once the context is given.
We can use thisphenomenon to reduce the search space of decod-ing.
Let cidenote the ithcharacter relative to thecurrent character c0and tidenote the tag of ci.
Thecontext templates we use are listed in TABLE 7.Uni-gram Contexts c0, c?1, c1Bi-gram Contexts c?1c0, c?1c1, c0c1Table 7: Context templates used in pruningWith respect to a training corpus, if a contextC relative to c0always assigns a certain tag t toc0, then we can use this constraint in pruning.
Wejudge the degree of ?always?
by checking whethercount(C?t0=t)count(C)> threshold.
The threshold is anon-negative real number under 1.0.3 Experiments3.1 Data and Evaluation MetricWe use the abbreviation corpus provided by Insti-tute of Computational Linguistics (ICL) of PekingUniversity in our experiments.
The corpus is sim-ilar to the corpus used in Sun et al.
(2008, 2009);Zhang et al.
(2012).
It contains 8, 015 Chinese ab-breviations, including noun phrases, organizationnames and some other types.
Some examples arepresented in TABLE 8.
We use 80% abbreviationsas training data and the rest as testing data.
Insome cases, a long phrase may contain more thanone abbreviation.
For these cases, the corpus justkeeps their most commonly used abbreviation foreach full form.The evaluation metric used in our experimentis the top-K accuracy, which is also used byTsuruoka et al.
(2005), Sun et al.
(2009) and14091.
xi?
{0, 1}, yi?
{0, 1}, zi?
{0, 1}2.?Ni=1yi?
13.?Qi=1zi?
14.
?Ri?
R, Sj?
S, if Riand Sjhave a same position but the position gets different labels,then yi+ xj?
15.
?Ti?
T , Sj?
S, if Tiand Sjhave a same position but the position gets different labels,then zi+ xj?
16.
?Ri?
R, Tj?
T , if Riand Tjhave a same position but the position gets different labels,then xi+ zj?
17.
?Si, Sj?
S if Siand Sjhave a same position but the position gets different labels, thenzi+ zj?
18.
?Si, Sj?
S if the last character Sikeeps is the same as the first character Sjkeeps, thenzi+ zj?
1Table 6: Constraints for ILPType Full form AbbreviationNoun Phrase ????
(Excellent articles) ?
?Organization ????(Writers?
Association) ?
?Coordinate phrase ????
(Injuries and deaths) ?
?Proper noun ????
(Media) ?
?Table 8: Examples of the corpus (Noun Phrase, Organization, Coordinate Phrase, Proper Noun)Zhang et al.
(2012).
The top-K accuracy measureswhat percentage of the reference abbreviations arefound if we take the top N candidate abbreviationsfrom all the results.
In our experiment, top-10 can-didates are considered in re-ranking phrase and themeasurement used is top-1 accuracy (which is theaccuracy we usually refer to) because the final aimof the algorithm is to detect the exact abbreviation.CRF++7, an open source linear chain CRF tool,is used in the sequence labeling part.
For ILP part,we use lpsolve8, which is also an open source tool.The parameters of these tools are tuned throughcross-validation on the training data.3.2 ResultsTABLE 9 shows the top-K accuracy of thecharacter-based and MSU-based method.
We cansee that the MSU-based tagging method can uti-lize word information, which can get better perfor-mance than the character-based method.
We canalso figure out that the top-5 candidates include thereference abbreviation for most full forms.
There-fore reasonable decoding by considering all possi-ble labeling of sequences may improve the perfor-mance.
Although the MSU-based methods onlyoutperforms character-based methods by 0.75%7http://crfpp.sourceforge.net/8http://lpsolve.sourceforge.net/5.5/for top-1 accuracy, it is much better when consid-ering top-2 to top-5 accuracy (+2.5%).
We furtherselect the top-ranked candidates for ILP decod-ing.
Therefore the MSU-based method can furtherimprove the performance in the global decodingphase.K char-based MSU-based1 0.5714 0.57892 0.6879 0.71553 0.7681 0.78194 0.8070 0.82835 0.8333 0.8583Table 9: Top-K (K ?
5) results of character-basedtagging and MSU-based taggingWe then use the top-5 candidates of character-based method and MSU-based method, as wellas the top-2 results of sub-sequence labeling inthe ILP decoding phase.
Then we select the top-ranked candidate as the final abbreviation of eachinstance.
TABLE 10 shows the results.
We can seethat the accuracy of our method is 61.0%, whichimproved by +3.89% compared to the character-based method, and +3.14% compared to the MSU-based method.We find that the ILP decoding phase do playan important role in generating the right an-1410Method Top-1 AccuracyChar-based 0.5714MSU-based 0.5789ILP Result 0.6103Table 10: Top-1 Accuracy after ILP decodingswer.
Some reference abbreviations which are notpicked out by either tagging method can be foundout after decoding.
TABLE 11 shows the exam-ple of the organization name ?????????????
(Higher Education Admissions Office).Neither the character-based method nor the MSU-based method finds the correct answer ????
?,while after ILP decoding, ?????
becomes thefinal result.
TABLE 12 and TABLE 13 give twomore examples.True Result ??
?Char-based ?
?MSU-based ??
?ILP Decoding ??
?Table 11: Top-1 result of ?????????????
(Higher Education Admissions Office)True Result ?
?Char-based ?
?MSU-based ??
?ILP Decoding ?
?Table 12: Top-1 result of ??????
(Articlesexceed the value)True Result ???
?Char-based ??
?MSU-based ??
?ILP Decoding ???
?Table 13: Top-1 result of ??????????
(Visual effects of sound and lights)3.3 Improvements Considering LengthFull forms that are longer than five characters arelong terms.
Long terms contain more characters,which is much easier to make mistakes.
Figure1 shows the top-1 accuracy respect to the termlength using different tagging methods and usingILP decoding.
The x-axis represents the length ofthe full form.
The y-axis represents top-1 accu-racy.
We find that our method works especiallybetter than pure character-based or MSU-basedapproach when the full form is long.
By decod-ing using ILP, both local and global informationare incorporated.
Therefore many of these errorscan be eliminated.Figure 1: Top-1 accuracy of different methodsconsidering length3.4 Effect of pruningAs discussed in previous sections, if we are ableto pre-determine that some characters in a certaincontext should be kept or skipped, then the num-ber of possible boolean variable x can be reduced.TABLE 14 shows the differences.
To guaranteea high accuracy, we set the threshold to be 0.99.When the original full form is partially tagged bythe pruning constraints, the number of booleanvariables per full form is reduced from 34.4 to25.5.
By doing this, we can improve the predic-tion speed over taking the raw input.From TABLE 14 we can also see that the top-1 accuracy is not affected by these pruning con-straints.
This is obvious, because CRF itself hasa strong modeling ability.
The pruning constraintscannot improve the model accuracy.
But they canhelp eliminate those false candidates to make theILP decoding faster.Accuracy Average length Time(s)raw 0.6103 34.4 12.5pruned 0.6103 25.5 7.1Table 14: Comparison of testing time of raw inputand pruned input3.5 Compare with the State-of-the-artSystemsWe also compare our method with previous meth-ods, including Sun et al.
(2009) and Zhang et al.(2012).
Because we use a different corpus, were-implement the system Sun et al.
(2009), Zhang1411et al.
(2012) and Sun et al.
(2013), and experi-ment on our corpus.
The first two are CRF+GIand DPLVM+GI in Sun et al.
(2009), which arereported to outperform the methods in Tsuruokaet al.
(2005) and Sun et al.
(2008).
For DPLVMwe use the same model in Sun et al.
(2009) andexperiment on our own data.
We also compareour approach with the method in Zhang et al.(2012).
However, Zhang et al.
(2012) uses dif-ferent sources of search engine result informationto re-rank the original candidates.
We do not useany extra web resources.
Because Zhang et al.
(2012) uses web information only in its secondstage, we use ?BIEP?
(the tag set used by Zhanget al.
(2012)) to denote the first stage of Zhanget al.
(2012), which also uses no web information.TABLE 15 shows the results of the comparisons.We can see that our method outperforms all othermethods which use no extra resource.
BecauseZhang et al.
(2012) uses extra web resource, thetop-1 accuracy of Zhang et al.
(2012) is slightlybetter than ours.Method Top-1 AccuracyCRF+GI 0.5850DPLVM+GI 0.5990BIEP 0.5812Zhang et al.
(2012) 0.6205Our Result 0.6103Table 15: Comparison with the state-of-the-artsystems4 Related WorkPrevious research mainly focuses on ?abbrevia-tion disambiguation?, and machine learning ap-proaches are commonly used (Park and Byrd,2001; HaCohen-Kerner et al., 2008; Yu et al.,2006; Ao and Takagi, 2005).
These ways of link-ing abbreviation pairs are effective, however, theycannot solve our problem directly.
In many casesthe full form is definite while we don?t know thecorresponding abbreviation.To solve this problem, some approaches main-tain a database of abbreviations and their corre-sponding ?full form?
pairs.
The major problemof pure database-building approach is obvious.
Itis impossible to cover all abbreviations, and thebuilding process is quit laborious.
To find thesepairs automatically, a powerful approach is to findthe reference for a full form given the context,which is referred to as ?abbreviation generation?.There is research on heuristic rules for gen-erating abbreviations Barrett and Grems (1960);Bourne and Ford (1961); Taghva and Gilbreth(1999); Park and Byrd (2001); Wren et al.
(2002);Hearst (2003).
Most of them achieved high per-formance.
However, hand-crafted rules are timeconsuming to create, and it is not easy to transferthe knowledge of rules from one language to an-other.Recent studies of abbreviation generation havefocused on the use of machine learning tech-niques.
Sun et al.
(2008) proposed a supervisedlearning approach by using SVM model.
Tsu-ruoka et al.
(2005); Sun et al.
(2009) formal-ized the process of abbreviation generation as asequence labeling problem.
In Tsuruoka et al.
(2005) each character in the full form is associatedwith a binary value label y, which takes the valueS (Skip) if the character is not in the abbreviation,and value P (Preserve) if the character is in the ab-breviation.
Then a MEMM model is used to modelthe generating process.
Sun et al.
(2009) followedthis schema but used DPLVM model to incor-porate both local and global information, whichyields better results.
Sun et al.
(2013) also usesmachine learning based methods, but focuses onthe negative full form problem, which is a littledifferent from our work.Besides these pure statistical approaches, thereare also many approaches using Web as a corpusin machine learning approaches for generating ab-breviations.Adar (2004) proposed methods to de-tect such pairs from biomedical documents.
Jainet al.
(2007) used web search results as well assearch logs to find and rank abbreviates full pairs,which show good result.
The disadvantage is thatsearch log data is only available in a search en-gine backend.
The ordinary approaches do nothave access to search engine internals.
Zhang et al.
(2012) used web search engine information to re-rank the candidate abbreviations generated by sta-tistical approaches.
Compared to their approaches,our method uses no extra resource, but reachescomparable results.ILP shows good results in many NLP tasks.Punyakanok et al.
(2004); Roth and Yih (2005)used it in semantic role labeling (SRL).
Martinset al.
(2009) used it in dependency parsing.
(Zhaoand Marcus, 2012) used it in Chinese word seg-mentation.
(Riedel and Clarke, 2006) used ILP1412in dependency parsing.
However, previous worksmainly focus on the constraints of avoiding bound-ary confliction.
For example, in SRL, two argu-ment of cannot overlap.
In CWS, two Chinesewords cannot share a same character.
Different totheir methods, we investigate on the conflict of la-bels of character sub-sequences.5 Conclusion and Future workWe propose a new Chinese abbreviation predic-tion method which can incorporate rich local in-formation while generating the abbreviation glob-ally.
We propose the MSU, which is more coarse-grained than character but more fine-grained thanword, to capture word information in the se-quence labeling framework.
Besides the MSU-based method, we use a substring tagging strategyto generate local substring tagging candidates.
Weuse an ILP formulation with various constraintsto globally decode the final abbreviation from thegenerated candidates.
Experiments show that ourmethod outperforms the state-of-the-art systems,without using any extra resource.
This methodis not limited to Chinese abbreviation generation,it can also be applied to similar languages likeJapanese.The results are promising and outperform thebaseline methods.
The accuracy can still be im-proved.
Potential future works may include usingsemi-supervised methods to incorporate unlabeleddata and design reasonable features from large cor-pora.
We are going to study on these issues in thefuture.AcknowledgmentsThis research was partly supported by Na-tional Natural Science Foundation of China(No.61370117,61333018,61300063),MajorNational Social Science Fund ofChina(No.12&ZD227), National High Tech-nology Research and Development Program ofChina (863 Program) (No.
2012AA011101), andDoctoral Fund of Ministry of Education of China(No.
20130001120004).
The contact author ofthis paper, according to the meaning given tothis role by Key Laboratory of ComputationalLinguistics, Ministry of Education, School ofElectronics Engineering and Computer Science,Peking University, is Houfeng Wang.
We thankKe Wu for part of our work is inspired by hisprevious work at KLCL.ReferencesAdar, E. (2004).
Sarad: A simple and ro-bust abbreviation dictionary.
Bioinformatics,20(4):527?533.Ao, H. and Takagi, T. (2005).
Alice: an algorithmto extract abbreviations from medline.
Journalof the American Medical Informatics Associa-tion, 12(5):576?586.Barrett, J. and Grems, M. (1960).
Abbreviatingwords systematically.
Communications of theACM, 3(5):323?324.Bourne, C. and Ford, D. (1961).
A study ofmethods for systematically abbreviating englishwords and names.
Journal of the ACM (JACM),8(4):538?552.HaCohen-Kerner, Y., Kass, A., and Peretz, A.(2008).
Combined one sense disambiguationof abbreviations.
In Proceedings of the 46thAnnual Meeting of the Association for Compu-tational Linguistics on Human Language Tech-nologies: Short Papers, pages 61?64.
Associa-tion for Computational Linguistics.Hearst, M. S. (2003).
A simple algorithm foridentifying abbreviation definitions in biomed-ical text.Jain, A., Cucerzan, S., and Azzam, S. (2007).Acronym-expansion recognition and ranking onthe web.
In Information Reuse and Integration,2007.
IRI 2007.
IEEE International Conferenceon, pages 209?214.
IEEE.Martins, A. F., Smith, N. A., and Xing, E. P.(2009).
Concise integer linear programmingformulations for dependency parsing.
In Pro-ceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 1-Volume 1,pages 342?350.
Association for ComputationalLinguistics.Park, Y. and Byrd, R. (2001).
Hybrid text miningfor finding abbreviations and their definitions.In Proceedings of the 2001 conference on em-pirical methods in natural language processing,pages 126?133.Punyakanok, V., Roth, D., Yih, W.-t., and Zimak,D.
(2004).
Semantic role labeling via integerlinear programming inference.
In Proceedingsof the 20th international conference on Compu-1413tational Linguistics, page 1346.
Association forComputational Linguistics.Riedel, S. and Clarke, J.
(2006).
Incremental in-teger linear programming for non-projective de-pendency parsing.
In Proceedings of the 2006Conference on Empirical Methods in NaturalLanguage Processing, pages 129?137.
Associ-ation for Computational Linguistics.Roth, D. and Yih, W.-t. (2005).
Integer linearprogramming inference for conditional randomfields.
In Proceedings of the 22nd internationalconference on Machine learning, pages 736?743.
ACM.Sun, X., Li, W., Meng, F., and Wang, H. (2013).Generalized abbreviation prediction with nega-tive full forms and its application on improv-ing chinese web search.
In Proceedings of theSixth International Joint Conference on NaturalLanguage Processing, pages 641?647, Nagoya,Japan.
Asian Federation of Natural LanguageProcessing.Sun, X., Okazaki, N., and Tsujii, J.
(2009).
Ro-bust approach to abbreviating terms: A discrim-inative latent variable model with global infor-mation.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Nat-ural Language Processing of the AFNLP: Vol-ume 2-Volume 2, pages 905?913.
Associationfor Computational Linguistics.Sun, X., Wang, H., and Wang, B.
(2008).
Pre-dicting chinese abbreviations from definitions:An empirical learning approach using supportvector regression.
Journal of Computer Scienceand Technology, 23(4):602?611.Taghva, K. and Gilbreth, J.
(1999).
Recognizingacronyms and their definitions.
InternationalJournal on Document Analysis and Recogni-tion, 1(4):191?198.Tsuruoka, Y., Ananiadou, S., and Tsujii, J.
(2005).A machine learning approach to acronym gen-eration.
In Proceedings of the ACL-ISMB Work-shop on Linking Biological Literature, Ontolo-gies and Databases: Mining Biological Seman-tics, pages 25?31.
Association for Computa-tional Linguistics.Wren, J., Garner, H., et al.
(2002).
Heuristicsfor identification of acronym-definition patternswithin text: towards an automated construc-tion of comprehensive acronym-definition dic-tionaries.
Methods of information in medicine,41(5):426?434.Yu, H., Kim, W., Hatzivassiloglou, V., and Wilbur,J.
(2006).
A large scale, corpus-based approachfor automatically disambiguating biomedicalabbreviations.
ACM Transactions on Informa-tion Systems (TOIS), 24(3):380?404.Zhang, L., Li, S., Wang, H., Sun, N., and Meng,X.
(2012).
Constructing Chinese abbreviationdictionary: A stacked approach.
In Proceedingsof COLING 2012, pages 3055?3070, Mumbai,India.
The COLING 2012 Organizing Commit-tee.Zhao, Q. and Marcus, M. (2012).
Exploring deter-ministic constraints: from a constrained englishpos tagger to an efficient ilp solution to chineseword segmentation.
In Proceedings of the 50thAnnual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers),pages 1054?1062, Jeju Island, Korea.
Associa-tion for Computational Linguistics.1414
