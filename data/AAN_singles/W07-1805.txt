Proceedings of SPEECHGRAM 2007, pages 33?40,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsHandling Out-of-Grammar Commands in Mobile Speech InteractionUsing Backoff Filler ModelsTim Paek1, Sudeep Gandhe2, David Maxwell Chickering1, Yun Cheng Ju11 Microsoft Research, One Microsoft Way, Redmond, WA 98052 USA2USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, CA 90292, USA{timpaek|dmax|yuncj}@microsoft.com, gandhe@usc.eduAbstractIn command and control (C&C) speech in-teraction, users interact by speaking com-mands or asking questions typically speci-fied in a context-free grammar (CFG).
Un-fortunately, users often produce out-of-grammar (OOG) commands, which can re-sult in misunderstanding or non-understanding.
We explore a simple ap-proach to handling OOG commands thatinvolves generating a backoff grammarfrom any CFG using filler models, and util-izing that grammar for recognition when-ever the CFG fails.
Working within thememory footprint requirements of a mobileC&C product, applying the approachyielded a 35% relative reduction in seman-tic error rate for OOG commands.
It alsoimproved partial recognitions for enablingclarification dialogue.1 IntroductionIn command and control (C&C) speech interaction,users interact with a system by speaking com-mands or asking questions.
By defining a rigidsyntax of possible phrases, C&C reduces the com-plexity of having to recognize unconstrained natu-ral language.
As such, it generally affords higherrecognition accuracy, though at the cost of requir-ing users to learn the syntax of the interaction(Rosenfeld et al, 2001).
To lessen the burden onusers, C&C grammars are authored in an iterativefashion so as to broaden the coverage of likely ex-pressions for commands, while remaining rela-tively simple for faster performance.
Nevertheless,users can, and often still do, produce OOG com-mands.
They may neglect to read the instructions,or forget the valid expressions.
They may mistak-enly believe that recognition is more robust than itreally is, or take too long to articulate the rightwords.
Whatever the reason, OOG commands canengender misunderstanding (i.e., recognition of thewrong command) or non-understanding (i.e., norecognition), and aggravate users who otherwisemight not realize that their commands were OOG.In this paper, we explore a simple approach tohandling OOG commands, designed specifically tomeet the memory footprint requirements of a C&Cproduct for mobile devices.
This paper is dividedinto three sections.
First, we provide backgroundon the C&C product and discuss the different typesof OOG commands that occur with personal mo-bile devices.
Second, we explain the details of theapproach and how we applied it to the product do-main.
Finally, we evaluate the approach on datacollected from real users, and discuss possibledrawbacks.2 Mobile C&CWith the introduction of voice dialing on mobiledevices, C&C speech interaction hit the widerconsumer market, albeit with rudimentary patternrecognition.
Although C&C has beencommonplace in telephony and accessibility formany years, only recently have mobile deviceshave the memory and processing capacity tosupport not only automatic speech recognition(ASR), but a whole range of multimediafunctionalities that can be controlled with speech.Leveraging this newfound computational capacityis Voice Command, a C&C application for high-end mobile devices that allows users to look upcontacts, place phone calls, retrieve appointments,obtain device status information, controlmultimedia and launch applications.
It uses anembedded, speaker-independent recognizer andoperates on 16 bit, 16 kHz, Mono audio.33OOG commands pose a serious threat to the us-ability of Voice Command.
Many mobile users ex-pect the product to ?just work?
without having toread the manual.
So, if they should say ?Dial Bob?,when the proper syntax for making a phone call isCall {Name}, the utterance will likely be mis-recognized or dropped as a false recognition.
Ifthis happens enough, users may abandon the prod-uct, concluding that it or ASR in general, does notwork.2.1 OOG frequencyGiven that C&C speech interaction is typicallygeared towards a relatively small number of wordsper utterance, an important question is, how oftendo OOG commands really occur in C&C?
In Pro-ject54 (Kun & Turner, 2005), a C&C applicationfor retrieving police information in patrol cars,voice commands failed on average 15% of the time,roughly 63% of which were due to human error.Of that amount, roughly 54% were from extrane-ous words not found in the grammar, 12% fromsegmentation errors, and the rest from speakingcommands that were not active.To examine whether OOG commands might beas frequent on personal mobile devices, we col-lected over 9700 commands of roughly 1 to 3 sec-onds each from 204 real users of Voice Command,which were recorded as sound (.wav) files.
Wealso logged all device data such as contact entriesand media items.
All sound files were transcribedby a paid professional transcription service.
Weignored all transcriptions that did not have an asso-ciated command; the majority of such cases camefrom accidental pressing of the push-to-talk button.Furthermore, we focused on user-initiated com-mands, during which time the active grammar hadthe highest perplexity, instead of yes-no responsesand clarification dialogue.
This left 5061 tran-scribed utterances.2.2 Emulation methodWith the data transcribed, we first needed a me-thod to distinguish between In-Grammar (ING)and OOG utterances.
We developed a simulationenvironment built around a desktop version of theembedded recognizer which could load the sameVoice Command grammars and update them withuser device data, such as contact entries, for eachsound file.
It is important to note the desktop ver-sion was not the engine that is commerciallyshipped and optimized for particular devices, butrather one that serves testing and research purposes.The environment could not only recognize soundfiles, but also parse string input using the dynami-cally updated grammars as if that were the recog-nized result.
We utilized the latter to emulate rec-ognition of all transcribed utterances for VoiceCommand.
If the parse succeeded, we labeled theutterance ING, otherwise it was labeled OOG.Overall, we found that slightly more than oneout of every four (1361 or 26.9%) transcribed ut-terances were OOG.
We provide a completebreakdown of OOG types, including extraneouswords and segmentation errors similar to Project54,in the next section.
It is important to keep in mindthat being OOG by emulation does not necessarilyentail that the recognizer will fail on the actualsound file.
For example, if a user states ?Call Bobat mobile phone?
when the word ?phone?
is OOG,the recognizer will still perform well.
The OOGpercentage for Voice Command may also reflectthe high perplexity of the name-dialing task.
Usershad anywhere from 5 to over 2000 contacts, eachof which could be expressed in multiple ways (e.g.,first name, first name + last name, prefix + lastname, etc.).
In summary, our empirical analysis ofthe data suggests that OOG utterances for mobileC&C on personal devices can indeed occur on afrequent basis, and as such, are worth handling.2.3 OOG typeIn order to explore how we might handle differenttypes of OOG commands, we classified them ac-cording to functional anatomy and basic edit op-erations.
With respect to the former, a C&C utter-ance consists of three functional components:1.
Slot: A dynamically adjustable list repre-senting a semantic argument, such as {Con-tact} or {Date}, where the value of the ar-gument is typically one of the list members.2.
Keyword: A word or phrase that uniquelyidentifies a semantic predicate, such as Callor Battery, where the predicate correspondsin a one-to-one mapping to a type of com-mand.3.
Carrier Text: A word or phrase that is de-signed to facilitate naturalistic expression ofcommands and carries no attached semanticcontent, such as ?What is?
or ?Tell me?.34For example, in the command ?Call Bob at mo-bile?, the word ?Call?
is the keyword, ?Bob?
and?mobile?
are slots, and ?at?
is a carrier word.If we were to convert an ING command tomatch an OOG command, we could perform a se-ries of edit operations: substitution, deletion, andinsertion.
For classifying OOG commands, substi-tution implies the use of an unexpected word, dele-tion implies the absence of an expected word, andinsertion implies the addition of a superfluousword.Starting with both functional anatomy and editoperations for classification, Table 1 displays thedifferent types of OOG commands we labeledalong with their relative frequencies.
Becausemore than one label might apply to an utterance,we first looked to the slot for an OOG type label,then keyword, then everything else.The most frequent OOG type, at about 60%, wasOOG Slot, which referred to slot values that didnot exist in the grammar.
The majority of thesecases came from two sources: 1) contact entriesthat users thought existed but did not ?
sometimesthey did exist, but not in any normalized form (e.g.,?Rich?
for ?Richard?
), and 2) mislabeling of most-ly foreign names by transcribers.
Although wetried to correct as many names as we could, giventhe large contact lists that many users had, thisproved to be quite challenging.The second most frequent OOG type was Inser-tion at about 14%.
The majority of these insertionswere single words.
Note that similar to Project54,segmentation errors occurred quite often at about9%, when the different segmentation types areadded together.3 Backoff ApproachHaving identified the different types of OOGcommands, we needed to devise an approach forhandling them that satisfied the requirements ofVoice Command for supporting C&C on mobiledevices.3.1 Mobile requirementsFor memory footprint, the Voice Command teamspecified that our approach should operate withless than 100KB of ROM and 1MB of RAM.
Fur-thermore, the approach could not require changesOOGType % Total Description ExamplesInsertion 14.2% adding a non-keyword, non-slot word call britney porter on mobile phone [?phone?
issuperfluous]Deletion 3.1% deleting a non-keyword, non-slotword my next appointments [?what are?
missing]Substitution 2.5% replacing a non-keyword, non-slotwordwhere is my next appointment[?where?
is not supported]Segmentation 8.2% incomplete utterance show, call, startKeywordSubstitution 4.6% replacing a keywordcall 8 8 2 8 0 8 0 [?dial?
is keyword] ,dial john horton [?call?
is keyword]KeywordSegmentation 0.1% incomplete keyword what are my appointKeywordDeletion 2.2% deleting the keyword marsha porter at home [?call?
missing]SlotSubstitution 0.4% replacing slot wordscall executive 5 on desk[?desk?
is not slot value]SlotSegmentation 0.9% incomplete slot call alexander woods on mobSlot Deletion 1.0% deleted slot call tracy morey atDisfluencies 1.8% disfluencies - mostly repetitions start expense start microsoft excelOrderRearrangement 0.6%changing the order of words within akeywordwhat meeting is next [Should be ?what is mynext meeting?
]Noise 0.7% non primary speaker oregon state home coming call brandon joneson mobile phoneOOG Slot 59.8% The slot associated with this utterance is out of domainShow Rich Lowry [?Richard?
is contact entry] ,dial 0 2 1 6 [Needs > 7 digits]Table 1.
Different OOG command types and their relative frequencies for the Voice Command product.
The brack-eted text in the ?Examples?
column explicates the cause of the error35to the existing embedded Speech API (SAPI).Because the team also wanted to extend the func-tionality of Voice Command to new domains, wecould not assume that we would have any data fortraining models.
Although statistical languagemodels (SLM) offer greater robustness to varia-tions in phrasing than fixed grammars (Rosenfeld,2000), the above requirements essentially prohib-ited them.
So, we instead focused on extending theuse of the base grammar, which for Voice Com-mand was a context-free grammar (CFG): a formalspecification of rules allowing for embedded recur-sion that defines the set of possible phrases (Man-ning & Sch?tze, 1999).Despite the manual effort that CFGs often re-quire, they are widely prevalent in industry (Knightet al, 2001) for several reasons.
First, they are easyfor designers to understand and author.
Second,they are easy to modify; new phrases can be addedand immediately recognized with little effort.
Andthird, they produce transparent semantics withoutrequiring a separate natural language understand-ing component; semantic properties can be at-tached to CFG rules and assigned during recogni-tion.
By focusing on CFGs, our approach allowsindustry designers who are more accustomed tofixed grammars to continue using their skill set,while hopefully improving the handling of utter-ances that fall outside of their grammar.3.2 Leveraging a backoff grammarAs long as utterances remain ING, a CFG affordsfast and accurate recognition, especially becauseengines are often tuned to optimize C&C recogni-tion.
For example, in comparing recognition per-formance in a statistical and a CFG-based recog-nizer for the same domain, Knight et al (2001)found that the CFG outperformed the SLM.
Inorder to exploit the optimization of the engine forC&C utterances that are ING, we decided to utilizea two-pass approach where each command is ini-tially submitted to the base CFG.
If the confidencescore of the top recognition C1 falls below a rejec-tion threshold RCFG, or if the recognizer declares afalse recognition (based on internal engine fea-tures), then the audio stream is passed to a backoffgrammar which then attempts to recognize thecommand.
If the backoff grammar fails to recog-nize the command, or the top recognition fallsagain below a rejection threshold RBG, then usersexperience the same outcome as they normallywould otherwise, except with a longer delay.
Fig-ure 1(a) summarizes the approach.In order to generate the backoff grammar andstill stay within the required memory bounds ofVoice Command, we explored the use of the built-in filler or garbage model, which is a context-independent, acoustic phone loop.
Expressed inthe syntax as ?...
?, filler models capture phones inwhatever context they are placed.
The functionalanatomy of a C&C utterance, as explained in Sec-tion 2.3, sheds light on where to place them: beforeand/or after keywords and/or slots.
As shown Fig-ure 1(b), to construct a backoff grammar from aCFG during design time, we simply parse eachCFG rule for keywords and slots, remove all car-rier phrases, and insert filler models before and/orafter the keywords and/or slots.
Although it isstraightforward to automatically identify keywords(words that uniquely map to a CFG rule) and slots(lists with semantic properties), developers maywant to edit the generated backoff grammar for anykeywords and slots they wish to exclude; for ex-ample, in cases where more than one keyword isfound for a CFG rule.For both slots and keywords, we could employany number of different patterns for placing thefiller models, if any.
Table 2 displays some of thepatterns in SAPI 5 format, which is an XML for-mat where question marks indicate optional use.Although the Table is for keywords, the same pat-terns apply for slots.
As shown in k4, even thefunctional constituent itself can be optional.
Fur-thermore, alternate lists of patterns can be com-posed, as in kn.
Depending on the number and typeFigure 1.
(a) A two-pass approach which leverages abase CFG for ING recognition and a backoff grammarfor failed utterances.
(b) Design time procedure forgenerating a backoff grammar36of functional constituents for a CFG rule, backoffrules can be constructed by adjoining patterns foreach constituent.
We address the situation when abackoff rule corresponds to multiple CFG rules inSection 3.4.3.3 Domain feasibilityBecause every C&C utterance can be characterizedby its functional constituents, the backoff filler ap-proach generically applies to C&C domains, re-gardless of the actual keywords and slots.
But thequestion remains, is this generic approach feasiblefor handling the different OOG types for VoiceCommand discussed in Section 2.3?The filler model is clearly suited for Insertions,which are the second most frequent OOG type,because it would capture the additional phones.However, the most frequent OOG type, OOG Slot,cannot be handled by the backoff approach.
Thatrequires the developer to write better code forproper name normalization (e.g, ?Rich?
from ?Ri-chard?)
as well as breaking down the slot valueinto further components for better partial matchingof names.
Because new C&C domains may notutilize name slots, we decided to treat improvingname recognition as separate research.
Fortu-nately, opportunity for applying the backoff fillerapproach to OOG Slot types still exists.3.4 Clarification of partial recognitionsAs researchers have observed, OOG words con-tribute to increased word-error rates (Bazzi &Glass, 2000) and degrade the recognition perform-ance of surrounding ING words (Gorrell, 2003).Hence, even if a keyword surrounding an OOGslot is recognized, its confidence score and theoverall phrase confidence score will often be de-graded.
This is in some ways an unfortunate by-product of confidence annotation, which might becircumvented if SAPI exposed word lattice prob-abilities.
Because SAPI does not, we can insteadgenerate partial backoff rules that comprise only asubset of the functional constituents of a CFG rule.For example, if a CFG rule contains both a key-word and slot, then we can generate a partial back-off rule with just one or the other surrounded byfiller models.
Using partial backoff rules preventsdegradation of confidence scores for ING constitu-ents and improves partial recognitions, as we showin Section 4.
Partial backoff rules not only handleOOG Slot commands where, for example, thename slot is not recognized, but also many types ofsegmentation, deletion and substitution commandsas well.Following prior research (Gorrell et al, 2002;Hockey et al, 2003), we sought to improve partialrecognitions so that the system could provide feed-back to users on what was recognized, and to en-courage them to stay within the C&C syntax.
Cla-rification dialogue with implicit instruction of thesyntax might proceed as follows: If a partial recog-nition only corresponded to one CFG rule, then thesystem could assume the semantics of that rule andremind the user of the proper syntax.
On the otherhand, if a partial recognition corresponded to morethan one rule, then a disambiguation dialoguecould relate the proper syntax for the choices.
Forexample, suppose a user says ?Telephone Bob?,using the OOG word ?Telephone?.
Although theoriginal CFG would most likely misrecognize oreven drop this command, our approach would ob-tain a partial recognition with higher confidencescore for the contact slot.
If only one CFG rulecontained the slot, then the system could engage inthe confirmation, ?Did you mean to say, callBob??
On the other hand, if more than one CFGrule contained the slot, then the system could en-gage in a disambiguation dialogue, such as ?Iheard 'Bob'.
You can either call or show Bob?.Either way, the user is exposed to and implicitlytaught the proper C&C syntax.3.5 Related researchIn related research, several researchers have inves-tigated using both a CFG and a domain-trainedSLM simultaneously for recognition (Gorrell et al,2002; Hockey et al, 2003).
To finesse the per-formance of a CFG, Gorrell (2003) advocated atwo-pass approach where an SLM trained on CFGScheme Keyword Patternk1 <keyword/>k2 (?)?
<keyword>k3 (?)?
<keyword/>  (?
)?k4 (?)?
<keyword/>?
(?)?kn<list>(?)?
<keyword/>?
(?)?(?
)</list>Table 2.
Possible patterns in SAPI 5 XML format forplacing the filler model, which appears as?...
?.Question marks indicate optional use.37data (and slightly augmented) is utilized as a back-off grammar.
However, only the performance ofthe SLM on a binary OOG classification task wasevaluated and not the two-pass approach itself.
Indesigning a multimodal language acquisition sys-tem, Dusan & Flanagan (2002) developed a two-pass approach where they utilized a dictation n-gram as a backoff grammar and added words rec-ognized in the second pass into the base CFG.
Un-fortunately, they only evaluated the general usabil-ity of their architecture.Because of the requirements outlined in Section3.1, we have focused our efforts on generating abackoff grammar from the original CFG, takingadvantage of functional anatomy and filler models.The approach is agnostic about what the actual fil-ler model is, and as such, the built-in phone loopcan easily be replaced by word-level (e.g., Yu etal., 2006) and sub-word level filler models (e.g.,Liu et al, 2005).
In fact, we did explore the word-level filler model, though so far we have not beenable to meet the footprint requirements.
We arecurrently investigating phone-based filler models.Outside of recognition with a CFG, researchershave pursued methods that directly model OOGwords as sub-word units in the recognition searchspace of a finite state transducer (FST) (Bazzi &Glass, 2000).
OOG words can also be dynamicallyincorporated into the FST (Chung et al, 2004).Because this line of research depends on entirelydifferent engine architecture, we could not applythe techniques.4 EvaluationIn C&C speech interaction, what matters most isnot word-error rate, but semantic accuracy and taskcompletion.
Because task completion is difficult toevaluate without collecting new data, we evaluatedthe semantic accuracy of the two-pass approachagainst the baseline of using just the CFG on thedata we collected from real users, as discussed inSection 2.1.
Furthermore, because partialrecognitions can ultimately result in a successfuldialogue, we carried out separate evaluations forthe functional constituents of a command (i.e.,keyword and slot) as well as the completecommand (keyword + slot).
For Voice Command,no command contained more than one slot, andbecause the vast majority of single slot commandswere commands to either call or show a contactentry, we focused on those two commands as aproof of concept.For any utterance, the recognizer can either ac-cept or reject it.
If it is accepted, then the seman-tics of the utterance can either be correct (i.e., itmatches what the user intended) or incorrect.
Thefollowing metrics can now be defined:precision = CA / (CA + IA)   (1)recall = CA / (CA + R)    (2)accuracy = CA / (CA + IA + R)   (3)where CA denotes accepted commands that arecorrect, IA denotes accepted commands that areincorrect, and R denotes the number of rejectedcommands.
Although R could be decomposed intocorrect and incorrect rejections, for C&C,recognition failure is essentially perceived thesame way by users: that is, as a non-understanding.4.1 ResultsFor every C&C command in Voice Command, theembedded recognizer returns either a falserecognition (based on internal engine parameters)or a recognition event with a confidence score.
Asdescribed in Section 3.2, if the confidence scorefalls below a rejection threshold RCFG, then theaudio stream is processed by the backoff grammarwhich also enforces its own threshold RBG.
TheRCFG for Voice Command was set to 45% by aproprietary tuning procedure for optimizingacoustic word-error rate.
For utterances thatexceeded RCFG, 84.2% of them were ING and15.8% OOG.
For utterances below RCFG, 48.5%Figure 2.
The semantic accuracies comparing thebaseline CFG against both the BG (backoff grammaralone) and the two-pass approach (CFG + Backoff)separated into functional constituent groups and fur-ther separated by ING and OOG commands.38were ING and 51.5% OOG.
Because aconsiderable number of utterances may be ING inthe second pass, as it was in our case, RBG requirestuning as well.
Instead of using a developmentdataset to tune RBG, we decided to evaluate ourapproach on the entire data with RBG set to thesame proprietary threshold as RCFG.
In post-hocanalyses, this policy of setting the two thresholdsequal and reverting to the CFG recognition if thebackoff confidence score falls below RBG achievedresults comparable to optimizing the thresholds.Figure 2 displays semantic accuracies separatedby ING and OOG commands.
Keyword evalua-tions comprised 3700 ING and 1361 OOG com-mands.
Slot and keyword + slot evaluations com-prised 2111 ING and 138 OOG commands.
Over-all, the two-pass approach was significantly higherin semantic accuracy than the baseline CFG, usingMcNemar's test (p<0.001).
Not surprisingly, thelargest gains were with OOG commands.
Noticethat for partial recognitions (i.e., keyword or slotonly), the approach was able to improve accuracy,which with further clarification dialogue, couldresult in task completions.
Interestingly, the ap-proach performed the same for keyword + slot as itdid for slot, which suggests that getting the slotcorrect is crucial to recognizing surrounding key-words.
Despite the high percentage of OOG Slots,slot accuracy still increased due to better handlingof other OOG types such as deletions, insertionsand substitutions.Finally, as a comparison, for the keyword + slottask, an upper bound of 74.3% ?
1.1% (10-foldcross-validated standard error) overall semanticaccuracy was achieved using a small footprint sta-tistical language modeling technique that re-rankedCFG results (Paek & Chickering, 2007), thoughthe comparison is not completely fair given that thetechnique was focused on predictive languagemodeling and not on explicitly handling OOG ut-terances.
Also note that in all cases, the backoffgrammar alone performed worse than either theCFG or the two-pass approach.Table 3 provides a more detailed view of the re-sults for the just OOG commands as well as therelative reductions in semantic error rate (RER).Notice that the approach increases recall, whichsignifies less non-understandings.
However, thiscomes at the price of a small increase in misunder-standings, as seen in the decrease in precision.Overall, the best reduction in semantic error rateachieved by the approach was about 35%.Decomposing RER by OOG types, we foundthat for keyword evaluations, the biggest im-provement (52% RER), came about for Deletiontypes, or commands with missing carrier words.This makes sense because the backoff grammaronly cares about the keyword.
For slot and key-word + slot evaluations, Insertion types maintainedthe biggest improvement at 38% RER.Note that the results presented are those ob-tained without tuning.
If application developerswanted to find an optimal operating point, theywould need to decide what is more important fortheir application: precision or recall, and adjust thethresholds until they reach acceptable levels of per-formance.
Ideally, these levels should accord withwhat real users of the application would accept.4.2 EfficiencyGiven that the approach was aimed at satisfyingthe mobile requirements stated in Section 3.1,which it did, we also compared the processing timeit takes to arrive at a recognition or falserecognition between the CFG alone and the two-pass approach.
Because of the filler models, thebackoff grammar is a more relaxed version of CFGwith a larger search space, and as such, takesslightly more processing time.
The averageprocessing time for the CFG in our simulationenvironment was about 395 milliseconds, whereasthe average processing time for the two passes wasabout 986 milliseconds.
Hence, when the backoffgrammar is used, the total computation time isapproximately 2.5 times that of a single pass alone.In our experiments, a total of 1570 commands (i.e.31%) required the two passes, while 3491 of themwere accepted after a single CFG pass.CFG 2-PASS RERPrec 85.0% 79.0% -39.7%Recall 36.8% 58.6% 34.5% KeywordAcc 34.5% 50.7% 24.7%Prec 89.3% 88.2% -10.3%Recall 58.1% 77.6% 46.5% SlotAcc 54.4% 70.3% 34.9%Prec 89.3% 88.2% -10.3%Recall 58.1% 77.6% 46.5% Keyword+ SlotAcc 54.4% 70.3% 34.9%Table 3.
Relative reductions in semantic error rate, orRelative Error Reduction (RER) for OOG commandsgrouped by keyword, slot and keyword + slot evalua-tions.
?2-PASS?
denotes the two-pass approach.394.3 DrawbacksIn exploring the backoff filler approach, weencountered a few drawbacks that are worthconsidering when applying this approach to otherdomains.
The first issue dealt with false positives.In the data collection for Voice Command, a totalof 288 utterances contained no discernable speech.If these were included in the data set, they wouldamount to about 5% of all utterances.
Asmentioned previously, these were mostly caseswhen the push-to-talk button was accidentallytriggered.
When we evaluated the approach onthese utterances, we found that the CFG accepted36 or roughly 13% of them, while the proposedapproach accepted 115 or roughly 40% of them.For our domain, this problem can be avoided byinstructing users to lock their devices when not inuse to prevent spurious initiations.
For other C&Cdomains where unintentional command initiationsoccur frequently, this may be a serious concern,though we suspect that users will be moreforgiving of accidental errors than real errors.Another drawback dealt with generating thebackoff grammar.
As we discussed in Section 3.2,various patterns for placing filler models can beutilized.
Although we did explore the possibilitythat perhaps certain patterns might generalizeacross domains, we found that it was better tohand-craft patterns to the application.
For VoiceCommand, we used the kn pattern specified in Ta-ble 2 for keywords, and the identical sn pattern forslots because they proved to be best suited to theproduct grammars in pre-trial experiments.5 Conclusion & Future DirectionIn this paper, we classified the different types ofOOG commands that might occur in a mobileC&C application, and presented a simple two-passapproach for handling them that leverages the baseCFG for ING recognition and a backoff grammarOOG recognition.
The backoff grammar is gener-ated from the original CFG by surrounding key-words and/or slots with filler models.
Operatingwithin the memory footprint requirements of amobile C&C product, the approach yielded a 35%relative reduction in semantic error rate for OOGcommands, and improved partial recognitions,which can facilitate clarification dialogue.We are now exploring small footprint, phone-based filler models.
Another avenue for futureresearch is to further investigate optimal policiesfor deciding when to pass to the backoff grammarand when to use the backoff grammar recognition.ReferencesI.
Bazzi & J.
Glass.
2000.
Modeling out-of-vocabularywords for robust speech recognition.
In Proc.
ICSLP.G.
Chung, S. Seneff, C.Wang, & I. Hetherington.
2004.A dynamic vocabulary spoken dialogue interface.
InProc.
ICSLP.S.
Dusan & J. Flanagan.
2002.
Adaptive dialog basedupon multimodal language acquisition.
In Proc.
IC-MI.G.
Gorrell, I. Lewin, & M. Rayner.
2002.
Adding intel-ligent help to mixed initiative spoken dialogue sys-tems.
In Proc.
ICSLP.G.
Gorrell.
2003.
Using statistical language modeling toidentify new vocabulary in a grammar-based speechrecognition system.
In Proc.
Eurospeech.B.
Hockey, O.
Lemon, E. Campana, L. Hiatt, G. Aist, J.Hieronymus, A. Gruenstein, & J. Dowding.
2003.Targeted help for spoken dialogue systems: intelli-gent feedback improves naive users?
performance.
InProc.
EACL, pp.
147?154.S.
Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-ing, & I. Lewin.
2001.
Comparing grammar-basedand robust approaches to speech understanding: Acase study.
In Proc.
Eurospeech.A.
Kun & L. Turner.
2005.
Evaluating the project54speech user interface.
In Proc.
Pervasive.P.
Liu, Y. Tian, J. Zhou, & F. Soong.
2005.
Backgroundmodel based posterior probability for measuringconfidence.
In Proc.
Interspeech.C.D.
Manning & H. Sch?utze.
1999.
Foundations ofStatistical Natural Language Processing.
MIT Press,Cambridge,Massachusetts.Paek, T. & Chickering, D. 2007.
Improving commandand control speech recognition: Using predictive us-er models for language modeling.
UMUAI, 17(1):93-117.Rosenfeld, R. 2000.
Two decades of statistical languagemodeling: Where do we go from here?
In Proc.
of theIEEE, 88(8): 1270?1278.R.
Rosenfeld, D. Olsen, & A. Rudnicky.
2001.
Univer-sal speech interfaces.
Interactions, 8(6):34?44.D.
Yu, Y.C.
Ju, Y. Wang, & A. Acero.
2006.
N-grambased filler model for robust grammar authoring.
InProc.
ICASSP.40
