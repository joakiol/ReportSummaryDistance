Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 136?144,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPTransliteration AlignmentVladimir Pervouchine, Haizhou LiInstitute for Infocomm ResearchA*STAR, Singapore 138632{vpervouchine,hli}@i2r.a-star.edu.sgBo LinSchool of Computer EngineeringNTU, Singapore 639798linbo@pmail.ntu.edu.sgAbstractThis paper studies transliteration align-ment, its evaluation metrics and applica-tions.
We propose a new evaluation met-ric, alignment entropy, grounded on theinformation theory, to evaluate the align-ment quality without the need for the goldstandard reference and compare the metricwith F -score.
We study the use of phono-logical features and affinity statistics fortransliteration alignment at phoneme andgrapheme levels.
The experiments showthat better alignment consistently leads tomore accurate transliteration.
In transliter-ation modeling application, we achieve amean reciprocal rate (MRR) of 0.773 onXinhua personal name corpus, a signifi-cant improvement over other reported re-sults on the same corpus.
In transliterationvalidation application, we achieve 4.48%equal error rate on a large LDC corpus.1 IntroductionTransliteration is a process of rewriting a wordfrom a source language to a target language in adifferent writing system using the word?s phono-logical equivalent.
The word and its translitera-tion form a transliteration pair.
Many efforts havebeen devoted to two areas of studies where thereis a need to establish the correspondence betweengraphemes or phonemes between a transliterationpair, also known as transliteration alignment.One area is the generative transliteration model-ing (Knight and Graehl, 1998), which studies howto convert a word from one language to another us-ing statistical models.
Since the models are trainedon an aligned parallel corpus, the resulting statisti-cal models can only be as good as the alignment ofthe corpus.
Another area is the transliteration vali-dation, which studies the ways to validate translit-eration pairs.
For example Knight and Graehl(1998) use the lexicon frequency, Qu and Grefen-stette (2004) use the statistics in a monolingualcorpus and the Web, Kuo et al (2007) use proba-bilities estimated from the transliteration model tovalidate transliteration candidates.
In this paper,we propose using the alignment distance betweenthe a bilingual pair of words to establish the evi-dence of transliteration candidacy.
An example oftransliteration pair alignment is shown in Figure 1.e5e1e2e3e4c1c2c3A   L I  C E?
?
?source graphemestarget graphemese1e2e3grapheme tokensFigure 1: An example of grapheme alignment (Al-ice, ???
), where a Chinese grapheme, a char-acter, is aligned to an English grapheme token.Like the word alignment in statistical ma-chine translation (MT), transliteration alignmentbecomes one of the important topics in machinetransliteration, which has several unique chal-lenges.
Firstly, the grapheme sequence in a wordis not delimited into grapheme tokens, resultingin an additional level of complexity.
Secondly, tomaintain the phonological equivalence, the align-ment has to make sense at both grapheme andphoneme levels of the source and target languages.This paper reports progress in our ongoing spokenlanguage translation project, where we are inter-ested in the alignment problem of personal nametransliteration from English to Chinese.This paper is organized as follows.
In Section 2,we discuss the prior work.
In Section 3, we in-troduce both statistically and phonologically mo-tivated alignment techniques and in Section 4 weadvocate an evaluation metric, alignment entropythat measures the alignment quality.
We report theexperiments in Section 5.
Finally, we conclude inSection 6.1362 Related WorkA number of transliteration studies have touchedon the alignment issue as a part of the translit-eration modeling process, where alignment isneeded at levels of graphemes and phonemes.
Intheir seminal paper Knight and Graehl (1998) de-scribed a transliteration approach that transfers thegrapheme representation of a word via the pho-netic representation, which is known as phoneme-based transliteration technique (Virga and Khu-danpur, 2003; Meng et al, 2001; Jung et al,2000; Gao et al, 2004).
Another technique isto directly transfer the grapheme, known as di-rect orthographic mapping, that was shown tobe simple and effective (Li et al, 2004).
Someother approaches that use both source graphemesand phonemes were also reported with good per-formance (Oh and Choi, 2002; Al-Onaizan andKnight, 2002; Bilac and Tanaka, 2004).To align a bilingual training corpus, some take aphonological approach, in which the crafted map-ping rules encode the prior linguistic knowledgeabout the source and target languages directly intothe system (Wan and Verspoor, 1998; Meng et al,2001; Jiang et al, 2007; Xu et al, 2006).
Oth-ers adopt a statistical approach, in which the affin-ity between phonemes or graphemes is learnedfrom the corpus (Gao et al, 2004; AbdulJaleel andLarkey, 2003; Virga and Khudanpur, 2003).In the phoneme-based technique where an in-termediate level of phonetic representation is usedas the pivot, alignment between graphemes andphonemes of the source and target words isneeded (Oh and Choi, 2005).
If source and tar-get languages have different phoneme sets, align-ment between the the different phonemes is alsorequired (Knight and Graehl, 1998).
Althoughthe direct orthographic mapping approach advo-cates a direct transfer of grapheme at run-time,we still need to establish the grapheme correspon-dence at the model training stage, when phonemelevel alignment can help.It is apparent that the quality of transliterationalignment of a training corpus has a significantimpact on the resulting transliteration model andits performance.
Although there are many stud-ies of evaluation metrics of word alignment forMT (Lambert, 2008), there has been much less re-ported work on evaluation metrics of translitera-tion alignment.
In MT, the quality of training cor-pus alignment A is often measured relatively tothe gold standard, or the ground truth alignmentG, which is a manual alignment of the corpus ora part of it.
Three evaluation metrics are used:precision, recall, and F -score, the latter being afunction of the former two.
They indicate howclose the alignment under investigation is to thegold standard alignment (Mihalcea and Pedersen,2003).
Denoting the number of cross-lingual map-pings that are common in both A and G as CAG,the number of cross-lingual mappings in A as CAand the number of cross-lingual mappings in G asCG, precision Pr is given as CAG/CA, recall Rcas CAG/CG and F -score as 2Pr ?Rc/(Pr+Rc).Note that these metrics hinge on the availabilityof the gold standard, which is often not available.In this paper we propose a novel evaluation metricfor transliteration alignment grounded on the in-formation theory.
One important property of thismetric is that it does not require a gold standardalignment as a reference.
We will also show thathow this metric is used in generative transliterationmodeling and transliteration validation.3 Transliteration alignment techniquesWe assume in this paper that the source languageis English and the target language is Chinese, al-though the technique is not restricted to English-Chinese alignment.Let a word in the source language (English) be{ei} = {e1 .
.
.
eI} and its transliteration in thetarget language (Chinese) be {cj} = {c1 .
.
.
cJ},ei ?
E, cj ?
C, and E, C being the English andChinese sets of characters, or graphemes, respec-tively.
Aligning {ei} and {cj} means for each tar-get grapheme token c?j finding a source graphemetoken e?m, which is an English substring in {ei}that corresponds to cj , as shown in the example inFigure 1.
As Chinese is syllabic, we use a Chinesecharacter cj as the target grapheme token.3.1 Grapheme affinity alignmentGiven a distance function between graphemes ofthe source and target languages d(ei, cj), the prob-lem of alignment can be formulated as a dynamicprogramming problem with the following functionto minimize:Dij = min(Di?1,j?1 + d(ei, cj),Di,j?1 + d(?, cj),Di?1,j + d(ei, ?
))(1)137Here the asterisk * denotes a null grapheme thatis introduced to facilitate the alignment betweengraphemes of different lengths.
The minimum dis-tance achieved is then given byD =I?i=1d(ei, c?
(i)) (2)where j = ?
(i) is the correspondence between thesource and target graphemes.
The alignment canbe performed via the Expectation-Maximization(EM) by starting with a random initial alignmentand calculating the affinity matrix count(ei, cj)over the whole parallel corpus, where element(i, j) is the number of times character ei wasaligned to cj .
From the affinity matrix conditionalprobabilities P (ei|cj) can be estimated asP (ei|cj) = count(ei, cj)/?jcount(ei, cj) (3)Alignment j = ?
(i) between {ei} and {cj} thatmaximizes probabilityP =?iP (c?
(i)|ei) (4)is also the same alignment that minimizes align-ment distance D:D = ?
logP = ?
?ilogP (c?
(i)|ei) (5)In other words, equations (2) and (5) are the samewhen we have the distance function d(ei, cj) =?
logP (cj |ei).
Minimizing the overall distanceover a training corpus, we conduct EM iterationsuntil the convergence is achieved.This technique solely relies on the affinitystatistics derived from training corpus, thus iscalled grapheme affinity alignment.
It is alsoequally applicable for alignment between a pair ofsymbol sequences representing either graphemesor phonemes.
(Gao et al, 2004; AbdulJaleel andLarkey, 2003; Virga and Khudanpur, 2003).3.2 Grapheme alignment via phonemesTransliteration is about finding phonologicalequivalent.
It is therefore a natural choice to usethe phonetic representation as the pivot.
It iscommon though that the sound inventory differsfrom one language to another, resulting in differ-ent phonetic representations for source and tar-get words.
Continuing with the earlier example,?AE L AH SA L I C EAY l i s iz?
?graphemesphonemesphonemesgraphemessourcetargetFigure 2: An example of English-Chinese translit-eration alignment via phonetic representations.Figure 2 shows the correspondence between thegraphemes and phonemes of English word ?Al-ice?
and its Chinese transliteration, with CMUphoneme set used for English (Chase, 1997) andIIR phoneme set for Chinese (Li et al, 2007a).A Chinese character is often mapped to a uniquesequence of Chinese phonemes.
Therefore, ifwe align English characters {ei} and Chinesephonemes {cpk} (cpk ?
CP set of Chinesephonemes) well, we almost succeed in aligningEnglish and Chinese grapheme tokens.
Alignmentbetween {ei} and {cpk} becomes the main task inthis paper.3.2.1 Phoneme affinity alignmentLet the phonetic transcription of English word{ei} be {epn}, epn ?
EP , where EP is the set ofEnglish phonemes.
Alignment between {ei} and{epn}, as well as between {epn} and {cpk} canbe performed via EM as described above.
We esti-mate conditional probability of Chinese phonemecpk after observing English character ei asP (cpk|ei) =?
{epn}P (cpk|epn)P (epn|ei) (6)We use the distance function between Englishgraphemes and Chinese phonemes d(ei, cpk) =?
logP (cpk|ei) to perform the initial alignmentbetween {ei} and {cpk} via dynamic program-ming, followed by the EM iterations until con-vergence.
The estimates for P (cpk|epn) andP (epn|ei) are obtained from the affinity matrices:the former from the alignment of English and Chi-nese phonetic representations, the latter from thealignment of English words and their phonetic rep-resentations.3.2.2 Phonological alignmentAlignment between the phonetic representationsof source and target words can also be achievedusing the linguistic knowledge of phonetic sim-ilarity.
Oh and Choi (2002) define classes of138phonemes and assign various distances betweenphonemes of different classes.
In contrast, wemake use of phonological descriptors to define thesimilarity between phonemes in this paper.Perhaps the most common way to measure thephonetic similarity is to compute the distances be-tween phoneme features (Kessler, 2005).
Suchfeatures have been introduced in many ways, suchas perceptual attributes or articulatory attributes.Recently, Tao et al (2006) and Yoon et al (2007)have studied the use of phonological features andmanually assigned phonological distance to mea-sure the similarity of transliterated words for ex-tracting transliterations from a comparable corpus.We adopt the binary-valued articulatory at-tributes as the phonological descriptors, which areused to describe the CMU and IIR phoneme setsfor English and Chinese Mandarin respectively.Withgott and Chen (1993) define a feature vec-tor of phonological descriptors for English sounds.We extend the idea by defining a 21-element bi-nary feature vector for each English and Chinesephoneme.
Each element of the feature vectorrepresents presence or absence of a phonologi-cal descriptor that differentiates various kinds ofphonemes, e.g.
vowels from consonants, frontfrom back vowels, nasals from fricatives, etc1.In this way, a phoneme is described by a fea-ture vector.
We express the similarity betweentwo phonemes by the Hamming distance, alsocalled the phonological distance, between the twofeature vectors.
A difference in one descriptorbetween two phonemes increases their distanceby 1.
As the descriptors are chosen to differenti-ate between sounds, the distance between similarphonemes is low, while that between two very dif-ferent phonemes, such as a vowel and a consonant,is high.
The null phoneme, added to both Englishand Chinese phoneme sets, has a constant distanceto any actual phonemes, which is higher than thatbetween any two actual phonemes.We use the phonological distance to performthe initial alignment between English and Chi-nese phonetic representations of words.
After thatwe proceed with recalculation of the distances be-tween phonemes using the affinity matrix as de-scribed in Section 3.1 and realign the corpus again.We continue the iterations until convergence is1The complete table of English and Chinese phonemeswith their descriptors, as well as the translitera-tion system demo is available at http://translit.i2r.a-star.edu.sg/demos/transliteration/reached.
Because of the use of phonological de-scriptors for the initial alignment, we call this tech-nique the phonological alignment.4 Transliteration alignment entropyHaving aligned the graphemes between two lan-guages, we want to measure how good the align-ment is.
Aligning the graphemes means aligningthe English substrings, called the source graphemetokens, to Chinese characters, the target graphemetokens.
Intuitively, the more consistent the map-ping is, the better the alignment will be.
We canquantify the consistency of alignment via align-ment entropy grounded on information theory.Given a corpus of aligned transliteration pairs,we calculate count(cj , e?m), the number of timeseach Chinese grapheme token (character) cj ismapped to each English grapheme token e?m.
Weuse the counts to estimate probabilitiesP (e?m, cj) = count(cj , e?m)/?m,jcount(cj , e?m)P (e?m|cj) = count(cj , e?m)/?mcount(cj , e?m)The alignment entropy of the transliteration corpusis the weighted average of the entropy values forall Chinese tokens:H = ?
?jP (cj)?mP (e?m|cj) logP (e?m|cj)= ?
?m,jP (e?m, cj) logP (e?m|cj)(7)Alignment entropy indicates the uncertainty ofmapping between the English and Chinese tokensresulting from alignment.
We expect and willshow that this estimate is a good indicator of thealignment quality, and is as effective as the F -score, but without the need for a gold standard ref-erence.
A lower alignment entropy suggests thateach Chinese token tends to be mapped to fewerdistinct English tokens, reflecting better consis-tency.
We expect a good alignment to have asharp cross-lingual mapping with low alignmententropy.5 ExperimentsWe use two transliteration corpora: Xinhua cor-pus (Xinhua News Agency, 1992) of 37,637personal name pairs and LDC Chinese-English139named entity list LDC2005T34 (Linguistic DataConsortium, 2005), containing 673,390 personalname pairs.
The LDC corpus is referred to asLDC05 for short hereafter.
For the results to becomparable with other studies, we follow the samesplitting of Xinhua corpus as that in (Li et al,2007b) having a training and testing set of 34,777and 2,896 names respectively.
In contrast to thewell edited Xinhua corpus, LDC05 contains erro-neous entries.
We have manually verified and cor-rected around 240,000 pairs to clean up the corpus.As a result, we arrive at a set of 560,768 English-Chinese (EC) pairs that follow the Chinese pho-netic rules, and a set of 83,403 English-JapaneseKanji (EJ) pairs, which follow the Japanese pho-netic rules, and the rest 29,219 pairs (REST) be-ing labeled as incorrect transliterations.
Next weconduct three experiments to study 1) alignmententropy vs. F -score, 2) the impact of alignmentquality on transliteration accuracy, and 3) how tovalidate transliteration using alignment metrics.5.1 Alignment entropy vs. F -scoreAs mentioned earlier, for English-Chinesegrapheme alignment, the main task is to align En-glish graphemes to Chinese phonemes.
Phonetictranscription for the English names in Xinhuacorpus are obtained by a grapheme-to-phoneme(G2P) converter (Lenzo, 1997), which generatesphoneme sequence without providing the exactcorrespondence between the graphemes andphonemes.
G2P converter is trained on the CMUdictionary (Lenzo, 2008).We align English grapheme and phonetic repre-sentations e?
ep with the affinity alignment tech-nique (Section 3.1) in 3 iterations.
We furtheralign the English and Chinese phonetic represen-tations ep ?
cp via both affinity and phonologicalalignment techniques, by carrying out 6 and 7 it-erations respectively.
The alignment methods areschematically shown in Figure 3.To study how alignment entropy varies accord-ing to different quality of alignment, we wouldlike to have many different alignment results.
Wepair the intermediate results from the e ?
ep andep ?
cp alignment iterations (see Figure 3) toform e ?
ep ?
cp alignments between Englishgraphemes and Chinese phonemes and let themconverge through few more iterations, as shownin Figure 4.
In this way, we arrive at a total of 114phonological and 80 affinity alignments of differ-ent quality.
{cpk}{ei}Englishgraphemes{epn}EnglishphonemesChinesephonemesaffinity alignment affinity alignmente?
epiteration 1e?
epiteration 2e?
epiteration 3ep?
cpiteration 1ep?
cpiteration 2...ep?
cpiteration 6phonological alignmentep?
cpiteration 1ep?
cpiteration 2...ep?
cpiteration 7Figure 3: Aligning English graphemes tophonemes e?ep and English phonemes to Chinesephonemes ep?cp.
Intermediate e?ep and ep?cpalignments are used for producing e ?
ep ?
cpalignments.e?
epalignmentsep?
cpaffinity /phonologicalalignmentsiteration 1iteration 2iteration 3iteration 1iteration 2iteration n......calculatingd(ei, cpk)affinityalignmentiteration 1iteration 2...e?
ep?
cpetcFigure 4: Example of aligning English graphemesto Chinese phonemes.
Each combination of e?epand ep?
cp alignments is used to derive the initialdistance d(ei, cpk), resulting in several e?ep?cpalignments due to the affinity alignment iterations.We have manually aligned a random set of3,000 transliteration pairs from the Xinhua train-ing set to serve as the gold standard, on which wecalculate the precision, recall and F -score as wellas alignment entropy for each alignment.
Eachalignment is reflected as a data point in Figures 5aand 5b.
From the figures, we can observe a clearcorrelation between the alignment entropy and F -score, that validates the effectiveness of alignmententropy as an evaluation metric.
Note that wedon?t need the gold standard reference for report-ing the alignment entropy.We also notice that the data points seem to formclusters inside which the value of F -score changesinsignificantly as the alignment entropy changes.Further investigation reveals that this could be dueto the limited number of entries in the gold stan-dard.
The 3,000 names in the gold standard are notenough to effectively reflect the change across dif-ferent alignments.
F -score requires a large goldstandard which is not always available.
In con-trast, because the alignment entropy doesn?t de-pend on the gold standard, one can easily reportthe alignment performance on any unaligned par-allel corpus.140????????????????????????
???
???
?????
?
?????
??
?
?
???
??
(a) 80 affinity alignments????????????????????????
???
???
???????
??
?
?
???
????
?
?
(b) 114 phonological alignmentsFigure 5: Correlation between F -score and align-ment entropy for Xinhua training set algnments.Results for precision and recall have similar trends.5.2 Impact of alignment quality ontransliteration accuracyWe now further study how the alignment affectsthe generative transliteration model in the frame-work of the joint source-channel model (Li et al,2004).
This model performs transliteration bymaximizing the joint probability of the source andtarget names P ({ei}, {cj}), where the source andtarget names are sequences of English and Chi-nese grapheme tokens.
The joint probability isexpressed as a chain product of a series of condi-tional probabilities of token pairs P ({ei}, {cj}) =P ((e?k, ck)|(e?k?1, ck?1)), k = 1 .
.
.
N , where welimit the history to one preceding pair, resulting ina bigram model.
The conditional probabilities fortoken pairs are estimated from the aligned trainingcorpus.
We use this model because it was shownto be simple yet accurate (Ekbal et al, 2006; Liet al, 2007b).
We train a model for each of the114 phonological alignments and the 80 affinityalignments in Section 5.1 and conduct translitera-tion experiment on the Xinhua test data.During transliteration, an input English nameis first decoded into a lattice of all possible En-glish and Chinese grapheme token pairs.
Then thejoint source-channel transliteration model is usedto score the lattice to obtain a ranked list ofmmostlikely Chinese transliterations (m-best list).We measure transliteration accuracy as themean reciprocal rank (MRR) (Kantor andVoorhees, 2000).
If there is only one correctChinese transliteration of the k-th English wordand it is found at the rk-th position in the m-bestlist, its reciprocal rank is 1/rk.
If the list containsno correct transliterations, the reciprocal rank is0.
In case of multiple correct transliterations, wetake the one that gives the highest reciprocal rank.MRR is the average of the reciprocal ranks acrossall words in the test set.
It is commonly used asa measure of transliteration accuracy, and alsoallows us to make a direct comparison with otherreported work (Li et al, 2007b).We take m = 20 and measure MRR on Xinhuatest set for each alignment of Xinhua training setas described in Section 5.1.
We report MRR andthe alignment entropy in Figures 6a and 7a for theaffinity and phonological alignments respectively.The highest MRR we achieve is 0.771 for affin-ity alignments and 0.773 for phonological align-ments.
This is a significant improvement over theMRR of 0.708 reported in (Li et al, 2007b) on thesame data.
We also observe that the phonologicalalignment technique produces, on average, betteralignments than the affinity alignment techniquein terms of both the alignment entropy and MRR.We also report the MRR and F -scores for eachalignment in Figures 6b and 7b, from which weobserve that alignment entropy has stronger corre-lation with MRR than F -score does.
The Spear-man?s rank correlation coefficients are ?0.89 and?0.88 for data in Figure 6a and 7a respectively.This once again demonstrates the desired propertyof alignment entropy as an evaluation metric ofalignment.To validate our findings from Xinhua corpus,we further carry out experiments on the EC setof LDC05 containing 560,768 entries.
We splitthe set into 5 almost equal subsets for cross-validation: in each of 5 experiments one subset isused for testing and the remaining ones for train-ing.
Since LDC05 contains one-to-many English-Chinese transliteration pairs, we make sure that anEnglish name only appears in one subset.Note that the EC set of LDC05 containsmany names of non-English, and, generally, non-European origin.
This makes the G2P converterless accurate, as it is trained on an English pho-netic dictionary.
We therefore only apply the affin-ity alignment technique to align the EC set.
We141?????????????????????
???
???
??
?MRRAlignment?entropy(a) 80 affinity alignments?????????????????????
???
???
???
???
???
??
?MRRF?score(b) 80 affinity alignmentsFigure 6: Mean reciprocal ratio on Xinhua testset vs. alignment entropy and F -score for mod-els trained with different affinity alignments.use each iteration of the alignment in the translit-eration modeling and present the resulting MRRalong with alignment entropy in Figure 8.
TheMRR results are the averages of five values pro-duced in the five-fold cross-validations.We observe a clear correlation between thealignment entropy and transliteration accuracy ex-pressed by MRR on LDC05 corpus, similar to thaton Xinhua corpus, with the Spearman?s rank cor-relation coefficient of ?0.77.
We obtain the high-est average MRR of 0.720 on the EC set.5.3 Validating transliteration usingalignment measureTransliteration validation is a hypothesis test thatdecides whether a given transliteration pair is gen-uine or not.
Instead of using the lexicon fre-quency (Knight and Graehl, 1998) or Web statis-tics (Qu and Grefenstette, 2004), we propose vali-dating transliteration pairs according to the align-ment distance D between the aligned Englishgraphemes and Chinese phonemes (see equations(2) and (5)).
A distance function d(ei, cpk) isestablished from each alignment on the Xinhuatraining set as discussed in Section 5.2.An audit of LDC05 corpus groups the corpusinto three sets: an English-Chinese (EC) set of560,768 samples, an English-Japanese (EJ) setof 83,403 samples and the REST set of 29,219?????????????????????
???
???
??
?MRRAlignment?entropy(a) 114 phonological alignments?????????????????????
???
???
???
???
???
??
?MRRF?score(b) 114 phonological alignmentsFigure 7: Mean reciprocal ratio on Xinhua testset vs. alignment entropy and F -score for modelstrained with different phonological alignments.??????????????????????????????
???
???
?????????
??
?
?
?Figure 8: Mean reciprocal ratio vs. alignment en-tropy for alignments of EC set.samples that are not transliteration pairs.
Wemark the EC name pairs as genuine and the rest112,622 name pairs that do not follow the Chi-nese phonetic rules as false transliterations, thuscreating the ground truth labels for an English-Chinese transliteration validation experiment.
Inother words, LDC05 has 560,768 genuine translit-eration pairs and 112,622 false ones.We run one iteration of alignment over LDC05(both genuine and false) with the distance func-tion d(ei, cpk) derived from the affinity matrix ofone aligned Xinhua training set.
In this way, eachtransliteration pair in LDC05 provides an align-ment distance.
One can expect that a genuinetransliteration pair typically aligns well, leadingto a low distance, while a false transliteration pairwill do otherwise.
To remove the effect of wordlength, we normalize the distance by the Englishname length, the Chinese phonetic transcription142length, and the sum of both, producing score1,score2 and score3 respectively.Miss?probability?(%)False?alarm?probability?
(%)2 5101251012020score2EER:?4.48?%score1EER:?7.13?%score3EER:?4.80?%(a) DET with score1, score2,score3.1 2 5 1012510Miss?probability?(%)False?alarm?probability?
(%)Entropy:?2.396MRR:?0.773EER:?4.48?%Entropy:?2.529MRR:?0.764EER:?4.52%Entropy:?2.625MRR:?0.754EER:?4.70%(b) DET results vs. three differentalignment quality.Figure 9: Detection error tradeoff (DET) curvesfor transliteration validation on LDC05.We can now classify each LDC05 name pair asgenuine or false by having a hypothesis test.
Whenthe test score is lower than a pre-set threshold, thename pair is accepted as genuine, otherwise false.In this way, each pre-set threshold will present twotypes of errors, a false alarm and a miss-detectrate.
A common way to present such results is viathe detection error tradeoff (DET) curves, whichshow all possible decision points, and the equal er-ror rate (EER), when false alarm and miss-detectrates are equal.Figure 9a shows three DET curves based onscore1, score2 and score3 respectively for oneone alignment solution on the Xinhua training set.The horizontal axis is the probability of miss-detecting a genuine transliteration, while the verti-cal one is the probability of false-alarms.
It is clearthat out of the three, score2 gives the best results.We select the alignments of Xinhua trainingset that produce the highest and the lowest MRR.We also randomly select three other alignmentsthat produce different MRR values from the poolof 114 phonological and 80 affinity alignments.Xinhua trainset algnmentAlignment entropyof Xinhua train setMRR on Xinhuatest setLDCclassificationEER, %123452.396 0.773 4.482.529 0.764 4.522.586 0.761 4.512.621 0.757 4.712.625 0.754 4.70Table 1: Equal error ratio of LDC transliterationpair validation for different alignments of Xinhuatraining set.We use each alignment to derive distance func-tion d(ei, cpk).
Table 1 shows the EER of LDC05validation using score2, along with the alignmententropy of the Xinhua training set that derivesd(ei, cpk), and the MRR on Xinhua test set in thegenerative transliteration experiment (see Section5.2) for all 5 alignments.
To avoid cluttering Fig-ure 9b, we show the DET curves for alignments1, 2 and 5 only.
We observe that distance func-tion derived from better aligned Xinhua corpus,as measured by both our alignment entropy met-ric and MRR, leads to a higher validation accuracyconsistently on LDC05.6 ConclusionsWe conclude that the alignment entropy is a re-liable indicator of the alignment quality, as con-firmed by our experiments on both Xinhua andLDC corpora.
Alignment entropy does not re-quire the gold standard reference, it thus can beused to evaluate alignments of large transliterationcorpora and is possibly to give more reliable esti-mate of alignment quality than the F -score metricas shown in our transliteration experiment.The alignment quality of training corpus hasa significant impact on the transliteration mod-els.
We achieve the highest MRR of 0.773 onXinhua corpus with phonological alignment tech-nique, which represents a significant performancegain over other reported results.
Phonologicalalignment outperforms affinity alignment on cleandatabase.We propose using alignment distance to validatetransliterations.
A high quality alignment on asmall verified corpus such as Xinhua can be effec-tively used to validate a large noisy corpus, suchas LDC05.
We believe that this property would beuseful in transliteration extraction, cross-lingualinformation retrieval applications.143ReferencesNasreen AbdulJaleel and Leah S. Larkey.
2003.
Sta-tistical transliteration for English-Arabic cross lan-guage information retrieval.
In Proc.
ACM CIKM.Yaser Al-Onaizan and Kevin Knight.
2002.
Machinetransliteration of names in arabic text.
In Proc.
ACLWorkshop: Computational Apporaches to SemiticLanguages.Slaven Bilac and Hozumi Tanaka.
2004.
A hybridback-transliteration system for Japanese.
In Proc.COLING, pages 597?603.Lin L. Chase.
1997.
Error-responsive feedback mech-anisms for speech recognizers.
Ph.D. thesis, CMU.Asif Ekbal, Sudip Kumar Naskar, and Sivaji Bandy-opadhyay.
2006.
A modified joint source-channelmodel for transliteration.
In Proc.
COLING/ACL,pages 191?198Wei Gao, Kam-Fai Wong, and Wai Lam.
2004.Phoneme-based transliteration of foreign names forOOV problem.
In Proc.
IJCNLP, pages 374?381.Long Jiang, Ming Zhou, Lee-Feng Chien, and ChengNiu.
2007.
Named entity translation with web min-ing and transliteration.
In IJCAI, pages 1629?1634.Sung Young Jung, SungLim Hong, and Eunok Paek.2000.
An English to Korean transliteration model ofextended Markov window.
In Proc.
COLING, vol-ume 1.Paul.
B. Kantor and Ellen.
M. Voorhees.
2000.
TheTREC-5 confusion track: comparing retrieval meth-ods for scanned text.
Information Retrieval, 2:165?176.Brett Kessler.
2005.
Phonetic comparison algo-rithms.
Transactions of the Philological Society,103(2):243?260.Kevin Knight and Jonathan Graehl.
1998.
Machinetransliteration.
Computational Linguistics, 24(4).Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang.
2007.A phonetic similarity model for automatic extractionof transliteration pairs.
ACM Trans.
Asian LanguageInformation Processing, 6(2).Patrik Lambert.
2008.
Exploiting lexical informa-tion and discriminative alignment training in statis-tical machine translation.
Ph.D. thesis, UniversitatPolite`cnica de Catalunya, Barcelona, Spain.Kevin Lenzo.
1997. t2p: text-to-phoneme converterbuilder.
http://www.cs.cmu.edu/?lenzo/t2p/.Kevin Lenzo.
2008.
The CMU pronounc-ing dictionary.
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.Haizhou Li, Min Zhang, and Jian Su.
2004.
A jointsource-channel model for machine transliteration.In Proc.
ACL, pages 159?166.Haizhou Li, Bin Ma, and Chin-Hui Lee.
2007a.
Avector space modeling approach to spoken languageidentification.
IEEE Trans.
Acoust., Speech, SignalProcess., 15(1):271?284.Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and MinghuiDong.
2007b.
Semantic transliteration of personalnames.
In Proc.
ACL, pages 120?127.Linguistic Data Consortium.
2005.
LDC Chinese-English name entity lists LDC2005T34.Helen M. Meng, Wai-Kit Lo, Berlin Chen, and KarenTang.
2001.
Generate phonetic cognates to han-dle name entities in English-Chinese cross-languagespoken document retrieval.
In Proc.
ASRU.Rada Mihalcea and Ted Pedersen.
2003.
An evaluationexercise for word alignment.
In Proc.
HLT-NAACL,pages 1?10.Jong-Hoon Oh and Key-Sun Choi.
2002.
An English-Korean transliteration model using pronunciationand contextual rules.
In Proc.
COLING 2002.Jong-Hoon Oh and Key-Sun Choi.
2005.
Machinelearning based english-to-korean transliteration us-ing grapheme and phoneme information.
IEICETrans.
Information and Systems, E88-D(7):1737?1748.Yan Qu and Gregory Grefenstette.
2004.
Finding ideo-graphic representations of Japanese names written inLatin script via language identification and corpusvalidation.
In Proc.
ACL, pages 183?190.Tao Tao, Su-Youn Yoon, Andrew Fisterd, RichardSproat, and ChengXiang Zhai.
2006.
Unsupervisednamed entity transliteration using temporal and pho-netic correlation.
In Proc.
EMNLP, pages 250?257.Paola Virga and Sanjeev Khudanpur.
2003.
Translit-eration of proper names in cross-lingual informationretrieval.
In Proc.
ACL MLNER.Stephen Wan and Cornelia Maria Verspoor.
1998.
Au-tomatic English-Chinese name transliteration for de-velopment of multilingual resources.
In Proc.
COL-ING, pages 1352?1356.M.
M. Withgott and F. R. Chen.
1993.
Computationalmodels of American speech.
Centre for the study oflanguage and information.Xinhua News Agency.
1992.
Chinese transliterationof foreign personal names.
The Commercial Press.LiLi Xu, Atsushi Fujii, and Tetsuya Ishikawa.
2006.Modeling impression in probabilistic transliterationinto Chinese.
In Proc.
EMNLP, pages 242?249.Su-Youn Yoon, Kyoung-Young Kim, and RichardSproat.
2007.
Multilingual transliteration using fea-ture based phonetic method.
In Proc.
ACL, pages112?119.144
