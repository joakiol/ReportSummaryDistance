Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 316?323,Sydney, July 2006. c?2006 Association for Computational LinguisticsDiscriminative Classifiers for Deterministic Dependency ParsingJohan HallVa?xjo?
Universityjni@msi.vxu.seJoakim NivreVa?xjo?
University andUppsala Universitynivre@msi.vxu.seJens NilssonVa?xjo?
Universityjha@msi.vxu.seAbstractDeterministic parsing guided by treebank-induced classifiers has emerged as asimple and efficient alternative to morecomplex models for data-driven parsing.We present a systematic comparison ofmemory-based learning (MBL) and sup-port vector machines (SVM) for inducingclassifiers for deterministic dependencyparsing, using data from Chinese, Englishand Swedish, together with a variety ofdifferent feature models.
The comparisonshows that SVM gives higher accuracy forrichly articulated feature models across alllanguages, albeit with considerably longertraining times.
The results also confirmthat classifier-based deterministic parsingcan achieve parsing accuracy very close tothe best results reported for more complexparsing models.1 IntroductionMainstream approaches in statistical parsing arebased on nondeterministic parsing techniques,usually employing some kind of dynamic pro-gramming, in combination with generative prob-abilistic models that provide an n-best ranking ofthe set of candidate analyses derived by the parser(Collins, 1997; Collins, 1999; Charniak, 2000).These parsers can be enhanced by using a discrim-inative model, which reranks the analyses out-put by the parser (Johnson et al, 1999; Collinsand Duffy, 2005; Charniak and Johnson, 2005).Alternatively, discriminative models can be usedto search the complete space of possible parses(Taskar et al, 2004; McDonald et al, 2005).A radically different approach is to performdisambiguation deterministically, using a greedyparsing algorithm that approximates a globally op-timal solution by making a sequence of locallyoptimal choices, guided by a classifier trained ongold standard derivations from a treebank.
Thismethodology has emerged as an alternative tomore complex models, especially in dependency-based parsing.
It was first used for unlabeled de-pendency parsing by Kudo and Matsumoto (2002)(for Japanese) and Yamada and Matsumoto (2003)(for English).
It was extended to labeled depen-dency parsing by Nivre et al (2004) (for Swedish)and Nivre and Scholz (2004) (for English).
Morerecently, it has been applied with good results tolexicalized phrase structure parsing by Sagae andLavie (2005).The machine learning methods used to induceclassifiers for deterministic parsing are dominatedby two approaches.
Support vector machines(SVM), which combine the maximum marginstrategy introduced by Vapnik (1995) with the useof kernel functions to map the original featurespace to a higher-dimensional space, have beenused by Kudo and Matsumoto (2002), Yamada andMatsumoto (2003), and Sagae and Lavie (2005),among others.
Memory-based learning (MBL),which is based on the idea that learning is thesimple storage of experiences in memory and thatsolving a new problem is achieved by reusing so-lutions from similar previously solved problems(Daelemans and Van den Bosch, 2005), has beenused primarily by Nivre et al (2004), Nivre andScholz (2004), and Sagae and Lavie (2005).Comparative studies of learning algorithms arerelatively rare.
Cheng et al (2005b) report thatSVM outperforms MaxEnt models in Chinese de-pendency parsing, using the algorithms of Yamadaand Matsumoto (2003) and Nivre (2003), whileSagae and Lavie (2005) find that SVM gives better316performance than MBL in a constituency-basedshift-reduce parser for English.In this paper, we present a detailed comparisonof SVM and MBL for dependency parsing usingthe deterministic algorithm of Nivre (2003).
Thecomparison is based on data from three differentlanguages ?
Chinese, English, and Swedish ?
andon five different feature models of varying com-plexity, with a separate optimization of learningalgorithm parameters for each combination of lan-guage and feature model.
The central importanceof feature selection and parameter optimization inmachine learning research has been shown veryclearly in recent research (Daelemans and Hoste,2002; Daelemans et al, 2003).The rest of the paper is structured as follows.Section 2 presents the parsing framework, includ-ing the deterministic parsing algorithm and thehistory-based feature models.
Section 3 discussesthe two learning algorithms used in the experi-ments, and section 4 describes the experimentalsetup, including data sets, feature models, learn-ing algorithm parameters, and evaluation metrics.Experimental results are presented and discussedin section 5, and conclusions in section 6.2 Inductive Dependency ParsingThe system we use for the experiments uses nogrammar but relies completely on inductive learn-ing from treebank data.
The methodology is basedon three essential components:1.
Deterministic parsing algorithms for buildingdependency graphs (Kudo and Matsumoto,2002; Yamada and Matsumoto, 2003; Nivre,2003)2.
History-based models for predicting the nextparser action (Black et al, 1992; Magerman,1995; Ratnaparkhi, 1997; Collins, 1999)3.
Discriminative learning to map histories toparser actions (Kudo and Matsumoto, 2002;Yamada and Matsumoto, 2003; Nivre et al,2004)In this section we will define dependency graphs,describe the parsing algorithm used in the experi-ments and finally explain the extraction of featuresfor the history-based models.2.1 Dependency GraphsA dependency graph is a labeled directed graph,the nodes of which are indices corresponding tothe tokens of a sentence.
Formally:Definition 1 Given a set R of dependency types(arc labels), a dependency graph for a sentencex = (w1, .
.
.
, wn) is a labeled directed graphG = (V,E, L), where:1.
V = Zn+12.
E ?
V ?
V3.
L : E ?
RThe set V of nodes (or vertices) is the set Zn+1 ={0, 1, 2, .
.
.
, n} (n ?
Z+), i.e., the set of non-negative integers up to and including n. Thismeans that every token index i of the sentence is anode (1 ?
i ?
n) and that there is a special node0, which does not correspond to any token of thesentence and which will always be a root of thedependency graph (normally the only root).
Weuse V + to denote the set of nodes correspondingto tokens (i.e., V + = V ?
{0}), and we use theterm token node for members of V +.The set E of arcs (or edges) is a set of orderedpairs (i, j), where i and j are nodes.
Since arcs areused to represent dependency relations, we willsay that i is the head and j is the dependent ofthe arc (i, j).
As usual, we will use the notationi ?
j to mean that there is an arc connecting iand j (i.e., (i, j) ?
E) and we will use the nota-tion i ??
j for the reflexive and transitive closureof the arc relation E (i.e., i ??
j if and only ifi = j or there is a path of arcs connecting i to j).The function L assigns a dependency type (arclabel) r ?
R to every arc e ?
E.Definition 2 A dependency graph G is well-formed if and only if:1.
The node 0 is a root.2.
Every node has in-degree at most 1.3.
G is connected.14.
G is acyclic.5.
G is projective.2Conditions 1?4, which are more or less standard independency parsing, together entail that the graphis a rooted tree.
The condition of projectivity, bycontrast, is somewhat controversial, since the anal-ysis of certain linguistic constructions appears to1To be more exact, we require G to be weakly connected,which entails that the corresponding undirected graph is con-nected, whereas a strongly connected graph has a directedpath between any pair of nodes.2An arc (i, j) is projective iff there is a path from i toevery node k such that i < j < k or i > j > k. A graph Gis projective if all its arcs are projective.317JJEconomic ?NMODNNnews ?SBJVBhadJJlittle ?NMODNNeffect ?OBJINon ?NMODJJfinancial ?NMODNNmarkets ?PMOD..? PFigure 1: Dependency graph for an English sentence from the WSJ section of the Penn Treebankrequire non-projective dependency arcs.
For thepurpose of this paper, however, this assumption isunproblematic, given that all the treebanks used inthe experiments are restricted to projective depen-dency graphs.Figure 1 shows a well-formed dependencygraph for an English sentence, where each wordof the sentence is tagged with its part-of-speechand each arc labeled with a dependency type.2.2 Parsing AlgorithmWe begin by defining parser configurations and theabstract data structures needed for the definition ofhistory-based feature models.Definition 3 Given a set R = {r0, r1, .
.
.
rm}of dependency types and a sentence x =(w1, .
.
.
, wn), a parser configuration for x is aquadruple c = (?, ?, h, d), where:1. ?
is a stack of tokens nodes.2.
?
is a sequence of token nodes.3.
h : V +x ?
V is a function from token nodesto nodes.4.
d : V +x ?
R is a function from token nodesto dependency types.5.
For every token node i ?
V +x , h(i) = 0 ifand only if d(i) = r0.The idea is that the sequence ?
represents the re-maining input tokens in a left-to-right pass overthe input sentence x; the stack ?
contains partiallyprocessed nodes that are still candidates for de-pendency arcs, either as heads or dependents; andthe functions h and d represent a (dynamically de-fined) dependency graph for the input sentence x.We refer to the token node on top of the stack asthe top token and the first token node of the inputsequence as the next token.When parsing a sentence x = (w1, .
.
.
, wn),the parser is initialized to a configuration c0 =(?, (1, .
.
.
, n), h0, d0) with an empty stack, withall the token nodes in the input sequence, and withall token nodes attached to the special root node0 with a special dependency type r0.
The parserterminates in any configuration cm = (?, ?, h, d)where the input sequence is empty, which happensafter one left-to-right pass over the input.There are four possible parser transitions, twoof which are parameterized for a dependency typer ?
R.1.
LEFT-ARC(r) makes the top token i a (left)dependent of the next token j with depen-dency type r, i.e., j r?
i, and immediatelypops the stack.2.
RIGHT-ARC(r) makes the next token j a(right) dependent of the top token i with de-pendency type r, i.e., i r?
j, and immediatelypushes j onto the stack.3.
REDUCE pops the stack.4.
SHIFT pushes the next token i onto the stack.The choice between different transitions is nonde-terministic in the general case and is resolved by aclassifier induced from a treebank, using featuresextracted from the parser configuration.2.3 Feature ModelsThe task of the classifier is to predict the nexttransition given the current parser configuration,where the configuration is represented by a fea-ture vector ?
(1,p) = (?1, .
.
.
, ?p).
Each feature ?iis a function of the current configuration, definedin terms of an address function a?i , which identi-fies a specific token in the current parser configu-ration, and an attribute function f?i , which picksout a specific attribute of the token.Definition 4 Let c = (?, ?, h, d) be the currentparser configuration.1.
For every i (i ?
0), ?i and ?i are addressfunctions identifying the ith token of ?
and?
, respectively (with indexing starting at 0).3182.
If ?
is an address function, then h(?
), l(?
),and r(?)
are address functions, identifyingthe head (h), the leftmost child (l), and therightmost child (r), of the token identified by?
(according to the function h).3.
If ?
is an address function, then p(?
), w(?
)and d(?)
are feature functions, identifyingthe part-of-speech (p), word form (w) and de-pendency type (d) of the token identified by?.
We call p, w and d attribute functions.A feature model is defined by specifying a vectorof feature functions.
In section 4.2 we will definethe feature models used in the experiments.3 Learning AlgorithmsThe learning problem for inductive dependencyparsing, defined in the preceding section, is a pureclassification problem, where the input instancesare parser configurations, represented by featurevectors, and the output classes are parser transi-tions.
In this section, we introduce the two ma-chine learning methods used to solve this problemin the experiments.3.1 MBLMBL is a lazy learning method, based on the ideathat learning is the simple storage of experiencesin memory and that solving a new problem isachieved by reusing solutions from similar previ-ously solved problems (Daelemans and Van denBosch, 2005).
In essence, this is a k nearest neigh-bor approach to classification, although a vari-ety of sophisticated techniques, including differentdistance metrics and feature weighting schemescan be used to improve classification accuracy.For the experiments reported in this paper weuse the TIMBL software package for memory-based learning and classification (Daelemans andVan den Bosch, 2005), which directly handlesmulti-valued symbolic features.
Based on resultsfrom previous optimization experiments (Nivre etal., 2004), we use the modified value differencemetric (MVDM) to determine distances betweeninstances, and distance-weighted class voting fordetermining the class of a new instance.
The para-meters varied during experiments are the numberk of nearest neighbors and the frequency thresholdl below which MVDM is replaced by the simpleOverlap metric.3.2 SVMSVM in its simplest form is a binary classifierthat tries to separate positive and negative cases intraining data by a hyperplane using a linear kernelfunction.
The goal is to find the hyperplane thatseparates the training data into two classes withthe largest margin.
By using other kernel func-tions, such as polynomial or radial basis function(RBF), feature vectors are mapped into a higherdimensional space (Vapnik, 1998; Kudo and Mat-sumoto, 2001).
Multi-class classification withn classes can be handled by the one-versus-allmethod, with n classifiers that each separate oneclass from the rest, or the one-versus-one method,with n(n ?
1)/2 classifiers, one for each pair ofclasses (Vural and Dy, 2004).
SVM requires allfeatures to be numerical, which means that sym-bolic features have to be converted, normally byintroducing one binary feature for each value ofthe symbolic feature.For the experiments reported in this paperwe use the LIBSVM library (Wu et al, 2004;Chang and Lin, 2005) with the polynomial kernelK(xi, xj) = (?xTi xj +r)d, ?
> 0, where d, ?
andr are kernel parameters.
Other parameters that arevaried in experiments are the penalty parameter C,which defines the tradeoff between training errorand the magnitude of the margin, and the termina-tion criterion ?, which determines the tolerance oftraining errors.We adopt the standard method for convertingsymbolic features to numerical features by bina-rization, and we use the one-versus-one strategyfor multi-class classification.
However, to reducetraining times, we divide the training data intosmaller sets, according to the part-of-speech ofthe next token in the current parser configuration,and train one set of classifiers for each smallerset.
Similar techniques have previously been usedby Yamada and Matsumoto (2003), among others,without significant loss of accuracy.
In order toavoid too small training sets, we pool together allparts-of-speech that have a frequency below a cer-tain threshold t (set to 1000 in all the experiments).4 Experimental SetupIn this section, we describe the experimental setup,including data sets, feature models, parameter op-timization, and evaluation metrics.
Experimentalresults are presented in section 5.3194.1 Data SetsThe data set used for Swedish comes from Tal-banken (Einarsson, 1976), which contains bothwritten and spoken Swedish.
In the experiments,the professional prose section is used, consistingof about 100k words taken from newspapers, text-books and information brochures.
The data hasbeen manually annotated with a combination ofconstituent structure, dependency structure, andtopological fields (Teleman, 1974).
This annota-tion has been converted to dependency graphs andthe original fine-grained classification of gram-matical functions has been reduced to 17 depen-dency types.
We use a pseudo-randomized datasplit, dividing the data into 10 sections by allocat-ing sentence i to section i mod 10.
Sections 1?9are used for 9-fold cross-validation during devel-opment and section 0 for final evaluation.The English data are from the Wall Street Jour-nal section of the Penn Treebank II (Marcus et al,1994).
We use sections 2?21 for training, sec-tion 0 for development, and section 23 for thefinal evaluation.
The head percolation table ofYamada and Matsumoto (2003) has been usedto convert constituent structures to dependencygraphs, and a variation of the scheme employedby Collins (1999) has been used to construct arclabels that can be mapped to a set of 12 depen-dency types.The Chinese data are taken from the Penn Chi-nese Treebank (CTB) version 5.1 (Xue et al,2005), consisting of about 500k words mostlyfrom Xinhua newswire, Sinorama news magazineand Hong Kong News.
CTB is annotated witha combination of constituent structure and gram-matical functions in the Penn Treebank style, andhas been converted to dependency graphs using es-sentially the same method as for the English data,although with a different head percolation tableand mapping scheme.
We use the same kind ofpseudo-randomized data split as for Swedish, butwe use section 9 as the development test set (train-ing on section 1?8) and section 0 as the final testset (training on section 1?9).A standard HMM part-of-speech tagger withsuffix smoothing has been used to tag the test datawith an accuracy of 96.5% for English and 95.1%for Swedish.
For the Chinese experiments we haveused the original (gold standard) tags from thetreebank, to facilitate comparison with results pre-viously reported in the literature.Feature ?1 ?2 ?3 ?4 ?5p(?0) + + + + +p(?0) + + + + +p(?1) + + + + +p(?2) + +p(?3) + +p(?1) +d(?0) + + + +d(l(?0)) + + + +d(r(?0)) + + + +d(l(?0)) + + + +w(?0) + + +w(?0) + + +w(?1) +w(h(?0)) +Table 1: Feature models4.2 Feature ModelsTable 1 describes the five feature models ?1?
?5used in the experiments, with features specifiedin column 1 using the functional notation definedin section 2.3.
Thus, p(?0) refers to the part-of-speech of the top token, while d(l(?0)) picks outthe dependency type of the leftmost child of thenext token.
It is worth noting that models ?1?
?2are unlexicalized, since they do not contain anyfeatures of the form w(?
), while models ?3?
?5are all lexicalized to different degrees.4.3 OptimizationAs already noted, optimization of learning algo-rithm parameters is a prerequisite for meaningfulcomparison of different algorithms, although anexhaustive search of the parameter space is usu-ally impossible in practice.For MBL we have used the modified valuedifference metric (MVDM) and class votingweighted by inverse distance (ID) in all experi-ments, and performed a grid search for the op-timal values of the number k of nearest neigh-bors and the frequency threshold l for switchingfrom MVDM to the simple Overlap metric (cf.section 3.1).
The best values are different for dif-ferent combinations of data sets and models butare generally found in the range 3?10 for k and inthe range 1?8 for l.The polynomial kernel of degree 2 has beenused for all the SVM experiments, but the kernelparameters ?
and r have been optimized togetherwith the penalty parameter C and the termination320Swedish English ChineseFM LM AS EM AS EM AS EMU L U L U L U L U L U L?1 MBL 75.3 68.7 16.0 11.4 *76.5 73.7 9.8 7.7 66.4 63.6 14.3 12.1SVM 75.4 68.9 16.3 12.1 76.4 73.6 9.8 7.7 66.4 63.6 14.2 12.1?2 MBL 81.9 74.4 31.4 19.8 81.2 78.2 19.8 14.9 73.0 70.7 22.6 18.8SVM *83.1 *76.3 *34.3 *24.0 81.3 78.3 19.4 14.9 *73.2 *71.0 22.1 18.6?3 MBL 85.9 81.4 37.9 28.9 85.5 83.7 26.5 23.7 77.9 76.3 26.3 23.4SVM 86.2 *82.6 38.7 *32.5 *86.4 *84.8 *28.5 *25.9 *79.7 *78.3 *30.1 *25.9?4 MBL 86.1 82.1 37.6 30.1 87.0 85.2 29.8 26.0 79.4 77.7 28.0 24.7SVM 86.0 82.2 37.9 31.2 *88.4 *86.8 *33.2 *30.3 *81.7 *80.1 *31.0 *27.0?5 MBL 86.6 82.3 39.9 29.9 88.0 86.2 32.8 28.4 81.1 79.2 30.2 25.9SVM 86.9 *83.2 40.7 *33.7 *89.4 *87.9 *36.4 *33.1 *84.3 *82.7 *34.5 *30.5Table 2: Parsing accuracy; FM: feature model; LM: learning method; AS: attachment score, EM: exactmatch; U: unlabeled, L: labeledcriterion e. The intervals for the parameters are:?
: 0.16?0.40; r: 0?0.6; C: 0.5?1.0; e: 0.1?1.0.4.4 Evaluation MetricsThe evaluation metrics used for parsing accuracyare the unlabeled attachment score ASU , which isthe proportion of tokens that are assigned the cor-rect head (regardless of dependency type), and thelabeled attachment score ASL, which is the pro-portion of tokens that are assigned the correct headand the correct dependency type.
We also considerthe unlabeled exact match EMU , which is the pro-portion of sentences that are assigned a completelycorrect dependency graph without considering de-pendency type labels, and the labeled exact matchEML, which also takes dependency type labelsinto account.
Attachment scores are presented asmean scores per token, and punctuation tokens areexcluded from all counts.
For all experiments wehave performed a McNemar test of significance at?
= 0.01 for differences between the two learningmethods.
We also compare learning and parsingtimes, as measured on an AMD 64-bit processorrunning Linux.5 Results and DiscussionTable 2 shows the parsing accuracy for the com-bination of three languages (Swedish, English andChinese), two learning methods (MBL and SVM)and five feature models (?1?
?5), with algorithmparameters optimized as described in section 4.3.For each combination, we measure the attachmentscore (AS) and the exact match (EM).
A signif-icant improvement for one learning method overthe other is marked by an asterisk (*).Independently of language and learningmethod, the most complex feature model ?5gives the highest accuracy across all metrics.
Notsurprisingly, the lowest accuracy is obtained withthe simplest feature model ?1.
By and large, morecomplex feature models give higher accuracy,with one exception for Swedish and the featuremodels ?3 and ?4.
It is significant in this contextthat the Swedish data set is the smallest of thethree (about 20% of the Chinese data set andabout 10% of the English one).If we compare MBL and SVM, we see thatSVM outperforms MBL for the three most com-plex models ?3, ?4 and ?5, both for English andChinese.
The results for Swedish are less clear,although the labeled accuracy for ?3 and ?5 aresignificantly better.
For the ?1 model there is nosignificant improvement using SVM.
In fact, thesmall differences found in the ASU scores are tothe advantage of MBL.
By contrast, there is a largegap between MBL and SVM for the model ?5 andthe languages Chinese and English.
For Swedish,the differences are much smaller (except for theEML score), which may be due to the smaller sizeof the Swedish data set in combination with thetechnique of dividing the training data for SVM(cf.
section 3.2).Another important factor when comparing twolearning methods is the efficiency in terms of time.Table 3 reports learning and parsing time for thethree languages and the five feature models.
Thelearning time correlates very well with the com-plexity of the feature model and MBL, being a lazylearning method, is much faster than SVM.
For theunlexicalized feature models ?1 and ?2, the pars-ing time is also considerably lower for MBL, espe-cially for the large data sets (English and Chinese).But as model complexity grows, especially withthe addition of lexical features, SVM graduallygains an advantage over MBL with respect to pars-ing time.
This is especially striking for Swedish,321Method Model Swedish English ChineseLT PT LT PT LT PT?1 MBL 1 s 2 s 16 s 26 s 7 s 8 sSVM 40 s 14 s 1.5 h 14 min 1.5 h 17 min?2 MBL 3 s 5 s 35 s 32 s 13 s 14 sSVM 40 s 13 s 1 h 11 min 1.5 h 15 min?3 MBL 6 s 1 min 1.5 min 9.5 min 46 s 10 minSVM 1 min 15 s 1 h 9 min 2 h 16 min?4 MBL 8 s 2 min 1.5 min 9 min 45 s 12 minSVM 2 min 18 s 2 h 12 min 2.5 h 14 min?5 MBL 10 s 7 min 3 min 41 min 1.5 min 46 minSVM 2 min 25 s 1.5 h 10 min 6 h 24 minTable 3: Time efficiency; LT: learning time, PT: parsing timewhere the training data set is considerably smallerthan for the other languages.Compared to the state of the art in dependencyparsing, the unlabeled attachment scores obtainedfor Swedish with model ?5, for both MBL andSVM, are about 1 percentage point higher than theresults reported for MBL by Nivre et al (2004).For the English data, the result for SVM withmodel ?5 is about 3 percentage points below theresults obtained with the parser of Charniak (2000)and reported by Yamada and Matsumoto (2003).For Chinese, finally, the accuracy for SVM withmodel ?5 is about one percentage point lower thanthe best reported results, achieved with a deter-ministic classifier-based approach using SVM andpreprocessing to detect root nodes (Cheng et al,2005a), although these results are not based onexactly the same dependency conversion and datasplit as ours.6 ConclusionWe have performed an empirical comparison ofMBL (TIMBL) and SVM (LIBSVM) as learningmethods for classifier-based deterministic depen-dency parsing, using data from three languagesand feature models of varying complexity.
Theevaluation shows that SVM gives higher parsingaccuracy and comparable or better parsing effi-ciency for complex, lexicalized feature modelsacross all languages, whereas MBL is superiorwith respect to training efficiency, even if trainingdata is divided into smaller sets for SVM.
The bestaccuracy obtained for SVM is close to the state ofthe art for all languages involved.AcknowledgementsThe work presented in this paper was partially sup-ported by the Swedish Research Council.
We aregrateful to Hiroyasu Yamada and Yuan Ding forsharing their head percolation tables for Englishand Chinese, respectively, and to three anonymousreviewers for helpful comments and suggestions.ReferencesEzra Black, Frederick Jelinek, John D. Lafferty,David M. Magerman, Robert L. Mercer, and SalimRoukos.
1992.
Towards history-based grammars:Using richer models for probabilistic parsing.
InProceedings of the 5th DARPA Speech and NaturalLanguage Workshop, pages 31?37.Chih-Chung Chang and Chih-Jen Lin.
2005.
LIB-SVM: A library for support vector machines.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 173?180.Eugene Charniak.
2000.
A Maximum-Entropy-Inspired Parser.
In Proceedings of the First AnnualMeeting of the North American Chapter of the As-sociation for Computational Linguistics (NAACL),pages 132?139.Yuchang Cheng, Masayuki Asahara, and Yuji Mat-sumoto.
2005a.
Chinese deterministic dependencyanalyzer: Examining effects of global features androot node finder.
In Proceedings of the FourthSIGHAN Workshop on Chinese Language Process-ing, pages 17?24.Yuchang Cheng, Masayuki Asahara, and Yuji Mat-sumoto.
2005b.
Machine learning-based depen-dency analyzer for Chinese.
In Proceedings ofthe International Conference on Chinese Computing(ICCC).Michael Collins and Nigel Duffy.
2005.
Discrimina-tive reranking for natural language parsing.
Compu-tational Linguistics, 31:25?70.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 16?23.322Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Walter Daelemans and Veronique Hoste.
2002.
Eval-uation of machine learning methods for natural lan-guage processing tasks.
In Proceedings of the ThirdInternational Conference on Language Resourcesand Evaluation (LREC), pages 755?760.Walter Daelemans and Antal Van den Bosch.
2005.Memory-Based Language Processing.
CambridgeUniversity Press.Walter Daelemans, Veronique Hoste, Fien De Meulder,and Bart Naudts.
2003.
Combined optimization offeature selection and algorithm parameter interac-tion in machine learning of language.
In Proceed-ings of the 14th European Conference on MachineLearning (ECML), pages 84?95.Jan Einarsson.
1976.
Talbankens skrift-spra?kskonkordans.
Lund University, Department ofScandinavian Languages.Mark Johnson, Stuart Geman, Steven Canon, ZhiyiChi, and Stefan Riezler.
1999.
Estimators forstochastic ?unification-based?
grammars.
In Pro-ceedings of the 37th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages535?541.Taku Kudo and Yuji Matsumoto.
2001.
Chunkingwith support vector machines.
In Proceedings ofthe Second Meeting of the North American Chap-ter of the Association for Computational Linguistics(NAACL).Taku Kudo and Yuji Matsumoto.
2002.
Japanese de-pendency analysis using cascaded chunking.
In Pro-ceedings of the Sixth Workshop on ComputationalLanguage Learning (CoNLL), pages 63?69.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proceedings of the 33rd An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 276?283.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, Robert MacIntyre, Ann Bies, MarkFerguson, Karen Katz, and Britta Schasberger.1994.
The Penn Treebank: Annotating predicate-argument structure.
In Proceedings of the ARPA Hu-man Language Technology Workshop, pages 114?119.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 91?98.Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of English text.
In Proceedingsof the 20th International Conference on Computa-tional Linguistics (COLING), pages 64?70.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-based dependency parsing.
In Proceed-ings of the 8th Conference on Computational Nat-ural Language Learning (CoNLL), pages 49?56.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing Technologies(IWPT), pages 149?160.Adwait Ratnaparkhi.
1997.
A linear observed timestatistical parser based on maximum entropy mod-els.
In Proceedings of the Second Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 1?10.Kenji Sagae and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proceed-ings of the 9th International Workshop on ParsingTechnologies (IWPT), pages 125?132.Ben Taskar, Dan Klein, Michael Collins, DaphneKoller, and Christopher Manning.
2004.
Max-margin parsing.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP), pages 1?8.Ulf Teleman.
1974.
Manual fo?r grammatisk beskriv-ning av talad och skriven svenska.
Studentlitteratur.Vladimir Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer.Vladimir Vapnik.
1998.
Statistical Learning Theory.John Wiley and Sons, New York.Volkan Vural and Jennifer G. Dy.
2004.
A hierarchi-cal method for multi-class support vector machines.ACM International Conference Proceeding Series,69:105?113.Ting-Fan Wu, Chih-Jen Lin, and Ruby C. Weng.
2004.Probability estimates for multi-class classificationby pairwise coupling.
Journal of Machine LearningResearch, 5:975?1005.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese Treebank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proceedings of the 8th InternationalWorkshop on Parsing Technologies (IWPT), pages195?206.323
