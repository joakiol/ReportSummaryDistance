A Simple but Powerful Automatic Term Extraction MethodHiroshi NakagawaInformation Technology Center,The University of Tokyo7-3-1, Bunkyo, HongoTokyo, Japan, 113-0033nakagawa@r.dl.itc.u-tokyo.ac.jpTatsunori MoriYokohama National University79-5, Tokiwadai,HodogayaYokohama, Japan,240-0085mori@forest.dnj.ynu.ac.jpAbstractIn this paper, we propose a new idea forthe automatic recognition of domainspecific terms.
Our idea is based on thestatistics between a compound noun andits component single-nouns.
Moreprecisely, we focus basically on howmany nouns adjoin the noun in questionto form compound nouns.
We proposeseveral scoring methods based on thisidea and experimentally evaluate them onthe NTCIR1 TMREC test collection.
Theresults are very promising especially inthe low recall area.IntroductionAutomatic term recognition, ATR in short,aims at extracting domain specific termsfrom a corpus of a certain academic ortechnical domain.
The majority of domainspecific terms are compound nouns, inother words, uninterrupted collocations.85% of domain specific terms are said tobe compound nouns.
They includesingle-nouns of the remaining 15% veryfrequently as their components, where?single-noun?
means a noun which couldnot be further divided into severalshorter and more basic nouns.
In otherwords, the majority of compound nounsconsist of the much smaller number ofthe remaining 15% single-noun termsand other single-nouns.
In this situation,it is natural to pay attention to therelation among single-nouns andcompound nouns, especially howsingle-noun terms contribute to make upcompound noun terms.Another important feature of domainspecific terms is termhood proposed in(Kageura & Umino 96) where ?termhood?refers to the degree that a linguistic unitis related to a domain-specific concept.Thus, what we really have to pursue is anATR method which directly uses thenotion of termhood.Considering these factors, the way ofmaking up compound nouns must beheavily related to the termhood of thecompound nouns.
The first reason is thattermhood is usually calculated based onterm frequency and bias of termfrequency like inverse documentfrequency.
Even though thesecalculations give a good approximation oftermhood, still they are not directlyrelated to termhood because thesecalculations are based on superficialstatistics.
That means that they are notnecessarily meanings in a writer's mindbut meanings in actual use.
Apparently,termhood is intended to reflect this typeof meaning.
The second reason is that if acertain single-noun, say N, expresses thekey concept of a domain that thedocument treats, the writer of thedocument must be using N not onlyfrequently but also in various ways.
Forinstance, he/she composes quite a fewcompound nouns using N and uses thesecompound nouns in documents he/shewrites.
Thus, we focus on the relationamong single-nouns and compound nounsin pursuing new ATR methods.The first attempt to make use of thisrelation has been done by (Nakagawa &Mori 98) through the number of distinctsingle-nouns that come to the left or rightof a single-noun term when used incompound noun terms.
Using this type ofnumber associated with a single-nounterm, Nakagawa and Mori proposed ascoring function for term candidates.Their term extraction method however isjust one example of employing therelation among single-nouns andcompound nouns.
Note that thisrelation is essentially based on a nounbigram.
In this paper, we expand therelation based on noun bigrams thatmight be the components of longercompound nouns.
Then weexperimentally evaluate the power ofseveral variations of scoring functionsbased on the noun bigram relation usingthe NTCIR1 TMREC test collection.
Bythis experimental clarification, we couldconclude that the single-noun term?spower of generating compound nounterms is useful and essential in ATR.In this paper, section 1 gives thebackground of ATR methods.
Section 2describes the proposed method of thenoun bigram based scoring function forterm extraction.
Section 3 describes theexperimental results and discusses them.1 Background1.1 Candidates ExtractionThe first thing to do in ATR is to extractterm candidates from the given textcorpus.
Here we only focus on nouns,more precisely a single-noun and acompound noun, which are exactly thetargets of the NTCIR1 TMRECtask(Kageura et al1999).
To extractcompound nouns which are promisingterm candidates and at the same time toexclude undesirable strings such as ?is a?or ?of the?, the frequently used method isto filter out the words that are membersof a stop-word-list.
More complexstructures like noun phrases, collocationsand so on, become focused on (Frantziand Ananiadou 1996).
All of these aregood term candidates in a corpus of aspecific domain because all of them havea strong unithood (Kageura&Umino96)which refers to the degree of strength orstability of syntagmatic combinations orcollocations.
We assume the followingabout compound nouns or collocations:Assumption   Terms having complexstructure a e t  be made of xistingsimple termsr o eThe structure of complex terms isanother important factor for automaticterm candidates extraction.
It isexpressed syntactically or semantically.As a syntactic structure, dependencystructures that are the results of parsingare focused on in many works.
Since wefocus on these complex structures, thefirst task in extracting term candidates isa morphological analysis including partof speech (POS) tagging.
For Japanese,which is an agglutinative language, amorphological analysis was carried outwhich segmented words from a sentenceand did POS tagging (Matsumoto et al1996).After POS tagging, the complexstructures mentioned above are extractedas term candidates.
Previous studieshave proposed many promising ways forthis purpose, Hisamitsu(2000) andNakagawa (1998) concentrated theirefforts on compound nouns.
Frantzi andAnaniadou (1996) tried to treat moregeneral structures like collocations.1.2 ScoringThe next thing to do is to assign a score toeach term candidate in order to rankthem in descending order of termhood.Many researchers have sought thedefinition of the term candidate?s scorewhich approximates termhood.
In fact,many of those proposals make use ofsurface statistics like tf?idf.
Ananiadou etal.
proposed C-value (Frantzi andAnaniadou 1996) and NC-value (Frantziand Ananiadou 1999) which count howindependently the given compound nounis used in the given corpus.
Hisamitsu(2000) propose a way to measuretermhood that counts how far the giventerm is different from the distribution ofnon-domain-specific terms.
All of themtried to capture how important andindependent a writer regards and usesindividual terms in a corpus2 Single-Noun Bigrams as Components ofCompound Nouns2.1 Single-Noun BigramsThe relation between a single-noun andcomplex nouns that include thissingle-noun is very important.Nevertheless, to our knowledge, thisrelation has not been paid enoughattention so far.
Nakagawa and Mori(1998) proposed a term scoring methodthat utilizes this type of relation.
In thispaper, we extend our ideacomprehensively.
Here we focus oncompound nouns among the various typesof complex terms.
In technical documents,the majority of domain-specific terms arenoun phrases or compound nounsconsisting of a small number of singlenouns.
Considering this observation, wepropose a new scoring method thatmeasures the importance of eachsingle-noun.
In a nutshell, this scoringmethod for a single-noun measures howmany distinct compound nouns contain aparticular single-noun as their part in agiven document or corpus.
Here, thinkabout the situtation where single-noun Noccurs with other single-nouns whichmight be a part of many compound nounsshown in Figure 1 where [N M] meansbigram of noun N and M.[LN1  N] (#L1)         [N  RN1](#R1):                              :[LNn  N](LN)         [N  RNm](#Rm)Figure 1.
Noun Bigram and their FrequencyIn Figure 1, [LNi  N] (i=1,..,n) and [NRNj] (j=1,...,m) are single-noun bigramswhich constitute (parts of) compoundnouns.
#Li and #Rj (i=1,..,n and j=1,..,m)mean the frequency of the bigram [LNiN] and [N RNj] respectively.
Note thatsince we depict only bigrams, compoundnouns like [LNi N RNj]  which contains[LNi  N] and/or [N RNj] as their partsmight actually occur in a corpus.
Againthis noun trigram might be a part oflonger compound nouns.Let us show an example of a noun bigram.Suppose that we extract compound nounsincluding ?trigram?
as candidate termsfrom a corpus shown in the followingexample.Example 1.trigram statistics, word trigram, classtrigram, word trigram, trigramacquisition, word trigram statistics,character trigramThen, noun bigrams consisting of asingle-noun ?trigram?
are shown in thefollowing where the number bewteen( and ) shows the frequency.word  trigram (3)  trigram statistics (2)class trigram (1)  trigram acquisition (1)character trigram(1)Figure 2.
An example of noun bigramWe just focus on and utilize single-nounbigrams to define the function on whichscoring is based.
Note that we areconcerned only with single-noun bigramsand not with a single-noun per se.
Thereason is that we try to sharply focus onthe fact that the majority of domainspecific terms are compound nouns.Compound nouns are well analyzed asnoun bigram.2.2 Scoring Function2.2.1 The direct score of a noun bigramSince a scoring function based on [LNi N]or [N RNj] could have an infinite numberof variations, we here consider thefollowing simple but representativescoring functions.#LDN(N) and #RDN(N) : These are thenumber of distinct single-nouns whichdirectly precede or succeed N. These areexactly ?n?
and ?m?
in Figure 1.
Forinstance, in an example shown in Figure2, #LDN(trigram)=3, #RDN(trigram)=2LN(N,k) and RN(N,k): The generalfunctions that take into account thenumber of occurrences of  each nounbigram like [LNi N] and [N RNj] aredefined as follows.For instance, if we use LN(N,1) andRN(N,1) in example 1, GM(trigram,1) =)15()13( +?+  = 4.90.
In (3), GM doesnot depend on the length of a compoundnoun that is the number of single-nounswithin the compound noun.
This isbecause we have not yet had any ideaabout the relation between theimportance of a compound noun and alength of the compound noun.
It is fair totreat all compound nouns, includingsingle-nouns, equally no matter how longor short each compound noun is.
?==LDN(N)#1ik     Li)(#k)LN(N,           (1)?==RDN(N)#1jk    Rj)(#k)RN(N,           (2)We can find various functions by varyingparameter k of (1) and (2).
For instance,#LDN(N) and #RDN(N)  can be definedas  LN(N,0) and RN(N,0).
LN(N,1) andRN(N,1) are the frequencies of nouns thatdirectly precede or succeed N.  In theexample shown in Figure 2, for example,LN(trigram,1)=5, and RN(trigram,1)=3.Now we think about the nature of (1) and(2) with various value of the parameter k.The larger k is, the more we take intoaccount the frequencies of each nounbigram.
One extreme is the case k=0,namely LN(N,0) and RN(N,0), where wedo not take into account the frequency ofeach noun bigram at all.
LN(N,0) andRN(N,0) describe how linguistically anddomain dependently productive the nounN is in a given corpus.
That means thatnoun N presents a key and/or basicconcept of the domain treated by thecorpus.
Other extreme cases are large k,like k=2 , 4, etc.
In these cases, we ratherfocus on frequency of each noun bigram.In other words, statistically biased use ofnoun N is the main concern.
In theexample shown in Figure 2, for example,LN(trigram,2)=11, and RN(trigram,2)=5.If k<0, we discount the frequency of eachnoun bigram.
However, this case does notshow good results of in our ATRexperiment.2.2.3 Combining Compound Noun FrequencyInformation we did not use in the bigrambased methods described in 2.2.1 and2.2.2 is the frequency of single-nouns andcompound-nouns that occurindependently, namely left and rightadjacent words not being nouns.
Forinstance,  ?word patterns?
occursindependently in ??
use the wordpatterns occurring in ?
.?
Since thescoring functions proposed in 2.2.1 arenoun bigram statistics,  the number ofthis kind of independent occurrences ofnouns themselves are not used.
If we takethis information into account, a new typeof information is used and  better resultsare expected.In this paper, we employ a very simplemethod for this.
We observe that if asingle-noun or a compound noun occursindependently, the score of the noun ismultiplied by the number of itsindependent occurrences.
ThenGM(CN,k) of the formula (3) is  revised.We call this new GM FGM(CN,k)  anddefine it as follows.2.2.2 Score of compound nounsThe next thing to do is to extend thescoring functions of a single-noun to thescoring functions of a compound noun.
Weadopt a very simple method, namely ageometric mean.
Now  think about acompound noun : CN = N1 N2?N L. Thena geometric mean: GM of CN is defined asfollows.if N occurs independentlythen f(CN)k)GM(CN,k)FGM(CN, ?=where f(CN) means the number ofindependent occurrences of noun CN)3(1)k),1)(RN(Nk),(LN(Nk)GM(CN,L21L1iii ?????????
++= ?=(33 +For instance, in example 1, if we findindependent ?trigram?
three times in the corpus,FGM(trigram,1)= 1)(51) +??
=14.70---  (4)2.2.4 Modified C-valueWe compare our methods with theC-value based method(Frantzi andAnaniadou 1996) because  1) theirmethod is very powerful to extract andproperly score compound nouns., and 2)their method is basically based onunithood.
On the contrary, our scoringfunctions proposed in 2.2.1 try to capturetermhood.
However the originaldefinition of C-value can not score asingle-noun because the important partof the definition C-value is:)c(a)t(a)-1)(n(a)-length(a)(value(a)-C =--- (5)where a is compound noun, length(a) isthe number of single-nouns which makeup a, n(a) is  the total frequency ofoccurrence of a on the corpus, t(a) is thefrequency of occurrence of a in longercandidate terms, and c(a) is the numberof those candidate terms.As known from (5), all single-noun?sC-value come to be 0.
The reason why thefirst term of right hand side is(length(a)-1) is that C-value originallyseemed to capture how muchcomputational effort is to be made inorder to recognize the important part ofthe term.
Thus, if the length(a)  is 1, wedo not need any effort to recognize itspart because the term a is a  single-wordand does not have its part.
But we intendto capture how important the term is forthe writer or reader, namely its termhood.In order to make the C-value capturetermhood, we modify (5) as follows.
)c(a)t(a)-n(a)length(a)(value(a)-MC =  (6)Where ?MC-value?
means ?ModifiedC-value.
?3 Experimental Evaluation3.1 ExperimentIn our experiment, we use the NTCIR1TMREC test collection (Kageura et al1999).
As an activity of TMREC, theyhave provided us with a Japanese testcollection of a term recognition task.
Thegoal of this task is to automaticallyrecognize and extract terms from a textcorpus which contains  1,870 abstractsgathered from the computer science andcommunication engineering domaincorpora of the NACSIS AcademicConference Database, and 8,834manually collected correct terms.
TheTMREC text corpus is morphologicallyanalyzed and POS tagged by hand.
Fromthis POS tagged text, we extractuninterrupted noun sequences as termcandidates.
Actually 16,708 termcandidates are extracted and severalscoring methods are applied to them.
Allthe extracted term candidates CNs areranked according to their  GM(CN,k),FGM(CN,k) and MC-value(CN) indescending order.
As for parameter k of(1) and (2), we choose k=1 because itsperformance is the best among variousvalues of k in the range from 0 to 4.
Thus,henceforth, we omit k from GM and FGM,like GM(CN) and FGM(CN).
We useGM(CN) as the baseline.In evaluation, we conduct experimentswhere we pick up the highest rankedterm candidate down to the PNth highestranked term candidate by these threescoring methods, and evaluate the set ofselected terms with the number of correctterms, we call it CT, within it.
In thefollowing figures, we only show CTbecause recall is CT/8834, where 8834 isthe number of all correct terms, precisionis CT/PN.Another measure NTCIR1 provides uswith is the terms which include thecorrect term as its part.
We call it ?longerterm?
or LT.
They are sometimes valuedterms and also indicate in what contextthe correct terms are used.
Then we alsouse the number of longer terms in ourevaluation.3.2 ResultsIn Figure 3 through 5, PN of X-axismeans PN.010002000300040005000050010001500200025003000PNExtractedcorrecttermsGM GM - longer termFigure 3.
CT and LT of GM(CN) for each PN-50050100150200250300350040080012001600200024002800PNDiffrenceofcorrenttermsFGM-GM MCvalue-GMFigure 4.
CT of FGM(CN) minus CT ofGM(CN), and CT of MC-value(CN) minus CTof GM(CN) for each PN-600-400-2000050010001500200025003000PNDiffrenceoflongertermsMCvalue-GM FGM-GMFigure 5.
LT of GM(CN) minus LT ofFGM(CN) , and LT of GM(CN) minus LT ofMC-value(CN) for each PNIn Figure 3, the Y-axis represents CT inother words the number of correct termspicked up by GM(CN) and the number oflonger terms picked up by GM(CN) foreach PN.
They are our baseline.
TheFigure 4 shows the difference between CTof FGM(CN) and CT of GM(CN) and thedifference between CT of MC-value(CN)and CT of GM(CN)  for each PN.
Figure5 shows the difference between LT ofGM(CN) and LT of FGM(CN) or LT ofMC-value(CN) for each PN.
As knownfrom Figure 4, FGM based methodoutperforms MC-value up to 1,400highest ranked terms.
Since in thedomains of TMREC task that arecomputer science and communicationengineering, 1,400 technical terms areimportant core terms, FGM method wepropose is very promising to extract andrecognize domain specific terms.
We alsoshow CT of each method for larger PN,say, from 3000 up to 15000 in Table 1 and2.Table 1.
CT of each ranking method for PNlarger than 3000PN GM FGM MC-value3000 1784 1970 21116000 3286 3456 36719000 4744 4866 493012000 6009 6090 604615000 7042 7081 7068Table 2.
LT of each ranking method for PNlarger than 3000PN GM FGM MC-Value3000 2893 2840 25316000 5644 5576 50119000 8218 8152 757812000 10523 10488 985215000 12174 12186 12070As seen in these figures and tables, if wewant more terms about these domains,MC-value is more powerful, but when PNis larger than 12,000, again FGMoutperforms.
As for recognizing longerterms, GM(CN), which is the baseline,performs best for every PN.
MC-value isthe worst.
From this observation we cometo know that MC-value tends to assignhigher score to shorter terms than GM orFGM.
We are also interested in what kindof term is favored by each method.
Forthis, we show the average length of thehighest PN ranked terms of each methodin Figure 6 where length of CN meansthe number of single-words CN consistsof.
Clearly, GM prefers longer terms.
Sodoes FGM.
On the contrary, MC-valueprefers shorter terms.
However, as shownin Figure 6, the average length of theMC-value is more fluctuating.
Thatmeans GM and FGM have moreconsistent tendency in ranking compoundnouns.
Finally we compare our resultswith NTCIR1 results (Kageura et al1999).
Unfortunately since (Kageura et al1999) only provides the number of the allextracted terms and also the number ofthe all extracted correct terms, we couldnot directly compare our results withother NTCIR1 participants.
Then, whatis important is the fact that we extracted7,082 correct terms from top 15,000 termcandidates with the FGM methods.
Thisfact is indicating that our methods showthe highest performance among all otherparticipants of NTCIR1 TMREC taskbecause 1) the highest number of termswithin the top 16,000 term candidates is6,536 among all the participants ofNTCIR1 TMREC task, and 2) the highestnumber or terms in all the participants ofNTCIR1 TMREC task is 7,944, but theyare extracted from top 23,270 termcandidates, which means extremely lowprecision.0123450 1000 2000 3000 4000PNtermlengthGM FGM MC-valueFigure 6.
The average length of extractedterms by GM(CN), FGM(CN) andMC-value(CN) for each PNReferencesFrantzi, T.K.
and Ananiadou, S.
1996.?Extracting nested collocations?.
InProceedings of 16th InternationalConference on Compu ationalLinguistics, 41-46.tetezeFrantzi, T.K.
and Ananiadou, S.
1999.?The c-value/nc-value method for atr?.Journal of Natural LanguageProcessing 6(3), 145-179.Hisamitsu, T, 2000.
?A Method ofMeasuring Term Representativeness?.In Proce dings of 18th InternationalConference on Compu ationalLinguistics,  320-326.Kageura, K. and Umino, B.
1996.?Methods of automatic termrecognition: a review?.
Terminology3(2), 259-289.Conclusion Kageura, K. et al 1999.
?TMREC Task:Overview and Evaluation?.
InProceedings of the First NTCIRWorkshop on Research in JapaneseT xt Retrieval and Term Recognition,411-440.In this paper, we introduce a newngle-noun bigram based statisticalmethods for ATR, which capture howmany nouns adjoin the single-noun inquestion to form compound nouns.Through experimental evaluation usingthe NTCIR1 TMREC test collection, theFGM method we proposed showed thebest performance in selecting up to 1,400domain specific terms.Matsumoto, Y., Kurohashi, S., Yamaji, O.,Taeki, H. and Nagao, M. 1996.Instruction Manual of JapaneseMorphological Analy er JUMAN3.1.Nagao Lab.
at Kyoto University.Nakagawa, H. and Mori, T. 1998.
?NestedCollocation and Compound Noun forTerm Recognition?.
In Proceedings ofthe First Workshop on ComputationalT rminology COMPTERM?98, 64-70.AcknowledgementsThis research is funded by the Ministry ofEducation Science and Academic, Japan.
