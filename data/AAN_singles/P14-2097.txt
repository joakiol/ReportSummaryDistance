Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 592?598,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsNonparametric Method for Data-driven Image CaptioningRebecca Mason and Eugene CharniakBrown Laboratory for Linguistic Information Processing (BLLIP)Brown University, Providence, RI 02912{rebecca,ec}@cs.brown.eduAbstractWe present a nonparametric density esti-mation technique for image caption gener-ation.
Data-driven matching methods haveshown to be effective for a variety of com-plex problems in Computer Vision.
Thesemethods reduce an inference problem foran unknown image to finding an exist-ing labeled image which is semanticallysimilar.
However, related approaches forimage caption generation (Ordonez et al,2011; Kuznetsova et al, 2012) are ham-pered by noisy estimations of visual con-tent and poor alignment between imagesand human-written captions.
Our workaddresses this challenge by estimating aword frequency representation of the vi-sual content of a query image.
This al-lows us to cast caption generation as anextractive summarization problem.
Ourmodel strongly outperforms two state-of-the-art caption extraction systems accord-ing to human judgments of caption rele-vance.1 IntroductionAutomatic image captioning is a much studiedtopic in both the Natural Language Processing(NLP) and Computer Vision (CV) areas of re-search.
The task is to identify the visual contentof the input image, and to output a relevant naturallanguage caption.Much prior work treats image captioning asa retrieval problem (see Section 2).
These ap-proaches use CV algorithms to retrieve similar im-ages from a large database of captioned images,and then transfer text from the captions of thoseimages to the query image.
This is a challengingproblem for two main reasons.
First, visual simi-larity measures do not perform reliably and do notQuery Image: Captioned Images:1.
2.
3.4.
5.
6.1.)
3 month old baby girl with blue eyes in her crib2.)
A photo from the Ismail?s portrait shoot3.)
A portrait of a man, in black and white4.)
Portrait in black and white with the red rose5.)
I apparently had this saved in black and white as well6.)
Portrait in black and whiteTable 1: Example of a query image from the SBU-Flickr dataset (Ordonez et al, 2011), along withscene-based estimates of visually similar images.Our system models visual content using words thatare frequent in these captions (highlighted) and ex-tracts a single output caption.capture all of the relevant details which humansmight describe.
Second, image captions collectedfrom the web often contain contextual or back-ground information which is not visually relevantto the image being described.In this paper, we propose a system for transfer-based image captioning which is designed to ad-dress these challenges.
Instead of selecting an out-put caption according to a single noisy estimateof visual similarity, our system uses a word fre-quency model to find a smoothed estimate of vi-sual content across multiple captions, as Table 1illustrates.
It then generates a description of thequery image by extracting the caption which bestrepresents the mutually shared content.The contributions of this paper are as follows:5921.
Our caption generation system effectively lever-ages information from the massive amounts ofhuman-written image captions on the internet.
Inparticular, it exhibits strong performance on theSBU-Flickr dataset (Ordonez et al, 2011), a noisycorpus of one million captioned images collectedfrom the web.
We achieve a remarkable 34%improvement in human relevance scores over arecent state-of-the-art image captioning system(Kuznetsova et al, 2012), and 48% improvementover a scene-based retrieval system (Patterson etal., 2014) using the same computed image fea-tures.2.
Our approach uses simple models which canbe easily reproduced by both CV and NLP re-searchers.
We provide resources to enable com-parison against future systems.12 Image Captioning by TransferThe IM2TEXT model by Ordonez et al (2011)presents the first web-scale approach to image cap-tion generation.
IM2TEXT retrieves the imagewhich is the closest visual match to the query im-age, and transfers its description to the query im-age.
The COLLECTIVE model by Kuznetsova etal.
(2012) is a related approach which uses trainedCV recognition systems to detect a variety of vi-sual entities in the query image.
A separate de-scription is retrieved for each visual entity, whichare then fused into a single output caption.
LikeIM2TEXT, their approach uses visual similarity asa proxy for textual relevance.Other related work models the text more di-rectly, but is more restrictive about the sourceand quality of the human-written training data.Farhadi et al (2010) and Hodosh et al (2013)learn joint representations for images and cap-tions, but can only be trained on data with verystrong alignment between images and descriptions(i.e.
captions written by Mechanical Turkers).
An-other line of related work (Fan et al, 2010; Akerand Gaizauskas, 2010; Feng and Lapata, 2010)generates captions by extracting sentences fromdocuments which are related to the query image.These approaches are tailored toward specific do-mains, such as travel and news, where images tendto appear with corresponding text.1See http://bllip.cs.brown.edu/download/captioning_resources.zip or ACLAnthology.3 DatasetIn this paper, we use the SBU-Flickr dataset2.
Or-donez et al (2011) query Flickr.com using ahuge number of words which describe visual en-tities, in order to build a corpus of one millionimages with captions which refer to image con-tent.
However, further analysis by Hodosh et al(2013) shows that many captions in SBU-Flickr(?67%) describe information that cannot be ob-tained from the image itself, while a substantialfraction (?23%) contain almost no visually rel-evant information.
Nevertheless, this dataset isthe only web-scale collection of captioned images,and has enabled notable research in both CV andNLP.34 Our Approach4.1 OverviewFor a query image Iq, our task is to generate a rele-vant description by selecting a single caption fromC, a large dataset of images with human-writtencaptions.
In this section, we first define the featurespace for visual similarity, then formulate a den-sity estimation problem with the aim of modelingthe words which are used to describe visually sim-ilar images to Iq.
We also explore methods forextractive caption generation.4.2 Measuring Visual SimilarityData-driven matching methods have shown to bevery effective for a variety of challenging prob-lems (Hays and Efros, 2008; Makadia et al,2008; Tighe and Lazebnik, 2010).
Typically thesemethods compute global (scene-based) descriptorsrather than object and entity detections.
Scene-based techniques in CV are generally more robust,and can be computed more efficiently on largedatasets.The basic IM2TEXT model uses an equallyweighted average of GIST (Oliva and Torralba,2001) and TinyImage (Torralba et al, 2008) fea-tures, which coarsely localize low-level featuresin scenes.
The output is a multi-dimensionalimage space where semantically similar scenes(e.g.
streets, beaches, highways) are projectednear each other.2http://tamaraberg.com/CLSP11/3In particular, papers stemming from the 2011 JHU-CLSPSummer Workshop (Berg et al, 2012; Dodge et al, 2012;Mitchell et al, 2012) and more recently, the best paper awardwinner at ICCV (Ordonez et al, 2013).593Patterson and Hays (2012) present ?scene at-tribute?
representations which are characterizedusing low-level perceptual attributes as used byGIST (e.g.
openness, ruggedness, naturalness),as well as high-level attributes informed by open-ended crowd-sourced image descriptions (e.g., in-door lighting, running water, places for learning).Follow-up work (Patterson et al, 2014) showsthat their attributes provide improved matching forimage captioning over IM2TEXT baseline.
Weuse their publicly available4scene attributes forour experiments.
Training set and query imagesare represented using 102-dimensional real-valuedvectors, and similarity between images is mea-sured using the Euclidean distance.4.3 Density EstimationAs shown in Bishop (2006), probability densityestimates at a particular point can be obtained byconsidering points in the training data within somelocal neighborhood.
In our case, we define someregion R in the image space which contains Iq.The probability mass of that space isP =?Rp(Iq)dIq(1)and if we assume thatR is small enough such thatp(Iq) is roughly constant in R, we can approxi-matep(Iq) ?kimgnimgVimg(2)where kimgis the number of images within R inthe training data, nimgis the total number of im-ages in the training data, and Vimgis the volumeofR.
In this paper, we fix kimgto a constant value,so that Vimgis determined by the training dataaround the query image.5At this point, we extend the density estima-tion technique in order to estimate a smoothedmodel of descriptive text.
Let us begin by consid-ering p(w|Iq), the conditional probability of theword6w given Iq.
This can be described using a4https://github.com/genp/sun_attributes5As an alternate approach, one could fix the value ofVimgand determine kimgfrom the number of points in R,giving rise to the kernel density approach (a.k.a.
Parzenwindows).
However we believe the KNN approach is moreappropriate here, because the number of samples is nearly10000 times greater than the number of dimensions in theimage representation.6Here, we use word to refer to non-function words, andassume all function words have been removed from the cap-tions.Bayesian model:p(w|Iq) =p(Iq|w)p(w)p(Iq)(3)The prior for w is simply its unigram frequency inC, where ntxtwand ntxtare word token counts:p(w) =ntxtwntxt(4)Note that ntxtis not the same as nimgbecause asingle captioned image can have multiple wordsin its caption.
Likewise, the conditional densityp(Iq|w) ?ktxtwntxtwVimg(5)considers instances of observed words within R,although the volume of R is still defined by theimage space.
ktxtwis the number of times w is usedwithinR while ntxtwis the total number of times wis observed in C.Combining Equations 2, 4, and 5 and cancelingout terms gives us the posterior probability:p(w|Iq) =ktxtwkimg?nimgntxt(6)If the number of words in each caption is inde-pendent of its image?s location in the image space,then p(w|Iq) is approximately the observed uni-gram frequency for the captions insideR.4.4 Extractive Caption GenerationWe compare two selection methods for extractivecaption generation:1.
SumBasic SumBasic (Nenkova and Vander-wende, 2005) is a sentence selection algorithm forextractive multi-document summarization whichexclusively maximizes the appearance of wordswhich have high frequency in the original docu-ments.
Here, we adapt SumBasic to maximize theaverage value of p(w|Iq) in a single extracted cap-tion:output = argmaxctxt?R?w?ctxt1|ctxt|p(w|Iq) (7)The candidate captions ctxtdo not necessarilyhave to be observed in R, but in practice we didnot find increasing the number of candidate cap-tions to be more effective than increasing the sizeofR directly.594Figure 1: BLEU scores vs k for SumBasic extrac-tion.2.
KL Divergence We also consider a KLDivergence selection method.
This method out-performs the SumBasic selection method for ex-tractive multi-document summarization (Haghighiand Vanderwende, 2009).
It also generates the bestextractive captions for Feng and Lapata (2010),who caption images by extracting text from a re-lated news article.
The KL Divergence method isoutput = argminctxt?R?wp(w|Iq) logp(w|Iq)p(w|ctxt)(8)5 Evaluation5.1 Automatic EvaluationAlthough BLEU (Papineni et al, 2002) scoresare widely used for image caption evaluation, wefind them to be poor indicators of the quality ofour model.
As shown in Figure 1, our system?sBLEU scores increase rapidly until about k = 25.Past this point we observe the density estimationseems to get washed out by oversmoothing, but theBLEU scores continue to improve until k = 500but only because the generated captions becomeincreasingly shorter.
Furthermore, although weobserve that our SumBasic extracted captions ob-tain consistently higher BLEU scores, our per-sonal observations find KL Divergence captions tobe better at balancing recall and precision.
Never-theless, BLEU scores are the accepted metric forrecent work, and our KL Divergence captions withk = 25 still outperform all other previously pub-lished systems and baselines.
We omit full resultshere due to space, but make our BLEU setup withcaptions for all systems and baselines available fordocumentary purposes.System RelevanceCOLLECTIVE 2.38 (?
= 1.45)SCENE ATTRIBUTES 2.15 (?
= 1.45)SYSTEM 3.19 (?
= 1.50)HUMAN 4.09 (?
= 1.14)Table 2: Human evaluations of relevance: meanratings and standard deviations.
See Section 5.2.5.2 Human EvaluationWe perform our human evaluation of caption rele-vance using a similar setup to that of Kuznetsovaet al (2012), who have humans rate the image cap-tions on a 1-5 scale (5: perfect, 4: almost per-fect, 3: 70-80% good, 2: 50-70% good, 1: to-tally bad).
Evaluation is performed using AmazonMechanical Turk.
Evaluators are shown both thecaption and the query image, and are specificallyinstructed to ignore errors in grammaticality andcoherence.We generate captions using our system with KLDivergence sentence selection and k = 25.
Wealso evaluate the original HUMAN captions forthe query image, as well as generated captionsfrom two recently published caption transfer sys-tems.
First, we consider the SCENE ATTRIBUTESsystem (Patterson et al, 2014), which representsboth the best scene-based transfer model and ak = 1 nearest-neighbor baseline for our system.We also compare against the COLLECTIVE system(Kuznetsova et al, 2012), which is the best object-based transfer model.In order to facilitate comparison, we use thesame test/train split that is used in the publiclyavailable system output for the COLLECTIVE sys-tem7.
However, we remove some query imageswhich have contamination between the train andtest set (this occurs when a photographer takesmultiple shots of the same scene and gives all theimages the exact same caption).
We also note thattheir test set is selected based on images wheretheir object detection systems had good perfor-mance, and may not be indicative of their perfor-mance on other query images.Table 2 shows the results of our human study.Captions generated by our system have 48%improvement in relevance over the SCENE AT-TRIBUTES system captions, and 34% improve-7http://www.cs.sunysb.edu/?pkuznetsova/generation/cogn/captions.html595COLLECTIVE: One of the birds seen incompany of female andjuvenile.View of this woman sit-ting on the sidewalk inMumbai by the stainedglass.
The boy walk-ing by next to match-ing color walls in gov tbuilding.Found this mother birdfeeding her babies inour maple tree on thephone.Found in floating grassspotted alongside thescenic North CascadesHwy near Ruby arm ablack bear.SCENEATTRIBUTES:This small bird is prettymuch only found in theancient Caledonian pineforests of the ScottishHighlands.me and allison in frontof the white houseThe sand in this beachwas black...I repeatBLACK SANDNot the green one, butthe almost ghost-likewhite one in front of it.SYSTEM: White bird found inpark standing on brickwallby the white house pine tree covered in ice:)Pink flower in garden w/mothHUMAN: Some black head birdtaken in bray head.Us girls in front of thewhite houseMale cardinal in snowytree knotsBlack bear by the roadbetween Ucluelet andPort Alberni, B.C.,CanadaTable 3: Example query images and generated captions.ment over the COLLECTIVE system captions.
Al-though our system captions score lower than thehuman captions on average, there are some in-stances of our system captions being judged asmore relevant than the human-written captions.6 Discussion and ExamplesExample captions are shown in Table 3.
In manyinstances, scene-based image descriptors provideenough information to generate a complete de-scription of the image, or at least a sufficientlygood one.
However, there are some kinds ofimages for which scene-based features alone areinsufficient.
For example, the last example de-scribes the small pink flowers in the background,but misses the bear.Image captioning is a relatively novel task forwhich the most compelling applications are prob-ably not yet known.
Much previous work in im-age captioning focuses on generating captions thatconcretely describe detected objects and entities(Kulkarni et al, 2011; Yang et al, 2011; Mitchellet al, 2012; Yu and Siskind, 2013).
However,human-generated captions and annotations alsodescribe perceptual features, contextual informa-tion, and other types of content.
Additionally, oursystem is robust to instances where entity detec-tion systems fail to perform.
However, one couldconsider combined approaches which incorporatemore regional content structures.
For example,previous work in nonparametric hierarchical topicmodeling (Blei et al, 2010) and scene labeling(Liu et al, 2011) may provide avenues for furtherimprovement of this model.
Compression meth-ods for removing visually irrelevant information(Kuznetsova et al, 2013) may also help increasethe relevance of extracted captions.
We leave theseideas for future work.ReferencesAhmet Aker and Robert Gaizauskas.
2010.
Generatingimage descriptions using dependency relational pat-terns.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, ACL?10, pages 1250?1258, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Alexander C Berg, Tamara L Berg, Hal Daume, JesseDodge, Amit Goyal, Xufeng Han, Alyssa Mensch,Margaret Mitchell, Aneesh Sood, Karl Stratos, et al2012.
Understanding and predicting importance inimages.
In Computer Vision and Pattern Recog-nition (CVPR), 2012 IEEE Conference on, pages3562?3569.
IEEE.Christopher M Bishop.
2006.
Pattern recognition andmachine learning, volume 1.
Springer New York.David M. Blei, Thomas L. Griffiths, and Michael I. Jor-dan.
2010.
The nested chinese restaurant process596and bayesian nonparametric inference of topic hier-archies.
J. ACM, 57(2):7:1?7:30, February.Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-sch, Margaret Mitchell, Karl Stratos, Kota Yam-aguchi, Yejin Choi, Hal Daum?e III, Alexander C.Berg, and Tamara L. Berg.
2012.
Detecting visualtext.
In North American Chapter of the Associationfor Computational Linguistics (NAACL).Xin Fan, Ahmet Aker, Martin Tomko, Philip Smart,Mark Sanderson, and Robert Gaizauskas.
2010.Automatic image captioning from the web for gpsphotographs.
In Proceedings of the internationalconference on Multimedia information retrieval,MIR ?10, pages 445?448, New York, NY, USA.ACM.Ali Farhadi, Mohsen Hejrati, Mohammad AminSadeghi, Peter Young, Cyrus Rashtchian, JuliaHockenmaier, and David Forsyth.
2010.
Every pic-ture tells a story: generating sentences from images.In Proceedings of the 11th European conference onComputer vision: Part IV, ECCV?10, pages 15?29,Berlin, Heidelberg.
Springer-Verlag.Yansong Feng and Mirella Lapata.
2010.
How manywords is a picture worth?
automatic caption genera-tion for news images.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 1239?1249, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Aria Haghighi and Lucy Vanderwende.
2009.
Ex-ploring content models for multi-document summa-rization.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 362?370.
Association forComputational Linguistics.James Hays and Alexei A Efros.
2008.
Im2gps: esti-mating geographic information from a single image.In Computer Vision and Pattern Recognition, 2008.CVPR 2008.
IEEE Conference on, pages 1?8.
IEEE.Micah Hodosh, Peter Young, and Julia Hockenmaier.2013.
Framing image description as a ranking task:Data, models and evaluation metrics.
Journal of Ar-tificial Intelligence Research, 47:853?899.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, SimingLi, Yejin Choi, Alexander C. Berg, and Tamara L.Berg.
2011.
Baby talk: Understanding and gener-ating simple image descriptions.
In CVPR, pages1601?1608.Polina Kuznetsova, Vicente Ordonez, Alexander C.Berg, Tamara L. Berg, and Yejin Choi.
2012.
Col-lective generation of natural image descriptions.
InACL.Polina Kuznetsova, Vicente Ordonez, Alexander Berg,Tamara Berg, and Yejin Choi.
2013.
Generalizingimage captions for image-text parallel corpus.
InACL.Ce Liu, Jenny Yuen, and Antonio Torralba.
2011.Nonparametric scene parsing via label transfer.Pattern Analysis and Machine Intelligence, IEEETransactions on, 33(12):2368?2382.Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Ku-mar.
2008.
A new baseline for image annotation.In Computer Vision?ECCV 2008, pages 316?329.Springer.Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-maguchi, Karl Stratos, Xufeng Han, Alyssa Men-sch, Alexander C. Berg, Tamara L. Berg, and HalDaum?e III.
2012.
Midge: Generating image de-scriptions from computer vision detections.
In Euro-pean Chapter of the Association for ComputationalLinguistics (EACL).Ani Nenkova and Lucy Vanderwende.
2005.
The im-pact of frequency on summarization.Aude Oliva and Antonio Torralba.
2001.
Modeling theshape of the scene: A holistic representation of thespatial envelope.
International Journal of ComputerVision, 42:145?175.V.
Ordonez, G. Kulkarni, and T.L.
Berg.
2011.Im2text: Describing images using 1 million cap-tioned photographs.
In NIPS.Vicente Ordonez, Jia Deng, Yejin Choi, Alexander CBerg, and Tamara L Berg.
2013.
From large scaleimage categorization to entry-level categories.
In In-ternational Conference on Computer Vision.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Genevieve Patterson and James Hays.
2012.
Sun at-tribute database: Discovering, annotating, and rec-ognizing scene attributes.
In Computer Vision andPattern Recognition (CVPR), 2012 IEEE Confer-ence on, pages 2751?2758.
IEEE.Genevieve Patterson, Chen Xu, Hang Su, and JamesHays.
2014.
The sun attribute database: Beyondcategories for deeper scene understanding.
Interna-tional Journal of Computer Vision.Joseph Tighe and Svetlana Lazebnik.
2010.
Su-perparsing: scalable nonparametric image parsingwith superpixels.
In Computer Vision?ECCV 2010,pages 352?365.
Springer.Antonio Torralba, Robert Fergus, and William T Free-man.
2008.
80 million tiny images: A large dataset for nonparametric object and scene recognition.Pattern Analysis and Machine Intelligence, IEEETransactions on, 30(11):1958?1970.597Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yian-nis Aloimonos.
2011.
Corpus-guided sentence gen-eration of natural images.
In Empirical Methodsin Natural Language Processing (EMNLP), Edin-burgh, Scotland.Haonan Yu and Jeffrey Mark Siskind.
2013.
Groundedlanguage learning from video described with sen-tences.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), volume 1, pages 53?63,Sofia, Bulgaria.
Association for Computational Lin-guistics.598
