GRAMMAR SPECIAL IZAT ION THROUGHENTROPY THRESHOLDSChrister SamuelssonSwedish  Ins t i tu te  of Computer  Sc ienceBox  1263 S-164 28 K is ta ,  SwedenIn ternet :  chr i s te r@s ics ,  seAbstractExplanation-based generalization is used to extract aspecialized grammar from the original one using a train-ing corpus of parse trees.
This allows very much fasterparsing and gives a lower error rate, at the price of asmall loss in coverage.
Previously, it has been necessaryto specify the tree-cutting criteria (or operationality cri-teria) manually; here they are derived automaticallyfrom the training set and the desired coverage of thespecialized grammar.
This is done by assigning an en-tropy value to each node in the parse trees and cuttingin the nodes with sufficiently high entropy values.BACKGROUNDPrevious work by Manny Rayner and the author, see\[Samuelsson &~Rayner 1991\] attempts to tailor an ex-isting natural-language system to a specific applicationdomain by extracting a specialized grammar from theoriginal one using a large set of training examples.
Thetraining set is a treebank consisting of implicit parsetrees that each specify a verified analysis of an inputsentence.
The parse trees are implicit in the sense thateach node in the tree is the (mnemonic) name of thegrammar rule resolved on at that point, rather than thesyntactic ategory of the LHS of the grammar rule as isthe case in an ordinary parse tree.
Figure 1 shows fiveexamples of implicit parse trees.
The analyses are ver-ified in the sense that each analysis has been judged tobe the preferred one for that input sentence by a humanevaluator using a semi-automatic evaluation method.A new grammar is created by cutting up each implicitparse tree in the treebank at appropriate points, creat-ing a set of new rules that consist of chunks of originalgrammar ules.
The LHS of each new rule will be theLHS phrase of the original grammar ule at the root ofthe tree chunk and the RHS will be the RHS phrases ofthe rules in the leaves of the tree chunk.
For example,cutting up the first parse tree of Figure 1 at the NP ofthe rule vp_v_np yields rules 2 and 3 of Figure 3.The idea behind this is to create a specialized gram-mar that retains a high coverage but allows very muchfaster parsing.
This has turned out to be possible - -speedups compared to using the original grammar ofin median 60 times were achieved at a cost in cover-age of about ten percent, see \[Samuelsson 1994a\].1 An-other benefit from the method is a decreased error ratewhen the system is required to select a preferred ana-lysis.
In these experiments the scheme was applied tothe grammar of a version of the SRI Core LanguageEngine \[Alshawi ed.
1992\] adapted to the Atis domainfor a speech-translation task \[Rayner el al 1993\] andlarge corpora of real user data collected using Wizard-of-Oz simulation.
The resulting specialized gram-mar was compiled into LR parsing tables, and a spe-cial LR parser exploited their special properties, see\[Samuelsson 1994b\].The technical vehicle previously used to extract thespecialized grammar is explanation-based generaliza-tion (EBG), see e.g.
\[Mitchell et al1986\].
Very briefly,this consists of redoing the derivation of each train-ing example top-down by letting the implicit parse treedrive a rule expansion process, and aborting the expan-sion of the specialized rule currently being extracted ifthe current node of the implicit parse tree meets a setof tree-cutting criteria 2.
In this case the extraction pro-cess is invoked recursively to extract subrules rooted inthe current node.
The tree-cutting criteria can be local("The LHS of the original grammar ule is an NP,") ordependent on the rest of the parse tree ("that doesn'tdominate the empty string only,") and previous choicesof nodes to cut at ("and there is no cut above the cur-rent node that is also labelled NP.
").A problem not fully explored yet is how to arriveat an optimal choice of tree-cutting criteria.
In theprevious cheme, these must be specified manually, andthe choice is left to the designer's intuitions.
This articleaddresses the problem of automating this process andpresents amethod where the nodes to cut at are selectedautomatically using the information-theoretical conceptof entropy.
Entropy is well-known from physics, but theconcept of perplexity is perhaps better known in thespeech-recognition a d natural-language communities.1Other more easily obtainable publications about this arein preparation.2These are usually referred to as "operationality criteria"in the EBG literature.188For this reason, we will review the concept of entropyat this point, and discuss its relation to perplexity.Ent ropyEntropy is a measure of disorder.
Assume for exam-ple that a physical system can be in any of N states,and that it will be in state si with probability Pi.
Theentropy S of that system is thenNS -= ~ -- Pi " In Pii=1If each state has equal probability, i.e.
if Pi ~- 1 for alli, thenN 1 1S=Z-~- - ln  ~-=lnNi=1In this case the entropy is simply the logarithm of thenumber of states the system can be in.To take a linguistic example, assume that we are try-ing to predict the next word in a word string from theprevious ones.
Let the next word be wk and the pre-vious word string wl, .
.
.
,wk-1.
Assume further thatwe have a language model that estimates the proba-bility of each possible next word (conditional on theprevious word string).
Let these probabilities be Pifor i = 1 .... ,N  for the N possible next words w~,i.e.
Pi = p(wik I Wl, .
.
.
,  wk-a).
The entropy is then ameasure of how hard this prediction problem is:S(L / ) I , .
.
.
,Wk-x )  :N- -  P(Wik I w , ,  .
.
.
,  Wk- i )  .
In  p (w~ I Wl, ' " ,  Wk- i )i=1If all words have equal probability, the entropy is thelogarithm of the branching factor at this point in theinput string.Perp lex i tyPerplexity is related to entropy as follows.
The observedperplexity Po of a language model with respect to an(imaginary) infinite test sequence wl, w2, ... is definedthrough the formula (see \[Jelinek 1990\])In Po = lim - - l ln  p(wi, ..., wn)rl--* OO nHere p(wl, .
.
.
,  Wn) denotes the probability of the wordstring Wl,  ..., W n.Since we cannot experimentally measure infinite lim-its, we terminate after a finite test string wl, ...,WM,arriving at the measured perplexity Pro:Pm = -- --~--ln p(wl, .
.
.
,  WM) InRewriting p(wl , .
.
.
,wk)  as p(wk \[ wl , .
.
.
,wk-1)  ?p(wl, ..., wk-1) gives usM 1In Pm ----- ~ ~ - In  p(wk I wl, ..., w~-l)k=lLet us call the exponential of the expectation value of- In  p(w \[ String) the local perplexity P~(String), whichcan be used as a measure of the information content ofthe initial String.In Pt(wl, ..., wk-1) = E( - ln  P(~k I wl,..., wk-i))  =N-- p(wik I wl, ..., wk-1)" In p(wik I wl .... , wk-i)i=1Here E(q) is the expectation value of q and the sum-mation is carried out over all N possible next words w~.Comparing this with the last equation of the previoussection, we see that this is precisely the entropy S atpoint k in the input string.
Thus, the entropy is thelogarithm of the local perplexity at a given point in theword string.
If all words are equally probable, then thelocal perplexity is simply the branching factor at thispoint.
I f  the probabilities differ, the local perplexitycan be viewed as a generalized branching factor thattakes this into account.T ree  ent ropyWe now turn to the task of calculating the entropy of anode in a parse tree.
This can be done in many differentways; we will only describe two different ones here.Consider the small test and training sets of Figure 1.Assume that we wish to calculate the entropy of thephrases of the rule PP  --* Prep NP, which is namedpp_prep_np.
In the training set, the LHS PP is at-tached to the RHS PP of the rule np_np_pp in twocases and to the RHS PP of the rule vp_vp_pp in onecase, giving it tile entropy -2 ln~ - ?1n?
,~ 0.64.
TheRHS preposition Prep is always a lexical lookup, andthe entropy is thus zero 3, while the RHS NP in one caseattaches to the LHS of rule np_det_np, in one case tothe LHS of rule np_num, and in one case is a lexicallookup, and the resulting entropy is thus - ln?
~ 1.10.The complete table is given here:Rule LHS 1st RHS 2nd RHSs_np_vp 0.00np_np_pp 0.00np_det_n 1.33np_pron 0.00np_num 0.00vp_vp_pp 0.00vp_v_np 0.00vp_v 0.00pp_prep_np 0.640.56 0.560.00 0.000.00 0.000.000.000.00 0.000.00 0.640.000.00 1.10If we want to calculate the entropy of a particularnode in a parse tree, we can either simply use the phrase3Since there is only one alternative, namely a lexiea\]lookup.
In fact, the scheme could easily be extended to en-compass including lexical lookups of particular words intothe specialized rules by distinguishing lexical lookups of dif-ferent words; the entropy would then determine whether ornot to cut in a node corresponding to a lookup, just as forany other node, as is described in the following.189Training examples:s_np_vp/knp_pron vp_v_npI / \l ex  lex  np_det_nI I / \I want lex  lexI Ia tickets_np_vp/ \np_pron  vp_v_npI / \lex / \I / \I lex np_np_ppI /\need np_det_n pp_prep_np/ \  / \lex lex lex lexI I I Ia flight to Bostons_np_vp/\/ \s_np_vp np_det_n vp_vp_pp/\ /\ /\np_pron vp_v_np lex lex vp_v pp_prep_npI / \  I I I / \lex / \ The flight lex lexI / \ I IWe lex np_np_pp departs at\] /\have / \np_det_n pp_prep_np/ \  / \lex lex lex np_det_nI I I / \a depar ture  I lex lexi n  I Ithe morningnp_nuRIlexItenTest example:s_np_vp/ \np_pron  vp_v_npI / \l ex  / \I / \He lex  np_np_ppI / \booked / \np_det_n  pp_prep_np/ \  / \lex lex / \I I / \a ticket lex np_np_ppI /\\] np_det_n pp_prep_npfo r  / \  / \lex lex lex lexI I I Ia flight to DallasFigure 1: A tiny training setentropy of the RttS node, or take the sum of the en-tropies of the two phrases that are unified in this node.For example, the entropy when the RHS NP of therule pp_prep_np is unified with the LHS of the rulenp_det n will in the former case be 1.10 and in thelatter case be 1.10 + 1.33 = 2.43.SCHEME OVERVIEWIn the following scheme, the desired coverage of the spe-cialized grammar is prescribed, and the parse trees arecut up at appropriate places without having to specifythe tree-cutting criteria manually:1.
Index the treebank in an and-or tree where the or-nodes correspond to alternative choices of grammarrules to expand with and the and-nodes correspondto the RHS phrases of each grammar ule.
Cuttingup the parse trees will involve selecting a set of or-nodes in the and-or tree.
Let us call these nodes"cutnodes".2.
Calculate the entropy of each or-node.
We will cut ateach node whose entropy exceeds a threshold value.The rationale for this is that we wish to cut up theparse trees where we can expect a lot of variationi.e.
where it is difficult to predict which rule will beresolved on next.
This corresponds exactly to thenodes in the and-or tree that exhibit high entropyvalues.3.
The nodes of the and-or tree must be partitionedinto equivalence classes dependent on the choice ofcutnodes in order to avoid redundant derivations atparse time.
4 Thus, selecting some particular node asa cutnode may cause other nodes to also become cut-nodes, even though their entropies are not above thethreshold.4.
Determine a threshold entropy that yields the desiredcoverage.
This can be done using for example intervalbisection.5.
Cut up the training examples by matching themagainst he and-or tree and cutting at the determinedcutnodes.It is interesting to note that a textbook methodfor conslructing decision trees for classification fromattribute-value pairs is to minimize the (weighted aver-age of the) remaining entropy 5 over all possible choicesof root attribute, see \[Quinlan 1986\].4This can most easily be seen as follows: Imagine twoidentical, but different portions of the and-or tree.
If theroots and leaves of these portions are all selected as cut-nodes, but the distribution of cutnodes within them differ,then we will introduce multiple ways of deriving the portionsof the parse trees that match any of these two portions ofthe and-or tree.5Defined slightly differently, as described below.190DETAILED SCHEMEFirst, the treebank is partitioned into a training set anda test set.
The training set will be indexed in an and-or tree and used to extract the specialized rules.
Thetest set will be used to check the coverage of the set ofextracted rules.I ndex ing  the  t reebankThen, the set of implicit parse trees is stored in an and-or tree.
The parse trees have the general form of a ruleidentifier Id dominating a list of subtrees or a word ofthe training sentence.
From the current or-node of theand-or tree there will be arcs labelled with rule iden-tifiers corresponding to previously stored parse trees.From this or-node we follow an arc labelled Id, or adda new one if there is none.
We then reach (or add)an and-node indicating the RHS phrases of the gram-mar rule named Id.
Here we follow each arc leadingout from this and-node in turn to accommodate all thesubtrees in the list.
Each such arc leads to an or-node.We have now reached a point of recursion and can indexthe corresponding subtree.
The recursion terminates ifId is the special rule identifier lex and thus dominatesa word of the training sentence, rather than a list ofsubtrees.Indexing the four training examples of Figure 1 willresult in the and-or tree of Figure 2.F ind ing  the  cutnodesNext, we find the set of nodes whose entropies exceed athreshold value.
First we need to calculate the entropyof each or-node.
We will here describe three differentways of doing this, but there are many others.
Beforedoing this, though, we will discuss the question of re-dundancy in the resulting set of specialized rules.We must equate the cutnodes that correspond to thesame type of phrase.
This means that if we cut at anode corresponding to e.g.
an NP, i.e.
where the arcsincident from it are labelled with grammar rules whoseleft-hand-sides are NPs, we must allow all specializedNP rules to be potentially applicable at this point, notjust the ones that are rooted in this node.
This requiresthat we by transitivity equate the nodes that are dom-inated by a cutnode in a structurally equivalent way; ifthere is a path from a cutnode cl to a node nl and apath from a cutnode c2 to a node n2 with an identicalsequence of labels, the two nodes nl and n2 must beequated.
Now if nl is a cutnode, then n2 must alsobe a cutnode even if it has a low entropy value.
Thefollowing iterative scheme accomplishes this:Funct ion  N* (N ?)1.
i :=0;2.
Repeat  i := i + 1; N i := N(NI-1) ;3.
Unt i l  N i = N i-14.
Return  N~;rootI s_np_vp/ \/ k/ \1/ \2/ X/ X/ Xni(0.89) n2(0.56)IX IXnp_pronl \np_det_n I \/ \ / \11 1/\2 / \n n n I \lex I lex  I I l ex  / \/ \vp_v_np/ \vp_vp_pp/ \/ \/ \/ \  /kil X2 il \2/ X / \n n3(1.08) (O.O0)n7 n8(0.64)lex I / \  vp_v I I pp_prep_np/ \ I I 11 \2np_det_n/ \np_np_pp n n n9(l. I0)/ \ lex~ lex~ I np_numI \ l li lk2 I \  nn n / \ llexlexl  I lex 1/ \2/ \/ \(1.33)n4np_de?_n I11 \2nlexln5(0.64)Ipp_prep_np/ \n 11 \2flex / \n n6(1.76)lexl /\lex/ \np_det_n/ \1/\2nnlexl  JlexFigure 2: The resulting and-or tree191Here N(N j) is the set of cutnodes NJ augmented withthose induced in one step by selecting N~ as the set ofcutnodes.
In ~ practice this was accomplished by compil-ing an and-or graph from the and-or tree and the setof selected cutnodes, where each set of equated nodesconstituted a vertex of the graph, and traversing it.In the simplest scheme for calculating the entropy ofan or-node, only the RHS phrase of the parent rule,i.e.
the dominating and-node, contributes to the en-tropy, and there is in fact no need to employ an and-ortree at all, since the tree-cutting criterion becomes localto the parse tree being cut up.In a slightly more elaborate scheme, we sum over theentropies of the nodes of the parse trees that match thisnode of the and-or tree.
However, instead of letting eachdaughter node contribute with the full entropy of theLHS phrase of the corresponding grammar ule, theseentropies are weighted with the relative frequency ofuse of each alternative choice of grammar ule.For example, the entropy of node n3 of the and-or tree of Figure 2 will be calculated as follows: Themother rule vp_v_np will contribute the entropy asso-ciated with the RHS NP, which is, referring to the tableabove, 0.64.
There are 2 choices of rules to resolve on,namely np_det_n and np_np_pp with relative frequen-cies ?
and ~ respectively.
Again referring to the entropytable above, we find that the LHS phrases of these ruleshave entropy 1.33 and 0.00 respectively.
This results inthe following entropy for node n3:1 2S(n3) = 0.64+ ~- 1.33+ ~-0.00 = 1.08The following function determines the set of cutnodesN that either exceed the entropy threshold, or are in-duced by structural equivalence:Funct ion  N ( Smin )1.
N : :  {n :  S(n) > S, ni,-,};2.
Return  N*(N);Here S(n) is the entropy of node n.In a third version of the scheme, the relative frequen-cies of the daughters of the or-nodes are used directlyto calculate the node entropy:S(n)  = ~.
,  - p (n i ln ) .
In p(n, lu )ni:(n,ni)EAHere A is the set of arcs, and {n, ni) is an arc from n tohi.
This is basically the entropy used in \[Quinlan 1986\].Unfortunately, this tends to promote daughters of cut-nodes to in turn become cutnodes, and also results in aproblem with instability, especially in conjunction withthe additional constraints discussed in a later section,since the entropy of each node is now dependent on thechoice of cutnodes.
We must redefine the function N(S)accordingly:Funct ion  N(Smin)1.
N O := 0;2.
Repeat  i := i+  1;N := {n: S(nlg '-1) > S,~i,~}; g i := N*(N);3.
Unt i l  N*" = N i-14.
Return  N i;Here S(n\]N j) is the entropy of node n given that theset of cutnodes is NJ.
Convergence can be ensured 6 bymodifying the termination criterion to be3.
Unt i l  3j e \[0, i -  1\] : p(Ni ,Y  j) < 6(Y i ,N j)for some appropriate set metric p(N1, N2) (e.g.
the sizeof the symmetric difference) and norm-like function6(N1,N2) (e.g.
ten percent of the sum of the sizes),but this is to little avail, since we are not interested insolutions far away from the initial assignment of cut-nodes.F ind ing  the  thresho ldWe will use a simple interval-bisection technique forfinding the appropriate threshold value.
We operatewith a range where the lower bound gives at least thedesired coverage, but where the higher bound doesn't.We will take the midpoint of the range, find the cut-nodes corresponding to this value of the threshold, andcheck if this gives us the desired coverage.
If it does,this becomes the new lower bound, otherwise it becomesthe new upper bound.
If the lower and upper boundsare close to each other, we stop and return the nodescorresponding to the lower bound.
This termination cri-terion can of course be replaced with something moreelaborate.
This can be implemented as follows:Funct ion  N(Co)1.
Stow := 0; Shigh := largenumber; Nc := N(0);2.
I f  Shigh - Sto~o < 6sthen  goto  6Sto,,, + Sh ih .
else Staid := 2 '3.
N := N(Smla);4.
I f  c(g)  < Cothen  Shiflh : :  Srnidelse Sio~, := Smld; N?
:= N;5.
Goto  2;6.
Return  Arc;Here C(N) is the coverage on the test set of the spe-cialized grammar determined by the set of cutnodes N.Actually, we also need to handle the boundary casewhere no assignment of cutnodes gives the required cov-erage.
Likewise, the coverages of the upper and lowerbound may be far apart even though the entropy dif-ference is small, and vice versa.
These problems canreadily be taken care of by modifying the terminationcriterion, but the solutions have been omitted for thesake of clarity.6albeit in exponential time1921) "S => Det N V Prep ~IP"s_np_vp/ \/ \np_det_n vp_vp_pp/\ /\lex lex vp_v pp_prep_npI /\lex lex NP2) "S => Pron V NP"s_npvp/ \np_pron vp_v_npI / \lex lex NP3) "NP => Det N"npdet_n/\lex lex4) "NP => NP Prep NP"np_np_pp/\NP pp_prep_np/\lex NP5) "NP => Nu~"np _hUmIlexFigure 3: The specialized rulesIn the running example, using the weighted sum ofthe phrase entropies as the node entropy, if any thresh-old value less than 1.08 is chosen, this will yield anydesired coverage, since the single test example of Fig-ure 1 is then covered.Retr iev ing  the  spec ia l i zed  ru lesWhen retrieving the specialized rules, we will matcheach training example against the and-or tree.
If thecurrent node is a cutnode, we will cut at this point inthe training example.
The resulting rules will be theset of cut-up training examples.
A threshold value ofsay 1.00 in our example will yield the set of cutnodes{u3, n4, n6, ng} and result in the set of specialized rulesof Figure 3.If we simply let the and-or tree determine the setof specialized rules, instead of using it to cut up thetraining examples, we will in general arrive at a largernumber of rules, since some combinations of choices in6) "S => Det N V NP"s_np_vp/\np_det_n vp_vnp/ \  / \lex lex lex NP7) "S => Pron Y Prep NP"s_np_vp/ \np_pren vpvp_ppI / \lex vp_v pp_prep_npI / \l ex  lex  NPFigure 4: Additional specialized rulesthe and-or tree may not correspond to any training ex-ample.
If this latter strategy is used in our example,this will give us the two extra rules of Figure 4.
Notethat they not correspond to any training example.ADDIT IONAL CONSTRAINTSAs mentioned at the beginning, the specialized gram-mar is compiled into LR parsing tables.
Just findingany set of cutnodes that yields the desired coveragewill not necessarily result in a grammar that is wellsuited for LP~ parsing.
In particular, LR parsers, likeany other parsers employing a bottom-up arsing strat-egy, do not blend well with empty productions.
This isbecause without top-down filtering, any empty produc-tion is applicable at any point in the input string, and anaive bottom-up arser will loop indefinitely.
The LRparsing tables constitute a type of top-down filtering,but this may not be sufficient o guarantee termination,and in any case, a lot of spurious applications of emptyproductions will most likely take place, degrading per-formance.
For these reasons we will not allow learnedrules whose RHSs are empty, but simply refrain fromcutting in nodes of the parse trees that do not dominateat least one lexical lookup.Even so, the scheme described this far is not totallysuccessful, the performance is not as good as usinghand-coded tree-cutting criteria.
This is conjecturedto be an effect of the reduction lengths being far tooshort.
The first reason for this is that for any spuriousrule reduction to take place, the corresponding RHSphrases must be on the stack.
The likelihood for this tohappen by chance decreases drastically with increasedrule length.
A second reason for this is that the numberof states visited will decrease with increasing reductionlength.
This can most easily be seen by noting that thenumber of states visited by a deterministic LR parserequals the number of shift actions plus the number ofreductions, and equals the number of nodes in the cot-193responding parse tree, and the longer the reductions,the more shallow the parse tree.The hand-coded operationality criteria result in anaverage rule length of four, and a distribution of reduc-tion lengths that is such that only 17 percent are oflength one and 11 percent are of length two.
This is insharp contrast o what the above scheme accomplishes;the corresponding figures are about 20 or 30 percenteach for lengths one and two.An attempted solution to this problem is to imposerestrictions on neighbouring cutnodes.
This can bedone in several ways; one that has been tested is toselect for each rule the RHS phrase with the least en-tropy, and prescribe that if a node corresponding to theLHS of the rule is chosen as a cutnode, then no nodecorresponding to this RHS phrase may be chosen as acutnode, and vice versa.
In case of such a conflict, thenode (class) with the lowest entropy is removed fromthe set of cutnodes.We modify the function N* to handle this:2.
Repeat  i := i+  1; N i := N(N i-1) \ B(Ni-1);Here B(NJ) is the set of nodes in NJ that should be re-moved to avoid violating the constraints on neighbour-ing cutnodes.
It is also necessary to modify the termi-nation criterion as was done for the function N(S,,~in)above.
Now we can no longer safely assume that thecoverage increases with decreased entropy, and we mustalso modify the interval-bisection scheme to handle this.It has proved reasonable to assume that the coverageis monotone on both sides of some maximum, whichsimplifies this task considerably.EXPERIMENTAL  RESULTSA module realizing this scheme has been implementedand applied to the very setup used for the previous ex-periments with the hand-coded tree-cutting criteria, see\[Samuelsson 1994a\].
2100 of the verified parse trees con-stituted the training set, while 230 of them were usedfor the test set.
The table below summarizes the re-sults for some grammars of different coverage xtractedusing:1.
Hand-coded tree-cutting criteria.2.
Induced tree-cutting criteria where the node entropywas taken to be the phrase entropy of the RHIS phraseof the dominating rammar ule.3.
Induced tree-cutting criteria where the node entropywas the sum of the phrase entropy of the RHS phraseof the dominating rammar ule and the weightedsum of the phrase entropies of the LHSs of the alter-native choices of grammar ules to resolve on.In the latter two cases experiments were carried outboth with and without the restrictions on neighbouringcutnodes discussed in the previous ection.Coverage90.2 %Hand-coded tree-cutting criteriaReduction lengths (%) Times (ms)1 2 3 > 4 Ave. Med.17.3 11.3 21.6 49.8 72.6 48.0RHS phrase entropy.
Neighbour estrictionsCoverage Reduction lengths (%) Times (ms)1 2 3 > 4 Ave. Med.75 .8% 11.8 26.1 17.7 44.4 128 38.580.5% 11.5 27.4 20.0 41.1 133 47.285.3% 14.0 37.3 24.3 24.4 241 70.5RI-IS phrase entropy.Coverage Reduction1 275.8 % 8.3 12.479.7 % 9.0 16.285.3 9{ 8.4 17.390.9 % 18.2 27.5No neighbour estrictionslengths (%) Times (ms)3 > 4 Ave. Med.25.6 53.7 76.7 37.026.9 47.9 99.1 49.431.1 43.2 186 74.021.7 32.6 469 126Mixed phrase entropies.
Neighbour estrictionsCoverage Reduction lengths (%) Times (ms)1 2 3 > 4 Ave. Med.75.3 % 6.1 11.7 30.8 51.4 115.4 37.5Mixed phrase entropies.
No neighbour estrictionsCoverage Reduction lengths (%) Times (ms)1 2 3 > 4 Ave. Med.75 % 16.1 13.8 19.8 50.3 700 92.080 % 18.3 16.3 20.1 45.3 842 108With the mixed entropy scheme it seems importantto include the restrictions on neighbouring cutnodes,while this does not seem to be the case with the RHSphrase entropy scheme.
A potential explanation for thesignificantly higher average parsing times for all gram-mars extracted using the induced tree-cutting criteriais that these are in general recursive, while the hand-coded criteria do not allow recursion, and thus onlyproduce grammars that generate finite languages.Although the hand-coded tree-cutting criteria aresubstantially better than the induced ones, we mustremember that the former produce a grammar that inmedian allows 60 times faster processing than the orig-inal grammar and parser do.
This means that even ifthe induced criteria produce grammars that are a fac-tor two or three slower than this, they are still approx-imately one and a half order of magnitude faster thanthe original setup.
Also, this is by no means a closedresearch issue, but merely a first attempt o realize thescheme, and there is no doubt in my mind that it canbe improved on most substantially.SUMMARYThis article proposes a method for automatically find-ing the appropriate tree-cutting criteria in the EBGscheme, rather than having to hand-code them.
TheEBG scheme has previously proved most successful for194tuning a natural-language rammar to a specific ap-plication domain and thereby achieve very much fasterparsing, at the cost of a small reduction in coverage.Instruments have been developed and tested for con-trolling the coverage and for avoiding a large numberof short reductions, which is argued to be the mainsource to poor parser performance.
Although theseinstruments are currently slightly too blunt to enableproducing rammars with the same high performanceas the hand-coded tree-cutting criteria, they can mostprobably be sharpened by future research, and in par-ticular refined to achieve the delicate balance betweenhigh coverage and a distribution of reduction lengthsthat is sufficiently biased towards long reductions.
Also,banning recursion by category specialization, i.e.
by forexample distinguishing NPs that dominate other NPsfrom those that do not, will he investigated, since this isbelieved to be an important ingredient in the version ofthe scheme mploying hand-coded tree-cutting criteria.ACKNOWLEDGEMENTSThis research was made possible by the basic researchprogramme at the Swedish Institute of Computer Sci-ence (SICS).
I wish to thank Manny Rayner of SRIInternational, Cambridge, for help and support in mat-ters pertaining to the treebank, and for enlighteningdiscussions of the scheme as a whole.
I also wish tothank the NLP group at SICS for contributing to avery conductive atmosphere towork in, and in particu-lar Ivan Bretan for valuable comments on draft versionsof this article.
Finally, I wish to thank the anonymousreviewers for their comments.Re ferences\[Alshawi ed.
1992\] Hiyan Alshawi, editor.
The CoreLanguage Engine, MIT Press 1992.\[Jelinek 1990\] Fred Jelinek.
"Self-Organizing LanguageModels for Speech Recognition", in Readings inSpeech Recognition, pp.
450-506, Morgan Kauf-mann 1990.\[Mitchell el al 1986\]Tom M. Mitchell, Richard M. Keller and SmadarT.
Kedar-Cabelli.
"Explanation-Based Generaliza-tion: A Unifying View", in Machine Learning 1,No.
l, pp.
47-80, 1986.\[Quinlan 1986\] J. Ross Quinlan.
"Induction of DecisionTrees", in Machine Learning 1, No.
1, pp.
81-107,1986.\[Rayner et al1993\] M. Rayner, H. Alshawi, I. Bretan,D.
Carter, V. Digalakis, B. Gamb?ck, J. Kaja,J.
Karlgren, B. Lyberg, P. Price, S. Pulman andC.
Samuelsson.
"A Speech to Speech Transla-tion System Built From Standard Components",in Procs.
ARPA Workshop on Human LanguageTechnology, Princeton, NJ 1993.\[Samuelsson 1994a\] Christer Samuelsson.
Fast Natural-Language Parsing Using Explanation-Based Learn-ing, PhD thesis, Royal Institute of Technology,Stockholm, Sweden 1994.\[Samuelsson 1994b\] Christer Samuelsson.
"Notes onLR Parser Design" to appear in Procs.
15th In-ternational Conference on Computational Linguis-tics, Kyoto, Japan 1994.\[Samuelsson  Rayner 1991\] Christer Samuelsson andManny Rayner.
"Quantitative Evaluation of Ex-planation-Based Learning as an Optimization Toolfor a Large-ScMe Natural Language System", inProcs.
12th International Joint Conference on Ar-tificial Intelligence, pp.
609-615, Sydney, Australia1991.195
