On the Interaction Between True Source,Training, and Testing Language Models*Douglas B. Pault, James K. Baker:F, and Janet M. Baker:tLincoln Laboratory, MITLexington, Ma.
02173:IDragon Systems, Inc.90 Bridge St.Newton, Ma.
02158AbstractAn interaction has been found between the true sourcelanguage model, training language model, and the test-ing language model.
This interaction has implicationsfor vocabulary independent modeling, testing method-ologies, discriminative training, and the adequacy ofour current databases for continuous peech recognition(CSR) development.
The current DARPA databases suf-fer from the described difficulties which suggests thatnew CSR databases are needed if we are to further ad-vance the state-of-the-art.The Interaction During TrainingWhen a category model (e.g.
a context-free (CF)model such as a monophone) is used to a model a setof subcategories (e.g.
context-dependent (CD) modelssuch as triphones), the category model becomes the sub-category prior-probability weighted average of the sub-category models:Meat E PsubeatMsubcatwhere M denotes a model.
(The mathematics used hereare intended to be conceptual rather than rigorous.
Thusmodels will be considered to be averages.
In practice, themethod for deriving a model from a set of sub-models orobservations is highly dependent upon the form of modelused.)
In a field, such as speech recognition, where mod-els are trained from exemplars, the subcategory modelwill generally be:N 1 Msttbcat = ~ ~.= Osubeat,iwhere 08=bcat,i is an observation emitted from the sub-category.
Mcat combines both the subcategory modelsand the prior-probability of the subcategories and simi-larly Msubcat combines the observations and their (sam-pled) prior-probabilities.
*This work was sponsored by the Defense Advanced ResearchProjects Agency.In speech recognition, a phone category would containsome set of subcategories and a subcategory would de-fined by some specific set of context factors.
There aremany factors which may be used to define the subcate-gories \[3\]; a commonly used set is triphone \[18\] subcate-gories and monophone categories.
Alternatively, stressedand unstressed phones might be combined.
(Note thatthis averaging is recursive: subcategories are the combi-nation of some set of subsubcategories and so on...)We assume that speech is generated from some "truesource" language model.
(This language model wouldchange as a function of many factors such as topic, his-tory, and participants, but we will assume it to be con-stunt for each task.)
This true language model is knownfor some artificial tasks such as the DARPA ResourceManagement (RM) database \[16\], but can be estimatedfor naturally elicited speech and text if sufficient data isavailable.
(However, current techniques for estimatinglanguage models are fairly rudimentary.
)Since the acoustic realization of the phones will be afunction of this true language model, any acoustic mod-els averaged over any group of subcategories will learnthis true language model to some degree.
(Learning thelanguage model "to some degree" may be viewed as fa-voring the more likely subcategories.)
Pragmatically, wehave insufficient data to model all relevant subcategoriesseparately and, even if we had sufficient data, we cur-rently have insufficient computational resources to pro-cess all of it in any practical manner.
Thus, since wemust combine subcategories into larger models, a recog-nition system would favor the subcategories that weremore commonly observed uring training.Implications for PerformanceTestingRecognition is performed using some explicit languagemodel.
(No-grammar is a language model in which allfollowing words are equally likely.)
If the performance ofa system is tested using a weaker language model thanthe true source language model, the acoustic models, ifthey have been affected by the training data languagemodel, will strengthen the the total language model inthe recognizer.
Thus, one would expect better recogni-185oIJJ'2o10-5 -I ILL302 ,-.
20IJJ"2o10I ILL0 I I 0 I IWPG NG WPG NGFigure 1: February 89 SD Evaluation Tests Figure 2: October 89 SI-109 Evaluation Teststion performance than would be predicted by the per-plexity of the explicit recognition language model.For example, the RM database was generated from aset of perplexity 9 patterns.
The two official test con-ditions are a perplexity 60 word-pair grammar (WPG)and a perplexity 991 no-grammar (NG).
But since thetraining data is perplexity 9, the acoustic training dataincludes a limited set of contexts.
The explicit testinglanguage model may allow a greater set of contexts, butthe acoustic models in the recognizer are biased towardthe contexts that are actually observed in the trainingdata.
WPG recognition performance would be expectedto be worse if trained on data actually generated by theWPG.
One would expect a similar effect if NG train-ing data were used for NG tests.
The net effect is thatour performance testing is misleading: the performanceobtained with the WPG does not tell us what the perfor-mance would be on a true perplexity 60 task.
Similarlythe NG tests do not tell us what the performance wouldbe on a true perplexity 991 task.In fact, rank orderings of systems tested using the RMdatabase are not necessarily the same for WPG and NGtests.
Figures 1 and 2 show the results of selected site-pairs from the February 89 speaker-dependent (SD) eval-uation tests \[11\] and October 89 speaker-independentwith 109 training speakers (SI-109) evaluation tests\[6, 14\].
The LL systems may be able to make betteruse of language model information stored in the acous-tic models than are the other systems.
(There are otherpossible explanations, but this one cannot currently beruled out.
)Imp l i ca t ions  for D isc r iminat iveT ra in ingDiscriminative training (training for the right an-swer rather than for the "most accurate model") is per-formed in the context of the training data which consistsof samples from true source language model.
In someforms, including corrective training \[2\], it is performedin the additional context of an explicit training languagemodel.
(Corrective training uses a recognition pass toobtain possible confusions with the correct answer andthe training language model is applied in this recognitionpass.
It is the only form of discriminative training thathas been shown to improve performance on large vocab-ulary recognition tasks \[2, 7\].)
Errors or near misses areused to perturb the acoustic models to lessen the possi-bility of error.
But these errors are a strong function ofthe true source model of the training data and the train-ing language model.
Thus, these techniques increase theamount of the true source and training language modelsthat are included in the acoustic models.One of the stated advantages of discriminative train-ing is that it corrects for an incorrect (form of) model.
Itdoes this by altering the trainable factors (parameters)of the model to account both for improper choice of func-tion and for aspects of the true source which have notbeen included in the model.
The second effect is exactlythe above stated problem.Evidence showing that corrective training inserts thetraining language model into the acoustic models has ap-peared in results reported using the CMU SPHINX sys-tem operating on the RM database.
It was found thatan NG corrective-trained set of models, which improvedNG recognition performance, damaged WPG recogni-tion performance compared to a maximum likelihoodtrained set of models \[8\].Imp l i ca t ions  for Vocabu lary  Inde-pendenceThe above suggests that any training methods thataverage over a number of contexts and/or use discrimi-native techniques include the true source language modelin the acoustic models.
Thus any set of acoustic mod-els would be optimized for that specific task and there-fore would be inferior when tested upon another task.CMU has investigated this problem and found RM mod-els to yield poor performance on a different ask \[5\].
We186have also performed some informal experiments whichattempted to port RM triphones to our flight demonstra-tion task (28 word vocabulary, perplexity 7 finite-stategrammar) and found inferior performance compared totask-trained models.What  can be  done?Several things can be done to minimize the damagedone to the acoustic models by the true source languagemodel and any training language models.
Since we havetechniques which implement he language model inde-pendently of the acoustic model, it would help if theacoustic models were as free as possible from biases dueto the training environment.The first technique is to use a source of training datawith a rich and realistic true language model.
(The rich-ness can be further increased by using data from a num-ber of different sources.)
This will provide a rich set ofcontexts to allow our systems to see the full range of con-texts in which a real system will have to operate.
CMUhas already shown that a richer data source improvesone's ability to produce task independent acoustic mod-els \[5\]A second technique is to minimize averaging acrosscontexts in order to limit the ability of the acoustic mod-els to model the language.
A number of sites have al-ready moved in this direction by changing from context-independent phone (monophone) models to left and rightcontext dependent phone (triphone) models \[18\].
h fur-ther step along this line has been the inclusion of cross-word triphone models \[7, 10, 13\] which has minimizedthe ability of the acoustic models to learn the bigramlanguage model.
1 These changes have improved recogni-tion performance when trained and tested on the samedatabase, but their effects on vocabulary independencehave not been tested.A third technique is to use larger training datasets.
("There is no data like more data.").
This allows us totrain more contexts and minimize the smoothing (aver-aging) required to train models.
Comparisons of per-formance with increased amounts of training data us-ing the RM1 database for speaker-independent training(109 vs. 72 speakers) and the RM2 database for speaker-dependent training (2400 vs. 600 sentences per speaker)have shown improved results \[15\] within task.
The CMUvocabulary-independence experiments showed improvedcross-task performance \[5\], but since the amount of data1 If sufficient raining data is available to train the cross-word-context phone models without averaging, cross-word-context-dependent phone modeling will clearly minimize the effects of thebigram language model.
However, the training data for the ref-erenced systems was sufficiently limited that it was necessary tosmooth (weighted average) some phone models for robustness andit was necessary to use cross-word-context-independent (i.e.
aver-aged over observed word-boundary phones) phone models for theunobserved cross-word phone models needed by the recognizer.The net effect for limited training data is unknown--the cross-wordphone models may increase the bigram language model earning forlimited amounts of training data.
Definitive xperiments isolatingthis effect have not been reported.and the richness of the data were increased simultane-ously, it is not currently possible to separate the twoeffects.
We currently use less than ten hours of trainingdata.
In comparison, a typical school age child has heardthousands of hours of speech.A fourth technique is to limit discriminative train-ing to cases where the language model and vocabularyare known at training time.
Discriminative training ex-plicitly alters the acoustic models to reflect the truesource and training language models.
This may im-prove within task performance, but will not help andmay impair cross-task performance.
(The phone modelshave, in effect, been made fragile with respect to vo-cabulary and task.)
However, it is not clear that thiswithin-task advantage will be maintained with a goodlanguage model and better acoustic modeling techniquesusing non-discriminative training on adequate amountsand kinds of data.A final technique is to test with a good languagemodel.
Testing with intentionally weakened languagemodels asks the acoustic modeling to perform a taskthat it has not been trained to do.
While the simplicityof no-grammar testing may be attractive, it is also themost misleading test condition and does not always pre-dict the performance of a system with a good languagemodel.
If a realistic language model is used with prop-erly trained acoustic models, the language model willperform the word sequence modeling and the acousticmodels will perform the acoustic modeling without eachtrying to do the other's job.Given an adequate amount of adequately rich data totrain the acoustic models and an appropriate languagemodel for the task, it should be possible to obtain goodrecognition performance using vocabulary and task in-dependent acoustic models.A Proposa l  for a R ich  and  Real is -t ic DARPA CSR DatabaseNone of the CSR databases currently available to theDARPA community meets all of the above requirements.The RM database was a good database for its time, butwe have since found a number of weaknesses (and createdsome by improving our recognizers.)
It was, however,a focal point around which much progress in CSR wasachieved and if we design and produce a successor prop-erly, we can initiate a similarly productive ra.
(Thefollowing proposed database serves a different purposethan and should be recorded in addition to the DARPAAir Traffic Information System (ATIS) database \[4, 17\].
)A list of desirable features for a CSR database are:1.
Should be based on real human communication toinsure richness and realism(a) Should be based on a large corpus of text ortranscribed speech(b) Transcriptions hould be available to the re-search community to allow language modeling.1872.
A good standardized stochastic language model (orset of models)3.
A large amount of acoustic data(a) Both SD and SI training data(b) Standardized training, development test, andevaluation test data(c) A large number of test speakers to minimizethe effects of speaker variation.4.
The task should be difficult enough to have a suf-ficiently high error rate even with a good languagemodel.
(a) We make the best progress when our tasks havea moderate rror rate(b) We need enough errors for statistically signifi-cant experiments.5.
Extendibility to more difficult acoustic tasks to al-low future growth (e.g.
larger vocabularies).The standardized language models and acoustic datasetsare essential to allow rigorous inter-site system compar-isons.
If the language model training data is available,sites will also be able to work on improved language mod-eling.The specific proposal is:1.
The ACL/DCI contains everal large text databases\[9\].
The best one for our purposes is probably thetranscriptions of Canadian parliamentary hearingspart of the Canadian Hansard database.
(Other vi-able alternatives are the parliamentary debate por-tion of the Hansard database or the Wall StreetJournal texts.)
This is the transcription of about50M words of speech.
It should be possible to de-rive a good bigram or trigram language model fromthis text and several other databases are availableto facilitate cross-database language model investi-gations.
The data is available to the research com-munity.2.
5000 words is probably a good vocabulary size giventhe current state of the art.
We could use the5000 most common words in the text database.If we set aside a block of about 10% of the textfor CSR testing, we can obtain in-vocabulary CSRtest sets and CSR test sets which include out-of-vocabulary words.
This would also allow for ex-tension to larger vocabularies when the CSR tech-nology has improved sufficiently.
The training sen-tences need not be limited to the chosen vocabulary.3.
The database would consist of read speech.
This isfast and cheap to enable us record sufficient acous-tic data.
It would not attempt o cover extem-poraneous peech phenomena.
(Extemporaneousspeech phenomena can be explored using the ATISdatabase.
The ATIS database, however, does nothave sufficient text backup to generate a good sta-tistical anguage model.
)A cheaper, but less useful alternative is the continuousspeech version of the IBM 5000 word vocabulary officecorrespondence (OC-5000) database \[1\].
This databasehas a good bigram language model, has sentence lists(but the test list is only 50 sentences), and has acousticdata which has already been recorded.
However, theunderlying text is not available and the availability ofthe language model and the acoustic data is currentlyuncertain due to unsettled legal issues \[12\].ConclusionOur training methods have been found to include alanguage model bias into our acoustic models.
This biascauses misleading test results and impairs the vocab-ulary independence of our models.
Richer and largeracoustic databases, context dependent modeling, avoid-ing discriminative training methods, and good testinglanguage models all will serve to minimize this bias.
Aspeech database based upon one of the ACL/DCI textdatabases would provide a good arena for continued CSRdevelopment while minimizing problems due to the train-ing biases.References\[1\] A. Averbuch, L. Bahl, R. Bakis, P. Brown, A. Cole,G.
Daggett, S. Das, K. Davies, S. De Gennaro,P.
de Souza, E. Epstein, D. Fraleigh, F. Jeninek,S.
Katz, B. Lewis, R. Mercer, A. Nadas, D. Na-hamoo, M. Picheny, G. Shichman, and P. Spinelli,"An IBM-PC Based Large-Vocabulary Isolated-Utterance Speech Recognizer," ICASSP 86, Tokyo,April 1986.\[2\] L. R. Bahl, P.F.
Brown, P. V. de Souza, and R. L.Mercer, "A New Algorithm for the Estimation ofHidden Markov Model Parameters," Proc.
ICASSP88, New York, NY, April 1988.\[3\] F. R. Chen, "Identification f Contextual Factor ForPronunciation Networks," Proc.
ICASSP90, Albu-querque, New Mexico, April 1990.\[4\] C. Hemphill, "TI Implementation f Corpus Collec-tion," this proceedings, June 1990.\[5\] H. W. Hon and K. F. Lee, "On Vocabulary-Independent Speech Modeling," Proc.
ICASSP90,Albuquerque, New Mexico, April 1990.\[6\] C. H. Lee, L. R. Rabiner, R. Pieraccini, and J.G.
Wilpon, "Acoustic Modeling of Subword Unitsfor Large Vocabulary Speaker Independent SpeechRecognition," Proceedings October 1989 DARPASpeech and Natural Language Workshop, MorganKaufmann Publishers, October 1989.\[7\] K. F. Lee, It.
W. Hon, and M. Y. Hwang, "RecentProgress in the SPHINX Speech Recognition Sys-tem," Proceedings February 1989 DARPA Speech188and Natural Language Workshop, Morgan Kauf-mann Publishers, February 1989.
[8] K. F. Lee and S. Mahajan, "Corrective and Rein-forcement Learning for Speaker-Independent Con-tinuous Speech Recognition," Technical ReportCMU-CS-89-100, Carnegie-Mellon University, Jan-uary 1989.
[9] M. Liberman, "Text on Tap: the ACL/DCI," Pro-ceedings October 1989 DARPA Speech and NaturalLanguage Workshop, Morgan Kaufmann Publish-ers, October 1989.
[10] H. Murveit, M. Cohen, P. Price, G. Baldwin,M.
Weintraub, and J. Bernstein, "SRI's DE-CIPHER System," Proceedings February 1989DARPA Speech and Natural Language Workshop,Morgan Kaufmann Publishers, February 1989.
[11] D. Pallett, "Speech Results on Resource Manage-ment Task," Proceedings February 1989 DARPASpeech and Natural Language Workshop, MorganKaufmann Publishers, February 1989.
[12] D. Pallett, personal communication, June 1990.
[13] D. B. Paul, "The Lincoln Continuous Speech Recog-nition System: Recent Developments and Results,"Proceedings February 1989 DARPA Speech andNatural Language Workshop, Morgan KaufmannPublishers, February 1989.
[14] D. B. Paul, "Tied Mixtures in the Lincoln RobustCSR," Proceedings October 1989 DARPA Speechand Natural Language Workshop, Morgan Kauf-mann Publishers, October 1989.
[15] D. B. Paul, "The Lincoln Tied Mixture CSR," thisproceedings, June 1990.
[16] P. Price, W. Fischer, J. Bernstein, and D. Pal-lett, "The DARPA 1000-word Resource Manage-ment Database for Continuous Speech Recogni-tion," Proc.
ICASSP 88, New York, April 1988.
[17] P. Price, The ATIS Common Task: Selection andOverview," this proceedings, June 1990.
[18] R. Schwartz, Y. Chow, O. Kimball, S. Roucos,M.
Krasner, and J. Makhoul, "Context-DependentModeling for Acoustic-Phonetic Recognition ofContinuous Speech," Proc.
ICASSP 85, Tampa, FL,April 1985.189
