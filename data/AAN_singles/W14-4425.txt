Proceedings of the 8th International Natural Language Generation Conference, pages 152?156,Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational LinguisticsTwo-Stage Stochastic Natural Language Generation for Email Synthesisby Modeling Sender Style and Topic StructureYun-Nung Chen and Alexander I. RudnickySchool of Computer Science, Carnegie Mellon University5000 Forbes Ave., Pittsburgh, PA 15213-3891, USA{yvchen, air}@cs.cmu.eduAbstractThis paper describes a two-stage pro-cess for stochastic generation of email, inwhich the first stage structures the emailsaccording to sender style and topic struc-ture (high-level generation), and the sec-ond stage synthesizes text content basedon the particulars of an email elementand the goals of a given communication(surface-level realization).
Synthesizedemails were rated in a preliminary experi-ment.
The results indicate that sender stylecan be detected.
In addition we foundthat stochastic generation performs betterif applied at the word level than at anoriginal-sentence level (?template-based?
)in terms of email coherence, sentence flu-ency, naturalness, and preference.1 IntroductionThis paper focuses on generating language for theemail domain, with the goal of producing mailsthat reflect sender style and the intent of the com-munication.
Such a process might be used for thegeneration of common messages (for example arequest for a meeting without direct interventionfrom the sender).
It can also be used in situationswhere naturalistic email is needed for other ap-plications.
For instance, our email generator wasdeveloped to provide emails to be used as part ofsynthetic evidence of insider threats for purposesof training, prototyping, and evaluating anomalydetectors (Hershkop et al., 2011).There are two approaches to natural languagegeneration (NLG), one focuses on generating textusing templates or rules (linguistic) methods, theanother uses corpus-based statistical techniques.Oh and Rudnicky (2002) showed that stochasticgeneration benefits from two factors: 1) it takesadvantage of the practical language of a domainexpert instead of the developer and 2) it restatesthe problem in terms of classification and label-ing, where expertise is not required for developinga rule-based generation system.
They found thatnaive listeners found such utterances as accept-able as human-generated utterances.
Belz (2005)also proposed a probabilistic NLG approach tomake systems more robust and components morereusable, reducing manual corpus analysis.However, most work usually focused on well-structured documents such as news and Wikipedia,while email messages differ from them, whichreflect senders?
style and are more spontaneous.Lampert et al.
(2009) segmented email messagesinto zones, including sender zones, quoted con-versation zones, and boilerplate zones.
This paperonly models the text in the sender zone, new con-tent from the current sender.
In the present work,we investigate the use of stochastic techniques forgeneration of a different class of communicationsand whether global structures can be convincinglycreated in the email domain.A lot of NLG systems are applied in dialoguesystems, some of which focus on topic model-ing (Sauper and Barzilay, 2009; Barzilay and Lap-ata, 2008; Barzilay and Lee, 2004), proposing al-gorithms to balance local fit of information andglobal coherence.
However, they seldom con-sider to model the speaker?s characteristics.
Gillet al.
(2012) considered sentiment such as open-ness and neuroticism to specify characters for di-alogue generation.
In stead of modeling authors?attitudes, this paper proposes the first approach ofsynthesizing emails by modeling their writing pat-terns.
Specifically we investigate whether stochas-tic techniques can be used to acceptably modellonger texts and individual speaker characteristicsin the emails, both of which may require highercohesion to be acceptable.2 Overview of FrameworkOur proposed NLG approach has three steps: pre-processing training data, modeling sender styleand topic structure for email organization, fol-lowed by surface realization, shown in Figure 1.In preprocessing, we segment sentences foreach email, and label email structural elements.This is used to create a structural label sequencefor each email, and then used to model senderstyle and topic structure for email organization(1st stage in the figure).
Content slots are alsoannotated for surface realization (2nd stage in thefigure).
Details are in Section 3.From the annotated corpus, we build sender-specific and topic-specific structure languagemodels based on structural label sequences, anduse a mixture sender-topic-specific model tostochastically generate email structure in the firststage.
The process is detailed in Section 4.152PredictingMixtureModelsEmailDocumentArchiveBuildingStructureLM Structural LabelAnnotation StructuralLabelSequencesGeneratingEmailStructuresGenerated StructuralLabel Sequences<greeting><inform>?Slot-Value PairsSlotAnnotation Emails w/SlotsBuildingContentLMGeneratingText ContentScoringEmailCandidatesEmail CandidatesFillingSlotsSynthesized EmailsHi PeterToday?s ...1st Stage: Modeling Sender Style and Topic Structure for Email Organization2nd Stage: Surface RealizationHi [Person]Today?s ...Sender-SpecificModelTopic-SpecificModelSender TopicInput to NLGTraining Data PreprocessingFigure 1: The proposed framework of two-stage NLG component.In the second stage, we build a content lan-guage model for each structural element and thenstochastically generate sentences using the se-quence generated in the first stage.
To ensure thatrequired slot-value pairs occur in the text, candi-dates emails are filtered to retain only those textsthat contain the desired content slots.
These slotsare then filled to produce the final result.
Section 5explains the process.3 Training Data PreprocessingTo model sender style and topic structure, we an-notate the data with defined structural labels inSection 3.1, and data with slots to model text con-tent of language in Section 3.2.3.1 Structural Label AnnotationBased on examination of the corpus, we defined10 email structure elements:1. greeting: a friendly expression or respectfulphrase, typically at the start of an email.2.
inform: to give or impart knowledge of a factor circumstance.3.
request: the act of asking for something to begiven or done, especially as a favor or cour-tesy.4.
suggestion: to mention or introduce (an idea,proposition, plan, etc.)
for consideration orpossible action.5.
question: an interrogative sentence in anform, requesting information in reply.6.
answer: a reply or response to a question, etc.7.
regard: to have or show respect or concernfor, usually at the end of an email.8.
acknowledgement: to show or express appre-ciation or gratitude.9.
sorry: express regret, compunction, sympa-thy, pity, etc.10.
signature: a sender?s name usually at the endof the email.We perform sentence segmentation using punc-tuation and line-breaks and then manually tag eachsentence with a structure label.
We exclude theheader of emails for labeling.
Figure 2 shows anexample email with structural labels.From:  Kitchen, Louise Sent: Thursday, April 05, 2001 11:15 AM To: Beck, Sally Cc: Piper, Greg; Jafry, Rahil Subject: Re: Costs   Shukaly resigned and left.
But I assume the invitation will be extended to all of their groups so that whoever they want can attend.I would actually prefer that the presentation is actually circulated to the groups on Friday rather than presented as we will wait forever on getting an offsite together.
How about circulating the presentation and then letting them refer all questions to Rahil - see how much interest you get.
One on ones are much better and I think this is how Rahil should proceed.We need to get in front of customers in the next couple of weeks.
Let's aim to get a least three customers this quarter.
LouisesuggestioninformrequestsignatureheadercontentFigure 2: The email with structural labels.3.2 Slot AnnotationThe input to NLG may contain the informationthat needs to be included in the synthesized emails.Tokens in the corpus text corresponding to slotsare replaced by slot (or concept) tokens prior tobuilding content language models.
Slots are clas-sified into general class and topic class below.3.2.1 General ClassWe use existing named entity recognition (NER)tools for identifying general classes.
Finkel et al.
(2005) used CRF to label sequences of words intext that are names of things, such as person, or-ganization, etc.
There are three models trained ondifferent data, which are a 4-class model trainedfor CoNLL1, a 7-class model trained for MUC,and a 3-class model trained on both data sets forthe intersection of those class sets below.?
4-class: location, person, organization, misc?
7-class: location, person, organization, time,money, percent, dateConsidering that 3-class model performs higheraccuracy and 7-class model provides better cover-age, we take the union of outputs produced by 3-class and 7-class models and use the labels outputby 3-class model if the two models give differentresults, since the 3-class model is trained on bothdata sets and provides better accuracy.1http://www.cnts.ua.ac.be/conll2003/ner/153sender-specific modeltopic-specific modelmixture modelFigure 3: The visualization of the mixture model.3.2.2 Topic ClassMany named entities cannot be recognized by ageneral NER, because they are topic-specific in-formation.
Accordingly we define additional enti-ties that are part of the email domain.4 Modeling Sender Style and TopicStructure for Email OrganizationGiven a target sender and topic focus specified insystem input, email structures can be generated bypredicted sender-topic-specific mixture models.4.1 Building Structure Language ModelsBased on the annotation of structural labels, eachemail can be expressed as a structural label se-quence.
Then we can train a sender-specific anda topic-specific structure model using the emailsfrom each sender and the emails related to eachtopic respectively.
Here the structure models aren-gram models with Good-Turing smoothing (n =3) (Good, 1953).4.2 Predicting Mixture ModelsUsing sender-specific and topic-specific structuremodels, we predict sender-topic-specific mixturemodels by interpolation:Pi,j(l) = ?P si (l) + (1?
?
)P tj (l), (1)where Pi,j(l) is the estimated probability that thestructural label l occurs from the sender i and forthe topic j, P si (l) is the probability of the struc-tural label l from the sender i (regardless of top-ics), P tj (l) is the probability of the structural labell related to the topic j (regardless of senders), and?
is the interpolation weight, balancing betweensender style and topic focus.
Figure 3 illustratesthe mixture models combined by sender-specificand topic-specific models.4.3 Generating Email StructureWe generate structural label sequences randomlyaccording to the distribution from sender-topic-specific models.
To generate the structural labelsequences from the sender i and related to thetopic j, the probability of the structural label lkusing n-gram language model isPi,j(lk) = Pi,j(lk | lk?1, lk?2, ..., lk?(n?1)).
(2)Since we use smoothed trigrams, we may gen-erate unseen trigrams based on back-off methods,resulting in some undesirable randomness.
Wetherefore exclude unreasonable emails that don?tfollow two simple rules.1.
The structural label ?greeting?
only occurs atthe beginning of the email.2.
The structural label ?signature?
only occursat the end of the email.5 Surface RealizationOur surface realizer has four elements: buildinglanguage models, generating text content, scoringemail candidates, and filling slots.5.1 Building Content Language ModelsAfter replacing the tokens with slots, for eachstructural label, we train an unsmoothed n-gramlanguage model using all sentences with that struc-tural label.
We make a simplifying assumptionthat the usage of within-sentence language can betreated as independent across senders; generatingthe text content only considers the structural la-bels.
We use 5-gram to balance variability in gen-erated sentences while minimizing nonsense sen-tences.Given a structural label, we use the content lan-guage model probability directly to predict thenext word.
The most likely sentence is W ?
=argmaxP (W | l), where W is a word sequenceand l is a structural label.
However, in order tointroduce more variation, we do not look for themost likely sentence but generate each word ran-domly according to the distribution similar to Sec-tion 4.3 and illustrated below.5.2 Generating Text ContentThe input to surface realization is the generatedstructural label sequence.
We use the correspond-ing content language model trained for the givenstructural label to generate word sequences ran-domly according to the distribution from the lan-guage model.
The probability of a word wi usingthe n-gram language model isP (wi) = P (wi | wi?1, wi?2, ..., wi?
(n?1), l),(3)where l is the input structural label.
Since we buildseparate models for different structural labels, (3)can be written asP (wi) = P (wi | wi?1, wi?2, ..., wi?
(n?1)) (4)using the model for l.Using unsmoothed 5-grams will not generateany unseen 5-grams (or smaller n-grams at the be-ginning and end of a sentence).
This precludesgeneration of nonsense sentences within the 5-word window.
Given a generated structural labelsequence, we can generate multiple sentences tocreate a synthesized email.1545.3 Scoring Email CandidatesThe input to NLG contains the required informa-tion that needs to be in the output email, as de-scribed in Section 3.2.
For each synthesized email,we penalize it if the email 1) contains slots forwhich there is no provided valid value, or 2) doesnot have the required slots.The content generation engine stochasticallygenerates an email candidate and scores it.
If theemail has a zero penalty it is passed on.5.4 Filling SlotsThe last step is to fill slots with the appropriatevalues.
For example, the sentence ?Tomorrow?s[meeting] is at [location].?
could become ?Tomor-row?s speech seminar is at Gates building.
?6 Experiments6.1 SetupThe corpus used for our experiments is the EnronEmail Dataset2, which contains a total of about0.5M messages.
We selected the data related todaily business for our use, including data fromabout 150 users.
We randomly picked 3 senders,ones who wrote many emails, and defined addi-tional 3 topic classes (meeting, discussion, issue)as topic-specific entities for the task.
Each sender-specific model (across topics) or topic-specificmodel (across senders) is trained on 30 emails.6.2 Evaluation of Sender Style ModelingTo evaluate the performance of sender style, 7 sub-jects were given 5 real emails from each senderand then 9 synthesized emails.
They were askedto rate each synthesized email for each sender ona scale of 1 (highly confident that the email is notfrom the sender) to 5 (highly confident that theemail is from that sender).With ?
= 0.75 in (1) for predicting mix-ture models (higher weight for sender-specificmodel), average normalized scores the corre-sponding senders receives account for 45%; thisis above chance (which would be 33%).
This sug-gests that sender style can be noticed by subjects,although the effect is weak, and we are in the pro-cess of designing a larger evaluation.
In a follow-up questionnaire, subjects indicated that their rat-ings were based on greeting usage, politeness, thelength of email and other characteristics.6.3 Evaluation of Surface RealizationWe conduct a comparative evaluation of two dif-ferent generation algorithms, template-based gen-eration and stochastic generation, on the sameemail structures.
The average number of sen-tences in synthesized emails is 3.8, because ourdata is about daily business and has relatively shortemails.
Given a structural label, template-based2https://www.cs.cmu.edu/?enron/generation consisted of randomly selecting an in-tact whole sentence with the target structural label.This could be termed sentence-level NLG, whilestochastic generation is word-level NLG.We presented 30 pairs of (sentence-, word-)synthesized emails, and 7 subjects were asked tocompare the overall coherence of an email, its sen-tence fluency and naturalness; then select theirpreference.
Table 1 shows subjects?
preferenceaccording to the rating criteria.
The word-basedstochastic generation outperforms or performs aswell as the template-based algorithm for all cri-teria, where a t-test on an email as a random vari-able shows no significant improvement but p-valueis close to 0.05 (p = 0.051).
Subjects indicatedthat emails from word-based stochastic genera-tion are more natural; word-level generation is lesslikely to produce an unusual sentences from thereal data; word-level generation produces moreconventional sentences.
Some subjects noted thatneither email seemed human-written, perhaps anartifact of our experimental design.
Nevertheless,we believe that this stochastic approach would re-quire less effort compared to most rule-based ortemplate-based systems in terms of knowledge en-gineering.Template Stochastic No Diff.Coherence 36.19 38.57 25.24Fluency 28.10 40.48 31.43Naturalness 35.71 45.71 18.57Preference 36.67 42.86 20.48Overall 34.17 41.90 23.93Table 1: Generation algorithm comparison (%).7 ConclusionThis paper presents a two-stage stochastic NLGfor synthesizing emails: first a structure is gener-ated, and then text is generated for each structureelement, where sender style and topic structurecan be modeled.
Subjects appear to notice senderstyle and can also tell the difference between tem-plates using original sentences and stochasticallygenerated sentences.
We believe that this tech-nique can be used to create realistic emails and thatemail generation could be carried out using mix-tures containing additional models based on othercharacteristics.
The current study shows that emailcan be synthesized using a small corpus of labeleddata; however these models could be used to boot-strap the labeling of a larger corpus which in turncould be used to create more robust models.AcknowledgmentsThe authors wish to thank Brian Lindauer and KurtWallnau from the Software Engineering Instituteof Carnegie Mellon University for their guidance,advice, and help.155ReferencesRegina Barzilay and Mirella Lapata.
2008.
Modeling lo-cal coherence: An entity-based approach.
ComputationalLinguistics, 34(1):1?34.Regina Barzilay and Lillian Lee.
2004.
Catching the drift:Probabilistic content models, with applications to genera-tion and summarization.
In HLT-NAACL, pages 113?120.Anja Belz.
2005.
Corpus-driven generation of weather fore-casts.
In Proc.
3rd Corpus Linguistics Conference.Jenny Rose Finkel, Trond Grenager, and Christopher Man-ning.
2005.
Incorporating non-local information into in-formation extraction systems by gibbs sampling.
In Proc.of ACL, pages 363?370.Alastair J Gill, Carsten Brockmann, and Jon Oberlander.2012.
Perceptions of alignment and personality in gener-ated dialogue.
In Proceedings of the Seventh InternationalNatural Language Generation Conference, pages 40?48.Association for Computational Linguistics.Irving J Good.
1953.
The population frequencies of speciesand the estimation of population parameters.
Biometrika,40(3-4):237?264.Shlomo Hershkop, Salvatore J Stolfo, Angelos D Keromytis,and Hugh Thompson.
2011.
Anomaly detection at multi-ple scales (ADAMS).Andrew Lampert, Robert Dale, and Ce?cile Paris.
2009.
Seg-menting email message text into zones.
In Proceedingsof the 2009 Conference on Empirical Methods in NaturalLanguage Processing, volume 2, pages 919?928.
Associ-ation for Computational Linguistics.Alice H Oh and Alexander I Rudnicky.
2002.
Stochastic nat-ural language generation for spoken dialog systems.
Com-puter Speech & Language, 16(3):387?407.Christina Sauper and Regina Barzilay.
2009.
Automati-cally generating wikipedia articles: A structure-aware ap-proach.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing of theAFNLP: Volume 1-Volume 1, pages 208?216.
Associationfor Computational Linguistics.156
