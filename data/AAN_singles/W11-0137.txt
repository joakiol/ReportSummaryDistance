Question Classification for Email?Rachel CotterillUniversity of Sheffieldr.cotterill@sheffield.ac.ukAbstractQuestion classifiers are used within Question Answering to predict the expected answer typefor a given question.
This paper describes the first steps towards applying a similar methodology toidentifying question classes in dialogue contexts, beginning with a study of questions drawn from theEnron email corpus.
Human-annotated data is used as a gold standard for assessing the output froman existing, open-source question classifier (QA-SYS).
Problem areas are identified and potentialsolutions discussed.1 Introduction and MotivationIn information retrieval, question classification is an important first stage of the question answering task.A question classification module typically takes a question as input and returns the class of answer whichis anticipated.
In an IR context, this enables candidate answers to be identified within a set of documents,and further methods can then be applied to find the most likely candidate.The present work is motivated by a desire to identify questions and their answers in the context ofwritten dialogue such as email, with the goal of improving inbox management and search.
Reconstruc-tion of meaning in a single email may often be impossible without reference to earlier messages in thethread, and automated systems are not yet equipped to deal with this distribution of meaning, as textmining techniques developed from document-based corpora such as newswire do not translate naturallyinto the dialogue-based world of email.
Take the following hypothetical exchange:From: john@example.com To: jane@example.comJane,Can you let me know the name of your lawyer?
Thanks.JohnFrom: jane@example.com To: john@example.comAlly McBeal.-- JaneThis is an extreme example, but it serves to illustrate the ?separate document problem?
in emailprocessing.
Context is critical to pragmatic analysis, but with email and related media the context (andconsequently, a single piece of information) may be spread across more than one document.
In this casethe second message in isolation gives no information concerning ?Ally McBeal?
as we do not have anycontext to put her in.
However, by considering the question and answer pair together, we can discoverthat she is a lawyer (or, at the very least, that Jane believes or claims that to be the case; a philosophicaldistinction best left aside for the time being).It is anticipated that questions in a dialogue context will exhibit a somewhat different range of typesto typical IR questions, but that some will indeed be seeking the kind of factual information for whichQA classifiers are currently designed.
If this subset of fact-seeking questions can be reliably identified?The author would like to thank GCHQ for supporting this research.330by an automated process, then existing question classifiers could be used to identify the expected answertype.
Candidate answers could then be sought within the text of replies to the message in which thequestion is asked.This paper briefly describes the gold standard data (Section 2), compares human annotation to theoutput of Ng & Kan?s (2010) QA-SYS question classifier (Section 3), and proposes some future direc-tions for research (Section 4).2 The DataIn order to investigate question types in email, a suitable set of questions was required.
To this endquestions were automatically extracted from CMU?s deduplicated copy of the Enron corpus (Klimt &Yang 2004).
Of the 39,530 unique question strings identified in Enron outboxes, a random sample of1147 were manually examined and annotated with the expected question type.A number of taxonomies have been proposed for classifying answer types, of which Li & Roth?s(2002) two-tier hierarchy is a reasonably comprehensive and widely-adopted example.
Their coarseclasses are Abbreviation (ABBR), Description (DESC), Entity (ENTY), Human (HUM), Location (LOC),and Numeric (NUM), and they then define a set of 50 subclasses.
Table 1 shows how Li & Roth?s taxon-omy was mapped to the category labels adopted for the current work.Cotterill 2010 Li & Roth 2002 %Person(s) HUM{individual,title} 2.53Group or Organisation HUM{group} 0.17Descriptive textHUM{description}11.51DESC{manner, definition, description}Reason DESC{reason} 1.57Date or Time NUM{date, period} 3.57Numeric NUM{weight, volume/size, ordinal, percentage, count, speed,money, temperature, distance, other}1.92Phone NUM{code}1 0.40URL 0.17Email 0.17Place LOC{country, state, city, mountain, other} 0.96Animal ENTY{animal} 0.00Physical Object ENTY{instrument, plant, body part, vehicle, food, product,substance}0.30Concept ENTY{language, religion, letter, color, creative/artwork, dis-ease/medical, currency}0.40Event or Activity ENTY{event, sport, technique/method} 0.87OtherENTY{symbol, term, word, other}0.00ABBR{abbreviation, expression}Yes/No 41.33Action Request 8.98Rhetorical 5.23Multiple 3.23Non-Question 16.74Table 1: The new dialogue taxonomy, with mappings to Li & Roth where applicable, and percentagedistribution in the Enron sample1Phone number is actually a subset of the NUM:code category, but it accounts for all instances in the Enron sample.331?Are you guys still thinking of maybe joining us skiing??
Yes/No?Did you know Moller was going to be on TV or were you justchannel surfing?
?Multiple choice?Do you stock those wheels and tires or would I have to orderthem??
?Will it ever end????
Rhetorical?Would you please handle this?
?Action Request?Also, could you check for reservations at the Georgian hotel inSanta Monica?
?Table 2: Examples of questions in dialogue-specific categoriesA number of extra categories were added to account for the nature of the data, as identified bypreliminary experiments.
Examples of questions falling into some of the new categories are presented inTable 2.It is important to observe that a massive 75.5% of questions in the Enron sample do not fall into anyof the categories defined by Li & Roth.
Assuming that this is a fair representation of the distributionacross the Enron corpus (if not email as a whole) then we are clearly justified in stating that some furtherwork will be required before question classification can be meaningfully applied to the email task.The most common category is Yes/No, giving a ?most common class?
baseline of 41.3%.
That isto say, a classification system which classified every question as a Yes/No question would expect to seeaccuracy in this region, and any results must be considered in this context.The most common of the IR-derived categories is Description, representing 11.51% of questionsoverall, or 46.2% of those falling into IR categories.
This compares to 26.6% reported across the equiv-alent categories in Li & Roth?s analysis of TREC questions.Full details of the Enron question dataset will be published in due course.3 Performance of QA-SYSQANUS (Ng & Kan 2010) is an open-source question answering framework which uses the Li & Rothcategories in its question classification module.
The framework is designed to be extensible, whichmakes it a good candidate for further work.
However, the results presented in this section deal only withthe output of the QA-SYS default question processing module as supplied with QANUS v26Jan2010.The question classification component of QA-SYS is an instance of the Stanford classifier, a supervisedlearning module trained on a dataset of information retrieval questions.Ng & Kan do not report their question classification accuracy, providing figures only for the ?factoidaccuracy?
of the end-to-end question answering system, which makes it difficult to compare their resultsto the present study.
However Huang, Thint & Cellikyilmaz (2009) publish results for a maximumentropy classifier trained on similar IR data, reporting an encouragingly high accuracy of 89.0%.QA-SYS question classification was used to provide an automatic classification for each of the ques-tions extracted from the Enron dataset.
In order to assess the performance of the system, the results werecompared to the hand-annotated examples.QA-SYS output agreed with human annotation in only 13.4% of cases overall ?
much lower than the?most common class?
baseline defined above.
However, this figure is artificially low as QA-SYS suppliesa result in all circumstances, without any associated level of confidence.
The system will thereforeprovide an incorrect result in cases where it does not have an appropriate category (even when faced witha nonsense string).This may be acceptable behaviour within information retrieval, particularly for participating in com-petitions when there is a high expectation of the question falling into one of Li & Roth?s categories, butfor dialogue questions it produces a number of undesirable effects.
Any competent end-to-end systemwould need (at a minimum) to filter out nonsense strings, and direct questions to appropriate classifiers332?Remind me when your wedding date is??
NUM:date DateTime?Also, who is following up on the VA license??
HUM:ind Person?What is our strategy/plan in agricultural commodities training??
DESC:desc DescriptionTable 3: Examples of questions correctly classifiedCategory Recall Precision F-measureDescription 62.8 28.6 39.3DateTime 53.7 28.6 37.3Numeric 40.9 15.5 22.5Reason 61.1 10.2 17.5Person 65.5 6.7 12.2Place 45.5 5.2 9.3Event 10.0 2.9 4.6Table 4: Recall and precision by categorybased on language (therefore removing the need to attempt an intelligent classification of texts in multi-ple languages).
Considering the proportion of questions in our sample which fell into the new categoriesof our extended taxonomy, the framework should also be extended to include a number of classifiers tohandle these data types specifically.We are therefore justified in considering what might happen if a pre-classifier fed to QA-SYS onlythose questions which it may stand some chance of categorising correctly.
Including only those questionsfalling into categories on which QA-SYS has been trained, output agrees with human annotation in 55.0%of cases.
Table 3 presents a small number of examples where the QA-SYS annotation agreed with humanassessment.It may also be instructive to consider the recall and precision on a per-category basis, as there is astrong variation between the success rates for different QA-SYS categories.
Table 4 gives the figures forthose classes with at least 10 examples in the current dataset, and which QA-SYS claims to address.This shows that some categories with the highest recall (e.g.
Person, Reason) suffer from low pre-cision, but examination of the full confusion matrix shows that the incorrect categorisation is largelyaccounted for by the categories for which QA-SYS is not trained (particularly Yes/No questions).
Ifreliable classifiers could be trained to filter out these question types at an earlier stage, the validity ofQA-SYS results would be significantly improved.However, there are some features of QA-SYS question classification which cannot be resolved bysimply adding additional categories to the classifier framework.Most notably, the system exhibits a high degree of case sensitivity.
For example, the two strings?What do you think??
and ?what do you think??
are both present in the Enron corpus.
To a human eyethe lack of capitalisation is unlikely to affect the meaning, but QA-SYS categorises these two sentencesdifferently: the former as DESC:desc, the latter as ENTY:term.A further example of case-sensitivity is found in the response of QA-SYS to questions written en-tirely in uppercase.
Of the eleven examples in the dataset which contain only uppercase letters, all areclassified as ABBR:exp.
The ?uppercase?
feature seems to overwhelm any other considerations (suchas question word) which may be present.
For instance ?WHAT??
is classified as ABBR:exp, whereas?What??
and ?what??
are (correctly) classified as DESC:desc.Certain words also have a significant impact on the classification, regardless of the syntax of thequestion.
For example, a question containing the word ?percent?
is likely to be classified as NUM:perc,a question containing the word ?week?
is likely to be classified as NUM:date, and a question containingthe word ?state?
is likely to be classifed as some subtype of LOCATION.Other lexical effects were surprising by their absence.
For instance, of 111 questions (in the entireEnron question-set) beginning ?What time.
.
.
?
only eleven are classified as requiring the NUM:dateresponse.333?How many kids are in the class and who is the instructor??
NUM:count?do you want to get together on friday or saturday and where??
LOC:other?How (and when) do you plan to get there??
DESC:mannerTable 5: Examples of compound questionsAnother small but important set of questions, which are barely represented in the current dataset, arecompound questions.
These are cases, such as the examples in Table 5, in which more than one answeris expected.
In all of these examples, the category generated by QA-SYS can hardly be called incorrect,however it is not the whole story.
Presently QA-SYS does not allow for multiple answer types.
This isworthy of further study.4 Future WorkThe present work should be extended using a larger dataset to train additional classifiers for the answertypes which are beyond the scope of IR classifiers such as QA-SYS.
A larger dataset will also enablefurther analysis, for example to identify any common features of questions which prove particularlyhard to categorise.
Specific work to identify further examples in the very small categories (including arepresentative sample of compound questions) would also be beneficial.The next step is to extend the QANUS framework with additional classifiers trained on Enron data,and this work should be thoroughly tested to ensure it is not over-fitted to Enron.
There is a wealth ofpublic dialogue data on the web, available from textual media such as web forums and Twitter, whichmay be reasonably expected to have some characteristics in common with email and which could be usedfor testing the classifiers.Recent work on email has considered the task of highlighting messages within an inbox which requireaction (e.g Bennett & Carbonell 2005, achieving 81.7% accuracy).
This is an interesting result for usas the set of actions intersects with the set of questions: some questions have the pragmatic force of anaction request.
It would be interesting to examine the size of this intersection.5 ReferencesBennett, Paul and Carbonell, Jaime.
2005.
?Detecting Action-Items in Email?
in proceedings of SIGIR?05.Huang, Zhiheng, Thint, Marcus, and Celikyilmaz, Asli.
2009.
?Investigation of Question Classifier inQuestion Answering?
in proceedings of EMNLP ?09.Klimt, Bryan and Yang, Yiming.
2004.
?Introducing the Enron Corpus?
in proceedings of First Confer-ence on Email and Anti-Spam.Li, Xin and Roth, Dan.
2002.
?Learning Question Classifiers?
in proceedings of COLING 2002.Ng, Jun-Ping and Kan, Min-Yen.
2010.
QANUS: An Open-source Question-Answering Platform, fromhttp://www.comp.nus.edu.sg/?junping/docs/qanus.pdf334
