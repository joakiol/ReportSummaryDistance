Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 70?76,Sydney, July 2006. c?2006 Association for Computational LinguisticsStatistical Machine ReorderingMarta R. Costa-jussa` and Jose?
A. R. FonollosaDepartment of Signal Theory and CommunicationsTALP Research Center (UPC)Barcelona 08034, Spain(mruiz,adrian)@gps.tsc.upc.eduAbstractReordering is currently one of the mostimportant problems in statistical machinetranslation systems.
This paper presentsa novel strategy for dealing with it: sta-tistical machine reordering (SMR).
It con-sists in using the powerful techniques de-veloped for statistical machine translation(SMT) to translate the source language(S) into a reordered source language (S?
),which allows for an improved translationinto the target language (T).
The SMT taskchanges from S2T to S?2T which leads to amonotonized word alignment and shortertranslation units.
In addition, the use ofclasses in SMR helps to infer new wordreorderings.
Experiments are reported inthe EsEn WMT06 tasks and the ZhEnIWSLT05 task and show significant im-provement in translation quality.1 IntroductionDuring the last few years, SMT systemshave evolved from the original word-based ap-proach (Brown et al, 1993) to phrase-based trans-lation systems (Koehn et al, 2003).
In parallelto the phrase-based approach, the use of bilin-gual n-grams gives comparable results, as shownby Crego et al (2005a).
Two basic issues differ-entiate the n-gram-based system from the phrase-based: training data are monotonously segmentedinto bilingual units; and, the model considers n-gram probabilities rather than relative frequencies.This translation approach is described in detail byMarin?o et al (2005).
The n-gram-based systemfollows a maximum entropy approach, in which alog-linear combination of multiple models is im-plemented (Och and Ney, 2002), as an alternativeto the source-channel approach.In both systems, introducing reordering capabil-ities is of crucial importance for certain languagepairs.
Recently, new reordering strategies havebeen proposed in the literature on SMT such as thereordering of each source sentence to match theword order in the corresponding target sentence,see Kanthak et al (2005) and Crego et al (2005b).Similarly, Matusov et al (2006) describe a methodfor simultaneously aligning and monotonizing thetraining corpus.
The main problems of these ap-proaches are: (1) the fact that the proposed mono-tonization is based on the alignment and cannot beapplied to the test sets, and (2) the lack of reorder-ing generalization.This paper presents a reordering approachcalled statistical machine reordering (SMR) whichimproves the reordering capabilities of SMT sys-tems without incurring any of the problems men-tioned above.
SMR is a first-pass translationperformed on the source corpus, which convertsit into an intermediate representation, in whichsource-language words are presented in an orderthat more closely matches that of the target lan-guage.
SMR and SMT are performed using thesame modeling tools as n-gram-based systems butusing different statistical log-linear models.In order to be able to infer new reorderings weuse word classes instead of words themselves asthe input to the SMR system.
In fact, the use ofclasses to help in the reordering is a key differencebetween our approach and standard SMT systems.This paper is organized as follows: Section 2outlines the baseline system.
Section 3 describesthe reordering strategy in detail.
Section 4 presentsand discusses the results, and Section 5 presentsour conclusions and suggestions for further work.702 N-gram-based SMT SystemThis section briefly describes the n-gram-basedSMT which uses a translation model based onbilingual n-grams.
It is actually a language modelof bilingual units, referred to as tuples, which ap-proximates the joint probability between sourceand target languages by using bilingual n-grams(de Gispert and Marin?o, 2002).Bilingual units (tuples) are extracted from anyword alignment according to the following con-straints:1. a monotonous segmentation of each bilingualsentence pairs is produced,2.
no word inside the tuple is aligned to wordsoutside the tuple, and3.
no smaller tuples can be extracted without vi-olating the previous constraints.As a result of these constraints, only one seg-mentation is possible for a given sentence pair.Figure 1 presents a simple example which illus-trates the tuple extraction process.I would like NULL to eat a huge ice-creamNULL quisiera ir a comer un helado gigantet1 t2 t3 t4 t5 t6Figure 1: Example of tuple extraction from analigned bilingual sentence pair.Two important issues regarding this translationmodel must be considered.
First, it often occursthat large number of single-word translation prob-abilities are left out of the model.
This happensfor all words that are always embedded in tuplescontaining two or more words.
Consider for ex-ample the word ?ice-cream?
in Figure 1.
As seenfrom the Figure, ?ice-cream?
is embedded into tu-ple t6.
If a similar situation is encountered for alloccurrences of ?ice-cream?
in the training corpus,then no translation probability for an independentoccurrence of this word will exist.To overcome this problem, the tuple 4-grammodel is enhanced by incorporating 1-gram trans-lation probabilities for all the embedded words de-tected during the tuple extraction step.
These 1-gram translation probabilities are computed fromthe intersection of both, the source-to-target andthe target-to-source alignments.The second issue has to do with the fact thatsome words linked to NULL end up producing tu-ples with NULL source sides.
Consider for exam-ple the tuple t3 in Figure 1.
Since no NULL is ac-tually expected to occur in translation inputs, thistype of tuple is not allowed.
Any target word thatis linked to NULL is attached either to the wordthat precedes or the word that follows it.
To de-termine this, we use the IBM1 probabilities, seeCrego et al (2005a).In addition to the bilingual n-gram transla-tion model, the baseline system implements alog-linear combination of four feature functions,which are described as follows:?
A target language model.
This feature con-sists of a 4-gram model of words, which istrained from the target side of the bilingualcorpus.?
A word bonus function.
This feature intro-duces a bonus based on the number of targetwords contained in the partial-translation hy-pothesis.
It is used to compensate for the sys-tem?s preference for short output sentences.?
A source-to-target lexicon model.
This fea-ture, which is based on the lexical param-eters of the IBM Model 1 (Brown et al,1993), provides a complementary probabil-ity for each tuple in the translation table.These lexicon parameters are obtained fromthe source-to-target algnments.?
A target-to-source lexicon model.
Similarlyto the previous feature, this feature is basedon the lexical parameters of the IBM Model1 but, in this case, these parameters are ob-tained from target-to-source alignments.All these models are combined in the de-coder.
Additionally, the decoder allows for a non-monotonous search with the following distorsionmodel.71?
A word distance-based distorsion model.P (tK1 ) = exp(?K?k=1dk)where dk is the distance between the firstword of the kth tuple (unit), and the lastword+1 of the (k ?
1)th tuple.
Distanceare measured in words referring to the unitssource side.To reduce the computational cost we place lim-its on the search using two parameters: the dis-tortion limit (the maximum distance measured inwords that a tuple is allowed to be reordered, m)and the reordering limit (the maximum number ofreordering jumps in a sentence, j).
This feature isindependent of the reordering approach presentedin this paper, so they can be used simultaneously.In order to combine the models in the decodersuitably, an optimization tool is needed to computelog-linear weights for each model.3 Statistical Machine ReorderingAs mentioned in the introduction, SMR and SMTare based on the same principles.
Here, we givea detailed description of the SMR reordering ap-proach proposed.3.1 ConceptThe aim of SMR consists in using an SMT sys-tem to deal with reordering problems.
Therefore,the SMR system can be seen as an SMT systemwhich translates from an original source language(S) to a reordered source language (S?
), given atarget language (T).
Then, the translation taskschanges from S2T to S?2T.
The main differencebetween the two tasks is that the latter allows for:(1) monotonized word alignment, and (2) higherquality monotonized translation.3.2 DescriptionFigure 2 shows the SMR block diagram.
The in-put is the initial source sentence (S) and the outputis the reordered source sentence (S?).
There threeblocks inside SMR: (1) class replacing ; (2) the de-coder, which requires the translation model; and,(3) the block which reorders the original sentenceusing the indexes given by the decoder.
The fol-lowing example specifies the input and output ofeach block inside the SMR.Figure 2: SMR block diagram.1.
Source sentence (S):El compromiso s?olo podr?a mejorar2.
Source sentence classes (S-c):C38 C43 C49 C42 C223.
Decoder output (translation, T ):C38#0 | C43 C49 C42#1 2 0 | C22#0where | indicates the segmentation into trans-lation units and # divides the source and tar-get.
The source part is composed of wordclasses and the target part is composed ofthe new positions of the source word classes,starting at 0.4.
SMR output (S?).
The reordering informationinside each translation unit of the decoderoutput (T ) is applied to the original sourcesentence (S):El s?olo podr?a compromiso mejorar3.3 TrainingFor the reordering translation, we used an n-gram-based SMT system (and considered only the trans-lation model).
Figure 3 shows the block diagramof the training process of the SMR translationmodel, which is a bilingual n-gram-based model.The training process uses the training source andtarget corpora and consists of the following steps:1.
Determine source and target word classes.2.
Align parallel training sentences at the wordlevel in both translation directions.
Computethe union of the two alignments to obtain asymmetrized many-to-many word alignment.3.
Extract reordering tuples, see Figure 4.
(a) From union word alignment, extractbilingual S2T tuples (i.e.
source andtarget fragments) while maintaining the72Figure 3: Block diagram of the training process of the SMR translation model.Figure 4: Example of the extraction of reorderingtuples (step 3).alignment inside the tuple.
As an ex-ample of a bilingual S2T tuple consider:only possible compromise # compromisos?olo podr?a # 0-1 1-1 1-2 2-0, as shownin Figure 4, where the different fields areseparated by # and correspond to: (1)the target fragment; (2) the source frag-ment; and (3) the word alignment (inthis case, the fields that respectively cor-respond to a target and source word areseparated by ?).
(b) Modify the many-to-many word align-ment from each tuple to many-to-one.If one source word is aligned to two ormore target words, the most probablelink given IBM Model 1 is chosen, whilethe other are omitted (i.e.
the num-ber of source words is the same beforeand after the reordering translation).
Inthe above example, the tuple would bechanged to: only possible compromise# compromiso s?olo podr?a # 0-1 1-2 2-0, as Pibm1(only, so?lo) is higher thanPibm1(possible, so?lo).
(c) From bilingual S2T tuples (with many-to-one inside alignment), extract bilin-gual S2S?
tuples (i.e.
the source frag-ment and its reordering).
As in the ex-ample: compromiso s?olo podr?a # 1 2 0,where the first field is the source frag-ment, and the second is the reorderingof these source words.
(d) Eliminate tuples whose source fragmentconsists of the NULL word.
(e) Replace the words of each tuple sourcefragment with the classes determined inStep 1.4.
Compute the bilingual language model of thebilingual S2S?
tuple sequence composed ofthe source fragment (in classes) and its re-order.Once the translation model is built, the origi-nal source corpus S is translated into the reorderedsource corpus S?
with the SMR system, see Fig-ure 2.
The reordered training source corpus andthe original training target corpus are used to trainthe SMT system (as explained in Section 2).
Fi-nally, with this system, the reordered test sourcecorpus is translated.4 Evaluation FrameworkIn this section, we present experiments carried outusing the EsEn WMT06 and the ZhEn IWSLT05parallel corpus.
We detail the tools which havebeen used and the corpus statistics.73EuroParl Spanish EnglishTraining Sentences 727.1 k 727.1 kWords 15.7 M 15.2 MVocabulary 108.7 k 72.3 kDevelopment Sentences 500 500Words 15.2 k 14.8 kVocabulary 3.6 k 3 kTest Sentences 3064 3064Words 91.9 k 85.2 kVocabulary 11.1 k 9.1 kTable 1: Spanish to English task.
EuroParl cor-pus: training, development and test data sets.4.1 Tools?
The word alignments were computed usingthe GIZA++ tool (Och, 2003).?
The word classes were determined us-ing ?mkcls?, a freely-available tool withGIZA++.?
The language model was estimated using theSRILM toolkit (Stolcke, 2002).?
We used MARIE as a decoder (Crego et al,2005b).?
The optimization tool used for computinglog-linear weights (see Section 2) is basedon the simplex method (Nelder and Mead,1965).4.2 Corpus StatisticsExperiments were carried out on the Spanish andEnglish task of the WMT06 evaluation1 (EuroParlCorpus) and on the Chinese to English task of theIWSLT05 evaluation2 (BTEC Corpus).
The for-mer is a large corpus, whereas the latter is a smallcorpus translation task.
Table 1 and 2 show themain statistics of the data used, namely the numberof sentences, words, vocabulary, and mean sen-tence lengths for each language.4.3 UnitsIn this section different statistics units of both ap-proaches (S2T and S?2T) are shown (using theZhEn task).
All the experiments in this sectionwere carried out using 100 classes in the SMRstep.1www.statmt.org/wmt06/shared-task/2www.slt.atr.jp/IWSLT2005BTEC Chinese EnglishTraining Sentences 20 k 20 kWords 176.2 k 182.3 kVocabulary 8.7 k 7.3 kDevelopment Sentences 506 506Words 3.5 k 3.3 kVocabulary 870 799Test Sentences 506 506Words 4 k 3 kVocabulary 916 818Table 2: Chinese to English task.
BTEC corpus:training, development and test data sets.
Develop-ment and test data sets have 16 references.Table 3 shows the vocabulary of bilingual n-grams and embedded words in the translationmodel.
Once the reordering translation has beencomputed, alignment becomes more monotonic.
Itis commonly known that non-monotonicity posesdifficulties for word alignments.
Therefore, whenthe alignment becomes more monotonic, we ex-pect an improvement in the alignment, and, there-fore in the translation.
Here, we can observe asignificant enlargement of the number of transla-tion units, which leads to a growth of the transla-tion vocabulary.
We also observe a decrease in thenumber of embedded words (around 20%).
FromSection 2, we know that the probability of embed-ded words is estimated independently of the trans-lation model.
Reducing embedded words allowsfor a better estimation of the translation model.Figure 5 shows the histogram of the tuple size inthe two approaches.
We observe that the numberof tuples is similar over length 5.
However, thereare a greater number of shorter units in the case ofSMR+NB (shorter units lead to a reduction in datasparseness).010000200003000040000500006000070000800000  2  4  6  8  10  12  14NBSMR + NBFigure 5: Comparison of the histogram of the tuplesize in the two approaches (NB and SMR+NB).74System 1gr 2gr 3gr 4gr EmbeddedNB 34487 57597 3536 1918 5735SMR + NB 35638 70947 5894 3412 4632Table 3: Vocabulary of n-grams and embedded words in the translation model.System Total VocabularyNB 4460 959SMR + NB 4628 1052Table 4: Tuples used to translate the test set (totalnumber and vocabulary).Table 4 shows the tuples used to translate thetest set (total number and vocabulary).
Note thatthe number of tuples and vocabulary used to trans-late the test set is significantly greater after the re-ordering translation.4.4 ResultsHere, we introduce the experiments that were car-ried out in order to evaluate the influence of theSMR approach in both tasks EsEn and ZhEn.
Thelog-linear translation model was optimized withthe simplex algorithm by maximizing over theBLEU score.
The evaluation was carried out us-ing references and translation in lowercase and, inthe ZhEn task, without punctuation marks.We studied the influence of the proposed SMRapproach on the n-gram-based SMT system de-scribed using a monotonous search (NBm ormonotonous baseline configuration) in the twotasks and a non-monotonous search (NBnm ornon-monotonous baseline configuration) in theZhEn task.
In allowing for reordering in the SMTdecoder, the distortion limit (m) and reorderinglimit (j) (see Section 2) were empirically set to5 and 3, as they showed a good trade-off betweenquality and efficiency.
Both systems include thefour features explained in Section 2: the languagemodel, the word bonus, and the source-to-targetand target-to-source lexicon models.Tables 5 and 6 show the results in the test set.The former corresponds to the influence of theSMR system on the EsEn task (NBm), whereasthe latter corresponds to the influence of the SMRsystem on the ZhEn task (NBm and NBnm).4.5 DiscussionBoth BLEU and NIST coherently increase afterthe inclusion of the SMR step when 100 classesare used.
The improvement in translation qualitycan be explained as follows:?
SMR takes advantage of the use of classesand correctly captures word reorderings thatare missed in the standard SMT system.
Inaddition, the use of classes allows new re-orderings to be inferred.?
The new task S?2T becomes moremonotonous.
Therefore, the translationunits tend to be shorter and SMT systemsperform better.The gain obtained in the SMR+NBnm case indi-cates that the reordering provided by SMR systemand the non-monotonous search are complemen-tary.
It means that the output of the SMR couldstill be further monotonized.
Note that the ZhEntask has complex word reorderings.These preliminary results also show that SMRitself provides further improvements to those pro-vided by the non-monotonous search.5 Conclusions and Further ResearchIn this paper we have mainly dealt with the re-ordering problem for an n-gram-based SMT sys-tem.
However, our approach could be used sim-ilarly for a phrase-based system.
We have ad-dressed the reordering problem as a translationfrom the source sentence to a monotonized sourcesentence.
The proposed SMR system is appliedbefore a standard SMT system.
The SMR andSMT systems are based on the same principles andshare the same type of decoder.In extracting bilingual units, the change of orderperformed in the source sentence has allowed themodeling of the translation units to be improved(shorter units mean a reduction in data sparse-ness).
Also, note that the SMR approach allowsthe coherence between the change of order in thetraining and test source corpora to be maintained.75System Classes BLEU NIST WER PERNBm - 27.69 7.31 61.6 45.34SMR + NBm - 28.60 7.53 59.89 43.53SMR + NBm 100 30.89 7.75 55.77 42.85Table 5: Results in the test set of the EsEn task using a monotonous search.System Classes BLEU NIST WER PERNBm - 42.42 8.3 42.87 33.44NBnm - 43.58 8.9 43.89 34.05SMR + NBm 100 43.75 8.49 42.45 33.85SMR + NBnm 100 45.97 9.0 40.92 32.32Table 6: Results in the test set of the ZhEn task using a monotonous and a non-monotonous search.Performing reordering as a preprocessing stepand independently from the SMT system allowsfor a more efficient final system implementationand a quicker translation.
Additionally, usingword classes helps to infer unseen reorderings.These preliminary results show consistent and sig-nificant improvements in translation quality.As further research, we would like to add extrafeatures to the SMR system, and study new typesof classes for the reordering task.6 AcknowledgmentsThis work has been partially funded by the Eu-ropean Union under the integrated project TC-STAR - Technology and Corpora for Speechto Speech Translation - (IST-2002-FP6-506738,http://www.tc-star.org) and the Spanish govern-ment under a FPU grant.ReferencesE.
Matusov A. Mauser and H. Ney.
2006.
Train-ing a statistical machine translation system withoutgiza++.
5th Int.
Conf.
on Language Resources andEvaluation, LREC?06, May.P.
Brown, S. Della Pietra, V. Della Pietra, and R. Mer-cer.
1993.
The mathematics of statistical machinetranslation.
Computational Linguistics, 19(2):263?311.J.
M. Crego, M. R. Costa-jussa`, J. Marin?o, and J. A.Fonollosa.
2005a.
Ngram-based versus phrase-based statistical machine translation.
Proc.
of theInt.
Workshop on Spoken Language Translation,IWSLT?05, October.J.M.
Crego, J. Marin?o, and A. de Gispert.
2005b.An Ngram-based statistical machine translation de-coder.
Proc.
of the 9th Int.
Conf.
on Spoken Lan-guage Processing, ICSLP?05.A.
de Gispert and J. Marin?o.
2002.
Using X-grams forspeech-to-speech translation.
Proc.
of the 7th Int.Conf.
on Spoken Language Processing, ICSLP?02,September.S.
Kanthak, D. Vilar, E. Matusov, R. Zens, and H.Ney.
2005.
Novel reordering approaches in phrase-based statistical machine translation.
Proceedingsof the ACL Workshop on Building and Using Par-allel Texts: Data-Driven Machine Translation andBeyond, pages 167?174, June.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
Proc.
of the Human Lan-guage Technology Conference, HLT-NAACL?2003,May.J.B.
Marin?o, R.E.
Banchs, J.M.
Crego, A. de Gispert,P.
Lambert, J.A.R.
Fonollosa, and M. Ruiz.
2005.Bilingual n-gram statistical machine translation.
InProc.
of the MT Summit X, pages 275?82, Pukhet(Thailand), May.J.A.
Nelder and R. Mead.
1965.
A simplex methodfor function minimization.
The Computer Journal,7:308?313.F.J.
Och and H. Ney.
2002.
Discriminative train-ing and maximum entropy models for statistical ma-chine translation.
40th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 295?302, July.F.J.
Och.
2003.
Giza++ software.
http://www-i6.informatik.rwth-aachen.de/?och/ soft-ware/giza++.html.A.
Stolcke.
2002.
Srilm - an extensible language mod-eling toolkit.
Proc.
of the 7th Int.
Conf.
on SpokenLanguage Processing, ICSLP?02, September.76
