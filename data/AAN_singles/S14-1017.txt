Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 132?140,Dublin, Ireland, August 23-24 2014.Using Text Segmentation Algorithms for the Automatic Generation ofE-Learning CoursesCan Beck, Alexander Streicher and Andrea ZielinskiFraunhofer IOSBKarlsruhe, Germany{can.beck, alexander.streicher,andrea.zielinski}@iosb.fraunhofer.deAbstractWith the advent of e-learning, there is astrong demand for tools that help to cre-ate e-learning courses in an automatic orsemi-automatic way.
While resources fornew courses are often freely available,they are generally not properly structuredinto easy to handle units.
In this paper,we investigate how state of the art textsegmentation algorithms can be appliedto automatically transform unstructuredtext into coherent pieces appropriate fore-learning courses.
The feasibility tocourse generation is validated on a testcorpus specifically tailored to this scenar-io.
We also introduce a more generictraining and testing method for text seg-mentation algorithms based on a LatentDirichlet Allocation (LDA) topic model.In addition we introduce a scalable ran-dom text segmentation algorithm, in or-der to establish lower and upper boundsto be able to evaluate segmentation re-sults on a common basis.1 IntroductionThe creation of e-learning courses is generally atime consuming effort.
However, separating textinto topically cohesive segments can help to re-duce this effort whenever textual content is al-ready available but not properly structured ac-cording to e-learning standards.
Since these seg-ments textually describe the content of learningunits, automatic pedagogical annotation algo-rithms could be applied to categorize them intointroductions, descriptions, explanations, exam-ples and other pedagogical meaningful concepts(K.Sathiyamurthy & T.V.Geetha, 2011).Course designers generally assume that learn-ing content is composed of small inseparablelearning objects at the micro level which in turnare wrapped into Concept Containers (CCs) atthe macro level.
This approach is followed, e.g.,in the Web-Didactic approach by Swertz et al.
(2013) where CCs correspond to chapters in abook and Knowledge Objects (KOs) correspondto course pages.
To automate the partition of anunstructured text source into appropriate seg-ments for the macro and micro level we applieddifferent text segmentation algorithms (segment-ers) on each level.To evaluate the segmenters in the describedscenario, we created a test corpus based on fea-tured Wikipedia articles.
For the macro level weexploit sections from different articles and thecorresponding micro structure consists of subse-quent paragraphs from these sections.
On themacro level the segmenter TopicTiling (TT) byRiedl and Biemann (2012) is used.
It is based ona LDA topic model which we train based on thearticles from Wikipedia to extract a predefinednumber of different topics.
On the micro level,the segmenter BayesSeg (BS) is applied(Eisenstein & Barzilay, 2008).We achieved overall good results measured inthree different metrics over a baseline approach,i.e., a scalable random segmenter, that indicateThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/132text segmentation algorithms are ready to be ap-plied to facilitate the creation of e-learningcourses.This paper is organized as follows: Section 2gives an overview of related work on automaticcourse generation as well as text segmentationapplications.
In the main sections 3 and 4 we de-scribe our approach and evaluation results on ourcorpus.
In the last section we summarize the pre-sented findings and give an outlook on furtherresearch needed for the automatic generation ofe-learning courses.2 Related WorkAutomatic course generation can roughly be di-vided into two different areas.
One is concernedwith generation from existing courses and ismainly focused on adaption to the learner or in-structional plans see Lin et al.
(2009), Capuno etal.
(2009) and Tan et al.
(2010).
The other area isthe course creation itself on which we focus onin this paper.Since the publication of the segmenter Text-Tiling by Hearst (1997) at least a dozen differentsegmenters have been developed.
They can bedivided into linear and hierarchical segmenters.Linear segmenters process the text sequentiallysentence by sentence.
Hierarchical segmentersfirst process the whole text and extract topicswith varying granularities.
These topics are thenagglomerated based on a predefined criterion.Linear segmenters have been developed byKan et al.
(1998) and Galley et al.
(2003).
One ofthe first probabilistic algorithms has been intro-duced by Utiyama and Isahara (2001).
LDAbased approaches were first described by Sun etal.
(2008) and improved by Misra et al.
(2009).The newest LDA based segmenter is TT.
It per-forms linear text segmentation based on a pre-trained LDA topic model and calculates the simi-larity between segments (adjacent sentences) tomeasure text coherence on the basis of a topicvector representation using cosine similarity.
Forreasons of efficiency, only the most frequent top-ic ID is assigned to each word in the sentence,using Gibbs sampling.Hierarchical text segmentation algorithmswere first introduced by Yaari (1997).
The latestapproach by Eisenstein (2008) uses a generativeBayesian model BS for text segmentation, as-suming that a) topic shifts are likely to occur atpoints marked by cue phrases and b) a linear dis-course structure.
Each sentence in the documentis modeled by a language model associated witha segment.
The algorithm then calculates themaximum likelihood estimates of observing thewhole sequence of sentences at selected topicboundaries.The applications of text segmentation algo-rithms range from information retrieval (Huang,et al., 2002) to topic tracking and segmentationof multi-party conversations (Galley, et al.,2003).Similar to our work Sathiyamurthy and Geetha(2011) showed how LDA based text segmenta-tion algorithms combined with hierarchical do-main ontology and pedagogical ontology can beapplied to content generation for e-learningcourses.
They focussed on the segmentation ofexisting e-learning material in the domain ofcomputer science and introduced new metrics tomeasure the segmentation results with respect toconcepts from the ontologies.
Our work focusseson the appropriate segmentation of unstructuredtext instead of existing e-learning material.
Alt-hough the usage of domain models is an interest-ing approach the availability of such models isvery domain dependent.
We rely on the LDAmodel parameters and training to accomplish aword to topic assignment.Rather than introducing new aspects such aspedagogical concepts we investigated the generalusability of segmentation algorithms with focuson the macro and micro structure which is char-acteristic for most e-learning content.3 Automatic Generation of E-LearningCoursesThe main objective is to provide e-learningcourse designers with a tool to efficiently organ-ize existing textual content for new e-learningcourses.
This can be done by the application oftext segmenters that automatically generate thebasic structure of the course.
The intended web-didactic conform two-level structure differenti-ates between macro and micro levels.
The levelshave different requirements with respect to the-matic coherence: the CCs are thematically ratherindependent and the KOs within each CC need tobe intrinsically coherent but still separable.We chose the linear LDA-based segmenter TTto find the boundaries between CCs.
The LDA-based topic model can be trained on contentwhich is topically related to the target course.This approach gives the course creator flexibilityin the generation of the macro level structure byeither adjusting the training documents or by133changing the number and size of topics thatshould be extracted for the topic model.On the micro level we did not use TT.
Thetraining of an appropriate LDA model wouldhave to be done for every CC separately sincethey are thematically relatively unrelated.
Apartfrom that the boundaries between the KOsshould be an optimal division for a given numberof expected boundaries.
The reason for this isthat the length of KOs should be adapted to theintended skill and background of the learners.This is why we decided to use the hierarchicalsegmenter BS.3.1 Application Setting and CorpusTo evaluate segmenters many different corporahave been created.
The most commonly usedcorpus was introduced by Choi (2000).
It isbased on the Brown Corpus and contains 700samples, each containing a fixed number of sen-tences from 10 different news texts, which arerandomly chosen from the Brown Corpus.
Twoother widely tested corpora were introduced byGalley et al.
(2003).
Both contain 500 samples,one with concatenated texts from the Wall StreetJournal (WSJ) and the other with concatenatedtexts from the Topic Detection and Tracking(TDT) corpus (Strassel, et al., 2000).
A standardfor the segmentation of speech is the corpus fromthe International Computer Science Institute(ICSI) by Janin et al.
(2003).
A medical textbook has been used by Eisenstein and Barzilay(2008).
The approaches to evaluate segmentersare always similar: they have to find the bounda-ries in artificially concatenated texts.We developed our own dataset because wewanted to use text that potentially could be usedas a basis for creating e-learning courses.
Wetherefore need samples which, on the one hand,have relatively clear topic boundaries on themacro level and, on the other hand resemble thedifferences in number of topics and inter-topiccohesion on the micro level.We based our corpus on 530 featured1 articlesfrom 6 different categories of the English Wik-ipedia.
It can be assumed that Wikipedia articlesare often the source for learning courses.
Weused featured articles because the content struc-ture is very consistent and clear, i.e., sections andparagraphs are well defined.The corpus is divided into a macro and microdataset in the following manner: The macro da-1http://en.wikipedia.org/wiki/Wikipedia:Featured_articlestaset contains 1200 samples.
Each sample is aconcatenation of paragraphs from 6-8 differentsections from featured articles.
Each topic in asample consists of 3-6 subsequent paragraphsfrom a randomly selected section.
We proposethat one paragraph describes one KO.
One CCcontains all KOs which are from the same sec-tion in the article.
Thus, one sample from themacro dataset contains 6-8 CCs, each containing3-6 KOs.
The segmentation task is to find thetopic boundaries between the CCs.
The macrodataset is quite similar in structure to the Choi-Corpus.The micro dataset is extracted from the macrodataset.
It contains 8231 samples, where eachsample contains all KOs from one CC of themacro dataset.
The segmentation task is to findthe topic boundaries between the KOs, i.e, sub-sequent paragraphs of one section, see Figure 1.Figure 1: Schema for corpus samples: left andright Wikipedia articles with sections and para-graphs, in the middle three samples, dashed rec-tangle is a macro sample and dashed circles aremicro samples.
Filled squares indicate topicboundaries in the macro sample and filled circlesin the micro samples.All texts in our corpus are stemmed and stop-words are removed with the NLP-Toolkit forPython (Bird, et al., 2009) using an adapted vari-ant2 of the keyword extraction method by Kim etal.
(2013).The macro and micro dataset themselves aredivided into multiple subsets to evaluate the sta-bility of the segmenters when the number of sen-tences per topic or the number of topics per sam-ple have changed.
The detailed configuration isshown in Table 1 and 2.
Each subset is identifiedby the number of CCs per sample and the num-ber of KOs per CC (the subset is denoted as#CC_#KO).
Subsets of the micro dataset areidentified by a single value which is the number2https://gist.github.com/alexbowe/879414134of KOs per sample (#KO).
In Table 1 the identi-fier R means that the number of CCs or KOs isnot the same for all samples, it is chosen random-ly from the set depicted by curly brackets.ID CCs persampleKOs perCCmean sen-tences perCC7_3 7 3 207_4 7 4 277_5 7 5 337_6 7 6 407_R 7 {3,4,5,6} 30R_R {6,7,8} {3,4,5,6} 30Table 1: Macro dataset and its subsets each with200 samples.ID KOs per sam-plemean sentences perKO3 3 94 4 85 5 76 6 7Table 2: Micro dataset and its subsets.The important difference between the macro andmicro dataset is that every subset of the macrodataset contains a constant number of topicswhich differ in number of sentences per topicbetween 20 and 40, except the subset R_R whichcontains a random number of topics between 6and 8.
In contrast, each micro-level subset differsin number of topics but not significantly in thenumber of sentences per topic.This difference between the datasets allows usto focus on the different level-specific aspects.On the macro dataset we can evaluate the stabil-ity of TT over topics with highly varying lengthsand on the micro dataset we can evaluate BSwhen the number of strongly coherent topicschanges.3.2 Text Segmentation MetricsThe performance of a segmenter cannot simplybe measured by false positive and false negativeboundaries compared to the true boundaries be-cause, if the predicted boundary is only one sen-tence away from the true boundary this couldstill be very close, e.g., if the next true topicboundary is 30 sentences away.
Thus, the rela-tive proximity to true boundaries should also beconsidered.
There is an ongoing discussion aboutwhat kind of metric is appropriate to measure theperformance of segmenters (Fournier & Inkpen,2012).
Most prominent and widely used areWindowDiff wd (Pevzner & Hearst, 2002) andthe probabilistic metric pk (Beeferman, et al.,1999).
The basic principle is to slide a window offixed size over the segmented text, i.e., fixednumber of words or sentences, and assess wheth-er the sentences on the edges are correctly seg-mented with respect to each other.
Both metricswd and pk are penalty metrics, therefore lowervalues indicate better segmentations.
The prob-lem with these metrics is that they strongly de-pend on the arbitrarily defined window size pa-rameter and do not penalize all error types equal-ly, e.g., pk penalizes false negatives more thanfalse positives and wd penalizes false positiveand negative boundaries more at the beginningand end of the text (Lamprier, et al., 2007).
Be-cause of that we also used a rather new metriccalled BoundarySimilarity b.
This metric is pa-rameter independent and has been developed byFournier and Inkpen (2013) to solve the men-tioned deficiencies.
Since b measures the similar-ity between the boundaries, higher values indi-cate better segmentations.
We used the imple-mentations of wd, pk and b by Fournier3 (wd andpk with default parameters).3.3 LDA Topic Model TrainingRiedl and Biemann evaluated TT on the Choi-Corpus based on a 10-fold cross validation.Thus, the LDA topic model was generated with90% of the samples and TT then tested on theremaining 10% of the samples.
The 700 samplesin the Choi-Corpus are only concatenations of1111 different excerpts from the Brown Corpusand each sample contains 10 of these excepts it isclear that there are just not enough excerpts tomake sure that the samples in the training set donot contain any excerpt that is also part of somesamples in the testing set.That is one reason why we do not use thesame approach since we want to make sure thattraining and testing sets are truly disjoint to eval-uate TT on the macro dataset.
The other reason isthat the topic structure generated by TT shouldbe based on an LDA topic model with topics ex-tracted from documents which are thematicallyrelated to certain parts of the course that is to becreated without using its text source.3https://github.com/cfournie/segmentation.evaluation135We train the LDA topic model to extract top-ics from the real Wikipedia articles.
This modelis then used to evaluate TT on the macro datasetand not the Wikipedia articles.
This approach hasconsequences for the LDA topic model trainingand respective TT testing sets, since the LDAtraining set contains real articles and the TT testset contains the samples from the macro dataset.Because training and testing set should truly bedisjoint we cannot train with any article that ispart of a sample from the test set.
Because eachtest sample from the macro dataset contains partsof 6 to 8 articles, the training set is reduced by alarge factor, even with little test set size, which isshown for different number of folds (k) for crossvalidation in Table 3.k Test Set Size Training Set Size10 120?0 Samples(10% of themacro dataset)139?7 featuredArticles(26% of all arti-cles)20 60?0 Samples(5% of the macrodataset)267?8 featuredArticles(51% of all arti-cles)30 40?0 Samples(3% of the macrodataset)338?7featured Articles(64% of all arti-cles)Table 3: Mean size and standard deviation oftruly disjunctive LDA training and respective TTtesting set.If we truly separate training and testing sets andtrain the LDA topic model with real articles a 10-fold cross validation  leads to very small trainingsets (only 26% of all articles are used), which iswhy we also used higher folds to evaluate theresults of TT on the macro dataset.4 Evaluation ResultsWe evaluated TT on the macro dataset withoutproviding the number of boundaries.
On the mi-cro dataset we evaluated BS with the expectednumber of boundaries provided.
We also imple-mented a scalable random segmenter (RS) tocompare TT and BS against some algorithm withinterpretable performance.
The interpretation ofthe values in any metric even with respect to dif-ferent metrics is very difficult without compari-son to another segmenter.
For every true bounda-ry in a document, RS predicts a boundary drawnfrom a normally distributed set around the trueboundary with scalable standard deviation ?.Thus smaller values for ?
result in better seg-mentations because the probability of selectingthe true boundary increases, e.g., for ?
= 2, morethan 68% of all predicted boundaries are at most2 sentences away from the true boundary andmore than 99% of all predicted boundaries arelocated within a range of 6 sentences from it.
Butwhether 6 sentences is a large or small distanceshould depend on the average topic size.
Wetherefore relate the performance of RS to themean number of sentence per topic by defining ?in percentages of that number as shown in thetable below.Distance from TrueBoundary:Standard Deviationvery close ?
= 0% - 5%close ?
= 5% -15%large ?
= 15% - 30%Table 4: Defined performance of RS for differentstandard deviations ?, given in percentage ofmean sentences per topic.To give an example, the subset 7_6 of the macrodataset has an average of 40 sentences per topic,therefore RS with ?=15% means that it is set to 6which is 15% of 40.
This is defined as a mediumperformance in Table 4 because 68% of theboundaries predicted are within a range of 6 sen-tences from the true boundaries and 99% within18 sentences.One important difference between the macroand micro dataset is that all subsets of the macrodataset have 7 topics, differing in length, exceptfor subset R_R where this number is only slightlyvaried (Table 1).
In contrast, all topics subsets ofthe micro dataset have roughly the same numberof sentences but highly differ in the number oftopics (Table 2).
We therefore do not comparethe performance of BS and TT since they areevaluated on quite different datasets designed fortesting different types of segmentation tasks rel-evant to course generation, as explained earlier.We compare both to RS for different standarddeviations ?.4.1 Results for TopicTiling on the MacroDatasetFor the LDA topic model training we used thefollowing default parameters:  alpha=0.5,beta=0.1,ntopics=100,niters=1000,136twords=20,savestep=100, for details werefer to (Griffiths & Steyvers, 2004).
To compareTT?s performance for different folds of the mac-ro dataset we optimized the window parameterwhich has to be set for TT, it specifies the num-ber of sentences to the left and to the right of thecurrent position p between two sentences that areused to calculate the coherence score betweenthese sentences (Riedl & Biemann, 2012).
Theperformance for TT has been best with windowsizes between 9 and 11 for all metrics as shownin Figure 2.
As expected, higher folds increaseTT?s overall performance especially with respectto metric b (Figure 3).
This is due to the largertraining set sizes of the LDA topic model.Figure 2: TT performance for different windowsizes with 30-fold cross validation.Figure 3: TT performance for different folds andwindow size set to 9.In general smaller window sizes increase thenumber of predicted boundaries.
The optimalwindow size is between 9 and 11 and we wouldexpect the measures for 5 and 15 to be similar(Figure 2).
This is only the case for metric b, themetrics wd and pk seem to penalize false posi-tives more than false negatives.
This would be acontradiction to the findings of Lamprier et al.
(2007) since they actually found the opposite tobe true.
This behaviour is explained by the non-linear relation between the window parameterand number of predicted boundaries by TT asshown in Figure 4.Figure 4: Mean number of predicted boundariesby TT for different window sizes and an LDAtopic model trained with 30 folds.Another important finding is the stability of TT?sperformance over different window sizes (from 9to 11).
This is important since a very sensitivebehaviour would be very difficult to handle forcourse creators because they would have to esti-mate this parameter in advance.For the following detailed evaluation TT win-dow size is set to 9 because of the best overallresults with respect to metric b and 30-fold crossvalidation.
The detailed performance with re-spect to metric wd, pk and b of TT compared toRS with different standard deviations ?
is shownin Figure 5 i), ii) and iii).i.
TT measured with metric b.0.00.10.20.30.40.55 6 7 8 9 10 11 12 13 14 15windowmeanmetricwdpkb0.00.10.20.30.410 20 30foldsmeanmetricwdpkb7.510.012.55 6 7 8 9 10 11 12 13 14 15window sizemeanboundariespersample0.00.20.40.60.87_3 7_4 7_5 7_6 7_R R_RSubsetmean137ii.
TT measured with metric wd.iii.
TT measured with metric pk.Figure 5: Performance of TT on the macro da-taset.First of all we want to point out that the graphsof RS for different values of ?
are ordered as ex-pected by all metrics.
Lower percentages indicatebetter results.
And with respect to metric wd andpk the performance for each ?
is nearly constantover all subsets, which indicates that the metricscorrectly consider the relative distance of a pre-dicted boundary from the true boundary by usingthe mean number of sentences per topic.
In met-ric b only the RS with ?=30%, 15% and 5% areconstant.
For ?=5% there is a strong decrease inperformance for subsets with more sentences pertopic.The overall performance of TT is between thatof RS for ?=1% and ?=15%, except for subset7_6 with respect to metric wd.
With respect tometric b TT even predicts very close boundaries.In all metrics TT has the worst results on subset7_6, which has the largest number of sentencesper topic (see Table 1).
This is due to TT?s win-dow parameter which influences the number ofpredicted boundaries as shown in Figure 4.4.2 Results for BayesSeg on the Micro Da-tasetBS does not need any training or parameter fit-ting, since it is provided with the number of ex-pected segments.
We therefore used the defaultparameter settings.i.
BS measured with metric b.ii.
BS measured with metric wd.iii.
BS measured with metric pk.Figure 6: Performance of BS on the micro da-taset.As expected, the performance of RS is decreas-ing for higher values of ?
in all metrics (Figure 6i), ii), iii)).
For metric wd and pk the increasing0.00.20.40.60.87_3 7_4 7_5 7_6 7_R R_RSubsetmean0.00.20.40.60.87_3 7_4 7_5 7_6 7_R R_RSubsetmean0.00.20.40.60.83 4 5 6Subsetmean0.00.20.40.60.83 4 5 6Subsetmean0.00.20.40.60.83 4 5 6Subsetmean138number of topics leads to slightly increasingpenalties for constant values of ?, which clearlyindicates that the metrics do not treat all errorsequally, as repeatedly pointed out.
Metric b treatserrors equally over increasing number of topicsfor RS.
BS predicts with respect to all metricsclose boundaries since it is better than RS with?=15% except on subset 6 (Table 4).
With anincreasing number of topics BS is getting worsein all metrics.Comparing the measures of metric b for macroand micro dataset it seems that it handles increas-ing numbers of topics better than increasing sizeof topics.
On the micro dataset the results withrespect to all metrics are far more similar thanthe once on the macro dataset, where the differ-ences are very large.
Since we are only interestedin comparative measures of the performance ofthe segmenters and RS, which has shown to be avery useful approach to interpret segmentationresults, we leave detailed explanations of themetrics behaviours itself to further research.5 ConclusionWe demonstrated that text segmentation algo-rithms can be applied to the generation of e-learning courses.
We use a web-didactic ap-proach that is based on a flat two-level hierar-chical structure.
A new corpus has been com-piled based on featured articles from the EnglishWikipedia that reflects this kind of course struc-ture.
On the broader macro level we applied thelinear LDA-based text segmentation algorithmTopicTiling without providing the expectednumber of boundaries.
The LDA topic model isusually trained with concatenated texts from thevery same dataset TopicTiling is tested on.
Weshowed that it is very difficult to ensure that thetwo sets are always truly disjoint.
The reason isthat concatenated texts normally always haveidentical parts.
This problem is solved by apply-ing a different training and testing method.The more fine grained micro level was seg-mented using BayesSeg, a hierarchical algorithmwhich we provided with the expected number ofboundaries.We used three different evaluation metrics andpresented a scalable random segmentation algo-rithm to establish upper and lower bounds forbaseline comparison.
The results, especially onthe macro level, demonstrate that text segmenta-tion algorithms have evolved enough to be usedfor the automatic generation of e-learning cours-es.An interesting aspect of future research wouldbe the application and creation of real e-learningcontent.
Based on the textual segments, summa-rization and question generation algorithms aswell as automatic replacement with appropriatepictures and videos instead of text could be usedto finally evaluate an automatically generated e-learning course with real learners.Regarding text segmentation in general, futureresearch especially needs to address the difficulttask of transparently and equally measuring theperformance of segmentation algorithms.
Ourresults, i.e., the ones from the random segmenta-tion algorithm, indicate that there are still un-solved issues regarding the penalization of falsepositives and false negatives when the number oftopics or sentences per topic is changed.ReferenceBeeferman, D., Berger, A.
& Lafferty, J., 1999.Statistical Models for Text Segmentation.
Mach.Learn., #feb#, 34(1-3), pp.
177-210.Bird, S., Klein, E. & Loper, E., 2009.
NaturalLanguage Processing with Python.
s.l.
:O'ReillyMedia.Capuano, N. et al., 2009.
LIA: an intelligent advisorfor e-learning.
Interactive Learning Environments,17(3), pp.
221-239.Choi, F. Y. Y., 2000.
Advances in DomainIndependent Linear Text Segmentation.Stroudsburg, PA, USA, Association forComputational Linguistics, pp.
26-33.Eisenstein, J.
& Barzilay, R., 2008.
BayesianUnsupervised Topic Segmentation.
Honolulu,Hawaii, Association for Computational Linguistics,pp.
334-343.Fournier, C., 2013.
Evaluating Text Segmentationusing Boundary Edit Distance.
Stroudsburg, PA,USA, Association for Computational Linguistics,p.
To appear.Fournier, C. & Inkpen, D., 2012.
SegmentationSimilarity and Agreement.
Montreal, Canada,Association for Computational Linguistics, pp.152-161.Galley, M., McKeown, K., Fosler-Lussier, E. & Jing,H., 2003.
Discourse Segmentation of Multi-partyConversation.
Stroudsburg, PA, USA, Associationfor Computational Linguistics, pp.
562-569.Griffiths, T. L. & Steyvers, M., 2004.
Findingscientific topics.
Proceedings of the NationalAcademy of Sciences, April, 101(Suppl.
1), pp.5228-5235.139Hearst, M. A., 1997.
TextTiling: Segmenting Textinto Multi-paragraph Subtopic Passages.
Comput.Linguist., #mar#, 23(1), pp.
33-64.Huang, X. et al., 2002.
Applying Machine Learning toText Segmentation for Information Retrieval.s.l.
:s.n.Janin, A. et al., 2003.
The ICSI Meeting Corpus.
s.l.,s.n., pp.
I-364--I-367 vol.1.Kan, M.-Y., Klavans, J. L. & McKeown, K. R., 1998.Linear Segmentation and Segment Significance.s.l., s.n., pp.
197-205.Kim, S., Medelyan, O., Kan, M.-Y.
& Baldwin, T.,2013.
Automatic keyphrase extraction fromscientific articles.
Language Resources andEvaluation, 47(3), pp.
723-742.Lamprier, S., Amghar, T., Levrat, B.
& Saubion, F.,2007.
On Evaluation Methodologies for TextSegmentation Algorithms.
s.l., s.n., pp.
19-26.Lin, Y.-T., Cheng, S.-C., Yang, J.-T. & Huang, Y.-M., 2009.
An Automatic Course GenerationSystem for Organizing Existent Learning ObjectsUsing Particle Swarm Optimization.
In: M. Chang,et al.
Hrsg.
Learning by Playing.
Game-basedEducation System Design and Development.s.l.
:Springer Berlin Heidelberg, pp.
565-570.Misra, H., Yvon, F., Jose, J. M. & Cappe, O., 2009.Text Segmentation via Topic Modeling: AnAnalytical Study.
New York, NY, USA, ACM, pp.1553-1556.Pevzner, L. & Hearst, M. A., 2002.
A Critique andImprovement of an Evaluation Metric for TextSegmentation.
Comput.
Linguist., #mar#, 28(1), pp.19-36.Riedl, M. & Biemann, C., 2012.
TopicTiling: A TextSegmentation Algorithm Based on LDA.Stroudsburg, PA, USA, Association forComputational Linguistics, pp.
37-42.Strassel, S., Graff, D., Martey, N. & Cieri, C., 2000.Quality Control in Large Annotation ProjectsInvolving Multiple Judges: The Case of the TDTCorpora.
s.l., s.n.Sun, Q., Li, R., Luo, D. & Wu, X., 2008.
TextSegmentation with LDA-based Fisher Kernel.Stroudsburg, PA, USA, Association forComputational Linguistics, pp.
269-272.Swertz, C. et al., 2013.
A Pedagogical Ontology as aPlayground in Adaptive Elearning Environments..s.l., GI, pp.
1955-1960.Tan, X., Ullrich, C., Wang, Y.
& Shen, R., 2010.
TheDesign and Application of an Automatic CourseGeneration System for Large-Scale Education.
s.l.,s.n., pp.
607-609.Utiyama, M. & Isahara, H., 2001.
A Statistical Modelfor Domain-independent Text Segmentation.Stroudsburg, PA, USA, Association forComputational Linguistics, pp.
499-506.Yaari, Y., 1997.
Segmentation of Expository Texts byHierarchical Agglomerative Clustering.
s.l.
:s.n.140
