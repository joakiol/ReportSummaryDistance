A DOP Model  for Semantic Interpretation*Remko Bonnema,  Rens  Bod  and Remko SchaIns t i tu te  for Logic, Language and Computat ionUn ivers i ty  of AmsterdamSpu is t raat  134, 1012 VB AmsterdamBonnema~mars.
let.
uva.
nlRens.
Bod@let.
uva.
nlRemko.
Scha@let.
uva.
nlAbst rac tIn data-oriented language processing, anannotated language corpus is used as astochastic grammar.
The most probableanalysis of a new sentence is constructedby combining fragments from the corpus inthe most probable way.
This approach asbeen successfully used for syntactic anal-ysis, using corpora with syntactic annota-tions such as the Penn Tree-bank.
If a cor-pus with semantically annotated sentencesis used, the same approach can also gen-erate the most probable semantic interpre-tation of an input sentence.
The presentpaper explains this semantic interpretationmethod.
A data-oriented semantic inter-pretation algorithm was tested on two se-mantically annotated corpora: the EnglishATIS corpus and the Dutch OVIS corpus.Experiments how an increase in seman-tic accuracy if larger corpus-fragments aretaken into consideration.1 In t roduct ionData-oriented models of language processing em-body the assumption that human language per-ception and production works with representationsof concrete past language xperiences, rather thanwith abstract grammar rules.
Such models thereforemaintain large corpora of linguistic representationsof previously occurring utterances.
When processinga new input utterance, analyses of this utterance areconstructed by combining fragments from the cor-pus; the occurrence-frequencies of the fragments areused to estimate which analysis is the most probableone.
* This work was partially supported by NWO, theNetherlands Organization for Scientific Research (Prior-ity Programme Language and Speech Technology).For the syntactic dimension of language, vari-ous instantiations of this data-oriented processingor "DOP" approach have been worked out (e.g.Bod (1992-1995); Charniak (1996); Tugwell (1995);Sima'an et al (1994); Sima'an (1994; 1996a);Goodman (1996); Rajman (1995ab); Kaplan (1996);Sekine and Grishman (1995)).
A method for ex-tending it to the semantic domain was first intro-duced by van den Berg et al (1994).
In the presentpaper we discuss a computationally effective versionof that method, and an implemented system thatuses it.
We first summarize the first fully instanti-ated DOP model as presented in Bod (1992-1993).Then we show how this method can be straightfor-wardly extended into a semantic analysis method, ifcorpora are created in which the trees are enrichedwith semantic annotations.
Finally, we discuss animplementation a d report on experiments with twosemantically analyzed corpora (ATIS and OVIS).2 Data -Or iented  Syntact i c  Ana lys i sSo far, the data-oriented processing method hasmainly been applied to corpora with simple syntac-tic annotations, consisting of labelled trees.
Let usillustrate this with a very simple imaginary example.Suppose that a corpus consists of only two trees:S SNP VP NP VPDet N sings Det N whistlesI I I Ievery woman a manFigure 1: Imaginary corpus of two treesWe employ one operation for combining subtrees,called composition, indicated as o; this operationidentifies the leftmost nonterminal leaf node of onetree with the root node of a second tree (i.e., thesecond tree is substituted on the leftmost nontermi-159nal leaf node of the first tree).
A new input sentencelike "A woman whistles" can now be parsed by com-bining subtrees from this corpus.
For instance:S N SNP VP wom~ NP VPDet N whistles Det N whistlest I Ia a womanFigure 2: Derivation and parse for "A womanwhistles"Other derivations may yield the same parse tree;for instance I :S NP Det VPI o INP VP Det N a whistles IwonlaNSNP VPDet N whistlesI Ia womanFigure 3: Different derivation generating the sameparse for ".4 woman whistles"orS Det N SNP VP a woman NP VPDet N whistles Det N whistlesI Ia womanFigure 4: Another derivation generating the sameparse for "A woman whistles"Thus, a parse tree can have many derivations in-volving different corpus-subtrees.
DOP estimatesthe probability of substituting a subtree t on a spe-cific node as the probability of selecting t among allsubtrees in the corpus that could be substituted onthat node.
This probability is equal to the number ofoccurrences ofa subtree t, divided by the total num-ber of occurrences of subtrees t' with the same rootnode label as t : P(t)  = Itl/~t':root(e)=roo~(t) It'l"The probability of a derivation tl o ... o tn can becomputed as the product of the probabilities of thesubtrees this derivation consists of: P( tl o .
.
.
o t,~) =rL P(ti).
The probability of a parse tree is equal to1Here t o u o v o w should be read as ((t o u) o v) o w.the probability that any of its distinct derivations igenerated, which is the sum of the probabilities of allderivations of that parse tree.
Let t~ be the i-th sub-tree in the derivation d that yields tree T, then theprobability of T is given by: P(T)  = ~d 1-Ii P(tid).The DOP method differs from other statisti-cal approaches, uch as Pereira and Schabes (1992),Black et al (1993) and Briscoe (1994), in that itdoes not predefine or train a formal grammar; in-stead it takes subtrees directly from annotated sen-tences in a treebank with a probability propor-tional to the number of occurrences of these sub-trees in the treebank.
Bod (1993b) shows thatDOP can be implemented using context-free pars-ing techniques.
To select the most probable parse,Bod (1993a) gives a Monte Carlo approximation al-gorithm.
Sima'an (1995) gives an efficient polyno-mial algorithm for a sub-optimal solution.The model was tested on the Air Travel In-formation System (ATIS) corpus as analyzed inthe Penn Treebank (Marcus et al (1993)), achiev-ing better test results than other stochasticgrammars (cf.
Bod (1996), Sima'an (1996a),Goodman (1996)).
On Penn's Wall Street Jour-nal corpus, the data-oriented processing approachhas been tested by Sekine and Grishman (1995) andby Charniak (1996).
Though Charniak only usescorpus-subtrees smaller than depth 2 (which in ourexperience constitutes a less-than-optimal versionof the data-oriented processing method), he reportsthat it "outperforms all other non-word-based sta-tistical parsers/grammars on this corpus".
For anoverview of data-oriented language processing, werefer to (Bod and Scha, 1996).3 Data -Or iented  Semant ic  Ana lys i sTo use the DOP method not just for syntactic anal-ysis, but also for semantic interpretation, four stepsmust be taken:1. decide on a formalism for representing themeanings of sentences and surface-constituents.2.
annotate the corpus-sentences and theirsurface-constituents with such semantic repre-sentations.3.
establish a method for deriving the mean-ing representations a sociated with arbitrarycorpus-subtrees and with compositions of suchsubtrees.4.
reconsider the probability calculations.We now discuss these four steps.3.1 Semant ic  formal ismThe decision about the representational formalismis to some extent arbitrary, as long as it has a well-160S :Vx(woman(x)-*sing(x)) S'.qx(man(x)Awhistle(x))NP: ~.YVx(woman(?
)-*Y(x)) VP:sing NP: ;~.Y"'lx(man(x)AY(x)) VP:whistleDet:kX~Y~(X(x)-*Y(x)) N:woman sings Det:XXXY3x(X(?
)AY(x)) N:man whistlesI I Ievery woman a manFigure 5: Imaginary corpus of two trees with syntactic and semantic labels.S:dl(d2)NP:d 1 (d2) VP:sing NP:dl(d2) VP:whistle.i L Det:kXkY~(X(x)---~Y(x)) N:woman stags Det: ~,X~.Y3?
(X(x)^Y(x)) N:man whi tiesI Ievery woman a manFigure 6: Same imaginary corpus of two trees with syntactic and semantic labels using the daughter notation.defined model-theory and is rich enough for repre-senting the meanings of sentences and constituentsthat are relevant for the intended application do-main.
For our exposition in this paper we willuse a wellknown standard formalism: extensionaltype theory (see Gamut (1991)), i.e., a higher-orderlogical language that combines lambda-abstractionwith connectives and quantifiers.
The first imple-mented system for data-oriented semantic interpre-tation, presented in Bonnema (1996), used a differ-ent logical language, however.
And in many appli-cation contexts it probably makes sense to use anA.I.-style language which highlights domain struc-ture (frames, slots, and fillers), while limiting theuse of quantification and negation (see section 5).3.2  Semant ic  annotat ionWe assume a corpus that is already syntacticallyannotated as before: with labelled trees that indi-cate surface constituent structure.
Now the basicidea, taken from van den Berg et al (1994), is toaugment this syntactic annotation with a semanticone: to every meaningful syntactic node, we add atype-logical formula that expresses the meaning ofthe corresponding surface-constituent.
H we wouldcarry out this idea in a completely direct way, thetoy corpus of Figure 1 might, for instance, turn intothe toy corpus of Figure 5.Van den Berg et al indicate how a corpus of thissort may be used for data-oriented semantic inter-pretation.
Their algorithm, however, requires a pro-cedure which can inspect he semantic formula of anode and determine the contribution of the seman-tics of a lower node, in order to be able to "fac-tor out" that contribution.
The details of this pro-cedure have not been specified.
However, van denBerg et ai.
also propose a simpler annotation con-vention which avoids the need for this procedure,and which is computationally more effective: an an-notation convention which indicates explicitly howthe semantic formula for a node is built up on thebasis of the semantic formulas of its daughter nodes.Using this convention, the semantic annotation ofthe corpus trees is indicated as follows:?
For every meaningful lexical node a type logicalformula is specified that represents its meaning.?
For every meaningful non-lexical node a for-mula schema is specified which indicates howits meaning representation may be put togetherout of the formulas assigned to its daughternodes.In the examples below, these schemata use the vari-able dl to indicate the meaning of the leftmostdaughter constituent, d2 to indicate the meaningof the second daughter constituent, etc.
Using thisnotation, the semantically annotated version of thetoy corpus of Figure 1 is the toy corpus rendered inFigure 6.
This kind of semantic annotation is whatwill be used in the construction of the corpora de-scribed in section 5 of this paper.
It may be notedthat the rather oblique description of the semanticsof the higher nodes in the tree would easily lead tomistakes, if annotation would be carried out com-pletely manually.
An annotation tool that makesthe expanded versions of the formulas visible for theannotator is obviously called for.
Such a tool wasdeveloped by Bonnema (1996), it will be briefly de-scribed in section 5.161This annotation convention obviously, assumesthat the meaning representation of a surface-constituent can in fact always be composed out ofthe meaning representations of its subconstituents.This assumption is not unproblematic.
To maintainit in the face of phenomena such as non-standardquantifier scope or discontinuous constituents cre-ates complications in the syntactic or semantic anal-yses assigned to certain sentences and their con-stituents.
It is therefore not clear yet whetherour current treatment ought to be viewed as com-pletely general, or whether a treatment in the veinof van den Berg et al (1994) should be worked out.3.3 The  mean ings  o f  subt rees  and theircompos i t ionsAs in the purely syntactic version of DOP, we nowwant to compute the probability of a (semantic)analysis by considering the most probable way inwhich it can be generated by combining subtreesfrom the corpus.
We can do this in virtually thesame way.
The only novelty is a slight modificationin the process by which a corpus tree is decomposedinto subtrees, and a corresponding modification inthe composition operation which combines ubtrees.If we extract a subtree out of a tree, we replace thesemantics of the new leaf node with a unificationvariable of the same type.
Correspondingly, whenthe composition operation substitutes a subtree atthis node, this unification variable is unified withthe semantic formula on the substituting tree.
(Itis required that the semantic type of this formulamatches the semantic type of the unification vari-able.
)A simple example will make this clear.
First, letus consider what subtrees the corpus makes avail-able now.
As an example, Figure 7 shows one of thedecompositions of the annotated corpus sentence "Aman whist les".
We see that by decomposing the treeinto two subtrees, the semantics at the breakpoint-node N: man is replaced by a variable.
Now ananalysis for the sentence "A woman whistles" can,for instance, be generated in the way shown in Fig-ure 8.3.4 The Stat is t ica l  Mode l  o f  Data -Or ientedSemant ic  In terpretat ionWe now define the probability of an interpretationof an input string.Given a partially annotated corpus as definedabove, the multiset of corpus subtrees consists ofall subtrees with a well-defined top-node seman-tics, that are generated by applying to the trees ofthe corpus the decomposition mechanism describedNP:dl(d2) VP:whistlew !t,os Det: kX~.Y~x(X(x)^Y(x)) N:manI Ia mailVP:whistleDet: ~,XkY3x(X(x)AY(x)) N:U whist lesIaN:manmanFigure 7: Decomposing a tree into subtrees with uni-fication variables.N:womano LNP:dl(d2) VP:whistle womanDet: kXkY--Jx(X(x) AY(x)) N:U whist lesaNP:d I (d2) VP:whistleDec kXkY3x(X(x)^Y(x)) N:woman whist lesIa womanFigure 8: Generating an analysis for "A womanwhist les" .above.
The probability of substituting a subtree t ona specific node is the probability of selecting t amongall subtrees in the multiset hat could be substitutedon that node.
This probability is equal to the num-ber of occurrences ofa subtree t, divided by the totalnumber of occurrences of subtrees t' with the sameroot node label as t:NP( t )  = Et':root(t')=root(t) Irl (1)A derivation of a string is a tuple of subtrees, suchthat their composition results in a tree whose yield isthe string.
The probability of a derivation t l o .
.
.
o tnis the product of the probabilities of these subtrees:P(tl o .
.
.
o tn) = I I  P(td (2)iA tree resulting from a derivation of a string is calleda parse of this string.
The probability of a parse is162the probability that any of its derivations occurs;this is the sum of the probabilities of all its deriva-tions.
Let rid be the i-th subtree in the derivation dthat yields tree T, then the probability of T is givenby:P(T) = E H P(t,d) (3)d iAn interpretation of a string is a formula which isprovably equivalent o the semantic annotation ofthe top node of a parse of this string.
The proba-bility of an interpretation I of a string is the sum ofthe probabilities of the parses of this string with atop node annotated with a formula that is provablyequivalent to I.
Let ti4p be the i-th subtree in thederivation d that yields parse p with interpretationI, then the probability of I is given by:P(I) = E E H P(t,d,) (4)p d iWe choose the most probable interpretation/.of astring s as the most appropriate interpretation of s.In Bonnema (1996) a semantic extension of theDOP parser of Sima'an (1996a) is given.
But in-stead of computing the most likely interpretationof a string, it computes the interpretation of themost likely combination of semantically annotatedsubtrees.
As was shown in Sima'an (1996b), themost likely interpretation ofa string cannot be com-puted in deterministic polynomial time.
It is not yetknown how often the most likely interpretation andthe interpretation of the most likely combination ofsemantically enriched subtrees do actually coincide.4 Imp lementat ionsThe first implementation f a semantic DOP-modelyielded rather encouraging preliminary results on asemantically enriched part of the ATIS-corpus.
Im-plementation details and experimental results canbe found in Bonnema (1996), and Bod et al (1996).We repeat he most important observations:* Data-oriented semantic interpretation seems tobe robust; of the sentences that could be parsed,a significantly higher percentage r ceived a cor-rect semantic interpretation (88%), than an ex-actly correct syntactic analysis (62%).
* The coverage of the parser was rather low(72%), because of the sheer number of differ-ent semantic types and constructs in the trees.?
The parser was fast: on the average six timesas fast as a parser trained on syntax alone.The current implementation is again an extensionof Sima'an (1996a), by Bonnema 2.
In our experi-ments, we notice a robustness and speed-up compa-rable to our experience with the previous implemen-tation.
Besides that, we observe higher accuracy,and higher coverage, due to a new method of orga-nizing the information in the tree-bank before it isused for building the actual parser.A semantically enriched tree-bank will generallycontain a wealth of detail.
This makes it hard fora probabilistic model to estimate all parameters.
Insections 4.1 and 4.2, we discuss a way of generalizingover semantic information in the tree-bank, be\]ore aDOP-parser is trained on the material.
We automat-ically learn a simpler, less redundant representationof the same information.
The method is employedin our current implementation.4.1 Simpli fying the t ree-bankA tree-bank annotated in the manner describedabove, consists of tree-structures with syntactic andsemantic attributes at every node.
The semanticattributes are rules that indicate how the meaning-representation f the expression dominated by thatnode is built-up out of its parts.
Every instance ofa semantic rule at a node has a semantic type asso-ciated with it.
These types usually depend on thelexical instantiations of a syntactic-semantic struc-ture.If we decide to view subtrees as identical iff theirsyntactic structure, the semantic rule at each node,and the semantic type of each node is identical,any fine-grained type-system will cause a huge in-crease in different instantiations of subtrees.
In thetwo tree-banks we tested on, there are many sub-trees that differ in semantic type, hut otherwiseshare the same syntactic/semantic structure.
Disre-garding the semantic types completely, on the otherhand, will cause syntactic onstraints o govern bothsyntactic substitution and semantic unification.
Thesemantic types of constituents often give rise to dif-ferences in semantic structure.
If this type informa-tion is not available during parsing, important clueswill be missing, and loss of accuracy will result.Apparently, we do need some of the informationpresent in the types of semantic expressions.
Ignor-ing semantic types will result in loss of accuracy, butdistinguishing all different semantic types will resultin loss of coverage and generalizing power.
Withthese observations in mind, we decided to group thetypes, and relax the constraints on semantic unifi-cation.
In this approach, every semantic expression,2With thanks to Khalil Sima'an for fruitful discus-sions, and for the use of his parser163and every variable, has a set of types associated withit.
In our semantic DOP model, we modify the con-straints on semantic unification as follows: A vari-able can be unified with an expression, if the inter-section of their respective sets of types is not empty.The semantic types are classified into sets thatcan be distinguished on the basis of their behaviorin the tree-bank.
We let the tree-bank data decidewhich types can be grouped together, and whichtypes should be distinguished.
This way we cangeneralize over semantic types, and exploit relevanttype-information i the parsing process at the sametime.
In learning the optimal grouping of types, wehave two concerns: keeping the number of differentsets of types to a minimum, and increasing the se-mantic determinacy of syntactic structures enhancedwith type-information.
We say that a subtree T,with type-information at every node, is semanticallydeterminate, iff we can determine a unique, correctsemantic rule for every CFG rule R 3 occurring in T.Semantic determinacy is very attractive from a com-putational point of view: if our processed tree-bankhas semantic determinacy, we do not need to involvethe semantic rules in the parsing process.
Instead,the parser yields parses containing information re-garding syntax and semantic types, and the actualsemantic rules can be determined on the basis ofthat information.
In the next section we will elabo-rate on how we learn the grouping of semantic typesfrom the data.4.2 Classification of semant ic  typesThe algorithm presented in this section proceeds bygrouping semantic types occurring with the samesyntactic label into mutually exclusive sets, and as-signing to every syntactic label an index that indi-cates to which set of types its corresponding seman-tic type belongs.
It is an iterative, greedy algorithm.In every iteration a tuple, consisting of a syntacticcategory and a set of types, is selected.
Distinguish-ing this tuple in the tree bank, leads to the great-est increase in semantic determinacy that could befound.
Iteration continues until the increase in se-mantic determinacy is below a certain threshold.Before giving the algorithm, we need some defini-tions:3By "CFG rule", we mean a subtree of depth 1, with-out a specified root-node semantics, but with the featuresrelevant for substitution, i.e.
syntactic ategory and se-mantic type.
Since the subtree of depth 1 is the smalleststructural building block of our DOP model, semanticdeterminacy of every CFG rule in a subtree, means thewhole subtree is semantically determinate.tuplesOtuples(T) is the set of all pairs (c, s) in a tree-bank T, where c is a syntactic ategory, and s isthe set of all semantic types that a constituentof category c in T can have.apply()if c is a category, s is a set of types, and T is atree-bankthen  apply((c, s), T) yields a tree-bank T', byindexing each instance of category c in T, suchthat the c constituent is of semantic type t E s,with a unique index i.ambOi f  T is a tree-bankthen  arab(T) yields an n E N, such that n is thesum of the frequencies of all CFG rules R thatoccur in T with more than one correspondingsemantic rule.The algorithm starts with a tree-bank To; in To,the cardinality of tuples(To) equals the number ofdifferent syntactic ategories in To.1.
Ti=orepeat2.3.4.until5.
Ti-1D((c, s)) = amb(T/)-amb( apply( c, s), Ti) )= {(c,s')13(c, s)tuples(T~)& s' E 21sl)7-/= argmax D( r ' )r'ET;i := i+1Ti := apply(ri, Ti-1)D(T~-I) <-- 5(5)21sl is the powerset of s. In the implementation,a limit can be set to the cardinality of s' E 21sl, toavoid excessively long processing time.
Obviously,the iteration will always end, if we require 5 to be> 0.
When the algorithm finishes, TO,... , Ti--1 con-tain the category/set-of-types pairs that took thelargest steps towards semantic determinacy, and aretherefore distinguished in the tree-bank.
The se-mantic types not occurring in any of these pairs aregrouped together, and treated as equivalent.Note that the algorithm cannot be guaranteed toachieve full semantic determinacy.
The degree of se-mantic determinacy reached, depends on the consis-tency of annotation, annotation errors, the granular-ity of the type system, peculiarities of the language,in short: on the nature of the tree-bank.
To forcesemantic determinacy, we assign a unique index tothose rare instances of categories, i.e, left hand sides164PERUSerIik VwantsIwi!ADV MP# todayI Iniet vandaagSdl.d2VPdl.d2MPMP MPdl.d2MP CON MP P NP!
tomorrow destinatlon.placeI Imaar  morgen naar NP NPtown.almere suffix.buitenI Ialmere buitenFigure 9: A tree from the OVIS tree-bankof CFG-rules, that do not have any distinguishingfeatures to account for their differing semantic rule.Now the resulting tree-bank embodies a functionfrom CFG rules to semantic rules.
We store thisfunction in a table, and strip all semantic rules fromthe trees.
As the experimental results in the nextsection show, using a tree-bank obtained in this wayfor data oriented semantic interpretation, results inhigh coverage, and good probability estimations.5 Exper iments  on  the  OVISt ree -bankThe NWO 4 Priority Programme "Language andSpeech Technology" is a five year research pro-gramme aiming at the development of advancedtelephone-based information systems.
Within thisprogramme, the OVIS 5 tree-bank is created.
Usinga pilot version of the OVIS system, a large numberof human-machine dialogs were collected and tran-scribed.
Currently, 10.000 user utterances have re-ceived a full syntactic and semantic analysis.
Re-grettably, the tree-bank is not available (yet) to thepublic.
More information on the tree-bank can befound on ht tp  : ~~grid.
l e t .
rug.
nZ : 4321/.
The se-mantic domain of all dialogs, is the Dutch railwaysschedule.
The user utterances are mostly answersto questions, like: "From where to where do youwant to travel?
", "At what time do you want toarrive in Amsterdam?
", "Could you please repeatyour destination?".
The annotation method is ro-bust and flexible, as we are dealing with real, spo-ken data, containing a lot of clearly ungrammaticalutterances.
For the annotation task, the annotation4Netherlands Organization for Scientific Research5Public Transport Information Systemworkbench SEMTAGS is used.
It is a graphical inter-face, written by Bonnema, offering all functionalityneeded for examining, evaluating, and editing syn-tactic and semantic analyses.
SEMTAGS is mainlyused for correcting the output of the DOP-parser.It incrementally builds a probabilistic model of cor-rected annotations, allowing it to quickly suggest al-ternative semantic analyses to the annotator.
It tookapproximately 600 hours to annotate these 10.000utterances (supervision included).Syntactic annotation of the tree-bank is conven-tional.
There are 40 different syntactic ategories inthe OVIS tree-bank, that appear to cover the syn-tactic domain quite well.
No grammar is used todetermine the correct annotation; there is a smallset of guidelines, that has the degree of detail nec-essary to avoid an "anything oes"-attitude in theannotator, but leaves room for his/her perception ofthe structure of an utterance.
There is no concep-tual division in the tree-bank between POS-tags andnonterminal categories.Figure 9 shows an example tree from the tree-bank.
It is an analysis of the Dutch sentence: "Ik(I)wil( want ) niet( not ) vandaag( today) maar( but ) mor-gen(tomorrow) naar(to) Almere Buiten(AlmereBuiten)".
The analysis uses the formula schematadiscussed in section 3.2, but here the interpreta-tions of daughter-nodes are so-called "update" ex-pressions, conforming to a frame structure, thatare combined into an update of an informationstate.
The complete interpretation of this utteranceis: user.wants.((\[#today\];\[itomorrow\]);destination.-place.(town.almere;suffix.buiten)).
The semantic for-malism employed in the tree-bank is the topic of thenext section.5.1 The Semant ic  fo rmal i smThe semantic formalism used in the OVIStree-bank, is a frame semantics, defined inVeldhuijzen van Zanten (1996).
In this section, wegive a very short impression.
The well-formednessand validity of an expression is decided on the ba-sis of a type-lattice, called a frame structure.
Theinterpretation of an utterance, is an update of aninformation state.
An information state is a repre-sentation of objects and the relations between them,that complies to the frame structure.
For OVIS, thevarious objects are related to concepts in the traintravel domain.
In updating an information state,the notion of a slot-value assignment is used.
Everyobject can be a slot or a value.
The slot-value assign-ments are defined in a way that corresponds closelyto the linguistic notion of a ground-focus structure.The slot is part of the common ground, the value165Interpretation: Exact Match95 %85 , , ,1 2 3 4 5Max.
subtree depthFigure 10: Size of training set: 8500Sem./Synt.
Analysis: Exact Match90 % 87.69 88.2185.64 - -  88.6685 83 .08- -80" , I I I I1 2 3 4 5Max.
subtree depthFigure 11: Size of training set: 8500is new information.
Added to the semantic formal-ism are pragmatic operators, corresponding to de-nial, confirmation , correction and assertion 6 thatindicate the relation between the value in its scope,and the information state.An update expression is a set of paths through theframe structure, enhanced with pragmatic operatorsthat have scope over a certain part of a path.
Forthe semantic DOP model, the semantic type of anexpression ?
is a pair of types (tz,t2).
Given thetype-lattice "/-of the frame structure, tl is the lowestupper bound in T of the paths in ?, and t2 is thegreatest lower bound in To f  the paths in ?.5.2 Exper imenta l  resultsWe performed a number of experiments, using a ran-dom division of the tree-bank data into test- andtraining-set.
No provisions were taken for unknownwords.
The results reported here, are obtained byrandomly selecting 300 trees from the tree-bank.
Allutterances of length greater than one in this selectionare used as testing material.
We varied the size ofthe training-set, and the maximal depth of the sub-trees.
The average length of the test-sentences was4.74 words.
There was a constraint on the extrac-tion of subtrees from the training-set trees: subtreescould have a maximum of two substitution-sites, andno more than three contiguous lexical nodes (Expe-rience has shown that such limitations improve prob-6In the example in figure 9, the pragmatic opera-tors #, denial, and !, correction, axe usedInterpretation: Exact Match90.76 92.31/0 88 .21~ 90 87.1871.2770"  , ' ,1000 2500 40'00 5500 7000 85'00Tralningset sizeFigure 12: Max.
depth of subtrees = 4Sem.
/Synt .
Analysis:  Exact  Match90 % 87.18 88.2180 79 4968 71170 i , ,1000 2500 40'00 5500 7000 8500Tralningset sizeFigure 13: Max.
depth of subtrees = 4ability estimations, while retaining the full power ofDOP).
Figures 10 and 11 show results using a train-ing set size of 8500 trees.
The maximal depth of sub-trees involved in the parsing process was varied from1 to 5.
Results in figure 11 concern a match withthe total analysis in the test-set, whereas Figure 10shows success on just the resulting interpretation.Only exact matches with the trees and interpreta-tions in the test-set were counted as successes.
Theexperiments show that involving larger fragments inthe parsing process leads to higher accuracy.
Appar-ently, for this domain fragments of depth 5 are toolarge, and deteriorate probability estimations 7.
Theresults also confirm our earlier findings, that seman-tic parsing is robust.
Quite a few analysis trees thatdid not exactly match with their counterparts in thetest-set, yielded a semantic interpretation that didmatch.
Finally, figures 12 and 13 show results fordiffering training-set sizes, using subtrees of maxi-mal depth 4.Re ferencesM.
van den Berg, R. Bod, and R. Scha.
1994.A Corpus-Based Approach to Semantic Interpre-tation.
In Proceedings Ninth Amsterdam Collo-quium.
ILLC,University of Amsterdam.7Experiments using fragments of maximal depth 6and maximal depth 7 yielded the same results as maxi-mal depth 5166E.
Black, R. Garside, and G. Leech.
1993.Statistically-Driven Computer Grammars of En-glish: The IBM/Lancaster Approach.
Rodopi,Amsterdam-Atlanta.R.
Bod.
1992.
A computational model of languageperformance: Data Oriented Parsing.
In Proceed-ings COLING'92, Nantes.R.
Bod.
1993a.
Monte Carlo Parsing.
In ProceedingsThird International Workshop on Parsing Tech-nologies, Tilburg/Durbuy.R.
Bod.
1993b.
Using an Annotated Corpus as aStochastic Grammar.
In Proceedings EACL'93,Utrecht.R.
Bod.
1995.
Enriching Linguistics withStatistics: Performance models of NaturalLanguage.
Phd-thesis, ILLC-dissertationseries 1995-14, University of Amsterdam.ftp : / / f tp .
fwi.
uva.
n l /pub/ theory / i l l c / -d issert  at ions/DS-95-14, text.
ps.
gzR Bod.
1996.
Two Questions about Data-OrientedParsing.
In Proceedings Fourth Workshop on VeryLarge Corpora, Copenhagen, Denmark.
(cmp-lg/9606022).R.
Bod, R. Bonnema, and R. Scha.
1996.
A data-oriented approach to semantic interpretation.
InProceedings Workshop on Corpus-Oriented Se-mantic Analysis, ECAI-96, Budapest, Hungary.(cmp-lg/9606024).R.
Bod and R. Scha.
1996.
Data-oriented lan-guage processing, an overview.
Technical Re-port LP-96-13, Institute for Logic, Language andComputation, University of Amsterdam.
(cmp-lg/9611003).R.
Bonnema.
1996.
Data oriented se-mantics.
Master's thesis, Department ofComputational Linguistics, University of Am-sterdam, http ://mars.
let.
uva.
nl/remko_b/-dopsem/script ie.
htmlT.
Briscoe.
1994.
Prospects for practical parsing ofunrestricted text: Robust statistical parsing tech-niques.
In N. Oostdijk and P de Haan, editors,Corpus-based Research into Language.
Rodopi,Amsterdam.E.
Charniak.
1996.
Tree-bank grammars.
In Pro-ceedings AAAI'96, Portland, Oregon.L.
Gamut.
1991.
Logic, Language and Meaning.Chicago University Press.J Goodman.
1996.
Efficient Algorithms for Parsingthe DOP Model.
In Proceedings Empirical Meth-ods in Natural Language Processing, Philadelphia,Pennsylvania.R.
Kaplan.
1996.
A probabilistic approachto Lexical-Functional Grammar.
Keynote pa-per held at the LFG-workshop 1996, Greno-ble, France.
f tp : / / f tp .parc .
xerox, com/pub/-nl/slides/grenoble96/kaplan-dopt alk .ps.M.
Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a Large Annotated Corpus of En-glish: The Penn Treebank.
Computational Lin-guistics, 19(2).F.
Pereira and Y. Schabes.
1992.
Inside-outsidereestimation from partially bracketed corpora.
InProceedings of the 30th Annual Meeting of theACL, Newark, De.M.
Rajman.
1995a.
Apports d'une approche a basede corpus aux techniques de traitement automa-tique du langage naturel.
Ph.D. thesis, Ecole Na-tionale Superieure des Telecommunications, Paris.M.
Rajman.
1995b.
Approche probabiliste del'analyse syntaxique.
Traitement Automatique desLangues, 36:1-2.S.
Sekine and R. Grishman.
1995.
A corpus-based probabilistic grammar with only twonon-terminals.
In Proceedings Fourth Interna-tional Workshop on Parsing Technologies, Prague,Czech Republic.K.
Sima'an, R. Bod, S. Krauwer, and R. Scha.
1994.Efficient Disambiguation by means of StochasticTree Substitution Grammars.
In Proceedings In-ternational Conference on New Methods in Lan-guage Processing.
CCL, UMIST, Manchester.K.
Sima'an.
1995.
An optimized algorithm for DataOriented Parsing.
In Proceedings InternationalConference on Recent Advances in Natural Lan-guage Processing.
Tzigov Chark, Bulgaria.K.
Sima'an.
1996a.
An optimized algorithm forData Oriented Parsing.
In R. Mitkov and N. Ni-colov, editors, Recent Advances in Natural Lan-guage Processing 1995, volume 136 of Current Is-sues in Linguistic Theory.
John Benjamins, Ams-terdam.K.
Sima'an.
1996b.
Computational Complexity ofProbabilistic Disambiguation by means of Tree-Grammars.
In Proceedings COLING'96, Copen-hagen, Denmark.D.
Tugwell.
1995.
A state-transition grammar fordata-oriented parsing.
In Proceedings EuropeanChapter of the ACL'95, Dublin, Ireland.G.
Veldhuijzen van Zanten.
1996.
Seman-tics of update expressions.
NWO priorityProgramme Language and Speech Technology,http ://grid.
let.
rug.
nl : 4321/.167
