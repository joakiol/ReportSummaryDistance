Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 464?469,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsNon-distributional Word Vector RepresentationsManaal Faruqui and Chris DyerLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USA{mfaruqui, cdyer}@cs.cmu.eduAbstractData-driven representation learning forwords is a technique of central importancein NLP.
While indisputably useful as asource of features in downstream tasks,such vectors tend to consist of uninter-pretable components whose relationship tothe categories of traditional lexical seman-tic theories is tenuous at best.
We presenta method for constructing interpretableword vectors from hand-crafted linguis-tic resources like WordNet, FrameNet etc.These vectors are binary (i.e, contain only0 and 1) and are 99.9% sparse.
We analyzetheir performance on state-of-the-art eval-uation methods for distributional modelsof word vectors and find they are competi-tive to standard distributional approaches.1 IntroductionDistributed representations of words have beenshown to benefit a diverse set of NLP tasks in-cluding syntactic parsing (Lazaridou et al., 2013;Bansal et al., 2014), named entity recognition(Guo et al., 2014) and sentiment analysis (Socheret al., 2013).
Additionally, because they can beinduced directly from unannotated corpora, theyare likewise available in domains and languageswhere traditional linguistic resources do not ex-haust.
Intrinsic evaluations on various tasks arehelping refine vector learning methods to discoverrepresentations that captures many facts about lex-ical semantics (Turney, 2001; Turney and Pantel,2010).Yet induced word vectors do not look anythinglike the representations described in most lexi-cal semantic theories, which focus on identifyingclasses of words (Levin, 1993; Baker et al., 1998;Schuler, 2005; Miller, 1995).
Though expensiveto construct, conceptualizing word meanings sym-bolically is important for theoretical understand-ing and interpretability is desired in computationalmodels.Our contribution to this discussion is a newtechnique that constructs task-independent wordvector representations using linguistic knowledgederived from pre-constructed linguistic resourceslike WordNet (Miller, 1995), FrameNet (Baker etal., 1998), Penn Treebank (Marcus et al., 1993)etc.
In such word vectors every dimension is a lin-guistic feature and 1/0 indicates the presence orabsence of that feature in a word, thus the vec-tor representations are binary while being highlysparse (?
99.9%).
Since these vectors do not en-code any word cooccurrence information, they arenon-distributional.
An additional benefit of con-structing such vectors is that they are fully inter-pretable i.e, every dimension of these vectors mapsto a linguistic feature unlike distributional wordvectors where the vector dimensions have no in-terpretability.Of course, engineering feature vectors from lin-guistic resources is established practice in manyapplications of discriminative learning; e.g., pars-ing (McDonald and Pereira, 2006; Nivre, 2008)or part of speech tagging (Ratnaparkhi, 1996;Collins, 2002).
However, despite a certain com-mon inventories of features that re-appear acrossmany tasks, feature engineering tends to be seenas a task-specific problem, and engineered featurevectors are not typically evaluated independentlyof the tasks they are designed for.
We evaluate thequality of our linguistic vectors on a number oftasks that have been proposed for evaluating dis-tributional word vectors.
We show that linguisticword vectors are comparable to current state-of-the-art distributional word vectors trained on bil-lions of words as evaluated on a battery of seman-tic and syntactic evaluation benchmarks.11Our vectors can be downloaded at: https://github.com/mfaruqui/non-distributional464Lexicon Vocabulary FeaturesWordNet 10,794 92,117Supersense 71,836 54FrameNet 9,462 4,221Emotion 6,468 10Connotation 76,134 12Color 14,182 12Part of Speech 35,606 20Syn.
& Ant.
35,693 75,972Union 119,257 172,418Table 1: Sizes of vocabualry and features inducedfrom different linguistic resources.2 Linguistic Word VectorsWe construct linguistic word vectors by extractingword level information from linguistic resources.Table 1 shows the size of vocabulary and numberof features induced from every lexicon.
We nowdescribe various linguistic resources that we usefor constructing linguistic word vectors.WordNet.
WordNet (Miller, 1995) is an En-glish lexical database that groups words into setsof synonyms called synsets and records a num-ber of relations among these synsets or theirmembers.
For a word we look up its synsetfor all possible part of speech (POS) tags thatit can assume.
For example, film will haveSYNSET.FILM.V.01 and SYNSET.FILM.N.01 asfeatures as it can be both a verb and a noun.
In ad-dition to synsets, we include the hyponym (for ex.HYPO.COLLAGEFILM.N.01), hypernym (for ex.HYPER:SHEET.N.06) and holonym synset of theword as features.
We also collect antonyms andpertainyms of all the words in a synset and includethose as features in the linguistic vector.Supsersenses.
WordNet partitions nouns andverbs into semantic field categories known assupsersenses (Ciaramita and Altun, 2006; Nas-tase, 2008).
For example, lioness evokes the su-persense SS.NOUN.ANIMAL.
These supersenseswere further extended to adjectives (Tsvetkov etal., 2014).2We use these supsersense tags fornouns, verbs and adjectives as features in the lin-guistic word vectors.FrameNet.
FrameNet (Baker et al., 1998; Fill-more et al., 2003) is a rich linguistic resource thatcontains information about lexical and predicate-argument semantics in English.
Frames can berealized on the surface by many different word2http://www.cs.cmu.edu/?ytsvetko/adj-supersenses.tar.gztypes, which suggests that the word types evok-ing the same frame should be semantically related.For every word, we use the frame it evokes alongwith the roles of the evoked frame as its features.Since, information in FrameNet is part of speech(POS) disambiguated, we couple these featurewith the corresponding POS tag of the word.
Forexample, since appreciate is a verb, it will havethe following features: VERB.FRAME.REGARD,VERB.FRAME.ROLE.EVALUEE etc.Emotion & Sentiment.
Mohammad and Turney(2013) constructed two different lexicons that as-sociate words to sentiment polarity and to emo-tions resp.
using crowdsourcing.
The polar-ity is either positive or negative but there areeight different kinds of emotions like anger, an-ticipation, joy etc.
Every word in the lexicon isassociated with these properties.
For example,cannibal evokes POL.NEG, EMO.DISGUST andEMO.FEAR.
We use these properties as featuresin linguistic vectors.Connotation.
Feng et al.
(2013) construct a lex-icon that contains information about connotationof words that are seemingly objective but oftenallude nuanced sentiment.
They assign positive,negative and neutral connotations to these words.This lexicon differs from Mohammad and Tur-ney (2013) in that it has a more subtle shade ofsentiment and it extends to many more words.For example, delay has a negative connotationCON.NOUN.NEG, floral has a positive connota-tion CON.ADJ.POS and outline has a neutral con-notation CON.VERB.NEUT.Color.
Most languages have expressions involv-ing color, for example green with envy and greywith uncertainly are phrases used in English.
Theword-color associtation lexicon produced by Mo-hammad (2011) using crowdsourcing lists the col-ors that a word evokes in English.
We use everycolor in this lexicon as a feature in the vector.
Forexample, COLOR.RED is a feature evoked by theword blood.Part of Speech Tags.
The Penn Treebank (Mar-cus et al., 1993) annotates naturally occurring textfor linguistic structure.
It contains syntactic parsetrees and POS tags for every word in the corpus.We collect all the possible POS tags that a word isannotated with and use it as features in the linguis-tic vectors.
For example, love has PTB.NOUN,465Word POL.POS COLOR.PINK SS.NOUN.FEELING PTB.VERB ANTO.FAIR ?
?
?
CON.NOUN.POSlove 1 1 1 1 0 1hate 0 0 1 1 0 0ugly 0 0 0 0 1 0beauty 1 1 0 0 0 1refundable 0 0 0 0 0 1Table 2: Some linguistic word vectors.
1 indicates presence and 0 indicates absence of a linguisticfeature.PTB.VERB as features.Synonymy & Antonymy.
We use Roget?s the-saurus (Roget, 1852) to collect sets of synony-mous words.3For every word, its synonymousword is used as a feature in the linguistic vec-tor.
For example, adoration and affair havea feature SYNO.LOVE, admissible has a fea-ture SYNO.ACCEPTABLE.
The synonym lexi-con contains 25,338 words after removal of mul-tiword phrases.
In a similar manner, we alsouse antonymy relations between words as fea-tures in the word vector.
The antonymous wordsfor a given word were collected from Ordway(1913).4An example would be of impartial-ity, which has features ANTO.FAVORITISM andANTO.INJUSTICE.
The antonym lexicon has10,355 words.
These features are different fromthose induced from WordNet as the former en-code word-word relations whereas the latter en-code word-synset relations.After collecting features from the various lin-guistic resources described above we obtain lin-guistic word vectors of length 172,418 dimen-sions.
These vectors are 99.9% sparse i.e, eachvector on an average contains only 34 non-zerofeatures out of 172,418 total features.
On averagea linguistic feature (vector dimension) is active for15 word types.
The linguistic word vectors con-tain 119,257 unique word types.
Table 2 showslinguistic vectors for some of the words.3 ExperimentsWe first briefly describe the evaluation tasks andthen present results.3.1 Evaluation TasksWord Similarity.
We evaluate our word repre-sentations on three different benchmarks to mea-sure word similarity.
The first one is the widely3http://www.gutenberg.org/ebooks/106814https://archive.org/details/synonymsantonyms00ordwialaused WS-353 dataset (Finkelstein et al., 2001),which contains 353 pairs of English words thathave been assigned similarity ratings by humans.The second is the RG-65 dataset (Rubenstein andGoodenough, 1965) of 65 words pairs.
The thirddataset is SimLex (Hill et al., 2014) which hasbeen constructed to overcome the shortcomingsof WS-353 and contains 999 pairs of adjectives,nouns and verbs.
Word similarity is computedusing cosine similarity between two words andSpearman?s rank correlation is reported betweenthe rankings produced by vector model against thehuman rankings.Sentiment Analysis.
Socher et al.
(2013) cre-ated a treebank containing sentences annotatedwith fine-grained sentiment labels on phrases andsentences from movie review excerpts.
Thecoarse-grained treebank of positive and negativeclasses has been split into training, development,and test datasets containing 6,920, 872, and 1,821sentences, respectively.
We use average of theword vectors of a given sentence as features inan `2-regularized logistic regression for classifica-tion.
The classifier is tuned on the dev set and ac-curacy is reported on the test set.NP-Bracketing.
Lazaridou et al.
(2013) con-structed a dataset from the Penn TreeBank (Mar-cus et al., 1993) of noun phrases (NP) of lengththree words, where the first can be an adjective ora noun and the other two are nouns.
The task is topredict the correct bracketing in the parse tree fora given noun phrase.
For example, local (phonecompany) and (blood pressure) medicine exhibitleft and right bracketing respectively.
We appendthe word vectors of the three words in the NP in or-der and use them as features in an `2-regularizedlogistic regression classifier.
The dataset contains2,227 noun phrases split into 10 folds.
The clas-sifier is tuned on the first fold and cross-validationaccuracy is reported on the remaining nine folds.466Vector Length (D) Params.
Corpus Size WS-353 RG-65 SimLex Senti NPSkip-Gram 300 D ?N 300 billion 65.6 72.8 43.6 81.5 80.1Glove 300 D ?N 6 billion 60.5 76.6 36.9 77.7 77.9LSA 300 D ?N 1 billion 67.3 77.0 49.6 81.1 79.7Ling Sparse 172,418 ?
?
44.6 77.8 56.6 79.4 83.3Ling Dense 300 D ?N ?
45.4 67.0 57.8 75.4 76.2Skip-Gram ?
Ling Sparse 172,718 ?
?
67.1 80.5 55.5 82.4 82.8Table 3: Performance of different type of word vectors on evaluation tasks reported by Spearman?scorrelation (first 3 columns) and Accuracy (last 2 columns).
Bold shows the best performance for a task.3.2 Linguistic Vs. Distributional VectorsIn order to make our linguistic vectors comparableto publicly available distributional word vectors,we perform singular value decompostion (SVD)on the linguistic matrix to obtain word vectors oflower dimensionality.
If L ?
{0, 1}N?Dis the lin-guistic matrix with N word types and D linguisticfeatures, then we can obtain U ?
RN?Kfrom theSVD of L as follows: L = U?V>, with K beingthe desired length of the lower dimensional space.We compare both sparse and dense linguisticvectors to three widely used distributional wordvector models.
The first two are the pre-trainedSkip-Gram (Mikolov et al., 2013)5and Glove(Pennington et al., 2014)6word vectors each oflength 300, trained on 300 billion and 6 billionwords respectively.
We used latent semantic anal-ysis (LSA) to obtain word vectors from the SVDdecomposition of a word-word cooccurrence ma-trix (Turney and Pantel, 2010).
These were trainedon 1 billion words of Wikipedia with vector length300 and context window of 5 words.3.3 ResultsTable 3 shows the performance of different wordvector types on the evaluation tasks.
It can be seenthat although Skip-Gram, Glove & LSA performbetter than linguistic vectors on WS-353, the lin-guistic vectors outperform them by a huge mar-gin on SimLex.
Linguistic vectors also performbetter at RG-65.
On sentiment analysis, linguis-tic vectors are competitive with Skip-Gram vec-tors and on the NP-bracketing task they outper-form all distributional vectors with a statisticallysignificant margin (p < 0.05, McNemar?s test Di-etterich (1998)).
We append the sparse linguis-tic vectors to Skip-Gram vectors and evaluate theresultant vectors as shown in the bottom row ofTable 3.
The combined vector outperforms Skip-5https://code.google.com/p/word2vec6http://www-nlp.stanford.edu/projects/glove/Gram on all tasks, showing that linguistic vectorscontain useful information orthogonal to distribu-tional information.It is evident from the results that linguistic vec-tors are either competitive or better to state-of-the-art distributional vector models.
Sparse linguis-tic word vectors are high dimensional but they arealso sparse, which makes them computationallyeasy to work with.4 DiscussionLinguistic resources like WordNet have found ex-tensive applications in lexical semantics, for ex-ample, for word sense disambiguation, word simi-larity etc.
(Resnik, 1995; Agirre et al., 2009).
Re-cently there has been interest in using linguisticresources to enrich word vector representations.In these approaches, relational information amongwords obtained from WordNet, Freebase etc.
isused as a constraint to encourage words with sim-ilar properties in lexical ontologies to have simi-lar word vectors (Xu et al., 2014; Yu and Dredze,2014; Bian et al., 2014; Fried and Duh, 2014;Faruqui et al., 2015a).
Distributional represen-tations have also been shown to improve by us-ing experiential data in addition to distributionalcontext (Andrews et al., 2009).
We have shownthat simple vector concatenation can likewise beused to improve representations (further confirm-ing the established finding that lexical resourcesand cooccurrence information provide somewhatorthogonal information), but it is certain that morecareful combination strategies can be used.Although distributional word vector dimensionscannot, in general, be identified with linguisticproperties, it has been shown that some vectorconstruction strategies yield dimensions that arerelatively more interpretable (Murphy et al., 2012;Fyshe et al., 2014; Fyshe et al., 2015; Faruqui etal., 2015b).
However, such analysis is difficultto generalize across models of representation.
Inconstrast to distributional word vectors, linguistic467word vectors have interpretable dimensions as ev-ery dimension is a linguistic property.Linguistic word vectors require no training asthere are no parameters to be optimized, meaningthey are computationally economical.
While goodquality linguistic word vectors may only be ob-tained for languages with rich linguistic resources,such resources do exist in many languages andshould not be disregarded.5 ConclusionWe have presented a novel method of constructingword vector representations solely using linguisticknowledge from pre-existing linguistic resources.These non-distributional, linguistic word vectorsare competitive to the current models of distribu-tional word vectors as evaluated on a battery oftasks.
Linguistic vectors are fully interpretableas every dimension is a linguistic feature and arehighly sparse, so they are computationally easy towork with.AcknowledgementWe thank Nathan Schneider for giving commentson an earlier draft of this paper and the anonymousreviewers for their feedback.ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distri-butional and wordnet-based approaches.
In Proc.
ofNAACL.Mark Andrews, Gabriella Vigliocco, and David Vin-son.
2009.
Integrating experiential and distribu-tional data to learn semantic representations.
Psy-chological review, 116(3):463.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proc.
ofACL.Mohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring continuous word representations fordependency parsing.
In Proc.
of ACL.Jiang Bian, Bin Gao, and Tie-Yan Liu.
2014.Knowledge-powered deep learning for word embed-ding.
In Proc.
of MLKDD.Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informa-tion extraction with a supersense sequence tagger.
InProc.
of EMNLP.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and ex-periments with perceptron algorithms.
In Proc.
ofEMNLP.Thomas G. Dietterich.
1998.
Approximate statisticaltests for comparing supervised classification learn-ing algorithms.
Neural Computation.Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, ChrisDyer, Eduard Hovy, and Noah A. Smith.
2015a.Retrofitting word vectors to semantic lexicons.
InProc.
of NAACL.Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, ChrisDyer, and Noah A. Smith.
2015b.
Sparse overcom-plete word vector representations.
In Proc.
of ACL.Song Feng, Jun Seok Kang, Polina Kuznetsova, andYejin Choi.
2013.
Connotation lexicon: A dash ofsentiment beneath the surface meaning.
In Proc.
ofACL.Charles Fillmore, Christopher Johnson, and MiriamPetruck.
2003.
Lexicographic relevance: select-ing information from corpus evidence.
InternationalJournal of Lexicography.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2001.
Placing search in context: theconcept revisited.
In Proc.
of WWW.Daniel Fried and Kevin Duh.
2014.
Incorporating bothdistributional and relational semantics in word rep-resentations.
arXiv preprint arXiv:1412.4369.Alona Fyshe, Partha P. Talukdar, Brian Murphy, andTom M. Mitchell.
2014.
Interpretable semantic vec-tors from a joint model of brain- and text- basedmeaning.
In Proc.
of ACL.Alona Fyshe, Leila Wehbe, Partha P. Talukdar, BrianMurphy, and Tom M. Mitchell.
2015.
A composi-tional and interpretable semantic space.
In Proc.
ofNAACL.Jiang Guo, Wanxiang Che, Haifeng Wang, and TingLiu.
2014.
Revisiting embedding features for sim-ple semi-supervised learning.
In Proc.
of EMNLP.Felix Hill, Roi Reichart, and Anna Korhonen.
2014.Simlex-999: Evaluating semantic models with (gen-uine) similarity estimation.
CoRR, abs/1408.3456.Angeliki Lazaridou, Eva Maria Vecchi, and MarcoBaroni.
2013.
Fish transporters and miraclehomes: How compositional distributional semanticscan help NP parsing.
In Proc.
of EMNLP.Beth Levin.
1993.
English verb classes and alterna-tions : a preliminary investigation.
University ofChicago Press.Mitchell P Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Compu-tational linguistics, 19(2):313?330.468Ryan T McDonald and Fernando CN Pereira.
2006.Online learning of approximate dependency parsingalgorithms.
In Proc.
of EACL.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.George A Miller.
1995.
Wordnet: a lexical databasefor english.
Communications of the ACM.Saif M. Mohammad and Peter D. Turney.
2013.Crowdsourcing a word-emotion association lexicon.Computational Intelligence, 29(3):436?465.Saif Mohammad.
2011.
Colourful language: Mea-suring word-colour associations.
In Proc.
of theWorkshop on Cognitive Modeling and Computa-tional Linguistics.Brian Murphy, Partha Talukdar, and Tom Mitchell.2012.
Learning effective and interpretable seman-tic models using non-negative sparse embedding.
InProc.
of COLING.Vivi Nastase.
2008.
Unsupervised all-words wordsense disambiguation with grammatical dependen-cies.
In Proc.
of IJCNLP.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
Computational Lin-guistics, 34(4):513?553.Edith Bertha Ordway.
1913.
Synonyms and Antonyms:An Alphabetical List of Words in Common Use,Grouped with Others of Similar and Opposite Mean-ing.
Sully and Kleinteich.Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning.
2014.
Glove: Global vectors forword representation.
In Proc.
of EMNLP.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proc.
ofEMNLP.Philip Resnik.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
In Proc.of IJCAI.P.
M. Roget.
1852.
Roget?s Thesaurus of Englishwords and phrases.
Available from Project Gutem-berg.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Commun.
ACM,8(10).Karin Kipper Schuler.
2005.
Verbnet: A Broad-coverage, Comprehensive Verb Lexicon.
Ph.D. the-sis, University of Pennsylvania.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proc.
of EMNLP.Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, ArchnaBhatia, Manaal Faruqui, and Chris Dyer.
2014.Augmenting english adjective senses with super-senses.
In Proc.
of LREC.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning : Vector space models of seman-tics.
JAIR, pages 141?188.Peter D. Turney.
2001.
Mining the web for synonyms:Pmi-ir versus lsa on toefl.
In Proc.
of ECML.Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, GangWang, Xiaoguang Liu, and Tie-Yan Liu.
2014.
Rc-net: A general framework for incorporating knowl-edge into word representations.
In Proc.
of CIKM.Mo Yu and Mark Dredze.
2014.
Improving lexicalembeddings with semantic knowledge.
In Proc.
ofACL.469
