Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 399?409,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsSubgroup Detection in Ideological DiscussionsAmjad Abu-JbaraEECS DepartmentUniversity of MichiganAnn Arbor, MI, USAamjbara@umich.eduMona DiabCenter for Computational Learning SystemsColumbia UniversityNew York, NY, USAmdiab@ccls.columbia.eduPradeep DasigiDepartment of Computer ScienceColumbia UniversityNew York, NY, USApd2359@columbia.eduDragomir RadevEECS DepartmentUniversity of MichiganAnn Arbor, MI, USAradev@umich.eduAbstractThe rapid and continuous growth of socialnetworking sites has led to the emergence ofmany communities of communicating groups.Many of these groups discuss ideological andpolitical topics.
It is not uncommon that theparticipants in such discussions split into twoor more subgroups.
The members of each sub-group share the same opinion toward the dis-cussion topic and are more likely to agree withmembers of the same subgroup and disagreewith members from opposing subgroups.
Inthis paper, we propose an unsupervised ap-proach for automatically detecting discussantsubgroups in online communities.
We analyzethe text exchanged between the participants ofa discussion to identify the attitude they carrytoward each other and towards the various as-pects of the discussion topic.
We use attitudepredictions to construct an attitude vector foreach discussant.
We use clustering techniquesto cluster these vectors and, hence, determinethe subgroup membership of each participant.We compare our methods to text clusteringand other baselines, and show that our methodachieves promising results.1 IntroductionOnline forums discussing ideological and politicaltopics are common1.
When people discuss a dis-puted topic they usually split into subgroups.
Themembers of each subgroup carry the same opinion1www.politicalforum.com, www.createdebate.com,www.forandagainst.com, etctoward the discission topic.
The member of a sub-group is more likely to show positive attitude to themembers of the same subgroup, and negative atti-tude to the members of opposing subgroups.For example, let us consider the following twosnippets from a debate about the enforcement of anew immigration law in Arizona state in the UnitedStates:(1) Discussant 1: Arizona immigration law is good.Illegal immigration is bad.
(2) Discussant 2: I totally disagree with you.
Ari-zona immigration law is blatant racism, and quiteunconstitutional.In (1), the writer is expressing positive attituderegarding the immigration law and negative attituderegarding illegal immigration.
The writer of (2) isexpressing negative attitude towards the writer of(1) and negative attitude regarding the immigrationlaw.
It is clear from this short dialog that the writerof (1) and the writer of (2) are members of twoopposing subgroups.
Discussant 1 is supporting thenew law, while Discussant 2 is against it.In this paper, we present an unsupervised ap-proach for determining the subgroup membership ofeach participant in a discussion.
We use linguistictechniques to identify attitude expressions, their po-larities, and their targets.
The target of attitude couldbe another discussant or an entity mentioned in thediscussion.
We use sentiment analysis techniquesto identify opinion expressions.
We use named en-399tity recognition and noun phrase chunking to iden-tify the entities mentioned in the discussion.
Theopinion-target pairs are identified using a number ofsyntactic and semantic rules.For each participant in the discussion, we con-struct a vector of attitude features.
We call this vec-tor the discussant attitude profile.
The attitude pro-file of a discussant contains an entry for every otherdiscussant and an entry for every entity mentionedin the discission.
We use clustering techniques tocluster the attitude vector space.
We use the clus-tering results to determine the subgroup structure ofthe discussion group and the subgroup membershipof each participant.The rest of this paper is organized as follows.
Sec-tion 2 examines the previous work.
We describe thedata used in the paper in Section 2.4.
Section 3presents our approach.
Experiments, results andanalysis are presented in Section 4.
We concludein Section 52 Related Work2.1 Sentiment AnalysisOur work is related to a huge body of work on sen-timent analysis.
Previous work has studied senti-ment in text at different levels of granularity.
Thefirst level is identifying the polarity of individualwords.
Hatzivassiloglou and McKeown (1997) pro-posed a method to identify the polarity of adjec-tives based on conjunctions linking them.
Turneyand Littman (2003) used pointwise mutual infor-mation (PMI) and latent semantic analysis (LSA)to compute the association between a given wordand a set of positive/negative seed words.
Taka-mura et al (2005) proposed using a spin model topredict word polarity.
Other studies used Word-Net to improve word polarity prediction (Hu andLiu, 2004a; Kamps et al, 2004; Kim and Hovy,2004; Andreevskaia and Bergler, 2006).
Hassanand Radev (2010) used a random walk model builton top of a word relatedness network to predict thesemantic orientation of English words.
Hassan etal.
(2011) proposed a method to extend their randomwalk model to assist word polarity identification inother languages including Arabic and Hindi.Other work focused on identifying the subjectiv-ity of words.
The goal of this work is to deter-mine whether a given word is factual or subjective.We use previous work on subjectivity and polar-ity prediction to identify opinion words in discus-sions.
Some of the work on this problem classi-fies words as factual or subjective regardless of theircontext (Wiebe, 2000; Hatzivassiloglou and Wiebe,2000; Banea et al, 2008).
Some other work no-ticed that the subjectivity of a given word dependson its context.
Therefor, several studies proposedusing contextual features to determine the subjec-tivity of a given word within its context (Riloff andWiebe, 2003; Yu and Hatzivassiloglou, 2003; Na-sukawa and Yi, 2003; Popescu and Etzioni, 2005).The second level of granularity is the sentencelevel.
Hassan et al (2010) presents a method foridentifying sentences that display an attitude fromthe text writer toward the text recipient.
They de-fine attitude as the mental position of one partici-pant with regard to another participant.
A very de-tailed survey that covers techniques and approachesin sentiment analysis and opinion mining could befound in (Pang and Lee, 2008).2.2 Opinion Target ExtractionSeveral methods have been proposed to identifythe target of an opinion expression.
Most of thework have been done in the context of product re-views mining (Hu and Liu, 2004b; Kobayashi etal., 2007; Mei et al, 2007; Stoyanov and Cardie,2008).
In this context, opinion targets usually referto product features (i.e.
product components or at-tributes, as defined by Liu (2009)).
In the work ofHu and Liu (2004b), they treat frequent nouns andnoun phrases as product feature candidates.
In ourwork, we extract as targets frequent noun phrasesand named entities that are used by two or more dif-ferent discussants.
Scaffidi et al (2007) propose alanguage model approach to product feature extrac-tion.
They assume that product features are men-tioned more often in product reviews than they ap-pear in general English text.
However, such statisticsmay not be reliable when the corpus size is small.In another related work, Jakob andGurevych (2010) showed that resolving theanaphoric links in the text significantly improvesopinion target extraction.
In our work, we useanaphora resolution to improve opinion-target400Participant A posted: I support Arizona because they have every right to do so.
They are just upholding well-establishedfederal law.
All states should enact such a law.Participant B commented on A?spost:I support the law because the federal government is either afraid or indifferent to the issue.
Arizonahas the right and the responsibility to protect the people of the State of Arizona.
If this requires apossible slight inconvenience to any citizen so be it.Participant C commented on B?spost:That is such a sad thing to say.
You do realize that under the 14th Amendment, the very interactionof a police officer asking you to prove your citizenship is Unconstitutional?
As soon as you starttrading Constitutional rights for ?security?, then you?ve lost.Table 1: Example posts from the Arizona Immigration Law threadpairing as shown in Section 3 below.2.3 Community MiningPrevious work also studied community mining in so-cial media sites.
Somasundaran and Wiebe (2009)presents an unsupervised opinion analysis methodfor debate-side classification.
They mine the webto learn associations that are indicative of opinionstances in debates and combine this knowledge withdiscourse information.
Anand et al (2011) presenta supervised method for stance classification.
Theyuse a number of linguistic and structural featuressuch as unigrams, bigrams, cue words, repeatedpunctuation, and opinion dependencies to build astance classification model.
This work is limited todual sided debates and defines the problem as a clas-sification task where the two debate sides are knowbeforehand.
Our work is characterized by handlingmulti-side debates and by regarding the problem asa clustering problem where the number of sides isnot known by the algorithm.
This work also uti-lizes only discussant-to-topic attitude predictions fordebate-side classification.
Out work utilizes bothdiscussant-to-topic and discussant-to-discussant at-titude predictions.In another work, Kim and Hovy (2007) predictthe results of an election by analyzing discussionthreads in online forums that discuss the elections.They use a supervised approach that uses unigrams,bigrams, and trigrams as features.
In contrast, ourwork is unsupervised and uses different types infor-mation.
Moreover, although this work is related toours at the goal level, it does not involve any opinionanalysis.Another related work classifies the speakers sidein a corpus of congressional floor debates, usingthe speakers final vote on the bill as a labelingfor side (Thomas et al, 2006; Bansal et al, 2008;Yessenalina et al, 2010).
This work infers agree-ment between speakers based on cases where onespeaker mentions another by name, and a simple al-gorithm for determining the polarity of the sentencein which the mention occurs.
This work shows thateven with the resulting sparsely connected agree-ment structure, the MinCut algorithm can improveover stance classification based on textual informa-tion alone.
This work also requires that the de-bate sides be known by the algorithm and it onlyidentifies discussant-to-discussant attitude.
In ourexperiments below we show that identifying bothdiscussant-to-discussant and discussant-to-topic at-titudes achieves better results.2.4 DataIn this section, we describe the datasets used inthis paper.
We use three different datasets.
Thefirst dataset (politicalforum, henceforth) consists of5,743 posts collected from a political forum2.
Allthe posts are in English.
The posts cover 12 dis-puted political and ideological topics.
The discus-sants of each topic were asked to participate in apoll.
The poll asked them to determine their stanceon the discussion topic by choosing one item from alist of possible arguments.
The list of participantswho voted for each argument was published withthe poll results.
Each poll was accompanied by adiscussion thread.
The people who participated inthe poll were allowed to post text to that thread tojustify their choices and to argue with other partic-ipants.
We collected the votes and the discussionthread of each poll.
We used the votes to identifythe subgroup membership of each participant.The second dataset (createdebate, henceforth)comes from an online debating site 3.
It consists of2http://www.politicalforum.com3http://www.createdebate.com401Source Topic Question #Sides #Posts #ParticipantsPoliticalforumArizona Immigration Law Do you support Arizona in its decision to enact theirImmigration Enforcement law?2 738 59Airport Security Should we pick muslims out of the line and give ad-ditional scrutiny/screening?4 735 69Vote for Obama Will you vote for Obama in the 2012 Presidentialelections?2 2599 197CreatedebateEvolution Has evolution been scientifically proved?
2 194 98Social networking sites It is easier to maintain good relationships in socialnetworking sites such as Facebook.2 70 31Abortion Should abortion be banned 3 477 70WikipediaIreland Misleading description of Irland island partition 3 40 10South Africa Goverment Was the current form of South African governmentborn in May 1910?3 23 5Oil Spill Obama?s response to gulf oil spill 3 30 12Table 2: Example threads from our three datasets30 debates containing a total of 2,712 posts.
Eachdebate is about one topic.
The description of eachdebate states two or more positions regarding the de-bate topic.
When a new participant enters the discus-sion, she explicitly picks a position and posts text tosupport it, support a post written by another partici-pant who took the same position, or to dispute a postwritten by another participant who took an opposingposition.
We collected the discussion thread and theparticipant positions for each debate.The third dataset (wikipedia, henceforth) comesfrom the Wikipedia4 discussion section.
When atopic on Wikipedia is disputed, the editors of thattopic start a discussion about it.
We collected 117Wikipeida discussion threads.
The threads containsa total of 1,867 posts.The politicalforum and createdebate datasets areself labeled as described above.
To annotate theWikipedia data, we asked an expert annotator (aprofessor in sociolinguistics who is not one of theauthors) to read each of the Wikipedia discussionthreads and determine whether the discussants splitinto subgroups in which case he was asked to deter-mine the subgroup membership of each discussant.Table 2 lists few example threads from our threedatasets.
Table 1 shows a portion of discussionthread between three participants about enforcing anew immigration law in Arizona.
This thread ap-peared in the polictalforum dataset.
The text postedby the three participants indicates that A?s position4http://www.wikipedia.comis with enforcing the law, that B agrees with A, andthat C disagrees with both.
This means that A and Bbelong to the same opinion subgroup, while belongsto an opposing subgroup.We randomly selected 6 threads from our datasets(2 from politicalforum, 2 from createdebate, and 2from Wikipedia) and used them as development set.This set was used to develop our approach.3 ApproachIn this section, we describe a system that takes adiscussion thread as input and outputs the subgroupmembership of each discussant.
Figure 1 illustratesthe processing steps performed by our system to de-tect subgroups.
In the following subsections we de-scribe the different stages in the system pipeline.3.1 Thread ParsingWe start by parsing the thread to identify posts, par-ticipants, and the reply structure of the thread (i.e.who replies to whom).
In the datasets described inSection 2.4, all this information was explicitly avail-able in the thread.
We tokenize the text of each postand split it into sentences using CLAIRLib (Abu-Jbara and Radev, 2011).3.2 Opinion Word IdentificationThe next step is to identify the words that expressopinion and determine their polarity (positive ornegative).
Lehrer (1974) defines word polarity asthe direction the word deviates to from the norm.
We402use OpinionFinder (Wilson et al, 2005a) to identifypolarized words and their polarities.The polarity of a word is usally affected bythe context in which it appears.
For example, theword fine is positive when used as an adjective andnegative when used as a noun.
For another example,a positive word that appears in a negated contextbecomes negative.
OpinionFinder uses a large set offeatures to identify the contextual polarity of a givenpolarized word given its isolated polarity and thesentence in which it appears (Wilson et al, 2005b).Snippet (3) below shows the result of applying thisstep to snippet (1) above (O means neutral; POSmeans positive; NEG means negative).
(3) Arizona/O Immigration/O law/O good/POS ./OIllegal/O immigration/O bad/NEG ./O3.3 Target IdentificationThe goal of this step is to identify the possible tar-gets of opinion.
A target could be another discus-sant or an entity mentioned in the discussion.
Whenthe target of opinion is another discussant, either thediscussant name is mentioned explicitly or a secondperson pronoun is used to indicate that the opinionis targeting the recipient of the post.
For example,in snippet (2) above the second person pronoun youindicates that the opinion word disagree is targetingDiscussant 1, the recipient of the post.The target of opinion can also be an entitymentioned in the discussion.
We use two methods toidentify such entities.
The first method uses shallowparsing to identify noun groups (NG).
We use theEdinburgh Language Technology Text TokenizationToolkit (LT-TTT) (Grover et al, 2000) for this pur-pose.
We consider as an entity any noun group thatis mentioned by at least two different discussants.We replace each identified entity with a uniqueplaceholder (ENTITYID).
For example, the noungroup Arizona immigration law is mentioned byDiscussant 1 and Discussant 2 in snippets 1 and 2above respectively.
Therefore, we replace it with aplacehold as illustrated in snippets (4) and (5) below.
(4) Discussant 1: ENTITY1 is good.
Illegal im-NER NP ChunkingBarack Obama the Republican nomineeMiddle East the maverick economistsBush conservative ideologuesBob McDonell the Nobel PrizeIraq Federal GovernmentTable 3: Some of the entities identified using NER andNP Chunking in a discussion thread about the US 2012electionsmigration is bad.
(5) Discussant 2: I totally disagree with you.
ENTITY1is blatant racism, and quite unconstitutional.We only consider as entities noun groups thatcontain two words or more.
We impose this require-ment because individual nouns are very commonand regarding all of them as entities will introducesignificant noise.In addition to this shallow parsing method, wealso use named entity recognition (NER) to identifymore entities.
We use the Stanford Named EntityRecognizer (Finkel et al, 2005) for this purpose.
Itrecognizes three types of entities: person, location,and organization.
We impose no restrictions on theentities identified using this method.
Again, we re-place each distinct entity with a unique placeholder.The final set of entities identified in a thread is theunion of the entities identified by the two aforemen-tioned methods.
Table 3Finally, a challenge that always arises whenperforming text mining tasks at this level of gran-ularity is that entities are usually expressed byanaphorical pronouns.
Previous work has shownthat For example, the following snippet containsan explicit mention of the entity Obama in the firstsentence, and then uses a pronoun to refer to thesame entity in the second sentence.
The opinionword unbeatable appears in the second sentenceand is syntactically related to the pronoun He.In the next subsection, it will become clear whyknowing which entity does the pronoun He refers tois essential for opinion-target pairing.
(6) It doesn?t matter whether you vote for Obama.403DiscussionThread?.??.?.??.?.?
?.Opinion Identification?
Identify polarized words?
Identify the contextualpolarity of each wordTarget Identification?
Anaphora resolution?
Identify named entities?
Identify Frequent nounphrases.?
Identify mentions ofother discussantsOpinion-Target Pairing?
Dependency RulesDiscussant AttitudeProfiles (DAPs)ClusteringSubgroupsThread Parsing?
Identify posts?
Identify discussants?
Identify the replystructure?
Tokenize text.?
Split posts into sentencesFigure 1: An overview of the subgroups detection systemHe is unbeatable.Jakob and Gurevych (2010) showed experi-mentally that resolving the anaphoric links in thetext significantly improves opinion target extraction.We use the Beautiful Anaphora Resolution Toolkit(BART) (Versley et al, 2008) to resolve all theanaphoric links within the text of each post sepa-rately.
The result of applying this step to snippet (6)is:(6) It doesn?t matter whether you vote for Obama.Obama is unbeatable.Now, both mentions of Obama will be recog-nized by the Stanford NER system and will beidentified as one entity.3.4 Opinion-Target PairingAt this point, we have all the opinion words andthe potential targets identified separately.
The nextstep is to determine which opinion word is target-ing which target.
We propose a rule based approachfor opinion-target pairing.
Our rules are based onthe dependency relations that connect the words ina sentence.
We use the Stanford Parser (Klein andManning, 2003) to generate the dependency parsetree of each sentence in the thread.
An opinion wordand a target form a pair if they stratify at least oneof our dependency rules.
Table 4 illustrates someof these rules 5.
The rules basically examine thetypes of the dependencies on the shortest path thatconnect the opinion word and the target in the de-pendency parse tree.
It has been shown in previouswork on relation extraction that the shortest depen-dency path between any two entities captures the in-formation required to assert a relationship betweenthem (Bunescu and Mooney, 2005).If a sentence S in a post written by participantPi contains an opinion word OPj and a target TRk,and if the opinion-target pair satisfies one of our de-pendency rules, we say that Pi expresses an attitudetowards TRk.
The polarity of the attitude is deter-mined by the polarity of OPj .
We represent this asPi+?
TRk if OPj is positive and Pi??
TRk if OPjis negative.It is likely that the same participant Pi expresssentiment toward the same target TRk multipletimes in different sentences in different posts.
Wekeep track of the counts of all the instances of posi-tive/negative attitude Pi expresses toward TRk.
Werepresent this as Pim+??
?n?TRk where m (n) is thenumber of times Pi expressed positive (negative) at-titude toward TRk.3.5 Discussant Attitude ProfileWe propose a representation of discussantsa?ttitudestowards the identified targets in the discussionthread.
As stated above, a target could be anotherdiscussant or an entity mentioned in the discussion.5The code will be made publicly available at the time ofpublication404ID Rule In Words ExampleR1 OP ?
nsubj ?
TR The target TR is the nominal subject of the opinionword OPENTITY1TR is goodOP .R2 OP ?
dobj ?
TR The target T is a direct object of the opinion OP I hateOP ENTITY2TRR3 OP ?
prep ?
?
TR The target TR is the object of a preposition thatmodifies the opinion word OPI totally disagreeOP with youTR.R4 TR?
amod?
OP The opinion is an adjectival modifier of the target The badOP ENTITY3TR is spreading liesR5 OP ?
nsubjpass?
TR The target TR is the nominal subject of the passiveopinion word OPENTITY4TR is hatedOP by everybody.R6 OP ?
prep ?
?
poss?
TR The opinion word OP connected through a prep ?relation as in R2 to something possessed by thetarget TRThe main flawOP in yourTR analysis isthat it?s based on wrong assumptions.R7 OP ?
dobj ?
poss?
TR The target TR possesses something that is the directobject of the opinion word OPI likeOP ENTITY5TR?s brilliant ideas.R8 OP ?
csubj ?
nsubj ?
TR The opinon word OP is a causal subject of a phrasethat has the target TR as its nominal subjectWhat ENTITY6TR announced wasmisleadingOP .Table 4: Examples of the dependency rules used for opinion-target pairing.Our representation is a vector containing numeri-cal values.
The values correspond to the counts ofpositive/negative attitudes expressed by the discus-sant toward each of the targets.
We call this vectorthe discussant attitude profile (DAP).
We construct aDAP for every discussant.
Given a discussion threadwith d discussants and e entity targets, each attitudeprofile vector has n = (d + e) ?
3 dimensions.
Inother words, each target (discussant or entity) hasthree corresponding values in the DAP: 1) the num-ber of times the discussant expressed positive atti-tude toward the target, 2) the number of times thediscussant expressed a negative attitude towards thetarget, and 3) the number of times the the discussantinteracted with or mentioned the target.
It has to benoted that these values are not symmetric since thediscussions explicitly denote the source and the tar-get of each post.3.6 ClusteringAt this point, we have an attitude profile (or vec-tor) constructed for each discussant.
Our goal is touse these attitude profiles to determine the subgroupmembership of each discussant.
We can achieve thisgoal by noticing that the attitude profiles of discus-sants who share the same opinion are more likely tobe similar to each other than to the attitude profilesof discussants with opposing opinions.
This sug-gests that clustering the attitude vector space willachieve the goal and split the discussants into sub-groups according to their opinion.4 EvaluationIn this section, we present several levels of evalu-ation of our system.
First, we compare our sys-tem to baseline systems.
Second, we study how thechoice of the clustering algorithm impacts the re-sults.
Third, we study the impact of each componentin our system on the performance.
All the resultsreported in this section that show difference in theperformance are statistically significant at the 0.05level (as indicated by a 2-tailed paired t-test).
Be-fore describing the experiments and presenting theresults, we first describe the evaluation metrics weuse.4.0.1 Evaluation MetricsWe use two evaluation metrics to evaluate sub-groups detection accuracy: Purity and Entropy.
Tocompute Purity (Manning et al, 2008), each clus-ter is assigned the class of the majority vote withinthe cluster, and then the accuracy of this assignmentis measured by dividing the number of correctly as-signed members by the total number of instances.
Itcan be formally defined as:purity(?, C) =1N?kmaxj|?k ?
cj | (1)where ?
= {?1, ?2, ..., ?k} is the set of clustersand C = {c1, c2, ..., cJ} is the set of classes.
?k isinterpreted as the set of documents in ?k and cj as405the set of documents in cj .
The purity increases asthe quality of clustering improves.The second metric is Entropy.
The Entropy of acluster reflects how the members of the k distinctsubgroups are distributed within each resulting clus-ter; the global quality measure is computed by aver-aging the entropy of all clusters:Entropy = ?j?
njni?P (i, j)?
log2P (i, j)(2)where P (i, j) is the probability of finding an ele-ment from the category i in the cluster j, nj is thenumber of items in cluster j, and n the total num-ber of items in the distribution.
In contrast to purity,the entropy decreases as the quality of clustering im-proves.4.1 Comparison to Baseline SystemsWe compare our system (DAPC) that was describedin Section 3 to two baseline methods.
The first base-line (GC) uses graph clustering to partition a net-work based on the interaction frequency betweenparticipants.
We build a graph where each noderepresents a participant.
Edges link participants ifthey exchange posts, and edge weights are based onthe number of interactions.
We tried two methodsfor clustering the resulting graph: spectral partition-ing (Luxburg, 2007) and a hierarchical agglomera-tion algorithm which works by greedily optimizingthe modularity for graphs (Clauset et al, 2004).The second baseline (TC) is based on the premisethat the member of the same subgroup are morelikely to use vocabulary drawn from the same lan-guage model.
We collect all the text posted by eachparticipant and create a tf-idf representations of thetext in a high dimensional vector space.
We thencluster the vector space to identify subgroups.
Weuse k-means (MacQueen, 1967) as our clusteringalgorithm in this experiment (comparison of vari-ous clustering algorithms is presented in the nextsubsection).
The distances between vectors areEculidean distances.
Table 5 shows that our sys-tem performs significantly better the baselines on thethree datasets in terms of both the purity (P ) and theentropy (E) (notice that lower entropy values indi-cate better clustering).
The values reported are theMethod Createdebate Politicalforum WikipediaP E P E P EGC - Spectral 0.50 0.85 0.50 0.88 0.49 0.89GC - Hierarchical 0.48 0.86 0.47 0.89 0.49 0.87TC - kmeans 0.51 0.84 0.49 0.88 0.52 0.85DAPC - kmeans 0.64 0.68 0.61 0.80 0.66 0.55Table 5: Comparison to baseline systemsMethod Createdebate Politicalforum WikipediaP E P E P EDAPC - EM 0.63 0.71 0.61 0.82 0.63 0.61DAPC - FF 0.63 0.70 0.60 0.83 0.64 0.59DAPC - kmeans 0.64 0.68 0.61 0.80 0.66 0.55Table 6: Comparison of different clustering algorithmsaverage results of the threads of each dataset.
Webelieve that the baselines performed poorly becausethe interaction frequency and the text similarity arenot key factors in identifying subgroup structures.Many people would respond to people they disagreewith more, while others would mainly respond topeople they agree with most of the time.
Also, peo-ple in opposing subgroups tend to use very similartext when discussing the same topic and hence textclustering does not work as well.4.2 Choice of the clustering algorithmWe experimented with three different clustering al-gorithms: expectation maximization (EM), and k-means (MacQueen, 1967), and FarthestFirst (FF)(Hochbaum and Shmoys, 1985; Dasgupta, 2002).As we did in the previous subsection, we useEculidean distance to measure the distance betweenvectors All the system (DAP) components are in-cluded as described in Section 3.
The purity andentropy values using each algorithm are shown inTable 6.
Although k-means seems to be performingslightly better than other algorithms, the differencesin the results are not significant.
This indicates thatthe choice of the clustering algorithm does not havea noticeable impact on the results.
We also exper-imented with using Manhattan distance and cosinesimilarity instead of Euclidean distance to measurethe distance between attitude vectors.
We noticedthat the choice of the distance does not have signifi-cant impact on the results as well.4064.3 Component EvaluationIn this subsection, we evaluate the impact of the dif-ferent components in the pipeline on the system per-formance.
We do that by removing each componentfrom the pipeline and measuring the change in per-formance.
We perform the following experiments:1) We run the full system with all its componentsincluded (DAPC).
2) We run the system and in-clude only discussant-to-discussant attitude featuresin the attitude vectors (DAPC-DD).
3) We includeonly discussant-to-entity attitude features in the atti-tude vectors (DAPC-DE).
4) We include only senti-ment features in the attitude vector; i.e.
we excludethe interaction count features (DAPC-SE).
5) We in-clude only interaction count features to the attitudevector; i.e.
we exclude sentiment features (DAPC-INT).
6) We skip the anaphora resolution step in theentity identification component (DAPC-NO AR).
7)We only use named entity recognition to identify en-tity targets; i.e.
we exclude the entities identifiedthrough noun phrasing chunking (DAPC-NER).
8)Finally, we only noun phrase chunking to identifyentity targets (DAPC-NP).
In all these experimentsk-means is used for clustering and the number ofclusters is set as explained in the previous subsec-tion.The results show that all the components in thesystem contribute to better performance of the sys-tem.
We notice from the results that the performanceof the system drops significantly if sentiment fea-tures are not included.
This is result corroboratesour hypothesis that interaction features are not suffi-cient factors for detecting rift in discussion groups.Including interaction features improve the perfor-mance (although not by a big difference) becausethey help differentiate between the case where par-ticipants A and B never interacted with each otherand the case where they interact several time butnever posted text that indicate difference in opin-ion between them.
We also notice that the perfor-mance drops significantly in DAPC-DD and DAPC-DD which also supports our hypotheses that boththe sentiment discussants show toward one anotherand the sentiment they show toward the aspects ofthe discussed topic are important for the task.
Al-though using both named entity recognition (NER)and noun phrase chunking achieves better results, itMethod Createdebate Politicalforum WikipediaP E P E P EDAPC 0.64 0.68 0.61 0.80 0.66 0.55DAPC-DD 0.59 0.77 0.57 0.86 0.62 0.61DAPC-DE 0.60 0.69 0.58 0.84 0.58 0.78DAPC-SE 0.62 0.70 0.60 0.83 0.61 0.62DAPC-INT 0.54 0.88 0.52 0.91 0.57 0.85DAPC-NO AR 0.62 0.72 0.60 0.84 0.64 0.60DAPC-NER 0.61 0.71 0.58 0.86 0.63 0.59DAPC-NP 0.63 0.75 0.59 0.84 0.65 0.62Table 7: Impact of system components on the perfor-mancecan also be noted from the results that NER con-tributes more to the system performance.
Finally,the results support Jakob and Gurevych (2010) find-ings that anaphora resolution aids opinion miningsystems.5 ConclusionsIn this paper, we presented an approach for subgroupdetection in ideological discussions.
Our systemuses linguistic analysis techniques to identify the at-titude the participants of online discussions carry to-ward each other and toward the aspects of the discus-sion topic.
Attitude prediction as well as interactionfrequency to construct an attitude vector for eachparticipant.
The attitude vectors of discussants arethen clustered to form subgroups.
Our experimentsshowed that our system outperforms text clusteringand interaction graph clustering.
We also studied thecontribution of each component in our system to theoverall performance.AcknowledgmentsThis research was funded by the Office of the Di-rector of National Intelligence (ODNI), IntelligenceAdvanced Research Projects Activity (IARPA),through the U.S. Army Research Lab.
All state-ments of fact, opinion or conclusions containedherein are those of the authors and should not beconstrued as representing the official views or poli-cies of IARPA, the ODNI or the U.S. Government.407ReferencesAmjad Abu-Jbara and Dragomir Radev.
2011.
Clairlib:A toolkit for natural language processing, informationretrieval, and network analysis.
In Proceedings of theACL-HLT 2011 System Demonstrations, pages 121?126, Portland, Oregon, June.
Association for Compu-tational Linguistics.Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.Fox Tree, Robeson Bowmani, and Michael Minor.2011.
Cats rule and dogs drool!
: Classifying stancein online debate.
In Proceedings of the 2nd Workshopon Computational Approaches to Subjectivity and Sen-timent Analysis (WASSA 2.011), pages 1?9, Portland,Oregon, June.
Association for Computational Linguis-tics.Alina Andreevskaia and Sabine Bergler.
2006.
Miningwordnet for fuzzy sentiment: Sentiment tag extractionfrom wordnet glosses.
In EACL?06.Carmen Banea, Rada Mihalcea, and Janyce Wiebe.2008.
A bootstrapping method for building subjec-tivity lexicons for languages with scarce resources.
InLREC?08.Mohit Bansal, Claire Cardie, and Lillian Lee.
2008.
Thepower of negative thinking: Exploiting label disagree-ment in the min-cut classification framework.Razvan Bunescu and Raymond Mooney.
2005.
A short-est path dependency kernel for relation extraction.
InProceedings of Human Language Technology Confer-ence and Conference on Empirical Methods in Nat-ural Language Processing, pages 724?731, Vancou-ver, British Columbia, Canada, October.
Associationfor Computational Linguistics.Aaron Clauset, Mark E. J. Newman, and CristopherMoore.
2004.
Finding community structure in verylarge networks.
Phys.
Rev.
E, 70:066111.Sanjoy Dasgupta.
2002.
Performance guarantees forhierarchical clustering.
In 15th Annual Conferenceon Computational Learning Theory, pages 351?363.Springer.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 363?370, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Claire Grover, Colin Matheson, Andrei Mikheev, andMarc Moens.
2000.
Lt ttt - a flexible tokenisationtool.
In In Proceedings of Second International Con-ference on Language Resources and Evaluation, pages1147?1154.Ahmed Hassan and Dragomir Radev.
2010.
Identifyingtext polarity using random walks.
In ACL?10.Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.2010.
What?s with the attitude?
: identifying sentenceswith attitude in online discussions.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1245?1255.Ahmed Hassan, Amjad AbuJbara, Rahul Jha, andDragomir Radev.
2011.
Identifying the semanticorientation of foreign words.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 592?597, Portland, Oregon, USA, June.
Associ-ation for Computational Linguistics.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In EACL?97, pages 174?181.Vasileios Hatzivassiloglou and Janyce Wiebe.
2000.
Ef-fects of adjective orientation and gradability on sen-tence subjectivity.
In COLING, pages 299?305.Hochbaum and Shmoys.
1985.
A best possible heuristicfor the k-center problem.
Mathematics of OperationsResearch, 10(2):180?184.Minqing Hu and Bing Liu.
2004a.
Mining and summa-rizing customer reviews.
In KDD?04, pages 168?177.Minqing Hu and Bing Liu.
2004b.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, KDD ?04, pages 168?177, New York, NY, USA.
ACM.Niklas Jakob and Iryna Gurevych.
2010.
Using anaphoraresolution to improve opinion target identification inmovie reviews.
In Proceedings of the ACL 2010 Con-ference Short Papers, pages 263?268, Uppsala, Swe-den, July.
Association for Computational Linguistics.Jaap Kamps, Maarten Marx, Robert J. Mokken, andMaarten De Rijke.
2004.
Using wordnet to measuresemantic orientations of adjectives.
In National Insti-tute for, pages 1115?1118.Soo-Min Kim and Eduard Hovy.
2004.
Determining thesentiment of opinions.
In COLING, pages 1367?1373.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In IN PROCEEDINGS OFTHE 41ST ANNUAL MEETING OF THE ASSOCIA-TION FOR COMPUTATIONAL LINGUISTICS, pages423?430.Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.2007.
Extracting aspect-evaluation and aspect-of re-lations in opinion mining.
In Proceedings of the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL.Adrienne Lehrer.
1974.
Semantic fields and lezical struc-ture.
North Holland, Amsterdam and New York.408Bing Liu.
2009.
Web Data Mining: Exploring Hyper-links, Contents, and Usage Data (Data-Centric Sys-tems and Applications).
Springer, 1st ed.
2007. corr.2nd printing edition, January.Ulrike Luxburg.
2007.
A tutorial on spectral clustering.Statistics and Computing, 17:395?416, December.J.
B. MacQueen.
1967.
Some methods for classificationand analysis of multivariate observations.
In L. M. LeCam and J. Neyman, editors, Proc.
of the fifth BerkeleySymposium on Mathematical Statistics and Probabil-ity, volume 1, pages 281?297.
University of CaliforniaPress.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schtze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, New York, NY,USA.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, andChengXiang Zhai.
2007.
Topic sentiment mixture:modeling facets and opinions in weblogs.
In Pro-ceedings of the 16th international conference on WorldWide Web, WWW ?07, pages 171?180, New York, NY,USA.
ACM.Soo min Kim and Eduard Hovy.
2007.
Crystal: Ana-lyzing predictive opinions on the web.
In In EMNLP-CoNLL 2007.Tetsuya Nasukawa and Jeonghee Yi.
2003.
Sentimentanalysis: capturing favorability using natural languageprocessing.
In K-CAP ?03: Proceedings of the 2ndinternational conference on Knowledge capture, pages70?77.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.Ana-Maria Popescu and Oren Etzioni.
2005.
Extractingproduct features and opinions from reviews.
In HLT-EMNLP?05, pages 339?346.Ellen Riloff and Janyce Wiebe.
2003.
Learningextraction patterns for subjective expressions.
InEMNLP?03, pages 105?112.Swapna Somasundaran and Janyce Wiebe.
2009.
Rec-ognizing stances in online debates.
In Proceedingsof the Joint Conference of the 47th Annual Meetingof the ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP, pages226?234, Suntec, Singapore, August.
Association forComputational Linguistics.Veselin Stoyanov and Claire Cardie.
2008.
Topic iden-tification for fine-grained opinion analysis.
In In Col-ing.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting semantic orientations of words usingspin model.
In ACL?05, pages 133?140.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition from con-gressional floor-debate transcripts.
In In Proceedingsof EMNLP, pages 327?335.Peter Turney and Michael Littman.
2003.
Measuringpraise and criticism: Inference of semantic orientationfrom association.
ACM Transactions on InformationSystems, 21:315?346.Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-sio, Vladimir Eidelman, Alan Jern, Jason Smith, Xi-aofeng Yang, and Alessandro Moschitti.
2008.
Bart:A modular toolkit for coreference resolution.
In Pro-ceedings of the ACL-08: HLT Demo Session, pages9?12, Columbus, Ohio, June.
Association for Compu-tational Linguistics.Janyce Wiebe.
2000.
Learning subjective adjectivesfrom corpora.
In Proceedings of the SeventeenthNational Conference on Artificial Intelligence andTwelfth Conference on Innovative Applications of Ar-tificial Intelligence, pages 735?740.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi,Claire Cardie, Ellen Riloff, and Siddharth Patward-han.
2005a.
Opinionfinder: a system for subjectiv-ity analysis.
In Proceedings of HLT/EMNLP on Inter-active Demonstrations, HLT-Demo ?05, pages 34?35,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005b.
Recognizing contextual polarity in phrase-level sentiment analysis.
In HLT/EMNLP?05, Vancou-ver, Canada.Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010.Multi-level structured models for document-level sen-timent classification.
In In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: separating facts fromopinions and identifying the polarity of opinion sen-tences.
In EMNLP?03, pages 129?136.409
