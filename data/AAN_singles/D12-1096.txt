Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1048?1059, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsParser Showdown at the Wall Street Corral:An Empirical Investigation of Error Types in Parser OutputJonathan K. Kummerfeld?
David Hall?
James R. Curran?
Dan Klein?
?Computer Science Division ?
e-lab, School of ITUniversity of California, Berkeley University of SydneyBerkeley, CA 94720, USA Sydney, NSW 2006, Australia{jkk,dlwh,klein}@cs.berkeley.edu james@it.usyd.edu.auAbstractConstituency parser performance is primarilyinterpreted through a single metric, F-scoreon WSJ section 23, that conveys no linguis-tic information regarding the remaining errors.We classify errors within a set of linguisti-cally meaningful types using tree transforma-tions that repair groups of errors together.
Weuse this analysis to answer a range of ques-tions about parser behaviour, including whatlinguistic constructions are difficult for state-of-the-art parsers, what types of errors are be-ing resolved by rerankers, and what types areintroduced when parsing out-of-domain text.1 IntroductionParsing has been a major area of research withincomputational linguistics for decades, and con-stituent parser F-scores on WSJ section 23 have ex-ceeded 90% (Petrov and Klein, 2007), and 92%when using self-training and reranking (McCloskyet al 2006; Charniak and Johnson, 2005).
Whilethese results give a useful measure of overall per-formance, they provide no information about the na-ture, or relative importance, of the remaining errors.Broad investigations of parser errors beyond thePARSEVAL metric (Abney et al 1991) have eitherfocused on specific parsers, e.g.
Collins (2003), orhave involved conversion to dependencies (Carrollet al 1998; King et al 2003).
In all of these cases,the analysis has not taken into consideration how aset of errors can have a common cause, e.g.
a singlemis-attachment can create multiple node errors.We propose a new method of error classifica-tion using tree transformations.
Errors in the parsetree are repaired using subtree movement, node cre-ation, and node deletion.
Each step in the process isthen associated with a linguistically meaningful er-ror type, based on factors such as the node that ismoved, its siblings, and parents.Using our method we analyse the output of thir-teen constituency parsers on newswire.
Some ofthe frequent error types that we identify are widelyrecognised as challenging, such as prepositionalphrase (PP) attachment.
However, other significanttypes have not received as much attention, such asclause attachment and modifier attachment.Our method also enables us to investigate wherereranking and self-training improve parsing.
Pre-viously, these developments were analysed only interms of their impact on F-score.
Similarly, the chal-lenge of out-of-domain parsing has only been ex-pressed in terms of this single objective.
We are ableto decompose the drop in performance and show thata disproportionate number of the extra errors are dueto coordination and clause attachment.This work presents a comprehensive investigationof parser behaviour in terms of linguistically mean-ingful errors.
By applying our method to multipleparsers and domains we are able to answer questionsabout parser behaviour that were previously only ap-proachable through approximate measures, such ascounts of node errors.
We show which errors havebeen reduced over the past fifteen years of parsingresearch; where rerankers are making their gains andwhere they are not exploiting the full potential of k-best lists; and what types of errors arise when mov-ing out-of-domain.
We have released our system1 toenable future work to apply our methodology.1http://code.google.com/p/berkeley-parser-analyser/10482 BackgroundMost attempts to understand the behaviour of con-stituency parsers have focused on overall evaluationmetrics.
The three main methods are intrinsic eval-uation with PARSEVAL, evaluation on dependenciesextracted from the constituency parse, and evalua-tion on downstream tasks that rely on parsing.Intrinsic evaluation with PARSEVAL, which calcu-lates precision and recall over labeled tree nodes, isa useful indicator of overall performance, but doesnot pinpoint which structures the parser has mostdifficulty with.
Even when the breakdown for par-ticular node types is presented (e.g.
Collins, 2003),the interaction between node errors is not taken intoaccount.
For example, a VP node could be missingbecause of incorrect PP attachment, a coordinationerror, or a unary production mistake.
There has beensome work that addresses these issues by analysingthe output of constituency parsers on linguisticallymotivated error types, but only by hand on sets ofaround 100 sentences (Hara et al 2007; Yu et al2011).
By automatically classifying parse errors weare able to consider the output of multiple parsers onthousands of sentences.The second major parser evaluation method in-volves extraction of grammatical relations (King etal., 2003; Briscoe and Carroll, 2006) or dependen-cies (Lin, 1998; Briscoe et al 2002).
These met-rics have been argued to be more informative andgenerally applicable (Carroll et al 1998), and havethe advantage that the breakdown over dependencytypes is more informative than over node types.There have been comparisons of multiple parsers(Foster and van Genabith, 2008; Nivre et al 2010;Cer et al 2010), as well as work on finding rela-tions between errors (Hara et al 2009), and break-ing down errors by a range of factors (McDonald andNivre, 2007).
However, one challenge is that resultsfor constituency parsers are strongly influenced bythe dependency scheme being used and how easy itis to extract the dependencies from a given parser?soutput (Clark and Hockenmaier, 2002).
Our ap-proach does not have this disadvantage, as we anal-yse parser output directly.The third major approach involves extrinsic eval-uation, where the parser?s output is used in a down-stream task, such as machine translation (Quirkand Corston-Oliver, 2006), information extraction(Miyao et al 2008), textual entailment (Yuret etal., 2010), or semantic dependencies (Dridan andOepen, 2011).
While some of these approaches givea better sense of the impact of parse errors, they re-quire integration into a larger system, making it lessclear where a given error originates.The work we present here differs from existingapproaches by directly and automatically classifyingerrors into meaningful types.
This enables the firstvery broad, yet detailed, study of parser behaviour,evaluating the output of thirteen parsers over thou-sands of sentences.3 ParsersOur evaluation is over a wide range of PTB con-stituency parsers and their variants from the past fif-teen years.
For all parsers we used the publicly avail-able version, with the standard parameter settings.Berkeley (Petrov et al 2006; Petrov and Klein,2007).
An unlexicalised parser with a grammarconstructed with automatic state splitting.Bikel (2004) implementation of Collins (1997).BUBS (Dunlop et al 2011; Bodenstab et al2011).
A ?grammar-agnostic constituentparser,?
which uses a Berkeley Parser grammar,but parses with various pruning techniques toimprove speed, at the cost of accuracy.Charniak (2000).
A generative parser with a max-imum entropy-inspired model.
We also use thereranker (Charniak and Johnson, 2005), and theself-trained model (McClosky et al 2006).Collins (1997).
A generative lexicalised parser,with three models, a base model, a model thatuses subcategorisation frames for head words,and a model that takes into account traces.SSN (Henderson, 2003; Henderson, 2004).
A sta-tistical left-corner parser, with probabilities es-timated by a neural network.Stanford (Klein and Manning, 2003a; Klein andManning, 2003b).
We consider both the un-lexicalised PCFG parser (-U) and the factoredparser (-F), which combines the PCFG parserwith a lexicalised dependency parser.1049System F P R Exact SpeedENHANCED TRAINING / SYSTEMSCharniak-SR 92.07 92.44 91.70 44.87 1.8Charniak-R 91.41 91.78 91.04 44.04 1.8Charniak-S 91.02 91.16 90.89 40.77 1.8STANDARD PARSERSBerkeley 90.06 90.30 89.81 36.59 4.2Charniak 89.71 89.88 89.55 37.25 1.8SSN 89.42 89.96 88.89 32.74 1.8BUBS 88.50 88.57 88.43 31.62 27.6Bikel 88.16 88.23 88.10 32.33 0.8Collins-3 87.66 87.82 87.50 32.22 2.0Collins-2 87.62 87.77 87.48 32.51 2.2Collins-1 87.09 87.29 86.90 30.35 3.3Stanford-L 86.42 86.35 86.49 27.65 0.7Stanford-U 85.78 86.48 85.09 28.35 2.7Table 1: PARSEVAL results on WSJ section 23 for theparsers we consider.
The columns are F-score, precision,recall, exact sentence match, and speed (sents/sec).
Cov-erage was left out as it was above 99.8% for all parsers.In the ENHANCED TRAINING / SYSTEMS section we in-clude the Charniak parser with reranking (R), with a self-trained model (S), and both (SR).Table 1 shows the standard performance metrics,measured on section 23 of the WSJ, using all sen-tences.
Speeds were measured using a Quad-CoreXeon CPU (2.33GHz 4MB L2 cache) with 16GBof RAM.
These results clearly show the variation inparsing performance, but they do not show whichconstructions are the source of those variations.4 Error ClassificationWhile the statistics in Table 1 give a sense of over-all parser performance they do not provide linguisti-cally meaningful intuition for the source of remain-ing errors.
Breaking down the remaining errors bynode type is not particularly informative, as a sin-gle attachment error can cause multiple node errors,many of which are for unrelated node types.
Forexample, in Figure 1 there is a PP attachment errorthat causes seven bracket errors (extra S, NP, PP, andNP, missing S, NP, and PP).
Determining that thesecorrespond to a PP attachment error from just the la-bels of the missing and extra nodes is difficult.
Incontrast, the approach we describe below takes intoconsideration the relations between errors, groupingthem into linguistically meaningful sets.We classify node errors in two phases.
First, weSVPVPSNPPPNPPPin 1986NPNNPAppliedINofNPchief executive officerVBNnamedVBDwasNPPRPHe(a) Parser outputSVPVPPPin 1986SNPPPNPNNPAppliedINofNPchief executive officerVBNnamedVBDwasNPPRPHe(b) Gold treeFigure 1: Grouping errors by node type is of limited use-fulness.
In this figure and those that follow the top treeis the incorrect parse and the bottom tree is the correctparse.
Bold, boxed nodes are either extra (marked in theincorrect tree) or missing (marked in the correct tree).This is an example of PP Attachment (in 1986 is toolow), but that is not at all clear from the set of incorrectnodes (extra S, NP, PP, and NP, missing S, NP, and PP).find a set of tree transformations that convert the out-put tree into the gold tree.
Second, the transforma-tion are classified into error types such as PP attach-ment and coordination.
Pseudocode for our methodis shown in Algorithm 1.
The tree transformationstage corresponds to the main loop, while the sec-ond stage corresponds to the final loop.4.1 Tree TransformationThe core of our transformation process is a set of op-erations that move subtrees, create nodes, and deletenodes.
Searching for the shortest path to transformone tree into another is prohibitively slow.2 We find2We implemented various search procedures and found sim-ilar results on the sentences that could be processed in a reason-1050Algorithm 1 Tree transformation error classificationU = initial set of node errorsSort U by the depth of the error in the tree, deepest firstG = ?repeatfor all errors e ?
U doif e fits an environment template t theng = new error groupCorrect e as specified by tfor all errors f that t corrects doRemove f from UInsert f into gend forAdd g to Gend ifend foruntil unable to correct any further errorsfor all remaining errors e ?
U doInsert a group into G containing eend forfor all groups g ?
G doClassify g based on properties of the groupend fora path by applying a greedy bottom?up approach,iterating through the errors in order of tree depth.We match each error with a template based onnearby tree structure and errors.
For example, inFigure 1 there are four extra nodes that all coverspans ending at Applied in 1986: S, NP, PP, NP.There are also three missing nodes with spans end-ing between Applied and in: PP, NP, and S. Figure 2depicts these errors as spans, showing that this casefits three criteria: (1) there are a set of extra spans allending at the same point, (2) there are a set of miss-ing spans all ending at the same point, and (3) the ex-tra spans cross the missing spans, extending beyondtheir end-point.
This indicates that the node start-ing after Applied is attaching too low and should bemoved up, outside all of the extra nodes.
Together,the criteria and transformation form a template.Once a suitable template is identified we correctthe error by moving subtrees, adding nodes and re-moving nodes.
In the example this is done by mov-ing the node spanning in 1986 up in the tree until itis outside of all the extra spans.
Since moving the PPleaves a unary production from an NP to an NP, wealso collapse that level.
In total this corrects sevenable amount of time.named chief executive officer of Applied in 1986Figure 2: Templates are defined in terms of extra andmissing spans, shown here with unbroken lines above anddashed lines below, respectively.
This is an example of aset of extra spans that cross a set of missing spans (whichin both cases all end at the same position).
If the last twowords are moved, two of the extra spans will match thetwo missing spans.
The other extra span is deleted duringthe move as it creates an NP?NP unary production.errors, as there are three cases in which an extra nodeis present that matches a missing node once the PPis moved.
All of these errors are placed in a singlegroup and information about the nearby tree struc-ture before and after the transformation is recorded.We continue to make passes through the list untilno errors are corrected on a pass.
For each remainingnode error an individual error group is created.The templates were constructed by hand based onmanual analysis of parser output.
They cover a rangeof combinations of extra and missing spans, withfurther variation for whether crossing is occurringand if so whether the crossing bracket starts or endsin the middle of the correct bracket.
Errors that donot match any of our templates are left uncorrected.4.2 Transformation ClassificationWe began with a large set of node errors, in the firststage they were placed into groups, one group pertree transformation used to get from the test tree tothe gold tree.
Next we classify each group as one ofthe error types below.PP Attachment Any case in which the transforma-tion involved moving a Prepositional Phrase, orthe incorrect bracket is over a PP, e.g.He was (VP named chief executive officer offill(NP Applied (PP in 1986)))where (PP in 1986) should modify the entireVP, rather than just Applied.NP Attachment Several cases in which NPs had tobe moved, particularly for mistakes in appos-itive constructions and incorrect attachmentswithin a verb phrase, e.g.The bonds (VP go (PP on sale (NP Oct. 19)))where Oct. 19 should be an argument of go.1051VPNPNNtodayNPVBGappearingNPNNadJJnewDTanotherVBDwrote(a) Parser outputVPNPVPNPNNtodayVBGappearingNPNNadJJnewDTanotherVBDwrote(b) Gold treeFigure 3: NP Attachment: today is too high, it shouldbe the argument of appearing, rather than wrote.
Thiscauses three node errors (extra NP, missing NP and VP).VPADVPahead of timeSVPVPPPabout itVBthinkTOtoVBDhad(a) Parser outputVPSVPVPADVPahead of timePPabout itVBthinkTOtoVBDhad(b) Gold treeFigure 4: Modifier Attachment: ahead of time is toohigh, it should modify think, not had.
This causes sixnode errors (extra S, VP, and VP, missing S, VP, and VP).Modifier Attachment Cases involving incorrectlyplaced adjectives and adverbs, including errorscorrected by subtree movement and errors re-quiring only creation of a node, e.g.
(NP (ADVP even more) severe setbacks)where there should be an extra ADVP nodeover even more severe.Clause Attachment Any group that involves move-ment of some form of S node.VPSVPVPSBARunless the agency .
.
.NPthe RTC to .
.
.VBrestrictTOtoVBZintends(a) Parser outputVPSBARunless the agency .
.
.SVPVPNPthe RTC to .
.
.VBrestrictTOtoVBZintends(b) Gold treeFigure 5: Clause Attachment: unless the agency re-ceives specific congressional authorization is attachingtoo low.
This causes six node errors (extra S, VP, andVP, missing S, VP and VP).SINVNPPPof major market activityNPa breakdownVBZisVPVBGFollowing(a) Parser outputSINVNPNPPPof major market activityNPa breakdownVBZisSVPVBGFollowing(b) Gold treeSINV::NP-SBJ-1NPPPof major market activityNPa breakdownVBZisS-ADVVPVBGFollowingNP-SBJ-NONE-*-1(c) Gold tree with traces and function tagsFigure 6: Two Unary errors, a missing S and a missingNP.
The third tree is the PTB tree before traces and func-tion tags are removed.
Note that the missing NP is overanother NP, a production that does occur widely in thetreebank, particularly over the word it.1052NPPPNPNPDresdner AG?s 10% declineCCandNPMannesmann AGINforNPA 16% drop(a) Parser outputNPNPDresdner AG?s 10% declineCCandNPPPNPMannesmann AGINforNPA 16% drop(b) Gold treeFigure 7: Coordination: and Dresdner AG?s 10% de-cline is too low.
This causes four node errors (extra PPand NP, missing NP and PP).Unary Mistakes involving unary productions thatare not linked to a nearby error such as a match-ing extra or missing node.
We do not include abreakdown by unary type, though we did findthat clause labeling (S, SINV, etc) accountedfor a large proportion of the errors.Coordination Cases in which a conjunction is animmediate sibling of the nodes being moved, oris the leftmost or rightmost node being moved.NP Internal Structure While most NP structure isnot annotated in the PTB, there is some use ofADJP, NX, NAC and QP nodes.
We form asingle group for each NP that has one or moreerrors involving these types of nodes.Different label In many cases a node is present inthe tree that spans the correct set of words, buthas the wrong label, in which case we group thetwo node errors, (one extra, one missing), as asingle error.Single word phrase A range of node errors thatspan a single word, with checks to ensure thisis not linked to another error (e.g.
one part of aset of internal noun phrase errors).Other There is a long tail of other errors.
Somecould be placed within the categories above,but would require far more specific rules.For many of these error types it would be diffi-cult to extract a meaningful understanding from onlyNPPPNPNNPBakerNNPStateINofNNPSecretary(a) Parser outputNPNNPBakerPPNPNNPStateINofNNPSecretary(b) Gold treeFigure 8: NP Internal Structure: Baker is too low, caus-ing four errors (extra PP and NP, missing PP and NP).the list of node errors involved.
Even for error typesthat can be measured by counting node errors or ruleproduction errors, our approach has the advantagethat we identify groups of errors with a single cause.For example, a missing unary production may corre-spond to an extra bracket that contains a subtree thatattached incorrectly.4.3 MethodologyWe used sections 00 and 24 as development datawhile constructing the tree transformation and errorgroup classification methods.
All of our examplesin text come from these sections as well, but for alltables of results we ran our system on section 23.We chose to run our analysis on section 23 as it isthe only section we are sure was not used in the de-velopment of any of the parsers, either for tuning orfeature development.
Our evaluation is entirely fo-cused on the errors of the parsers, so unless there isa particular construction that is unusually prevalentin section 23, we are not revealing any informationabout the test set that could bias future work.5 ResultsOur system enables us to answer questions aboutparser behaviour that could previously only beprobed indirectly.
We demonstrate its usefulness byapplying it to a range of parsers (here), to rerankedK-best lists of various lengths, and to output for out-of-domain parsing (following sections).In Table 2 we consider the breakdown of parser1053PP Clause Diff Mod NP 1-Word NPParser F-score Attach Attach Label Attach Attach Co-ord Span Unary aInt.a OtherBest 0.60 0.38 0.31 0.25 0.25 0.23 0.20 0.14 0.14 0.50Charniak-RS 92.07Charniak-R 91.41Charniak-S 91.02Berkeley 90.06Charniak 89.71SSN 89.42BUBS 88.63Bikel 88.16Collins-3 87.66Collins-2 87.62Collins-1 87.09Stanford-F 86.42Stanford-U 85.78Worst 1.12 0.61 0.51 0.39 0.45 0.40 0.42 0.27 0.27 1.13Table 2: Average number of bracket errors per sentence due to the top ten error types.
For instance, Stanford-Uproduces output that has, on average, 1.12 bracket errors per sentence that are due to PP attachment.
The scale foreach column is indicated by the Best and Worst values.NodesError Type Occurrences Involved RatioPP Attachment 846 1455 1.7Single word phrase 490 490 1.0Clause Attachment 385 913 2.4Modifier Attachment 383 599 1.6Different Label 377 754 2.0Unary 347 349 1.0NP Attachment 321 597 1.9NP Internal Structure 299 352 1.2Coordination 209 557 2.7Unary Clause Label 185 200 1.1VP Attachment 64 159 2.5Parenthetical Attachment 31 74 2.4Missing Parenthetical 12 17 1.4Unclassified 655 734 1.1Table 3: Breakdown of errors on section 23 for the Char-niak parser with self-trained model and reranker.
Errorsare sorted by the number of times they occur.
Ratio is theaverage number of node errors caused by each error weidentify (i.e.
Nodes Involved / Occurrences).errors on WSJ section 23.
The shaded area ofeach bar indicates the frequency of parse errors (i.e.empty means fewest errors).
The area filled in isdetermined by the expected number of node errorsper sentence that are attributed to that type of error.The average number of node errors per sentence fora completely full bar is indicated by the Worst row,and the value for a completely empty bar is indicatedby the Best row.
Exact error counts are available athttp://code.google.com/p/berkeley-parser-analyser/.We use counts of node errors to make the con-tributions of each type of error more interpretable.As Table 3 shows, some errors typically cause onlya single node error, where as others, such as co-ordination, generally cause several.
This meansthat considering counts of error groups would over-emphasise some error types, e.g.
single word phraseerrors are second most important by number ofgroups (in Table 3), but seventh by total number ofnode errors (in Table 2).As expected, PP attachment is the largest contrib-utor to errors, across all parsers.
Interestingly, coor-dination is sixth on the list, though that is partly dueto the fact that there are fewer coordination decisionsto be made in the treebank.3By looking at the performance of the Collinsparser we can see the development over the pastfifteen years.
There has been improvement acrossthe board, but in some cases, e.g.
clause attach-ment errors and different label errors, the change hasbeen more limited (24% and 29% reductions respec-tively).
We investigated the breakdown of the differ-ent label errors by label, but no particular cases of la-3This is indicated by the frequency of CCs and PPs in sec-tions 02?21 of the treebank, 16,844 and 95,581 respectively.These counts are only an indicator of the number of decisionsas the nodes can be used in ways that do not involve a decision,such as sentences that start with a conjunction.1054PP Clause Diff Mod NP 1-Word NPSystem K F-score Attach Attach Label Attach Attach Co-ord Span Unary aInt.a OtherBest 0.08 0.04 0.08 0.05 0.06 0.04 0.08 0.04 0.04 0.111000 98.30100 97.5450 97.18Oracle 20 96.4010 95.665 94.612 92.591000 92.07100 92.0850 92.07Charniak 20 92.0510 92.165 91.942 91.561 91.02Worst 0.66 0.43 0.33 0.26 0.28 0.26 0.23 0.16 0.19 0.60Table 4: Average number of bracket errors per sentence for a range of K-best list lengths using the Charniak parserwith reranking and the self-trained model.
The oracle results are determined by taking the parse in each K-best listwith the highest F-score.bel confusion stand out, and we found that the mostcommon cases remained the same between Collinsand the top results.It is also interesting to compare pairs of parsersthat share aspects of their architecture.
One suchpair is the Stanford parser, where the factored parsercombines the unlexicalised parser with a lexicaliseddependency parser.
The main sources of the 0.64gain in F-score are PP attachment and coordination.Another interesting pair is the Berkeley parser andthe BUBS parser, which uses a Berkeley grammar,but improves speed by pruning.
The pruning meth-ods used in BUBS are particularly damaging for PPattachment errors and unary errors.Various comparisons can be made between Char-niak parser variants.
We discuss the reranker be-low.
For the self-trained model McClosky et al(2006) performed some error analysis, consideringvariations in F-score depending on the frequency oftags such as PP, IN and CC in sentences.
Here wesee gains on all error types, though particularly forclause attachment, modifier attachment and coordi-nation, which fits with their observations.5.1 RerankingThe standard dynamic programming approach toparsing limits the range of features that can be em-ployed.
One way to deal with this issue is to mod-ify the parser to produce the top K parses, ratherthan just the 1-best, then use a model with more so-phisticated features to choose the best parse fromthis list (Collins, 2000).
While re-ranking has led togains in performance (Charniak and Johnson, 2005),there has been limited analysis of how effectivelyrerankers are using the set of available options.
Re-cent work has explored this question in more depth,but focusing on how variation in the parametersimpacts performance on standard metrics (Huang,2008; Ng et al 2010; Auli and Lopez, 2011; Ngand Curran, 2012).In Table 4 we present a breakdown over errortypes for the Charniak parser, using the self-trainedmodel and reranker.
The oracle results use the parsein each K-best list with the highest F-score.
Whilethis may not give the true oracle result, as F-scoredoes not factor over sentences, it gives a close ap-proximation.
The table has the same columns as Ta-ble 2, but the ranges on the bars now reflect the minand max for these sets.While there is improvement on all errors when us-ing the reranker, there is very little additional gainbeyond the first 5-10 parses.
Even for the oracleresults, most of the improvement occurs within thefirst 5-10 parses.
The limited utility of extra parses1055PP Clause Diff Mod NP 1-Word NPCorpus F-score Attach Attach Label Attach Attach Co-ord Span Unary aInt.a OtherBest 0.022 0.016 0.013 0.011 0.011 0.010 0.009 0.006 0.005 0.021WSJ 23 92.07Brown-F 85.91Brown-G 84.56Brown-K 84.09Brown-L 83.95Brown-M 84.65Brown-N 85.20Brown-P 84.09Brown-R 83.60G-Web Blogs 84.15G-Web Email 81.18Worst 0.040 0.035 0.053 0.020 0.034 0.023 0.046 0.009 0.029 0.073Table 5: Average number of node errors per word for a range of domains using the Charniak parser with reranking andthe self-trained model.
We use per word error rates here rather than per sentence as there is great variation in averagesentence length across the domains, skewing the per sentence results.for the reranker may be due to the importance ofthe base parser output probability feature (which, bydefinition, decreases within the K-best list).Interestingly, the oracle performance improvesacross all error types, even at the 2-best level.
Thisindicates that the base parser model is not particu-larly biased against a single error.
Focusing on therows for K = 2 we can also see two interesting out-liers.
The PP attachment improvement of the ora-cle is considerably higher than that of the reranker,particularly compared to the differences for other er-rors, suggesting that the reranker lacks the featuresnecessary to make the decision better than the parser.The other interesting outlier is NP internal structure,which continues to make improvements for longerlists, unlike the other error types.5.2 Out-of-DomainParsing performance drops considerably when shift-ing outside of the domain a parser was trained on(Gildea, 2001).
Clegg and Shepherd (2005) evalu-ated parsers qualitatively on node types and rule pro-ductions.
Bender et al(2011) designed a Wikipediatest set to evaluate parsers on dependencies repre-senting ten specific linguistic phenomena.To provide a deeper understanding of the er-rors arising when parsing outside of the newswiredomain, we analyse performance of the Charniakparser with reranker and self-trained model on theeight parts of the Brown corpus (Marcus et alCorpus Description Sentences Av.
LengthWSJ 23 Newswire 2416 23.5Brown F Popular 3164 23.4Brown G Biographies 3279 25.5Brown K General 3881 17.2Brown L Mystery 3714 15.7Brown M Science 881 16.6Brown N Adventure 4415 16.0Brown P Romance 3942 17.4Brown R Humour 967 22.7G-Web Blogs Blogs 1016 23.6G-Web Email E-mail 2450 11.9Table 6: Variation in size and contents of the domains weconsider.
The variation in average sentence lengths skewsthe results for errors per sentences, and so in Table 5 weconsider errors per word.1993), and two parts of the Google Web corpus(Petrov and McDonald, 2012).
Table 6 shows statis-tics for the corpora.
The variation in average sen-tence lengths skew the results for errors per sen-tence.
To handle this we divide by the number ofwords to determine the results in Table 5, rather thanby the number of sentences, as in previous figures.There are several interesting features in the table.First, on the Brown datasets, while the general trendis towards worse performance on all errors, NP in-ternal structure is a notable exception and in somecases PP attachment and unaries are as well.In the other errors we see similar patterns acrossthe corpora, except humour (Brown R), on which theparser is particularly bad at coordination and clause1056attachment.
This makes sense, as the colloquial na-ture of the text includes more unusual uses of con-junctions, for example:She was a living doll and no mistake ?
the ...Comparing the Brown corpora and the GoogleWeb corpora, there are much larger divergences.
Wesee a particularly large decrease in NP internal struc-ture.
Looking at some of the instances of this error, itappears to be largely caused by incorrect handling ofstructures such as URLs and phone numbers, whichdo not appear in the PTB.
There are also some moredifficult cases, for example:... going up for sale in the next month or do .where or do is a QP.
This typographical error is ex-tremely difficult to handle for a parser trained onlyon well-formed text.For e-mail there is a substantial drop on singleword phrases.
Breaking the errors down by label wefound that the majority of the new errors are miss-ing or extra NPs over single words.
Here the mainproblem appears to be temporal expressions, thoughthere also appear to be a substantial number of errorsthat are also at the POS level, such as when NNP isassigned to ta in this case:... let you know that I ?m out ta here !Some of these issues, such as URL handling,could be resolved with suitable training data.
Otherissues, such as ungrammatical language and uncon-ventional use of words, pose a greater challenge.6 ConclusionThe single F-score objective over brackets or depen-dencies obscures important differences between sta-tistical parsers.
For instance, a single attachment er-ror can lead to one or many mismatched brackets.We have created a novel tree-transformationmethodology for evaluating parsers that categoriseserrors into linguistically meaningful types.
Usingthis approach, we presented the first detailed exam-ination of the errors produced by a wide range ofconstituency parsers for English.
We found that PPattachment and clause attachment are the most chal-lenging constructions, while coordination turns outto be less problematic than previously thought.
Wealso noted interesting variations in error types forparsers variants.We investigated the errors resolved in reranking,and introduced by changing domains.
We found thatthe Charniak rerankers improved most error types,but made little headway on improving PP attach-ment.
Changing domain has an impact on all errortypes, except NP internal structure.We have released our system so that future con-stituent parsers can be evaluated using our method-ology.
Our analysis provides new insight into thedevelopment of parsers over the past fifteen years,and the challenges that remain.AcknowledgmentsWe would like to thank the anonymous reviewersfor their helpful suggestions.
This research was par-tially supported by a General Sir John Monash Fel-lowship to the first author, the Office of Naval Re-search under MURI Grant No.
N000140911081, anNSF Fellowship to the second author, ARC Discov-ery grant DP1097291, the Capital Markets CRC, andthe NSF under grant 0643742.ReferencesS.
Abney, S. Flickenger, C. Gdaniec, C. Grishman,P.
Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-vans, M. Liberman, M. Marcus, S. Roukos, B. San-torini, and T. Strzalkowski.
1991.
Procedure for quan-titatively comparing the syntactic coverage of englishgrammars.
In Proceedings of the workshop on Speechand Natural Language, pages 306?311, Pacific Grove,California, USA, February.Michael Auli and Adam Lopez.
2011.
A comparison ofloopy belief propagation and dual decomposition forintegrated ccg supertagging and parsing.
In Proceed-ings of ACL, pages 470?480, Portland, Oregon, USA,June.Emily M. Bender, Dan Flickinger, Stephan Oepen, andYi Zhang.
2011.
Parser evaluation over local and non-local deep dependencies in a large corpus.
In Proceed-ings of EMNLP, pages 397?408, Edinburgh, UnitedKingdom, July.Daniel M. Bikel.
2004.
Intricacies of collins?
parsingmodel.
Computational Linguistics, 30(4):479?511.Nathan Bodenstab, Aaron Dunlop, Keith Hall, and BrianRoark.
2011.
Beam-width prediction for efficientcontext-free parsing.
In Proceedings of ACL, pages440?449, Portland, Oregon, USA, June.1057Ted Briscoe and John Carroll.
2006.
Evaluating theaccuracy of an unlexicalized statistical parser on thePARC DepBank.
In Proceedings of ACL, pages 41?48, Sydney, Australia, July.Ted Briscoe, John Carroll, Jonathan Graham, and AnnCopestake, 2002.
Relational Evaluation Schemes,pages 4?8.
Las Palmas, Canary Islands, Spain, May.John Carroll, Ted Briscoe, and Antonio Sanfilippo.
1998.Parser evaluation: a survey and a new proposal.In Proceedings of LREC, pages 447?454, Granada,Spain, May.Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-sky, and Christopher D. Manning.
2010.
Parsing tostanford dependencies: Trade-offs between speed andaccuracy.
In Proceedings of LREC, Valletta, Malta,May.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of ACL, pages 173?180, Ann Ar-bor, Michigan, USA, June.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL, pages 132?139,Seattle, Washington, USA, April.Stephen Clark and Julia Hockenmaier.
2002.
Evaluat-ing a wide-coverage ccg parser.
In Proceedings of theLREC Beyond Parseval Workshop, Las Palmas, Ca-nary Islands, Spain, May.Andrew B. Clegg and Adrian J. Shepherd.
2005.
Evalu-ating and integrating treebank parsers on a biomedicalcorpus.
In Proceedings of the ACL Workshop on Soft-ware, pages 14?33, Ann Arbor, Michigan, USA, June.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of ACL,pages 16?23, Madrid, Spain, July.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Proceedings of ICML, pages175?182, Palo Alto, California, USA, June.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguis-tics, 29(4):589?637.Rebecca Dridan and Stephan Oepen.
2011.
Parser evalu-ation using elementary dependency matching.
In Pro-ceedings of IWPT, pages 225?230, Dublin, Ireland,October.Aaron Dunlop, Nathan Bodenstab, and Brian Roark.2011.
Efficient matrix-encoded grammars and low la-tency parallelization strategies for cyk.
In Proceedingsof IWPT, pages 163?174, Dublin, Ireland, October.Jennifer Foster and Josef van Genabith.
2008.
Parserevaluation and the bnc: Evaluating 4 constituencyparsers with 3 metrics.
In Proceedings of LREC, Mar-rakech, Morocco, May.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proceedings of EMNLP, pages 167?202,Pittsburgh, Pennsylvania, USA, June.Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.2007.
Evaluating impact of re-training a lexical dis-ambiguation model on domain adaptation of an hpsgparser.
In Proceedings of IWPT, pages 11?22, Prague,Czech Republic, June.Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.2009.
Descriptive and empirical approaches to captur-ing underlying dependencies among parsing errors.
InProceedings of EMNLP, pages 1162?1171, Singapore,August.James Henderson.
2003.
Inducing history representa-tions for broad coverage statistical parsing.
In Pro-ceedings of NAACL, pages 24?31, Edmonton, Canada,May.James Henderson.
2004.
Discriminative training of aneural network statistical parser.
In Proceedings ofACL, pages 95?102, Barcelona, Spain, July.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL, pages 586?594, Columbus, Ohio, USA, June.Tracy H. King, Richard Crouch, Stefan Riezler, MaryDalrymple, and Ronald M. Kaplan.
2003.
The PARC700 dependency bank.
In Proceedings of the 4th Inter-national Workshop on Linguistically Interpreted Cor-pora at EACL, Budapest, Hungary, April.Dan Klein and Christopher D. Manning.
2003a.
Ac-curate unlexicalized parsing.
In Proceedings of ACL,pages 423?430, Sapporo, Japan, July.Dan Klein and Christopher D. Manning.
2003b.
Fastexact inference with a factored model for natural lan-guage parsing.
In Proceedings of NIPS, pages 3?10,Vancouver, British Columbia, Canada, December.Dekang Lin.
1998.
A dependency-based method forevaluating broad-coverage parsers.
Natural LanguageEngineering, 4(2):97?114.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: the penn treebank.
Computational Lin-guistics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of NAACL, pages 152?159, New York, New York,USA, June.Ryan McDonald and Joakim Nivre.
2007.
Charac-terizing the errors of data-driven dependency parsingmodels.
In Proceedings of EMNLP, pages 122?131,Prague, Czech Republic, June.Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-suzaki, and Jun?ichi Tsujii.
2008.
Task-oriented eval-uation of syntactic parsers and their representations.
In1058Proceedings of ACL, pages 46?54, Columbus, Ohio,USA, June.Dominick Ng and James R. Curran.
2012.
N-best CCGparsing and reranking.
In Proceedings of ACL, Jeju,South Korea, July.Dominick Ng, Matthew Honnibal, and James R. Curran.2010.
Reranking a wide-coverage ccg parser.
In Pro-ceedings of ALTA, pages 90?98, Melbourne, Australia,December.Joakim Nivre, Laura Rimell, Ryan McDonald, and CarlosGo?mez-Rodr??guez.
2010.
Evaluation of dependencyparsers on unbounded dependencies.
In Proceedingsof Coling, pages 833?841, Beijing, China, August.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACL,pages 404?411, Rochester, New York, USA, April.Slav Petrov and Ryan McDonald.
2012.
SANCL SharedTask.
LDC2012E43.
Linguistic Data Consortium.Philadelphia, Philadelphia, USA.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of ACL, pages433?440, Sydney, Australia, July.Chris Quirk and Simon Corston-Oliver.
2006.
The im-pact of parse quality on syntactically-informed statis-tical machine translation.
In Proceedings of EMNLP,pages 62?69, Sydney, Australia, July.Kun Yu, Yusuke Miyao, Takuya Matsuzaki, XiangliWang, and Junichi Tsujii.
2011.
Analysis of the dif-ficulties in chinese deep parsing.
In Proceedings ofIWPT, pages 48?57, Dublin, Ireland, October.Deniz Yuret, Aydin Han, and Zehra Turgut.
2010.Semeval-2010 task 12: Parser evaluation using textualentailments.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 51?56, Up-psala, Sweden, July.1059
