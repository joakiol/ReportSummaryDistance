Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1288?1298,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsDual Decomposition for Parsing with Non-Projective Head AutomataTerry Koo Alexander M. Rush Michael Collins Tommi Jaakkola David SontagMIT CSAIL, Cambridge, MA 02139, USA{maestro,srush,mcollins,tommi,dsontag}@csail.mit.eduAbstractThis paper introduces algorithms for non-projective parsing based on dual decomposi-tion.
We focus on parsing algorithms for non-projective head automata, a generalization ofhead-automata models to non-projective struc-tures.
The dual decomposition algorithms aresimple and efficient, relying on standard dy-namic programming and minimum spanningtree algorithms.
They provably solve an LPrelaxation of the non-projective parsing prob-lem.
Empirically the LP relaxation is very of-ten tight: for many languages, exact solutionsare achieved on over 98% of test sentences.The accuracy of our models is higher than pre-vious work on a broad range of datasets.1 IntroductionNon-projective dependency parsing is useful formany languages that exhibit non-projective syntacticstructures.
Unfortunately, the non-projective parsingproblem is known to be NP-hard for all but the sim-plest models (McDonald and Satta, 2007).
There hasbeen a long history in combinatorial optimization ofmethods that exploit structure in complex problems,using methods such as dual decomposition or La-grangian relaxation (Lemare?chal, 2001).
Thus far,however, these methods are not widely used in NLP.This paper introduces algorithms for non-projective parsing based on dual decomposition.
Wefocus on parsing algorithms for non-projective headautomata, a generalization of the head-automatamodels of Eisner (2000) and Alshawi (1996) to non-projective structures.
These models include non-projective dependency parsing models with higher-order (e.g., sibling and/or grandparent) dependencyrelations as a special case.
Although decoding of fullparse structures with non-projective head automatais intractable, we leverage the observation that keycomponents of the decoding can be efficiently com-puted using combinatorial algorithms.
In particular,1.
Decoding for individual head-words can be ac-complished using dynamic programming.2.
Decoding for arc-factored models can be ac-complished using directed minimum-weightspanning tree (MST) algorithms.The resulting parsing algorithms have the followingproperties:?
They are efficient and easy to implement, relyingon standard dynamic programming and MST al-gorithms.?
They provably solve a linear programming (LP)relaxation of the original decoding problem.?
Empirically the algorithms very often give an ex-act solution to the decoding problem, in whichcase they also provide a certificate of optimality.In this paper we first give the definition for non-projective head automata, and describe the parsingalgorithm.
The algorithm can be viewed as an in-stance of Lagrangian relaxation; we describe thisconnection, and give convergence guarantees for themethod.
We describe a generalization to modelsthat include grandparent dependencies.
We then in-troduce a perceptron-driven training algorithm thatmakes use of point 1 above.We describe experiments on non-projective pars-ing for a number of languages, and in particu-lar compare the dual decomposition algorithm toapproaches based on general-purpose linear pro-gramming (LP) or integer linear programming (ILP)solvers (Martins et al, 2009).
The accuracy of ourmodels is higher than previous work on a broadrange of datasets.
The method gives exact solutionsto the decoding problem, together with a certificateof optimality, on over 98% of test examples for manyof the test languages, with parsing times ranging be-tween 0.021 seconds/sentence for the most simplelanguages/models, to 0.295 seconds/sentence for the1288most complex settings.
The method compares favor-ably to previous work using LP/ILP formulations,both in terms of efficiency, and also in terms of thepercentage of exact solutions returned.While the focus of the current paper is on non-projective dependency parsing, the approach opensup new ways of thinking about parsing algorithmsfor lexicalized formalisms such as TAG (Joshi andSchabes, 1997), CCG (Steedman, 2000), and pro-jective head automata.2 Related WorkMcDonald et al (2005) describe MST-based parsingfor non-projective dependency parsing models witharc-factored decompositions; McDonald and Pereira(2006) make use of an approximate (hill-climbing)algorithm for parsing with more complex models.McDonald and Pereira (2006) and McDonald andSatta (2007) describe complexity results for non-projective parsing, showing that parsing for a varietyof models is NP-hard.
Riedel and Clarke (2006) de-scribe ILP methods for the problem; Martins et al(2009) recently introduced alternative LP and ILPformulations.
Our algorithm differs in that we do notuse general-purpose LP or ILP solvers, instead usingan MST solver in combination with dynamic pro-gramming; thus we leverage the underlying struc-ture of the problem, thereby deriving more efficientdecoding algorithms.Both dual decomposition and Lagrangian relax-ation have a long history in combinatorial optimiza-tion.
Our work was originally inspired by recentwork on dual decomposition for inference in graph-ical models (Wainwright et al, 2005; Komodakiset al, 2007).
However, the non-projective parsingproblem has a very different structure from thesemodels, and the decomposition we use is very dif-ferent in nature from those used in graphical mod-els.
Other work has made extensive use of de-composition approaches for efficiently solving LPrelaxations for graphical models (e.g., Sontag etal.
(2008)).
Methods that incorporate combinato-rial solvers within loopy belief propagation (LBP)(Duchi et al, 2007; Smith and Eisner, 2008) arealso closely related to our approach.
Unlike LBP,our method has strong theoretical guarantees, suchas guaranteed convergence and the possibility of acertificate of optimality.Finally, in other recent work, Rush et al (2010)describe dual decomposition approaches for otherNLP problems.3 Sibling ModelsThis section describes a particular class of models,sibling models; the next section describes a dual-decomposition algorithm for decoding these models.Consider the dependency parsing problem for asentence with n words.
We define the index setfor dependency parsing to be I = {(i, j) : i ?
{0 .
.
.
n}, j ?
{1 .
.
.
n}, i 6= j}.
A dependencyparse is a vector y = {y(i, j) : (i, j) ?
I}, wherey(i, j) = 1 if a dependency with head word i andmodifier j is in the parse, 0 otherwise.
We use i = 0for the root symbol.
We define Y to be the set of allwell-formed non-projective dependency parses (i.e.,the set of directed spanning trees rooted at node 0).Given a function f : Y 7?
R that assigns scores toparse trees, the optimal parse isy?
= argmaxy?Yf(y) (1)A particularly simple definition of f(y) is f(y) =?
(i,j)?I y(i, j)?
(i, j) where ?
(i, j) is the score fordependency (i, j).
Models with this form are oftenreferred to as arc-factored models.
In this case theoptimal parse tree y?
can be found efficiently usingMST algorithms (McDonald et al, 2005).This paper describes algorithms that compute y?for more complex definitions of f(y); in this sec-tion, we focus on algorithms for models that captureinteractions between sibling dependencies.
To thisend, we will find it convenient to define the follow-ing notation.
Given a vector y, definey|i = {y(i, j) : j = 1 .
.
.
n, j 6= i}Hence y|i specifies the set of modifiers to word i;note that the vectors y|i for i = 0 .
.
.
n form a parti-tion of the full set of variables.We then assume that f(y) takes the formf(y) =n?i=0fi(y|i) (2)Thus f(y) decomposes into a sum of terms, whereeach fi considers modifiers to the i?th word alone.In the general case, finding y?
=argmaxy?Y f(y) under this definition of f(y)is an NP-hard problem.
However for certain1289definitions of fi, it is possible to efficiently computeargmaxy|i?Zi fi(y|i) for any value of i, typicallyusing dynamic programming.
(Here we use Zi torefer to the set of all possible values for y|i: specifi-cally, Z0 = {0, 1}n and for i 6= 0, Zi = {0, 1}n?1.
)In these cases we can efficiently computez?
= argmaxz?Zf(z) = argmaxz?Z?ifi(z|i) (3)where Z = {z : z|i ?
Zi for i = 0 .
.
.
n} bysimply computing z?|i = argmaxz|i?Zi fi(z|i) fori = 0 .
.
.
n. Eq.
3 can be considered to be an approx-imation to Eq.
1, where we have replaced Y withZ .
We will make direct use of this approximationin the dual decomposition parsing algorithm.
Notethat Y ?
Z , and in all but trivial cases, Y is a strictsubset of Z .
For example, a structure z ?
Z couldhave z(i, j) = z(j, i) = 1 for some (i, j); it couldcontain longer cycles; or it could contain words thatdo not modify exactly one head.
Nevertheless, withsuitably powerful functions fi?for example func-tions based on discriminative models?z?
may be agood approximation to y?.
Later we will see thatdual decomposition can effectively use MST infer-ence to rule out ill-formed structures.We now give the main assumption underlying sib-ling models:Assumption 1 (Sibling Decompositions) A modelf(y) satisfies the sibling-decomposition assumptionif: 1) f(y) =?ni=0 fi(y|i) for some set of functionsf0 .
.
.
fn.
2) For any i ?
{0 .
.
.
n}, for any valueof the variables u(i, j) ?
R for j = 1 .
.
.
n, it ispossible to computeargmaxy|i?Zi??fi(y|i)?
?ju(i, j)y(i, j)?
?in polynomial time.The second condition includes additional terms in-volving u(i, j) variables that modify the scores ofindividual dependencies.
These terms are benign formost definitions of fi, in that they do not alter de-coding complexity.
They will be of direct use in thedual decomposition parsing algorithm.Example 1: Bigram Sibling Models.
Recall thaty|i is a binary vector specifying which words aremodifiers to the head-word i.
Define l1 .
.
.
lp to bethe sequence of left modifiers to word i under y|i,and r1 .
.
.
rq to be the set of right modifiers (e.g.,consider the case where n = 5, i = 3, and we havey(3, 1) = y(3, 5) = 0, and y(3, 2) = y(3, 4) = 1:in this case p = 1, l1 = 2, and q = 1, r1 = 4).
Inbigram sibling models, we havefi(y|i) =p+1?k=1gL(i, lk?1, lk) +q+1?k=1gR(i, rk?1, rk)where l0 = r0 = START is the initial state, andlp+1 = rq+1 = END is the end state.
The functionsgL and gR assign scores to bigram dependencies tothe left and right of the head.
Under this model cal-culating argmaxy|i?Zi(fi(y|i)?
?j u(i, j)y(i, j))takes O(n2) time using dynamic programming,hence the model satisfies Assumption 1.Example 2: Head Automata Head-automatamodels constitute a second important model typethat satisfy the sibling-decomposition assumption(bigram sibling models are a special case of headautomata).
These models make use of functionsgR(i, s, s?, r) where s ?
S, s?
?
S are variables in aset of possible states S, and r is an index of a wordin the sentence such that i < r ?
n. The functiongR returns a cost for taking word r as the next depen-dency, and transitioning from state s to s?.
A similarfunction gL is defined for left modifiers.
We definefi(y|i, s0 .
.
.
sq, t0 .
.
.
tp) =q?k=1gR(i, sk?1, sk, rk) +p?k=1gL(i, tk?1, tk, ll)to be the joint score for dependencies y|i, and leftand right state sequences s0 .
.
.
sq and t0 .
.
.
tp.
Wespecify that s0 = t0 = START and sq = tp = END.In this case we definefi(y|i) = maxs0...sq ,t0...tpfi(y|i, s0 .
.
.
sq, t0 .
.
.
tp)and it follows that argmaxy|i?Zi fi(y|i) can be com-puted inO(n|S|2) time using a variant of the Viterbialgorithm, hence the model satisfies the sibling-decomposition assumption.4 The Parsing AlgorithmWe now describe the dual decomposition parsing al-gorithm for models that satisfy Assumption 1.
Con-sider the following generalization of the decoding1290Set u(1)(i, j)?
0 for all (i, j) ?
Ifor k = 1 to K doy(k) ?
argmaxy?Y?(i,j)?I(?
(i, j) + u(k)(i, j))y(i, j)for i ?
{0 .
.
.
n},z(k)|i ?
argmaxz|i?Zi(fi(z|i)?
?ju(k)(i, j)z(i, j))if y(k)(i, j) = z(k)(i, j) for all (i, j) ?
I thenreturn (y(k), z(k))for all (i, j) ?
I,u(k+1)(i, j)?
u(k)(i, j)+?k(z(k)(i, j)?y(k)(i, j))return (y(K), z(K))Figure 1: The parsing algorithm for sibling decompos-able models.
?k ?
0 for k = 1 .
.
.K are step sizes, seeAppendix A for details.problem from Eq.
1, where f(y) =?i fi(y|i),h(y) =?
(i,j)?I ?
(i, j)y(i, j), and ?
(i, j) ?
R forall (i, j):1argmaxz?Z,y?Yf(z) + h(y) (4)such that z(i, j) = y(i, j) for all (i, j) ?
I (5)Although the maximization w.r.t.
z is taken over theset Z , the constraints in Eq.
5 ensure that z = y forsome y ?
Y , and hence that z ?
Y .Without the z(i, j) = y(i, j) constraints, theobjective would decompose into the separate max-imizations z?
= argmaxz?Z f(z), and y?
=argmaxy?Y h(y), which can be easily solved us-ing dynamic programming and MST, respectively.Thus, it is these constraints that complicate the op-timization.
Our approach gets around this difficultyby introducing new variables, u(i, j), that serve toenforce agreement between the y(i, j) and z(i, j)variables.
In the next section we will show that theseu(i, j) variables are actually Lagrange multipliersfor the z(i, j) = y(i, j) constraints.Our parsing algorithm is shown in Figure 1.
Ateach iteration k, the algorithm finds y(k) ?
Y us-ing an MST algorithm, and z(k) ?
Z through sep-arate decoding of the (n + 1) sibling models.
Theu(k) variables are updated if y(k)(i, j) 6= z(k)(i, j)1This is equivalent to Eq.
1 when ?
(i, j) = 0 for all (i, j).In some cases, however, it is convenient to have a model withnon-zero values for the ?
variables; see the Appendix.
Note thatthis definition of h(y) allows argmaxy?Y h(y) to be calculatedefficiently, using MST inference.for some (i, j); these updates modify the objectivefunctions for the two decoding steps, and intuitivelyencourage the y(k) and z(k) variables to be equal.4.1 Lagrangian RelaxationRecall that the main difficulty in solving Eq.
4 wasthe z = y constraints.
We deal with these con-straints using Lagrangian relaxation (Lemare?chal,2001).
We first introduce Lagrange multipliers u ={u(i, j) : (i, j) ?
I}, and define the LagrangianL(u, y, z) = (6)f(z) + h(y) +?
(i,j)?Iu(i, j)(y(i, j)?
z(i, j))If L?
is the optimal value of Eq.
4 subject to theconstraints in Eq.
5, then for any value of u,L?
= maxz?Z,y?Y,y=zL(u, y, z) (7)This follows because if y = z, the right term in Eq.
6is zero for any value of u.
The dual objective L(u)is obtained by omitting the y = z constraint:L(u) = maxz?Z,y?YL(u, y, z)= maxz?Z(f(z)?
?i,ju(i, j)z(i, j))+maxy?Y(h(y) +?i,ju(i, j)y(i, j)).Since L(u) maximizes over a larger space (y maynot equal z), we have that L?
?
L(u) (compare thisto Eq.
7).
The dual problem, which our algorithmoptimizes, is to obtain the tightest such upper bound,(Dual problem) minu?R|I|L(u).
(8)The dual objective L(u) is convex, but not differen-tiable.
However, we can use a subgradient methodto derive an algorithm that is similar to gradient de-scent, and which minimizes L(u).
A subgradient ofa convex function L(u) at u is a vector du such thatfor all v ?
R|I|, L(v) ?
L(u) + du ?
(v ?
u).
Bystandard results,du(k) = y(k) ?
z(k)is a subgradient for L(u) at u = u(k), where z(k) =argmaxz?Z f(z)?
?i,j u(k)(i, j)z(i, j) and y(k) =1291argmaxy?Y h(y) +?i,j u(k)(i, j)y(i, j).
Subgra-dient optimization methods are iterative algorithmswith updates that are similar to gradient descent:u(k+1) = u(k) ?
?kdu(k) = u(k) ?
?k(y(k) ?
z(k)),where ?k is a step size.
It is easily verified that thealgorithm in Figure 1 uses precisely these updates.4.2 Formal GuaranteesWith an appropriate choice of the step sizes ?k, thesubgradient method can be shown to solve the dualproblem, i.e.limk?
?L(u(k)) = minuL(u).See Korte and Vygen (2008), page 120, for details.As mentioned before, the dual provides an up-per bound on the optimum of the primal problem(Eq.
4),maxz?Z,y?Y,y=zf(z) + h(y) ?
minu?R|I|L(u).
(9)However, we do not necessarily have strongduality?i.e., equality in the above equation?because the sets Z and Y are discrete sets.
Thatsaid, for some functions h(y) and f(z) strong du-ality does hold, as stated in the following:Theorem 1 If for some k ?
{1 .
.
.K} in the al-gorithm in Figure 1, y(k)(i, j) = z(k)(i, j) for all(i, j) ?
I, then (y(k), z(k)) is a solution to the max-imization problem in Eq.
4.Proof.
We have that f(z(k)) + h(y(k)) =L(u(k), z(k), y(k)) = L(u(k)), where the last equal-ity is because y(k), z(k) are defined as the respectiveargmax?s.
Thus, the inequality in Eq.
9 is tight, and(y(k), z(k)) and u(k) are primal and dual optimal.Although the algorithm is not guaranteed to sat-isfy y(k) = z(k) for some k, by Theorem 1 if it doesreach such a state, then we have the guarantee of anexact solution to Eq.
4, with the dual solution u pro-viding a certificate of optimality.
We show in theexperiments that this occurs very frequently, in spiteof the parsing problem being NP-hard.It can be shown that Eq.
8 is the dual of an LPrelaxation of the original problem.
When the con-ditions of Theorem 1 are satisfied, it means that theLP relaxation is tight for this instance.
For brevitywe omit the details, except to note that when the LPrelaxation is not tight, the optimal primal solution tothe LP relaxation could be recovered by averagingmethods (Nedic?
and Ozdaglar, 2009).5 Grandparent Dependency ModelsIn this section we extend the approach to considergrandparent relations.
In grandparent models eachparse tree y is represented as a vectory = {y(i, j) : (i, j) ?
I} ?
{y?
(i, j) : (i, j) ?
I}where we have added a second set of duplicate vari-ables, y?
(i, j) for all (i, j) ?
I.
The set of all validparse trees is then defined asY = {y : y(i, j) variables form a directed tree,y?
(i, j) = y(i, j) for all (i, j) ?
I}We again partition the variables into n + 1 subsets,y|0 .
.
.
y|n, by (re)definingy|i = {y(i, j) : j = 1 .
.
.
n, j 6= i}?{y?
(k, i) : k = 0 .
.
.
n, k 6= i}So as before y|i contains variables y(i, j) which in-dicate which words modify the i?th word.
In addi-tion, y|i includes y?
(k, i) variables that indicate theword that word i itself modifies.The set of all possible values of y|i is nowZi = {y|i : y(i, j) ?
{0, 1} for j = 1 .
.
.
n, j 6= i;y?
(k, i) ?
{0, 1} for k = 0 .
.
.
n, k 6= i;?ky?
(k, i) = 1}Hence the y(i, j) variables can take any values, butonly one of the y?
(k, i) variables can be equal to 1(as only one word can be a parent of word i).
As be-fore, we define Z = {y : y|i ?
Zi for i = 0 .
.
.
n}.We introduce the following assumption:Assumption 2 (GS Decompositions)A model f(y) satisfies the grandparent/sibling-decomposition (GSD) assumption if: 1) f(z) =?ni=0 fi(z|i) for some set of functions f0 .
.
.
fn.
2)For any i ?
{0 .
.
.
n}, for any value of the variablesu(i, j) ?
R for j = 1 .
.
.
n, and v(k, i) ?
R fork = 0 .
.
.
n, it is possible to computeargmaxz|i?Zi(fi(z|i)?
?ju(i, j)z(i, j)?
?kv(k, i)z?
(k, i))in polynomial time.1292Again, it follows that we can approxi-mate y?
= argmaxy?Y?ni=0 fi(y|i) byz?
= argmaxz?Z?ni=0 fi(z|i), by definingz?|i = argmaxz|i?Zi fi(z|i) for i = 0 .
.
.
n. Theresulting vector z?
may be deficient in two respects.First, the variables z?
(i, j) may not form a well-formed directed spanning tree.
Second, we mayhave z??
(i, j) 6= z?
(i, j) for some values of (i, j).Example 3: Grandparent/Sibling Models Animportant class of models that satisfy Assumption 2are defined as follows.
Again, for a vector y|i de-fine l1 .
.
.
lp to be the sequence of left modifiers toword i under y|i, and r1 .
.
.
rq to be the set of rightmodifiers.
Define k?
to the value for k such thaty?
(k, i) = 1.
Then the model is defined as follows:fi(y|i) =p+1?j=1gL(i, k?, lj?1, lj)+q+1?j=1gR(i, k?, rj?1, rj)This is very similar to the bigram-sibling model, butwith the modification that the gL and gR functionsdepend in addition on the value for k?.
This al-lows these functions to model grandparent depen-dencies such as (k?, i, lj) and sibling dependenciessuch as (i, lj?1, lj).
Finding z?|i under the definitioncan be accomplished inO(n3) time, by decoding themodel using dynamic programming separately foreach of the O(n) possible values of k?, and pick-ing the value for k?
that gives the maximum valueunder these decodings.A dual-decomposition algorithm for models thatsatisfy the GSD assumption is shown in Figure 2.The algorithm can be justified as an instance of La-grangian relaxation applied to the problemargmaxz?Z,y?Yf(z) + h(y) (10)with constraintsz(i, j) = y(i, j) for all (i, j) ?
I (11)z?
(i, j) = y(i, j) for all (i, j) ?
I (12)The algorithm employs two sets of Lagrange mul-tipliers, u(i, j) and v(i, j), corresponding to con-straints in Eqs.
11 and 12.
As in Theorem 1, if at anypoint in the algorithm z(k) = y(k), then (z(k), y(k))is an exact solution to the problem in Eq.
10.Set u(1)(i, j)?
0, v(1)(i, j)?
0 for all (i, j) ?
Ifor k = 1 to K doy(k) ?
argmaxy?Y?
(i,j)?Iy(i, j)?
(i, j)where ?
(i, j) = ?
(i, j) + u(k)(i, j) + v(k)(i, j).for i ?
{0 .
.
.
n},z(k)|i ?
argmaxz|i?Zi(fi(z|i) ?
?ju(k)(i, j)z(i, j)?
?jv(k)(j, i)z?
(j, i))if y(k)(i, j) = z(k)(i, j) = z(k)?
(i, j) for all (i, j) ?
Ithenreturn (y(k), z(k))for all (i, j) ?
I,u(k+1)(i, j)?
u(k)(i, j)+?k(z(k)(i, j)?y(k)(i, j))v(k+1)(i, j)?
v(k)(i, j)+?k(z(k)?
(i, j)?y(k)(i, j))return (y(K), z(K))Figure 2: The parsing algorithm for grandparent/sibling-decomposable models.6 The Training AlgorithmIn our experiments we make use of discriminativelinear models, where for an input sentence x, thescore for a parse y is f(y) = w ?
?
(x, y) wherew ?
Rd is a parameter vector, and ?
(x, y) ?
Rdis a feature-vector representing parse tree y in con-junction with sentence x.
We will assume that thefeatures decompose in the same way as the sibling-decomposable or grandparent/sibling-decomposablemodels, that is ?
(x, y) =?ni=0 ?
(x, y|i) for somefeature vector definition ?
(x, y|i).
In the bigram sib-ling models in our experiments, we assume that?
(x, y|i) =p+1?k=1?L(x, i, lk?1, lk) +q+1?k=1?R(x, i, rk?1, rk)where as before l1 .
.
.
lp and r1 .
.
.
rq are left andright modifiers under y|i, and where ?L and ?Rare feature vector definitions.
In the grandparentmodels in our experiments, we use a similar defi-nition with feature vectors ?L(x, i, k?, lk?1, lk) and?R(x, i, k?, rk?1, rk), where k?
is the parent forword i under y|i.We train the model using the averaged perceptronfor structured problems (Collins, 2002).
Given thei?th example in the training set, (x(i), y(i)), the per-ceptron updates are as follows:?
z?
= argmaxy?Z w ?
?
(x(i), y)?
If z?
6= y(i), w = w+?
(x(i), y(i))??
(x(i), z?
)1293The first step involves inference over the set Z ,rather than Y as would be standard in the percep-tron.
Thus, decoding during training can be achievedby dynamic programming over head automata alone,which is very efficient.Our training approach is closely related to localtraining methods (Punyakanok et al, 2005).
Wehave found this method to be effective, very likelybecause Z is a superset of Y .
Our training algo-rithm is also related to recent work on training usingouter bounds (see, e.g., (Taskar et al, 2003; Fin-ley and Joachims, 2008; Kulesza and Pereira, 2008;Martins et al, 2009)).
Note, however, that the LP re-laxation optimized by dual decomposition is signifi-cantly tighter than Z .
Thus, an alternative approachwould be to use the dual decomposition algorithmfor inference during training.7 ExperimentsWe report results on a number of data sets.
Forcomparison to Martins et al (2009), we perform ex-periments for Danish, Dutch, Portuguese, Slovene,Swedish and Turkish data from the CoNLL-Xshared task (Buchholz and Marsi, 2006), and En-glish data from the CoNLL-2008 shared task (Sur-deanu et al, 2008).
We use the official training/testsplits for these data sets, and the same evaluationmethodology as Martins et al (2009).
For com-parison to Smith and Eisner (2008), we also re-port results on Danish and Dutch using their alter-nate training/test split.
Finally, we report results onthe English WSJ treebank, and the Prague treebank.We use feature sets that are very similar to thosedescribed in Carreras (2007).
We use marginal-based pruning, using marginals calculated from anarc-factored spanning tree model using the matrix-tree theorem (McDonald and Satta, 2007; Smith andSmith, 2007; Koo et al, 2007).In all of our experiments we set the value K, themaximum number of iterations of dual decompo-sition in Figures 1 and 2, to be 5,000.
If the al-gorithm does not terminate?i.e., it does not return(y(k), z(k)) within 5,000 iterations?we simply takethe parse y(k) with the maximum value of f(y(k)) asthe output from the algorithm.
At first sight 5,000might appear to be a large number, but decoding isstill fast?see Sections 7.3 and 7.4 for discussion.22Note also that the feature vectors ?
and inner productsw ?
?The strategy for choosing step sizes ?k is describedin Appendix A, along with other details.We first discuss performance in terms of accu-racy, success in recovering an exact solution, andparsing speed.
We then describe additional experi-ments examining various aspects of the algorithm.7.1 AccuracyTable 1 shows results for previous work on the var-ious data sets, and results for an arc-factored modelwith pure MST decoding with our features.
(We usethe acronym UAS (unlabeled attachment score) fordependency accuracy.)
We also show results for thebigram-sibling and grandparent/sibling (G+S) mod-els under dual decomposition.
Both the bigram-sibling and G+S models show large improvementsover the arc-factored approach; they also comparefavorably to previous work?for example the G+Smodel gives better results than all results reported inthe CoNLL-X shared task, on all languages.
Notethat we use different feature sets from both Martinset al (2009) and Smith and Eisner (2008).7.2 Success in Recovering Exact SolutionsNext, we consider how often our algorithms returnan exact solution to the original optimization prob-lem, with a certificate?i.e., how often the algo-rithms in Figures 1 and 2 terminate with y(k) = z(k)for some value of k < 5000 (and are thus optimal,by Theorem 1).
The CertS and CertG columns in Ta-ble 1 give the results for the sibling and G+S modelsrespectively.
For all but one setting3 over 95% of thetest sentences are decoded exactly, with 99% exact-ness in many cases.For comparison, we also ran both the single-commodity flow and multiple-commodity flow LPrelaxations of Martins et al (2009) with our mod-els and features.
We measure how often these re-laxations terminate with an exact solution.
The re-sults in Table 2 show that our method gives exactsolutions more often than both of these relaxations.4In computing the accuracy figures for Martins et alonly need to be computed once, thus saving computation.3The exception is Slovene, which has the smallest trainingset at only 1534 sentences.4Note, however, that it is possible that the Martins et al re-laxations would have given a higher proportion of integral solu-tions if their relaxation was used during training.1294Ma09 MST Sib G+S Best CertS CertG TimeS TimeG TrainS TrainGDan 91.18 89.74 91.08 91.78 91.54 99.07 98.45 0.053 0.169 0.051 0.109Dut 85.57 82.33 84.81 85.81 85.57 98.19 97.93 0.035 0.120 0.046 0.048Por 92.11 90.68 92.57 93.03 92.11 99.65 99.31 0.047 0.257 0.077 0.103Slo 85.61 82.39 84.89 86.21 85.61 90.55 95.27 0.158 0.295 0.054 0.130Swe 90.60 88.79 90.10 91.36 90.60 98.71 98.97 0.035 0.141 0.036 0.055Tur 76.34 75.66 77.14 77.55 76.36 98.72 99.04 0.021 0.047 0.016 0.036Eng1 91.16 89.20 91.18 91.59 ?
98.65 99.18 0.082 0.200 0.032 0.076Eng2 ?
90.29 92.03 92.57 ?
98.96 99.12 0.081 0.168 0.032 0.076Sm08 MST Sib G+S ?
CertS CertG TimeS TimeG TrainS TrainGDan 86.5 87.89 89.58 91.00 ?
98.50 98.50 0.043 0.120 0.053 0.065Dut 88.5 88.86 90.87 91.76 ?
98.00 99.50 0.036 0.046 0.050 0.054Mc06 MST Sib G+S ?
CertS CertG TimeS TimeG TrainS TrainGPTB 91.5 90.10 91.96 92.46 ?
98.89 98.63 0.062 0.210 0.028 0.078PDT 85.2 84.36 86.44 87.32 ?
96.67 96.43 0.063 0.221 0.019 0.051Table 1: A comparison of non-projective automaton-based parsers with results from previous work.
MST: Our first-order baseline.
Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded viadual decomposition.
Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al (2009).
Sm08:The best UAS of any LBP-based parser in Smith and Eisner (2008).
Mc06: The best UAS reported by McDonaldand Pereira (2006).
Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task(Buchholz and Marsi, 2006) or in any column of Martins et al (2009, Table 1); note that the latter includes McDonaldand Pereira (2006), Nivre and McDonald (2008), and Martins et al (2008).
CertS/CertG: Percent of test examplesfor which dual decomposition produced a certificate of optimality, for Sib/G+S.
TimeS/TimeG: Seconds/sentence fortest decoding, for Sib/G+S.
TrainS/TrainG: Seconds/sentence during training, for Sib/G+S.
For consistency of timing,test decoding was carried out on identical machines with zero additional load; however, training was conducted onmachines with varying hardware and load.
We ran two tests on the CoNLL-08 corpus.
Eng1: UAS when testing onthe CoNLL-08 validation set, following Martins et al (2009).
Eng2: UAS when testing on the CoNLL-08 test set.
(2009), we project fractional solutions to a well-formed spanning tree, as described in that paper.Finally, to better compare the tightness of ourLP relaxation to that of earlier work, we considerrandomly-generated instances.
Table 2 gives resultsfor our model and the LP relaxations of Martins et al(2009) with randomly generated scores on automatatransitions.
We again recover exact solutions moreoften than the Martins et al relaxations.
Note thatwith random parameters the percentage of exact so-lutions is significantly lower, suggesting that the ex-actness of decoding of the trained models is a specialcase.
We speculate that this is due to the high perfor-mance of approximate decoding with Z in place ofY under the trained models for fi; the training algo-rithm described in section 6 may have the tendencyto make the LP relaxation tight.7.3 SpeedTable 1, columns TimeS and TimeG, shows decod-ing times for the dual decomposition algorithms.Table 2 gives speed comparisons to Martins et al(2009).
Our method gives significant speed-ups over0510152025300  1000  2000  3000  4000  5000%ofHeadAutomataRecomputedIterations of Dual Decomposition% recomputed, g+s% recomputed, sibFigure 3: The average percentage of head automata thatmust be recomputed on each iteration of dual decompo-sition on the PTB validation set.the Martins et al (2009) method, presumably be-cause it leverages the underlying structure of theproblem, rather than using a generic solver.7.4 Lazy DecodingHere we describe an important optimization in thedual decomposition algorithms.
Consider the algo-rithm in Figure 1.
At each iteration we must findz(k)|i = argmaxz|i?Zi(fi(z|i)?
?ju(k)(i, j)z(i, j))1295Sib Acc Int Time RandLP(S) 92.14 88.29 0.14 11.7LP(M) 92.17 93.18 0.58 30.6ILP 92.19 100.0 1.44 100.0DD-5000 92.19 98.82 0.08 35.6DD-250 92.23 89.29 0.03 10.2G+S Acc Int Time RandLP(S) 92.60 91.64 0.23 0.0LP(M) 92.58 94.41 0.75 0.0ILP 92.70 100.0 1.79 100.0DD-5000 92.71 98.76 0.23 6.8DD-250 92.66 85.47 0.12 0.0Table 2: A comparison of dual decomposition with lin-ear programs described by Martins et al (2009).
LP(S):Linear Program relaxation based on single-commodityflow.
LP(M): Linear Program relaxation based onmulti-commodity flow.
ILP: Exact Integer Linear Pro-gram.
DD-5000/DD-250: Dual decomposition with non-projective head automata, with K = 5000/250.
Upperresults are for the sibling model, lower results are G+S.Columns give scores for UAS accuracy, percentage of so-lutions which are integral, and solution speed in secondsper sentence.
These results are for Section 22 of the PTB.The last column is the percentage of integral solutions ona random problem of length 10 words.
The (I)LP experi-ments were carried out using Gurobi, a high-performancecommercial-grade solver.for i = 0 .
.
.
n. However, if for some i, u(k)(i, j) =u(k?1)(i, j) for all j, then z(k)|i = z(k?1)|i .
Inlazy decoding we immediately set z(k)|i = z(k?1)|i ifu(k)(i, j) = u(k?1)(i, j) for all j; this check takesO(n) time, and saves us from decoding with the i?thautomaton.
In practice, the updates to u are verysparse, and this condition occurs very often in prac-tice.
Figure 3 demonstrates the utility of this methodfor both sibling automata and G+S automata.7.5 Early StoppingWe also ran experiments varying the value of K?the maximum number of iterations?in the dual de-composition algorithms.
As before, if we do not findy(k) = z(k) for some value of k ?
K, we choosethe y(k) with optimal value for f(y(k)) as the finalsolution.
Figure 4 shows three graphs: 1) the accu-racy of the parser on PTB validation data versus thevalue for K; 2) the percentage of examples wherey(k) = z(k) at some point during the algorithm,hence the algorithm returns a certificate of optimal-ity; 3) the percentage of examples where the solution50607080901000  200  400  600  800  1000PercentageMaximum Number of Dual Decomposition Iterations% validation UAS% certificates% match K=5000Figure 4: The behavior of the dual-decomposition parserwith sibling automata as the value of K is varied.Sib P-Sib G+S P-G+SPTB 92.19 92.34 92.71 92.70PDT 86.41 85.67 87.40 86.43Table 3: UAS of projective and non-projective decodingfor the English (PTB) and Czech (PDT) validation sets.Sib/G+S: as in Table 1.
P-Sib/P-G+S: Projective versionsof Sib/G+S, where the MST component has been re-placed with the Eisner (2000) first-order projective parser.returned is the same as the solution for the algorithmwith K = 5000 (our original setting).
It can be seenfor K as small as 250 we get very similar accuracyto K = 5000 (see Table 2).
In fact, for this set-ting the algorithm returns the same solution as forK = 5000 on 99.59% of the examples.
Howeveronly 89.29% of these solutions are produced with acertificate of optimality (y(k) = z(k)).7.6 How Good is the Approximation z?
?We ran experiments measuring the quality of z?
=argmaxz?Z f(z), where f(z) is given by theperceptron-trained bigram-sibling model.
Becausez?
may not be a well-formed tree with n dependen-cies, we report precision and recall rather than con-ventional dependency accuracy.
Results on the PTBvalidation set were 91.11%/88.95% precision/recall,which is accurate considering the unconstrained na-ture of the predictions.
Thus the z?
approximation isclearly a good one; we suspect that this is one reasonfor the good convergence results for the method.7.7 Importance of Non-Projective DecodingIt is simple to adapt the dual-decomposition algo-rithms in figures 1 and 2 to give projective depen-dency structures: the set Y is redefined to be the set1296of all projective structures, with the argmax over Ybeing calculated using a projective first-order parser(Eisner, 2000).
Table 3 shows results for projec-tive and non-projective parsing using the dual de-composition approach.
For Czech data, where non-projective structures are common, non-projectivedecoding has clear benefits.
In contrast, there is littledifference in accuracy between projective and non-projective decoding on English.8 ConclusionsWe have described dual decomposition algorithmsfor non-projective parsing, which leverage existingdynamic programming and MST algorithms.
Thereare a number of possible areas for future work.
Asdescribed in section 7.7, the algorithms can be easilymodified to consider projective structures by replac-ing Y with the set of projective trees, and then usingfirst-order dependency parsing algorithms in placeof MST decoding.
This method could be used toderive parsing algorithms that include higher-orderfeatures, as an alternative to specialized dynamicprogramming algorithms.
Eisner (2000) describesextensions of head automata to include word senses;we have not discussed this issue in the current pa-per, but it is simple to develop dual decompositionalgorithms for this case, using similar methods tothose used for the grandparent models.
The gen-eral approach should be applicable to other lexical-ized syntactic formalisms, and potentially also to de-coding in syntax-driven translation.
In addition, ourdual decomposition approach is well-suited to paral-lelization.
For example, each of the head-automatacould be optimized independently in a multi-core orGPU architecture.
Finally, our approach could beused with other structured learning algorithms, e.g.Meshi et al (2010).A Implementation DetailsThis appendix describes details of the algorithm,specifically choice of the step sizes ?k, and use ofthe ?
(i, j) parameters.A.1 Choice of Step SizesWe have found the following method to be effec-tive.
First, define ?
= f(z(1)) ?
f(y(1)), where(z(1), y(1)) is the output of the algorithm on the firstiteration (note that we always have ?
?
0 sincef(z(1)) = L(u(1))).
Then define ?k = ?/(1 + ?k),where ?k is the number of times that L(u(k?))
>L(u(k?
?1)) for k?
?
k. Hence the learning rate dropsat a rate of 1/(1+ t), where t is the number of timesthat the dual increases from one iteration to the next.A.2 Use of the ?
(i, j) ParametersThe parsing algorithms both consider a general-ized problem that includes ?
(i, j) parameters.
Wenow describe how these can be useful.
Re-call that the optimization problem is to solveargmaxz?Z,y?Y f(z) + h(y), subject to a set ofagreement constraints.
In our models, f(z) canbe written as f ?
(z) +?i,j ?
(i, j)z(i, j) wheref ?
(z) includes only terms depending on higher-order (non arc-factored features), and ?
(i, j) areweights that consider the dependency between iand j alone.
For any value of 0 ?
?
?1, the problem argmaxz?Z,y?Y f2(z) + h2(y) isequivalent to the original problem, if f2(z) =f ?
(z) + (1 ?
?
)?i,j ?
(i, j)z(i, j) and h2(y) =?
?i,j ?
(i, j)y(i, j).
We have simply shifted the?
(i, j) weights from one model to the other.
Whilethe optimization problem remains the same, the al-gorithms in Figure 1 and 2 will converge at differ-ent rates depending on the value for ?.
In our ex-periments we set ?
= 0.001, which puts almostall the weight in the head-automata models, but al-lows weights on spanning tree edges to break ties inMST inference in a sensible way.
We suspect this isimportant in early iterations of the algorithm, whenmany values for u(i, j) or v(i, j) will be zero, andwhere with ?
= 0 many spanning tree solutions y(k)would be essentially random, leading to very noisyupdates to the u(i, j) and v(i, j) values.
We havenot tested other values for ?.Acknowledgments MIT gratefully acknowledges thesupport of Defense Advanced Research Projects Agency(DARPA) Machine Reading Program under Air Force ResearchLaboratory (AFRL) prime contract no.
FA8750-09-C-0181.Any opinions, findings, and conclusion or recommendations ex-pressed in this material are those of the author(s) and do notnecessarily reflect the view of the DARPA, AFRL, or the USgovernment.
A.
Rush was supported by the GALE program ofthe DARPA, Contract No.
HR0011-06-C-0022.
D. Sontag wassupported by a Google PhD Fellowship.1297ReferencesH.
Alshawi.
1996.
Head Automata and Bilingual Tiling:Translation with Minimal Representations.
In Proc.ACL, pages 167?176.S.
Buchholz and E. Marsi.
2006.
CoNLL-X SharedTask on Multilingual Dependency Parsing.
In Proc.CoNLL, pages 149?164.X.
Carreras.
2007.
Experiments with a Higher-OrderProjective Dependency Parser.
In Proc.
EMNLP-CoNLL, pages 957?961.M.
Collins.
2002.
Discriminative Training Methodsfor Hidden Markov Models: Theory and Experimentswith Perceptron Algorithms.
In Proc.
EMNLP, pages1?8.J.
Duchi, D. Tarlow, G. Elidan, and D. Koller.
2007.
Us-ing Combinatorial Optimization within Max-ProductBelief Propagation.
In NIPS, pages 369?376.J.
Eisner.
2000.
Bilexical grammars and their cubic-time parsing algorithms.
Advances in Probabilisticand Other Parsing Technologies, pages 29?62.T.
Finley and T. Joachims.
2008.
Training structuralsvms when exact inference is intractable.
In ICML,pages 304?311.A.K.
Joshi and Y. Schabes.
1997.
Tree-AdjoiningGrammars.
Handbook of Formal Languages: BeyondWords, 3:69?123.N.
Komodakis, N. Paragios, and G. Tziritas.
2007.
MRFOptimization via Dual Decomposition: Message-Passing Revisited.
In Proc.
ICCV.T.
Koo, A. Globerson, X. Carreras, and M. Collins.
2007.Structured Prediction Models via the Matrix-Tree The-orem.
In Proc.
EMNLP-CoNLL, pages 141?150.B.H.
Korte and J. Vygen.
2008.
Combinatorial Opti-mization: Theory and Algorithms.
Springer Verlag.A.
Kulesza and F. Pereira.
2008.
Structured learningwith approximate inference.
In NIPS.C.
Lemare?chal.
2001.
Lagrangian Relaxation.
In Com-putational Combinatorial Optimization, Optimal orProvably Near-Optimal Solutions [based on a SpringSchool], pages 112?156, London, UK.
Springer-Verlag.A.F.T.
Martins, D. Das, N.A.
Smith, and E.P.
Xing.
2008.Stacking Dependency Parsers.
In Proc.
EMNLP,pages 157?166.A.F.T.
Martins, N.A.
Smith., and E.P.
Xing.
2009.
Con-cise Integer Linear Programming Formulations for De-pendency Parsing.
In Proc.
ACL, pages 342?350.R.
McDonald and F. Pereira.
2006.
Online Learningof Approximate Dependency Parsing Algorithms.
InProc.
EACL, pages 81?88.R.
McDonald and G. Satta.
2007.
On the Complexity ofNon-Projective Data-Driven Dependency Parsing.
InProc.
IWPT.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005.Non-Projective Dependency Parsing using SpanningTree Algorithms.
In Proc.
HLT-EMNLP, pages 523?530.O.
Meshi, D. Sontag, T. Jaakkola, and A. Globerson.2010.
Learning Efficiently with Approximate Infer-ence via Dual Losses.
In Proc.
ICML.A.
Nedic?
and A. Ozdaglar.
2009.
ApproximatePrimal Solutions and Rate Analysis for Dual Sub-gradient Methods.
SIAM Journal on Optimization,19(4):1757?1780.J.
Nivre and R. McDonald.
2008.
Integrating Graph-Based and Transition-Based Dependency Parsers.
InProc.
ACL, pages 950?958.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2005.Learning and Inference over Constrained Output.
InProc.
IJCAI, pages 1124?1129.S.
Riedel and J. Clarke.
2006.
Incremental Integer LinearProgramming for Non-projective Dependency Parsing.In Proc.
EMNLP, pages 129?137.A.M.
Rush, D. Sontag, M. Collins, and T. Jaakkola.2010.
On Dual Decomposition and Linear Program-ming Relaxations for Natural Language Processing.
InProc.
EMNLP.D.A.
Smith and J. Eisner.
2008.
Dependency Parsing byBelief Propagation.
In Proc.
EMNLP, pages 145?156.D.A.
Smith and N.A.
Smith.
2007.
Probabilistic Mod-els of Nonprojective Dependency Trees.
In Proc.EMNLP-CoNLL, pages 132?140.D.
Sontag, T. Meltzer, A. Globerson, T. Jaakkola, andY.
Weiss.
2008.
Tightening LP Relaxations for MAPusing Message Passing.
In Proc.
UAI.M.
Steedman.
2000.
The Syntactic Process.
MIT Press.M.
Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, andJ.
Nivre.
2008.
The CoNLL-2008 Shared Task onJoint Parsing of Syntactic and Semantic Dependencies.In Proc.
CoNLL.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginMarkov networks.
In NIPS.M.
Wainwright, T. Jaakkola, and A. Willsky.
2005.
MAPestimation via agreement on trees: message-passingand linear programming.
In IEEE Transactions on In-formation Theory, volume 51, pages 3697?3717.1298
