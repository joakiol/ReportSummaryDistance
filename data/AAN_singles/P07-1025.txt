Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 192?199,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsGeneralizing Semantic Role AnnotationsAcross Syntactically Similar VerbsAndrew S. Gordon Reid SwansonInstitute for Creative Technologies Institute for Creative TechnologiesUniversity of Southern California University of Southern CaliforniaMarina del Rey, CA 90292 USA Marina del Rey, CA 90292 USAgordon@ict.usc.edu swansonr@ict.usc.eduAbstractLarge corpora of parsed sentences withsemantic role labels (e.g.
PropBank) pro-vide training data for use in the creationof high-performance automatic semanticrole labeling systems.
Despite the size ofthese corpora, individual verbs (or role-sets) often have only a handful of in-stances in these corpora, and only afraction of English verbs have even a sin-gle annotation.
In this paper, we describean approach for dealing with this sparsedata problem, enabling accurate semanticrole labeling for novel verbs (rolesets)with only a single training example.
Ourapproach involves the identification ofsyntactically similar verbs found in Prop-Bank, the alignment of arguments in theircorresponding rolesets, and the use oftheir corresponding annotations in Prop-Bank as surrogate training data.1 Generalizing Semantic Role AnnotationsA recent release of the PropBank (Palmer et al,2005) corpus of semantic role annotations of Tree-bank parses contained 112,917 labeled instances of4,250 rolesets corresponding to 3,257 verbs, asillustrated by this example for the verb buy.
[arg0 Chuck] [buy.01 bought] [arg1 a car] [arg2 fromJerry] [arg3 for $1000].Annotations similar to these have been used to cre-ate automated semantic role labeling systems(Pradhan et al, 2005; Moschitti et al, 2006) foruse in natural language processing applications thatrequire only shallow semantic parsing.
As with allmachine-learning approaches, the performance ofthese systems is heavily dependent on the avail-ability of adequate amounts of training data.
How-ever, the number of annotated instances inPropBank varies greatly from verb to verb; thereare 617 annotations for the want roleset, only 7 fordesire, and 0 for any sense of the verb yearn.
Dowe need to keep annotating larger and larger cor-pora in order to generate accurate semantic label-ing systems for verbs like yearn?A better approach may be to generalize the datathat exists already to handle novel verbs.
It is rea-sonable to suppose that there must be a number ofverbs within the PropBank corpus that behavenearly exactly like yearn in the way that they relateto their constituent arguments.
Rather than annotat-ing new sentences that contain the verb yearn, wecould simply find these similar verbs and use theirannotations as surrogate training data.This paper describes an approach to generalizingsemantic role annotations across different verbs,involving two distinct steps.
The first step is toorder all of the verbs with semantic role annota-tions according to their syntactic similarity to thetarget verb, followed by the second step of aligningargument labels between different rolesets.
Toevaluate this approach we developed a simpleautomated semantic role labeling algorithm basedon the frequency of parse-tree paths, and thencompared its performance when using real and sur-rogate training data from PropBank.1922 Parse Tree PathsA key concept in understanding our approach toboth automated semantic role annotation and gen-eralization is the notion of a parse tree path.
Parsetree paths were used for semantic role labeling byGildea and Jurafsky (2002) as descriptive featuresof the syntactic relationship between predicatesand their arguments in the parse tree of a sentence.Predicates are typically assumed to be specific tar-get words (verbs), and arguments are assumed tobe spans of words in the sentence that are domi-nated by nodes in the parse tree.
A parse tree pathcan be described as a sequence of transitions upfrom the target word then down to the node thatdominates the argument span (e.g.
Figure 1).Figure 1: An example parse tree path from thepredicate ate to the argument NP He, representedas VBVPSNPParse tree paths are particularly interesting forautomated semantic role labeling because theygeneralize well across syntactically similar sen-tences.
For example, the parse tree path in Figure 1would still correctly identify the ?eater?
argumentin the given sentence if the personal pronoun ?he?were swapped with a markedly different nounphrase, e.g.
?the attendees of the annual holidaybreakfast.
?3 A Simple Semantic Role LabelerTo explore issues surrounding the generalization ofsemantic role annotations across verbs, we beganby authoring a simple automated semantic role la-beling algorithm that assigns labels according tothe frequency of the parse tree paths seen in train-ing data.
To construct a labeler for a specific role-set, training data consisting of parsed sentenceswith role-labeled parse tree constituents are ana-lyzed to identify all of the parse tree paths betweenpredicates and arguments, which are then tabulatedand sorted by frequency.
For example, Table 1 liststhe 10 most frequent pairs of arguments and parsetree paths for the want.01 roleset in a recent releaseof PropBank.Count Argument Parse tree path189 ARG0 VBPVPSNP159 ARG1 VBPVPS125 ARG0 VBZVPSNP110 ARG1 VBZVPS102 ARG0 VBVPVPSNP98 ARG1 VBVPS96 ARG0 VBDVPSNP79 ARGM VBVPVPRB76 ARG1 VBDVPS43 ARG1 VBPVPNPTable 1.
Top 10 most frequent parse tree paths forarguments of the PropBank want.01 roleset, basedon 617 annotationsTo automatically assign role labels to an unla-beled parse tree, each entry in the table is consid-ered in order of highest frequency.
Beginning fromthe target word in the sentence (e.g.
wants) a checkis made to determine if the entry includes a possi-ble parse tree path in the parse tree of the sentence.If so, then the constituent is assigned the role labelof the entry, and all subsequent entries in the tablethat have the same argument label or lead to sub-constituents of the labeled node are invalidated.Only subsequent entries that assign core argumentsof the roleset (e.g.
ARG0, ARG1) are invalidated,allowing for multiple assignments of non-core la-bels (e.g.
ARGM) to a test sentence.
In caseswhere the path leads to more than one node in asentence, the leftmost path is selected.
This processthen continues down the list of valid table entries,assigning additional labels to unlabeled parse treeconstituents, until the end of the table is reached.This approach also offers a simple means ofdealing with multiple-constituent arguments,which occasionally appear in PropBank data.
Inthese cases, the data is listed as unique entries inthe frequency table, where each of the parse treepaths to the multiple constituents are listed as a set.The labeling algorithm will assign the argument ofthe entry only if all parse tree paths in the set arepresent in the sentence.The expected performance of this approach tosemantic role labeling was evaluated using thePropBank data using a leave-one-out cross-validation experimental design.
Precision and re-call scores were calculated for each of the 3,086193rolesets with at least two annotations.
Figure 2graphs the average precision, recall, and F-scorefor rolesets according to the number of trainingexamples of the roleset in the PropBank corpus.An additional curve in Figure 2 plots the percent-age of these PropBank rolesets that have the givenamount of training data or more.
For example, F-scores above 0.7 are first reached with 62 trainingexamples, but only 8% of PropBank rolesets havethis much training data available.Figure 2.
Performance of our semantic role label-ing approach on PropBank rolesets4 Identifying Syntactically Similar VerbsA key part of generalizing semantic role annota-tions is to calculate the syntactic similarity be-tween verbs.
The expectation here is that verbs thatappear in syntactically similar contexts are goingto behave similarly in the way that they relate totheir arguments.
In this section we describe a fullyautomated approach to calculating the syntacticsimilarity between verbs.Our approach is strictly empirical; the similarityof verbs is determined by examining the syntacticcontexts in which they appear in a large text cor-pus.
Our approach is analogous to previous workin extracting collocations from large text corporausing syntactic information (Lin, 1998).
In ourwork, we utilized the GigaWord corpus of Englishnewswire text (Linguistic Data Consortium, 2003),consisting of nearly 12 gigabytes of textual data.To prepare this corpus for analysis, we extractedthe body text from each of the 4.1 million entriesin the corpus and applied a maximum-entropy al-gorithm to identify sentence boundaries (Reynarand Ratnaparkhi, 1997).Next we executed a four-step analysis processfor each of the 3,257 verbs in the PropBank cor-pus.
In the first step, we identified each of the sen-tences in the prepared GigaWord corpus thatcontained any inflection of the given verb.
Toautomatically identify all verb inflections, we util-ized the English DELA electronic dictionary(Courtois, 2004), which contained all but 21 of thePropBank verbs (for which we provided the inflec-tions ourselves), with old-English verb inflectionsremoved.
We extracted GigaWord sentences con-taining these inflections by using the GNU grepprogram and a template regular expression for eachinflection list.
The results of these searches werecollected in 3,257 files (one for each verb).
Thelargest of these files was for inflections of the verbsay (15.9 million sentences), and the smallest wasfor the verb namedrop (4 sentences).The second step was to automatically generatesyntactic parse trees for the GigaWord sentencesfound for each verb.
It was our original intention toparse all of the found sentences, but we found thatthe slow speed of contemporary syntactic parsersmade this impractical.
Instead, we focused our ef-forts on the first 100 sentences found for each ofthe 3,257 verbs with 100 or fewer tokens: a total of324,461 sentences (average of 99.6 per verb).
Forthis task we utilized the August 2005 release of theCharniak parser with the default speed/accuracysettings (Charniak, 2000), which required roughly360 hours of processor time on a 2.5 GHzPowerPC G5.The third step was to characterize the syntacticcontext of the verbs based on where they appearedwithin the parse trees.
For this purpose, we utilizedparse tree paths as a means of converting treestructures into a flat, feature-vector representation.For each sentence, we identified all possible parsetree paths that begin from the verb inflection andterminate at a constituent that does not include theverb inflection.
For example, the syntactic contextof the verb in Figure 1 can be described by the fol-lowing five parse tree paths:1.
VBVPSNP2.
VBVPSNPPRP3.
VBVPNP4.
VBVPNPDT5.
VBVPNPNNPossible parse tree paths were identified forevery parsed sentence for a given verb, and thefrequencies of each unique path were tabulated194into a feature vector representation.
Parse treepaths where the first node was not a Treebank part-of-speech tag for a verb were discarded, effectivelyfiltering the non-verb homonyms of the set of in-flections.
The resulting feature vectors were nor-malized by dividing the values of each feature bythe number of verb instances used to generate theparse tree paths; the value of each feature indicatesthe proportion of observed inflections in which theparse tree path is possible.
As a representative ex-ample, 95 verb forms of abandon were found inthe first 100 GigaWord sentences containing anyinflection of this verb.
For this verb, 4,472 possibleparse tree paths were tabulated into 3,145 uniquefeatures, 2501 of which occurred only once.The fourth step was to compute the distance be-tween a given verb and each of the 3,257 featurevector representations describing the syntactic con-text of PropBank verbs.
We computed and com-pared the performance of a wide variety of possiblevector-based distance metrics, including Euclidean,Manhattan, and Chi-square (with un-normalizedfrequency counts), but found that the ubiquitouscosine measure was least sensitive to variations insample size between verbs.
To facilitate a com-parative performance evaluation (section 6), pair-wise cosine distance measures were calculatedbetween each pair of PropBank verbs and sortedinto individual files, producing 3,257 lists of 3,257verbs ordered by similarity.Table 2 lists the 25 most syntactically similarpairs of verbs among all PropBank verbs.
Thereare a number of notable observations in this list.First is the extremely high similarity between bindand bound.
This is partly due to the fact that theyshare an inflection (bound is the irregular pasttense form of bind), so the first 100 instances ofGigaWord sentences for each verb overlap signifi-cantly, resulting in overlapping feature vector rep-resentations.
Although this problem appears to berestricted to this one pair of verbs, it could beavoided in the future by using the part-of-speechtag in the parse tree to help distinguish betweenverb lemmas.A second observation of Table 2 is that severalverbs appear multiple times in this list, yieldingsets of verbs that all have high syntactic similarity.Three of these sets account for 19 of the verbs inthis list:1. plunge, tumble, dive, jump, fall, fell, dip2.
assail, chide, lambaste3.
buffet, embroil, lock, superimpose, whip-saw, pluck, whisk, mar, ensconceThe appearance of these sets suggests that ourmethod of computing syntactic similarity could beused to identify distinct clusters of verbs that be-have in very similar ways.
In future work, it wouldbe particularly interesting to compare empirically-derived verb clusters to verb classes derived fromtheoretical considerations (Levin, 1993), and to theautomated verb classification techniques that usethese classes (Joanis and Stevenson, 2003).A third observation of Table 2 is that the verbpairs with the highest syntactic similarity are oftensynonyms, e.g.
the cluster of assail, chide, andlambaste.
As a striking example, the 14 most syn-tactically similar verbs to believe (in order) arethink, guess, hope, feel, wonder, theorize, fear,reckon, contend, suppose, understand, know,doubt, and suggest ?
all mental action verbs.
Thisobservation further supports the distributional hy-pothesis of word similarity and correspondingtechnologies for identifying synonyms by similar-ity of lexical-syntactic context (Lin, 1998).Verb pairs (instances) Cosinebind (83) bound (95) 0.950plunge (94) tumble (87) 0.888dive (36) plunge (94) 0.867dive (36) tumble (87) 0.866jump (79) tumble (87) 0.865fall (84) fell (102) 0.859intersperse (99) perch (81) 0.859assail (100) chide (98) 0.859dip (81) fell (102) 0.858buffet (72) embroil (100) 0.856embroil (100) lock (73) 0.856embroil (100) superimpose (100) 0.856fell (102) jump (79) 0.855fell (102) tumble (87) 0.855embroil (100) whipsaw (63) 0.850pluck (100) whisk (99) 0.849acquit (100) hospitalize (99) 0.849disincline (70) obligate (94) 0.848jump (79) plunge (94) 0.848dive (36) jump (79) 0.847assail (100) lambaste (100) 0.847festoon (98) strew (100) 0.846mar (78) whipsaw (63) 0.846pluck (100) whipsaw (63) 0.846ensconce (101) whipsaw (63) 0.845Table 2.
Top 25 most syntactically similar pairs ofthe 3257 verbs in PropBank.
Each verb is listedwith the number of inflection instances used tocalculate the cosine measurement.1955 Aligning Arguments Across RolesetsThe second key aspect of our approach to general-izing annotations is to make mappings between theargument roles of the novel target verb and theroles used for a given roleset in the PropBank cor-pus.
For example, if we?d like to apply the trainingdata for a roleset of the verb desire in PropBank toa novel roleset for the verb yearn, we need to knowthat the desirer corresponds to the yearner, the de-sired to the yearned-for, etc.
In this section, wedescribe an approach to argument alignment thatinvolves the application of the semantic role label-ing approach described in section 3 to a singletraining example for the target verb.To simplify the process of aligning argument la-bels across rolesets, we make a number of assump-tions.
First, we only consider cases where tworolesets have exactly the same number of argu-ments.
The version of the PropBank corpus that weused in this research contained 4250 rolesets, eachwith 6 or fewer roles (typically two or three).
Ac-cordingly, when attempting to apply PropBankdata to a novel roleset with a given argument count(e.g.
two), we only consider the subset of Prop-Bank data that labels rolesets with exactly the samecount.Second, our approach requires at least one fully-annotated training example for the target roleset.
Afully-annotated sentence is one that contains a la-beled constituent in its parse tree for each role inthe roleset.
As an illustration, the example sentencein section 1 (for the roleset buy.01) would not beconsidered a fully-annotated training example, asonly four of the five arguments of the PropBankbuy.01 roleset are present in the sentence (it ismissing a benefactor, as in ?Chuck bought hismother a car from Jerry for $1000?
).In both of these simplifying requirements, weignore role labels that may be assigned to a sen-tence but that are not defined as part of the roleset,specifically the ARGM labels used in PropBank tolabel standard proposition modifiers (e.g.
location,time, manner).Our approach begins with a list of verbs orderedby their calculated syntactic similarity to the targetverb, as described in section 4 of this paper.
Wesubsequently apply two steps that transform thislist into an ordered set of rolesets that can bealigned with the roles used in one or more fully-annotated training examples of the target verb.
Indescribing these two steps, we use instigate as anexample target verb.
Instigate already appears inthe PropBank corpus as a two-argument roleset,but it has only a single training example:[arg0 The Mahatma, or "great souled one,"][instigate.01 instigated] [arg1 several campaigns ofpassive resistance against the Britishgovernment in India].The syntactic similarity of instigate to all Prop-Bank verbs was calculated in the manner describedin the previous section.
This resulting list of 3,180entries begins with the following fourteen verbs:orchestrate, misrepresent, summarize, wreak, rub,chase, refuse, embezzle, harass, spew, thrash, un-earth, snub, and erect.The first step is to replace each of the verbs inthe ordered list with corresponding rolesets fromPropBank that have the same number of roles asthe target verb.
As an example, our target rolesetfor the verb instigate has two arguments, so eachverb in the ordered list is replaced with the set ofcorresponding rolesets that also have two argu-ments, or removed if no two-argument rolesetsexist for the verb in the PropBank corpus.
The or-dered list of verbs for instigate is transformed intoan ordered list of 2,115 rolesets with two argu-ments, beginning with the following five entries:orchestrate.01, chase.01, unearth.01, snub.01, anderect.01.The second step is to identify the alignments be-tween the arguments of the target roleset and eachof the rolesets in the ordered list.
Beginning withthe first roleset on the list (e.g.
orchestrate.01), webuild a semantic role labeler (as described in sec-tion 3) using its available training annotations fromthe PropPank corpus.
We then apply this labeler tothe single, fully-annotated example sentence forthe target verb, treating it as if it were a test exam-ple of the same roleset.
We then check to see if anyof the core (numbered) role labels overlap with theannotations that are provided.
In cases where anannotated constituent of the target test sentence isassigned a label from the source roleset, then theroleset mappings are noted along with the entry inthe ordered list.
If no mappings are found, the role-set is removed from the ordered list.For example, the roleset for orchestrate.01 con-tains two arguments (ARG0 and ARG1) that corre-spond to the ?conductor, manager?
and the ?things196being coordinated or managed?.
This roleset isused for only three sentence annotations in thePropBank corpus.
Using these annotations as train-ing data, we build a semantic role labeler for thisroleset and apply it to the annotated sentence forinstigate.01, treating it as if it were a test sentencefor the roleset orchestrate.01.
The labeler assignsthe orchestrate.01 label ARG1 to the same con-stituent labeled ARG1 in the test sentence, but failsto assign a label to the other argument constituentin the test sentence.
Therefore, a single mapping isrecorded in the ordered list of rolesets, namely thatARG1 of orchestrate.01 can be mapped to ARG1of instigate.01.After all of the rolesets are considered, we areleft with a filtered list of rolesets with their argu-ment mappings, ordered by their syntactic similar-ity to the target verb.
For the roleset instigate.01,this list consists of 789 entries, beginning with thefollowing 5 mappings.1.
orchestrate.01, 1:12. chase.01, 0:0, 1:13. unearth.01, 0:0, 1:14. snub.01, 1:15. erect.01, 0:0, 1:1Given this list, arbitrary amounts of PropBankannotations can be used as surrogate training datafor the instigate.01 roleset, beginning at the top ofthe list.
To utilize surrogate training data in oursemantic role labeling approach (Section 3), wecombine parse tree path information for a selectedportion of surrogate training data into a single listsorted by frequency, and apply these files to testsentences as normal.Although we use an existing PropBank roleset(instigate.01) as an example in this section, thisapproach will work for any novel roleset whereone fully-annotated training example is available.For example, arbitrary amounts of surrogate Prop-Bank data can be found for the novel verb yearn by1) searching for sentences with the verb yearn inthe GigaWord corpus, 2) calculating the syntacticsimilarity between yearn and all PropBank verbsas described in Section 4, 3) aligning the argu-ments in a single fully-annotated example of yearnwith ProbBank rolesets with the same number ofarguments using the method described in this sec-tion, and 4) selecting arbitrary amounts of Prop-Bank annotations to use as surrogate training data,starting from the top of the resulting list.6 EvaluationWe conducted a large-scale evaluation to deter-mine the performance of our semantic role labelingalgorithm when using variable amounts of surro-gate training data, and compared these results tothe performance that could be obtained using vari-ous amounts of real training data (as described insection 3).
Our hypothesis was that learning-curvesfor surrogate-trained labelers would be somewhatless steep, but that the availability of large-amountsof surrogate training data would more than makeup for the gap.To test this hypothesis, we conducted an evalua-tion using the PropBank corpus as our testing dataas well as our source for surrogate training data.
Asdescribed in section 5, our approach requires theavailability of at least one fully-annotated sentencefor a given roleset.
Only 28.5% of the PropBankannotations assign labels for each of the numberedarguments in their given roleset, and only 2,858 ofthe 4,250 rolesets used in PropBank annotations(66.5%) have at least one fully-annotated sentence.Of these, 2,807 rolesets were for verbs that ap-peared at least once in our analysis of the Giga-Word corpus (Section 4).
Accordingly, weevaluated our approach using the annotations forthis set of 2,807 rolesets as test data.
For each ofthese rolesets, various amounts of surrogate train-ing data were gathered from all 4,250 rolesets rep-resented in PropBank, leaving out the data forwhichever roleset was being tested.For each of the target 2,807 rolesets, we gener-ated a list of semantic role mappings ordered bysyntactic similarity, using the methods described insections 4 and 5.
In aligning arguments, only a sin-gle training example from the target roleset wasused, namely the first annotation within the Prop-Bank corpus where all of the rolesets argumentswere assigned.
Our approach failed to identify anyargument mappings for 41 of the target rolesets,leaving them without any surrogate training data toutilize.
Of the remaining 2,766 rolesets, the num-ber of mapped rolesets for a given target rangedfrom 1,041 to 1 (mean = 608, stdev = 297).For each of the 2,766 target rolesets with aligna-ble roles, we gathered increasingly larger amountsof surrogate training data by descending the or-dered list of mappings translating the PropBankdata for each entry according to its argument map-pings.
Then each of these incrementally larger sets197of training data was then used to build a semanticrole labeler as described in section 3.
The perform-ance of each of the resulting labelers was thenevaluated by applying it to all of the test dataavailable for target roleset in PropBank, using thesame scoring methods described in section 3.
Theperformance scores for each labeler were recordedalong with the total number of surrogate trainingexamples used to build the labeler.Figure 3 presents the performance result of oursemantic role labeling approach using variousamounts of surrogate training data.
Along withprecision, recall, and F-score data, Figure 3 alsographs the percentage of PropBank rolesets forwhich a given amount of training data had beenidentified using our approach, of the 2,858 rolesetswith at least one fully-annotated training example.For instance, with 120 surrogate annotations oursystem achieves an F-score above 0.5, and weidentified this much surrogate training data for96% of PropBank rolesets with at least one fully-annotated sentence.
This represents 64% of allPropBank rolesets that are used for annotation.Beyond 120 surrogate training examples, F-scoresremain around 0.6 before slowly declining afteraround 700 examples.Figure 3.
Performance of our semantic role label-ing approach on PropBank rolesets using variousamounts of surrogate training dataSeveral interesting comparisons can be made be-tween the results presented in Figure 3 and those inFigure 2, where actual PropBank training data isused instead of surrogate training data.
First, theprecision obtained with surrogate training data isroughly 10% lower than with real data.
Second, therecall performance of surrogate data performssimilar to real data at first, but is consistently 10%lower than with real data after the first 50 trainingexamples.
Accordingly, F-scores for surrogatetraining data are 10% lower overall.Even though the performance obtained usingsurrogate training data is less than with actual data,there is abundant amounts of it available for mostPropBank rolesets.
Comparing the ?% of rolesets?plots in Figures 2 and 3, the real value of surrogatetraining data is apparent.
Figure 2 suggests thatover 20 real training examples are needed toachieve F-scores that are consistently above 0.5,but that less than 20% of PropBank rolesets havethis much data available.
In contrast, 64% of allPropBank rolesets can achieve this F-score per-formance with the use of surrogate training data.This percentage increases to 96% if every Prop-Bank roleset is given at least one fully annotatedsentence, where all of its numbered arguments areassigned to constituents.In addition to supplementing the real trainingdata available for existing PropBank rolesets, theseresults predict the labeling performance that can beobtained by applying this technique to a novelroleset with one fully-annotated training example,e.g.
for the verb yearn.
Using the first 120 surro-gate training examples and our simple semanticrole labeling approach, we would expect F-scoresthat are above 0.5, and that using the first 700would yield F-scores around 0.6.7 DiscussionThe overall performance of our semantic role la-beling approach is not competitive with leadingcontemporary systems, which typically employsupport vector machine learning algorithms withsyntactic features (Pradhan et al, 2005) or syntac-tic tree kernels (Moschitti et al, 2006).
However,our work highlights a number of characteristics ofthe semantic role labeling task that will be helpfulin improving performance in future systems.
Parsetree paths features can be used to achieve high pre-cision in semantic role labeling, but much of thisprecision may be specific to individual verbs.
Bygeneralizing parse tree path features only acrosssyntactically similar verbs, we have shown that thedrop in precision can be limited to roughly 10%.The approach that we describe in this paper isnot dependent on the use of PropBank rolesets; anylarge corpus of semantic role annotations could be198generalized in this manner.
In particular, our ap-proach would be applicable to corpora with frame-specific role labels, e.g.
FrameNet (Baker et al,1998).
Likewise, our approach to generalizingparse tree path feature across syntactically similarverbs may improve the performance of automatedsemantic role labeling systems based on FrameNetdata.
Our work suggests that feature generalizationbased on verb-similarity may compliment ap-proaches to generalization based on role-similarity(Gildea and Jurafsky, 2002; Baldewein et al,2004).There are a number of improvements that couldbe made to the approach described in this paper.Enhancements to the simple semantic role labelingalgorithm would improve the alignment of argu-ments across rolesets, which would help align role-sets with greater syntactic similarity, as well asimprove the performance obtained using the surro-gate training data in assigning semantic roles.This research raises many questions about therelationship between syntactic context and verbsemantics.
An important area for future researchwill be to explore the correlation between our dis-tance metric for syntactic similarity and variousquantitative measures of semantic similarity(Pedersen, et al, 2004).
Particularly interestingwould be to explore whether different senses of agiven verb exhibited markedly different profiles ofsyntactic context.
A strong syntactic/semantic cor-relation would suggest that further gains in the useof surrogate annotation data could be gained if syn-tactic similarity was computed between rolesetsrather than their verbs.
However, this would firstrequire accurate word-sense disambiguation bothfor the test sentences as well as for the parsed cor-pora used to calculate parse tree path frequencies.Alternatively, parse tree path profiles associatedwith rolesets may be useful for word sense disam-biguation, where the probability of a sense is com-puted as the likelihood that an ambiguous verb'sparse tree paths are sampled from the distributionsassociated with each verb sense.
These topics willbe the focus of our future work in this area.AcknowledgmentsThe project or effort depicted was or is sponsoredby the U.S. Army Research, Development, andEngineering Command (RDECOM), and that thecontent or information does not necessarily reflectthe position or the policy of the Government, andno official endorsement should be inferred.ReferencesBaker, C., Fillmore, C., and Lowe, J.
1998.
The Ber-keley FrameNet Project, In Proceedings of COLING-ACL, Montreal.Baldewein, U., Erk, K., Pado, S., and Prescher, D. 2004.Semantic role labeling with similarity-based gener-alization using EM-based clustering.
Proceedings ofSenseval-3, Barcelona.Charniak, E. 2000.
A maximum-entropy-inspiredparser, Proceedings NAACL-ANLP, Seattle.Courtois, B.
2004.
Dictionnaires ?lectroniques DELAFanglais et fran?ais.
In C. Lecl?re, E. Laporte, M. Piotand M. Silberztein (eds.)
Syntax, Lexis and Lexicon-Grammar: Papers in Honour of Maurice Gross.
Am-sterdam: John Benjamins.Gildea, D. and Jurafsky, D. 2002.
Automatic Labelingof Semantic Roles.
Computational Linguistics 28:3,245-288.Joanis, E. and Stevenson, S. 2003.
A general featurespace for automatic verb classification.
ProceedingsEACL, Budapest.Levin, B.
1993.
English Verb Classes and Alterna-tions:A Preliminary Investigation.
Chicago, IL: Universityof Chicago Press.Lin, D. 1998.
Automatic Retrieval and Clustering ofSimilar Words.
COLING-ACL, Montreal.Linguistic Data Consortium.
2003.
English Gigaword.Catalog number LDC2003T05.
Available from LDCat http://www.ldc.upenn.edu.Moschitti, A., Pighin, D. and Basili, R. 2006.
SemanticRole Labeling via Tree Kernel joint inference.
Pro-ceedings of CoNLL, New York.Palmer, M., Gildea, D., and Kingsbury, P. 2005.
TheProposition Bank: An Annotated Corpus of SemanticRoles.
Computational Linguistics 31(1):71-106.Pedersen, T., Patwardhan, S. and Michelizzi, J.
2004.WordNet::Similarity - Measuring the Relatedness ofConcepts.
Proceedings NAACL-04, Boston, MA.Pradhan, S., Ward, W., Hacioglu, K., Martin, J., andJurafsky, D. 2005.
Semantic role labeling using dif-ferent syntactic views.
Proceedings ACL-2005, AnnArbor, MI.Reynar, J. and Ratnaparkhi, A.
1997.
A Maximum En-tropy Approach to Identifying Sentence Boundaries.Proceedings of ANLP, Washington, D.C.199
