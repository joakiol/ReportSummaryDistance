Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 190?199,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsEvaluating a Semantic Network Automatically Constructed from LexicalCo-occurrence on a Word Sense Disambiguation TaskSean SzumlanskiDepartment of EECSUniversity of Central Floridaseansz@cs.ucf.eduFernando GomezDepartment of EECSUniversity of Central Floridagomez@eecs.ucf.eduAbstractWe describe the extension and objective eval-uation of a network1 of semantically relatednoun senses (or concepts) that has been au-tomatically acquired by analyzing lexical co-occurrence in Wikipedia.
The acquisition pro-cess makes no use of the metadata or linksthat have been manually built into the ency-clopedia, and nouns in the network are auto-matically disambiguated to their correspond-ing noun senses without supervision.
Forthis task, we use the noun sense inventory ofWordNet 3.0.
Thus, this work can be con-ceived of as augmenting the WordNet nounontologywith unweighted, undirected related-to edges between synsets.
Our network con-tains 208,832 such edges.We evaluate our network?s performance on aword sense disambiguation (WSD) task andshow: a) the network is competitive withWordNet when used as a stand-alone knowl-edge source for two WSD algorithms; b) com-bining our network with WordNet achievesdisambiguation results that exceed the perfor-mance of either resource individually; and c)our network outperforms a similar resourcethat has been automatically derived from se-mantic annotations in the Wikipedia corpus.1 IntroductionA growing interest in using semantic relatedness inword sense disambiguation (WSD) tasks has spurredinvestigations into the limitations of the WordNetontology (Fellbaum, 1998) for this purpose.
Al-though WordNet comprises a rich set of semantic1http://www.cs.ucf.edu/?
seansz/semlinks between word senses (or concepts), indicat-ing semantic similarity through subsumptive hyper-nymic and hyponymic relations (among others), itlacks a general indication of semantic relatedness.We present a semantic network that is automat-ically acquired from lexical co-occurrence in Wi-kipedia, and indicates general semantic relatednessbetween noun senses in WordNet 3.0.
In this work,the discovery of relatedness is a context-sparse affairthat takes place in absentia of the semantic annota-tions of Wikipedia, such as inter-article links, entriesin disambiguation pages, the title of the article fromwhich a sentence is extracted, and so on.We released an earlier version of such a networkthat was limited by the fact that only relationshipsinvolving at least one monosemous noun had beenincluded, and it was not evaluated on a WSD task(Szumlanski and Gomez, 2010).In contrast, the network we present here has relat-edness data for over 4,500 polysemous noun targetsand 3,000 monosemous noun targets, each of whichare related to an average of 27.5 distinct noun senses.It consists of 208,832 undirected edges ?
a 181% in-crease in size over the previous network.
The resultis a semantic network that has reached maturity and,as we will show, can be successfully applied to aWSD task.This paper proceeds as follows.
In the next sec-tion (Section 2), we discuss related work.
We thengive an overview of the method we use to con-struct our network (Sections 3 and 4).
The networkis evaluated through its application to a WSD task(Sections 5?7), where we compare its performanceto WordNet and another automatically acquired se-mantic network called WordNet++ (Ponzetto andNavigli, 2010).
A discussion follows (Section 8),190and we present our conclusions in Section 9.2 Related WorkOur work bears strong relation to WordNet++(henceforth WN++), which is constructed automat-ically from the semantic annotations in Wikipedia(Ponzetto and Navigli, 2010).2 Links in WN++ areestablished between words whose articles link to oneanother.
For example, the article on astronomy inWikipedia links to the article on celestial naviga-tion, so we find an edge from astronomy#n#1 tocelestial navigation#n#1 in WN++.3 The nouns re-lated in WN++ are disambiguated automatically us-ing further semantic annotation data from Wikipe-dia, including sense labels, the titles of other pageslinked to by any two related nouns, and the folk-sonomic categories to which articles belong.
Theseserve as context words that are compared with con-text words from various WordNet relations in or-der to map the nouns to their appropriate WordNetsenses.
The resulting resource contains 1,902,859unique edges between noun senses.Augmenting the structure of Wikipedia itself hasbeen the subject of research as well, and involvesthe discovery of relations between articles.
Mihal-cea and Csomai (2007), for example, added linksbetween Wikipedia pages after automatically iden-tifying keywords in each article and disambiguatingthose words to their appropriate Wikipedia concepts(article titles), while Ponzetto and Navigli (2009)used graph theoretic approaches to augment the tax-onomic organization of Wikipedia articles.In terms of automatically discovering semantic re-lations, many pattern-based approaches have beenused to extract specific types of relations from largecorpora, e.g., hyponymy, meronymy, and synonymy(Hearst, 1992; Pantel and Pennacchiotti, 2006).Approaches based on distributional similarity havebeen applied toward the same end (Harris, 1985;Gorman and Curran, 2006), and there are sev-eral approaches that rely on the underlying struc-ture of WordNet or Wikipedia to measure the re-latedness between two concepts or nouns quantita-tively (Hughes and Ramage, 2007; Gabrilovich and2http://lcl.uniroma1.it/wordnetplusplus3The notation astronomy#n#1 refers to sense 1 (#1) of thenoun (#n) ?astronomy?
in WordNet.
Other parts of speech aredenoted by #v (verbs), #a (adjectives), or #r (adverbs).Markovitch, 2007; Zaragoza et al, 2007; Patward-han and Pedersen, 2006; Strube and Ponzetto, 2006;Budanitsky and Hirst, 2006; Resnik, 1995).Other quantitative approaches have leveraged thelarge amounts of data available on the Web to dis-cover relatedness.
Notably, Agirre and de Lacalle(2004) employed web queries to associate WordNetsynsets with representative context words, known astopic signatures.
Cuadros and Rigau (2008) haveused these data to construct four KnowNets, seman-tic knowledge bases derived by disambiguating thetop 5, 10, 15, and 20 nouns, respectively, from thetopic signatures of Agirre and de Lacalle.3 Automatic Acquisition of the SemanticNetworkThe semantic network is automatically acquired inthree distinct stages (Szumlanski and Gomez, 2010):(1) quantitative measurement of relatedness betweennouns that co-occur in a large corpus; (2) categori-cal determination of whether the quantitative mea-sure indicates strong and mutual semantic related-ness between a given pair of nouns; and (3) unsuper-vised disambiguation of all the nouns that are foundto be semantically related.
We provide an overviewof each of these steps below (Sections 3.1?3.3), andthen discuss how we have expanded this method-ology to create a more complete semantic network(Section 4).3.1 Quantitatively measuring relatedness fromlexical co-occurrenceWe first measure the semantic relatedness, or re-lational strength, of a target, t, to one of its co-occurring nouns, or co-targets, c, with the followingasymmetric function:Srel(t, c) = P (t|c)P (c|t)logP (c|t)P (c)where P (c|t) is the relative frequency of c among allnouns co-occurring with t, and vice versa for P (t|c).P (c) is the relative frequency of c among all nounsoccurring in the corpus.
For these values, we rely onlexical co-occurrence data extracted from Wikipe-dia.
Co-occurrence is considered intra-sententially(as opposed to co-occurrence in entire articles orparagraphs, or co-occurrence within variable-sizedwindows of context).191This function essentially measures the degree towhich an occurrence of t in a sentence predicts theco-occurrence of c. It is an adaptation of Resnik?s(1999) selectional association measure.Table 1 shows the results of applying this functionto the co-targets of ?yoga?
and ?meditation.
?Target (t): yoga Target (t): meditationCo-target (c) Srel Co-target (c) Srelhatha yoga .1801 yoga .0707asana .0761 mindfulness .0415meditation .0673 contemplation .0165bhakti .0508 prayer .0139raja .0410 practice .0068tantra .0148 technique .0060yogi .0132 mantra .0053karma .0125 relaxation .0048posture .0104 retreat .0047aerobics .0093 enlightenment .0031tai chi .0089 monk .0025exercise .0036 posture .0024practice .0032 breathing .0017instructor .0031 - - - - - - - - - - - - -- - - - - - - - - - - - - exercise .0015guru .0027 teaching .0014massage .0026 practitioner .0014exercise .0019 ascetic .0014............Table 1: The most strongly related co-targets of ?yoga?and ?meditation,?
sorted by decreasing value of relationalstrength (Srel).
Nouns above dashed lines are the top 5%of the target?s most strongly related co-targets.3.2 Establishing categorical relatednessWe then use a mutual relatedness algorithm to as-certain whether two nouns are semantically relatedby determining whether the nouns under considera-tion reciprocate a high degree of relatedness to oneanother.
It proceeds as follows:For some target noun of interest, t, let Cx(t) bethe set of the top x% of t?s co-targets as sorted bySrel(t, c).
For each c ?
Cx(t), if we have t ?
Cx(c),then we say that t and c are categorically related andadd the noun pair (t, c) to our semantic network.
Wethen increment x by one and repeat the process: forevery c ?
Cx+1(t) such that (t, c) is not already inour network, we look for t in Cx+1(c), and add (t, c)to our network if we find it.
This process continuesuntil we have incremented x some number of timeswithout adding any new relations to the semanticnetwork.
We then take the symmetric closure of thenetwork, so that if (t, c) is in the network, (c, t) is, aswell.
(That is, the relation is considered undirected.
)Consider, for example, the nouns in Table 1.Given the target ?yoga,?
we might first examine thetop 5% of its most strongly related co-targets (an ar-bitrary initial threshold chosen simply for illustra-tive purposes).
In this case, we have all the nounsabove the dashed line: C5(yoga) = {hatha yoga,asana, meditation, bhakti, raja, tantra, yogi, karma,posture, aerobics, tai chi, exercise, practice, instruc-tor}.
The algorithm then searches C5(hatha yoga),C5(asana), and so on, for ?yoga,?
adding a new re-lation to the network every time ?yoga?
is found.Thus, we can see by the inclusion of ?yoga?
inC5(meditation) (all nouns above the dashed line inthe second column of Table 1), that the pair (yoga,meditation) will be included in the network.This reliance on mutual relatedness ensures thatonly noun pairs exhibiting strong semantic related-ness are admitted to the network.3.3 DisambiguationDisambiguation of the resulting noun-noun pairs isthe product of majority-rules voting by the followingthree algorithms.Subsumption.
The most frequently occurringimmediate hypernyms of all nouns related to ourtarget are permitted to disambiguate the polyse-mous nouns.
This is useful because of the semanticclustering that tends to occur among related nouns.
(E.g., ?astronomer?
is related to several terms cat-egorized as celestial bodies in WordNet, such as?planet,?
?star,?
?minor planet,?
and ?quasar.?)Glosses.
Senses of polysemous co-targets withoccurrences of monosemous co-targets in theirglosses are preferentially taken as the intendedmeanings of the polysemous nouns.
Monosemousco-targets are matched directly, or by suffix replace-ment.
(E.g., ?biology?
can be matched by the oc-currence of ?biologist?
in a gloss, ?engineering?
by?engineers,?
and so on.
)Selectional Preferences.
This method associatesa numerical score with all superordinate synsetsfrom the WordNet noun ontology that categorize192the monosemous nouns related to a target.
Forexample, the noun ?unicorn?
strongly predicts re-lated nouns categorized as monsters (monster#1)4and mythical beings (mythical being#1) in Word-Net.
These selectional preferences are applied topolysemous co-targets in decreasing order of theirrelational strength to the target noun.
A polysemousnoun is disambiguated to the first sense or sensessubsumed by one of these selectional preferences.For example, ?phoenix,?
as it relates to ?unicorn,?
isdisambiguated to phoenix#3 in WordNet (the fierybird that is reborn from its own ashes) by virtue ofits subsumption by mythical being#1.4 Creating a More Complete NetworkA shortcoming of our previously released network isthat it lacked concept-level relations between pairsof polysemous nouns.When humans encounter a pair of ambiguous butclosely related words, like bus?horn, we automat-ically disambiguate to the automobile and the carhorn, as opposed to a computer?s front-side bus ora rhinoceros?s horn.
The human ability to performthis disambiguation stems from the fact that humansemantic memory relates not just individual words,but specific concepts denoted by those words.
But ifour goal is to establish such a link in our computa-tional model of semantic relatedness, then we can-not rely on the link to perform that disambiguationfor us; another approach is called for.One reasonable approach (the one taken in ourprevious work) is to go where the problem nolonger exists ?
to relationships that involve atleast one monosemous noun.
Monosemous-to-monosemous noun relationships require no disam-biguation.
Monosemous-to-polysemous noun rela-tionships, on the other hand, require that only onenoun be disambiguated.
This ameliorates our prob-lem tremendously, because the monosemous nounin the pair anchors the polysemous noun in an un-ambiguous context where disambiguation can morereadily take place.
That context includes all thenouns related to our monosemous noun, which,through their transitive relatedness to the polyse-mous noun in question, can assist in the act of disam-4We sometimes drop the part of speech from our word sensenotation for brevity, but only in the case of noun senses.biguation vis-a`-vis the algorithms described in Sec-tion 3.3.Consider, in contrast, the polysemous ?batter,?which can refer to the baseball player or the cakebatter.
The algorithm for discovering semantic relat-edness yields several nouns related to each of thesesenses of ?batter?
(see Table 2).
If we wish to dis-ambiguate the pair (batter, cake), we are left with thequestion: which of the nouns in Table 2 should wetake as contextual anchors for the disambiguation?baking fastball inning strikeball flour outfielder strikeoutbase glove pancake swingbaseball hitter pitch tempurabat home plate pudding umpirecake home run runner waffledugout infielder shortstopTable 2: An excerpt of some of the nouns related to ?bat-ter?
by the algorithm for automatic acquisition.In considering this question, it is important to notethat although the ontological categories that sub-sume the nouns related to ?batter?
exhibit greaterentropy than we usually observe among the termsrelated to a monosemous noun, clear delineationsstill exist.
For example, Figure 1 shows the clustersthat form as we consider shared hypernymic rela-tionships between all senses of the nouns related to?batter?
(gray nodes in the graph).
We see that manyof the nouns related to ?batter?
have senses catego-rized by food#1, cake#3, pitch#2, ballplayer#1, orequipment#1 ?
the heads of five distinct clusters bysemantic similarity.It is worth noting that some nouns related to ?bat-ter?
(such as ?baking,?
?swing,?
and ?umpire?)
donot fall into any of these semantic clusters.
In thesecases, the WordNet glosses serve as our primarytool for disambiguation.
(For example, the glossesof both swing#8 and umpire#1 include mention of?baseball,?
which is also related to ?batter.?
)Conversely, some of the polysemous nouns in ourexample have senses that join semantic clusters un-intendedly.
For instance, cake#2 (?
[a] small flatmass of chopped food,?
according to WordNet) fallsunder the cluster headed by food#1.
Although this ispotentially problematic, cake#2 is discarded in thisparticular case in favor of cake#3 (the baked good),193which has a greater mass because of its subsump-tion of waffle#1 and pancake#1, and is indeed theintended meaning of ?cake?
as it relates to ?batter.
?Another example of unintended cluster member-ship comes from bat#4 (the cricket bat), which iscategorized by sports equipment#1.
In contrast, thebaseball bat does not have its own entry in WordNet,and the most reasonable sense choice, bat#5 (?a clubused for hitting a ball in various games?
), is cate-gorized as a stick (stick#1), and not as equipment,sports equipment, or game equipment.These unintended cluster memberships are boundto cause minor errors in our disambiguation efforts.However, our analysis reveals that we do not findsuch high entropy among the relatives of a polyse-mous noun that the semantic clustering effect (whichis necessary for the success of the disambiguationalgorithms described above in Section 3.3) is dimin-ished.
Thus, to construct our network, we apply thedisambiguation algorithms described above, withthe following modification: when confronted witha pair of semantically related polysemous nouns,we apply the disambiguation mechanism describedabove in both directions, and then fuse the results to-gether.
So, in one direction, the various baked goodsrelated to ?batter?
help us to properly disambiguate?cake?
to cake#3 in WordNet, yielding the pair (bat-ter, cake#3).
A similar scenario yields the pair (cake,batter#2) when disambiguating in the other direc-tion, and we fuse the results together into the prop-erly disambiguated pair (batter#2, cake#3).Using this method, we have automatically createda semantic network that has 208,832 pairs of relatednoun senses ?
the most extensive semantic networkbetween WordNet noun senses to be derived auto-matically from a simple lexical co-occurrence mea-sure.
For the remainder of this paper, we will referto our network as the Szumlanski-Gomez network(SGN).5 Coarse-Grained WSD ExperimentsTo evaluate our semantic network, and to providefair comparison to related work, we take our cuefrom Ponzetto and Navigli (2010), who evaluatedthe performance of WN++ on the SemEval-2007(Navigli et al, 2007) coarse-grained all-words WSDtask using extended gloss overlaps (Banerjee andentity#1food#1cake#2equipment#1ballplayer#1pitch#2cake#3dessert#1tempura#1game_equipment#1sports_equipment#1runner#4fielder#1hitter#1fastball#1strike#5waffle#1pancake#1pudding#2pudding#3ball#1infielder#1outfielder#1baseball#2bat#4base#3glove#1glove#3shortstop#1centerfielder#1Figure 1: A partial view of the WordNet graph, showingsenses of nouns related to ?batter?
(gray nodes) and inter-mediary concepts (white nodes) that connect them to theroot of the taxonomy through hypernymic relationships.Pedersen, 2003) and the graph-based degree central-ity algorithm (Navigli and Lapata, 2010).In this particular SemEval task, we are presentedwith 237 sentences in which lemmatized targetwords have been flagged for disambiguation.
In ourexperiments, we disambiguate nouns only (as didPonzetto and Navigli), since both SGN (our net-work) and WN++ relate only concepts denoted bynouns, and no other parts of speech.
In our exper-imental setup, each sentence is considered in isola-tion from the rest, and all lemmatized content wordsin a sentence are provided to the disambiguationalgorithms; the verbs, adjectives, and adverbs, al-though we do not resolve their senses, lend addi-tional context to the disambiguation algorithms.The coarse-grained nature of the SemEval-2007task provides that there may be more than one ac-ceptable sense assignment for many of the targets.
Inthe coarse-grained setting, an algorithm?s sense as-signment is considered correct when it appears in thelist of acceptable senses for the given target word.The algorithms below both allow for multiple dis-ambiguation results to be returned in the event of atie.
In these cases (although they are rare), we adoptthe approach of Banerjee and Pedersen (2003), whoaward partial credit and discredit proportionally forall the senses returned by the algorithm.1946 Extended Gloss Overlaps (ExtLesk)The first disambiguation algorithm we employ isthe extended gloss overlaps measure (henceforthExtLesk) of Banerjee and Pedersen (2003), whichis an extension of the Lesk (1986) gloss overlapmeasure.
Loosely speaking, the algorithm disam-biguates a target noun by maximizing the overlap(number of words in common) between the glossesof word senses related5 to the target?s noun sensesand those related to all context words (all lemma-tized targets in the sentence under considerationother than the target itself).
The sense with the great-est overlap is selected as the intended meaning of atarget noun.In the event of a tie, multiple senses may be se-lected.
ExtLesk does not attempt to perform senseassignment if the score for every sense of a targetnoun is zero, except when dealing with a monose-mous noun, in which case we default to the onlysense possible.6.1 ResultsWe have run ExtLesk on the SemEval-2007 task us-ing five combinations of semantic resources: Word-Net only, SGN (our semantic network) only, SGNand WordNet combined (that is, the union of alllinks contained in both networks), WN++ only, andWN++ combined withWordNet.
We include the tra-ditional baselines of most frequent sense (MFS) as-signment and random sense assignment for compari-son, and measure precision (number of correct senseassignments divided by the number of attemptedsense assignments), recall (number of correct senseassignments divided by the number of target nounsto be disambiguated), and the harmonic mean of thetwo, F1, defined as:F1 =2 ?
precision ?
recallprecision + recallWe present our results in Table 3, and offer thefollowing observations.
Firstly, SGN as a stand-alone network rivals the performance of WordNet.This is particularly impressive given the fact that5We use all relations available in WordNet, as well as arelated-to relation derived from the links in our semantic net-work.Resource P R F1WordNet 78.80 74.82 76.76SGN 78.64 72.82 75.62SGN and WordNet 82.35 78.11 80.18WN++ 74.67 61.87 67.67WN++ and WordNet 77.35 73.38 75.31MFS Baseline 77.40 77.40 77.40Random Baseline 63.50 63.50 63.50Table 3: ExtLesk disambiguation results on the SemEval-2007 all-words coarse-grained WSD task (nouns only).the edges in SGN were derived automatically froma simple lexical co-occurrence measure.Equally impressive is the ability of SGN andWordNet, when used in combination, to achieve re-sults that exceed what either network is able to ac-complish as a stand-alone knowledge source.
Whencombined, we see improvements of 3.42% and4.56% over WordNet and SGN as stand-alone re-sources, respectively.
It is also only with these re-sources combined that we are able to outperform theMFS baseline, and we do so by 2.78%.6In contrast, WN++ fails to perform as a stand-alone resource, falling behind the MFS baseline by9.73%.7 Of all the resources tested, WN++ yieldsthe lowest results.
When combined with WordNet,WN++ actually diminishes the ability of WordNet toperform on this WSD task by 1.45%.
We defer ourdiscussion of factors impacting the performance ofWN++ to Section 8 (Discussion).7 WSD with Degree CentralityDegree centrality is a graph-based measure of se-mantic relatedness (Navigli and Lapata, 2010) inwhich we search through a semantic network forpaths of length l ?
maxLength between all sensenodes for all lemmas in our context.
The edges alongall such paths are added to a new graph, G?, and foreach target noun to be disambiguated, the sense nodewith the greatest number of incident edges (highestvertex degree) in G?
is taken as its intended sense.6Other systems have obtained better results on the samedataset, but we focus only on SGN and WN++ because our aimis to compare the resources themselves.7Ponzetto and Navigli (2010) report results of F1 = 68.3 and72.0 for WN and WN++ as stand-alone resources.
Space con-siderations prevent us from discussing this disparity in detail.195In these graphs, nodes represent synsets, as op-posed to instantiating separate nodes for differentmembers of the same synset and allowing edges tobe constructed between them.
We include all lem-mas from a sentence in our context, but only returndisambiguation results for the nouns.With SGN and WN++, the implementation of thisalgorithm is straightforward.
We initiate a breadth-first search (BFS)8 at each target sense node in thenetwork, and proceed through ?maxLength+12 ?
itera-tions of spreading activation.
Whenever the tendrilsof this spreading activation from one target sensenode in the graph connect to those of another,9 weadd the path between the nodes to our new graph, G?,potentially incrementing the degree of the involvedtarget sense nodes in G?
as we do so.Because BFS is an admissible algorithm (guaran-teed to find the shortest path from an initial state toa goal), it provides a computationally efficient ap-proach to finding all paths between all target nodes.Also, because any node on a path of length l ?maxLength between two target nodes is at most?
l2?
nodes removed from at least one of those tar-get sense nodes, we only need to perform a BFS ofdepth ?maxLength+12 ?
from every target sense nodein order to guarantee that every such path betweenthem will be discovered.
Since the time complexityof BFS is exponential with respect to the depth ofthe search, cutting this depth in half (in comparisonto performing a BFS of depth maxLength) greatlyreduces the running time of our algorithm.We take the same approach in traversing theWordNet noun graph, using all possible sense re-lations as edges.
In keeping with the approach ofNavigli and Lapata (2010), an edge is also inducedbetween synsets if the gloss of one synset contains amonosemous content word.
For example, the glossfor leprechaun#n#1, ?a mischievous elf in Irish folk-lore,?
contains the monosemous noun ?folklore;?thus, we have an edge between leprechaun#n#1 and8This is in contrast to the DFS implementation of Navigliand Lapata (2010), so for the sake of posterity, we expoundupon our approach in this section.9When maxLength is odd, this requires an additionalcheck to ensure that the intersection is not taking place at a nodethat is exactly ?maxLength+12 ?
degrees removed from each ofthe two target nodes it is connecting, as this would result in apath with overall length maxLength + 1 between the targetnodes.folklore#n#1 in the WordNet graph.Once we have our new graph, G?, constructed inthis manner, the vertex degree is considered an in-dication of the semantic relatedness of a particularsynset to all other lemmas in our context.
For eachtarget noun, we use its sense node with the highestdegree in G?
for sense assignment.7.1 ResultsWe have tested the degree centrality algorithm withthe following combinations of semantic resources:WordNet, SGN, WN++, Refined WN++, SGN andWordNet combined, and Refined WN++ and Word-Net combined.
(Refined WN++ consists of 79,422of WN++?s strongest relations, and was created in anunsupervised setting by Ponzetto and Navigli specif-ically for use with degree centrality when they dis-covered that WN++ had too many weak relations toperform well with the algorithm.
)We have observed that the performance of de-gree centrality rapidly levels off as maxLengthincreases.
Ponzetto and Lapata (2010) also re-port this so-called ?plateau?
effect, and employ amaxLength of 6 in their experiments, despite find-ing that results level off around maxLength = 4.We, too, find that performance levels off aroundmaxLength = 4 in almost all cases, and so onlycontinue up to maxLength = 5.We find that, in all cases tested, degree centralityis unable to outperform the MFS baseline (with re-spect to F1) (see Table 4).
SGN and WN++ exhibitcomparable performance with this algorithm, withmaximum F1 values of 68.4% (maxLength = 2)and 67.3% (maxLength = 3?5), respectively.
Nei-ther achieves the performance of WordNet with de-gree centrality (F1 = 74.0%), which underperformsthe MFS baseline (F1 = 77.4%) by 3.4%.10 Ponzettoand Navigli (2010) reported that only performingsense assignment when the max degree exceeded anempirically derived but non-disclosed threshold im-proved performance, but we have found that imple-menting such a threshold universally lowers resultsfor all resources we tested with degree centrality.10Although Ponzetto and Navigli (2010) reported similar re-sults with WordNet (F1 = 74.5), we have been unable to repro-duce their results using Refined WN++, either combined withWordNet (F1 = 79.4) or as a stand-alone resource (F1 = 57.4).196The lowest performance using degree central-ity comes from Refined WN++ as a stand-aloneresource.
We attribute this to the fact that Re-fined WN++ is so semantically sparse.
On average,noun senses in Refined WN++ are related to only3.42 other noun senses, while those in WN++ andSGN relate to an average of 44.59 and 10.92 nounsenses, respectively.
Accordingly, the success of Re-fined WN++ and WordNet combined is attributablemostly to the success of WordNet as a stand-aloneresource; as maxLength increases, the contribu-tions made by the sparse Refined WN++ networkrapidly become negligible in comparison to thoseprovided by the WordNet ontology.l P R F1 P R F1WordNet SGN1 96.9 16.8 28.6 79.7 32.9 46.62 77.6 45.1 57.0 72.0 64.6 68.43 76.7 65.6 70.7 68.7 63.5 66.04 76.9 71.0 73.9 68.0 63.9 65.95 76.6 71.6 74.0 68.0 64.2 66.1SGN & WN WN++1 77.4 52.4 62.5 87.2 23.5 37.12 74.7 70.7 72.7 71.6 60.2 65.43 70.3 67.1 68.7 70.7 64.3 67.34 70.5 67.4 68.9 70.4 64.5 67.35 70.1 67.0 68.5 70.4 64.5 67.3WN++refined WN++refined& WN1 98.3 15.3 26.5 83.3 31.2 45.42 91.4 23.4 37.3 77.5 66.6 71.63 88.7 29.9 44.7 77.6 73.6 75.54 83.7 32.3 46.7 74.7 71.4 73.05 80.2 35.3 49.0 74.7 71.4 73.0MFS Baseline Random Baseline77.4 77.4 77.4 63.5 63.5 63.5Table 4: Degree centrality disambiguation results onthe SemEval-2007 all-words coarse-grained WSD task(nouns only).
l is maximum path length.8 DiscussionThe fact that the performance of degree centralityquickly plateaus hints at the root cause of its weakperformance compared to ExtLesk and the MFSbaseline.
As the maximum path length is increasedin a dense semantic network, all possible edges fromour target sense nodes rapidly find themselves in-volved with paths to other target sense nodes.
This isparticularly true of WN++ (notice its rapid and sta-ble convergence), where certain ?sticky?
nodes formbridges between seemingly unrelated concepts.
Forexample, the frequent appearance of ?United States?in Wikipedia articles, and its tendency to be linkedto the United States Wikipage when it occurs, causesthe term to serve as a bridge between such diverseconcepts as automaton#2 and burrito#1, which onewould typically expect to be far removed from oneanother in a model of semantic relatedness.Nonetheless, the degree centrality algorithm hasno difficulty finding short paths between target sensenodes when traversing any of the semantic networkswe tested.
In fact, we have discovered that as theresults of degree centrality converge, they approachthe performance obtained by foregoing the algo-rithm altogether and simply disambiguating eachnoun to the sense with the most edges in the net-work (regardless of whether those edges ultimatelyconnect two word senses from the disambiguationcontext).
The expected values of convergence at-tained by defaulting to the most semantically well-connected sense of each target noun in each networkare F1 = 66.3%, 67.5%, and 74.6% for SGN,WN++,and WordNet, respectively ?
remarkably close to theexperimentally derived degree centrality results ofF1 = 66.1%, 67.3%, and 74.0%.9 ConclusionWe have constructed a semantic network of relatednoun senses automatically from intra-sentential lex-ical co-occurrence data, and shown that on a WSDtask, it outperforms a similar resource, WN++,which is derived from the rich set of semantic anno-tations available in the Wikipedia corpus.
Our net-work has also shown competitive performance withthe WordNet ontology onWSD, and when combinedwith WordNet, improves disambiguation results ina coarse-grained setting using the ExtLesk disam-biguation algorithm.AcknowledgmentsThis research was supported in part by theNASA Engineering and Safety Center underGrant/Cooperative Agreement NNX08AJ98A.197ReferencesEneko Agirre and Oier Lopez de Lacalle.
2004.
Pub-licly available topic signatures for all WordNet nom-inal senses.
In Proceedings of the 4th InternationalConference on Language Resources and Evaluations(LREC ?04), pages 1123?1126, Lisbon, Portugal.Satanjeev Banerjee and Ted Pedersen.
2003.
Extendedgloss overlaps as a measure of semantic relatedness.In Proceedings of the 18th International Joint Confer-ence on Artificial Intelligence (IJCAI ?03), pages 805?810, Acapulco, Mexico.Alexander Budanitsky and Graeme Hirst.
2006.
Evalu-ating WordNet-based measures of lexical semantic re-latedness.
Computational Linguistics, 32(1):13?47.Montse Cuadros and German Rigau.
2008.
KnowNet:building a large net of knowledge from the web.
InProceedings of the 22nd International Conference onComputational Linguistics (COLING ?08), pages 161?168, Manchester, UK.
Association for ComputationalLinguistics.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness usingWikipedia-based ex-plicit semantic analysis.
In Proceedings of the 20th In-ternational Joint Conference on Artificial Intelligence(IJCAI ?07), pages 1606?1611, Hyderabad, India.James Gorman and James R. Curran.
2006.
Scaling dis-tributional similarity to large corpora.
In Proceedingsof the 21st InternationalConference on ComputationalLinguistics and the 44th Annual Meeting of the Asso-ciation for Computational Linguistics (COLING-ACL?06), pages 361?368, Sydney, Australia.
Associationfor Computational Linguistics.Zellig S. Harris.
1985.
Distributional structure.
In J. J.Katz, editor, The Philosophy of Linguistics, pages 26?47.
Oxford University Press.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 14th International Conference on ComputationalLinguistics (COLING ?92), pages 539?545, Nantes,France.Thad Hughes and Daniel Ramage.
2007.
Lexical se-mantic relatedness with random graph walks.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Compu-tational Natural Language Learning (EMNLP-CoNLL?07), pages 581?589, Prague, Czech Republic.
Asso-ciation for Computational Linguistics.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: how to tell a pinecone from an ice cream cone.
In Proceedings of the5th Annual International Conference on Systems Doc-umentation (SIGDOC ?86), pages 24?26, Toronto, On-tario, Canada.
ACM.RadaMihalcea and Andras Csomai.
2007.
Wikify!
: link-ing documents to encyclopedic knowledge.
In Pro-ceedings of the 16th ACM Conference on Informationand Knowledge Management (CIKM ?07), pages 233?242, Lisbon, Portugal.
ACM.Roberto Navigli and Mirella Lapata.
2010.
An exper-imental study of graph connectivity for unsupervisedword sense disambiguation.
IEEE Transactions onPattern Analysis andMachine Intelligence, 32(4):678?692.Roberto Navigli, Kenneth C. Litkowski, and Orin Har-graves.
2007.
SemEval-2007 Task 07: coarse-grainedEnglish all-words task.
In Proceedings of the 4th In-ternational Workshop on Semantic Evaluations (Sem-Eval ?07), pages 30?35, Prague, Czech Republic.
As-sociation for Computational Linguistics.Patrick Pantel and Marco Pennacchiotti.
2006.
Espresso:leveraging generic patterns for automatically harvest-ing semantic relations.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand the 44th Annual Meeting of the Association forComputational Linguistics (COLING-ACL ?06), pages113?120, Sydney, Australia.
Association for Compu-tational Linguistics.Siddharth Patwardhan and Ted Pedersen.
2006.
UsingWordNet-based context vectors to estimate the seman-tic relatedness of concepts.
In Proceedings of the 11thConference of the European Chapter of the Associa-tion for Computational Linguistics Workshop on Mak-ing Sense of Sense, pages 1?8, Trento, Italy.Simone Paolo Ponzetto and Roberto Navigli.
2009.Large-scale taxonomy mapping for restructuring andintegrating Wikipedia.
In Proceedings of the 21st In-ternational Joint Conference on Artifical Intelligence(IJCAI ?09), pages 2083?2088, Pasadena, CA.Simone Paolo Ponzetto and Roberto Navigli.
2010.Knowledge-rich word sense disambiguation rivalingsupervised systems.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics (ACL ?10), pages 1522?1531, Uppsala, Sweden.Association for Computational Linguistics.Philip Resnik.
1995.
Using information content to eval-uate semantic similarity in a taxonomy.
In Proceed-ings of the 14th International Joint Conference on Ar-tificial Intelligence (IJCAI ?95), pages 448?453, Mon-treal, QC.Philip Resnik.
1999.
Semantic similarity in a taxonomy:an information-based measure and its application toproblems of ambiguity in natural language.
Journalof Artificial Intelligence Research, 11:95?130.198Michael Strube and Simone Paolo Ponzetto.
2006.Wikirelate!
computing semantic relatedness using wi-kipedia.
In Proceedings of the 21st National Confer-ence on Artificial Intelligence (AAAI ?06), pages 1419?1424, Boston, MA.
AAAI Press.Sean Szumlanski and Fernando Gomez.
2010.
Auto-matically acquiring a semantic network of related con-cepts.
In Proceedings of the 19th ACM Conference onInformation and KnowledgeManagement (CIKM ?10),pages 19?28, Toronto, Ontario, Canada.
ACM.Hugo Zaragoza, Henning Rode, Peter Mika, Jordi Atse-rias, Massimiliano Ciaramita, and Giuseppe Attardi.2007.
Ranking very many typed entities on Wikipe-dia.
In Proceedings of the 16th ACM Conference onInformation and KnowledgeManagement (CIKM ?07),pages 1015?1018, Lisbon, Portugal.
ACM.199
