Proceedings of ACL-08: HLT, pages 825?833,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCan you summarize this?
Identifying correlates of input difficulty forgeneric multi-document summarizationAni NenkovaUniversity of PennsylvaniaPhiladelphia, PA 19104, USAnenkova@seas.upenn.eduAnnie LouisUniversity of PennsylvaniaPhiladelphia, PA 19104, USAlannie@seas.upenn.eduAbstractDifferent summarization requirements couldmake the writing of a good summary more dif-ficult, or easier.
Summary length and the char-acteristics of the input are such constraints in-fluencing the quality of a potential summary.In this paper we report the results of a quanti-tative analysis on data from large-scale evalu-ations of multi-document summarization, em-pirically confirming this hypothesis.
We fur-ther show that features measuring the cohe-siveness of the input are highly correlated witheventual summary quality and that it is possi-ble to use these as features to predict the diffi-culty of new, unseen, summarization inputs.1 IntroductionIn certain situations even the best automatic sum-marizers or professional writers can find it hard towrite a good summary of a set of articles.
If thereis no clear topic shared across the input articles, orif they follow the development of the same event intime for a longer period, it could become difficultto decide what information is most representativeand should be conveyed in a summary.
Similarly,length requirements could pre-determine summaryquality?a short outline of a story might be confus-ing and unclear but a page long discussion mightgive an excellent overview of the same issue.Even systems that perform well on average pro-duce summaries of poor quality for some inputs.
Forthis reason, understanding what aspects of the in-put make it difficult for summarization becomes aninteresting and important issue that has not been ad-dressed in the summarization community untill now.In information retrieval, for example, the variablesystem performance has been recognized as a re-search challenge and numerous studies on identify-ing query difficulty have been carried out (most re-cently (Cronen-Townsend et al, 2002; Yom-Tov etal., 2005; Carmel et al, 2006)).In this paper we present results supporting the hy-potheses that input topicality cohesiveness and sum-mary length are among the factors that determinesummary quality regardless of the choice of summa-rization strategy (Section 2).
The data used for theanalyses comes from the annual Document Under-standing Conference (DUC) in which various sum-marization approaches are evaluated on commondata, with new test sets provided each year.In later sections we define a suite of features cap-turing aspects of the topicality cohesiveness of theinput (Section 3) and relate these to system perfor-mance, identifying reliable correlates of input diffi-culty (Section 4).
Finally, in Section 5, we demon-strate that the features can be used to build a clas-sifier predicting summarization input difficulty withaccuracy considerably above chance level.2 Preliminary analysis and distinctions:DUC 2001Generic multi-document summarization was fea-tured as a task at the Document Understanding Con-ference (DUC) in four years, 2001 through 2004.In our study we use the DUC 2001 multi-documenttask submissions as development data for in-depthanalysis and feature selection.
There were 29 in-put sets and 12 automatic summarizers participatingin the evaluation that year.
Summaries of different825lengths were produced by each system: 50, 100, 200and 400 words.
Each summary was manually eval-uated to determine the extent to which its contentoverlaped with that of a human model, giving a cov-erage score.
The content comparison was performedon a subsentence level and was based on elementarydiscourse units in the model summary.1The coverage scores are taken as an indicator ofdifficultly of the input: systems achieve low cover-age for difficult sets and higher coverage for easysets.
Since we are interested in identifying charac-teristics of generally difficult inputs rather than indiscovering what types of inputs might be difficultfor one given system, we use the average systemscore per set as indicator of general difficulty.2.1 Analysis of varianceBefore attempting to derive characteristics of inputsdifficult for summarization, we first confirm that in-deed expected performance is influenced by the in-put itself.
We performed analysis of variance forDUC 2001 data, with automatic system coveragescore as the dependent variable, to gain some insightinto the factors related to summarization difficulty.The results of the ANOVA with input set, summa-rizer identity and summary length as factors, as wellas the interaction between these, are shown in Ta-ble 1.As expected, summarizer identity is a significantfactor: some summarization strategies/systems aremore effective than others and produce summarieswith higher coverage score.
More interestingly, theinput set and summary length factors are also highlysignificant and explain more of the variability incoverage scores than summarizer identity does, asindicated by the larger values of the F statistic.Length The average automatic summarizer cov-erage scores increase steadily as length requirementsare relaxed, going up from 0.50 for 50-word sum-maries to 0.76 for 400-word summaries as shown inTable 2 (second row).
The general trend we observeis that on average systems are better at producingsummaries when more space is available.
The dif-1The routinely used tool for automatic evaluation ROUGEwas adopted exactly because it was demonstrated it is highlycorrelated with the manual DUC coverage scores (Lin andHovy, 2003a; Lin, 2004).Type 50 100 200 400Human 1.00 1.17 1.38 1.29Automatic 0.50 0.55 0.70 0.76Baseline 0.41 0.46 0.52 0.57Table 2: Average human, system and baseline coveragescores for different summary lengths of N words.
N =50, 100, 200, and 400.ferences are statistically significant2 only between50-word and 200- and 400-word summaries and be-tween 100-word and 400-word summaries.
The factthat summary quality improves with increasing sum-mary length has been observed in prior studies aswell (Radev and Tam, 2003; Lin and Hovy, 2003b;Kolluru and Gotoh, 2005) but generally little atten-tion has been paid to this fact in system developmentand no specific user studies are available to showwhat summary length might be most suitable forspecific applications.
In later editions of the DUCconference, only summaries of 100 words were pro-duced, focusing development efforts on one of themore demanding length restrictions.
The interactionbetween summary length and summarizer is smallbut significant (Table 1), with certain summariza-tion strategies more successful at particular sum-mary lengths than at others.Improved performance as measured by increasein coverage scores is observed for human summa-rizers as well (shown in the first row of Table 2).Even the baseline systems (first n words of the mostrecent article in the input or first sentences fromdifferent input articles) show improvement whenlonger summaries are allowed (performance shownin the third row of the table).
It is important tonotice that the difference between automatic sys-tem and baseline performance increases as the sum-mary length increases?the difference between sys-tems and baselines coverage scores is around 0.1for the shorter 50- and 100-word summaries but 0.2for the longer summaries.
This fact has favorableimplications for practical system developments be-cause it indicates that in applications where some-what longer summaries are appropriate, automati-cally produced summaries will be much more infor-mative than a baseline summary.2One-sided t-test, 95% level of significance.826Factor DF Sum of squares Expected mean squares F stat Pr(> F )input 28 150.702 5.382 59.4227 0summarizer 11 34.316 3.120 34.4429 0length 3 16.082 5.361 59.1852 0input:summarizer 306 65.492 0.214 2.3630 0input:length 84 36.276 0.432 4.7680 0summarizer:length 33 6.810 0.206 2.2784 0Table 1: Analysis of variance for coverage scores of automatic systems with input, summarizer, and length as factors.Input The input set itself is a highly significantfactor that influences the coverage scores that sys-tems obtain: some inputs are handled by the systemsbetter than others.
Moreover, the input interacts bothwith the summarizers and the summary length.This is an important finding for several reasons.First, in system evaluations such as DUC the inputsfor summarization are manually selected by anno-tators.
There is no specific attempt to ensure thatthe inputs across different years have on average thesame difficulty.
Simply assuming this to be the casecould be misleading: it is possible in a given year tohave ?easier?
input test set compared to a previousyear.
Then system performance across years can-not be meaningfully compared, and higher systemscores would not be indicative of system improve-ment between the evaluations.Second, in summarization applications there issome control over the input for summarization.
Forexample, related documents that need to summa-rized could be split into smaller subsets that are moreamenable to summarization or routed to an appropri-ate summarization system than can handle this kindof input using a different strategy, as done for in-stance in (McKeown et al, 2002).Because of these important implications we inves-tigate input characteristics and define various fea-tures distinguishing easy inputs from difficult ones.2.2 Difficulty for people and machinesBefore proceeding to the analysis of input difficultyin multi-document summarization, it is worth men-tioning that our study is primarily motivated by sys-tem development needs and consequently the focusis on finding out what inputs are easy or difficultfor automatic systems.
Different factors might makesummarization difficult for people.
In order to see towhat extent the notion of summarization input dif-summary length correlation50 0.50100 0.57*200 0.77**400 0.70**Table 3: Pearson correlation between average human andsystem coverage scores on the DUC 2001 dataset.
Sig-nificance levels: *p < 0.05 and **p < 0.00001.ficulty is shared between machines and people, wecomputed the correlation between the average sys-tem and average human coverage score at a givensummary length for all DUC 2001 test sets (shownin Table 3).
The correlation is highest for 200-wordsummaries, 0.77, which is also highly significant.For shorter summaries the correlation between hu-man and system performance is not significant.In the remaining part of the paper we deal ex-clusively with difficulty as defined by system per-formance, which differs from difficulty for peoplesummarizing the same material as evidenced by thecorrelations in Table 3.
We do not attempt to drawconclusions about any cognitively relevant factorsinvolved in summarizing.2.3 Type of summary and difficultyIn DUC 2001, annotators prepared test sets from fivepossible predefined input categories:3 .Single event (3 sets) Documents describing a singleevent over a timeline (e.g.
The Exxon Valdezoil spill).3Participants in the evaluation were aware of the differentcategories of input and indeed some groups developed systemsthat handled different types of input employing different strate-gies (McKeown et al, 2001).
In later years, the idea of multi-strategy summarization has been further explored by (Lacatusuet al, 2006)827Subject (6 sets) Documents discussing a singletopic (e.g.
Mad cow disease)Biographical (2 sets) All documents in the inputprovide information about the same person(e.g.
Elizabeth Taylor)Multiple distinct events (12 sets) The documentsdiscuss different events of the same type (e.g.different occasions of police misconduct).Opinion (6 sets) Each document describes a differ-ent perspective to a common topic (e.g.
viewsof the senate, congress, public, lawyers etc onthe decision by the senate to count illegal aliensin the 1990 census).Figure 1 shows the average system coverage scorefor the different input types.
The more topically co-hesive input types such as biographical, single eventand subject, which are more focused on a single en-tity or news item and narrower in scope, are eas-ier for systems.
The average system coverage scorefor them is higher than for the non-cohesive setssuch as multiple distinct events and opinion sets, re-gardless of summary length.
The difference is evenmore apparently clear when the scores are plotted af-ter grouping input types into cohesive (biographical,single event and subject) and non-cohesive (multi-ple events and opinion).
Such grouping also givesthe necessary power to perform statistical test forsignificance, confirming the difference in coveragescores for the two groups.
This is not surprising: asummary of documents describing multiple distinctevents of the same type is likely to require higherdegree of generalization and abstraction.
Summa-rizing opinions would in addition be highly subjec-tive.
A summary of a cohesive set meanwhile wouldcontain facts directly from the input and it would beeasier to determine which information is important.The example human summaries for set D32 (singleevent) and set D19 (opinions) shown below give anidea of the potential difficulties automatic summa-rizers have to deal with.
set D32 On 24 March 1989,the oil tanker Exxon Valdez ran aground on a reef nearValdez, Alaska, spilling 8.4 million gallons of crude oilinto Prince William Sound.
In two days, the oil spreadover 100 miles with a heavy toll on wildlife.
Cleanupproceeded at a slow pace, and a plan for cleaning 364miles of Alaskan coastline was released.
In June, thetanker was refloated.
By early 1990, only 5 to 9 percent ofspilled oil was recovered.
A federal jury indicted Exxonon five criminal charges and the Valdez skipper was guiltyof negligent discharge of oil.set D19 Congress is debating whether or not to count ille-gal aliens in the 1990 census.
Congressional House seatsare apportioned to the states and huge sums of federalmoney are allocated based on census population.
Cali-fornia, with an estimated half of all illegal aliens, will begreatly affected.
Those arguing for inclusion say that theConstitution does not mention ?citizens?, but rather, in-structs that House apportionment be based on the ?wholenumber of persons?
residing in the various states.
Thoseopposed say that the framers were unaware of this issue.
?Illegal aliens?
did not exist in the U.S. until restrictiveimmigration laws were passed in 1875.The manual set-type labels give an intuitive ideaof what factors might be at play but it is desirable todevise more specific measures to predict difficulty.Do such measures exist?
Is there a way to automati-cally distinguish cohesive (easy) from non-cohesive(difficult) sets?
In the next section we define a num-ber of features that aim to capture the cohesivenessof an input set and show that some of them are in-deed significantly related to set difficulty.3 FeaturesWe implemented 14 features for our analysis of in-put set difficulty.
The working hypothesis is that co-hesive sets with clear topics are easier to summarizeand the features we define are designed to captureaspects of input cohesiveness.Number of sentences in the input, calculatedover all articles in the input set.
Shorter inputsshould be easier as there will be less information lossbetween the summary and the original material.Vocabulary size of the input set, equal to thenumber of unique words in the input.
Smaller vo-cabularies would be characteristic of easier sets.Percentage of words used only once in the input.The rationale behind this feature is that cohesive in-put sets contain news articles dealing with a clearlydefined topic, so words will be reused across docu-ments.
Sets that cover disparate events and opinionsare likely to contain more words that appear in theinput only once.Type-token ratio is a measure of the lexical vari-ation in an input set and is equal to the input vo-cabulary size divided by the number of words in the828Figure 1: Average system coverage scores for summaries in a categoryinput.
A high type-token ratio indicates there is little(lexical) repetition in the input, a possible side-effectof non-cohesiveness.Entropy of the input set.
Let X be a discrete ran-dom variable taking values from the finite set V ={w1, ..., wn} where V is the vocabulary of the in-put set and wi are the words that appear in the input.The probability distribution p(w) = Pr(X = w)can be easily calculated using frequency counts fromthe input.
The entropy of the input set is equal to theentropy of X:H(X) = ?i=n?i=1p(wi) log2 p(wi) (1)Average, minimum and maximum cosine over-lap between the news articles in the input.
Repeti-tion in the input is often exploited as an indicator ofimportance by different summarization approaches(Luhn, 1958; Barzilay et al, 1999; Radev et al,2004; Nenkova et al, 2006).
The more similar thedifferent documents in the input are to each other,the more likely there is repetition across documentsat various granularities.Cosine similarity between the document vectorrepresentations is probably the easiest and mostcommonly used among the various similarity mea-sures.
We use tf*idf weights in the vector represen-tations, with term frequency (tf) normalized by thetotal number of words in the document in order to re-move bias resulting from high frequencies by virtueof higher document length alone.The cosine similarity between two (documentrepresentation) vectors v1 and v2 is given by cos?
=v1.v2||v1||||v2|| .
A value of 0 indicates that the vectors areorthogonal and dissimilar, a value of 1 indicates per-fectly similar documents in terms of the words con-tained in them.To compute the cosine overlap features, we findthe pairwise cosine similarity between each twodocuments in an input set and compute their aver-age.
The minimum and maximum overlap featuresare also computed as an indication of the overlapbounds.
We expect cohesive inputs to be composedof similar documents, hence the cosine overlaps inthese sets of documents must be higher than those innon-cohesive inputs.KL divergence Another measure of relatednessof the documents comprising an input set is the dif-ference in word distributions in the input comparedto the word distribution in a large collection of di-verse texts.
If the input is found to be largely dif-ferent from a generic collection, it is plausible to as-sume that the input is not a random collection of ar-ticles but rather is defined by a clear topic discussedwithin and across the articles.
It is reasonable to ex-pect that the higher the divergence is, the easier it isto define what is important in the article and hencethe easier it is to produce a good summary.For computing the distribution of words in a gen-eral background corpus, we used all the inputs setsfrom DUC years 2001 to 2006.
The divergence mea-sure we used is the Kullback Leibler divergence, or829relative entropy, between the input (I) and collectionlanguage models.
Let pinp(w) be the probability ofthe word w in the input and pcoll(w) be the proba-bility of the word occurring in the large backgroundcollection.
Then the relative entropy between the in-put and the collection is given byKL divergence =?w?Ipinp(w) log2pinp(w)pcoll(w)(2)Low KL divergence from a random backgroundcollection may be characteristic of highly non-cohesive inputs consisting of unrelated documents.Number of topic signature terms for the inputset.
The idea of topic signature terms was intro-duced by Lin and Hovy (Lin and Hovy, 2000) in thecontext of single document summarization, and waslater used in several multi-document summarizationsystems (Conroy et al, 2006; Lacatusu et al, 2004;Gupta et al, 2007).Lin and Hovy?s idea was to automatically iden-tify words that are descriptive for a cluster of docu-ments on the same topic, such as the input to a multi-document summarizer.
We will call this cluster T .Since the goal is to find descriptive terms for thecluster, a comparison collection of documents noton the topic is also necessary (we will call this back-ground collection NT ).Given T and NT , the likelihood ratio statistic(Dunning, 1994) is used to identify the topic signa-ture terms.
The probabilistic model of the data al-lows for statistical inference in order to decide whichterms t are associated with T more strongly thanwith NT than one would expect by chance.More specifically, there are two possibilities forthe distribution of a term t: either it is very indicativeof the topic of cluster T , and appears more often inT than in documents from NT , or the term t is nottopical and appears with equal frequency across bothT and NT .
These two alternatives can be formallywritten as the following hypotheses:H1: P (t|T ) = P (t|NT ) = p (t is not a descrip-tive term for the input)H2: P (t|T ) = p1 and P (t|NT ) = p2 and p1 >p2 (t is a descriptive term)In order to compute the likelihood of each hypoth-esis given the collection of the background docu-ments and the topic cluster, we view them as a se-quence of words wi: w1w2 .
.
.
wN .
The occurrenceof a given word t, wi = t, can thus be viewed aBernoulli trial with probability p of success, withsuccess occurring when wi = t and failure other-wise.The probability of observing the term t appearingk times in N trials is given by the binomial distribu-tionb(k,N, p) =(Nk)pk(1 ?
p)N?k (3)We can now compute?
= Likelihood of the data given H1Likelihood of the data given H2 (4)which is equal to?
= b(ct,N, p)b(cT ,NT , p1) ?
b(cNT ,NNT , p2)(5)The maximum likelihood estimates for the proba-bilities can be computed directly.
p = ctN , where ct isequal to the number of times term t appeared in theentire corpus T+NT, and N is the number of wordsin the entire corpus.
Similarly, p1 = cTNT , where cTis the number of times term t occurred in T and NTis the number of all words in T .
p2 = cNTNNT , wherecNT is the number of times term t occurred in NTand NNT is the total number of words in NT.?2log?
has a well-know distribution: ?2.
Biggervalues of ?2log?
indicate that the likelihood of thedata under H2 is higher, and the ?2 distribution canbe used to determine when it is significantly higher(?2log?
exceeding 10 gives a significance level of0.001 and is the cut-off we used).For terms for which the computed ?2log?
ishigher than 10, we can infer that they occur moreoften with the topic T than in a general corpus NT ,and we can dub them ?topic signature terms?.Percentage of signature terms in vocabularyThe number of signature terms gives the total countof topic signatures over all the documents in the in-put.
However, the number of documents in an inputset and the size of the individual documents acrossdifferent sets are not the same.
It is therefore possi-ble that the mere count feature is biased to the length830and number of documents in the input set.
To ac-count for this, we add the percentage of topic wordsin the vocabulary as a feature.Average, minimum and maximum topic sig-nature overlap between the documents in the in-put.
Cosine similarity measures the overlap betweentwo documents based on all the words appearing inthem.
A more refined document representation canbe defined by assuming the document vectors con-tain only the topic signature words rather than allwords.
A high overlap of topic words across twodocuments is indicative of shared topicality.
Theaverage, minimum and maximum pairwise cosineoverlap between the tf*idf weighted topic signaturevectors of the two documents are used as featuresfor predicting input cohesiveness.
If the overlap islarge, then the topic is similar across the two docu-ments and hence their combination will yield a co-hesive input.4 Feature selectionTable 4 shows the results from a one-sided t-testcomparing the values of the various features forthe easy and difficult input set classes.
The com-parisons are for summary length of 100 words be-cause in later years only such summaries were evalu-ated.
The binary easy/difficult classes were assignedbased on the average system coverage score for thegiven set, with half of the sets assigned to each class.In addition to the t-tests we also calculated Pear-son?s correlation (shown in Table 5) between the fea-tures and the average system coverage score for eachset.
In the correlation analysis the input sets are notclassified into easy or difficult but rather the real val-ued coverage scores are used directly.
Overall, thefeatures that were identified by the t-test as most de-scriptive of the differences between easy and diffi-cult inputs were also the ones with higher correla-tions with real-valued coverage scores.Our expectations in defining the features are con-firmed by the correlation results.
For example, sys-tems have low coverage scores for sets with high-entropy vocabularies as indicated by the negativeand high by absolute value correlation (-0.4256).Sets with high entropy are those in which there islittle repetition within and across different articles,and for which it is subsequently difficult to deter-feature t-stat p-valueKL divergence* -2.4725 0.01% of sig.
terms in vocab* -2.0956 0.02average cosine overlap* -2.1227 0.02vocabulary size* 1.9378 0.03set entropy* 2.0288 0.03average sig.
term overlap* -1.8803 0.04max cosine overlap -1.6968 0.05max topic signature overlap -1.6380 0.06number of sentences 1.4780 0.08min topic signature overlap -0.9540 0.17number of signature terms 0.8057 0.21min cosine overlap -0.2654 0.39% of words used only once 0.2497 0.40type-token ratio 0.2343 0.41?Significant at a 95% confidence level(p < 0.05)Table 4: Comparison of non-cohesive (average systemcoverage score < median average system score) vs cohe-sive sets for summary length of 100 wordsmine what is the most important content.
On theother hand, sets characterized by bigger KL diver-gence are easier?there the distribution of words isskewed compared to a general collection of articles,with important topic words occurring more often.Easy to summarize sets are characterized by lowentropy, small vocabulary, high average cosine andaverage topic signature overlaps, high KL diver-gence and a high percentage of the vocabulary con-sists of topic signature terms.5 Classification resultsWe used the 192 sets from multi-document summa-rization DUC evaluations in 2002 (55 generic sets),2003 (30 generic summary sets and 7 viewpoint sets)and 2004 (50 generic and 50 biography sets) to trainand test a logistic regression classifier.
The sets fromall years were pooled together and evenly dividedinto easy and difficult inputs based on the averagesystem coverage score for each set.Table 6 shows the results from 10-fold cross val-idation.
SIG is a classifier based on the six featuresidentified as significant in distinguishing easy fromdifficult inputs based on a t-test comparison (Ta-ble 4).
SIG+yt has two additional features: the yearand the type of summarization input (generic, view-point and biographical).
ALL is a classifier based onall 14 features defined in the previous section, and831feature correlationset entropy -0.4256KL divergence 0.3663vocabulary size -0.3610% of sig.
terms in vocab 0.3277average sig.
term overlap 0.2860number of sentences -0.2511max topic signature overlap 0.2416average cosine overlap 0.2244number of signature terms -0.1880max cosine overlap 0.1337min topic signature overlap 0.0401min cosine overlap 0.0308type-token ratio -0.0276% of words used only once -0.0025Table 5: Correlation between coverage score and featurevalues for the 29 DUC?01 100-word summaries.features accuracy P R FSIG 56.25% 0.553 0.600 0.576SIG+yt 69.27% 0.696 0.674 0.684ALL 61.45% 0.615 0.589 0.600ALL+yt 65.10% 0.643 0.663 0.653Table 6: Logistic regression classification results (accu-racy, precision, recall and f-measure) for balanced data of100-word summaries from DUC?02 through DUC?04.ALL+yt also includes the year and task features.Classification accuracy is considerably higherthan the 50% random baseline.
Using all featuresyields better accuracy (61%) than using solely the6 significant features (accuracy of 56%).
In bothcases, adding the year and task leads to extra 3%net improvement.
The best overall results are forthe SIG+yt classifier with net improvement over thebaseline equal to 20%.
At the same time, it shouldbe taken into consideration that the amount of train-ing data for our experiments is small: a total of 192sets.
Despite this, the measures of input cohesive-ness capture enough information to result in a clas-sifier with above-baseline performance.6 ConclusionsWe have addressed the question of what makes thewriting of a summary for a multi-document inputdifficult.
Summary length is a significant factor,with all summarizers (people, machines and base-lines) performing better at longer summary lengths.An exploratory analysis of DUC 2001 indicated thatsystems produce better summaries for cohesive in-puts dealing with a clear topic (single event, subjectand biographical sets) while non-cohesive sets aboutmultiple events and opposing opinions are consis-tently of lower quality.
We defined a number of fea-tures aimed at capturing input cohesiveness, rangingfrom simple features such as input length and sizeto more sophisticated measures such as input set en-tropy, KL divergence from a background corpus andtopic signature terms based on log-likelihood ratio.Generally, easy to summarize sets are character-ized by low entropy, small vocabulary, high averagecosine and average topic signature overlaps, highKL divergence and a high percentage of the vocab-ulary consists of topic signature terms.
Experimentswith a logistic regression classifier based on the fea-tures further confirms that input cohesiveness is pre-dictive of the difficulty it will pose to automatic sum-marizers.Several important notes can be made.
First, it isimportant to develop strategies that can better handlenon-cohesive inputs, reducing fluctuations in sys-tem performance.
Most current systems are devel-oped with the expectation they can handle any inputbut this is evidently not the case and more attentionshould be paid to the issue.
Second, the interpre-tations of year to year evaluations can be affected.As demonstrated, the properties of the input have aconsiderable influence on summarization quality.
Ifspecial care is not taken to ensure that the difficultyof inputs in different evaluations is kept more or lessthe same, results from the evaluations are not com-parable and we cannot make general claims aboutprogress and system improvements between evalua-tions.
Finally, the presented results are clearly just abeginning in understanding of summarization diffi-culty.
A more complete characterization of summa-rization input will be necessary in the future.ReferencesRegina Barzilay, Kathleen McKeown, and Michael El-hadad.
1999.
Information fusion in the context ofmulti-document summarization.
In Proceedings of the37th Annual Meeting of the Association for Computa-tional Linguistics.David Carmel, Elad Yom-Tov, Adam Darlow, and Dan832Pelleg.
2006.
What makes a query difficult?
In SI-GIR ?06: Proceedings of the 29th annual internationalACM SIGIR conference on Research and developmentin information retrieval, pages 390?397.John Conroy, Judith Schlesinger, and Dianne O?Leary.2006.
Topic-focused multi-document summarizationusing an approximate oracle score.
In Proceedings ofACL, companion volume.Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft.2002.
Predicting query performance.
In Proceedingsof the 25th Annual International ACM SIGIR confer-ence on Research and Development in Information Re-trieval (SIGIR 2002), pages 299?306.Ted Dunning.
1994.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics, 19(1):61?74.Surabhi Gupta, Ani Nenkova, and Dan Jurafsky.
2007.Measuring importance and query relevance in topic-focused multi-document summarization.
In ACL?07,companion volume.BalaKrishna Kolluru and Yoshihiko Gotoh.
2005.
Onthe subjectivity of human authored short summaries.In ACL Workshop on Intrinsic and Extrinsic Evalua-tion Measures for Machine Translation and/or Sum-marization.Finley Lacatusu, Andrew Hickl, Sanda Harabagiu, andLuke Nezda.
2004.
Lite gistexter at duc2004.
In Pro-ceedings of the 4th Document Understanding Confer-ence (DUC?04).F.
Lacatusu, A. Hickl, K. Roberts, Y. Shi, J. Bensley,B.
Rink, P. Wang, and L. Taylor.
2006.
Lcc?s gistexterat duc 2006: Multi-strategy multi-document summa-rization.
In DUC?06.Chin-Yew Lin and Eduard Hovy.
2000.
The automatedacquisition of topic signatures for text summarization.In Proceedings of the 18th conference on Computa-tional linguistics, pages 495?501.Chin-Yew Lin and Eduard Hovy.
2003a.
Automatic eval-uation of summaries using n-gram co-occurance statis-tics.
In Proceedings of HLT-NAACL 2003.Chin-Yew Lin and Eduard Hovy.
2003b.
The potentialand limitations of automatic sentence extraction forsummarization.
In Proceedings of the HLT-NAACL 03on Text summarization workshop, pages 73?80.Chin-Yew Lin.
2004.
ROUGE: a package for automaticevaluation of summaries.
In ACL Text SummarizationWorkshop.H.
P. Luhn.
1958.
The automatic creation of literatureabstracts.
IBM Journal of Research and Development,2(2):159?165.K.
McKeown, R. Barzilay, D. Evans, V. Hatzivassiloglou,B.
Schiffman, and S. Teufel.
2001.
Columbia multi-document summarization: Approach and evaluation.In DUC?01.Kathleen McKeown, Regina Barzilay, David Evans,Vasleios Hatzivassiloglou, Judith Klavans, AniNenkova, Carl Sable, Barry Schiffman, and SergeySigelman.
2002.
Tracking and summarizing newson a daily basis with columbia?s newsblaster.
In Pro-ceedings of the 2nd Human Language TechnologiesConference HLT-02.Ani Nenkova, Lucy Vanderwende, and Kathleen McKe-own.
2006.
A compositional context sensitive multi-document summarizer: exploring the factors that influ-ence summarization.
In Proceedings of SIGIR.Dragomir Radev and Daniel Tam.
2003.
Single-document and multi-document summary evaluationvia relative utility.
In Poster session, InternationalConference on Information and Knowledge Manage-ment (CIKM?03).Dragomir Radev, Hongyan Jing, Malgorzata Sty, andDaniel Tam.
2004.
Centroid-based summarizationof multiple documents.
Information Processing andManagement, 40:919?938.Elad Yom-Tov, Shai Fine, David Carmel, and Adam Dar-low.
2005.
Learning to estimate query difficulty: in-cluding applications to missing content detection anddistributed information retrieval.
In SIGIR ?05: Pro-ceedings of the 28th annual international ACM SIGIRconference on Research and development in informa-tion retrieval, pages 512?519.833
