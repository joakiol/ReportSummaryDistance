Improving Testsuites via Instrumentat ionNorber t  BrSkerEschenweg 369231 RauenbergGermanynorber t ,  b roeker@sap,  comAbst ractThis paper explores the usefulness of a techniquefrom software ngineering, namely code instrumen-tation, for the development of large-scale naturallanguage grammars .
Information about the usageof g rammar  rules in test sentences is used to detectuntested rules, redundant test sentences, and likelycauses of overgeneration.
Results show that lessthan half of a large-coverage grammar  for German isactually tested by two large testsuites, and that i0-30% of testing t ime is redundant.
The  methodo logyapplied can be seen as a re-use of g rammar  writingknowledge for testsuite compilation.1 In t roduct ionComputational Linguistics (CL) has moved towardsthe marketplace: One finds programs employing CL-techniques in every software shop: Speech Recogni-tion, Grammar and Style Checking, and even Ma-chine Translation are available as products.
Whilethis demonstrates the applicability of the researchdone, it also calls for a rigorous developmentmethodology of such CL application products.In this paper,lI describe the adaptation of a tech-nique from Software Engineering, namely code in-strumentation, to grammar development.
Instru-mentation is based on the simple idea of markingany piece of code used in processing, and evaluatingthis usage information afterwards.
The applicationI present here is the evaluation and improvement ofgrammar and testsuites; other applications are pos-sible.1.1 Software Engineering vs. GrammarEngineeringBoth software and grammar development are simi-lar processes: They result in a system transformingsome input into some output, based on a functionalspecification (e.g., cf.
(Ciravegna et al, 1998) for theapplication of a particular software design method-ology to linguistic engineering).
Although Grammar1The work reported here was conducted uring my t imeat the Institut fiir Maschinelle Sprachverarbeitung (IMS),Stuttgart University, Germany.Engineering usually is not based on concrete specifi-cations, research from linguistics provides an infor-mal specification.Software Engineering developed many methods toassess the quality of a program, ranging from staticanalysis of the program code to dynamic testing ofthe program's behavior.
Here, we adapt dynamictesting, which means running the implemented pro-gram against a set of test cases.
The test casesare designed to maximize the probability of detect-ing errors in the program, i.e., incorrect conditions,incompatible assumptions on subsequent branches,etc.
(for overviews, cf.
(Hetzel, 1988; Liggesmeyer,1990)).1.2 Instrumentation in GrammarEngineeringHow can we fruitfully apply the idea of measuringthe coverage of a set of test cases to grammar devel-opment?
I argue that by exploring the relation be-tween grammar and testsuite, one can improve bothof them.
Even the traditional usage of testsuites toindicate grammar gaps or overgeneration can profitfrom a precise indication of the grammar ules usedto parse the sentences (cf.
Sec.4).
Conversely, onemay use the grammar to improve the testsuite, bothin terms of its coverage (cf.
Sec.3.1) and its economy(cf.
Sec.3.2).Viewed this way, testsuite writing can benefit fromgrammar development because both describe thesyntactic onstructions of a natural anguage.
Test-suites systematically ist these constructions, whilegrammars give generative procedures to constructthem.
Since there are currently many more gram-mars than testsuites, we may re-use the work thathas gone into the grammars for the improvement oftestsuites.The work reported here is situated in a large coop-erative project aiming at the development of large-coverage grammars for three languages.
The gram-mars have been developed over years by differentpeople, which makes the existence of tools for navi-gation, testing, and documentation mandatory.
Al-though the sample rules given below are in the for-mat of LFG, nothing of the methodology relies on325VP~V $=T;NP?$= (I" OBJ);PP* {$= (T OBL);156 ($ ADJUNCT);}.Figure 1: Sample Rulethe choice of linguistic or computational paradigm.2 Grammar  Ins t rumentat ionMeasures from Software Engineering cannot be sim-ply transferred to Grammar Engineering, becausethe structure of programs is different from that ofunification grammars.
Nevertheless, the s t ructureof a grammar allows the derivation of suitable mea-sures, similar to the structure of programs; this isdiscussed in Sec.2.1.
The actual instrumentation ofthe grammar depends on the formalism used, and isdiscussed in Sec.2.2.2.1 Coverage  Cr i te r iaConsider the LFG grammar ule in Fig.
1.
2 On firstview, one could require of a testsuite that each suchrule is exercised at least once.
~r ther  thought willindicate that there are hidden alternatives, namelythe optionality of the NP and the PP.
The rule canonly be said to be thoroughly tested if test cases existwhich test both presence and absence of optionalconstituents (requiring 4 test cases for this rule).In addition to context-free rules, unification gram-mars contain equations of various sorts, as illus-trated in Fig.1.
Since these annotations may alsocontain disjunctions, a testsuite with complete rulecoverage is not guaranteed to exercise all equationalternatives.
The phrase-structure-based criteriondefined above must be refined to cover all equationalternatives in the rule (requiring two test cases forthe PP annotation).
Even if we assume that (as,e.g., in LFG) there is at least one equation associ-ated with each constituent, equation coverage doesnot subsume rule coverage: Optional constituentsintroduce a rule disjunct (without the constituent)that is not characterizable by an equation.
A mea-sure might thus be defined as follows:d is junct  coverage The disjunct coverage of a test-suite is the quotientnumber of disjuncts testedTdis = number of disjuncts in grammar2Notation: ?/*/+ represent optionality/iteration includ-ing/excluding zero occurrences on categories.
Annotationsto a category specify equality (=) or set membership (6) offeature values, or non-existence offeatures (-1); they are ter-minated by a semicolon ( ; ).
Disjunctions are given in braces({... I-.. }).
$ ($) are metavariables representing the featurestructure corresponding tothe mother (daughter) of the rule.Comments are enclosed in quotation marks ("... ").
Cf.
(Ka-plan and Bresnan, 1982) for an introduction to LFG notation.where a disjunct is either a phrase-structure al-ternative, or an annotation alternative.
Op-tional constituents (and equations, if the for-malism allows them) have to be treated as adisjunction of the constituent and an empty cat-egory (cf.
the instrumented rule in Fig.2 for anexample).Instead of considering disjuncts in isolation, onemight take their interaction into account.
The mostcomplete test criterion, doing this to the fullest ex-tent possible, can be defined as follows:interact ion coverage The interaction coverage ofa testsuite is the quotientnumber of disjunct combinations testedTinter = number of legal disjunct combinationsThere are methodological problems in this cri-terion, however.
First, the set of legal com-binations may not be easily definable, due tofar-reaching dependencies between disjuncts indifferent rules, and second, recursion leads toinfinitely many legal disjunct combinations assoon as we take the number of usages of a dis-junct into account.
Requiring complete inter-action coverage is infeasible in practice, similarto the path coverage criterion in Software En-gineering.We will say that an analysis (and the sentencereceiving this analysis) rel ies on a grammar disjunctif this disjunct was used in constructing the analysis.2.2 Ins t rumentat ionBasically, grammar instrumentation is identical toprogram instrumentation: For each disjunct in agiven source grammar, we add grammar code thatwill identify this disjunct in the solution produced,iff that disjunct has been used in constructing thesolution.Assuming a unique numbering of disjuncts, an an-notation of the form DISJUNCT-nn = + can be usedfor marking.
To determine whether a certain dis-junct was used in constructing a solution, one onlyneeds to check whether the associated feature oc-curs (at some level of embedding) in the solution.Alternatively, if set-valued features are available,one can use a set-valued feature DISJUNCTS to col-lect atomic symbols representing one disjunct each:DISJUNCT-nn 6 DISJUNCTS.One restriction is imposed by using the unificationformalism, though: One occurrence of the mark can-not be distinguished from two occurrences, ince thesecond application of the equation introduces no newinformation.
The markers merely unify, and there isno way of counting.326VP=~V S-----t;{ e DISJUNCT-001 E o*;I NP $= ($ OBJ)DISJUNCT-002 E o*;}{ e DISJUNCT-003 E o*;I PP+{$= ($ OBL)DISJUNCT-004 E o.;I SE (J" ADJUNCT)DISJUNCT-005 E o*;}.
}Figure 2: Instrumented ruleTherefore, we have used a special feature of ourgrammar development environment: Following theLFG spirit of different representation levels associ-ated with each solution (so-called projections), itprovides for a multiset of symbols associated withthe complete solution, where structural embeddingplays no role (so-called optimality projection; see(Frank et al, 1998)).
In this way, from the rootnode of each solution the set of all disjuncts usedcan be collected, together with a usage count.Fig.
2 shows the rule from Fig.1 with such an in-strumentation; equations of the form DISJUNCT-nnEo* express membership of the disjunct-specific atomDISJUNCT-nn in the sentence's multiset of disjunctmarkers.2.3 Process ing  ToolsTool support is mandatory for a scenario such asinstrumentation: Nobody will manually add equa-tions such as those in Fig.
2 to several hundred rules.Based on the format of the grammar ules, an algo-rithm instrumenting a grammar can be written downeasily.Given a grammar and a testsuite or corpus to com-pare, first an instrumented grammar must be con-structed using such an algorithm.
This instrumentedgrammar is then used to parse the testsuite, yieldinga set of solutions associated with information aboutusage of grammar disjuncts.
Up to this point, theprocess is completely automatic.
The following twosections discuss two possibilities to evaluate this in-formation.3 Qua l i ty  o f  Tes tsu i tesThis section addresses the aspects of completeness("does the testsuite exercise all disjuncts in thegrammar?")
and economy of a testsuite ("is it min-imal?
").Complement ing  other work  on testsuite construc-tion (cf.
Sec.5), we  will assume that a g rammaris already available, and  that a testsuite has to beconstructed or extended.
Whi le  one may argue thatg rammar  and testsuite should be developed in paral-lel, such that the coding of a new grammar  disjunctis accompanied by the addition of suitable test cases,and vice versa, this is seldom the case.
Apart fromthe existence of grammars which lack a testsuite,and to which this procedure could be usefully ap-plied, there is the more principled obstacle of theevolution of the grammar, leading to states wherepreviously necessary rules silently loose their use-fulness, because their function is taken over by someother rules, structured ifferently.
This is detectableby instrumentation, as discussed in Sec.3.1.On the other hand, once there is a testsuite, youwant to use it in the most economic way, avoidingredundant ests.
Sec.3.2 shows that there are dif-ferent levels of redundancy in a testsuite, dependenton the specific grammar used.
Reduction of this re-dundancy can speed up the test activity, and give aclearer picture of the grammar's performance.3.1 Testsuite CompletenessIf the disjunct coverage of a testsuite is 1 for somegrammar, the testsuite is complete w.r.t, this gram-mar.
Such a testsuite can reliably be used to mon-itor changes in the grammar: Any reduction in thegrammar's coverage will show up in the failure ofsome test case (for negative test cases, cf.
Sec.4).If there is no complete testsuite, one can - via in-strumentation - identify disjuncts in the grammarfor which no test case exists.
There might be either(i) appropriate, but untested, disjuncts calling forthe addition of a test case, or (ii) inappropriate dis-juncts, for which one cannot construct a grammat-ical test case relying on them (e.g., left-overs fromrearranging the grammar).
Grammar instrumenta-tion singles out all untested isjuncts automatically,but cases (i) and (ii) have to be distinguished man-ually.Checking completeness of our local testsuite of1787 items, we found that only 1456 out of 3730grammar disjuncts ir~ our German grammar weretested, yielding Tdis = 0.39 (the TSNLP testsuitecontaining 1093 items tests only 1081 disjuncts,yielding Tdis = 0.28).
3 Fig.3 shows an exampleof a gap in our testsuite (there are no examples ofcircumpositions), while Fig.4 shows an inapproppri-ate disjunct thus discovered (the category ADVadjhas been eliminated in the lexicon, but not in allrules).
Another error class is illustrated by Fig.5,which shows a rule that can never be used due to anLFG coherence violation; the grammar is inconsis-tent here.
43There are, of course, unparsed but grammatical test casesin both testsuites, which have not been taken into accountin these figures.
This explains the difference to the overallnumber of 1582 items in the German TSNLP testsuite.4Test cases using a free dative pronoun may be in the test-suite, but receive no analysis ince the grammatical functionFREEDAT is not defined as such in the configuration section.327PPstd=> Pprae 4=$;NPstd 4 = (1" OBJ);{ e DISJUNCT-011 E o*;I Pcircum4=1`;DISJUNCT-012 E o.
"unused disjunct" ;}Figure 3: Appropriate untested isjunctADVP=~ { { e DISJUNCT-021 E o*;I ADVadj 4=1`DISJUNCT-022 E o*"unused disjunct" ;}ADVstd 4=1"DISJUNCT-023 E o,"unused disjunct" ;}I .
, .
}Figure 4: Inappropriate disjunct3.2 Testsuite EconomyBesides being complete, a testsuite must be econom-ical, i.e., contain as few items as possible withoutsacrificing its diagnostic apabilities.
Instrumenta-tion can identify redundant test cases.
Three criteriacan be applied in determining whether a test case isredundant:s imi lar i ty  There is a set of other test cases whichjointly rely on all disjunct on which the test caseunder consideration relies.equivalence There is a single test case which relieson exactly the same combination(s) ofdisjuncts.st r ict  equ iva lence There is a single test casewhich is equivalent o and, additionally, relieson the disjuncts exactly as often as, the testcase under consideration.For all criteria, lexical and structural ambiguitiesmust be taken into account.
Fig.6 shows some equiv-alent test cases derived from our testsuite: Exam-ple 1 illustrates the distinction between equivalenceand strict equivalence; the test cases contain differ-ent numbers of attributive adjectives, but are nev-ertheless considered equivalent.
Example 2 showsthat our grammar does not make any distinction be-tween adverbial usage and secondary (subject or ob-ject) predication.
Example 3 shows test cases whichshould not be considered equivalent, and is discussedbelow.The reduction we achieved in size and processingtime is shown in Table 1, which contains measure-ments for a test run containing only the parseabletest cases, one without equivalent test cases (for ev-ery set of equivalent est cases, one was arbitrar-VPargs ~ { ...I PRONstd4= (1" FREEDAT)(4 CASE) -- dat(4 PRON-TYPE) = pers-~(t OBJ2)DISJUNCT-041 E o*"unused disjunct" ;I .
.
.
}.Figure 5: Inconsistent disjunct1 ein guter alter Weinein guter alter trockener Wein'a good old (dry) wine'2 Er ifit das Schnitzel roh.Er iBt das Schnitzel nackt.Er ii3t das Schnitzel schnell.
'He eats the schnitzel naked/raw/quickly.
'3 Otto versucht oft zu lachen.Otto versucht zu lachen.
'Otto (often) tries to laugh.
'Figure 6: Sets of equivalent test casesily selected), and one without similar test cases.The last was constructed using a simple heuristic:Starting with the sentence relying on the most dis-juncts, working towards sentences relying on fewerdisjuncts, a sentence was selected only if it relied ona disjunct on which no previously selected sentencerelied.
Assuming that a disjunct working correctlyonce will work correctly more than once, we did notconsider strict equivalence.We envisage the following use of this redundancydetection: There clearly are linguistic reasons to dis-tinguish all test cases in example 2, so they cannotsimply be deleted from the testsuite.
Rather, theirequivalence indicates that the grammar is not yetperfect (or never will be, if it remains purely syn-tactic).
Such equivalences could be interpreted as%-TSNLP testsuiteparseable 1093 100% 1537 100%no equivalents 783 71% 665.3 43%no similar cases 214 19% 128.5 8%3561local testsuiteparseable 1787 100% 1213 100%no equivalents 1600 89% 899.5 74%no similar cases 331 18% 175.0 14%5480Table 1: Reduction of Testsuites328a reminder which linguistic distinctions need to beincorporated into the grammar.
Thus, this level ofredundancy may drive your grammar developmentagenda.
The level of equivalence can be taken asa limited interaction test: These test cases repre-sent one complete selection of grammar disjuncts,and (given the grammar) there is nothing we cangain by checking a test case if an equivalent one wastested.
Thus, this level of redundancy may be usedfor ensuring the quality of grammar changes priorto their incorporation i to the production version ofthe grammar.
The level of similarity contains muchless test cases, and does not test any (systematic)interaction between disjuncts.
Thus, it may be usedduring development as a quick rule-of-thumb proce-dure detecting serious errors only.Coming back to example 3 in Fig.6, buildingequivalence classes also helps in detecting rammarerrors: If, according to the grammar, two cases areequivalent which actually aren't, the grammar is in-correct.
Example 3 shows two test cases which aresyntactically different in that the first contains theadverbial oft, while the other doesn't.
The reasonwhy they are equivalent is an incorrect rule that as-signs an incorrect reading to the second test case,where the infinitival particle "zu" functions as anadverbial.4 Negat ive  Test  CasesTo control overgeneration, appropriately marked un-grammatical sentences are important in every test-suite.
Instrumentation as proposed here only looksat successful parses, but can still be applied in thiscontext: If an ungrammatical test case receives ananalysis, instrumentation i forms us about the dis-juncts used in the incorrect analysis.
One (or more)of these disjuncts must be incorrect, or the sentencewould not have received a solution.
We exploit thisinformation by accumulation across the entire testsuite, looking for disjuncts that appear in unusu-ally high proportion in parseable ungrammatical testcases .In this manner, six grammar disjuncts are singledout by the parseable ungrammatical test cases in theTSNLP testsuite.
The most prominent disjunct ap-pears in 26 sentences (listed in Fig.7), of which group1 is really grammatical and the rest fall into twogroups: A partial VP with object NP, interpretedas an imperative sentence (group 2), and a weirdinteraction with the tokenizer incorrectly handlingcapitalization (group 3).Far from being conclusive, the similarity of thesesentences derived from a suspicious grammar dis-junct, and the clear relation of the sentences to onlytwo exactly specifiable grammar errors make it plau-sible that this approach is very promising in reducingovergeneration.1 Der Test fg.llt leicht.Die schlafen.3 Man schlafen .Dieser schlafen .Ich schlafen .Der schlafen.Jeder schlafen.Derjenige schlafen .Jener schlafen .Keiner schlafen .Derselbe schlafen.Er schlafen.Irgendjemand schlafen .Dieselbe schlafen .Das schlafen .Eines schlafen.Jede schlafen .Dieses schlafen.Eine schlafen .Meins schlafen.Dasjenige schlafen.Jedes schlafen .Diejenige schlafen.Jenes schlafen.Keines schlafen .Dasselbe schlafen.Figure 7: Sentences relying on a suspicious disjunct5 Other  Approaches  to  Testsu i teConst ruct ionAlthough there are a number of efforts to constructreusable large-coverage t stsuites, none has to myknowledge xplored how existing grammars could beused for this purpose.Starting with (Flickinger et al, 1987), testsuiteshave been drawn up from a linguistic viewpoint, "in-\]ormed by \[the\] study of linguistics and \[reflecting\]the grammatical issues that linguists have concernedthemselves with" (Flickinger et al, 1987, , p.4).
Al-though the question is not explicitly addressed in(Balkan et al, 1994), all the testsuites reviewed therealso seem to follow the same methodology.
TheTSNLP project (Lehmann and Oepen, 1996) andits successor DiET (Netter et al, 1998), which builtlarge multilingual testsuites, likewise fall into thiscategory.The use of corpora (with various levels of annota-tion) has been studied, but even here the recommen-dations are that much manual work is required toturn corpus examples into test cases (e.g., (Balkanand Fouvry, 1995)).
The reason given is that cor-pus sentences neither contain linguistic phenomenain isolation, nor do they contain systematic varia-tion.
Corpora thus are used only as an inspiration.
(Oepen and Flickinger, 1998) stress the inter-dependence between application and testsuite, butdon't comment on the relation between grammarand testsuite.6 Conc lus ionThe approach presented tries to make available thelinguistic knowledge that went into the grammar fordevelopment of testsuites.
Grammar developmentand testsuite compilation are seen as complemen-tary and interacting processes, not as isolated mod-ules.
We have seen that even large testsuites coveronly a fraction of existing large-coverage rammars,329and presented evidence that there is a considerableamount of redundancy within existing testsuites.To empirically validate that the procedures out-lined above improve grammar and testsuite, carefulgrammar development is required.
Based on the in-formation derived from parsing with instrumentedgrammars, the changes and their effects need to beevaluated.
In addition to this empirical work, instru-mentation can be applied to other areas in Gram-mar Engineering, e.g., to detect sources of spuriousambiguities, to select sample sentences relying on adisjunct for documentation, or to assist in the con-struction of additional test cases.
Methodologicalwork is also required for the definition of a practicaland intuitive criterion to measure limited interactioncoverage.Each existing rammar development environmentundoubtely offers at least some basic tools for com-paring the grammar's coverage with a testsuite.
Re-grettably, these tools are seldomly presented pub-licly (which accounts for the short list of such refer-ences).
It is my belief that the thorough discussionof such infrastructure items (tools and methods) isof more immediate importance to the quality of thelingware than the discussion of open linguistic prob-lems.ReferencesL.
Balkan and F. Fouvry.
1995.
Corpus-based testsuite generation.
TSNLP-WP 2.2, University ofEssex.L.
Balkan, S. Meijer, D. Arnold, D. Estival, andK.
Falkedal.
1994.
Test Suite Design AnnotationScheme.
TSNLP-WP2.2, University of Essex.F.
Ciravegna, A. Lavelli, D. Petrelli, and F. Pianesi.1998.
Developing language reesources and appli-cations with geppetto.
In Proc.
1st Int'l Con/.
onLanguage Resources and Evaluation, pages 619-625.
Granada/Spain, 28-30 May 1998.D.
Flickinger, J. Nerbonne, I.
Sag, and T. Wa-sow.
1987.
Toward Evaluation o/ NLP Systems.Hewlett-Packard Laboratories, Palo Alto/CA.A.
Frank, T.H.
King, J. Kuhn, and J. Maxwell.1998.
Optimality theory style constraint rankingin large-scale fg gramma.
In Proc.
of the LFG98Con/erence.
Brisbane/AUS, Aug 1998, CSLI On-line Publications.W.C.
Hetzel.
1988.
The complete guide to softwaretesting.
QED Information Sciences, Inc. Welles-ley/MA 02181.R.M.
Kaplan and J. Bresnan.
1982.
Lexical-functional grammar: A formal system for gram-matical representation.
In J. Bresnan and R.M.Kaplan, editors, The Mental Representation ofGrammatical Relations, pages 173-281.
Cam-bridge, MA: MIT Press.S.
Lehmann and S. Oepen.
1996.
TSNLP - TestSuites for Natural Language Processing.
In Proc.16th Int'l Con\].
on Computational Linguistics,pages 711-716.
Copenhagen/DK.P.
Liggesmeyer.
1990.
Modultest und Modulverij~ka-tion.
Angewandte Informatik 4.
Mannheim: BIWissenschaftsverlag.K.
Netter, S. Armstrong, T. Kiss, J. Klein, andS.
Lehman.
1998.
Diet - diagnostic and eval-uation tools for nlp applications.
In Proc.
1stInt'l Con/.
on Language Resources and Evalua-tion, pages 573-579.
Granada/Spain, 28-30 May1998.S.
Oepen and D.P.
Flickinger.
1998.
Towards sys-tematic grammar profiling:test suite techn.
10years afte.
Journal of Computer Speech and Lan-guage, 12:411-435.330
