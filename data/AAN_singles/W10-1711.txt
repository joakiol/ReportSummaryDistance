Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 93?97,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsThe RWTH Aachen Machine Translation System for WMT 2010Carmen Heger, Joern Wuebker, Matthias Huck, Gregor Leusch,Saab Mansour, Daniel Stein and Hermann NeyRWTH Aachen UniversityAachen, Germanysurname@cs.rwth-aachen.deAbstractIn this paper we describe the statisti-cal machine translation system of theRWTH Aachen University developed forthe translation task of the Fifth Workshopon Statistical Machine Translation.
State-of-the-art phrase-based and hierarchicalstatistical MT systems are augmentedwith appropriate morpho-syntactic en-hancements, as well as alternative phrasetraining methods and extended lexiconmodels.
For some tasks, a system combi-nation of the best systems was used to gen-erate a final hypothesis.
We participatedin the constrained condition of German-English and French-English in each trans-lation direction.1 IntroductionThis paper describes the statistical MT systemused for our participation in the WMT 2010 sharedtranslation task.
We used it as an opportunity to in-corporate novel methods which have been investi-gated at RWTH over the last year and which haveproven to be successful in other evaluations.For all tasks we used standard alignment andtraining tools as well as our in-house phrase-based and hierarchical statistical MT decoders.When German was involved, morpho-syntacticpreprocessing was applied.
An alternative phrase-training method and additional models were testedand investigated with respect to their effect for thedifferent language pairs.
For two of the languagepairs we could improve performance by systemcombination.An overview of the systems and models will fol-low in Section 2 and 3, which describe the base-line architecture, followed by descriptions of theadditional system components.
Morpho-syntacticanalysis and other preprocessing issues are cov-ered by Section 4.
Finally, translation results forthe different languages and system variants arepresented in Section 5.2 Translation SystemsFor the WMT 2010 Evaluation we used stan-dard phrase-based and hierarchical translation sys-tems.
Alignments were trained with a variant ofGIZA++.
Target language models are 4-gram lan-guage models trained with the SRI toolkit, usingKneser-Ney discounting with interpolation.2.1 Phrase-Based SystemOur phrase-based translation system is similar tothe one described in (Zens and Ney, 2008).
Phrasepairs are extracted from a word-aligned bilingualcorpus and their translation probability in both di-rections is estimated by relative frequencies.
Ad-ditional models include a standard n-gram lan-guage model, phrase-level IBM1, word-, phrase-and distortion-penalties and a discriminative re-ordering model as described in (Zens and Ney,2006).2.2 Hierarchical SystemOur hierarchical phrase-based system is similar tothe one described in (Chiang, 2007).
It allows forgaps in the phrases by employing a context-freegrammar and a CYK-like parsing during the de-coding step.
It has similar features as the phrase-based system mentioned above.
For some sys-tems, we only allowed the non-terminals in hierar-chical phrases to be substituted with initial phrasesas in (Iglesias et al, 2009), which gave better re-sults on some language pairs.
We will refer to thisas ?shallow rules?.2.3 System CombinationThe RWTH approach to MT system combinationof the French?English systems as well as theGerman?English systems is a refined version ofthe ROVER approach in ASR (Fiscus, 1997) with93German?English French?English English?FrenchBLEU # Phrases BLEU # Phrases BLEU # PhrasesStandard 19.7 128M 25.5 225M 23.7 261MFA 20.0 12M 25.9 35M 24.0 33MTable 1: BLEU scores on Test and phrase table sizes with and without forced alignment (FA).
ForGerman?English and English?French phrase table interpolation was applied.additional steps to cope with reordering betweendifferent hypotheses, and to use true casing infor-mation from the input hypotheses.
The basic con-cept of the approach has been described by Ma-tusov et al (2006).
Several improvements havebeen added later (Matusov et al, 2008).
This ap-proach includes an enhanced alignment and re-ordering framework.
Alignments between the sys-tems are learned by GIZA++, a one-to-one align-ment is generated from the learned state occupa-tion probabilities.From these alignments, a confusion network(CN) is then built using one of the hypotheses as?skeleton?
or ?primary?
hypothesis.
We do notmake a hard decision on which of the hypothe-ses to use for that, but instead combine all pos-sible CNs into a single lattice.
Majority voting onthe generated lattice is performed using the priorprobabilities for each system as well as other sta-tistical models such as a special trigram languagemodel.
This language model is also learned onthe input hypotheses.
The intention is to favorlonger phrases contained in individual hypotheses.The translation with the best total score within thislattice is selected as consensus translation.
Scal-ing factors of these models are optimized similarto MERT using the Downhill Simplex algorithm.As the objective function for this optimization, weselected a linear combination of BLEU and TERwith a weight of 2 on the former; a combinationthat has proven to deliver stable results on sev-eral MT evaluation measures in preceding experi-ments.In contrast to previous years, we now include aseparate consensus true casing step to exploit thetrue casing capabilities of some of the input sys-tems: After generating a (lower cased) consensustranslation from the CN, we sum up the counts ofdifferent casing variants of each word in a sen-tence over the input hypotheses, and use the ma-jority casing over those.
In previous experiments,this showed to work significantly better than us-ing a fixed non-consensus true caser, and main-tains flexibility on the input systems.3 New Additional Models3.1 Forced AlignmentFor the German?English, French?English andEnglish?French language tasks we applied aforced alignment procedure to train the phrasetranslation model with the EM algorithm, sim-ilar to the one described in (DeNero et al,2006).
Here, the phrase translation probabil-ities are estimated from their relative frequen-cies in the phrase-aligned training data.
Thephrase alignment is produced by a modifiedversion of the translation decoder.
In addi-tion to providing a statistically well-foundedphrase model, this has the benefit of produc-ing smaller phrase tables and thus allowingmore rapid experiments.
For the language pairsGerman?English and English?French the bestresults were achieved by log-linear interpolationof the standard phrase table with the generativemodel.
For French?English we directly used themodel trained by forced alignment.
A detaileddescription of the training procedure is given in(Wuebker et al, 2010).
Table 1 shows the systemperformances and phrase table sizes with the stan-dard phrase table and the one trained with forcedalignment after the first EM iteration.
We can seethat the generative model reduces the phrase tablesize by 85-90% while increasing performance by0.3% to 0.4% BLEU.3.2 Extended Lexicon ModelsIn previous work, RWTH was able to show thepositive impact of extended lexicon models thatcope with lexical context beyond the limited hori-zon of phrase pairs and n-gram language models.Mauser et al (2009) report improvements ofup to +1% in BLEU on large-scale systems forChinese?English and Arabic?English by incor-porating discriminative and trigger-based lexiconmodels into a state-of-the-art phrase-based de-coder.
They discuss how the two types of lexicon94models help to select content words by capturinglong-distance effects.The triplet model is a straightforward extensionof the IBM model 1 with a second trigger, and likethe former is trained iteratively using the EM al-gorithm.
In search, the triggers are usually on thesource side, i.e., p(e|f, f ?)
is modeled.
The path-constrained triplet model restricts the first sourcetrigger to the aligned target word, whereas the sec-ond trigger can move along the whole source sen-tence.
See (Hasan et al, 2008) for a detailed de-scription and variants of the model and its training.For the WMT 2010 evaluation, triplets mod-eling p(e|f, f ?)
were trained and applied di-rectly in search for all relevant language pairs.Path-constrained models were trained on the in-domain news-commentary data only and on thenews-commentary plus the Europarl data.
Al-though experience from similar setups indicatesthat triplet lexicon models can be beneficial formachine translation between the languages En-glish, French, and German, on this year?s WMTtranslation tasks slight improvements on the devel-opment sets did not or only partially carry over tothe held-out test sets.
Nevertheless, systems withtriplets were used for system combination, as ex-tended lexicon models often help to predict con-tent words and to capture long-range dependen-cies.
Thus they can help to find a strong consensushypothesis.3.3 Unsupervised TrainingDue to the small size of the English?German re-sources available for language modeling as well asfor lexicon extraction, we decided to apply the un-supervised adaptation suggested in (Schwenk andSenellart, 2009).
We use a baseline SMT system totranslate in-domain monolingual source data, fil-ter the translations according to a decoder scorenormalized by sentence length, add this syntheticbilingual data to the original one and rebuild theSMT system from scratch.The motivation behind the method is that thephrase table will adapt to the genre, and thuslet phrases which are domain related have higherprobabilities.
Two phenomena are observed fromphrase tables and the corresponding translations:?
Phrase translation probabilities are changed,making the system choose better phrasetranslation candidates.Running WordsEnglish GermanBilingual 44.3M 43.4MDict.
1.4M 1.2MAFP 610.7MAFP unsup.
152.0M 157.3MTable 2: Overview on data for unsupervised train-ing.BLEUDev Testbaseline 15.0 14.7+dict.
15.1 14.6+unsup.+dict 15.4 14.9Table 3: Results for unsupervised training method.?
Phrases which appear repeatedly in the do-main get higher probabilities, so that the de-coder can better segment the sentence.To implement this idea, we translate the AFP partof the English LDC Gigaword v4.0 and obtain thesynthetic data.To decrease the number of OOV words, we usedictionaries from the stardict directory as addi-tional bilingual data to translate the AFP corpus.We filter sentences with OOV words and sentenceslonger than 100 tokens.
A summary of the addi-tional data used is shown in Table 2.We tried to use the best 10%, 20% and 40% ofthe synthetic data, where the 40% option workedbest.
A summary of the results is given in Table 3.Although this is our best result for theEnglish?German task, it was not submitted, be-cause the use of the dictionary is not allowed inthe constrained track.4 Preprocessing4.1 Large Parallel DataIn addition to the provided parallel Europarl andnews-commentary corpora, also the large French-English news corpus (about 22.5 Mio.
sentencepairs) and the French-English UN corpus (about7.2 Mio.
sentence pairs) were available.
Sincemodel training and tuning with such large cor-pora takes a very long time, we extracted about2 Mio.
sentence pairs of both of these corpora.
Wefilter sentences with the following properties:95?
Only sentences of minimum length of 4 to-kens were considered.?
At least 92% of the vocabulary of each sen-tence occur in the development set.?
The ratio of the vocabulary size of a sen-tence and the number of its tokens is mini-mum 80%.4.2 Morpho-Syntactic AnalysisGerman, as a flexible and morphologically richlanguage, raises a couple of problems in machinetranslation.
We picked two major problems andtackled them with morpho-syntactic pre- and post-processing: compound splitting and long-rangeverb reordering.For the translation from German into English,German compound words were split using thefrequency-based method described in (Koehn andKnight, 2003).
Thereby, we forbid certain wordsand syllables to be split.
For the other trans-lation direction, the English text was first trans-lated into the modified German language withsplit compounds.
The generated output was thenpostprocessed by re-merging the previously gen-erated components using the method described in(Popovic?
et al, 2006).Additionally, for the German?English phrase-based system, the long-range POS-based reorder-ing rules described in (Popovic?
and Ney, 2006)were applied on the training and test corpora as apreprocessing step.
Thereby, German verbs whichoccur at the end of a clause, like infinitives andpast participles, are moved towards the beginningof that clause.
With this, we improved our baselinephrase-based system by 0.6% BLEU.5 Experimental ResultsFor all translation directions, we used the providedparallel corpora (Europarl, news) to train the trans-lation models and the monolingual corpora to trainBLEUDev Testphrase-based baseline 19.9 19.2phrase-based (+POS+mero+giga) 21.0 20.3hierarchical baseline 20.2 19.6hierarchical (+giga) 20.5 20.1system combination 21.4 20.4Table 4: Results for the German?English task.the language models.
We improved the French-English systems by enriching the data with parts ofthe large addional data, extracted with the methoddescribed in Section 4.1.
Depending on the sys-tem this gave an improvement of 0.2-0.7% BLEU.We also made use of the large giga-news as wellas the LDC Gigaword corpora for the French andEnglish language models.
All systems were opti-mized for BLEU score on the development data,newstest2008.
The newstest2009 data isused as a blind test set.In the following, we will give the BLEU scoresfor all language tasks of the baseline system andthe best setup for both, the phrase-based and thehierarchical system.
We will use the followingnotations to indicate the several methods we used:(+POS) POS-based verb reordering(+mero) maximum entropy reordering(+giga) including giga-news andLDC Gigaword in LM(fa) trained by forced alignment(shallow) allow only shallow rulesWe applied system combination of up to 6 sys-tems with several setups.
The submitted systemsare marked in tables 4-7.6 ConclusionFor the participation in the WMT 2010 sharedtranslation task, RWTH used state-of-the-artphrase-based and hierarchical translation systems.To deal with the rich morphology and word or-der differences in German, compound splittingand long range verb reordering were applied in apreprocessing step.
For the French-English lan-guage pairs, RWTH extracted parts of the largenews corpus and the UN corpus as additionaltraining data.
Further, training the phrase trans-lation model with forced alignment yielded im-provements in BLEU.
To obtain the final hypothe-sis for the French?English and German?EnglishBLEUDev Testphrase-based baseline 14.8 14.5phrase-based (+mero) 15.0 14.7hierarchical baseline 14.2 13.9hierarchical (shallow) 14.5 14.3Table 5: Results for the English?German task.96BLEUDev Testphrase-based baseline 21.8 25.1phrase-based (fa+giga) 23.0 26.1hierarchical baseline 21.9 25.0hierarchical (shallow+giga) 22.7 25.6system combination 23.1 26.1Table 6: Results for the French?English task.BLEUDev Testphrase-based baseline 20.9 23.2phrase-based (fa+mero+giga) 23.0 24.6hierarchical baseline 20.6 22.5hierarchical (shallow,+giga) 22.4 24.3Table 7: Results for the English?French task.language pairs, RWTH applied system combina-tion.
Altogether, by application of these meth-ods RWTH was able to increase performance inBLEU by 0.8% for German?English, 0.2% forEnglish?German, 1.0% for French?English and1.4% for English?French on the test set over therespective baseline systems.AcknowledgmentsThis work was realized as part of the Quaero Pro-gramme, funded by OSEO, French State agencyfor innovation.ReferencesD.
Chiang.
2007.
Hierarchical Phrase-Based Transla-tion.
Computational Linguistics, 33(2):201?228.J.
DeNero, D. Gillick, J. Zhang, and D. Klein.
2006.Why Generative Phrase Models Underperform Sur-face Heuristics.
In Proceedings of the Workshop onStatistical Machine Translation, pages 31?38.J.G.
Fiscus.
1997.
A Post-Processing System to YieldReduced Word Error Rates: Recognizer Output Vot-ing Error Reduction (ROVER).
In IEEE Workshopon Automatic Speech Recognition and Understand-ing.S.
Hasan, J. Ganitkevitch, H. Ney, and J. Andre?s-Ferrer.
2008.
Triplet Lexicon Models for Statisti-cal Machine Translation.
In Proceedings of Emperi-cal Methods of Natural Language Processing, pages372?381.G.
Iglesias, A. de Gispert, E.R.
Banga, and W. Byrne.2009.
Rule Filtering by Pattern for Efficient Hierar-chical Translation.
In Proceedings of the 12th Con-ference of the European Chapter of the ACL (EACL2009), pages 380?388.P.
Koehn and K. Knight.
2003.
Empirical Methods forCompound Splitting.
In Proceedings of EuropeanChapter of the ACL (EACL 2009), pages 187?194.E.
Matusov, N. Ueffing, and H. Ney.
2006.
Computingconsensus translation from multiple machine trans-lation systems using enhanced hypotheses align-ment.
In Conference of the European Chapter of theAssociation for Computational Linguistics (EACL),pages 33?40.E.
Matusov, G. Leusch, R.E.
Banchs, N. Bertoldi,D.
Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,J.B.
Marino, M. Paulik, S. Roukos, H. Schwenk, andH.
Ney.
2008.
System Combination for MachineTranslation of Spoken and Written Language.
IEEETransactions on Audio, Speech and Language Pro-cessing, 16(7):1222?1237.A.
Mauser, S. Hasan, and H. Ney.
2009.
Extend-ing Statistical Machine Translation with Discrimi-native and Trigger-Based Lexicon Models.
In Con-ference on Empirical Methods in Natural LanguageProcessing, pages 210?217.M.
Popovic?
and H. Ney.
2006.
POS-based Word Re-orderings for Statistical Machine Translation.
In In-ternational Conference on Language Resources andEvaluation, pages 1278?1283.M.
Popovic?, D. Stein, and H. Ney.
2006.
StatisticalMachine Translation of German Compound Words.In FinTAL - 5th International Conference on Nat-ural Language Processing, Springer Verlag, LNCS,pages 616?624.H.
Schwenk and J. Senellart.
2009.
Translation ModelAdaptation for an Arabic/French News TranslationSystem by Lightly-Supervised Training.
In MTSummit XII.J.
Wuebker, A. Mauser, and H. Ney.
2010.
TrainingPhrase Translation Models with Leaving-One-Out.In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics.
To ap-pear.R.
Zens and H. Ney.
2006.
Discriminative Reorder-ing Models for Statistical Machine Translation.
InWorkshop on Statistical Machine Translation, pages55?63.R.
Zens and H. Ney.
2008.
Improvements in DynamicProgramming Beam Search for Phrase-based Statis-tical Machine Translation.
In International Work-shop on Spoken Language Translation.97
