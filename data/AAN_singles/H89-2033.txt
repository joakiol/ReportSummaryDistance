Improved HMM Models for High Performance Speech RecognitionSteve Aust in,  Chr is  Barry ,  Yen-Lu  Chow,Man Derr,  Owen K imba l l ,  Francis  Kuba la ,  John Makhou lPaul  P laceway,  Wi l l iam Russel l ,  R ichard  Schwartz ,  George  YuBBN Systems and Techno log ies  Corporat ionCambr idge ,  MA 02138:~BSTRACTIn this paper we report on the various techniques thatwe tmplemented in order to improve the basic speechrecognition performance of the BYBLOS system.
Someot these methods are new, while others are not.
Wepresent methods that improved pertbrmance as well asthose that did not.
The methods include Linear Discrirn-inant Analysis, Supervised Vector Quantization, SharedMixture VQ.
Deleted Estimation of Context Weights,MMI Estimation Using "N-Best" Alternatives, Cross-Word Triphone Models.
While we have not yet com-bined all of the methods in one system, the overall wordrecognition error rate on the May 1988 test set using theWord-Pair grammar has decreased from 3.4% to 1.7%.l In t roduct ionWe considered several directions for trying to improvethe recognition accuracy within the basic framework ofthe BYBLOS system.
The various techniques can be rea-sonably grouped into three general topics: changing theunderlying distance metric in the spectral space, optimiz-ing the few weights that are used with the system, andimproving the phonetic oarticulation model by addingcross-word triphone context.
We introduce ach of theseareas below and discuss them in more detail in the bodyof the paper.
Finally, we will present recognition resultsfor a combination of two of the methods.Even in a discrete HMM system, there is an under-lying distance metric that is used to divide the spectralspace into distinct regions.
It has been suggested thatit is possible to improve recognition accuracy by per-forming a linear discriminant analysis.
We have alsoconsidered several methods of nonlinearly warping thespectral space as part of the vector quanlization process.We classify these methods as "supervised clustering"techniques.
In addition, we implemented the techniquethat has been called "tied mixture vector quantization"(Bellagarda, 1989) or semi-continuous densities (Huang,1989),In the BYBLOS system there are a number of systemparameters that are fixed for all speakers based on intu-itions and as a result of running a small number of luningexperiments.
Among these are the weights for the dif-ferent context-dependent models of phonemes and therelative weights for different feature sets (codebooks).While the weights chosen are certainly reasonable, onthe average, it would seem inconsistent to estimate mil-lions of ~ probabilities automatically while havinga handful of parameters set manually.
Therefore, weimplemented a deleted estimation algorithm to estimatethe context model weights and developed a new MMItechnique for estimating the teature set weights automat-ically.One obvious extension to context-dependent model-ing (which was introduced in BYBLOS in 1984) is tomodel context between phonemes that are not in thesame word.
In fact, three research sites (Paul, 1989;Lee, 1989; Murveit, 1989) reported modeling triphonecontext across word boundaries at the February 1989meeting.
We have now implemented a similar :algorithmin the BYBLOS system.
However, due to remarks fromother researchers that the changes to the training andrecognition programs were extensive and difficult to im-plement, we chose to implement the effect by precompil-mg all of the models in such a way that we did not needto change either the training or recognition programs.In sections 2 to 4 we describe the algorithms imple-mented under each of these areas along with results.In section 5 we present recogmtion results under sev-eral different conditions, including the test results forthe October '89 test set.2492 Dis tance Measures  and  Superv ised  VQThis section deals with techniques for improving the dis-tance measure used in VQ, in particular, using lineardiscriminant analysis, and nonlinear supervised cluster-ing techniques.
In addition, we present results when wereplace the discrete densities with shared-mixture d nsi-ties.2.1 L inear  D isc r iminant  Analys isIn our baseline system we compute 14 reel-frequencywarped cepstral coefficients (cl-cl4) every 10 ms di-rectly from the speech power spectrum.
These parame-ters are grouped in one codebook.
These 14 parametersare then used to compute "difference" parameters, laycomputing the slope of a least squares finear fit to afive-frame window centered around each frame.
The 14slopes of this fit for the coefficients then make up thesecond set ~codebook) of teatures.
Finally, we use athird codebook that has the log rms energy and the "dif-ference" of this energy.
The energy parameter is normal-ized relative to a decaying running maximum, so as tobe insensitive to arbitrary changes in amplitude.
We di-vide the 30 features among three codebooks to avoid thetraining problem associated with high dimensionalit3,.The recognition group at IBM (Brown, 1987) has pro-posed using several successive frames jointly in order tomodel the joint density more accurately together withlinear discriminant analysis (LDA) to reduce the num-ber of dimensions.
We have attempted to use LDA tofind a better set of features that could then be dividedinto sets that would, in fact be more independent.
In ad-dition, we might hope that we would automatically find amore beneficial weighting on the different features thansimple Euclidean distance (which is what we use in theVQ).First, we needed to define several classes that wewanted to discriminate.
We chose the (50 or sol basicphonemes as that set.
under the assumption that thesemodeled most of the distinctions that must be made inlarge vocabulary speech recognition.
We segment allof the training data into phonemes automatically usingthe decoder constrained to find the correct answer.
Therecognized segment boundaries are then used to assigna phoneme label to each frame.
Second, we computethe within (phoneme~ class and between class meansand covariances.
We use the generalized eigenvectorsolution to find best set of linear discriminant features.Third, we simply cluster and quantize the new featuresas usual.
Alternatively, we can divide the new featuresup into a small number of codebooks in order to reducethe quantization error.We pertbrmed experiments with several variations inthe number of codebooks and assignment of linear dis-criminants to codebooks.
However, the results (aver-aged over several test speakers) did not improve overthe baseline 3-codebook condition described at the be-ginning of this section.
We can draw two possible con-clusiotls from these results relative to previous uccesseswith this technique.
First, while it might be possible tofind a small number of discriminant directions that areimportant or a small vocabulary task - especially onewith minimal pair differences - it may not be as easy ina large vocabulary task, where the important distinctionsare many and also very varied.
That is, any choice ofdiscriminants hat is better for some distinctions may beworse for others.
Second.
it is not clear that optimiz-ing phonetic distinctions on single frames will help arecognition system that uses models of lriphones.2.2 Superv ised  Vector Quant i za t ionSince the simple linear discriminants did not improveresults, we chose to consider a more complex warpingof the feature space.
We classify the general area as su-pervised clustering or supervised VQ.
The basic idea isthat instead of finding a codebook that mimmizes meansquare error, without regard to phonetic similarity, weshould be able to use the training data to generate a code-book that tends to preserve differences that are phoneti-cally important, and disregard feature differences (evenif they are large) that are not phonetically important.Thus we attempt o maximize the mutual informationbetween the VQ clusters and phonetic identity.
We de-scribe two techniques below that seemed like they shouldaccomplish this goal.
While both methods were ableto derive a codebook that was more closely related tophonetic distance, neither esulted in an improvement inoverall continuous speech recognition accuracy.2.2.1 Binary Division of SpaceThe first algorithm is most closely related to thenonuniform binary clustering algorithm that we useto derive an initial estimate for k-means clustering(Roucos,Makhoul,Gish 85).
We label all the speechframes phonetically as described in the previous sec-uon.
All the labeled flames are initially in one cluster.Then, we iteratively divide the clusters until we have the250desired number.
One of the many clustering algorithmswe tried is given below.First we have a procedure to measure the entropy re-duction that would result from dividing a single clusterinto two:1. estimate a single Gaussian for the frames with eachphoneme label in the cluster.2.
in general there will be several different phonemelabels in the cluster?
Identify the two most "promi-nent" phonemes within the cluster.
The most effec-tive measure for this was simply the phoneme withthe most frames.3.
divide all data into two new clusters using thesetwo guassian distributions.4.
compute the difference between the entropy of thephoneme labels in the original cluster, and the av-?
erage entropy of the two new clusters, weighted bythe number samples in each subcluster.The outer loop repeatedly divides the cluster that willresult in the largest enropy reduction.l.
Place all the labeled frames initially in one cluster.2.
Using the above procedure compute the potentialentropy reduction that would be obtained upon fordividing each of the clusters.3.
Adopt the division that resulted in the largest en-tropy reduction.4.
Create two new clusters and measure the potentialentropy reduction for dividing each of the two re-sulting clusters as described above.5.
If we have fewer than 256 clusters, go to (3)The resulting hierarchical codebook was then used toquantize all of the training and test data.
When we ap-plied the above algorithm to a single set of features (say14), we found only a minor improvement in the mutualinformation above the case for unsupervised k-means.When we used all the features in one codebook, therewas a larger gain.
However, as with LDA, there was nogain in the overall recognition accuracy.2.2.2 LVQ2: Kohonen's Learning Vector QuantizerThe LVQ2 algorithm (Kohonen, 1988) was used very ef-fectively in a phoneme recognition system (McDermott,1989).
The algorithm amounts to a discriminative train-ing of the codebook means to maximize recognition offrame the labels.As before, we start with the set of phonemically la-beled frames.
Then we use the binary and k-means al-gorithm to divide the feature vectors from each phonemeinto several clusters.
We made the number of clustersfor each phoneme proportional to the square root of thenumber of frames in that phoneme, such that the totalnumber of clusters was 256.
Each cluster has the nameof the phoneme data in it.
Then, we use LVQ2 to jigglethe means to optimize frame recognition.For each teature vector:1. find the nearest two clusters2.
if the nearest cluster is from the wrong phoneme andthe second nearest is the correct phoneme, shift themean of the correct cluster toward the feature vectorin question and shift wrong cluster mean away.The above algorithm is iterated until convergence(which requires some care).
As suggested in the ref-erence, we used several adjacent speech frames togetheras a longer feature vector.
This resulted m significantlyhigher phoneme-frame r cognition rates, both from thek-means initial estimate, and after improvement withLVQ2.The LVQ2 algorithm was found to improve the framerecognition accuracy significantly (from 48% to 70%)on the training set, particularly for a large number ofdimensions.
However, the accuracy increased only to57% on an independent test set.
As before, there wasno gain overall system recognition accuracy.
This resultis in contrast o the vast improvements seen in (Mc-Dermott, 1989).
While one possible difference is thatthey used handmarked phoneme boundaries, m isolatedword utterances for both training and test, we believethat the important difference was probably that the finalrecognition task in their case was simply to recognizethe identity of the phoneme?
This was quite similar tothe optimization i  the LVQ2.The conclusion from these several efforts at improvingthe vector quantization or distance measure by lookingat the phoneme labels of single frames (or even clustersof frames) was that any gains that were achieved werenot relevant o the performance of the entire system?251Any method that would improve the vector quantizationmust be done witMn the context of the whole recogmtionsystem.2.3 Shared Mixture VQOne technique that partially avoids problems attributedto VQ is to use a fuzzy VQ technique (Tseng,ICASSP87) or a more rigorous hared mixture technique(Bellagarda, 1989).
The basic notion is that each of theVQ regions is now treated as a guassian distribution thatis shared by all of the probability densities in the entireHMM system.
One of the effects of this is that an inputfeature vector is no longer "in" one cluster or another.Instead, there is a probability that it belongs to severalclusters.
The probability of an input feature vector tor astate is now a weighted combination of the discrete prob-abilities of  the nearby clusters.
This might have somesmoothing effect on the discrete probability densities.
Italso might avoid some of the quantization effects, sincethe probability for an input feature vector would varycontinuously between two or more clusters.We implemented a subset of the pieces of the sharedmixture algorithms.
In particular, we decided to avoidthe computationally expensive reestimation of the mix-ture means and variances.
Instead, we estimated a meanand full covariance matrix from the training data thatfell within each of the original clusters.
Then, we couldcompute for each training or test frame, the probabilitythat it belonged to each of the 256 clusters.
We foundthat the nearest five clusters accounted for 99% of theprobability, and therefore discarded all but the nearestfive.
The five pairs of numbers (index and probabifity)then could replace the single VQ index in the probabilitylookup of either the training or recognition algorithms.We performed experiments with the shared mixturesin the decoder alone, or in the training and decoder.
Wefound a 10%-20% gain for just using it in the decoder.There was no gain for using it in the training.
While theeffect of shared mixtures might be similar to those ofother density smoothing algorithms, we found an addi-tional 5%-20% reduction in error rates for mixtures.
Thiscondition is included in the recognition results given atthe end of this paper.3 Optimizing System ParametersHere we describe two techmques for estimating lobalsystem parameters in the BYBLOS system.3.1 Deleted Estimation Of Context WeightsThe BYBLOS system interpolates all the different prob-ability densities of the context-dependent phonemes toobtain a robust estimate of the densities.
Currently weuse heuristic weights that are a function of:?
type of context (phone, left, right, triphone)?
number of occurrences in training (5 ranges)?
state in phone model (left.
middle, right)The values of these weights were set based on reason-able intuitions about the importance of phonetic ontextsand amount of training on different parts of a phoneme.We ran a few tuning experiments (on an earlier database)to determine rough scaling factors on the initial weights.Therefore, it is likely that we would see no further im-provement by estimating the weights automatically withdeleted estimation.
However, we might expect that ifwe estimated the weights automatically, we could usedifferent weights for each speaker.
We wanted to avoidany approximations if possible, due to assumptions aboutthe alignments remaimng fixed, and so we chose to it-eratively estimate the weights and then reestimate theprobability densities.We were womed about the effectiveness of thejackknifing procedure that is normally used, since theweights for combining models are estimated for the casewhere only half of the data was used to estimate themodels.
Therefore, we developed a method for hold-ing out only one utterance at a time, that was still veryefficient:Each normal pass of forward-backward is followedby a second pass that estimates the weights.
At the endof the forward-backward pass, we retain the "counts".In the second pass we remove the "counts" from onesentence at a time and then estimate context weightsusing that deleted sentence.1.
Run usual tbrward-backward iteration on all sen-tences2.
For each sentence:(a) Run forward-backward on this sentence using"old" model to determine its contribution tothe new model.tb) Subtract he contribution of this sentence fromthose models relevant o this sentence.252(c) Run forward-backward to compute weightcounts ffirom this sentence using the modelwith the contribution for this sentence re-moved.3.
Reesfimate the context weights from the weightcounts.4.
iterateThis algorithm requires only two times the compu-tation of the normal forward-backward algorithm, andshould result in a more accurate stimate of the weightsthan the usual procedure.
Unfortunately, when we ranour initial experiments, we found no improvement, de-spite the fact that the likelihood of the training data hadincreased somewhat.
It is possible that the initial heuris-tic weights are close enough, or that the "reasonable"comanuity constraints existing in the initial weights werelost when each weight was estimated independently.3.2 MMI  Es t imat ion  Us ing"N-Best"  A l ter -nat ivesWe have found in the past that the recognition resultscan be improved by optimizing the weights for the dif-ferent sets of features.
We felt that it would make sense,therefore, to estimate these weights automatically.
How-ever, since these weights are actually exponents on theprobability densities, it is not possible to estimate themusing maximum likelihood (ML) techniques.
Clearly,the largest likelihood would occur when all the weightswere large.
If we constrain the weights to sum to one,there is still a problem, since the ML solution woulddetermine one weight that would be equal to one, andthe others would be zero.
This can be shown easilyfor the Viterbi case by realizing that the final likelihoodis simply the product of the whole sentence likelihoodsdue to each codebook.
Therefore, we needed to use adiscriminative t chnique to estimate the feature weights.We chose to use Maximum Mutual Information(MMI) Estimation to estimate these (and possibly other)parameters.
In MMI, we want to maximize the likeli-hood of the correct answer (given the input) relative tothe likelihood of all the possible answers.
This typicallyis done by determining a set of alternative answers andperforming a gradient descent to improve the mutual in-formation.
The problem of finding good alternatives tothe correct answer is harder for continuous speech thanfor isolated words, where each alternative can be consid-ered explicitly.
However, the N-Best algorithm, (Chow,1989) which is described elsewhere in these proceedingscan be used to solve this problem.The N-Best algorithm is a time-synchronous Viterbi-style beam search algorithm that can be made to findthe most likely N whole sentence alternatives that arewithin a given a "beam" of the most likely utterance.The algorithm can be shown to be exact  under somereasonable constraints.
The computation is linear withthe length of the utterance, and faster than linear in N.We use the N-Best algorithm to generate a list of themost likely alternatives for each sentence in a held-outset.
We then explicitly compute the likelihood of thecorrect sentence (if it is not already in the list).
The mu-tual information for each sentence and its correspondingimposters is used to compute a set of weights for eachsentence hypothesis.
The weights for the correct sen-tences are positive, while the imposter sentences havenegative weights.
Then, we use all of the sentences (realand imposter) in the usual forward-backward algorithmwith the counts multiplied by the weight for the sen-tence.
States common to all sentence hypotheses for a~entence will get no counts.
Then, we compute the gra-dient directly from the counts and adjust the parametersaccordingly.We used the above algorithm to estimate the (three)feature set weights for each speaker separately.
We usedthe 600 training sentences to generate the models, whichwe assumed would not change.
Then we generated 10imposter sentences for each of the 100 development testsentences.
We used five iterations to optimize the code-book weights.
Then we evaluated the resulting modelson the February 1989 test data.
The result was a 10%reduction in error rate, relative to the initial weights,which were empirically optimized for all the speakers.The gain is somewhat small, but we are not sure howmuch gain to expect from optimizing only three param-eters.
Furthermore, we noticed that the gradient descentwas dominated by a few bad sentences that it probablycould not fix anyway.
We believe that this area needsmore work.4 Cross -Word  Tr iphone  Mode lsA model of phonetic oarticulation between words hasbeen proven to be effective by researchers at CMU,SRI, and Lincoln Labs (Paul, 1989; Lee, 1989; Murveit,1989).
However, we wanted to avoid changes to existingtraining and decoding programs.
Therefore, we devel-oped a compiler that reads a phonetic dictionary and a253word grammar and writes out a dictionary of triphonesand a triphone grammar.
That is, the new dictionary hasone "word" for each triphone (about 7,000 in this case),and the new grammar specifies allowable sequences ofthese triphones.
There are approximately 60,000 tri-phone arcs in the resulting grammar (for the word-pairgrammar).
Given this new dictionary and grammar, thetraining program did not need to change at all and therecognition program only needed to know how to writeout the real words instead of the triphone names - asmall change.
As a result, we were able to implementthe cross-word triphone ffect in only 5 weeks.We tested the new models on the May 1988 test data.The addition of cross-word triphone models reduced theword error rate by 30% as will be seen in the tables ofresults below.5 System Recognition ResultsThe table below compares the word error rate with sev-end combinations of smoothing, mixtures, and cross-word triphones.
The "smoothing" algorithm is the Tri-phone Coocurrence Smoothing algorithm that was pre-sented at the DARPA meeting in June '88.
"Mix-tures" means using the Shared Mixtures VQ as describedabove.
"X-Word" means using Cross-Word Triphonemodels (without smoothing or mixtures), And, the lastline includes all three algorithms.Results are shown for the different speaker-dependenttest sets, indicated by the dates of the test set.
Theresults for the baseline system and for the system withsmoothing have been reported previously for the May'88and Feb'89 test sets, and are given for reference.
Wehave been using the May'88 test set as our developmenttest set.
Therefore.
each of the conditions is shown forthis test set.
As can be seen, the error rate with the Word-Pair grammar has been reduced from 3.4% to 1.7%.
Wenever tested this configuration with no grammar until theOct'89 test.The results for the Oct'89 test set using all three al-gorithm extensions indicate that the word error rate withthe Word-Pair grammar is 2.5%, and the error rate withno grammar is 10.6%.
While these error rates representthe best performance r ported so far on this database, wewere surprised at the large increase in error rate from theMay'88 test to the Oct'89 test.
Therefore, we reran thesystem configuration used in February, 1989, which in-cluded only the smoothing algorithm.
As can be seen,the word error rate was 3.8% on the Oct. '89 test, ascompared with 2.7% on the May '88 test, which is con-sistent with the other esults.
It is clear that the October1989 test is significantly harder (at least for our system)than the May 1988 test set, perhaps because it comesfrom a different recording session.
However, the rela-tive improvements in the algorithms were observed inthe new test set as well as the old.Percent word error using Word-pair grammarSystemBaseline SystemSmoothSmooth + MixX-WordSmooth + Mix +X-WordMay '883.42.72.52.31.7Test SetFeb.
'892.93.1Oct.
'893.82.5Percent word error using no grammarTest SetSystemBaseline SystemSmoothSmooth + MixX-WordSmooth + Mix +X-WordMay '8816.215.812.6Feb.
'8915.313.8Oct.
'8910.66 Conc lus ionsWe draw several conclusions from this work:?
Supervising the VQ with phoneme identity does nothelp overall recognition performance.Shared mixtures in the decoder educes error rateby 10%-20% depending on the grammar, but aftersmoothing only by 5%-20%.We found no improvement for replacing the heuris-tically derived weights for the context-dependentmodels with weights determined by deleted estima-tion.We have implemented an algorithm for MMI train-ing in continuous speech that uses alternatives gen-erated by the N-Best algorithm.
Initial experimentsto optimize the three feature set weights using thisprocedure reduced word error rate by 10%.254As expectecL using cross-word tfiphone models re-duced word error rate by 30%.The word error rate using the Word-Pair grammar isnow close to 2%, depending on the test set.
Whenno grammar is used the error rate was 10.6% on theOct.
'89 test set.
Due to the very low error rate withthe Word-Pair grammar, we will use the statisticalclass grammar (Derr, 1989) for most of our testingas it will be easier to measure improvements u ingthis more difficult and more realistic grammar.AcknowledgementThis work was supported by the Defense AdvancedResearch Projects Agency and monitored by the Officeof Naval Research under Conllact Nos.
N0001~85-C-0279 and N00014-89-C-0008.\[7\] Lee, K.F., I-t.W.
Hon,,  and M.Y.
Hwang (1989) "Re-cent Progress in the Sphinx Speech Recognition System"Proceedings of the Feb. 1989 DARPA Speech and Natu-ral Language Workshop Morgan Kaufmann Publishers,Inc., Feb. 1989.\[8\] McDermott, E. and S. Katagifi (1989) "Shift-Invariant, Multi-Category Phoneme Recognition usingKohonen's LVQ2," IEEE ICASSP-89, pp.
81-84\[9\] Paul, D. (1989) "The Lincoln continuous speechrecognition system recent developments and results"Proceedings of the Feb. 1989 DARPA Speech and Natu-ral Language Workshop Morgan Kaufmann Publishers,Inc., Feb. 1989.References\[i\] BeUagard, J. and D. Nahamoo (1989) "Tied mixturecontinuous parameter models tor large vocabulary iso-lated speech recogmtion" IEEE ICASSP89\[2\] Brown, P. (1987) "The Acoustic-Modeling Problemin Automatic Speech Recognition" PhD Thesis, CMU,1987\[3\] Chow, Y.C.
and R.M.
Schwartz (1989) "The N-Best Algorithm: An Efficient Procedure for FindingTop N Sentence Hypotheses" Elsewhere in these Pro-ceedings of the Oct. 1989 DARPA Speech and NaturalLanguage Workshop Morgan Kaufmann Publishers, Inc.,Oct.
1989.\[4\] Derr, A. and R.M.
Schwartz (1989) "A StatisticalClass Grammar for Measunng Speech Recognition Per-forrnance" Elsewhere in these Proceedings of the Oct.1989 DARPA Speech and Natural Language WorkshopMorgan Kaulinann Publishers, Inc., Oct. 1989.\[5\] Huang, X.D.
and M.A.
Jack (1989) "Semi-continuoushidden Markov models for speech recognition" Com-puter Speech and Language.
Vol 3, 1989\[6\] Kohonen, T., G. Bama, and R. Chrisley (1988)"Statistical Pattern Recognition with Nerual Networks:Benchmarldng Studies," IEEE, Proc.
of lCNN, Vol.
1, pp.61-68, July, 1988255
