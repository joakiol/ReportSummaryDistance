Proceedings of the ACL Student Research Workshop, pages 13?18,Ann Arbor, Michigan, June 2005. c?2005 Association for Computational LinguisticsAn Extensive Empirical Study of Collocation Extraction MethodsPavel PecinaInstitute of Formal and Applied LinguisticsCharles University, Prague, Czech Republicpecina@ufal.mff.cuni.czAbstractThis paper presents a status quo of anongoing research study of collocations ?an essential linguistic phenomenon hav-ing a wide spectrum of applications inthe field of natural language processing.The core of the work is an empirical eval-uation of a comprehensive list of auto-matic collocation extraction methods us-ing precision-recall measures and a pro-posal of a new approach integrating mul-tiple basic methods and statistical classi-fication.
We demonstrate that combiningmultiple independent techniques leads toa significant performance improvement incomparisonwith individualbasic methods.1 Introduction and motivationNatural language cannot be simply reduced to lex-icon and syntax.
The fact that individual wordscannot be combined freely or randomly is commonfor most natural languages.
The ability of a wordto combine with other words can be expressed ei-ther intensionally or extensionally.
The former caserefers to valency.
Instances of the latter case arecalled collocations ( ?Cerm?k and Holub, 1982).
Theterm collocation has several other definitions butnone of them is widely accepted.
Most attemptsare based on a characteristic property of colloca-tions: non-compositionality.
Choueka (1988) de-fines a collocational expression as ?a syntactic andsemantic unit whose exact and unambiguous mean-ing or connotation cannot be derived directly fromthe meaning or connotation of its components?.The term collocation has both linguistic and lexi-cographic character.
It covers a wide range of lexicalphenomena, such as phrasal verbs, light verb com-pounds, idioms, stock phrases, technological ex-pressions, and proper names.
Collocations are ofhigh importance for many applications in the fieldof NLP.
The most desirable ones are machine trans-lation, word sense disambiguation, language genera-tion, and information retrieval.
The recent availabil-ity of large amounts of textual data has attracted in-terest in automatic collocation extraction from text.In the last thirty years a number of different methodsemploying various association measures have beenproposed.
Overview of the most widely used tech-niques is given e.g.
in (Manning and Sch?tze, 1999)or (Pearce, 2002).
Several researches also attemptedto compare existing methods and suggested differentevaluation schemes, e.g Kita (1994) or Evert (2001).A comprehensive study of statistical aspects of wordcooccurrences can be found in (Evert, 2004).In this paper we present a compendium of 84methods for automatic collocation extraction.
Theycame from different research areas and some of themhave not been used for this purpose yet.
A briefoverview of these methods is followed by their com-parative evaluation against manually annotated databy the means of precision and recall measures.
Inthe end we propose a statistical classification methodfor combining multiple methods and demonstrate asubstantial performance improvement.In our research we focus on two-word (bigram)collocations, mainly for the reason that experimentswith longer expressions would require processing ofmuch larger amounts of data and limited scalabilityof some methods to high order n-grams.
The exper-iments are performed on Czech data.132 Collocation extractionMost methods for collocation extraction are basedon verification of typical collocation properties.These properties are formally described by mathe-matical formulas that determine the degree of as-sociation between components of collocation.
Suchformulas are called association measures and com-pute an association score for each collocation candi-date extracted from a corpus.
The scores indicate achance of a candidate to be a collocation.
They canbe used for ranking or for classification ?
by settinga threshold.
Finding such a threshold depends on theintended application.The most widely tested property of collocations isnon-compositionality: If words occur together moreoften than by a chance, then this is the evidence thatthey have a special function that is not simply ex-plained as a result of their combination (Manningand Sch?tze, 1999).
We think of a corpus as a ran-domly generated sequence of words that is viewed asa sequence of word pairs.
Occurrence frequenciesof these bigrams are extracted and kept in contin-gency tables (Table 1a).
Values from these tables areused in several association measures that reflect howmuch the word coocurrence is accidental.
A list ofsuch measures is given in Table 2 and includes: es-timation of bigram and unigram probabilities (rows3?5), mutual information and derived measures (6?11), statistical tests of independence (12?16), likeli-hood measures (17?18), and various other heuristicassociation measures and coefficients (19?57).Another frequently tested property is taken di-rectly from the definition that a collocation is a syn-tactic and semantic unit.
For each bigram occurringin the corpus, information of its empirical context(frequencies of open-class words occurring withina specified context window) and left and right im-mediate contexts (frequencies of words immediatelypreceding or following the bigram) is extracted (Ta-ble 1b).
By determining the entropy of the im-mediate contexts of a word sequence, the associa-tion measures rank collocations according to the as-sumption that they occur as units in a (information-theoretically) noisy environment (Shimohata et al,1997) (58?62).
By comparing empirical contexts ofa word sequence and its components, the associa-tion measures rank collocations according to the as-a) a=f(xy) b=f(xy?)
f(x?
)c=f(x?y) d=f(x?y?)
f(x??
)f(?y) f(?y?)
Nb) Cw empirical context of wCxy empirical context of xyClxy left immediate context of xyCrxy right immediate context of xyTable 1: a) A contingency table with observed frequencies andmarginal frequencies for a bigram xy; w?
stands for any wordexcept w; ?
stands for any word; N is a total number of bi-grams.
The table cells are sometimes referred as fij .
Statisticaltests of independence work with contingency tables of expectedfrequencies f?(xy)=f(x?
)f(?y)/N .
b) Different notions of em-pirical contexts.sumption that semantically non-compositional ex-pressions typically occur in different contexts thantheir components (Zhai, 1997).
Measures (63?76)have information theory background and measures(77?84) are adopted from the field of informationretrieval.
Context association measures are mainlyused for extracting idioms.Besides all the association measures describedabove, we also take into account other recommendedmeasures (1?2) (Manning and Sch?tze, 1999) andsome basic linguistic characteristics used for filter-ing non-collocations (85?87).
This information canbe obtained automatically from morphological tag-gers and syntactic parsers available with reasonablyhigh accuracy for many languages.3 Empirical evaluationEvaluation of collocation extraction methods is acomplicated task.
On one hand, different applica-tions require different setting of association scorethresholds.
On the other hand, methods give differ-ent results within different ranges of their associa-tion scores.
We need a complex evaluation schemecovering all demands.
In such a case, Evert (2001)and other authors suggest using precision and recallmeasures on a full reference data or on n-best lists.Data.
All the presented experiments were per-formed on morphologically and syntactically anno-tated Czech text from the Prague Dependency Tree-bank (PDT) (Hajic?
et al, 2001).
Dependency treeswere broken down into dependency bigrams consist-ing of: lemmas and part-of-speech of the compo-nents, and type of dependence between the compo-nents.For each bigram type we counted frequencies inits contingency table, extracted empirical and imme-diate contexts, and computed all the 84 associationmeasures from Table 2.
We processed 81 614 sen-14# Name Formula1.
Mean component offset 1nPni=1 di2.
Variance component offset 1n?1Pni=1`di?d??23.
Joint probability P (xy)4.
Conditional probability P (y|x)5.
Reverse conditional prob.
P (x|y)?6.
Pointwise mutual inform.
log P (xy)P (x?
)P (?y)7.
Mutual dependency (MD) log P (xy)2P (x?
)P (?y)8.
Log frequency biased MD log P (xy)2P (x?
)P (?y)+logP (xy)9.
Normalized expectation 2f(xy)f(x?)+f(?y)?10.
Mutual expectation 2f(xy)f(x?
)+f(?y) ?P (xy)11.
Salience log P (xy)2P (x?
)P (?y) ?
logf(xy)12.
Pearson?s ?2 test Pi,j(fij?f?ij)2f?ij13.
Fisher?s exact test f(x?)!f(x??)!f(?y)!f(?y?)!N!f(xy)!f(xy?)!f(x?y)!f(x?y?)!14.
t test f(xy)?f?(xy)?f(xy)(1?(f(xy)/N))15.
z score f(xy)?f?(xy)?f?(xy)(1?(f?(xy)/N))16.
Poison significance measure f?
(xy)?f(xy) logf?(xy)+logf(xy)!logN17.
Log likelihood ratio ?2Pi,jfij logfijf?ij18.
Squared log likelihood ratio ?2Pi,jlogfij2f?ijAssociation coefficients:19.
Russel-Rao aa+b+c+d20.
Sokal-Michiner a+da+b+c+d?21.
Rogers-Tanimoto a+da+2b+2c+d22.
Hamann (a+d)?(b+c)a+b+c+d23.
Third Sokal-Sneath b+ca+d24.
Jaccard aa+b+c?25.
First Kulczynsky ab+c26.
Second Sokal-Sneath aa+2(b+c)27.
Second Kulczynski 12 (aa+b +aa+c )28.
Fourth Sokal-Sneath 14 (aa+b +aa+c +dd+b +dd+c )29.
Odds ratio adbc30.
Yulle?s ??ad??bc?ad+?bc?31.
Yulle?s Q ad?bcad+bc32.
Driver-Kroeber a?(a+b)(a+c)33.
Fifth Sokal-Sneath ad?(a+b)(a+c)(d+b)(d+c)34.
Pearson ad?bc?(a+b)(a+c)(d+b)(d+c)35.
Baroni-Urbani a+?ada+b+c+?ad36.
Braun-Blanquet amax(a+b,a+c)37.
Simpson amin(a+b,a+c)38.
Michael 4(ad?bc)(a+d)2+(b+c)239.
Mountford 2a2bc+ab+ac40.
Fager a?(a+b)(a+c)?
12max(b, c)41.
Unigram subtuples log adbc ?3.29q1a +1b +1c +1d42.
U cost log(1+ min(b,c)+amax(b,c)+a )43.
S cost log(1+min(b,c)a+1 )?1244.
R cost log(1+ aa+b )?log(1+aa+c )45.
T combined cost ?U?S?R46.
Phi P (xy)?P (x?
)P (?y)?P (x?
)P (?y)(1?P (x?
))(1?P (?y))47.
Kappa P (xy)+P (x?y?
)?P (x?
)P (?y)?P (x??
)P (?y?
)1?P (x?
)P (?y)?P (x??
)P (?y?)48.
J measure max[P (xy)logP (y|x)P (?y) +P (xy?
)logP (y?|x)P (?y?)
,P (xy)logP (x|y)P (x?)
+P (x?y)logP (x?|y)P (x??)
]# Name Formula49.
Gini index max[P (x?
)(P (y|x)2+P (y?|x)2)?P (?y)2+P (x??
)(P (y|x?
)2+P (y?|x?
)2)?P (?y?
)2,P (?y)(P (x|y)2+P (x?|y)2)?P (x?
)2+P (?y?
)(P (x|y?
)2+P (x?|y?
)2)?P (x??)2]50.
Confidence max[P (y|x), P (x|y)]51.
Laplace max[NP (xy)+1NP (x?
)+2 ,NP (xy)+1NP (?y)+2 ]52.
Conviction max[P (x?
)P (?y)P (xy?)
,P (x??
)P (?y)P (x?y) ]53.
Piatersky-Shapiro P (xy)?P (x?
)P (?y)54.
Certainity factor max[P (y|x)?P (?y)1?P (?y) ,P (x|y)?P (x?
)1?P (x?)
]55.
Added value (AV) max[P (y|x)?P (?y), P (x|y)?P (x?)]?56.
Collective strength P (xy)+P (x?y?
)P (x?
)P (y)+P (x??
)P (?y) ?1?P (x?
)P (?y)?P (x??
)P (?y)1?P (xy)?P (x?y?)57.
KlosgenpP (xy) ?AVContext measures:?58.
Context entropy ?Pw P (w|Cxy) logP (w|Cxy)59.
Left context entropy ?Pw P (w|Clxy) logP (w|Clxy)60.
Right context entropy ?Pw P (w|Crxy) logP (w|Crxy)?61.
Left context divergence P (x?)
logP (x?
)?PwP (w|Clxy) logP (w|Clxy)62.
Right context divergence P (?y) logP (?y)?PwP (w|Crxy) logP (w|Crxy)63.
Cross entropy ?PwP (w|Cx) logP (w|Cy)64.
Reverse cross entropy ?PwP (w|Cy) logP (w|Cx)65.
Intersection measure 2|Cx?Cy||Cx|+|Cy|66.
Euclidean normqPw(P (w|Cx)?P (w|Cy))267.
Cosine normPw P (w|Cx)P (w|Cy)Pw P (w|Cx)2?Pw P (w|Cy)268.
L1 norm Pw |P (w|Cx)?P (w|Cy)|69.
Confusion probability PwP (x|Cw)P (y|Cw)P (w)P (x?)70.
Reverse confusion prob.
PwP (y|Cw)P (x|Cw)P (w)P (?y)?71.
Jensen-Shannon diverg.
12 [D(p(w|Cx)|| 12 (p(w|Cx)+p(w|Cy)))+D(p(w|Cy)||12 (p(w|Cx)+p(w|Cy)))]72.
Cosine of pointwise MIPw MI(w,x)MI(w,y)?Pw MI(w,x)2?
?Pw MI(w,y)2?73.
KL divergence Pw P (w|Cx) logP (w|Cx)P (w|Cy)?74.
Reverse KL divergence Pw P (w|Cy) logP (w|Cy)P (w|Cx)75.
Skew divergence D(p(w|Cx)||?(w|Cy)+(1??)p(w|Cx))76.
Reverse skew divergence D(p(w|Cy)||?p(w|Cx)+(1??)p(w|Cy))77.
Phrase word coocurrence 12 (f(x|Cxy)f(xy) +f(y|Cxy)f(xy) )78.
Word association 12 (f(x|Cy)?f(xy)f(xy) +f(y|Cx)?f(xy)f(xy) )Cosine context similarity: 12 (cos(cx,cxy)+cos(cy,cxy))cz=(zi); cos(cx,cy)=Pxiyi?Pxi2??Pyi2?79.
in boolean vector space zi=?(f(wi|Cz))80.
in tf vector space zi=f(wi|Cz)81. in tf?idf vector space zi=f(wi|Cz)?
Ndf(wi); df(wi)= |{x :wi?Cx}|Dice context similarity: 12 (dice(cx,cxy)+dice(cy,cxy))cz=(zi); dice(cx,cy)=2PxiyiPxi2+Pyi2?82.
in boolean vector space zi=?(f(wi|Cz))?83.
in tf vector space zi=f(wi|Cz)?84.
in tf?idf vector space zi=f(wi|Cz)?
Ndf(wi); df(wi)= |{x :wi?Cx}|Linguistic features:?85.
Part of speech {Adjective:Noun, Noun:Noun, Noun:Verb, .
.
.
}?86.
Dependency type {Attribute, Object, Subject, .
.
.
}87.
Dependency structure {?,?
}Table 2: Association measures and linguistic features used in bigram collocation extraction methods.
?
denotes those selected bythe attribute selection method discussed in Section 4.
References can be found at the end of the paper.15tences with 1 255 590 words and obtained a total of202 171 different dependency bigrams.Krenn (2000) argues that collocation extractionmethods should be evaluated against a reference setof collocations manually extracted from the full can-didate data from a corpus.
However, we reduced thefull candidate data from PDT to 21 597 bigram byfiltering out any bigrams which occurred 5 or lesstimes in the data and thus we obtained a referencedata set which fulfills requirements of a sufficientsize and a minimal frequency of observations whichis needed for the assumption of normal distributionrequired by some methods.We manually processed the entire reference dataset and extracted bigrams that were considered to becollocations.
At this point we applied part-of-speechfiltering: First, we identified POS patterns that neverform a collocation.
Second, all dependency bigramshaving such a POS pattern were removed from thereference data and a final reference set of 8 904 bi-grams was created.
We no longer consider bigramswith such patterns to be collocation candidates.This data set contained 2 649 items considered tobe collocations.
The a priori probability of a bi-gram to be a collocation was 29.75 %.
A strati-fied one-third subsample of this data was selectedas test data and used for evaluation and testing pur-poses in this work.
The rest was taken apart and usedas training data in later experiments.Evaluation metrics.
Since we manually anno-tated the entire reference data set we could use thesuggested precision and recall measures (and theirharmonic mean F-measure).
A collocation extrac-tion method using any association measure with agiven threshold can be considered a classifier andthe measures can be computed in the following way:Precision =# correctly classified collocations# total predicted as collocationsRecall =# correctly classified collocations# total collocationsThe higher these scores, the better the classifier is.By changing the threshold we can tune the clas-sifier performance and ?trade?
recall for precision.Therefore, collocation extraction methods can bethoroughly compared by comparing their precision--recall curves: The closer the curve to the top rightcorner, the better the method is.10090806030100806040200Precision(%)Recall (%)baseline = 29.75 %Pointwise mutual informationPearson?s testMountfordKappaLeft context divergenceContext intersection measureCosine context similarity in boolean VSFigure 1: Precision-recall curves for selected assoc.
measures.Results.
Presenting individual results for all ofthe 84 association measures is not possible in a paperof this length.
Therefore, we present precision-recallgraphs only for the best methods from each groupmentioned in Section 2; see Figure 1.
The baselinesystem that classifies bigrams randomly, operateswith a precision of 29.75 %.
The overall best re-sult was achieved by Pointwise mutual information:30 % recall with 85.5 % precision (F-measure 44.4),60 % recall with 78.4 % precision (F-measure 68.0),and 90 % recall with 62.5 % precision (F-measure73.8).4 Statistical classificationIn the previous section we mentioned that collo-cation extraction is a classification problem.
Eachmethod classifies instances of the candidate data setaccording to the values of an association score.
Nowwe have several association scores for each candi-date bigram and want to combine them together toachieve better performance.
A motivating exampleis depicted in Figure 3: Association scores of Point-wise mutual information and Cosine context simi-larity are independent enough to be linearly com-bined to provide better results.
Considering all as-sociation measures, we deal with a problem of high-dimensional classification into two classes.In our case, each bigram x is described by theattribute vector x=(x1, .
.
.
, x87) consisting of lin-guistic features and association scores from Table 2.Now we look for a function assigning each bigramone class : f(x)?
{collocation, non-collocation}.The result of this approach is similar to setting athreshold of the association score in methods us-160.90.50.116.98.80.7CosinecontextsimilarityinbooleanvectorspacePointwise mutual informationcollocationsnon-collocationslinear discriminantFigure 2: Data visualization in two dimensions.
The dashed linedenotes a linear discriminant obtained by logistic linear regres-sion.
By moving this boundary we can tune the classifier output(a 5 % stratified sample of the test data is displayed).ing one association measure, which is not very use-full for our purpose.
Some classification meth-ods, however, output also the predicted probabilityP (x is collocation) that can be considered a regularassociation measure as described above.
Thus, theclassification method can be also tuned by changinga threshold of this probability and can be comparedwith other methods by the same means of precisionand recall.One of the basic classification methods that givesa predicted probability is Logistic linear regression.The model defines the predicted probability as:P (x is collocation) =exp?0+?1x1...+?nxn1 + exp?0+?1x1...+?nxnwhere the coefficients ?i are obtained by the iter-atively reweighted least squares (IRLS) algorithmwhich solves the weighted least squares problemat each iteration.
Categorial attributes need to betransformed to numeric dummy variables.
It is alsorecommended to normalize all numeric attributes tohave zero mean and unit variance.We employed the datamining software Weka byWitten and Frank (2000) in our experiments.
Astraining data we used a two-third subsample of thereference data described above.
The test data wasthe same as in the evaluation of the basic methods.By combining all the 87 attributes, we achievedthe results displayed in Table 3 and illustrated in Fig-ure 3.
At a recall level of 90 % the relative increasein precision was 35.2 % and at a precision level of90 % the relative increase in recall was impressive242.3 %.10090806030100806040200Precision(%)Recall (%)baseline = 29.75 %Logistic regression on all attributesLogistic regression on 17 selected attributesFigure 3: Precision-recall curves of two classifiers based oni) logistic linear regression on the full set of 87 attributes andii) on the selected subset with 17 attributes.
The thin unlabeledcurves refer to the methods from the 17 selected attributesAttribute selection.
In the final step of our exper-iments, we attempted to reduce the attribute space ofour data and thus obtain an attribute subset with thesame prediction ability.
We employed a greedy step-wise search method with attribute subset evaluationvia logistic regression implemented in Weka.
It per-forms a greedy search through the space of attributesubsets and iteratively merges subsets that give thebest results until the performance is no longer im-proved.We ended up with a subset consisting of the fol-lowing 17 attributes: (6, 10, 21, 25, 31, 56, 58, 61, 71,73, 74, 79, 82, 83, 84, 85, 86) which are also marked inTable 2.
The overview of achieved results is shownin Table 3 and precision-recall graphs of the selectedattributes and their combinations are in Figure 3.5 Conclusions and future workWe implemented 84 automatic collocation extrac-tion methods and performed series of experimentson morphologically and syntactically annotateddata.
The methods were evaluated against a refer-ence set of collocations manually extracted from theRecall Precision30 60 90 70 80 90P.
mutual information 85.5 78.4 62.5 78.0 56.0 16.3Logistic regression-17 92.6 89.5 84.5 96.7 86.7 55.8Absolute improvement 7.1 11.1 22.0 17.7 30.7 39.2Relative improvement 8.3 14.2 35.2 23.9 54.8 242.3Table 3: Precision (the 3 left columns) and recall (the 3 rightcolumns) scores (in %) for the best individual method and linearcombination of the 17 selected ones.17same source.
The best method (Pointwise mutual in-formation) achieved 68.3 % recall with 73.0 % pre-cision (F-measure 70.6) on this data.
We proposedto combine the association scores of each candidatebigram and employed Logistic linear regression tofind a linear combination of the association scoresof all the basic methods.
Thus we constructed a col-location extraction method which achieved 80.8 %recall with 84.8 % precision (F-measure 82.8).
Fur-thermore, we applied an attribute selection tech-nique in order to lower the high dimensionality ofthe classification problem and reduced the numberof regressors from 87 to 17 with comparable perfor-mance.
This result can be viewed as a kind of evalu-ation of basic collocation extraction techniques.
Wecan obtain the smallest subset that still gives the bestresult.
The other measures therefore become unin-teresting and need not be further processed and eval-uated.The reseach presented in this paper is in progress.The list of collocation extraction methods and as-sociation measures is far from complete.
Our longterm goal is to collect, implement, and evaluate allavailable methods suitable for this task, and releasethe toolkit for public use.In the future, we will focus especially on im-proving quality of the training and testing data, em-ploying other classification and attribute-selectiontechniques, and performing experiments on Englishdata.
A necessary part of the work will be a rigoroustheoretical study of all applied methods and appro-priateness of their usage.
Finally, we will attempt todemonstrate contribution of collocations in selectedapplication areas, such as machine translation or in-formation retrieval.AcknowledgmentsThis research has been supported by the Ministryof Education of the Czech Republic, project MSM0021620838.
I would also like to thank my advisor,Dr.
Jan Hajic?, for his continued support.ReferencesY.
Choueka.
1988.
Looking for needles in a haystack or lo-cating interesting collocational expressions in large textualdatabases.
In Proceedings of the RIAO, pages 43?38.I.
Dagan, L. Lee, and F. Pereira.
1999.
Similarity-based modelsof word cooccurrence probabilities.
Machine Learning, 34.T.
E. Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguistics,19(1):61?74.S.
Evert and B. Krenn.
2001.
Methods for the qualitative eval-uation of lexical association measures.
In Proceedings 39thAnnual Meeting of the Association for Computational Lin-guistics, pages 188?195.S.
Evert.
2004.
The Statistics of Word Cooccurrences: WordPairs and Collocations.
Ph.D. thesis, University of Stuttgart.J.
Hajic?, E.
Hajic?ov?, P. Pajas, J.
Panevov?, P. Sgall, andB.
Vidov?-Hladk?.
2001.
Prague dependency treebank 1.0.Published by LDC, University of Pennsylvania.K.
Kita, Y. Kato, T. Omoto, and Y. Yano.
1994.
A comparativestudy of automatic extraction of collocations from corpora:Mutual information vs. cost criteria.
Journal of Natural Lan-guage Processing, 1(1):21?33.B.
Krenn.
2000.
Collocation Mining: Exploiting Corpora forCollocation Idenfication and Representation.
In Proceedingsof KONVENS 2000.L.
Lee.
2001.
On the effectiveness of the skew divergencefor statistical language analysis.
Artificial Inteligence andStatistics, pages 65?72.C.
D. Manning and H. Sch?tze.
1999.
Foundations of Statis-tical Natural Language Processing.
The MIT Press, Cam-bridge, Massachusetts.D.
Pearce.
2002.
A comparative evaluation of collocation ex-traction techniques.
In Third International Conference onlanguage Resources and Evaluation, Las Palmas, Spain.T.
Pedersen.
1996.
Fishing for exactness.
In Proceedings ofthe South Central SAS User?s Group Conference, pages 188?200, Austin, TX.S.
Shimohata, T. Sugio, and J. Nagata.
1997.
Retrieving col-locations by co-occurrences and word order constraints.
InProc.
of the 35th Annual Meeting of the ACL and 8th Con-ference of the EACL, pages 476?81, Madrid.
Spain.P.
Tan, V. Kumar, and J. Srivastava.
2002.
Selecting the rightinterestingness measure for association patterns.
In Proceed-ings of the Eight A CM SIGKDD International Conferenceon Knowledge Discovery and Data Mining.A.
Thanopoulos, N. Fakotakis, and G. Kokkinakis.
2002.
Com-parative evaluation of collocation extraction metrics.
In 3rdInternational Conference on Language Resources and Eval-uation, volume 2, pages 620?625, Las Palmas, Spain.F.
?Cerm?k and J. Holub.
1982.
Syntagmatika a paradigmatikac?esk eho slova: Valence a kolokabilita.
St?tn?
pedagogick?nakladatelstv?, Praha.I.
H. Witten and E. Frank.
2000.
Data Mining: Practicalmachine learning tools with Java implementations.
MorganKaufmann, San Francisco.C.
Zhai.
1997.
Exploiting context to identify lexical atoms?
A statistical view of linguistic context.
In Internationaland Interdisciplinary Conference on Modelling and UsingContext (CONTEXT-97).18
