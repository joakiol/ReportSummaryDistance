Semantic Role Labeling using Maximum Entropy ModelJoon-Ho Lim, Young-Sook Hwang, So-Young Park, Hae-Chang RimDepartment of Computer Science & Engineering Korea University5-ka, Anam-dong, SEOUL, 136-701, KOREA{jhlim, yshwang, ssoya, rim}@nlp.korea.ac.krAbstractIn this paper, we propose a semantic role label-ing method using a maximum entropy model,which enables not only to exploit rich featuresbut also to alleviate the data sparseness prob-lem in a well-founded model.
For applying themaximum entropy model to semantic role la-beling, we take a incremental approach as fol-lows: firstly, the semantic roles are assigned tothe arguments in the immediate clause includ-ing a predicate, and then, the semantic roles areassigned to the arguments in the upper clausesby using previously assigned labels.
The exper-imental result shows that the proposed methodhas about 64.76% (F1-measure) on the test set.1 IntroductionThe semantic role represents the relationship between apredicate and an argument.
It provides a general semanticinterpretation of the sentence, and it can play a key rolein NLP.
The shared task of CoNLL-2004 concerns theautomatic semantic role labeling (Carreras, 2004).
Thechallenge for this task is to come forward with machinelearning approaches which based on only partial syntacticinformation such as words, POS tags, chunks, clauses,and named entities.Some machine learning approaches for semantic rolelabeling have been previously developed (Gildea, 2002;Pradhan, 2003; Thompson, 2003).
Gildea (2002) pro-posed a probabilistic discriminative model to assign asemantic roles to the constituent.
However, it needs acomplex interpolation for smoothing because of the datasparseness problem.
Pradhan (2003) applied a supportvector machine to semantic role labeling, but if it usea polynomial kernel function for the dependencies be-tween features, it requires high computational complex-ity.
Futhermore, becuase the SVM is a binary classifier,one-vs-rest or pairwise method is required for multi-classclassification.
Thompson (2003) proposed a probabilis-tic generative model which the constituents is generatedby the semantic roles.
In this model, because a con-stituent depends only on the role that generated it, andconstituents are independent of each other, so this modelcan not utilize contextual information or a relational in-formation between the constituent and the predicate.In this paper, we propose a semantic role labelingmethod using a maximum entropy model.
It is moti-vated by the thought of that for building a successfulmodel, some knowledge of the task are reflected intothe model based on the machine learning technique.
Inthis method, we try to combine the structural linguisticknowledge linking syntax to semantics into the machinelearning technique.
It is realized in terms of two aspects:one is the model framework, the other is the design offeature sets.
First of all, for the model framework, we uti-lize the syntactic knowledge of representing the semanticroles in a clause: the arguments of a predicate are locatedin the immediate clause or the upper clauses.
Secondly,for the feature sets, we consider the relation between syn-tactic and semantic characteristics of a given context.
Forimplementing the method with a machine learning algo-rithm, we take a maximum entropy model, which enablesnot only to exploit rich features but also to alleviate thedata sparseness problem in a well-founded model.The remaining of the paper is organized as follows:section 2 describes the proposed semantic role labelingmethod using a maximum entropy model.
Section 3presents feature sets for semantic role labeling.
Section 4shows some experimental results of the proposed method.Finally, section 5 concludes with some directions of fu-ture works.2 Semantic Role Labeling using MEIn the maximum entropy framework (Berger, 1996), theconditional probability of predicting an outcome y givenFigure 1: An example of the semantic role labels and an incremental approach.a history x is defined as follows :P (y|x) =1Z(x)exp(k?i=1?ifi(x, y))where fi(x, y) is the feature function, ?iis the weightingparameter of fi(x, u), k is the number of features, andZ(x) is the normalization factor for?yp(y|x) = 1.Given a predicate and its partial parse tree representedby constituents such as chunks and clauses, the proba-bilistic model for semantic role labeling assigns the se-mantic role labels to the constituents as described in theequation (1).Rbest= argmaxRP (R|c1n, pred)= argmaxR?ni=1P (ri|c1n, pred, r1...i?1) (1)where R is a sequence of the semantic roles, c1nis a se-quence of constituents, pred is the given predicate, riisthe i-th semantic role, n is the number of constituents.In order to apply the equation (1) to an incrementalapproach, we classify clauses into the immediate clauseand the upper clause.
The immediate clause is the clausewhich contains the target predicate, and the upper clauseis the clause which includes the immediate clause.
Gen-erally, most of the arguments of the predicate are locatedin the immediate clause while some of them are locatedin the upper clauses, especially the first or second up-per clauses.
Since it is much easier and more reliable toidentify the arguments in the immediate clause, the pro-posed method first assigns the semantic role labels to theconstituents 1 in the immediate clause.
Then, it assignsthe semantic role labels to the constituents in the upperclauses by using previously assigned labels.
This incre-mental approach is described in the equation (2) derivedfrom the equation (1).Rbest= argmaxR?ni=1P (ri|c1n, pred, r1...i?1)?
argmaxR?mi=1P (ri|?1(c1n, pred, r1...i?1))?
?ni=m+1P (ri|?2(c1n, pred, r1...i?1)) (2)1Here, we regard a chunk or a clause as a constituent.where m is the number of constituents covered by the im-mediate clause, ?1is a feature set for immediate clause,and ?2is a feature set for upper clauses.A semantic role label(ri) is represented by using a BIOnotation such as B-A*, I-A*, etc.
However, O is too fre-quently occurred than other semantic role labels, it canhave a somewhat high probability than others.
Therefore,to degrade its probability, we divide the single O into O-,O+, O0with respect to the position of a constituent whichis relative to the predicate.
Therefore, B-A*, I-A*, O-,O+, and O0 are used as semantic roles as shown in Fig-ure 1.After processing the equation (2), we use some heuris-tic to attach the some semantic roles and to adjust theboundary of semantic arguments in the post-processingstep.
More specifically, we use some rules to attach theV, AM-MOD, and AM-NEG, and extend the boundaryof core roles to include to infinitive of the VP chunk like?expect/B-VP (A1 to/I-VP take/I-VP dive/B-NP)?.3 Feature Sets for Semantic Role LabelingFor accurate semantic role labeling, we regard that thefollowing information is important: the contextual infor-mation of the constituent, the syntactic information of thepredicate, and the relation between the constituent andthe predicate.
Therefore, we use the features presented inTable 1 for semantic role labeling.
For example, Figureprevious-label(pl)predicate-POS(predpos), predicate-lex(predlex)predicate-type(predtype)tag(ctag), voice(v), position(p), path(path)head-lex(hl), head-POS(hp), content-head(chl)prev-tag(ptag), prev-head-lex(phl)next-tag(ntag), next-head-lex(nhl)path-immediate-clause(path-im-cl)path-begin-end(path-beg-end)level-of-clause(l-cl), is-clause-boundary(cl-bn)immediate-clause-roles(im-cl-roles)Table 1: Features for semantic role labeling.Figure 2: Some instances extracted from example of Figure 1.feature set ?1for immediate clause feature set ?2for upper clausepl, ctag, ctag+v+p, ctag+v+p+pl pl, ctag, ctag+v+pptag+ctag, ctag+ntag ptag+ctag, ctag+ntaghp+p, hp+p+ntag hp+p, hl+ctag,predtype+ctag predtype+ctagpredlex+hl, predlex+ctag+v+p, predlex+ctag+pl predlex+hl, predlex+ctag+v+p, predlex+ctag+plpredpos+p, predpos+hp+pl, predpos+ctagpath, path+hp+v, path+nhl, path+predlex path-im-cl, path-im-cl+ctag+v, path-beg-endhl+p, hl+ctag, hl+ctag+predlex ctag+l-cl, ptag+ctag+l-cl, ctag+ntag+l-clchl+pl, chl+pl+predlex ctag+cl-bn, ptag+ctag+cl-bn, ctag+ntag+cl-bnchl+phl, chl+phl+predlex im-cl-rolesTable 2: Conjoined Feature Sets2 shows how the features in Table 1 are used for labelingsemantic roles to the proposition in Figure 1.Because the maximum entropy model assumes the in-dependence of features, we should conjoin the coherentfeatures.
As presented in Table 2, we use the conjoinedfeature sets to assign semantic roles to the constituents ofthe immediate clause and the upper clauses.The predicate-type feature represents the predicate us-age such as to-infinitive form (TO), the beginning of theimmediate clause (BEG), and otherwise (SEN).
The tagfeature represents the tag of the current constituent.
If itis a clause, it is subdivided into a relative pronoun, a in-finitival relative clause, etc according to its representedform.The path feature indicates the sequence of constituenttags between the current constituent and the predicate.The voice feature is determined to be an active or pas-sive voice of the predicate, and the position feature is as-signed by the constituent position with respect to pred-icate.
These features implicitly represent the predicate-argument relation such as predicate-subject or predicate-object.For the headword feature, we use the Collins?
head-word rules, and as a complementary feature to the headword feature, a content word feature2 is used to representthe content of the PP, VP, or CONJP chunk.The path-immediate-clause feature is the sequence ofconstituent tags between the current constituent and theimmediate clause, and the path-begin-end feature is thesequence between current constituent and beginning/endof clause.
The level-of-clause feature indicates whetherthe current constituent is located in the first upper clauseor in the second upper clause, and the is-clause-boundaryfeature is the binary value which indicates the existenceof the starting clause.
The immediate-clause-roles fea-tures are the binary indicators to represent whether thecore arguments exist in the immediate clause or not.The path-immediate-clause, path-begin-end, level-of-clause, is-clause-boundary, and immediate-clause-rolesfeatures are used only in the second phase, and the othersexcept the path feature and the content-head feature areused in common.2For example, if the PP-chunk is because of, the headwordfeature is of, and the content word feature is because.Precision Recall F?=1Overall 68.42% 61.47% 64.76A0 79.20% 75.73% 77.42A1 67.41% 64.65% 66.00A2 52.65% 45.94% 49.07A3 52.53% 34.67% 41.77A4 63.16% 48.00% 54.55A5 0.00% 0.00% 0.00AM-ADV 46.75% 35.18% 40.15AM-CAU 57.69% 30.61% 40.00AM-DIR 48.28% 28.00% 35.44AM-DIS 60.11% 50.23% 54.73AM-EXT 58.33% 50.00% 53.85AM-LOC 35.56% 35.09% 35.32AM-MNR 51.26% 23.92% 32.62AM-MOD 89.77% 91.10% 90.43AM-NEG 86.15% 88.19% 87.16AM-PNC 48.98% 28.24% 35.82AM-PRD 100.00% 33.33% 50.00AM-TMP 59.51% 42.70% 49.73R-A0 86.96% 75.47% 80.81R-A1 57.89% 62.86% 60.27R-A2 50.00% 33.33% 40.00R-A3 0.00% 0.00% 0.00R-AM-LOC 33.33% 25.00% 28.57R-AM-MNR 0.00% 0.00% 0.00R-AM-PNC 0.00% 0.00% 0.00R-AM-TMP 42.86% 21.43% 28.57V 97.99% 97.99% 97.99Table 3: Experimental results on the test set.4 ExperimentsTo test the proposed method, we have experimented onCoNLL-2004 datasets.
For our experiments, we use theZhang le?s MaxEnt toolkit 3, and the L-BFGS parame-ter estimation algorithm with Gaussian Prior smoothing(Chen, 1999).
The results on the test set are shown inTable 3, and Table 4 shows the overall results when themodel is tested on the training set, the development set,and the test set.From these experimental results, we can find that theproposed model has relatively high performance on thelabels related to A0 and A1, while it has relatively lowperformance on the other labels.
This may be caused byfollowing two reasons.
Firstly, the instances of A0 or A1are provided enough for accurate semantic role labeling.Secondly, the thematic roles of A0 and A1 are more clearthan other core semantic roles.
For example, agent is la-beled as mainly A0 while benefactive can be labeled asA2 or A3.
Therefore, the maximum entropy model can3http://www.nlplab.cn/zhangle/maxent toolkit.htmlPrecision Recall F?=1Overall(training) 96.40% 92.28% 94.29Overall(dev) 69.78% 62.56% 65.97Overall(test) 68.42% 61.47% 64.76Table 4: The results when the model is tested on the train-ing set, the development set, and the test setget a good generalize performance in case of A0 or A1,but can?t generalize well in other cases.5 ConclusionIn this paper, we propose a semantic role labeling methodusing a maximum entropy model.
Because the maximumentropy model enables not only to exploit rich featuresbut also to alleviate the data sparseness problem, we useit to model the probability of a semantic role label se-quence.
The proposed method has following characteris-tics: firstly, it assigns the semantic role labels to the con-stituents in the immediate clause, and then assigns rolelabels to the constituents in the upper clauses, and it uti-lizes the relation between syntactic and semantic charac-teristics of a given context.For the future work, we will device a method of clus-tering for the path and predicate features, and include theclustering results as additional features.ReferencesAdam Berger, Stephen Della Pietra, and Vincent DellaPietra.
1996.
A maximum entropy approach to natu-ral language processing .
Computational Linguistics,22(1):39?71.Xavier Carreras, and Lluis Marquez.
2004.
Introduc-tion to the CoNLL-2004 Shared Task: Semantic RoleLabeling .
Proceedings of CoNLL-2004.S.
Chen and R. Rosenfeld.
1999.
A Gaussian prior forsmoothing maximum entropy models .
Technical Re-port CMUCS-99-108, Carnegie Mellon University.Daniel Gildea, and Daniel Jurafsky.
2002.
Automatic La-beling of Semantic Roles .
Computational Linguistics,28(3):245-288.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James H. Martin and Daniel Jurafsky.Oct 2003.
Support Vector Learning for Semantic Ar-gument Classification .
Technical Report, TR-CSLR-2003-03.Cynthia A. Thompson, Roger Levy and Christopher D.Manning.
A Generative Model for Semantic Role La-beling .
ECML 2003, 397-408.
