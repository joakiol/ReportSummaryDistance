Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 121?128Manchester, August 2008Other-Anaphora Resolution in Biomedical Texts with AutomaticallyMined PatternsChen Bin#, Yang Xiaofeng$, Su Jian^ and Tan Chew Lim*#*School of Computing, National University of Singapore$^Institute for Infocomm Research, A-STAR, Singapore{#chenbin, *tancl}@comp.nus.edu.sg{$xiaofengy, ^sujian}@i2r.a-star.edu.sg?
AbstractThis paper proposes an other-anaphoraresolution approach in bio-medical texts.It utilizes automatically mined patterns todiscover the semantic relation between ananaphor and a candidate antecedent.
Theknowledge from lexical patterns is incor-porated in a machine learning frameworkto perform anaphora resolution.
The ex-periments show that machine learningapproach combined with the auto-minedknowledge is effective for other-anaphora resolution in the biomedicaldomain.
Our system with auto-mined pat-terns gives an accuracy of 56.5%., yield-ing 16.2% improvement against the base-line system without pattern features, and9% improvement against the system us-ing manually designed patterns.1 IntroductionThe last decade has seen an explosive growth inthe amount of textual information in biomedi-cine.
There is a need for an effective and effi-cient text-mining system to gather and utilize theknowledge encoded in the biomedical literature.For a correct discourse analysis, a text-miningsystem should have the capability of understand-ing the reference relations among different ex-pressions in texts.
Hence, anaphor resolution, thetask of resolving a given text expression to itsreferred expression in prior texts, is important foran intelligent text processing system.?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.In linguistics, an expression that points backto a previously mentioned expression is called ananaphor, and the expression being referred to bythe anaphor is called its antecedent.
Most pre-vious work on anaphora resolution aims at identi-ty-anaphora in which both an anaphor and itsantecedent are mentions of the same entity.In this paper, we focus on a special type ofanaphora resolution, namely, other-anaphoraresolution, in which an anaphor to be resolvedhas a prefix modifier ?other?
or ?another?.
Theantecedent of an other-anaphor is a complementexpression to the anaphor in a super set.
In otherwords, an other-anaphor is a set of elements ex-cluding the element(s) specified by the antece-dent.
If the modifier ?other?
or ?another?
is re-moved, an anaphor becomes the super set includ-ing the antecedent.
Thus, other-anaphora in factrepresents a ?part-whole?
relation.
Consider thefollowing text?IL-10 inhibits nuclear stimulation of nuclearfactor kappa B (NF kappa B).Several other transcription factors including NF-IL-6, AP-1, AP-2, GR, CREB, Oct-1, and Sp-1are not affected by IL-10.
?Here, the expression ?other transcription fac-tors?
is an other-anaphor, while the ?NF kappaB?
is its antecedent.
The anaphor refers to anytranscription factors except the antecedent.
Byremoving the lexical modifier ?other?, we canget a supper set ?transcription factors?
that in-cludes the antecedent.
The anaphor and antece-dent thus have a ?part-whole?
relation1.Other-anaphora resolution is an importantsub-task in information extraction for biomedical1 Other-anaphora could be also held between ex-pressions that have subset-set or member-collectionrelations.
In this paper, we treat them in a uniformway by using the patterned-based method.121domain.
It also contributes to biomedical ontolo-gy building as it targeted at a ?part-whole?
rela-tion which is in the same hierarchical orders as inontology.
Furthermore, other-anaphora resolu-tion is a first-step exploration in the resolution ofbridging anaphora.
Furthermore, other-anaphoraresolution is a first-step exploration in the resolu-tion of bridging, a special anaphora phenomenonin which the semantic relation between an ana-phor and its antecedent is more complex (e.g.part-whole) than co-reference.Previous work on other-anaphora resolutionrelies on knowledge resources, for example, on-tology like WordNet to determine the ?part-whole?
relation.
However, in the biomedical do-main, a document is full of technical terms whichare usually missing in a general-purpose ontolo-gy.
To deal with this problem, pattern-based ap-proaches have been widely employed, in which apattern that represents the ?part-whole?
relationis designed.
Two expressions are connected withthe specific pattern and form a query.
The queryis searched in a large corpus for the occurrencefrequency which would indicate how likely thetwo given expressions have the part-whole rela-tion.
The solution can avoid the efforts of con-structing the ontology knowledge for the "part-whole" relation.
However, the pattern is designedin an ad-hoc method, usually from linguistic in-tuition and its effectiveness for other-anaphoraresolution is not guaranteed.In this paper, we propose a method to auto-matically mine effective patterns for other-anaphora resolution in biomedical texts.
Our me-thod runs on a small collection of seed wordpairs.
It searches a large corpus (e.g., PubMedabstracts as in our system) for the texts where theseed pairs co-occur, and collects the surroundingwords as the surface patterns.
The automaticallyfound patterns will be used in a machine learningframework for other-anaphora resolution.
To ourknowledge, our work is the first effort of apply-ing the pattern-base technique to other-anaphoraresolution in biomedical texts.The rest of this paper is organized as follows.Section 2 introduces previous related work.
Sec-tion 3 describes the machine learning frameworkfor other-anaphora resolution.
Section 4 presentsin detail our method for automatically patternmining.
Section 5 gives experiment results andhas some discussions.
Finally, Section 6 con-cludes the paper and shows some future work.2 Related WorkPrevious work on other-anaphora resolutioncommonly depends on human engineered know-ledge and/or deep semantic knowledge for the?part-whole?
relation, and mostly works only inthe news domain.Markert et al, (2003) presented a pattern-based algorithm for other-anaphor resolution.They used a manually designed pattern ?ANTE-CEDENT and/or other ANAPHOR ?.
Given twoexpression to be resolved, a query is formed byinstantiating the pattern with the two given ex-pressions.
The query is searched in the Web.
Thehigher the hit number returned, the more likelythat the anaphor and the antecedent candidatehave the ?part-whole?
relation.
The anaphor isresolved to the candidate with the highest hitnumber.
Their work was tested on 120 other-anaphora cases extracted from Wall Street Jour-nal.
The final accuracy was 52.5%.Modjeska et al, (2003) also presented a simi-lar pattern-based method for other-anaphora res-olution, using the same pattern ?ANTECEDENTand/or other ANAPHOR?.
The hit number re-turned from the Web is used as a feature for aNa?ve Bayesian Classifier to resolve other-anaphors.
Other features include surface words,substring matching, distance, gender/numberagreement, and semantic tag of the NP.
Theyevaluated their method with 500 other-anaphorcases extracted from Wall Street Journal, andreported a result of 60.8% precision and 53.4%recall.Markert and Nissim (2005) compared threesystems for other-anaphora resolution, using thesame data set as in (Modjeska et al, 2003).The first system consults WordNet for thepart-whole relation.
The WordNet provides in-formation on meronym/holonym (part-of rela-tion) and hypernym/ hyponym (type-of relation).Their system achieves a performance of 56.8%for precision and 37.0% for recall.The second and third systems employ the pat-tern based approach, employing the same manualpattern ?ANTECEDENT and/or other ANA-PHOR?.
The second system did search in BritishNation Corpus, giving 62.6% precision and26.2% recall.
The third system did search in theWeb as in (Markert et al, 2003), giving 53.8%precision and 51.7% recall.1223 Anaphora Resolution System3.1 CorpusIn our study, we used the GENIA corpus2 for ourother-anaphora resolution in biomedical texts.The corpus consists of 2000 MEDLINE abstracts(around 440,000 words).
From the GENIA cor-pus, we extracted 598 other-anaphora cases.
The598 cases do not contain compound prepositionsor idiomatic uses of ?other?, like ?on the otherhand?
and ?other than?.
And all these anaphorshave their antecedents found in the current andprevious two sentences of the other-anaphor.
Onaverage, there are 15.33 candidate antecedentsfor each anaphor to be resolved.To conduct other-anaphora resolution, an in-put document is preprocessed through a pipelineof NLP components, including tokenization, sen-tence boundary detection, part-of-speech (POS)tagging, noun phrase (NP) chunking, and named-entity recognition (NER).
These preprocessingmodules are aimed to determine the boundariesof each NP in a text, and to provide necessaryinformation of an NP for subsequent processing.In our system, we employed the tool-kits built byour group for these components.
The POS taggerwas trained and tested on the GENIA corpus(version 2.1) and achieved an accuracy of 97.4%.The NP-chunking module, evaluated on UPENWSJ TreeBank, produced 94% F-measure.
TheNER module, trained on GENIA corpus (version3.0), achieved 71.2% F-measure covering 22 ent-ity types (e.g., Virus, Protein, Cell, DNA, etc).3.2 Learning FrameworkOur other-anaphora resolution system adopts thecommon learning-based model for identity-anaphora resolution, as employed by (Soon et al,2001) and (Ng and Cardie, 2002).In the learning framework, a training or test-ing instance has the form of ??
??????
,??
?where ??????
is the ?th candidates of the antece-dent of anaphor ???.
An instance is labelled aspositive if ??????
is the antecedent of  ???
, ornegative if ??????
is not the antecedent of  ??
?.An instance is associated with a feature vectorwhich records different properties and relationsbetween ???
and ??????
.
The features used inour system will be discussed later in the paper.During training, for each other-anaphor, weconsider as the candidate antecedents the preced-ing NPs in its current and previous two sentences.2 http://www-tsujii.is.s.u-tokyo.ac.jp/~genia/topics/Corpus/A positive instance is formed by pairing the ana-phor and the correct antecedent.
And a set ofnegative instances is formed by pairing the ana-phor and each of the other candidates.Based on these generated training instances,we can train a binary classifier using any dis-criminative learning algorithm.
In our work, weemployed support vector machine (SVM) due toits good performance in high dimensional featurevector spaces.During the resolution process, for each other-anaphor encountered, all of the preceding NPs ina three-sentence window are considered.
A testinstance is created for each of the candidate ante-cedents.
The feature vector is presented to thetrained classifier to determine the other-anaphoric relation.
The candidate with highestSVM outcome value is selected as the antecedent.3.3 Baseline FeaturesKnowledge is usually represented as features formachine learning.
In our system, we used thefollowing groups of features for other-anaphoraresolution?
Word Distance IndicatorThis feature measures the word distance betweenan anaphor and a candidate antecedent, with theassumption that the candidate closer to the ana-phor has a higher preference to be the antecedent.?
Same Sentence IndicatorThis feature is either 0 or 1 indicating whether ananaphor and a candidate antecedent are in thesame sentence.
Here, the assumption is that thecandidate in the same sentence as the anaphor ispreferred for the antecedent.?
Semantic Group IndicatorsA named-entity can be classified to a semanticcategory such as ?DNA?, ?RNA?, ?Protein?
andso on3.
Thus we use a set of features to record thecategory pair of an anaphor and a candidate ante-cedent.
For example, ?DNA-DNA?
is generatedfor the case when both anaphor and candidate areDNAs.
And ?DNA-Protein?
is generated if ananaphor is a DNA and a candidate is a protein.These features indicate whether a semantic groupcan refer to another.Note that an anaphor and its antecedent maypossibly belong to different semantic categories.For example, in the GENIA corpus we found that3 In our study, we followed the semantic categories definedin the annotation scheme of the GENIA corpus.123in some cases an expression of a protein nameactually denotes the gene that encodes the pro-tein.
Thus for a given anaphor and a candidateunder consideration, it is necessary to record thepair-wise semantic groups, instead of using asingle feature indicating whether two expressionsare of the same group.The semantic group for a named entity is giv-en by our preprocessing NER.
For the commonNPs produced from the NP chunker, we classifythe semantic group by looking for the words in-side NPs.
For example, an NP ending with?cells?
is classified to ?Cell?
group while an NPending with ?gene?
or ?allele?
is classified to?DNA?
group.?
Lexical Pattern IndicatorsIn some cases, the surrounding words of an ana-phor and a candidate antecedent strongly indicatethe ?part-whole?
relation.
For example, in?...asthma and other hypereosinophilic diseas-es?, the reference between ?other hypereosino-philic diseases?
and ?asthma?
is clear if the in-between words ?and other?
are taken into con-sideration.
Another example of such a hint pat-tern is ?one?
the other ??
The feature is 1 if thespecific patterns are present for the current ana-phor and candidate pair.
A candidate with such afeature is preferred to be the antecedent.?
Hierarchical Name IndicatorThis feature indicates whether an antecedentcandidate is a substring of an anaphor or viceversa.
This feature is used to capture cases like?Jun?
and ?JunB?
(?Jun?
is a family of proteinwhile ?JunB?
is a member of this family).
Inmany cases, an expression that is a super setcomes with certain postfix words, for example,?family members?
in?Fludarabine caused a specific depletion ofSTAT1 protein (and mRNA) but not of otherSTAT family members.
?This kind of phenomenon is more common inbio-medical texts than in news articles.3.4 SVM Training and ClassificationIn our system, we utilized the open-source soft-ware SVM-Light4 for the classifier training andtesting.
SVM is a robust statistical model whichhas been applied to many NLP tasks.
SVM triesto learn a separating line to separate the positiveinstances from negative instances.
Kernel trans-formations are applied for non-linear separable4 http://svmlight.joachims.org/cases (Vapnik, 1995).
In our study, we just usedthe default learning parameters provided bySVM-Light with the linear kernel.
A more so-phisticated kernel may further improve the per-formance.4 Using Auto-mined Pattern FeaturesThe baseline features listed in Section 3.3 onlyrely on shallow lexical, position and semanticinformation about an anaphor and a candidateantecedent.
It could not, nevertheless, disclosethe ?part-whole?
relation between two given ex-pressions.
In section 2, we have shown some ex-isting pattern-based solutions that mine the ?part-whole?
relation in a large corpus with some pat-terns that can represent the relation.
However,these manually designed patterns are usually se-lected by heuristics, which may not necessarilylead to a high coverage with a good accuracy indifferent domains.
To overcome this shortcom-ing, we would like to use an automatic method tomine effective patterns from a large data set.First, we create a set of seed pairs of the ?part-whole?
relation.
And then, we use the seed pairsto discover the patterns that encode the ?part-whole?
relation from a large data set (PubMed asin our system).
Such a solution is supposed toimprove the coverage of lexical patterns, whilestill retain the desired ?part-whole?
relation forother-anaphora resolution.The overview of our system with the automat-ic mined patterns is illustrated in figure 1.Seed PairsGenerationPattern MiningSVMGENIACorpusSeedPairsLexicalPatternsGENIAT stCas sPubMEDCorpusFigure 1: System OverviewThere are three major parts in our system,namely, seed-pairs generation, pattern miningand SVM learning and classification.
In the sub-sequent subsections, we will discuss each of thethree parts in details.1244.1 Seed Pairs PreparationA seed pair is a pair of phrases/words following?part-whole?
order, for example,?integrin alpha?
- ?adhesion molecules?where ?integrin alpha?
is a kind of ?adhesionmolecules?.We extracted the seed pairs automaticallyfrom the GENIA corpus.
The auto-extractingprocedure makes uses of some lexical clues like?A, such as B, C and D?, ?A (e.g.
B and C)?, ?Aincluding B?
and etc.
The capital letter A, B, Cand D refer to a noun phrase such as ?integrinalpha?
and ?adhesion molecules?.
For each oc-currence of ?A such as B, C and D?, the programwill generate seed pairs ?B-A?, ?C-A?
and ?D-A?.Consider the following example,?Mouse thymoma line EL-4 cells produce cyto-kines such as interleukin (IL) -2, IL-3, IL-4, IL-10, and granulocyte-macrophage colony-stimulating factor in response to phorbol 12-myristate 13-acetate (PMA).
?We can extract the following seed pairs,?interleukin (IL) -2?
?
?cytokines?
?IL -3?
?
?cytokines?
?IL -4?
?
?cytokines?
?IL -10?
?
?cytokines?
?granulocyte-macrophage colony-stimulatingfactor?
?
?cytokines?A similar action is taken for other lexicalclues.
Totally, we got 909 distinct seed pairs ex-tracted from the GENIA corpus.After the seed pairs have been extracted, anautomatic verification of the seed pairs is per-formed.
The first purpose of the verification is tocorrect chunking errors.
For example, ?HLAClass II Gene?
may likely be wrongly split into?HLA Class?
and ?II Gene?.
This kind of errorsis repaired by several simple syntactic rules.
Thesecond purpose of the verification is to removethe inappropriate seed pairs.
In our system, weabandoned the seed pairs containing pronounslike ?those?, ?they?, or nouns like ?element?,?member?
and ?agent?.
Such seed pairs may ei-ther find no patterns, or lead to meaningless pat-terns because ?those?
or ?elements?
have no spe-cific semantics and could refer to anything.4.2 Pattern MiningHaving obtained the set of seed pairs, we will usethem to mine patterns for the ?part-whole?
rela-tion.
For each seed pair ?antecedent - anaphor?
(anaphor represents the NP for the ?whole?,while antecedent represents the NP for the?part?
), our system will search in a large data setfor two queries: ?antecedent * anaphor?
and?anaphor * antecedent?
where the ?*?
denotesany sequence of words or symbols.
For a re-turned search results, the text in between ?ante-cedent?
and ?anaphora?
is extracted as a pattern.In our study, we used PubMed 2007 data setfor the pattern mining.
The data set containsabout 52,000 abstracts with around 9,400,000words, and is an ideal large-scale resource forpattern mining.Consider, as an example, a seed pair ?NKkappa B ?
?
?transcription factor?.
Suppose thata returned sentence for the query ?NK kappa B *transcription factor?
is?...NK kappa B family transcription factors...?And a returned sentence for the query ?transcrip-tion factor * NK kappa B?
is?...transcription factors, including NF kappaB...?We can extract a pattern,?ANTECEDENT family ANAPHOR?
from thefirst sentence and a pattern?ANAPHOR, including ANTECEDENT?
fromthe second sentence.We restrict the patterns so that no pattern spanacross two or more sentences.
In other words, thepattern shall not contain the symbol ?.?.
The vi-olated patterns will be removed.The count that a pattern occurs in the PubMedfor a seed pair is recorded.
As a pattern could bereduced by different seed pairs, we define theoccurrence frequency of a pattern as the sum ofthe counts of the pattern for all the seed pairs,using following formula:????
?
=  ???(????
, ??
)??????
(1)where ????
?
is the frequency of pattern ????
; ??
isa seed pair; ?
is the set of all seed pairs.???(????
, ?? )
is the count of the pattern ????
for??
.All the mined patterns are sorted according toits frequency as defined in ??
(1).4.3 Pattern ApplicationFor classifier training and testing, the patternswith high frequency are used as features.
In oursystem, we used the top 40 patterns, while wealso examined the influence the number of thepatterns on the performance.
(See Section 5.2)Given an instance ??(??????
, ???)
and a pat-tern feature ????
, a query is constructed by in-125stantiating with ??????
and ???
.
For example,for an instance ??("??
?????
?
", "???????-??????
???????")
and a pattern feature ?ANA-PHOR, including ANTECEDENT?, we can geta query ?transcription factors, including NFkappa B?.
The query is searched in the PubMeddata set.
The count of the query is recorded.
Thevalue of the pattern feature of a candidate is cal-culated by normalizing the occurrence frequencyamong all the candidates of the anaphor.For demonstration, suppose we have an ana-phor ?other transcription factors?
with two ante-cedent candidates ?IL-10?
and ?NF kappa B?.Given a pattern feature ?ANAPHOR, includingANTECEDENT?, the count of the query ?tran-scription factors, including IL-10?
is 100 whilethat for ?transcription factors, including NF-Kappa B?
is 300.
Then the values of the patternfeature for ?IL-10?
and ?NF kappa B?
are 0.25(100100+300) and 0.75 (300100+300), respectively.The value of a pattern feature can be inter-preted as a degree of belief that an anaphor and acandidate antecedent have the ?part-whole?
rela-tion, with regard to the specific pattern.
Since thevalue of a pattern feature is normalized amongall the candidates, it could indicate the preferenceof a candidate against other competing candi-dates.5 Experiment Results5.1 Experiments SetupIn our experiments, we conducted a 3-fold crossvalidation to evaluate the performances.
The total598 other-anaphora cases were divided into 3sets of size 200, 199 and 199 respectively.
Foreach experiment, two sets were used for trainingwhile the other set was used for testing.For evaluation, we used the accuracy as theperformance metric, which is defined as the cor-rectly resolved other-anaphors divided by all thetesting other-anaphors, that is,????????
=# of correctly resolved anaphors# of total anaphors5.2 Experiments ResultsTable 1 shows the performance of differentother-anaphora resolution systems.
The first lineis for the baseline system with only the normalfeatures as described in Section 3.3.
From thetable, we can find that the baseline system onlyachieves around 40% accuracy.
A performance islower than a similar system in news domain byModjeska et al, (2003) where they reported51.6 % precision with 40.6% recall.
This differ-ence is probably because they utilized more se-mantic knowledge such as hypernymy and mero-nymy acquired from WordNet.
Such knowledge,nevertheless, is not easily available in the bio-medical domain.Sys Fold-1 Fold-2 Fold-3 OverallBaselineNo Pattern42.0 %84/20038.2 %76/19940.7 %81/19940.3 %241/598ManualPattern49.0 %98/20045.7 %91/19947.7 %95/19947.5 %284/598Auto-minedPattern59.0 %118/20053.8 %107/19956.8 %113/19956.5 %338/598Table 1: Performance ComparisonsIn our experiments, we tested the system withmanually designed pattern features.
We tried 10patterns that can represent the ?part-whole?
rela-tion.
Table 2 summaries the patterns used in thesystem.
Among them, the pattern ?Anaphor suchas Antecedent?
and ?Antecedent and other Ana-phor?
are commonly used in previous patternbased approaches (Markert et al, 2003; Mod-jeska et al, 2003).PatternANTECEDENT is a kind of ANAPHORANTECEDENT is a type of ANAPHORANTECEDENT is a member of ANAPHORANTECEDENT is a part of ANAPHORANAPHOR such as ANTECEDENTANTECEDENT and other ANAPHORANTECEDENT within ANAPHORANTECEDENT is a component of ANAPHORANTECEDENT is a sort of ANAPHORANTECEDENT belongs to ANAPHORTable 2: Manually Selected PatternsThe second line of Table 1 shows the resultsof the system with the manual pattern features.We can find that adding these pattern featuresproduces an overall accuracy of 47%, yielding anincrease of 7% accuracy against the baseline sys-tem without the pattern features.The improvement in accuracy is consistentwith previous work using the pattern-based ap-proaches in the news domain (Modjeska et al,2003).
However, we found the performance inthe biomedical domain is worse than that in thenews domain.
For example, Modjeska et al(2003) reported a precision around 53%.
Thisdifference of performance suggests that the ma-126nually designed patterns may not necessarilywork equally well in different domains.The last system we examined in the experi-ment is the one with the automatically minedpattern features.
Table 3 summarizes the topmined patterns ranked based on their occurrencefrequency.
Some of the patterns are intuitivelygood representation of the ?part-whole?
relation.For example, ?ANAPHOR, including ANTE-CEDENT?.
?ANAPHOR, such as ANTECE-DENT?
and ?ANAPHOR and other ANTECE-DENT?
which are in the manually designed pat-tern list, are generated.The last line of Table 1 lists the result of thesystem with automatically mined pattern fea-tures.
It outperforms the baseline system (up to16% accuracy), and the system with manuallyselected patterns (9% accuracy).
These resultsprove that our pattern features are effective forthe other-anaphora resolution.Pattern FreqANAPHOR, including ANTECEDENT 1213ANAPHOR including ANTECEDENT 726ANTECEDENT family ANAPHOR 583ANAPHOR such as ANTECEDENT 542ANTECEDENT transcription ANAPHOR 439ANAPHOR, such as ANTECEDENT 295ANTECEDENT and other ANAPHOR 270ANAPHOR and ANTECEDENT 250ANTECEDENT, dendritic ANAPHOR 246ANTECEDENT and ANAPHOR 238ANTECEDENT human ANAPHOR 223ANAPHOR (e.g., ANTECEDENT  213ANTECEDENT/rel ANAPHOR 188ANTECEDENT-like ANAPHOR 188ANAPHOR against ANTECEDENT  163Table 3: Auto-Mined PatternsTo further compare the manually designedpatterns and the automatically discovered pat-terns.
We examined the coverage rate of the twopattern sets.
The coverage rate measures the ca-pability that a set of patterns could lead to posi-tive anaphor-antecedent pairs.
An other-anaphoris said to be covered by a pattern set, if the ana-phor and its antecedent could be hit (i.e., the cor-responding query has a non-zero hit number) byat least one pattern in the list.
Thus the coveragerate could be defined as????????(?
)=#anaphors covered by the pattern set P# total anaphorsThe coverage rates of the two pattern sets aretabulated in table 4.
It is apparent that the auto-mined patterns have a significantly higher cover-age (more than twice) than the manually de-signed patterns.Patterns Coverage RateManually Designed 36.0 %Auto-Mined 92.1 %Table 4: Coverage ComparisonIn our experiments we were also concernedabout the usefulness of each individual pattern.For this purpose, we examined the loss of theaccuracy when withdrawing a pattern featurefrom the feature list.
The top 10 patterns with thelargest accuracy loss are summarized in table 5.PatternAccLossANAPHOR, including ANTECEDENT 4.18%ANAPHOR including ANTECEDENT 3.18%ANAPHOR such as ANTECEDENT 2.84%ANTECEDENT transcription ANAPHOR 2.17%ANTECEDENT and other ANAPHOR 2.01%ANAPHOR, such as ANTECEDENT 1.84%ANTECEDENT family ANAPHOR 1.84%ANAPHOR (e.g., ANTECEDENT 1.51%ANTECEDENT-like ANAPHOR 1.17%ANTECEDENT/rel ANAPHOR 1.17%Table 5: Usefulness of Each PatternThe process of automatic pattern miningwould generate numerous surface patterns.
It isnot reasonable to use all the patterns as features.As mentioned in section 4.3, we rank the patternbased on their occurrence frequency and selectthe top ones as the features.
It would be interest-ing to see how the number of patterns influencesthe performance of anaphora resolution.
In figure2, we plot the accuracy under different numbertop pattern features.
We can find by using morepatterns, the coverage keeps increasing.
The ac-curacy also increases, but it reaches the peakwith around 40 patterns.
With more patterns, theaccuracy remains at the same level.
This is be-cause the low frequency patterns usually are notthat indicative of the ?part-whole?
relation.
In-cluding these pattern features would bring noisesbut not help the performance.
The flat curve afterthe peak point suggests that the machine learningalgorithm can effectively identify the importanceof the pattern features for the resolution decision,and therefore including non-indicative patternswould not damage the performance.In our experiment, we also interested to com-pare the utility of PubMed with other generaldata sets.
Thus, we tested pattern mining by us-127ing the Google-5-grams corpus5 which lists thehit number of all the queries of five words or lessin the Web.
Unfortunately, we found that the per-formance is worse than using PubMed.
The pat-terns mined from the Web corpus only gives anaccuracy of around 41%, almost the same as thebaseline system without using any pattern fea-tures.
The bad performance is due to the fact thatmost of bio-medical names are quite long (2~4words) and occur infrequently in the non-technique data set.
Consequently, a query formedby a biomedical seed pair usually cannot befound in the Web corpus (We found the coverageof the auto-mined patterns mined from the corpusis only about 20%).Figure 2: Performance of Various No.
of Patterns6 Conclusion & Future WorksIn this paper, we have presented how to automat-ically mined pattern features for learning-basedother-anaphora resolution in bio-medical texts.The patterns that represent the ?part-whole?
rela-tions are automatically mined from a large dataset.
They are used as features for a SVM-basedclassifier learning and testing.
The results of ourexperiments show a reasonably good perfor-mance with 56.5% accuracy).
It outperforms(16% in accuracy) the baseline system withoutthe pattern features, and also beats (9%) the sys-tem with manually designed pattern features.There are several directions for future work.We would like to employ a pattern pruningprocess to remove those less indicative patternssuch as ?ANAPHOR, ANTECEDENT?.
And wealso plan to perform pattern normalization whichintegrates two similar or literally identical pat-5 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13terns into a single one.
By doing so, the usefulpatterns may come to the top of the pattern list.Also we would like to explore ontology re-sources like MESH and Genes Ontology, whichcan provide enriched hierarchies of bio-medicalterms and thus would benefit other-anaphora res-olution.AcknowledgementsThis study on co-reference resolution is partially supportedby a Specific Targeted Research Project (STREP) of theEuropean Union's 6th Framework Programme within ISTcall 4, Bootstrapping of Ontologies and TerminologiesSTrategic REsearch Project (BOOTStrep).ReferencesCastano J, Zhang J and Pustejovsky J. Anaphora Resolutionin Biomedical Literature.
Submitted to International Sym-posium on Reference Resolution 2002, Alicante, SpainClark H. Bridging.
In Thinking.
Readings in CognitiveScience.
Johnson-Laird and Wason edition.
Cambridge.Cambridge University Press; 1977.411?420Gasperin C and Vieira R. Using Word Similarity Lists forResolving Indirect Anaphora.
In Proceedings of ACLWorkshop on Reference Resolution and Its Application.30 June 2004; Barcelona.
2004.40-46Girju R, Badulescu A and Moldovan D. Automatic Discov-ery of Part-Whole Relations.
Computational Linguistics,2006, 32(2):83-135Bernauer J..
Analysis of Part-Whole Relation and Subsump-tion in Medical Domain.
Data Knowledge Enginnering1996, 20:405-415Markert K. and Nissim M. Comparing Knowledge Sourcesfor Nominal Anaphora Resolution.
Computational Lin-guistics, 2005, 31(3):367-402Markert K, Modjeska N and Nissim M. Using the Web forNominal Anaphora Resolution.
In Proceedings of EACLWorkshop on the Computational Treatment of Anaphora.14 April 2003; Budapest.
2003.39-46Mitokov R. Anaphor Resolution.
The State of The Art.Working Paper, University of Wolverhampton, UK, 1999Modjeska N, Markert K and Nissim M. Using the Web inMachine Learning for Other-anaphor Resolution.
In Pro-ceedings of the 2003 Conference on Empirical Methods inNatural Language Processing.
July2003,Sapporo.176-183Soon WM, Ng HT and Lim CY.
A Machine Learning Ap-proach to Coreference Resolution of Noun Phrases.
Com-putational Linguistics, 2001, 27(4).521-544Vapnik, V. Chapter 5 Methods of Pattern Recognition.
InThe Nature of Statistical Learning Theory.
New York.Springer-Verlag, 1995.123-167Varzi C.  Parts, Wholes, and Part-whole Relation.
The Pros-pects of the Mereotopology.
Data & Knowledge Engi-neering, 1996, 20.259-286Vieira R, Bick E, Coelho J, Muller V, Collovini S, Souza Jand Rino L. Semantic Tagging for Resolution of IndirectAnaphora.
In Proceedings of 7th SIGdial Workshop onDiscourse and Dialogue.
July 2006; Sydney.76-79Burges C. A Tutorial on Supporting Vector Machines forPattern Recognition.
Data Mining and Knowledge Dis-covery 1998, 2:121-167Ng V. and Cardie C. Improving machine learning ap-proaches to coreference resolution.
In Proceedings of An-nual Conference for Association of Computational Lin-guistics 2002, Philadelphia.104-111128
