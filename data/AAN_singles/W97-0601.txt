Evaluating Interactive Dialogue Systems: Extending Component Evaluation toIntegrated System EvaluationMarilyn A. Walker, Diane J. Litman, Candace A. Kamm and Alicia AbellaAT&T Labs--Research180 Park AvenueFlorham Park, NJ 07932-0971 USAwalker, diane,cak,abella@ research.att.comAbstractThis paper discusses the range of ways in whichspoken dialogue system components have beenevaluated and discusses approaches to evalua-tion that attempt to integrate component eval-uation into an overall view of system perfor-mance.
We will argue that the PARADISE(PARAdigm for Dialogue System Evaluation)framework has several advantages over otherproposals.I IntroductionInteractive spoken dialogue systems are based on manycomponent technologies: speech recognition, text-to-speech, natural language understanding, natural languagegeneration, and database query languages.
While eval-uation metrics for these components are well under-stand (Sparck-Jones and Galliers, 1996; Walker, 1989;Hirschman et al, 1990), it has been difficult to developstandard metrics for complete systems that integrate allthese technologies.
One problem is that here are so manypotential metrics that can be used to evaluate a dialog sys-tem.
For example, a dialog system can be evaluated bymeasuring the system's ability to help users achieve theirgoals, the system's robustness in detecting and recover-ing from errors of speech recognition or of understanding,and the overall quality of the system's interactions withusers (Danieli and Gerbino, 1995; Hirschman and Pao,1993; Polifroni et al, 1992; Price et al, 1992; Simp-son and Fraser, 1993).
Another problem is that dialogevaluation is not reducible to transcript evaluation, or tocomparison with a wizard's reference answers (Bates andAyuso, 1993; Polifroni et al, 1992; Price et al, 1992),because the set of potentially acceptable dialogs can bevery large.Current proposals for dialog evaluation metrics areboth objective and subjective.
The objective metricsthat have been used to evaluate a dialog as a whole in-clude (Abella, Brown, and Buntschuh, 1996; Ciaremella,1993; Danieli and Gerbino, 1995; Hirschman et al, 1990;Hirschman et al, 1993; Polifroni et al, 1992; Price et al,1992; Smith and Hipp, 1994; Smith and Gordon, 1997;Walker, 1996):?
percentage of correct answers with respect o a setof reference answers?
transaction success, task completion, or quality ofsolution?
number of turns or utterances;?
dialogue time or task completion time?
mean user response time?
mean system response time?
frequency of diagnostic error messages?
percentage of "non-trivial" (more than one word)utterances.?
mean length of "non-trivial" utterancesObjective metrics can be calculated without recourseto human judgement, and in many cases, can be calcu-lated automatically by the spoken dialogue system.
Onepossible xception is task-based success measures, uchas transaction success, task completion or quality of solu-tion metrics, which can be either an objective or a subjec-tive measure depending on whether the users' goals arewell-defined at the beginning of the dialogue.
This is thecase in controlled experiments, but in field studies, deter-mining whether the user accomplished the task requiressubjective judgements.Subjective metrics require subjects using the system orhuman evaluators to categorize the dialogue or utteranceswithin the dialog along various qualitative dimensions.Because these metrics are based on human judgements,such judgements need to be reliable across judges in orderto compete with the reproducibility of metrics based onobjective criteria.
Subjective metrics can still be quanti-tative, as when a ratio between two subjective categoriesis computed.
Subjective metrics that have been used in-clude (Danieli and Gerbino, 1995; Hirschman and Pao,1993; Simpson and Fraser, 1993; Danieli et al, 1992;Bernsen, Dybkjaer, and Dybkjaer, 1996) :?
Implicitrecovery (IR): the system's ability to use di-alog context o recover from errors of partial recog-nition or understanding.?
Explicit Recovery: the proportion of explicit recov-ery utterances made by both the system system turncorrection (STC), and the user, user turn correction(UTC).?
Contextual ppropriateness (CA): the coherence ofsystem utterances with respect o dialog context.Utterances can be either appropriate (AP), inappro-priate (IP), or ambiguous (AM).?
Cooperativity ofsystem utterances: classified on thebasis of the adherance of the system's behavior toGrice's conversational maxims (Grice, 1967).?
Correct and Partially Correct Answers.?
Appropriate or Inappropriate Directives and Diag-nostics: directives are instructions the system givesto the user, while diagnostics are messages in whichthe system tells the user what caused an error or whyit can't do what the user asked.?
User Satisfaction: a metric that attempts to capturesuser's perceptions about he usability of the system.This is usually assessed with multiple choice ques-tionnaires that ask users to rank the system's perfor-mance on a range of usability features according toa scale of potential assessments.Both the objective and the subjective metrics have beenvery useful to the spoken dialogue community in com-paring different systems for carrying out the same task,but these metrics are also limited.One widely acknowledged limitation is that the use ofreference answers makes it impossible to compare sys-tems that use different dialog strategies for carrying outthe same task.
The reference answer approach requirescanonical responses (i.e., a single "correct" answer) to bedefined for every user utterance.
Thus it is not possible touse the same reference set to evaluate a system that maychoose to give a summary as a response in one case, aska disambiguating question in another, or respond with aset of database values in another.A second limitation is that various metrics may behighly correlated with one another, and provide redun-dant information on performance.
Determining correla-tions requires asuite of metrics that are widely used, andtesting whether correlations hold across multiple dialogueapplications.A third limitation arises from the inability to tradeoffor combine various metrics and to make generalizations(Fraser, 1995; Sparck-Jones and Galliers, 1996).
Forexample, consider a comparison of two train timetableinformation agents (Danieli and Gerbino, 1995), whereAgent A in Dialogue 1 uses an explicit confirmation strat-egy, while Agent B in Dialogue 2 uses an implicit confir-mation strategy:(1) User: I want to go from Torino to Milano.Agent A: Do you want to go from Trento to Milano?Yes or No?User: No.
(2) User: I want to travel from Torino to Milano.Agent B: At which time do you want to leave fromMerano to Milano?User: No, I want to leave from Torino in the evening.Danieli and Gerbino found that Agent A had a highertransaction success rate and produced less inappropriateand repair utterances than Agent B.
In addition, theyfound that Agent A's dialogue strategy produced dia-logues that were approximately twice as long as AgentB's, but they could not determine whether Agent A'shigher transaction success or Agent B's efficiency wasmore critical to performance.The ability to identify factors that affect performance isa critical basis for making generalizations across systemsperforming different asks (Cohen, 1995; Sparck-Jonesand Galliers, 1996).
It would be useful to know howusers' perceptions ofperformance depend on the strategyused, and on tradeoffs among factors such as efficiency,speed, and accuracy.
In addition to agent factors such asthe differences in dialogue strategy seen in Dialogues 1and 2, task factors uch as database size and environmen-tal factors uch as background noise may also be relevantpredictors of performance.In the remainder of this paper, we discuss the PAR-ADISE framework (PARAdigm for Dialogue SystemEvaluation) (Walker et al, 1997), and that it addressesthese limitations, as well as others.
We will show thatPARADISE provides a useful methodology for evaluat-ing dialog systems that integrates and enhances previouswork.2 Integrating Previous Approaches toEvaluation in the PARADISEFrameworkMAXIMIZE USER SATISFACTIONMAXIMIZE TASK MINIMIZE COSTSSUCCESS~ MF~SOIIESFigure 1: PARADISE's structure of objectives for spokendialogue performanceThe PARADISE framework for spoken dialogue valua-tion is based on methods from decision theory (Keeneyand Raiffa, 1976; Doyle, 1992), which supports combin-ing the disparate set of performance measures discussedabove into a single performance evaluation function.
Theuse of decision theory requires aspecification of both theobjectives of the decision problem and a set of measures(known as attributes in decision theory) for operational-izing the objectives.
The PARADISE model is based onthe structure of objectives (rectangles) hown in Figure 1.At the top level, this model posits that performance anbe correlated with a meaningful external criterion suchas usability, and thus that the overall goal of a spokendialogue agent is to maximize an objective related to us-ability.
User satisfaction ratings (Kamm, 1995; Shriberg,Wade, and Price, 1992; Polifroni et al, 1992) are themost widely used external indicator of the usability of adialogue agent.The model further posits that two types of factorsare potential relevant contributors to user satisfaction,namely task success and dialogue costs.
PARADISE useslinear regression to quantify the relative contribution ofthe success and cost factors to user satisfaction.
The tasksuccess measure builds on previous measures of transac-tion success and task completion (Danieli and Gerbino,1995; Polifroni et al, 1992), but makes use of the Kappacoefficient (Carletta, 1996; Siegel and Castellan, 1988)to operationalize task success.The cost factors consist of two types.
The efficiencymeasures arise from the list of objective performancemeasures used in previous work as described above.Qualitative measures try to capture aspects of the qual-ity of the dialog.
These are based on both objective andsubjective measures used in previous work, such as thefrequency of diagnostic or error messages, inappropriateutterance ratios, or the proportion of repair utterances.The remainder of this section explains the measures(ovals in Figure 1) used to operationalize the set of objec-tives, and the methodology for estimating a quantitativeperformance function that reflects the objective structure.Section 2.1 describes PARADISE's task representation,which is needed to calculate the task-based success mea-sure described in Section 2.2.
Section 2.3 describes thecost measures considered in PARADISE, which reflectboth the efficiency and the naturalness of an agent's dia-logue behaviors.
Section 2.4 describes the use of linearregression and user satisfaction to estimate the relativecontribution of the success and cost measures in a singleperformance function.
Finally, Section 2.5 summarizesthe method.2.1 Tasks as Attribute Value MatricesA general evaluation framework requires atask represen-tation that decouples what an agent and user accomplishfrom how the task is accomplished using dialogue strate-gies.
PARADISE uses an attribute value matrix (AVM)to represent dialogue tasks.
The AVM consists of theinformation that must be exchanged between the agentand the user during the dialogue, represented asa set ofordered pairs of attributes and their possible values.
1As a first illustrative xample, consider asimplificationof the train timetable domain of Dialogues 1 and 2, wherethe timetable only contains information about rush-hourtrains between four cities, as shown in Table 1.
This AVMconsists of four attributes (abbreviations for each attributename are also shown).
2 In Table 1, these attribute-valuepairs are annotated with the direction of information flowto represent who acquires the information, although thisinformation is not used for evaluation.
DUring the dia-logue the agent must acquire from the user the values ofDC, AC, and DR, while the user must acquire DT.attribute possible values information flowdepart-city (DC) Milano, Roma, Tofino, Trento to agentarrival-city (AC) Milano, Roma, Tonno, Trento to agentdepart-range (DR) morning,evening to agentdepart-time (DT) 6am,8am,6pm,8pm to userTable 1: Attribute value matrix, simplified train timetabledomainA 1: Hello, This is Train Enquiry Service.
DC,AC,DR,DTPlease speak after the tone.
DC,AC,DR,DTWhich information do you need?
DC,AC,DR,DTUI : I want o go from Torino to Milano.
DC,ACA2: Do you want o go from Trento to Milano?
DC,ACYes or No?
DC,ACU2: No.
DC,ACA3: Do you want o leave from Trento?
DCYes or No?
DCU3: No.
DCA4: Where do you want o leave from?
DCU4: I want o leave from Torino.
DCA5: Do you want o leave from Torino?
DCYes or No?
DCU5: Yes.
DCA6: Do you want o go to Milano?
ACU6: Yes.
ACA7: At which time do you want o leave?
DRU7: I want o travel in the evening.
DRA8: Do you want o leave between 6 and 9 p.m.?
DRYes or No?
DRU8: Yes.
DRA9: There is a train leaving at 8:00 p.m. DTFigure 2: Agent A dialogue interaction (Danieli andGerbino, 1995)Like previous approaches to evaluation, performanceevaluation using PARADISE requires a corpus of dia-logues between users and the agent, in which users ex-ecute a set of scenarios.
Each scenario execution hasa corresponding AVM instantiation i dicating the taskinformation requirements for the scenario, where eachattribute is paired with the attribute value obtained viathe dialogue.~For infinite sets of values, actual values found in the exper-imental data constitute the required finite set.2The AVM serves as an evaluation mechanism only.
We arenot claiming that AVMs determine an agent's behavior or serveas an utterance's semantic representation.B 1: Hello, This is Train Enquiry Service.
DC,AC,DR,DTPlease speak after the tone.
DC,AC,DR,DTWhich information do you need?
DC,AC,DR,DTU1 : I want o travel from Torino to Milano.
DC,ACB2: At which time do you want o leavefrom Merano to Milano?
DC,AC, DRU2: No, I want o leave from Tofinoin the evening.
DC, DRB3: Do you want o leave from Torinobetween 6 and 11 p.m?
DC, DRPlease answer Yes or No.
DC, DRU3: Yes.
DC, DRB4: A train leaves at 8 p.m. DTFigure 3: Agent B dialogue interaction (Danieli andGerbino, 1995)attribute actual valuedepart-city Torinoarrival-city Milanodepart-range eveningdepart-time 8pmTable 2: Attribute value matrix instantiation, scenariokey for Dialogues 1 and 2For example, assume that a scenario requires the userto find a train from Torino to Milano that leaves in theevening, as in the longer versions of Dialogues 1 and 2 inFigures 2 and 3.3 Table 2 contains an AVM correspondingto a "key" for this scenario.
All dialogues resulting fromexecution of this scenario in which the agent and theuser correctly convey all attribute values (as in Figures2 and 3) would have the same AVM as the scenario keyin Table 2.
The AVMs of the remaining dialogues woulddiffer from the key by at least one value.
Thus, eventhough the dialogue strategies in Figures 2 and 3 areradically different, the AVM task representation for thesedialogues is identical and the performance of the systemfor the same task can thus be assessed on the basis of theAVM representation.2.2 Measuring Task SuccessSuccess at the task for a whole dialogue (or subdia-logue) is measured by how well the agent and user achievethe information requirements of the task by the end of thedialogue (or subdialogue).
This section explains howPARADISE uses the Kappa coefficient (Carletta, 1996;Siegel and Castellan, 1988) to operationalize the task-based success measure in Figure 1.The Kappa coefficient, ~, is calculated from a confu-sion matrix that summarizes how well an agent achievesthe information requirements of a particular task for aset of dialogues instantiating a set of scenarios.
4 For3These dialogues have been slightly modified from (Danieliand Gerbino, 1995).
The attribute names at the end of eachutterance will be explained below.4Confusion matrices can be constructed to summarize theresult of dialogues for any subset of the scenarios, attributes,users or dialogues.4example, Table 3 shows a hypothetical confusion matrixthat could have been generated in an evaluation of 100complete dialogues with train timetable agent A (perhapsusing the confirmation strategy illustrated in Figure 2).
5When comparing Agent A to Agent B, a similar tablewould also be constructed for Agent B.In Table 3, the values in the matrix cells are basedon comparisons between the dialogue and scenario keyAVMs.
Table 3 summarizes how the 100 AVMs repre-senting each dialogue with Agent A compare with theAVMs representing the relevant scenario keys.
Labelsvl to v4 in each matrix represent the possible values ofdepart-city shown in Table 1; v5 to v8 are for arrival-city, etc.
Columns represent the key, specifying whichinformation values the agent and user were supposed tocommunicate oone another given a particular scenario.Rows represent the data collected from the dialogue cor-pus, reflecting what attribute values were actually com-municated between the agent and the user.Whenever an attribute value in a dialogue (i.e., data)AVM matches the value in its scenario key, the numberin the appropriate diagonal cell of the matrix (boldfacefor clarity) is incremented by 1.
The off diagonal cellsrepresent misunderstandings that are not corrected in thedialogue.
Note that depending on the strategy that a spo-ken dialogue agent uses, confusions across attributes arepossible, e.g., "Milano" could be confused with "morn-ing."
The effect of misunderstandings that are correctedduring the course of the dialogue are reflected in the costsassociated with the dialogue, as will be discussed below.Given a confusion matrix M, success at achieving theinformation requirements of the task is measured with theKappa coefficient (Carletta, 1996; Siegel and Castellan,1988):P(A) - P (Z)1 - P (z )P(A) is the proportion of times that the AVMs for the ac-tual set of dialogues agree with the AVMs for the scenariokeys, and P(E) is the proportion of times that the AVMsfor the dialogues and the keys are expected to agree bychance.
6 When there is no agreement other than thatwhich would be expected by chance, ~ = 0.
When thereis total agreement, ~ = 1. x is superior to other mea-sures of success uch as transaction success (Danieli andGerbino, 1995), concept accuracy (Simpson and Fraser,1993), and percent agreement (Carletta, 1996) becausetakes into account he inherent complexity of the taskby correcting for chance expected agreement.
Thus nprovides a basis for comparisons across agents that areperforming different tasks.5The distributions in the table are roughly based on perfor-mance results in (Danieli and Gerbino, 1995).6n has been used to measure pairwise agreement amongcoders making category judgments (Carletta, 1996; Krippen-doff, 1980; Siegel and Castellan, 1988).
Thus, the observeduser/agent interactions are modeled as a coder, and the idealinteractions as an expert coder.DATAvlv2v3v4v5v6v7v8v9vl0v l lv12v13v14sumKEYDEPART-CITY ARRIVAL-CITY DEPART-RANGE DEPART-TIMEvl v2 v3 v4 v5 v6 v7 v8 v9 vl0 v i i  v12 v13 v1422 1 3294 16 4 11 1 5 11 13 20222 1 1 20 51 I 2 8 1545 105 4020 21 19 2 42 182 6 3 2130 30 25 15 25 25 30 20 50 50 25 25 25 25Table 3: Confusion matrix, Agent AWhen the prior distribution of the categories is un-known, P(E), the expected chance agreement betweenthe data and the key, can be estimated from the distri-bution of the values in the keys.
This can be calculatedfrom confusion matrix M, since the columns representthe values in the keys.
In particular:i=1where ti is the sum of the frequencies in column i of M,and T is the sum of the frequencies in M (tl + .
.
.
+ tn).P(A), the actual agreement between the data and thekey, is always computed from the confusion matrix M:P(A) - Ei~=l M(i, i)TGiven the confusion matrix in Table 3, P(E) = 0.079,P(A) = 0.795 and g = 0.777.
Given similar calculationson a confusion matrix for Agent B, we can determinewhether Agent A or Agent B is more successful at achiev-t ing the task goals.2.3 Measuring Dialogue CostsAs shown in Figure 1, performance is also a function of acombination of cost measures.
Intuitively, cost measuresshould be calculated on the basis of any user or agent di-alogue behaviors that should be minimized.
PARADISEsupports the use of any of the wide range of cost measuresused in previous work, and provides a way of combiningthese measures by normalizing them.Each cost measure is represented asa function ci thatcan be applied to any (sub)dialogue.
First, consider thesimplest case of calculating efficiency measures over awhole dialogue.
For example, let cl be the total numberof utterances.
For the whole dialogue D1 in Figure 2,o(D1) is 23 utterances.
For the whole dialogue D2 inFigure 3, cl (D2) is 10 utterances.To calculate costs over subdialogues and for some ofthe qualitative measures, it is necessary to be able to spec-ify which information goals each utterance contributesto.
PARADISE uses its AVM representation to link theinformation goals of the task to any arbitrary dialoguebehavior, by tagging the dialogue with the attributes forthe task.
7 This makes it possible to evaluate any potentialdialogue strategies for achieving the task, as well as toevaluate dialogue strategies that operate at the level ofdialogue subtasks (subdialogues).Consider the longer versions of Dialogues 1 and 2in Figures 2 and 3.
Each utterance in Figures 2 and3 has been tagged using one or more of the attributeabbreviations in Table 1, according to the subtask(s) theutterance contributes to.
As a convention of this type oftagging, utterances that contribute to the success of thewhole dialogue, such as greetings, are tagged with all theattributes.
Thus the goal of the tagging is to show howthe structure of the dialogue reflects the structure of thetask (Carbelrry, 1989; Grosz and Sidner, 1986; Litmanand Allen, 1990).Tagging by AVM attributes is required to calculatecosts over subdialogues, ince for any subdialogue, taskattributes define the subdialogue.
For example, the sub-dialogue about he attribute arrival-city (SA) consists ofutterances A6 and U6, its cost Cl (SA) is 2.Tagging by AVM attributes i also required to calculatethe cost of some of the qualitative measures, such asnumber of repair utterances.
(Note that to calculate suchcosts, each utterance in the corpus of dialogues must alsobe tagged with respect to the qualitative phenomenon iquestion, e.g.
whether the utterance is a repair.
8) Forexample, let c2 be the number of repair utterances.
Therepair utterances in Figure 2 are A3 through U6, thusc2(D1) is 10 utterances and c2(SA) is 2 utterances.
Therepair utterance in Figure 3 is U2, but note that accordingto the AVM task tagging, U2 simultaneously addressesthe information goals for arrival-city and depart-range.
In7This tagging can be hand generated, or system generatedand hand corrected.
Preliminary studies indicate that reliabilityfor human tagging is higher for AVM attribute tagging thanfor other types of discourse segment tagging (Passonneau andLitman, 1997; Hirschberg and Nakatani, 1996).8Previous work has shown that this can be done with highreliability (Hirschman and Pao, 1993).general, if an utterance U contributes to the informationgoals of N different attributes, each attribute accounts for1/N of any costs derivable from U.
Thus, c2(D2) is .5.Given a set of ci, it is necessary to combine the dif-ferent cost measures in order to determine their relativecontribution to performance.
The next section explainshow to combine ~ with a set of ci to yield an overallperformance measure.2.4 Estimating a Performance FunctionGiven the definition of success and costs above and themodel in Figure 1, performance for any (sub)dialogue Dis defined as follows: 9Performance = (~ * .Af(t?))
- ~ wi * .Af(ci)i=1Here c~ is a weight on ~, the cost functions ci are weightedby wi, and.Af is a Z score normalization function (Cohen,1995).The normalization function is used to overcome theproblem that the values of ci are not on the same scale as~, and that the cost measures ci may also be calculatedover widely varying scales (e.g.
response delay couldbe measured using seconds while, in the example, costswere calculated in terms of number of utterances).
Thisproblem is easily solved by normalizing each factor x toits Z score:N(x)  -O" xwhere cr~ is the standard eviation for x.To illustrate the method for estimating a performancefunction, we will use a subset of the data from Table 3,and add data for Agent B, as shown in Table 4.
Table4 represents the results from a hypothetical experimentin which eight users were randomly assigned to com-municate with Agent A and eight users were randomlyassigned to communicate with Agent B.
Table 4 showsuser satisfaction (US) ratings (discussed below), ~, num-ber of utterances (#utt) and number of repair utterances(#rep) for each of these users.
Users 5 and 11 correspondto the dialogues in Figures 2 and 3 respectively.
To nor-malize cl for user 5, we determine that N- is 38.6 and ~rc~is 18.9.
Thus, .Af(Cl) is -0.83.
Similarly .Af(cl) for user11 is-1.51.To estimate the performance function, the weights c~and wi must be solved for.
Recall that the claim implicit inFigure 1 was that the relative contribution of task successand dialogue costs to performance should be calculated byconsidering their contribution to user satisfaction.
User9We assume an additive performance (utility) function be-cause it appears that n and the various cost factors ci are util-ity independent and additive independent (Keeney and Raiffa,1976).
It is possible however that user satisfaction data col-lected in future experiments (or other data such as willingnessto pay or use) would indicate otherwise.
If so, continuing use ofan additive function might require a transformation f the data,a reworking of the model shown in Figure 1, or the inclusion ofinteraction terms in the model (Cohen, 1995).user agent US ~ el (#utt) e2 (#rep)1 A 1 1 46 302 A 2 1 i 50 303 A 2 1 I 52 304 A 3 1 1 40 205 A 4 1 : 23 106 A 2 1 50 367 A 1 0.46 75 308 A 1 0.19 60 309 B 6 1 8 010 B 5 1 15 111 B 6 1 10 0.512 B 5 1 20 313 B 1 0.19 45 1814 B 1 !
0.46 50 2215 B 2 0.19 34 1816 ; B 2 0.46 40 18Mean(A) :  A 2 0.83 49.5 27Mean(B) B 3.5 0.66 27.8 10.1Mean NA 2.75 0.75 38.6 18.5Table 4: Hypothetical performance data from users ofAgents A and Bsatisfaction is typically calculated with surveys that askusers to specify the degree to which they agree with oneor more statements about he behavior or the performanceof the system.
A single user satisfaction measure can becalculated from a single question, or as the mean of aset of ratings.
The hypothetical user satisfaction ratingsshown in Table 4 range from a high of 6 to a low of 1.Given a set of dialogues for which user satisfaction(US), ~ and the set of ci have been collected experimen-tally, the weights c~ and wi can be solved for using multi-ple linear regression.
Multiple linear regression producesa set of coefficients (weights) describing the relative con-tribution of each predictor factor in accounting for thevariance in a predicted factor.
In this case, on the basisof the model in Figure 1, US is treated as the predictedfactor.
Normalization of the predictor factors (~ and ci)to their Z scores guarantees that the relative magnitudeof the coefficients directly indicates the relative contribu-tion of each factor.
Regression on the Table 4 data forboth sets of users tests which factors ~, #utt, #rep moststrongly predicts US.In this illustrative xample, the results of the regressionwith all factors included shows that only ~ and #rep aresignificant (p < .02).
In order to develop a performancefunction estimate that includes only significant factorsand eliminates redundancies, a second regression includ-ing only significant factors must then be done.
In thiscase, a second regression yields the predictive quation:Performance = .
40.Af(~ ) - .
78.Af(c2)i.e., c~ is .40 and w2 is .78.
The results also show n issignificant at p < .0003, #rep significant at p < .0001,and the combination of n and #rep account for 92% ofthe variance in US, the external validation criterion.
Thefactor #utt was not a significant predictor of performance,in part because #utt and #rep are highly redundant.
(Thecorrelation between #utt and #rep is 0.91).Given these predictions about the relative contributionof different factors to performance, it is then possibleto return to the problem first introduced in Section 1:given potentially conflicting performance riteria such asrobustness and efficiency, how can the performance ofAgent A and Agent B be compared?
Given values forc~ and wi, performance an be calculated for both agentsusing the equation above.
The mean performance of Ais -.44 and the mean performance ofB is .44, suggestingthat Agent B may perform better than Agent A overall.The evaluator must then however test these perfor-mance differences for statistical significance.
In this case,a t test shows that differences are only significant at the p< .07 level, indicating a trend only.
In this case, an eval-uation over a larger subset of the user population wouldprobably show significant differences.2.5 SummaryWe illustrated the PARADISE framework by using it tocompare the performance of two hypothetical dialogueagents in a simplified train timetable task domain.
Weused PARADISE to derive a performance function forthis task, by estimating the relative contribution ofa set ofpotential predictors to user satisfaction.
The PARADISEmethodology consists of the following steps:?
definition of a task and a set of scenarios;?
specification of the AVM task representation;?
experiments with alternate dialogue agents for thetask;?
calculation of user satisfaction using surveys;?
calculation of task success using to;?
calculation of dialogue cost using efficiency andqualitative measures;?
estimation of a performance function using linearregression and values for user satisfaction, x anddialogue costs;?
comparison with other agents/tasks to determinewhich factors that are most strongly weighted inthe performance function generalize as importantfactors in other applications;?
refinement of the performance model.Note that all of these steps are required to develop theperformance function.
However once the weights in theperformance function have been solved for, user satis-faction ratings no longer need to be collected.
Instead,predictions about user satisfaction can be made on thebasis of the predictor variables, which is illustrated in theapplication of PARADISE to subdialogues in (Walker etal., 1997).Given the current state of knowledge, many experi-ments would need to be done to develop a generalizedperformance function.
Performance function estimationmust be done iteratively over many different asks anddialogue strategies to see which factors generalize.
Inthis way, the field can make progress in identifying therelationships among various factors and can move to-wards more predictive models of spoken dialogue agentperformance.3 DiscussionIn this paper, we reviewed the current state of the artin spoken dialogue system evaluation and argued thatthe PARADISE framework both integrates and enhancesprevious work.
PARADISE provides amethod for deter-mining a performance function for a spoken dialogue sys-tem, and for calculating performance over subdialoguesas well as whole dialogues.
The factors that can con-tribute to the performance function include any of thecost metrics used in previous work.
However, becausethe performance function is developed on the basis oftesting the correlation of performance measures with anexternal validation criterion, user satisfaction, significantmetrics are identified and redundant metrics are elimi-nated.A key aspect of the framework isthe decoupling of taskgoals from the system's dialogue behavior.
This requirexa representation f the task's information requirementsin terms of an attribute-value matrix (AVM).
The notionof a task-based success measure builds on previous workusing transaction success, task completion, and quality ofsolution metrics.
While we discussed the representationof an information-seeking dialogue here, AVM repre-sentations for negotiation and diagnostic dialogue tasksare also easily constructed (Walker et al, 1997).
Fi-nally, the use of x means that the task success measure inPARADISE normalizes performance for task complex-ity, providing abasis for comparing systems performingdifferent tasks.4 AcknowledgmentsThanks to James Allen, Jennifer Chu-Carroll, MorenaDanieli, Wieland Eckert, Giuseppe Di Fabbrizio, DonHindle, Julia Hirschberg, Shri Narayanan, Jay Wilpon,and Steve Whittaker for helpful discussion on this work.ReferencesAbella, Alicia, Michael K Brown, and Bruce Buntschuh.1996.
Development principles for dialog-based inter-faces.
In ECAI-96 Spoken Dialog Processing Work-shop, Budapest, Hungary.Bates, Madeleine and Damaris Ayuso.
1993.
A proposalfor incremental dialogue valuation.
In Proceedings ofthe DARPA Speech and NL Workshop, pages 319-322.Bernsen, Niels Ole, Hans Dybkjaer, and Laila Dybkjaer.1996.
Principles for the design of cooperative spo-ken human-machine dialogue.
In International Con-ference on Spoken Language Processing, ICSLP 96,pages 729-732.Carberry, S. 1989.
Plan recognition and its use in un-derstanding dialogue.
In A. Kobsa and W. Wahlster,editors, User Models in Dialogue Systems.
SpringerVerlag, Berlin, pages 133-162.Carletta, Jean C. 1996.
Assessing the reliabilityof subjective codings.
Computational Linguistics,22(2):249-254.Ciaremella, A.
1993.
A prototype performance eval-uation report.
Technical Report Project Esprit 2218SUNDIAL, WP8000-D3.Cohen, Paul.
R. 1995.
Empirical Methods for ArtificialIntelligence.
MIT Press, Boston.Danieli, M., W. Eckert, N. Fraser, N. Gilbert, M. Guy-omard, P. Heisterkamp, M. Kharoune, J. Magadur,S.
McGlashan, D. Sadek, J. Siroux, and N. Youd.1992.
Dialogue manager design evaluation.
TechnicalReport Project Esprit 2218 SUNDIAL, WP6000-D3.Danieli, Morena and Elisabetta Gerbino.
1995.
Metricsfor evaluating dialogue strategies ina spoken languagesystem.
In Proceedings ofthe 1995 AAAI Spring Sym-posium on Empirical Methods in Discourse Interpre-tation and Generation, pages 34-39.Doyle, Jon.
1992.
Rationality and its roles in reasoning.Computational Intelligence, 8(2):376--409.Fraser, Norman M. 1995.
Quality standards for spokendialogue systems: areport on progress in EAGLES.
InESCA Workshop on Spoken Dialogue Systems Vigso,Denmark, pages 157-160.Grice, H. P. 1967.
Logic and conversation.Grosz, Barbara J. and Candace L. Sidner.
1986.
Atten-tions, intentions and the structure of discourse.
Com-putational Linguistics, 12:175-204.Hirschberg, Julia and Christine Nakatani.
1996.
Aprosodic analysis of discourse segments in direction-giving monologues.
In 34th Annual Meeting of theAssociation for Computational Linguistics, pages 286-293.Hirschman, L., M. Bates, D. Dahl, W. Fisher, J. Garofolo,D.
Pallett, K. Hunicke-Smith, E Price, A. Rudnicky,and E. Tzoukermann.
1993.
Multi-site data collec-tion and evaluation i  spoken language understanding.In Proceedings of the Human Language TechnologyWorkshop, ages 19-24.Hirschman, Lynette, Deborah A. Dahl, Donald P. McKay,Lewis M. Norton, and Marcia C. Linebarger.
1990.Beyond class A: A proposal for automatic evaluationof discourse.
In Proceedings ofthe Speech and NaturalLanguage Workshop, ages 109-113.Hirschman, Lynette and Christine Pao.
1993.
The costof errors in a spoken language system.
In Proceedingsof the Third European Conference on Speech Commu-nication and Technology, pages 1419-1422.Kamm, Candace.
1995.
User interfaces for voice appli-cations.
In David Roe and Jay Wilpon, editors, VoiceCommunication between Humans and Machines.
Na-tional Academy Press, pages 422--442.Keeney, Ralph and Howard Raiffa.
1976.
Decisions withMultiple Objectives: Preferences and Value Tradeoffs.John Wiley and Sons.Krippendorf, Klaus.
1980.
Content Analysis: An Intro-duction to its Methodology.
Sage Publications, Bev-erly Hills, Ca.Litman, Diane and James Allen.
1990.
Recognizing andrelating discourse intentions and task-oriented plans.In Philip Cohen, Jerry Morgan, and Martha Pollack,editors, Intentions inCommunication.
MIT Press.Passonneau, Rebecca J. and Diane Litman.
1997.
Dis-course segmentation byhuman and automated means.Computational Linguistics, 23 (1).Polifroni, Joseph, Lynette Hirschman, Stephanie Seneff,and Victor Zue.
1992.
Experiments in evaluating in-teractive spoken language systems.
In Proceedings ofthe DARPA Speech and NL Workshop, ages 28-33.Price, Patti, Lynette Hirschman, Elizabeth Shriberg, andElizabeth Wade.
1992.
Subject-based valuation mea-sures for interactive spoken language systems.
In Pro-ceedings of the DARPA Speech and NL Workshop,pages 34-39.Shriberg, Elizabeth, Elizabeth Wade, and Patti Price.1992.
Human-machine problem solving using spo-ken language systems (SLS): Factors affecting perfor-mance and user satisfaction.
In Proceedings of theDARPA Speech and NL Workshop, ages 49-54.Siegel, Sidney and N. J. Castellan.
1988.
NonparametricStatistics for the Behavioral Sciences.
McGraw Hill.Simpson, A. and N. A. Fraser.
1993.
Black box andglass box evaluation of the SUNDIAL system.
In Pro-ceedings of the Third European Conference on SpeechCommunication a d Technology, pages 1423-1426.Smith, Ronnie W. and Steven A. Gordon.
1997.
Effectsof variable initiative on linguistic behavior in human-computer spoken natural language dialog.
Computa-tional Linguistics, 23(1).Smith, Ronnie W. and D. Richard Hipp.
1994.
Spo-ken Natural Language Dialog Systems: A PracticalApproach.
Oxford University Press.Sparck-Jones, Karen and Julia R. Galliers.
1996.
Evalu-ating Natural Language Processing Systems.
Springer.Walker, Marilyn A.
1989.
Evaluating discourse pro-cessing algorithms.
In Proc.
27th Annual Meeting ofthe Association of Computational Linguistics, pages251-261.Walker, Marilyn A.
1996.
The Effect of Resource Limitsand Task Complexity on Collaborative Planning in Di-alogue.
Artificial Intelligence Journal, 85(1-2): 181-243.Walker, Marilyn A., Diane Litman, Candace Kamm, andAlicia Abella.
1997.
Paradise: A general frameworkfor evaluating spoken dialogue agents.
In Proceed-ings of the 35th Annual Meeting of the Association ofComputationaI Linguistics, ACL/EACL 97.
