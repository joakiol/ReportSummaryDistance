Proceedings of the 12th Conference of the European Chapter of the ACL, pages 594?602,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsUsing Cycles and Quasi-Cycles to Disambiguate Dictionary GlossesRoberto NavigliDipartimento di InformaticaSapienza - Universita` di RomaVia Salaria, 113 - 00198 Roma Italynavigli@di.uniroma1.itAbstractWe present a novel graph-based algo-rithm for the automated disambiguationof glosses in lexical knowledge resources.A dictionary graph is built starting fromsenses (vertices) and explicit or implicitrelations in the dictionary (edges).
Theapproach is based on the identification ofedge sequences which constitute cycles inthe dictionary graph (possibly with oneedge reversed) and relate a source to atarget word sense.
Experiments are per-formed on the disambiguation of ambigu-ous words in the glosses of WordNet andtwo machine-readable dictionaries.1 IntroductionIn the last two decades, we have witnessed anincreasing availability of wide-coverage lexicalknowledge resources in electronic format, mostnotably thesauri (such as Roget?s Thesaurus (Ro-get, 1911), the Macquarie Thesaurus (Bernard,1986), etc.
), machine-readable dictionaries (e.g.,the Longman Dictionary of Contemporary En-glish (Proctor, 1978)), computational lexicons(e.g.
WordNet (Fellbaum, 1998)), etc.The information contained in such resourcescomprises (depending on their kind) sense inven-tories, paradigmatic relations (e.g.
flesh3n is a kindof plant tissue1n),1 text definitions (e.g.
flesh3n isdefined as ?a soft moist part of a fruit?
), usage ex-amples, and so on.Unfortunately, not all the semantics are madeexplicit within lexical resources.
Even Word-Net, the most widespread computational lexiconof English, provides explanatory information inthe form of textual glosses, i.e.
strings of text1We denote as wip the ith sense in a reference dictionaryof a word w with part of speech p.which explain the meaning of concepts in termsof possibly ambiguous words.Moreover, while computational lexicons likeWordNet contain semantically explicit informa-tion such as, among others, hypernymy andmeronymy relations, most thesauri, glossaries, andmachine-readable dictionaries are often just elec-tronic transcriptions of their paper counterparts.As a result, for each entry (e.g.
a word sense orthesaurus entry) they mostly provide implicit in-formation in the form of free text.The production of semantically richer lexicalresources can help alleviate the knowledge ac-quisition bottleneck and potentially enable ad-vanced Natural Language Processing applications(Cuadros and Rigau, 2006).
However, in order toreduce the high cost of manual annotation (Ed-monds, 2000), and to avoid the repetition of thiseffort for each knowledge resource, this task mustbe supported by wide-coverage automated tech-niques which do not rely on the specific resourceat hand.In this paper, we aim to make explicitlarge quantities of semantic information implic-itly contained in the glosses of existing wide-coverage lexical knowledge resources (specifi-cally, machine-readable dictionaries and computa-tional lexicons).
To this end, we present a methodfor Gloss Word Sense Disambiguation (WSD),called the Cycles and Quasi-Cycles (CQC) algo-rithm.
The algorithm is based on a novel notionof cycles in the dictionary graph (possibly withone edge reversed) which support a disambigua-tion choice.
First, a dictionary graph is built fromthe input lexical knowledge resource.
Next, themethod explicitly disambiguates the informationassociated with sense entries (i.e.
gloss words)by associating senses for which the richest sets ofpaths can be found in the dictionary graph.In Section 2, we provide basic definitions,present the gloss disambiguation algorithm, and il-594lustrate the approach with an example.
In Section3, we present a set of experiments performed ona variety of lexical knowledge resources, namelyWordNet and two machine-readable dictionaries.Results are discussed in Section 4, and relatedwork is presented in Section 5.
We give our con-clusions in Section 6.2 Approach2.1 DefinitionsGiven a dictionary D, we define a dictionarygraph as a directed graph G = (V,E) whose ver-tices V are the word senses in the sense inventoryof D and whose set of unlabeled edges E is ob-tained as follows:i) Initially, E := ?
;ii) For each sense s ?
V , and for each lexico-semantic relation in D connecting sense s tos?
?
V , we perform: E := E ?
{(s, s?
)};iii) For each sense s ?
V , let gloss(s) be the setof content words in its part-of-speech taggedgloss.
Then for each content word w?
ingloss(s) and for each sense s?
of w?, weadd the corresponding edge to the dictionarygraph, i.e.
: E := E ?
{(s, s?
)}.For instance, consider WordNet as our inputdictionary D. As a result of step (ii), given the se-mantic relation ?sport1n is a hypernym of racing1n?,the edge (racing1n, sport1n) is added toE (similarly,an inverse edge is added due to the hyponymy rela-tion holding between sport1n and racing1n).
Duringstep (iii), the gloss of racing1n ?the sport of engag-ing in contests of speed?
is part-of-speech tagged,obtaining the following set of content words:{ sportn, engagev, contestn, speedn }.
The fol-lowing edges are then added to E: { (racing1n,sport1n), (racing1n, sport2n), .
.
.
, (racing1n, sport6n),.
.
.
, (racing1n, speed1n), .
.
.
, (racing1n, speed5n) }.The above steps are performed for all the senses inV .We now recall the definition of graph cycle.
Acycle in a graphG is a sequence of edges ofG thatforms a path v1 ?
v2 ?
?
?
?
?
vn (vi ?
V ) suchthat the first vertex of the path corresponds to thelast, i.e.
v1 = vn (Cormen et al, 1990, p. 88).For example, the cycle in Figure 1(a) is given bythe path racing1n ?
contest1n ?
race3n ?
run3n ?racing1n in the WordNet dictionary graph.
In factracing1ncontest1nrace3nrun3n(a)racing1ncontest1ncompete1vrace2v(b)Figure 1: An example of cycle (a) and quasi-cycle(b) in WordNet.contestn occurs in the gloss of racing1n, race3n is ahyponym of contest1n, and so on.We further provide the definition of quasi-cycleas a sequence of edges in which the reversal ofthe orientation of a single edge creates a cycle(Bohman and Thoma, 2000).
For instance, thequasi-cycle in Figure 1(b) is given by the path rac-ing1n ?
contest1n ?
compete1v ?
race2v ?
rac-ing1n.
In fact, the reversal of the edge (racing1n,race2v) creates a cycle.Finally, we call a path a (quasi-)cycle if it is ei-ther a cycle or a quasi-cycle.
Further, we say thata path is (quasi-)cyclic if it forms a (quasi-)cyclein the graph.2.2 The CQC AlgorithmGiven a dictionary graph G = (V,E) built as de-scribed in the previous section, our objective isto disambiguate dictionary glosses with the sup-port of (quasi-)cycles.
(Quasi-)cyclic paths are in-tuitively better than unconstrained paths as eachsense choice s is reinforced by the very fact of sbeing reachable from itself through a sequence ofother senses.Let a(s) be the set of ambiguous words to bedisambiguated in the part-of-speech tagged glossof sense s. Given a word w?
?
a(s), our aim isto disambiguate w?
according to the sense inven-tory of D, i.e.
to assign it the right sense chosenfrom its set of senses Senses(w?).
To this end, wepropose the use of a graph-based algorithm whichsearches the dictionary graph and collects the fol-lowing kinds of (quasi-)cyclic paths:i) s?
s?
?
s1 ?
?
?
?
?
sn?2 ?
s (cycle)ii) s?
s?
?
s1 ?
?
?
?
?
sn?2 ?
s(quasi-cycle)595CQC-Algorithm(s, w?
)1 for each sense s?
?
Senses(w?
)2 CQC(s?)
:= DFS(s?, s)3 All CQC :=?s??Senses(w?)CQC(s?
)4 for each sense s?
?
Senses(w?
)5 score(s?)
:= 06 for each path c ?
CQC(s?
)7 l := length(c)8 v := ?
(l) ?
1NumCQC(All CQC,l)9 score(s?)
:= score(s?)
+ v10 return argmaxs??Senses(w?)score(s?
)Table 1: The Cycles and Quasi-Cycles (CQC) al-gorithm in pseudocode.where s is our source sense, s?
is a candidate senseof w?
?
gloss(s), si is a sense in V , and n isthe length of the path (given by the number of itsedges).
We note that both kinds of paths start andend with the same vertex s, and that we restrictquasi-cycles to those whose inverted edge departsfrom s. To avoid any redundancy, we require thatno vertex is repeated in the path aside from thestart/end vertex (i.e.
s 6= s?
6= si 6= sj for anyi, j ?
{1, .
.
.
, n?
2}).The Cycles and Quasi-Cycles (CQC) algorithm,reported in pseudo-code in Table 1, takes as input asource sense s and a target wordw?
(in our setting2w?
?
a(s)).
It consists of two main phases.During steps 1-3, cycles and quasi-cycles aresought for each sense of w?.
This step is per-formed with a depth-first search (DFS, cf.
(Cor-men et al, 1990, pp.
477?479)) up to a depth?.
To this end, we first define next(s) = {s??
:(s, s??)
?
E}, that is the set of senses which canbe directly reached from sense s. The DFS startsfrom a sense s?
?
Senses(w?
), and recursively ex-plores the senses in next(s?)
until sense s or asense in next(s) is encountered, obtaining a cy-cle or a quasi-cycle, respectively.
For each senses?
of w?
the DFS returns the full set CQC(s?
)of (quasi-)cyclic paths collected.
Note that theDFS recursively keeps track of previously visitedsenses, so as to discard (quasi-)cycles includingthe same sense twice.
Finally, in step 3, All CQCis set to store the cycles and quasi-cycles for allthe senses of w?.2Note that potentially w?
can be any word of interest.
Thevery same algorithm can be applied to determine semanticsimilarity or to disambiguate collocations.The second phase (steps 4-10) computes a scorefor each sense s?
of w?
based on the paths col-lected for s?
during the first phase.
Let c be sucha path, and let l be its length, i.e.
the number ofedges in the path.
Then the contribution of c to thescore of s?
is given by a function of its length ?
(l),which associates with l a number between 0 and 1.This contribution is normalized by a factor givenbyNumCQC(All CQC, l), which calculates theoverall number of paths of length l. In this work,we will employ the function ?
(l) = 1/el, whichweighs a path with the inverse of the exponentialof its length (so as to exponentially decrease thecontribution of longer paths)3.
Steps 4-9 are re-peated for each candidate sense ofw?.
Finally, step10 returns the highest-scoring sense of w?.As a result of the systematic application ofthe CQC algorithm to the dictionary graph G =(V,E) associated with a dictionary D, a graphG?
= (V, E?)
is output, where V is again the senseinventory of D, and E?
?
E, such that each edge(s, s?)
?
E?
either represents an unambiguous re-lation in E (i.e.
it was either a lexico-semantic re-lation in D or a relation between s and a monose-mous word occurring in its gloss) or is the resultof an execution of the CQC algorithm with input sand w?
?
a(s).2.3 An ExampleConsider the following example: WordNet definesthe third sense of fleshn as ?a soft moist part of afruit?.
As a result of part-of-speech tagging, weobtain:gloss(flesh3n) = {softa ,moista , partn , fruitn}Let us assume we aim to disambiguate the nounfruit.
Our call to the CQC algorithm in Table 1 isthen CQC-Algorithm(flesh3n, fruitn).As a result of the first two steps of the algorithm,a set of cycles and quasi-cycles for each sense offruitn is collected, based on a DFS starting fromthe respective senses of our target word (we as-sume ?
= 5).
In Figure 2, we show some of the(quasi-)cycles collected for senses #1 and #3 offruitn, respectively defined as ?the ripened repro-ductive body of a seed plant?
and ?an amount ofa product?
(we neglect sense #2 as the length andnumber of its paths is not dissimilar from that ofsense #3).3Other weight functions, such as ?
(l) = 1 (which weighseach path independent of its length) proved to perform worse.596flesh3nfruit1n berry11npulpy1aparenchyma1nplant tissue1nlychee1ncustard apple1nmango2nmoist1aflora2nedible fruit1nskin2nhygrophyte1n(a)flesh3nfruit3nnewspaper4nmag1nproduction4n(b)Figure 2: Some cycles and quasi-cycles connect-ing flesh3n to fruit1n (a), and fruit3n (b).During the second phase of the algorithm, andfor each sense of fruitn, the contribution of each(quasi-)cycle is calculated (steps 6-9 of the algo-rithm).
For example, for sense fruit1n in Figure2(a), 5 (quasi-)cycles of length 4 and 2 of length 5were returned by DFS(fruit1n, flesh3n).
As a result,the following score is calculated:4score(fruit1n) =5e4 ?1NumCQC(all chains,4)+ 2e5 ?1NumCQC(all chains,5)= 5e4?7 +2e5?2= 0.013 + 0.006 = 0.019whereas for fruit3n (see Figure 2(b)) we get:score(fruit3n) =2e4 ?1NumCQC(all chains,4)= 2e4?7 = 0.005where NumCQC(All CQC, l) is the total num-ber of cycles and quasi-cycles of length l over allthe senses of fruitn (according to Figure 2, thisamounts to 7 paths for l = 4 and 2 paths for l = 5).Finally, the sense with the highest score (i.e.fruit1n) is returned.3 ExperimentsTo test and compare the performance of our al-gorithm, we performed a set of experiments on a4Note that, for the sake of simplicity, we are calculatingour scores based on the paths shown in Figure 2.
However,we tried to respect the proportion of paths collected by thealgorithm for the two senses.variety of resources.
First, we summarize the re-sources (Section 3.1) and algorithms (Section 3.2)that we adopted.
In Section 3.3 we report our ex-perimental results.3.1 ResourcesThe following resources were used in our experi-ments:?
WordNet (Fellbaum, 1998), the mostwidespread computational lexicon of En-glish.
It encodes concepts as synsets, andprovides textual glosses and lexico-semanticrelations between synsets.
Its latest version(3.0) contains around 155,000 lemmas, andover 200,000 word senses;?
Macquarie Concise Dictionary (Yallop,2006), a machine-readable dictionary of(Australian) English, which includes around50,000 lemmas and almost 120,000 wordsenses, for which it provides textual glossesand examples;?
Ragazzini/Biagi Concise (Ragazzini and Bi-agi, 2006), a bilingual English-Italian dic-tionary, containing over 90,000 lemmas and150,000 word senses.
The dictionary pro-vides Italian translations for each Englishword sense, and vice versa.We used TreeTagger (Schmid, 1997) to part-of-speech tag the glosses in the three resources.3.2 AlgorithmsHereafter we briefly summarize the algorithmsthat we applied in our experiments:?
CQC: we applied the CQC algorithm as de-scribed in Section 2.2;?
Cycles, which applies the CQC algorithm butsearches for cycles only (i.e.
quasi-cycles arenot collected);?
An adaptation of the Lesk algorithm (Lesk,1986), which, given a source sense s of wordw and a word w?
occurring in the gloss of s,determines the right sense of w?
as that whichmaximizes the (normalized) overlap betweeneach sense s?
of w?
and s:argmaxs??Senses(w?)|next?
(s) ?
next?(s?)|max{|next?
(s)|, |next?(s?
)|}597where we define next?
(s) = words(s) ?next(s), and words(s) is the set of lexical-izations of sense s (e.g.
the synonyms in thesynset s).
When WordNet is our reference re-source, we employ an extension of the Leskalgorithm, namely Extended Gloss Overlap(Banerjee and Pedersen, 2003), which ex-tends the sense definition with words fromthe definitions of related senses (such as hy-pernyms, hyponyms, etc.).
We use the sameset of relations available in the authors?
im-plementation of the algorithm.We also compared the performance of the abovealgorithms with two standard baselines, namelythe First Sense Baseline (abbreviated as FS BL)and the Random Baseline (Random BL).3.3 ResultsOur experiments concerned the disambiguation ofthe gloss words in three datasets, one for each re-source, namely WordNet, Macquarie Concise, andRagazzini/Biagi.
In all datasets, given a sense s,our set a(s) is given by the set of part-of-speech-tagged ambiguous content words in the gloss ofsense s from our reference dictionary.WordNet.
When using WordNet as a referenceresource, given a sense s whose gloss we aim todisambiguate, the dictionary graph includes notonly edges connecting s to senses of gloss words(step (iii) of the graph construction procedure, cf.Section 2.1), but also those obtained from any ofthe WordNet lexico-semantics relations (step (ii)).For WordNet gloss disambiguation, we em-ployed the dataset used in the Senseval-3 GlossWSD task (Litkowski, 2004), which contains15,179 content words from 9,257 glosses5.
Wecompared the performance of CQC, Cycles, Lesk,and the two baselines.
To get full coverage andhigh performance, we learned a threshold for eachsystem below which they recur to the FS heuris-tic.
The threshold and maximum path length weretuned on a small in-house manually-annotateddataset of 100 glosses.
The results are shown inTable 2.
We also included in the table the perfor-mance of the best-ranking system in the Senseval-5Recently, Princeton University released a richer corpusof disambiguated glosses, namely the ?Princeton WordNetGloss Corpus?
(http://wordnet.princeton.edu).However, in order to allow for a comparison with the stateof the art (see below), we decided to adopt the Senseval-3dataset.Algorithm Prec./RecallCQC 64.25Cycles 63.74Lesk 51.75TALP 68.60/68.30FS BL 55.44Random BL 26.29Table 2: Gloss WSD performance on WordNet.3 Gloss WSD task, namely the TALP system(Castillo et al, 2004).CQC outperforms all other proposed ap-proaches, obtaining a 64.25% precision and recall.We note that Cycles also gets high performance,compared to Lesk and the baselines.
Also, com-pared to CQC, the difference is not statisticallysignificant.
However, we observe that, if we donot recur to the first sense as a backoff strategy, weget a much lower recall for Cycles (P = 65.39, R =26.70 for CQC, P = 72.03, R = 16.39 for Cycles).CQC performs about 4 points below the TALPsystem.
As also discussed later, we believe this re-sult is relevant, given that our approach does notrely on additional knowledge resources, as TALPdoes (though both algorithms recur to the FS back-off strategy).Finally, we observe that the FS baseline haslower performance than in typical all-words dis-ambiguation settings (usually above 60% accu-racy).
We believe that this is due to the absenceof monosemous words from the test set, and tothe possibly different distribution of senses in thedataset.Macquarie Concise.
Automatically disam-biguating glosses in a computational lexiconsuch as WordNet is certainly useful.
However,disambiguating a machine-readable dictionaryis an even more ambitious task.
In fact, whilecomputational lexicons typically encode some ex-plicit semantic relations which can be used as anaid to the disambiguation task, machine-readabledictionaries only rarely provide sense-taggedinformation (often in the form of references toother word senses).
As a result, in this lattersetting the dictionary graph typically containsonly edges obtained from the gloss words of senses (step (iii), Section 2.1).To experiment with machine-readable dictio-naries, we employed the Macquarie Concise Dic-598Algorithm Prec./RecallCQC 77.13Cycles 67.63Lesk 30.16FS BL 51.48Random BL 23.28Table 3: Gloss WSD performance on MacquarieConcise.tionary (Yallop, 2006).
A dataset was preparedby randomly selecting 1,000 word senses fromthe dictionary and annotating the content words intheir glosses according to the dictionary sense in-ventory.
Overall, 2,678 words were sense tagged.The results are shown in Table 3.
CQC obtainsan accuracy of 77.13% (in case of ties, a randomchoice is made, thus leading to the same precisionand recall), Cycles achieves an accuracy of almost10% less than CQC (the difference is statisticallysignificant; p < 0.01).
The FS baseline, here, isbased on the first sense listed in the Macquariesense inventory, which ?
in contrast to WordNet?
does not depend on the occurrence frequency ofsenses in a semantically-annotated corpus.
How-ever, we note that the FS baseline is not very dif-ferent from that of the WordNet experiment.We observe that the Lesk performance is verylow on this dataset (around 7 points above the Ran-dom BL), due to the impossibility of using theExtended Gloss Overlap approach (semantic rela-tions are not available in the Macquarie Concise)and to the low number of matches between sourceand target entries.Ragazzini/Biagi.
Finally, we performed an ex-periment on the Ragazzini/Biagi English-Italianmachine-readable dictionary.
In this experiment,disambiguating a word w?
in the gloss of a senses from one section (e.g.
Italian-English) equals toselecting a word sense s?
of w?
listed in the othersection of the dictionary (e.g.
English-Italian).
Forexample, given the English entry race1n, translatedas ?corsan, garan?, our objective is to assign theright Italian sense from the Italian-English sectionto corsan and garan.To apply the CQC algorithm, a simple adapta-tion is needed, so as to allow (quasi-)cycles to con-nect word senses from the two distinct sections.The algorithm must seek cyclic and quasi-cyclicpaths, respectively of the kind:Algorithm Prec./RecallCQC 89.34Cycles 85.40Lesk 63.89FS BL 73.15Random BL 51.69Table 4: Gloss WSD performance on Ragazz-ini/Biagi.i) s?
s?
?
s1 ?
?
?
?
?
sn?2 ?
sii) s?
s?
?
s1 ?
?
?
?
?
sn?2 ?
swhere n is the path length, s and s?
are senses re-spectively from the source (e.g.
Italian/English)and the target (e.g.
English/Italian) section of thedictionary, si is a sense from the target section fori ?
k and from the source section for i > k,for some k such that 0 ?
k ?
n ?
2.
In otherwords, the DFS can jump at any time from the tar-get section to the source section.
After the jump,the depth search continues in the source section, inthe hope to reach s. For example, the following isa cycle with k = 1:race1n?
corsa2n?
gara2n?
race1nwhere the edge between corsa2n and gara2n is dueto the occurrence of garan in the gloss of corsa2nas a domain label for that sense.To perform this experiment, we randomly se-lected 250 entries from each section (500 over-all), including a total number of 1,069 translationsthat we manually sense tagged.
In Table 4 we re-port the results of CQC, Cycles and Lesk on thistask.
Overall, the figures are higher than in previ-ous experiments, thanks to a lower average degreeof polysemy of the resource, which also impactspositively on the FS baseline.
However, given arandom baseline of 51.69%, the performance ofCQC, over 89% precision and recall, is signif-icantly higher.
Cycles obtains around 4 pointsless than CQC (the difference is statistically sig-nificant; p < 0.01).
The performance of Lesk(63.89%) is also much higher than in our previ-ous experiments, thanks to the higher chance offinding a 1:1 correspondence between the two sec-tions.
However, we observed that this does not al-ways hold, as also supported by the better resultsof CQC.5994 DiscussionThe experiments presented in the previous sectionare inherently heterogeneous, due to the differentnature of the resources adopted (a computationallexicon, a monolingual and a bilingual machine-readable dictionary).
Our aim was to show theflexibility of our approach in tagging gloss wordswith senses from the same dictionary.We show the average polysemy of the threedatasets in Table 5.
Notice that none of thedatasets included monosemous items, so our ex-periments cannot be compared to typical all-wordsdisambiguation tasks, where monosemous wordsare part of the test set.Given that words in the Macquarie dataset havea higher average polysemy than in the Word-Net dataset, one might wonder why disambiguat-ing glosses from a computational lexicon such asWordNet is more difficult than performing a sim-ilar task on a machine-readable dictionary suchas the Macquarie Concise Dictionary, which doesnot provide any explicit semantic hint.
We be-lieve there are at least two reasons for this out-come: the first specifically concerns the Senseval-3 Gloss WSD dataset, which does not reflect thedistribution of genus-differentiae terms in dictio-nary glosses: less than 10% of the items were hy-pernyms, thus making the task harder.
As for thesecond reason, we believe that the Macquarie Con-cise provides more clear-cut definitions, thus mak-ing sense assignments relatively easier.An analytical comparison of the results of Cy-cles and CQC show that, especially for machine-readable dictionaries, employing both cycles andquasi-cycles is highly beneficial, as additional sup-port is provided by the latter patterns.
Our resultson WordNet prove to be more difficult to analyze,because of the need of employing the first senseheuristic to get full coverage.
Also, the maximumpath length used for WordNet was different (?
= 3according to our tuning, compared to ?
= 4 forMacquarie and Ragazzini/Biagi).
However, quasi-cycles are shown to provide over 10% improve-ment in terms of recall (at the price of a decreasein precision of 6.6 points).Further, we note that the performance of theCQC algorithm dramatically improves as the max-imum score (i.e.
the score which leads to a senseassignment) increases.
As a result, users can tunethe disambiguation performance based on theirspecific needs (coverage, precision, etc.).
For in-WN Mac R/BPolysemy 6.68 7.97 3.16Table 5: Average polysemy of the three datasets.stance, WordNet Gloss WSD can perform up to85.7% precision and 10.1% recall if we require thescore to be?
0.2 and do not use the FS baseline asa backoff strategy.
Similarly, we can reach up to93.8% prec., 20.0% recall for Macquarie Concise(score?
0.12) and even 95.2% prec., 70.6% recall(score ?
0.1) for Ragazzini/Biagi.5 Related WorkWord Sense Disambiguation is a large researchfield (see (Navigli, 2009) for an up-to-dateoverview).
However, in this paper we focused ona specific kind of WSD, namely the disambigua-tion of dictionary definitions.
Seminal works onthe topic date back to the late 1970s, with the de-velopment of models for the identification of tax-onomies from lexical resources (Litkowski, 1978;Amsler, 1980).
Subsequent works focused on theidentification of genus terms (Chodorow et al,1985) and, more in general, on the extraction ofexplicit information from machine-readable dic-tionaries (see, e.g., (Nakamura and Nagao, 1988;Ide and Ve?ronis, 1993)).
Kozima and Furugori(1993) provide an approach to the constructionof ambiguous semantic networks from glosses inthe Longman Dictionary of Contemporary English(LDOCE).
In this direction, it is worth citing thework of Vanderwende (1996) and Richardson etal.
(1998), who describe the construction of Mind-Net, a lexical knowledge base obtained from theautomated extraction of lexico-semantic informa-tion from two machine-readable dictionaries.
As aresult, weighted relation paths are produced to in-fer the semantic similarity between pairs of words.Several heuristics have been presented for thedisambiguation of the genus of a dictionary defini-tion (Wilks et al, 1996; Rigau et al, 1997).
Morerecently, a set of heuristic techniques has been pro-posed to semantically annotate WordNet glosses,leading to the release of the eXtended WordNet(Harabagiu et al, 1999; Moldovan and Novischi,2004).
Among the methods, the cross referenceheuristic is the closest technique to our notion ofcycles and quasi-cycles.
Given a pair of words wand w?, this heuristic is based on the occurrence of600w in the gloss of a sense s?
of w?
and, vice versa,ofw?
in the gloss of a sense s ofw.
In other words,a graph cycle s?
s?
?
s of length 2 is sought.Based on the eXtended WordNet, a gloss dis-ambiguation task was organized at Senseval-3(Litkowski, 2004).
Interestingly, the best perform-ing systems, namely the TALP system (Castillo etal., 2004), and SSI (Navigli and Velardi, 2005),are knowledge-based and rely on rich knowledgeresources: respectively, the Multilingual CentralRepository (Atserias et al, 2004), and a propri-etary lexical knowledge base.In contrast, the approach presented in this paperperforms the disambiguation of ambiguous wordsby exploiting only the reference dictionary itself.Furthermore, as we showed in Section 3.3, ourmethod does not rely on WordNet, and can be ap-plied to any lexical knowledge resource, includingbilingual dictionaries.Finally, methods in the literature more focusedon a specific disambiguation task include statisti-cal methods for the attachment of hyponyms un-der the most likely hypernym in the WordNet tax-onomy (Snow et al, 2006), structural approachesbased on semantic clusters and distance met-rics (Pennacchiotti and Pantel, 2006), supervisedmachine learning methods for the disambiguationof meronymy relations (Girju et al, 2003), etc.6 ConclusionsIn this paper we presented a novel approach to dis-ambiguate the glosses of computational lexiconsand machine-readable dictionaries, with the aim ofalleviating the knowledge acquisition bottleneck.The method is based on the identification of cy-cles and quasi-cycles, i.e.
circular edge sequences(possibly with one edge reversed) relating a sourceto a target word sense.The strength of the approach lies in its weaklysupervised nature: (quasi-)cycles rely exclusivelyon the structure of the input lexical resources.
Noadditional resource (such as labeled corpora or ex-ternal knowledge bases) is required, assuming wedo not resort to the FS baseline.
As a result, theapproach can be applied to obtain a semantic net-work from the disambiguation of virtually any lex-ical resource available in machine-readable formatfor which a sense inventory is provided.The utility of gloss disambiguation is evengreater in bilingual dictionaries, as idiosyncrasiessuch as missing or redundant translations can bediscovered, thus helping lexicographers improvethe resources6.
An adaptation similar to that de-scribed for disambiguating the Ragazzini/Biagican be employed for mapping pairs of lexicalresources (e.g.
FrameNet (Baker et al, 1998)to WordNet), thus contributing to the beneficialknowledge integration process.
Following this di-rection, we are planning to further experiment onthe mapping of FrameNet, VerbNet (Kipper et al,2000), and other lexical resources.The graphs output by the CQC algo-rithm for our datasets are available fromhttp://lcl.uniroma1.it/cqc.
Weare scheduling the release of a software pack-age which includes our implementation of theCQC algorithm and allows its application to anyresource for which a standard interface can bewritten.Finally, starting from the work of Budanitskyand Hirst (2006), we plan to experiment with theCQC algorithm when employed as a semantic sim-ilarity measure, and compare it with the most suc-cessful existing approaches.
Although in this pa-per we focused on the disambiguation of dictio-nary glosses, the same approach can be applied fordisambiguating collocations according to a dictio-nary of choice, thus providing a way to further en-rich lexical resources with external knowledge.AcknowledgmentsThe author is grateful to Ken Litkowski and theanonymous reviewers for their useful comments.He also wishes to thank Zanichelli and Macquariefor kindly making their dictionaries available forresearch purposes.ReferencesRobert A. Amsler.
1980.
The structure of theMerriam-Webster pocket dictionary, Ph.D. Thesis.University of Texas, Austin, TX, USA.Jordi Atserias, Lu?
?s Villarejo, German Rigau, EnekoAgirre, John Carroll, Bernardo Magnini, and PiekVossen.
2004.
The meaning multilingual centralrepository.
In Proceedings of GWC 2004, pages 23?30, Brno, Czech Republic.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceed-ings of COLING-ACL 1998, pages 86?90, Montreal,Canada.6This is indeed an ongoing line of research in collabora-tion with the Zanichelli dictionary publisher.601Satanjeev Banerjee and Ted Pedersen.
2003.
Extendedgloss overlaps as a measure of semantic relatedness.In Proceedings of IJCAI 2003, pages 805?810, Aca-pulco, Mexico.John Bernard, editor.
1986.
Macquarie Thesaurus.Macquarie, Sydney, Australia.Tom Bohman and Lubos Thoma.
2000.
A note onsparse random graphs and cover graphs.
The Elec-tronic Journal of Combinatorics, 7:1?9.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating wordnet-based measures of semantic dis-tance.
Computational Linguistics, 32(1):13?47.Mauro Castillo, Francis Real, Jordi Asterias, and Ger-man Rigau.
2004.
The talp systems for dis-ambiguating wordnet glosses.
In Proceedings ofACL 2004 SENSEVAL-3 Workshop, pages 93?96,Barcelona, Spain.Martin Chodorow, Roy Byrd, and George Heidorn.1985.
Extracting semantic hierarchies from a largeon-line dictionary.
In Proceedings of ACL 1985,pages 299?304, Chicago, IL, USA.Thomas H. Cormen, Charles E. Leiserson, andRonald L. Rivest.
1990.
Introduction to algorithms.MIT Press, Cambridge, MA.Montse Cuadros and German Rigau.
2006.
Qualityassessment of large scale knowledge resources.
InProceedings of EMNLP 2006, pages 534?541, Syd-ney, Australia.Philip Edmonds.
2000.
Designing a task forSENSEVAL-2.
Technical note.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Database.
MIT Press, Cambridge, MA.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2003.
Learning semantic constraints for the auto-matic discovery of part-whole relations.
In Proceed-ings of NAACL 2003, pages 1?8, Edmonton, Canada.Sanda Harabagiu, George Miller, and Dan Moldovan.1999.
Wordnet 2 - a morphologically and se-mantically enhanced resource.
In Proceedings ofSIGLEX-99, pages 1?8, Maryland, USA.Nancy Ide and Jean Ve?ronis.
1993.
Extractingknowledge bases from machine-readable dictionar-ies: Have we wasted our time?
In Proceedingsof Workshop on Knowledge Bases and KnowledgeStructures, pages 257?266, Tokyo, Japan.Karin Kipper, Hoa Trang Dang, and Martha Palmer.2000.
Class-based construction of a verb lexicon.
InProceedings of AAAI 2000, pages 691?696, Austin,TX, USA.Hideki Kozima and Teiji Furugori.
1993.
Similaritybetween words computed by spreading activation onan english dictionary.
In Proceedings of ACL 1993,pages 232?239, Utrecht, The Netherlands.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: How to tell apine cone from an ice cream cone.
In Proceedingsof the 5th SIGDOC, pages 24?26, New York, NY.Kenneth C. Litkowski.
1978.
Models of the semanticstructure of dictionaries.
American Journal of Com-putational Linguistics, (81):25?74.Kenneth C. Litkowski.
2004.
Senseval-3 task:Word-sense disambiguation of wordnet glosses.
InProceedings of ACL 2004 SENSEVAL-3 Workshop,pages 13?16, Barcelona, Spain.Dan Moldovan and Adrian Novischi.
2004.
Wordsense disambiguation of wordnet glosses.
ComputerSpeech & Language, 18:301?317.Jun-Ichi Nakamura and Makoto Nagao.
1988.
Extrac-tion of semantic information from an ordinary en-glish dictionary and its evaluation.
In Proceedingsof COLING 1988, pages 459?464, Budapest, Hun-gary.Roberto Navigli and Paola Velardi.
2005.
Structuralsemantic interconnections: a knowledge-based ap-proach to word sense disambiguation.
IEEE Trans-actions of Pattern Analysis and Machine Intelligence(TPAMI), 27(7):1075?1088.Roberto Navigli.
2009.
Word sense disambiguation: asurvey.
ACM Computing Surveys, 41(2):1?69.Marco Pennacchiotti and Patrick Pantel.
2006.
On-tologizing semantic relations.
In Proceedings ofCOLING-ACL 2006, pages 793?800, Sydney, Aus-tralia.Paul Proctor, editor.
1978.
Longman Dictionary ofContemporary English.
Longman Group, UK.Giuseppe Ragazzini and Adele Biagi, editors.
2006.
IlRagazzini-Biagi, 4th Edition.
Zanichelli, Italy.Stephen D. Richardson, William B. Dolan, and LucyVanderwende.
1998.
Mindnet: acquiring and struc-turing semantic information from text.
In Proceed-ings of COLING 1998, pages 1098?1102, Montreal,Quebec, Canada.German Rigau, Jordi Atserias, and Eneko Agirre.1997.
Combining unsupervised lexical knowledgemethods for word sense disambiguation.
In Pro-ceedings of ACL/EACL 1997, pages 48?55, Madrid,Spain.Peter M. Roget.
1911.
Roget?s International The-saurus (1st edition).
Cromwell, New York, USA.Helmut Schmid.
1997.
Probabilistic part-of-speechtagging using decision trees.
In Daniel Jones andHarold Somers, editors, New Methods in LanguageProcessing, Studies in Computational Linguistics,pages 154?164.
UCL Press, London, UK.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.Semantic taxonomy induction from heterogenousevidence.
In Proceedings of COLING-ACL 2006,pages 801?808, Sydney, Australia.Lucy Vanderwende.
1996.
The analysis of noun se-quences using semantic information extracted fromon-line dictionaries, Ph.D. Thesis.
GeorgetownUniversity, Washington, USA.Yorick Wilks, Brian Slator, and Louise Guthrie, editors.1996.
Electric words: Dictionaries, computers andmeanings.
MIT Press, Cambridge, MA.Colin Yallop, editor.
2006.
The Macquarie ConciseDictionary 4th Edition.
Macquarie Library Pty Ltd,Sydney, Australia.602
