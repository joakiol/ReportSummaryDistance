Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1591?1601,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsKnowledge Graph and Text Jointly EmbeddingZhen Wang?
?, Jianwen Zhang?, Jianlin Feng?, Zheng Chen??{v-zw,jiazhan,zhengc}@microsoft.com?
{wangzh56@mail2,fengjlin@mail}.sysu.edu.cn?Microsoft Research?Sun Yat-sen UniversityAbstractWe examine the embedding approach toreason new relational facts from a large-scale knowledge graph and a text corpus.We propose a novel method of jointly em-bedding entities and words into the samecontinuous vector space.
The embeddingprocess attempts to preserve the relationsbetween entities in the knowledge graphand the concurrences of words in the textcorpus.
Entity names and Wikipedia an-chors are utilized to align the embeddingsof entities and words in the same space.Large scale experiments on Freebaseand a Wikipedia/NY Times corpus showthat jointly embedding brings promisingimprovement in the accuracy of predictingfacts, compared to separately embeddingknowledge graphs and text.
Particularly,jointly embedding enables the predictionof facts containing entities out of theknowledge graph, which cannot be han-dled by previous embedding methods.
Atthe same time, concerning the quality ofthe word embeddings, experiments on theanalogical reasoning task show that jointlyembedding is comparable to or slightlybetter than word2vec (Skip-Gram).1 IntroductionKnowledge graphs such as Freebase (Bollacker etal., 2008) and WordNet (Miller, 1995) have be-come important resources for many AI & NLP ap-plications such as Q & A.
Generally, a knowledgegraph is a collection of relational facts that are of-ten represented in the form of a triplet (head en-tity, relation, tail entity), e.g., ?
(Obama, Born-in,Honolulu)?.
An urgent issue for knowledge graph-s is the coverage, e.g., even the largest knowledgegraph of Freebase is still far from complete.Recently, targeting knowledge graph comple-tion, a promising paradigm of embedding was pro-posed, which is able to reason new facts only fromthe knowledge graph (Bordes et al., 2011; Bor-des et al., 2013; Socher et al., 2013; Wang et al.,2014).
Generally, in this series of methods, eachentity is represented as a k-dimensional vector andeach relation is characterized by an operation in<kso that a candidate fact can be asserted by sim-ple vector operations.
The embeddings are usuallylearnt by minimizing a global loss function of allthe entities and relations in the knowledge graph.Thus, the vector of an entity may encode globalinformation from the entire graph, and hence scor-ing a candidate fact by designed vector operationsplays a similar role to long range ?reasoning?
inthe graph.
However, since this requires the vectorsof both entities to score a candidate fact, this typeof methods can only complete missing facts forwhich both entities exist in the knowledge graph.However, a missing fact often contains entities outof the knowledge graph (called out-of-kb for shortin this paper), e.g., one or both entities are phras-es appearing in web text but not included in theknowledge graph yet.
How to deal with these fact-s is a significant obstacle to widely applying theembedding paradigm.In addition to knowledge embedding, anoth-er interesting approach is the word embeddingmethod word2vec (Mikolov et al., 2013b), whichshows that learning word embeddings from anunlabeled text corpus can make the vectors con-necting the pairs of words of some certainrelation almost parallel, e.g., vec(?China?)
?vec(?Beijing?)
?
vec(?Japan?)
?
vec(?Tokyo?
).However, it does not know the exact relation be-tween the pairs.
Thus, it cannot be directly appliedto complete knowledge graphs.The capabilities and limitations of knowledgeembedding and word embedding have inspired usto design a mechanism to mosaic the knowledge1591graph and the ?word graph?
together in a vectorspace so that we can score any candidate relation-al facts between entities and words1.
Therefore,we propose a novel method to jointly embed enti-ties and words into the same vector space.
In oursolution, we define a coherent probabilistic modelfor both knowledge and text, which is composedof three components: the knowledge model, textmodel, and alignment model.
Both the knowledgemodel and text model use the same core transla-tion assumption for the fact modeling: a candidatefact (h, r, t) is scored based on ?h + r ?
t?.
Theonly difference is, in the knowledge model the re-lation r is explicitly supervised and the goal is tofit the fact triplets, while in the text model we as-sume any pair of words h and t that concur in sometext windows are of certain relation r but r is a hid-den variable, and the goal is to fit the concurringpairs of words.
The alignment model guaranteesthe embeddings of entities and words/phrases liein the same space and impels the two models to en-hance each other.
Two mechanisms of alignmentare introduced in this paper: utilizing names of en-tities and utilizing Wikipedia anchors.
This way ofjointly embedding knowledge and text can be con-sidered to be semi-supervised knowledge embed-ding: the knowledge graph provides explicit su-pervision of facts while the text corpus providesmuch more ?relation-unlabeled?
pairs of words.We conduct extensive large scale experimentson Freebase and Wikipedia corpus, which showjointly embedding brings promising improve-ments to the accuracy of predicting facts, com-pared to separately embedding the knowledgegraph and the text corpus, respectively.
Particu-larly, jointly embedding enables the prediction ofa candidate fact with out-of-kb entities, which cannot be handled by any existing embedding meth-ods.
We also use embeddings to provide a priorscore to help fact extraction on the benchmark da-ta set of Freebase+NYTimes and also observe verypromising improvements.
Meanwhile, concerningthe quality of word embeddings, experiments onthe analogical reasoning task show that jointly em-bedding is comparable to or slightly better thanword2vec (Skip-Gram).1We do not distinguish between ?words?
and ?phrases?,i.e., ?words?
means ?words/phrases?.2 Related WorkKnowledge Embedding.
A knowledge graph isembedded into a low-dimensional continuous vec-tor space while certain properties of it are pre-served (Bordes et al., 2011; Bordes et al., 2013;Socher et al., 2013; Chang et al., 2013; Wang etal., 2014).
Generally, each entity is representedas a point in that space while each relation is inter-preted as an operation over entity embeddings.
Forinstance, TransE (Bordes et al., 2013) interprets arelation as a translation from the head entity to thetail entity.
The embedding representations are usu-ally learnt by minimizing a global loss function in-volving all entities and relations so that each entityembedding encodes both local and global connec-tivity patterns of the original graph.
Thus, we canreason new facts from learnt embeddings.Word Embedding.
Generally, word embeddingsare learned from a given text corpus without su-pervision by predicting the context of each wordor predicting the current word given its contex-t (Bengio et al., 2003; Collobert et al., 2011;Mikolov et al., 2013a; Mikolov et al., 2013b).
Al-though relations between words are not explicitlymodeled, continuous bag-of-words (CBOW) andSkip-gram (Mikolov et al., 2013a; Mikolov et al.,2013b) learn word embeddings capturing manysyntactic and semantic relations between wordswhere a relation is also represented as the trans-lation between word embeddings.Relational Facts Extraction.
Another pivotalchannel for knowledge graph completion is ex-tracting relational facts from external sources suchas free text (Mintz et al., 2009; Riedel et al., 2010;Hoffmann et al., 2011; Surdeanu et al., 2012;Zhang et al., 2013; Fan et al., 2014).
This se-ries of methods focuses on identifying local textpatterns that express a certain relation and makingpredictions based on them.
However, they havenot fully utilized the evidences from a knowledgegraph, e.g., knowledge embedding is able to rea-son new facts without any external sources.
Ac-tually, knowledge embedding is very complemen-tary to traditional extraction methods, which wasfirst confirmed by (Weston et al., 2013).
To es-timate the plausibility of a candidate fact, theyadded scores from embeddings to scores from anextractor, which showed significant improvemen-t.
However, as pointed out in the introduction,their knowledge embedding method cannot pre-dict facts involving out-of-kb entities.15923 Jointly Embedding Knowledge andTextWe will first describe the notation used in this pa-per.
A knowledge graph ?
is a set of triplets inthe form (h, r, t), h, t ?
E and r ?
R where E isthe entity vocabulary and R is a collection of pre-defined relations.
We use bold letters h, r, t to de-note the corresponding embedding representation-s of h, r, t. A text corpus is a sequence of wordsdrawn from the word vocabulary V .
Note that weperform some preprocessing to detect phrases inthe text and the vocabulary here already includesthe phrases.
For simplicity?s sake, without spe-cial explanation, when we say ?word(s)?, it means?word(s)/phrase(s)?.
Since we consider triplets in-volving not only entities but also words, we denoteI = E ?V .
Additionally, we denote anchors byA.3.1 ModelingOur model is composed of three components:the knowledge model, text model, and alignmentmodel.Before defining the component models, we firstdefine the element model for a fact triplet.
In-spired by TransE, we also represent a relation ras a vector r ?
<kand score a fact triplet (h, r, t)by z(h, r, t) = b ?12?h + r ?
t?2where b is aconstant for bias designated for adjusting the scalefor better numerical stability and b = 7 is a sensi-ble choice.
z(h, r, t) is expected to be large if thetriplet is true.
Based on the same element model offact, we define the component models as follows.3.1.1 Knowledge ModelWe define the following conditional probability ofa fact (h, r, t) in a knowledge graph:Pr(h|r, t) =exp{z(h, r, t)}?
?h?Iexp{z(?h, r, t)}(1)and we have named our model pTransE (Proba-bilistic TransE) to show respect to TransE.
We alsodefine Pr(r|h, t) and Pr(t|h, r) in the same wayby choosing corresponding normalization termsrespectively.
We define the likelihood of observ-ing a fact triplet as:Lf(h, r, t) = log Pr(h|r, t)+ log Pr(t|h, r)+ log Pr(r|h, t)(2)The goal of the knowledge model is to maximizethe conditional likelihoods of existing fact tripletsin the knowledge graph:LK=?(h,r,t)?
?Lf(h, r, t) (3)3.1.2 Text ModelWe propose the following key assumption formodeling text, which connects word embeddingand knowledge embedding: there are relationsbetween words although we do not know whatthey are.Relational Concurrence Assumption.
If twowords w and v concur in some context, e.g., a win-dow of text, then there is a relation rwvbetweenthe two words.
That is, we can state the triplet of(w, rwv, v) is a fact.We define the conditional probabilityPr(w|rwv, v) following the same formulationof Eq.
(1) to model why two words concur in somecontext.
In contrast to knowledge embedding,here rwvis a hidden variable rather than explicitlysupervised.The challenge is to deal with the hidden variablerwv.
Obviously, without any more assumption-s, the number of distinct rwvis around |V| ?
?N ,where?N is the average number of unique word-s concurred with each word.
This number is ex-tremely large.
Thus it is almost impossible to esti-mate a vector for each rwv.
And the problem is ac-tually ill-posed.
We need to constrain the freedomdegree of rwv.
Here we use auxiliary variables toreduce the size of variables we need to estimate:let w?= w + rwv, thenz(w, rwv,v) , z(w?,v) = b?12?w?
?v?2(4)andPr(w|rwv, v) , Pr(w|v) =exp{z(w?,v)}?w??Vexp{z(w?
?,v)}(5)In this way we need to estimate vectors w and w?for each word w, and a total of 2?
|V| vectors.The goal of the text model is to maximize thelikelihood of the concurrences of pairs of words intext windows:LT=?
(w,v)?Cnwvlog Pr(w|v).
(6)In the above equation, C is all the distinct pairs ofwords concurring in text windows of a fixed size.And nwvis the number of concurrences of the pair(w, v).
Interestingly, as explained in Sec.
(3.3),this text model is almost equivalent to Skip-Gram.15933.1.3 Alignment ModelIf we only have the knowledge model and textmodel, the entity embeddings and word embed-dings will be in different spaces and any comput-ing between them is meaningless.
Thus we needmechanisms to align the two spaces into the sameone.
We propose two mechanisms in this paper: u-tilizing Wikipedia anchors, and utilizing names ofentities.Alignment by Wikipedia Anchors.
This mod-el is based on the connection between Wikipediaand Freebase: for most Wikipedia (English) pages,there is an unique corresponding entity in Free-base.
As a result, for most of the anchors inWikipedia, each of which refers to a Wikipedi-a page, we know that the surface phrase v of ananchor actually refers to the Freebase entity ev.Thus, we define a likelihood for this part of an-chors as Eq.
(6) but replace the word pair (w, v)with the word-entity pair (w, ev), i.e., using thecorresponding entity evrather than the surfaceword v in Eq.(5):LAA=?
(w,v)?C,v?Alog Pr(w|ev) (7)where A denotes the set of anchors.In addition to Wikipedia anchors, we can alsouse an entity linking system with satisfactory per-formance to produce the pseudo anchors.Alignment by Names of Entities.
Another wayis to use the names of entities.
For a fact triplet(h, r, t) ?
?, if h has a namewhandwh?
V , thenwe will generate a new triplet of (wh, r, t) and addit to the graph.
Similarly, we also add (h, r, wt)and (wh, r, wt) into the graph if the names existand belong to the word vocabulary.
We call thissub-graph containing names the name graph anddefine a likelihood for the name graph by observ-ing its triplets:LAN=?(h,r,t)?
?I[wh?V ?wt?V]?Lf(wh, r, wt)+I[wh?V]?
Lf(wh, r, t) + I[wt?V]?
Lf(h, r, wt)(8)Both alignment models have advantages anddisadvantages.
Alignment by names of entities isstraightforward and does not rely on additional da-ta sources.
The number of triplets generated by thenames is also large and can significantly changethe results.
However, this model is risky.
On theone hand, the name of an entity is ambiguous be-cause different entities sometimes have the samename so that the name graph may contaminate theknowledge embedding.
On the other hand, an en-tity often has several different aliases when men-tioned in the text but we do not have the completeset, which will break the semantic balance of wordembedding.
For example, for the entity Apple In-c., suppose we only have the standard name ?Ap-ple Inc.?
but do not have the alias ?apple?.
And forthe entity Apple that is fruit, suppose we have thename ?apple?
included in the name graph.
Thenthe vector of the word ?apple?
will be biased tothe concept of fruit rather than the company.
But ifno name graph intervenes, the unsupervised wordembedding is able to learn a vector that is closer tothe concept of the company due to the polarities.Alignment by anchors relies on the additional datasource of Wikipedia anchors.
Moreover, the num-ber of matched Wikipedia anchors (?40M) is rela-tively small compared to the total number of wordpairs (?2.0B in Wikipedia) and hence the contri-bution is limited.
However, the advantage is thatthe quality of the data is very high and there are noambiguity/completeness issues.Considering the above three component modelstogether, the likelihood we maximize is:L = LK+ LT+ LA(9)where LAcould be LAAor LANor LAA+ LAN.3.2 Training3.2.1 Approximation to the NormalizersIt is difficult to directly compute the normalizers inPr(h|r, t) (or Pr(t|h, r), Pr(r|h, t)) and Pr(w|v)as the normalizers sum over |I| or |V| terms whereboth |I| and |V| reach tens of millions.
To pre-vent having to exactly calculate the normalizer-s, we use negative sampling (NEG) (Mikolov etal., 2013b) to transform the original objective, i.e.,Eq.
(9) to a simple objective of the binary classifi-cation problem?differentiating the observed datafrom noise.First, we define: (i) the probability of a giventriplet (h, r, t) to be true (D = 1); and (ii) theprobability of a given word pair (w, v) to co-occur(D = 1):Pr(D = 1|h, r, t) = ?
(z(h, r, t)) (10)Pr(D = 1|w, v) = ?
(z(w?,v)) (11)where ?
(x) =11+exp{?x}and D ?
{0, 1}.1594Instead of maximizing log Pr(h|r, t) in Eq.
(2),we maximize:log Pr(1|h, r, t)+c?i=1E?hi?Prneg(?hi)[Pr(0|?hi, r, t)](12)where c is the number of negative examples tobe discriminated for each positive example.
NEGguarantees that maximizing Eq.
(12) can approxi-mately maximize log Pr(h|r, t).
Thus, we also re-place log Pr(r|h, t), log Pr(t|r, h) in Eq.
(2), andlog Pr(w|v) in Eq.
(6) in the same way by choosingcorresponding negative distributions respectively.As a result, the objectives of both the knowledgemodel LK(Eq.
(3)) and text model LT(Eq.
(6)) arefree from cumbersome normalizers.3.2.2 OptimizationWe use stochastic gradient descent (SGD) to max-imize the simplified objectives.Knowledge model.
?
is randomly tra-versed multiple times.
When a positive example(h, r, t) ?
?
is considered, to maximize (12), weconstruct c negative triplets by sampling elementsfrom an uniform distribution over I and replacingthe head of (h, r, t).
The transformed objective oflog Pr(r|h, t) is maximized in the same manner,but by sampling from a uniform distribution overR and corrupting the relation of (h, r, t).
After amini-batch, computed gradients are used to updatethe involved embeddings.Text model.
The text corpus is traversed one ormore times.
When current word v and a contextword w are considered, c words are sampled fromthe unigram distribution raised to the 3/4rd powerand regarded as negative examples (w?, v) that arenever concurrent.
Then we compute and updatethe related gradients.Alignment model.
LAAand LANare absorbedby the text model and knowledge model respec-tively, since anchors are considered to predict con-text given an entity and the name graph are homo-geneous to the original knowledge graph.Joint.
All three component objectives are si-multaneously optimized.
To deal with large-scaledata, we implement a multi-thread version withshared memory.
Each thread is in charge of a por-tion of the data (either knowledge or text corpus),and traverses through them, calculates gradientsand commits the update to the global model andis stored in a block of shared memory.
For theTable 1: Data: triplets used in our experiments.#R #E #Triplet (Train/Valid/Test)4,490 43,793,608 123,062,855 40,528,963 40,528,963sake of efficiency, no lock is used on the sharedmemory.3.3 Connections to Related ModelsTransE.
(Bordes et al., 2013) proposed to mod-el a relation r as a translation vector r ?
<kwhich is expected to connect h and t with lowerror if (h, r, t) ?
?.
We also follow it.
How-ever, TransE uses a margin based ranking loss{?h+r?t?2+????h+r??t?2}+.
It is not a proba-bilistic model and hence it needs to restrict the nor-m of either entity embedding and/or relation em-bedding.
Bordes et al.
(2013) intuitively addressesthis problem by simply normalizing the entity em-beddings to the unit sphere before computing gra-dients at each iteration.
We define pTransE as aprobabilistic model, which doesn?t need addition-al constraints on the norms of embeddings of en-tities/words/relations, and thus eliminates the nor-malization operations.Skip-gram.
(Mikolov et al., 2013a; Mikolov et al.,2013b) defines the probability of the concurrenceof two words in a window as:Pr(w|v) =exp{w?Tv}?w??Vexp{w?
?Tv}(13)which is based on the inner product, while our textmodel (Eqs.
(4), (5)) is based on distance.
If weconstrain ?w?
= 1 for each w, then w?Tv =1 ?12?w??
v?2.
It is easy to see that our textmodel is equivalent to Skip-gram in this case.
Ourdistance-based text model is directly derived fromthe triplet fact model, which clearly explains whyit is able to make the pairs of entities of a certainrelation parallel in the vector space.4 ExperimentsWe empirically evaluate and compare related mod-els with regards to three tasks: triplet classifica-tion (Socher et al., 2013), improving relation ex-traction (Weston et al., 2013), and the analogi-cal reasoning task (Mikolov et al., 2013a).
Therelated models include: for knowledge embed-ding alone, TransE (Bordes et al., 2013), pTransE(proposed in this paper); for word embeddingalone, Skip-gram (Mikolov et al., 2013b); for both1595Table 2: Data: the number of e ?
e, w ?
e, e ?w, w ?
w triplets/analogies where w representsthe out-of-kb entity, which is regarded as word andreplaced by its corresponding entity name.Type#Triplet (Valid/Test)#Analogye?
e12,305,200 12,305,20071,441w ?
e3,655,164 3,654,40470,878e?
w3,643,914 3,642,97870,442w ?
w460,762 451,38140,980knowledge and text, we use ?respectively?
to re-fer to the embeddings learnt by TransE/pTransEand Skip-gram, respectively, ?jointly?
to refer toour jointly embedding method, in which ?anchor?and ?name?
refer to ?Alignment by Wikipedia An-chors?
and ?Alignment by Names of Entities?, re-spectively.4.1 DataTo learn the embedding representations of entitiesand words, we use a knowledge graph, a text cor-pus, and some connections between them.Knowledge.
We adopt Freebase as our knowl-edge graph.
First, we remove the user profiles,version control, and meta data, leaving 52,124,755entities, 4,490 relations, and 204,120,782 triplet-s. We call this graph main facts.
Then we heldout 8,331,147 entities from main facts and regardthem as out-of-kb entities.
Under such a setting,from main facts, we held out all the triplets in-volving out-of-kb entities, as well as 24,610,400triplets that don?t contain out-of-kb entities.
Held-out triplets are used for validation and testing; theremaining triplets are used for training.
See Table1 for the statistics.We regard out-of-kb entities as words/phrasesand thus divide the held-out triplets into four type-s: no out-of-kb entity (e?e), the head is out-of-kbentity but the tail is not (w ?
e), the tail is out-of-kb entity but the head is not (e?
w), and both thehead and tail are out-of-kb entities (w ?
w).
Thenwe replace the out-of-kb entities among the held-out triplets by their corresponding entity names.The mapping from a Freebase entity identifier toits name is done through the Freebase predicate??/type/object/name?.
Since some entity namesare not present in our vocabulary V , we removetriplets involving these names (see Table 2).
Insuch a way, besides the missing edges between ex-isting entities, the related models can be evaluatedon triplets involving words/phrases as their headTable 3: Triplet Classification: comparison be-tween TransE and pTransE over e?
e triplets.Method Accuracy (%) Area under PR curveTransE 93.1 0.86pTransE 93.4 0.97and/or tail.Text.
We adopt the Wikipedia (English) cor-pus.
After removing pages designated for nav-igation, disambiguation, or discussion purpos-es, there are 3,469,024 articles left.
We ap-ply sentence segmentation, tokenization, Part-of-Speech (POS) tagging, and named entity recog-nition (NER) to these articles using ApacheOpenNLP package2.
Then we conduct some sim-ple chunking to acquire phrases: if several con-secutive tokens are identically tagged as ?Loca-tion?/?Person?/?Organization?, or covered by ananchor, we combine them as a chunk.
After thepreprocessing, our text corpus contain 73,675,188sentences consisting of 1,522,291,723 chunks.
A-mong them, there are around 20 millions distinctchunks, including words and phrases.
We filter outpunctuation and rare words/phrases that occur lessthan three times in the text corpus, reducing |V| to5,240,003.Alignment.
One of our alignment models need-s Wikipedia anchors.
There are around 45 millionsuch anchors in our text corpus and 41,970,548 ofthem refer to entities in E .
Another mechanism u-tilizes the name graph constructed through namesof entities.
Specifically, for each training triplet(h, r, t), suppose h and t have entity names whand wt, respectively and wh, wt?
V , the train-ing triplet contributes (wh, r, wt), (wh, r, t), and(h, r, wt) to the name graph.
There are 81,753,310triplets in our name graphs.
Note that there is nooverlapping between the name graph and held-outtriplets of e?
w, w ?
e, and w ?
w types.4.2 Triplet ClassificationThis task judges whether a triplet (h, r, t) is trueor false, i.e., binary classification of a triplet.Evaluation protocol.
Following the same pro-tocol in NTN (Socher et al., 2013), for each truetriplet, we construct a false triplet for it by ran-domly sampling an element from I to corrupt itshead or tail.
Since |E| is significantly larger than|V| in our data, sampling from a uniform distri-2https://opennlp.apache.org1596Table 4: Triplet classification: accuracy (%) over various types of triplets.Type e?
e w ?
e e?
w w?w allrespectively 93.4 52.1 51.4 71.0 77.5jointly (anchor) 94.4 67.0 66.7 79.8 81.9jointly (name) 94.5 80.5 80.0 89.0 87.7jointly (anchor+name) 95.0 82.0 81.5 90.0 88.8bution over I will let triplets involving no worddominate the false triplets.
To avoid that, when wecorrupt the head of (h, r, t), if h ?
E , h?is sam-pled from E while if h ?
V , h?is sampled from V .The same rule is applied when we corrupt the tailof (h, r, t).
In this way, for each of the four typesof triplets, we ensure the number of true triplets isequal to that of false ones.To classify a triplet (h, r, t), we first use the con-sidered methods to score it.
TransE scores it by?|h + r ?
t|.
Our models score it by Pr(D =1|h, r, t) (see Eq.(10)).
Then the considered meth-ods label a triplet (h, r, t) as true if its score islarger than the relation-specific threshold of r, asfalse otherwise.
The relation-specific thresholdsare chosen to maximize the classification accura-cy over the validation set.We report the classification accuracy.
Addition-ally, we rank all the testing triplets by their scoresin descending order.
Then we draw a precision-recall (PR) curve based on this ranking and reportthe area under the PR curve.Implementation.
We implement TransE (Bor-des et al., 2013), Skip-gram (Mikolov et al.,2013a), and our models.First, we train TransE and pTransE over ourtraining triplets with embedding dimension kin {50, 100, 150}.
Adhering to (Bordes et al.,2013), we use the fixed learning rate ?
in{0.005, 0.01, 0.05} for TransE during its 300 e-pochs.
For pTransE, we use the number of neg-ative examples per positive example c among{5, 10}, the learning rate ?
among {0.01, 0.025}where ?
decreases along with its 40 epochs.
Theoptimal configurations of TransE are: k = 100,?
= 0.01.
The optimal configurations of pTransEare: k = 100, c = 10, and ?
= 0.025.Then we train Skip-gram with the embeddingdimension k in {50, 100, 150}, the max skip-ranges in {5, 10}, the number of negative examples perpositive example c in {5, 10}, and learning rate?
= 0.025 linearly decreasing along with the 6epochs over our text corpus.
Popular words whosefrequencies are larger than 10?5are subsampledaccording to the trick proposed in (Mikolov et al.,2013b).
The optimal configurations of Skip-gramare: k = 150, s = 5, and c = 10.Combining entity embeddings and word em-beddings learnt by pTransE and Skip-gram respec-tively, ?respectively?
model can score all types ofheld-out triplets.
For our jointly embedding mod-el, we consider various alignment mechanisms anduse equal numbers of threads for knowledge mod-el and text model.
The best configurations of?jointly?
model are: k = 150, s = 5, c = 10, and?
= 0.025 which linearly decreases along with the6 epochs of traversing text corpus.Results.
We first illustrate the comparison be-tween TransE and pTransE over e?
e type triplet-s in Table 3.
Observing the scores assigned totrue triplets by TransE, we notice that triplets ofpopular relations generally have larger scores thanthose of rare relations.
In contrast, pTransE, asa probabilistic model, assigns comparable scoresto true triplets of both popular and rare relations.When we use a threshold to separate true tripletsfrom false triplets of the same relation, there is noobvious difference between the two models.
How-ever, when all triplets are ranked together, assign-ing scores in a more uniform scale is definitely anadvantage.
Thus, the contradiction stems from thedifferent training strategies of the two models andthe consideration of relation-specific thresholds.Classification accuracies over various types ofheld-out triplets are presented in Table 4.
The?jointly?
model outperforms the ?respectively?model no matter which alignment mechanism(s)are used.
Actually, for the ?respectively?
model,there is no interaction between entity embeddingsand word embeddings during training and thus it-s predictions, over triplets that involve both enti-ty and word at the same time, are not much bet-ter than random guessing.
It is also a natural re-sult that alignment by names is more effective thanalignment by anchors.
The number of anchors ismuch smaller than the number of overall chunksin our text corpus.
In addition, the number of en-tities mentioned by anchors is very limited com-15970.0 0.2 0.4 0.6 0.8 1.0Recall0.750.800.850.900.951.001.05precisionMintz (0.864752658197)Mintz+Jointly (0.891043673778)Mintz+Knowledge (0.917260610051)0.0 0.2 0.4 0.6 0.8 1.0Recall0.30.40.50.60.70.80.91.0precisionMintz (0.512956668019)Mintz+Jointly (0.636313453126)0.0 0.2 0.4 0.6 0.8 1.0Recall0.40.50.60.70.80.91.0precisionMintz (0.662641417434)Mintz+Jointly (0.695062505333)0.0 0.2 0.4 0.6 0.8 1.0Recall0.000.050.100.150.200.250.300.35precisionMintz (0.0914506877334)Mintz+Jointly (0.0993184342972)0.0 0.2 0.4 0.6 0.8 1.0Recall0.10.20.30.40.50.60.70.80.91.0precisionMintz (0.363969399646)Mintz+Jointly (0.480909766665)Mintz+Knowledge (0.418875519101)Figure 1: Improving Relation Extraction: PR curves of Mintz alone or combined with knowledge(pTransE) / jointly model over (a) e?
e, (b) w ?
e, (c) e?
w, (d) w ?
w, and (e) all triplets.pared with |E|.
Thus, interactions brought in byanchors are not as significant as that of the namegraph.4.3 Improving Relation ExtractionIt has been shown that embedding models are verycomplementary to extractors (Weston et al., 2013).However, some entities detected from text are out-of-kb entities.
In such a case, triplets involvingthese entities cannot be handled by any existingknowledge embedding method, but our jointly em-bedding model can score them.
As our model cancover more candidate triplets provided by extrac-tors, it is expected to provide more significant im-provements to extractors than any other embed-ding model.
We confirm this point as follow.Evaluation protocol.
For relation extraction,we use a public dataset?NYT+FB (Riedel et al.,2010)3, which distantly labels the NYT corpus byFreebase facts.
We consider (Mintz et al., 2009)and Sm2r (Weston et al., 2013) as our extractorsto provide candidate triplets as well as their plau-sibilities estimated according to text features.For embedding, we first held out triplets fromour training set that appear in the test set ofNYT+FB.
Then we train TransE, pTransE and the?jointly?
model on the remaining training tripletsas well as on our text corpus.
Then we use thesemodels to score each candidate triplet in the same3http://iesl.cs.umass.edu/riedel/ecml/way as the previous triplet classification experi-ment.For combination, we first divide each candidatetriplet into one of these categories: e ?
e, e ?
w,w ?
e, w ?
w, and ?out-of-vocabulary?.
Be-cause there is no embedding model that can scoretriplets involving out-of-vocabulary word/phrase,we just ignore these triplets.Please note that, forour jointly embedding model, there are no ?out-of-vocabulary?
triplets if we include the NYT cor-pus for training.
We use the embedding modelsto score candidate triplets and combine the scoresgiven by the embedding model with scores givenby the extractors.
For each type e?e, e?w, w?e,w?w and their union (i.e.
all), we rank the candi-date triplets by their revisited scores and draw PRcurve to observe which embedding method pro-vides the most significant improvements to the ex-tractors.Implementation.
For (Mintz et al., 2009), weuse the implementation in (Surdeanu et al., 2012)4.We implement Sm2r by ourselves with the best hy-perparameters introduced in (Weston et al., 2013).For TransE, pTransE, and the ?jointly?
model, weuse the same implementations, scoring schemes,and optimal configurations as the triplet classifica-tion experiment.To combine extractors with embedding mod-4http://nlp.stanford.edu/software/mimlre.shtml15980.0 0.2 0.4 0.6 0.8 1.0Recall0.550.600.650.700.750.800.850.900.951.00precisionSm2r (0.773014296476)Sm2r+Jointly (0.858251870864)Sm2r+Knowledge (0.858251870864)0.0 0.2 0.4 0.6 0.8 1.0Recall0.860.880.900.920.940.960.981.00precisionSm2r (0.908752270146)Sm2r+Jointly (0.966914103913)0.0 0.2 0.4 0.6 0.8 1.0Recall0.820.840.860.880.900.920.940.960.981.00precisionSm2r (0.875810724647)Sm2r+Jointly (0.966676169402)0.0 0.2 0.4 0.6 0.8 1.0Recall0.20.30.40.50.60.70.80.91.0precisionSm2r (0.431460754321)Sm2r+Jointly (0.536200901997)0.0 0.2 0.4 0.6 0.8 1.0Recall0.500.550.600.650.700.750.800.850.90precisionSm2r (0.67446140565)Sm2r+Jointly (0.802071230237)Sm2r+Knowledge (0.697899016348)Figure 2: Improving Relation Extraction: PR curves of Sm2r alone or combined with knowledge(TransE) / jointly model over (a) e?
e, (b) w ?
e, (c) e?
w, (d) w ?
w, and (e) all triplets.els, we consider two schemes.
Since Mintz s-cores candidate triplets in a probabilistic man-ner, we linearly combine its scores with the s-cores given by pTransE or the ?jointly?
mod-el: ?
PrMintz+(1 ?
?)
PrpTransE/Jointlywhere ?
isenumerated from 0 to 1 with 0.025 as a searchstep.
On the other hand, neither Sm2r nor TransEis a probabilistic model.
Thus, we combineSm2r with TransE or the ?jointly?
model ac-cording to the scheme proposed in (Weston etal., 2013) where for each candidate (h, r, t), if?r?6=r?
(Score(h, r, t) < Score(h, r?, t)) is lessthan ?
, we increase ScoreSm2r(h, r, t) by p. Wesearch for the best ?, ?
, and p on another dataset?Wikipedia corpus distantly labeled by Freebase.Result.
We present the PR curves in Fig.
(1,2).
Over candidate triplets provided by eitherMintz or Sm2r, the ?jointly?
model is consis-tently comparable with the ?knowledge?
model(TransE/pTransE) over e ?
e triplets while it out-performs the ?knowledge?
model by a consider-able margin over triplets of other types.
Theseresults confirm the advantage of jointly embed-ding and are actually straightforward results of ourtriplet classification experiment because the onlydifference is that the triplets here are provided bythe extractor.Table 6: Phrases Analogical Reasoning Task.Method Accuracy (%) Hits@10 (%)Skip-gram 18.0 56.1Jointly (anchor) 27.6 65.0Jointly (name) 11.3 40.6Jointly (anchor+name) 18.3 54.0Table 7: Constructed Analogical ReasoningTask.Method Accuracy (%) Hits@10 (%)Skip-gram 10.5 14.1Jointly (anchor) 10.5 14.3Jointly (name) 11.5 16.2Jointly (anchor+name) 11.6 16.54.4 Analogical Reasoning TaskWe compare our method with Skip-gram on thistask to observe and study the influences of bothknowledge embedding and alignment mechanismson the quality of word embeddings.Evaluation protocol.
We use the same pub-lic datasets as in (Mikolov et al., 2013b): 19,544word analogies5; 3,218 phrase analogies6.
We al-so construct analogies from our held-out triplet-s (see Table 2) by first concatenating two entitypairs of the same relation to form an analogy and5code.google.com/p/word2vec/source/browse/trunk/questions-words.txt6code.google.com/p/word2vec/source/browse/trunk/questions-phrases.txt1599Table 5: Words Analogical Reasoning Task.Method Accuracy (%) Hits@10 (%)Semantic Syntactic Total Semantic Syntactic TotalSkip-gram 71.4 69.0 70.0 90.4 89.3 89.8Jointly (anchor) 75.3 68.3 71.2 91.5 88.9 89.9Jointly (name) 54.5 54.2 59.0 75.8 86.5 82.1Jointly (anchor+name) 56.5 65.7 61.9 78.1 87.6 83.6then replacing the entities by corresponding entitynames, e.g., ?
(Obama, Honolulu, David Beckham,London)?
where the relation is ?Born-in?.Following (Mikolov et al., 2013b), we only con-sider analogies that consist of the top-K most fre-quent words/phrases in the vocabulary.
For eachanalogy denoted by (h1, t1, h2, t2), we enumer-ate all the top-K most frequent words/phrases wexcept for h1, t1, h2, and calculate the distance(Cosine/Euclidean according to specific model)between h2+ (t1?
h1) and w. Ordering allthese words/phrases by their distances in ascend-ing order, we obtain the rank of the correct an-swer t2.
Finally, we report Hits@10 (i.e., the pro-portion of correct answers whose ranks are notlarger than 10) and accuracy (i.e., Hits@1).
Forword analogies and constructed analogies, we setK = 200, 000; while for phrase analogies, we setK = 1, 000, 000 to recall sufficient analogies.Implementation.
For Skip-gram and the?Jointly?
(anchor/name/anchor+name) model, weuse the same implementations and optimal config-urations as the triplet classification experiment.Results.
Jointly embedding using Wikipedi-a anchors for alignment consistently outperformsSkip-gram (Table 5, 6, 7) showing that the influ-ence of knowledge embedding, injected into wordembedding through Wikipedia anchors, is benefi-cial.
The vector of an ambiguous word is often amixture of its several meanings but, in a specificcontext, the word is disambiguated and refers toa specific meaning.
Using global word embeddingto predict words within a specific context may pol-lute the embeddings of surrounding words.
Align-ment by anchors enables entity embeddings to al-leviate the propagation of ambiguities and thus im-proves the quality of word embeddings.Using entity names for alignment hurts the per-formance of analogies of words and phrases (Ta-ble 5, 6).
The main reason is that these analo-gies are popular facts frequently mentioned in tex-t while a name graph forces word embeddings tosatisfy both popular and rare facts.
Another rea-son stems from the versatility of mentioning anentity.
Consider ?
(Japan, yen, Europe, euro)?
forexample.
Knowledge embedding is supposed togive significant help to completing this analogy as?/location/country/currency??
R. However, theentity of Japanese currency is named ?Japaneseyen?
rather than ?yen?
and thus the explicit trans-lation learnt from knowledge embedding is not di-rectly imposed on the word embedding of ?yen?.In contrast, using entity names for alignment im-proves the performances on constructed analogies(Table 7).
Since there is a relation r ?
R foreach constructed analogy (wh1, wt1, wh2, wt2), al-though neither (wh1, r, wt1) nor (wh2, r, wt2) ispresent in the name graph, other facts involvingthese words act on the vectors of these words, inthe same manner of traditional knowledge embed-ding.Overall, any high-quality entity linking systemcan be used to further improve the performance.5 ConclusionsIn this paper, we introduced a novel method ofjointly embedding knowledge graphs and a textcorpus so that entities and words/phrases are rep-resented in the same vector space.
In such a way,our method can perform prediction on any can-didate facts between entities/words/phrases, goingbeyond previous knowledge embedding methods,which can only predict facts whose entities existin knowledge graph.
Extensive, large-scale exper-iments show that the proposed method is very ef-fective at reasoning new facts.
In addition, we alsoprovides insights into word embedding, especiallyon the capability of analogical reasoning.
In thisaspect, we empirically observed some hints thatjointly embedding also helps word embedding.ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.1600Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-turge, and Jamie Taylor.
2008.
Freebase: a collab-oratively created graph database for structuring hu-man knowledge.
In Proceedings of the 2008 ACMSIGMOD International Conference on Managementof Data, pages 1247?1250.
ACM.Antoine Bordes, Jason Weston, Ronan Collobert, andYoshua Bengio.
2011.
Learning structured embed-dings of knowledge bases.
In Proceedings of theTwenty-Fifth AAAI Conference on Artificial Intelli-gence, pages 301?306.Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.2013.
Translating embeddings for modeling multi-relational data.
In Advances in Neural InformationProcessing Systems, pages 2787?2795.Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.2013.
Multi-relational latent semantic analysis.
InProceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing, pages1602?1612, Seattle, Washington, USA, October.Association for Computational Linguistics.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu,Thomas Fang Zheng, and Edward Y. Chang.
2014.Distant supervision for relation extraction with ma-trix completion.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 839?849,Baltimore, Maryland, June.
Association for Compu-tational Linguistics.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S Weld.
2011.
Knowledge-based weak supervision for information extractionof overlapping relations.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies-Volume 1, pages 541?550.
Association for Compu-tational Linguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word rep-resentations in vector space.
arXiv preprint arX-iv:1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems 26, pages 3111?3119.George A Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 2-Volume 2, pages 1003?1011.
Association forComputational Linguistics.Sebastian Riedel, Limin Yao, and Andrew McCal-lum.
2010.
Modeling relations and their mention-s without labeled text.
In Machine Learning andKnowledge Discovery in Databases, pages 148?163.Springer.Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Ng.
2013.
Reasoning with neural ten-sor networks for knowledge base completion.
In Ad-vances in Neural Information Processing Systems,pages 926?934.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,and Christopher D Manning.
2012.
Multi-instancemulti-label learning for relation extraction.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 455?465.
Association for Computational Linguistics.Zhen Wang, Jianwen Zhang, Jianlin Feng, and ZhengChen.
2014.
Knowledge graph embedding by trans-lating on hyperplanes.
In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,pages 1112?1119.Jason Weston, Antoine Bordes, Oksana Yakhnenko,and Nicolas Usunier.
2013.
Connecting languageand knowledge bases with embedding models for re-lation extraction.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1366?1371, Seattle, Washington,USA, October.
Association for Computational Lin-guistics.Xingxing Zhang, Jianwen Zhang, Junyu Zeng, JunYan, Zheng Chen, and Zhifang Sui.
2013.
Towardsaccurate distant supervision for relational facts ex-traction.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistic-s (Volume 2: Short Papers), pages 810?815, Sofi-a, Bulgaria, August.
Association for ComputationalLinguistics.1601
