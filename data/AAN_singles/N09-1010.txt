Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 83?91,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAdding More Languages Improves Unsupervised MultilingualPart-of-Speech Tagging: A Bayesian Non-Parametric ApproachBenjamin Snyder, Tahira Naseem, Jacob Eisenstein, and Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{bsnyder, tahira, jacobe, regina}@csail.mit.eduAbstractWe investigate the problem of unsupervisedpart-of-speech tagging when raw parallel datais available in a large number of languages.Patterns of ambiguity vary greatly across lan-guages and therefore even unannotated multi-lingual data can serve as a learning signal.
Wepropose a non-parametric Bayesian model thatconnects related tagging decisions across lan-guages through the use of multilingual latentvariables.
Our experiments show that perfor-mance improves steadily as the number of lan-guages increases.1 IntroductionIn this paper we investigate the problem of unsu-pervised part-of-speech tagging when unannotatedparallel data is available in a large number of lan-guages.
Our goal is to develop a fully joint multilin-gual model that scales well and shows improved per-formance for individual languages as the total num-ber of languages increases.Languages exhibit ambiguity at multiple levels,making unsupervised induction of their underlyingstructure a difficult task.
However, sources of lin-guistic ambiguity vary across languages.
For exam-ple, the word fish in English can be used as either averb or a noun.
In French, however, the noun pois-son (fish) is entirely distinct from the verbal formpe?cher (to fish).
Previous work has leveraged thisidea by building models for unsupervised learningfrom aligned bilingual data (Snyder et al, 2008).However, aligned data is often available for manylanguages.
The benefits of bilingual learning varymarkedly depending on which pair of languages isselected, and without labeled data it is unclear howto determine which supplementary language is mosthelpful.
In this paper, we show that it is possi-ble to leverage all aligned languages simultaneously,achieving accuracy that in most cases outperformseven optimally chosen bilingual pairings.Even in expressing the same meaning, languagestake different syntactic routes, leading to variationin part-of-speech sequences.
Therefore, an effec-tive multilingual model must accurately model com-mon linguistic structure, yet remain flexible to theidiosyncrasies of each language.
This tension onlybecomes stronger as additional languages are addedto the mix.
From a computational standpoint, themain challenge is to ensure that the model scaleswell as the number of languages increases.
Caremust be taken to avoid an exponential increase inthe parameter space as well as the time complexityof inference procedure.We propose a non-parametric Bayesian model forjoint multilingual tagging.
The topology of ourmodel connects tagging decisions within a languageas well as across languages.
The model scales lin-early with the number of languages, allowing us toincorporate as many as are available.
For each lan-guage, the model contains an HMM-like substruc-ture and connects these substructures to one anotherby means of cross-lingual latent variables.
Thesevariables, which we refer to as superlingual tags,capture repeated multilingual patterns and thus re-duce the overall uncertainty in tagging decisions.We evaluate our model on a parallel corpus ofeight languages.
The model is trained once using all83languages, and its performance is tested separatelyfor each on a held-out monolingual test set.
When acomplete tag lexicon is provided, our unsupervisedmodel achieves an average accuracy of 95%, in com-parison to 91% for an unsupervised monolingualBayesian HMM and 97.4% for its supervised coun-terpart.
Thus, on average, the gap between unsu-pervised and supervised monolingual performanceis cut by nearly two thirds.
We also examined sce-narios where the tag lexicon is reduced in size.
Inall cases, the multilingual model yielded substantialperformance gains.
Finally, we examined the per-formance of our model when trained on all possiblesubsets of the eight languages.
We found that perfor-mance improves steadily as the number of availablelanguages increases.2 Related WorkBilingual Part-of-Speech Tagging Early work onmultilingual tagging focused on projecting annota-tions from an annotated source language to a targetlanguage (Yarowsky and Ngai, 2001; Feldman et al,2006).
In contrast, we assume no labeled data atall; our unsupervised model instead symmetricallyimproves performance for all languages by learningcross-lingual patterns in raw parallel data.
An addi-tional distinction is that projection-based work uti-lizes pairs of languages, while our approach allowsfor continuous improvement as languages are addedto the mix.In recent work, Snyder et al (2008) presenteda model for unsupervised part-of-speech taggingtrained from a bilingual parallel corpus.
This bilin-gual model and the model presented here share anumber of similarities: both are Bayesian graphi-cal models building upon hidden Markov models.However, the bilingual model explicitly joins eachaligned word-pair into a single coupled state.
Thus,the state-space of these joined nodes grows exponen-tially in the number of languages.
In addition, cross-ing alignments must be removed so that the result-ing graph structure remains acyclic.
In contrast, ourmultilingual model posits latent cross-lingual tagswithout explicitly joining or directly connecting thepart-of-speech tags across languages.
Besides per-mitting crossing alignments, this structure allows themodel to scale gracefully with the number of lan-guages.Beyond Bilingual Learning While most work onmultilingual learning focuses on bilingual analysis,some models operate on more than one pair of lan-guages.
For instance, Genzel (2005) describes amethod for inducing a multilingual lexicon froma group of related languages.
His model first in-duces bilingual models for each pair of languagesand then combines them.
Our work takes a differentapproach by simultaneously learning from all lan-guages, rather than combining bilingual results.A related thread of research is multi-source ma-chine translation (Och and Ney, 2001; Utiyama andIsahara, 2006; Cohn and Lapata, 2007) where thegoal is to translate from multiple source languages toa single target language.
Rather than jointly trainingall the languages together, these models train bilin-gual models separately, and then use their output toselect a final translation.
The selection criterion canbe learned at training time since these models haveaccess to the correct translation.
In unsupervised set-tings, however, we do not have a principled meansfor selecting among outputs of different bilingualmodels.
By developing a joint multilingual modelwe can automatically achieve performance that ri-vals that of the best bilingual pairings.3 ModelWe propose a non-parametric directed Bayesiangraphical model for multilingual part-of-speech tag-ging using a parallel corpus.
We perform a jointtraining pass over the corpus, and then apply theparameters learned for each language to a held-outmonolingual test set.The core idea of our model is that patterns ofambiguity vary across languages and therefore evenunannotated multilingual data can serve as a learn-ing signal.
Our model is able to simultaneously har-ness this signal from all languages present in thecorpus.
This goal is achieved by designing a sin-gle graphical model that connects tagging decisionswithin a language as well as across languages.The model contains language-specific HMM sub-structures connected to one another by cross-linguallatent variables spanning two or more languages.These variables, which we refer to as superlingualtags, capture repeated cross-lingual patterns and84I l o v e f i s h J ?
a d o r e l e s p o i s s o na n i o h e v d a g i m M u j h e m a c h c h l i p a s a n d h a iFigure 1: Model structure for parallel sentences in English, French, Hebrew, and Urdu.
In this example, there are threesuperlingual tags, each connected to the part-of-speech tag of a word in each of the four languages.thus reduce the overall uncertainty in tagging deci-sions.
To encourage the discovery of a compact setof such cross-lingual patterns, we place a Dirichletprocess prior on the superlingual tag values.3.1 Model StructureFor each language, our model includes an HMM-like substructure with observed word nodes, hid-den part-of-speech nodes, and directed transitionand emission edges.
For each set of aligned wordsin parallel sentences, we add a latent superlingualvariable to capture the cross-lingual context.
A setof directed edges connect this variable to the part-of-speech nodes of the aligned words.
Our modelassumes that the superlingual tags for parallel sen-tences are unordered and are drawn independentlyof one another.Edges radiate outward from superlingual tags tolanguage-specific part-of-speech nodes.
Thus, ourmodel implicitly assumes that superlingual tags aredrawn prior to the part-of-speech tags of all lan-guages and probabilistically influence their selec-tion.
See Figure 1 for an example structure.The particular model structure for each set of par-allel sentences (i.e.
the configuration of superlingualtags and their edges) is determined by bilingual lexi-cal alignments and ?
like the text itself ?
is consid-ered an observed variable.
In practice, these lexicalalignments are obtained using standard techniquesfrom machine translation.Our model design has several benefits.
Crossingand many-to-many alignments may be used with-out creating cycles in the graph, as all cross-lingualinformation emanates from the hidden superlingualtags.
Furthermore, the model scales gracefully withthe number of languages, as the number of newedges and nodes will be proportional to the numberof words for each additional language.3.2 Superlingual TagsEach superlingual tag value specifies a set of dis-tributions ?
one for each language?s part-of-speechtagset.
In order to learn repeated cross-lingual pat-terns, we need to constrain the number of superlin-gual tag values and thus the number of distributionsthey provide.
For example, we might allow the su-perlingual tags to take on integer values from 1 toK, with each integer value indexing a separate setof distributions.
Each set of distributions should cor-respond to a discovered cross-lingual pattern in thedata.
For example, one set of distributions might fa-vor nouns in each language and another might favorverbs.Rather than fixing the number of superlingualtag values to an arbitrary and predetermined size1, .
.
.
,K, we allow them to range over the entire setof integers.
In order to encourage the desired multi-lingual clustering behavior, we use a Dirichlet pro-cess prior for the superlingual tags.
This prior allowshigh posterior probability only when a small number85of values are used repeatedly.
The actual number ofsampled values will be dictated by the data and thenumber of languages.More formally, suppose we have n lan-guages, ?1, .
.
.
, ?n.
According to our genera-tive model, a countably infinite sequence of sets??
?11 , .
.
.
, ?
?n1 ?, ??
?12 , .
.
.
, ?
?n2 ?, .
.
.
is drawn fromsome base distribution.
Each ?
?i is a distributionover the parts-of-speech in language ?.In parallel, an infinite sequence of mixing compo-nents ?1, ?2, .
.
.
is drawn from a stick-breaking pro-cess (Sethuraman, 1994).
These components definea distribution over the integers with most probabil-ity mass placed on some initial set of values.
Thetwo sequences ??
?11 , .
.
.
, ?
?n1 ?, ??
?12 , .
.
.
, ?
?n2 ?
.
.
.and ?1, ?2 .
.
.
now define the distribution over su-perlingual tags and their associated distributions onparts-of-speech.
That is, each superlingual tag z ?N is drawn with probability ?z , and indexes the setof distributions ??
?1z , .
.
.
, ?
?nz ?.3.3 Part-of-Speech TagsFinally, we need to define the generative probabili-ties of the part-of-speech nodes.
For each such nodethere may be multiple incoming edges.
There willalways be an incoming transition edge from the pre-vious tag (in the same language).
In addition, theremay be incoming edges from zero or more superlin-gual tags.
Each edge carries with it a distributionover parts-of-speech and these distributions must becombined into the single distribution from which thetag is ultimately drawn.We choose to combine these distributions as aproduct of experts.
More formally: for language ?and tag position i, the part-of-speech tag yi is drawnaccording toyi ?
?yi?1(yi)?z ?
?z(yi)Z (1)Where ?yi?1 indicates the transition distribution,and the z?s range over the values of the incomingsuperlingual tags.
The normalization term Z is ob-tained by summing the numerator over all part-of-speech tags yi in the tagset.This parameterization allows for a relatively sim-ple and small parameter space.
It also leads to adesirable property: for a tag to have high probabil-ity each of the incoming distributions must allow it.That is, any expert can ?veto?
a potential tag by as-signing it low probability, generally leading to con-sensus decisions.We now formalize this description by giving thestochastic process by which the observed data (rawparallel text) is generated, according to our model.3.4 Generative ProcessFor n languages, we assume the existence of ntagsets T 1, .
.
.
, Tn and vocabularies, W 1, .
.
.
,Wn,one for each language.
For clarity, the generativeprocess is described using only bigram transitiondependencies, but our experiments use a trigrammodel.1.
Transition and Emission Parameters: Foreach language ?
and for each tag t ?
T ?, drawa transition distribution ?
?t over tags T?
andan emission distribution ?
?t over words W ?, allfrom symmetric Dirichlet priors of appropriatedimension.2.
Superlingual Tag Parameters:Draw an infinite sequence of sets??
?11 , .
.
.
, ?
?n1 ?, ??
?12 , .
.
.
, ?
?n2 ?, .
.
.
frombase distribution G0.
Each ?
?i is a distributionover the tagset T ?.
The base distribution G0 isa product of n symmetric Dirichlets, where thedimension of the ith such Dirichlet is the sizeof the corresponding tagset T ?i .At the same time, draw an infinite sequenceof mixture weights ?
?
GEM(?
), whereGEM(?)
indicates the stick-breaking distribu-tion (Sethuraman, 1994), and ?
= 1.
Theseparameters together define a prior distributionover superlingual tags,p(z) =?
?k?k?k=z, (2)or equivalently over the part-of-speech distri-butions ??
?1 , .
.
.
, ??n?
that they index:??k?k???
?1k ,...,?
?nk ?=??
?1 ,...,?
?n ?.
(3)In both cases, ?v=v?
is defined as one whenv = v?
and zero otherwise.
Distribution 3 issaid to be drawn from a Dirichlet process, con-ventionally written as DP (?,G0).863.
Data: For each multilingual parallel sentence,(a) Draw an alignment a specifying sets ofaligned indices across languages.
Eachsuch set may consist of indices in any sub-set of the languages.
We leave the distri-bution over alignments undefined, as weconsider alignments observed variables.
(b) For each set of indices in a, draw a super-lingual tag value z according to Distribu-tion 2.
(c) For each language ?, for i = 1, .
.
.
(untilend-tag reached):i.
Draw a part-of-speech tag yi ?
T ?
ac-cording to Distribution 1ii.
Draw a word wi ?
W ?
according tothe emission distribution ?yi .To perform Bayesian inference under this modelwe use a combination of sampling techniques, whichwe describe in detail in the next section.3.5 InferenceIdeally we would like to predict the part-of-speechtags which have highest marginal probability giventhe observed words x and alignments a. Morespecifically, since we are evaluating our accuracy pertag-position, we would like to predict, for languageindex ?
and word index i, the single part-of-speechtag:argmaxt?T ?P(y?i = t?
?x,a)which we can rewrite as the argmaxt?T ?
of the inte-gral,?
[P(y?i = t???y?
(?,i),?,?, z,?,x,a)?P(y?
(?,i),?,?, z,pi,?,???x,a)]dy?
(?,i) d?
d?
dz dpi d?,in which we marginalize over the settings of alltags other than y?i (written as y?
(?,i)), the tran-sition distributions ?
= {??t?
}, emission distri-butions ?
= {??t?
}, superlingual tags z, and su-perlingual tag parameters pi = {?1, ?2, .
.
.}
and?
= {??
?11 , .
.
.
, ?
?n1 ?, ??
?12 , .
.
.
, ?
?n2 ?
.
.
.}
(where t?ranges over all part-of-speech tags).As these integrals are intractable to compute ex-actly, we resort to the standard Monte Carlo approx-imation.
We collect N samples of the variables overwhich we wish to marginalize but for which we can-not compute closed-form integrals, where each sam-ple samplek is drawn from P (samplek|x,a).
Wethen approximate the tag marginals as:P(y?i = t?
?x,a) ?
?k P(y?i = t?
?samplek,x,a)N (4)We employ closed forms for integrating out theemission parameters ?, transition parameters ?, andsuperlingual tag parameters pi and ?.
We explic-itly sample only part-of-speech tags y, superlingualtags z, and the hyperparameters of the transition andemission Dirichlet priors.
To do so, we apply stan-dard Markov chain sampling techniques: a Gibbssampler for the tags and a within-Gibbs Metropolis-Hastings subroutine for the hyperparameters (Hast-ings, 1970).Our Gibbs sampler samples each part-of-speechand superlingual tag separately, conditioned on thecurrent value of all other tags.
In each case, we usestandard closed forms to integrate over all parametervalues, using currently sampled counts and hyperpa-rameter pseudo-counts.
We note that conjugacy istechnically broken by our use of a product form inDistribution 1.
Nevertheless, we consider the sam-pled tags to have been generated separately by eachof the factors involved in the numerator.
Thus ourmethod of using count-based closed forms should beviewed as an approximation.3.6 Sampling Part-of-Speech TagsTo sample the part-of-speech tag for language ?
atposition i we draw fromP (y?i |y?
(?,i),x,a, z) ?P (y?i+1|y?i ,y?
(?,i),a, z) P (y?i |y?
(?,i),a, z)?P (x?i |x??i,y?)
,where the first two terms are the generative proba-bilities of (i) the current tag given the previous tagand superlingual tags, and (ii) the next tag given thecurrent tag and superlingual tags.
These two quan-tities are similar to Distribution 1, except here weintegrate over the transition parameter ?yi?1 and thesuperlingual tag parameters ?
?z .
We end up with aproduct of integrals.
Each integral can be computedin closed form using multinomial-Dirichlet conju-gacy (and by making the above-mentioned simpli-fying assumption that all other tags were gener-ated separately by their transition and superlingual87parameters), just as in the monolingual BayesianHMM of (Goldwater and Griffiths, 2007).For example, the closed form for integrating overthe parameter of a superlingual tag with value z isgiven by:??
?z(yi)P (??z|?0)d?
?z = count(z, yi, ?)
+ ?0count(z, ?)
+ T ?
?0where count(z, yi, ?)
is the number of times that tagyi is observed together with superlingual tag z inlanguage ?, count(z, ?)
is the total number of timesthat superlingual tag z appears with an edge into lan-guage ?, and ?0 is a hyperparameter.The third term in the sampling formula is theemission probability of the current word x?i giventhe current tag and all other words and sampled tags,as well as a hyperparameter which is suppressed forthe sake of clarity.
This quantity can be computedexactly in closed form in a similar way.3.7 Sampling Superlingual TagsFor each set of aligned words in the observed align-ment a we need to sample a superlingual tag z.Recall that z is an index into an infinite sequence??
?11 , .
.
.
, ?
?n1 ?, ??
?12 , .
.
.
, ?
?n2 ?
.
.
., where each ?
?z isa distribution over the tagset T ?.
The generative dis-tribution over z is given by equation 2.
In our sam-pling scheme, however, we integrate over all possi-ble settings of the mixing components pi using thestandard Chinese Restaurant Process (CRP) closedform (Antoniak, 1974):P(zi??z?i,y)???P(y?i??z,y?(?,i))?
{1k+?count(zi) if zi ?
z?i?k+?
otherwiseThe first term is the product of closed form tag prob-abilities of the aligned words, given z.
The final termis the standard CRP closed form for posterior sam-pling from a Dirichlet process prior.
In this term,k is the total number of sampled superlingual tags,count(zi) is the total number of times the value zioccurs in the sampled tags, and ?
is the Dirichletprocess concentration parameter (see Step 2 in Sec-tion 3.4).Finally, we perform standard hyperparameter re-estimation for the parameters of the Dirichlet distri-bution priors on ?
and ?
(the transition and emis-sion distributions) using Metropolis-Hastings.
Weassume an improper uniform prior and use a Gaus-sian proposal distribution with mean set to the pre-vious value, and variance to one-tenth of the mean.4 Experimental SetupWe test our model in an unsupervised frameworkwhere only raw parallel text is available for eachof the languages.
In addition, we assume that foreach language a tag dictionary is available that cov-ers some subset of words in the text.
The task is tolearn an independent tagger for each language thatcan annotate non-parallel raw text using the learnedparameters.
All reported results are on non-parallelmonolingual test data.Data For our experiments we use the Multext-East parallel corpus (Erjavec, 2004) which has beenused before for multilingual learning (Feldman etal., 2006; Snyder et al, 2008).
The tagged portion ofthe corpus includes a 100,000 word English text, Or-well?s novel ?Nineteen Eighty Four?, and its trans-lation into seven languages: Bulgarian, Czech, Es-tonian, Hungarian, Romanian, Slovene and Serbian.The corpus also includes a tag lexicon for each ofthese languages.
We use the first 3/4 of the text forlearning and the last 1/4 as held-out non-parallel testdata.The corpus provides sentence level alignments.To obtain word level alignments, we run GIZA++(Och and Ney, 2003) on all 28 pairings of the 8 lan-guages.
Since we want each latent superlingual vari-able to span as many languages as possible, we ag-gregate the pairwise lexical alignments into largersets of aligned words.
These sets of aligned wordsare generated as a preprocessing step.
During sam-pling they remain fixed and are treated as observeddata.We use the set of 14 basic part-of-speech tags pro-vided by the corpus.
In our first experiment, weassume that a complete tag lexicon is available, sothat for each word, its set of possible parts-of-speechis known ahead of time.
In this setting, the aver-age number of possible tags per token is 1.39.
Wealso experimented with incomplete tag dictionaries,where entries are only available for words appearingmore than five or ten times in the corpus.
For otherwords, the entire tagset of 14 tags is considered.
Inthese two scenarios, the average per-token tag ambi-88Lexicon: Full Lexicon: Frequency > 5 Lexicon: Frequency > 10MONOBIMULTI MONOBIMULTI MONOBIMULTIAVG BEST AVG BEST AVG BESTBG 88.8 91.3 94.7 92.6 73.5 80.2 82.7 81.3 71.9 77.8 80.2 78.8CS 93.7 97.0 97.7 98.2 72.2 79.0 79.7 83.0 66.7 75.3 76.7 79.4EN 95.8 95.9 96.1 95.0 87.3 90.4 90.7 88.1 84.4 88.8 89.4 86.1ET 92.5 93.4 94.3 94.6 72.5 76.5 77.5 80.6 68.3 72.9 74.9 77.9HU 95.3 96.8 96.9 96.7 73.5 77.3 78.0 80.8 69.0 73.8 75.2 76.4RO 90.1 91.8 94.0 95.1 77.1 82.7 84.4 86.1 73.0 80.5 82.1 83.1SL 87.4 89.3 94.8 95.8 75.7 78.7 80.9 83.6 70.4 76.1 77.6 80.0SR 84.5 90.2 94.5 92.3 66.3 75.9 79.4 78.8 63.7 72.4 76.1 75.9Avg.
91.0 93.2 95.4 95.0 74.7 80.1 81.7 82.8 70.9 77.2 79.0 79.7Table 1: Tagging accuracy for Bulgarian, Czech, English, Estonian, Hungarian, Romanian, Slovene, and Serbian.
Inthe first scenario, a complete tag lexicon is available for all the words.
In the other two scenarios the tag lexicononly includes words that appear more than five or ten times.
Results are given for a monolingual Bayesian HMM(Goldwater and Griffiths, 2007), a bilingual model (Snyder et al, 2008), and the multilingual model presented here.In the case of the bilingual model, we present both the average accuracy over all pairings as well as the result from thebest performing pairing for each language.
The best results for each language in each scenario are given in boldface.guity is 4.65 and 5.58, respectively.Training and testing In the full lexicon ex-periment, each word is initialized with a randompart-of-speech tag from its dictionary entry.
In thetwo reduced lexicon experiments, we initialize thetags with the result of our monolingual baseline (seebelow) to reduce sampling time.
In both cases,we begin with 14 superlingual tag values ?
corre-sponding to the parts-of-speech ?
and initially as-sign them based on the most common initial part-of-speech of words in each alignment.We run our Gibbs sampler for 1,000 iterations,and store the conditional tag probabilities for the last100 iterations.
We then approximate marginal tagprobabilities on the training data using Equation 4and predict the highest probability tags.
Finally, wecompute maximum likelihood transition and emis-sion probabilities using these tag counts, and thenapply smoothed viterbi decoding to each held-outmonolingual test set.
All reported results are aver-aged over five runs of the sampler.Monolingual and bilingual baselines Wereimplemented the Bayesian HMM model of Gold-water and Griffiths (2007) (BHMM1) as our mono-lingual baseline.
It has a standard HMM structurewith conjugate Bayesian priors over transitions andemissions.
We note that our model, in the absenceof any superlingual tags, reduces to this BayesianHMM.
As an additional baseline we use a bilingualmodel (Snyder et al, 2008).
It is a directed graphicalmodel that jointly tags two parallel streams of textaligned at the word level.
The structure of the modelconsists of two parallel HMMs, one for each lan-guage.
The aligned words form joint nodes that areshared by both HMMs.
These joint nodes are sam-pled from a probability distribution that is a prod-uct of the transition and emission distributions in thetwo languages and a coupling distribution.We note that the numbers reported here forthe bilingual model differ slightly from those re-ported by Snyder et al (2008) for two reasons: weuse a slightly larger set of sentences, and an im-proved sampling scheme.
The new sampling schememarginalizes over the transition and coupling param-eters by using the same count-based approximationthat we utilize for our multilingual model.
This leadsto higher performance, and thus a stronger baseline.15 ResultsTable 1 shows the tagging accuracy of our multilin-gual model on the test data, when training is per-formed on all eight languages together.
Results fromboth baselines are also reported.
In the case of thebilingual baseline, seven pairings are possible foreach language, and the results vary by pair.
There-1Another difference is that we use the English lexicon pro-vided with the Multext-East corpus, whereas (Snyder et al,2008) augment this lexicon with tags found in WSJ.89fore, for each language, we present the average accu-racy over all seven pairings, as well as the accuracyof its highest performing pairing.We provide results for three scenarios.
In the firstcase, a tag dictionary is provided for all words, lim-iting them to a restricted set of possible tags.
In theother two scenarios, dictionary entries are limited towords that appear more than five or ten times in thecorpus.
All other words can be assigned any tag,increasing the overall difficulty of the task.
In thefull lexicon scenario, our model achieves an averagetagging accuracy of 95%, compared to 91% for themonolingual baseline and 93.2% for the bilingualbaseline when averaged over all pairings.
This ac-curacy (95%) comes close to the performance of thebilingual model when the best pairing for each lan-guage is chosen by an oracle (95.4%).
This demon-strates that our multilingual model is able to effec-tively learn from all languages.
In the two reducedlexicon scenarios, the gains are even more striking.In both cases the average multilingual performanceoutpaces even the best performing pairs.Looking at individual languages, we see that inall three scenarios, Czech, Estonian, Romanian, andSlovene show their best performance with the mul-tilingual model.
Bulgarian and Serbian, on theother hand, give somewhat better performance withtheir optimal pairings under the bilingual model, buttheir multilingual performance remains higher thantheir average bilingual results.
The performance ofEnglish under the multilingual model is somewhatlower, especially in the full lexicon scenario, whereit drops below monolingual performance.
One pos-sible explanation for this decrease lies in the fact thatEnglish, by far, has the lowest trigram tag entropy ofall eight languages (Snyder et al, 2008).
It is pos-sible, therefore, that the signal it should be gettingfrom its own transitions is being drowned out by lessreliable information from other languages.In order to test the performance of our model asthe number of languages increases, we ran the fulllexicon experiment with all possible subsets of theeight languages.
Figure 2 plots the average accuracyas the number of languages varies.
For comparison,the monolingual and average bilingual baseline re-sults are given, along with supervised monolingualperformance.
Our multilingual model steadily gainsin accuracy as the number of available languages in-Figure 2: Performance of the multilingual model as thenumber of languages varies.
Performance of the mono-lingual and average bilingual baselines as well as a su-pervised monolingual performance are given for compar-ison.creases.
Interestingly, it even outperforms the bilin-gual baseline (by a small margin) when only two lan-guages are available, which may be attributable tothe more flexible non-parametric dependencies em-ployed here.
Finally, notice that the gap betweenmonolingual supervised and unsupervised perfor-mance is cut by nearly two thirds under the unsu-pervised multilingual model.6 ConclusionIn this paper we?ve demonstrated that the benefits ofunsupervised multilingual learning increase steadilywith the number of available languages.
Our modelscales gracefully as languages are added and effec-tively incorporates information from them all, lead-ing to substantial performance gains.
In one experi-ment, we cut the gap between unsupervised and su-pervised performance by nearly two thirds.
A fu-ture challenge lies in incorporating constraints fromadditional languages even when parallel text is un-available.AcknowledgmentsThe authors acknowledge the support of the National Sci-ence Foundation (CAREER grant IIS-0448168 and grant IIS-0835445).
Thanks to Tommi Jaakkola and members of theMIT NLP group for helpful discussions.
Any opinions, find-ings, or recommendations expressed above are those of the au-thors and do not necessarily reflect the views of the NSF.90ReferencesC.
E. Antoniak.
1974.
Mixtures of Dirichlet processeswith applications to Bayesian nonparametric prob-lems.
The Annals of Statistics, 2:1152?1174, Novem-ber.Trevor Cohn and Mirella Lapata.
2007.
Machine trans-lation by triangulation: Making effective use of multi-parallel corpora.
In Proceedings of ACL.T.
Erjavec.
2004.
MULTEXT-East version 3: Multi-lingual morphosyntactic specifications, lexicons andcorpora.
In Fourth International Conference on Lan-guage Resources and Evaluation, LREC, volume 4,pages 1535?1538.Anna Feldman, Jirka Hana, and Chris Brew.
2006.A cross-language approach to rapid creation of newmorpho-syntactically annotated resources.
In Pro-ceedings of LREC, pages 549?554.Dmitriy Genzel.
2005.
Inducing a multilingual dictio-nary from a parallel multitext in related languages.
InProceedings of the HLT/EMNLP, pages 875?882.Sharon Goldwater and Thomas L. Griffiths.
2007.A fully Bayesian approach to unsupervised part-of-speech tagging.
In Proceedings of the ACL, pages744?751.W.
K. Hastings.
1970.
Monte Carlo sampling meth-ods using Markov chains and their applications.Biometrika, 57:97?109.Franz Josef Och and Hermann Ney.
2001.
Statisticalmulti-source translation.
In MT Summit 2001, pages253?258.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.J.
Sethuraman.
1994.
A constructive definition of Dirich-let priors.
Statistica Sinica, 4:639?650.Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, andRegina Barzilay.
2008.
Unsupervised multilinguallearning for POS tagging.
In Proceedings of theEMNLP, pages 1041?1050.Masao Utiyama and Hitoshi Isahara.
2006.
A com-parison of pivot methods for phrase-based statisticalmachine translation.
In Proceedings of NAACL/HLT,pages 484?491.David Yarowsky and Grace Ngai.
2001.
Inducing multi-lingual POS taggers and NP bracketers via robust pro-jection across aligned corpora.
In Proceedings of theNAACL, pages 1?8.91
