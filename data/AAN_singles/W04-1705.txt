Indexing Student Essays Paragraphs using LSA over an Integrated OntologicalSpaceGaston G.  Burek  Maria Vargas-VeraEmanuela MorealeKnowledge Media Institute,The Open UniversityMilton Keynes , UK, MK7 6AA{g.g.burek,m.vargas-vera,e.moreale}@open.ac.ukAbstractA full understanding of text is out of reach ofcurrent human language technology.
However, ashallow Natural Language Processing (NLP)approach can be used to provide automated help inthe evaluation of essays.
The main idea of thispaper is that Latent Semantic Indexing (LSA) canbe used in conjunction with ontologies and Firstorder Logic (FOL) to locate segments relevant to aquestion in a student essay.
Our test bed, in a firstinstance, is a set of ontologies such the AKTreference ontology (describing academic life),Newspaper and a Koala ontology (concerningkoalas?
habitat).1 IntroductionThis paper describes a novel methodologyaiming to support evaluators during the essaymarking process.
This approach allows measuringsemantic similarity between structured (i.e.ontology and binary relations derived from theessay question) and unstructured (i.e.
textprocessed as a bag of words) information by meansof Latent Semantic Analysis (LSA) and the cosinesimilarity measure (Deewerster et al, 1990).Previous studies (Foltz et al, 1998; Wiemer-Hastings and Graesser, 1999) have used LSA tomeasure text coherence and comprehension bycomparing units of text (i.e.
sentences, terms orparagraphs) to determine how semantically relatedthey are.
The work presented in this paper  is basedon the use of ?pseudo?
documents: these aretemporary documents containing a description ofknowledge entities extracted from availabledomain ontologies (i.e.
ontological relations).
Bothpseudo documents and paragraphs in studentessays are represented as vectors.
Essay paragraphsare indexed according to a measure of semanticsimilarity (called cosine similarity).
Theontological space acts as a mediated schema, a setof virtual relations among knowledge entitiesrelated by their degree of similarity.
A newknowledge entity can be added in this space andautomatically a similarity measure is calculated forall the entities within the space.1.1 Motivation and ContextThe main motivation for this work derives froma need for semantics in essay evaluation, whetherby a tutor or by the student author in the process ofwriting.
Page (Page, 1968) makes a usefuldistinction between marking for syntax (i.e.linguistic style) and for content (subject matter)which we will use in our outline.
Based on thisdistinction, four main approaches to essayassessment have been reported (Williams, 2001).Early systems such as PEG (Page, 1966) reliedmainly on syntactic and linguistic features andrequired a sample of the essays to be marked by anumber of human judges.
E-rater (Burstein et al,1998) uses a combination of statistical and naturallanguage processing techniques for the purpose ofextracting linguistic features of the essays to begraded.
Again, the essays are evaluated against aset of human?graded essays acting as a benchmark.In the LSA method of essay grading, an LSA spaceis constructed based on domain specific materialand the student essays.
LSA grading performanceis about as reliable as human graders (Foltz, 1996).Text categorisation (Larkey, 1998) also requires adatabase of graded essays, so that new essays canbe categorised in relation to them.In short, the approaches seen so far 1 have eitherconcentrated on syntactic and linguistic features orused domain knowledge in the form of keywordsand documents about the domain.
What we areproposing in this paper is that a further distinctionshould be made between using implicit (keywords,documents) and explicit content representations(see Fig.1, our contribution is marked in bold).
We1 Kukich presents in her article Beyond AutomatedEssay Scoring  a time line of research developments inthe field of writing evaluation (Kukich, 2000).then argue the case for adding explicit domainknowledge in the form of domain ontologies.
Inparticular, we merge ontologies, LSA and FOL.
Anadvantage of this approach is that it does notrequire a corpus of graded essays, except forvalidation.
This feature enables tutors (or studentsin need of feedback) to evaluate essays onparticular topics even when there are no pre-scoredessay examples available.
Effectively, thiscapability may reduce the overall time required toprepare a reliable evaluation scheme for a newessay question.Figure 1 - Grading Criteria for Student Essays2 LSA and the Cosine SimilarityIn the vector space model, a term-to-documentmatrix is built in which the entries are weightedfrequencies of pre-processed terms occurring in acollection of documents.
Dimension reductionmethods (such as LSA), when applied to thesemantic vector space model, improve informationretrieval, information filtering and word sensedisambiguation.
The reduction in dimensionsreduces the noise in text categorisation, reduces thecomputational complexity of cluster creation, andproduces the best statistical approximation to theoriginal vector space model.
Likelihood curvescharacterise with a quantity the level ofsignificance of the reduced model dimensions.Also, the significance of each dimension follows aZipf distribution (Li, 1992) indicating that thereduced model dimensions represent latentconcepts (Ding, 1999).
The dimensions in thereduced vector space model can be comparedmeasuring semantic similarity between each ofthem by means of the cosine similarity.
The cosineof the angle between two vectors is defined as theinner product between the vectors v and w dividedby the product of the length of the two vectors.||||.||||.wvwvCos =?3 Indexing Essays ParagraphsAn index of relations within the ontologiesrelated to the semantic space is obtained for eachbinary relation derived from the essay question.Then a subset containing the higher rankedrelations is selected and the similarity betweeneach of the relations in the subset and all thedocuments containing essays paragraphs is alsocalculated by applying LSA.
Finally, an averagesimilarity value is obtained for the paragraph overthe number of relations in the subset.3.1 An Ontology Integration Method to Buildthe Semantic SpaceA collection of ?pseudo?
documents is createdfor each of the classes within the ontologiesdescribing the domains tackled in the essay.
Theontologies are described quantitatively usingprobabilistic knowledge (Florescu et al, 1997).Each of these documents contains information(name, properties and relations) about a class.
Thedocuments are represented by a vector space model(Salton et al, 1971) where each column in theterm-to-document matrix represents the ontologicalclasses and the rows represent terms occurring inthe pseudo documents describing those knowledgeentities.Relations within the available ontologies are alsorepresented by a vector space model where thecolumns in the term?to?document matrix are acombination of two or more vectors from the term?to?document matrix representing classes.
Eachcolumn represents the relation held between thecombined classes.
A new column representing thebinary relation derived from the essay question isadded to the new matrix: this new column containsthe weighted frequencies of terms appearing asarguments within the relation.
For each essayquestion, one or more binary relations are derivedthrough parsing.
For instance: given the query ?Dokoalas live in the jungle??
the binary relation islive_in (koala, jungle).
In the case of this example,the vector representing the question contains afrequency of one in the rows corresponding to theterms koala and jungle.LSA is applied to the term?to?document matrixrepresenting the ontological relations, the vectorspace model is reduced and the cosine similarity iscalculated to obtain the semantic similaritybetween the vectors of the reduced space model.For each column, a ranking of similarity with therest of the columns will be obtained.Researcher   (APO)StandartAd(NO)Salesperson(NO)Student(KO)Parent(KO)Koala(KO)Newspaper(APO)Newspaper(NO)Researcher (APO) 1.0000 0.5160 0.5215 0.8811 0.8524 0.8536 0.5036 0.5905Standart Ad.
(NO) 0.5160 1.0000 0.9999 0.0496 -0.0078 -0.0057 0.9998 0.9959Salesperson (NO) 0.5215 0.9999 1.0000 0.0561 -0.0013 0.0007 0.9997 0.9965Student (KO) 0.8811 0.0496 0.0562 1.0000 0.9983 0.9984 0.0353 0.1388Parent (KO) 0.8524 -0.0078 -0.0014 0.9983 1.0000 0.9999 -0.0222 0.0815Koala (KO) 0.8536 -0.0057 0.0008 0.9985 0.9999 1.0000 -0.0201 0.0837Newspaper (APO) 0.5036 0.9998 0.9997 0.0353 -0.0222 -0.0201 1.0000 0.9945Newspaper (NO) 0.5905 0.9959 0.9965 0.1388 0.0815 0.0837 0.9945 1.0000Table 1 ?
Cosine similarity for classes belonging to different ontologies.3.1.1 Weighting SchemeGiven the term?to?document matrix containinga frequency f ij ,the occurrence of a term in all thepseudo documents j is weighted  to obtain matrix.The entries of the matrix are defined as,ij ij ij ja l g d= ,where,  lij is the local weight for term i in thepseudo document j, gj is the global weight for termi in the collection  and dij  is a normalisation factor.Then, as defined by Guo and Berry (Guo andBerry, 2003),( ) ( )( )222loglog 1 1logij ijjij ijp pa fn?
?= + +????
????
?,where,ijijijjfpf= ?
.4 Experiments on semantic similarityIn order to evaluate how well LSA capturessimilarity, this section will describe threepreliminary experiments for measuring semanticsimilarity between knowledge entities (i.e.
binaryrelations and classes) of three different ontologies,the Aktive Portal Ontology (APO), the KoalaOntology (KO) and the Newspaper Ontology(NO).4.1 Experiment 1The aim of this experiment is to evaluate howwell LSA captures similarity between classes thatbelong to different ontologies.
Eight classes havebeen selected randomly from within threeontologies and described in ?pseudo?
documents.The words included in each of the documentscorrespond to the names of the classes and slotsrelated to the class described.
The terms have beenstemmed and stop words deleted before applyingLSA to the term?to?document matrix built usingthe weighted frequencies of the term occurringwithin the eight documents describing the classes.Terms have been weighted according to theweighting scheme presented in section 2.1.1 withdj=1, the only difference being that termscorresponding to classes names have beenmultiplied by two.
The similarity measures for theeight classes are obtained (See Table 1) afterapplying LSA with a rank of two and the cosinesimilarity measure to the term?to?documentmatrix.The results from this experiment show that, interms of the cosine similarity measure, the class?Researcher?
appears to be very similar to the class?Student?
in a different ontology.
The same resultsalso show that the two classes ?Newspaper?belonging to two different ontologies are verysimilar to each other.4.2 Experiment 2The aim of this experiment is to evaluate theability of LSA to measure similarity between apredicate argument and different classes.
Thequery is represented as an added column in theterm?to?document matrix which already containsas columns the same documents representing theeight classes used in the first experiment.
Thecolumn representing the query argument containsonly one term corresponding to the name of one ofthe classes within the ontologies used in thisexperiment.
The frequency of this term is the entryin the added column with a frequency of onemultiplied by two as all the other termsrepresenting names of classes.
The results for thecosine similarity measure between the eight classesplus the query containing the term ?student?,?newspaper?
and ?animal?
after applying LSAwith a rank of four (see Table 2) indicate that themost similar classes for the query containing theterm ?student?
are the following classes:?Student?
from KO, ?Researcher?
from APO, and?Parent?
from KO.Argument(student)Argument(animal)Argument(newspaper) Classes0.0018 0.0000 0.0099 Researcher (APO)0.0000 0.0000 0.1403 Standart  Ad.
(NO)-0.0001 0.0000 -0.0042 Salesperson (NO)0.4473 0.0000 -0.0080 Student (KO)-0.0013 0.5563 -0.0084 Parent (KO)-0.0006 0.4374 -0.0085 Koala (KO)0.0000 0.0000 0.5127 Newspaper (APO)-0.0001 0.0000 0.8112 Newspaper (NO)1.0000 1.0000 1.0000 QueriesTable 2 ?
Semantic similarity between argumentsand classes belonging to different ontologiesFor the query containing the term ?newspaper?the results shows that the most similar classes are?Newspaper?
from APO, ?Newspaper?
from NOand ?Standard Advertising?
also from NO.Finally, for the query containing the term?animal?, the most similar classes in order ofsimilarity closeness are ?Parent?
from KO and?Koala?
also from KO.The results of this experiment indicate that LSImay be accurately used as a measure of similaritybetween a keyword representing a query predicateargument and a set of documents representingclasses that belong to a set of different availableontologies.4.3 Experiment 3The aim of this experiment is to evaluate thecosine similarity measure as a measure of semanticsimilarity between binary relations derived from aquestion or query and relations held between twoclasses.
This measure is based on the samemethodology and procedures applied to bothexperiments described above.
For this experiment,eighteen classes have been selected arbitrarily fromthe three available ontologies (see Table 3).The binary relations held among the selectedclasses are represented as documents in a term?to?document matrix that is the union of the twopseudo documents describing the related classes.Following the same procedure as in the previousexperiment, a new column representing the binaryrelation derived from a question is added to thematrix, but in this case it contains the termsdescribing the two arguments of the binaryrelation.Newspapers Ontology (NO)ID Relation Relation name Class1 Class2OBR1 Sales Person Advertisement SalespersonOBR2 Purchaser Advertisement PersonOBR3 Published in Content NewspaperOBR4 Content Newspaper ContentOBR5 Employees Organisation EmployeeOBR6 Prototype Newspaper Prot.
NewspaperAktive Portal Ontology (APO)ID Relation Relation name Class1 Class2OBR7 Has gender Researcher GenderOBR8 Has appellation Researcher AppellationOBR9 Owned by Newspaper Legal AgentOBR10 Has Size Organisation OrganisationsizeOBR11 Headed by Organisation Afiliated PersonOBR12 Organisation part of Organisation OrganisationKoala Ontology (KO)ID  Relation Relation Name Class 1 Class 2OBR13 Has gender Animal GenderOBR14 Has habitat Animal AppellationOBR15 Has children Animal AnimalTable 3 ?
Ontological Binary Relations (OBR)used in Experiment 3The cosine similarity between fifteen predicatesand the available relations after applying LSA witha rank of four (see Table 4) show that, in eight ofthe fifteen cases, the similarity value is higher forthe relations held between classes than betweenpredicate arguments.
In the rest of the cases, thesimilarity values are very close for two or morerelations including the one held between classesthat are the same as the predicate arguments.Another interesting observation is that, QuestionBinary Relation 3 (QBR3) has a cosine value moresimilar to Ontological Binary Relation 9 (OBR9),OBR3 and OBR4.
In the case of QBR5, the cosinevalue is higher when measuring similarity withOBR11 and OBR12 than, for example, the cosinevalue when measuring similarity with  OBR3 andOBR4.
Similar results were obtained for QBR6where, apart from OBR6, OBR9 has the cosinevalue closest to one.
Other similar results areobtained for QBR11 and QBR12 where OBR5 iscloser to a value of one than OBR7, OBR8 andOBR9.QBR1 QBR2 QBR3 QBR4 QBR5 QBR6 QBR7 QBR8 QBR9 QBR10 QBR11 QBR12 QBR13 QBR14 QBR15OBR1 0.3520 0.3033 0.1993 0.1993 0.2588 0.1713 -0.0007 0.0000 0.0487 -0.0030 0.0051 -0.0048 0.0000 0.0000 0.0000OBR2 0.3628 0.3286 0.2170 0.2170 0.1896 0.1864 0.0006 0.0000 0.0528 -0.0033 0.0053 -0.0053 0.0000 0.0000 0.0000OBR3 0.0900 0.0023 0.2864 0.2864 0.0002 0.2631 -0.0005 0.0000 0.0771 0.0017 0.0223 0.0027 0.0000 0.0000 0.0000OBR4 0.0900 0.0023 0.2864 0.2864 0.0002 0.2631 -0.0005 0.0000 0.0771 0.0017 0.0223 0.0027 0.0000 0.0000 0.0000OBR5 -0.0007 0.0039 -0.0013 -0.0013 0.3925 0.0000 0.0001 0.0000 -0.0006 0.0304 0.0566 0.0468 0.0000 0.0000 0.0000OBR6 -0.0003 0.0024 0.2730 0.2730 0.0003 0.3284 0.0010 0.0000 0.0880 0.0011 0.0184 0.0017 0.0000 0.0000 0.0000OBR7 0.0000 0.0032 -0.0004 -0.0004 0.0001 -0.0004 0.9572 0.3621 -0.0013 -0.0016 0.0143 -0.0012 0.0130 0.0130 0.0000OBR8 0.0000 0.0032 -0.0004 -0.0004 0.0001 -0.0004 0.9567 0.3666 -0.0014 -0.0016 0.0143 -0.0012 0.0109 0.0109 0.0000OBR9 0.0002 0.0002 0.2971 0.2971 -0.0029 0.2738 0.1477 -0.0028 0.9300 0.0147 0.0264 0.0082 -0.0002 -0.0002 0.0000OBR10 -0.0002 0.0115 0.0014 0.0014 0.0633 0.0014 0.0999 0.0458 0.3012 0.4894 0.5181 0.3599 0.0190 0.0190 0.0000OBR11 -0.0002 0.0113 0.0012 0.0012 0.0545 0.0012 0.1153 0.0454 0.2882 0.4304 0.4759 0.3161 0.0196 0.0196 0.0000OBR12 -0.0002 0.0119 0.0015 0.0015 0.0522 0.0014 0.1019 0.0478 0.3146 0.4189 0.4623 0.3061 0.0221 0.0221 0.0000OBR13 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.7312 0.0000 0.0000 0.0000 0.0000 0.0000 0.5397 0.5397 0.4910OBR14 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.7490 0.0000 0.0000 0.0000 0.0000 0.0000 0.5202 0.5202 0.4767OBR15 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.7550 0.0000 0.0000 0.0000 0.0001 0.0000 0.5594 0.5594 0.5261QBR 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000Table 4 ?Cosine similarity between the Question Binary Relations (QBR) and the Ontological BinaryRelations (OBR).The results of this experiment indicate that thepresented methodology is able to detect similaritybetween compact representations (binary relationarguments) and more expanded representationssuch as the pseudo documents representing thebinary relations within the three availableontologies.4.4 Experiments discussionWe expect that using LSA together with thecosine similarity measure, we will be able to pickup semantic similarity between the compacted andexpanded representations of the binary relation andparagraphs from student essays.
The maindifference between our approach and other essayscoring approaches (e.g.
The Intelligent EssayAssessor; Laundauer et al, 2000) where the scoresare calibrated using LSA with pre-scored essayexamples, is that our approach scores paragraphsusing LSA and the cosine similarity withontologies describing the essay domain.
Theexperiment results in the previous sections validateour view showing that the cosine similarity may beused as a reliable score for semantic similaritybetween knowledge entities belonging to differentdata sources (i.e.
terms, classes and binaryrelations).5 Conclusion and Future WorkThis paper introduces the idea of ?explicitcontent?
and its use in essay evaluation.
The maincontribution of the paper is then the idea thatontologies and First Order Logic (FOL) can beused together with LSA to locate segmentsrelevant to a question in a student essay.Our main interest is to provide help to tutors ingrading and to students for feedback purposes.
Infact, even outside the realms of grading, we believethat it will help annotate and rank paragraphs morerelevant to queries.
In our proposal, we went aboutdoing this supplementing the widely-used LSAmethod with added semantics (ontologies) andFirst Order Logic (FOL).
Our approach thereforeattempts to bridge the gap between statistical andsemantic approaches.There is clearly a lot more work needed to makethis technology work well enough for large-scaledeployment.
Further work may include avisualisation service that also provides avisualisation of annotation of segments relevant tothe current question types around the lines of thework described in (Moreale and Vargas-Vera,2003; Moreale and Vargas-Vera, 2004).6 AcknowledgementsThis work was funded by the AdvancedKnowledge Technologies (AKT) InterdisciplinaryResearch Collaboration (IRC), which is sponsoredby the UK Engineering and Physical SciencesResearch Council under grant numberGR/N157764/01.ReferencesJ.
Burstein, K. Kukich, S. Wolff, C. Lu and M.Chodorow.
1998.
Enriching Automated EssayScoring Using Discourse Marking.
Proceedingsof the Workshop on Discourse Relations andDiscourse Markers, Annual Meeting of theAssociation of Computational Linguistics,August, Montreal, Canada.C.
H. Q. Ding.
1999.
A Similarity-BasedProbability Model for Latent Semantic Indexing.Proc.
22nd ACM SIGIR Conference, p. 59?65.S.C.
Deerwester, S. T. Dumais, T. K. Landauer, G.W.
Furnas, R.A. Harshman.
1990.
Indexing byLatent Semantic Analysis.
JASIS  41(6): 391?407.D.
Florescu, D. Koller, A.
Levy.
1997.
UsingProbabilistic Information in Data Integration.Proceedings of the 23rd VLDB Conference,Athens, Greece.P.W.
Foltz,W.
Kintsch, and T.K.
Landauer.
1998.The Measurement of Textual Coherence withLatent Semantic Analysis, Discourse Processes,Vol.
25, Nos.
2?3, 1998, p. 285?308.P.W.
Foltz.
1996.
Latent semantic analysis fortext-based research.
Behavior Research Methods,Instruments and Computers, 28, 197-202.D.
Guo and M. W. Berry.
Knowledge ?EnhancedLatent Semantic Indexing.
Information Retrieval,6 (2): 225-250, 2003.K.
Kukich.
2000.
The Debate on automated essaygrading, Beyond Automated Essay Scoring.IEEE Transactions on Intelligent Systems.September/October 15 (5):27?31.L.S.
Larkey.
1998.
Automatic Essay GradingUsing Text Categorization Techniques.Proceedings of the Twenty First AnnualInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval, Melbourne, Australia, p. 90?95.T.
K. Laundauer , D. Laham and P.W.Foltz.
2000.The Debate on automated essay grading, TheIntelligent Essay Assessor.
IEEE Transactionson Intelligent Systems.
September/October 15(5):27?31.W.
Li.
1992.
Random texts exhibit Zipf's-law-likeword frequency distribution.
IEEE Transactionson Information Theory, 38(6):1842?1845.E.
Moreale and M. Vargas-Vera.
2004.
AQuestion-Answering System UsingArgumentation.
Third International MexicanConference on Artificial Intelligence (MICAI-2004), Lecture Notes in Computer Science(LNCS 2972), Springer-Verlag, p. 26?30.
ISBN3-540-21459-3.E.
Moreale and M. Vargas-Vera.
2003.
GenreAnalysis and the Automated Extraction ofArguments from Student Essays.
The SeventhInternational Computer Assisted AssessmentConference (CAA-2003).
LoughboroughUniversity, 8-9.E.B.
Page.
1968.
Analyzing Student Essays byComputer.
International Review of Education,14, 210?225.E.B.
Page.
1966.
The Imminence of GradingEssays by Computer.
Phi Delta Kappan, p. 238?243.G.
Salton, A. Wong, and C. Yang.
1971.
A VectorSpace Model for Automatic Indexing.Communications of the ACM, 18(11):613?620,1971.P.
Wiemer-Hastings and A.C Graesser.
2000.Select-a-Kibitzer: A Computer Tool that GivesMeaningful Feedback on Student Compositions.Interactive Learning Environments 2000.Vol.8,No.2, p. 49?169.
Curtin University ofTechnologyR.
Williams.
2001.
Automated essay grading: Anevaluation of four conceptual models.
In A.Herrmann and M. M. Kulski (Eds), ExpandingHorizons in Teaching and Learning.
Proceedingsof the 10th Annual Teaching Learning Forum, 7-9 February 2001.
Perth: Curtin University ofTechnology.
