Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 852?861,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsStatistical Machine Translation Improves Question Retrieval inCommunity Question Answering via Matrix FactorizationGuangyou Zhou, Fang Liu, Yang Liu, Shizhu He, and Jun ZhaoNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences95 Zhongguancun East Road, Beijing 100190, China{gyzhou,fliu,liuyang09,shizhu.he,jzhao}@nlpr.ia.ac.cnAbstractCommunity question answering (CQA)has become an increasingly popular re-search topic.
In this paper, we focus on theproblem of question retrieval.
Questionretrieval in CQA can automatically findthe most relevant and recent questions thathave been solved by other users.
However,the word ambiguity and word mismatchproblems bring about new challenges forquestion retrieval in CQA.
State-of-the-artapproaches address these issues by implic-itly expanding the queried questions withadditional words or phrases using mono-lingual translation models.
While use-ful, the effectiveness of these models ishighly dependent on the availability ofquality parallel monolingual corpora (e.g.,question-answer pairs) in the absence ofwhich they are troubled by noise issue.In this work, we propose an alternativeway to address the word ambiguity andword mismatch problems by taking advan-tage of potentially rich semantic informa-tion drawn from other languages.
Our pro-posed method employs statistical machinetranslation to improve question retrievaland enriches the question representationwith the translated words from other lan-guages via matrix factorization.
Experi-ments conducted on a real CQA data showthat our proposed approach is promising.1 IntroductionWith the development of Web 2.0, communityquestion answering (CQA) services like Yahoo!Answers,1 Baidu Zhidao2 and WkiAnswers3 haveattracted great attention from both academia andindustry (Jeon et al, 2005; Xue et al, 2008;Adamic et al, 2008; Wang et al, 2009; Cao et al,2010).
In CQA, anyone can ask and answer ques-tions on any topic, and people seeking informationare connected to those who know the answers.
Asanswers are usually explicitly provided by human,they can be helpful in answering real world ques-tions.In this paper, we focus on the task of questionretrieval.
Question retrieval in CQA can automati-cally find the most relevant and recent questions(historical questions) that have been solved byother users, and then the best answers of these his-torical questions will be used to answer the users?queried questions.
However, question retrieval ischallenging partly due to the word ambiguity andword mismatch between the queried questionsand the historical questions in the archives.
Wordambiguity often causes the retrieval models to re-trieve many historical questions that do not matchthe users?
intent.
This problem is also amplifiedby the high diversity of questions and users.
Forexample, depending on different users, the word?interest?
may refer to ?curiosity?, or ?a chargefor borrowing money?.Another challenge is word mismatch betweenthe queried questions and the historical questions.The queried questions may contain words that aredifferent from, but related to, the words in the rele-vant historical questions.
For example, if a queriedquestion contains the word ?company?
but a rele-vant historical question instead contains the word?firm?, then there is a mismatch and the historical1http://answers.yahoo.com/2http://zhidao.baidu.com/3http://wiki.answers.com/852English Chineseword ambiguityHow do I get a loan ?(w?)??(r?h?)?
(c?ng)from a bank?
??(y?nh?ng)??
(d?iku?n)?How to reach the ??(r?h?)??
(qi?nw?ng)bank of the river?
??(h?
?n)?word mismatchcompany ??(g?ngs?
)firm ??(g?ngs?
)rheum ??
(g?nm?o)catarrh ??
(g?nm?o)Table 1: Google translate: some illustrative examples.question may not be easily distinguished from anirrelevant one.Researchers have proposed the use of word-based translation models (Berger et al, 2000;Jeon et al, 2005; Xue et al, 2008; Lee et al,2008; Bernhard and Gurevych, 2009) to solvethe word mismatch problem.
As a principle ap-proach to capture semantic word relations, word-based translation models are built by using theIBM model 1 (Brown et al, 1993) and havebeen shown to outperform traditional models (e.g.,VSM, BM25, LM) for question retrieval.
Be-sides, Riezler et al (2007) and Zhou et al (2011)proposed the phrase-based translation models forquestion and answer retrieval.
The basic idea isto capture the contextual information in model-ing the translation of phrases as a whole, thusthe word ambiguity problem is somewhat allevi-ated.
However, all these existing studies in theliterature are basically monolingual approacheswhich are restricted to the use of original languageof questions.
While useful, the effectiveness ofthese models is highly dependent on the availabil-ity of quality parallel monolingual corpora (e.g.,question-answer pairs) in the absence of whichthey are troubled by noise issue.
In this work,we propose an alternative way to address the wordambiguity and word mismatch problems by takingadvantage of potentially rich semantic informationdrawn from other languages.
Through other lan-guages, various ways of adding semantic informa-tion to a question could be available, thereby lead-ing to potentially more improvements than usingthe original language only.Taking a step toward using other languages, wepropose the use of translated representation by al-ternatively enriching the original questions withthe words from other languages.
The idea of im-proving question retrieval with statistical machinetranslation is based on the following two observa-tions: (1) Contextual information is exploited dur-ing the translation from one language to another.For example in Table 1, English words ?interest?and ?bank?
that have multiple meanings underdifferent contexts are correctly addressed by us-ing the state-of-the-art translation tool ?
?GoogleTranslate.4 Thus, word ambiguity based on con-textual information is naturally involved whenquestions are translated.
(2) Multiple words thathave similar meanings in one language may betranslated into an unique word or a few words in aforeign language.
For example in Table 1, Englishwords such as ?company?
and ?firm?
are trans-lated into ???
(g?ngs?
)?, ?rheum?
and ?catarrh?are translated into ???(g?nm?o)?
in Chinese.Thus, word mismatch problem can be somewhatalleviated by using other languages.Although Zhou et al (2012) exploited bilin-gual translation for question retrieval and obtainedthe better performance than traditional monolin-gual translation models.
However, there are twoproblems with this enrichment: (1) enrichingthe original questions with the translated wordsfrom other languages increases the dimensionalityand makes the question representation even moresparse; (2) statistical machine translation may in-troduce noise, which can harm the performance ofquestion retrieval.
To solve these two problems,we propose to leverage statistical machine transla-tion to improve question retrieval via matrix fac-torization.The remainder of this paper is organized as fol-lows.
Section 2 describes the proposed methodby leveraging statistical machine translation to im-prove question retrieval via matrix factorization.Section 3 presents the experimental results.
In sec-tion 4, we conclude with ideas for future research.4http://translate.google.com/translate t8532 Our Approach2.1 Problem StatementThis paper aims to leverage statistical machinetranslation to enrich the question representation.In order to address the word ambiguity and wordmismatch problems, we expand a question byadding its translation counterparts.
Statistical ma-chine translation (e.g., Google Translate) can uti-lize contextual information during the questiontranslation, so it can solve the word ambiguity andword mismatch problems to some extent.Let L = {l1, l2, .
.
.
, lP } denote the languageset, where P is the number of languages con-sidered in the paper, l1 denotes the original lan-guage (e.g., English) while l2 to lP are the for-eign languages.
Let D1 = {d(1)1 , d(1)2 , .
.
.
, d(1)N }be the set of historical question collection in origi-nal language, where N is the number of historicalquestions in D1 with vocabulary size M1.
Nowwe first translate each original historical questionfrom language l1 into other languages lp (p ?
[2, P ]) by Google Translate.
Thus, we can ob-tain D2, .
.
.
, DP in different languages, and Mp isthe vocabulary size of Dp.
A question d(p)i in Dpis simply represented as a Mp dimensional vectord(p)i , in which each entry is calculated by tf-idf.The N historical questions in Dp are then repre-sented in a Mp ?
N term-question matrix Dp ={d(p)1 ,d(p)2 , .
.
.
,d(p)N }, in which each row corre-sponds to a term and each column corresponds toa question.Intuitively, we can enrich the original ques-tion representation by adding the translated wordsfrom language l2 to lP , the original vocabu-lary size is increased from M1 to ?Pp=1 Mp.Thus, the term-question matrix becomes D ={D1,D2, .
.
.
,DP } and D ?
R(?Pp=1 Mp)?N .However, there are two problems with this enrich-ment: (1) enriching the original questions with thetranslated words from other languages makes thequestion representation even more sparse; (2) sta-tistical machine translation may introduce noise.5To solve these two problems, we propose toleverage statistical machine translation to improvequestion retrieval via matrix factorization.
Figure1 presents the framework of our proposed method,where qi represents a queried question, and qi is avector representation of qi.5Statistical machine translation quality is far from satis-factory in real applications.???????
?HistoricalQuestionCollectionRepresentationQueryRepresentationFigure 1: Framework of our proposed approachfor question retrieval.2.2 Model FormulationTo tackle the data sparseness of question represen-tation with the translated words, we hope to findtwo or more lower dimensional matrices whoseproduct provides a good approximate to the orig-inal one via matrix factorization.
Previous stud-ies have shown that there is psychological andphysiological evidence for parts-based representa-tion in the human brain (Wachsmuth et al, 1994).The non-negative matrix factorization (NMF) isproposed to learn the parts of objects like textdocuments (Lee and Seung, 2001).
NMF aimsto find two non-negative matrices whose productprovides a good approximation to the original ma-trix and has been shown to be superior to SVD indocument clustering (Xu et al, 2003; Tang et al,2012).In this paper, NMF is used to induce the reducedrepresentation Vp of Dp, Dp is independent on{D1,D2, .
.
.
,Dp?1,Dp+1, .
.
.
,DP }.
When ig-noring the coupling between Vp, it can be solvedby minimizing the objective function as follows:O1(Up,Vp) = minUp?0,Vp?0 ?Dp ?UpVp?2F (1)where ?
?
?F denotes Frobenius norm of a matrix.Matrices Up ?
RMp?K and Vp ?
RK?N are thereduced representation for terms and questions inthe K dimensional space, respectively.To reduce the noise introduced by statistical ma-chine translation, we assume that Vp from lan-guage Dp (p ?
[2, P ]) should be close to V1854from the original language D1.
Based on this as-sumption, we minimize the distance between Vp(p ?
[2, P ]) and V1 as follows:O2(Vp) = minVp?0P?p=2?Vp ?V1?2F (2)Combining equations (1) and (2), we get the fol-lowing objective function:O(U1, .
.
.
,UP ;V1, .
.
.
,VP ) (3)=P?p=1?Dp ?UpVp?2F +P?p=2?p?Vp ?V1?2Fwhere parameter ?p (p ?
[2, P ]) is used to adjustthe relative importance of these two components.If we set a small value for ?p, the objective func-tion behaves like the traditional NMF and the im-portance of data sparseness is emphasized; while abig value of ?p indicatesVp should be very closedto V1, and equation (3) aims to remove the noiseintroduced by statistical machine translation.By solving the optimization problem in equa-tion (4), we can get the reduced representation ofterms and questions.minO(U1, .
.
.
,UP ;V1, .
.
.
,VP ) (4)subject to : Up ?
0,Vp ?
0, p ?
[1, P ]2.3 OptimizationThe objective function O defined in equation (4)performs data sparseness and noise removing si-multaneously.
There are 2P coupling componentsin O, and O is not convex in both U and V to-gether.
Therefore it is unrealistic to expect an al-gorithm to find the global minima.
In the follow-ing, we introduce an iterative algorithm which canachieve local minima.
In our optimization frame-work, we optimize the objective function in equa-tion (4) by alternatively minimizing each compo-nent when the remaining 2P ?
1 components arefixed.
This procedure is summarized in Algorithm1.2.3.1 Update of MatrixUpHolding V1, .
.
.
,VP and U1, .
.
.
,Up?1,Up+1,.
.
.
,UP fixed, the update of Up amounts to thefollowing optimization problem:minUp?0?Dp ?UpVp?2F (5)Algorithm 1 Optimization frameworkInput: Dp ?
Rmp?N , p ?
[1, P ]1: for p = 1 : P do2: V(0)p ?
RK?N ?
random matrix3: for t = 1 : T do  T is iteration times4: U(t)p ?
UpdateU(Dp,V(t?1)p )5: V(t)p ?
UpdateV(Dp,U(t)p )6: end for7: returnU(T )p , V(T )p8: end forAlgorithm 2 Update UpInput: Dp ?
RMp?N , Vp ?
RK?N1: for i = 1 : Mp do2: u?
(p)?i = (VpVTp )?1Vpd?
(p)i3: end for4: returnUpLet d?
(p)i = (d(p)i1 , .
.
.
, d(p)iK )T and u?
(p)i =(u(p)i1 , .
.
.
, u(p)iK )T be the column vectors whose en-tries are those of the ith row of Dp and Up re-spectively.
Thus, the optimization of equation (5)can be decomposed into Mp optimization prob-lems that can be solved independently, with eachcorresponding to one row of Up:minu?
(p)i ?0?d?
(p)i ?VTp u?
(p)i ?22 (6)for i = 1, .
.
.
,Mp.Equation (6) is a standard least squares prob-lems in statistics and the solution is:u?
(p)?i = (VpVTp )?1Vpd?
(p)i (7)Algorithm 2 shows the procedure.2.3.2 Update of MatrixVpHolding U1, .
.
.
,UP and V1, .
.
.
,Vp?1,Vp+1,.
.
.
,VP fixed, the update of Vp amounts to theoptimization problem divided into two categories.if p ?
[2, P ], the objective function can be writ-ten as:minVp?0?Dp ?UpVp?2F + ?p?Vp ?V1?2F (8)if p = 1, the objective function can be writtenas:minVp?0?Dp ?UpVp?2F + ?p?Vp?2F (9)855Let d(p)j be the jth column vector of Dp, andv(p)j be the jth column vector of Vp, respectively.Thus, equation (8) can be rewritten as:min{v(p)j ?0}N?j=1?d(p)j ?Upv(p)j ?22+N?j=1?p?v(p)j ?v(1)j ?22(10)which can be decomposed into N optimizationproblems that can be solved independently, witheach corresponding to one column of Vp:minv(p)j ?0?d(p)j ?Upv(p)j ?22+?p?v(p)j ?v(1)j ?22 (11)for j = 1, .
.
.
, N .Equation (12) is a least square problem with L2norm regularization.
Now we rewrite the objectivefunction in equation (12) asL(v(p)j ) = ?d(p)j ?Upv(p)j ?22 + ?p?vpj ?
v(1)j ?22(12)where L(v(1)j ) is convex, and hence has a uniquesolution.
Taking derivatives, we obtain:?L(v(p)j )?v(p)j= ?2UTp (d(p)j ?Upv(p)j )+2?p(v(p)j ?v(1)j )(13)Forcing the partial derivative to be zero leads tov(p)?j = (UTpUp + ?pI)?1(UTp d(p)j + ?pv(1)j )(14)where p ?
[2, P ] denotes the foreign language rep-resentation.Similarly, the solution of equation (9) is:v(p)?j = (UTpUp + ?pI)?1UTp d(p)j (15)where p = 1 denotes the original language repre-sentation.Algorithm 3 shows the procedure.2.4 Time Complexity AnalysisIn this subsection, we discuss the time complex-ity of our proposed method.
The optimizationu?
(p)i using Algorithm 2 should calculate VpVTpand Vpd?
(p)i , which takes O(NK2 + NK) op-erations.
Therefore, the optimization Up takesO(NK2 + MpNK) operations.
Similarly, thetime complexity of optimization Vi using Algo-rithm 3 is O(MpK2 + MpNK).Another time complexity is the iteration timesT used in Algorithm 1 and the total number ofAlgorithm 3 Update VpInput: Dp ?
RMp?N , Up ?
RMp?K1: ??
(UTpUp + ?pI)?12: ??
UTpDp3: if p = 1 then4: for j = 1 : N do5: v(p)j ?
?
?j , ?j is the jth column of ?6: end for7: end if8: returnV19: if p ?
[2, P ] then10: for j = 1 : N do11: v(p)j ?
?
(?j + ?pv(1)j )12: end for13: end if14: returnVplanguages P , the overall time complexity of ourproposed method is:P?p=1T ?O(NK2 + MpK2 + 2MpNK) (16)For each language Dp, the size of vocabularyMp is almost constant as the number of questionsincreases.
Besides, K ?
min(Mp, N), theoreti-cally, the computational time is almost linear withthe number of questions N and the number of lan-guages P considered in the paper.
Thus, the pro-posed method can be easily adapted to the large-scale information retrieval task.2.5 Relevance RankingThe advantage of incorporating statistical machinetranslation in relevance ranking is to reduce ?wordambiguity?
and ?word mismatch?
problems.
Todo so, given a queried question q and a historicalquestion d from Yahoo!
Answers, we first trans-late q and d into other foreign languages (e.g., Chi-nese, French etc.)
and get the corresponding trans-lated representation qi and di (i ?
[2, P ]), whereP is the number of languages considered in the pa-per.
For queried question q = q1, we represent itin the reduced space:vq1 = argminv?0 ?q1 ?U1v?22 + ?1?v?22 (17)where vector q1 is the tf-idf representation ofqueried question q1 in the term space.
Similarly,for historical question d = d1 (and its tf-idf repre-sentation d1 in the term space) we represent it inthe reduced space as vd1 .856The relevance score between the queried ques-tion q1 and the historical question d1 in the re-duced space is, then, calculated as the cosine sim-ilarity between vq1 and vd1 :s(q1, d1) =< vq1 ,vd1 >?vq1?2 ?
?vd1?2(18)For translated representation qi (i ?
[2, P ]), wealso represent it in the reduced space:vqi = argminv?0 ?qi?Uiv?22+?i?v?vq1?22 (19)where vector qi is the tf-idf representation of qiin the term space.
Similarly, for translated rep-resentation di (and its tf-idf representation di inthe term space) we also represent it in the reducedspace as vdi .
The relevance score s(qi, di) be-tween qi and di in the reduced space can be cal-culated as the cosine similarity between vqi andvdi .Finally, we consider learning a relevance func-tion of the following general, linear form:Score(q, d) = ?T ??
(q, d) (20)where feature vector ?
(q, d) =(sV SM (q, d), s(q1, d1), s(q2, d2), .
.
.
, s(qP , dP )),and ?
is the corresponding weight vector, weoptimize this parameter for our evaluation metricsdirectly using the Powell Search algorithm (Paulet al, 1992) via cross-validation.
sV SM (q, d) isthe relevance score in the term space and can becalculated using Vector Space Model (VSM).3 Experiments3.1 Data Set and Evaluation MetricsWe collect the data set from Yahoo!
Answers anduse the getByCategory function provided in Ya-hoo!
Answers API6 to obtain CQA threads fromthe Yahoo!
site.
More specifically, we utilizethe resolved questions and the resulting questionrepository that we use for question retrieval con-tains 2,288,607 questions.
Each resolved ques-tion consists of four parts: ?question title?, ?ques-tion description?, ?question answers?
and ?ques-tion category?.
For question retrieval, we only usethe ?question title?
part.
It is assumed that ques-tion title already provides enough semantic infor-mation for understanding the users?
informationneeds (Duan et al, 2008).
There are 26 categories6http://developer.yahoo.com/answersCategory #Size Category # SizeArts & Humanities 86,744 Home & Garden 35,029Business & Finance 105,453 Beauty & Style 37,350Cars & Transportation 145,515 Pet 54,158Education & Reference 80,782 Travel 305,283Entertainment & Music 152,769 Health 132,716Family & Relationships 34,743 Sports 214,317Politics & Government 59,787 Social Science 46,415Pregnancy & Parenting 43,103 Ding out 46,933Science & Mathematics 89,856 Food & Drink 45,055Computers & Internet 90,546 News & Events 20,300Games & Recreation 53,458 Environment 21,276Consumer Electronics 90,553 Local Businesses 51,551Society & Culture 94,470 Yahoo!
Products 150,445Table 2: Number of questions in each first-levelcategory.at the first level and 1,262 categories at the leaflevel.
Each question belongs to a unique leaf cat-egory.
Table 2 shows the distribution across first-level categories of the questions in the archives.We use the same test set in previous work (Caoet al, 2009; Cao et al, 2010).
This set contains252 queried questions and can be freely down-loaded for research communities.7The original language of the above data set isEnglish (l1) and then they are translated into fourother languages (Chinese (l2), French (l3), Ger-man (l4), Italian (l5)), thus the number of languageconsidered is P = 5) by using the state-of-the-arttranslation tool ?
?Google Translate.Evaluation Metrics: We evaluate the perfor-mance of question retrieval using the followingmetrics: Mean Average Precision (MAP) andPrecision@N (P@N).
MAP rewards methods thatreturn relevant questions early and also rewardscorrect ranking of the results.
P@N reports thefraction of the top-N questions retrieved that arerelevant.
We perform a significant test, i.e., a t-test with a default significant level of 0.05.We tune the parameters on a small developmentset of 50 questions.
This development set is alsoextracted from Yahoo!
Answers, and it is not in-cluded in the test set.
For parameter K, we do anexperiment on the development set to determinethe optimal values among 50, 100, 150, ?
?
?
, 300 interms of MAP.
Finally, we set K = 100 in the ex-periments empirically as this setting yields the bestperformance.
For parameter ?1, we set ?1 = 1empirically, while for parameter ?i (i ?
[2, P ]),we set ?i = 0.25 empirically and ensure that?i ?i = 1.7http://homepages.inf.ed.ac.uk/gcong/qa/857# Methods MAP P@101 VSM 0.242 0.2262 LM 0.385 0.2423 Jeon et al (2005) 0.405 0.2474 Xue et al (2008) 0.436 0.2615 Zhou et al (2011) 0.452 0.2686 Singh (2012) 0.450 0.2677 Zhou et al (2012) 0.483 0.2758 SMT + MF (P = 2, l1, l2) 0.527 0.2849 SMT + MF (P = 5) 0.564 0.291Table 3: Comparison with different methods forquestion retrieval.3.2 Question Retrieval ResultsTable 3 presents the main retrieval performance.Row 1 and row 2 are two baseline systems, whichmodel the relevance score using VSM (Cao et al,2010) and language model (LM) (Zhai and Laf-ferty, 2001; Cao et al, 2010) in the term space.Row 3 and row 6 are monolingual translation mod-els to address the word mismatch problem andobtain the state-of-the-art performance in previ-ous work.
Row 3 is the word-based translationmodel (Jeon et al, 2005), and row 4 is the word-based translation language model, which linearlycombines the word-based translation model andlanguage model into a unified framework (Xue etal., 2008).
Row 5 is the phrase-based translationmodel, which translates a sequence of words aswhole (Zhou et al, 2011).
Row 6 is the entity-based translation model, which extends the word-based translation model and explores strategies tolearn the translation probabilities between wordsand the concepts using the CQA archives and apopular entity catalog (Singh, 2012).
Row 7 isthe bilingual translation model, which translatesthe English questions from Yahoo!
Answers intoChinese questions using Google Translate and ex-pands the English words with the translated Chi-nese words (Zhou et al, 2012).
For these previ-ous work, we use the same parameter settings inthe original papers.
Row 8 and row 9 are our pro-posed method, which leverages statistical machinetranslation to improve question retrieval via ma-trix factorization.
In row 8, we only consider twolanguages (English and Chinese) and translate En-glish questions into Chinese using Google Trans-late in order to compare with Zhou et al (2012).In row 9, we translate English questions into otherfour languages.
There are some clear trends in theresult of Table 3:(1) Monolingual translation models signifi-cantly outperform the VSM and LM (row 1 androw 2 vs. row 3, row 4, row 5 and row 6).
(2) Taking advantage of potentially rich seman-tic information drawn from other languages viastatistical machine translation, question retrievalperformance can be significantly improved (row 3,row 4, row 5 and row 6 vs. row 7, row 8 and row 9,all these comparisons are statistically significant atp < 0.05).
(3) Our proposed method (leveraging statisti-cal machine translation via matrix factorization,SMT + MF) significantly outperforms the bilin-gual translation model of Zhou et al (2012) (row7 vs. row 8, the comparison is statistically signifi-cant at p < 0.05).
The reason is that matrix factor-ization used in the paper can effectively solve thedata sparseness and noise introduced by the ma-chine translator simultaneously.
(4) When considering more languages, ques-tion retrieval performance can be further improved(row 8 vs. row 9).Note that Wang et al (2009) also addressed theword mismatch problem for question retrieval byusing syntactic tree matching.
We do not comparewith Wang et al (2009) in Table 3 because pre-vious work (Ming et al, 2010) demonstrated thatword-based translation language model (Xue etal., 2008) obtained the superior performance thanthe syntactic tree matching (Wang et al, 2009).Besides, some other studies attempt to improvequestion retrieval with category information (Caoet al, 2009; Cao et al, 2010), label ranking (Li etal., 2011) or world knowledge (Zhou et al, 2012).However, their methods are orthogonal to ours,and we suspect that combining the category infor-mation or label ranking into our proposed methodmight get even better performance.
We leave it forfuture research.3.3 Impact of the Matrix FactorizationOur proposed method (SMT +MF) can effectivelysolve the data sparseness and noise via matrix fac-torization.
To further investigate the impact ofthe matrix factorization, one intuitive way is toexpand the original questions with the translatedwords from other four languages, without consid-ering the data sparseness and noise introduced bymachine translator.
We compare our SMT + MFwith this intuitive enriching method (SMT + IEM).Besides, we also employ our proposed matrix fac-torization to the original question representation(VSM + MF).
Table 4 shows the comparison.858# Methods MAP P@101 VSM 0.242 0.2262 VSM + MF 0.411 0.2533 SMT + IEM (P = 5) 0.495 0.2804 SMT + MF (P = 5) 0.564 0.291Table 4: The impact of matrix factorization.
(1) Our proposed matrix factorization can sig-nificantly improve the performance of question re-trieval (row 1 vs. row2; row3 vs. row4, theimprovements are statistically significant at p <0.05).
The results indicate that our proposed ma-trix factorization can effectively address the issuesof data spareness and noise introduced by statisti-cal machine translation.
(2) Compared to the relative improvements ofrow 3 and row 4, the relative improvements of row1 and row 2 is much larger.
The reason may bethat although matrix factorization can be used toreduce dimension, it may impair the meaningfulterms.
(3) Compared to VSM, the performance ofSMT + IEM is significantly improved (row 1vs.
row 3), which supports the motivation thatthe word ambiguity and word mismatch problemscould be partially addressed by Google Translate.3.4 Impact of the Translation LanguageOne of the success of this paper is to take ad-vantage of potentially rich semantic informationdrawn from other languages to solve the word am-biguity and word mismatch problems.
So we con-struct a dummy translator (DT) that translates anEnglish word to itself.
Thus, through this trans-lation, we do not add any semantic informationinto the original questions.
The comparison is pre-sented in Table 5.
Row 1 (DT + MF) representsintegrating two copies of English questions withour proposed matrix factorization.
From Table 5,we have several different findings:(1) Taking advantage of potentially rich seman-tic information drawn from other languages cansignificantly improve the performance of questionretrieval (row 1 vs. row 2, row 3, row 4 and row 5,the improvements relative to DT + MF are statisti-cally significant at p < 0.05).
(2) Different languages contribute unevenly forquestion retrieval (e.g., row 2 vs. row 3).
Thereason may be that the improvements of leverag-ing different other languages depend on the qual-ity of machine translation.
For example, row 3# Methods MAP1 DT + MF (l1, l1) 0.3522 SMT + MF (P = 2, l1, l2) 0.5273 SMT + MF (P = 2, l1, l3) 0.5534 SMT + MF (P = 2, l1, l4) 0.5365 SMT + MF (P = 2, l1, l5) 0.5456 SMT + MF (P = 3, l1, l2, l3) 0.5597 SMT + MF (P = 4, l1, l2, l3, l4) 0.5638 SMT + MF (P = 5, l1, l2, l3, l4, l5) 0.564Table 5: The impact of translation language.Method Translation MAPSMT + MF (P = 2, l1, l2) Dict 0.468GTrans 0.527Table 6: Impact of the contextual information.is better than row 2 because the translation qual-ity of English-French is much better than English-Chinese.
(3) Using much more languages does not seemto produce significantly better performance (row 6and row 7 vs. row 8).
The reason may be that in-consistency between different languages may existdue to statistical machine translation.3.5 Impact of the Contextual InformationIn this paper, we translate the English questionsinto other four languages using Google Translate(GTrans), which takes into account contextual in-formation during translation.
If we translate aquestion word by word, it discards the contextualinformation.
We would expect that such a transla-tion would not be able to solve the word ambiguityproblem.To investigate the impact of contextual infor-mation for question retrieval, we only considertwo languages and translate English questionsinto Chinese using an English to Chinese lexicon(Dict) in StarDict8.
Table 6 shows the experi-mental results, we can see that the performance isdegraded when the contextual information is notconsidered for the translation of questions.
Thereason is that GTrans is context-dependent andthus produces different translated Chinese wordsdepending on the context of an English word.Therefore, the word ambiguity problem can besolved during the English-Chinese translation.4 Conclusions and Future WorkIn this paper, we propose to employ statistical ma-chine translation to improve question retrieval and8StarDict is an open source dictionary software, availableat http://stardict.sourceforge.net/.859enrich the question representation with the trans-lated words from other languages via matrix fac-torization.
Experiments conducted on a real CQAdata show some promising findings: (1) the pro-posed method significantly outperforms the pre-vious work for question retrieval; (2) the pro-posed matrix factorization can significantly im-prove the performance of question retrieval, nomatter whether considering the translation lan-guages or not; (3) considering more languages canfurther improve the performance but it does notseem to produce significantly better performance;(4) different languages contribute unevenly forquestion retrieval; (5) our proposed method canbe easily adapted to the large-scale information re-trieval task.As future work, we plan to incorporate the ques-tion structure (e.g., question topic and question fo-cus (Duan et al, 2008)) into the question represen-tation for question retrieval.
We also want to fur-ther investigate the use of the proposed method forother kinds of data set, such as categorized ques-tions from forum sites and FAQ sites.AcknowledgmentsThis work was supported by the National NaturalScience Foundation of China (No.
61070106, No.61272332 and No.
61202329), the National HighTechnology Development 863 Program of China(No.
2012AA011102), the National Basic Re-search Program of China (No.
2012CB316300),We thank the anonymous reviewers for their in-sightful comments.
We also thank Dr. Gao Congfor providing the data set and Dr. Li Cai for somediscussion.ReferencesL.
Adamic, J. Zhang, E. Bakshy, and M. Ackerman.2008.
Knowledge sharing and yahoo answers: ev-eryone knows and something.
In Proceedings ofWWW.A.
Berger, R. Caruana, D. Cohn, D. Freitag, and V.Mit-tal.
2000.
Bridging the lexical chasm: statistical ap-proach to answer-finding.
In Proceedings of SIGIR,pages 192-199.D.
Bernhard and I. Gurevych.
2009.
Combininglexical semantic resources with question & answerarchives for translation-based answer finding.
InProceedings of ACL, pages 728-736.P.
F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.Mercer.
1993.
The mathematics of statistical ma-chine translation: parameter estimation.
Computa-tional Linguistics, 19(2):263-311.X.
Cao, G. Cong, B. Cui, C. Jensen, and C. Zhang.2009.
The use of categorization information in lan-guage models for question retrieval.
In Proceedingsof CIKM, pages 265-274.X.
Cao, G. Cong, B. Cui, and C. Jensen.
2010.
Ageneralized framework of exploring category infor-mation for question retrieval in community questionanswer archives.
In Proceedings of WWW, pages201-210.H.
Duan, Y. Cao, C. Y. Lin, and Y. Yu.
2008.
Searchingquestions by identifying questions topics and ques-tion focus.
In Proceedings of ACL, pages 156-164.C.
L. Lawson and R. J. Hanson.
1974.
Solving leastsquares problems.
Prentice-Hall.J.
-T. Lee, S. -B. Kim, Y.
-I.
Song, and H. -C. Rim.2008.
Bridging lexical gaps between queries andquestions on large online Q&A collections withcompact translation models.
In Proceedings ofEMNLP, pages 410-418.W.
Wang, B. Li, and I.
King.
2011.
Improving ques-tion retrieval in community question answering withlabel ranking.
In Proceedings of IJCNN, pages 349-356.D.
D. Lee and H. S. Seung.
2001.
Algorithms fornon-negative matrix factorization.
In Proceedingsof NIPS.Z.
Ming, K. Wang, and T. -S. Chua.
2010.
Prototypehierarchy based clustering for the categorization andnavigation of web collections.
In Proceedings of SI-GIR, pages 2-9.J.
Jeon, W. Croft, and J. Lee.
2005.
Finding similarquestions in large question and answer archives.
InProceedings of CIKM, pages 84-90.C.
Paige and M. Saunders.
1982.
LSQR: an algo-rithm for sparse linear equations and sparse leastsquares.
ACM Transaction on Mathematical Soft-ware, 8(1):43-71.W.
H. Press, S. A. Teukolsky, W. T. Vetterling, and B.P.
Flannery.
1992.
Numerical Recipes In C. Cam-bridge Univ.
Press.S.
Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal,and Y. Liu.
2007.
Statistical machine translation forquery expansion in answer retrieval.
In Proceedingsof ACL, pages 464-471.A.
Singh.
2012.
Entity based q&a retrieval.
In Pro-ceedings of EMNLP-CoNLL, pages 1266-1277.J.
Tang, X. Wang, H. Gao, X. Hu, and H. Liu.
2012.Enriching short text representation in microblog forclustering.
Front.
Comput., 6(1):88-101.860E.
Wachsmuth, M. W. Oram, and D. I. Perrett.
1994.Recognition of objects and their component parts:responses of single units in the temporal cortex ofteh macaque.
Cerebral Cortex, 4:509-522.K.
Wang, Z. Ming, and T-S. Chua.
2009.
A syntac-tic tree matching approach to find similar questionsin community-based qa services.
In Proceedings ofSIGIR, pages 187-194.B.
Wang, X. Wang, C. Sun, B. Liu, and L. Sun.
2010.Modeling semantic relevance for question-answerpairs in web social communities.
In Proceedings ofACL, pages 1230-1238.W.
Xu, X. Liu, and Y. Gong.
2003.
Document cluster-ing based on non-negative matrix factorization.
InProceedings of SIGIR, pages 267-273.X.
Xue, J. Jeon, and W. B. Croft.
2008.
Retrieval mod-els for question and answer archives.
In Proceedingsof SIGIR, pages 475-482.C.
Zhai and J. Lafferty.
2001.
A study of smooth meth-ods for language models applied to ad hoc informa-tion retrieval.
In Proceedings of SIGIR, pages 334-342.G.
Zhou, L. Cai, J. Zhao, and K. Liu.
2011.
Phrase-based translation model for question retrieval incommunity question answer archives.
In Proceed-ings of ACL, pages 653-662.G.
Zhou, K. Liu, and J. Zhao.
2012.
Exploiting bilin-gual translation for question retrieval in community-based question answering.
In Proceedings of COL-ING, pages 3153-3170.G.
Zhou, Y. Liu, F. Liu, D. Zeng, and J. Zhao.
2013.Improving Question Retrieval in Community Ques-tion Answering Using World Knowledge.
In Pro-ceedings of IJCAI.861
