Proceedings of the 1st Workshop on Speech and Multimodal Interaction in Assistive Environments, pages 1?7,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsMultimodal Human-Machine Interactionfor Service Robots in Home-Care EnvironmentsStefan Goetze1, Sven Fischer1, Niko Moritz1, Jens-E. Appell1, Frank Wallhoff 1,21Fraunhofer Institute for Digital Media Technology (IDMT), Project groupHearing, Speech and Audio Technology (HSA), 26129 Oldenburg, Germany2Jade University of Applied Sciences, 26129 Oldenburg, Germany{s.goetze,sven.fischer,niko.moritz,jens.appell,frank.wallhoff}@idmt.fraunhofer.deAbstractThis contribution focuses on multimodal in-teraction techniques for a mobile communi-cation and assistance system on a robot plat-form.
The system comprises of acoustic, vi-sual and haptic input modalities.
Feedback isgiven to the user by a graphical user interfaceand a speech synthesis system.
By this, mul-timodal and natural communication with therobot system is possible.1 IntroductionThe amount of older people in modern societiesconstantly grows due to demographic changes (Eu-ropean Commision Staff, 2007; Statistical FederalOffice of Germany, 2008).
These people desireto stay in their own homes as long as possible,however suffer from first health problems, such asdecreased physical strength, cognitive decline (Pe-tersen, 2004), visual and hearing impairments (Rud-berg et al, 1993; Uimonen et al, 1999; Goetze etal., 2010b).
This poses great challenges to the caresystems since care services require a high amount oftemporal and personnel efforts.
Furthermore, olderpeople living alone may suffer from social isolationsince family members, friends and acquaintancesmay live at distant places and frequent face-to-facecommunication may be hard to realize.It is nowadays commonly accepted that supportby means of technical systems in the care sectorwill be inevitable in the future to cope with thesechallenges (Alliance, 2009).
Examples for such as-sistive devices are reminder systems (Boll et al,2010), medical assistance and tele-healthcare sys-tems (Lisetti et al, 2003), personal emergency re-sponse systems, accessible human-machine interac-tion (Rennies et al, 2011) or social robotics (Chewet al, 2010).This contribution describes the human-machineinteraction modalities for a social robot calledALIAS (adaptable ambient living assistant) that isdepicted in Figure 1.
ALIAS is a mobile robot plat-form to foster communication and social interactionbetween the user and his/her social network as wellas between the user and the robot platform.
The aimof ALIAS is to ensure the maintenance of existingcontacts to prevent social isolation instead of makinghuman-to-human communication obsolete.
ALIASis supposed to act as a companion that encouragesits owner to cultivate relationships and contacts tothe real world.Figure 1: ALIAS robot platform.Instead of classical interaction techniques solely1by using mouse and keybord, multi-modal human-machine interaction techniques allow for more natu-ral and convenient human-machine interaction (Ovi-att, 1999; Jaimes and Sebe, 2007; Goetze et al,2010a).
Especially for technology in the domain ofambient assisted living (AAL) which is mostly in-tended to be used by older users - these users oftenare less technophile than younger users (Meis et al,2007) - multi-modal interaction strategies includingmodalities like speech and touch pads show high ac-ceptance (Boll et al, 2010).A touch display and a robust speech recognitionand synthesis system enable the ALIAS robot plat-form to interact with the user via speech or usingthe mounted touch display (cf.
Figure 1).
Besidescommunication with the robot by speech input andoutput, communication with relatives and acquain-tances via telephone channels, mobile phone chan-nels and the internet is a central goal.
An automaticreminder system motivating the user to participateactively in social interaction is developed.
In ad-dition, the user is encouraged to perform cognitiveactivities in order to preserve quality of life.The following Section 2 briefly describes the sys-tem components of the ALIAS robot platform be-fore Section 3 focuses on the multi-modal user-interaction strategies.2 System Components and AppliedTechnologiesThe ALIAS robot system has a variety of human-machine communication features and sensors.
Fig-ure 2 shows the general overview of the robots soft-ware modules which will be briefly introduced in thefollowing.The dialogue manager (DM) is the robot?s mostcentral component since it is the software modulewhich is responsible for all decisions the robot has totake.
Therefore, it is connected to almost every othermodule.
The DM collects inputs and events from allthese modules, interprets them, and decides whichactions to perform, i.e.
commands to send to whichmodules.
It may move the robot to check on its user,initiate a video telephone call, or ask for a game ofchess.
The dialogue manager runs on the Windowscomputer, which is one of the two computer systemsin the ALIAS system.Figure 2: Overview of the ALIAS robot?s software mod-ules, distributed on two computers.The graphical user interface (GUI) has a closelink to the dialogue manager since it integrates sev-eral applications and receives user inputs of the Win-dows computer?s operating system.
Thus, it reactsto touch input and displays menus and all softwaremodules with graphical output.
(Section 3.1 pro-vides more detailed information on the GUI.
)The automatic speech recognition (ASR) mod-ule enables the robot to understand and react on spo-ken commands (Moritz et al, 2011).
It receivesrecorded audio signals from the Jack audio-serverand converts it to a textual representation of spokenwords.
This list of recognized words will be sentto the DM for interpretation (cf.
Section 3.2 for de-tails).The speech synthesis module enables the robot tocommunicate with its owner verbally (together withthe ASR sytem).
Speech synthesis (Taylor, 2009)is the artificial production of human voice.
Text-to-speech (TTS) systems are used to convert writ-ten text into speech.
An advanced system should beable to take any arbitrary text input and convert itinto speech, whereby the language of the text mustbe known to be able to create the correct pronunci-ation.
Several systems for speech synthesis are al-ready commercially available to realize such a sys-tem.
Speech output was found to be a desired userinteraction strategy for assistive systems (Goetze etal., 2010a) if output phrases are properly designed2since there?s no need to reach out for the robot?s dis-play unit in order to interact with it.A link to the world-wide web is established byintegration of an easy-to-use web browser whichis seamlessly integrated into the GUI.
To counter-act isolation an event search web service was real-ized (Khrouf and Troncy., 2011) that visualizes var-ious events and corresponding pictures to the userthat have taken place or will take place close to theuser?s location.
To achieve this the robot connectsto an online event search service.
The service willprovide him/her with a personalized selection of so-cial event near his/her current location and personalpreferences.An input modality suitable for users that areunable to touch the robot?s screen or to verbal-ize a speech command (e.g.
after a stroke) is thebrain computer interface (BCI) of the robot (Hin-termu?ller et al, 2011).
It uses a set of electrodesplaced on the user?s skull to measure electrical re-sponses of the brain.
These electrical potentials areevoked by means of visual stimuli, e.g.
flashing im-ages on a control display.
By focusing on certainitems on the BCI control display the user?s brain ac-tivity can control the GUI of the robot.
The BCI mayalso be used for writing text messages which can besent using the integrated Skype?
chat functionalityof the robot.To distinguish between its owner and other per-sons, the robot uses an acoustic speaker recogni-tion module.
This provides ALIAS with additionalinformation which can be used to differentiate be-tween persons and interprete multiple speech inputsaccording to their individual context.In order to achieve more human-like character-istics, the robot uses a face identification module.So it is able to adjust its eyes to face the person it?stalking to.
The face detection algorithm utilizes therobot?s 360?
panorama camera located on top of therobot?s head, and thus covers the robots surround-ings, completely.The navigation module handles the actual move-ment, collision prevention, and odometry of therobot.
It drives ALIAS by plotting waypoints ona pre-recorded map.
Obstacles are detected usingultra-sonic sensors, the laser scanner, and the frontcamera.
In case the robot?s path is blocked, the nav-igation module will plot an alternative route in orderto reach the designated target location, evading theobstacle (Kessler et al, 2011).
The navigation mod-ule may also be remotely controlled by another per-son in order to check on the robot?s owner in casean accident has been detected or the owner has re-quested for help.3 Multimodal Interaction StrategiesThe robot?s user interface features different inputmodalities; speech commands, the BCI, or the touchscreen (GUI).
For speech input, the ASR mod-ule processes the recorded speech commands andtranslates them into multiple textual representations,which are then sent to the DM for interpretation.BCI and GUI include a display unit to providefeedback to the user.
Thus they require an addi-tional pathway for receiving commands from theDM.
In case of the BCI, available items on its con-trol screen may be switched by the DM to reflect thecurrent dialogue state, i.e.
a selection of audio booksif the audio book module has been accessed.
For theGUI, which integrates several software applicationsinto one single module, there is also the possibilityof non-user related events, such as incoming phonecalls from the integrated Skype module.
The GUIhas to relay these events to the DM for decision.All user inputs and relevant system events aregathered by the DM.
As the ALIAS system?s cen-tral control unit, the DM keeps track of all activerobot modules and relevant sensor data.
It mergesall provided inputs, puts them into context, inter-prets them, and decides which actions to perform.Whereas some inputs may be redundant, others maybe invalid or highly dependent on the context.For example, pushing a button on the touch screenis most likely related to the application that is run-ning on the screen.
Whereas the spoken phrase ?onthe right?
could mean that the user wants ALIAS topush a button that is located on the right hand side ofits screen.
Another interpretation would be that theuser wants the robot to turn to the right and moveaside.
Or the user was talking to another person inthe room, possibly even on ALIAS?
video telephone,and the spoken statement is not to concern the robotat all.This section provides a closer look on the ALIASrobot?s most frequently used user interfaces and3their design.3.1 Graphical User InterfaceThe GUI consists of a series of menus containing afew large buttons, each of them leading to anothermenu or starting an application, i.e.
an integratedsoftware module.
The GUI?s main menu is shownin Figure 3.The GUI uses a minimalistic design, includingsome light gradients and blur for non-essential back-ground components.
Whereas the actual buttons fea-ture comprehensive icons and text labels with largefonts, enclosed by high-contrast black frames.
Thiseases distinction between buttons and background.Taking visual impairments into account the GUIremains usable, while still being visually pleasingfor people with unimpaired vision.
Due to eachusers individual color perception, colors are usedsparsely and mustn?t be the sole cue to carry es-sential information.
Instead combinations of colors,shapes, and labels are preferred.Figure 3: ALIAS robot?s main menu.The GUI depends on animations; buttons flash ina different (dark) colors when pressed and menussliding on and off the screen when switched.
Suchanimations provide visual feedback to user inputsand are unlikely to be missed by the user, since theyinvolve the whole screen, usually.The GUI makes a clear distinction between menusand application modules, though both are supposedto look quite similar on the screen.
Menus provideaccess to sub-menus and integrated software mod-ules i.e.
applications, using a tree-like menu struc-ture which is defined by a configuration file.Application modules implement their very ownindividual layouts, buttons, features, and remote-control capabilities for the DM.
By this, some fea-tures are available after the related application hasbeen started, only.
The GUI features a selectionof integrated application modules, like a Skype?-based video telephone, a web-browser, a televisionmodule, an audio book player, a selection of seriousgames, and access to the robot?s Wii gaming con-sole.The GUI processes two kinds of user inputs; di-rect inputs and indirect inputs.
Both input types willbe further outlined below.3.1.1 Direct InputsThe GUI accepts normal user inputs, as they areprovided by the host computer?s operating system.In case of the ALIAS robot the main source of suchinputs will be the touch screen.
These inputs areconsidered as direct inputs, since they are providedby the computer?s operating system and are handledby the GUI directly.More generally every input falls into the groupof direct inputs if the GUI is directly receiving it.Accordingly even an incoming phone call is a di-rect input, because it is triggered by an integratedGUI module.
So, unless properly handled and prop-agated, no other module would ever know about it.Thus, most direct inputs also need to be relayed tothe dialogue manager that takes over the role of astate machine to keep all modules on the robot syn-chronized.
If, for example, any input in the currentsituation is not allowed or even undesirable the DMcan intervene and reject those inputs.3.1.2 Indirect InputsA second kind of user inputs is represented by thegroup of indirect inputs.
Indirect inputs are systemmessages, received by the GUI.
Basically indirectinputs are inputs that are handled by another mod-ule, but require a reaction by the GUI.
Typically suchindirect inputs are generated by the Dialogue Man-ager, as response to a speech input for example.The user may issue a verbal command to therobot: ?Call Britta, please!?
The sound wave ispicked up by the robot?s microphones, converted4into a sampled audio signal that is redirected by theJack Audio Server to the speech recognition mod-ule.
The speech recognition module converts theaudio signal to a textual representation that will beinterpreted and processed by the dialogue manager.In case the dialogue manager finds a contact named?Britta?
in its data base, it sends a series of networkmessages to the GUI, containing the required com-mands to bring up the telephone application and ini-tiate the phone call.3.1.3 Multi-modal InputMost parts of the GUI can be controlled by touchdisplay as well as by spoken commands.
Further-more, a control by the BCI is possible for parts of theGUI (currently Skype chat and entertainment suchas audio books).Figure 4: Multi-modal input dialog for appointments.An example for a multi-modal interaction is theappointment input window depicted in Figure 4.
Itcontains information about the category, the title, thestart and end time of the appointment and a possibil-ity to set a reminder.
The interface can be controlledby mouse and keyboard as well as via speech com-mands following a structured dialogue.
By this, theused is free to chose if he/she wants to use mouseand keyboard as a fast way to enter an appointmentor speech if he/she is not close enough to the robot?stouch display and is either not willing or not capableto reach it.3.2 Speech RecognitionCreating an automatic speech recognition (ASR) de-vice requires different processing steps.
Figure 5 il-lustrates exemplary the structure of such a system.Figure 5: Schematic technical design of the ASR system.A very important step is to collect a sufficientlylarge amount of speech data from many differentspeakers.
This speech data is used to train the acous-tic models, which in this case are hidden Markovmodels (HMM), and described in terms of well-known Mel frequency cepstral coefficients (MFCCs)(Benesty et al, 2008).
Besides the HMM modelsof known words also so-called garbage models aretrained, since the ASR device needs to be capable todistinguish not only between words that were trainedfrom the training utterances but also between knownand out-of-vocabulary (OOV) words.In addition to the acoustic models a proper speechrecognition system also needs a language model.The language model provides grammatical infor-mation about the utterances that are presented bythe subjects to the ASR system.
Language modelscan be separated into groups of statistical and non-statistical models.
The ALIAS ASR system com-prises of two recognition systems that are running at5different grammatical rules (cf.
Figure 6).
The firstASR system uses a non-statistical language modelthat is typically used for ASR systems with smallvocabulary size and very strict input expectations.This ASR system can be considered as a keywordspotter.
In contrast, N-gram models can also be usedfor continuous speech recognition systems, wherethe grammatical information can get a lot more com-plex.
Thus, the second recognizer uses statisticalgrammar rules (N-gram) which consists of a 2-gramforward and 3-gram backward model and enablesthe system to make a more soft decision on the rec-ognized sentence.By this two-way approach, the keyword spottingsystem can do a reliable search for important catch-words, whereby the second recognizer tries to under-stand more context from the spoken sentence.
Thisensures an even broader heuristic processing for theDM.LVASRkeywordspottersystemdialogmanageractionGUIFigure 6: Two-way ASR system.With the acoustic models and a valid languagemodel the speech recognition device is now able tooperate.
The user utters any command, which ispicked up by a microphone.
Since in real-world sce-narios the microphones do not only pick up the de-sired speech content but also disturbances like ambi-ent noise or sounds produced by the (moving) robotsystem itself, the microphone signal has to be en-hanced by appropriate signal processing schemes(Ha?nsler and Schmidt, 2004; Goetze et al, 2010a;Cauchi et al, 2012) before ASR features (MFCCs)are extracted from the speech input.A The extractedfeatures are then transferred to the decoding systemwhere the content of speech is analyzed.ASR processing deals in terms of probabilities.Although speech recognition has been identified asa highly desired input modality for assistive systems(Goetze et al, 2010a) the acceptance drastically de-creases if the recognition rate is not sufficiently high.For every acoustic input there are multiple recog-nition alternatives, with varying probabilities.
In-stead of using only the most probable recognitionfor output, the ASR module provides the DM witha few additional alternatives.
This allows the DM amore thorough analysis and thus a more precise in-terpretation of the provided speech input to decidefor an output on the GUI or an action (e.g.
movingthe roboter).4 ConclusionThis paper presented multimodal interaction strate-gies for an robot assistant which has its main fo-cus on support of communication.
This includesboth, fostering of human-to-human communicationby providing communication capabilities over dif-ferent channels and reminding on neglected relation-ships as well as communication between the techni-cal system and its user by means of speech recogni-tion and speech output.AcknowledgmentsThis work was partially supported by the projectAAL-2009-2-049 ?Adaptable Ambient Living As-sistant?
(ALIAS) co-funded by the European Com-mission and the Federal Ministry of Education andResearch (BMBF) in the Ambient Assisted Living(AAL) program and the by the project Design of En-vironments for Ageing (GAL) funded by the LowerSaxony Ministry of Science and Culture through theNiederschsisches Vorab grant programme (grant ZN2701).ReferencesThe European Ambient Assisted Living Innovation Al-liance.
2009.
Ambient Assisted Living Roadmap.VDI/VDE-IT AALIANCE Office.J.
Benesty, M.M.
Sondhi, and Y. Huang.
2008.
Springerhandbook of speech recognition.
Springer, New York.S.
Boll, W. Heuten, E.M. Meyer, and M. , Meis.
2010.Development of a Multimodal Reminder System forOlder Persons in their Residential Home.
Informaticsfor Health and Social Care, SI Ageing & Technology,35(4), December.6B.
Cauchi, S. Goetze, and S. Doclo.
2012.
Reduction ofNon-stationary Noise for a Robotic Living Assistantusing Sparse Non-negative Matrix Factorization.
InProc.
Speech and Multimodal Interaction in AssistiveEnvironments (SMIAE 2012), Jeju Island, Republic ofKorea, Jul.Selene Chew, Willie Tay, Danielle Smit, and ChristophBartneck.
2010.
Do social robots walk or roll?
InShuzhi Ge, Haizhou Li, John-John Cabibihan, andYeow Tan, editors, Social Robotics, volume 6414 ofLecture Notes in Computer Science, pages 355?361.Springer Berlin / Heidelberg.European Commision Staff.
2007.
Working Document.Europes Demografic Future: Facts and Figures.
Tech-nical report, Commission of the European Communi-ties, May.S.
Goetze, N. Moritz, J.-E. Appell, M. Meis, C. Bartsch,and J. Bitzer.
2010a.
Acoustic User Interfaces forAmbient Assisted Living Technologies.
Informaticsfor Health and Social Care, SI Ageing & Technology,35(4):161?179, December.S.
Goetze, F. Xiong, J. Rennies, T. Rohdenburg, and J.-E. Appell.
2010b.
Hands-Free Telecommunicationfor Elderly Persons Suffering from Hearing Deficien-cies.
In 12th IEEE International Conference on E-Health Networking, Application and Services (Health-com?10), Lyon, France, July.S.
Goetze, J. Schro?der, S. Gerlach, D. Hollosi, J.-E. Ap-pell, and F. Wallhoff.
2012.
Acoustic Monitoring andLocalization for Social Care.
Journal of ComputingScience and Engineering (JCSE), SI on uHealthcare,6(1):40?50, March.E.
Ha?nsler and G. Schmidt.
2004.
Acoustic Echo andNoise Control: a Practical Approach.
Wiley, Hobo-ken.C.
Hintermu?ller, C. Guger, and G. Edlinger.
2011.
Brain-computer interface: Generic control interface for so-cial interaction applications.A.
Jaimes and N. Sebe.
2007.
Multimodal human-computer interaction: A survey.
Comput.
Vis.
ImageUnderst., 108(1-2):116?134, October.J.
Kessler, A. Scheidig, and H.-M.
Gross.
2011.
Ap-proaching a person in a socially acceptable manner us-ing expanding random trees.
In Proceedings of the 5thEuropean Conference on Mobile Robots, pages 95?100, Orebro, Sweden.H.
Khrouf and R. Troncy.
2011.
Eventmedia: Visual-izing events and associated media.
In Demo Sessionat the 10th International Semantic Web Conference(ISWC?2011), Bonn, Germany, Oct.C.
Lisetti, F. Nasoz, C. LeRouge, O. Ozyer, and K. Al-varez.
2003.
Developing multimodal intelligent affec-tive interfaces for tele-home health care.
InternationalJournal of Human-Computer Studies, 59(1-2):245 ?255.
Applications of Affective Computing in Human-Computer Interaction.M.
Meis, J.-E. Appell, V. Hohmann, N v. Son,H.
Frowein, A.M. ?Oster, and A. Hein.
2007.
Tele-monitoring and Assistant System for People withHearing Deficiencies: First Results from a User Re-quirement Study.
In Proceedings of European Confer-ence on eHealth (ECEH), pages 163?175.N.
Moritz, S. Goetze, and J.-E. Appell.
2011.
Ambi-ent Voice Control for a Personal Activity and House-hold Assistant.
In R. Wichert and B. Eberhardt, ed-itors, Ambient Assisted Living - Advanced Technolo-gies and Societal Change, Springer Lecture Notes inComputer Science (LNCS), number 978-3-642-18166-5, pages 63?74.
Springer Science, January.S.T.
Oviatt.
1999.
Ten myths of multimodal interaction.Communications of the ACM.
ACM New York, USA,42(11):74?81, Nov.R.C.
Petersen.
2004.
Mild Cognitive Impairment asa Diagnostic Entity.
Journal of Internal Medicine,256:183?194.J.
Rennies, S. Goetze, and J.-E. Appell.
2011.
Consid-ering Hearing Deficiencies in Human-Computer Inter-action.
In M. Ziefle and C.Ro?cker, editors, Human-Centered Design of E-Health Technologies: Concepts,Methods and Applications, chapter 8, pages 180?207.IGI Global.
In press.M.A.
Rudberg, S.E.
Furner, J.E.
Dunn, and C.K.
Cassel.1993.
The Relationship of Visual and Hearing Im-pairments to Disability: An Analysis Using the Lon-gitudinal Study of Aging.
Journal of Gerontology,48(6):M261?M265.Statistical Federal Office of Germany.
2008.
Demo-graphic Changes in Germany: Impacts on HospitalTreatments and People in Need of Care (In Germanlanguage: Demografischer Wandel in Deutschland -Heft 2 - Auswirkungen auf Krankenhausbehandlungenund Pflegebedu?rftige im Bund und in den La?ndern).Technical report.P.
Taylor.
2009.
Text-to-Speech Synthesis.
CambridgeUniversity Press.S.
Uimonen, Huttunen K., K. Jounio-Ervasti, andM.
Sorri.
1999.
Do We Know the Real Need for Hear-ing Rehabilitation at the Population Level?
HearingImpairments in the 5- to 75-Year Old Cross-SectionalFinnish Population.
British J. Audiology, 33:53?59.7
