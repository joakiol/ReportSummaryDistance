Evahmtion Metrics t'oi- Knowledge-Based Machine TranslationEr ic  I1.
Nyberg ,  3 rdTeruko  Mi tamuraJa imc G. Carbone l lCcnte, r for Mach ine  Trans lat ionCarnegie  Me l lon  Un ivers i tyP i t tsburgh,  PA 15213Topical Paper: machine translationAbstractA methodology is presented for coml)onent-l)ase(lmachine translation (MT) evaluation through causalerror analysis to complement existing lobal evalu-ation methods.
This methodology is particularly :q)-propriate for knowledgc-I)ased machine translation(KBMT) systems.
After a discussion o\[ M'I' eval-uation criteria and the particular evahlatiou metricsproposed for KBMT, we apply this methodologyto a large-scale application of the KANT ,nachinctranslation system, and present some sample results.1 I n t roduct ionMachine Translation (MT) is considered the paradigm taskof Natural Language Processing (NLP) hy some researchersbecause it combines almost all NLP research :treas: syntacticparsing, semantic disambigt, ation, knowledge rel)reseutation,language generation, lexical acquisition, and morphologicalanalysis and synthesis.
However, the evaluation method-ologies for MT systems have heretofore centered on hlackbox approaches, where global properties of tile system areevaluated, such as semantic fidelity of the translation or com-prehensibility of the target langt,age output.
There is a longtradition of such black-box MT evaluations (Van Slype, 1979;Nagao, 1985; JEIDA, 1989; Wilks, 1991), to the point thatYorick Wilks has stated: "MT Evaluation is better understoodthan MT" (Carbonell&Wilks.
1991 ).
While these valt,,'ltionsare extremely important, hey should be augmented with de-tailed error analyses and with component cval uation s in ordcrto produce causal analyses l)inpointing errors and therefm'eleading to system improvement.
Inessence, we advocate bothcausal component analyses as well as gloi)al behavioral anal-yses, preferably when the latter is consistent with tile Iormervia composition of the component analyses.Tim advent of Knowledge Based Machine Translation(KBMT) facilitates component evaluation and error attribu-tion because of its modular nature, though this ol)servalionby no means excludes transfer-based systems from similaraualyses.
After reviewing the reasons att(I criteria for MTevaluation, this paper describes a specific evaluation method-ology and its application to the KANT system, developedat CMU's Center for Machine Translation (Mitamura, et al1991).
The KANT KBMT architecture is particularly well-suited for detailed evaluation because of its relative simplicity':ompared to other KBMT systems, and because it has beenscaled up to industrial-sized al)plications.2 Reasous  for  Eva luat ionMachine Translation is evaluated for a number of differentreqsons, and when possihle these should be kept clear andseparate, as diflerent ypes of ev,'duation are best suited tomeasure different aspects of an MT system, l.et ns review thereasons wily MT systems may be evaluated:?
Com/)arison with l lumans.
It is useltd to establish aglobal comparison with hurmm-qu:.dity ranslation as afunction of task.
For general-ptnl)OSe accurate tralls-lation, most MT systelns have a long way to go.
Abehavioral black-box evahmtion is appropriate here.?
Decision to use or buy a particular MT syMet~.t.
Thisevahmliou is task dependent, aud nmst take both qualityof trallslation as well as economics inR) accf)nllt (e.g.cost of purchase and of adapting the MT system to thetask, vs. hum:in translator cost).
Behavioral black-boxevaluations arc appropriate here too.,, Comparison of multiple MT' systems.
The compariso~lmay be to evahmte research progress ;is iu the ARPAMT evahmtions, or to determine which system shouldbe considered for Imrchase and use.
If the systems em-l)loy radically different MT paradigms, such ;is EBMTand KP, MT, only 1)lack-box evahmtions are meaningful,but if they employ similar methods, then I)oth forms ofevaluation tire appropriate.
It can he very informative todetermine which system has the better parser, or which isable to perform certain difficult (lisaml)iguatkms helter,atRl SO O11, wi 1\[1 ;Ill eye towards futt,re synthesis of the bestideas l,onl differeut systems.
The Sl~CeCh-recognilioncmnmunily has benelited from such comparisons.?
Trackit,g technological progress.
In order to determinehow a system evolves over time it is very useful O knowwhich components ,'ue improving and which are not, aswell tls their contribution Io overall MT 1)erformance.Moreover, aphenomena-based evaluation is useful here:Which l)reviously problematic linguistic phenomena arebeing handled better and by having improved whichmodule or knowledge source?
This is exactly the kindof information that other MT researchers would find ex-tremely valu,:thle to improve their own systems - muchmore so than a relalively empty glohal statement suchas: "KANT is doing 5% better this month.
",, Improvement of  a particular system.
Ilere is whereCOlnponent an,'llysis and error attribution are most vahl-able.
Systcul engineers and!
linguistic knowledge sourcenlainiamers ( uch tls lexicographers) perforni hest when95given a causal analysis of each error, lleuce module-by-module performance metrics ,are key, as well as ananalysis of how each potentially problematic linguisticphenomenon is handled by each module.Different communities will benefit from different evalua-tions.
For instance, the MT user community (actual or poten-tial) will benefit most from global black-box evaluations, astheir easons are most clearly aligned with the first three itemsabove.
The funding community (e.g., EEC, ARPA, MITI),wants to improve the technological infrastructure and deter-mine which approaches work best.
Thus, their interests aremost clearly aligned with the third and fourth reasons above,and consequently with both global and component evalua-tions.
The system developers and researchers need to knowwhere to focus their efforts in order to improve system per-formance, and thus are most interested in the last two items:the causal error analysis and component evaluation both fortheir own systems and for those of their colleagues.
In thelatter case, researchers learn both from blame-assigmnent ierror analysis of their own systems, as well as fiom successesof specific mechanisms tested by their colleagues, leading toimportation and extension of specific ideas and methods thathave worked well elsewhere.3 MT Evaluat ion Criter iaThere are three major criteria that we use to evaluate tileperformance ofa KBMT system: Completeness, Correctness,and Stylistics.3.1 CompletenessA system is complete if it assigns ome output string to everyinput string it is given to translate.
There are three types ofcompleteness which must be considered:?
Lexical Completeness.
A system is lexieally completeif it has source and target language lexicon entries forevery word or phrase in the translation domain.,, Grammatical Completeness.
A system is grammaticallycomplete if it can analyze of the grammatical structuresencountered in the source language, and it can generateall of the grammatical structures necessary in the targetlanguage translation.
Note that the notion of "grammat-ical structure" may be extended to include constructionslike SGML tagging conventions, etc.
found in technicaldocumentation.?
Mapping Rule Completeness.
A system is complete withrespect o mapping rules if it assigns an output struc-ture to every input structure in the translation domain,regardless of whether this mapping is direct or via aninterlingua.
This implies completeness of either transferrules in transfer systems or tile semantic inteq)retationrules and structure selection rules in interlingtta systems.3.2 CorrectnessA system is correct if it assigns acorrect output string to everyinput string it is given to translate.
There are three types ofcorrectness to consider:?
Lexical Correctness.
Each of the words selected in thetarget sentence is correctly chosen for the concept hat itis intended to realize.?
Syntactic Correctness.
The grammatical structure ofeach target sentence should be completely correct (nogrammatical errors);?
Setnanlic Correctness.
Senlanlic correctness presup-poses lexical correctness, but also requires that the corn-positional meaning of each target sentence should beequivalent to tile meaning of the source sentence.3.3 StylisticsA correct OUtpUt ext must be ineaning invariall\[ and untler-standable.
System evahmtion may go beyond correctness andtest additional, interrelated stylistic factors:?
Syntactic Style.
An output sentence may contain agram-matical structure which is correct, but less appropriate forthe context han another structure which was not chosen.?
Lexical Appropriateness.
Each of the words chosen isnot only a correct choice but tile most appropriate choicefor the context.,, Usage Appropriateness.
The most conventional or nat-ural expression should be chosen, whether technicalnomenclature or comlnou figures of speech.?
Oilier.
l:orm'41ity, level of difficulty of the text, and othe,'snch parameters shotlJd be preserved in the translation orappropriately selected when absent from the source.4 I (BMT Evahml iou  Cr i ie r ia  and  Cor rectnessMet, - icsIn order to evahmte an inlerlingnal KBMT system, we definethe following KBMT evahmtion criteria, which are based onthe general criteria discussed in the previous ection:?
Analysis Coverage (AC).
Tile percentage of test sen-tences for which tile analysis module produces all inter-lingua expression.?
Analysis Correctness (AA).
"File percentage of the inter-linguas produced which are complete and correct repre-senlatious of the meaning of tile input sentence.?
Generat ionCoverage(GC).Thepercentageofcoml) leteand correct iuterlingna expressions R}r which the gener-ation module produces a target language sentence.?
Generation Correctness (GA).
The percentage of targetlanguage senlences which are complete and correct re-alizations of the given complete and correct interlinguaexpression.More precise deliuitions of these Rnu quantities, as well asweighted ve,sions thereof, are preseuted ill Figure 11.Given these four basic quantities, we can define translationcorrccmess as follows:?
Translation Correctness (TA).
This is tile percentage ofthe input sentences for which the system produces acomplete and correct ot,tput sentence, and call be c,'ltcu-lated by mt,ltiplying together Analysis Coverage, Anal-ysis Correctness, Generatiou Coverage, and GenerationCorrectness:TA = ACx AA x GC x (,'A (I)For example, consider a test scenario where 100 sen-tences are given .
'Is input; 90 sentences produce interliu-guas; 85 of tile interlinguas are correct; for 82 of theseIAn additional quantity shown i!n Figure 1 is the fluency of thetarget hmguage generation (leA), which will not be discussed furtherin this paper.96Criterion FormulaNo.
Sentences SNo.
Sent.
w/It, StLNo.
Comp./Corr.
IL 5'tL-CCAnalysis Coverage A C = S's # / '?Analysis Accuracy i l  A =: ,q'l l.-.
('(' /,q'# l.IL Error 1 LiWeighted AA I/VAA = I - F'>V,(S, t., )/,b'11,No.
TL l:'roduced .q"rt.No.
Correct TL ,b'TLCNo.
Fluent TL ,qr u,'Generation Coverage G'C : S~'I./.S'sL-cc~Generation Accuracy GA : ,5'7't,c /S's't,TL Corr.
Error 7'L iTL Fluency Error TLCIWeighted GA W(I / I  = 1 - EWi (.b'~-,t, i)/,b',#, tGeneration Flnency ,S'<t'sm /,S'Tt, cWeighted FA I'V I"A = 1 -- )\]!
'Vi(,q'7,t:< ?
)l__q"s'(.cFigure 1: l)etinitlons and Ftlr innlas for O<ileulating Str idand lgrror-Weighted Fxaluation Measures in Analysis and(;eneratinn Componentsinterlingnas tile system produces French otutpt~t; ,'lnd 80of those culprit sentences fire correct.
Then90 85 82 80rA  ~: l - \ ] )~x ,~x~?~ (2)= .90 x .94 x .96 x .98 = .80Of course, we can easily calctlltlte TA ovcii.lll if we knowtile number of input sentences arid the numl)er el  corrk'ctoutput sentences for a given test suite, but often ntod-ules are tested separately and it is usclul to comhine theanalysis and generation ligures in this way.
It is alsoimportant to note that even if each module in tile systemintroduces only a small error, the cuutuhttive ffect canbe very substantial.All interlingua-based systems contain separate analysis andgeneration modules, aud therefore all can be subjected to thestyle of evalnation preseuted in this paper.
Some systems,however, fttrthcr modularize the trausl.
'ttion process.
KANT,for example, has two SeXluential analysis modules (source textto syntactic f-structures; f-structures to interlingua) (Mita-mnra, et al, 1991).
Ilence tile evahtation could be conductedat a finer-grained level.
Of course, for transfer-based systemsthe modular decomposition is analysis, transfer and gorier-at;on moclules, and for example-based MT (Nagao, 1984)modnles are the tnatcher and the modifier.
APl~ropriate met-ties for completeness and correctness can be detined for eachMT paradigm hated on its modular decomposition.5 P re l iminary  Eva luat ion  o f  KANTIn order to test a partictdar application of tile KANT system,we identify a set of test suites which meet certain criteria:?
Grammar  Test Suite.
This test suite contains enteuceswhich exemplify all of the grammatical constructionsallowed in the controlled input text, anti is inttended totest whether 1he system can trauslate all of them,?
Domain  Lexicon Test Suite.
This test suite ctmtai~ts extswhich exemplify all the ways in which general domaiutte,ms (especially verbs) are used in different corttexts.
Itis intended to test whether the systent can translate ;ill ofthe usage variants for general domaill ISills.
* Preselected hJput Texts.
These test suites cont,'tin lextsfrom different parts of the domain (e.g., different ypesof nlanmtls for different pmducls), selecled in advance.These are intended to demonstrate hat the system cantransl;tte well in all parts of tile ct~stomer domain.,, &mdomly  Selet:tcd Ilq)ttl Texts.
These test suites tirecomprised of texts that are selected randomly by theevaluator, and which have not been used to lest the sys-tem before.
These ztre inteuded to illustrate how well thesystem will do on text it has not seeu before, which givesthe l)esl cnmpleteness-in-context measure.The first three types of test suite fire employed for regres-sion testing as the system evolves, whereas tile latter type is~generated anew for each major evaluation, l)uring develop-ment, each successive version of the system is tested on theavailable test data to prodt ce ~ gg egate lil?ures for AC, AA,(;(2, and (CA.5.1 Cnverage "lk'stlngThe coverage rcsults (AC aucl GC) are ealct,lated atltomat-;tally by a program which cotmts output structt,res duringanalysis and generation.
During evaluatiou, the translationsystem is split into two halves: SotLrce-to-lnterlingua antiInterliulgua-to-'lhrget.
l:or ,I j;ivt;u text, this allows us to ,'ltllo-matically count how many sellteuces l)rOduccd inlerlingttas,thus deriving AC.
This also allows t,s to automatically counthow ilia.lily iuterlingtias prodtlce(I otttput sentences, thtzs tie.-rivitlg ( ;C.5.2 Correctness Testinp,The correctness results (AA anti (;A) are calcuhtted l'of ,'l giventext by a process of hunlan evaluation.
Tiffs requires tile effortof a humau evah~ator who is skilled in lhe source language,target lauguage> ,'ttld translation domain.
We have developeda method for calculating the correctness of the OUtl)Ut whichinvolves tile following steps:1.
The text to be evaluated is translated, and the input andouti)ut Senlences are aligned ill a sop:irate l i l t  for evalu-atiolt.2.
A scoring program presenls each translation to the oval-uator, l{ach transl,<ltimt is assigned a score frorfl tilefollowing sot of l)ossihilities:* C (Ct/rrt!cI).
The OUtllul sentence is COml)letelycorrect; it preserves the liieailiug of llie iUl)tlt seri-tenco conipletcly, is understandal)le without difli-eillty, a~itl does liot violtlte any rules of gran/m;ir.?
1 (Incorrect).
The ?/tllpUt seutencc is inconipletc (oreinpty), or not easily undcrsi;iudable.?
A (Accq/table).
The sentence is complete ,'utd eas-ily ullclerslaltdablo, I)tlt is IlOt COmliletoly gramm,'lt-ical or violates some ~q(iMl.
lagging convention.3.
The score lor the whole text is calculated by tallying thedifferent scores.
TIle overall correctlleSS of the trans-latioli is staled in terms of a range between the strictlycorrect (C) aud the acceptahle (C + A) (cf.
Figure 2) 2.2111 tile gerieral case, one I y ssigll a specific em)r coeflicientto each citer type, and multiply that coeflicient I)y lhe nunlber ofselltel/ces exhibiting the error.
The StilnlllatiOll of these productsacross all the erroiful sellLences i then used to lm~duce a we;pillederror rate.
Tilts level of detail llas not yet proven lo be necessary incurrent KANTewiluatioi~..qee Figure 1 I~r exainplesoflorlnulasweighted by elror.975.3 Causal Component AnalysisThe scoring program used to present ranslations for eval-uation also displays intermediate data structures (syntacticparse, interlingua, etc.)
if the evahmtor wishes to performcomponent analysis in tandem with correctness evaluation.ht this case, the evaluator may assign different machine-readable rror codes to each sentence, indicating the It)cationof the error and its type, along with any comments that areappropriate.
The machine-readable error codes allow all ofthe scored output o be sorted and forwarded to maintainers ofdifferent modules, while the unrestricted comntents capturemore detailed information.For example, in figure 2, Sentence 2 is marked with theerror codes ( :NAP : SEX), indicating that tile error is theselection of an incorrect target lexeme (ouvrez), occurring inthe q,uget Language Mapper 3.
It is interesting to note thatour evaluation method will assign a correctness score of 0%(strictly correct) 25% (acceptable) to this small text, sinceno sentences are marked with "C" and only one sentences imarkexl with "A".
However, if we use the metric of"countingthe percentage of words translated correctly" this text wouldscore much higher (37/44, or 84%).
A sample set of errorcodes used for KANT evahmtion is shown in Figure 3.1.
"Do not heat above the following temaperature:""Ne rdchauffez pas la tempdrature st,ivante au-dessus:"Score: I ; Error: :GEN :ORD2.
"Cut the bolt to a length of 203.2 ,'am.
""Ouvrez le boulon fi une longueur de 203,2 nam.
"Score: 1 ; Error: :MAP :LEX3.
"Typical ocation of the 3F0025 Bolts, which must beused on the 826C Compactors:""Position typique des boulons 319025 sur lescompacteurs:"Score: I ; Error: :INT :IR; :MAP :SNM4.
"Use spacers (2) evenly on both sides to eliminateside movement of the frame assembly.
""Employez les entretoises (2) sur les deux c6tdspour 61iminer jeu lat6ral de I'ensemble tie bStiuniform6ment.
"Score: A ; Error: :MAP :ORDFigure 2: Sample Excerpt from Scoring Sheet5,4 Current ResultsThe process described above is performed for each of the testsuites used to evaluate the system.
Then, an aggregate table isproduced which derives AC, AA, GC, and GA for the systemover all the test suites.At the time of this writing, we arc in the process or com-pleting a large-scale English-to-French application of KANTin the domain of heavy equipment documentation.
We have.
used the process detailed in this section to evaluate tile systemon a bi-wcckly basis during developmcnt, using a randomly-selected sct of texts each time.
An example containing ,qggre-gate results for a set of 17 randomly-selected texts is shownin Figure 4.In the strict case, a correct sentence rcccivcs a vahle of land a scntence containing any error receives a value of zero.3For brevity, the sample xcerpt dots not show the intermediatedata structures that he evaluator would have exalnirled to make thisdecision.Modtde Code Colnment:PAR :Lt-X Source lexicon, word missipg/incorrect:GRA Ungrammatical sentence accel)ted,Grammatical sentence not accepted:INT :SNI F-structure slot ,tot interpreted:FNI F-structure feature not interpreted:IR Incorrect inted ingua representation--MAP :LEX Target lexicon, word missing/incorrect:SNM semantic role not ,napped:FNM semantic feature not maapped--GEN :GRA Ungrann`aatical sentence produced:ORD Incorrect constituent ordering:PAR Syntactic Parser: INT Semantic Interpreter:MAP "l,trget Language Mapper:GEN Target Language GeneratorFig ure 3: Saml)le Errm" Codes Used in KANT levahtati(mI NAME S .5",'t.
/,"r ;.c' GA TA JResult 1 608 5,16 ,167-491 86-90% 7%81%Result 2 608 546 467-519.46 86-95% 77-85%Figure 4: KANT Ev'4hiation Results, 17 R.'mdnndy-Selected Texts, 4/21/94In tile weighted case, a sentence containing an error receivesa partial score which is equal to the percentage of correctly-translated words.
When the weighted method is used, thepercentages are considerably higher.
For both Result 1 andResult 2, the nt, maber of correct target language sentences(given as .5"vrc) is shown as ranging between comapletelycorrect (C) and acceptable (C + A).We are still working to improve both coverage and accaracyof the heavy-equipment KANT application.
These numbersshould ,tot be taken as the upper bound for KANT accuracy,since we are still in tile l)roccss of i,nproving the system.Nevertheless, our ongoing evahmtion results are useful, bothto illustrate the evaluation methodology and also to focus theeffort of the system dcvelol)ers in increasing accur:lcy.6 D iscuss ionOur ongoing evalt, atitm of the lirst large-scale KANT applica-tion Ires benefitted from the detailed error analysis presentedhere.
Following tile tabulation of error codes l)rOduced dur-ing catlsal comp(mcnt analysis, we can attril)ute the ntajorityof the completeness problems to identiliable gaps in lexiealcoverage, :rod the majority of the accuracy prol)lefns to areasof the domain ntodel which are known Io be incolnplctc orinsufiiciently general.
On the other hand, the grammars ofboth source and target language, as well as tile software mod-ules, are relatively solid, as very few errors can be attributedthereto.
As lexieal coverage and domain model generaliza-tion reach completion, the component and global ewlh,ationof the KANT system will t)ecome a more accurate rellectionof the potential of the nnde,lying technology in large-scaleapl) lications.As illustr,'tted in Figm-e 5, traditional transfer-based MTsystems tart with general coverage, and gradt, ally seek toimprove accuracy and later fluency.
In contrast, the KBMTphilosophy has been to start with high accuracy and gradu-ally improve coverage and Iluen~ay.
ht tile KANT systema,we combine both approaches by starting with coverage of alarge specific dontain :rod achieving high accuracy and Iluency98~ 100% l. 'hmn{:y\] 0()%C{}v{2 r ,:~{IO100%ACCH t ' i lC /  ~ /KBMT Traditional MT ,,,.,o.ooo ,oo .
.
.
.
.
.
.
.
.
.
.
.
.
.
o,H,,Start: High Accuracy Start: lligh CovaragoXmprove: Coverage, Improvo: AccuracyFlusncy FluencyFigure 5: Lnngltudln'.d lmprovemewl in Coverage, Accu-racy and lque.cywithin that domain.The evaluation methodol{}gy devtloped here is ,no:mr t{} I)eustd in conjunction with glnbal black-box evaluation meth-ods, indtl}endtnt of the course of develol}ment.
The coml}o-ntnt evaluations arc meant o provide insight for the sysltmdevtlopers, avid to identify prol)ltmatic phenomena prior tosystem coml}letion an{l dtlivefy.
In particular, the methodl}resented here c'm combine coml}onent evalttation and !gl{}l}a\]evaluation to support efficient system testing and nlaintenancebeyond development.7 AclmowledgementsWe woul{I like to thank Radha Rao, To{ld Kaufnlann, andall of our colleaguts on tile KANT project, includirig JanttsAltncher, Kathy Baktr, Alex Franz, Mildred Gahtrza, Sutllohn, Kathi lannamico, Pare Jordan, Kevin Keck, MarionKee, Sarah Law, John Leavitt, Daniela l.ons{lale, DeryleLonsdale, Jeanne Mier, Ve.nkatesh Narayan, Amalio Nieto,and Will Walker.
We would also like to th:mk our Sl}{msors atCaterpillar, Inc. and our colltagues at Carnegie GrOUl}, luc.References\[11 Carbonell, J., Mitamura, T., and E. Nyberl; (1993).
"Evahmting KBMT in tht I,arge," Japan-US Workshopon Machine-Aided Translaliov, Nov. 22-24, Washing-ton, D.C.{2\] Carbonell, J. and Y. Wilks (1991).
"Machint Transhl-tion: An In-Depth Tntorial," 29th Annual Meeting ofthe Association for Compntational Linguistics, Univerosity of CaliR)rnia, Be,'keley, CA, June 18-21,\[371 Ooo{Imml and Nirtnburg, eds.
(1991).
A Case Studyin Knowledge-Based Machine Translation, San Mateo,CA: Morgan Kaufmann.\[4\] Isalmra, Sin-nou, Yamabana, Moriguchi and Nonmra,(1993).
"JEIDA's l'roposed Method for l'valuating Ma-chine "\['ranslalion (Translation Quality)," l',oceedmgs o/SIGNLP 93-NL-96, July.151 J\[tp{ll/ Dlcctlonic h~dustry 1)evclolmlent Association, AJaprmese View of Machine "l'ran.
','httion ti l,ight of tileConsiderations and Recommendations Reported by AL .PAC, U.S.A., JEIDA Machine Translation System P,t-search Commitlec, Tokyo.\[6\] King, M. (1993).
"Panel on Evaluation: MT SummitIV.
Introduction."
l'roceedit~gs of MT Summit IV, July20-22, Kobe, Japan.17\] Mitamura, "i,., E. Nyberg "and J. Cmbonell (1991).
"An Efficient lntcrlinl.,,ua Translation System for Multi-l ivlglml Docunlent Production," Proceedings o/'Machine7?anslatim) Summit III, Washinglon, DC, July 2-d.\[81 Nagao, M. (1984) .
"A Ftamework of a MechanicalTransl:ltion l}ctween Japanese and Enp, lish I)y AnalogyPrincil}lC," Artificial and Iluman Intelligence, Elithorn,A.
and Bauerii, P,.
(eds.
), Elsevier Science Publishers,B.
V. 1984.\[91 Nagao, M. (1985) .
"Evaluation of tilt Quality ofMachint-Transhtttd Sentences and the Control of Lan-guage," .lournal of h!formatioa Proeessi,\]g Society of.lapan, 26(1 (}): 1197-1202.\[l{}l Nakaiwa, Morimoto, Matsndaira, Na,'ita anti Nomura,(1993).
"J F.I DA's Pml}osed Metho{I for Evahmtinl; Ma-chine Traushltion (1)evClOl}tr's Guideliiles)," I'roceed-ings o/ SIGNI.I' 9.
}'-NL-96, J lily.\[111 Nomura, If.
(1093).
"l.~v.
'lhmtion Method of Ma{:hitm' \[ 'r\[ l l IsltttiOll: t't'Olll tile Viewl}oit,t of Natural hanguapeProcessing," I'roceedings of MT Summit IV, Jtlly 20-22,Kobt, J al}an.\[12\] Nyberg, 12. and T. Mitamura (1992).
"Tim KANT Sys--tom: I:tlS\[, Accurtlte, lligh-Quality TratlsIatioll in Ptac-Ileal Domains," Proceedings of COEING 1992, Nante.,;,France ,  Ju ly .\[131 Rinscht, Adriane (1993).
"Towards a MT EvaluationMethodolop, y," I'roceedings of the t,',fth InternationalCon/?rem:e o,,* Theoretical and Methodological lssttesiH Machine 7)'anslatiott, July 14-16, Kyolo, J.2tl}tlll.
(ld\] Rolling, I... (1993).
"P:mel Contribution on MT Evahm-tion," I'roceedMgs o/MT Summil IV, July 20-22, Kobt,Japan.\[15\] Takzly,:una, ltoh, Yagisawa, Mogi and Nomura (1993),"JHDA's Proposed Method for l:.vahmtiug MachineTranshltio, (End User System Selection)," l'r~)ceedingsof SIGNI,I' 93-NL-96, July.\[16J VanSlype, G. (1979).
"l:.valuation of the 1978 Versionof tile SYSTRAN F.uglish-French Automatic System orthe Coml\]tissioll of the I!tu{}l}tan COmmuvlitics," 7'heIncorporated Linguist 18:86-89.117 \] Vasconcellos, M. (1993).
"P:mtl Discussion: Eval u,'ltionMethod of Machine Translati{m," Proceedings of MTSummit IV, July 2{}-22, Kobe, Japan.\[18l Wilks, Y.
(1991).
"SYSTP, AN: It Obviously Works, butIlow Much Can it be hnprovtd?," Teclnfical Rtt}ottMCCS-.91-215, (2}reputing Research 1 ,aborat{n'y, NewMexico Statt University, l.as Cruces.99
