Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 23?31,Suntec, Singapore, 7 August 2009.c?2009 ACL and AFNLPRandom Walks for Text Semantic SimilarityDaniel Ramage, Anna N. Rafferty, and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305{dramage,manning}@cs.stanford.edurafferty@eecs.berkeley.eduAbstractMany tasks in NLP stand to benefit fromrobust measures of semantic similarity forunits above the level of individual words.Rich semantic resources such as WordNetprovide local semantic information at thelexical level.
However, effectively com-bining this information to compute scoresfor phrases or sentences is an open prob-lem.
Our algorithm aggregates local re-latedness information via a random walkover a graph constructed from an underly-ing lexical resource.
The stationary dis-tribution of the graph walk forms a ?se-mantic signature?
that can be comparedto another such distribution to get a relat-edness score for texts.
On a paraphraserecognition task, the algorithm achieves an18.5% relative reduction in error rate overa vector-space baseline.
We also show thatthe graph walk similarity between textshas complementary value as a feature forrecognizing textual entailment, improvingon a competitive baseline system.1 IntroductionMany natural language processing applicationsmust directly or indirectly assess the semantic sim-ilarity of text passages.
Modern approaches toinformation retrieval, summarization, and textualentailment, among others, require robust numericrelevance judgments when a pair of texts is pro-vided as input.
Although each task demands itsown scoring criteria, a simple lexical overlap mea-sure such as cosine similarity of document vectorscan often serve as a surprisingly powerful base-line.
We argue that there is room to improve thesegeneral-purpose similarity measures, particularlyfor short text passages.Most approaches fall under one of two cate-gories.
One set of approaches attempts to explic-itly account for fine-grained structure of the twopassages, e.g.
by aligning trees or constructinglogical forms for theorem proving.
While theseapproaches have the potential for high precisionon many examples, errors in alignment judgmentsor formula construction are often insurmountable.More broadly, it?s not always clear that there is acorrect alignment or logical form that is most ap-propriate for a particular sentence pair.
The otherapproach tends to ignore structure, as canonicallyrepresented by the vector space model, where anylexical item in common between the two passagescontributes to their similarity score.
While theseapproaches often fail to capture distinctions im-posed by, e.g.
negation, they do correctly capturea broad notion of similarity or aboutness.This paper presents a novel variant of the vec-tor space model of text similarity based on a ran-dom walk algorithm.
Instead of comparing twobags-of-words directly, we compare the distribu-tion each text induces when used as the seed ofa random walk over a graph derived from Word-Net and corpus statistics.
The walk posits the ex-istence of a distributional particle that roams thegraph, biased toward the neighborhood surround-ing an input bag of words.
Eventually, the walkreaches a stationary distribution over all nodes inthe graph, smoothing the peaked input distributionover a much larger semantic space.
Two such sta-tionary distributions can be compared using con-ventional measures of vector similarity, producinga final relatedness score.This paper makes the following contributions.We present a novel random graph walk algorithm23Word Step 1 Step 2 Step 3 Conv.eat 3 8 9 9corrode 10 33 53 >100pasta ?
2 3 5dish ?
4 5 6food ?
?
21 12solid ?
?
?
26Table 1: Ranks of sample words in the distribu-tion for I ate a salad and spaghetti after a givennumber of steps and at convergence.
Words in thevector are ordered by probability at time step t; theword with the highest probability in the vector hasrank 1.
???
indicates that node had not yet beenreached.for semantic similarity of texts, demonstrating itsefficiency as compared to a much slower but math-ematically equivalent model based on summedsimilarity judgments of individual words.
Weshow that walks effectively aggregate informationover multiple types of links and multiple inputwords on an unsupervised paraphrase recognitiontask.
Furthermore, when used as a feature, thewalk?s semantic similarity score can improve theperformance of an existing, competitive textualentailment system.
Finally, we provide empiri-cal results demonstrating that indeed, each step ofthe random walk contributes to its ability to assessparaphrase judgments.2 A random walk exampleTo provide some intuition about the behavior ofthe random walk on text passages, consider thefollowing example sentence: I ate a salad andspaghetti.No measure based solely on lexical identitywould detect overlap between this sentence andanother input consisting of only the word food.But if each text is provided as input to the randomwalk, local relatedness links from one word to an-other allow the distributional particle to explorenearby parts of the semantic space.
The number ofnon-zero elements in both vectors increases, even-tually converging to a stationary distribution forwhich both vectors have many shared non-zero en-tries.Table 1 ranks elements of the sentence vectorbased on their relative weights.
Observe that at thebeginning of the walk, corrode has a high rank dueto its association with the WordNet sense of eatcorresponding to eating away at something.
How-ever, because this concept is not closely linkedwith other words in the sentence, its relative rankdrops as the distribution converges and other wordsenses more related to food are pushed up.
Therandom walk allows the meanings of words to re-inforce one another.
If the sentence above hadended with drank wine rather than spaghetti, thefinal weight on the food node would be smallersince fewer input words would be as closely linkedto food.
This matches the intuition that the firstsentence has more to do with food than does thesecond, although both walks should and do givesome weight to this node.3 Related workSemantic relatedness for individual words hasbeen thoroughly investigated in previous work.Budanitsky and Hirst (2006) provide an overviewof many of the knowledge-based measures derivedfrom WordNet, although other data sources havebeen used as well.
Hughes and Ramage (2007) isone such measure based on random graph walks.Prior work has considered random walks on var-ious text graphs, with applications to query expan-sion (Collins-Thompson and Callan, 2005), emailaddress resolution (Minkov and Cohen, 2007), andword-sense disambiguation (Agirre and Soroa,2009), among others.Measures of similarity have also been proposedfor sentence or paragraph length text passages.Mihalcea et al (2006) present an algorithm forthe general problem of deciding the similarity ofmeaning in two text passages, coining the name?text semantic similarity?
for the task.
Corleyand Mihalcea (2005) apply this algorithm to para-phrase recognition.Previous work has shown that similarity mea-sures can have some success as a measure of tex-tual entailment.
Glickman et al (2005) showedthat many entailment problems can be answeredusing only a bag-of-words representation and webco-occurrence statistics.
Many systems integratelexical relatedness and overlap measures withdeeper semantic and syntactic features to createimproved results upon relatedness alone, as inMontejo-R?aez et al (2007).4 Random walks on lexical graphsIn this section, we describe the mechanics ofcomputing semantic relatedness for text passages24based on the random graph walk framework.
Thealgorithm underlying these computations is relatedto topic-sensitive PageRank (Haveliwala, 2002);see Berkhin (2005) for a survey of related algo-rithms.To compute semantic relatedness for a pair ofpassages, we compare the stationary distributionsof two Markov chains, each with a state space de-fined over all lexical items in an underlying corpusor database.
Formally, we define the probability offinding the particle at a node niat time t as:n(t)i=?nj?Vn(t?1)jP (ni| nj)where P (ni| nj) is the probability of transition-ing from njto niat any time step.
If those transi-tions bias the particle to the neighborhood aroundthe words in a text, the particle?s distribution canbe used as a lexical signature.To compute relatedness for a pair of texts, wefirst define the graph nodes and transition proba-bilities for the random walk Markov chain froman underlying lexical resource.
Next, we deter-mine an initial distribution over that state space fora particular input passage of text.
Then, we sim-ulate a random walk in the state space, biased to-ward the initial distribution, resulting in a passage-specific distribution over the graph.
Finally, wecompare the resulting stationary distributions fromtwo such walks using a measure of distributionalsimilarity.
The remainder of this section discusseseach stage in more detail.4.1 Graph constructionWe construct a graph G = (V,E) with vertices Vand edges E extracted from WordNet 3.0.
Word-Net (Fellbaum, 1998) is an annotated graph ofsynsets, each representing one concept, that arepopulated by one or more words.
The set of ver-tices extracted from the graph is all synsets presentin WordNet (e.g.
foot#n#1 meaning the part ofthe human leg below the ankle), all part-of-speechtagged words participating in those synsets (e.g.foot#n linking to foot#n#1 and foot#n#2 etc.
), andall untagged words (e.g.
foot linking to foot#n andfoot#v).
The set of edges connecting synset nodesis all inter-synset edges contained in WordNet,such as hyponymy, synonomy, antonymy, etc., ex-cept for regional and usage links.
All WordNetrelational edges are given uniform weight.
Edgesalso connect each part-of-speech tagged word toall synsets it takes part in, and from each word toall its part-of-speech.
These edge weights are de-rived from corpus counts as in Hughes and Ram-age (2007).
We also included a low-weight self-loop for each node.Our graph has 420,253 nodes connected by1,064,464 edges.
Because synset nodes do not linkoutward to part-of-speech tagged nodes or wordnodes in this graph, only the 117,659 synset nodeshave non-zero probability in every random walk?i.e.
the stationary distribution will always be non-zero for these 117,659 nodes, but will be non-zerofor only a subset of the remainder.4.2 Initial distribution constructionThe next step is to seed the random walk with aninitial distribution over lexical nodes specific tothe given sentence.
To do so, we first tag the in-put sentence with parts-of-speech and lemmatizeeach word based on the finite state transducer ofMinnen et al (2001).
We search over consecu-tive words to match multi-word collocation nodesfound in the graph.
If the word or its lemma ispart of a sequence that makes a complete colloca-tion, that collocation is used.
If not, the word orits lemma with its part of speech tag is used if itis present as a graph node.
Finally, we fall backto the surface word form or underlying lemmaform without part-of-speech information if neces-sary.
For example, the input sentence: The boywent with his dog to the store, would result in massbeing assigned to underlying graph nodes boy#n,go with, he, dog#n, store#n.Term weights are set with tf.idf and then nor-malized.
Each term?s weight is proportional to thenumber of occurrences in the sentence times thelog of the number of documents in some corpusdivided by the number of documents containingthat term.
Our idf counts were derived from theEnglish Gigaword corpus 1994-1999.4.3 Computing the stationary distributionWe use the power iteration method to compute thestationary distribution for the Markov chain.
Letthe distribution over the N states at time step t ofthe random walk be denoted ~v(t)?
RN, where~v(0)is the initial distribution as defined above.
Wedenote the column-normalized state-transition ma-trix as M ?
RN?N.
We compute the stationarydistribution of the Markov chain with probability?
of returning to the initial distribution at each25time step as the limit as t??
of:~v(t)= ?~v(0)+ (1?
?
)M~v(t?1)In practice, we test for convergence by examiningif?Ni=1?v(t)i?
v(t?1)i?
< 10?6, which in our ex-periments was usually after about 50 iterations.Note that the resulting stationary distributioncan be factored as the weighted sum of the sta-tionary distributions of each word represented inthe initial distribution.
Because the initial distri-bution ~v(0)is a normalized weighted sum, it canbe re-written as ~v(0)=?k?k?
~w(0)kfor ~wkhav-ing a point mass at some underlying node in thegraph and with ?kpositive such that?k?k= 1.A simple proof by induction shows that the sta-tionary distribution ~v(?
)is itself the weighted sumof the stationary distribution of each underlyingword, i.e.
~v?=?k?k?
~w(?
)k.In practice, the stationary distribution for apassage of text can be computed from a singlespecially-constructed Markov chain.
The processis equivalent to taking the weighted sum of everyword type in the passage computed independently.Because the time needed to compute the station-ary distribution is dominated by the sparsity pat-tern of the walk?s transition matrix, the computa-tion of the stationary distribution for the passagetakes a fraction of the time needed if the station-ary distribution for each word were computed in-dependently.4.4 Comparing stationary distributionsIn order to get a final relatedness score for a pairof texts, we must compare the stationary distribu-tion from the first walk with the distribution fromthe second walk.
There exist many measures forcomputing a final similarity (or divergence) mea-sure from a pair of distributions, including geo-metric measures, information theoretic measures,and probabilistic measures.
See, for instance, theoverview of measures provided in Lee (2001).In system development on training data, wefound that most measures were reasonably effec-tive.
For the rest of this paper, we report num-bers using cosine similarity, a standard measure ininformation retrieval; Jensen-Shannon divergence,a commonly used symmetric measure based onKL-divergence; and the dice measure extended toweighted features (Curran, 2004).
A summary ofthese measures is shown in Table 2.
JustificationCosine~x?~y?~x?2?~y?2Jensen-Shannon12D(x?x+y2) +12D(y?x+y2)Dice2Pimin(xi,yi)Pixi+PiyiTable 2: Three measures of distributional similar-ity between vectors ~x and ~y used to compare thestationary distributions from passage-specific ran-dom walks.
D(p?q) is KL-divergence, defined as?ipilogpiqi.for the choice of these three measures is discussedin Section 6.5 EvaluationWe evaluate the system on two tasks that mightbenefit from semantic similarity judgments: para-phrase recognition and recognizing textual entail-ment.
A complete solution to either task will cer-tainly require tools more tuned to linguistic struc-ture; the paraphrase detection evaluation arguesthat the walk captures a useful notion of semanticsat the sentence level.
The entailment system eval-uation demonstrates that the walk score can im-prove a larger system that does make use of morefine-grained linguistic knowledge.5.1 Paraphrase recognitionThe Microsoft Research (MSR) paraphrase dataset (Dolan et al, 2004) is a collection of 5801pairs of sentences automatically collected fromnewswire over 18 months.
Each pair was hand-annotated by at least two judges with a binaryyes/no judgment as to whether one sentence wasa valid paraphrase of the other.
Annotators wereasked to judge whether the meanings of eachsentence pair were reasonably equivalent.
Inter-annotator agreement was 83%.
However, 67% ofthe pairs were judged to be paraphrases, so the cor-pus does not reflect the rarity of paraphrases in thewild.
The data set comes pre-split into 4076 train-ing pairs and 1725 test pairs.Because annotators were asked to judge if themeanings of two sentences were equivalent, theparaphrase corpus is a natural evaluation testbedfor measures of semantic similarity.
Mihalcea etal.
(2006) defines a measure of text semantic sim-ilarity and evaluates it in an unsupervised para-phrase detector on this data set.
We present their26algorithm here as a strong reference point for se-mantic similarity between text passages, based onsimilar underlying lexical resources.The Mihalcea et al (2006) algorithm is a wrap-per method that works with any underlying mea-sure of lexical similarity.
The similarity of a pairof texts T1and T2, denoted as simm(T1, T2), iscomputed as:simm(T1, T2) =12f(T1, T2) +12f(T2, T1)f(Ta, Tb) =Pw?TamaxSim(w, Tb) ?
idf(w)Pw?Taidf(w)where the maxSim(w, T ) function is defined asthe maximum similarity of the word w within thetext T as determined by an underlying measure oflexical semantic relatedness.
Here, idf(w) is de-fined as the number of documents in a backgroundcorpus divided by the number of documents con-taining the term.
maxSim compares only withinthe same WordNet part-of-speech labeling in or-der to support evaluation with lexical relatednessmeasures that cannot cross part-of-speech bound-aries.Mihalcea et al (2006) presents results for sev-eral underlying measures of lexical semantic re-latedness.
These are subdivided into corpus-basedmeasures (using Latent Semantic Analysis (Lan-dauer et al, 1998) and a pointwise-mutual infor-mation measure) and knowledge-based resourcesdriven by WordNet.
The latter include the methodsof Jiang and Conrath (1997), Lesk (1986), Resnik(1999), and others.In this unsupervised experimental setting, weconsider using only a thresholded similarity valuefrom our system and from the Mihalcea algorithmto determine the paraphrase or non-paraphrasejudgment.
For consistency with previous work, wethreshold at 0.5.
Note that this threshold could betuned on the training data in a supervised setting.Informally, we observed that on the training data athreshold of near 0.5 was often a good choice forthis task.Table 3 shows the results of our system anda representative subset of those reported in (Mi-halcea et al, 2006).
All the reported measuresfrom both systems do a reasonable job of para-phrase detection ?
the majority of pairs in the cor-pus are deemed paraphrases when the similaritymeasure is thresholded at 0.5, and indeed this isreasonable given the way in which the data wereSystem Acc.
F1: c1F1: c0Macro F1Random Graph WalkWalk (Cosine) 0.687 0.787 0.413 0.617Walk (Dice) 0.708 0.801 0.453 0.645Walk (JS) 0.688 0.805 0.225 0.609Mihalcea et.
al., Corpus-basedPMI-IR 0.699 0.810 0.301 0.625LSA 0.684 0.805 0.170 0.560Mihalcea et.
al., WordNet-basedJ&C 0.693 0.790 0.433 0.629Lesk 0.693 0.789 0.439 0.629Resnik 0.690 0.804 0.254 0.618BaselinesVector-based 0.654 0.753 0.420 0.591Random 0.513 0.578 0.425 0.518Majority (c1) 0.665 0.799 ?
0.399Table 3: System performance on 1725 examples ofthe MSR paraphrase detection test set.
Accuracy(micro-averaged F1), F1for c1?paraphrase?
andc0?non-paraphrase?
classes, and macro-averagedF1are reported.collected.
The first three rows are the perfor-mance of the similarity judgments output by ourwalk under three different distributional similar-ity measures (cosine, dice, and Jensen-Shannon),with the walk score using the dice measure outper-forming all other systems on both accuracy andmacro-averaged F1.
The output of the Mihalceasystem using a representative subset of underly-ing lexical measures is reported in the second andthird segments.
The fourth segment reports the re-sults of baseline methods?the vector space simi-larity measure is cosine similarity among vectorsusing tf.idf weighting, and the random baselinechooses uniformly at random, both as reported in(Mihalcea et al, 2006).
We add the additionalbaseline of always guessing the majority class la-bel because the data set is skewed toward ?para-phrase.
?In an unbalanced data setting, it is important toconsider more than just accuracy and F1on themajority class.
We report accuracy, F1for eachclass label, and the macro-averaged F1on all sys-tems.
F1: c0and Macro-F1are inferred for the sys-tem variants reported in (Mihalcea et al, 2006).Micro-averaged F1in this context is equivalent toaccuracy (Manning et al, 2008).Mihalcea also reports a combined classifierwhich thresholds on the simple average of the in-dividual classifiers, resulting in the highest num-bers reported in that work, with accuracy of 0.703,?paraphrase?
class F1: c1= 0.813, and inferredMacro F1= 0.648.
We believe that the scores27Data Set Cosine Dice Jensen-ShannonRTE2 dev 55.00 51.75 55.50RTE2 test 57.00 54.25 57.50RTE3 dev 59.00 57.25 59.00RTE3 test 55.75 55.75 56.75Table 4: Accuracy of entailment detection whenthresholding the text similarity score output by therandom walk.from the various walk measures might also im-prove performance when in a combination clas-sifier, but without access to the individual judg-ments in that system we are unable to evaluatethe claim directly.
However, we did create an up-per bound reference by combining the walk scoreswith easily computable simple surface statistics.We trained a support vector classifier on the MSRparaphrase training set with a feature space con-sisting of the walk score under each distributionalsimilarity measure, the length of each text, the dif-ference between those lengths, and the number ofunigram, bigram, trigram, and four-gram overlapsbetween the two texts.
The resulting classifierachieved accuracy of 0.719 with F1: c1= 0.807and F1: c0= 0.487 and Macro F1= 0.661.
Thisis a substantial improvement, roughly on the sameorder of magnitude as from switching to the bestperforming distributional similarity function.Note that the running time of the Mihalcea etal.
algorithm for comparing texts T1and T2re-quires |T1| ?
|T2| individual similarity judgments.By contrast, this work allows semantic profiles tobe constructed and evaluated for each text in a sin-gle pass, independent of the number of terms inthe texts.The performance of this unsupervised applica-tion of walks to paraphrase recognition suggeststhat the framework captures important intuitionsabout similarity in text passages.
In the next sec-tion, we examine the performance of the measureembedded in a larger system that seeks to makefine-grained entailment judgments.5.2 Textual entailmentThe Recognizing Textual Entailment Challenge(Dagan et al, 2005) is a task in which systems as-sess whether a sentence is entailed by a short pas-sage or sentence.
Participants have used a varietyof strategies beyond lexical relatedness or overlapfor the task, but some have also used only rela-tively simple similarity metrics.
Many systemsData Set Baseline Cosine Dice JSRTE2 dev 66.00 66.75 65.75 66.25RTE2 test 63.62 64.50 63.12 63.25RTE3 dev 70.25 70.50 70.62 70.38RTE3 test 65.44 65.82 65.44 65.44Table 5: Accuracy when the random walk isadded as a feature of an existing RTE system(left column) under various distance metrics (rightcolumns).incorporate a number of these strategies, so weexperimented with using the random walk to im-prove an existing RTE system.
This addresses thefact that using similarity alone to detect entailmentis impoverished: entailment is an asymmetric de-cision while similarity is necessarily symmetric.However, we also experiment with thresholdingrandom walk scores as a measure of entailment tocompare to other systems and provide a baselinefor whether the walk could be useful for entail-ment detection.We tested performance on the development andtest sets for the Second and Third PASCAL RTEChallenges (Bar-Haim et al, 2006; Giampiccoloet al, 2007).
Each of these data sets contains 800pairs of texts for which to determine entailment.In some cases, no words from a passage appearin WordNet, leading to an empty vector.
In thiscase, we use the Levenshtein string similarity mea-sure between the two texts; this fallback is used infewer than five examples in any of our data sets(Levenshtein, 1966).Table 4 shows the results of using the simi-larity measure alone to determine entailment; thesystem?s ability to recognize entailment is abovechance on all data sets.
Since the RTE data sets arebalanced, we used the median of the random walkscores for each data set as the threshold rather thanusing an absolute threshold.
While the measuredoes not outperform most RTE systems, it doesoutperform some systems that used only lexicaloverlap such as the Katrenko system from the sec-ond challenge (Bar-Haim et al, 2006).
These re-sults show that the measure is somewhat sensitiveto the distance metric chosen, and that the best dis-tance metric may vary by application.To test the random walk?s value for improv-ing an existing RTE system, we incorporated thewalk as a feature of the Stanford RTE system(Chambers et al, 2007).
This system computes28a weighted sum of a variety of features to makean entailment decision.
We added the randomwalk score as one of these features and scaled itto have a magnitude comparable to the other fea-tures; other than scaling, there was no system-specific engineering to add this feature.As shown in Table 5, adding the random walkfeature improves the original RTE system.
Thus,the random walk score provides meaningful ev-idence for detecting entailment that is not sub-sumed by other information, even in a system withseveral years of feature engineering and competi-tive performance.
In particular, this RTE systemcontains features representing the alignment scorebetween two passages; this score is composed of acombination of lexical relatedness scores betweenwords in each text.
The ability of the random walkto add value to the system even given this score,which contains many common lexical relatednessmeasures, suggests we are able to extract text sim-ilarity information that is distinct from other mea-sures.
To put the gain we achieve in perspective,an increase in the Stanford RTE system?s score ofthe same magnitude would have moved the sys-tem?s two challenge entries from 7th and 25thto 6th and 17th, respectively, in the second RTEChallenge.
It is likely the gain from this featurecould be increased by closer integration with thesystem and optimizing the initial distribution cre-ation for this task.By using the score as a feature, the system isable to take advantage of properties of the scoredistribution.
While Table 4 shows performancewhen a threshold is picked a priori, experiment-ing with that threshold increases performance byover two percent.
By lowering the threshold (clas-sifying more passages as entailments), we increaserecall of entailed pairs without losing as much pre-cision in non-entailed pairs since many have verylow scores.
As a feature, this aspect of the scoredistribution can be incorporated by the system, butit cannot be used in a simple thresholding design.6 DiscussionThe random walk framework smoothes an initialdistribution of words into a much larger lexicalspace.
In one sense, this is similar to the techniqueof query expansion used in information retrieval.A traditional query expansion model extends a bagof words (usually a query) with additional relatedwords.
In the case of pseudo-relevance feedback,Figure 1: Impact of number of walk steps on cor-relation with MSR paraphrase judgments.
Theleft column shows absolute correlation across tenresampled runs (y-axis) versus number of stepstaken (x-axis).
The right column plots the meanratio of performance at step t (x-axis) versus per-formance at convergence.29these words come from the first documents re-turned by the search engine, but other modes of se-lecting additional words exist.
In the random walkframework, this expansion is analogous to takingonly a single step of the random walk.
Indeed,in the case of the translation model introduced in(Berger and Lafferty, 1999), they are mathemati-cally equivalent.
However, we have argued that thewalk is an effective global aggregator of related-ness information.
We can formulate the questionas an empirical one?does simulating the walk un-til convergence really improve our representationof the text document?To answer this question, we extracted a 200items subset of the MSR training data and trun-cated the walk at each time step up until our con-vergence threshold was reached at around 50 it-erations.
We then evaluated the correlation ofthe walk score with the correct label from theMSR data for 10 random resamplings of 66 doc-uments each.
Figure 1 plots this result for dif-ferent distributional similarity measures.
We ob-serve that as the number of steps increases, per-formance under most of the distributional similar-ity measures improves, with the exception of theasymmetric skew-divergence measure introducedin (Lee, 2001).This plot also gives some insight into the qual-itative nature of the stability of the various distri-butional measures for the paraphrase task.
For in-stance, we observe that the Jensen-Shannon scoreand dice score tend to be the most consistent be-tween runs, but the dice score has a slightly highermean.
This explains in part why the dice score wasthe best performing measure for the task.
In con-trast, cosine similarity was observed to performpoorly here, although it was found to be the bestmeasure when combined with our textual entail-ment system.
We believe this discrepancy is duein part to the feature scaling issues described insection 5.2.7 Final remarksNotions of similarity have many levels of gran-ularity, from general metrics for lexical related-ness to application-specific measures between textpassages.
While lexical relatedness is well stud-ied, it is not directly applicable to text passageswithout some surrounding environment.
Becausethis work represents words and passages as in-terchangeable mathematical objects (teleport vec-tors), our approach holds promise as a generalframework for aggregating local relatedness infor-mation between words into reliable measures be-tween text passages.The random walk framework can be used toevaluate changes to lexical resources because itcovers the entire scope of a resource: the wholegraph is leveraged to construct the final distribu-tion, so changes to any part of the graph are re-flected in each walk.
This means that the meaning-fulness of changes in the graph can be evaluatedaccording to how they affect these text similarityscores; this provides a more semantically relevantevaluation of updates to a resource than, for ex-ample, counting how many new words or links be-tween words have been added.
As shown in Jar-masz and Szpakowicz (2003), an updated resourcemay have many more links and concepts but stillhave similar performance on applications as theoriginal.
Evaluations of WordNet extensions, suchas those in Navigli and Velardi (2005) and Snow etal.
(2006), are easily conducted within the frame-work of the random walk.The presented framework for text semantic sim-ilarity with random graph walks is more generalthan the WordNet-based instantiation exploredhere.
Transition matrices from alternative linguis-tic resources such as corpus co-occurrence statis-tics or larger knowledge bases such as Wikipediamay very well add value as a lexical resource un-derlying the walk.
One might also consider tailor-ing the output of the walk with machine learningtechniques like those presented in (Minkov andCohen, 2007).ReferencesE.
Agirre and A. Soroa.
2009.
Personalizing pagerankfor word sense disambiguation.
In EACL, Athens,Greece.R.
Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Gi-ampiccolo, B. Magnini, and I. Szpektor.
2006.
The2nd PASCAL recognizing textual entailment chal-lenge.
In PASCAL Challenges Workshop on RTE.A.
Berger and J. Lafferty.
1999.
Information retrievalas statistical translation.
SIGIR 1999, pages 222?229.P.
Berkhin.
2005.
A survey on pagerank computing.Internet Mathematics, 2(1):73?120.A.
Budanitsky and G. Hirst.
2006.
Evaluatingwordnet-based measures of lexical semantic related-ness.
Computational Linguistics, 32(1):13?47.30N.
Chambers, D. Cer, T. Grenager, D. Hall, C. Kiddon,B.
MacCartney, M. de Marneffe, D. Ramage, E. Yeh,and C. D. Manning.
2007.
Learning alignments andleveraging natural logic.
In ACL-PASCAL Workshopon Textual Entailment and Paraphrasing.K.
Collins-Thompson and J. Callan.
2005.
Query ex-pansion using random walk models.
In CIKM ?05,pages 704?711, New York, NY, USA.
ACM Press.C.
Corley and R. Mihalcea.
2005.
Measuring the se-mantic similarity of texts.
In ACL Workshop on Em-pirical Modeling of Semantic Equivalence and En-tailment, pages 13?18, Ann Arbor, Michigan, June.ACL.J.
R. Curran.
2004.
From Distributional to SemanticSimilarity.
Ph.D. thesis, University of Edinburgh.I.
Dagan, O. Glickman, and B. Magnini.
2005.The PASCAL recognizing textual entailment chal-lenge.
In Quinonero-Candela et al, editor, MLCW2005, LNAI Volume 3944, pages 177?190.
Springer-Verlag.B.
Dolan, C. Quirk, and C. Brockett.
2004.
Unsu-pervised construction of large paraphrase corpora:Exploiting massively parallel news sources.
In Col-ing 2004, pages 350?356, Geneva, Switzerland, Aug23?Aug 27.
COLING.C.
Fellbaum.
1998.
WordNet: An electronic lexicaldatabase.
MIT Press.D.
Giampiccolo, B. Magnini, I. Dagan, and B. Dolan.2007.
The 3rd PASCAL Recognizing Textual En-tailment Challenge.
In ACL-PASCAL Workshop onTextual Entailment and Paraphrasing, pages 1?9,Prague, June.O.
Glickman, I. Dagan, and M. Koppel.
2005.
Webbased probabilistic textual entailment.
In PASCALChallenges Workshop on RTE.T.
H. Haveliwala.
2002.
Topic-sensitive pagerank.
InWWW ?02, pages 517?526, New York, NY, USA.ACM.T.
Hughes and D. Ramage.
2007.
Lexical semanticrelatedness with random graph walks.
In EMNLP-CoNLL, pages 581?589.M.
Jarmasz and S. Szpakowicz.
2003.
Roget?s the-saurus and semantic similarity.
In Proceedings ofRANLP-03, pages 212?219.J.
J. Jiang and D. W. Conrath.
1997.
Semantic similar-ity based on corpus statistics and lexical taxonomy.In ROCLING X, pages 19?33.T.K.
Landauer, P.W.
Foltz, and D. Laham.
1998.
Anintroduction to latent semantic analysis.
DiscourseProcesses, 25(2-3):259?284.L.
Lee.
2001.
On the effectiveness of the skew diver-gence for statistical language analysis.
In ArtificialIntelligence and Statistics 2001, pages 65?72.M.
Lesk.
1986.
Automatic sense disambiguation us-ing machine readable dictionaries: how to tell a pinecone from an ice cream cone.
ACM SIGDOC: Pro-ceedings of the 5th Annual International Conferenceon Systems Documentation, 1986:24?26.V.
I. Levenshtein.
1966.
Binary Codes Capableof Correcting Deletions, Insertions, and Reversals.Ph.D.
thesis, Soviet Physics Doklady.C.
Manning, P. Raghavan, and H. Schutze, 2008.
In-troduction to information retrieval, pages 258?263.Cambridge University Press.R.
Mihalcea, C. Corley, and C. Strapparava.
2006.Corpus-based and knowledge-based measures oftext semantic similarity.
AAAI 2006, 6.E.
Minkov and W. W. Cohen.
2007.
Learning to ranktyped graph walks: Local and global approaches.
InWebKDD and SNA-KDD joint workshop 2007.G.
Minnen, J. Carroll, and D. Pearce.
2001.
Appliedmorphological processing of English.
Natural Lan-guage Engineering, 7(03):207?223.A.
Montejo-R?aez, J.M.
Perea, F.
Mart??nez-Santiago,M.
A.
Garc?
?a-Cumbreras, M. M. Valdivia, andA.
Ure?na L?opez.
2007.
Combining lexical-syntacticinformation with machine learning for recognizingtextual entailment.
In ACL-PASCAL Workshop onTextual Entailment and Paraphrasing, pages 78?82,Prague, June.
ACL.R.
Navigli and P. Velardi.
2005.
Structural seman-tic interconnections: A knowledge-based approachto word sense disambiguation.
IEEE Trans.
PatternAnal.
Mach.
Intell., 27(7):1075?1086.P.
Resnik.
1999.
Semantic similarity in a taxonomy:An information-based measure and its application toproblems of ambiguity in natural language.
JAIR,(11):95?130.R.
Snow, D. Jurafsky, and A. Y. Ng.
2006.
Semantictaxonomy induction from heterogenous evidence.
InACL, pages 801?808.31
