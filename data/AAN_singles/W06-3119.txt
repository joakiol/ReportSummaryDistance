Proceedings of the Workshop on Statistical Machine Translation, pages 138?141,New York City, June 2006. c?2006 Association for Computational LinguisticsSyntax Augmented Machine Translation via Chart ParsingAndreas Zollmann and Ashish VenugopalSchool of Computer ScienceCarnegie Mellon University{zollmann,ashishv}@cs.cmu.eduAbstractWe present translation results on theshared task ?Exploiting Parallel Texts forStatistical Machine Translation?
gener-ated by a chart parsing decoder operatingon phrase tables augmented and general-ized with target language syntactic cate-gories.
We use a target language parserto generate parse trees for each sentenceon the target side of the bilingual train-ing corpus, matching them with phrasetable lattices built for the correspondingsource sentence.
Considering phrases thatcorrespond to syntactic categories in theparse trees we develop techniques to aug-ment (declare a syntactically motivatedcategory for a phrase pair) and general-ize (form mixed terminal and nonterminalphrases) the phrase table into a synchro-nous bilingual grammar.
We present re-sults on the French-to-English task for thisworkshop, representing significant im-provements over the workshop?s baselinesystem.
Our translation system is avail-able open-source under the GNU GeneralPublic License.1 IntroductionRecent work in machine translation has evolvedfrom the traditional word (Brown et al, 1993) andphrase based (Koehn et al, 2003a) models to in-clude hierarchical phrase models (Chiang, 2005) andbilingual synchronous grammars (Melamed, 2004).These advances are motivated by the desire to in-tegrate richer knowledge sources within the transla-tion process with the explicit goal of producing morefluent translations in the target language.
The hi-erarchical translation operations introduced in thesemethods call for extensions to the traditional beamdecoder (Koehn et al, 2003a).
In this work weintroduce techniques to generate syntactically mo-tivated generalized phrases and discuss issues inchart parser based decoding in the statistical ma-chine translation environment.
(Chiang, 2005) generates synchronous context-free grammar (SynCFG) rules from an existingphrase translation table.
These rules can be viewedas phrase pairs with mixed lexical and non-terminalentries, where non-terminal entries (occurring aspairs in the source and target side) represent place-holders for inserting additional phrases pairs (whichagain may contain nonterminals) at decoding time.While (Chiang, 2005) uses only two nonterminalsymbols in his grammar, we introduce multiple syn-tactic categories, taking advantage of a target lan-guage parser for this information.
While (Yamadaand Knight, 2002) represent syntactical informationin the decoding process through a series of transfor-mation operations, we operate directly at the phraselevel.
In addition to the benefits that come froma more structured hierarchical rule set, we believethat these restrictions serve as a syntax driven lan-guage model that can guide the decoding process,as n-gram context based language models do in tra-ditional decoding.
In the following sections, wedescribe our phrase annotation and generalizationprocess followed by the design and pruning deci-sions in our chart parser.
We give results on theFrench-English Europarl data and conclude withprospects for future work.1382 Rule GenerationWe start with phrase translations on the paralleltraining data using the techniques and implementa-tion described in (Koehn et al, 2003a).
This phrasetable provides the purely lexical entries in the finalhierarchical rule set that will be used in decoding.We then use Charniak?s parser (Charniak, 2000) togenerate the most likely parse tree for each Eng-lish target sentence in the training corpus.
Next,we determine all phrase pairs in the phrase tablewhose source and target side occur in each respec-tive source and target sentence pair defining thescope of the initial rules in our SynCFG.Annotation If the target side of any of these ini-tial rules correspond to a syntactic category C of thetarget side parse tree, we label the phrase pair withthat syntactic category.
This label corresponds to theleft-hand side of our synchronous grammar.
Phrasepairs that do not correspond to a span in the parsetree are given a default category ?X?, and can stillplay a role in the decoding process.
In work done af-ter submission to the 2006 data track, we assign suchphrases an extended category of the form C1 + C2,C1/C2, or C2\C1, indicating that the phrase pair?starget side spans two adjacent syntactic categories(e.g., she went: NP+V), a partial syntactic cate-gory C1 missing a C2 to the right (e.g., the great:NP/NN), or a partial C1 missing a C2 to the left (e.g.,great wall: DT\NP), respectively.Generalization In order to mitigate the effectsof sparse data when working with phrase and n-gram models we would like to generate generalizedphrases, which include non-terminal symbols thatcan be filled with other phrases.
Therefore, afterannotating the initial rules from the current train-ing sentence pair, we adhere to (Chiang, 2005) torecursively generalize each existing rule; however,we abstract on a per-sentence basis.
The grammarextracted from this evaluation?s training data con-tains 75 nonterminals in our standard system, and4000 nonterminals in the extended-category system.Figure 1 illustrates the annotation and generalizationprocess.NP->@DT session/DT sessionS -> reprise de @NP/resumptionof @NPNP->lasession/thesessionX -> reprise de/resumptionofN->session/sessionDT->la/theIN->de/ofN->reprise/resumptionreprisedelasessionS -> [NP (N resumption) ][PP (IN of)][NP [ (DT the) (N session)]Figure 1: Selected annotated and generalized (dotted arc)rules for the first sentence of Europarl.3 ScoringWe employ a log-linear model to assign costs to theSynCFG.
Given a source sentence f , the preferredtranslation output is determined by computing thelowest-cost derivation (combination of hierarchicaland glue rules) yielding f as its source side, wherethe cost of a derivation R1 ?
?
?
?
?Rn with respectivefeature vectors v1, .
.
.
, vn ?
Rm is given bym?i=1?in?j=1(vj)i .Here, ?1, .
.
.
, ?m are the parameters of the log-linear model, which we optimize on a held-out por-tion of the training set (2005 development data) us-ing minimum-error-rate training (Och, 2003).
Weuse the following features for our rules:?
source- and target-conditioned neg-log lexicalweights as described in (Koehn et al, 2003b)?
neg-log relative frequencies: left-hand-side-conditioned, target-phrase-conditioned,source-phrase-conditioned?
Counters: n.o.
rule applications, n.o.
targetwords?
Flags: IsPurelyLexical (i.e., contains only ter-minals), IsPurelyAbstract (i.e., contains onlynonterminals), IsXRule (i.e., non-syntacticalspan), IsGlueRule139?
Penalties: rareness penalty exp(1 ?RuleFrequency); unbalancedness penalty|MeanTargetSourceRatio ?
?n.o.
source words???n.o.
target words?|4 ParsingOur SynCFG rules are equivalent to a probabilisticcontext-free grammar and decoding is therefore anapplication of chart parsing.
Instead of the commonmethod of converting the CFG grammar into Chom-sky Normal Form and applying a CKY algorithmto produce the most likely parse for a given sourcesentence, we avoided the explosion of the rule setcaused by the introduction of new non-terminals inthe conversion process and implemented a variantof the CKY+ algorithm as described in (J.Earley,1970).Each cell of the parsing process in (J.Earley,1970) contains a set of hypergraph nodes (Huangand Chiang, 2005).
A hypergraph node is an equiv-alence class of complete hypotheses (derivations)with identical production results (left-hand sides ofthe corresponding applied rules).
Complete hy-potheses point directly to nodes in their backwardsstar, and the cost of the complete hypothesis is cal-culated with respect to each back pointer node?s bestcost.This structure affords efficient parsing with mini-mal pruning (we use a single parameter to restrict thenumber of hierarchical rules applied), but sacrificeseffective management of unique language modelstates contributing to significant search errors dur-ing parsing.
At initial submission time we simplyre-scored a K-Best list extracted after first best pars-ing using the lazy retrieval process in (Huang andChiang, 2005).Post-submission After our workshop submission,we modified the K-Best list extraction process to in-tegrate an n-gram language model during K-Best ex-traction.
Instead of expanding each derivation (com-plete hypothesis) in a breadth-first fashion, we ex-pand only a single back pointer, and score this newderivation with its translation model scores and alanguage model cost estimate, consisting of an ac-curate component, based on the words translated sofar, and an estimate based on each remaining (notexpanded) back pointer?s top scoring hypothesis.To improve the diversity of the final K-Best list,we keep track of partially expanded hypotheses thathave generated identical target words and refer to thesame hypergraph nodes.
Any arising twin hypothe-sis is immediately removed from the K-Best extrac-tion beam during the expansion process.5 ResultsWe present results that compare our system againstthe baseline Pharaoh implementation (Koehn et al,2003a) and MER training scripts provided for thisworkshop.
Our results represent work done beforethe submission due date as well as after with the fol-lowing generalized phrase systems.?
Baseline - Pharaoh with phrases extracted fromIBM Model 4 training with maximum phraselength 7 and extraction method ?diag-growth-final?
(Koehn et al, 2003a)?
Lex - Phrase-decoder simulation: using onlythe initial lexical rules from the phrase table,all with LHS X , the Glue rule, and a binaryreordering rule with its own reordering-feature?
XCat - All nonterminals merged into a singleX nonterminal: simulation of the system Hiero(Chiang, 2005).?
Syn - Syntactic extraction using the Penn Tree-bank parse categories as nonterminals; rulescontaining up to 4 nonterminal abstractionsites.?
SynExt - Syntactic extraction using theextended-category scheme, but with rules onlycontaining up to 2 nonterminal abstractionsites.We also explored the impact of longer initialphrases by training another phrase table with phrasesup to length 12.
Our results are presented in Ta-ble 1.
While our submission time system (Syn usingLM for rescoring only) shows no improvement overthe baseline, we clearly see the impact of integratingthe language model into the K-Best list extractionprocess.
Our final system shows at statistically sig-nificant improvement over the baseline (0.78 BLEUpoints is the 95 confidence level).
We also see atrend towards improving translation quality as we140System Dev: w/o LM Dev: LM-rescoring Test: LM-r. Dev: integrated LM Test: int.
LMBaseline - max.
phr.
length 7 ?
?
?
31.11 30.61Lex - max.
phrase length 7 27.94 29.39 29.95 28.96 29.12XCat - max.
phrase length 7 27.56 30.27 29.81 30.89 31.01Syn - max.
phrase length 7 29.20 30.95 30.58 31.52 31.31SynExt - max.
phrase length 7 ?
?
?
31.73 31.41Baseline - max.
phr.
length 12 ?
?
?
31.16 30.90Lex - max.
phr.
length 12 ?
?
?
29.30 29.51XCat - max.
phr.
length 12 ?
?
?
30.79 30.59SynExt - max.
phr.
length 12 ?
?
?
31.07 31.76Table 1: Translation results (IBM BLEU) for each system on the Fr-En ?06 Shared Task ?Development Set?
(used for MERparameter tuning) and ?06 ?Development Test Set?
(identical to last year?s Shared Task?s test set).
The system submitted forevaluation is highlighted in bold.employ richer extraction techniques.
The relativelypoor performance of Lex with LM in K-Best com-pared to the baseline shows that we are still makingsearch errors during parsing despite tighter integra-tion of the language model.We also ran an experiment with CMU?s phrase-based decoder (Vogel et al, 2003) using the length-7 phrase table.
While its development-set score wasonly 31.01, the decoder achieved 31.42 on the testset, placing it at the same level as our extended-category system for that phrase table.6 ConclusionsIn this work we applied syntax based resources(the target language parser) to annotate and gener-alize phrase translation tables extracted via exist-ing phrase extraction techniques.
Our work reaf-firms the feasibility of parsing approaches to ma-chine translation in a large data setting, and il-lustrates the impact of adding syntactic categoriesto drive and constrain the structured search space.While no improvements were available at submis-sion time, our subsequent performance highlightsthe importance of tight integration of n-gram lan-guage modeling within the syntax driven parsing en-vironment.
Our translation system is available open-source under the GNU General Public License at:www.cs.cmu.edu/?zollmann/samtReferencesPeter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Comput.
Linguist., 19(2):263?311.Eugene Charniak.
2000.
A maximum entropy-inspiredparser.
In Proceedings of the North American Associ-ation for Computational Linguistics (HLT/NAACL).David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
of the As-sociation for Computational Linguistics.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the 9th International Work-shop on Parsing Technologies.J.Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the Assocation for Com-puting Machinery, 13(2):94?102.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003a.
Pharaoh: A beam search decoder for phrase-base statistical machine translation models.
In Pro-ceedings of the Sixth Confernence of the Associationfor Machine Translation in the Americas, Edomonton,Canada, May 27-June 1.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003b.
Statistical phrase-based translation.
InProceedings of the Human Language Technologyand North American Association for ComputationalLinguistics Conference (HLT/NAACL), Edomonton,Canada, May 27-June 1.I.
Dan Melamed.
2004.
Statistical machine translationby parsing.
In ACL, pages 653?660.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of the Associ-ation for Computational Linguistics, Sapporo, Japan,July 6-7.Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-ble, Ashish Venogupal, Bing Zhao, and Alex Waibel.2003.
The CMU statistical translation system.
In Pro-ceedings of MT Summit IX, New Orleans, LA, Septem-ber.Kenji Yamada and Kevin Knight.
2002.
A decoder forsyntax-based statistical mt.
In Proc.
of the Associationfor Computational Linguistics.141
