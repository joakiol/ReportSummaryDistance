2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 602?606,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsPredicting Responses to Microblog PostsYoav Artzi ?Computer Science & EngineeringUniversity of WashingtonSeattle, WA, USAyoav@cs.washington.eduPatrick Pantel, Michael GamonMicrosoft ResearchOne Microsoft WayRedmond, WA, USA{ppantel,mgamon}@microsoft.comAbstractMicroblogging networks serve as vehicles forreaching and influencing users.
Predictingwhether a message will elicit a user responseopens the possibility of maximizing the viral-ity, reach and effectiveness of messages andad campaigns on these networks.
We proposea discriminative model for predicting the like-lihood of a response or a retweet on the Twit-ter network.
The approach uses features de-rived from various sources, such as the lan-guage used in the tweet, the user?s social net-work and history.
The feature design processleverages aggregate statistics over the entiresocial network to balance sparsity and infor-mativeness.
We use real-world tweets to trainmodels and empirically show that they are ca-pable of generating accurate predictions for alarge number of tweets.1 IntroductionMicroblogging networks are increasingly evolvinginto broadcasting networks with strong social as-pects.
The most popular network today, Twitter, re-ported routing 200 million tweets (status posts) perday in mid-2011.
As the network is increasinglyused as a channel for reaching out and marketingto its users, content generators aim to maximize theimpact of their messages, an inherently challeng-ing task.
However, unlike for conventionally pro-duced news, Twitter?s public network allows one toobserve how messages are reaching and influencingusers.
One such direct measure of impact are mes-sage responses.?
This work was conducted at Microsoft Research.In this work, we describe methods to predict if agiven tweet will elicit a response.
Twitter providestwo methods to respond to messages: replies andretweets (re-posting of a message to one?s follow-ers).
Responses thus serve both as a measure of dis-tribution and as a way to increase it.
Being able topredict responses is valuable for any content gener-ator, including advertisers and celebrities, who useTwitter to increase their exposure and maintain theirbrand.
Furthermore, this prediction ability can beused for ranking, allowing the creation of better op-timized news feeds.To predict if a tweet will receive a response priorto its posting we use features of the individual tweettogether with features aggregated over the entire so-cial network.
These features, in combination withhistorical activity, are used to train a predictionmodel.2 Related WorkThe public nature of Twitter and the unique char-acteristics of its content have made it an attractiveresearch topic over recent years.
Related work canbe divided into several types:Twitter Demographics One of the most fertile av-enues of research is modeling users and their inter-actions on Twitter.
An extensive line of work char-acterizes users (Pear Analytics, 2009) and quantifiesuser influence (Cha et al, 2010; Romero et al, 2011;Wu et al, 2011; Bakshy et al, 2011).
Popescu andJain (2011) explored how businesses use Twitter toconnect with their customer base.
Popescu and Pen-nacchiotti (2011) and Qu et al (2011) investigated602how users react to events on social media.
Therealso has been extensive work on modeling conver-sational interactions on Twitter (Honeycutt and Her-ring, 2009; Boyd et al, 2010; Ritter et al, 2010;Danescu-Niculescu-Mizil et al, 2011).
Our workbuilds on these findings to predict response behavioron a large scale.Mining Twitter Social media has been used to de-tect events (Sakaki et al, 2010; Popescu and Pennac-chiotti, 2010; Popescu et al, 2011), and even predicttheir outcomes (Asur and Huberman, 2010; Culotta,2010).
Similarly to this line of work, we mine thesocial network for event prediction.
In contrast, ourfocus is on predicting events within the network.Response Prediction There has been significantwork addressing the task of response prediction innews articles (Tsagkias et al, 2009; Tsagkias et al,2010) and blogs (Yano et al, 2009; Yano and Smith,2010; Balasubramanyan et al, 2011).
The task ofpredicting responses in social networks has been in-vestigated previously: Hong et al (2011) focusedon predicting responses for highly popular items,Rowe et al (2011) targeted the prediction of con-versations and their length and Suh et al (2010) pre-dicted retweets.
In contrast, our work targets tweetsregardless of their popularity and attempts to predictboth replies and retweets.
Furthermore, we presenta scalable method to use linguistic lexical features indiscriminative models by leveraging global networkstatistics.
A related task to ours is that of responsegeneration, as explored by Ritter et al (2011).
Ourwork complements their approach by allowing todetect when the generation of a response is appro-priate.
Lastly, the task of predicting the spread ofhashtags in microblogging networks (Tsur and Rap-poport, 2012) is also closely related to our work andboth approaches supplement each other as measuresof impact.Ranking in News Feeds Different approacheswere suggested for ranking items in social media(Das Sarma et al, 2010; Lakkaraju et al, 2011).
Ourwork provides an important signal, which can be in-corporated into any ranking approach.3 Response Prediction on TwitterOur goal is to learn a function f that maps a tweetx to a binary value y ?
{0, 1}, where y indicates ifx will receive a response.
In this work we make nodistinction between different kinds of responses.In addition to x, we assume access to a social net-work S, which we view as a directed graph ?U,E?.The set of vertices U represents the set of users.
Foreach u?, u??
?
U , ?u?, u???
?
E if and only if thereexists a following relationship from u?
to u?
?.For the purpose of defining features we denote xtas the text of the tweet x and xu ?
U the user whoposted x.
For training we assume access to a set ofn labeled examples {?xi, yi?
: i = 1 .
.
.
n}, wherethe label indicates whether the tweet has received aresponse or not.3.1 FeaturesFor prediction we represent a given tweet x using sixfeature families:Historical Features Historical behavior is oftenstrong evidence of future trends.
To account for thisinformation, we compute the following features: ra-tio of tweets by xu that received a reply, ratio oftweets by xu that were retweeted and ratio of tweetsby xu that received both a reply and retweet.Social Features The immediate audience of a userxu is his followers.
Therefore, incorporating socialfeatures into our model is likely to contribute to itsprediction ability.
For a user xu ?
U we includefeatures for the number of followers (indegree in S),the number of users xu follows (outdegree in S) andthe ratio between the two.Aggregate Lexical Features To detect lexicalitems that trigger certain response behavior we de-fine features for all bigrams and hashtags in our setof tweets.
To avoid sparsity and maintain a manage-able feature space we compress the features usingthe labels: for each lexical item l we define Rl tobe the set of tweets that include l and received a re-sponse, and Nl to be the set of tweets that contain land received no response.
We then define the inte-ger n to be the rounding of |Rl||Nl| to the nearest integer.For each such integer we define a feature, which weincrease by 1 when the lexical item l is present in xt.603We use this process separately for bigrams and hash-tags, creating separate sets of aggregate features.Local Content Features We introduce 45 featuresto capture how the content of xt influences responsebehavior, including features such as the number ofstop words and the percentage of English words.
Inaddition we include features specific to Twitter, suchas the number of hash tags and user references.Posting Features Past analysis of Twitter showedthat posting time influences response potential (PearAnalytics, 2009).
To examine temporal influences,we include features to account for the user?s localtime and day of the week when x was created.Sentiment Features To measure how sentimentinfluences response behavior we define features thatcount the number of positive and negative sentimentwords in xt.
To detect sentiment words we use a pro-prietary Microsoft lexicon of 7K positive and nega-tive terms.4 Evaluation4.1 Learning AlgorithmWe experimented with two different learning al-gorithms: Multiple Additive Regression-Trees(MART) (Wu et al, 2008) and a maximum entropyclassifier (Berger et al, 1996).
Both provide fastclassification, a natural requirement for large-scalereal-time tasks.4.2 DatasetIn our evaluation we focus on English tweets only.Since we use local posting time in our features, wefiltered users whose profile did not contain locationinformation.
To collect Tweeter messages we usedthe entire public feed of Twitter (often referred to asthe Twitter Firehose).
We randomly sampled 943Ktweets from one week of data.
We allowed an ex-tra week for responses, giving a response windowof two weeks.
The majority of tweets in our set(90%) received no response.
We used 750K tweetsfor training and 188K for evaluation.
A separate dataset served as a development set.
For the computationof aggregate lexical features we used 186M tweetsfrom the same week, resulting in 14M bigrams and400K hash tags.
To compute historical features, wesampled 2B tweets from the previous three months.Figure 1: Precision-recall curves for predicting that atweet will get a response.
The marked area highlightsthe area of the curve we focus on in our evaluation.Figure 2: Precision-recall curves with increasing numberof features removed for the marked area in Figure 1.
Foreach curve we removed one additional feature set fromthe one above it.4.3 ResultsOur evaluation focuses on precision-recall curvesfor predicting that a given tweet will get a response.The curves were generated by varying the confi-dence measure threshold, which both classifiers pro-vided.
As can be seen in Figure 1, MART outper-forms the maximum entropy model.
We can also seethat it is hard to predict response behavior for mosttweets, but for a large subset we can provide a rela-tively accurate prediction (highlighted in Figure 1).The rest of our analysis focuses on this subset andon results based on MART.To better understand the contribution of each fea-ture set, we removed features in a greedy manner.After learning a model and testing it, we removedthe feature family that was overall most highlyranked by MART (i.e., was used in high-level splitsin the decision trees) and learned a new model.
Fig-ure 2 shows how removing feature sets degrades pre-diction performance.
Removing historical featureslowers the model?s prediction abilities, although pre-diction quality remains relatively high.
Removingsocial features creates a bigger drop in performance.Lastly, removing aggregate lexical features and lo-604cal content features further decreases performance.At this point, removing posting time features is notinfluential.
Following the removal of posting timefeatures, the model includes only sentiment features.5 Discussion and ConclusionThe first trend seen by removing features is that localcontent matters less, or at least is more complex tocapture and use for response prediction.
Despite theinfluence of chronological trends on posting behav-ior on Twitter (Pear Analytics, 2009), we were un-able to show influence of posting time on responseprediction.
Historical features were the most promi-nent in our experiments.
Second were social fea-tures, showing that developing one?s network is crit-ical for impact.
The third most prominent set of fea-tures, aggregate lexical features, shows that users aresensitive to certain expressions and terms that tendto trigger responses.The natural path for future work is to improve per-formance using new features.
These may includeclique-specific language features, more properties ofthe user?s social network, mentions of named enti-ties and topics of tweets.
Another direction is to dis-tinguish between replies and retweets and to predictthe number of responses and the length of conversa-tions that a tweet may generate.
There is also po-tential in learning models for the prediction of othermeasures of impact, such as hashtag adoption andinclusion in ?favorites?
lists.AcknowledgmentsWe would like to thank Alan Ritter, Bill Dolan,Chris Brocket and Luke Zettlemoyer for their sug-gestions and comments.
We wish to thank ChrisQuirk and Qiang Wu for providing us with accessto their learning software.
Thanks to the reviewersfor the helpful comments.ReferencesS.
Asur and B.A.
Huberman.
2010.
Predicting the futurewith social media.
In Proceedings of the InternationalConference on Web Intelligence and Intelligent AgentTechnology.E.
Bakshy, J. M. Hofman, W. A. Mason, and D. J. Watts.2011.
Everyone?s an influencer: quantifying influenceon twitter.
In Peoceedings of the ACM InternationalConference on Web Search and Data Mining.R.
Balasubramanyan, W.W. Cohen, D. Pierce, and D.P.Redlawsk.
2011.
What pushes their buttons?
predict-ing comment polarity from the content of political blogposts.
In Proceedings of the Workshop on Language inSocial Media.Adam L. Berger, Vincent J. Della Pietra, and Stephen A.Della Pietra.
1996.
A maximum entropy approach tonatural language processing.
Computational Linguis-tics.D.
Boyd, S. Golder, and G. Lotan.
2010.
Tweet, tweet,retweet: Conversational aspects of retweeting on twit-ter.
In Proceedings of the International Conference onSystem Sciences.M.
Cha, H. Haddadi, F. Benevenuto, and K.P.
Gummadi.2010.
Measuring user influence in twitter: The millionfollower fallacy.
In Proceedings of the InternationalAAAI Conference on Weblogs and Social Media.A.
Culotta.
2010.
Towards detecting influenza epidemicsby analyzing twitter messages.
In Proceedings of theWorkshop on Social Media Analytics.C.
Danescu-Niculescu-Mizil, M. Gamon, and S. Dumais.2011.
Mark my words!
: linguistic style accommoda-tion in social media.
In Proceedings of the Interna-tional Conference on World Wide Web.A.
Das Sarma, A. Das Sarma, S. Gollapudi, and R. Pan-igrahy.
2010.
Ranking mechanisms in twitter-like fo-rums.
In Proceedings of the ACM International Con-ference on Web Search and Data Mining.C.
Honeycutt and S.C.
Herring.
2009.
Beyond mi-croblogging: Conversation and collaboration via twit-ter.
In Proceedings of the International Conference onSystem Sciences.L.
Hong, O. Dan, and B. D. Davison.
2011.
Predict-ing popular messages in twitter.
In Proceedings of theInternational Conference on World Wide Web.H.
Lakkaraju, A. Rai, and S. Merugu.
2011.
Smartnews feeds for social networks using scalable joint la-tent factor models.
In Proceedings of the InternationalConference on World Wide Web.Pear Analytics.
2009.
Twitter study.A.M.
Popescu and A. Jain.
2011.
Understanding thefunctions of business accounts on twitter.
In Proceed-ings of the International Conference on World WideWeb.A.M.
Popescu and M. Pennacchiotti.
2010.
Detect-ing controversial events from twitter.
In Proceedingsof the International Conference on Information andKnowledge Management.A.M.
Popescu and M. Pennacchiotti.
2011.
Dancingwith the stars, nba games, politics: An exploration oftwitter users response to events.
In Proceedings of the605International AAAI Conference on Weblogs and SocialMedia.A.M.
Popescu, M. Pennacchiotti, and D. Paranjpe.
2011.Extracting events and event descriptions from twit-ter.
In Proceedings of the International Conferenceon World Wide Web.Y.
Qu, C. Huang, P. Zhang, and J. Zhang.
2011.
Mi-croblogging after a major disaster in china: a casestudy of the 2010 yushu earthquake.
In Proceedingsof the ACM Conference on Computer Supported Co-operative Work.A.
Ritter, C. Cherry, and B. Dolan.
2010.
Unsupervisedmodeling of twitter conversations.
In Proceedings ofthe Annual Conference of the North American Chapterof the Association for Computational Linguistics.A.
Ritter, C. Cherry, and B. Dolan.
2011.
Data-drivenresponse generation in social media.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.D.
Romero, W. Galuba, S. Asur, and B. Huberman.
2011.Influence and passivity in social media.
MachineLearning and Knowledge Discovery in Databases,pages 18?33.M.
Rowe, S. Angeletou, and H. Alani.
2011.
Predictingdiscussions on the social semantic web.
In Proceed-ings of the Extended Semantic Web Conference.T.
Sakaki, M. Okazaki, and Y. Matsuo.
2010.
Earth-quake shakes twitter users: real-time event detectionby social sensors.
In Proceedings of the InternationalConference on World Wide Web.B.
Suh, L. Hong, P. Pirolli, and E. H. Chi.
2010.
Want tobe retweeted?
large scale analytics on factors impact-ing retweet in twitter network.
In Proceedings of theIEEE International Conference on Social Computing.M.
Tsagkias, W. Weerkamp, and M. De Rijke.
2009.Predicting the volume of comments on online newsstories.
In Proceedings of the ACM Conference on In-formation and Knowledge Management.M.
Tsagkias, W. Weerkamp, and M. De Rijke.
2010.News comments: Exploring, modeling, and onlineprediction.
Advances in Information Retrieval, pages191?203.O.
Tsur and A. Rappoport.
2012.
What?s in a hash-tag?
: content based prediction of the spread of ideasin microblogging communities.
In Proceedings of theACM International Conference on Web Search andData Mining.Q.
Wu, C.J.C.
Burges, K.M.
Svore, and J. Gao.
2008.Ranking, boosting, and model adaptation.
TecnicalReport, MSR-TR-2008-109.S.
Wu, J.M.
Hofman, W.A.
Mason, and D.J.
Watts.
2011.Who says what to whom on twitter.
In Proceedings ofthe International Conference on World Wide Web.T.
Yano and N.A.
Smith.
2010.
Whats worthy of com-ment?
content and comment volume in political blogs.Proceedings of the International AAAI Conference onWeblogs and Social Media.T.
Yano, W.W. Cohen, and N.A.
Smith.
2009.
Predict-ing response to political blog posts with topic mod-els.
In Proceedings of the Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics.606
