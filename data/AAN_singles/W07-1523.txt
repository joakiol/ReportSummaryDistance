Proceedings of the Linguistic Annotation Workshop, pages 140?147,Prague, June 2007. c?2007 Association for Computational LinguisticsWeb-based Annotation of Anaphoric Relations and Lexical ChainsMaik St?hrenberg and Daniela Goecke and Nils Diewald and Alexander MehlerBielefeld UniversityGermany{maik.stuehrenberg|daniela.goecke|nils.diewald|alexander.mehler}@uni-bielefeld.deIrene CramerDortmund UniversityGermanyirene.cramer@uni-dortmund.deAbstractAnnotating large text corpora is a time-consuming effort.
Although single-user an-notation tools are available, web-based an-notation applications allow for distributedannotation and file access from different lo-cations.
In this paper we present the web-based annotation application Serengeti forannotating anaphoric relations which will beextended for the annotation of lexical chains.1 IntroductionThe relevance of corpus work for different tasks inthe fields of linguistics is widely accepted.
Thisholds especially for the area of (semi-)automatictext and discourse analysis which demands referencecorpora in which instances of various levels of dis-course structure have been annotated.
Such anno-tation tasks are typically carried out by a combina-tion of automatic and manual techniques.
Manualannotation of large text corpora is a time consum-ing effort.
Therefore, annotation tools are an indis-pensable means to overcome the limits of manualannotations.
In spite of their limited level of au-tomatization, such tools nevertheless help to semi-automatically support the annotation process and tosecure consistency of manual annotations.
This pa-per describes such an annotation tool which focuseson a certain type of discourse structures.
Morespecifically, we deal with anaphoric relations andlexical cohesion.
Our starting point is the obser-vation that these two resources of textual cohesion(Halliday and Hasan, 1976) homogeneously inducechain-like discourse structures: one the one hand wehave reference chains started by some antecedenceand continued by some anaphora linked to the sameantecedence.
On the other hand, lexical cohesiongenerates so called lexical chains of semanticallyrelated tokens.
Based on this observation we de-scribe the annotation tool Serengeti which reflectsthis structural homogeneity on the level of its struc-tural representation model as well as by its proce-dural annotation model.
Serengeti includes an an-notation scheme which is extended in order to sup-port the annotation of reference chains and lexicalchains.
The paper is organized as follows: Section2.1 describes the application scenario of anaphoricrelations and the scheme we use to annotate them.Section 2.2 deals with the second application sce-nario: lexical chains.
As our starting point was theformer scenario, its extension to the latter one will bemotivated by a separate case study of lexical chain-ing.
Section 3 refers to related work, while Section4 describes our annotation tool in detail.
Finally, theapplication of Serengeti to annotating lexical chainsis described in Section 5.2 Annotating Large Text CorporaThe main focus of the joint work presented in thispaper1 is text technological information modellingand analysis of various types of discourse.
Withinour research group we deal with the integration of1The work presented in this paper is a joint ef-fort of the projects A2, A4 and B1 of the ResearchGroup Text-technological modelling of information fundedby the German Research Foundation.
See http://www.text-technology.de for further details.140heterogeneous linguistic resources.
This applies es-pecially to the Sekimo project (A2) which focusseson the application domain of anaphora resolution.We use the term ?heterogeneity?
to refer to resourcesthat differ either in terms of form (text, audio, video)or in terms of function (e. g. lexicons, annotatedtexts).
Connection between these resources canbe established with the means of XML, cf.
Si-mons (2004).
Integrating resources via an abstractinterface is necessary due to different reasons: Theresources used have often been developed indepen-dently from each other and a cascaded applicationof one resource to the output of another resource isnot always possible.
Furthermore, the output of dif-ferent resources often cannot be encoded in a singlestructure without driving into incompatibilites (i. e.XML overlap).
Therefore an architecture was devel-oped which allows for the combination of the out-put structures of several linguistic resources into asingle XML annotated document and which is de-scribed in detail in Witt et al (2005) and St?hren-berg et al (2006) .2.1 Anaphoric RelationsMotivation and Background Resolving anapho-ric relations needs a variety of different informa-tion (e. g. POS, distance information, grammati-cal function, semantic knowledge, see, for exam-ple, Mitkov (2002) for an overview).
Several re-sources are applied to a corpus of 47 texts and theoutput structures are combined into a single XMLdocument using the architecture mentioned above.In order not only to integrate but also evaluate re-sources for a given linguistic task formally in termsof precision and recall, it should be possible to ei-ther switch on or switch off a given resource.
Inthe application domain of anaphora resolution eval-uation is done as follows.
Each discourse entityor referent (cf.
Karttunen (1976)) is annotated asan XML element which holds a variety of attributeinformation.
Each XML element is reinterpretedas a feature vector; pairs of discourse entities be-tween which an anaphoric relation holds form a sin-gle feature vector with additional information rele-vant for anaphora resolution (e. g. distance informa-tion, identity of grammatical form, semantic relat-edness of underlying lemmata and the like).
In or-der to evaluate different resource settings, decisiontrees with varying sets of feature vectors are usedfor the process of anaphora resolution.
Xiaofeng etal.
(2004) or Strube and M?ller (2003) have shownthe feasibility of decision trees for the domain ofanaphora resolution; we have chosen this approachas it makes it possible to easily switch the informa-tion set for training and evaluation as opposed to e. g.rewriting rule sets.
Both, training and evaluation aswell as empirically based analysis of anaphora needan annotated reference corpus (Poesio et al, 2002).Scheme and annotation process are described in thefollowing section.The Annotation Scheme for Anaphoric Rela-tions Several annotation schemes for annotat-ing anaphoric relations have been developed inthe last years, e. g. the UCREL anaphora an-notation scheme (Fligelstone, 1992; Garside etal., 1997), the SGML-based MUC annotationscheme (Hirschmann, 1997), and the MATE/G-NOME Scheme (Poesio, 2004), amongst others.In order to annotate discourse relations ?
eitheranaphoric relations or lexical chains (cf.
Sec-tion 2.2) ?
two types of information have to be spec-ified.
First, the markables, i. e. the elements that canbe part of a relation, have to be specified (cf.
M?llerand Strube (2003)).
Second, the relation(s) betweenmarkables and their respective types and subtypeshave to be defined.
The markables form a basis forthe annotation process and therefore have to be an-notated in advance.
Normally, for a domain underinvestigation, elements are denoted as being mark-ables either via a specific element or via the use ofa universal attribute.
In our system, discourse enti-ties are detected automatically on the basis of POSand parsing information.
The annotation schemefor annotating anaphoric relations is an extensionof the scheme presented by Holler et al (2004) thathas been developed for annotations in the context oftext-to-hypertext conversion in the project B1 Hy-Tex.
We adopt the distinction between coreferenceand cospecification but we extend the annotationscheme for an explicit distinction between cospec-ification (direct anaphora) and bridging (associativeor indirect anaphora).
Thus, we add the primary re-lation type bridgingLink (denoting bridging) to thealready existing one (cospecLink).
Each primaryrelation type includes different secondary relation141Listing 1: The annotation format for anaphoric relations.
Shortened and manually revised output1 <chs:chs>2 <chs:text>3 <cnx:de deID="de8" deType="namedEntity" headRef="w36">4 <cnx:token ref="w36">Maik</cnx:token></cnx:de>5 <cnx:token ref="w37">hat</cnx:token> <cnx:token ref="w38">kein</cnx:token>6 <cnx:token ref="w39">eigenes</cnx:token> <cnx:token ref="w40">Fahrrad</cnx:token>,7 <cnx:token ref="w42">und</cnx:token>8 <cnx:de deID="de10" deType="namedEntity" headRef="w43">9 <cnx:token ref="w43">Marie</cnx:token></cnx:de>10 <cnx:token ref="w45">f?hrt</cnx:token> <cnx:token ref="w46">nicht</cnx:token>11 <cnx:token ref="w47">in</cnx:token>12 <cnx:de deID="de11" deType="nom" headRef="w49">13 <cnx:token ref="w48">den</cnx:token>14 <cnx:token ref="w49">Urlaub</cnx:token></cnx:de>.15 <cnx:de deID="de12" deType="nom" headRef="w53">16 <cnx:token ref="w52">Zwei</cnx:token>17 <cnx:token ref="w53">Kinder</cnx:token></cnx:de>,18 <cnx:de deID="de13" deType="nom" headRef="w56">19 <cnx:token ref="w55">eine</cnx:token>20 <cnx:token ref="w56">Gemeinsamkeit</cnx:token></cnx:de>:21 </chs:text>22 <cnx:token_ref id="w36" head="w37" pos="N" syn="@NH" depV="subj" morph="MSC SG NOM" />23 <chs:semRel>24 <chs:bridgingLink relType="hasMember" antecedentIDRefs="de8 de10" phorIDRef="de12"/>25 </chs:semRel>26 </chs:chs>types that specify the subtype of the relation, e. g.ident or hypernym as secondary types of cospecLinkor meronym or setMember as secondary types ofbridgingLink.
An example annotation of an indirectanaphoric relation (element bridgingLink, line30) between the discourse entities de12 (lines 18 to21) and de8 (lines 3 to 5) and de10 (lines 9 to 11)can be seen in Listing 1.2.2 Lexical ChainingMotivation and Background Based on the con-cept of lexical cohesion (Halliday and Hasan,1976), computational linguists (inter alia Morris andHirst (1991)) developed a method to compute a par-tial text representation: lexical chains.
These spanover passages or even the complete text linking lex-ical items.
The exemplary annotation in Figure 1illustrates that lexical chaining is achieved by theselection of vocabulary and significantly accountsfor the cohesive structure of a text passage.
Itemsin a lexical chain are connected via semantic re-lations.
Accordingly, lexical chains are computedon the basis of a lexical semantic resource such asWordNet (Fellbaum, 1998).
Figure 1 also depictsFigure 1: Chaining Example (adapted from Hallidayet al (1976))several unsystematic relations, which should in prin-ciple be considered.
Unfortunately, common lexicalresources do not incorporate them sufficiently.
Mostsystems consist of the fundamental modules shownin Table 1.However, in order to formally evaluate the perfor-mance of a given chainer in terms of precision andrecall, a (preferably standardized and freely avail-able) test set would be required.
To our knowledgesuch a resource does not exist ?
neither for English142Module Subtaskschaining candidate selection preprocessing of corpora:determine chaining window,sentence boundaries,tokens, POS-taggingchunks etc.calculation of chains / look-up: lexical semanticmeta-chains resource (e.g.
WordNet),scoring of relations,sense disambiguationoutput creation rate chain strength(e.g.
select strong chains),build application specificrepresentationTable 1: Overview of Chainer Modulesnor for German.
We therefore plan to develop anevaluation corpus (gold standard), which on the onehand includes the annotation of lexical chains andon the other hand reveals the rich interaction be-tween various principles to achieve a cohesive textstructure.
In order to systematically construct soundguidelines for the annotation of this gold standard,we conducted a case study.Case Study Six subjects were asked to annotatelexical chains in three short texts and in doing sorecord all challenges and uncertainties they experi-enced.
The subjects were asked to read three texts?
a wikipedia entry (137 words), a newspaperarticle (233 words), and an interview (306 words).They were then given a list of all nouns occurringin the articles (almost all chainers exclusively con-sider nouns as chaining candidates), which they hadto rate with respect to their ?importance?
in under-standing the text.
On this basis they were askedto determine the semantic relations of every pos-sible chaining candidate pair, thus chain the nounsand annotate the three texts.
Just like previously re-ported case studies (Beigman Klebanov, 2005; Mor-ris and Hirst, 2004; Morris and Hirst, 2005) aim-ing at the annotation of lexical chains, we foundthat the inter-annotator agreement was in generalrelatively low.
Only the annotation of very promi-nent items in the three texts, which accounted forapproximately one fifth of the chaining candidates,resulted in a satisfying agreement (that is: the ma-jority of the subjects produced an identical or verysimilar annotation).
However, all subjects com-plained about the task.
They found it rather diffi-cult to construct linearized or quasi-linearized struc-tures, in short, chains.
Instead, most of the subjectsbuilt clusters and drew very complex graphs to illus-trate the cohesive relations they found.
They alsopointed out that only a small fraction of the can-didate list contributed to their text understanding.This clearly supports our observation that most ofthe subjects first skimmed through the text to findthe most prominent items, established chains for thisselection and then worked the text over to distributethe remaining items to these chains.
We therefore as-sume that lexical chains do not directly reflect read-ing and understanding processes.
Nevertheless, theydo in some way contribute to them.
Many subjectsadditionally noted that a reasonable candidate listshould also include multi-word units (e.g.
techni-cal terms) or even phrases.
Furthermore, as alreadyreported in previous work (Morris and Hirst, 2004),the semantic relations usually considered seem notto suffice.
Accordingly, some subjects proposed newrelations to characterize the links connecting can-didate pairs.
Given our own findings and the re-sults reported in previous work, it is obviously de-manding to find a clear-cut border between the con-cepts of lexical chaining, semantic fields, and co-reference/anaphora resolution.
Definitely, the anno-tation of co-reference/anaphora and lexical chains isinherently analogous.
In both cases an annotationlayer consisting of labelled edges between pairs ofannotation candidates is constructed.
However, weassume that the lexical chaining layer might containmore edges between annotation candidates.
As aconsequence, its structure presumably is more com-plex and its connectivity higher.
We thus plan toconduct an extended follow-up study in order to ex-plore these differences between the annotation oflexical chains and co-reference/anaphora.
We alsointend to take advantage of ?
amongst other aspects?
the inter-annotator comparison functionality pro-vided by Serengeti (see Section 4 for a detailed de-scription) in order to implement a formally correctinter-annotator agreement test.3 Available Tools for AnnotatingLinguistic CorporaBoth the anaphora resolution and the lexical chain-ing scenario have shown the importance of an easy-143to-use annotation tool.
Although a wide range ofannotation tools is available, one has to separatetools for annotating multimodal corpora from toolsfor annotating unimodal (i. e. text) corpora.
Dip-per et al (2004) evaluated some of the most com-monly used tools of both categories (TASX Anno-tator, EXMARaLDA, MMAX, PALinkA and Sys-tematic Coder).
Besides, other tools such as ELAN2or Anvil3 are available as well, as are tool kits suchas the Annotation Graph Toolkit (AGTK)4 or theNITE XML Toolkit.5 While multimodal annotationdemands a framework supporting the time-alignedhandling of video and audio streams and, therefore,much effort has been spent on the design and devel-opment of tools, unimodal annotation has often beenfulfilled by using ordinary XML editors which canbe error-prone.
Nevertheless, specialized annota-tion frameworks are available as well, e. g. MMAXcan be used for multi-level annotation projects (cf.M?ller and Strube (2001; 2003)).
However, as an-notation projects grow in size and complexity (oftenmultiple annotation layers are generated), collabo-rative annotation and the use of annotation tools isvital.?
Ma et al (2002), for example, describe collab-orative annotation in the context of the AGTK.But since most of the aforementioned applica-tions have to be installed locally on a PC, work-ing on a corpus and managing annotations ex-ternally can be difficult.?
Another problem worth to be mentioned is datamanagement.
Having several annotators work-ing on one text, unification and comparison ofthe markup produced is quite difficult.?
Furthermore, annotation tools help to increaseboth the quality and quantity of the annotationprocess.Recent web technologies allow the design of web-based applications that resemble locally installeddesktop programs on the one hand and provide cen-tral data management on the other hand.
Therefore2http://www.lat-mpi.eu/tools/elan/3http://www.dfki.de/~kipp/anvil/4http://agtk.sourceforge.net/5http://www.ltg.ed.ac.uk/NITE/distributed annotation is possible regardless of loca-tion, provided that an internet connection is avail-able.
In this paper we propose the web-based anno-tation application Serengeti.4 A new Approach: SerengetiAs the Sekimo project is part of a research groupwith interrelated application domains, annotationlayers from different projects have been evaluatedfor their interrelationship (e. g. Bayerl et al (2003;2006)).
This led directly to the open design ofSerengeti ?
an annotation tool with the fundamen-tal idea in mind: making possible the annotationof a single layer (or resource) and the use of thebest annotation possible and the best available re-sources.
Serengeti allows for several experts to an-notate a single text at the same time as well as tocompare the different annotations (inter-annotator-agreement) and merge them afterwards.
Access tothe documents is available from everywhere (an in-ternet connection and a browser is required).4.1 Technical OverviewSerengeti is a web application developed for MozillaFirefox,6 thus its architecture is separated into aclient and a server side, following the principles andtools of AJAX (Asynchronous JavaScript and XML,cf.
Garrett (2005)).
While groups, documents andannotations are managed centrally on the server side,all user interactions are rendered locally on the clientside.74.2 Graphical User InterfaceThe Graphical User Interface (GUI) of Serengeti issubdivided into several areas (cf.
Figure 2).
Themain area renders the text to be annotated, roughlylaid out in terms of paragraphs, lists, tables and non-text sections according to the input XML data.
Ad-ditionally, predefined markables are underlined andfollowed by boxes containing the markables?
uniqueidentifiers.
These boxes serve as clickable buttonsto choose markables during the annotation.
At this6Serengeti is targeted at platform independence, so we?vechosen Firefox, which is freely available for several operatingsystems.
Future versions will support other browsers as well.7Each Serengeti installation supports more than one work-group.
Server sided data management allows the use of ver-sioning systems like CVS or, in our case, Subversion.144time, adding markables, i. e. changing the inputdata, is not allowed.8 This ensures that all annota-tors use the same base layer.
A section at the bottomof the interface represents the annotation panel witha list of all annotated relations on the left and allediting tools on the right side.
An application bar atthe top of the GUI provides functions for choosingand managing groups, documents and annotations.4.3 Annotation ProcessAfter logging in and choosing a document to anno-tate, new relations between markables can be cre-ated.
The markables that take part in the relationare chosen by left-clicking the boxes attached to theunderlined markables in the text and, if necessary,unchecked by clicking them once again.
To encodethe type of a relation between chosen markables, aninput form at the bottom right of the page providesvarious options for specifying the relation accord-ing to the annotation scheme.
The OKAY commandadds created relations to the list, which can subse-quently be edited or deleted.
In regard to their state,relation bars in the list can be highlighted differ-ently to simplify the post-editing (i. e. new relations,old/saved relations, commented relations or incom-plete relations).9 The user can save his work to theserver at any time.
After the annotation process iscompleted, the COMMIT command (located in thedocument menu) declares the annotation as finished.4.4 Comparing Annotations and Reaching aConsensusIn order to achieve the best annotation results it isnecessary to provide an opportunity for the evalua-tion of single annotations or comparing of multipleannotations on one single document (either by dif-ferent annotators or identical annotators at differentpoints in time).
This allows for verification of thequality of the annotation scheme and for valid train-ing data for automated natural language processingtools.
For this purpose, a special user access, theConsensus User (CU), has been developed as part ofSerengeti?s concept.
Loading a document as a CU, it8The definition of XML elements as markables and the lay-out and relation type specification is driven via an external con-figuration script, adjustable for each group.9It is possible to hide relations according to their state aswell.is possible to choose a single annotation done by anyother annotator (either work in progress or commit-ted) as the basis for the final annotation.
This is donewith the same tools as those for the annotation pro-cess.
If satisfied, the CU can declare the annotationas ultimately closed via the COMMIT command.Figure 3: Serengeti?s comparison window in thelower left part of the GUI.Furthermore, the CU can compare two annota-tions with each other.
The relations annotated byboth users are then displayed in the relation list andjuxtaposed in case they differ in at least one aspect(e. g. different relation types as in Figure 3).10 Onthis basis the CU can decide which relation to acceptand which one to reject.
Again, all editing optionsare at the user?s disposal.While editing single or multiple user annotations,the CU can save the current state of his work at anytime.
Afterwards these annotations will appear inthe ANNOTATIONS MENU as well and can be se-lected for further evaluation and comparison.115 Extending SerengetiAlthough one might doubt that Serengeti is directlyapplicable to annotating lexical chains, this can nev-ertheless be done straightforwardly using the anno-tation described in Section 2.1.
Our starting point isas follows: As markables we refer to entities of theparser output (i. e. tokens) where a user can marka token as the initial vertex of a chain.
In orderto reflect the findings of our case study on lexicalchaining we distinguish two cases: Either the an-notator decides that a newly entered token enlarges10At this point the assignment of relations is important.Anaphoric relations, for example, are assigned to each otherif their anaphoric element is the same.
If there is more thanone relation with identical anaphoric elements, the relations aresorted by their relation types and their antecedent(s).11Comparisons require conflictless annotations, i. e. savedcomparisons have to be free from juxtaposed relations.145Figure 2: Serengeti?s User Interface.
Screenshots of Serengeti Version 0.7.1an already marked-up chain by explicitly relating itto one of its links or he implicitly assigns the to-ken to that chain as a whole which is visually rep-resented as part of Serengeti?s interface.
In the firstcase we just face another use case of our annota-tion scheme, that is, a link between two tokens orspans of a text where this link may be typed accord-ing to some linguistic relation that holds between thespans, e. g. hyponymy.
In the second case of an im-plicit chain assignment we proceed as follows: Welink the newly processed token to the last vertex ofthe lexical chain to which the token is attached andtype this relation non-specifically as association.
Asa result, we reduce this use case to the one alreadymapped by our general annotation scheme.
In or-der to make this a workable solution, we will in-tegrate a representation of lexical chains by meansof tag clouds where each chain is represented by asubset of those lexical units which because of theirfrequency are most important in representing thatchain.
Following this line of extending Serengeti, wemanage to use it as an annotation tool which handlesanaphoric relations as well as lexical chains.6 Discussion and OutlookSerengeti can be used to create corpus data fortraining and evaluation purposes.
An installationof Serengeti is available online.12 Currently, thetool is being generalized to allow the annotationof lexical chains and several other annotation tasks.More specifically, we plan to incorporate any kind ofchain-like structuring of text segments and to makethe chains an object of annotation so that they canbe interrelated.
This will allow to incorporate con-stituency relations into the annotation process.
Be-yond that we will incorporate metadata handling todocument all steps of the annotation process.ReferencesP.
S. Bayerl, H. L?ngen, D. Goecke, A. Witt, andD.
Naber.
2003.
Methods for the Semantic Analy-sis of Document Markup.
In C. Roisin, E. Muson,and C. Vanoirbeek, editors, Proceedings of the 2003ACM symposium on Document engineering (DocEng),pages 161?170, Grenoble.
ACM Press.12http://coli.lili.uni-bielefeld.de/serengeti/146B.
Beigman Klebanov.
2005.
Using readers to identifylexical cohesive structures in texts.
In Proceedings ofACL Student Research Workshop.S.
Dipper, M. G?tze, and M. Stede.
2004.
Simple Anno-tation Tools for Complex Annotation Tasks: an Evalu-ation.
In Proceedings of the LREC Workshop on XML-based Richly Annotated Corpora, pages 54?62, Lis-bon, Portugal.C.
Fellbaum, editor.
1998.
WordNet.
An Electronic Lexi-cal Database.
The MIT Press.S.
Fligelstone.
1992.
Developing a Scheme for Annotat-ing Text to Show Anaphoric Relations.
In G. Leitner,editor, New Directions in English Language Corpora:Methodology, Results, Software Developments, pages153?170.
Mouton de Gruyter, Berlin.J.
J. Garrett, 2005.
AJAX: A New Approach to WebApplications.
Adaptive Path LLC, February, 18.Online: http://www.adaptivepath.com/publications/essays/archives/000385.php.R.
Garside, S. Fligelstone, and S. Botley.
1997.
Dis-course Annotation: Anaphoric Relations in Corpora.In R. Garside, G. Leech, and A. McEnery, editors,Corpus Annotation: Linguistic Information from Com-puter Text Corpora, pages 66?84.
Addison-WesleyLongman, London.D.
Goecke and A. Witt.
2006.
Exploiting Logical Docu-ment Structure for Anaphora Resolution.
In Proceed-ings of the 5th International Conference., Genoa, Italy.Michael A. K. Halliday and Ruqaiya Hasan.
1976.
Co-hesion in English.
Longman, London.L.
Hirschmann.
1997.
MUC-7 Coreference Task Defini-tion (version 3.0).
In L. Hirschman and N. Chinchor,editors, Proceedings of Message Understanding Con-ference (MUC-7).A.
Holler, J.-F. Maas, and A. Storrer.
2004.
ExploitingCoreference Annotations for Text-to-Hypertext Con-version.
In Proceeding of LREC, volume II, pages651?654, Lisbon, Portugal.L.
Karttunen.
1976.
Discourse Referents.
Syntax andSemantics: Notes from the Linguistic Underground,7:363?385.X.
Ma, L. Haejoong, S. Bird, and K. Maeda.
2002.Models and Tools for Collaborative Annotation.
InProceedings of the Third International Conference onLanguage Resources and Evaluation, Paris.
EuropeanLanguage Resources Association.R.
Mitkov.
2002.
Anaphora Resolution.
Longman, Lon-don.J.
Morris and G. Hirst.
1991.
Lexical cohesion computedby thesaural relations as an indicator of the structure oftext.
Computational linguistics, 17(1):21?48, March.J.
Morris and G. Hirst.
2004.
Non-classical lexicalsemantic relations.
In Proceedings of HLT-NAACLWorkshop on Computational Lexical Semantics.J.
Morris and G. Hirst.
2005.
The subjectivity of lexi-cal cohesion in text.
In J. C. Chanahan, C. Qu, andJ.
Wiebe, editors, Computing attitude and affect in text.Springer.C.
M?ller and M.l Strube.
2001.
Annotating Anaphoricand Bridging Relations with MMAX.
In Proceedingsof the 2nd SIGdial Workshop on Discourse and Dia-logue, pages 90?95, Aalborg, Denmark.C.
M?ller and M. Strube.
2003.
Multi-Level Annotationin MMAX.
In Proceedings of the 4th SIGdial Work-shop on Discourse and Dialogue, pages 198?207, Sap-poro, Japan.M.
Poesio, T. Ishikawa, S. Schulte im Walde, andR.
Viera.
2002.
Acquiring lexical knowledge foranaphora resolution.
In Proc.
of the 3rd Conferenceon Language Resources and Evaluation (LREC).M.
Poesio.
2004.
The MATE/GNOME Scheme forAnaphoric Annotation, Revisited.
In Proceedings ofSIGDIAL, Boston, April.G.
Simons, W. Lewis, S. Farrar, T. Langendoen, B. Fitzsi-mons, and H. Gonzalez.
2004.
The semantics ofmarkup.
In Proceedings of the ACL 2004 Workshopon RDF/RDFS and OWL in Language Technology(NLPXML-2004), Barcelona.M.
Strube and C. M?ller.
2003.
A Machine LearningApproach to Pronoun Resolution in Spoken Dialogue.In Proceedings of the 41st Annual Meeting on Associ-ation for Computational Linguistics, volume 1, pages168?175.
ACL 03.M.
St?hrenberg, A. Witt, D. Goecke, D. Metzing, andO.
Schonefeld.
2006.
Multidimensional Markupand Heterogeneous Linguistic Resources.
In D. Ahn,E.
T. K. Sang, and G. Wilcock, editors, Proceedings ofthe 5th Workshop on NLP and XML (NLPXML-2006):Multi-Dimensional Markup in Natural Language Pro-cessing, pages 85?88.A.
Witt, D. Goecke, F. Sasaki, and H. L?ngen.2005.
Unification of XML Documents with Con-current Markup.
Literary and Lingustic Computing,20(1):103?116.Y.
Xiaofeng, J. Su, G. Zhou, and C. L. Tan.
2004.
Im-proving Pronoun Resolution by Incorporating Coref-erential Information of Candidates.
In Proceedings ofACL.147
