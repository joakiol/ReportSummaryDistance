Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 993?1000,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Phonetic-Based Approach to Chinese Chat Text NormalizationYunqing Xia, Kam-Fai WongDepartment of S.E.E.M.The Chinese University of Hong KongShatin, Hong Kong{yqxia, kfwong}@se.cuhk.edu.hkWenjie LiDepartment of ComputingThe Hong Kong Polytechnic UniversityKowloon, Hong Kongcswjli@comp.polyu.edu.hkAbstractChatting is a popular communicationmedia on the Internet via ICQ, chatrooms, etc.
Chat language is differentfrom natural language due to its anoma-lous and dynamic natures, which rendersconventional NLP tools inapplicable.
Thedynamic problem is enormously trouble-some because it makes static chat lan-guage corpus outdated quickly in repre-senting contemporary chat language.
Toaddress the dynamic problem, we pro-pose the phonetic mapping models topresent mappings between chat terms andstandard words via phonetic transcrip-tion, i.e.
Chinese Pinyin in our case.
Dif-ferent from character mappings, the pho-netic mappings can be constructed fromavailable standard Chinese corpus.
Toperform the task of dynamic chat lan-guage term normalization, we extend thesource channel model by incorporatingthe phonetic mapping models.
Experi-mental results show that this method iseffective and stable in normalizing dy-namic chat language terms.1 IntroductionInternet facilitates online chatting by providingICQ, chat rooms, BBS, email, blogs, etc.
Chatlanguage becomes ubiquitous due to the rapidproliferation of Internet applications.
Chat lan-guage text appears frequently in chat logs ofonline education (Heard-White, 2004), customerrelationship management (Gianforte, 2003), etc.On the other hand, wed-based chat rooms andBBS systems are often abused by solicitors ofterrorism, pornography and crime (McCullagh,2004).
Thus there is a social urgency to under-stand online chat language text.Chat language is anomalous and dynamic.Many words in chat text are anomalous to naturallanguage.
Chat text comprises of ill-edited termsand anomalous writing styles.
We refer chatterms to the anomalous words in chat text.
Thedynamic nature reflects that chat languagechanges more frequently than natural languages.For example, many popular chat terms used inlast year have been discarded and replaced bynew ones in this year.
Details on these two fea-tures are provided in Section 2.The anomalous nature of Chinese chat lan-guage is investigated in (Xia et al, 2005).
Patternmatching and SVM are proposed to recognizethe ambiguous chat terms.
Experiments showthat F-1 measure of recognition reaches 87.1%with the biggest training set.
However, it is alsodisclosed that quality of both methods drops sig-nificantly when training set is older.
The dy-namic nature is investigated in (Xia et al,2006a), in which an error-driven approach is pro-posed to detect chat terms in dynamic Chinesechat terms by combining standard Chinese cor-pora and NIL corpus (Xia et al, 2006b).
Lan-guage texts in standard Chinese corpora are usedas negative samples and chat text pieces in theNIL corpus as positive ones.
The approach calcu-lates confidence and entropy values for the inputtext.
Then threshold values estimated from thetraining data are applied to identify chat terms.Performance equivalent to the methods in exis-tence is achieved consistently.
However, the is-sue of normalization is addressed in their work.Dictionary based chat term normalization is not agood solution because the dictionary cannotcover new chat terms appearing in the dynamicchat language.In the early stage of this work, a method basedon source channel model is implemented for chatterm normalization.
The problem we encounter isaddressed as follows.
To deal with the anoma-lous nature, a chat language corpus is constructedwith chat text collected from the Internet.
How-993ever, the dynamic nature renders the static corpusoutdated quickly in representing contemporarychat language.
The dilemma is that timely chatlanguage corpus is nearly impossible to obtain.The sparse data problem and dynamic problembecome crucial in chat term normalization.
Webelieve that some information beyond charactershould be discovered to help addressing thesetwo problems.Observation on chat language text reveals thatmost Chinese chat terms are created via phonetictranscription, i.e.
Chinese Pinyin in our case.
Amore exciting finding is that the phonetic map-pings between standard Chinese words and chatterms remain stable in dynamic chat language.We are thus enlightened to make use of the pho-netic mapping models, in stead of character map-ping models, to design a normalization algorithmto translate chat terms to their standard counter-parts.
Different from the character mappingmodels constructed from chat language corpus,the phonetic mapping models are learned from astandard language corpus because they attempt tomodel mappings probabilities between any twoChinese characters in terms of phonetic tran-scription.
Now the sparse data problem can thusbe appropriately addressed.
To normalize thedynamic chat language text, we extend thesource channel model by incorporating phoneticmapping models.
We believe that the dynamicproblem can be resolved effectively and robustlybecause the phonetic mapping models are stable.The remaining sections of this paper are or-ganized as follows.
In Section 2, features of chatlanguage are analyzed with evidences.
In Section3, we present methodology and problems of thesource channel model approach to chat termnormalization.
In Section 4, we present defini-tion, justification, formalization and parameterestimation for the phonetic mapping model.
InSection 5, we present the extended source chan-nel model that incorporates the phonetic mappingmodels.
Experiments and results are presented inSection 6 as well as discussions and error analy-sis.
We conclude this paper in Section 7.2 Feature Analysis and EvidencesObservation on NIL corpus discloses the anoma-lous and dynamic features of chat language.2.1 AnomalousChat language is explicitly anomalous in twoaspects.
Firstly, some chat terms are anomalousentries to standard dictionaries.
For example, ???
(here, jie4 li3)?
is not a standard word in anycontemporary Chinese dictionary while it is oftenused to replace ???
(here, zhe4 li3)?
in chatlanguage.
Secondly, some chat terms can befound in  standard dictionaries while their mean-ings in chat language are anomalous to the dic-tionaries.
For example, ??
(even, ou3)?
is oftenused to replace ??
(me, wo2)?
in chat text.
Butthe entry that ???
occupies in standard diction-ary is used to describe even numbers.
The lattercase is constantly found in chat text, whichmakes chat text understanding fairly ambiguousbecause it is difficult to find out whether theseterms are used as standard words or chat terms.2.2 DynamicChat text is deemed dynamic due to the fact thata large proportion of chat terms used in last yearmay become obsolete in this year.
On the otherhand, ample new chat terms are born.
This fea-ture is not as explicit as the anomalous nature.But it is as crucial.
Observation on chat text inNIL corpus reveals that chat term set changesalong with time very quickly.An empirical study is conducted on five chattext collections extracted from YESKY BBS sys-tem (bbs.yesky.com) within different time peri-ods, i.e.
Jan. 2004, July 2004, Jan. 2005, July2005 and Jan. 2006.
Chat terms in each collec-tion are picked out by hand together with theirfrequencies so that five chat term sets are ob-tained.
The top 500 chat terms with biggest fre-quencies in each set are selected to calculate re-occurring rates of the earlier chat term sets on thelater ones.Set Jul-04 Jan-05 Jul-05 Jan-06 Avg.Jan-04 0.882 0.823 0.769 0.706 0.795Jul-04 - 0.885 0.805 0.749 0.813Jan-05 - - 0.891 0.816 0.854Jul-05 - - - 0.875 0.875Table 1.
Chat term re-occurring rates.
The rowsrepresent the earlier chat term sets and the col-umns the later ones.The surprising finding in Table 1 is that 29.4%of chat terms are replaced with new ones withintwo years and about 18.5% within one year.
Thechanging speed is much faster than that in stan-dard language.
This thus proves that chat text isdynamic indeed.
The dynamic nature renders thestatic corpus outdated quickly.
It poses a chal-lenging issue on chat language processing.9943 Source Channel Model and ProblemsThe source channel model is implemented asbaseline  method in this work for chat term nor-malization.
We brief its methodology and prob-lems as follows.3.1 The ModelThe source channel model (SCM) is a successfulstatistical approach in speech recognition andmachine translation (Brown, 1990).
SCM isdeemed applicable to chat term normalizationdue to similar task nature.
In our case, SCM aimsto find the character string niicC ,...,2,1}{ ==  thatthe given input chat text njitT ,...,2,1}{ ==  is mostprobably translated to, i.e.
ii ct ?
, as follows.
)()()|(maxarg)|(maxarg?TpCpCTpTCpCCC==     (1)Since )(Tp  is a constant for C , so C?
shouldalso maximize )()|( CpCTp .
Now )|( TCp  isdecomposed into two components, i.e.
chat termtranslation observation model )|( CTp  and lan-guage model )(Cp .
The two models can be bothestimated with maximum likelihood method us-ing the trigram model in NIL corpus.3.2 ProblemsTwo problems are notable in applying SCM inchat term normalization.
First, data sparsenessproblem is serious because timely chat languagecorpus is expensive thus small due to dynamicnature of chat language.
NIL corpus containsonly 12,112 pieces of chat text created in eightmonths, which is far from sufficient to train thechat term translation model.
Second, trainingeffectiveness is poor due to the dynamic nature.Trained on static chat text pieces, the SCM ap-proach would perform poorly in processing chattext in the future.
Robustness on dynamic chattext thus becomes a challenging issue in our re-search.Updating the corpus with recent chat text con-stantly is obviously not a good solution to theabove problems.
We need to find some informa-tion beyond character to help addressing thesparse data problem and dynamic problem.
For-tunately, observation on chat terms provides usconvincing evidence that the underlying phoneticmappings exist between most chat terms andtheir standard counterparts.
The phonetic map-pings are found promising in resolving the twoproblems.4 Phonetic Mapping Model4.1 Definition of Phonetic MappingPhonetic mapping is the bridge that connects twoChinese characters via phonetic transcription, i.e.Chinese Pinyin in our case.
For example, ??????
??
)56.0,,( jiezhe ??
is the phonetic mapping con-necting ??
(this, zhe4)?
and ??
(interrupt, jie4)?,in which ?zhe?
and ?jie?
are Chinese Pinyin for???
and ???
respectively.
0.56 is phoneticsimilarity between the two Chinese characters.Technically, the phonetic mappings can be con-structed between any two Chinese characterswithin any Chinese corpus.
In chat language, anyChinese character can be used in chat terms, andphonetic mappings are applied to connect chatterms to their standard counterparts.
Differentfrom the dynamic character mappings, the pho-netic mappings can be produced with standardChinese corpus before hand.
They are thus stableover time.4.2 Justifications on Phonetic AssumptionTo make use of phonetic mappings in normaliza-tion of chat language terms, an assumption mustbe made that chat terms are mainly formed viaphonetic mappings.
To justify the assumption,two questions must be answered.
First, howmany percent of chat terms are created via pho-netic mappings?
Second, why are the phoneticmapping models more stable than character map-ping models in chat language?Mapping type Count PercentageChinese word/phrase 9370 83.3%English capital 2119 7.9%Arabic number 1021 8.0%Other  1034 0.8%Table 2.
Chat term distribution in terms of  map-ping type.To answer the first question, we look into chatterm distribution in terms of mapping type inTable 2.
It is revealed that 99.2 percent of chatterms in NIL corpus fall into the first four pho-netic mapping types that make use of phoneticmappings.
In other words, 99.2 percent of chatterms can be represented by phonetic mappings.0.8% chat terms come from the OTHER type,emoticons for instance.
The first question is un-doubtedly answered with the above statistics.To answer the second question, an observationis conducted again on the five chat term sets de-scribed in Section 2.2.
We create phonetic map-995pings manually for the 500 chat terms in eachset.
Then five phonetic mapping sets are ob-tained.
They are in turn compared against thestandard phonetic mapping set constructed withChinese Gigaword.
Percentage of phonetic map-pings in each set covered by the standard set ispresented in Table 3.Set Jan-04 Jul-04 Jan-05 Jul-05 Jan-06percentage 98.7 99.3 98.9 99.3 99.1Table 3.
Percentages of phonetic mappings ineach set covered by standard set.By comparing Table 1 and Table 3, we findthat phonetic mappings remain more stable thancharacter mappings in chat language text.
Thisfinding is convincing to justify our intention todesign effective and robust chat language nor-malization method by introducing phonetic map-pings to the source channel model.
Note thatabout 1% loss in these percentages comes fromchat terms that are not formed via phonetic map-pings, emoticons for example.4.3 FormalismThe phonetic mapping model is a five-tuple, i.e.>< )|(Pr),(),(,, CTCptTptCT pm ,which comprises of chat term character T , stan-dard counterpart character C , phonetic transcrip-tion of T  and C , i.e.
)(Tpt  and )(Cpt , and themapping probability )|(Pr CTpm  that T  ismapped to C  via the  phonetic mapping ( ) CT CTCptTpt pm ???????
??
)|(Pr),(),(  (hereafter briefed byCT M???
).As they manage mappings between any twoChinese characters, the phonetic mapping modelsshould be constructed with a standard languagecorpus.
This results in two advantages.
One,sparse data problem can be addressed appropri-ately because standard language corpus is used.Two, the phonetic mapping models are as stableas standard language.
In chat term normalization,when the phonetic mapping models are used torepresent mappings between chat term charactersand standard counterpart characters, the dynamicproblem can be addressed in a robust manner.Differently, the character mapping model usedin the SCM (see Section 3.1) connects two Chi-nese characters directly.
It is a three-tuple, i.e.>< )|(Pr,, CTCT cm ,which comprises of chat term character T , stan-dard counterpart character C  and the mappingprobability )|(Pr CTcm  that T  is mapped to Cvia this character mapping.
As they must be con-structed from chat language training samples, thecharacter mapping models suffer from datasparseness problem and dynamic problem.4.4 Parameter EstimationTwo questions should be answered in parameterestimation.
First, how are the phonetic mappingspace constructed?
Second, how are the phoneticmapping probabilities estimated?To construct the phonetic mapping models, wefirst extract all Chinese characters from standardChinese corpus and use them to form candidatecharacter mapping models.
Then we generatephonetic transcription for the Chinese charactersand calculate phonetic probability for each can-didate character mapping model.
We excludethose character mapping models holding zeroprobability.
Finally, the character mapping mod-els are converted to phonetic mapping modelswith phonetic transcription and phonetic prob-ability incorporated.The phonetic probability is calculated bycombining phonetic similarity and character fre-quencies in standard language as follows.
( )( )?
?
?=i iislcslcpmAApsAfrAApsAfrAAob),()(),()(),(Pr    (2)In Equation (2) }{ iA  is the character set inwhich each element iA  is similar to character Ain terms of phonetic transcription.
)(cfrslc  is afunction returning frequency of given characterc  in standard language corpus and ),( 21 ccpsphonetic similarity between character 1c  and 2c .Phonetic similarity between two Chinese char-acters is calculated based on Chinese Pinyin asfollows.
)))(()),((()))(()),((())(),((),(ApyfinalApyfinalSimApyinitialApyinitialSimApyApySimAAps?==(3)In Equation (3) )(cpy  is a function that returnsChinese Pinyin of given character c , and)(xinitial  and )(xfinal  return initial (shengmu)and final (yunmu) of given Chinese Pinyin xrespectively.
For example, Chinese Pinyin for theChinese character ???
is ?zhe?, in which ?zh?
isinitial and ?e?
is final.
When initial or final is996empty for some Chinese characters, we only cal-culate similarity of the existing parts.An algorithm for calculating similarity of ini-tial pairs and final pairs is proposed in (Li et al,2003) based on letter matching.
Problem of thisalgorithm is that it always assigns zero similarityto those pairs containing no common letter.
Forexample, initial similarity between ?ch?
and ?q?is set to zero with this algorithm.
But in fact,pronunciations of the two initials are very closeto each other in Chinese speech.
So non-zerosimilarity values should be assigned to these spe-cial pairs before hand (e.g., similarity between?ch?
and ?q?
is set to 0.8).
The similarity valuesare agreed by some native Chinese speakers.Thus Li et al?s algorithm is extended to output apre-defined similarity value before letter match-ing is executed in the original algorithm.
For ex-ample, Pinyin similarity between ?chi?
and ?qi?is calculated as follows.8.018.0),(),()( =?=?= iiSimqchSimchi,qiSim5 Extended Source Channel ModelWe extend the source channel model by insertingphonetic mapping models niimM ,...,2,1}{ ==  intoequation (1), in which chat term character it  ismapped to standard character ic  via im , i.e.imi ct i???
.
The extended source channel model(XSCM) is mathematically addressed as follows.
)()()|(),|(maxarg),|(maxarg?,,TpCpCMpCMTpTMCpCMCMC==(4)Since )(Tp  is a constant, C?
and M?
shouldalso maximize )()|(),|( CpCMpCMTp .
Nowthree components are involved in XSCM, i.e.chat term normalization observation model),|( CMTp , phonetic mapping model )|( CMpand language model )(Cp .Chat Term Normalization ObservationModel.
We assume that mappings between chatterms and their standard Chinese counterparts areindependent of each other.
Thus chat term nor-malization probability can be calculated as fol-lows.
?= i iii cmtpCMTp ),|(),|(              (5)The ),|( iii cmtp ?s are estimated using maxi-mum likelihood estimation method with Chinesecharacter trigram model in NIL corpus.Phonetic Mapping Model.
We assume that thephonetic mapping models depend merely on thecurrent observation.
Thus the phonetic mappingprobability is calculated as follows.
?= i ii cmpCMp )|()|(                 (6)in which )|( ii cmp ?s are estimated with equation(2) and (3) using a standard Chinese corpus.Language Model.
The language model )(Cp ?scan be estimated using maximum likelihood es-timation method with Chinese character trigrammodel on NIL corpus.In our implementation, Katz Backoff smooth-ing technique (Katz, 1987) is used to handle thesparse data problem, and Viterbi algorithm isemployed to find the optimal solution in XSCM.6 Evaluation6.1 Data DescriptionTraining SetsTwo types of training data are used in our ex-periments.
We use news from Xinhua NewsAgency in LDC Chinese Gigaword v.2(CNGIGA) (Graf et al, 2005) as standard Chi-nese corpus to construct phonetic mapping mod-els because of its excellent coverage of standardSimplified Chinese.
We use NIL corpus (Xia etal., 2006b) as chat language corpus.
To evaluateour methods on size-varying training data, sixchat language corpora are created based on NILcorpus.
We select 6056 sentences from NIL cor-pus randomly to make the first chat languagecorpus, i.e.
C#1.
In every next corpus, we addextra 1,211 random sentences.
So 7,267 sen-tences are contained in C#2, 8,478 in C#3, 9,689in C#4, 10,200 in C#5, and 12,113 in C#6.Test SetsTest sets are used to prove that chat language isdynamic and XSCM is effective and robust innormalizing dynamic chat language terms.
Sixtime-varying test sets, i.e.
T#1 ~ T#6, are createdin our experiments.
They contain chat languagesentences posted from August 2005 to Jan 2006.We randomly extract 1,000 chat language sen-tences posted in each month.
So timestamp of thesix test sets are in temporal order, in which time-stamp of T#1 is the earliest and that of T#6 thenewest.The normalized sentences are created by handand used as standard normalization answers.9976.2 Evaluation CriteriaWe evaluate two tasks in our experiments, i.e.recognition and normalization.
In recognition,we use precision (p), recall (r) and f-1 measure(f) defined as follows.2rprpfzxxryxxp +?
?=+=+=      (7)where x denotes the number of true positives, ythe false positives and z the true negatives.For normalization, we use accuracy (a), whichis commonly accepted by machine translationresearchers as a standard evaluation criterion.Every output of the normalization methods iscompared to the standard answer so that nor-malization accuracy on each test set is produced.6.3 Experiment I: SCM vs. XSCM UsingSize-varying Chat Language CorporaIn this experiment we investigate on quality ofXSCM and SCM using same size-varying train-ing data.
We intend to prove that chat language isdynamic and phonetic mapping models used inXSCM are helpful in addressing the dynamicproblem.
As no standard Chinese corpus is usedin this experiment, we use standard Chinese textin chat language corpora to construct phoneticmapping models in XSCM.
This violates the ba-sic assumption that the phonetic mapping modelsshould be constructed with standard Chinesecorpus.
So results in this experiment should beused only for comparison purpose.
It would beunfair to make any conclusion on general per-formance of XSCM method based on results inthis experiments.We train the two methods with each of the sixchat language corpora, i.e.
C#1 ~ C#6 and testthem on six time-varying test sets, i.e.
T#1 ~ T#6.F-1 measure values produced by SCM andXSCM in this experiment are present in Table 3.Three tendencies should be pointed out ac-cording to Table 3.
The first tendency is that f-1measure in both methods drops on time-varyingtest sets (see Figure 1) using same training chatlanguage corpora.
For example, both SCM andXSCM perform best on the earliest test set T#1and worst on newest T#4.
We find that the qual-ity drop is caused by the dynamic nature of chatlanguage.
It is thus revealed that chat language isindeed dynamic.
We also find that quality ofXSCM drops less than that of SCM.
This provesthat phonetic mapping models used in XSCM arehelpful in addressing the dynamic problem.However, quality of XSCM in this experimentstill drops by 0.05 on the six time-varying testsets.
This is because chat language text corpus isused as standard language corpus to model thephonetic mappings.
Phonetic mapping modelsconstructed with chat language corpus are farfrom sufficient.
We will investigate in Experi-ment-II to prove that stable phonetic mappingmodels can be constructed with real standardlanguage corpus, i.e.
CNGIGA.Test Set T#1 T#2 T#3 T#4 T#5 T#6C#1 0.829 0.805 0.762 0.701 0.739 0.705C#2 0.831 0.807 0.767 0.711 0.745 0.715C#3 0.834 0.811 0.774 0.722 0.751 0.722C#4 0.835 0.814 0.779 0.729 0.753 0.729C#5 0.838 0.816 0.784 0.737 0.761 0.737SCMC#6 0.839 0.819 0.789 0.743 0.765 0.743C#1 0.849 0.840 0.820 0.790 0.805 0.790C#2 0.850 0.841 0.824 0.798 0.809 0.796C#3 0.850 0.843 0.824 0.797 0.815 0.800C#4 0.851 0.844 0.829 0.805 0.819 0.805C#5 0.852 0.846 0.833 0.811 0.823 0.811XSCMC#6 0.854 0.849 0.837 0.816 0.827 0.816Table 3.
F-1 measure by SCM and XSCM on sixtest sets with six chat language corpora.0.690.710.730.750.770.790.810.830.850.870.890.91T#1 T#2 T#3 T#4 T#5 T#6SCM-C#1SCM-C#2SCM-C#3SCM-C#4SCM-C#5SCM-C#6XSCM-C#1XSCM-C#2XSCM-C#3XSCM-C#4XSCM-C#5XSCM-C#6Figure 1.
Tendency on f-1 measure in SCM andXSCM on six test sets with six chat languagecorpora.The second tendency is f-1 measure of bothmethods on same test sets drops when trainedwith size-varying chat language corpora.
For ex-ample, both SCM and XSCM perform best onthe largest training chat language corpus C#6 andworst on the smallest corpus C#1.
This tendencyreveals that both methods favor bigger trainingchat language corpus.
So extending the chat lan-guage corpus should be one choice to improvequality of chat language term normalization.The last tendency is found on quality gap be-tween SCM and XSCM.
We calculate f-1 meas-ure gaps between two methods using same train-ing sets on same test sets (see Figure 2).
Then thetendency is made clear.
Quality gap betweenSCM and XSCM becomes bigger when test set998becomes newer.
On the oldest test set T#1, thegap is smallest, while on the newest test set T#6,the gap reaches biggest value, i.e.
around 0.09.This tendency reveals excellent capability ofXSCM in addressing dynamic problem using thephonetic mapping models.0.010.020.030.040.050.060.070.080.09T#1 T#2 T#3 T#4 T#5 T#6C#1C#2C#3C#4C#5C#6Figure 2.
Tendency on f-1 measure gap in SCMand XSCM on six test sets with six chat languagecorpora.6.4 Experiment II: SCM vs. XSCM UsingSize-varying Chat Language Corporaand CNGIGAIn this experiment we investigate on quality ofSCM and XSCM when a real standard Chineselanguage corpus is incorporated.
We want toprove that the dynamic problem can be addressedeffectively and robustly when CNGIGA is usedas standard Chinese corpus.We train the two methods on CNGIGA andeach of the six chat language corpora, i.e.
C#1 ~C#6.
We then test the two methods on six time-varying test sets, i.e.
T#1 ~ T#6.
F-1 measurevalues produced by SCM and XSCM in this ex-periment are present in Table 4.Test Set T#1 T#2 T#3 T#4 T#5 T#6C#1 0.849 0.840 0.820 0.790 0.735 0.703C#2 0.850 0.841 0.824 0.798 0.743 0.714C#3 0.850 0.843 0.824 0.797 0.747 0.720C#4 0.851 0.844 0.829 0.805 0.748 0.727C#5 0.852 0.846 0.833 0.811 0.758 0.734SCMC#6 0.854 0.849 0.837 0.816 0.763 0.740C#1 0.880 0.878 0.883 0.878 0.881 0.878C#2 0.883 0.883 0.888 0.882 0.884 0.880C#3 0.885 0.885 0.890 0.884 0.887 0.883C#4 0.890 0.888 0.893 0.888 0.893 0.887C#5 0.893 0.892 0.897 0.892 0.897 0.892XSCMC#6 0.898 0.896 0.900 0.897 0.901 0.896Table 4.
F-1 measure by SCM and XSCM on sixtest sets with six chat language corpora andCNGIGA.Three observations are conducted on our re-sults.
First, according to Table 4, f-1 measure ofSCM with same training chat language corporadrops on time-varying test sets, but XSCM pro-duces much better f-1 measure consistently usingCNGIGA and same training chat language cor-pora (see Figure 3).
This proves that phoneticmapping models are helpful in XSCM method.The phonetic mapping models contribute in twoaspects.
On the one hand, they improve qualityof chat term normalization on individual test sets.On the other hand, satisfactory robustness isachieved consistently.0.690.710.730.750.770.790.810.830.850.870.890.91T#1 T#2 T#3 T#4 T#5 T#6SCM-C#1SCM-C#2SCM-C#3SCM-C#4SCM-C#5SCM-C#6XSCM-C#1XSCM-C#2XSCM-C#3XSCM-C#4XSCM-C#5XSCM-C#6`Figure 3.
Tendency on f-1 measure in SCM andXSCM on six test sets with six chat languagecorpora and CNGIGA.The second observation is conducted on pho-netic mapping models constructed withCNGIGA.
We find that 4,056,766 phonetic map-ping models are constructed in this experiment,while only 1,303,227 models are constructedwith NIL corpus in Experiment I.
This revealsthat coverage of standard Chinese corpus is cru-cial to phonetic mapping modeling.
We thencompare two character lists constructed with twocorpora.
The 100 characters most frequently usedin NIL corpus are rather different from those ex-tracted from CNGIGA.
We can conclude thatphonetic mapping models should be constructedwith a sound corpus that can represent standardlanguage.The last observation is conducted on f-1 meas-ure achieved by same methods on same test setsusing size-varying training chat language corpora.Both methods produce best f-1 measure with big-gest training chat language corpus C#6 on sametest sets.
This again proves that bigger  trainingchat language corpus could be helpful to improvequality of chat language term normalization.
Onequestion might be asked whether quality ofXSCM converges on size of the training chatlanguage corpus.
This question remains open dueto limited chat language corpus available to us.6.5 Error AnalysisTypical errors in our experiments belong mainlyto the following two types.999Err.1 Ambiguous chat termsExample-1: ???
8?In this example, XSCM finds no chat termwhile the correct normalization answer is ??????
(I still don?t understand)?.
Error illus-trated in Example-1 occurs when chat terms?8(eight, ba1)?
and ??
(meter, mi3)?
appear in achat sentence together.
In chat language, ???
insome cases is used to replace ??
(understand,ming2)?, while in other cases, it is used to repre-sent a unit for length, i.e.
meter.
When number?8?
appears before ??
?, it is difficult to tellwhether they are chat terms within sententialcontext.
In our experiments, 93 similar errorsoccurred.
We believe this type of errors can beaddressed within discoursal context.Err.2 Chat terms created in manners otherthan phonetic mappingExample-2: ??
ingIn this example, XSCM does not recognize?ing?
while the correct answer is ?(??)??
(I?m worrying)?.
This is because chat terms cre-ated in manners other than phonetic mapping areexcluded by the phonetic assumption in XSCMmethod.
Around 1% chat terms fall out of pho-netic mapping types.
Besides chat terms holdingsame form as showed in Example-2, we find thatemoticon is another major exception type.
Fortu-nately, dictionary-based method is powerfulenough to handle the exceptions.
So, in a realsystem, the exceptions are handled by an extracomponent.7 ConclusionsTo address the sparse data problem and dynamicproblem in Chinese chat text normalization, thephonetic mapping models are proposed in thispaper to represent mappings between chat termsand standard words.
Different from charactermappings, the phonetic mappings are constructedfrom available standard Chinese corpus.
We ex-tend the source channel model by incorporatingthe phonetic mapping models.
Three conclusionscan be made according to our experiments.Firstly, XSCM outperforms SCM with sametraining data.
Secondly, XSCM produces higherperformance consistently on time-varying testsets.
Thirdly, both SCM and XSCM performbest with biggest training chat language corpus.Some questions remain open to us regardingoptimal size of training chat language corpus inXSCM.
Does the optimal size exist?
Then whatis it?
These questions will be addressed in ourfuture work.
Moreover, bigger context will beconsidered in chat term normalization, discoursefor instance.AcknowledgementResearch described in this paper is partially sup-ported by the Chinese University of Hong Kongunder the Direct Grant Scheme project(2050330) and Strategic Grant Scheme project(4410001).ReferencesBrown, P. F., J. Cocke, S. A. D. Pietra, V. J. D. Pietra,F.
Jelinek, J. D. Lafferty, R. L. Mercer and P. S.Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, v.16 n.2,p.79-85.Gianforte, G.. 2003.
From Call Center to ContactCenter: How to Successfully Blend Phone, Email,Web and Chat to Deliver Great Service and SlashCosts.
RightNow Technologies.Graf, D., K. Chen, J.Kong and K. Maeda.
2005.
Chi-nese Gigaword Second Edition.
LDC CatalogNumber LDC2005T14.Heard-White, M., Gunter Saunders and Anita Pincas.2004.
Report into the use of CHAT in education.Final report for project of Effective use of CHATin Online Learning, Institute of Education, Univer-sity of London.James, F.. 2000.
Modified Kneser-Ney Smoothing ofn-gram Models.
RIACS Technical Report 00.07.Katz, S. M.. Estimation of probabilities from sparsedata for the language model component of a speechrecognizer.
IEEE Transactions on Acoustics,Speech and Signal Processing, 35(3):400-401.Li, H., W. He and B. Yuan.
2003.
An Kind of ChineseText Strings' Similarity and its Application inSpeech Recognition.
Journal of Chinese Informa-tion Processing, 2003 Vol.17 No.1 P.60-64.McCullagh, D.. 2004.
Security officials to spy on chatrooms.
News provided by CNET Networks.
No-vember 24, 2004.Xia, Y., K.-F. Wong and W. Gao.
2005.
NIL is notNothing: Recognition of Chinese Network Infor-mal Language Expressions.
4th SIGHAN Work-shop at IJCNLP'05, pp.95-102.Xia, Y. and K.-F. Wong.
2006a.
Anomaly Detectingwithin Dynamic Chinese Chat Text.
EACL?06NEW TEXT workshop, pp.48-55.Xia, Y., K.-F. Wong and W. Li.
2006b.
ConstructingA Chinese Chat Text Corpus with A Two-StageIncremental Annotation Approach.
LREC?06.1000
