Error-tolerant Finite-state Recognitionwith Applications to MorphologicalAnalysis and Spelling CorrectionKemal Oflazer*Bilkent UniversityThis paper presents the notion of error-tolerant recognition with finite-state recognizers alongwith results from some applications.
Error-tolerant recognition enables the recognition of stringsthat deviate mildly from any string in the regular set recognized by the underlying finite-staterecognizer.
Such recognition has applications to error-tolerant morphological processing, spellingcorrection, and approximate string matching in information retrieval.
After a description of theconcepts and algorithms involved, we give examples from two applications: in the context of mor-phological analysis, error-tolerant recognition allows misspelled input word forms to be correctedand morphologically analyzed concurrently.
We present an application of this to error-tolerantanalysis of the agglutinative morphology of Turkish words.
The algorithm can be applied tomorphological nalysis of any language whose morphology has been fully captured by a single(and possibly very large) finite-state transducer, egardless of the word formation processes andmorphographemic phenomena involved.
In the context of spelling correction, error-tolerant recog-nition can be used to enumerate candidate correct forms from a given misspelled string withina certain edit distance.
Error-tolerant recognition can be applied to spelling correction for anylanguage, if (a) it has a word list comprising all inflected forms, or (b) its morphology has beenfully described by a finite-state transducer.
We present experimental results for spelling correc-tion for a number of languages.
These results indicate that such recognition works very efficientlyfor candidate generation in spelling correction for many European languages (English, Dutch,French, German, and Italian, among others) with very large word lists of root and inflected forms(some containing well over 200,000 forms), generating all candidate solutions within 10 to 45milliseconds (with an edit distance of 1) on a SPARCStation 10/41.
For spelling correction inTurkish, error-tolerant recognition operating with a (circular) recognizer of Turkish words (withabout 29,000 states and 119,000 transitions) can generate all candidate words in less than 20milliseconds, with an edit distance of 1.1.
IntroductionError-tolerant finite-state recognition enables the recognition of strings that deviatemildly from any string in the regular set recognized by the underlying finite-staterecognizer.
For example, suppose we have a recognizer for the regular set over {a, b}described by the regular expression (aba + bab)*, and we would like to recognizeinputs that may be slightly corrupted, for example, abaaaba may be matched to abaaba(correcting for a spurious a), or babbb may be matched to babbab (correcting for a* Department of Computer Engineering and Information Science, Bilkent University, Ankara, TR-06533,Turkey@ 1996 Association for Computational LinguisticsComputational Linguistics Volume 22, Number 1deletion), or ababba may be matched to either abaaba (correcting a b to an a) or to ababab(correcting the reversal of the last two symbols).
Error-tolerant recognition can be usedin many applications that are based on finite-state recognition, such as morphologicalanalysis, spelling correction, or even tagging with finite-state models (Voutilainen andTapanainen 1993; Roche and Schabes 1995).
The approach presented in this paperuses the finite-state recognizer built to recognize the regular set, but relies on a veryefficiently controlled recognition algorithm based on depth-first searching of the stategraph of the recognizer.
In morphological nalysis, misspelled input word forms canbe corrected and morphologically analyzed concurrently.
In the context of spellingcorrection, error-tolerant recognition can universally be applied to the generation ofcandidate correct forms for any language, provided it has a word list comprisingall inflected forms, or its morphology has been fully described by automata such astwo-level finite-state transducers (Karttunen and Beesley 1992; Karttunen, Kaplan, andZaenen 1992).
The algorithm for error-tolerant recognition is very fast and applicableto languages that have productive compounding, or agglutination, or both, as wordformation processes.There have been a number of approaches to error-tolerant searching.
Wu and Man-ber (1991) describe an algorithm for fast searching, allowing for errors.
This algorithm(called agrep) relies on a very efficient pattern matching scheme whose steps can beimplemented with arithmetic and logical operations.
It is most efficient when the sizeof the pattern is limited to 32 to 64 symbols, though it allows for an arbitrary numberof insertions, deletions, and substitutions.
It is particularly suitable when the patternis small and the sequence to be searched is large.
Myers and Miller (1989) describealgorithms for approximate matching to regular expressions with arbitrary costs, butlike the algorithm described in Wu and Manber, these are best suited to applicationswhere the pattern or the regular expression is small and the sequence is large.
Schnei-der, Lim, and Shoaff (1992) present a method for imperfect string recognition usingfuzzy logic.
Their method is for context-free grammars (hence, it can be applied tofinite state recognition as well), but it relies on introducing new productions to allowfor errors; this may increase the size of the grammar substantially.2.
Error-tolerant Finite-State RecognitionWe can informally define error-tolerant recognition with a finite-state recognizer as therecognition of all strings in the regular set (accepted by the recognizer), and additionalstrings that can be obtained from any string in the set by a small number of unit editingoperations.The notion of error-tolerant recognition requires an error metric for measuringhow much two strings deviate from each other.
The edit distance between two stringsmeasures the minimum number of unit editing operations of insertion, deletion, re-placement of a symbol, and transposition of adjacent symbols (Damerau 1964) thatare necessary to convert one string into another.
Let Z = zl, z2 .
.
.
.
, Zp denote a genericstring of p symbols from an alphabet A. Z~\] denotes the initial substring of any stringZ up to and including the jth symbol.
We will use X (of length m) to denote themisspelled string, and Y (of length n) to denote the string that is a (possibly partial)candidate string.
Given two strings X and Y, the edit distance d(X\[m\], Y\[n\]) computedaccording to the recurrence below (Du and Chang 1992) gives the minimum numberof unit editing operations required to convert one string to the other.74Kemal Oflazer Error-tolerant Finite-state Recognitioned(X\[i+ 1\],Y\[j+ 1\]) = ed(X\[i\],Y~'\]) if xi+l = yj+l(last characters are the same)1 + min{ed(X\[i - 1\], Y\[j - 1\]),ed(X\[i + 1\], Y\[j\]),ed(X\[i\], Y~" + 1\])}if both xi = yj+land xi+l = yj(last two characters aretransposed)= 1 + min{ed(X\[i\], Y\[j\]), otherwiseed(X\[i + 1\], Y\[j\]),ed(X\[i\], Y~" + 1\])}ed(X\[O\],Y~'\]) = j 0 < j < ned(X\[i\],Y\[O\]) = i 0 < i < med(X\[-1\], Y~'\]) = ed(X\[i\], Y\[-1\]) = max(re, n) (boundary definitions)For example, ed(recoginze, recognize) = 1, since transposing i and n in the first stringwould give the second.
Similarly, ed(sailn,failing) = 3 since one could change the initials of the first string to f, insert an i before the n, and insert a g at the end to obtain thesecond string.A (deterministic) finite-state recognizer, R, is described by a 5-tuple R = (Q, A, 6,q0, F) with Q denoting the set of states, A denoting the input alphabet, 8 : Q x A ---, Qdenoting the state transition function, q0 E Q denoting the initial state, and F C_ Qdenoting the final states (Hopcroft and Ullman 1979).
Let L c A* be the regularlanguage accepted by  R. Given an edit distance error threshold t > 0, we define astring X\[m\] ~ L to be recognized by R with an error at most t, if the setC = {Y\[n\] I Y\[n\] c L and ed(X\[m\],Y\[n\]) < t}is not empty.2.1 An Algorithm for Error-tolerant RecognitionAny finite-state recognizer can also be viewed as a directed graph with arcs labeledwith symbols in A.
1 Standard finite-state recognition corresponds to traversing a path(possibly involving cycles) in the graph of the recognizer, starting from the start node,to one of the final nodes, so that the concatenation of the labels on the arcs alongthis path matches the input string.
For error-tolerant recognition, one needs to findall paths from the start node to one of the final nodes, so that when the labels on thelinks along a path are concatenated, the resulting string is within a given edit distancethreshold t, of the (erroneous) input string.
With t > 0, the recognition procedurebecomes a search on this graph, as shown in Figure 1.Searching the graph of the recognizer has to be fast if error-tolerant recognitionis to be of any practical use.
This means that paths that can lead to no solutionsmust be pruned, to limit the search to a very small percentage of the search space.Thus, we need to make sure that any candidate string generated as the search is beingperformed oes not deviate from certain initial substrings of the erroneous tring bymore than the allowed threshold.
To detect such cases, we use the notion of a cut-off1 We use state interchangably with node, and transition interchangeably with arc.75Computational Linguistics Volume 22, Number 1Figure 1Searching the recognizer graph.edit distance.
The cut-off edit distance measures the minimum edit distance betweenan initial substring of the incorrect input string, and the (possibly partial) candidatecorrect string.
Let Y be a partial candidate string whose length is n, and let X be theincorrect string of length m. Let 1 = max(l ,  n - t) and u = min(m, n + t).
The cut-offedit distance cuted(X\[m\], Y [n\]) is defined ascuted(X\[m\], Y [n\]) = min ed(X\[i\], Y\[n\]).l~ i~uFor example, with t = 2:cuted(reprter, repo)= min{ed(re, repo) = 2,ed(rep, repo) = 1,ed(repr, repo) = 1,ed(reprt, repo) = 2,ed(reprte, repo) = 3} = 1.Note that, except at the boundaries, the initial substrings of the incorrect string Xconsidered are of length n - t to length n + t. Any initial substring of X shorter than76Kemal Oflazer Error-tolerant Finite-state Recognition1 1 =n- t  = 2 U = n+t = 6 mX e P eCut-off distance is the min imumedit distance between Y and any init ialsubstr ing of X that ends in this range.Y e P 01 n=4Figure 2The cutoff edit distance.n - t needs more than t insertions, and any initial substring of X longer than n + trequires more than t deletions, to at least equal Y in length, violating the edit distanceconstraint (see Figure 2).Given an incorrect string X, a partial candidate string Y is generated by succes-sively concatenating relevant labels along the arcs as transitions are made, startingwith the start state.
Whenever we extend Y, we check if the cut-off edit distance of Xand the partial Y is within the bound specified by the threshold t. If the cut-off editdistance goes beyond the threshold, the last transition is backed off to the source node(in parallel with the shortening of Y) and some other transition is tried.
Backtrackingis recursively applied when the search cannot be continued from that state.
If, duringthe construction of Y, a final state is reached without violating the cut-off edit distanceconstraint, and ed(X\[m\], Y\[n\]) < t at that point, then Y is a valid correct form of theincorrect input string}Denoting the states by subscripted q's (q0 being the initial state) and the symbolsin the alphabet (and labels on the directed edges) by a, we present he algorithm forgenerating all Y's by a (slightly modified) depth-first probing of the graph in Figure 3.The crucial point in this algorithm is that the cut-off edit distance computation can beperformed very efficiently by maintaining a matrix H, an m by n matrix with elementH(i,j) = ed(X\[i\], Y\[j\]) (Du and Chang 1992).
We can note that the computation of theelement H(i + 1,j + 1) recursively depends on only H(i,j),H(i, j  + 1),H(i + 1,j) andH(i - 1, j -  1), from the earlier definition of edit distance (see Figure 4).During the depth-first search of the state graph of the recognizer, entries in columnn of the matrix H have to be (re)computed only when the candidate string is of2 Note that this check is essential, since we may come to other irrelevant final states dur ing the search.77Computational Linguistics Volume 22, Number 1/*push empty candidate, and start node to start search */push (( G qo ) )while stack not emptybeginpop((Y',qi)) /* pop partial surface string Y'and the node */for all qj and a such that 6(qi, a)=qjbegin /* extend the candidate string */Y = concat(Y',a) /* n is the current length of Y *//* check if Y has deviated too much, if not push-*/if cuted(X\[m\],Y\[n\]) K t then push((Y, qj))/* also see if we are at a final state */if ed(X\[m\],Y\[n\]) K t and qj 6 F then output YendendFigure 3Algorithm for error-tolerant recognition.
?...
H ( i -  1 , j -  1) .
.
.
.
.
.. .
.
.
.
.
H(i,j) H(i;/'+ 1) .... .
.
.
.
.
H( i+ I , j )  H( i+ l , j+ l )  ...Figure 4Computation of the elements of the H matrix.length n. During backtracking, the entries for the last column are discarded, but theentries in prior columns are still valid.
Thus, all entries required by H(i + 1,j + 1),except H(i , j  + 1), are already available in the matrix in columns i - 1 and i. Thecomputation of cuted(X\[m\], Y\[n\]) involves a loop in which the minimum is computed.This loop (indexing along column j + 1) computes H(i , j  + 1) before it is needed for thecomputation of H(i + 1,j + 1).We present in Figure 5 an example of this search algorithm for a simple finite-staterecognizer for the regular expression (aba + bab)*, and the search graph for the inputstring ababa.
The thick circles from left to right indicate the nodes at which we havethe matching strings abaaba, ababab, and bababa, respectively.
Prior visits to the finalstate 1 violate the final edit distance constraint.
(Note that the visit order of siblingsdepends on the order of the outgoing arcs from a state.)3.
Application to Error-tolerant Morphological AnalysisError-tolerant finite-state recognition can be applied to morphological nalysis.
Insteadof rejecting a given misspelled form, the analyzer attempts to apply the morphologicalanalysis to forms that are within a certain (configurable) dit distance of the incorrectform.
Two-level transducers (Karttunen and Beesley 1992; Karttunen, Kaplan, andZaenen 1992) provide a suitable model for the application of error-tolerant recognition.Such transducers capture all morphotactic and morphographemic phenomena, s wellas alternations in the language, in a uniform manner.
They can be abstracted as finite-state transducers over an alphabet of lexical and surface symbol pairs 1 : s, where either78Kemal Oflazer Error-tolerant Finite-state RecognitionFSR fo r  (ababa a+ bab)  *\[1\]\[0\]Eo/a\[0\] A\[0\]a\[1\]a , ,\[1\]bbb\[U\[2\]\[1l\[1\] ( 3 } \[0\]\[1\]\[1\]a\[i\]\[2\])  2lt l t21Search graph for matching ababa with threshold 1Figure 5Recognizer for (aba + bab)* and search graph for ababa.1 or s (but not both) may be the null symbol 0.
It is possible to apply error-tolerantrecognition to languages whose word formations employ productive compounding,or agglutination, or both.
In fact, error-tolerant recognition can be applied to anylanguage whose morphology has been described completely as one (very large) finite-state transducer.
Full-scale descriptions using this approach already exist for a numberof languages such as English, French, German, Turkish, and Korean (Karttunen 1994).Application of error-tolerant recognition to morphological nalysis proceeds asdescribed earlier.
After a successful match with a surface symbol the correspondinglexical symbol is appended to the output gloss string.
During backtracking the can-didate surface string and the gloss string are again shortened in tandem.
The basicalgorithm for this case is given in Figure 6.
3 The actual algorithm is a slightly optimizedversion of this, in which transitions with null surface symbols are treated as specialduring forward and backtracking traversals to avoid unnecessary computations of thecut-off edit distance.3 Note that transitions are now labeled with l : s pairs.79Computational Linguistics Volume 22, Number 1F igure  6/~push empty candidate string, and start nodeto start search on to the stack ~/push((G ?,q0))while stack not emptybeginpop((surface',lexical',qi)) /* pop part ia l  str ingsand the node from the stack ~/for a l l  qj and l :s  such that ~(qi,/:s) =qjbegin /~ extend the candidate string ~/surface = concat (surface', s)i f  cuted(X\[m\],surface\[n\]) G t thenbeginlexical = concat(lexical', 1)push ((surface, lexical, q j) )i f  ed(X\[m\],surface\[n\]) <_ t and qj E F thenoutput lexicalendendendAlgorithm for error-tolerant morphological nalysis.We can demonstrate error-tolerant morphological nalysis with a two-level trans-ducer for the analysis of Turkish morphology.
Agglutinative languages, such as Turk-ish, Hungarian or Finnish, differ from languages like English in the way lexical formsare generated.
Words are formed by productive affixations of derivational and in-flectional affixes to roots or stems, like beads on a string (Sproat 1992).
Furthermore,roots and affixes may undergo changes due to various phonetic interactions.
A typicalnominal or verbal root gives rise to thousands of valid forms that never appear inthe dictionary.
For instance, we can give the following (rather exaggerated) adverbexample from Turkish:uygarla~tzramayabileceklerimizdenmi~sinizcesinewhose root is the adjective uygar 'civilized'.
4 The morpheme breakdown (with mor-phological glosses underneath) is: 5uygar +la~ +tlr +ama +yabil +ecekcivilized +AtoV +CAUS +NEG +POT +VtoA(AtoN)+ler +imiz +den +mi~ +siniz +cesine+3PL +POSS-1PL +ABL(+NtoV) +PAST +2PL +VtoAdvThe portion of the word following the root consists of 11 morphemes, each of whicheither adds further syntactic or semantic information to, or changes the part-of-speechof, the part preceding it.
Although most words used in Turkish are considerably shorterthan this, this example serves to point out that the nature of word structure in Turkishand other agglutinative languages is fundamentally different from word structure inlanguages like English.Our morphological nalyzer for Turkish is based on a lexicon of about 28,000 root4 This is a manner adverb meaning roughly '(behaving) as if you were one of those whom we might notbe able to civilize.
'5 Glosses in parentheses indicate derivations not explicitly indicated by a morpheme.?
80Kemal Oflazer Error-tolerant Finite-state Recognitionwords and is a re-implementation, using Xerox two-level transducer technology (Kart-tunen and Beesley 1992), of an earlier version of the same description by the author(Oflazer 1993) (using the PC-KIMMO environment \[Antworth 1990\]).
This descriptionof Turkish morphology has 31 two-level rules that implement the morphographemicphenomena, such as vowel harmony and consonant changes across morpheme bound-aries, and about 150 additional rules, again based on the two-level formalism, thatfine-tune the morphotactics by enforcing long-distance feature sequencing and co-occurrence constraints.
They also enforce constraints imposed by standard alternationlinkage among various lexicons to implement he paradigms.
Turkish morphotacticsis circular, due to the presence of a relativization suffix in the nominal paradigm andmultiple causative suffixes in the verb paradigm.
There is also considerable linkagebetween ominal and verbal morphotactics, because derivational suffixation is produc-tive.
The minimized finite-state transducer constructed by composing the transducersfor root lexicons, morphographemic rules, and morphotactic constraints, has 32,897states and 106,047 transitions, with an average fan-out of about 3.22 transitions perstate (including transitions with null surface symbols).
It analyzes a given Turkishlexical form into a sequence of feature-value tuples (instead of the more conventionalsequence of morpheme glosses) that are used in a number of natural anguage appli-cations.
The Xerox software allows the resulting finite-state transducer to be exportedin a tabular form, which can be imported to other applications.This transducer has been used as input to an analyzer implementing the error-tolerant recognition algorithm in Figure 6.
The analyzer first attempts to parse theinput with t = 0, and if it fails, relaxes t up to 2 if it cannot find any parse with asmaller t. It can process about 150 (correct) forms a second on a SPARCstation 10/41.
6Below, we provide a transcript of a run: 7ENTER WORD > evaThreshold 0 ... i ...ela => ((CATevla => ((CATava => ((CATdeva => ((CAT NOUN)(ROOTeda => ((CAT NOUN)(ROOTela => ((CAT NOUN)(ROOTenva => ((CAT NOUN)(ROOTreva => ((CAT NOUN)(ROOTevi => ((CAT NOUN)(ROOTeve  => ((CAT NOUN)(ROOTev => ((CAT NOUN)(ROOTevi => ((CAT NOUN)(ROOTeza => ((CAT NOUN)(ROOTleva => ((CAT NOUN)(ROOTneva => ((CAT NOUN)(ROOTova => ((CAT NOUN)(ROOTova => ((CAT VERB)(ROOTADJ)(ROOT ela))ADJ)(ROOT evla))NOUN)(ROOT av)(AGR 3SG)(POSS NONE)(CASE DAT))deva)(AGR 3SG)(POSS NONE)(CASE NOM))eda)(AGR 3SG)(POSS NONE)(CASE NOM))ela)(AGR 3SG)(POSS NONE)(CASE NOM))enva)(AGR 3SG)(POSS NONE)(CASE NOM))reva)(AGR 3SG)(POSS NONE)(CASE NOM))ev)(AGR 3SG)(POSS NONE)(CASE ACC))ev)(AGR 3SG)(POSS NONE)(CASE OAT))ev)(AGR 3SG)(POSS NONE)(CASE NOM))ev)(AGR 3SG)(POSS 3SG)(CASE NOM))eza)(AGR 3SG)(POSS NONE)(CASE NOM))leva)(AGR 3SG)(POSS NONE)(CASE NOM))neva)(AGR 3SG)(POSS NONE)(CASE NOM))ova)(AGR 3SG)(POSS NONE)(CASE NOM))ov)(SENSE POS)(MOOD OPT)(AGR 3SG))ENTER WORD > ak111mnnikiler6 No attempt was made to compress the finite-state r cognizer.
The Xerox infl program working on theproprietary compressed representation f the same transducer can process about 1,000 forms/sec onthe same platform.7 The outputs have been slightly edited for formatting.
The feature names denote the usualmorphosyntactic features.
C0NV denotes derivations to the category indicated by the second token witha suffix or derivation type denoted by the third token, if any.81Computational Linguistics Volume 22, Number 1Threshold 0 ... i ... 2 ...ak1111nlnkiler =>((CATak1111nlnkiler =>((CATak1111ndakiler =>((CATNOUN)(ROOT ak11)(CONV ADJ LI)(CONV NOUN)(AGR 3SG) (POSS NONE)(CASE GEN)(CONV PRONOUN REL)(AGR 3PL)(POSS NONE)(CASE NOM))NOUN)(ROOT ak11)(CONV AD3 LI)(CONV NOUN)(AGR 3SG)(POSS 2SG)(CASE GEN)(CONV PRONOUN REL)(AGR 3PL)(POSS NONE)(CASE NOM))NOUN)(ROOT akxl)(CONV ADJ LI)(CONV NOUN)(AGR 3SG)(POSS 2SG)(CASE LOC)(CONV ADJ REL)(CONV NOUN)(AGR 3PL)(POSS NONE)(CASE NOM))ENTER WORD > eviminkinnThreshold 0 ... 1 ...eviminkini =>((CAT NOUN)(ROOT ev)(AGR 3SG)(POSS ISG)(CASE GEN)(CONV PRONOUN REL)(AGR 3SG)(POSS NONE)(CASE ACC))eviminkine =>((CAT NOUN)(ROOT ev)(AGR 3SG)(POSS ISG)(CASE GEN)(CONV PRONOUN REL)(AGR 3SG)(POSS NONE)(CASE DAT))eviminkinin =>((CAT NOUN)(ROOT ev)(AGR 3SG)(PGSS lSG)(CASE GEN)(CONV PRONOUN REL)(AGR 3SG)(POSS NONE)(CASE GEN))ENTER WORD > teeplerdekiThreshold 0 ...
I ...tepelerdeki =>((CAT NOUN)(ROOT tepe)(AGR 3PL)(POSS NONE)(CASE LOC)(CONV ADJ REL))teyplerdeki =>((CAT NOUN)(ROOT teyb)(AGR 3PL)(POSS NONE)(CASE LOC)(CONV ADJ REL))ENTER WORD > uygarla~tlramadlklarmllzdanml?slnlzcaslnaThreshold 0 ... 1 ...uygarla?tmramadlklarlmlzdanm1~slnlzcaslna =>((CAT ADJ)(ROOT uygar)(CONV VERB LAS)(VOICE CAUS)(SENSE NEG)(CONV ADJ DIK)(AGR 3PL)(POSS IPL)(CASE ABL)(CONV VERB)(TENSE NARR-PAST)(AGR 2PL)(CONV ADVERB CASINA)(TYPE MANNER))ENTER WORD > okatulnaThreshold 0 ... 1 ... 2 ...82Kemal Oflazer Error-tolerant Finite-state Recognitionokutulma =>((CATokutulma =>((CATokutulan =>((CATokutulana =>((CATokutulsa => ((CATokutula =>VERB)(RODT oku)(VOICE CAUS)(VOICE PASS)(SENSE NEG)(MOOD IMP)(AGR=2SG))VERB)(ROOT oku)(VOICE CAUS)(VOICE PASS)(SENSE POS)(CONV NOUN MA)(TYPE INFINITIVE)(AGE 3SG)(POSS NONE)(CASE NOM))VEKB)(ROOT oku)(VOICE CAUS)(VOICE PASS)(SENSE POS)(CONV ADJ YAN))VERB)(ROOT oku)(VOICE CAUS)(VOICE PASS)(SENSE POS)(CONV ADJ YAN)(CONV NOUN)(AGR 3SG)(POSS NONE)(CASE DAT))VERB)(ROOT oku)(VOICE CAUS)(VOICE PASS)(SENSE POS)(MOOD COND)(AGE 3SG))(CAT VERB)(ROOT oku)(VOICE CAUS)(VOICE PASS)(SENSE POS)(MOOD OPT)(AGR 3SG))In an application context, the candidates that are generated by such a morphologicalanalyzer can be disambiguated or filtered to a certain extent by constraint-based tag-ging techniques (see Oflazer and Kuru6z 1994; Voutilainen and Tapanainen 1993) thattake into account syntactic ontext for morphological disambiguation.4.
Applications to Spelling CorrectionSpelling correction is an important application for error-tolerant recognition.
Therehas been substantial work on spelling correction (see the excellent review by Ku-kich \[1992\]).
All methods essentially enumerate plausible candidates that resemble theincorrect word, and use additional heuristics to rank the results.
8 Most techniquesassume a word list of all words in the language.
These approaches are suitable forlanguages like English, for which it is possible to enumerate such a list.
They are notdirectly suitable or applicable to languages like German, which have very produc-tive compounding, or agglutinative languages like Finnish, Hungarian, or Turkish,in which the concept of a word is much larger than what is normally found in aword list.
For example, Finnish nouns have about 2,000 distinct forms, while Finnishverbs have about 12,000 forms (Gazdar and Mellish 1989, 59--60).
Turkish is similar:nouns, for instance, may have about 170 different forms, not counting the forms foradverbs, verbs, adjectives, or other nominal forms, generated (sometimes circularly)by derivational suffixes.
Hankamer (1989) gives much higher figures (in the millions)for Turkish; presumably he took derivations into account in his calculations.Some recent approaches to spelling correction have used morphological analysistechniques.
Veronis (1988) presents a method for handling quite complex combinationsof typographical nd phonographic errors (phonographic errors are the kind usuallymade by language learners using computer-aided instruction).
This method takes intoaccount phonetic similarity, in addition to standard errors.
Aduriz et al (1993) presenta two-level morphology approach to spelling correction in Basque.
They use two-level rules to describe common insertion and deletion errors, in addition to the two-level rules for the morphographemic component.
Oflazer and G6zey (1994) presenta two-level morphology approach to spelling correction in agglutinative languagesusing a coarser morpheme-based morphotactic description rather than the finer lexi-8 Ranking is dependent on the language, the application, and the error model.
It is an importantcomponent of the spelling correction problem, but is not addressed in this paper.83Computational Linguistics Volume 22, Number 1Recognizer for the word listabacus, abacuses, abalone, abandone, abandoned, abandoningaccess .Figure 7A finite-state recognizer for the word list: abacus, abacuses, abalone, abandone, abandoned,abandoning, access.cal/surface symbol approach presented here.
The approach presented in Oflazer andG6zey 1994 generates a valid sequence of the lexical forms of root and suffixes anduses a separate morphographemic component that implements the two-level rules toderive surface forms.
However, that approach is very slow, mainly because of the un-derlying PC-KIMMO morphological nalysis and generation system, and cannot dealwith compounding because of its approach to root selection.
More recently, Bowdenand Kiraz (1995) have used a multitape morphological nalysis technique for spellingcorrection in Semitic languages which, in addition to insertion, deletion, substitution,and transposition errors, allows for various language-specific errors.For languages like English, all inflected forms can be included in a word list, whichcan be used to construct a finite-state recognizer structured as a standard letter-treerecognizer (with an acyclic graph) as shown in Figure 7.
Error-tolerant recognition canbe applied to this finite-state recognizer.
Furthermore, transducers for morphologicalanalysis can be used for spelling correction, so the same algorithm can be appliedto any language whose morphology has been described using such transducers.
Wedemonstrate he application of error-tolerant recognition to spelling correction by con-structing finite-state recognizers in the form of letter trees from large word lists thatcontain root and inflected forms of words for 10 languages, obtained from a number ofresources on the Internet (Table 1).
The Dutch, French, German, English (two differentlists), Italian, Norwegian, Swedish, Danish, and Spanish word lists contained some orall inflected forms in addition to the basic root forms.
The Finnish word list containedunique word forms compiled from a corpus, although the language is agglutinative.For edit distance thresholds 1, 2, and 3, we selected 1,000 words at random fromeach word list and perturbed them by random insertions, deletions, replacements, andtranspositions, o that each misspelled word had the required edit distance from thecorrect form.
Kukich (1992), citing a number of studies, reports that typically 80%of misspelled words contain a single error of one of the unit operations, although84Kemal Oflazer Error-tolerant Finite-state RecognitionTable 1Statistics about the word lists used.Language Words Arcs Average Maximum AverageWord Word Fan-outLength LengthFinnish 276,448 968,171 12.01 49 1.31English-1 213,557 741,835 10.93 25 1.33Dutch 189,249 501,822 11.29 33 1.27German 174,573 561,533 12.95 36 1.27French 138,257 286,583 9.52 26 1.50English-2 104,216 265,194 10.13 29 1.40Spanish 86,061 257,704 9.88 23 1.40Norwegian 61,843 156,548 9.52 28 1.32Italian 61,183 115,282 9.36 19 1.84Danish 25,485 81,766 10.18 29 1.27Swedish 23,688 67,619 8.48 29 1.36Table 2Correction Statistics for Threshold 1.Average Average Average Time Average AverageLanguage Misspelled Correction to F i rst  Number of % ofWord Time Solution Solutions SpaceLength (msec) (msec) Found SearchedFinnish 11.08 45.45 25.02 1.72 0.21English-1 9.98 26.59 12.49 1.48 0.19Dutch 10.23 20.65 9.54 1.65 0.20German 11.95 27.09 14.71 1.48 0.20French 10.04 15.16 6.09 1.70 0.28English-2 9.26 17.13 7.51 1.77 0.35Spanish 8.98 18.26 7.91 1.63 0.37Norwegian 8.44 16.44 6.86 2.52 0.62Italian 8.43 9.74 4.30 1.78 0.46Danish 8.78 14.21 1.98 2.25 1.00Swedish 7.57 16.78 8.87 2.83 1.57Turkish (FSR) 8.63 17.90 7.41 4.92 1.23in specific applications the percentage of such errors is lower.
Our earlier study ofan error model developed for spelling correction in Turkish indicated similar results(Oflazer and G/izey 1994).Tables 2, 3, and 4 present he results from correcting these misspelled word listsfor edit distance thresholds 1, 2, and 3, respectively.
The runs were performed on aSPARCstation 10/41.
The second column in these tables gives the average length ofthe misspelled string in the input list.
The third column gives the time in millisecondsto generate all solutions, while the fourth column gives the time to find the firstsolution.
The fifth column gives the average number of solutions generated from thegiven misspelled strings with the given edit distance.
Finally, the last column givesthe percentage of the search space (that is, the ratio of forward-traversed arcs to thetotal number of arcs) that is searched when generating all the solutions.85Computational Linguistics Volume 22, Number 1Table 3Correction Statistics for Threshold 2.LanguageAverage Average Average Time Average AverageMisspelled Correction to F irst  Number of % ofWord Time Solution Solutions SpaceLength (msec) (msec) Found SearchedFinnish 11.05 312.26 162.49 13.54 1.30English-1 9.79 232.56 108.69 7.90 1.51Dutch 10.24 148.62 68.19 9.35 1.25German 12.05 169.88 96.55 3.33 1.14French 9.88 95.07 37.52 6.99 1.44English-2 9.12 129.29 55.64 12.56 2.28Spanish 8.78 125.35 48.80 10.24 2.49Norwegian 8.36 112.06 42.13 27.27 3.47Italian 8.41 57.87 25.09 8.09 2.36Danish 9.15 82.39 34.80 13.25 4.23Swedish 7.44 90.59 16.47 36.37 6.84Turkish (FSR) 8.59 164.81 57.87 55.12 11.12Table 4Correction Statistics for Threshold 3.Average Average Average Time Average AverageLanguage Misspelled Correction to First Number of % ofWord Time Solution Solutions SpaceLength (msec) (msec) Found SearchedFinnish 11.08 1217.56 561.70 157.39 3.86English-1 9.73 1001.43 413.60 87.09 5.30Dutch 10.30 610.52 256.90 71.89 4.07German 11.82 582.45 305.80 21.39 3.14French 9.99 349.41 122.38 41.58 4.00English-2 9.36 519.83 194.69 97.24 6.97Spanish 8.90 507.46 176.77 88.31 7.79Norwegian 8.47 400.57 125.52 199.72 8.98Italian 8.34 198.79 66.80 55.47 6.41Danish 9.25 228.55 47.9 97.85 8.69Swedish 7.69 295.14 36.89 267.51 14.70Turkish (FSR) 8.57 907.02 63.59 442.17 60.004.1 Spelling Correction for Agglutinative Word FormsThe transducer for Turkish developed for morphological analysis, using the Xeroxsoftware, was also used for spelling correction.
However, the original transducer hadto be simplified into a recognizer for two reasons.
First, for morphological analysis,the concurrent generation of the lexical gloss string requires that occasional transitionswith an empty surface symbol be taken to generate the gloss properly.
Secondly, inmorphological nalysis, a given surface form may have many morphological interpre-tations.
This diversity must be accounted for in morphological processing.
In spellingcorrection, however, the presentation of only one surface form is sufficient.
To removeall empty transitions and analyses with the same surface form from the Turkish trans-ducer, a recognizer recognizing only the surface forms was extracted using the Xeroxtool ifsm.
The resulting recognizer had 28,825 states and 118,352 transitions labeled86Kemal Oflazer Error-tolerant Finite-state Recognitionwith just surface symbols.
The average fan-out of the states in this recognizer wasabout 4.
This transducer was then used to perform spelling correction experiments inTurkish.In the first set of experiments, three word lists of 1,000 words each were gener-ated from a Turkish corpus, and words were perturbed as described before, for errorthresholds of 1, 2, and 3, respectively.
The results for correcting these words are pre-sented in the last rows (labeled Turkish \[FSR\]) of the tables above.
It should be notedthat the percentage of search space searched may not be very meaningful in this casesince the same transitions may be taken in the forward direction more than once.In a separate xperiment that would simulate a real correction application, about3,000 misspelled Turkish words (again compiled from a corpus) were processed bysuccessively relaxing the error threshold starting with t = 1.
Of this set of words,79.6% had an edit distance of 1 from the intended correct form, while 15.0% had anedit distance of 2, and 5.4% had an edit distance of 3 or more.
The average lengthof the incorrect strings was 9.63 characters.
The average correction time was 77.43milliseconds (with 24.75 milliseconds for the first solution).
The average number ofcandidates offered per correction was 4.29, with an average of 3.62% of the search spacebeing traversed, indicating that this is a very viable approach for real applications.
Forcomparison, the same recognizer running as a spell checker (t = 0) can process correctforms at a rate of about 500 words/sec.5.
Conc lus ionsThis paper has presented an algorithm for error-tolerant finite-state r cognition that en-ables a finite-state r cognizer to recognize strings that deviate mildly from some stringin the underlying regular set.
Results of its application to error-tolerant morphologi-cal analysis and candidate generation i spelling correction were also presented.
Theapproach is very fast and applicable to any language with a list of root and inflectedforms, or with a finite-state transducer recognizing or analyzing its word forms.
Itdiffers from previous error-tolerant finite-state r cognition algorithms in that it uses agiven finite-state machine, and is more suitable for applications where the number ofpatterns (or the finite-state machine) is large and the string to be matched is small.In some cases, however, the proposed approach may not be efficient and may beaugmented with language-specific heuristics: For instance, in spelling correction, users(at least in Turkey, as indicated by our error model \[Oflazer and Gfizey 1994\]) usuallyreplace non-ASCII characters with their nearest ASCII equivalents because of inconve-niences uch as nonstandard keyboards, or having to input the non-ASCII charactersusing a sequence of keystrokes.
In the last spelling correction experiment for Turk-ish, almost all incorrect forms with an edit distance of 3 or more had three or morenon-ASCII Turkish characters, all of which were rendered with the nearest ASCII ver-sion (e.g., ya~g~n~m~zde (on our birthday) was written as yasgunumuzde).
These formscould surely be found with appropriate edit distance thresholds, but at the cost of gen-erating many words containing more substantial errors.
Under these circumstances,one may use language-specific heuristics first, before resorting to error-tolerant recog-nition, along the lines suggested by morphological-analysis-based pproaches (Adurizet al 1993; Bowden and Kiraz 1995).Although the method escribed here does not handle rroneous cases where omis-sion of space characters causes joining of otherwise correct forms (such as inspite of),such cases may be handled by augmenting the final state(s) of the recognizers with atransition for space characters and ignoring all but one of such space characters in theedit distance computation.87Computational Linguistics Volume 22, Number 1AcknowledgmentsThis research was supported in part by aNATO Science for Stability GrantTU-LANGUAGE.
I would like to thankXerox Advanced Document Systems, andLauri Karttunen of Xerox Parc and of RankXerox Research Centre (Grenoble), forproviding the two-level transducerdevelopment software.
Kemal Olkii andKurtulu~ Yorulmaz of Bilkent Universityimplemented some of the algorithms.
Iwould like to thank the anonymousreviewers for suggestions and commentsthat contributed to the improvement of thepaper in many respects.ReferencesAduriz, I., et al (1993).
A MorphologicalAnalysis-based Method for SpellingCorrection.
In Proceedings, Sixth Conferenceof the European Chapter of the Association forComputational Linguistics, Utrecht, TheNetherlands, 463-464.Antworth, Evan L. (1990).
PC-KIMMO: ATwo-level Processor for MorphologicalAnalysis.
Summer Institute of Linguistics,Dallas, Texas.Bowden, Tanya and Kiraz, George A.
(1995).A Morphographemic Model for ErrorCorrection in Nonconcatenative Strings.In Proceedings, 33 rd Annual Meeting of theAssociation for Computational Linguistics,Boston, MA, 24-30.Damerau, E J.
(1964).
A Technique forComputer Detection and Correction ofSpelling Errors.
Communications of theAssociation for Computing Machinery, 7(3):171-176.Du, M. W. and Chang, S. C. (1992).
A Modeland a Fast Algorithm for Multiple ErrorsSpelling Correction.
Acta Informatica, 29:281-302.Gazdar, Gerald and Mellish, Chris.
(1989).Natural Language Processing in PROLOG,An Introduction to Computational Linguistics.Addison-Wesley Publishing Company,Reading, MA.Hankamer, Jorge.
(1989).
"MorphologicalParsing and the Lexicon."
In LexicalRepresentation a d Process, edited byW.
Marslen-Wilson.
MIT Press, 392-408.Hopcroft, John E. and Ullman, Jeffrey D.(1979).
Introduction to Automata Theory,Languages, and Computation.Addison-Wesley Publishing Company,Reading, MA.Karttunen, Lauri.
(1994).
ConstructingLexical Transducers.
In Proceedings, 16 thInternational Conference on ComputationalLinguistics, Kyoto, Japan, 1: 406-411,International Committee onComputational Linguistics.Karttunen, Lauri and Beesley, Kenneth R.(1992).
"Two-level Rule Compiler.
"Technical Report, XEROX Palo AltoResearch Center.Karttunen, Lauri; Kaplan, Ronald M.; andZaenen, Annie.
(1992).
Two-levelMorphology with Composition.
InProceedings, 15 th International Conference onComputational Linguistics, Nantes, France,1: 141-148. International Committee onComputational Linguistics.Kukich, Karen.
(1992).
Techniques forAutomatically Correcting Words in Text.ACM Computing Surveys, 24: 377-439.Myers, Eugene W. and Miller, Webb.
(1989).Approximate Matching of RegularExpressions.
Bulletin of MathematicalBiology, 51(1): 5-37.Oflazer, Kemal.
(1993).
Two-levelDescription of Turkish Morphology.
InProceedings, Sixth Conference ofthe EuropeanChapter of the Association for ComputationalLinguistics, Utrecht, The Netherlands, 472.
(A full version appears in Literary andLinguistic Computing, 9(2): 137-148.
)Oflazer, Kemal and Giizey, Cemalettin.(1994).
Spelling Correction inAgglutinative Languages.
In Proceedings,4 th Conference on Applied Natural LanguageProcessing, Stuttgart, Germany, 194-195.Oflazer, Kemal and Kuru6z, ilker.
(1994).Tagging and MorphologicalDisambiguation of Turkish Text.
InProceedings, 4 th Conference on AppliedNatural Language Processing, Stuttgart,Germany, 144-149.Roche, Emmanuel and Schabes, Yves.(1995).
Deterministic Part-of-speechTagging with Finite-state Transducers.Computational Linguistics, 21(2): 227-253.Schneider, Mordechay; Lira, H.; and Shoaff,William.
(1992).
The Utilization of FuzzySets in the Recognition of ImperfectStrings.
Fuzzy Sets and Systems, 49:331-337.Sproat, Richard.
(1992).
Morphology andComputation.
MIT Press, Cambridge, MA.Veronis, Jean.
(1988).
MorphosyntacticCorrection in Natural LanguageInterfaces.
In Proceedings, 13 th InternationalConference on Computational Linguistics,708-713.
International Committee onComputational Linguistics.88Kemal Oflazer Error-tolerant Finite-state RecognitionVoutilainen, Atro and Tapanainen, Pasi.(1993).
Ambiguity Resolution in aReductionistic Parser.
In Proceedings, SixthConference ofthe European Chapter of theAssociation for Computational Linguistics,Utrecht, The Netherlands, 394-403.Wu, Sun and Manber, Udi.
(1991).
"FastText Searching with Errors."
TechnicalReport TR91-11, Department ofComputer Science, University of Arizona.89
