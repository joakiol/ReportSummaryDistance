Coling 2010: Poster Volume, pages 72?80,Beijing, August 2010Composition of Semantic Relations: Model and ApplicationsEduardo Blanco, Hakki C. Cankaya and Dan MoldovanHuman Language Technology Research InstituteThe University of Texas at Dallas{eduardo,candan,moldovan}@hlt.utdallas.eduAbstractThis paper presents a framework for com-bining semantic relations extracted fromtext to reveal even more semantics thatotherwise would be missed.
A set of 26 re-lations is introduced, with their argumentsdefined on an ontology of sorts.
A seman-tic parser is used to extract these relationsfrom noun phrases and verb argumentstructures.
The method was successfullyused in two applications: rapid customiza-tion of semantic relations to arbitrary do-mains and recognizing entailments.1 IntroductionSemantic representation of text facilitates infer-ences, reasoning, and greatly improves the per-formance of Question Answering, InformationExtraction, Machine Translation and other NLPapplications.
Broadly speaking, semantic rela-tions are unidirectional underlying connectionsbetween concepts.
For example, the noun phrasethe car engine encodes a PART-WHOLE relation:the engine is a part of the car.Semantic relations are the building blocks forcreating a semantic structure of a sentence.
Thereis a growing interest in text semantics fueled bythe new wave of semantic technologies and on-tologies that aim at transforming unstructured textinto structured knowledge.
More and more enter-prises and academic organizations have adoptedthe World Wide Web Consortium (W3C) Re-source Description Framework (RDF) specifica-tion as a standard representation of text knowl-edge.
This is based on semantic triples, which canbe used to represent semantic relations.The work reported in this paper aims at extract-ing as many semantic relations from text as possi-ble.
Semantic parsers (SP) extract semantic rela-tions from text.
Typically they detect relations be-tween adjacent concepts or verb argument struc-tures, leaving considerable semantics unrevealed.For example, given John is a rich man, a typicalSP extracts John is a man and man is rich, but notJohn is rich.
The third relation can be extractedby combining the two relations detected by theparser.
The observation that combining elemen-tary semantic relations yields more relations is thestarting point and the motivation for this work.2 Related WorkIn Computational Linguistics, WordNet (Miller,1995), FrameNet (Baker et al, 1998) and Prop-Bank (Palmer et al, 2005) are probably the mostused semantic resources.
Like our approach andunlike PropBank, FrameNet annotates semanticsbetween concepts regardless of their position in aparse tree.
Unlike us, they use a predefined set offrames to be filled.
PropBank adds semantic an-notation on top of the Penn TreeBank and it con-tains only annotations between a verb and its ar-guments.
Moreover, the semantics of a given labeldepends on the verb.
For example, ARG2 is usedfor INSTRUMENT and VALUE.Copious work has been done lately on seman-tic roles (Ma`rquez et al, 2008).
Approaches todetect semantic relations usually focus on partic-ular lexical and syntactic patterns or kind of ar-guments.
There are both unsupervised (Turney,2006) and supervised approaches.
The SemEval-2007 Task 4 (Girju et al, 2007) focused on rela-tions between nominals.
Work has been done ondetecting relations between noun phrases (Davi-dov and Rappoport, 2008; Moldovan et al, 2004),named entities (Hirano et al, 2007), and clauses(Szpakowicz et al, 1995).
There have been pro-72posals to detect a particular relation, e.g., CAUSE(Chang and Choi, 2006), INTENT (Tatu, 2005) andPART-WHOLE (Girju et al, 2006).Researchers have also worked on combining se-mantic relations.
Harabagiu and Moldovan (1998)combine WordNet relations and Helbig (2005)transforms chains of relations into theoretical ax-ioms.
Some use logic as the underlying formal-ism (Lakoff, 1970; Sa?nchez Valencia, 1991), moreideas can be found in (Copestake et al, 2001).3 ApproachIn contrast to First Order Logic used in AI to rep-resent text knowledge, we believe text semanticsshould be represented using a fixed set of rela-tions.
This facilitates a more standard represen-tation and extraction automation which in turn al-lows reasoning.
The fewer the relation types, theeasier it is to reason and perform inferences.
Thus,a compromise has to be made between havingenough relation types to adequately represent textknowledge and yet keeping the number small formaking the extraction and manipulation feasible.The main contributions of this paper are: (i) anextended definition of a set of 26 semantic rela-tions resulted after many iterations and pragmaticconsiderations; (ii) definition of a semantic calcu-lus, a framework to manipulate and compose se-mantic relations (CSR); (iii) use of CSR to rapidlycustomize a set of semantic relations; and (iv) useof CSR to detect entailments.
The adoption ofCSR to other semantic projects does not requireany modification of existing tools while being ableto detect relations ignored by such tools.4 Semantic RelationsFormally, a semantic relation is represented asR(x, y), where R is the relation type and x andy the first and second argument.
R(x, y) should beread as x is R of y.
The sentence ?John painted histruck?
yields AGENT(John, painted ), THEME(histruck, painted ) and POSSESSION(truck, John).Extended definition Given a semantic relation R,DOMAIN(R) and RANGE(R) are defined as the setof sorts of concepts that can be part of the firstand second argument.
A semantic relation R(x,y) is defined by its: (i) relation type R; (ii) DO-MAIN(R); and (iii) RANGE(R).
Stating restric-tions for DOMAIN and RANGE has several advan-tages: it (i) helps distinguishing between relations,e.g., [tall]ql and [John]aco can be linked throughVALUE, but not POSSESSION; (ii) helps discard-ing potential relations that do not hold, e.g., ab-stract objects do not have INTENT; and (iii) helpscombining semantic relations (Section 5).Ontology of Sorts In order to define DOMAIN(R)and RANGE(R), we use a customized ontologyof sorts (Figure 1) modified from (Helbig, 2005).The root corresponds to entities, which refers to allthings about which something can be said.Objects can be either concrete or abstract.
Theformer occupy space, are touchable and tangi-ble.
The latter are intangible; they are somehow aproduct of human reasoning.
Concrete objects arefurther divided into animate or inanimate.
The for-mer have life, vigor or spirit; the later are dull,without life.
Abstract objects are divided into tem-poral or non temporal.
The first corresponds to ab-stractions regarding points or periods of time (e.g.July, last week); the second to any other abstrac-tion (e.g.
disease, justice).
Abstract objects can besensually perceived, e.g., pain, odor.Situations are anything that happens at a timeand place.
Simply put, if one can think of the timeand location of an entity, it is a situation.
Events(e.g.
mix, grow) imply a change in the status ofother entities, states (e.g.
standing next to thedoor) do not.
Situations can be expressed by verbs(e.g.
move, print) or nouns (e.g.
party, hurricane).Descriptors complement entities by stating prop-erties about their spatial or temporal context.
Theyare composed of an optional non-content wordsignaling the local or temporal context and anotherentity.
Local descriptors are further composed ofa concrete object or situation, e.g., [above]prep [theroof]co; temporal descriptors by a temporal abstractobject or situation, e.g., [during]prep [the party]ev .The non-content word signaling the local or tempo-ral context is usually present, but not always, e.g.,?The [birthplace]ev of his mother is [Ankara]loc?.Qualities represent characteristics than can beassigned to entities.
They can be quantifiable liketall and heavy, or unquantifiable like difficult andsleepy.
Quantities represent quantitative character-istics of concepts, e.g., a few pounds, 22 yards.73Entity [ent]Situation [si]State [st] Event [ev]Quantity [qn] Object [o]Concrete [co]Animate [aco] Inanimate [ico]Abstract [ao]Temporal [tao] Non temporal [ntao]Quality [ql] Descriptor [des]Temporal [tmp] Local [loc]Figure 1: The ontology of sorts of concepts and their acronyms.PropertiesCluster Relation type Abr.
Class.
r s t DOMAIN ?
RANGE ExampleReasonCAUSE CAU iv - -?
[si] ?
[si] CAU(earthquake, tsunami )JUSTIFICATION JST iv - -?
[si ?
ntao] ?
[si] JST(it is forbidden, don?t smoke)INFLUENCE IFL iv - -?
[si] ?
[si] IFL(missing classes, poor grade)Goal INTENT INT i - - - [si] ?
[aco] INT(teach, professor)PURPOSE PRP v - -?
[si ?
ntao] ?
[si ?
co ?
ntao] PRP(storage, garage)Object modifiers VALUE VAL v - - - [ql] ?
[o ?
si] VAL(smart, kids)SOURCE SRC ii - -?
[loc ?
ql ?
ntao ?
ico] ?
[o] SRC(Mexican, students)Syntactic subjectsAGENT AGT iii - - - [aco] ?
[si] AGT(John, bought )EXPERIENCER EXP iii - - - [o] ?
[si] EXP(John, heard )INSTRUMENT INS iii - - - [co ?
ntao] ?
[si] INS(the hammer, broke)Direct objectsTHEME THM iii - - - [o] ?
[ev] THM(a car, bought )TOPIC TPC iii - - - [o ?
si] ?
[ev] TPC(flowers, gave)STIMULUS STI iii - - - [o] ?
[ev] STI(the train, heard )Association ASSOCIATION ASO v?
?
?
[ent] ?
[ent] ASO(fork, knife)KINSHIP KIN ii?
?
?
[aco] ?
[aco] KIN(John, his wife)NoneIS-A ISA ii - -?
[o] ?
[o] ISA(gas guzzler, car)PART-WHOLE PW ii - - * [o] ?
[o] ?
[l] ?
[l] ?
[t] ?
[t] PW(engine, car)MAKE MAK ii - - - [co ?
ntao] ?
[co ?
ntao] MAK(cars, BMW)POSSESSION POS ii - -?
[co] ?
[co] POS(Ford F-150, John)MANNER MNR iii - - - [ql ?
st ?
ntao] ?
[si] MNR(quick, delivery)RECIPIENT RCP iii - - - [co] ?
[ev] RCP(Mary, gave)SYNONYMY SYN v?
?
?
[ent] ?
[ent] SYN(a dozen, twelve)AT-LOCATION AT-L v?- * [o ?
si] ?
[loc] AT-L(party, John?s house)AT-TIME AT-T v?- * [o ?
si] ?
[tmp] AT-T(party, last Saturday)PROPERTY PRO v - - - [ntao] ?
[o ?
si] PRO(height, John)QUANTIFICATION QNT v - - - [qn] ?
[si ?
o] QNT(a dozen, eggs)Table 1: The set of 26 relations clustered and classified with their properties (reflexive, symmetric,transitive) and examples.
An asterisk indicates that the property holds under certain conditions.4.1 Semantic Relation TypesThis work focuses on the set of 26 semantic rela-tions depicted in Table 1.
We found this set spe-cific enough to capture the most frequent seman-tics of text without bringing unnecessary overspe-cialization.
The set is inspired by several pre-vious proposals.
Fillmore introduced the notionof case frames and proposed a set of nine roles:AGENT, EXPERIENCER, INSTRUMENT, OBJECT,SOURCE, GOAL, LOCATION, TIME and PATH(Fillmore, 1971).
Fillmore?s work was extendedto FrameNet (Baker et al, 1998).
PropBank(Palmer et al, 2005) annotates a set of 17 seman-tic roles in a per-verb basis.We aim to encode relations not only betweena verb and its arguments, but also between andwithin noun phrases and adjective phrases.
There-fore, more relations are added to the set.
Itincludes relations present in WordNet (Miller,1995), such as IS-A, PART-WHOLE and CAUSE.Szpakowicz et al (1995) proposed a set of ninerelations and Turney (2006) a set of five.
Rosarioand Hearst (2004) proposed a set of 38 relationsincluding standard case roles and a set of specificrelations for medical domain.
Helbig (2005) pro-posed a set of 89 relations, including ANTONYMYand several TEMPORAL relations, e.g.
SUCCES-SION, EXTENSION, END.Our set clusters some of the previous propos-als (e.g.
we only consider AT-TIME) and discardsrelations proposed elsewhere when they did notoccur frequently enough in our experiments.
Forexample, even though ANTONYMY and ENTAIL-MENT are semantically grounded, they are veryinfrequent and we do not deal with them.
Ourpragmatic goal is to capture as many semantics aspossible with as few relations as possible.
How-74ever, we show (Section 7.1) that our set can beeasily customized to a specific domain.The 26 relations are clustered such that rela-tions belonging to the same cluster are close inmeaning.
Working with clusters is useful becauseit allows us to: (i) map to other proposed relations,justifying the chosen set of relations; (ii) workwith different levels of specificity; and (iii) reasonwith the relations in a per cluster basis.The reason cluster includes relations between aconcept having a direct impact on another.
CAU(x,y) holds if y would not hold if x did not happen.JST(x, y) encodes a moral cause, motive or so-cially convened norm.
If IFL(x, y), x affects theintensity of y, but it does not affect its occurrence.The goal cluster includes INT and PRP.
INT(x,y) encodes intended consequences, which are vo-litional.
PRP(x, y) is a broader relation and can bedefined for inanimate objects.The object modifiers cluster encodes descriptionsof objects and situations: SRC(x, y) holds if x ex-presses the origin of y. VAL(x, y) holds for anyother attribute, e.g.
heavy, handsome.The syntactic subjects cluster includes relationslinking a syntactic subject and a situation.
The dif-ferences rely on the characteristics of the subjectand the connection per se.
AGT(x, y) encodes anintentional doer, x must be volitional.
If EXP(x,y), x does not change the situation, it only expe-riences y; it does not participate intentionally in yeither.
If INS(x, y), x is used to perform y, x is atool or device that facilitates y.The direct objects cluster includes relations en-coding syntactic direct objects.
THM(x, y) holdsif x is affected or directly involved by y. TPC(x, y)holds if y is a communication verb, like talk andargue.
STI(x, y) holds if y is a perception verband x a stimulus that makes y happen.The association cluster includes ASO and KIN.ASO is a broad relation between any pair of enti-ties; KIN encodes a relation between relatives.The rest of the relations do not fall into anycluster.
ISA, PW, SYN, AT-L and AT-T have beenwidely studied in the literature.
MAK(x, y) holdsif y makes or produces x; POS(x, y) holds if yowns x; MNR encodes the way a situation occurs.RCP captures the connection between an event andan object which is the receiver of the event.
PROdescribes links between a situation or object andits characteristics, e.g., height, age.
Values to thecharacteristics are given through VAL.
QNT(x, y)holds if y is quantitatively determined by x.Relations can also be classified depending onthe kind of concepts they describe and their in-tra or inter nature into: (i) Intra-Object; (ii) Inter-Objects; (iii) Intra-Situation; (iv) Inter-Situations;and (v) for Object and Situation description.4.2 Detection of Semantic RelationsRelations are extracted by an in-house SP froma wide variety of syntactic realizations.
For ex-ample, the compound nominal steel knife con-tains PW(steel, knife), whereas carving knife con-tains PRP(carving, knife); the genitive Mary?s toycontains POS(toy, Mary), whereas Mary?s brothercontains KIN(brother, Mary), and eyes of the babycontains a PW(eyes, baby).
Relations are also ex-tracted from a verb and its arguments (NP verb,verb NP, verb PP, verb ADVP and verb S), adjec-tive phrases and adjective clauses.The SP first uses a combination of state-of-the-art text processing tools, namely, part-of-speechtagging, named entity recognition, syntactic pars-ing and word sense disambiguation.
After a can-didate syntactic pattern has been found, a series ofmachine learning classifiers are applied to decideif a relation holds.
Four different algorithms areused: decision trees, Naive Bayes, SVM and Se-mantic Scattering combined in a hybrid approach.Some algorithms use a per-relation approach (i.e.,decide whether or not a given relation holds) andothers a per-pattern approach (i.e., which relation,if any, holds for a particular pattern).
Additionally,human-coded rules are used for a few unambigu-ous cases.
The SP participated in the SemEval2007 Task 4 (Badulescu and Srikanth, 2007).5 Composition of Semantic RelationsThe goal of semantic calculus (SC) is to providea formal framework for manipulating semantic re-lations.
CSR is a part of this, its goal is to applyinference axioms over already identified relationsin text in order to infer more relations.Semantic Calculus: Operators and PropertiesThe composition operator is represented by the75(R?1)?1 = RRi ?
Rj = (Rj?1 ?
Ri?1)?1R?1 inherits all the properties of R?
?1 = ?
?i: ?
??
RiR is reflexive iff ?x: R(x, x)R is symmetric iff R(x, y) = R(y, x)R is transitive iff R(x, y) ?
R(y, z) ?
R(x, z)Ri ?
Rj ?
Ri?1 ?
Rj?1Ri ??
Rj ?
Ri?1 ??
Rj?1If Ri is symmetric and Ri ??
Rj , Ri?1 ??
RjIf Rj is symmetric and Ri ??
Rj , Ri ??
Rj?1Table 2: Semantic calculus propertiessymbol ?.
It combines two relations and yieldsa third one.
Formally, we denote R1 ?
R2 ?
R3.The inverse of R is denoted R?1 and can be ob-tained by simply switching its arguments.
GivenR(x, y), R?1(y, x) always holds.
The easiest wayto read R?1(y, x) is x is R of y.R1 left dominates R2, denoted by R1 ?
R2,iff the composition of R1 and R2 yields R1, i.e.,R1 ?
R2 iff R1 ?
R2 ?
R1.
R1 right dominates R2,denoted by R1 ?
R2, iff the composition of R2 andR1 yields R1, i.e., R1 ?
R2 iff R2 ?
R1 ?
R1.
R1completely dominates R2, denoted by R1 ??
R2, iffR1 ?
R2 and R1 ?
R2, i.e., R1 ??
R2 iff R1 ?
R2 ?R1 and R2 ?
R1 ?
R1.An OTHER (?)
relation holds between x and yif no relation from the given set holds.
Formally,?
(x, y) iff ?
?Ri such that Ri(x, y).Using the notation above, the properties de-picted in Table 2 follow.Necessary conditions for Combining RelationsAxioms can be defined only for compatible rela-tions as premises.
R1 and R2 are compatible if itis possible, from a theoretical point of view, to ap-ply the composition operator to them.
Formally,RANGE(R1) ?
DOMAIN(R2) 6= ?If R1 and R2 are compatible but not equal arestriction occurs.
Let us denote RANGE(R1) ?DOMAIN(R2) = I .
A backward restriction takesplace if RANGE(R1) 6= I and a forward restric-tion if DOMAIN(R2) 6= I .
In the former caseRANGE(R1) is reduced; in the later DOMAIN(R2)is reduced.
A forward and backward restrictioncan be found with the same pair of relations.It is important to note that two compatible rela-tions may not be the premises for a valid axiom.For example, KIN and AT-L are compatible but donot yield any valid inference.Another necessary condition for combining tworelations R1(x, y) and R2(y, z) is that they have tohave a common argument, y.5.1 Unique axiomsAn axiom is defined as a set of relations calledpremises and a conclusion.
Given the premises itunequivocally yields a relation that holds as con-clusion.
The composition operator is the basicway of combining two relations to form an axiom.In general, for n relations there are(n2)=n(n?1)2 different pairs.
For each pair, taking intoaccount the two relations and their inverses, thereare 4 ?
4 = 16 different possible combinations.Applying property Ri ?
Rj = (Rj?1 ?
Ri?1)?1,only 10 combinations are unique: (i) 4 combineR1, R2 and their inverses; (ii) 3 combine R1 andR1?1; and (iii) 3 combine R2 and R2?1.
The mostinteresting axioms fall into category (i), since theother two can be resolved by the transitivity prop-erty of a relation and its inverse.For n relations there are 2n2 + n potential ax-ioms:(n2)?4+3n = 2?n(n?1)+3n = 2n2+n.For n = 26, there are 1300 potential axioms in (i),820 of which are compatible.The number can be further reduced.
After man-ual examination of combinations of ASO and KINwith other relations, we conclude that they do notyield any valid inferences, invalidating 150 poten-tial axioms.
This is due to the broad meaning ofthese relations.
QNT can be discarded as well, in-validating 45 more potential axioms.Some axioms can be easily validated.
Becausesynonymous concepts are interchangeable, SYN iseasily combined with any other relation: SYN(x,y) ?
R(y, z) ?
R(x, z) and R(x, y) ?
SYN(y, z) ?R(x, z).
Because hyponyms inherit relations fromtheir hypernyms, ISA(x, y) ?
R(y, z) ?
R(x, z)and R(x, y) ?
ISA?1(y, z) ?
R(x, z) hold.
Theseobservations allow us to validate 138 of the 625potential axioms left, still leaving 487.As noted before, relations belonging to thesame cluster tend to behave similarly.
This is es-pecially true for the reason and goal clusters dueto their semantic motivation.
Working with thesetwo clusters instead of the relations brings the76(1) reason ?
goal (2) reason?1 ?
goalx reason //IFL????????
ygoalzxPRP????????
ygoalreasonooz(3) goal ?
reason (4) goal ?
reason?1xgoalIFL???????
?yreason// zxIFL?1???????
?goaly zreasonooTable 3: The four axioms taking as premises rea-son and goal clusters.
Diagonal arrows indicateinferred relations.number of axioms to be examined down to 370.Out of the 370 axioms left, we have extensivelyanalyzed and defined the 35 involving AT-L, the43 involving reason and the 58 involving goal.
Be-cause of space constraints, in this paper we onlyfully introduce the axioms for reason and goal(Section 6), as well as a variety of axioms usefulto recognize textual entailments (Section 7.2).6 Case Study: Reason and GoalIn this section, we present the four unique axiomsfor reason and goal relations (Table 3).
(1) REA(x, y) ?
GOA(y, z) ?
IFL(x, z): anevent is influenced by the reason of its goal.For example: Bill saves money because he isunemployed; he spends far less than he used to.Therefore, being unemployed can lead to spendfar less.P REA(be unemployed, save money)GOA(save money, spend far less)C IFL(be unemployed, spend far less)(2) REA?1(x, y) ?
GOA(y, z) ?
PRP(x, z):events have as their purpose the effects of theirgoals.
This is a strong relation.For example: Since they have a better view,they can see the mountain range.
They cut the treeto have a better view.
Therefore, they cut the treeto see the mountain range.P REA?1(see the mountain range, better view)GOA(better view, cut the tree)C PRP(see the mountain range, cut the tree)Note that possible unintended effects of cuttingthe tree (e.g.
homeowners?
association complains)are caused by the event cut the tree, not by its ef-fect get a better view.
(3) GOA(x, y) ?
REA(y, z) ?
IFL(x, z): thegoal of an action influences its effects.For example: John crossed the street carelesslyto get there faster.
He got run over by a propanetruck.
Therefore, John got run over by a propanetruck influenced by (having the goal of) gettingthere faster.P GOA(get there faster, crossed carelessly)REA(crossed carelessly, got run over )C IFL(get there faster, got run over)(4) GOA(x, y) ?
REA?1(y, z) ?
IFL?1(x, z).Events influence the goals of its effects.For example: Jane exercises to lose weight.
Sheexercised because of the good weather.
Therefore,good weather helps to lose weight.P GOA(lose weight, exercise)REA?1(exercise, good weather)C IFL?1(lose weight, good weather)The axioms have been evaluated using manu-ally annotated data.
PropBank CAU and PNC areused as reason and goal.
Reason annotation is fur-ther collected from a corpus which adds causalannotation to the Penn TreeBank (Bethard et al,2008).
A total of 5 and 29 instances for axioms3 and 4 were found.
For all of them, the ax-ioms yield a valid inference.
For example, Buick[approached]y American express about [a jointpromotion]x because [its card holders generallyhave a good credit history]z .
PropBank annota-tion states GOA(x, y) and REA?1(y, z), axiom 4makes the implicit relation IFL?1(x, z) explicit.7 Applications and Results7.1 Customization of Semantic RelationsProblem There is no agreement on a set of rela-tions that best represent text semantics.
This isrightfully so since different applications and do-mains call for different relations.
CSR can be usedto rapidly customize a set of relations without hav-ing to train a new SP or modify any other tool.Given a text, the SP extracts 26 elementary se-mantic relations.
Axioms within the frameworkof CSR yield n new relations, resulting in a richersemantic representation (Figure 2).CSR axioms Two ways to get new relations are:(i) Direct mapping.
This is the easiest case andit is equivalent to rename a relation.
For example,we can map POS to BELONG or IS-OWNER-OF.77Axiom Rest.
on y ExampleAGT(x, y) ?
THM?1(y, z) ?
ARRESTED(x, z) arrested concept [Police]x [apprehended]y 51 [football fans]z.THM(x, y) ?
AT-L(y, z) ?
ARRESTED-AT(x, z) arrested concept Police [apprehended]y 51 [fans]x [near the Dome]z.AGT(x, y) ?
AT-L(y, z) ?
BANKS-AT(x, z) banking activity [John]x [withdrew]y $20 [at the nearest Chase]z.POS(x, y) ?
AT-L(y, z) ?
BANKS-AT(x, z) account concept [John]x got a [checkbook]y at [Chase]z.Table 4: Examples of semantic relation customization using CSR.Pair Text T Hypothesis H113Belknap married and lost his first two wives, Cora LeRoy and CarrieTomlinson, and married Mrs. John Bower, his second wife?s sister.Belknap was married to Carrie Tomlinson.T1 AGT(Belknap, married ) H1 AGT(Belknap, was married )T2 THM(wives, married ) H2 THM(Carrie Tomlinson, was married )T3 QNT(first two, wives)T4 ISA(Carrie Tomlinson, wives)429India?s yearly pilgrimage to the Ganges river, worshiped by Hindus asthe goddess Ganga, is the world?s largest gathering of people, .
.
.Ganga is a Hindu goddess.T1 AGT(Hindus, worship) H1 ISA(Ganga, goddess)T2 THM(Ganga, worship) H2 VAL(Hindu, goddess)T3 ISA(Ganga, goddess)445[.
.
. ]
At present day YouTube represents the most popular site sharingon-line video.YouTube is a video website.T1 ISA(YouTube, site) H1 ISA(YouTube, website)T2 EXP(site, sharing) H2 VAL(video, website)T3 THM(video, sharing)716The Czech and Slovak republics have been unable to agree a politicalbasis for their future coexistence in one country.The Czech and Slovak republics do not agree to coexist in one country.T1 AGT(The Czech and Slovak republics, have beenunable to agree)H1 AGT(The Czech and Slovak republics, do notagree)T2 THM(political basis, have been unable to agree) H2 PRP(coexist in one country, do not agree)T3 PRP(their future coexistence in one country, po-litical basis)771In 2003, Yunus brought the microcredit revolution to the streets ofBangladesh to support more than 50,000 beggars, whom the GrameenBank respectfully calls Struggling Members.Yunus supported more than 50,000 Struggling Members.T1 AGT(Yunus, brought ) H1 AGT(Yunus, supported )T2 PRP(support, brought )T3 RCP(beggars, support ) H2 RCP(Struggling Members, support )T4 QNT(more than 50,000, beggars) H3 QNT(more than 50,000, Struggling Members)T5 SYN(beggars, Struggling Members)Table 5: RTE3 examples and their elementary semantic relations (i.e., the ones the SP detects).
Onlyrelevant semantic relations for entailment detection are shown for T .Text // Semantic Parser 26 relations //Inference axioms // CSR n new sr //EDBC@AOOFigure 2: Flowchart for obtaining customized se-mantic relations using CSR.
(ii) Combinations of two elementary relationsyield new specialized relations.
In this case, re-strictions on the arguments must be fulfilled.Consider we need the new relation AR-RESTED(x, y), which encodes the relation be-tween two animate concrete objects x and y, wherex arrested y.
We can infer this relation by usingthe following axiom: AGENT(x, y) ?
THEME?1(y,z) ?
ARRESTED(x, z) provided that y is an ar-rested concept.
A simple way of checking if agiven concept is of a certain kind is to checkWordNet.
Collecting all the words belonging thethe synset arrest.v.1, we get the following list ofarrested concepts: collar, nail, apprehend, pickup, nab and cop.
Using lexical chains the listcould be further improved.More examples of axioms for generating cus-tomized semantic relations are shown in Table 4.Results Virtually any domain could be coveredby applying customization over the set of 26relations.
The set has been successfully cus-tomized to a law enforcement domain.
Ax-78ioms for a total of 37 new relations were de-fined and implemented.
Among others, ax-ioms to infer IS-EMPLOYER, IS-COWORKER, IS-PARAMOUR, IS-INTERPRETER, WAS-ASSASSIN,ATTENDS-SCHOOL-AT, JAILED-AT, COHABITS-WITH, AFFILIATED-TO, MARRIED-TO, RENTED-BY, KIDNAPPED-BY and the relations in Table 4were defined.
Note that a relation can be inferredby several axioms.
This customization effort toadd 37 new specialized relations took a persononly a few days and without modifying the SP.7.2 Textual EntailmentProblem An application of CSR is recognizingentailments.
Given text T and hypothesis H , thetask consists on determining whether or not H canbe inferred by T (Giampiccolo et al, 2007).CSR axioms Several examples of the RTE3 chal-lenge can be solved by applying CSR (Table 5).The rest of this section depicts the axioms in-volved in detecting entailment for each pair.Pair 113 is a simple one.
A perfect matchfor H in T can be obtained by an axiom readingall concepts inherit the semantic relations of theirhypernyms.
Formally, ISA(x, y) ?
THM(y, z) ?THM(x, z), T2 and T4 are the premises and theconclusion matches H2.
T1 matches H1.Pair 429 can be solved by an axiom read-ing agents are values for their themes.
Formally,AGT(x, y) ?
THM?1(y, z) ?
VAL(x, z); T1 andT2 yield VAL(Hindu, Ganga), which combinedwith T3 results in a match between T and H .Pair 445 follows a similar pattern, but the wayan EXP combines with its THM differs from theway an AGT does.
The theme is a value of theexperiencer, THM(x, y) ?
EXP?1(y, z) ?
VAL(x,z).
Given T2 and T3, the axiom yields T4:VAL(video, site).
Assuming that SYN(site, web-site), T1 and T4 match H .Pair 716 also requires only one inference step.Using T3 and T2, an axiom reading situationshave as their purpose the purpose of its theme in-fers H2, yielding a perfect match between T andH .
Formally, PRP(x, y) ?
THM(y, z) ?
PRP(x, z).Pair 771 Using as premises T1 and T2, an ax-iom reading an agent performs the purposes of itsactions infers H1.
Using T3 and T5, and T4and T5 as premises, an axiom reading synony-mous concepts are interchangeable infers H2 andH3, resulting in a perfect match between T andH .
Formally, AGT(x, y) ?
PRP?1(y, z) ?
AGT(x,z), RCP?1(x, y) ?
SYN(y, z) ?
RCP?1(x, z) andQNT(x, y) ?
SYN(y, z) ?
QNT(x, z).Results We conducted two experiments to quan-tify the impact of CSR in detecting entailments.First, 60 pairs were randomly selected from theRTE3 challenge and parsed with the SP.
14 ofthem (23%) could be solved by simply matchingthe elementary relations in T and H .
After apply-ing CSR, 21 more pairs (35%) were solved.
Thus,adding CSR on top of the SP clearly improves en-tailment detection.
Out of the 25 pairs not solved,5 (8%) need coreference resolution and 20 (34%)require commonsense knowledge or fairly com-plicated reasoning methods (e.g.
a shipwreck is aship that sank).CSR has also been added to a state of the artsystem for detecting textual entailment (Tatu andMoldovan, 2007).
Prior to the addition, the sys-tem made 222 errors consisting of 46 false nega-tives (examples in Table 5) and 176 false positives.CSR was able to correctly solve 18 (39%) of the46 false negatives.8 ConclusionsAlthough the idea of chaining semantic relationshas been proposed before, this paper provides aformal framework establishing necessary condi-tions for composition of semantic relations.
TheCSR presented here can be used to rapidly cus-tomize a set of relations to any arbitrary domain.In addition to the customization of an informa-tion extraction tool and recognizing textual entail-ments, CSR has the potential to contribute to otherapplications.
For example, it can help improve asemantic parser, it can be used to acquire com-monsense knowledge axioms and more.When an axiom that results from combiningtwo relations does not always hold, it may be pos-sible to add constraints that limit the arguments ofthe premises to only some concepts.This work stems from the need to automate theextraction of deep semantics from text and repre-senting text as semantic triples.
The paper demon-strates that CSR is able to extract more relationsthan a normal semantic parser would.79ReferencesBadulescu, Adriana and Munirathnam Srikanth.
2007.LCC-SRN: LCC?s SRN System for SemEval 2007Task 4.
In Proceedings of the Fourth InternationalWorkshop on Semantic Evaluations, pages 215?218.Baker, Collin F., Charles J. Fillmore, and John B.Lowe.
1998.
The Berkeley FrameNet Project.
InProceedings of the 17th international conference onComputational Linguistics, Montreal, Canada.Bethard, Steven, William Corvey, Sara Klingenstein,and James H. Martin.
2008.
Building a Corpus ofTemporal-Causal Structure.
In Proceedings of theSixth International Language Resources and Evalu-ation Conference, Marrakech, Morocco.Chang, Du S. and Key S. Choi.
2006.
Incrementalcue phrase learning and bootstrapping method forcausality extraction using cue phrase and word pairprobabilities.
Information Processing & Manage-ment, 42(3):662?678.Copestake, Ann, Alex Lascarides, and Dan Flickinger.2001.
An Algebra for Semantic Construction inConstraint-based Grammars.
In Proceedings of39th Annual Meeting of the ACL, pages 140?147.Davidov, Dmitry and Ari Rappoport.
2008.
Classifi-cation of Semantic Relationships between NominalsUsing Pattern Clusters.
In Proceedings of ACL-08:HLT, pages 227?235, Columbus, Ohio.Fillmore, Charles J.
1971.
Some Problems for CaseGrammar.
Monograph Series on Languages andLinguistics, 24:35?36.Giampiccolo, Danilo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The Third PASCAL Recog-nizing Textual Entailment Challenge.
In Proceed-ings of the ACL-PASCAL Workshop on Textual En-tailment and Paraphrasing, pages 1?9.Girju, Roxana, Adriana Badulescu, and DanMoldovan.
2006.
Automatic Discovery ofPart-Whole Relations.
Computational Linguistics,32(1):83?135.Girju, Roxana, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.SemEval-2007 Task 04: Classification of SemanticRelations between Nominals.
In Proceedings of theFourth International Workshop on Semantic Evalu-ations, pages 13?18, Prague, Czech Republic.Harabagiu, Sanda and Dan Moldovan.
1998.
Knowl-edge Processing on an Extended WordNet.
In Fell-baum, Christiane, editor, WordNet: An ElectronicLexical Database and Some of its Applications,chapter 17, pages 684?714.
The MIT Press.Helbig, Hermann.
2005.
Knowledge Representationand the Semantics of Natural Language.
Springer.Hirano, Toru, Yoshihiro Matsuo, and Genichiro Kikui.2007.
Detecting Semantic Relations betweenNamed Entities in Text Using Contextual Features.In Proceedings of the 45th Annual Meeting of theACL, Demo and Poster Sessions, pages 157?160.Lakoff, George.
1970.
Linguistics and Natural Logic.Synthese, 22(1):151?271.Ma`rquez, Llu?
?s, Xavier Carreras, Kenneth C.Litkowski, and Suzanne Stevenson.
2008.
Seman-tic Role Labeling: An Introduction to the SpecialIssue.
Computational Linguistics, 34(2):145?159.Miller, George A.
1995.
WordNet: A LexicalDatabase for English.
Communications of the ACM,38:39?41.Moldovan, Dan, Adriana Badulescu, Marta Tatu,Daniel Antohe, and Roxana Girju.
2004.
Mod-els for the Semantic Classification of Noun Phrases.In HLT-NAACL 2004: Workshop on ComputationalLexical Semantics, pages 60?67.Palmer, Martha, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?106.Rosario, Barbara and Marti Hearst.
2004.
ClassifyingSemantic Relations in Bioscience Texts.
In Proc.
ofthe 42nd Meeting of the ACL, pages 430?437.Sa?nchez Valencia, Victor.
1991.
Studies on NaturalLogic and Categorial Grammar.
Ph.D. thesis, Uni-versity of Amsterdam.Szpakowicz, Barker, Ken Barker, and Stan Szpakow-icz.
1995.
Interactive semantic analysis of Clause-Level Relationships.
In Proceedings of the SecondConference of the Pacific ACL, pages 22?30.Tatu, Marta and Dan Moldovan.
2007.
COGEXat RTE 3.
In Proceedings of the ACL-PASCALWorkshop on Textual Entailment and Paraphrasing,pages 22?27, Prague, Czech Republic.Tatu, Marta.
2005.
Automatic Discovery of Intentionsin Text and its Application to Question Answering.In Proceedings of the ACL Student Research Work-shop, pages 31?36, Ann Arbor, Michigan.Turney, Peter D. 2006.
Expressing Implicit Seman-tic Relations without Supervision.
In Proceedingsof the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of theACL, pages 313?320, Sydney, Australia.80
