Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 14?22,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsComparing Semantic Role Labeling with Typed Dependency Parsing inComputational Metaphor IdentificationEric P. S. BaumerDepartment of InformaticsUniv of California, Irvine5029 Donald Bren HallIrvine, CA 92627-3440 USAebaumer@ics.uci.eduJames P. WhiteSchool of Information andComputer SciencesUniv of California, IrvineIrvine, CA 92627jpwhite@uci.eduBill TomlinsonDepartment of InformaticsUniv of California, Irvine5068 Donald Bren HallIrvine, CA 92627-3440 USAwmt@uci.eduAbstractMost computational approaches to metaphorhave focused on discerning betweenmetaphorical and literal text.
Recent workon computational metaphor identification(CMI) instead seeks to identify overarchingconceptual metaphors by mapping selectionalpreferences between source and target cor-pora.
This paper explores using semantic rolelabeling (SRL) in CMI.
Its goals are two-fold:first, to demonstrate that semantic roles caneffectively be used to identify conceptualmetaphors, and second, to compare SRL tothe current use of typed dependency parsingin CMI.
The results show that SRL can beused to identify potential metaphors andthat it overcomes some of the limitationsof using typed dependencies, but also thatSRL introduces its own set of complications.The paper concludes by suggesting futuredirections, both for evaluating the use of SRLin CMI, and for fostering critical and creativethinking about metaphors.1 IntroductionMetaphor, the partial framing of one concept interms of another, pervades human language andthought (Lakoff and Johnson, 1980; Lakoff, 1993).A variety of computational approaches to metaphor-ical language have been developed, e.g., (Martin,1990; Fass, 1991; Gedigian et al, 2006; Krishnaku-maran and Zhu, 2007).
However, most such meth-ods see metaphor as an obstacle to be overcome inthe task of discerning the actual, literal meaning of aphrase or sentence.In contrast, the work presented here approachesconceptual metaphor not as an obstacle but as aresource.
Metaphor is an integral part in humanunderstanding of myriad abstract or complex con-cepts (Lakoff and Johnson, 1980), and metaphori-cal thinking can be a powerful component in crit-ical and creative thinking, cf.
(Gordon, 1974;Oxman-Michelli, 1991).
However, ?because theycan be used so automatically and effortlessly, wefind it hard to question [metaphors], if we caneven notice them?
(Lakoff and Turner, 1989, p.65).
Computational metaphor identification (CMI)(Baumer, 2009; Baumer et al, under review) ad-dresses this difficulty by identifying potential con-ceptual metaphors in written text.
Rather than at-tempting to discern whether individual phrases aremetaphorical or literal, this technique instead iden-tifies larger, overarching linguistic patterns.
Thegoal of CMI is not to state definitively the metaphorpresent in a text, but rather to draw potentialmetaphors to readers?
attention, thereby encourag-ing both critical examination of current metaphorsand creative generation of alternative metaphors.CMI identifies potential metaphors by mappingselectional preferences (Resnik, 1993) from a sourcecorpus to a target corpus.
Previous work on CMIutilized typed dependency parses (de Marneffe etal., 2006) to calculate these selectional preferences.This paper explores the use of semantic role labeling(SRL) (Gildea and Jurafsky, 2002; Johansson andNugues, 2008) to calculate selectional preferences.Typed dependencies focus on syntactic structure andgrammatical relations, while semantic roles empha-size conceptual and semantic structure, so SRL may14be more effective for identifying potential concep-tual metaphors.
This paper describes how SRL wasincorporated into CMI and compares both the rela-tional data and the metaphors identified with typeddependency parsing and semantic role labeling.
Theresults show that semantic roles enabled effectiveidentification of potential metaphors.
However, nei-ther typed dependencies nor semantic roles werenecessarily superior.
Rather, each provides certainadvantages, both in terms of identifying potentialmetaphors, and in terms of promoting critical think-ing and creativity.2 Related Work2.1 Computational Approaches to MetaphorMany computational approaches have been takentoward identifying metaphor in written text.
MI-DAS (Martin, 1990) attempts to detect when users ofthe Unix Consultant command line help system usemetaphors, for example, ?How do I enter Emacs??
isinterpreted as ?How do I invoke Emacs??
Anothersystem, met* (Fass, 1991), is designed to distinguishboth metaphor and metonymy from literal text, pro-viding special techniques for processing these in-stances of figurative language.
More recently, Gedi-gian et al (2006) used hand-annotated corpora totrain an automatic metaphor classifier.
Krishnaku-maran and Zhu (2007) used violations of WordNet-based (Fellbaum, 1998) verb-noun expectations toidentify the presence of a metaphor, e.g., ?he is abrave lion,?
would be considered metaphorical, be-cause ?he,?
taken to mean a ?person,?
which is not aWordNet hyponym of ?lion.
?These and similar approaches ascribe to somedegree to the literal meaning hypothesis (Reddy,1969), which states that every sentence has a lit-eral meaning, as derived from the meanings of itsconstituent words, while some also have a figurativemeaning that goes beyond the meanings of the wordsthemselves.
In this view, a figurative interpretationis only sought only after a literal interpretation hasbeen formed and found inconsistent, nonsensical, orotherwise faulty.
However, experimental evidencehas made this account suspect (Gibbs, 1984; Gen-tner et al, 2001).
Even distinguishing whether agiven expression is literal or figurative can be dif-ficult at best.
For example, ?the rock is becomingbrittle with age?
(Reddy, 1969, p. 242), has ?a lit-eral interpretation when uttered about a stone and ametaphorical one when said about a decrepit profes-sor emeritus?
(Fass, 1991, p. 54).One previous metaphor system avoids makingsuch literal/metaphorical distinctions.
CorMet (Ma-son, 2004) is designed to extract known con-ventional metaphors from domain-specific textualcorpora, which are derived from Google queries.CorMet calculates selectional preferences and asso-ciations (Resnik, 1993) for each corpus?s character-istic verbs, i.e., those verbs at least twice as frequentin the corpus as in general English.
Based on theseselectional associations, CorMet clusters the nounsfor which the characteristic verbs select.
To iden-tify metaphors, mappings are sought from clustersin the source corpus to clusters in the target corpus,based on the degree to which the same verbs selectfor members of both clusters.
For example, CorMetwas used to extract the metaphor MONEY IS A LIQ-UID1 by mapping from a cluster for the concept liq-uid in a corpus for the domain LABORATORY toa cluster for the concept money in a corpus for thedomain FINANCE, based on the selectional associ-ations of verbs such as ?pour,?
?flow,?
?freeze,?
and?evaporate.?
The CMI system described in this pa-per is informed largely by CorMet (Mason, 2004).2.2 Semantic Role LabelingWhile interpretations vary somewhat, semantic rolelabeling (SRL) generally aims to represent some-thing about the meaning of a phrase at a deeperlevel than surface syntactic structure.
One of themost common approaches to performing SRL au-tomatically is to use a statistical classifier trainedon labeled corpora (Gildea and Jurafsky, 2002),with FrameNet (Baker et al, 1998) and PropBank(Palmer et al, 2005) being the primary sources.
Animportant result of the Gildea and Jurafsky workwas identifying the significant utility of using pre-segmented constituents as input to their labeler, andaccordingly most SRL systems perform a syntacticanalysis as an initial step.The principal alternative to using a statistical clas-sifier is to use a rule-based labeler for operating on1SMALL CAPS are metaphors, italics are concepts, CAPSare domains, and ?quotes?
are example phrases.15the syntactic parse tree.
For example, Shi and Mi-halcea (2004) extract explicit SRL rules by analyz-ing FrameNet cases.
Another system, RelEx (Fun-del et al, 2006) also uses rules and is structured likethe implementation used here (see below for details),but despite having the same name, is a different sys-tem.
Statistical and rule-based methods may also beused within the same system, such as in LTH (Jo-hansson and Nugues, 2008).One reason for preferring a rule-based SRL sys-tem is that rule-based approaches may be less sus-ceptible to the loss of accuracy that statisticallytrained classifiers suffer when applied to domainsthat are different than the corpora they are trainedon (Johansson and Nugues, 2008).
That problem iscompounded by the limited domain coverage pro-vided by the labeled corpora currently available forSRL classifier training (Gildea and Jurafsky, 2002).3 Computational Metaphor IdentificationWhile space precludes a fully detailed descriptionof the algorithms involved, this section provides ahigh-level summary of the techniques employed inCMI (Baumer, 2009; Baumer et al, under review).Metaphors are conceptual mappings wherein asource concept partially structures the understand-ing of a target concept.
In ELECTION IS WAR, thetarget concept election is partially framed in termsof the source concept war.
CMI begins by gather-ing corpora for the source and target domains.
Inthis paper, the target corpus consists of posts frompolitical blogs, described in more detail in the meth-ods section below.
Source corpora are composed ofWikipedia articles, as they provide a readily avail-able, categorically organized, large source of con-tent on a wide variety of topics.
A source corpusfor a given domain consists of all the Wikipedia ar-ticles in the category for that domain, as well as allarticles in its subcategories.
All documents in thesource and target corpora are parsed to extract sen-tence structure and typed dependencies (Klein andManning, 2003; de Marneffe et al, 2006).The crux of CMI is selectional preference learn-ing (Resnik, 1993), which quantifies the tendency ofparticular words to appear with certain other classesof words in specific grammatical relationships.
Forexample, words for the concept of food are oftenthe direct object of the verb ?eat.?
Using the parseddocuments, CMI calculates selectional preferencesof the characteristic nouns in a corpus, where char-acteristic means that the noun is highly frequent inthe corpus relative to its frequency in general En-glish, as derived from (Kilgarriff, 1996).
Selectionalpreference is quantified as the relative entropy of theposterior distribution conditioned on a specific nounand grammatical relation with respect to the priordistribution of verbs in general English:S(c) =?vP (v|c) logP (v|c)P (v)(1)where c is a class of nouns (i.e., a concept likefood) and a grammatical relation (such as direct ob-ject), and v ranges over all the verbs for which c ap-pears in the given relation.
These selectional prefer-ence strengths are then divided among the verbs thatappear in each grammatical relation to determine thenoun class?s selectional association for each verb ineach relation (Resnik, 1993).Selectional associations are calculated for classesof words, but the corpora consist of words that mayrepresent many possible classes of nouns.
Thus, in-dividual nouns count as partial observations of eachword class that they might represent using WordNet(Fellbaum, 1998).
For example, ?vote,?
?primary,?and ?runoff?
can all represent the concept of elec-tion.
Here we use a customized version of WordNetthat includes major political figures from the 2008US Election.
These word classes are then clusteredusing two-nearest-neighbor clustering based on theverbs for which they select.
Each cluster representsa coherent concept in the corpus, and each is auto-matically labeled based on the synsets it contains.This approach of using clustered hypernyms res-onates with Lakoff?s argument that metaphoricalmappings occur not at the level of situationalspecifics, but at the superordinate level.
For exam-ple, in the metaphor LOVE IS A JOURNEY, the re-lationship is a vehicle.
Although specific instantia-tions of the metaphor may frame that vehicle var-iously as a train (?off the track?
), a car (?long,bumpy road?
), or a plane (?just taking off?
), ?thecategories mapped will tend to be at the superordi-nate level rather than the basic level?
(Lakoff, 1993,p.
212).
This method of counting each word ob-served as a partial observation of each of the synsets16it might represent causes observations at the basiclevel to accumulate in the superordinate levels theycollectively represent.
This is not to say that hier-archical conceptual relations capture every possiblemetaphor, but rather that these are the relations onwhich we focus here.To identify metaphors, CMI looks for correspon-dences between conceptual clusters in the sourceand target corpora.
For example, in the Military cor-pus, the cluster for war would frequently select to bethe direct object of ?win,?
the object of the preposi-tion ?during?
with the verb ?fight,?
the object of thepreposition ?in?
with the verb ?defeated,?
and so on.In some blog corpora, the cluster for election also se-lects for those same verbs in the same grammaticalrelationships.
Based on the similarity of these selec-tional associations, each mapping is given a confi-dence score to indicate how likely the linguistic pat-terns are to evidence a conceptual metaphor.
One ofthe strengths of CMI is that it works in the aggre-gate.
While individual instances of phrases such as?fought during the election?
and ?defeated in the pri-mary?
may not at first glance appear metaphorical,it is the systematicity of these patterns that becomescompelling evidence for the existence of a metaphor.An important aspect of CMI is that it identifiesonly linguistic patterns potentially indicative of con-ceptual metaphors, not the metaphors themselves.As mentioned above, Lakoff (1993) emphasizes thatmetaphor is primarily a cognitive phenomenon, andthat metaphorical language serves as evidence forthe cognitive phenomenon.
CMI leverages computa-tional power to search through large bodies of text toidentify patterns of potential interest, then presentsthose patterns to a human user along with the po-tential metaphors they might imply to foster criticalthinking about metaphor.
To reiterate, this places thejob of finding patterns in the hands of the computer,and the job of interpreting those patterns in the handsof the human user.4 CMI with Semantic Role LabelingThe work presented in this paper attempts to en-hance CMI by using SRL to expand the types ofrelations between nouns and verbs that can be seenas instantiating a metaphor.
The prior CMI imple-mentation treats each grammatical dependency typeas a distinct relation.
For example, in the sentence,?The city contained a sacred grove for performingreligious rites,?
?rites?
is the direct object of ?per-form,?
as denoted by the dobj dependency.
How-ever, the sentence, ?The religious rites were onceagain performed openly,?
uses a passive construc-tion, meaning that ?rites?
is the passive subject, ornsubjpass, of ?perform.?
With SRL, the relationsbetween ?perform?
and ?rite?
are the same for bothsentences; specifically, Intentionally act:Act (?rite?is the intentional act being performed) and Transi-tive action:Patient (?rite?
is the recipient of a tran-sitive action).
Because the relations in FrameNetare organized into an inheritance structure, both themore general frame Transitive action and the morespecialized frame Intentionally act apply here.This section describes how SRL was incorporatedinto CMI, compares the component data derivedfrom SRL with the data derived from a typed de-pendency parse, and compares resulting identifiedmetaphors.4.1 Implementation MethodsThe CMI system used here takes the prior im-plementation (described in section 3) and replacesthe Stanford typed dependency parser (de Marn-effe et al, 2006) with the RelEx SRL system(http://opencog.org/wiki/RelEx).
RelEx performs afull syntactic parse, then applies a set of syntacticpattern rules to annotate the parse tree with role la-bels based (not exactly or completely) on FrameNet.This implementation uses a rule-based labeler be-cause CMI hinges on differences in selectional pref-erences in corpora from different domains, and sta-tistically trained classifiers are biased by the distri-butions of the corpora on which they are trained.For syntactic parsing, RelEx uses the LinkGrammar Parser (LGP) which is based on theLink Grammar model (Sleator and Temper-ley, 1993).
LGP produces output very sim-ilar to typed dependencies.
The version ofRelEx we use integrates the Another Nearly-New Information Extraction (ANNIE) system(http://gate.ac.uk/sale/tao/splitch6.html#chap:annie)to tag named entities.
Sentences aresplit using the OpenNLP sentence splitter(http://opennlp.sourceforge.net/).Because CMI?s corpora are acquired from public17Blogs Religion (Wikipedia)Docs 546 (604) 3289 (3294)Sents 5732 (6708) 128,543 (145,193)Words 148,619 3,300,455Table 1: Sizes of the target and source corpora; parenthe-ses show totals including documents without valid sen-tences and sentences with no relations.Internet sources, the text must be cleaned to makeit suitable for parsing.
Text from Wikipedia arti-cles undergoes many small filtering steps in orderto remove wiki markup, omit article sections thatdo not consist primarily of prose (e.g., ?See Also?and ?References?
), and decompose Unicode lettersand punctuation into compatibility form.
Wikipediaarticles also tend to use bulleted lists in the middleof sentences rather than comma-separated clauses.We attempt to convert those constructions back intosentences, which only sometimes results in a rea-sonable sentence.
However, it helps to ensure thatthe following sentence is properly recognized by thesentence splitter.
For blog posts, HTML tags wereremoved, which at times required multiple decodingpasses due to improperly configured blog feeds, andcharacters decomposed into compatible form.4.2 DataTable 1 shows statistics on the sizes of the sourceand target corpora.
Numbers in parentheses are to-tals, including blank documents and sentences withno valid relations.
There are some sentences forwhich RelEx does not produce any parse, e.g., longsentences that LGP deems ungrammatical.
TheStanford parser produced some result for every sen-tence, because it will produce a result tree for anykind of text, even if it does not recognize any gram-matically valid tokens.Table 2 lists the number of verb-noun relationsfor each corpus, with parentheses showing aver-age relations per word.
Since RelEx often la-bels the same verb-noun relation with multiplehierarchically-related frames (as described above),Table 2 also lists the number of unique verb-nounpairs labeled.
For the blogs corpus, the Stan-ford parser generated 111 distinct dependency types,while RelEx labeled 1446 distinct roles.
The tenStanford Blogs ReligionReln(v, n) 19,303 (2.88) 425,367 (2.93)Unique(v, n) 19,303 (2.88) 425,367 (2.93)RelEx Blogs ReligionReln(v, n) 57,639 (8.59) 1,219,345 (8.40)Unique(v, n) 20,962 (3.12) 482,997 (3.33)Table 2: Relations for the target and source corpora;parentheses show average relations per word.Stanford RelExRelation Freq Relation Freqdobj 3815 Transitive action:Patient 4268nsubj 3739 Transitive action:Agent 3597prep in 1072 Inheritance:Item 2 1489prep to 695 Categorization:Category 1489prep on 563 Attributes:Attribute 1488nsubjpass 528 Existence:Entity 1277prep for 491 Categorization:Item 1270prep with 435 Inheritance:Item 1 1269prep at 285 Attributes:Entity 1268dep 279 Purpose:Means 569Table 3: Most common dependencies and frequencies.most common of each are listed with their frequen-cies in Table 3.These data show that RelEx provides more infor-mation, both in terms of successfully parsing moresentences, and in terms of relations-per-word.
Thenext section explores the impact of these differenceson identified metaphors.4.3 ResultsThis section describes metaphors identified whenmapping from the RELIGION source corpus to thepolitical blogs target corpus.
CMI results are usu-ally culled to include only the upper one percentilein terms of confidence, but space constraints prohibita full analysis of even this upper one percentile.
In-stead, this section compares mappings with the high-est confidence score from the typed dependency dataand from the semantic role data.
RELIGION waschosen as the source domain because the highestconfidence metaphors from both typed dependenciesand semantic roles had similar target and source con-cepts, facilitating a better comparison.
This analysis18Target (label and cluster) Source (label and cluster) Confmedicine - {medicine, medical specialty}, {medicine,medication, medicament, medicinal drug}, {music,medicine}, {medicine, practice of medicine}, {learnedprofession}, {drug}, {social control}, {profession},{punishment, penalty, penalization, penalisation}, {lifescience, bioscience}sacrament - {sacrament},{baptism}, {religious ceremony,religious ritual}1.968ritual - {ceremony}, {practice,pattern}, {custom, usage, usance},{ritual, rite}, {survival}1.465Table 4: Metaphors for medicine from RELIGION using typed dependencies.is not intended to demonstrate that either techniqueis superior (for more on possible evaluation meth-ods, see Discussion section below).
Rather, it pro-vides a detailed depiction of both to ascertain poten-tial benefits and drawbacks of each.Table 4 presents the strongest two mappingsfrom RELIGION: MEDICINE IS A SACRAMENT andMEDICINE IS A RITUAL; these were the only map-pings for medicine in the upper one percentile.
Eachmapping lists both the automatically identified la-bels and the full cluster contents for source and tar-get, along with the confidence score.
The table canbe read left-to-right, e.g., ?medicine is like a sacra-ment.?
Confidence scores typically fall in the range(0, 5) with a few high-confidence mappings andmany low-confidence mappings; see (Baumer, 2009;Baumer et al, under review) for details of confi-dence score calculation.
Table 5 shows details foreach mapping, including the verb-relation pairs thatmediate the mapping, along with an example frag-ment from the target and source corpora for eachverb-relation.
These examples show why and howmedicine might be like, variously, a sacrament ora ritual; both are ?practiced,?
?administered,?
?per-formed,?
etc.
Note that the passive subject and di-rect object relations are treated as distinct, e.g., ?Eu-charist is variously administered?
involves a differ-ent grammatical relation than ?administer the sacra-ment,?
even though the word for sacrament plays asimilar semantic role in both fragments.Tables 6 and 7 show mappings resulting from se-mantic roles labeled by RelEx, with formats simi-lar to those of tables 4 and 5, except that the verb-relations in table 7 are semantic roles rather thangrammatical relations.
The mapping in table 6 wasthe strongest mapping from RELIGION and the onlymapping for medication.Table 7 shows how RelEx can treat differentgrammatical relations as the same semantic role.
Forexample, ?medicine is practiced?
and ?practice therites?
use passive subjective and direct object, re-spectively, but are both treated as the patient of atransitive action.
Such examples confirm that SRLis, at least to some extent, performing the job forwhich it was intended.However, these results also expose some prob-lems with SRL, or at least with RelEx?s implemen-tation thereof.
For example, the phrase ?dispose ofprescription drugs?
is labeled with four separate se-mantic roles, which is an instance of a single verb-noun relation being labeled with both a superordi-nate relation, Physical entity:Entity, and a subordi-nate relation, Physical entity:Constituents (the con-stituents of a physical entity are themselves an en-tity).
While various approaches might avoid multi-ple labels, e.g., using only the most general or mostspecific frame, those are beyond the scope here.5 DiscussionAs mentioned above, these results do not provideconclusive evidence that either typed dependenciesor semantic roles are more effective for identify-ing potential metaphors.
However, they do providean understanding of both techniques?
strengths andweaknesses for this purpose, and they also suggestways in which each may be more or less effective atfostering critical and creative thinking.For metaphor identification, the previous sec-tion described how typed dependency parsing treatspassive subjects and direct object as distinct rela-tions, whereas SRL will at times conflate them intoidentical patient roles.
This means that the typeddependency-based metaphors appear to be mediatedby a greater number of relations.
However, it also19Target Source Verb-Reln Target Ex Frag Source Ex Fragmedicinesacramentpractice -nsubjpass?medicine is practiced?
?rites were practiced?administer -nsubjpass?antibiotics are regularlyadministered?
?Eucharist is variously ad-ministered?administer - dobj ?administered medicines?
?administer the sacra-ment?perform - dobj ?perform defensivemedicine?
?performed the last rites?receive - dobj ?received conventionalmedicines?
?received the rites?ritualperform - dobj ?perform defensivemedicine?
?performed the last rites?practice -nsubjpass?medicine is practiced?
?ceremonies are also prac-ticed?administer - dobj ?administered medicines?
?administering the rites?administer -nsubjpass?antibiotics are regularlyadministered?
?sacrament is ordinarilyadministered?Table 5: Details of RELIGION metaphors from typed dependencies, including mediators and example phrases.Target (label and cluster) Source (label and cluster) Confmedication - {medicine, medication, medica-ment, medicinal drug}, {drug}, {agent}ceremony - {ceremony}, {sacrament}, {rite, reli-gious rite}, {religious ceremony, religious ritual}2.570Table 6: Metaphor for medication from RELIGION using semantic roles.Target Source Verb-Reln Target Ex Frag Source Ex Fragmedication ceremonypractice -Transitive action:Patient?medicine is prac-ticed?
?practice the rites?perform -Transitive action:Patient?perform defensivemedicine?
?perform most reli-gious rites?include -Transitive action:Agent?medicine including?
?liturgies included?dispose -Physical entity:Constituents?dispose of prescrip-tion drugs?
?disposed of withoutceremony?dispose -Inheritance:Instance?dispose of prescrip-tion drugs?
?disposed of withoutceremony?dispose -Inheritance:Group?dispose of prescrip-tion drugs?
?disposed of withoutceremony?dispose -Physical entity:Entity?dispose of prescrip-tion drugs?
?disposed of withoutceremony?Table 7: Details of RELIGION metaphors from semantic roles, including mediators and example phrases.20means that less data are available to the selectionpreference calculation, in that there are fewer obser-vations for each relation.
On the other hand, SRLis a much finer-grained classification than typed de-pendencies.
The implementation used here included111 grammatical relations, whereas RelEx labeled1446 distinct roles.
Thus, overall, RelEx may beproviding fewer observations for each relation, butthose relations may have more semantic import.For fostering critical thinking and creativity, akey concern is making identified metaphors read-ily comprehensible.
Ortony (Ortony, 1980) and oth-ers have suggested that selectional restriction vi-olations are an important component of metaphorcomprehension.
Therefore, tools that employ CMIoften present parallel source and target fragmentsside-by-side to make clear the selectional restric-tion violation, e.g., metaViz, a system for present-ing computationally identified metaphors in politi-cal blogs (Baumer et al, 2010).
One might assumethat typed dependencies are more readily compre-hensible, since they are expressed as relatively sim-ple grammatical relations.
However, when present-ing example fragments to users, there is no need toexplicate the nature of the relationship being demon-strated, but rather the parallel examples can simplybe placed side-by-side.
It is an empirical questionwhether users would see phrases such as ?medicineis practiced?
and ?practice the rites?
as parallel ex-amples of the same psycholinguistic relationship.Thus, the question of whether typed dependenciesor semantic roles better facilitate metaphor compre-hension may not be as important as the question ofwhether example phrases are perceived as parallel.6 Future WorkThis paper is only an initial exploration, demonstrat-ing that semantic role labeling is viable for use inCMI.
For the sake of comparison, the analysis herefocuses on examples where metaphors identified us-ing the two techniques were relatively similar.
How-ever, such similarity does not always occur.
Forexample, using MILITARY as the source domain,typed dependencies led to results such as A NOM-INEE IS A FORCE and A NOMINEE IS AN ARMY,whereas semantic roles gave mappings including ANINDIVIDUAL IS A WEAPON (here, the label ?indi-vidual?
is a superordinate category including mostlypoliticians), and THE US IS A SOLDIER.
Futurework should analyze these differences in more de-tail to provide a broad and deep comparison acrossmultiple source domains and target corpora.But how should such an analysis be conducted?That is, how does one determine which identifiedmetaphors are ?better,?
and by what standard?
Insuggesting a number of potential evaluation methodsfor CMI, Baumer et al (under review) argue that themost sensible approach is asking human subjects toassess metaphors, potentially along a variety of cri-teria.
For example: Does the metaphor make sense?Is it unexpected?
Is it confusing?
Such assess-ments could help evaluate semantic roles vs. typeddependencies in two ways.
First, does either pars-ing technique lead to metaphors that are consistentlyassessed by subjects as better?
Second, does ei-ther parsing technique lead to better alignment (i.e.,stronger correlations) between human assessmentsand CMI confidence scores?
Such subjective as-sessments could provide evidence for an argumentthat either typed dependencies or semantic roles aremore effective at identifying conceptual metaphors.7 ConclusionThis paper explores using semantic role labeling(SRL) as a technique for improving computationalmetaphor identification (CMI).
The results show thatSRL can be successfully incorporated into CMI.Furthermore, they suggest that SRL may be moreeffective at identifying relationships with semanticimport than typed dependency parsing, but that SRLmay also make distinctions that are too fine-grainedto serve as effective input for the selectional pref-erence learning involved in CMI.
The results alsodemonstrate that, even though the notion of seman-tic roles may seem more complex than typed de-pendencies from a user?s perspective, it is possi-ble to present either in a way that may be readilycomprehensible.
Thus, while more work is neces-sary to compare these two parsing techniques morefully, semantic role labeling may present an effectivemeans of improving CMI, both in terms of the tech-nical process of identifying conceptual metaphors,and in terms of the broader goal of fostering criticalthinking and creativity.21AcknowledgmentsThis material is based on work supported by theNational Science Foundation under Grant No.
IIS-0757646, by the Donald Bren School of Informa-tion and Computer Sciences, by the California Insti-tute for Telecommunications and Information Tech-nology (Calit2), and by the Undergraduate ResearchOpportunities Program (UROP) at UCI.ReferencesColin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proc 17thInt?l Conf on Comp Ling, pages 86?90, Montral, Que-bec, Canada.Eric P. S. Baumer, Jordan Sinclair, and Bill Tomlinson.2010.
?America is like Metamucil:?
Critical and cre-ative thinking about metaphor in political blogs.
InACM SIGCHI Conf, Atlanta, GA. ACM Press.Eric P. S. Baumer, David Hubin, and Bill Tomlinson.
un-der review.
Computational metaphor identification.Eric Baumer.
2009.
Computational Metaphor Identifica-tion to Foster Critical Thinking and Creativity.
Dis-sertation, University of California, Irvine, Departmentof Informatics.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In LangRes and Eval (LREC), Genoa, Italy.Dan Fass.
1991.
Met*: A method for discriminatingmetonymy and metaphor by computer.
Comp Ling,17(1):49?90.Christine Fellbaum.
1998.
WordNet: An Electronic Lex-ical Database.
MIT Press, Cambridge, MA.Katrin Fundel, Robert Kuffner, and Ralf Zimmer.
2006.RelEx-Relation extraction using dependency parsetrees.
Bioinformatics, 23(3):365?371.Matt Gedigian, John Bryant, Srini Narayanan, and Bran-imir Ciric.
2006.
Catching metaphors.
In 3rd Work-shop on Scalable Natural Language Understanding,New York City.
Assoc Comp Ling.Dedre Gentner, Brian F. Bowdle, Phillip Wolff, andC.
Boronat.
2001.
Metaphor is like analogy.
In DedreGentner, Keith J. Holyoak, and Boicho Kokinov, edi-tors, The Analogical Mind, pages 199?253.
MIT Press,Cambridge, MA.Raymond W. Gibbs.
1984.
Literal meaning and psycho-logical theory.
Cognitive Science, 8:275?304.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Comp Ling, 28(3):245?288.W.J.J.
Gordon.
1974.
Some source material in discovery-by-analogy.
Journal of Creative Behavior, 8:239?257.Richard Johansson and Pierre Nugues.
2008.Dependency-based semantic role labeling of Prop-Bank.
In Proc Conf on Empirical Meth in Nat LangProc, pages 69?78, Honolulu, HI.
Assoc Comp Ling.Adam Kilgarriff.
1996.
BNC word frequency list.http://www.kilgarriff.co.uk/bnc-readme.html.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Mtg of the Assoc for CompLing, Sapporo, Japan.Saisuresh Krishnakumaran and Xiaojin Zhu.
2007.Hunting elusive metaphors using lexical resources.In Xiaofei Lu and Anna Feldman, editors, Computa-tional Approaches to Figurative Language, Workshopat HLT/NAACL 2007, Rochester, NY.George Lakoff and Mark Johnson.
1980.
Metaphors WeLive By.
University of Chicago Press, Chicago, IL,2003 edition.George Lakoff and Mark Turner.
1989.
More Than CoolReason: A Field Guide to Poetic Metaphor.
Universityof Chicago Press, Chicago and London.George Lakoff.
1993.
The contemporary theory ofmetaphor.
In A. Ortony, editor, Metaphor and thought,2nd.
ed., pages 202?251.
Cambridge Univ Press, NewYork.James H. Martin.
1990.
A Computational Model ofMetaphor Interpretation.
Acad Press, San Diego, CA.Zachary J. Mason.
2004.
CorMet: a computational,corpus-based conventional metaphor extraction sys-tem.
Comp Ling, 30(1):23?44, March.Andrew Ortony.
1980.
Some psycholinguistic aspects ofmetaphor.
In R.P.
Honeck and H.R.
Robert, editors,Cog and Fig Lang, pages 69?83.
Erlbaum Associates,Hillsdale, NJ.Wendy Oxman-Michelli.
1991.
Critical thinking as cre-ativity.
Technical Report SO 023 597, Montclair State,Institute for Critical Thinking, Montclair, NJ.Martha Palmer, Paul Kingsbury, and Daniel Gildea.2005.
The Proposition Bank: An annotated corpus ofsemantic roles.
Comp Ling, 31(1):71?106, March.Michael J. Reddy.
1969.
A semantic approach tometaphor.
In Chicago Linguistic Society Collected Pa-pers, pages 240?251.
Chicago Univ Press, Chicago.Philip Resnik.
1993.
Selection and Information: AClass-Based Approach to Lexical Relationships.
Dis-sertation, University of Pennsylvania, Department ofComputer and Information Science.Lei Shi and Rada Mihalcea.
2004.
Open text seman-tic parsing using FrameNet and WordNet.
In Demon-stration Papers at HLT-NAACL 2004, pages 19?22,Boston.
Assoc for Computational Linguistics.Daniel Sleator and Davy Temperley.
1993.
Parsing En-glish with a Link Grammar.
In Proc Third Interna-tional Workshop on Parsing Technologies, pages 277?292.22
