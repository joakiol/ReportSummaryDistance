Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1113?1122,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsPolitical Ideology Detection Using Recursive Neural NetworksMohit Iyyer1, Peter Enns2, Jordan Boyd-Graber3,4, Philip Resnik2,41Computer Science,2Linguistics,3iSchool, and4UMIACSUniversity of Maryland{miyyer,peter,jbg}@umiacs.umd.edu, resnik@umd.eduAbstractAn individual?s words often reveal their po-litical ideology.
Existing automated tech-niques to identify ideology from text focuson bags of words or wordlists, ignoring syn-tax.
Taking inspiration from recent work insentiment analysis that successfully modelsthe compositional aspect of language, weapply a recursive neural network (RNN)framework to the task of identifying the po-litical position evinced by a sentence.
Toshow the importance of modeling subsen-tential elements, we crowdsource politicalannotations at a phrase and sentence level.Our model outperforms existing models onour newly annotated dataset and an existingdataset.1 IntroductionMany of the issues discussed by politicians andthe media are so nuanced that even word choiceentails choosing an ideological position.
For ex-ample, what liberals call the ?estate tax?
conser-vatives call the ?death tax?
; there are no ideolog-ically neutral alternatives (Lakoff, 2002).
Whileobjectivity remains an important principle of jour-nalistic professionalism, scholars and watchdoggroups claim that the media are biased (Grosecloseand Milyo, 2005; Gentzkow and Shapiro, 2010;Niven, 2003), backing up their assertions by pub-lishing examples of obviously biased articles ontheir websites.
Whether or not it reflects an under-lying lack of objectivity, quantitative changes in thepopular framing of an issue over time?favoringone ideologically-based position over another?canhave a substantial effect on the evolution of policy(Dardis et al, 2008).Manually identifying ideological bias in polit-ical text, especially in the age of big data, is animpractical and expensive process.
Moreover, biasTheydubbed itthedeath tax?
?and created abig lie aboutits adverse effectson smallbusinessesFigure 1: An example of compositionality in ideo-logical bias detection (red?
conservative, blue?liberal, gray?
neutral) in which modifier phrasesand punctuation cause polarity switches at higherlevels of the parse tree.may be localized to a small portion of a document,undetectable by coarse-grained methods.
In this pa-per, we examine the problem of detecting ideologi-cal bias on the sentence level.
We say a sentencecontains ideological bias if its author?s politicalposition (here liberal or conservative, in the senseof U.S. politics) is evident from the text.Ideological bias is difficult to detect, even forhumans?the task relies not only on politicalknowledge but also on the annotator?s ability topick up on subtle elements of language use.
Forexample, the sentence in Figure 1 includes phrasestypically associated with conservatives, such as?small businesses?
and ?death tax?.
When we takemore of the structure into account, however, wefind that scare quotes and a negative propositionalattitude (a lie about X) yield an evident liberal bias.Existing approaches toward bias detection havenot gone far beyond ?bag of words?
classifiers, thusignoring richer linguistic context of this kind andoften operating at the level of whole documents.In contrast, recent work in sentiment analysis hasused deep learning to discover compositional ef-fects (Socher et al, 2011b; Socher et al, 2013b).Building from those insights, we introduce a re-cursive neural network (RNN) to detect ideologicalbias on the sentence level.
This model requires1113wb= changewa= climatewd= so-calledpc= climate changepe= so-called climate changexd= xc=xe=xa= xb=WLWRWRWLFigure 2: An example RNN for the phrase ?so-called climate change?.
Two d-dimensional wordvectors (here, d = 6) are composed to generate aphrase vector of the same dimensionality, whichcan then be recursively used to generate vectors athigher-level nodes.richer data than currently available, so we developa new political ideology dataset annotated at thephrase level.
With this new dataset we show thatRNNs not only label sentences well but also im-prove further when given additional phrase-levelannotations.
RNNs are quantitatively more effec-tive than existing methods that use syntactic andsemantic features separately, and we also illustratehow our model correctly identifies ideological biasin complex syntactic constructions.2 Recursive Neural NetworksRecursive neural networks (RNNs) are machinelearning models that capture syntactic and semanticcomposition.
They have achieved state-of-the-artperformance on a variety of sentence-level NLPtasks, including sentiment analysis, paraphrase de-tection, and parsing (Socher et al, 2011a; Hermannand Blunsom, 2013).
RNN models represent a shiftfrom previous research on ideological bias detec-tion in that they do not rely on hand-made lexicons,dictionaries, or rule sets.
In this section, we de-scribe a supervised RNN model for bias detectionand highlight differences from previous work intraining procedure and initialization.2.1 Model DescriptionBy taking into account the hierarchical nature oflanguage, RNNs can model semantic composition,which is the principle that a phrase?s meaning is acombination of the meaning of the words withinthat phrase and the syntax that combines thosewords.
While semantic composition does not ap-ply universally (e.g., sarcasm and idioms), mostlanguage follows this principle.
Since most ide-ological bias becomes identifiable only at higherlevels of sentence trees (as verified by our annota-tion, Figure 4), models relying primarily on word-level distributional statistics are not desirable forour problem.The basic idea behind the standard RNN modelis that each word w in a sentence is associatedwith a vector representation xw?
Rd.
Based on aparse tree, these words form phrases p (Figure 2).Each of these phrases also has an associated vectorxp?
Rdof the same dimension as the word vectors.These phrase vectors should represent the meaningof the phrases composed of individual words.
Asphrases themselves merge into complete sentences,the underlying vector representation is trained toretain the sentence?s whole meaning.The challenge is to describe how vectors com-bine to form complete representations.
If twowords waand wbmerge to form phrase p, we positthat the phrase-level vector isxp= f(WL?
xa+WR?
xb+ b1), (1)where WLand WRare d ?
d left and right com-position matrices shared across all nodes in thetree, b1is a bias term, and f is a nonlinear activa-tion function such as tanh.
The word-level vectorsxaand xbcome from a d ?
V dimensional wordembedding matrix We, where V is the size of thevocabulary.We are interested in learning representations thatcan distinguish political polarities given labeleddata.
If an element of this vector space, xd, repre-sents a sentence with liberal bias, its vector shouldbe distinct from the vector xrof a conservative-leaning sentence.Supervised RNNs achieve this distinction by ap-plying a regression that takes the node?s vector xpas input and produces a prediction y?p.
This is asoftmax layery?d= softmax(Wcat?
xp+ b2), (2)where the softmax function issoftmax(q) =exp q?kj=1exp qj(3)and Wcatis a k ?
d matrix for a dataset with k-dimensional labels.We want the predictions of the softmax layer tomatch our annotated data; the discrepancy betweencategorical predictions and annotations is measured1114through the cross-entropy loss.
We optimize themodel parameters to minimize the cross-entropyloss over all sentences in the corpus.
The cross-entropy loss of a single sentence is the sum overthe true labels yiin the sentence,`(y?s) =k?p=1yp?
log(y?p).
(4)This induces a supervised objective functionover all sentences: a regularized sum over all nodelosses normalized by the number of nodes N in thetraining set,C =1NN?i`(predi) +?2???2.
(5)We use L-BFGS with parameter averag-ing (Hashimoto et al, 2013) to optimize the modelparameters ?
= (WL,WR,Wcat,We, b1, b2).
Thegradient of the objective, shown in Eq.
(6), iscomputed using backpropagation through struc-ture (Goller and Kuchler, 1996),?C??=1NN?i?`(y?i)?
?+ ??.
(6)2.2 InitializationWhen initializing our model, we have two choices:we can initialize all of our parameters randomly orprovide the model some prior knowledge.
As wesee in Section 4, these choices have a significanteffect on final performance.Random The most straightforward choice is toinitialize the word embedding matrix Weand com-position matrices WLand WRrandomly such thatwithout any training, representations for words andphrases are arbitrarily projected into the vectorspace.word2vec The other alternative is to initialize theword embedding matrix Wewith values that reflectthe meanings of the associated word types.
Thisimproves the performance of RNN models over ran-dom initializations (Collobert and Weston, 2008;Socher et al, 2011a).
We initialize our model with300-dimensional word2vec toolkit vectors gener-ated by a continuous skip-gram model trained onaround 100 billion words from the Google Newscorpus (Mikolov et al, 2013).The word2vec embeddings have linear relation-ships (e.g., the closest vectors to the average of?green?
and ?energy?
include phrases such as ?re-newable energy?, ?eco-friendly?, and ?efficientlightbulbs?).
To preserve these relationships asphrases are formed in our sentences, we initializeour left and right composition matrices such thatparent vector p is computed by taking the averageof children a and b (WL= WR= 0.5Id?d).
Thisinitialization of the composition matrices has pre-viously been effective for parsing (Socher et al,2013a).3 DatasetsWe performed initial experiments on a dataset ofCongressional debates that has annotations on theauthor level for partisanship, not ideology.
Whilethe two terms are highly correlated (e.g., a memberof the Republican party likely agrees with conserva-tive stances on most issues), they are not identical.For example, a moderate Republican might agreewith the liberal position on increased gun controlbut take conservative positions on other issues.
Toavoid conflating partisanship and ideology we cre-ate a new dataset annotated for ideological bias onthe sentence and phrase level.
In this section wedescribe our initial dataset (Convote) and explainthe procedure we followed for creating our newdataset (IBC).13.1 ConvoteThe Convote dataset (Thomas et al, 2006) con-sists of US Congressional floor debate transcriptsfrom 2005 in which all speakers have been labeledwith their political party (Democrat, Republican,or independent).
We propagate party labels downfrom the speaker to all of their individual sentencesand map from party label to ideology label (Demo-crat?
liberal, Republican?
conservative).
Thisis an expedient choice; in future work we plan tomake use of work in political science characteriz-ing candidates?
ideological positions empiricallybased on their behavior (Carroll et al, 2009).While the Convote dataset has seen widespreaduse for document-level political classification, weare unaware of similar efforts at the sentence level.3.1.1 Biased Sentence SelectionThe strong correlation between US political partiesand political ideologies (Democrats with liberal,Republicans with conservative) lends confidencethat this dataset contains a rich mix of ideological1Available at http://cs.umd.edu/?miyyer/ibc1115statements.
However, the raw Convote dataset con-tains a low percentage of sentences with explicitideological bias.2We therefore use the featuresin Yano et al (2010), which correlate with politi-cal bias, to select sentences to annotate that havea higher likelihood of containing bias.
Their fea-tures come from the Linguistic Inquiry and WordCount lexicon (LIWC) (Pennebaker et al, 2001),as well as from lists of ?sticky bigrams?
(Brownet al, 1992) strongly associated with one party oranother (e.g., ?illegal aliens?
implies conservative,?universal healthcare?
implies liberal).We first extract the subset of sentences that con-tains any words in the LIWC categories of NegativeEmotion, Positive Emotion, Causation, Anger, andKill verbs.3After computing a list of the top 100sticky bigrams for each category, ranked by log-likelihood ratio, and selecting another subset fromthe original data that included only sentences con-taining at least one sticky bigram, we take the unionof the two subsets.
Finally, we balance the resultingdataset so that it contains an equal number of sen-tences from Democrats and Republicans, leavingus with a total of 7,816 sentences.3.2 Ideological BooksIn addition to Convote, we use the Ideologi-cal Books Corpus (IBC) developed by Gross etal.
(2013).
This is a collection of books and maga-zine articles written between 2008 and 2012 by au-thors with well-known political leanings.
Each doc-ument in the IBC has been manually labeled withcoarse-grained ideologies (right, left, and center) aswell as fine-grained ideologies (e.g., religious-right,libertarian-right) by political science experts.There are over a million sentences in the IBC,most of which have no noticeable political bias.Therefore we use the filtering procedure outlinedin Section 3.1.1 to obtain a subset of 55,932 sen-tences.
Compared to our final Convote dataset, aneven larger percentage of the IBC sentences exhibitno noticeable political bias.4Because our goalis to distinguish between liberal and conservative2Many sentences in Convote are variations on ?I think thisis a good/bad bill?, and there is also substantial parliamentaryboilerplate language.3While Kill verbs are not a category in LIWC, Yano etal.
(2010) adopted it from Greene and Resnik (2009) andshowed it to be a useful predictor of political bias.
It includeswords such as ?slaughter?
and ?starve?.4This difference can be mainly attributed to a historicaltopics in the IBC (e.g., the Crusades, American Civil War).In Convote, every sentence is part of a debate about 2005political policy.bias, instead of the more general task of classify-ing sentences as ?neutral?
or ?biased?, we filterthe dataset further using DUALIST (Settles, 2011),an active learning tool, to reduce the proportionof neutral sentences in our dataset.
To train theDUALIST classifier, we manually assigned class la-bels of ?neutral?
or ?biased?
to 200 sentences, andselected typical partisan unigrams to represent the?biased?
class.
DUALIST labels 11,555 sentences aspolitically biased, 5,434 of which come from con-servative authors and 6,121 of which come fromliberal authors.3.2.1 Annotating the IBCFor purposes of annotation, we define the task ofpolitical ideology detection as identifying, if pos-sible, the political position of a given sentence?sauthor, where position is either liberal or conser-vative.5We used the Crowdflower crowdsourcingplatform (crowdflower.com), which has previouslybeen used for subsentential sentiment annotation(Sayeed et al, 2012), to obtain human annotationsof the filtered IBC dataset for political bias on boththe sentence and phrase level.
While members ofthe Crowdflower workforce are certainly not ex-perts in political science, our simple task and theubiquity of political bias allows us to acquire usefulannotations.Crowdflower Task First, we parse the filteredIBC sentences using the Stanford constituencyparser (Socher et al, 2013a).
Because of the ex-pense of labeling every node in a sentence, we onlylabel one path in each sentence.
The process forselecting paths is as follows: first, if any pathscontain one of the top-ten partisan unigrams,6weselect the longest such path; otherwise, we selectthe path with the most open class constituencies(NP, VP, ADJP).
The root node of a sentence isalways included in a path.Our task is shown in Figure 3.
Open class con-stituencies are revealed to the worker incrementally,starting with the NP, VP, or ADJP furthest fromthe root and progressing up the tree.
We choosethis design to prevent workers from changing theirlower-level phrase annotations after reading the fullsentence.5This is a simplification, as the ideological hierarchy inIBC makes clear.6The words that the multinomial na?
?ve Bayes classifierin DUALIST marked as highest probability given a polarity:market, abortion, economy, rich, liberal, tea, economic, taxes,gun, abortion1116Filtering the Workforce To ensure our anno-tators have a basic understanding of US politics,we restrict workers to US IP addresses and requireworkers manually annotate one node from 60 dif-ferent ?gold ?
paths annotated by the authors.
Weselect these nodes such that the associated phrase iseither obviously biased or obviously neutral.
Work-ers must correctly annotate at least six of eightgold paths before they are granted access to the fulltask.
In addition, workers must maintain 75% accu-racy on gold paths that randomly appear alongsidenormal paths.
Gold paths dramatically improvethe quality of our workforce: 60% of contributorspassed the initial quiz (the 40% that failed werebarred from working on the task), while only 10%of workers who passed the quiz were kicked outfor mislabeling subsequent gold paths.Annotation Results Workers receive thefollowing instructions:Each task on this page contains a set ofphrases from a single sentence.
For eachphrase, decide whether or not the author fa-vors a political position to the left (Liberal) orright (Conservative) of center.?
If the phrase is indicative of a position tothe left of center, please choose Liberal.?
If the phrase is indicative of a position tothe right of center, please choose Conser-vative.?
If you feel like the phrase indicates someposition to the left or right of the politicalcenter, but you?re not sure which direc-tion, please mark Not neutral, but I?munsure of which direction.?
If the phrase is not indicative of a posi-tion to the left or right of center, pleasemark Neutral.We had workers annotate 7,000 randomly se-lected paths from the filtered IBC dataset, with halfof the paths coming from conservative authors andthe other half from liberal authors, as annotatedby Gross et al (2013).
Three workers annotatedeach path in the dataset, and we paid $0.03 persentence.
Since identifying political bias is a rela-tively difficult and subjective task, we include allsentences where at least two workers agree on alabel for the root node in our final dataset, exceptwhen that label is ?Not neutral, but I?m unsure ofFigure 3: Example political ideology annotationtask showing incremental reveal of progressivelylonger phrases.which direction?.
We only keep phrase-level an-notations where at least two workers agree on thelabel: 70.4% of all annotated nodes fit this defini-tion of agreement.
All unannotated nodes receivethe label of their closest annotated ancestor.
Sincethe root of each sentence is always annotated, thisstrategy ensures that every node in the tree has alabel.
Our final balanced IBC dataset consists of3,412 sentences (4,062 before balancing and re-moving neutral sentences) with a total of 13,640annotated nodes.
Of these sentences, 543 switchpolarity (liberal?
conservative or vice versa) onan annotated path.While we initially wanted to incorporate neutrallabels into our model, we observed that lower-levelphrases are almost always neutral while full sen-tences are much more likely to be biased (Figure 4).Due to this discrepancy, the objective function inEq.
(5) was minimized by making neutral predic-tions for almost every node in the dataset.4 ExperimentsIn this section we describe our experimental frame-work.
We discuss strong baselines that use lexi-cal and syntactic information (including framing-specific features from previous work) as well asmultiple RNN configurations.
Each of these mod-els have the same task: to predict sentence-levelideology labels for sentences in a test set.
To ac-count for label imbalance, we subsample the dataso that there are an equal number of labels andreport accuracy over this balanced dataset.11170 1 2 3 4 5 6 7 8 9 10Node Depth0.00.10.20.30.40.50.60.70.80.91.0Label ProbabilityLabel Probability vs. Node DepthConservativeLiberalNeutral / No AgreementFigure 4: Proportion of liberal, conservative, andneutral annotations with respect to node depth (dis-tance from root).
As we get farther from the rootof the tree, nodes are more likely to be neutral.4.1 Baselines?
The RANDOM baseline chooses a label at ran-dom from {liberal, conservative}.?
LR1, our most basic logistic regression base-line, uses only bag of words (BoW) features.?
LR2 uses only BoW features.
However, LR2also includes phrase-level annotations as sep-arate training instances.7?
LR3 uses BoW features as well as syntac-tic pseudo-word features from Greene &Resnik (2009).
These features from depen-dency relations specify properties of verbs(e.g., transitivity or nominalization).8?
LR-(W2V) is a logistic regression modeltrained on the average of the pretrained wordembeddings for each sentence (Section 2.2).The LR-(W2V) baseline allows us to compareagainst a strong lexical representation that encodessyntactic and semantic information without theRNN tree structure.
(LR1, LR2) offer a compari-son to simple bag of words models, while the LR3baseline contrasts traditional syntactic features withthose learned by RNN models.4.2 RNN ModelsFor RNN models, we generate a feature vector forevery node in the tree.
Equation 1 allows us to7The Convote dataset was not annotated on the phraselevel, so we only provide a result for the IBC dataset.8We do not include phrase-level annotations in the LR3feature set because the pseudo-word features can only becomputed from full sentence parses.Model Convote IBCRANDOM 50% 50%LR1 64.7% 62.1%LR2 ?
61.9%LR3 66.9% 62.6%LR-(W2V) 66.6% 63.7%RNN1 69.4% 66.2%RNN1-(W2V) 70.2% 67.1%RNN2-(W2V) ?
69.3%Table 1: Sentence-level bias detection accuracy.The RNN framework, adding phrase-level data, andinitializing with word2vec all improve performanceover logistic regression baselines.
The LR2 andRNN2-(W2V) models were not trained on Convotesince it lacks phrase annotations.percolate the representations to the root of the tree.We generate the final instance representation byconcatenating the root vector and the average ofall other vectors (Socher et al, 2011b).
We trainan L2-regularized logistic regression model overthese concatenated vectors to obtain final accuracynumbers on the sentence level.To analyze the effects of initialization andphrase-level annotations, we report results for threedifferent RNN settings.
All three models were im-plemented as described in Section 2 with the non-linearity f set to the normalized tanh function,f(v) =tanh(v)?tanh(v)?.
(7)We perform 10-fold cross-validation on the trainingdata to find the best RNN hyperparameters.9We report results for RNN models with the fol-lowing configurations:?
RNN1 initializes all parameters randomly anduses only sentence-level labels for training.?
RNN1-(W2V) uses the word2vec initializationdescribed in Section 2.2 but is also trained ononly sentence-level labels.?
RNN2-(W2V) is initialized using word2vecembeddings and also includes annotatedphrase labels in its training.
For this model,we also introduce a hyperparameter ?
thatweights the error at annotated nodes (1?
?
)higher than the error at unannotated nodes (?
);since we have more confidence in the anno-tated labels, we want them to contribute moretowards the objective function.9[?We=1e-6, ?W=1e-4, ?Wcat=1e-3, ?
= 0.3]1118For all RNN models, we set the word vectordimension d to 300 to facilitate direct comparisonagainst the LR-(W2V) baseline.105 Where Compositionality Helps DetectIdeological BiasIn this section, we examine the RNN models to seewhy they improve over our baselines.
We also giveexamples of sentences that are correctly classifiedby our best RNN model but incorrectly classified byall of the baselines.
Finally, we investigate sentenceconstructions that our model cannot handle andoffer possible explanations for these errors.Experimental Results Table 1 shows the RNNmodels outperforming the bag-of-words base-lines as well as the word2vec baseline on bothdatasets.
The increased accuracy suggests that thetrained RNNs are capable of detecting bias polar-ity switches at higher levels in parse trees.
Whilephrase-level annotations do not improve baselineperformance, the RNN model significantly bene-fits from these annotations because the phrases arethemselves derived from nodes in the network struc-ture.
In particular, the phrase annotations allow ourbest model to detect bias accurately in complexsentences that the baseline models cannot handle.Initializing the RNN Wematrix with word2vecembeddings improves accuracy over randomly ini-tialization by 1%.
This is similar to improvementsfrom pretrained vectors from neural language mod-els (Socher et al, 2011b).We obtain better results on Convote than on IBCwith both bag-of-words and RNN models.
Thisresult was unexpected since the Convote labelsare noisier than the annotated IBC labels; however,there are three possible explanations for the discrep-ancy.
First, Convote has twice as many sentencesas IBC, and the extra training data might help themodel more than IBC?s better-quality labels.
Sec-ond, since the sentences in Convote were originallyspoken, they are almost half as short (21.3 wordsper sentence) as those in the IBC (42.2 words persentence).
Finally, some information is lost at ev-ery propagation step, so RNNs are able to modelthe shorter sentences in Convote more effectivelythan the longer IBC sentences.Qualitative Analysis As in previous work(Socher et al, 2011b), we visualize the learned10Using smaller vector sizes (d ?
{50, 100}, as in previouswork) does not significantly change accuracy.vector space by listing the most probable n-gramsfor each political affiliation in Table 2.
As expected,conservatives emphasize values such as freedomand religion while disparaging excess governmentspending and their liberal opposition.
Meanwhile,liberals inveigh against the gap between the richand the poor while expressing concern for minoritygroups and the working class.Our best model is able to accurately model thecompositional effects of bias in sentences with com-plex syntactic structures.
The first three sentencesin Figure 5 were correctly classified by our bestmodel (RNN2-(W2V)) and incorrectly classified byall of the baselines.
Figures 5A and C show tradi-tional conservative phrases, ?free market ideology?and ?huge amounts of taxpayer money?, that switchpolarities higher up in the tree when combined withphrases such as ?made worse by?
and ?saved by?.Figure 5B shows an example of a bias polarityswitch in the opposite direction: the sentence neg-atively portrays supporters of nationalized healthcare, which our model picks up on.Our model often makes errors when polarityswitches occur at nodes that are high up in thetree.
In Figure 5D, ?be used as an instrument toachieve charitable or social ends?
reflects a lib-eral ideology, which the model predicts correctly.However, our model is unable to detect the polarityswitch when this phrase is negated with ?shouldnot?.
Since many different issues are discussedin the IBC, it is likely that our dataset has too fewexamples of some of these issues for the model toadequately learn the appropriate ideological posi-tions, and more training data would resolve manyof these errors.6 Related WorkA growing NLP subfield detects private states suchas opinions, sentiment, and beliefs (Wilson et al,2005; Pang and Lee, 2008) from text.
In general,work in this category tends to combine traditionalsurface lexical modeling (e.g., bag-of-words) withhand-designed syntactic features or lexicons.
Herewe review the most salient literature related to thepresent paper.6.1 Automatic Ideology DetectionMost previous work on ideology detection ignoresthe syntactic structure of the language in use infavor of familiar bag-of-words representations for1119be used as an instrument toachieve charitable or social endsshould notthe lawXXXnationalized health careAn entertainer oncesaid a sucker is bornevery minute , andsurely this is thecase withthose whosupportmade worse bythe implementingThus , the harshconditions forfarmers causedby a number offactors ,, have created acontinuing stream ofpeople leaving thecountryside and goingto live in cities that donot have jobs for them .of free-marketideologyhugeamounts oftaxpayermoneysaved byBut taxpayersdo knowalready thatTARP wasdesigned in away thatallowedto continue toshow the samearrogant traitsthat should havedestroyed theircompanies .the samecorporationswho wereA BC DFigure 5: Predictions by RNN2-(W2V) on four sentences from the IBC.
Node color is the true label (redfor conservative, blue for liberal), and an ?X?
next to a node means the model?s prediction was wrong.
InA and C, the model accurately detects conservative-to-liberal polarity switches, while in B it correctlypredicts the liberal-to-conservative switch.
In D, negation confuses our model.the sake of simplicity.
For example, Gentzkowand Shapiro (2010) derive a ?slant index?
to ratethe ideological leaning of newspapers.
A newspa-per?s slant index is governed by the frequency ofuse of partisan collocations of 2-3 tokens.
Simi-larly, authors have relied on simple models of lan-guage when leveraging inferred ideological posi-tions.
E.g., Gerrish and Blei (2011) predict thevoting patterns of Congress members based on bag-of-words representations of bills and inferred polit-ical leanings of those members.Recently, Sim et al (2013) have proposed amodel to infer mixtures of ideological positionsin documents, applied to understanding the evolu-tion of ideological rhetoric used by political can-didates during the campaign cycle.
They use anHMM-based model, defining the states as a setof fine-grained political ideologies, and rely ona closed set of lexical bigram features associatedwith each ideology, inferred from a manually la-beled ideological books corpus.
Although it takeselements of discourse structure into account (cap-turing the?burstiness?
of ideological terminologyusage), their model explicitly ignores intrasenten-tial contextual influences of the kind seen in Fig-ure 1.
Other approaches on the document level usetopic models to analyze bias in news articles, blogs,and political speeches (Ahmed and Xing, 2010; Linet al, 2008; Nguyen et al, 2013).6.2 Subjectivity DetectionDetecting subjective language, which conveys opin-ion or speculation, is a related NLP problem.
Whilesentences lacking subjective language may con-tain ideological bias (e.g., the topic of the sen-tence), highly-opinionated sentences likely haveobvious ideological leanings.
In addition, senti-ment and subjectivity analysis offers methodolog-ical approaches that can be applied to automaticbias detection.Wiebe et al (2004) show that low-frequencywords and some collocations are a good indica-tors of subjectivity.
More recently, Recasens et al(2013) detect biased words in sentences using indi-cator features for bias cues such as hedges and fac-tive verbs in addition to standard bag-of-words andpart-of-speech features.
They show that this type oflinguistic information dramatically improves per-formance over several standard baselines.Greene and Resnik (2009) also emphasize theconnection between syntactic and semantic rela-tionships in their work on ?implicit sentiment?,1120n Most conservative n-grams Most liberal n-grams1 Salt, Mexico, housework, speculated, consensus, lawyer,pharmaceuticals, ruthless, deadly, Clinton, redistributionrich, antipsychotic, malaria, biodiversity, richest, gene,pesticides, desertification, Net, wealthiest, labor, fertil-izer, nuclear, HIV3 prize individual liberty, original liberal idiots, stock mar-ket crash, God gives freedom, federal government inter-ference, federal oppression nullification, respect individ-ual liberty, Tea Party patriots, radical Sunni Islamists,Obama stimulus programsrich and poor,?corporate greed?, super rich pay, carryingthe rich, corporate interest groups, young women work-ers, the very rich, for the rich, by the rich, soaking therich, getting rich often, great and rich, the working poor,corporate income tax, the poor migrants5 spending on popular government programs, bailouts andunfunded government promises, North America fromexternal threats, government regulations place on busi-nesses, strong Church of Christ convictions, radical Is-lamism and other threatsthe rich are really rich, effective forms of worker partic-ipation, the pensions of the poor, tax cuts for the rich,the ecological services of biodiversity, poor children andpregnant women, vacation time for overtime pay7 government intervention helped make the DepressionGreat, by God in His image and likeness, producingwealth instead of stunting capital creation, the tradi-tional American values of limited government, trillionsof dollars to overseas oil producers, its troubled assets tofederal sugar daddies, Obama and his party as racialistfanaticsAfrican Americans and other disproportionately poorgroups; the growing gap between rich and poor; theBush tax cuts for the rich; public outrage at corporateand societal greed; sexually transmitted diseases , mostnotably AIDS; organize unions or fight for better condi-tions, the biggest hope for health care reformTable 2: Highest probability n-grams for conservative and liberal ideologies, as predicted by the RNN2-(W2V) model.which refers to sentiment carried by sentence struc-ture and not word choice.
They use syntactic depen-dency relation features combined with lexical infor-mation to achieve then state-of-the-art performanceon standard sentiment analysis datasets.
However,these syntactic features are only computed for athresholded list of domain-specific verbs.
Thiswork extends their insight of modeling sentimentas an interaction between syntax and semantics toideological bias.Future Work There are a few obvious directionsin which this work can be expanded.
First, we canconsider more nuanced political ideologies beyondliberal and conservative.
We show that it is pos-sible to detect ideological bias given this binaryproblem; however, a finer-grained study that alsoincludes neutral annotations may reveal more sub-tle distinctions between ideologies.
While acquir-ing data with obscure political biases from the IBCor Convote is unfeasible, we can apply a similaranalysis to social media (e.g., Twitter or Facebookupdates) to discover how many different ideologiespropagate in these networks.Another direction is to implement more sophis-ticated RNN models (along with more trainingdata) for bias detection.
We attempted to applysyntactically-untied RNNs (Socher et al, 2013a)to our data with the idea that associating separatematrices for phrasal categories would improve rep-resentations at high-level nodes.
While there weretoo many parameters for this model to work wellhere, other variations might prove successful, espe-cially with more data.
Finally, combining sentence-level and document-level models might improvebias detection at both levels.7 ConclusionIn this paper we apply recursive neural networksto political ideology detection, a problem whereprevious work relies heavily on bag-of-words mod-els and hand-designed lexica.
We show that ourapproach detects bias more accurately than existingmethods on two different datasets.
In addition, wedescribe an approach to crowdsourcing ideologicalbias annotations.
We use this approach to create anew dataset from the IBC, which is labeled at boththe sentence and phrase level.AcknowledgmentsWe thank the anonymous reviewers, Hal Daum?e,Yuening Hu, Yasuhiro Takayama, and Jyothi Vinju-mur for their insightful comments.
We also want tothank Justin Gross for providing the IBC and AsadSayeed for help with the Crowdflower task design,as well as Richard Socher and Karl Moritz Her-mann for assisting us with our model implemen-tations.
This work was supported by NSF GrantCCF-1018625.
Boyd-Graber is also supported byNSF Grant IIS-1320538.
Any opinions, findings,conclusions, or recommendations expressed hereare those of the authors and do not necessarily re-flect the view of the sponsor.1121ReferencesAmr Ahmed and Eric P Xing.
2010.
Staying informed: super-vised and semi-supervised multi-view topical analysis ofideological perspective.
In EMNLP.Peter F Brown, Peter V Desouza, Robert L Mercer, VincentJ Della Pietra, and Jenifer C Lai.
1992.
Class-based n-grammodels of natural language.
Comp.
Ling., 18(4):467?479.Royce Carroll, Jeffrey B Lewis, James Lo, Keith T Poole, andHoward Rosenthal.
2009.
Measuring bias and uncertaintyin dw-nominate ideal point estimates via the parametricbootstrap.
Political Analysis, 17(3):261?275.Ronan Collobert and Jason Weston.
2008.
A unified architec-ture for natural language processing: Deep neural networkswith multitask learning.
In ICML.Frank E Dardis, Frank R Baumgartner, Amber E Boydstun,Suzanna De Boef, and Fuyuan Shen.
2008.
Media framingof capital punishment and its impact on individuals?
cogni-tive responses.
Mass Communication & Society, 11(2):115?140.Matthew Gentzkow and Jesse M Shapiro.
2010.
What drivesmedia slant?
evidence from us daily newspapers.
Econo-metrica, 78(1):35?71.Sean Gerrish and David M Blei.
2011.
Predicting legislativeroll calls from text.
In ICML.Christoph Goller and Andreas Kuchler.
1996.
Learning task-dependent distributed representations by backpropagationthrough structure.
In Neural Networks, 1996., IEEE Inter-national Conference on, volume 1.Stephan Greene and Philip Resnik.
2009.
More than words:Syntactic packaging and implicit sentiment.
In NAACL.Tim Groseclose and Jeffrey Milyo.
2005.
A measure of mediabias.
The Quarterly Journal of Economics, 120(4):1191?1237.Justin Gross, Brice Acree, Yanchuan Sim, and Noah A Smith.2013.
Testing the etch-a-sketch hypothesis: A compu-tational analysis of mitt romney?s ideological makeoverduring the 2012 primary vs. general elections.
In APSA2013 Annual Meeting Paper.Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, andTakashi Chikayama.
2013.
Simple customization of recur-sive neural networks for semantic relation classification.
InEMNLP.Karl Moritz Hermann and Phil Blunsom.
2013.
The Role ofSyntax in Vector Space Models of Compositional Seman-tics.
In ACL.George Lakoff.
2002.
Moral Politics: How Liberals and Con-servatives Think, Second Edition.
University of ChicagoPress.Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008.A joint topic and perspective model for ideological dis-course.
In Machine Learning and Knowledge Discovery inDatabases, pages 17?32.
Springer.Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.2013.
Efficient estimation of word representations in vectorspace.
arXiv preprint arXiv:1301.3781.Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.2013.
Lexical and hierarchical topic regression.
In NIPS,pages 1106?1114.David Niven.
2003.
Objective evidence on media bias: News-paper coverage of congressional party switchers.
Journal-ism & Mass Communication Quarterly, 80(2):311?326.Bo Pang and Lillian Lee.
2008.
Opinion mining and sentimentanalysis.
Foundations and trends in information retrieval,2(1-2).James W. Pennebaker, Martha E. Francis, and Roger J. Booth.2001.
Linguistic inquiry and word count [computer soft-ware].
Mahwah, NJ: Erlbaum Publishers.Marta Recasens, Cristian Danescu-Niculescu-Mizil, and DanJurafsky.
2013.
Linguistic models for analyzing and de-tecting biased language.Asad B Sayeed, Jordan Boyd-Graber, Bryan Rusk, and AmyWeinberg.
2012.
Grammatical structures for word-levelsentiment detection.
In NAACL.Burr Settles.
2011.
Closing the loop: Fast, interactivesemi-supervised annotation with queries on features andinstances.
In EMNLP.Yanchuan Sim, Brice Acree, Justin H Gross, and Noah ASmith.
2013.
Measuring ideological proportions in politi-cal speeches.
In EMNLP.Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y.Ng, and Christopher D. Manning.
2011a.
Dynamic Pool-ing and Unfolding Recursive Autoencoders for ParaphraseDetection.
In NIPS.Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y.Ng, and Christopher D. Manning.
2011b.
Semi-SupervisedRecursive Autoencoders for Predicting Sentiment Distribu-tions.
In EMNLP.Richard Socher, John Bauer, Christopher D. Manning, andAndrew Y. Ng.
2013a.
Parsing With Compositional VectorGrammars.
In ACL.Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang,Christopher D Manning, Andrew Y Ng, and ChristopherPotts.
2013b.
Recursive deep models for semantic compo-sitionality over a sentiment treebank.
In EMNLP.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get out thevote: Determining support or opposition from Congres-sional floor-debate transcripts.
In EMNLP.Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell,and Melanie Martin.
2004.
Learning subjective language.Comp.
Ling., 30(3):277?308.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005.Recognizing contextual polarity in phrase-level sentimentanalysis.
In EMNLP.Tae Yano, Philip Resnik, and Noah A Smith.
2010.
Shedding(a thousand points of) light on biased language.
In Pro-ceedings of the NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s MechanicalTurk, pages 152?158.1122
