Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 169?174,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsWord Alignment without NULL WordsPhilip SchulzP.Schulz@uva.nlWilker AzizW.Aziz@uva.nlILLCUniversity of AmsterdamKhalil Sima?anK.Simaan@uva.nlAbstractIn word alignment certain source wordsare only needed for fluency reasons anddo not have a translation on the targetside.
Most word alignment models as-sume a target NULL word from whichthey generate these untranslatable sourcewords.
Hypothesising a target NULLword is not without problems, however.For example, because this NULL word hasa position, it interferes with the distribu-tion over alignment jumps.
We present aword alignment model that accounts foruntranslatable source words by generat-ing them from preceding source words.It thereby removes the need for a tar-get NULL word and only models align-ments between word pairs that are actu-ally observed in the data.
Translation ex-periments on English paired with Czech,German, French and Japanese show thatthe model outperforms its traditional IBMcounterparts in terms of BLEU score.1 IntroductionWhen the IBM models (Brown et al, 1993) weredesigned, some way of accounting for words thatlikely have no translation was needed.
The mod-ellers back then decided to introduce a NULLword on the target (generating) side1.
All wordson the source side without a proper target transla-tion would then be generated by that NULL word.While this solution is technically valid, it ne-glects that those untranslatable words are requiredfor source fluency.
Moreover, the NULL word,although hypothetical in nature, does have a po-sition.
It is well-known that this NULL posi-1The target side is often identified with English and thesource side is usually taken to be French.tion is problematic for distortion-based alignmentmodels.
Alignments to NULL demand a specialtreatment as they would otherwise induce verylong jumps that one does not usually observe indistortion-based alignment models.
Examples ofthis can be found in Vogel et al (1996), who dropthe NULL word entirely and thus force all sourcewords to align lexically, and Och and Ney (2003),who choose a fixed NULL probability.In the present work, we introduce a family ofIBM-style alignment models that can express de-pendencies between translated and untranslatedsource words.
The models do not use NULLwords and instead allow untranslatable sourcewords to be generated from translated words intheir context.
This is achieved by modellingsource word collocations.
From a technical pointof view the model can be seen as a mixture of analignment and a language model.2 IBM models 1 and 2Here, we quickly review the IBM alignment mod-els 1 and 2 (Brown et al, 1993).
We assume a ran-dom variable E over the English (target) vocabu-lary2, a variable F over the French (source) vocab-ulary and a variable A over alignment links3.
TheIBM models assign probabilities to alignment con-figurations and source sentences given the targetside.
Under the assumption that all source wordsare conditionally independent given the alignmentlinks, these probabilities factorise asP (fm1, am1|el0) = P (am1)m?j=1P (fj|eaj) (1)where xk1is a vector of outcomes x1, .
.
.
, xkandeajdenotes the English word that the French word2Crucially, this vocabulary includes a NULL word.3We denote realisations of random variables by the corre-sponding lower case letters.169in the jthposition (fj) is aligned to under am1.In IBM model 1 P (am1) is uniform.
In IBMmodel 2, all alignment links ajare assumed tobe independent and follow a categorical distribu-tion.
Here, we choose to parametrise this categori-cal based on the distance between the two words tobe aligned, as has been done by Vogel et al (1996)and Liang et al (2006).
Thus, in our IBM model 2P (am1) =m?j=1P (aj) =m?j=1P(i??jlm?
)(2)where i is the position of the English word that ajlinks to and the values l and m stand for the targetand source sentence lengths.
Notice that there is atarget position i = 0 for the NULL word.
Align-ment to this NULL position often causes unusuallylong alignment jumps.3 Removing the NULL word3.1 Model descriptionOur model consists of an alignment model com-ponent (which is either IBM model 1 or 2 withoutNULL words) and a language model component.It also contains a random variable Z that indicateswhich component to use.
If Z = 0 we use thealignment model, if Z = 1 we instead use the lan-guage model.
We generate each zjconditional onfj?1.
By making the outcome zjdepend on fj?1,we allow the model to capture the tendency of in-dividual source words to be part of a collocation,i.e.
to be followed by a closely related word.
Asimilar strategy has been employed for topic mod-elling by Griffiths et al (2007).When generating the source side, the modeldoes the following for each source word fj:1.
Depending on the previous source word fj?1,draw zj.2.
If zj= 1, generate fjfrom fj?1and chooseajaccording to P (aj).
Otherwise, if zj= 0,generate fjfrom the target side and choose ajaccording to the probability that it has underthe relevant alignment model without a targetNULL word.Our model thus induces a joint probability dis-tribution of the formP (fm1, am1, zm1|el1) (3)= P (am1)m?j=1P (zj|fj?1)P (fj|eaj, fj?1, zj)ffprvaz?fqeSl1?e?a??
?s, rSmVeVfVfDSFigure 1: A graphical representation of our modelfor S sentence pairs.
We use Vf/eto denote thesource/target vocabulary sizes andD to denote thenumber of possible alignment link configurations.Furthermore, Sm/lis the number of source/targetwords in the current sentence and fprvthe sourceword preceding the one that we currently generate.where it is crucial to note that there is no E0variable, standing for the NULL word, anymore.Therefore, jumps to a NULL position do not needto be modelled.
Notice further that the formula-tion of our model is general enough to be readilyextensible to an HMM alignment model (Vogel etal., 1996).Depending on the value of zj, Fjis distributedeither according to an alignment (4) or a languagemodel4(5).P (fj|eaj, fj?1, zj= 0) = P (fj|eaj) (4)P (fj|eaj, fj?1, zj= 1) = P (fj|fj?1) (5)3.2 The full modelOur full model is a Bayesian model, meaning thatwe treat all model parameters as random variablesthat are drawn from prior distributions.
A graphi-cal depiction of the model can be found in Figure1.
We impose Dirichlet priors on the translation(?e), language model (?f) and distortion parame-ters (?a).
This has been done before and improvedthe standard IBM models.In order to be able to bias the model against us-ing the language model component (5) too oftenand instead make it prefer the alignment modelcomponent (4), we impose a Beta prior on theBernoulli distributions over component choices.In effect, the model will only explain a sourceword with the language model if there is a lot of4We use a bigram LM to avoid conditioning Z on longer(n?
1)-grams.170evidence that this word cannot be translated fromthe target side.
The full model can be summarisedas follows:Fj|e, aj, zj= 0 ?
Cat(?eaj) ?eaj?
Dir(?
)Fj|fj?1, zj= 1 ?
Cat(?fj?1) ?fj?1?
Dir(?)Zj|fj?1?
Bernoulli(q) Q ?
Beta(s, r) .For IBM model 1, Ajis uniformly distributedwhereas for model 2 we haveA ?
Cat(?a) ?a?
Dir(?)
.3.3 InferenceWe use a Gibbs sampler to perform inference ofthe alignment and choice variables.
Since our pri-ors are conjugate to the model distributions, weintegrate over the model parameters, giving us acollapsed sampler5.
The sampler alternates be-tween sampling alignment links A and componentchoices Z.The predictive posterior probabilities for Zj=0 and Zj= 1 are given in Equations (6) and (7)(up to proportionality).
We use c(?)
as a (con-ditional) count function that counts how often anoutcome has been observed in a given context.
Wefurthermore use Vfto denote the French (source)vocabulary size.
To ease notation, we also in-troduce the context set C?Xjwhich contains thecurrent values of all variables in our model exceptXjand the setH which simply contains all hyper-parameters.P(Zj= 0|C?Zj,H)?
(6)(c(z = 0|fj?1) + s)P (aj)c(fj|eaj, z = 0) + ?c(eaj|z = 0) + ?VfP(Zj= 1|C?Zj,H)?
(7)(c(z = 1|fj?1) + r)c(fj|fj?1, z = 1) + ?c(z = 1|fj?1) + ?VfWhen Zj= 0, the predictive probability for align-ment link Ajis proportional to Equation (8).P(aj|C?Zj,?Aj, Zj= 0,H)?
(8)P (aj)c(fj|eaj, z = 0) + ?c(eaj|z = 0) + ?Vf5Derivations of samplers similar to ours can be found inthe appendices of Mermer et al (2013) and Griffiths et al(2007).
We omit the derivation here for space reasons.When Zj= 1, it is simply proportional to P (aj).In the case of IBM model 1, P (aj) is a constant.For IBM model 2, we useP (aj) ?
c(i??jlm?
)+ ?
.where l and m are the target and source sentencelengths.
Notice that target positions start at 1 aswe do not use a NULL word.Notice that a na?
?ve implementation of our sam-pler is unpractically slow.
We therefore augmentthe sampler with an auxiliary variable (Tanner andWong, 1987) that uniformly chooses only one pos-sible new assignment per sampled link.
The sam-pling complexity, which would normally be lin-ear in the size of the target sentence, thus becomesconstant.
In practice this speed up the sampler byseveral orders of magnitude, making our aligneras fast as Giza++.
Unfortunately, this strategy alsoslightly impairs the mobility of our sampler.3.4 DecodingOur samples contain assignments of the A and Zvariables.
If for a word fjwe have zj= 1, wetreat the word as not aligned.
We then use maxi-mum marginal decoding (Johnson and Goldwater,2009) over alignment links to generate final wordalignments.
This means that we align each sourceword to the target word it has been aligned to mostoften in the samples.
If the word was unaligned inmost samples, we leave it unaligned in the outputalignment.4 Experiments and resultsWe present translation experiments on Englishpaired with German, French, Czech and Japanese,thereby covering four language families.
We com-pare our model and the Bayesian IBM models 1and 2 of Mermer et al (2013) against IBM model2 as a baseline.4.1 ExperimentsData We use the news commentary data fromthe WMT 2014 translation task6for German,French and Czech paired with English.
We usenewstest-2013 as development data and we use thenewstest-2014 for testing.
We use all availablemonolingual data from WMT 2014 for languagemodelling.
All data are truecased and sentences6http://statmt.org/wmt14/translation-task.html171Model En-De En-Fr En-Cs En-Ja De-En Fr-En Cs-En Ja-EnBrown et al (model 2) 14.56 27.16 13.74 25.78 18.12 26.69 18.77 23.29Mermer et al (model 1) -0.09 -0.64 +0.38 -0.13 +0.32 -0.92 +0.66 -0.31Mermer et al (model 2) +1.07 -0.17 +1.76 +0.39 +1.63 -1.04 +1.63 -0.21This work (model 1) -0.03 -0.79 -0.42 +0.15 +0.29 -1.49 +0.45 -0.65This work (model 2) +0.92 +1.32 +1.66 +1.69 +1.73 +2.01 +1.42 +2.24Giza +0.96 +0.23 +1.58 +2.97 +2.27 +2.26 +1.96 +2.73fastAlign +0.88 +0.70 +1.47 +1.97 +2.27 +1.90 +1.86 +2.63(a) Directional: alignments obtained in target-to-source direction.Model En-De En-Fr En-Cs En-Ja De-En Fr-En Cs-En Ja-EnBrown et al (model 2) +0.84 +0.77 +1.14 +3.02 +1.80 +1.77 +1.15 +2.95Mermer et al (model 1) +0.52 +0.80 +1.30 +3.19 +1.51 +1.60 +1.77 +2.44Mermer et al (model 2) +0.63 +0.33 +1.94 +3.00 +2.02 +1.22 +2.34 +2.48This work (model 1) +0.39 +0.23 +1.31 +3.33 +1.61 +0.98 +1.87 +2.56This work (model 2) +1.07 +1.47 +2.08 +2.65 +2.30 +2.19 +2.13 +3.21Giza +1.59 +0.87 +1.70 +4.24 +2.54 +2.08 +2.36 +3.94fastAlign +1.39 +1.23 +1.87 +2.47 +2.44 +2.06 +2.21 +3.58(b) Symmetrised: alignments obtained in both directions independently and heuristically symmetrised (grow-diag-final-and).Table 1: Translation results from and into English.
Alignments in the top (1a) and bottom (1b) tableswere obtained in the target-to-source direction and symmetrised, respectively.
Differences are computedwith respect to the directional IBM model 2 in its original parameterisation (Brown et al, 1993).
Thebest Bayesian model in each column is boldfaced.with more than 100 words discarded as is stan-dardly done in SMT.The Japanese training data consist of 200.000randomly extracted sentence pairs from theNTCIR-8 Patent Translation Task.
The full dataare used for language modelling.
We use theNTCIR-7 dev sets for tuning and the NTCIR-9 testset for testing.7Training The maximum likelihood IBM model2 is initialized with model 1 parameter estimatesand trained for 5 EM iterations.
Following Mer-mer and Sarac?lar (2011), we initialize the Gibbssamplers of all Bayesian models with the Viterbialignment from IBM model 1.
We run each sam-pler for 1000 iterations and take a sample after ev-ery 25thiteration.
We do not use burn-in.8Hyperparameters All Bayesian models aretrained with ?
= 0.0001 and ?
= 0.0001 toinduce sparse lexical distributions.
We also sets = 1 and r = 0.1 when IBM1 is the align-ment component in our model.
This has the ef-fect of biasing the model towards using the align-7The Japanese data was provided to us by a colleague withthe pre-processing steps already performed, with sentencesshortened to at most 40 words.
Our algorithm can handle sen-tences of any length and there is actually no need to restrictthe sentence lengths.8Burn-in is simply a heuristic that is not guaranteed toimprove the samples in any way.
See http://users.stat.umn.edu/?geyer/mcmc/burn.html for fur-ther details.ment component.
For the IBM2 version we evenset r = 0.01 since IBM2 is a more trustworthyalignment model.
For IBM2, we furthermore set?
= 1 to obtain a flat distortion prior.Observe that experiments presented here usethe same fixed hyperparameters for all languagepairs.
We tried to add another level to our modelby imposing Gamma priors on the hyperparam-eters.
The hyperparameters were then inferredusing slice sampling after each Gibbs iteration.When run on the German-English and Czech-English data, this strategy increased the posteriorprobability of the states visited by our sampler buthad no effect on BLEU.
This may indicate thateither the hand-chosen hyperparameters are ade-quate for the task or that the model generally per-forms well for a large range of hyperparameters.Translation We train Moses systems (Koehnet al, 2007) with 5-gram language models withmodified Kneser-Ney-smoothing using KenLM(Heafield et al, 2013) and orientation-based lex-icalised reordering.
We tune the systems withMERT (Och, 2003) on the dev sets.
We report theBLEU score (Papineni et al, 2002) for all modelsaveraged over 5 MERT runs.4.2 ResultsWe report the translation results in Tables (1a)and (1b).
Results of the full Giza++ pipeline andfastAlign (Dyer et al, 2013) are reported as a com-172parison standard.
All symmetrised results wereobtained using the grow-diag-final-andheuristic.Using IBM2 as an alignment component, ourmodel mostly outperforms the standard IBM mod-els and their Bayesian variants.
Importantly, theimprovement that our model 2 achieves over itsmodel 1 variant is much larger than the differencebetween the corresponding models of Mermer etal.
(2013).
This indicates that our model makesbetter use of the distortion distribution that is notaltered by NULL alignments.
We also observe thatour model gains relatively little from symmetrisa-tion, likely because it is a very strong model al-ready.
It is interesting that although our model 2does not use fertility parameters or dependenciesbetween alignment links, it often approaches theperformance of Giza which does use these fea-tures.
Moreover, it also approaches the perfor-mance of fastAlign which does not use fertility nordependencies between alignment links, but has astronger inductive bias with respect to distortion.5 Discussion and future workWe have presented an IBM-style word alignmentmodel that does not need to hypothesise a NULLword as it explains untranslatable source words bygrouping them with translated words.
This alsoleads to a cleaner handling of distortion probabili-ties.In our present work, we have only consideredIBM models 1 and 2.
As we have mentioned al-ready, our model can easily be extended with theHMM alignment model.
We are currently explor-ing this possibility.
Our models also allow sym-metrisation (Liang et al, 2006) of all translationand distortion parameters where before the NULLdistortion parameters had to be fixed.
We thereforeplan to extend them towards model-based insteadof heuristic alignment symmetrisation.A limitation of our model is that it is only ca-pable of modelling left-to-right linear dependen-cies in the source language.
In languages like Ger-man or English, however, where an adjective ordeterminer is selected by the following noun, thismay not be appropriate to model selection biasesamongst neighbouring words.
An interesting ex-tension to our model is thus to add more structureto it such that it will be able to capture more com-plex source side dependencies.Another concern is the inference in our model.Using the auxiliary variable sampler, inference be-comes very fast but may sacrifice performance.This is why we are interested in improving theinference method, e.g.
by using a more mobilesampler or by employing a variational Bayes algo-rithm.The software used in our experiments canbe downloaded from https://github.com/philschulz/Aligner.AcknowledgementsWe would like to thank Milo?s Stanojevi?c for pro-viding us with the preprocessed Japanese data.
Wewould also like to thank our reviewers for theirhelpful feedback.
This work was supported by theNetherlands Organization for Scientific Research(NWO) VICI Grant nr.
277-89-002.ReferencesPeter F. Brown, Vincent J.Della Pietra, StephenA.
Della Pietra, and Robert.
L. Mercer.
1993.The mathematics of statistical machine translation:Parameter estimation.
Computational Linguistics,19:263?311.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameteriza-tion of IBM model 2.
In In Proceedings of NAACL.Thomas L. Griffiths, Mark Steyvers, and Joshua B.Tenenbaum.
2007.
Topics in semantic representa-tion.
Psychological review, 114(2):211?244.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable modi-fied Kneser-Ney language model estimation.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics, pages 690?696,Sofia, Bulgaria.Mark Johnson and Sharon Goldwater.
2009.
Im-proving nonparameteric Bayesian inference: Ex-periments on unsupervised word segmentation withadaptor grammars.
NAACL ?09, pages 317?325.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
HLT-NAACL ?06, pages 104?111.
Association for Computational Linguistics.173Cos?kun Mermer and Murat Sarac?lar.
2011.
Bayesianword alignment for statistical machine translation.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies: Short Papers - Volume 2,pages 182?187.Cos?kun Mermer, Murat Sarac?lar, and Ruhi Sarikaya.2013.
Improving statistical machine translation us-ing Bayesian word alignment and Gibbs sampling.IEEE Transactions on Audio, Speech & LanguageProcessing, 21(5):1090?1101.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
ACL ?03, pages160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, pages 311?318.Martin A. Tanner and Wing Hung Wong.
1987.
Thecalculation of posterior distributions by data aug-mentation.
Journal of the American Statistical As-sociation, 82(398):528?550.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statisticaltranslation.
In Proceedings of the 16th Conferenceon Computational Linguistics - Volume 2, COLING?96, pages 836?841, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.174
