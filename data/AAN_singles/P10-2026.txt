Proceedings of the ACL 2010 Conference Short Papers, pages 142?146,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsBetter Filtration and Augmentation for Hierarchical Phrase-BasedTranslation RulesZhiyang Wang ?
Yajuan Lu?
?
Qun Liu ?
Young-Sook Hwang ?
?Key Lab.
of Intelligent Information Processing ?HILab Convergence Technology CenterInstitute of Computing Technology C&I BusinessChinese Academy of Sciences SKTelecomP.O.
Box 2704, Beijing 100190, China 11, Euljiro2-ga, Jung-gu, Seoul 100-999, Koreawangzhiyang@ict.ac.cn yshwang@sktelecom.comAbstractThis paper presents a novel filtration cri-terion to restrict the rule extraction forthe hierarchical phrase-based translationmodel, where a bilingual but relaxed well-formed dependency restriction is used tofilter out bad rules.
Furthermore, a newfeature which describes the regularity thatthe source/target dependency edge trig-gers the target/source word is also pro-posed.
Experimental results show that, thenew criteria weeds out about 40% ruleswhile with translation performance im-provement, and the new feature brings an-other improvement to the baseline system,especially on larger corpus.1 IntroductionHierarchical phrase-based (HPB) model (Chiang,2005) is the state-of-the-art statistical machinetranslation (SMT) model.
By looking for phrasesthat contain other phrases and replacing the sub-phrases with nonterminal symbols, it gets hierar-chical rules.
Hierarchical rules are more powerfulthan conventional phrases since they have bettergeneralization capability and could capture longdistance reordering.
However, when the train-ing corpus becomes larger, the number of ruleswill grow exponentially, which inevitably resultsin slow and memory-consuming decoding.In this paper, we address the problem of reduc-ing the hierarchical translation rule table resortingto the dependency information of bilingual lan-guages.
We only keep rules that both sides arerelaxed-well-formed (RWF) dependency structure(see the definition in Section 3), and discard otherswhich do not satisfy this constraint.
In this way,about 40% bad rules are weeded out from the orig-inal rule table.
However, the performance is evenbetter than the traditional HPB translation system.SourceTargetf f?eFigure 1: Solid wire reveals the dependency rela-tion pointing from the child to the parent.
Targetword e is triggered by the source word f and it?shead word f ?, p(e|f ?
f ?
).Based on the relaxed-well-formed dependencystructure, we also introduce a new linguistic fea-ture to enhance translation performance.
In thetraditional phrase-based SMT model, there arealways lexical translation probabilities based onIBM model 1 (Brown et al, 1993), i.e.
p(e|f),namely, the target word e is triggered by the sourceword f .
Intuitively, however, the generation of eis not only involved with f , sometimes may alsobe triggered by other context words in the sourceside.
Here we assume that the dependency edge(f ?
f ?)
of word f generates target word e (wecall it head word trigger in Section 4).
Therefore,two words in one language trigger one word inanother, which provides a more sophisticated andbetter choice for the target word, i.e.
Figure 1.Similarly, the dependency feature works well inChinese-to-English translation task, especially onlarge corpus.2 Related WorkIn the past, a significant number of techniqueshave been presented to reduce the hierarchical ruletable.
He et al (2009) just used the key phrasesof source side to filter the rule table without takingadvantage of any linguistic information.
Iglesiaset al (2009) put rules into syntactic classes basedon the number of non-terminals and patterns, andapplied various filtration strategies to improve therule table quality.
Shen et al (2008) discarded142foundThegirllovelyhousea beautifulFigure 2: An example of dependency tree.
Thecorresponding plain sentence is The lovely girlfound a beautiful house.most entries of the rule table by using the con-straint that rules of the target-side are well-formed(WF) dependency structure, but this filtering led todegradation in translation performance.
They ob-tained improvements by adding an additional de-pendency language model.
The basic differenceof our method from (Shen et al, 2008) is that wekeep rules that both sides should be relaxed-well-formed dependency structure, not just the targetside.
Besides, our system complexity is not in-creased because no additional language model isintroduced.The feature of head word trigger which we ap-ply to the log-linear model is motivated by thetrigger-based approach (Hasan and Ney, 2009).Hasan and Ney (2009) introduced a second wordto trigger the target word without considering anylinguistic information.
Furthermore, since the sec-ond word can come from any part of the sentence,there may be a prohibitively large number of pa-rameters involved.
Besides, He et al (2008) builta maximum entropy model which combines richcontext information for selecting translation rulesduring decoding.
However, as the size of the cor-pus increases, the maximum entropy model willbecome larger.
Similarly, In (Shen et al, 2009),context language model is proposed for better ruleselection.
Taking the dependency edge as condi-tion, our approach is very different from previousapproaches of exploring context information.3 Relaxed-well-formed DependencyStructureDependency models have recently gained consid-erable interest in SMT (Ding and Palmer, 2005;Quirk et al, 2005; Shen et al, 2008).
Depen-dency tree can represent richer structural infor-mation.
It reveals long-distance relation betweenwords and directly models the semantic structureof a sentence without any constituent labels.
Fig-ure 2 shows an example of a dependency tree.
Inthis example, the word found is the root of the tree.Shen et al (2008) propose the well-formed de-pendency structure to filter the hierarchical rule ta-ble.
A well-formed dependency structure could beeither a single-rooted dependency tree or a set ofsibling trees.
Although most rules are discardedwith the constraint that the target side should bewell-formed, this filtration leads to degradation intranslation performance.As an extension of the work of (Shen etal., 2008), we introduce the so-called relaxed-well-formed dependency structure to filter the hi-erarchical rule table.
Given a sentence S =w1w2...wn.
Let d1d2...dn represent the position ofparent word for each word.
For example, d3 = 4means that w3 depends on w4.
If wi is a root, wedefine di = ?1.Definition A dependency structure wi...wj isa relaxed-well-formed structure, where there ish /?
[i, j], all the words wi...wj are directly orindirectly depended on wh or -1 (here we defineh = ?1).
If and only if it satisfies the followingconditions?
dh /?
[i, j]?
?k ?
[i, j], dk ?
[i, j] or dk = hFrom the definition above, we can see thatthe relaxed-well-formed structure obviously cov-ers the well-formed one.
In this structure, wedon?t constrain that all the children of the sub-rootshould be complete.
Let?s review the dependencytree in Figure 2 as an example.
Except for the well-formed structure, we could also extract girl founda beautiful house.
Therefore, if the modifier Thelovely changes to The cute, this rule also works.4 Head Word Trigger(Koehn et al, 2003) introduced the concept oflexical weighting to check how well words ofthe phrase translate to each other.
Source wordf aligns with target word e, according to theIBM model 1, the lexical translation probabilityis p(e|f).
However, in the sense of dependencyrelationship, we believe that the generation of thetarget word e, is not only triggered by the alignedsource word f , but also associated with f ?s headword f ?.
Therefore, the lexical translation prob-ability becomes p(e|f ?
f ?
), which of courseallows for a more fine-grained lexical choice of143the target word.
More specifically, the probabil-ity could be estimated by the maximum likelihood(MLE) approach,p(e|f ?
f ?)
= count(e, f ?
f?)?e?
count(e?, f ?
f ?
)(1)Given a phrase pair f , e and word alignmenta, and the dependent relation of the source sen-tence dJ1 (J is the length of the source sentence,I is the length of the target sentence).
Therefore,given the lexical translation probability distribu-tion p(e|f ?
f ?
), we compute the feature score ofa phrase pair (f , e) asp(e|f, dJ1 , a)= ?|e|i=11|{j|(j, i) ?
a}|??
(j,i)?ap(ei|fj ?
fdj) (2)Now we get p(e|f, dJ1 , a), we could obtainp(f |e, dI1, a) (dI1 represents dependent relation ofthe target side) in the similar way.
This new fea-ture can be easily integrated into the log-linearmodel as lexical weighting does.5 ExperimentsIn this section, we describe the experimental set-ting used in this work, and verify the effect ofthe relaxed-well-formed structure filtering and thenew feature, head word trigger.5.1 Experimental SetupExperiments are carried out on the NIST1Chinese-English translation task with two differ-ent size of training corpora.?
FBIS: We use the FBIS corpus as the firsttraining corpus, which contains 239K sen-tence pairs with 6.9M Chinese words and8.9M English words.?
GQ: This is manually selected from theLDC2 corpora.
GQ contains 1.5M sentencepairs with 41M Chinese words and 48M En-glish words.
In fact, FBIS is the subset ofGQ.1www.nist.gov/speech/tests/mt2It consists of six LDC corpora:LDC2002E18, LDC2003E07, LDC2003E14, Hansards partof LDC2004T07, LDC2004T08, LDC2005T06.For language model, we use the SRI LanguageModeling Toolkit (Stolcke, 2002) to train a 4-gram model on the first 1/3 of the Xinhua portionof GIGAWORD corpus.
And we use the NIST2002 MT evaluation test set as our developmentset, and NIST 2004, 2005 test sets as our blindtest sets.
We evaluate the translation quality us-ing case-insensitive BLEU metric (Papineni etal., 2002) without dropping OOV words, and thefeature weights are tuned by minimum error ratetraining (Och, 2003).In order to get the dependency relation of thetraining corpus, we re-implement a beam-searchstyle monolingual dependency parser accordingto (Nivre and Scholz, 2004).
Then we use thesame method suggested in (Chiang, 2005) toextract SCFG grammar rules within dependencyconstraint on both sides except that unalignedwords are allowed at the edge of phrases.
Pa-rameters of head word trigger are estimated as de-scribed in Section 4.
As a default, the maximuminitial phrase length is set to 10 and the maximumrule length of the source side is set to 5.
Besides,we also re-implement the decoder of Hiero (Chi-ang, 2007) as our baseline.
In fact, we just exploitthe dependency structure during the rule extrac-tion phase.
Therefore, we don?t need to changethe main decoding algorithm of the SMT system.5.2 Results on FBIS CorpusA series of experiments was done on the FBIS cor-pus.
We first parse the bilingual languages withmonolingual dependency parser respectively, andthen only retain the rules that both sides are in linewith the constraint of dependency structure.
InTable 1, the relaxed-well-formed structure filteredout 35% of the rule table and the well-formed dis-carded 74%.
RWF extracts additional 39% com-pared to WF, which can be seen as some kindof evidence that the rules we additional get seemcommon in the sense of linguistics.
Compared to(Shen et al, 2008), we just use the dependencystructure to constrain rules, not to maintain the treestructures to guide decoding.Table 2 shows the translation result on FBIS.We can see that the RWF structure constraint canimprove translation quality substantially both atdevelopment set and different test sets.
On theTest04 task, it gains +0.86% BLEU, and +0.84%on Test05.
Besides, we also used Shen et al(2008)?s WF structure to filter both sides.
Al-though it discard about 74% of the rule table, the144System Rule table sizeHPB 30,152,090RWF 19,610,255WF 7,742,031Table 1: Rule table size with different con-straint on FBIS.
Here HPB refers to the base-line hierarchal phrase-based system, RWF meansrelaxed-well-formed constraint and WF representsthe well-formed structure.System Dev02 Test04 Test05HPB 0.3285 0.3284 0.2965WF 0.3125 0.3218 0.2887RWF 0.3326 0.3370** 0.3050RWF+Tri 0.3281 / 0.2965Table 2: Results of FBIS corpus.
Here Tri meansthe feature of head word trigger on both sides.
Andwe don?t test the new feature on Test04 because ofthe bad performance on development set.
* or **= significantly better than baseline (p < 0.05 or0.01, respectively).over-all BLEU is decreased by 0.66%-0.78% onthe test sets.As for the feature of head word trigger, it seemsnot work on the FBIS corpus.
On Test05, it getsthe same score with the baseline, but lower thanRWF filtering.
This may be caused by the datasparseness problem, which results in inaccurateparameter estimation of the new feature.5.3 Result on GQ CorpusIn this part, we increased the size of the trainingcorpus to check whether the feature of head wordtrigger works on large corpus.We get 152M rule entries from the GQ corpusaccording to (Chiang, 2007)?s extraction method.If we use the RWF structure to constrain bothsides, the number of rules is 87M, about 43% ofrule entries are discarded.
From Table 3, the newSystem Dev02 Test04 Test05HPB 0.3473 0.3386 0.3206RWF 0.3539 0.3485** 0.3228RWF+Tri 0.3540 0.3607** 0.3339*Table 3: Results of GQ corpus.
* or ** = sig-nificantly better than baseline (p < 0.05 or 0.01,respectively).feature works well on two different test sets.
Thegain is +2.21% BLEU on Test04, and +1.33% onTest05.
Compared to the result of the baseline,only using the RWF structure to filter performs thesame as the baseline on Test05, and +0.99% gainson Test04.6 ConclusionsThis paper proposes a simple strategy to filter thehierarchal rule table, and introduces a new featureto enhance the translation performance.
We em-ploy the relaxed-well-formed dependency struc-ture to constrain both sides of the rule, and about40% of rules are discarded with improvement ofthe translation performance.
In order to make fulluse of the dependency information, we assumethat the target word e is triggered by dependencyedge of the corresponding source word f .
Andthis feature works well on large parallel trainingcorpus.How to estimate the probability of head wordtrigger is very important.
Here we only get the pa-rameters in a generative way.
In the future, we weare plan to exploit some discriminative approachto train parameters of this feature, such as EM al-gorithm (Hasan et al, 2008) or maximum entropy(He et al, 2008).Besides, the quality of the parser is another ef-fect for this method.
As the next step, we willtry to exploit bilingual knowledge to improve themonolingual parser, i.e.
(Huang et al, 2009).AcknowledgmentsThis work was partly supported by NationalNatural Science Foundation of China Contract60873167.
It was also funded by SK Telecom,Korea under the contract 4360002953.
We showour special thanks to Wenbin Jiang and Shu Caifor their valuable suggestions.
We also thankthe anonymous reviewers for their insightful com-ments.ReferencesPeter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: pa-rameter estimation.
Comput.
Linguist., 19(2):263?311.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In ACL145?05: Proceedings of the 43rd Annual Meeting on As-sociation for Computational Linguistics, pages 263?270.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Comput.
Linguist., 33(2):201?228.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependencyinsertion grammars.
In ACL ?05: Proceedings of the43rd Annual Meeting on Association for Computa-tional Linguistics, pages 541?548.Sas?a Hasan and Hermann Ney.
2009.
Comparison ofextended lexicon models in search and rescoring forsmt.
In NAACL ?09: Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, Companion Volume:Short Papers, pages 17?20.Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, andJesu?s Andre?s-Ferrer.
2008.
Triplet lexicon modelsfor statistical machine translation.
In EMNLP ?08:Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 372?381.Zhongjun He, Qun Liu, and Shouxun Lin.
2008.
Im-proving statistical machine translation using lexical-ized rule selection.
In COLING ?08: Proceedingsof the 22nd International Conference on Computa-tional Linguistics, pages 321?328.Zhongjun He, Yao Meng, Yajuan Lu?, Hao Yu, and QunLiu.
2009.
Reducing smt rule table with monolin-gual key phrase.
In ACL-IJCNLP ?09: Proceedingsof the ACL-IJCNLP 2009 Conference Short Papers,pages 121?124.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In EMNLP ?09: Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 1222?1231.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009.
Rule filtering by patternfor efficient hierarchical translation.
In EACL ?09:Proceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 380?388.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InNAACL ?03: Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology, pages 48?54.Joakim Nivre and Mario Scholz.
2004.
Determinis-tic dependency parsing of english text.
In COLING?04: Proceedings of the 20th international confer-ence on Computational Linguistics, pages 64?70.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In ACL ?03: Pro-ceedings of the 41st Annual Meeting on Associationfor Computational Linguistics, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In ACL ?02: Proceed-ings of the 40th Annual Meeting on Association forComputational Linguistics, pages 311?318.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: syntactically in-formed phrasal smt.
In ACL ?05: Proceedings ofthe 43rd Annual Meeting on Association for Com-putational Linguistics, pages 271?279.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL-08: HLT, pages 577?585.Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,and Ralph Weischedel.
2009.
Effective use of lin-guistic and contextual information for statistical ma-chine translation.
In EMNLP ?09: Proceedings ofthe 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 72?80.Andreas Stolcke.
2002.
Srilman extensible languagemodeling toolkit.
In In Proceedings of the 7th Inter-national Conference on Spoken Language Process-ing (ICSLP 2002), pages 901?904.146
