Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 23?30,New York City, New York, June 2006. c?2006 Association for Computational LinguisticsYou Are What You Say: Using Meeting Participants?
Speechto Detect their Roles and ExpertiseSatanjeev BanerjeeLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213banerjee@cs.cmu.eduAlexander I. RudnickySchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213air@cs.cmu.eduAbstractOur goal is to automatically detect thefunctional roles that meeting participantsplay, as well as the expertise they bring tomeetings.
To perform this task, we builddecision tree classifiers that use a combi-nation of simple speech features (speechlengths and spoken keywords) extractedfrom the participants?
speech in meetings.We show that this algorithm results in arole detection accuracy of 83% on unseentest data, where the random baseline is33.3%.
We also introduce a simple aggre-gation mechanism that combines evidenceof the participants?
expertise from multi-ple meetings.
We show that this aggre-gation mechanism improves the role de-tection accuracy from 66.7% (when ag-gregating over a single meeting) to 83%(when aggregating over 5 meetings).1 IntroductionA multitude of meetings are organized every dayaround the world to discuss and exchange impor-tant information, to make decisions and to collab-oratively solve problems.
Our goal is to create sys-tems that automatically understand the discussionsat meetings, and use this understanding to assistmeeting participants in various tasks during and af-ter meetings.
One such task is the retrieval of infor-mation from previous meetings, which is typicallya difficult and time consuming task for the humanto perform (Banerjee et al, 2005).
Another task isto automatically record the action items being dis-cussed at meetings, along with details such as whenthe action is due, who is responsible for it, etc.Meeting analysis is a quickly growing field ofstudy.
In recent years, research has focussed on au-tomatic speech recognition in meetings (Stolcke etal., 2004; Metze et al, 2004; Hain et al, 2005), ac-tivity recognition (Rybski and Veloso, 2004), auto-matic meeting summarization (Murray et al, 2005),meeting phase detection (Banerjee and Rudnicky,2004) and topic detection (Galley et al, 2003).
Rela-tively little research has been performed on automat-ically detecting the roles that meeting participantsplay as they participate in meetings.
These roles canbe functional (e.g.
the facilitator who runs the meet-ing, and the scribe who is the designated note takerat the meeting), discourse based (e.g.
the presenter,and the discussion participant), and expertise related(e.g.
the hardware acquisition expert and the speechrecognition research expert).
Some roles are tightlyscoped, relevant to just one meeting or even a partof a meeting.
For example, a person can be the fa-cilitator of one meeting and the scribe of another, orthe same person can be a presenter for one part ofthe meeting and a discussion participant for anotherpart.
On the other hand, some roles have a broaderscope and last for the duration of a project.
Thusa single person may be the speech recognition ex-pert in a project and have that role in all meetingson that project.
Additionally, the same person canplay multiple roles, e.g.
the scribe can be a speechrecognition expert too.Automatic role detection has many benefits, espe-23cially when used as a source of constraint for othermeeting understanding components.
For example,detecting the facilitator of the meeting might helpthe automatic topic detection module if we knowthat facilitators officially change topics and move thediscussion from one agenda item to the next.
Know-ing who the speech recognition expert is can helpthe automatic action item detector: If an action itemregarding speech recognition has been detected butthe responsible person field has not been detected,the module may place a higher probability on thespeech recognition expert as being the responsibleperson for that action item.
Additionally, detectingwho is an expert in which field can have benefits ofits own.
For example, it can be used to automaticallydirect queries on a particular subject to the persondeemed most qualified to answer the question, etc.Basic information such as participant role and ex-pertise needs to be robustly extracted if it is to be ofuse to the more sophisticated stages of understand-ing.
Accordingly, we have based our role detectionalgorithm on simple and highly accurate speech fea-tures, as described in section 5.1.2.
(Banerjee and Rudnicky, 2004) describes the au-tomatic detection of discourse roles in meetings.These roles included presenter (participants whomake formal presentations using either slides orthe whiteboard), discussion participant (participantsinvolved in a discussion marked by frequent turnchanges), observer (participants not speaking, butnevertheless consuming information during a pre-sentation or discussion), etc.
In this paper we focuson automatically detecting the functional and exper-tise based roles that participants play in a meeting.In the next section we describe the data that is usedin all our role detection work in this paper.
In subse-quent sections we describe the role detection algo-rithm in more detail, and present evaluation results.2 The Y2 Meeting Scenario DataOur research work is part of the Cognitive Assistantthat Learns and Organizes project (CALO, 2003).
Agoal of this project is to create an artificial assis-tant that can understand meetings and use this un-derstanding to assist meeting participants during andafter meetings.
Towards this goal, data is being col-lected by creating a rich multimodal record of meet-ings (e.g.
(Banerjee et al, 2004)).
While a largepart of this data consists of natural meetings (thatwould have taken place even if they weren?t beingrecorded), a small subset of this data is ?scenariodriven?
?
the Y2 Scenario Data.Meeting # Typical scenario1 Hiring Joe: Buy a computer andfind office space for him2 Hiring Cindy and Fred: Buy com?puters & find office space for them3 Buy printer for Joe, Cindy and Fred4 Buy a server machine for Joe,Cindy and Fred5 Buy desktop and printer for themeeting leaderTable 1: Typical Scenario InstructionsThe Y2 Scenario Data consists of meetings be-tween groups of 3 or 4 participants.
Each group par-ticipated in a sequence of up to 5 meetings.
Eachsequence had an overall scenario ?
the purchasingof computing hardware and the allocation of officespace for three newly hired employees.
Participantswere told to assume that the meetings in the se-quence were being held one week apart, and that be-tween any two meetings ?progress?
was made on theaction items decided at each meeting.
Participantswere given latitude to come up with their own sto-ries of what ?progress?
was made between meetings.At each meeting, participants were asked to reviewprogress since the last meeting and make changes totheir decisions if necessary.
Additionally, an extratopic was introduced at each meeting, as shown intable 1.In each group of participants, one participantplayed the role of the manager who has control overthe funds and makes the final decisions on the pur-chases.
The remaining 2 or 3 participants played theroles of either the hardware acquisition expert or thebuilding facilities expert.
The role of the hardwareexpert was to make recommendations on the buyingof computers and printers, and to actually make thepurchases once a decision was made to do so.
Sim-ilarly the role of the building expert was to makerecommendations on which rooms were available tofit the new employees into.
Despite this role assign-24ment, all participants were expected to contribute todiscussions on all topics.To make the meetings as natural as possible, theparticipants were given control over the evolution ofthe story, and were also encouraged to create con-flicts between the manager?s demands and the advicethat the experts gave him.
For example, managerssometimes requested that all three employees be putin a single office, but the facilities expert announcedthat no 3 person room was available, unless the man-ager was agreeable to pay extra for them.
Theseconflicts led to extended negotiations between theparticipants.
To promote fluency, participants wereinstructed to use their knowledge of existing facili-ties and equipment instead of inventing a completelyfictitious set of details (such as room numbers).The data we use in this paper consists of 8 se-quences recorded at Carnegie Mellon University andat SRI International between 2004 and 2005.
One ofthese sequences has 4 meetings, the remaining have5 meetings each, for a total of 39 meetings.
4 ofthese sequences had a total of 3 participants each;the remaining 4 sequences had a total of 4 partici-pants each.
On average each meeting was 15 min-utes long.
We partitioned this data into two roughlyequal sets, the training set containing 4 meeting se-quences, and the test set containing the remaining4 sets.
Although a few participants participated inmultiple meetings, there was no overlap of partici-pants between the training and the test set.3 Functional RolesMeeting participants have functional roles that en-sure the smooth conduct of the meeting, with-out regard to the specific contents of the meeting.These roles may include that of the meeting leaderwhose functions typically include starting the meet-ing, establishing the agenda (perhaps in consulta-tion with the other participants), making sure thediscussions remain on?agenda, moving the discus-sion from agenda item to agenda item, etc.
Anotherpossible functional role is that of a the designatedmeeting scribe.
Such a person may be tasked withthe job of taking the official notes or minutes for themeeting.Currently we are attempting to automatically de-tect the meeting leader for a given meeting.
In ourdata (as described in section 2) the participant play-ing the role of the manager is always the meetingleader.
In section 5 we describe our methodologyfor automatically detecting the meeting leader.4 ExpertiseTypically each participant in a meeting makes con-tributions to the discussions at the meeting (and tothe project or organization in general) based on theirown expertise or skill set.
For example, a projectto build a multi?modal note taking application mayinclude project members with expertise in speechrecognition, in video analysis, etc.
We define ex-pertise based roles as roles based on skills that arerelevant to participants?
contributions to the meetingdiscussions and the project or organization in gen-eral.
Note that the expertise role a participant playsin a meeting is potentially dependent on the exper-tise roles of the other participants in the meeting,and that a single person may play different expertiseroles in different meetings, or even within a singlemeeting.
For example, a single person may be the?speech recognition expert?
on the note taking appli-cation project that simply uses off?the?shelf speechrecognition tools to perform note taking, but a ?noisecancellation?
expert on the project that is attemptingto improve the in?house speech recognizer.
Auto-matically detecting each participant?s roles can helpsuch meeting understanding components as the ac-tion item detector.Ideally we would like to automatically discoverthe roles that each participant plays, and clusterthese roles into groups of similar roles so thatthe meeting understanding components can transferwhat they learn about particular participants to other(and newer) participants with similar roles.
Such arole detection mechanism would need no prior train-ing data about the specific roles that participantsplay in a new organization or project.
Currentlyhowever, we have started with a simplified partici-pant role detection task where we do have trainingdata pertinent to the specific roles that meeting par-ticipants play in the test set of meetings.
As men-tioned in section 2, our data consists of people play-ing two kinds of expertise?based roles ?
that of ahardware acquisition expert, and that of a buildingfacilities expert.
In the next section we discuss our25methodology of automatically detecting these rolesfrom the meeting participants?
speech.5 MethodologyGiven a sequence of longitudinal meetings, we de-fine our role detection task as a three?way classi-fication problem, where the input to the classifierconsists of features extracted from the speech of aparticular participant over the given meetings, andthe output is a probability distribution over the threepossible roles.
Note that although a single par-ticipant can simultaneously play both a functionaland an expertise?based role, in the Y2 ScenarioData each participant plays exactly one of the threeroles.
We take advantage of this situation to simplifythe problem to the three way classification definedabove.
We induce a decision tree (Quinlan, 1986)classifier from hand labeled data.
In the next sub-section we describe the steps involved in training thedecision tree role classifier, and in the subsequentsubsection we describe how the trained decision treeis used to arrive at a role label for each meeting par-ticipant.5.1 Training5.1.1 Keyword List CreationOne of the sources of information that we wishto employ to perform functional and expertise roledetection is the words that are spoken by each par-ticipant over the course of the meetings.
Our ap-proach to harness this information source is to uselabeled training data to first create a set of wordsmost strongly associated with each of the three roles,and then use only these words during the feature ex-traction phase to detect each participant?s role, as de-scribed in section 5.1.2.We created this list of keywords as follows.
Givena training set of meeting sequences, we aggregatedfor each role all the speech from all the participantswho had played that role in the training set.
We thensplit this data into individual words and removedstop words ?
closed class words (mainly articles andprepositions) that typically contain less informationpertinent to the task than do nouns and verbs.
For allwords across all the three roles, we computed the de-gree of association between each word and each ofthe three roles, using the chi squared method (Yangand Pedersen, 1997), and chose the top 200 highscoring word?role pairs.
Finally we manually exam-ined this list of words, and removed additional wordsthat we deemed to not be relevant to the task (essen-tially identifying a domain?specific stop list).
Thisreduced the list to a total of 180 words.
The 5 mostfrequently occurring words in this list are: computer,right, need, week and space.
Intuitively the goal ofthis keyword selection pre?processing step is to savethe decision tree role classifier from having to auto-matically detect the important words from a muchlarger set of words, which would require more datato train.5.1.2 Feature ExtractionThe input to the decision tree role classifier is a setof features abstracted from a specific participant?sspeech.
One strategy is to extract exactly one set offeatures from all the speech belonging to a partici-pant across all the meetings in the meeting sequence.However, this approach requires a very large num-ber of meetings to train.
Our chosen strategy is tosample the speech output by each participant multi-ple times over the course of the meeting sequence,classify each such sample, and then aggregate theevidence over all the samples to arrive at the overalllikelihood that a participant is playing a certain role.To perform the sampling, we split each meetingin the meeting sequence into a sequence of contigu-ous windows each n seconds long, and then computeone set of features from each participant?s speechduring each window.
The value of n is decidedthrough parametric tests (described in section 7.1).If a particular participant was silent during the en-tire duration of a particular window, then featuresare extracted from that silence.Note that in the above formulation, there is nooverlap (nor gap) between successive windows.
Ina separate set of experiments we used overlappingwindows.
That is, given a window size, we movedthe window by a fixed step size (less than the sizeof the window) and computed features from eachsuch overlapping window.
The results of theseexperiments were no better than those with non?overlapping windows, and so for the rest of this pa-per we simply report on the results with the non?overlapping windows.Given a particular window of speech of a partic-26ular participant, we extract the following 2 speechlength based features:?
Rank of this participant (among this meet-ing?s participants) in terms of the length of hisspeech during this window.
Thus, if this partic-ipant spoke the longest during the window, hehas a feature value of 1, if he spoke for the sec-ond longest number of times, he has a featurevalue of 2, etc.?
Ratio of the length of speech of this participantin this window to the total length of speechfrom all participants in this window.
Thus ifa participant spoke for 3 seconds, and the to-tal length of speech from all participants inthis window was 6 seconds, his feature valueis 0.5.
Together with the rank feature above,these two features capture the amount of speechcontributed by each participant to the window,relative to the other participants.In addition, for each window of speech of a par-ticular participant, and for each keyword in our listof pre?decided keywords, we extract the following2 features:?
Rank of this participant (among this meeting?sparticipants) in terms of the number of timesthis keyword was spoken.
Thus if in this win-dow of time, this participant spoke the keywordprinter more often than any of the other partic-ipants, then his feature value for this keywordis 1.?
Ratio of the number of times this participantuttered this keyword in this window to the totalnumber of times this keyword was uttered byall the participants during this window.
Thusif a participant spoke the word printer 5 timesin this window, and in total all participants saidthe word printer 7 times, then his feature valuefor this keyword is 5/7.
Together with the key-word rank feature above, these two featurescapture the number of times each participantutters each keyword, relative to the other par-ticipants.Thus for each participant, for each meeting win-dow, we extract two features based on the lengthsof speech, and 2 ?
180 features for each of the 180keywords, for a total of 362 features.
The true outputlabel for each such data point is the role of that par-ticipant in the meeting sequence.
We used these datapoints to induce a classifier using the Weka Java im-plementation (Witten and Frank, 2000) of the C4.5decision tree learning algorithm (Quinlan, 1986).This classifier takes features as described above asinput, and outputs class membership probabilities,where the classes are the three roles.
Note that forthe experiments in this paper we extract these fea-tures from the manual transcriptions of the speechof the meeting participants.
In the future we plan toperform these experiments using the transcriptionsoutput by an automatic speech recognizer.5.2 Detecting Roles in Unseen Data5.2.1 Classifying Windows of Unseen DataDetecting the roles of meeting participants in un-seen data is performed as follows: First the unseentest data is split into windows of the same size as wasused during the training regime.
Then the speech ac-tivity and keywords based features are extracted (us-ing the same keywords as was used during the train-ing) for each participant in each window.
Finallythese data points are used as input into the traineddecision tree, which outputs class membership prob-abilities for each participant in each window.5.2.2 Aggregating Evidence to Assign One RolePer ParticipantThus for each participant we get as many proba-bility distributions (over the three roles) as there arewindows in the test data.
The next step is to aggre-gate these probabilities over all the windows and ar-rive at a single role assignment per participant.
Weemploy the simplest possible aggregation method:We compute, for each participant, the average prob-ability of each role over all the windows, and thennormalize the three average role probabilities so cal-culated, so they still sum to 1.
In the future we planto experiment with more sophisticated aggregationmechanisms that jointly optimize the probabilities ofthe different participants, instead of computing themindependently.At this point, we could assign to each participanthis highest probability role.
However, we wish toensure that the set of roles that get assigned to the27participants in a particular meeting are as diverseas possible (since typically meetings are forums atwhich different people of different expertise con-vene to exchange information).
To ensure such di-versity, we apply the following heuristic.
Once wehave all the average probabilities for all the roles foreach participant in a sequence of meetings, we as-sign roles to participants in stages.
At each stagewe consider all participants not yet assigned roles,and pick that participant?role pair, say (p, r), thathas the highest probability value among all pairs un-der consideration.
We assign participant p the role r,and then discount (by a constant multiplicative fac-tor) the probability value of all participant?role pairs(pi, rj) where pi is a participant not assigned a roleyet, and rj = r. This makes it less likely (but notimpossible) that another participant will be assignedthis same role r again.
This process is repeated untilall participants have been assigned a role each.6 EvaluationWe evaluated the algorithm by computing the accu-racy of the detector?s role predictions.
Specifically,given a meeting sequence we ran the algorithm toassign a role to each meeting participant, and com-puted the accuracy by calculating the ratio of thenumber of correct assignments to the total numberof participants in the sequence.
Note that it is alsopossible to evaluate the window?by?window clas-sification of the decision tree classifiers; we reportresults on this evaluation in section 7.1.To evaluate this participant role detection algo-rithm, we first trained the algorithm on the trainingset of meetings.
The training phase included key-word list creation, window size optimization, andthe actual induction of the decision tree.
On thetraining data, a window size of 300 seconds resultedin the highest accuracy over the training set.
The testat the root of the induced tree was whether the par-ticipant?s rank in terms of speech lengths was 1, inwhich case he was immediately classified as a meet-ing leader.
That is, the tree learnt that the personwho spoke the most in a window was most likelythe meeting leader.
Other tests placed high in thetree included obvious ones such as testing for thekeywords computer and printer to classify a partici-pant as a hardware expert.We then tested this trained role detector on thetesting set of meetings.
Recall that the test set had5 meeting sequences, each consisting of 5 meetingsand a total of 20 meeting participants.
Over this testset we obtained a role detection accuracy of 83%.A ?classifier?
that randomly assigns one of the threeroles to each participant in a meeting (without re-gard to the roles assigned to the other participants inthe same meeting) would achieve a classification ac-curacy of 33.3%.
Thus, our algorithm significantlybeats the random classifier baseline.
Note that asmentioned earlier, the experiments in this paper arebased on the manually transcribed speech.7 Further Experiments7.1 Optimizing the Window SizeAs mentioned above, one of the variables to be tunedduring the training phase is the size of the windowover which to extract speech features.
We ran a se-quence of experiments to optimize this window size,the results of which are summarized in figure 1.
Inthis set of experiments, we performed the evaluationon two levels of granularity.
The larger granularitylevel was the ?meeting sequence?
granularity, wherewe ran the usual evaluation described above.
Thatis, for each participant we first used the classifier toobtain probability distributions over the 3 roles onevery window, and then aggregated these distribu-tions to reach a single role assignment for the par-ticipant over the entire meeting sequence.
This rolewas compared to the true role of the participant tomeasure the accuracy of the algorithm.
The smallergranularity level was the ?window?
level, where af-ter obtaining the probability distribution over thethree roles for a particular window of a particu-lar participant, we picked the role with the high-est probability, and assigned it to the participant forthat window.
Therefore, for each window we hada role assignment that we compared to the true roleof the participant, resulting in an accuracy value forthe classifier for every window for every participant.Note that the main difference between evaluation atthese two granularity levels is that in the ?window?granularity, we did not have any aggregation of evi-dence across multiple windows.For different window sizes, we plotted the accu-racy values obtained on the test set for the two evalu-284050607080901000  100  200  300  400  500  600  700  800  900  1000AccuracyWindow SizeAccuracy meeting sequenceAccuracy windowFigure 1: Effect of Different Window Sizes on Detection Ac-curacyation granularities, as shown in figure 1.
Notice thatby aggregating the evidence across the windows, thedetection accuracy improves for all window sizes.This is to be expected since in the window gran-ularity, the classifier has access to only the infor-mation contained in a single window, and is there-fore more error prone.
However by merging the ev-idence from many windows, the accuracy improves.As window sizes increase, detection accuracy at thewindow level improves, because the classifier hasmore evidence at its disposal to make the decision.However, detection at the meeting sequence levelgets steadily worse, potentially because the largerthe window size, the fewer the data points it has toaggregate evidence from.
These lines will eventu-ally meet when the window size equals the size ofthe entire meeting sequence.A valid concern with these results is the high levelof noise, particularly in the aggregated detection ac-curacy over the meeting sequence.
One reason forthis is that there are far fewer data points at the meet-ing sequence level than at the window level.
Withlarger data sets (more meeting sequences as well asmore participants per meeting) these results may sta-bilize.
Additionally, given the small amount of data,our feature set is quite large, so a more aggressivefeature set reduction might help stabilize the results.7.2 Automatic Improvement over Unseen DataOne of our goals is to create an expertise based roledetector system that improves over time as it has ac-cess to more and more meetings for a given par-4050607080901001  2  3  4  5AccuracyWindow SizeAccuracyFigure 2: Accuracy versus Number of Meetings over whichRoles were Detectedticipant.
This is especially important because theroles that a participant plays can change over time;we would like our system to be able to track thesechanges.
In the Y2 Scenario Data that we have usedin this current work, the roles do not change frommeeting to meeting.
However observe that our evi-dence aggregation algorithm fuses information fromall the meetings in a specific sequence of meetingsto arrive at a single role assignment for each partici-pant.To quantify the effect of this aggregation we com-puted the role detection accuracy using differentnumbers of meetings from each sequence.
Specif-ically, we computed the accuracy of the role detec-tion over the test data using only the last meeting ofeach sequence, only the last 2 meetings of each se-quence, and so on until we used every meeting in ev-ery sequence.
The results are summarized in figure2.
When using only the last meeting in the sequenceto assign roles to the participants, the accuracy isonly 66.7%, when using the last two meetings, theaccuracy is 75%, and using the last three, four orall meetings results in an accuracy of 83%.
Thus,the accuracy improves as we have more meetings tocombine evidence from, as is expected.
Howeverthe accuracy levels off at 83% when using three ormore meetings, perhaps because there is no new in-formation to be gained by adding a fourth or a fifthmeeting.8 Conclusions and Future WorkIn this paper we have discussed our current approachto detecting the functional and expertise based rolesof meeting participants.
We have induced decision29trees that use simple and robust speech based fea-tures to perform the role detection.
We have useda very simple evidence aggregation mechanism toarrive at a single role assignment per meeting partic-ipant over a sequence of meetings, and have shownthat we can achieve up to 83% accuracy on unseentest data using this mechanism.
Additionally wehave shown that by aggregating evidence across asequence of meetings, we perform better than if wewere to use a single meeting to perform the role de-tection.
As future work we plan to remove the con-straints that we have currently imposed ?
namely, wewill attempt to learn new roles in test data that do notexist in training data.
Additionally, we will attemptto use this role information as inputs to downstreammeeting understanding tasks such as automatic topicdetection and action item detection.9 AcknowledgementsThis work was supported by DARPA grant NBCH-D-03-0010.
The content of the information in thispublication does not necessarily reflect the positionor the policy of the US Government, and no officialendorsement should be inferred.ReferencesS.
Banerjee and A. I. Rudnicky.
2004.
Using simplespeech-based features to detect the state of a meet-ing and the roles of the meeting participants.
In Pro-ceedings of the 8th International Conference on Spo-ken Language Processing (Interspeech 2004 ?
ICSLP),Jeju Island, Korea.S.
Banerjee, J. Cohen, T. Quisel, A. Chan, Y. Pato-dia, Z. Al-Bawab, R. Zhang, P. Rybski, M. Veloso,A.
Black, R. Stern, R. Rosenfeld, and A. I. Rudnicky.2004.
Creating multi-modal, user?centric records ofmeetings with the Carnegie Mellon meeting recorderarchitecture.
In Proceedings of the ICASSP MeetingRecognition Workshop, Montreal, Canada.S.
Banerjee, C. Rose, and A. I. Rudnicky.
2005.
Thenecessity of a meeting recording and playback system,and the benefit of topic?level annotations to meetingbrowsing.
In Proceedings of the Tenth InternationalConference on Human-Computer Interaction, Rome,Italy, September.CALO.
2003. http://www.ai.sri.com/project/CALO.M.
Galley, K. McKeown, E. Fosler-Lussier, and HongyanJing.
2003.
Discourse segmentation of multi?partyconversation.
In Proceedings of the 41st Annual Meet-ing on Association for Computational Linguistics, vol-ume 1, pages 562 ?
569, Sapporo, Japan.T.
Hain, J. Dines, G. Garau, M. Karafiat, D. Moore,V.
Wan, R. Ordelman, and S. Renals.
2005.
Transcrip-tion of conference room meetings: An investigation.In Proceedings of Interspeech 2005, Lisbon, Portugal,September.F.
Metze, Q. Jin, C. Fugen, K. Laskowski, Y. Pan, andT.
Schultz.
2004.
Issues in meeting transcription ?the isl meeting transcription system.
In Proceedings ofthe 8th International Conference on Spoken LanguageProcessing (Interspeech 2004 ?
ICSLP), Jeju Island,Korea.G.
Murray, S. Renals, and J. Carletta.
2005.
Extractivesummarization of meeting recordings.
In Proceedingsof Interspeech 2005, Lisbon, Portugal, September.J.
Quinlan.
1986.
Induction of decision trees.
MachineLearning, 1:81?106.Paul E. Rybski and Manuela M. Veloso.
2004.
Usingsparse visual data to model human activities in meet-ings.
In Workshop on Modeling Other Agents fromObservations, International Joint Conference on Au-tonomous Agents and Multi-Agent Systems.A.
Stolcke, C. Wooters, N. Mirghafori, T. Pirinen, I. Bu-lyko, D. Gelbart, M. Graciarena, S. Otterson, B. Pe-skin, and M. Ostendorf.
2004.
Progress in meetingrecognition: The icsi?sri?uw spring 2004 evaluationsystem.
In NIST RT04 Meeting Recognition Work-shop, Montreal.I.
Witten and E. Frank.
2000.
Data Mining - Practi-cal Machine Learning Tools and Techniques with JavaImplementations.
Morgan?Kaufmann, San Francisco,CA.Y.
Yang and J. Pedersen.
1997.
A comparative study onfeature selection in text categorization.
In Proceedingsof the International Conference on Machine Learn-ing, pages 412?420, Nashville, US.
Morgan Kauf-mann Publishers.30
