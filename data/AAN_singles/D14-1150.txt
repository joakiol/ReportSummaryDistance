Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1435?1446,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsCitation-Enhanced Keyphrase Extraction from Research Papers:A Supervised ApproachCornelia Caragea1, Florin Bulgarov1, Andreea Godea1, Sujatha Das Gollapalli21Computer Science and Engineering, University of North Texas, TX, USA2Institute for Infocomm Research, A*STAR, Singaporeccaragea@unt.edu, FlorinBulgarov@my.unt.edu,AndreeaGodea@my.unt.edu, gsdas@cse.psu.eduAbstractGiven the large amounts of online textualdocuments available these days, e.g., newsarticles, weblogs, and scientific papers, ef-fective methods for extracting keyphrases,which provide a high-level topic descrip-tion of a document, are greatly needed.
Inthis paper, we propose a supervised modelfor keyphrase extraction from research pa-pers, which are embedded in citation net-works.
To this end, we design novel fea-tures based on citation network informa-tion and use them in conjunction with tra-ditional features for keyphrase extractionto obtain remarkable improvements in per-formance over strong baselines.1 IntroductionKeyphrase extraction is the problem of automat-ically extracting important phrases or concepts(i.e., the essence) of a document.
Keyphrasesprovide a high-level topic description of a docu-ment and are shown to be rich sources of informa-tion for many applications such as document clas-sification, clustering, recommendation, indexing,searching, and summarization (Jones and Stave-ley, 1999; Zha, 2002; Hammouda et al., 2005;Pudota et al., 2010; Turney, 2003).
Despite thefact that keyphrase extraction has been widely re-searched in the natural language processing com-munity, its performance is still far from being sat-isfactory (Hasan and Ng, 2014).Many previous approaches to keyphrase extrac-tion generally used only the textual content ofa target document to extract keyphrases (Hulth,2003; Mihalcea and Tarau, 2004; Liu et al., 2010).Recently, Wan and Xiao (2008) proposed a modelthat incorporates a local neighborhood of a doc-ument.
However, their neighborhood is limitedto textually-similar documents, where the cosinesimilarity between the tf-idf vectors of documentsis used to compute their similarity.
We positthat, in addition to a document?s textual contentand textually-similar neighbors, other informativeneighborhoods exist that have the potential to im-prove keyphrase extraction.
For example, in ascholarly domain, research papers are not isolated.Rather, they are highly inter-connected in giant ci-tation networks, in which papers cite or are citedby other papers.
In a citation network, informationflows from one paper to another via the citation re-lation (Shi et al., 2010).
This information flow andthe influence of one paper on another are specifi-cally captured by means of citation contexts, i.e.,short text segments surrounding a citation?s men-tion.
These contexts are not arbitrary, but theyserve as brief summaries of a cited paper.
Figure1 illustrates this idea using a small citation net-work of a paper by Rendle et al.
(2010) that cites(Zimdars et al., 2001), (Hu et al., 2008), (Pan andScholz, 2009) and (Shani et al., 2005) and is citedby (Cheng et al., 2013).
The citation mentionsand citation contexts are shown with a dashed line.Note the high overlap between the words in con-texts and those in the title and abstract (shown inbold) and the author-annotated keywords.One question that can be raised is the following:Can we effectively exploit information availablein large inter-linked document networks in orderto improve the performance of keyphrase extrac-tion?
The research that we describe in this paperaddresses specifically this question using citationnetworks of research papers as a case study.
Ex-tracting keyphrases that can accurately ?represent?research papers is crucial to dealing with the largenumbers of research papers published during these?big data?
times.
The importance of keyphrase ex-traction from research papers is also emphasizedby the recent SemEval 2010 Shared Task on thistopic (Kim et al., 2010; Kim et al., 2013).Our contributions.
We present a supervised1435Figure 1: A small citation network corresponding to a paper by Rendle et al.
(2010).approach to keyphrase extraction from researchpapers that, in addition to the information con-tained in a paper itself, effectively incorporates,in the learned models, information from the pa-per?s local neighborhood available in citation net-works.
To this end, we design novel features forkeyphrase extraction based on citation context in-formation and use them in conjunction with tradi-tional features in a supervised probabilistic frame-work.
We show empirically that the proposedmodels substantially outperform strong baselineson two datasets of research papers compiled fromtwo machine learning conferences: the WorldWide Web and Knowledge Discovery from Data.The rest of the paper is organized as follows:We summarize closely related work in Section 2.The supervised classification for keyphrase extrac-tion is discussed in Section 3.
Experiments and re-sults are presented in Section 4, followed by con-clusions and future directions of our work.2 Related WorkMany approaches to keyphrase extraction havebeen proposed in the literature along two lines ofresearch: supervised and unsupervised, using dif-ferent types of documents including scientific ab-stracts, newswire documents, meeting transcripts,and webpages (Frank et al., 1999; Hulth, 2003;Nguyen and Kan, 2007; Liu et al., 2009; Marujoet al., 2013; Mihalcea and Tarau, 2004).In the supervised line of research, keyphraseextraction is formulated as a binary classificationproblem, where candidate phrases are classified aseither positive (i.e., keyphrases) or negative (i.e.,non-keyphrases) (Frank et al., 1999; Turney, 2000;Hulth, 2003).
Different feature sets and classifica-tion algorithms gave rise to different models.
Forexample, Hulth (2003) used four different featuresin conjunction with a bagging technique.
Thesefeatures are: term frequency, collection frequency,the relative position of the first occurrence and thepart-of-speech tag of a term.
Frank et al.
(1999)developed a system called KEA that used onlytwo features: tf-idf (term frequency-inverse doc-ument frequency) of a phrase and the distance ofa phrase from the beginning of a document (i.e.,its relative position) and used them as input toNa?
?ve Bayes.
Nguyen and Kan (2007) extendedKEA to include features such as the distributionof keyphrases among different sections of a re-search paper, and the acronym status of a term.
Incontrast to these works, we propose novel featuresextracted from the local neighborhoods of docu-ments available in interlinked document networks.Medelyan et al.
(2009) extended KEA as well tointegrate information from Wikipedia.
In contrast,we used only information intrinsic to our data.
En-hancing our models with Wikipedia informationwould be an interesting future direction to pursue.In the unsupervised line of research, keyphraseextraction is formulated as a ranking problem,where keyphrases are ranked using their tf (Barkerand Cornacchia, 2000), tf-idf (Zhang et al., 2007;Lee and Kim, 2008; Liu et al., 2009; Tonella et al.,2003), and term informativeness (Wu and Giles,2013; Rennie and Jaakkola, 2005; Kireyev, 2009)(among others).
The ranking based on tf-idf has1436been shown to work well in practice (Liu et al.,2009; Hasan and Ng, 2010) despite its simplicity.Frantzi et al.
(1998) combined linguistics and sta-tistical information to extract technical terms fromdocuments in digital libraries.
Graph-based al-gorithms and centrality measures are also widelyused in unsupervised models.
A word graph isbuilt for each document such that nodes corre-spond to words and edges correspond to word as-sociation patterns.
Nodes are then ranked usinggraph centrality measures such as PageRank andits variants (Mihalcea and Tarau, 2004; Wan andXiao, 2008; Liu et al., 2010; Zhao et al., 2011),HITS scores (Litvak and Last, 2008), as well asnode degree and betweenness (Boudin, 2013; Xie,2005).
Wan and Xiao (2008) were the first toconsider modeling a local neighborhood of a tar-get document in addition to the document itself,and applied this approach to news articles on theWeb.
Their local neighborhood consists of textu-ally similar documents, and did not capture infor-mation contained in document networks.Using terms from citation contexts of scientificpapers is not a new idea.
It was used before invarious applications.
For example, Ritchie et al.
(2006) used a combination of terms from citationcontexts and existing index terms of a paper toimprove indexing of cited papers.
Citation con-texts were also used to improve the performance ofcitation recommendation systems (Kataria et al.,2010; He et al., 2010) and to study author influ-ence (Kataria et al., 2011).
This idea of usingterms from citation contexts resembles the anal-ysis of hyperlinks and the graph structure of theWeb, which are instrumental in Web search (Man-ning et al., 2008).
Many current Web search en-gines build on the intuition that the anchor textpointing to a page is a good descriptor of its con-tent, and thus use anchor text terms as additionalindex terms for a target webpage.
The use of linksand anchor text was thoroughly researched for IRtasks (Koolen and Kamps, 2010), broadening auser?s search (Chakrabarti et al., 1998), query re-finement (Kraft and Zien, 2004), and enrichingdocument representations (Metzler et al., 2009).Moreover, citation contexts were used for scien-tific paper summarization (Abu-Jbara and Radev,2011; Qazvinian et al., 2010; Qazvinian andRadev, 2008; Mei and Zhai, 2008; Lehnert et al.,1990; Nakov et al., 2004).
Among these, proba-bly the most similar to our work is the work byQazvinian et al.
(2010), where a set of importantkeyphrases is extracted first from the citation con-texts in which the paper to be summarized is citedby other papers and then the ?best?
subset of sen-tences that contain such keyphrases is returned asthe summary.
However, keyphrases in (Qazvinianet al., 2010) are extracted using frequent n-gramsin a language model framework, whereas in ourwork, we propose a supervised approach to a dif-ferent task: keyphrase extraction.
Mei and Zhai(2008) used information from citation contexts todetermine what sentences of a paper are of highimpact (as measured by the influence of a targetpaper on further studies of similar or related top-ics).
These sentences constitute the impact-basedsummary of the paper.Despite the use of citation contexts and anchortext in many IR and NLP tasks, to our knowl-edge, we are the first to propose the incorporationof information available in citation networks forkeyphrase extraction.
In our recent work (Gol-lapalli and Caragea, 2014), we designed a fullyunsupervised graph-based algorithm that incorpo-rates evidence from multiple sources (citation con-texts as well as document content) in a flexiblemanner to score keywords.
In the current work,we present a supervised approach to keyphrase ex-traction from research papers that are embedded inlarge citation networks, and propose novel featuresthat show improvement over strong supervised andunsupervised baselines.
To our knowledge, fea-tures extracted from citation contexts have notbeen used before for keyphrase extraction in a su-pervised learning framework.3 Problem CharacterizationIn citation networks, in addition to the informa-tion contained in a paper itself, citing and citedpapers capture different aspects (e.g., topicality,domain of study, algorithms used) about the tar-get paper (Teufel et al., 2006), with citation con-texts playing an instrumental role.
A citation con-text is defined as a window of n words surround-ing a citation mention.
We conjecture that cita-tion contexts, which act as brief summaries about acited paper, provide additional clues in extractingkeyphrases for a target paper.
These clues give riseto the unique design of our model, called citation-enhanced keyphrase extraction (CeKE).3.1 Citation-enhanced Keyphrase ExtractionOur proposed citation-enhanced keyphrase extrac-tion (CeKE) model is a supervised binary classifi-1437Feature Name DescriptionExisting features for keyphrase extractiontf-idf term frequency * inverse documentfrequency, computed from a targetpaper; used in KEArelativePos the position of the first occurrence of aphrase divided by the total number oftokens; used in KEA and Hulth?s methodsPOS the part-of-speech tag of the phrase;used in Hulth?s methodsNovel features - Citation Network BasedinCited if the phrase occurs in cited contextsinCiting if the phrase occurs in citing contextscitation tf-idf the tf-idf value of the phrase, computedfrom the aggregated citation contextsNovel features - Extensions of Existing Featuresfirst position the distance of the first occurrence ofa phrase from the beginning of a papertf-idf-Over tf-idf larger than a threshold ?firstPosUnder the distance of the first occurrence of aphrase from the beginning of a paper isbelow some value ?Table 1: The list of features used in our model.cation model, built on a combination of novel fea-tures that capture information from citation con-texts and existing features from previous works.The features are described in ?3.1.1.
CeKE classi-fies candidate phrases as keyphrases (i.e., positive)or non-keyphrases (i.e., negative) using Na?
?veBayes classifiers.
Positive examples for train-ing correspond to manually annotated keyphrasesfrom the training research papers, whereas nega-tive examples correspond to the remaining candi-date phrases from these papers.
The generation ofcandidate phrases is explained in ?3.2.Note that Na?
?ve Bayes classifies a phrase as akeyphrase if the probability of the phrase belong-ing to the positive class is greater than 0.5.
How-ever, the default threshold of 0.5 can be varied toallow only high-confidence (e.g., 0.9 confidence)phrases to be classified as keyphrases.3.1.1 FeaturesWe consider the following features in our model,which are shown in Table 1.
They are dividedinto three categories: (1) Existing features forkeyphrase extraction include: tf-idf, i.e., the termfrequency - inverse document frequency of a can-didate phrase, computed for each target paper;This feature was used in KEA (Frank et al., 1999);relative position, i.e., the position of the first oc-currence of a phrase normalized by the length (inthe number of tokens) of the target paper; POS,i.e., a phrase?s part-of-speech tag.
If a phrase iscomposed by more than one term, then the POSwill contain the tags of all terms.
The relative posi-tion was used in both KEA and Hulth (2003), andPOS was used in Hulth; (2) Novel features - Cita-tion Network Based include: inCited and inCiting,i.e., boolean features that are true if the candidatephrase occurs in cited and citing contexts, respec-tively.
We differentiate between cited and citingcontexts for a paper: let d be a target paper and Cbe a citation network such that d ?
C. A cited con-text for d is a context in which d is cited by somepaper diin C. A citing context for d is a contextin which d is citing some paper djin C. If a paperis cited in multiple contexts by another paper, thecontexts are aggregated into a single one; citationtf-idf, i.e., the tf-idf score of each phrase computedfrom the citation contexts; (3) Novel features - Ex-tend Other Existing Features include: first positionof a candidate phrase, i.e., the distance of the firstoccurrence of a phrase from the beginning of a pa-per; this is similar to relative position except thatit does not consider the length of a paper; tf-idf-Over, i.e., a boolean feature, which is true if thetf-idf of a candidate phrase is greater than a thresh-old ?, and firstPosUnder, also a boolean feature,which is true if the distance of the first occurrenceof a phrase from the beginning of a target paper isbelow some value ?.
This feature is similar to thefeature is-in-title, used previously in the literature(Litvak and Last, 2008; Jiang et al., 2009).
Bothtf-idf and citation tf-idf features showed better re-sults when each tf was divided by the maximum tfvalues from the target paper or citation contexts.The tf-idf features have high values for phrasesthat are frequent in a paper or citation contexts,but are less frequent in collection and have lowvalues for phrases with high collection frequency.We computed the idf component from each col-lection used in experiments.
Phrases that occur incited and citing contexts as well as early in a paperare likely to be keyphrases since: (1) they capturesome aspect about the target paper and (2) authorsstart to describe their problem upfront.3.2 Generating Candidate PhrasesWe generate candidate phrases from the textualcontent of a target paper by applying parts-of-1438Dataset Num.
(#) Average Average Average #uni- #bi- #tri-Papers Cited Ctx.
Citing Ctx.
Keyphrases grams grams gramsWWW 425 15.45 18.78 4.87 680 1036 247KDD 365 12.69 19.74 4.03 363 853 189Table 2: A summary of our datasets.speech filters.
Consistent with previous works(Hulth, 2003; Mihalcea and Tarau, 2004; Liuet al., 2010; Wan and Xiao, 2008), only nounsand adjectives are retained to form candidatephrases.
The generation process consists of twosteps.
First, using the NLP Stanford part of speechtagger, we preprocess each document and keeponly the nouns and adjectives corresponding to{NN,NNS,NNP,NNPS, JJ}.
We apply thePorter stemmer on every word.
The position ofeach word is kept consistent with the initial stateof the document before any word removal is made.Second, words extracted in the first step thathave contiguous positions in a document are con-catenated into n-grams.
We used unigrams, bi-grams, and trigrams (n = 1, 2, 3) as candidatephrases for classification.
Similar to Wan and Xiao(2008), we eliminated phrases that end with an ad-jective and the unigrams that are adjectives.4 Experiments and ResultsIn this section, we first describe our datasets andthen present experimental design and results.4.1 DatasetsIn order to test the performance of our proposedapproach, we built our own datasets since citation-enhanced evaluation benchmarks are not availablefor keyphrase extraction tasks.
In particular, wecompiled two datasets consisting of research pa-pers from two top-tier machine learning confer-ences: World Wide Web (WWW) and KnowledgeDiscovery and Data Mining (KDD).
Our choicefor WWW and KDD was motivated by the avail-ability of author-input keywords for each paper,which we used as gold-standard for evaluation.Using the CiteSeerxdigital library1, we re-trieved the papers published in WWW and KDD(available in CiteSeerx), and their citation networkinformation, i.e., their cited and citing contexts.Since our goal is to study the impact of citationnetwork information on extracting keyphrases, apaper was considered for analysis if it had at least1http://citeseerx.ist.psu.edu/one cited and one citing context.
For each paper,we used: the title and abstract (referred to as thetarget paper) and its citation contexts.
The rea-son for not considering the entire text of a paperis that scientific papers contain details, e.g., dis-cussion of results, experimental design, notation,that do not provide additional benefits for extract-ing keyphrases.
Hence, similar to (Hulth, 2003;Mihalcea and Tarau, 2004; Liu et al., 2009), wedid not use the entire text of a paper.
However, ex-tracting keyphrases from sections such as ?intro-duction?
or ?conclusion?
needs further attention.From the pdf of each paper, we extracted theauthor-input keyphrases.
An analysis of thesekeyphrases revealed that generally authors de-scribe their work using, almost half of the time,bigrams, followed by unigrams and only rarely us-ing trigrams (or higher n-grams).
A summary ofour datasets that contains the number of papers,the average number of cited and citing contextsper paper, the average number of keyphrases perpaper, and the number of unigrams, bigrams andtrigrams, in each collection, is shown in Table 2.Consistent with previous works (Frank et al.,1999; Hulth, 2003), the positive and negative ex-amples in our datasets correspond to candidatephrases that consist of up to three tokens.
Thepositive examples are candidate phrases that havea match in the author-input keyphrases, whereasnegative examples correspond to the remainingcandidate phrases.Context lengths.
In CiteSeerx, citation con-texts have about 50 words on each side of a citationmention.
A previous study by Ritchie et al.
(2008)shows that a fixed window length of about 100words around a citation mention is generally effec-tive for information retrieval tasks.
For this reason,we used the contexts provided by CiteSeerxdi-rectly.
However, in future, it would be interestingto incorporate in our models more sophisticatedapproaches to identifying the text that is relevantto a target citation (Abu-Jbara and Radev, 2012;Teufel, 1999) and study the influence of contextlengths on the quality of extracted keyphrase.1439WWW KDDMethod Precision Recall F1-score Precision Recall F1-scoreCitation - Enhanced (CeKE) 0.227 0.386 0.284 0.213 0.413 0.280Hulth - n-gram with tags 0.165 0.107 0.129 0.206 0.151 0.172KEA 0.210 0.146 0.168 0.178 0.124 0.145Table 3: The comparison of CeKE with supervised approaches on WWW and KDD collections.4.2 Experimental DesignOur experiments are designed around the follow-ing research questions:1.
How does the performance of citation-enhanced keyphrase extraction (CeKE) com-pare with the performance of existing super-vised models that use only information intrin-sic to the data and what are the most informa-tive features for classification?
We comparedCeKE?s performance with that of classifierstrained on KEA features only and Hulth?sfeatures only and present a ranking of fea-tures based on information gain.2.
How do supervised models that integrate ci-tation network information compare with re-cent unsupervised models?
Since recent un-supervised approaches are becoming compet-itive with supervised approaches (Hasan andNg, 2014), we also compared CeKE withunsupervised ranking of candidate phrasesby TF-IDF, TextRank (Mihalcea and Ta-rau, 2004) and ExpandRank (Wan and Xiao,2008).
For unsupervised, we considered top5 and top 10 ranked phrases when computing?@5?
and ?@10?
measures.3.
How well does our proposed model performin the absence of either cited or citing con-texts?
Since newly published scientific pa-pers are not cited by many other papers, e.g.,due to their recency, no cited contexts areavailable.
We studied the quality of predictedkeyphrases when either cited or citing con-texts are missing.
For this, we comparedthe performance of models trained using bothcited and citing contexts with that of modelsthat use either cited or citing contexts.Evaluation metrics.
To evaluate the perfor-mance of CeKE, we used the following metrics:precision, recall and F1-score for the positive classsince correct identification of keyphrases is ofmost interest.
These metrics were widely used inprevious works (Hulth, 2003; Mihalcea and Tarau,2004; Wan and Xiao, 2008; Hasan and Ng, 2010).The reported values are averaged in 10-fold cross-validation experiments, where folds were createdat document level and candidate phrases were ex-tracted from the documents in each fold to formthe training and test sets.
In all experiments, weused Na?
?ve Bayes and their Weka implementa-tion2.
However, any probabilistic classifier that re-turns a posterior probability of the class given anexample, can be used with our features.The ?
parameter was set to the (title and ab-stract) tf-idf averaged over the entire collection,while ?
was set to 20.
These values were esti-mated on a validation set sampled from training.4.3 Results and DiscussionThe impact of citation network information on thekeyphrase extraction task.
Table 3 shows the re-sults of the comparison of CeKE with two su-pervised approaches, KEA and Hulth?s approach.The features used in KEA are the tf-idf and therelative position of a candidate phrase, whereasthose used in Hulth?s approach are tf, cf (i.e., col-lection frequency), relative position and POS tags.CeKE is trained using all features from Table 1.Among the three methods for candidate phraseformation proposed in Hulth (2003), i.e., n-grams,NP-chunks, and POS Tag Patterns, our Hulth?s im-plementation is based on n-grams since this givesthe best results among all methods (see (Hulth,2003) for more details).
In addition, the n-gramsmethod is the most similar to our candidate phrasegeneration and that used in Frank et al.
(1999).As can be seen from Table 3, CeKE outperformsKEA and Hulth?s approach in terms of all perfor-mance measures on both WWW and KDD, witha substantial improvement in recall over both ap-proaches.
For example, on WWW, CeKE achievesa recall of 0.386 compared to 0.146 and 0.107 re-call achieved by KEA and Hulth?s, respectively.2http://www.cs.waikato.ac.nz/ml/weka/1440WWW KDDMethod Precision Recall F1-score Precision Recall F1-scoreCitation - Enhanced (CeKE) 0.227 0.386 0.284 0.213 0.413 0.280TF-IDF - Top 5 0.089 0.100 0.094 0.083 0.102 0.092TF-IDF - Top 10 0.075 0.169 0.104 0.080 0.203 0.115TextRank - Top 5 0.058 0.071 0.062 0.051 0.065 0.056TextRank - Top 10 0.062 0.133 0.081 0.053 0.127 0.072ExpandRank - 1 neigh.
- Top 5 0.088 0.109 0.095 0.077 0.103 0.086ExpandRank - 1 neigh.
- Top 10 0.078 0.165 0.101 0.071 0.177 0.098ExpandRank - 5 neigh.
- Top 5 0.093 0.113 0.100 0.080 0.108 0.090ExpandRank - 5 neigh.
- Top 10 0.080 0.172 0.104 0.068 0.172 0.095ExpandRank - 10 neigh.
- Top 5 0.094 0.113 0.100 0.077 0.103 0.086ExpandRank - 10 neigh.
- Top 10 0.076 0.162 0.099 0.065 0.164 0.091Table 5: The comparison of CeKE with unsupervised approaches on WWW and KDD collections.Rank Feature IG Score1 abstract tf-idf 0.02342 first position 0.01883 citation tf-idf 0.01774 relativePos 0.01545 firstPosUnder 0.01486 inCiting 0.01297 inCited 0.00988 POS 0.00859 tf-idf-Over 0.0078Table 4: Feature ranking by Info Gain on WWW.Although there are only small variations fromKEA to Hulth?s approach, KEA performs betteron WWW, but worse on KDD compared withHulth?s approach.
In contrast, CeKE shows con-sistent improvement over the two approaches onboth datasets, hence, effectively making use of theinformation available in the citation network.In order to understand the importance of ourfeatures, we ranked them based on InformationGain (IG), which determines how informative afeature is with respect to the class variable.
Table4 shows the features ranked in decreasing order oftheir IG scores for WWW.
As can be seen fromthe table, tf-idf and citation tf-idf are both highlyranked, first and third, respectively, illustratingthat they contain significant information in pre-dicting keyphrases.
The first position of a phraseis also of great impact.
This is consistent with thefact that almost half of the identified keywords andabout 20% of the annotated keyphrases appear intitle.
Similar ranking is obtained on KDD.The comparison of CeKE with unsupervisedstate-of-the-art models.
Table 5 shows the re-sults of the comparison of CeKE with three unsu-pervised ranking approaches: TF-IDF (Tonella etal., 2003), TextRank (Mihalcea and Tarau, 2004),and ExpandRank (Wan and Xiao, 2008).
TF-IDFand TextRank use information only from the targetpaper, whereas ExpandRank uses a small textualneighborhood in addition to the target paper.
Notethat, for all unsupervised methods, we used Porterstemmer and the same candidate phrase generationas in CeKE, as explained in ?3.2.For TF-IDF, we first tokenized the target paperand computed the score for each word, and thenformed phrases and summed up the score of everyword within a phrase.
For TextRank, we built anundirected graph for each paper, where the nodescorrespond to words in the target paper and edgesare drawn between two words that occur next toeach other in the text, i.e., the window size is 2.For ExpandRank, we built an undirected graphfor each paper and its local textual neighborhood.Again, nodes correspond to words in the target pa-per and its textually similar papers and edges aredrawn between two words that occur within a win-dow of 10 words from each other in the text, i.e.,the window size is 10.
We performed experimentswith 1, 5, and 10 textually-similar neighbors.
ForTextRank and ExpandRank, we summed up thescores of words within a phrase as in TF-IDF.1441WWW KDDMethod Precision Recall F1-score Precision Recall F1-scoreCeKE - Both contexts 0.227 0.386 0.284 0.213 0.413 0.280CeKE - Only cited contexts 0.222 0.286 0.247 0.192 0.300 0.233CeKE - Only citing contexts 0.203 0.342 0.253 0.195 0.351 0.250Table 6: Results of CeKE using both contexts and using with only cited or citing contexts.For each unsupervised method, we computedresults for top 5 and top 10 ranked phrases.
Ascan be seen from Table 5, CeKE substantially out-performs all the other methods for our domain ofstudy, i.e., papers from WWW and KDD, illustrat-ing again that the citation network of a paper con-tains important information that can show remark-able benefits for keyphrase extraction.
Among allunsupervised methods, ExpandRank with fewertextual similar neighbor (one or five) performs thebest.
This is generally consistent with the resultsshown in (Wan and Xiao, 2008) for news articles.The effect of cited and citing contexts informa-tion on models?
performance.
Table 6 shows theprecision, recall and F-score values for some vari-ations of our method when: (1) all the citation con-texts for a paper are used, (2) only cited contextsare used, (3) only citing contexts are used.
Themotivation behind this experiment was to deter-mine how well the proposed model would performon newly published research papers that have notaccumulated citations yet.
As shown in the table,there is no substantial difference in terms of preci-sion between CeKE models that use only cited oronly citing contexts, although the recall is substan-tially higher for the case when only citing contextsare used, for both WWW and KDD.
The CeKEthat uses both citing and cited contexts achievesa substantially higher recall and only a slightlyhigher precision compared with the cases whenonly one context type is available.
The fact thatthe citing context information provides a slight im-provement in performance over cited contexts isconsistent with the intuition that when citing a pa-per y, an author generally summarizes the mainideas from y using important words from a targetpaper x, making the citing contexts to have higheroverlap with words from x.
In turn, a paper z thatcites x may use paraphrasing to summarize ideasfrom x with words more similar to those from z.Note that the results of all above experimentsare statistically significant at p-values ?
0.05, us-ing a paired t-test on F1-scores.4.4 Anecdotal EvidenceIn order to check the transferability of our pro-posed approach to other research fields, e.g., nat-ural language processing, it would be interestingto use our trained classifiers on WWW and KDDcollections and evaluate them on new collectionssuch as NLP related collections.
Since NLP col-lections annotated with keyphrases are not avail-able, we show anecdotal evidence for only one pa-per.
We selected for this task an award winning pa-per published in the EMNLP conference.
The pa-per?s title is ?Unsupervised semantic parsing?
andhas won the Best Paper Award in the year 2009(Poon and Domingos, 2009).
In order for our al-gorithm to work, we gathered from the Web (usingGoogle Scholar) all the cited and citing contextsthat were available (49 cited contexts and 30 cit-ing contexts).
We manually annotated the targetpaper with keyphrases.
The title, abstract and allthe contexts were POS tagged using the NLP Stan-ford tool.
We then trained a classifier on the fea-tures shown in Table 1, on both WWW and KDDdatasets combined.
The trained classifier was usedto make predictions, which were compared againstthe manually annotated keyphrases.
The resultsare shown in Figure 2, which displays the title andabstract of the paper and the predicted keyphrases.Candidate phrases that are predicted as keyphrasesare marked in red bold, those predicted as non-keyphrases are shown in black, while the filteredout words are shown in light gray.We tuned our classifier trained on WWW andKDD to return as keyphrases only those that hadan extremely high probability to be keyphrases.Specifically, we used a threshold of 0.985.
Theprobability of each returned keyphrase (which isabove 0.985) is shown in the upper right cornerof a keyphrase.
Human annotated keyphrases aremarked in italic, under the figure.
There is a clearmatch between the predictions and the human an-notations.
It is also possible to extract more orless keyphrases simply by adjusting the threshold1442Unsupervised Semantic Parsing0.997We present the first unsupervised approach to the problem of learning a semantic parser1.000, usingMarkov logic0.991.
Our USP system0.985transforms dependency trees into quasi-logical forms, recur-sively induces lambda forms from these, and clusters them to abstract away syntactic variations of thesame meaning.
The MAP semantic parse1.000of a sentence is obtained by recursively assigning itsparts to lambda-form clusters and composing them.
We evaluate our approach by using it to extract aknowledge base from biomedical abstracts and answer questions.
USP1.000substantially outperformsTextRunner, DIRT and an informed baseline on both precision and recall on this task.Human annotated labels: unsupervised semantic parsing, Markov logic, USP systemFigure 2: The title and abstract of an EMNLP paper by Poon and Domingos (2009) and human annotatedkeyphrases for the paper.
Black words represent candidate phrases.
Red bold words represent predictedkeyphrases.
The numbers above predicted keyphrases are probabilities for the positive class assignment.on the probability output by Na?
?ve Bayes.
For ex-ample, if we decrease the threshold to 0.920 thefollowing phrases would be added to the returnedset of keyphrases: dependency trees, quasi-logicalforms and unsupervised approach.Another interesting aspect is the frequency ofoccurrence of the predicted keyphrases in the citedand citing contexts.
Table 7 shows the term-frequency of every predicted keyphrase within thecitation network.
For example, the phrase seman-tic parser appears in 29 cited contexts and 26 cit-ing contexts.
The reason for the higher cited con-text frequency is not necessarily due to impor-tance, but could be due to the larger number ofcited vs. citing contexts for this paper (49 vs. 30).The high rate of keyphrases within the citation net-work validates our assumption of the importanceof citation networks for keyphrase extraction.Finally, we performed the same experimentwith Hulth?s and KEA methods.
While the clas-sifier trained on Hulth?s features did not identifyany keyphrases, KEA managed to identify severalgood ones (e.g., USP, semantic parser), but leftout some important ones (e.g., Markov logic, un-supervised).
Moreover, the keyphrases predictedby KEA have a lower confidence.
For this reason,lowering the probability threshold would result inselecting other bad keyphrases.4.5 Error analysisWe performed an error analysis and found thatcandidate phrases are predicted as keyphrases(FPs), although they do not appear in gold stan-dard, i.e., the set of author-input keyphrases, incases when: 1) a more general terms is used todescribe an important concept of a document, e.g.,Keyphrase #cited c. #citing c.semantic parser 29 26USP 31 10Markov logic 15 10unsupervised semantic parsing 12 1USP system 3 2Table 7: Frequency of the predicted keyphrases incited / citing contexts.co-authorship prediction represented as link pre-diction or Twitter platform represented as socialmedia; 2) an important concept is omitted (eitherintentionally or forgetfully) from the set of author-input keyphrases.Hence, while we believe that authors are thebest keyphrase annotators for their own work,there are cases when important keyphrases areoverlooked or expressed in different ways, possi-bly due to the human subjective nature in choosingimportant keyphrases that describe a document.To this end, a limitation of our model is the use ofa single gold standard keyphrase annotation.
In fu-ture, we plan to acquire several human keyphraseannotation sets for our datasets and test the perfor-mance of the proposed approach on these annota-tion sets, independently and in combination.Keyphrases that appear in gold standard arepredicted as non-keyphrases (FNs) when: 1) akeyphrase is infrequent in abstract; 2) its distancefrom the beginning of a document is large; 3) doesnot occur or occurs only rarely in a document?scitation contexts, either citing or cited contexts.Examples of FNs are model/algorithm/approachnames, e.g., random walks, that appear in sen-tences such as: ?In this paper, we model the prob-lem [?
?
?]
by using random walks.?
Although such1443a sentence may appear further away from the be-ginning of an abstract, it contains significant in-formation from the point of view of keyphraseextraction.
The design of patters such as <by using $model > or < uses $model > couldlead to improved classification performance.Further investigation of FPs and FNs will beconsidered in future work.
We believe that a bet-ter understanding of errors has the potential to ad-vance state-of-the-art for keyphrase extraction.5 Conclusion and Future DirectionsIn this paper, we presented a supervised classifi-cation model for keyphrase extraction from scien-tific research papers that are embedded in citationnetworks.
More precisely, we designed novel fea-tures that take into account citation network in-formation for building supervised models for theclassification of candidate phrases as keyphrasesor non-keyphrases.
The results of our experi-ments show that the proposed supervised modeltrained on a combination of citation-based featuresand existing features for keyphrase extraction per-forms substantially better compared with state-of-the-art supervised and unsupervised models.Although we illustrated the benefits of leverag-ing inter-linked document networks for keyphraseextraction from scientific documents, the proposedmodel can be extended to other types of docu-ments such as webpages, emails, and weblogs.
Forexample, the anchor text on hyperlinks in weblogscan be seen as the ?citation context?.Another aspect of future work would be theuse of external sources to better identify candi-date phrases.
For example, the use of Wikipediawas studied before to check if the concept behinda phrase has its own Wikipedia page (Medelyanet al., 2009).
Furthermore, since citations occurin all sciences, extensions of the proposed modelto other domains, e.g., Biology and Chemistry,and other applications, e.g., document summariza-tion, similar to Mihalcea and Tarau (2004) andQazvinian et al.
(2010), are of particular interest.AcknowledgmentsWe are grateful to Dr. C. Lee Giles for theCiteSeerX data, which allowed the generation ofcitation graphs.
We also thank Kishore Nep-palli and Juan Fern?andez-Ram?
?zer for their helpwith various dataset construction tasks.
We verymuch appreciate the constructive feedback fromour anonymous reviewers.
This research wassupported in part by NSF awards #1353418 and#1423337 to Cornelia Caragea.
Any opinions,findings, and conclusions expressed here are thoseof the authors and do not necessarily reflect theviews of NSF.ReferencesAmjad Abu-Jbara and Dragomir Radev.
2011.
Co-herent citation-based summarization of scientific pa-pers.
In Proc.
of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, HLT ?11, pages 500?509.Amjad Abu-Jbara and Dragomir Radev.
2012.
Ref-erence scope identification in citing sentences.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL HLT ?12, pages 80?90.Ken Barker and Nadia Cornacchia.
2000.
Using NounPhrase Heads to Extract Document Keyphrases.
InProceedings of the 13th Biennial Conference of theCanadian Society on Computational Studies of Intel-ligence: Advances in Artificial Intelligence, AI ?00,pages 40?52, London, UK, UK.
Springer-Verlag.Florian Boudin.
2013.
A comparison of centralitymeasures for graph-based keyphrase extraction.
InProc.
of IJCNLP, pages 834?838, Nagoya, Japan.Soumen Chakrabarti, Byron Dom, Prabhakar Ragha-van, Sridhar Rajagopalan, David Gibson, and JonKleinberg.
1998.
Automatic resource compilationby analyzing hyperlink structure and associated text.Comput.
Netw.
ISDN Syst., 30(1-7):65?74, April.Chen Cheng, Haiqin Yang, Michael R. Lyu, and IrwinKing.
2013.
Where you like to go next: Succes-sive point-of-interest recommendation.
In Proc.
ofIJCAI?13, pages 2605?2611, Beijing, China.Eibe Frank, Gordon W. Paynter, Ian H. Witten, CarlGutwin, and Craig G. Nevill-Manning.
1999.Domain-specific keyphrase extraction.
In Proceed-ings of the 16th International Joint Conference onArtificial Intelligence - Volume 2, IJCAI?99, pages668?673, Stockholm, Sweden.Katerina T. Frantzi, Sophia Ananiadou, and Jun-ichiTsujii.
1998.
The c-value/nc-value method of au-tomatic recognition for multi-word terms.
In Proc.of ECDL ?98, pages 585?604.Sujatha Das Gollapalli and Cornelia Caragea.
2014.Extracting keyphrases from research papers usingcitation networks.
In Proceedings of the 28thAAAI Conference on Artificial Intelligence (AAAI-14), Qu?ebec City, Qu?ebec, Canada.Khaled M. Hammouda, Diego N. Matute, and Mo-hamed S. Kamel.
2005.
Corephrase: Keyphrase ex-traction for document clustering.
In Proc.
of the 4th1444International Conference on Machine Learning andData Mining in Pattern Recognition, MLDM?05,pages 265?274, Leipzig, Germany.Kazi Saidul Hasan and Vincent Ng.
2010.
Conun-drums in Unsupervised Keyphrase Extraction: Mak-ing Sense of the State-of-the-Art.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics, pages 365?373.Kazi Saidul Hasan and Vincent Ng.
2014.
Automatickeyphrase extraction: A survey of the state of the art.In Proc.
of the 52nd Annual Meeting of the Associa-tion for Computational Linguistics (ACL).Qi He, Jian Pei, Daniel Kifer, Prasenjit Mitra, and LeeGiles.
2010.
Context-aware citation recommenda-tion.
In Proc.
of WWW ?10, pages 421?430, Raleigh,North Carolina, USA.Yifan Hu, Yehuda Koren, and Chris Volinsky.2008.
Collaborative filtering for implicit feedbackdatasets.
In Proc.
of the 8th IEEE Intl.
Conferenceon Data Mining, ICDM ?08, pages 263?272.Anette Hulth.
2003.
Improved Automatic KeywordExtraction Given More Linguistic Knowledge.
InProceedings of the 2003 conference on Empiricalmethods in natural language processing, EMNLP?03, pages 216?223.Xin Jiang, Yunhua Hu, and Hang Li.
2009.
A RankingApproach to Keyphrase Extraction.
In Proceedingsof the 32nd international ACM SIGIR conference onResearch and development in information retrieval,pages 756?757.
ACM.Steve Jones and Mark S. Staveley.
1999.
Phrasier:A system for interactive document retrieval usingkeyphrases.
In Proceedings of SIGIR ?99, pages160?167, Berkeley, California, USA.Saurabh Kataria, Prasenjit Mitra, and Sumit Bhatia.2010.
Utilizing context in generative bayesian mod-els for linked corpus.
In In Proc.
of AAAI ?10, pages1340?1345, Atlanta, Georgia, USA.Saurabh Kataria, Prasenjit Mitra, Cornelia Caragea,and C. Lee Giles.
2011.
Context sensitive topicmodels for author influence in document networks.In Proceedings of IJCAI?11, pages 2274?2280,Barcelona, Catalonia, Spain.Su Nam Kim, Olena Medelyan, Min-Yen Kan, andTimothy Baldwin.
2010.
SemEval-2010 Task 5:Automatic Keyphrase Extraction from Scientific Ar-ticles.
In Proceedings of the 5th International Work-shop on Semantic Evaluation, SemEval ?10, pages21?26, Los Angeles, California.Su Nam Kim, Olena Medelyan, Min-Yen Kan, andTimothy Baldwin.
2013.
Automatic keyphraseextraction from scientific articles.
Language Re-sources and Evaluation, Springer, 47(3):723?742.Kirill Kireyev.
2009.
Semantic-based estimation ofterm informativeness.
In Proc.
of NAACL ?09, pages530?538, Boulder, Colorado.Marijn Koolen and Jaap Kamps.
2010.
The impor-tance of anchor text for ad hoc search revisited.
InProceedings of SIGIR ?10, pages 122?129, Geneva,Switzerland.Reiner Kraft and Jason Zien.
2004.
Mining anchor textfor query refinement.
In Proceedings of the 13th In-ternational Conference on World Wide Web, WWW?04, pages 666?674, New York, NY, USA.
ACM.Sungjick Lee and Han-joon Kim.
2008.
News Key-word Extraction for Topic Tracking.
In Proceedingsof the 2008 Fourth International Conference on Net-worked Computing and Advanced Information Man-agement - Volume 02, NCM ?08, pages 554?559,Washington, DC, USA.
IEEE Computer Society.Wendy Lehnert, Claire Cardie, and Ellen Rilofl.
1990.Analyzing research papers using citation sentences.In Proceedings of the 12th Annual Conference of theCognitive Science Society, pages 511?518.Marina Litvak and Mark Last.
2008.
Graph-Based Keyword Extraction for Single-DocumentSummarization.
In Proceedings of the Workshopon Multi-source Multilingual Information Extrac-tion and Summarization, MMIES ?08, pages 17?24,Manchester, United Kingdom.Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.2009.
Unsupervised Approaches for AutomaticKeyword Extraction Using Meeting Transcripts.
InProceedings of NAACL ?09, pages 620?628, Boul-der, Colorado.Zhiyuan Liu, Wenyi Huang, Yabin Zheng, andMaosong Sun.
2010.
Automatic Keyphrase Ex-traction via Topic Decomposition.
In Proceedingsof EMNLP ?10, pages 366?376, Cambridge, Mas-sachusetts.Christopher D. Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press, New York,NY, USA.Lu?
?s Marujo, Ricardo Ribeiro, David Martinsde Matos, Jo?ao Paulo Neto, Anatole Gershman, andJaime G. Carbonell.
2013.
Key phrase extraction oflightly filtered broadcast news.
CoRR.Olena Medelyan, Eibe Frank, and Ian H. Witten.2009.
Human-competitive tagging using automatickeyphrase extraction.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing: Volume 3 - Volume 3, EMNLP?09, pages 1318?1327, Singapore.Qiaozhu Mei and ChengXiang Zhai.
2008.
Generat-ing impact-based summaries for scientific literature.In Proceedings of ACL-08: HLT, pages 816?824,Columbus, Ohio.1445Donald Metzler, Jasmine Novak, Hang Cui, and SrihariReddy.
2009.
Building enriched document repre-sentations using aggregated anchor text.
In Proc.
ofSIGIR ?09, pages 219?226, Boston, MA, USA.Rada Mihalcea and Paul Tarau.
2004.
TextRank:Bringing Order into Texts.
In Proceedings ofEMNLP 2004, pages 404?411, Barcelona, Spain.Preslav I. Nakov, Ariel S. Schwartz, and Marti A.Hearst.
2004.
Citances: Citation sentences for se-mantic analysis of bioscience text.
In SIGIR Work-shop on Search and Discovery in Bioinformatics.Thuy Dung Nguyen and Min-Yen Kan. 2007.Keyphrase Extraction in Scientific Publications.
InProc.
of the Intl.
Conf.
on Asian digital libraries,ICADL?07, pages 317?326, Hanoi, Vietnam.Rong Pan and Martin Scholz.
2009.
Mind the gaps:Weighting the unknown in large-scale one-class col-laborative filtering.
In Proceedings of KDD ?09,pages 667?676, Paris, France.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proc.
of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?09, pages 1?10, Singapore.Nirmala Pudota, Antonina Dattolo, Andrea Baruzzo,Felice Ferrara, and Carlo Tasso.
2010.
Auto-matic keyphrase extraction and ontology mining forcontent-based tag recommendation.
InternationalJournal of Intelligent Systems.Vahed Qazvinian and Dragomir R. Radev.
2008.
Sci-entific paper summarization using citation summarynetworks.
In Proc.
of the 22nd Intl.
Conferenceon Computational Linguistics, COLING ?08, pages689?696, Manchester, United Kingdom.Vahed Qazvinian, Dragomir R. Radev, and Arzucan?Ozg?ur.
2010.
Citation summarization throughkeyphrase extraction.
In Proceedings of the 23rd In-ternational Conference on Computational Linguis-tics, COLING ?10, pages 895?903.Steffen Rendle, Christoph Freudenthaler, and LarsSchmidt-Thieme.
2010.
Factorizing personalizedmarkov chains for next-basket recommendation.
InWWW ?10, pages 811?820, Raleigh, North Carolina.Jason D. M. Rennie and Tommi Jaakkola.
2005.
UsingTerm Informativeness for Named Entity Detection.In Proc.
of SIGIR ?05, pages 353?360.Anna Ritchie, Simone Teufel, and Stephen Robertson.2006.
How to find better index terms through cita-tions.
In Proc.
of the Workshop on How Can Compu-tational Linguistics Improve Information Retrieval?,CLIIR ?06, pages 25?32, Sydney, Australia.Anna Ritchie, Stephen Robertson, and Simone Teufel.2008.
Comparing citation contexts for informationretrieval.
In Proc.
of CIKM ?08, pages 213?222,Napa Valley, California, USA.Guy Shani, David Heckerman, and Ronen I. Brafman.2005.
An mdp-based recommender system.
J.Mach.
Learn.
Res., 6:1265?1295, December.Xiaolin Shi, Jure Leskovec, and Daniel A. McFarland.2010.
Citing for high impact.
In Proceedings of the10th Annual Joint Conference on Digital Libraries,JCDL ?10, pages 49?58, Gold Coast, Queensland,Australia.S.
Teufel, A. Siddharthan, and D. Tidhar.
2006.
Auto-matic classification of citation function.
In Proceed-ings of EMNLP-06.S.
Teufel.
1999.
Argumentative Zoning: InformationExtraction from Scientific Text.
Ph.D. thesis, Uni-versity of Edinburgh.Paolo Tonella, Filippo Ricca, Emanuele Pianta, andChristian Girardi.
2003.
Using Keyword Extrac-tion for Web Site Clustering.
In Web Site Evolution,2003.
Theme: Architecture.
Proceedings.
Fifth IEEEInternational Workshop on, pages 41?48.Peter D. Turney.
2000.
Learning algorithms forkeyphrase extraction.
Inf.
Retr., 2.Peter D. Turney.
2003.
Coherent Keyphrase Extractionvia Web Mining.
In Proceedings of the 18th inter-national joint conference on Artificial intelligence,IJCAI?03, pages 434?439, Acapulco, Mexico.Xiaojun Wan and Jianguo Xiao.
2008.
Single Doc-ument Keyphrase Extraction Using NeighborhoodKnowledge.
In Proceedings of AAAI ?08, pages855?860, Chicago, Illinois.Zhaohui Wu and Lee C. Giles.
2013.
Measuringterm informativeness in context.
In Proceedings ofNAACL ?13, pages 259?269, Atlanta, Georgia.Zhuli Xie.
2005.
Centrality Measures in Text Mining:Prediction of Noun Phrases that Appear in Abstracts.In Proceedings of the ACL Student Research Work-shop, pages 103?108, Ann Arbor, Michigan.Hongyuan Zha.
2002.
Generic summarization andkeyphrase extraction using mutual reinforcementprinciple and sentence clustering.
In SIGIR.Yongzheng Zhang, Evangelos Milios, and Nur Zincir-Heywood.
2007.
A Comparative Study on KeyPhrase Extraction Methods in Automatic Web SiteSummarization.
Journal of Digital InformationManagement, 5(5):323.Wayne Xin Zhao, Jing Jiang, Jing He, Yang Song,Palakorn Achananuparp, Ee-Peng Lim, and Xiaom-ing Li.
2011.
Topical Keyphrase Extraction fromTwitter.
In Proceedings of HLT ?11, pages 379?388,Portland, Oregon.Andrew Zimdars, David Maxwell Chickering, andChristopher Meek.
2001.
Using temporal data formaking recommendations.
In Proceedings of the17th Conference in Uncertainty in Artificial Intelli-gence, UAI ?01, pages 580?588.1446
