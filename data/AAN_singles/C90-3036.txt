A PDP ARCHITECTURE FOR PROCESSING SENTENCES WITHRELAT IVE  CLAUSES *Ris to  Mi ikku la inenArt i f ic ia l  Inte l l igence Laboratory~ Computer  Science Dcpar tmentUn ivers i ty  of  Cal i forn ia,  Los Angeles,  CA 90024r i s to@es.uc la .eduAbst rac tA modular parallel distributed processing architec-ture for parsing, representing and paraphrasing sen-tences with multiple hierarchical relative clauses ispresented.
A lowel-level network reads the segmentsof the sentence word by word into partially specifiedcase-role representations of the acts.
A higher-levelnetwork combines these representations into a list ofcomplete act representations.
This internal represen-tation stores the information conveyed by the sen-tence independent of its linguistic form.
The infor-mation can be output in natural anguage in differentform or style, e,g.
as a sequence of simple sentences oras a complex sentence consisting of relative clauses.Generating output is independent from parsing, andwhat actually gets generated epends on the trainingof the generator modules.1 In t roduct ionParsing a sentence means reading the input text intoan internal representation, which makes the relationsof the constituents explicit.
In symbolic parsing, theresult is usually a semantic network structure, e.g.a conceptual dependency representation \[17; 3\], ora syntactic parse tree augmented with semantic re-strictions \[13; 9\].
The advantage of this approachis that sentences with arbitrary complexity can beparsed and represented.
IIowever, processing knowl-edge must be hand-coded with specific examples inmind.
Rules for expectations, defaults and general-izations must be explicitly programmed.The localist connectionist models \[2; 20; 5; 1; 19; 7\]provide more general mechanisms for inferencing andgive a more plausible account of the parsing processin terms of human performance, ltowever, these net-works need to be carefiflly crafted for each example.The main advantage of the distributed connec-tionist approach \[8; 18; 12\] is that processing islearned from examples.
Expectations about unspec-ified constituents arise automatically from the pro-cessing mechanism, and generalizations into new in-puts result automatically from the representations.Any statistical regularity in the training examplesis automatically utilized in making inferences.
Theresult in distributed parsing at the sentence level is*This research was supported in part by a grant fromthe ITA Foundation, and in part by grants from theAcademy of Finland, the Emil Aaltonen Foundation, thel;bundation Ibr the Advancement ofTechnology, and theAlfred Kordelin Foundation (Finland).
The simulationswere carried out on the Cray X-MP/48 at the San DiegoSupereomputer Center.e.g.
an assembly-based case-role representation f thesentence \[8; 10\].
The output layer of the network isdivided into partitions, each representing a case role,and distributed activity patterns in the assemblies in-dicate the words filling these roles.Representing complex structures is problematic inthe distributed approach \[6; 15\].
The proposed sen-tcncc processing architectures can only deal with sim-ple, straightforward sentences.
Case-role analysis isfeasible only when the sentences consist of singleacts, so that unique case role can be assigned foreach constituent.
The approach can be extended androles reserved for attributes of the constituents also.However, sentences with relative clauses remain in-tractable.A hierachical PDP architecture for parsing, repre-senting and paraphrasing sentences with multiple hi-erarchical relative clauses is described in this paper.Each relative clause is itself an act, and has its owncase-role representation.
The whole sentence is rep-resented as a collection of these acts.
The relationsof the acts are implicit in their content, rather thanexplicit in the structure of the representation.
Theoriginal eoraplex hierarchical sentence, as well as asimplified paraphrase of it, can be produced from thelist of the act representations.2 Sys tem arch i tec ture2.1 OverviewThe system consists of four hierarchically organizedsubnetworks (figure 1).
The act parser reads the in-put words one at a time, and forms a stationary case-role representation for each act fragment (defined aspart of sentence separated by commas).
The sen-tence parser eads these case-role representations oneat a time, and forms a stationary representation ofthe whole sentence as ~ts output.
This is the internalrepresentation f the sentence.The sentence generator takes the internal represen-tation as its input, and produces a sequence of case-role representations of the act fragments as its out-put.
These are fed one at a time to the act generator,which generates the sequence of words for each actfragment.
During performance, the four nctworks areconnected in a chain, the output of one network feed-ing the input of another (figure 1).
During training,each network is trained separately with compatibleI/O data (figure 6).The input/output of each network is composed ofdistributed representations of words.
These repre-sentations are stored in a central lexicon (figure 2),1 201Input weCase-reof the,rds (text)r. rep.l ie reps acts.Output words (text)\[~t~iiii\[l:!~!
:':"~i!ii!~i~!~!ii~!~i~i~iii~Jiii|" Words pCase-rio topsof th I actsComplete  sentence  re..~iiiii~i~i~i!i!i 1Figure 1: Overview of the model.
The model consistsof parsing and generating subsystems, and a central lexi-con of distributed word representations.
Each subsystemconsists of two hierarchically organized modules, with thecase-role assignment of the act as an intermediate r pre-sentation.liiNow representations|:~:,:::;.. T:;:;:~ :T:,:,:;:;: ..~:..~:~:~: .
::\] Input layer: , ~='~" .......... ~ Modify representationsH ldden~Uence  memory.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Output layer: l~:; : i" . '
: '~:m Form error signalTeaching patternFigure 3: Recurrent FGREP-module.
At the end ofeach backpropagation cycle, the current input represen-tations are modified at the input layer according to theerror signal.
The new representations are loaded back tothe lexicon, replacing the old ones.man = li:::i:il$1: i : i : :~  | i : i : i ; \ ]~ : \ ]~ , ,woman = ~ : :  .
:::h i t  = ,.,',,'~; ?
~,~- ':i:b lamed = \[i~ii !
11 i!i~l I~!i!i~{\[i!i~li~i!ii~ )~!
!
:'1~1the  - - !~:~: \ ]  \[:i:i~i I~ i~: i i !
l i : i : !~ letc .Figure 2: Lexicon.
The lexicon is an associative mem-ory, associating the text form of each word with its dis-tributed representation.
The representation is a vector ofreal numbers between 0 and 1, shown as grey-scale valuesfrom white to black.and all networks use the same representations.
Eachnetwork is a Recurrent FGREP module, i.e.
a three~layer backpropagation network with sequential inputor output, which develops the word representationsautomatically while it is learning the processing task.2.2 Recur rent  FGREP - A bui ld ing blockThe FGREP mechanism (Forming Global Represen-tations with Extended backPropagation) \[10; 12\] isbased on a basic three-layer backward error propa-gation network (figure 3).
The network learns theprocessing task by adapting the connection weightsaccording to the standard backpropagation equations\[16, pages 327-329\].
At the same time, representa-tions for the input data are developed at the inputlayer according to the error signal extended to the in-put layer.
Input and output layers are divided into as-semblies and several items are represented and mod-ified simultaneously.The representations are stored in an external lex-icon network.
A routing network forms each inputpattern and the corresponding teaching pattern byconcatenating the lexicon entries of the input andteaching items.
Thus the same representation foreach item is used in different parts of the backpropa-gation network, both in the input and in the output.The process begins with a random lexicon contain-ing no pro-encoded information.
During the course oflearning, the representations adapt o the regularitiesof the task.
It turns out that single units in the result-ing representation do not necessarily have a clear in-terpretation.
The representation does not implementa classification of the item along identifiable features.In the most general case, the representations are sim-ply profiles of continuous activity values over a setof processing units.
This representation pattern as awhole is meaningful and can bc claimed to code themeaning of that word.
The representations for wordswhich are used in similar ways become similar.Recurrent FGREP \[11; 12\] is an extension of thebasic FGREP architecture to sequentia\[ input andoutput, based on \[4\].
A copy of the hidden layer attime step t is saved and used along with the actual in-put at step t+ l  as input to the hidden layer (figure 3).The previous hidden layer serves as a sequence mem-ory, essentially remembering where in the sequencethe system currently is and what has occurred before.During learning, the weights from the previous hid-den layer to the hidden layer proper are modified asusual according to the backpropagation mechanism.The Recurrent FGREP module can be used forreading a sequence of input items into a stationaryoutput representation, or for generating an outputsequence from a stationary input.
In a sequential in-put network, the actual input changes at each timestep, while the teaching pattern stays the same.
Thenetwork is forming a stationary representation f thesequence.
In a sequential output network, the actualinput is stationary, but the teaching pattern changesat each step.
The network is producing a sequentialinterpretation of its input.
The error is backpropa-gated and weights are changed at each step.
Bothtypes of Recurrent FGREP networks dew;lop repre-sentations in their input layers.2.3 Connecting the building blocks in theper fo rmance  phaseLet us present the system with the followingsentence: The woman, who helped the g i r l ,  whothe boy hit, blamed the man.The task of the act parser network (figure 4) isto form a stationary case-role representation for eachpart of the sentence, for complete acts (who helpedthe g i r l  and who the boy h i t )  as well as for actfragments (the woman and blamed the man).
Thereis an assembly of units at the output layer of this net-202  2IN: Sequence of input word representationscTPARSEROI33": Case-roDe representation of the act fragmentS ~N: Sequence of carla-rata rspresenta!i.ons of the act fragmentsE \[Agent \] Act I Patient T ~ N blara~d manp trr~qllllIIT\[lA llllllllllmllmllll\]llllll|Hlllllll~llllllllllIllllllHgll~R ~.~7--s Ill llilfllilllJlllll|l\] ~II~ilI'II IIII H lJlHlm H I ~n L~22J .
A,=tt IPe,ontt I Agen, R I A0,~ 1"'.
"'"'4 hgen,31 ~A ~3 IP.,ient3JOUT: Stationary list of complete case-role representations of the ac~sFigure 4: Networks parsing the sentence.Snapshot of the simulation after the whole sen-tence The uoman, who helped Che girl, who Che boyh i t ,  blamed the man has been read in.
The output ofthe act parser shows the case-role representation of thelast act fragment, blamed the man.
The output of thesentence parser displays the result of the parse, the inter-nal ;eprescntation of the whole sentence.OUr: Sequence of output word representationscTGENERIN: Case-role representation of the act fragmentOUT: Soquonc~ at' cass-role representations of the act fragmentsENT~ ' i l  liitiltt~t Hitillitlltl't'tl II'h flt \]tt u tit I it u t ~ w,'mt Il i l l l l l l l l l l l  /.
l l l l l l l l l tt l l l lEI l i l l f lnl l l l l l l l l l l l l l l~J lt l l l l l l l l l l l l~ Gill\]!~lt:,!lllllllli ia~-BlolHttl~lill\[Hilllnllttt~'~lllllllll\[li\[illliiN\[/_Agentl_.h_gctl I_Patientl~ Agent2_\[ Act2 i PatlentL:~ Agent3,~, l, Act3 ~Patient3., ~IN: Stationary fist of complete case-r0te representations of the actsF igure  5: Networks  generat ing  the  sentence.
Thesystem is in the beginning of generating The woman, whohelped the  girl, who the boy hi%, blamed %he man.The sentence generator has produced the case-role rep-resentation of the first act fragment, The woman, and theact generator has output the first word of that fragment.The previous hidden layers are blank during the first step.work for each case role.
Each assembly is to be filledwith the distributed activity pattern of the word thatfills that role.
For example, the correct representationfor who the boy h i t  is agent=boy, ac t=h i t  and p~tientwho.As each word is read, its distributed representa-tion is obtained from the lexicon; and loaded into theinput layer of the act parser network.
The activitypropagates through the network, and a distributedpattern forms at the output layer, indicating expecta-tions about possible act representations at that point.The activity pattern at the hidden layer is copiedto the previous-hidden-layer assembly, and the nextword is loaded into the input layer.
Each successiveword narrows down the possible interpretations, andthe case-role representation f a specific act graduallyforms at the output.After reading the and woman, the network knowsto generate the pattern for woman in the agent assem-bly, because in our training examples, the first nounis always the agent.
The pattern in the act assemblyis an average of helped and blamed, the two possibleacts for woman in our data.
The pattern in the pa-tient assembly is an average of all patients for helpedand blamed.
Reading a verb next would establish theappropriate representation i the act assembly, andnarrow down the possibilities in the patient assembly.However, the network reads a comma next, whichmeans that the top-level act is interrupted by therelative clause (commas eparate clause fragments inour data; a way to segment clauses without commasis outlined in section 4).
The network is trained toclear the expectations in the unspecified assemblies,i.e.
to form an incomplete case-role interpretation ofthe top-level act so far.
This representation is passedon as the first input to the sentence parser module.The act parser then goes on to parse the rel-ative clause who helped the girl independentlyfrom what it read before, i.e.
the pattern in itsprevious-hidden-layer assembly is cleared before read-ing who.
The complete case-role representation of therelative clause is passed on to the sentence parseras its second input.
Similarly, who the boy h i t  isparsed and its representation passed on to tile sen-tence parser.
The act parser then receives the restof the top-level act, blamed the man, wlfich is againparsed independently, and its incomplete case-rolerepresentation (figure 4) passed on to the sentenceparser.The sentence parser reads the sequence of thesefour case-role representations, combines the incom-plete case-role representations into a complete rep-resentation of the top-level act, and determines timreferents of the who pronouns.
The result is a listof three completely specified case-role representa-tions, lwoman blamed man I, \[ woman helped girl Iand J boy hit girl\] (bottom of figure 4).The list of case-role representations is the final re-sult of the parse, the internal representation of thesentence.
It is a canonical representation with all thestructural information coded into simple acts.
All in-formation is accessible in parallel, and can be directlyused for further processing.The output side of the system (figure 5) demonstrates how the inibrmation in the internM represen--ration can be output in different ways in natural Inn-3 203guage.
The output process is basically the reverseof the reading process.
The sentence generator net-work takes the internal representation as its inputand produces the case-role representation f the firstact fragment, l woman (blank) (blank) I as its out-put (figure 5).
This is fed to the act generator, whichgenerates the distributed representation of the, thefirst word of the act fragment.
The representation inthe lexicon closest o the output pattern is obtained,and the text form of that entry is put into the streamof output text.The hidden layer pattern of the word generator iscopied into its previous-hidden-layer assembly, andthe next word is output.
The commas egment heoutput as well.
As soon as a comma is output, thesentence generator network is allowed to generate thecase-role representation f the next act fragment.The sentence generator can produce different out-put versions from the same internal representation,depending on its training.
(1) The acts can be out-put sequentially one at a time as separate simple sen-tences, or (2) a single output sentence with a complexrelative clause structure can be generated.
The pointis that it does not matter how the internal represen-tation is arrived at, i.e.
whether it was read in as asingle sentence, as several sentences, or maybe pro-duced as a result of a reasoning process.
Generatingoutput sentences i independent from parsing, and theform and style of the output depends on the processingknowledge of the sentence generator.In case (1) the sentence generator producesthe case-role representations I woman blamed man I,Iwomaxt helped girll and \[boy hit girll, andthe act generator generates The woman blamed theman, The woman helped the girl, The boy hit theg i r l .
In case (2) the sentence generator produces thesequence l woman (blank) (blank)l, I who helpedgirl I, \[ boy hit who \[, \[ (blank) blamed man \[,and the output text reads The woman, who helpedthe girl, who the boy hit, blamed the man.2.4 Training phaseA good advantage of the modular architecture canbe made in training the networks.
The tasks of thefour networks are separable, and they can be trainedseparately as long as compatible I /O material is used.The networks must be trained simultaneously, so thatthey are always using and developing the same rep-resentations (figure 6).The lexicon ties the separated tasks together.
Eachnetwork modifies the representations to improve itsperformance in its own task.
The pressure from othernetworks modifies the representations also, and theyevolve slightly differently than would be the most ef-ficient for each network independently.
The networkscompensate by adapting their weights, so that in theend the representations and the weights of all net-works are in harmony.
The requirements of the differ-ent tasks are combined, and the final representationsreflect the total use of the words.If the training is successful, the output patternsproduced by one network are exactly what the nextsERSENTPARSERFigure 6: Training configuration.ACTGENERSENTGENEREach networkis truined separately and simultaneously, developing thesame lexicon.network learned to process as its input.
But evenif the learning is less than complete, the networksperform well together.
Erroneous output patternsare noisy input to the next network, and neural net-works in general tolerate, even filter out noise veryefficiently.3 Exper iments3.1 Training dataThe system was trained with sentences generated us-ing the 17 templates shown in table 1.
The acts con-sisted of three case-roles: agent, the act (i.e.
theverb), and patient.
A relative clause could be at-tached to the agent or to the patient, and these couldfill the role of the agent or the patient in the relativeclause.Certain semantic restrictions were imposed on thetemplates to obtain more meaningful sentences.
Therestrictions also create enough differences in the us-age of the words, so that their representations do notbecome identical (see \[12\]).
A verb could have onlyspecified nouns as its agent and patient, listed in ta-ble 2.
Sentences with two instances of the same nounwere also excluded.
With these restrictions, the tem-plates generate a total of 388 sentences.
All sentenceswere used to train the system.
Generalization wasnot studied in these experiments (for a discussion ofthe generalization capabilities of FGREP systems ee\[121).Two different versions of the sentence generatorwere trained: one to produce the output as a sequenceof simple sentences, and another to produce a singlesentence with hierarchical relative clauses~ i.e.
to re-produce the input sentence.
The act generator wastrained only with the act fragments from the complexsentences.
Because these contain the simple acts, theact generator network effectively learned to processthe ouput of the first version of the sentence genera-tor as well.204 4" I ' i~~i f f6 -  sentence--~FTq'TTh-~wb'WaK Wl~m~---ff-gh~ man2.
24 The woman3.
20 The woman~.
24 The woman5.
20 The woman6.
28 The woman7.
24 The woman8.
24 The woman9.
28 The womanlO.
20 The ~oman11.
24 The ~oman12, 2~ The woman13.
24 The woman14.
28 The womani\[~.
20 The womml16.
24 The woman17.
20 The womanblamed the man, who hit the girlblamed the man, who hit the girl, who blamed the boyblamed the man, who hit the girl, .ho the boy hi~blamed the mml, who the girl blamedblamed the man, who the girl, who blamed the boy, blamedblamed the man, who the girl, who the boy hit, blamedwho helped the boy, blamed the man, who helped the girlwho helped the boy, blamed ~he man, who the girl blamedwho the boy hit, blamed the man.
who helped ~he girlwho the boy hit, blamed the man, who ~he girl blamedwho helped the girl, blamed the manwho helped the girl, who blamed the boy, blamed the manwho helped the girl, who the boy hit, blamed the manwho the boy hit, blamed the manwho the boy, who hit the giml, hit, blamed %he manwho the boy, who the girl blamed, hit, blamed the man _\ [he lped Agent: mini,womanI Patient: bo y, g i r lIh i t  Agent: man,boyPatient: ~oman, g i r lb lamed Agent: woman,g i r lI Patient: man,boytTable 1: Sentence templates.
Table 2: Restrict ions.There are 3 different verbs, with 2 possible agents and patients each (table 2).
These words are used to generate sentenceswith the 17 different sentence templates (table 1).
The same noun cannot occur in two places in the same sentence.
Anexample sentence for each template is given, together with the number of different sentences the template generates.Act.
parserSentence parserSentence gener(simple)--~ Act generatorSentence gener(clauses)--~ Act generator100 \] 100 .02793\]  86 .083100 I 96 .047100\] 98 .03998 I 87 .071_ .06__A0Table 3: Performance.
The first column indicates thepercentage of correct words out of all output words.
Thesecond column indicates the percentage of output unitswhich were within 0.15 of the correct value, and the lastcolumn shows the average rror per output unit.The four networks were trained separately and si-multaneously with compatible I /O data.
This meansthat the output patterns, which are more or less in-correct during training, were not directly fed into thenext network.
They were replaced by the correct pat~terns, obtained by concatenating the current wordrepresentations in the lexicon.
The word representa-tions consisted of 12 units, the hidden layers of theact networks of 25 units, and the hidden layers ofthe sentence networks of 75 traits.
The system wastrained for the first 100 epochs with 0.1 learning rate,then 25 epochs with 0.05 and another 25 epochs with0.025.
The training process took about one hour ona Cray X-MP/48.3?2 ResuRsThe performance of the system was tested with thesame set of sentences as used in the training.
Table 3show the performance figures for each network.
Inthe output text, the system gets approximately 97%of the words (and punctuation) correct.Even when the networks arc connected in a chain(output of one network feeding the input of the next),the errors do not cumulate in the chain.
The noisein the input is efficiently filtered out, and each net-work performs approximately at the same level.
Thefigures for the sentence parser are somewhat lowerbecause it generates expectations for the second aimthird acts.
For some one and two act sentences thesepatterns remain active after the whole sentence hasbeen read in.
For example, after reading The womanblamed the man the network generates an expecta-tion for a relative clause attached to man.
The actgenerator network learns not to output the expecta-tions, but they are counted as errors in the perfor-mance figures for the sentence generator.4 D iscuss ionIt is interesting to speculate how the model wouldmap onto human sentence processing.
The act parsernetwork models the lowest level of processing.
Aseach act fragment is read in, a surface semantic in-terpretation of it is immediately formed in terms ofcase roles.
Each act fragment is parsed independentlyfrom others.
A higher-level process (the sentenceparser) keeps track of the recursive relations of tileact fragments and combines them into complete rep-resentations.
It also ties the different acts together bydetermining the referents of the relative pronouns.The acts are stored in the memory as separate facts,without explicit high-level structure.
The structure isrepresented in the facts themself, e.g.
two acts havethe same agent, the agent of one act is the patient ofanother etc.
Sentences with relative clauses can beproduced from this unstructured internal representa-tion.In other words, the recursive structure is a prop-erty of the language, not the information itself.
Inter-nally, the information can be represented in a parallel,canonical form, which makes all information directlyaccessible.
In communication through narrow chan-nels, i.e.
in language, it is necessary to transformthe knowledge into a sequential form \[12\].
Paralleldependencies in the knowledge are then coded withrecursion.Generating output is seen as a task separate fromparsing.
Sentence generation is performed by a differ-ent module and learned separately.
The same modulecan learn to paraphrase the stone internal represen-tation in different ways, e.g.
as a single sentence con--sisting of relative clauses, or as a sequence of severalsimple sentences.
What actually gets generated e--pends on the connection weights of this module.5 205It would be possible to add a higher-level decision-making network to the system, which controls theconnection weight values in the sentence generatornetwork through multiplicative connections \[14\].
Adecision about the style, detail etc.
of the paraphrasewould be made by this module, and its output wouldassign the appropriate function to the sentence gen-erator.The model exhibits certain features of hu-man performance.
As recursion gets deeper,the sentence networks have to keep more in-formation in their sequence memories, and theperformance degrades.
Moreover, tail recursion(e.g.
The woman blamed the man, who hit thegirl, ~ho blamed the boy) is easier than relativeclauses in the middle of the sentence (e.g.
Thewoman, who the boy, who the girl blamed, hit,blamed the man), because the latter case involvesmore steps in the sequence, taxing the memory ca-pacity more.
Note that in symbolic modeling, thedepth or the type of the recursion makes absolutelyno difference.The scale-up prospects of the architecture seemfairly good.
The simple data used in the experimentsdid not come close to exhausting the processing powerof the system.
Larger vocabulary, more case rolesand sentences consisting with more acts could wellbe processed.
It seems possible to represent a widerange of acts by their case-role assignments.
Com-plex attributes, such as PPs, can be represented asadditional relative clauses (e.g.
The man with thehat... --+ The man, who has the hat...).Currently, the system depends on commas to sep-arate the clause fragments.
This is not a very seri-ous limitation, as segmenting could be based othermarkers uch as the relative pronouns.
A more fun-damental limitation, characteristic to PDP systemsin general, is that the system needs to be trainedwith a good statistical sample of the input/outputspace.
It does not have an abstract representationof the clause structure, and it cannot generalize intosentence structures it has not seen before.As sentences become more complex, a mechanismfor maintaining unique identities for the words isneeded.
For example, in representing The man, whohelped the boy, blamed the man, who hit the girlit is crucial to indicate that the man-who-helped isthe same as the man-who-blamed, but different fromthe man-who-hit.
A possible technique for doing thishas been proposed in \[12\].
The representation f theword could consist of two parts: the content part,which is developed by FGREP and codes the pro-cessing properties of the word, and an ID part, whichis unique for each separate instance of the word.
TheID approximates sensory grounding of the word, andallows us to tag the different instances and keep themseparate.5 Conc lus ionDividing the task of parsing and generating sentenceswith complex clause structure into hierarchical sub-tasks makes the task tractable with distributed neu-ral networks.
The scale-up prospects of the approachinto larger vocabulary and more complex sentencesseem fairly good.
The main drawback is that the sys-tem does not develop an abstract representation ofrecursive structures, but must be exposed to exam-ples of all possibilities.
The content of the sentencescan be represented internally in canonical form as acollection of simple acts, without explicit structure.The knowledge for generating different linguistic ex-pressions of the same content resides in the generatingmodules.References\[1\] E. Charniak.
A neat theory of marker passing.
InProceedings ofAAA\[-86,, Kaufmann, 1986.\[2\] G. W. Cottrell and S. L. Small.
A connection-ist scheme for modelling word sense disambiguation.Cognition and Brain Theory, 6(1):89-120, 1983.\[3\] M. G. Dyer.
In-Depth Understanding.
MIT Press,1983.\[4\] J. L. Elman.
Finding Structure in Time.
'Feehni-cal Report 8801, Center for Research in Language,UCSD, 1988.\[5\] R. Granger, K. Eiselt, and J. Holbrook.
Parsing withparallelism.
In Kolodner and Riesbeck, eds, Experi-ence, Memory and Reasoning, LEA, 1986.\[6\] G. E. Hinton.
Representing part-whole hierarchies inconnectionist networks.
In Proceedings ofCogSei.88.LEA, 1988.\[7\] T. E. Lunge and M. G. Dyer.
High-level inferene-ing in a connectionist network.
Connection Science,1(2), 1989.\[8\] J. L. McClelland and A. H. Kawamoto.
Mechanismsof sentence processing.
In McClelland and Rumel-hart, eds, Parallel Distributed Processing, MIT Press,1986.\[9\] M. C. McCord: Using slots and ruodifiers in logicgrammars for natural language.
Artificial httelli-gence, 18:327-367, 1982.\[10\] R. Miikkulalnen and M. G. Dyer.
Encoding in-put/output representations in connectionist cogni-tive systems.
In Touretzky, Hinton, ~ Sejnowski,eds, Proceedings of the 1988 Connectionist ModelsSummer School, Kaufmann, 1989.\[11\] R. Miikkulainen and M. G. Dyer.
A modular neu-ral network architecture for sequential paraphrasingof script-based stories.
In P,vceedings of IJCNN.89,1989.\[12\] R. Miikkulainen and M. G. Dyer.
Natural LanguageProcessing with Modular Neural Networks and Dis-tributed Lexicon.
Technical Report UCLA-AL90-02,Computer Science Department, UCLA, 1990.\[13\] F. C. N. Pereira and D. H. Warren.
Definite clausegrammars for language analysis.
Artificial h~telli-gence, 13:231-278, 1980.\[14\] d. Pollack.
Cascaded back-propagation dynamiceonnectionist networks.
In Proceedings ofCogSci-87,LEA, 1987.\[15\] J. Pollack.
Recursive auto-associative memory.
InProceedings ofCogSci.88, LEA, 1988.\[16\] D. E. Rumelhart, G. E. Hinton, and R. J. Williams.Learning internal representations by error propaga-tion.
In Rumelhart and McClelland, eds, ParallelDistributed Processing, MIT Press, 1986.\[17\] R. Sehank and R. Abelson.
Scripts, Plans, Goals,and Understanding.
LEA, 1977.\[18\] M. F. St. John and J. L. McClelland.
Learning andapplying contextual constraints in sentence compre-hension.
In Proceedings of CogSci-88, LEA, 1988.\[19\] R. A. Sumida, M. G. Dyer, and M. Flowers.
Integrat-ing market passing and eonnectionism for handlingconceptual and structural ambiguities.
In Proceed-ings el CogSei-88, LEA, 1988.\[20\] D. L. Waltz and J.
B. Pollack.
Massively parallelparsing.
Cognitive Science, 9:51-74, 1985.206 6
