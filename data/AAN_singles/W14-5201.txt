Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 1?11,Dublin, Ireland, August 23rd 2014.A broad-coverage collection of portable NLP componentsfor building shareable analysis pipelinesRichard Eckart de Castilho1Iryna Gurevych1,2(1) Ubiquitous Knowledge Processing Lab (UKP-TUDA)Dept.
of Computer Science, Technische Universit?at Darmstadt(2) Ubiquitous Knowledge Processing Lab (UKP-DIPF)German Institute for Educational Research and Educational Informationhttp://www.ukp.tu-darmstadt.deAbstractDue to the diversity of natural language processing (NLP) tools and resources, combining theminto processing pipelines is an important issue, and sharing these pipelines with others remainsa problem.
We present DKPro Core, a broad-coverage component collection integrating a widerange of third-party NLP tools and making them interoperable.
Contrary to other recent endeav-ors that rely heavily on web services, our collection consists only of portable components dis-tributed via a repository, making it particularly interesting with respect to sharing pipelines withother researchers, embedding NLP pipelines in applications, and the use on high-performancecomputing clusters.
Our collection is augmented by a novel concept for automatically selectingand acquiring resources required by the components at runtime from a repository.
Based on thesecontributions, we demonstrate a way to describe a pipeline such that all required software andresources can be automatically obtained, making it easy to share it with others, e.g.
in order toreproduce results or as examples in teaching, documentation, or publications.1 IntroductionSharing is a central concept to scientific work and to software development.
In science, information aboutexperimental setups and results is shared with fellow researchers, not only to disseminate new insights,but also to allow others to validate results or to improve on them.
In software development, librariesand component-based architectures are a central mechanism to promote the reuse of software.
Portablesoftware must operate in the same way across system platforms.
In the context of scientific research, thisis an important factor related to the reproducibility of results created from software-based experiments.The NLP software landscape provides a wealth of reusable software in the form of NLP tools ad-dressing language analysis at different levels from tokenization to sentiment analysis.
These tools arecombined into NLP pipelines that form essential parts of experiments in natural language research andbeyond, e.g.
in the emerging digital humanities.
Therefore, it is essential that such pipelines can easilybe shared between researchers, to reproduce results, to evolve experiments, and to allow for a betterunderstanding of the exact details of an experiment (cf.
Fokkens et al.
(2013)).Analyzing the current state of the art, we find that despite considerable effort that has been going intoprocessing frameworks enabling interoperability, workbenches to build and run pipelines, and all kindsof online services, it is still not possible to create a readily shareable description of an NLP pipeline.
Apipeline description is basically a configuration file referencing the components and resources used bythe pipeline.
Currently, these references are ambiguous, e.g.
because they do not incorporate versioninformation.
This causes a reproducibility problem, e.g.
when a pipeline is part of an experiment,because the use of a different version can easily lead to different results.
A sharable description must beself-contained in the sense that it uniquely identifies all involved components and resources, permittingthe execution environment for the pipeline to be set up reproducibly, in the best case automatically.Currently, the task of setting up the environment is largely left to the user and requires time and diligence.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1In this paper, we present a novel concept for self-contained NLP pipeline descriptions supported bya broad-coverage collection of interoperable NLP components.
Our approach is enabled by the com-bination of distributing portable NLP components and resources through a repository and by an auto-configuration mechanism allowing components to select suitable resources at runtime and to obtain themautomatically from the repository.
Our contributions facilitate the sharing of pipelines, e.g.
as part ofpublications or examples in documentation, and allow users to maintain control by providing the abilityto create backups of components and resources for a later reproduction of results.Section 2 reflects on the state of the art and identifies the need for a broad-coverage component col-lection of portable components, as well as the need for self-contained pipeline descriptions.
Section 3.1describes a novel concept for the automatic selection and acquisition of resources.
Section 3.2 presentsa broad-coverage collection of portable components integrating this concept.
Section 3.3 demonstrates ashareable workflow based on these contributions.
Section 4 gives further examples of how our contribu-tions could be applied.
Finally, Section 5 summarizes the paper and suggests future research directions.2 State of the artIn this section, we examine the current state of the art related to describing NLP pipelines, kinds ofcomponent collections, publishing of components and resources, and the selection of resources to usewith a component.
We start by defining the terminology used throughout the rest of this paper.Definition of terminology We make a distinction between a tool and a component.
Most NLP tools arestandalone tools addressing one particular task, e.g.
dependency parsing, relying on separate tokenizers,part-of-speech (POS) taggers, etc.
These tools cannot be easily combined into pipelines, because theirinput/output formats are often not compatible and because they lack a uniform programming interface.We speak of a component when a tool has been integrated into a processing framework, usually by im-plementing an adapter between the tool and the framework.
The framework defines a uniform program-ming interface, data model, and processing model enabling interoperability between the components.Through integration with the framework, components become easily composeable into pipelines.
Apipeline consists of components processing input documents one after the other and passing the outputon to the next component.
Each component adds annotations to the document, e.g.
sentence and tokenboundaries, POS tags, syntactic constituents, or dependency relations, etc.
These steps build upon eachother, e.g.
a component for dependency parsing requires at least sentences, tokens, and POS tags.Many components are generic engines that require some resource (e.g.
a probabilistic model or knowl-edge base) that configures them for a specific language, tagset, domain, etc.
We use the term resourceselection for the task of choosing a resource and configuring a component to use it.
The task of obtainingthe resource we call resource acquisition.As Thompson et al.
(2011) point out, achieving a consensus on the exact representation of differentlinguistic theories as annotations and thereby attaining full conceptual interoperability between com-ponents from different vendors is currently not considered feasible.
Thus, frameworks leave the typesystem (kinds of annotations) unspecified.
Therefore, the technical integration with a framework alonedoes not make the tools interoperable on the conceptual level (cf.
Chiarcos et al.
(2008)).
As a conse-quence, multiple component collections exist, each providing interoperable components centered arounda particular combination of processing framework and type system (e.g.
Buyko and Hahn (2008), Kanoet al.
(2011), Wu et al.
(2013)).
Each of these defines its own concepts of tokens, sentence, syntacticstructures, discourse structures, etc.
Yet, even these type systems leave certain aspects underspecified,e.g.
the various tagsets used to categorize parts-of-speech, syntactic constituents, etc.2.1 Sharing pipelinesProcessing frameworks offer a way to construct pipeline descriptions that instruct the framework toconfigure and execute a pipeline of NLP components.GATE (Cunningham et al., 2002) and Apache UIMA (Ferrucci and Lally, 2004) are currently the mostprominent processing frameworks.
Both describe pipelines by means of XML documents.
These refer toindividual components by name and expect that the user has taken precautions that these components are2accessible by the framework.
Neither framework includes provisions to automatically obtain the compo-nents or resources they require, e.g.
from a repository (Section 2.2).
In fact, the pipeline descriptions donot contain sufficient information to uniquely identify components.
Components are referred to only byname, but not by version.
The same is true for resources which are often referred to only by filename.Thus, both of the major processing frameworks to not offer self-contained descriptions for pipelines.When such pipeline descriptions are shared with another person, the recipient requires additional infor-mation about which exact versions of tools and resources are needed to run the pipeline.2.2 Publishing components and resourcesIn this section, we examine different approaches to publishing NLP components and resources so thatthey can be more easily found, accessed, or obtained in order to execute a particular pipeline.Directories META-SHARE (Thompson et al., 2011) and the CLARIN Virtual Language Observatory(VLO) (Uytvanck et al., 2010) are two directories of language resources, including NLP tools and re-sources.
These directories currently target primarily human users and offer rich metadata and a userinterface to browse it or to find specific kinds of entries.
However, these directories to not contain suffi-cient information to programmatically download the software or resources, or to access them as services.Repositories Repositories are online services from which components and resources can be obtained.The Central Repository (2014) is a repository within the Java-ecosystem used to distribute Java li-braries and resources they require, so-called artifacts.
It relies on concepts that have evolved aroundthe Maven project (Sonatype Company, 2008).
Meanwhile, these are supported by many build tools,development environments, and even by some programming languages (cf.
Section 3.3).
Several NLPtools (e.g.
ClearNLP (2014), Stanford CoreNLP (Manning et al., 2014), MaltParser (Nivre et al., 2007),ClearTK (Ogren et al., 2009)) are already distributed via this medium, some including their resources.There are many Maven repositories on the internet.
They are organized as a loosely federated network.The Central Repository merely serves as the default point of contact built into clients.
Repositories havethe ability to access each other and to cache those artifacts required by their immediate users.
Thisprovides resilience against network failures or remote data loss.
Artifacts can be addressed across thefederation by a set of coordinates (groupId, artifactId, and version).Another kind of repositories are plug-in repositories, such as those used by GATE (Cunningham et al.,2002).
From these, the user can conveniently download and install components within the GATE work-bench.
These plug-in repositories are specific to GATE, whereas the Maven repositories are a genericinfrastructure widely used by the Java community and that is supported by many tools and applications.Online Workbenches While many NLP tools are offered as portable software for offline use, weobserve a trend in recent years towards offering NLP tools as web-services for online use, sometimes asthe only way to access them.
Hinrichs et al.
(2010) cite incompatibilities between the software and theuser?s machine and insufficiently powerful workstations as reasons for this approach.
Another reasonmay be the ability to set up a walled garden in which the service provider is able to control the use ofservices, e.g.
to academic researchers or to paying commercial customers.Argo (Rak et al., 2013) is a web-based workbench.
It offers access to a collection of UIMA-basedNLP-services that can be executed in different environments.
Rak et al.
mention in particular a clusterenvironment but also plan support for a number of cloud platforms.
For this reason, we assume that mostof the components are integrated into Argo as portable software that can be deployed to these platformson-demand.
Yet, it appears that the components are only accessible through Argo and that they are notdistributed separately for use in other UIMA-based environments.U-Compare (Kano et al., 2011) is a Java application for building and running UIMA-based pipelinesand comparing their results.
While some components accessible through the workbench run locally,many components are only stubs calling out to web-services running at different remote locations.WebLicht (Hinrichs et al., 2010) is a distributed infrastructure of NLP services hosted at different lo-cations.
They exchange data in an XML format called TCF (Text Corpus Format).
Pipelines can be built3and run using the web-based WebLicht workbench.
Within this walled garden platform, authenticatedacademic users have access to resources that are free for academic research, but not otherwise.Online Marketplaces AnnoMarket (Tablan et al., 2013) is another distributed infrastructure of NLPservices based on GATE.
It does not seem to offer a workbench to compose custom pipelines.
Instead,it offers a set of pre-configured components and exposes them as web-services to be programmaticallyaccessed.
It is the only commercial offering in this overview that the user has to pay for.Note on service-based approaches Service-based approaches have also been taken in other scientificdomains to facilitate the creation of shareable and repeatable experiments, e.g.
on platforms such asmyExperiment (Goble et al., 2010).
However, G?omez-P?erez et al.
(2013) found service-based workflowsto be subject to decay as services are updated and change their input/output formats, their results, oras they become temporarily unavailable due to network problems.
We also expect they can becomepermanently unavailable, e.g.
due to a lack of funding unless supported by a sound business model.Furthermore, to our knowledge none of the offerings above allow the user to export their pipelines in-cluding all necessary software and resources, e.g.
to make a backup or to deploy it on a private computinginfrastructure, e.g.
a private cloud or cluster system.2.3 Component collectionsWe define a component collection as a set of interoperable components.
The interoperability between thecomponents is enabled by conventions that are typically rendered as a common annotation type system,a common API, or both.Standalone NLP tools Most NLP tools are not comprehensive suites that cover all tasks from tok-enization to e.g.
coreference resolution, but are rather standalone tools addressing only a particular task,e.g.
dependency parsing, relying on separate tokenizers, POS-taggers, etc.
Examples of such standalonetools are MaltParser (Nivre et al., 2007) and HunPos (Hal?acsy et al., 2007).
The major part of the analysislogic is implemented within the tool, such that they tend not to rely significantly on third-party libraries.However, many third-party resources can be found on the internet for popular standalone tools.NLP tool suites Some vendors offer tool suites that cover multiple analysis tasks, e.g.
ClearNLP,CoreNLP, and OpenNLP.
They consist of a set of interoperable tools.
Some even go so far as to in-clude a proprietary processing framework and pipeline mechanism.
For example, CoreNLP allows theuser to implement custom analysis components and to register them with their framework.
OpenNLP,on the other hand, provides UIMA-wrappers for their tools.
These wrappers are configurable for dif-ferent UIMA type systems, but unfortunately the configuration mechanism is not powerful enough toaccommodate for the design of various major type systems.We also refer to such tool suites as single vendor collections.
As for standalone tools, the major partof the analysis logic is a part of the suite and tends not to rely significantly on third-party libraries.
Also,again many third-parties offer resources for popular tool suites.Special purpose collections Special purpose collections combine NLP tools into a comprehensivemodular pipeline for a specific purpose.The Apache cTAKES project (Savova et al., 2010) offers a UIMA-based pipeline for the analysis ofmedical records which includes components from ClearNLP, OpenNLP, and more for the basic languageanalysis.
These third-party components are used in conjunction with resources created specifically forthe domain of medical records.
Higher-level tasks use original components from the project, e.g.
toidentify drugs or relations specific to the medical domain.Broad-coverage collections Broad-coverage collections cover multiple analysis tasks, but they do notfocus on a specific purpose.
Instead, they provide the user with a choice for each analysis task byintegrating tools from different vendors capable of doing the same task.
Because the languages supportedby each tool differ, this allows the collection to cover more languages than individual tools or even toolsuites alone.
Additionally, broad-coverage collections allow comparing different tools against each other.4The U-Compare workbench (Kano et al., 2011) focusses specifically on the ability to compare toolsagainst each other.
It offers a GUI for building analysis pipelines and comparing their results.
U-Comparealso offers a collection of UIMA-based components centered around the U-Compare type system.
Itstarted integrating analysis tools primarily from the biomedical domain, but many more tools were inte-grated as part of the META-NET project (Thompson et al., 2011).
This makes the collection accessiblethrough U-Compare one of the largest collections of interoperable NLP components available.ClearTK (Ogren et al., 2009) is actually a machine-learning framework based on Apache UIMA.However, it also integrates various NLP tools from different vendors and for this reason we list it underthe broad-coverage collections.
The tools are integrated to provide features for the machine-learningalgorithms.
The main reason for ClearTK not to use components from other existing UIMA componentcollections may have been the lack of a comprehensive UIMA component collection for NLP at the timeClearTK was in its early stages.Note on cross-collection interoperability An alternative to broad-coverage collections that integratemany tools and make them interoperable would be to achieve cross-collection interoperability.
Thatmeans, many vendors would provide small collections or even individual components and the end userswould combine them into pipelines as desired.
However, even within a framework like UIMA or GATE,a) some conventions, like a common type system, would need to be respected, b) extensive mappingbetween the individual components would be required, or c) the components would need to be adaptableto arbitrary type systems through configuration.
Until at least one of these points has been resolved ina user-friendly way, we consider broad-coverage collections to be the most convenient solution for theuser.
The insights gained in building the broad-coverage collections may eventually contribute to findingsolutions for these problems.2.4 Resource selection and acquisitionMany NLP tools are generic, language-independent engines that are parametrized for a particular lan-guage with a resource, e.g.
a probabilistic model, a set of rules, or another knowledge base.
We call thisresource selection.
The selection can happen manually or automatically.Manual selection is required, for example, in ClearTK or GATE.
Components that require a resourceoffer a parameter pointing to the location from where this resource can be loaded, typically a location onthe local file system.
This entails that the resource is either bundled with the component or that the usermust find and download the resource to the local machine.
We call this step resource acquisition.U-Compare (Kano et al., 2011), on the other hand, offers some components preconfigured with re-sources for certain languages.
In particular, components that call out to remote web-services tend tosupport multiple languages.
Based on the language they are invoked for, the service employs a particularresource.
However, in this case the users cannot invoke the service with a custom resource from theirlocal machine.
Portable components that are bundled with U-Compare also allow for custom resources.2.5 Need for shareable pipelines based on portable componentsCurrent workflow descriptions are inconvenient to share with others because they are not self-contained.They do not uniquely identify components and resources.
The responsibility to obtain, and install com-ponents and resources is largely left to the user.
Web-based workbenches and marketplaces provide someremedy in this aspect as they remove the need for any local installation by the user.
However, such onlineservice-based approaches have been found to be a cause of workflow decay (G?omez-P?erez et al., 2013).In consequence, we find that a shareable pipeline should rely on portable software and resources thatcan be automatically obtained from a repository.
Once obtained, these remain within the control of theuser, e.g.
to create backups, or to run them on alternative environments, such as a private compute cluster.In the latter case, the use of remote services would likely cause a performance bottleneck.
To make suchan approach to shareable pipelines attractive, it must be supported by a broad-coverage collection fromwhich pipelines can be assembled for various tasks.53 Contributions3.1 Automatic selection and acquisition of resourcesWe present a novel approach to the configuration of components with resources based on the data beingprocessed.
Resources are stored in a repository from where a component can obtain them on demand.The approach is based on a set of coordinates to address resources: tool, language, variant, and version.In many cases, this removes the need for the user to explicitly configure the resource to be used.By overriding specific coordinates (mainly variant), the user can choose between different resources.Additionally, the user can disable resource-resolution via coordinates and instruct the component to usea model at a specific location, e.g.
to use custom model from the local file system.As an example, consider a part-of-speech-tagger component being used to process English text:?
tool ?
this coordinate is uniquely identified by the component being used, e.g.
opennlp-tagger.?
language ?
this coordinate is obtained from the data being processed by the tagger, e.g.
en or de.?
variant ?
as there can be multiple applicable resources per language, this coordinate is used tochoose one of the resources.
A default variant is provided by the component, possibly a differentvariant depending on the language, e.g.
fast or accurate.?
version ?
resources are versioned, just as components are.
New versions of a resource are createdto fix bad data, to extend the data on which the resource is based, or to make it compatible with anew version of a tool.
We note that generally, the versioning of tools and resources is independentof each other: a resource may be compatible with multiple versions of a tool and multiple versionsof a resource may be compatible with one specific version of a tool.
Furthermore, some vendorsdo not version resources properly or at all.
For example, by comparing hash values, we observedthat from version to version only some of the models packaged with CoreNLP change, while othersremain identical.
We also found the models (and even binaries) of TreeTagger (Schmid, 1994) tochange from time to time without any apparent change in version.
As a consequence, we decidedto consequently use a time-based versioning scheme for resources.
The independence between tooland resource versions also has another effect: users find it hard to manually select a resource versioncompatible with the tool version they use.
Thus, we maintain a list of resources and default versionswith each component and use it to fill in the version coordinate.To operationalize this concept, we translate these coordinates into Maven coordinates and usethese to resolve the resource against the Maven repository infrastructure.
For example the coordi-nates [tool: opennlp-tagger, language: en, variant: maxent, version: 20120616.1] would be trans-lated into [groupId: de.tudarmstadt.ukp.dkpro.core, artifactId: de.tudarmstadt.ukp.dkpro.core.opennlp-model-tagger-en-maxent, version: 20120616.1].Mind that some vendors already distribute resources for their tools via Maven repositories (cf.
Sec-tion 2.2), but they do so at their own coordinates, e.g.
at [groupId: com.clearnlp, artifactId: clearnlp-general-en-dep, version: 1.2] and these resources can become of a significant size.1To avoid repub-lishing resources at coordinates matching our naming scheme, the artifact at the translated coordinatesserves only as a proxy that does not contain the resource itself.
Instead, it contains a redirection to theartifact containing the actual resource.
This allows us to maintain a common coordinate scheme for allresources while being able to incorporate existing third-party resources.
It also allows us to maintainadditional metadata, e.g.
about tagsets.
When vendors do not distribute their resources via Maven, wepackage them and distribute them via our own public repository ?
if their license does not prohibit this.3.2 The DKPro Core broad-coverage component collection or portable componentsWe presentDKPro Core, a broad-coverage collection of NLP components based on the UIMA processingframework.
Our collection relies only on portable software and resources and it is distributed via theMaven repository infrastructure.
It also served as a use-case and test-bed for the development of ourresource selection mechanism (Section 3.1).
DKPro Core is provided as open-source software.21For example, the ClearNLP dependency parser model for general English (version 1.2) has about 721 MB.2https://code.google.com/p/dkpro-core-asl/6Task Components LanguagesLanguage identification 2 de, en, es, fr, +65Tokenization and sentence boundary detection 5 de, en, es, fr, +25Lemmatization 7 de, enStemming 1 de, en, es, fr, +11Part-of-speech tagging 9 de, en, es, fr, +14Morphological analysis 2 de, en, fr, it, +1Named entity recognition 2 de, en, es, nlChunking 1 enConstituency parsing 3 de, en, fr, zh, +1Dependency parsing 5 de, en, es, fr, +7Coreference analysis 1 enSemantic role labelling 1 enSpell checking and grammar checking 3 de, en, es, fr, +25Figure 1: Analysis tasks covered by the DKPro Core component collectionThe collection targets users with a strong interest in the ability to programmatically assemble pipelines,e.g.
as part of dynamic scientific experiments or within NLP-enabled applications.
For this reason,our collection employs the Apache uimaFIT library (Apache UIMA Community, 2013) to allow theimplementation of pipelines with only a few lines of code (cf.
Section 3.3).Table 1 provides an overview over the analysis tasks currently covered by the collection.3Addi-tionally, our collection provides diverse input/output modules that support different file formats rangingfrom simple text, over various corpus formats (CoNLL, TIGER-XML, BNC-XML, TCF, etc.
), to tool-specific formats (IMSOpen CorpusWorkbench (Evert and Hardie, 2011), TGrep2 (Rohde, 2005), severalUIMA-specific formats, etc.).
These enable the processing of corpora from many sources and the furtherprocessing of results with specialized tools.We primarily integrate third-party tools with the UIMA framework and include only few original com-ponents, mainly for reading and writing the different supported data formats.
Our work focusses on theconcerns related to interoperability and usability, such as the resource selection mechanism (Section 3.1).It is our policy to integrate only third-party tools that are properly versioned and that are distributed viathe Central Repository, generally including their full source code.4As a considerable portion of the toolswe integrate do not initially meet this requirement, we regularly reach out to the respective communitiesand either help them publishing their tools the Central Repository or offer to do so on their behalf.
TheDKPro Core components themselves are also distributed via the Central Repository.3.3 Self-contained executable pipeline exampleWe define a self-contained pipeline description as uniquely identifying all required components andresources.
Assuming that the results of the pipeline are fully defined by these and by the input data,such a self-contained pipeline should allow for reproducible results.
In particular, the results must notinfluenced by the platform the pipeline is run on.We take a step further making self-contained pipelines also convenient for the users by removingthe need to manually obtain and install the required components and resources.
To do so, we rely ona generic bootstrapping mechanism which is capable of extracting the information about the requiredartifacts from the pipeline description and of obtaining them automatically from a repository.We achieve this goal most illustratively through a combination of these ingredients: our auto-configuration mechanism (Section 3.1) which removes the need for explicit configuration and whichidentifies and fetches the required resources from the repository at runtime; our component collection(Section 3.2) that is published through a Maven repository; Groovy (2014) and its Grape5subsystemserving as a bootstrapping mechanism to fetch the components from the repository; uimaFIT providinga concise way of assembling a pipeline of UIMA components in Groovy and making it executable.Listing 1 demonstrates such a self-contained and executable pipeline.
The example consists of three3Unfortunately, we cannot give a full account of the actually integrated third-party tools here, due to the lack of space.4An exception to this rule are tools that need to be integrated as binaries because they are not implemented in Java.5Groovy Adaptable Packaging Engine: http://groovy.codehaus.org/Grape7Listing 1: Executable pipeline implemented in Groovy1 @Grab(group=?de.tudarmstadt.ukp.dkpro.core?,2 module=?de.tudarmstadt.ukp.dkpro.core.textcat-asl?, version=?1.6.1?
)3 @Grab(group=?de.tudarmstadt.ukp.dkpro.core?,4 module=?de.tudarmstadt.ukp.dkpro.core.stanfordnlp-gpl?, version=?1.6.1?
)5 @Grab(group=?de.tudarmstadt.ukp.dkpro.core?,6 module=?de.tudarmstadt.ukp.dkpro.core.maltparser-asl?, version=?1.6.1?
)7 @Grab(group=?de.tudarmstadt.ukp.dkpro.core?,8 module=?de.tudarmstadt.ukp.dkpro.core.io.text-asl?, version=?1.6.1?
)9 @Grab(group=?de.tudarmstadt.ukp.dkpro.core?,10 module=?de.tudarmstadt.ukp.dkpro.core.io.conll-asl?, version=?1.6.1?
)1112 import de.tudarmstadt.ukp.dkpro.core.textcat.
*;13 import de.tudarmstadt.ukp.dkpro.core.stanfordnlp.
*;14 import de.tudarmstadt.ukp.dkpro.core.maltparser.
*;15 import de.tudarmstadt.ukp.dkpro.core.io.text.
*;16 import de.tudarmstadt.ukp.dkpro.core.io.conll.
*;17 import static org.apache.uima.fit.factory.AnalysisEngineFactory.
*;18 import static org.apache.uima.fit.factory.CollectionReaderFactory.
*;19 import static org.apache.uima.fit.pipeline.SimplePipeline.
*;2021 runPipeline(22 createReaderDescription(TextReader,23 TextReader.PARAM_SOURCE_LOCATION, args[0]),24 createEngineDescription(LanguageIdentifier),25 createEngineDescription(StanfordSegmenter),26 createEngineDescription(StanfordPosTagger),27 createEngineDescription(MaltParser),28 createEngineDescription(Conll2006Writer,29 Conll2006Writer.PARAM_TARGET_LOCATION, args[1]));sections.
Lines 1-10 identify the components used in the pipeline by name and version.
Lines 12-19are necessary boilerplate code making the components accessible within the Groovy script.
Lines 21-29employ uimaFIT to assemble and run a pipeline consisting of components from our collection.When the Groovy script representing the pipeline is executed, it downloads all required artifacts.Afterwards, these artifacts remain on the user?s system and they can be used again for a subsequentexecution of the script.
The user may also create backups of these artifacts or transfer them to a differentsystem.
Thus, in contrast to pipelines that rely on online services, our approach allows the user tomaintain control over the involved software and resources.The example pipeline given in Listing 1 can indeed be run on any computer that has Groovy installed.It is a life example of a self-contained NLP pipeline shared as part of a scientific publication.
By meansof this example, we demonstrate that we have reached our goal of providing a concept for shareablepipelines based on portable components and resources.Due to its conciseness, we consider the Groovy script to provide the most illustrative example of thebenefits provided by our contributions.
However, there are alternative ways to operationalize our con-cepts.
Alternatively we could use a Jython (2014) script and jip6to resolve the Maven dependencies, Javaand Maven, or a variety of other JVM-based languages and build tools supporting Maven repositories.4 ApplicationsThe DKPro Core collection has already been successfully used for linguistic pre-processing in varioustasks, including, but not limited to, temporal tagging (Str?otgen and Gertz, 2010), text segmentation basedon topic models (Riedl and Biemann, 2012), and textual entailment (Noh and Pad?o, 2013).The portable components and resources from our collection can be integrated into online workbenchesand can be run on cloud platforms by users that find this convenient.
Combined with our concept forexecutable pipelines, users can be enabled to export self-contained pipelines from such workbenches andto archive them for later reproduction.
Additionally, users can und run the pipelines on private hardware,possibly on sensitive data which users do not feel comfortable submitting to the cloud.
We believe thatservice-based offerings should be based as much as possible on portable software, and we focussed inthis paper on improving the availability and convenience of using such portable NLP software.
Thus, weconsider our approach not to be competing with service-based approaches but rather as complementingthem.6https://pypi.python.org/pypi/jip8Our concept of automatically selecting and acquiring resources can be immediately transferred toother component collections.
Although our component collection is based on UIMA, this aspect hasbeen implemented independent of the processing framework.
Having experienced the convenience of-fered by this concept, we believe that integrating a pluggable resource resolving mechanism directly intoprocessing frameworks such as GATE or UIMA would be beneficial.
A pluggable mechanism would beimportant because we expect that the underlying repository infrastructures and coordinate systems arelikely to evolve over time.
For example, we could envisage an integrated resolving mechanism that al-lows combining the rich metadata offered by directories such as META-SHARE or the Virtual LanguageObservatory with the ability to automatically acquire software and resources offered by Maven or withthe ability of invoking NLP tools as services such as via AnnoMarket.Our concept of rendering self-contained pipelines as executable scripts facilitates the sharing ofpipelines.
This can be either only the script which then downloads its dependencies upon execution,or the dependencies can be resolved beforehand and included with the script.
The concise pipeline de-scription is also useful for examples in teaching, in documentation, or on community platforms like StackOverflow.7We offer Groovy- and Jython-based quick-start examples for the DKPro Core collection tonew users.5 Summary and future workIn this paper, we have presented a novel concept for implementing shareable NLP pipelines supportedby a broad-coverage collection of interoperable NLP components.
Our approach is enabled by the com-bination of distributing portable NLP components and resources through a repository infrastructure andby an auto-configuration mechanism allowing components to select suitable resources at runtime and toobtain them automatically from the repository.We have demonstrated that our contributions enable a concise and self-contained pipeline description,which can easily be shared, e.g.
as examples in teaching, documentation, or publications.
The relianceon portable artifacts allow the user to maintain control, e.g.
by creating backups of the involved artifactsto reproduce results at a later time, even if the original repository may no longer be available.In the future, we plan to investigate a mechanism to automatically detect misalignments betweenresources and components within a pipeline to provide the user with an indication when suboptimalresults may occur and what may cause them.
This is necessary because components in the collectionare interoperable at the level of annotation types, whereas tagsets and tokenization are simply passedthrough.
While this is a common approach, it leads to the situation that the results may be negativelyaffected due to diverging tokenizations used while generate the resources for the components.
Also, theautomatic resource selection mechanism may currently choose resources with incompatible tagsets, e.g.a POS-tagger model producing tagset X while a subsequent dependency parser would require tagset Y .We also plan to extend the resource selection process to support additional metadata.
Eventually, thevariant coordinate should be replaced by a more fine-grained mechanism to select resources based e.g.on the domain, tagset, or other characteristics.AcknowledgementsThe project was partially funded by means of the German Federal Ministry of Education and Research(BMBF) under the promotional reference 01UG1110D, and partially by the Volkswagen Foundation aspart of the Lichtenberg-Professorship Program under grant No.
I/82806.
The authors take the responsi-bility for the contents.ReferencesApache UIMA Community.
2013.
Apache uimaFIT guide and reference, version 2.0.0.
Technical report, ApacheUIMA.7http://stackoverflow.com (Last accesses: 2014-02-14)9Ekaterina Buyko and Udo Hahn.
2008.
Fully embedded type systems for the semantic annotation layer.
In ICGL2008 - Proceedings of First International Conference on Global Interoperability for Language Resources, pages26?33, Hong Kong.Central Repository.
2014.
The Central Repository.
URL http://search.maven.org (Last accessed:2014-03-19), March.
Sonatype Inc. (http://www.sonatype.org/central).Christian Chiarcos, Stefanie Dipper, Michael G?otze, Ulf Leser, Anke L?udeling, Julia Ritz, and Manfred Stede.2008.
A flexible framework for integrating annotations from different tools and tagsets.
Traitement Automatiquedes Langues, 49(2):271?293.ClearNLP.
2014.
Version 2.0.2 - fast and robust NLP components implemented in Java.
URL http://opennlp.apache.org (Last accessed: 2014-03-19), January.Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan.
2002.
GATE: an architecture fordevelopment of robust HLT applications.
In Proceedings of 40th Annual Meeting of the Association for Com-putational Linguistics, pages 168?175, Philadelphia, Pennsylvania, USA, July.
Association for ComputationalLinguistics.Stefan Evert and Andrew Hardie.
2011.
Twenty-first century corpus workbench: Updating a query architecturefor the new millennium.
In Proceedings of the Corpus Linguistics 2011 conference, Birmingham, UK, July.University of Birmingham.David Ferrucci and Adam Lally.
2004.
UIMA: an architectural approach to unstructured information processingin the corporate research environment.
Natural Language Engineering, 10(3-4):327?348.Antske Fokkens, Marieke van Erp, Marten Postma, Ted Pedersen, Piek Vossen, and Nuno Freire.
2013.
Offspringfrom reproduction problems: What replication failure teaches us.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers), pages 1691?1701, Sofia, Bulgaria,August.
Association for Computational Linguistics.Carole A Goble, Jiten Bhagat, Sergejs Aleksejevs, Don Cruickshank, Danius Michaelides, David Newman, MarkBorkum, Sean Bechhofer, Marco Roos, Peter Li, et al.
2010. myExperiment: a repository and social networkfor the sharing of bioinformatics workflows.
Nucleic acids research, 38(suppl 2):W677?W682.Jos?e Manuel G?omez-P?erez, Esteban Garc?a-Cuesta, Jun Zhao, Aleix Garrido, Jos?e Enrique Ruiz, and GrahamKlyne.
2013.
How reliable is your workflow: Monitoring decay in scholarly publications.
In Proceedingsof the 3rd Workshop on Semantic Publishing (SePublica 2013) at 10th Extended Semantic Web Conference,page 75, Montpellier, France, May.Groovy.
2014.
Version 2.2.2 - A dynamic language for the Java platform.
URL http://groovy.codehaus.org (Last accessed: 2014-03-19, February.P?eter Hal?acsy, Andr?as Kornai, and Csaba Oravecz.
2007.
Hunpos ?
an open source trigram tagger.
In Proceedingsof the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedingsof the Demo and Poster Sessions, pages 209?212, Prague, Czech Republic, June.
Association for ComputationalLinguistics.Marie Hinrichs, Thomas Zastrow, and Erhard Hinrichs.
2010.
WebLicht: Web-based LRT Services in a DistributedeScience Infrastructure.
In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk,Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conferenceon Language Resources and Evaluation (LREC?10), pages 489?493, Valletta, Malta, May.
European LanguageResources Association (ELRA).Jython.
2014.
Jython: Python for the Java Platform.
URL http://www.jython.org (Last accessed:2014-03-19.Yoshinobu Kano, Makoto Miwa, Kevin Bretonnel Cohen, Lawrence E. Hunter, Sophia Ananiadou, and Jun?ichiTsujii.
2011.
U-Compare: A modular NLP workflow construction and evaluation system.
IBM Journal ofResearch and Development, 55(3):11:1?11:10, May.Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky.
2014.The stanford corenlp natural language processing toolkit.
In Proceedings of 52nd Annual Meeting of the As-sociation for Computational Linguistics: System Demonstrations, pages 55?60, Baltimore, Maryland, June.Association for Computational Linguistics.10Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav Marinov, andErwin Marsi.
2007.
Maltparser: A language-independent system for data-driven dependency parsing.
NaturalLanguage Engineering, 13(2):95?135.Tae-Gil Noh and Sebastian Pad?o.
2013.
Using UIMA to structure an open platform for textual entailment.
InPeter Kl?ugl, Richard Eckart de Castilho, and Katrin Tomanek, editors, Proceedings of the 3rd Workshop onUnstructured Information Management Architecture (UIMA@GSCL 2013), pages 26?33, Darmstadt, Germany,Sep.
CEUR-WS.org.Philip V. Ogren, Philipp G. Wetzler, and Steven J. Bethard.
2009.
ClearTK: a framework for statistical naturallanguage processing.
In Christian Chiarcos, Richard Eckart de Castilho, and Manfred Stede, editors, Proceed-ings of the Biennial GSCL Conference 2009, 2nd UIMA@GSCL Workshop, pages 241?248, Potsdam, Germany,September.
Gunter Narr Verlag.Rafal Rak, Andrew Rowley, Jacob Carter, and Sophia Ananiadou.
2013.
Development and analysis of nlppipelines in argo.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics:System Demonstrations, pages 115?120, Sofia, Bulgaria, August.
Association for Computational Linguistics.Martin Riedl and Chris Biemann.
2012.
Text segmentation with topic models.
JLCL, 27(1):47?69.Douglas LT Rohde.
2005.
Tgrep2 user manual version 1.15.
Massachusetts Institute of Technology.
http://ted-lab.mit.edu/dr/Tgrep2.Guergana K. Savova, James J. Masanz, Philip V. Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper-Schuler,and Christopher G. Chute.
2010.
Mayo clinical text analysis and knowledge extraction system (cTAKES):architecture, component evaluation and applications.
Journal of the American Medical Informatics Association,17(5):507?513.Helmut Schmid.
1994.
Probabilistic part-of-speech tagging using decision trees.
In Proceedings of InternationalConference on New Methods in Language Processing, pages 44?49, Manchester, UK.Sonatype Company.
2008.
Maven: The Definitive Guide.
O?Reilly Media, September.
ISBN: 9780596517335.Jannik Str?otgen and Michael Gertz.
2010.
HeidelTime: High Quality Rule-Based Extraction and Normalizationof Temporal Expressions.
In Proceedings of the 5th International Workshop on Semantic Evaluation, pages321?324, Uppsala, Sweden, July.
Association for Computational Linguistics.Valentin Tablan, Kalina Bontcheva, Ian Roberts, Hamish Cunningham, and Marin Dimitrov.
2013.
Annomarket:An open cloud platform for nlp.
In Proceedings of the 51st Annual Meeting of the Association for ComputationalLinguistics: System Demonstrations, pages 19?24, Sofia, Bulgaria, August.
Association for ComputationalLinguistics.Paul Thompson, Yoshinobu Kano, John McNaught, Steve Pettifer, Teresa Attwood, John Keane, and Sophia Ana-niadou.
2011.
Promoting interoperability of resources in meta-share.
In Proceedings of the Workshop onLanguage Resources, Technology and Services in the Sharing Paradigm, pages 50?58, Chiang Mai, Thailand,November.
Asian Federation of Natural Language Processing.Dieter Van Uytvanck, Claus Zinn, Daan Broeder, Peter Wittenburg, and Mariano Gardellini.
2010.
Virtual Lan-guage Observatory: The portal to the language resources and technology universe.
In Nicoletta Calzolari, KhalidChoukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, edi-tors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC?10),pages 900?903, Valletta, Malta, may.
European Language Resources Association (ELRA).Stephen Wu, Vinod Kaggal, Dmitriy Dligach, James Masanz, Pei Chen, Lee Becker, Wendy Chapman, GuerganaSavova, Hongfang Liu, and Christopher Chute.
2013.
A common type system for clinical natural languageprocessing.
Journal of Biomedical Semantics, 4(1):1.11
