Verb Sense and Subcategorization:Using Joint Inference to Improve Performance on Complementary TasksGalen Andrew, Trond Grenager, and Christopher ManningComputer Science DepartmentStanford UniversityStanford, CA 94305-9040{pupochik, grenager, manning}@cs.stanford.eduAbstractWe propose a general model for joint inference in corre-lated natural language processing tasks when fully anno-tated training data is not available, and apply this modelto the dual tasks of word sense disambiguation and verbsubcategorization frame determination.
The model usesthe EM algorithm to simultaneously complete partiallyannotated training sets and learn a generative probabilis-tic model over multiple annotations.
When applied to theword sense and verb subcategorization frame determina-tion tasks, the model learns sharp joint probability dis-tributions which correspond to linguistic intuitions aboutthe correlations of the variables.
Use of the joint modelleads to error reductions over competitive independentmodels on these tasks.1 IntroductionNatural language processing research has tradition-ally been divided into a number of separate tasks,each of which is believed to be an important sub-task of the larger language comprehension or gener-ation problem.
These tasks are usually addressedseparately, with systems designed to solve a sin-gle problem.
However, many of these tasks are nottruly independent; if solutions to one were knownthey would facilitate finding solutions to the others.For some sets of these problems, one would like tobe able to do joint inference, where information ofone kind can influence decisions about informationof another kind and vice versa.
For instance, in-formation about named entities can usefully informthe decisions of a part-of-speech tagger, but equally,part-of-speech information can help a named entityrecognizer.
If one had a large corpus annotated withall the information types of interest, one could es-timate a joint distribution over all of the variablessimply by counting.
However, it is more often thecase that one lacks any jointly annotated corpus,or at least one that is sufficiently large, given thatthe joint distribution is necessarily sparser than themarginal distributions.
It would therefore be usefulto be able to build a model for this joint inferencetask using only partially supervised data.
In thisSystem Name Accuracykunlp 57.6jhu-english-JHU-final 56.6SMUls 56.3LIA-Sinequa-Lexsample 53.5manning-cs224n 52.3Table 1: Performance of the top 5 Senseval-2 word sensedisambiguation systems when considering accuracy onlyon the 29 verbs.
Systems not guessing on all instanceshave been omitted.paper we examine these problems in the context ofjoint inference over verb senses and their subcate-gorization frames (SCFs).1.1 Verb Sense and SubcategorizationOf the syntactic categories tested in the Sensevalword sense disambiguation (WSD) competitions,verbs have proven empirically to be the most dif-ficult.
In Senseval-1, Kilgarriff and Rosenzweig(2000) found a 10-point difference between thebest systems?
performance on verbs compared withother parts-of-speech.
In Senseval-2, Yarowsky andFlorian (2002) also found that while accuracies ofaround 73% were possible for adjectives and nouns,even the most competitive systems have accuraciesof around 57% when tested on verbs (see Table 1).A likely explanation for this discrepancy is that dif-ferent senses of common verbs can occur in sim-ilar lexical contexts, thereby decreasing the effec-tiveness of ?bag-of-words?
models.Verbs also pose serious challenges in a very dif-ferent task: syntactic parsing.
Verb phrases are syn-tactically complex and frought with pitfalls for auto-mated parsers, such as prepositional phrase attach-ment ambiguities.
These challenges may be par-tially mitigated by the fact that particular verbs oftenhave strong preferences for particular SCFs.
Unfor-tunately, it is not the case that each verb consistentlytakes the same SCF.
More often, a verb has severalpreferred SCFs, with rarer forms also occurring, forexample, in idioms.
Jurafsky (1998) proposes us-?
NP PP NPPP VPto VPing2:30:00 4 1 0 0 20 332:30:01 1 7 0 4 0 02:42:04 12 0 3 0 0 1Table 2: The learned joint distribution over the sensesand subcategorizations of the verb begin (in percentprobability).
Low probability senses and subcategoriza-tions have been omitted.ing a probabilistic framework to represent subcate-gorization preferences, where each lexical item hasa corresponding distribution over the possible setsof arguments.
Modeling these distributions may beuseful: Collins (2003) has shown that verb subcate-gorization information can be used to improve syn-tactic parsing performance.It has also been recognized that a much more ac-curate prediction of verb subcategorization prefer-ence can be made if conditioned on the sense ofthe verb.
Roland and Jurafsky (2002) conclude thatfor a given lexical token in English, verb sense isthe best determiner of SCF, far outweighing eithergenre or dialect.
Demonstrating the utility of this,Korhonen and Preiss (2003) achieve significant im-provement at a verb subcategorization acquisitiontask by conditioning on the verb sense as predictedby a statistical word sense disambiguation system.Conversely, if different senses have distinct subcat-egorization preferences, it is reasonable to expectthat information about the way a verb subcatego-rizes in a particular case may be of significant util-ity in determining the verb?s sense.
As an example,Yarowsky (2000) makes use of rich syntactic fea-tures to improve the performance of a supervisedWSD system.As an illustration of this correlation, Table 2shows a learned joint distribution over sense andSCF for the common verb begin.1 Its commonsenses, taken from WordNet, are as follows: sense2:30:00, to initiate an action or activity, (?beginworking?
), sense 2:30:01, to set in motion or causeto start, (?to begin a war?
), and sense 2:42:04, tohave a beginning, (?the day began?).
The SCFsshown here are a subset of the complete set of SCFs,described in Table 3.
Note that the sense and SCFvariables are highly correlated for this verb.
Sense2:30:00 occurs almost entirely with verb phrase ar-guments, sense 2:30:01 occurs almost entirely as atransitive verb, and sense 2:42:04 occurs as an in-transitive verb (no arguments following the verb).It should be evident that the strong correlation be-1We cannot show an empirical joint distribution because ofthe lack of a sufficiently large jointly annotated corpus, as dis-cussed below.tween these two variables can be exploited to in-crease performance in the tasks of predicting theirvalues in either direction, even when the evidence isweak or uncertain.1.2 Learning a Joint ModelPerforming joint inference requires learning a jointdistribution over sense and SCF for each verb.
Inorder to estimate the joint distribution directly fromdata we would need a large corpus that is annotatedfor both verb sense and SCF.
Unfortunately, no suchcorpus of adequate size exists.2 Instead, there aresome corpora such as SemCor and Senseval-2 la-beled for sense, and others that are parsed and fromwhich it is possible to compute verb SCFs determin-istically.
In the current work we use two corpora tolearn a joint model: Senseval-2, labeled for sensebut not syntax, and the Penn Treebank, labeled forsyntax but not sense.
We do so by treating the twodata sets as a single one with incompletely labeledinstances.
This partially labeled data set then yieldsa semi-supervised learning problem, suitable for theExpectation-Maximization (EM) algorithm (Demp-ster et al, 1977).2 Tasks and Data SetsWe evaluate our system on both the WSD task andthe verb SCF determination task.
We describe eachtask in turn.2.1 Word Sense DisambiguationWe used as our sense-annotated corpus the datasets from the English lexical sample portion of theSenseval-2 word sense disambiguation competition(Kilgarriff and Rosenzweig, 2000).
This data setcontains multiple instances of 73 different Englishword types, divided into training and testing exam-ples.
Each word type is marked for part of speech,so that the sense disambiguation task does not needto distinguish between senses that have differentparts of speech.
We selected from this data set al29 words that were marked as verbs.Each example consists of a marked occurrence ofthe target word in approximately 100 words of sur-rounding context.
The correct sense of the word,marked by human annotators, is also given.
Eachinstance is labeled with a sense corresponding to asynset from WordNet (Miller, 1995).
The numberof senses per word varies enormously: some wordshave more than 30 senses, while others have five2A portion of the Brown corpus has been used both in theconstruction of the SemCor word sense database and in the con-struction of the Penn Treebank, but coverage is very low, espe-cially for sense markings, and the individual sentences have notto our knowledge been explicitly aligned.or fewer.
These ?fine-grained?
senses are also par-titioned into a smaller number of ?coarse-grained?senses, and systems are evaluated according to bothmetrics.
The number of training and testing exam-ples per word varies from tens to nearly a thousand.We used the same train/test division as in Senseval-2, so that our reported accuracy numbers are directlycomparable with those of other Senseval-2 submis-sions, as given in Table 1.2.2 Verb SubcategorizationWe use as our SCF-annotated corpus sentencesdrawn from the Wall Street Journal section of thePenn Treebank.
For each target verb we select sen-tences containing a form of the verb (tagged as averb) with length less than 40 words.
We selecttraining examples from sections 2 through 21, andtest examples from all other sections.3There are many conceivable ways to partitionthe set of possible verb argument combinations intoSCFs.
One possible approach would be to use as theSCF representation the raw sequence of constituentsoccurring in the verb phrase.
This is certainly anunbiased representation, but as there are many thou-sands of rewrites for VP in the Penn Treebank, datasparsity would present a significant problem.
In ad-dition, many of the variants do not contain usefulinformation for our task: for example, we wouldn?texpect to get much value from knowing about thepresence or absence of an adverb in the phrase.
In-stead, we chose to use a small number of linguis-tically motivated SCFs which form a partition overthe large space of possible verb arguments.We chose as a starting point the SCF partitionspecified in Roland (2001).
These SCFs are defineddeclaratively using a set of tgrep expressions thatmatch appropriate verb phrases.4 We made signifi-cant modifications to the set of SCFs, and also sim-plified the tgrep expressions used to match them.One difference from Roland?s SCF set is that weanalyze verb particles as arguments, so that severalSCFs differ only in the existence of a particle.
Thisis motivated by the fact that the particle is a syntacticfeature that provides strong evidence about the verbsense.
One might argue that the presence of a par-ticle should be considered a lexical feature modeledindependently from the SCF, but the distinction isblurry, and we have instead combined the variablesin favor of model simplicity.
A second difference is3Sections 2 through 21 of the WSJ are typically used fortraining PCFG parsers, and section 23 is typically used for test-ing.
Because of sparse data we drew our test examples from allnon-training sections.4tgrep is a tree node matching program written by RichardPito, distributed with the Penn Treebank.Subcat Description?
No argumentsNP TransitivePP Prepositional phraseNP PP Trans.
with prep.
phraseVPing Gerundive verb phraseNP VPing Perceptual complementVPto Intrans.
w/ infinitival VPNP VPto Trans.
w/ infinitival VPS for to Intrans.
w/ for PP and infin.
VPNP SBAR Trans.
w/ finite clauseNP NP DitransitivePRT Particle and no args.NP PRT Transitive w/ particlePP PRT Intrans.
w/ PP and particleVP PRT Intrans.
w/ VP and particleSBAR PRT Intrans.
w/ fin.
clause and part.Other None of the aboveTable 3: The 17 subcategorization frames we use.that unlike Roland, we do not put passive verb con-structions in a separate ?passive?
SCF, but insteadwe undo the passivization and put them in the un-derlying category.
Although one might expect thatpassivization itself is a weak indicator of sense, webelieve that the underlying SCF is more useful.
Ourfinal set of SCFs is shown in Table 3.Given a sentence annotated with a syntacticparse, the SCF of the target verb can be computed byattempting to match each of the SCF-specific tgrepexpressions with the verb phrase containing the tar-get verb.
Unlike those given by Roland, our tgrepexpressions are not designed to be mutually exclu-sive; instead we determine verb SCF by attemptingmatches in a prescribed sequence, using ?if-then-else?
logic.3 Model Structure and InferenceOur generative probabilistic model can be thoughtof as having three primary components: the sensemodel, relating the verb sense to the surroundingcontext, the subcategorization model, relating theverb subcategorization to the sentence, and the jointmodel, relating the sense and SCF of the verb toeach other.
More formally, the model is a factoredrepresentation of a joint distribution over these vari-ables and the data: the verb sense (V ), the verb SCF(C), the unordered context ?bag-of-words?
(W ),and the sentence as an ordered sequence of words(S).
The joint distribution P(V, C, W, S) is thenfactored asP(V )P(C|V )P(S|C)?iP(Wi |V )W SV CnFigure 1: A graphical representation of the combinedsense and subcategorization probabilistic model.
Notethat the box defines a plate, indicating that the modelcontains n copies of this variable.where Wi is the word type occurring in each po-sition of the context (including the target sentenceitself).
The first two terms together define a jointdistribution over verb sense (V ) and SCF (C), thethird term defines the subcategorization model, andthe last term defines the sense model.
A graphicalmodel representation is shown in Figure 1.The model assumes the following generative pro-cess for a data instance of a particular verb.
First wegenerate the sense of the target verb.
Conditionedon the sense, we generate the SCF of the verb.
(Notethat the decision to generate sense and then SCFis arbitrary and forced by the desire to factor themodel; we discuss reversing the order below.)
Then,conditioned on the sense of the verb, we generatean unordered collection of context words.
(For theSenseval-2 corpus, this collection includes not onlythe words in the sentence in which the verb occurs,but also the words in surrounding sentences.)
Fi-nally, conditioned on the SCF of the verb, we gen-erate the immediate sentence containing the verb asan ordered sequence of words.An apparent weakness of this model is that itdouble-generates the context words from the en-closing sentence: they are generated once by thesense model, as an unordered collection of words,and once by the subcategorization model, as an or-dered sequence of words.
The model is thus defi-cient in that it assigns a large portion of its probabil-ity mass to impossible cases: those instances whichhave words in the context which do not match thosein the sentence.
However because the sentences arealways observed, we only consider instances in theset of consistent cases, so the deficiency should beirrelevant for the purpose of reasoning about senseand SCF.We discuss each of the model components in turn.3.1 Verb Sense ModelThe verb sense component of our model is an or-dinary multinomial Naive Bayes ?bag-of-words?model: P(V )?i P(Wi |V ).
We learn the marginalover verb sense with maximum likelihood estima-tion (MLE) from the sense annotated data.
We learnthe sense-conditional word model using smoothedMLE from the sense annotated data, and to smoothwe use Bayesian smoothing with a Dirichlet prior.The free smoothing parameter is determined empir-ically, once for all words in the data set.
In the inde-pendent sense model, to infer the most likely sensegiven a context of words P(S|W), we just find the Vthat maximizes P(V )?i P(Wi |V ).
Inference in thejoint model over sense and SCF is more complex,and is described below.In order to make our system competitive withleading WSD systems we made an important modi-fication to this basic model: we added relative posi-tion feature weighting.
It is known that words closerto the target word are more predictive of sense, so itis reasonable to weight them more highly.
We de-fine a set of ?buckets?, or partition over the positionof the context word relative to the target verb, andwe weight each context word feature with a weightgiven by its bucket, both when estimating model pa-rameters at train time and when performing infer-ence at test time.
We use the following 8 relativeposition buckets: (?
?,?6], [?5,?3], ?2, ?1, 1,2, [3, 5], and [6,?).
The bucket weights are foundempirically using a simple optimization procedureon k-fold training set accuracy.
In ablation tests onthis system we found that the use of relative posi-tion feature weighting, when combined with corre-sponding evidence attenuation (see Section 3.3) in-creased the accuracy of the standalone verb sensedisambiguation model from 46.2% to 54.0%.3.2 Verb Subcategorization ModelThe verb SCF component of our model P(S|C)represents the probability of particular sentencesgiven each possible SCF.
Because there are in-finitely many possible sentences, a multinomial rep-resentation is infeasible, and we instead chose toencode the distribution using a set of probabilisticcontext free grammars (PCFGs).
A PCFG is createdfor each possible SCF: each PCFG yields only parsetrees in which the distinguished verb subcategorizesin the specified manner (but other verbs can parsefreely).
Given a SCF-specific PCFG, we can deter-mine the probability of the sentence using the insidealgorithm, which sums the probabilities of all pos-sible trees in the grammar producing the sentence.To do this, we modified the exact PCFG parser ofKlein and Manning (2003).
In the independent SCFmodel, to infer the most likely SCF given a sen-tence P(C|S), we just find the C that maximizesP(S|C)P(C).
(For the independent model, the SCFprior is estimated using MLE from the training ex-amples.)
Inference in the joint model over sense andSCF is more complex, and is described below.Learning this model, SCF-specific PCFGs, fromour SCF-annotated training data, requires somecare.
Commonly PCFGs are learned using MLEof rewrite rule probabilities from large sets of tree-annotated sentences.
Thus to learn SCF-specificPCFGs, it seems that we should select a set of an-notated sentences containing the target verb, deter-mine the SCF of the target verb in each sentence,create a separate corpus for each SCF of the targetverb, and then learn SCF-specific grammars fromthe SCF-specific corpora.
If we are careful to dis-tinguish rules which dominate the target verb fromthose which do not, then the grammar will be con-strained to generate trees in which the target verbsubcategorizes in the specified manner, and otherverbs can occur in general tree structures.
The prob-lem with this approach is that in order to create abroad-coverage grammar (which we will need in or-der for it to generalize accurately to unseen test in-stances) we will need a very large number of sen-tences in which the target verb occurs, and we donot have enough data for this approach.Because we want to maximize the use of theavailable data, we must instead make use of everyverb occurrence when learning SCF-specific rewriterules.
We can accomplish this by making a copyof each sentence for each verb occurrence (not justthe target verb), determining the SCF of the distin-guished verb in each sentence, partitioning the sen-tence copies by distinguished verb SCF, and learn-ing SCF-specific grammars using MLE.
Finally, wechange the lexicon by forcing the distinguished verbtag to rewrite to only our target verb.
The methodwe actually use is functionally equivalent to this lat-ter approach, but altered for efficiency.
Instead ofmaking copies of sentences with multiple verbs, weuse a dense representation.
We determine the SCFof each verb in the sentence, and then annotate theverb and all nonterminal categories occurring abovethe verb in the tree, up to the root, with the SCFof the verb.
Note that some nonterminals will thenhave multiple annotations.
Then to learn a SCF-specific PCFG, we count rules that have the speci-fied SCF annotation as rules which can dominate thedistinguished verb, and then count all rules (includ-ing the SCF-specific ones) as general rules whichcannot dominate the distinguished verb.3.3 The Joint ModelGiven a fully annotated dataset, it is trivial to learnthe parameters of the joint distribution over verbsense and SCF P(V, C) using MLE.
However, be-cause we do not have access to such a dataset, weinstead use the EM algorithm to ?complete?
themissing annotations with expectations, or soft as-signments, over the values of the missing variable(we present the EM algorithm in detail in the nextsection).
Given this ?completed?
data, it is againtrivial to learn the parameters of the joint proba-bility model using smoothed MLE.
We use simpleLaplace add-one smoothing to smooth the distribu-tion.However, a small complication arises from thefact that the marginal distributions over senses andSCFs for a particular verb may differ between thetwo halves of our data set.
They are, after all, whollydifferent corpora, assembled by different people fordifferent purposes.
For this reason, when testingthe system on the sense corpus we?d like to use asense marginal distribution trained from the sensecorpus, and when testing the system on the SCFcorpus we?d like to use a SCF marginal distribu-tion trained from the SCF corpus.
To address this,recall from above that the factoring we choose forthe joint distribution is arbitrary.
When performingsense inference we use the model Pv(V )P j(C|V )where P j(C|V ) was learned from the complete data,and Pv(V ) was learned from the sense-marked ex-amples only.
When performing SCF inference weuse the equivalent factoring Pc(C)P j(V |C), whereP j(V |C) was learned from the complete data, andPc(C) was learned from the SCF-annotated exam-ples only.We made one additional modification to this jointmodel to improve performance.
When performinginference in the model, we found it useful to dif-ferentially weight different probability terms.
Themost obvious need for this comes from the factthat the sense-conditional word model employs rel-ative position feature weighting, which can changethe relatively magnitude of the probabilities in thisterm.
In particular, by using feature weights greaterthan 1.0 during inference we overestimate the ac-tual amount of evidence.
Even without the featureweighting, however, the word model can still over-estimate the actual evidence given that it encodes anincorrect independence assumption between wordfeatures (of course word occurrence in text is ac-tually very highly correlated).
The PCFG modelalso suffers from a less severe instance of the sameproblem: human languages are of course not con-text free, and there is in fact correlation betweensupposedly independent tree structures in differentparts of the tree.
To remedy this evidence over-confidence, it is helpful to attenuate or downweightthe evidence terms accordingly.
More generally, weplace weights on each of the probability terms usedin inference calculations, yielding models of the fol-lowing form:P(V )?
(v)P(C|V )?(c)P(S|C)?
(s)[?iP(Wi |V )]?
(w)These ?(?)
weights are free parameters, and wefind them by simple optimization on k-fold accu-racy.
In ablation tests on this system, we foundthat term weighting (particularly evidence attenua-tion) increased the accuracy of the standalone sensemodel from 51.9% to 54.0% at the fine-grained verbsense disambiguation task.We now describe the precise EM algorithm used.Prior to running EM we first learn the independentsense and SCF model parameters from their respec-tive datasets.
We also initialize the joint sense andSCF distribution to the uniform distribution.
Thenwe iterate over the following steps:?
E-step: Using the current model parameters,for each datum in the sense-annotated corpus,compute expectations over the possible SCFs,and for each datum in the SCF-annotated cor-pus, compute expectations over the possiblesenses.?
M-step: use the completed data to reestimatethe joint distribution over sense and SCF.We run EM to convergence, which for our datasetoccurs within 6 iterations.
Additional iterations donot change the accuracy of our model.
Early stop-ping of EM after 3 iterations was found to hurt k-fold sense accuracy by 0.1% and SCF accuracy by0.2%.
Early stopping of EM after only 1 iterationwas found to hurt k-fold sense accuracy by a total of0.2% and SCF accuracy by 0.4%.
These may seemlike small differences, but significant relative to theadvantages given by the joint model (see below).In the E-step of EM, it is necessary to do infer-ence over the joint model, computing posterior ex-pectations of unknown variables conditioned on ev-idence variables.
During the testing phase, it is alsonecessary to do inference, computing maximum aposteriori (MAP) values of unknown variables con-ditioned on evidence variables.
In all cases we doexact Bayesian network inference, which involvesconditioning on evidence variables, summing overextraneous variables, and then either maximizingover the resulting factors of query variables, or nor-malizing them to obtain distributions of query vari-ables.
At test time, when querying about the MAPsense (or SCF) of an instance, we chose to max-imize over the marginal distribution, rather thanmaximize over the joint sense and SCF distribution.We found empirically that this gave us higher accu-racy at the individual tasks.
If instead we were do-ing joint prediction, we would expect high accuracyto result from maximizing over the joint.4 Results and DiscussionIn Figures 2, 3 and 4 we compare the perfor-mance of the independent and joint models on theverb sense disambiguation and verb SCF determina-tion problems, evaluated using both 10-fold cross-validation accuracy and test set accuracy.
In Figure2, we report the performance of a system resultingfrom doing optimization of free parameters (such asfeature and term weights) on a per-verb basis.
Wealso provide a baseline computed by guessing themost likely class.Although the parameter optimization of Figure2 was performed with respect to 10-fold cross-validation on the training sets, its lower perfor-mance on the test sets suggests that it suffers fromoverfitting.
To test this hypothesis we also trainedand tested on the test sets a version of the systemwith corpus-wide free parameter optimization, andthe results of this test are shown in Figure 3.
Thelower gap between the training set cross-validationand test set performance on the WSD task confirmsour overfitting hypothesis.
However, note that thegap between training set cross-validation and testset performance on the SCF determination task per-sists (although it is diminished slightly).
We believethat this results from the fact that there is significantdata drift between the training sections of the WSJin the Penn Treebank (sections 2 through 21) and allother sections.Using corpus-wide optimization, the joint modelimproves sense disambiguation accuracy by 1.9%over the independent model, bringing our systemto 55.9% accuracy on the test set, performance thatis comparable with that of the state of the art sys-tems on verbs given in Table 1.
The joint model re-duces sense disambiguation error by 4.1%.
On theverb SCF determination task, the joint model yieldsa 2.1% improvement in accuracy over the indepen-dent model, reducing total error by 5.1%.We also report results of the independent andjoint systems on each verb individually in Table 4Not surprisingly, making use of the joint distributionwas much more helpful for some verbs than others.40.538.859.970.860.272.854.759.755.661.435404550556065707580Sense SubcatTestAccuracyBaselineIndividual10-foldJoint 10-foldIndividualtestJoint testFigure 2: Chart comparing results of independent andjoint systems on the verb sense and SCF tasks, evaluatedwith 10-fold cross-validation on the training sets and onthe test sets.
The baseline shown is guessing most likelyclass.
These systems used per-verb optimization of freeparameters.40.538.852.468.554.769.854.059.355.961.435404550556065707580Sense SubcatTestAccuracyBaselineIndividual10-foldJoint 10-foldIndividualtestJoint testFigure 3: Chart comparing results of independent andjoint systems on the verb sense and SCF tasks.
Thesesystems used corpus-wide optimization of free parame-ters.40.538.845.068.549.269.146.259.350.759.635404550556065707580Sense SubcatTestAccuracyBaselineIndividual10-foldJoint 10-foldIndividualtestJoint testFigure 4: Chart comparing results of independent andjoint systems on the verb sense and SCF tasks.
This sys-tem has no relative position word feature weighting andno term weighting.Indep Joint Indep JointVerb Sense Sense Subcat Subcatbegin 76.8 84.3 57.0 63.3call 39.4 42.4 44.9 49.0carry 45.5 40.9 63.3 70.0collaborate 90.0 90.0 100.0 100.0develop 42.0 39.1 69.7 69.7draw 29.3 26.8 72.7 63.6dress 59.3 59.3 NA NAdrift 43.8 40.6 50.0 50.0drive 45.2 52.4 54.5 54.5face 81.7 80.6 82.4 82.4ferret 100.0 100.0 NA NAfind 23.5 29.4 61.1 64.8keep 46.3 58.2 52.1 53.5leave 47.0 54.5 36.4 40.0live 62.7 65.7 85.7 85.7match 57.1 54.8 58.3 66.7play 42.4 45.5 66.7 61.9pull 28.3 26.7 44.4 55.6replace 57.8 62.2 56.0 60.0see 40.6 39.1 53.6 55.1serve 60.8 52.9 72.0 72.0strike 37.0 27.8 50.0 50.0train 55.6 55.6 40.0 40.0treat 52.3 54.5 69.2 76.9turn 29.9 29.9 46.3 50.0use 65.8 68.4 69.7 68.8wander 78.0 80.0 NA NAwash 50.0 41.7 0.0 0.0work 41.7 43.3 67.9 66.1Table 4: Comparison of the performance of the indepen-dent and joint inference models on the verb sense andSCF tasks,evaluated on the Senseval-2 test set, for eachof the 29 verbs in the study.
These results were obtainedwith no per-verb parameter optimization.
Note the greatvariation in problem difficulty and joint model perfor-mance across verbs.For example, on the verbs begin, drive, find, keep,leave, and work, the joint model gives a greater than5% accuracy boost on the WSD task.
In contrast, forsome other verbs, the joint model showed a slightdecrease in accuracy on the test set relative to theindependent model.We present a few representative examples wherethe joint model makes better decisions than the in-dividual model.
In the sentence.
.
.
prices began weakening last month afterCampeau hit a cash crunch.the sense model (based on bag-of-words evidence)believes that the sense 2:42:04 is most likely (seeTable 2 for senses and joint distribution).
How-ever, the SCF model gives high weight to the framesVPto and VPing, which when combined with thejoint distribution, give much more probability tothe sense 2:30:00.
The joint model thus correctlychooses sense 2:30:00.
In the sentence.
.
.
before beginning a depressing eight-yearslide that continued through last year.the sense model again believes that the sense2:42:04 is most likely.
However, the SCF modelcorrectly gives high weight to the NP frame, whichwhen combined with the joint distribution, givesmuch more probability to the sense 2:30:01.
Thejoint model thus correctly chooses sense 2:30:01.Given the amount of information contained in thejoint distribution it is surprising that the joint modeldoesn?t yield a greater advantage over the indepen-dent models.
It seems to be the case that the wordsense model is able to capture much of the SCF in-formation by itself, without using an explicit syn-tactic model.
This results from the relative posi-tion weighting, since many of our SCFs correlatehighly with the presence of small sets of words inparticular positions (for instance, the infinitival ?to?,prepositions, and pronouns).
We tested this hypoth-esis by examining how the addition of SCF informa-tion affected performance of a weaker sense model,obtained by removing feature and term weighting.The results are shown in Figure 4.
Indeed, when us-ing this weaker word sense model, the joint modelyields a much larger 4.5% improvement in WSD ac-curacy.5 Future WorkWe can imagine several modifications to the ba-sic system that might improve performance.
Mostimportantly, more specific use could be made ofSCF information besides modeling its joint distribu-tion with sense, for example conditioning on head-words of (perceived) arguments, especially parti-cles and prepositions.
Second, although we madesome attempt at extracting the ?underlying?
SCF ofverbs by analyzing passive constructions separately,similar analysis of other types of movement suchas relative clauses may also be useful.
Third, wecould hope to get some improvement from changingour model structure to address the issue of double-generation of words discussed in section 3.
One waythis could be done would be to use a parser onlyto estimate the probability of the sequence of wordtags (i.e., parts of speech) in the sentence, then touse a sense-specific lexicon to estimate the proba-bility of finding the words under the tags.Although we chose WSD and SCF determinationas a test case, the approach of this paper is appli-cable to other pairs of tasks.
It may also be pos-sible to improve parsing accuracy on verb phrasesor other phrases, by simultaneously resolving wordsense ambiguities, as attempted unsuccessfully byBikel (2000).
This work is intended to introducea general methodology for combining disjoint NLPtasks that is of use outside of these specific tasks.6 AcknowledgementsThis paper is based on work supported in part bythe Advanced Research and Development Activity(ARDA)?s Advanced Question Answering for Intel-ligence (AQUAINT) Program, and by the NationalScience Foundation under Grant No.
IIS-0085896,as part of the Knowledge Discovery and Dissemina-tion program.
We additionally thank the reviewersfor their insightful comments.ReferencesDaniel M. Bikel.
2000.
A statistical model for parsingand word-sense disambiguation.
Joint SIGDAT Con-ference on Empirical Methods in Natural LanguageProcessing and Very Large Corpora.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
To appear in Computa-tional Linguistics.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.
Max-imum likelihood from incomplete data via the em al-gorithm.
Journal of the Royal Statistical Society, Se-ries B (Methodological), 39(1):1?38.Daniel Jurafsky.
1998.
A probabilistic model of lexicaland syntactic access and disambiguation.
CognitiveScience, 20(2):139?194.Adam Kilgarriff and Joseph Rosenzweig.
2000.
Frame-work and results for english senseval.
Computers andthe Humanities, 34(1-2):15?48.Dan Klein and Christopher Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting of the Association for ComputationalLinguistics.Anna Korhonen and Judita Preiss.
2003.
Improvingsubcategorization acquisition using word sense disam-biguation.
In Proceedings of the 41st Annual Meetingof the Association for Computational Linguistics, Sap-poro, Japan, pages 48?55.G.A.
Miller.
1995.
Wordnet: A lexical database for en-glish.
Communications of the ACM, 38(11):39?41.Douglas Roland and Daniel Jurafsky.
2002.
Verb senseand verb subcategorization probabilities.
In PaolaMerlo and Suzanne Stevenson, editors, The LexicalBasis of Sentence Processing, chapter 16.
John Ben-jamins, Amsterdam.Douglas Roland.
2001.
Verb Sense and Verb Subcate-gorization Probabilities.
Ph.D. thesis, University ofColorado.David Yarowsky and Radu Florian.
2002.
Evaluatingsense disambiguation across diverse parameter spaces.Natural Language Engineering, 8(4):293?310.David Yarowsky.
2000.
Hierarchical decision lists forword sense disambiguation.
Computers and the Hu-manities, 34(1-2).
