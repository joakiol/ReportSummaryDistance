Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 392?395,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsUCF-WS: Domain Word Sense Disambiguation using Web SelectorsHansen A. Schwartz and Fernando GomezSchool of Electrical Engineering and Computer ScienceUniversity of Central FloridaOrlando, FL 32816{hschwartz,gomez}@cs.ucf.eduAbstractThis paper studies the application of theWeb Selectors word sense disambiguationsystem on a specific domain.
The systemwas primarily applied without any domaintuning, but the incorporation of domainpredominant sense information was ex-plored.
Results indicated that the systemperforms relatively the same with domainpredominant sense information as without,scoring well above a random baseline, butstill 5 percentage points below results ofusing the first sense.1 IntroductionWe explore the use of the Web Selectors wordsense disambiguation system for disambiguatingnouns and verbs of a domain text.
Our method toacquire selectors from the Web for WSD was firstdescribed in (Schwartz and Gomez, 2008).
Thesystem is extended for the all-words domain taskby including part of speech tags from the StanfordParser (Klein and Manning, 2003).
Additionally, adomain adaptation technique of using domain pre-dominant senses (Koeling et al, 2005) is explored,but our primary goal is concerned with evaluatingthe performance of the existing Web Selectors sys-tem on domain text.In previous studies, the Web Selectors systemwas applied to text of a general domain.
However,the system was not directly tuned for the generaldomain.
The system may perform just as strongfor domain WSD since the selectors, which are thecore of disambiguation, can come from any do-main present on the Web.
In this paper, we studythe application of the Web Selectors WSD algo-rithm to an all-words task on a specific domain,the SemEval 2010: Task 17 (Agirre et al, 2010).2 Web SelectorsSelectors are words which take the place of a giventarget word within its local context (Lin, 1997).
Inthe case of acquiring selectors from the Web, wesearch with the text of local context (Schwartz andGomez, 2008).
For example, if one was search-ing for selectors of ?channel?
in the sentence, ?Thenavigation channel undergoes major shifts fromnorth to south banks?, then a search query wouldbe:The navigation * undergoes major shifts fromnorth to south banks .where * represents a wildcard to match every se-lector.
The query is shortened to produce moreresults until at least 300 selectors are acquired orthe query is less than 6 words.
The process ofacquiring selectors repeats for every content wordof the sentence.
Example selectors that might bereturned for ?channel?
include ?route?, ?pathway?,and ?passage?.Selectors serve for the system to essentiallylearn the areas or concepts of WordNet that thesense of a word should be similar or related.
Thetarget noun or verb is disambiguated by comparingits senses with all selectors for itself (target selec-tors), as well as with context selectors for othernouns, verbs, adjective, adverbs, proper nouns,and pronouns in the sentence.
Figure 1 shows theoverall process undertaken to rank the senses ofan ambiguous word.
A similarity measure is usedwhen comparing with target selectors and a relat-edness measure is used when comparing with con-text selectors.
Referring to our previous example,the senses of ?channel?
are compared to its own(target) selectors via similarity measures, whilerelatedness measures are used for the context se-lectors: noun selectors of ?navigation?, ?shifts?,?north?, ?south?, and ?banks?
; the verb selectors of392Figure 1: The overall process undertaken to disambiguate a word using Web selectors.?undergoes?
; plus the adjective selectors of ?ma-jor?.
Adverbs, proper nouns, and pronouns are notpresent in the sentence, and so no selectors fromthose parts of speech are considered.For this study, we implemented the Web Selec-tors system that was presented in (Schwartz andGomez, 2009).
This generalized version of thesystem may annotate verbs in addition to nouns,and it includes the previously unused context se-lectors of adverbs.
We used the path-based sim-ilarity measure of (Jiang and Conrath, 1997) fortarget selectors, and the gloss-based relatednessmeasure of (Banerjee and Pedersen, 2002) for con-text selectors.The incorporation of a part of speech tagger wasa necessary addition to the existing system.
Previ-ous evaluations of Web Selectors relied on the test-ing corpus to provide part of speech (POS) tagsfor content words.
In the case of SemEval-2010Task 17, words were only marked as targets, buttheir POS was not included.
We used the POStags from the Stanford Parser (Klein and Manning,2003).
We chose this system since the dependencyrelationship output was also useful for our domainadaptation (described in section 2.1).
A modifica-tion was made to the POS tags given the knowl-edge that the testing corpus only included nounsand verbs as targets.
Any target that was not ini-tially tagged as a noun or verb was reassigned asa noun, if the word existed as a noun in WordNet(Miller et al, 1993), or as a verb if not.2.1 Domain AdaptationOverall, the Web Selectors system is not explicitlytuned to the general domain.
Selectors themselvescan be from any domain.
However, sense taggeddata may be used indirectly within the system.First, the similarity and relatedness measures usedin the system may rely on SemCor data (Miller etal., 1994).
Also, the system breaks ties by choos-ing the most frequent sense according to WordNetfrequency data (based on SemCor).
These two as-pects of the system can be seen as tuned to thegeneral domain, and thus, they are likely aspectsof the system for adaptation to a specific domain.For this work, we focused on domain-adaptingthe tie breaker aspect of the Web Selectors sys-tem.
The system defines a tie occurring when mul-tiple sense choices are scored within 5% of the topsense choice.
In order to break the tie, the systemnormally chooses the most frequent sense amongthe tied senses.
However, it would be ideal tobreak the tie by choosing the most prevalent senseover the testing domain.
Because sense tagged do-main data is not typically available, Koeling et al(2005) presented the idea of estimating the mostfrequent sense of a domain by calculating senseprevalence scores from unannotated domain text.Several steps are taken to calculate the preva-lence scores.
First, a dependency database is cre-ated, listing the frequencies that each dependencyrelationship appears.
In our case, we used theStanford Parser (Klein and Manning, 2003) on thebackground data provided by the task organizers.From the dependency database, a thesaurus is cre-ated based on the method of (Lin, 1998).
In our ap-proach, we considered the following relationshipsfrom the dependency database:subject (agent, csubj, subjpass, nsubj, nsubjpass,xsubj)direct object (dobj)indirect object (iobj)393adjective modifier (amod)noun modifier (nn)prepositional modifier (any preposition, exclud-ing prep of and prep for)(typed dependency names listed in parenthesis)Finally, a prevalence score is calculated for eachsense of a noun or verb by finding the similaritybetween it and the top 50 most similar words ac-cording to the automatically created thesaurus.
AsKoeling et al did, we use the similarity measureof (Jiang and Conrath, 1997).3 Results and DiscussionThe results of our system are given in Table 1.
Thefirst set of results (WS) was a standard run of thesystem without any domain adaptation, while thesecond set (WSdom) was from a run including thedomain prevalence scores in order to break ties.The results show our domain adaptation techniquedid not lead to improved results.
Overall, WS re-sults came in ranked thirteenth among twenty-nineparticipating system results.We found that using the prevalence scores aloneto pick a sense (i.e.
the ?predominant sense?)
re-sulted in an F score of 0.514 (PS in Table 1).Koeling et al (2005) found the predominantsense to perform significantly better than the firstsense baseline (1sense: equivalent to most fre-quent sense for the English WordNet) on specificdomains (32% error reduction on a finance do-main, and 62% error reduction on a sports do-main).
Interestingly, there was no significant errorreduction over the 1sense for this task, implyingeither that the domain was more difficult to adaptto or that our implementation of the predominantsense algorithm was not as strong as that use byKoeling et al In any case, this lack of significanterror reduction over the 1sense may explain whyour WSdomresults were not stronger than the WSresults.
In WSdom, prevalence scores were usedinstead of 1sense to break ties.We computed a few figures to gain more in-sights on the system?s handling of domain data.Noun precision was 0.446 while verb precisionwas 0.449.
It was unexpected for verb disam-biguation results to be as strong as nouns becausea previous study using Web Selectors found nounsense disambiguation clearly stronger than verbsense disambiguation on a coarse-grained corpusP R F PnPvrand 0.23 0.23 0.231sense 0.505 0.505 0.505WS 0.447 0.441 0.444 .446 .449WSdom0.440 0.434 0.437 .441 .438PS 0.514 0.514 0.514 .53 .44Table 1: (P)recision, (R)ecall, and (F)-score ofvarious runs of the system on the Task 17 data.Pnand Pvcorrespond to precision results brokendown by nouns and verbs.Pen1Pen2Pen3WS 0.377 0.420 0.558WSdom0.384 0.415 0.531Table 2: Precision scores based on the three docu-ments of the English testing corpora (?en1?, ?en2?,and ?en3?).
(Schwartz and Gomez, 2009).
Ideally, our resultsfor noun disambiguation would have been strongerthan the the 1sense and PS results.
In order todetermine the effect of the POS tagger (parser inthis case) on the error, we determined 1.6% of theerror was due to the wrong POS tag at (0.9% ofall instances).
Lastly, Table 2 shows the precisionscores for each of the three documents from whichthe English testing corpus was created.
Withoutunderstanding the differences between the testingdocuments it is difficult to explain why the preci-sion varies, but the figures may be useful for com-parisons by others.Several aspects of the test data were unexpectedfor our system.
Some proper nouns were consid-ered as target words.
Our system was not orig-inally intended to annotate proper nouns, but wewere able to adjust it to treat them simply as nouns.To be sure this treatment was appropriate, we alsosubmitted results where proper nouns were ex-cluded, and got a precision of 0.437 and recallof 0.392.
One would expect the precision to in-crease at the expense of recall if the proper nounswere more problematic for the system than otherinstances.
This was not the case, and we concludeour handling of proper nouns was appropriate.Unfortunately, another unexpected aspect of thedata was not handled correctly by our system.
Oursystem only considered senses from one form ofthe target word according to WordNet, while thekey included multiple forms of a word.
For exam-ple, the key indicated low tide-1 was the answer to394an instance where our system had only consideredsenses of ?tide?.
We determined that for 10.2%of the instances that were incorrect in our WS re-sults we did not even consider the correct senseas a possible prediction due to using an inventoryfrom only one form of the word.
Since this issuemostly applied to nouns it may explain the obser-vation that the noun disambiguation performancewas not better than the verb disambiguation per-formance as was expected.4 ConclusionIn this paper we examined the application of theWeb Selectors WSD system to the SemEval-2010Task 17: All-words WSD on a Specific Domain.
Aprimary goal was to apply the pre-existing systemwith minimal changes.
To do this we incorporatedautomatic part of speech tags, which we foundonly had a small impact on the error (incorrectlytagged 0.9% of all target instances).
Overall, theresults showed the system to perform below the1sense baseline for both nouns and verbs.
This is alower relative performance than past studies whichfound the disambiguation performance above the1sense for nouns.
One reason for the lower nounperformance is that for 10.2 % of our errors, thesystem did not consider the correct sense choiceas a possibility.
Future versions of the system willneed to expand the sense inventory to include otherforms of a word (example: ?low tide?
when disam-biguating ?tide?
).Toward domain adaptation, we ran an exper-iment in which one aspect of our system wastuned to the domain by using domain prevalencescores (or ?predominant senses?).
We found no im-provement from using this adaptation technique,but we also discovered that results entirely basedon predictions of the domain predominant senseswere only minimally superior to 1sense (F-scoreof 0.514 versus 0.505 for 1sense).
Thus, futurestudies will examine better implementation of thepredominant sense algorithm, as well as exploreother complimentary techniques for domain adap-tation: customizing similarity measures for thedomain, or restricting areas of WordNet as sensechoices based on the domain.AcknowledgementThis research was supported by the NASAEngineering and Safety Center underGrant/Cooperative Agreement NNX08AJ98A.ReferencesEneko Agirre, Oier Lopez de Lacalle, Christiane Fell-baum, Shu kai Hsieh, Maurizio Tesconi, Mon-ica Monachini, Piek Vossen, and Roxanne Segers.2010.
Semeval-2010 task 17: All-words word sensedisambiguation on a specific domain.
In Proceed-ings of SemEval-2010.
Association for Computa-tional Linguistics.Satanjeev Banerjee and Ted Pedersen.
2002.
Anadapted lesk algorithm for word sense disambigua-tion using wordnet.
In Proceedings of the Third In-ternational Conference on Intelligent Text Process-ing and Computational Linguistics, Mexico City,Mexico.Jay J. Jiang and David W. Conrath.
1997.
Semanticsimilarity on corpus statistics and lexical taxonomy.In Proceedings of ROCLING X, Taiwan.Dan Klein and Christopher D. Manning.
2003.
Fastexact inference with a factored model for naturallanguage parsing.
In In Advances in Neural Infor-mation Processing Systems 15, pages 3?10.Rob Koeling, Diana McCarthy, and John Carroll.2005.
Domain-specific sense distributions and pre-dominant sense acquisition.
In Proceedings ofthe conference on Human Language Technologyand Experimental Methods in NLP, pages 419?426,Morristown, NJ, USA.Dekang Lin.
1997.
Using syntactic dependency as lo-cal context to resolve word sense ambiguity.
In Pro-ceedings of the 35th annual meeting on Associationfor Computational Linguistics, pages 64?71.Dekang Lin.
1998.
Automatic retrieval and cluster-ing of similar words.
In Proceedings of COLING-ACL 98, pages 768?774, Montreal, Canada.
MorganKaufmann.George Miller, R. Beckwith, Christiane Fellbaum,D.
Gross, and K. Miller.
1993.
Five papers on word-net.
Technical report, Princeton University.George A. Miller, Martin Chodorow, Shari L, ClaudiaLeacock, and Robert G. Thomas.
1994.
Using a se-mantic concordance for sense identification.
In InProc.
of ARPA Human Language Technology Work-shop.Hansen A. Schwartz and Fernando Gomez.
2008.
Ac-quiring knowledge from the web to be used as se-lectors for noun sense disambiguation.
In CoNLL2008: Proceedings of the Twelfth Conference onComputational Natural Language Learning, pages105?112, Manchester, England, August.Hansen A. Schwartz and Fernando Gomez.
2009.Using web selectors for the disambiguation of allwords.
In Proceedings of the NAACL-2009 Work-shop on Semantic Evaluations: Recent Achieve-ments and Future Directions, pages 28?36, Boulder,Colorado, June.395
