Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
658?666, Prague, June 2007. c?2007 Association for Computational LinguisticsMavenRank: Identifying Influential Members of the US Senate UsingLexical CentralityAnthony FaderUniversity of Michiganafader@umich.eduDragomir RadevUniversity of Michiganradev@umich.eduMichael H. CrespinThe University of Georgiacrespin@uga.eduBurt L. MonroeThe Pennsylvania State Universityburtmonroe@psu.eduKevin M. QuinnHarvard Universitykevin quinn@harvard.eduMichael ColaresiMichigan State Universitycolaresi@msu.eduAbstractWe introduce a technique for identifying themost salient participants in a discussion.
Ourmethod, MavenRank is based on lexical cen-trality: a random walk is performed on agraph in which each node is a participant inthe discussion and an edge links two partici-pants who use similar rhetoric.
As a test, weused MavenRank to identify the most influ-ential members of the US Senate using datafrom the US Congressional Record and usedcommittee ranking to evaluate the output.Our results show that MavenRank scores arelargely driven by committee status in mosttopics, but can capture speaker centrality intopics where speeches are used to indicateideological position instead of influence leg-islation.1 IntroductionIn a conversation or debate between a group ofpeople, we can think of two remarks as interact-ing if they are both comments on the same topic.For example, if one speaker says ?taxes shouldbe lowered to help business,?
while another argues?taxes should be raised to support our schools,?
thespeeches are interacting with each other by describ-ing the same issue.
In a debate with many peoplearguing about many different things, we could imag-ine a large network of speeches interacting with eachother in the same way.
If we associate each speechin the network with its speaker, we can try to iden-tify the most important people in the debate basedon how central their speeches are in the network.To describe this type of centrality, we borrow aterm from The Tipping Point (Gladwell, 2002), inwhich Gladwell describes a certain type of person-ality in a social network called a maven.
A mavenis a trusted expert in a specific field who influencesother people by passing information and advice.
Inthis paper, our goal is to identify authoritative speak-ers who control the spread of ideas within a topic.
Todo this, we introduce MavenRank, which measuresthe centrality of speeches as nodes in the type of net-work described in the previous paragraph.Significant research has been done in the areaof identifying central nodes in a network.
Vari-ous methods exist for measuring centrality, includ-ing degree centrality, closeness, betweenness (Free-man, 1977; Newman, 2003), and eigenvector cen-trality.
Eigenvector centrality in particular hasbeen successfully applied to many different typesof networks, including hyperlinked web pages (Brinand Page, 1998; Kleinberg, 1998), lexical net-works (Erkan and Radev, 2004; Mihalcea and Ta-rau, 2004; Kurland and Lee, 2005; Kurland andLee, 2006), and semantic networks (Mihalcea et al,2004).
The authors of (Lin and Kan, 2007) extendedthese methods to include timestamped graphs wherenodes are added over time and applied it to multi-document summarization.
In (Tong and Faloutsos,2006), the authors use random walks on a graph asa method for finding a subgraph that best connectssome or all of a set of query nodes.
In our paper,we introduce a new application of eigenvector cen-trality for identifying the central speakers in the typeof debate or conversation network described above.Our method is based on the one described in (Erkan658and Radev, 2004) and (Mihalcea and Tarau, 2004),but modified to rank speakers instead of documentsor sentences.In our paper, we apply our method to analyze theUS Congressional Record, which is a verbatim tran-script of speeches given in the United States Houseof Representatives and Senate.
The Record is adense corpus of speeches made by a large numberof people over a long period of time.
Using the tran-scripts of political speeches adds an extra layer ofmeaning onto the measure of speaker centrality.
Thecentrality of speakers in Congress can be thought ofas a measure of relative importance or influence inthe US legislative process.
We can also use speakercentrality to analyze committee membership: are thecentral speakers on a given issue ranking membersof a related committee?
Is there a type of impor-tance captured through speaker centrality that isn?tobvious in the natural committee rankings?There has been growing interest in using tech-niques from natural language processing in the areaof political science.
In (Porter et al, 2005) theauthors performed a network analysis of membersand committees of the US House of Representatives.They found connections between certain commit-tees and political positions that suggest that com-mittee membership is not determined at random.In (Thomas et al, 2006), the authors use the tran-scripts of debates from the US Congress to auto-matically classify speeches as supporting or oppos-ing a given topic by taking advantage of the vot-ing records of the speakers.
In (Wang et al, 2005),the authors use a generative model to simultane-ously discover groups of voters and topics usingthe voting records and the text from bills of theUS Senate and the United Nations.
The authorsof (Quinn et al, 2006) introduce a multinomial mix-ture model to perform unsupervised clustering ofCongressional speech documents into topically re-lated categories.
We rely on the output of this modelto cluster the speeches from the Record in order tocompare speaker rankings within a topic to relatedcommittees.We take advantage of the natural measures ofprestige in Senate committees and use them as astandard for comparison with MavenRank.
Our hy-pothesis is that MavenRank centrality will capturethe importance of speakers based on the naturalcommittee rankings and seniority.
We can test thisclaim by clustering speeches into topics and thenmapping the topics to related committees.
If the hy-pothesis is correct, then the speaker centrality shouldbe correlated with the natural committee rankings.There have been other attempts to link floor par-ticipation with topics in political science.
In (Hall,1996), the author found that serving on a commit-tee can positively predict participation in Congress,but that seniority was not a good predictor.
Hismeasure only looked at six bills in three commit-tees, so his method is by far not as comprehensiveas the one that we present here.
Our approach withMavenRank differs from previous work by provid-ing a large scale analysis of speaker centrality andbringing natural language processing techniques tothe realm of political science.2 Data2.1 The US Congressional Speech CorpusThe text used in the experiments is from the UnitedStates Congressional Speech corpus (Monroe etal., 2006), which is an XML formatted version ofthe electronic United States Congressional Recordfrom the Library of Congress1.
The CongressionalRecord is a verbatim transcript of the speeches madein the US House of Representatives and Senate be-ginning with the 101st Congress in 1998 and in-cludes tens of thousands of speeches per year.
Inour experiments we focused on the records from the105th and 106th Senates.
The basic unit of the USCongressional Speech corpus is a record, which cor-responds to a single subsection of the print versionof the Congressional Record and may contain zeroor more speakers.
Each paragraph of text withina record is tagged as either speech or non-speechand each paragraph of speech text is tagged with theunique id of the speaker.
Figure 1 shows an examplerecord file for the sixth record on July 14th, 1997 inthe 105th Senate.In our experiments we use a smaller unit of anal-ysis called a speech document by taking all of thetext of a speaker within a single record.
The cap-italization and punctuation is then removed fromthe text as in (Monroe et al, 2006) and then the1http://thomas.loc.gov659text stemmed using Porter?s Snowball II stemmer2.Figure 1 shows an example speech document forspeaker 15703 (Herb Kohl of Wisconsin) that hasbeen generated from the record in Figure 1.In addition to speech documents, we also usespeaker documents.
A speaker document is theconcatenation of all of a speaker?s speech docu-ments within a single session and topic (so a sin-gle speaker may have multiple speaker documentsacross topics).
For example within the 105th Senatein topic 1 (?Judicial Nominations?
), Senator Kohlhas four speech documents, so the speaker documentattributed to him within this session and topic wouldbe the text of these four documents treated as a sin-gle unit.
The order of the concatenation does notmatter since we will look at it as a vector of weightedterm frequencies (see Section 3.2).2.2 Topic ClustersWe used the direct output of the 42-topic model ofthe 105th-108th Senates from (Quinn et al, 2006)to further divide the speech documents into topicclusters.
In their paper, they use a model where theprobabilities of a document belonging to a certaintopic varies smoothly over time and the words withina given document have exactly the same probabil-ity of being drawn from a particular topic.
Thesetwo properties make the model different than stan-dard mixture models (McLachlan and Peel, 2000)and the latent Dirichlet alocation model of (Blei etal., 2003).
The model of (Quinn et al, 2006) is mostclosely related to the model of (Blei and Lafferty,2006), who present a generalization of the modelused by (Quinn et al, 2006).
Table 1 lists the 42topics and their related committees.The output from the topic model is a D ?
42 ma-trix Z where D is the number of speech documentsand the element zdk represents the probability of thedth speech document being generated by topic k.We clustered the speech documents by assigning aspeech document d to the kth cluster wherek = argmaxjzdj .If the maximum value is not unique, we arbitrarilyassign d to the lowest numbered cluster where zdj is2http://snowball.tartarus.org/algorithms/english/stemmer.htmla maximum.
A typical topic cluster contains severalhundred speech documents, while some of the largertopic clusters contain several thousand.2.3 Committee Membership InformationThe committee membership information that weused in the experiments is from Stewart andWoon?s committee assignment codebook (Stewartand Woon, 2005).
This provided us with a rosterfor each committee and rank and seniority informa-tion for each member.
In our experiments we usethe rank within party and committee seniority mem-ber attributes to test the output of our pipeline.
Therank within party attribute orders the members of acommittee based on the Resolution that appointedthe members with the highest ranking members hav-ing the lowest number.
The chair and ranking mem-bers always receive a rank of 1 within their party.
Acommittee member?s committee seniority attributecorresponds to the number of years that the memberhas served on the given committee.2.4 Mapping Topics to CommitteesIn order to test our hypothesis that lexical centralityis correlated with the natural committee rankings,we needed a map from topics to related commit-tees.
We based our mapping on Senate Rule XXV,3which defines the committees, and the descriptionson committee home pages.
Table 1 shows the map,where a topic?s related committees are listed in ital-ics below the topic name.
Because we are matchingshort topic names to the complex descriptions givenby Rule XXV, the topic-committee map is not oneto one or even particularly well defined: some top-ics are mapped to multiple committees, some top-ics are not mapped to any committees, and two dif-ferent topics may be mapped to the same commit-tee.
This is not a major problem because even if aone to one map between topics and committees ex-isted, speakers from outside a topic?s related com-mittee are free to participate in the topic simply bygiving a speech.
Therefore there is no way to rankall speakers in a topic using committee information.To test our hypotheses, we focused our attention ontopics that have at least one related committee.
InSection 4.3 we describe how the MavenRank scores3http://rules.senate.gov/senaterules/rule25.php660<?xml version="1.0" standalone="no"?><!DOCTYPE RECORD SYSTEM "record.dtd"><RECORD><HEADER><CHAMBER>Senate</CHAMBER><TITLE>NOMINATION OF JOEL KLEIN TO BE ASSISTANT ATTORNEYGENERAL IN CHARGE OF THE ANTITRUST DIVISION </TITLE><DATE>19970714</DATE></HEADER><BODY><GRAF><PAGEREF></PAGEREF><SPEAKER>NULL</SPEAKER><NONSPEECH>NOMINATION OF JOEL KLEIN TO BE ASSISTANTATTORNEY GENERAL IN CHARGE OF THE ANTITRUST DIVISION(Senate - July 14, 1997)</NONSPEECH></GRAF><GRAF><PAGEREF>S7413</PAGEREF><SPEAKER>15703</SPEAKER><SPEECH> Mr. President, as the ranking Democrat on theAntitrust Subcommittee, let me tell you why I support Mr.Klein?s nomination, why he is a good choice for the job,and why we ought to confirm him today.</SPEECH></GRAF>.
.
.<GRAF><PAGEREF>S7414</PAGEREF><SPEAKER>UNK1</SPEAKER><SPEECH> Without objection, it is so ordered.
</SPEECH></GRAF></BODY></RECORD>mr presid a the rank democrat on the antitrust subcommittelet me tell you why i support mr klein nomin why he i agood choic for the job and why we ought to confirm himtodaifirst joel klein i an accomplish lawyer with a distinguishcareer he graduat from columbia univers and harvard lawschool and clerk for the u court of appeal here inwashington then for justic powel just a importantli he ithe presid choic to head the antitrust divis and i believthat ani presid democrat or republican i entitl to a strongpresumpt in favor of hi execut branch nomine second joelklein i a pragmatist not an idealogu hi answer at hi confirmhear suggest that he i not antibusi a some would claim theantitrust divis wa in the late 1970 nor anticonsum a someargu the divis wa dure the 1980 instead he will plot a middlcours i believ that promot free market fair competit andconsum welfarthe third reason we should confirm joel klein i becaus no ondeserv to linger in thi type of legisl limbo here in congresswe need the input of a confirm head of the antitrust divisto give u the administr view on a varieti of import policimatter defens consolid electr deregul and telecommun mergeramong other we need someon who can speak with author for thedivis without a cloud hang over hi headmore than that without a confirm leader moral at theantitrust divis i suffer and given the pace at which thepresid ha nomin and the senat ha confirm appointe if we failto approv mr klein it will be at least a year befor we confirma replac mayb longer and mayb never so we need to act now wecan?t afford to let the antitrust divis continu to driftfinal mr presid i have great respect for the senat from southcarolina a well a the senat from nebraska and north dakotathei have been forc advoc for consum on telecommun matter and.
.
.Figure 1: A sample of the text from record 105.sen.19970714.006.xml and the speech document for SenatorHerb Kohl of Wisconsin (id 15703) generated from it.
The ?.
.
.
?
represents omitted text.1 Judicial Nominations 15 Health 2 (Economics - Seniors) 27 Procedural 1 (Housekeeping 1)Judiciary Health, Education, Labor, and Pensions 28 Procedural 2 (Housekeeping 2)2 Law & Crime 1 (Violence / Drugs) Veterans?
Affairs 29 Campaign FinanceJudiciary Agriculture, Nutrition, and Forestry Rules and Administration3 Banking / Finance Aging (Special Committee) 30 Law & Crime 2 (Federal)Banking, Housing, and Urban Affairs Finance Judiciary4 Armed Forces 1 (Manpower) 16 Gordon Smith re Hate Crime 31 Child ProtectionArmed Services 17 Debt / Deficit / Social Security Health, Education, Labor, and Pensions5 Armed Forces 2 (Infrastructure) Appropriations Agriculture, Nutrition, and ForestryArmed Services Budget 32 Labor 1 (Workers, esp.
Retirement)6 Symbolic (Tribute - Living) Finance Health, Education, Labor, and Pensions7 Symbolic (Congratulations - Sports) Aging (Special Committee) Aging (Special Committee)8 Energy 18 Supreme Court / Constitutional Small Business and EntrepreneurshipEnergy and Natural Resources Judiciary 33 Environment 2 (Regulation)9 Defense (Use of Force) 19 Commercial Infrastructure Environment and Public WorksArmed Services Commerce, Science, and Transportation Agriculture, Nutrition, and ForestryHomeland Security and Governmental Affairs 20 Symbolic (Remembrance - Military) Energy and Natural ResourcesIntelligence (Select Committee) 21 International Affairs (Diplomacy) 34 Procedural 3 (Legislation 1)10 Jesse Helms re Debt Foreign Relations 35 Procedural 4 (Legislation 2)11 Environment 1 (Public Lands) 22 Abortion 36 Procedural 5 (Housekeeping 3)Energy and Natural Resources Judiciary 37 Procedural 6 (Housekeeping 4)Agriculture, Nutrition, and Forestry Health, Education, Labor, and Pensions 38 Taxes12 Health 1 (Medical) 23 Symbolic (Tribute - Constituent) FinanceHealth, Education, Labor, and Pensions 24 Agriculture 39 Symbolic (Remembrance - Nonmilitary)13 International Affairs (Arms Control) Agriculture, Nutrition, and Forestry 40 Labor 2 (Employment)Foreign Relations 25 Intelligence Health, Education, Labor, and Pensions14 Social Welfare Intelligence (Select Committee) Small Business and EntrepreneurshipAgriculture, Nutrition, and Forestry Homeland Security and Governmental Affairs 41 Foreign TradeBanking, Housing, and Urban Affairs 26 Health 3 (Economics - General) FinanceHealth, Education, Labor, and Pensions Health, Education, Labor, and Pensions Banking, Housing, and Urban AffairsFinance Finance 42 EducationHealth, Education, Labor, and PensionsTable 1: The numbers and names of the 42 topics from (Quinn et al, 2006) with our mappings to relatedcommittees (listed below the topic name, if available).661of speakers who are not members of related commit-tees were taken into account when we measured therank correlations.3 MavenRank and Lexical SimilarityThe following sections describe MavenRank, a mea-sure of speaker centrality, and tf-idf cosine similar-ity, which is used to measure the lexical similarity ofspeeches.3.1 MavenRankMavenRank is a graph-based method for findingspeaker centrality.
It is similar to the methodsin (Erkan and Radev, 2004; Mihalcea and Tarau,2004; Kurland and Lee, 2005), which can be usedfor ranking sentences in extractive summaries anddocuments in an information retrieval system.
Givena collection of speeches s1, .
.
.
, sN and a measureof lexical similarity between pairs sim(si, sj) ?
0,a similarity graph can be constructed.
The nodesof the graph represent the speeches and a weightedsimilarity edge is placed between pairs that exceeda similarity threshold smin.
MavenRank is based onthe premise that important speakers will have cen-tral speeches in the graph, and that central speechesshould be similar to other central speeches.
A recur-sive explanation of this concept is that the score ofa speech should be proportional to the scores of itssimilar neighbors.Given a speech s in the graph, we can express therecursive definition of its score p(s) asp(s) =?t?adj[s]p(t)wdeg(t)(1)where adj[s] is the set of all speeches adjacent tos and wdeg(t) =?u?adj[t] sim(t, u), the weighteddegree of t. Equation (1) captures the idea that theMavenRank score of a speech is distributed to itsneighbors.
We can rewrite this using matrix notationasp = pB (2)where p = (p(s1), p(s2), .
.
.
, p(sN )) and the ma-trixB is the row normalized similarity matrix of thegraphB(i, j) =S(i, j)?k S(i, k)(3)where S(i, j) = sim(si, sj).
Equation (2) showsthat the vector of MavenRank scores p is the lefteigenvector of B with eigenvalue 1.We can prove that the eigenvector p exists by us-ing a techinque from (Page et al, 1999).
We cantreat the matrix B as a Markov chain describingthe transition probabilities of a random walk on thespeech similarity graph.
The vector p then repre-sents the stationary distribution of the random walk.It is possible that some parts of the graph are dis-connected or that the walk gets trapped in a com-ponent.
These problems are solved by reservinga small escape probability at each node that repre-sents a chance of jumping to any node in the graph,making the Markov chain irreducible and aperiodic,which guarantees the existence of the eigenvector.Assuming a uniform escape probability for eachnode on the graph, we can rewrite Equation (2) asp = p[dU+ (1?
d)B] (4)where U is a square matrix with U(i, j) = 1/Nfor all i and j, N is the number of nodes, andd is the escape probability chosen in the interval[0.1, 0.2] (Brin and Page, 1998).
Equation (4) isknown as PageRank (Page et al, 1999) and is usedfor determining prestige on the web in the Googlesearch engine.3.2 Lexical SimilarityIn our experiments, we used tf-idf cosine similarityto measure lexical similarity between speech docu-ments.
We represent each speech document as a vec-tor of term frequencies (or tf), which are weightedaccording to the relative importance of the giventerm in the cluster.
The terms are weighted by theirinverse document frequency or idf.
The idf of a termw is given by (Sparck-Jones, 1972)idf(w) = log(Nnw)(5)where N is the number of documents in the corpusand nw is the number of documents in the corpuscontaining the term w. It follows that very commonwords like ?of?
or ?the?
have a very low idf, whilethe idf values of rare words are higher.
In our experi-ments, we calculated the idf values for each topic us-ing all speech documents across sessions within the6622030405060708090100Abortion ChildProtection Education Workers,RetirementSantorumBoxerKennedyFigure 2: MavenRank percentiles for three speakersover four topics.given topic.
We calculated topic-specific idf valuesbecause some words may be relatively unimportantin one topic, but important in another.
For example,in topic 22 (?Abortion?
), the idf of the term ?abort?is near 0.20, while in topic 38 (?Taxes?
), its idf isnear 7.18.The tf-idf cosine similarity measuretf-idf-cosine(u, v) is defined asPw?u,v tfu(w) tfv(w) idf(w)2?Pw?u(tfu(w) idf(w))2?Pw?v(tfv(w) idf(w))2, (6)which is the cosine of the angle between the tf-idfvectors.There are other alternatives to tf-idf cosine sim-ilarity.
Some other possible similarity measuresare document edit distance, the language modelsfrom (Kurland and Lee, 2005), or generation proba-bilities from (Erkan, 2006).
For simplicity, we onlyused tf-idf similarities in our experiments, but any ofthese measures could be used in this case.4 Experiments and Results4.1 DataWe used the topic clusters from the 105th Senateas training data to adjust the parameter smin andobserve trends in the data.
We did not run experi-ments to test the effect of different values of smin onMavenRank scores, but our chosen value of 0.25 hasshown to give acceptable results in similar experi-ments (Erkan and Radev, 2004).
We used the topicclusters from the 106th Senate as test data.
For thespeech document networks, there was an average of351 nodes (speech documents) and 2142 edges pertopic.
For the speaker document networks, there wasan average of 63 nodes (speakers) and 545 edges pertopic.4.2 Experimental SetupWe set up a pipeline using a Perl implementationof tf-idf cosine similarity and MavenRank.
We ranMavenRank on the topic clusters and ranked thespeakers based on the output.
We used two differenttypes granularities of the graphs as input: one wherethe nodes are speech documents and another wherethe nodes are speaker documents (see Section 2.1).For the speech document graph, a speaker?s score isdetermined by the sum of the MavenRank scores ofthe speeches given by that speaker.4.3 Evaluation MethodsTo evaluate our output, we estimate independentordinary least squares linear regression models ofMavenRank centrality for topics with at least one re-lated committee (there are 29 total):MavenRankik = ?0k + ?skSeniorityik ++?rkRankingMemberjk + ik (7)where i indexes Senators, k indexes topics,Seniorityik is the number of years Senator i hasserved on the relevant committee for topic k (valuezero for those not on a relevant committee) andRankingMemberjk has the value of one only forthe Chair and ranking minority member of a rele-vant committee.
We are interested primarily in theoverall significance of the estimated model (indicat-ing committee effects) and, secondarily, in the spe-cific source of any committee effect in seniority orcommittee rank.4.4 ResultsTable 2 summarizes the results.
?Maven?
status onmost topics does appear to be driven by committeestatus, as expected.
There are particularly strong ef-fects of seniority and rank in topics tied to the Judi-ciary, Foreign Relations, and Armed Services com-mittees, as well as legislation-rich areas of domesticpolicy.
Perhaps of greater interest are the topics thatdo not have committee effects.
These are of threedistinct types.
The first are highly politicized top-ics for which speeches are intended not to influence663Topic p(F )a p(?s > 0)b p(?r > 0)c Topic p(F ) p(?s > 0) p(?r > 0)Seniority and Ranking Status Both Significant Seniority and Ranking Status Jointly Significant2 Law & Crime 1 [Violent] < .001 0.016 < .001 26 Health 3 [Economics] 0.001 0.106 0.06418 Constitutional < .001 0.003 < .001 32 Labor 1 [Workers] 0.007 0.156 0.18133 Environment 2 [Regulation] 0.007 0.063 0.056Seniority Significant 3 Banking / Finance 0.042 0.141 0.57912 Health 1 [Medical] < .001 < .001 0.56742 Education < .001 < .001 0.337 No Significant Effects of Committee Status41 Trade < .001 < .001 0.087 11 Environment 1 [Public Lands] 0.104 0.102 0.56521 Int?l Affairs [Nonmilitary] < .001 0.007 0.338 22 Abortion 0.419 0.609 0.2529 Defense [Use of Force] 0.002 0.001 0.926 5 Armed Forces 2 [Infrastructure] 0.479 0.267 0.91919 Commercial Infrastructure 0.007 0.032 0.332 24 Agriculture 0.496 0.643 0.42540 Labor 2 [Employment] 0.029 0.010 0.114 17 Debt / Social Security 0.502 0.905 0.29538 Taxes 0.037 0.033 0.895 15 Health 2 [Seniors] 0.706 0.502 0.92225 Intelligence 0.735 0.489 0.834Ranking Status Significant 29 Campaign Finance 0.814 0.748 0.56030 Crime 2 [Federal] < .001 0.334 < .001 31 Child Protection 0.856 0.580 0.7188 Energy < .001 0.145 < .0011 Judicial Nominations < .001 0.668 < .00114 Social Welfare < .001 0.072 0.00513 Int?l Affairs [Arms] < .001 0.759 0.0014 Armed Forces 1 [Manpower] 0.007 0.180 0.049aF-test for joint significance of committee variables.bT-test for significance of committee seniority.cT-test for significance of chair or ranking member status.Table 2: Significance tests for ordinary least squares (OLS) linear regressions ofMavenRank scores (Speech-documents graph) on committee seniority (in years) and ranking status (chair or ranking member), 106thSenate, topic-by-topic.
Results for the speaker-documents graph are similar.legislation as much as indicate an ideological or par-tisan position, so the mavens are not on particularcommittees (abortion, children, seniors, the econ-omy).
The second are ?distributive politics?
topicswhere many Senators speak to defend state or re-gional interests, so debate is broadly distributed andthere are no clear mavens (agriculture, military baseclosures, public lands).
Third are topics where thereare not enough speeches for clear results, becausemost debate occurred after 1999-2000 (post-9/11intelligence reform, McCain-Feingold campaign fi-nance reform).Alternative models, using measures of centralitybased on the centroid were also examined.
Dis-tance to centroid provides broadly similar results asMavenRank, with several marginal significance re-sults reversed in each direction.
Cosine similaritywith centroid, on the other hand, appears to have norelationship with committee structure.Figure 2 shows the MavenRank percentiles (us-ing the speech document network) for Senators RickSantorum, Barbara Boxer, and Edward Kennedyacross a few topics in the 106th Senate.
Thesesample scores conform to the expected rankings forthese speakers.
In this session, Santorum was thesponsor of a bill to ban partial birth abortions andwas a spokesman for Social Security reform, whichsupport his high ranking in abortion and work-ers/retirement.
Boxer acted as the lead oppositionto Santorum?s abortion bill and is known for hersupport of child abuse laws.
Kennedy was rankingmember of the Health, Education, Labor, and Pen-sions committee and the Judiciary committee (whichwas involved with the abortion bill).4.5 MavenRank in Other ContextsMavenRank is a general method for finding centralspeakers in a discussion and can be applied to areasoutside of political science.
One potential applica-tion would be analyzing blog posts to find ?Maven?bloggers by treating blogs as speakers and posts asspeeches.
Similarly, MavenRank could be used tofind central participants in a newsgroup, a forum, ora collection of email conversations.5 ConclusionWe have presented a technique for identifying lexi-cally central speakers using a graph based methodcalled MavenRank.
To test our method for find-ing central speakers, we analyzed the Congressional664Record by creating a map from the clusters ofspeeches to Senate committees and comparing thenatural ranking committee members to the output ofMavenRank.
We found evidence of a possible rela-tionship between the lexical centrality and commit-tee rank of a speaker by ranking the speeches us-ing MavenRank and computing the rank correlationwith the natural ordering of speakers.
Some spe-cific committees disagreed with our hypothesis thatMavenRank and committee position are correlated,which we propose is because of the non-legislativeaspects of those specific committees.
The resultsof our experiment suggest that MavenRank can in-deed be used to find central speakers in a corpus ofspeeches.We are currently working on applying our meth-ods to the US House of Representatives and otherrecords of parliamentary speech from the UnitedKingdom and Australia.
We have also developed adynamic version of MavenRank that takes time intoaccount when finding lexical centrality and plan onusing it with the various parliamentary records.
Weare interested in dynamic MavenRank to go furtherwith the idea of tracking how ideas get propagatedthrough a network of debates, including congres-sional records, blogs, and newsgroups.AcknowledgmentsThis paper is based upon work supported bythe National Science Foundation under Grant No.0527513, ?DHB: The dynamics of Political Rep-resentation and Political Rhetoric?.
Any opinions,findings, and conclusions or recommendations ex-pressed in this paper are those of the authors and donot necessarily reflect the views of the National Sci-ence Foundation.ReferencesDavid Blei and John Lafferty.
2006.
Dynamic topicmodels.
In Machine Learning: Proceedings of theTwenty-Third International Conference (ICML).David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Sergey Brin and Lawrence Page.
1998.
The anatomy ofa large-scale hypertextual Web search engine.
Com-puter Networks and ISDN Systems, 30(1?7):107?117.Gu?nes?
Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based centrality as salience in text summa-rization.
Journal of Artificial Intelligence Research(JAIR).Gunes Erkan.
2006.
Language model-based documentclustering using random walks.
In Proceedings ofthe Human Language Technology Conference of theNAACL, Main Conference, pages 479?486, New YorkCity, USA, June.
Association for Computational Lin-guistics.L.
C. Freeman.
1977.
A set of measures of central-ity based on betweenness.
Sociometry, 40(1):35?41,March.Malcolm Gladwell.
2002.
The Tipping Point: How LittleThings Can Make a Big Difference.
Back Bay Books,January.Richard L. Hall.
1996.
Participation in Congress.
YaleUniversity Press.Jon M. Kleinberg.
1998.
Authoritative sources in a hy-perlinked environment.
In Proceedings of the 9th An-nual ACM-SIAM Symposium on Discrete Algorithms,pages 668?677.Oren Kurland and Lillian Lee.
2005.
PageRank withouthyperlinks: Structural re-ranking using links inducedby language models.
In Proceedings of SIGIR, pages306?313.Oren Kurland and Lillian Lee.
2006.
Respect my author-ity!
HITS without hyperlinks, utilizing cluster-basedlanguage models.
In Proceedings of SIGIR, pages 83?90.Ziheng Lin and Min-Yen Kan. 2007.
Timestampedgraphs: Evolutionary models of text for multi-document summarization.
In Proceedings of the Sec-ond Workshop on TextGraphs: Graph-Based Algo-rithms for Natural Language Processing, pages 25?32,Rochester, NY, USA.
Association for ComputationalLinguistics.Geoffrey McLachlan and David Peel.
2000.
Finite Mix-ture Models.
New York: Wiley.Rada Mihalcea and Paul Tarau.
2004.
TextRank: Bring-ing order into texts.
In Proceedings of the Ninth Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP ?04).Rada Mihalcea, Paul Tarau, and Elizabeth Figa.
2004.Pagerank on semantic networks, with application toword sense disambiguation.
In Proceedings of theTwentieth International Conference on ComputationalLinguistics (COLING ?04), pages 1126?1132.665Burt L. Monroe, Cheryl L. Monroe, Kevin M. Quinn,Dragomir Radev, Michael H. Crespin, Michael P. Co-laresi, Anthony Fader, Jacob Balazer, and Steven P.Abney.
2006.
United states congressional speech cor-pus.
Department of Political Science, The Pennsylva-nia State University.Mark E. J. Newman.
2003.
A measure of betweennesscentrality based on random walks.
Technical Reportcond-mat/0309045, Arxiv.org.Lawrence Page, Sergey Brin, Rajeev Motwani, and TerryWinograd.
1999.
The PageRank citation ranking:Bringing order to the Web.
Technical Report 1999-66,Stanford Digital Library Technologies Project, Stan-ford University, November 11,.Mason A. Porter, Peter J. Mucha, M. E. J. Newman, andCasey M. Warmbrand.
2005.
A network analysis ofcommittees in the u.s. house of representatives.
PNAS,102(20), May.Kevin M. Quinn, Burt L. Monroe, Michael Colaresi,Michael H. Crespin, and Dragomir R. Radev.
2006.An automated method of topic-coding legislativespeech over time with application to the 105th-108thU.S.
senate.
In Midwest Political Science AssociationMeeting.K.
Sparck-Jones.
1972.
A statistical interpretation ofterm specificity and its application in retrieval.
Jour-nal of Documentation, 28(1):11?20.Charles Stewart and Jonathan Woon.
2005.
Con-gressional committee assignments, 103rd to 105thcongresses, 1993?1998: Senate, july 12, 2005.http://web.mit.edu/17.251/www/data_page.html.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Getout the vote: Determining support or opposition fromCongressional floor-debate transcripts.
In Proceed-ings of EMNLP, pages 327?335.Hanghang Tong and Christos Faloutsos.
2006.
Center-piece subgraphs: problem definition and fast solutions.In Tina Eliassi-Rad, Lyle H. Ungar, Mark Craven, andDimitrios Gunopulos, editors, KDD, pages 404?413.ACM.Xuerui Wang, Natasha Mohanty, and Andrew McCallum.2005.
Group and topic discovery from relations andtheir attributes.
In NIPS.666
