Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 172?181,Columbus, June 2008. c?2008 Association for Computational LinguisticsAn Evaluation Understudy for Dialogue Coherence ModelsSudeep Gandhe and David TraumInstitute for Creative TechnologiesUniversity of Southern California13274 Fiji way, Marina del Rey, CA, 90292{gandhe,traum}@ict.usc.eduAbstractEvaluating a dialogue system is seen as amajor challenge within the dialogue researchcommunity.
Due to the very nature of the task,most of the evaluation methods need a sub-stantial amount of human involvement.
Fol-lowing the tradition in machine translation,summarization and discourse coherence mod-eling, we introduce the the idea of evaluationunderstudy for dialogue coherence models.Following (Lapata, 2006), we use the infor-mation ordering task as a testbed for evaluat-ing dialogue coherence models.
This paper re-ports findings about the reliability of the infor-mation ordering task as applied to dialogues.We find that simple n-gram co-occurrencestatistics similar in spirit to BLEU (Papineniet al, 2001) correlate very well with humanjudgments for dialogue coherence.1 IntroductionIn computer science or any other research field, sim-ply building a system that accomplishes a certaingoal is not enough.
It needs to be thoroughly eval-uated.
One might want to evaluate the system justto see to what degree the goal is being accomplishedor to compare two or more systems with one another.Evaluation can also lead to understanding the short-comings of the system and the reasons for these.
Fi-nally the evaluation results can be used as feedbackin improving the system.The best way to evaluate a novel algorithm or amodel for a system that is designed to aid humansin processing natural language would be to employit in a real system and allow users to interact with it.The data collected by this process can then be usedfor evaluation.
Sometimes this data needs furtheranalysis - which may include annotations, collect-ing subjective judgments from humans, etc.
Sincehuman judgments tend to vary, we may need to em-ploy multiple judges.
These are some of the reasonswhy evaluation is time consuming, costly and some-times prohibitively expensive.Furthermore, if the system being developed con-tains a machine learning component, the problem ofcostly evaluation becomes even more serious.
Ma-chine learning components often optimize certainfree parameters by using evaluation results on held-out data or by using n-fold cross-validation.
Eval-uation results can also help with feature selection.This need for repeated evaluation can forbid the useof data-driven machine learning components.For these reasons, using an automatic evalua-tion measure as an understudy is quickly becominga common practice in natural language processingtasks.
The general idea is to find an automatic eval-uation metric that correlates very well with humanjudgments.
This allows developers to use the auto-matic metric as a stand-in for human evaluation.
Al-though it cannot replace the finesse of human evalu-ation, it can provide a crude idea of progress whichcan later be validated.
e.g.
BLEU (Papineni et al,2001) for machine translation, ROUGE (Lin, 2004)for summarization.Recently, the discourse coherence modeling com-munity has started using the information orderingtask as a testbed to test their discourse coherencemodels (Barzilay and Lapata, 2005; Soricut andMarcu, 2006).
Lapata (2006) has proposed an au-172tomatic evaluation measure for the information or-dering task.
We propose to use the same task as atestbed for dialogue coherence modeling.
We evalu-ate the reliability of the information ordering task asapplied to dialogues and propose an evaluation un-derstudy for dialogue coherence models.In the next section, we look at related work inevaluation of dialogue systems.
Section 3 sum-marizes the information ordering task and Lap-ata?s (2006) findings.
It is followed by the detailsof the experiments we carried out and our observa-tions.
We conclude with a summary future work di-rections.2 Related WorkMost of the work on evaluating dialogue systems fo-cuses on human-machine communication geared to-wards a specific task.
A variety of evaluation met-rics can be reported for such task-oriented dialoguesystems.
Dialogue systems can be judged basedon the performance of their components like WERfor ASR (Jurafsky and Martin, 2000), concept er-ror rate or F-scores for NLU, understandability forspeech synthesis etc.
Usually the core component,the dialogue model - which is responsible for keep-ing track of the dialogue progression and comingup with an appropriate response, is evaluated indi-rectly.
Different dialogue models can be comparedwith each other by keeping the rest of componentsfixed and then by comparing the dialogue systemsas a whole.
Dialogue systems can report subjectivemeasures such as user satisfaction scores and per-ceived task completion.
SASSI (Hone and Graham,2000) prescribes a set of questions used for elicit-ing such subjective assessments.
The objective eval-uation metrics can include dialogue efficiency andquality measures.PARADISE (Walker et al, 2000) was an attemptat reducing the human involvement in evaluation.
Itbuilds a predictive model for user satisfaction as alinear combination of some objective measures andperceived task completion.
Even then the systemneeds to train on the data gathered from user sur-veys and objective features retrieved from logs of di-alogue runs.
It still needs to run the actual dialoguesystem and collect objective features and perceivedtask completeion to predict user satisfaction.Other efforts in saving human involvement inevaluation include using simulated users for test-ing (Eckert et al, 1997).
This has become a popu-lar tool for systems employing reinforcement learn-ing (Levin et al, 1997; Williams and Young, 2006).Some of the methods involved in user simulationare as complex as building dialogue systems them-selves (Schatzmann et al, 2007).
User simulationsalso need to be evaluated as how closely they modelhuman behavior (Georgila et al, 2006) or as howgood a predictor they are of dialogue system perfor-mance (Williams, 2007).Some researchers have proposed metrics for eval-uating a dialogue model in a task-oriented system.
(Henderson et al, 2005) used the number of slots ina frame filled and/or confirmed.
Roque et al (2006)proposed hand-annotating information-states in a di-alogue to evaluate the accuracy of information stateupdates.
Such measures make assumptions aboutthe underlying dialogue model being used (e.g.,form-based or information-state based etc.
).We are more interested in evaluating types of di-alogue systems that do not follow these task-basedassumptions: systems designed to imitate human-human conversations.
Such dialogue systems canrange from chatbots like Alice (Wallace, 2003),Eliza (Weizenbaum, 1966) to virtual humans usedin simulation training (Traum et al, 2005).
Forsuch systems, the notion of task completion or ef-ficiency is not well defined and task specific objec-tive measures are hardly suitable.
Most evaluationsreport the subjective evaluations for appropriatenessof responses.
Traum et.
al.
(2004) propose a cod-ing scheme for response appropriateness and scoringfunctions for those categories.
Gandhe et.
al.
(2006)propose a scale for subjective assessment for appro-priateness.3 Information OrderingThe information ordering task consists of choos-ing a presentation sequence for a set of informationbearing elements.
This task is well suited for text-to-text generation like in single or multi-documentsummarization (Barzilay et al, 2002).
Recentlythere has been a lot of work in discourse coher-ence modeling (Lapata, 2003; Barzilay and Lap-ata, 2005; Soricut and Marcu, 2006) that has used173information ordering to test the coherence mod-els.
The information-bearing elements here are sen-tences rather than high-level concepts.
This frees themodels from having to depend on a hard to get train-ing corpus which has been hand-authored for con-cepts.Most of the dialogue models still work at thehigher abstraction level of dialogue acts and inten-tions.
But with an increasing number of dialoguesystems finding use in non-traditional applicationssuch as simulation training, games, etc.
; there is aneed for dialogue models which do not depend onhand-authored corpora or rules.
Recently Gandheand Traum (2007) proposed dialogue models thatdo not need annotations for dialogue-acts, seman-tics and hand-authored rules for information stateupdates or finite state machines.Such dialogue models focus primarily on gener-ating an appropriate coherent response given the di-alogue history.
In certain cases the generation ofa response can be reduced to selection from a setof available responses.
For such dialogue models,maintaining the information state can be consideredas a secondary goal.
The element that is commonto the information ordering task and the task of se-lecting next most appropriate response is the abilityto express a preference for one sequence of dialogueturns over the other.
We propose to use the informa-tion ordering task to test dialogue coherence models.Here the information bearing units will be dialogueturns.1There are certain advantages offered by using in-formation ordering as a task to evaluate dialogue co-herence models.
First the task does not require adialogue model to take part in conversations in aninteractive manner.
This obviates the need for hav-ing real users engaging in the dialogue with the sys-tem.
Secondly, the task is agnostic about the under-lying dialogue model.
It can be a data-driven statis-tical model or information-state based, form basedor even a reinforcement learning system based onMDP or POMDP.
Third, there are simple objectivemeasures available to evaluate the success of infor-mation ordering task.Recently, Purandare and Litman (2008) have used1These can also be at the utterance level, but for this paperwe will use dialogue turns.this task for modeling dialogue coherence.
But theyonly allow for a binary classification of sequencesas either coherent or incoherent.
For comparing dif-ferent dialogue coherence models, we need the abil-ity for finer distinction between sequences of infor-mation being put together.
Lapata (2003) proposedKendall?s ?
, a rank correlation measure, as one suchcandidate.
In a recent study they show that Kendall?s?
correlates well with human judgment (Lapata,2006).
They show that human judges can reliablyprovide coherence ratings for various permutationsof text.
(Pearson?s correlation for inter-rater agree-ment is 0.56) and that Kendall?s ?
is a good in-dicator for human judgment (Pearson?s correlationfor Kendall?s ?
with human judgment is 0.45 (p <0.01)).Before adapting the information ordering task fordialogues, certain questions need to be answered.We need to validate that humans can reliably per-form the task of information ordering and can judgethe coherence for different sequences of dialogueturns.
We also need to find which objective mea-sures (like Kendall?s ? )
correlate well with humanjudgments.4 Evaluating Information OrderingOne of the advantages of using information order-ing as a testbed is that there are objective measuresavailable to evaluate the performance of informationordering task.
Kendall?s ?
(Kendall, 1938), a rankcorrelation coefficient, is one such measure.
Givena reference sequence of length n, Kendall?s ?
for anobserved sequence can be defined as,?
= # concordant pairs ?
# discordant pairs# total pairsEach pair of elements in the observed sequenceis marked either as concordant - appearing in thesame order as in reference sequence or as discor-dant otherwise.
The total number of pairs is Cn2 =n(n?
1)/2.
?
ranges from -1 to 1.Another possible measure can be defined as thefraction of n-grams from reference sequence, thatare preserved in the observed sequence.bn = # n-grams preserved# total n-gramsIn this study we have used, b2, fraction of bigramsand b3, fraction of trigrams preserved from the ref-erence sequence.
These values range from 0 to 1.Table 1 gives examples of observed sequences and174Observed Sequence b2 b3 ?
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 1.00 1.00 1.00[8, 9, 0, 1, 2, 3, 4, 5, 6, 7] 0.89 0.75 0.29[4, 1, 0, 3, 2, 5, 8, 7, 6, 9] 0.00 0.00 0.60[6, 9, 8, 5, 4, 7, 0, 3, 2, 1] 0.00 0.00 -0.64[2, 3, 0, 1, 4, 5, 8, 9, 6, 7] 0.56 0.00 0.64Table 1: Examples of observed sequences and their re-spective b2, b3 & ?
values.
Here the reference sequenceis [0,1,2,3,4,5,6,7,8,9].respective b2, b3 and ?
values.
Notice how ?
al-lows for long-distance relationships whereas b2, b3are sensitive to local features only.
25 Experimental SetupFor our experiments we used segments drawn from 9dialogues.
These dialogues were two-party human-human dialogues.
To ensure applicability of ourresults over different types of dialogue, we chosethese 9 dialogues from different sources.
Three ofthese were excerpts from role-play dialogues involv-ing negotiations which were originally collected fora simulation training scenario (Traum et al, 2005).Three are from SRI?s Amex Travel Agent data whichare task-oriented dialogues about air travel plan-ning (Bratt et al, 1995).
The rest of the dialogues arescripts from popular television shows.
Fig 6 showsan example from the air-travel domain.
Each excerptdrawn was 10 turns long with turns strictly alternat-ing between the two speakers.Following the experimental design of (Lapata,2006) we created random permutations for these di-alogue segments.
We constrained our permutationsso that the permutations always start with the samespeaker as the original dialogue and turns strictly al-ternate between the speakers.
With these constraintsthere are still 5!?
5!
= 14400 possible permutationsper dialogue.
We selected 3 random permutationsfor each of the 9 dialogues.
In all, we have a totalof 27 dialogue permutations.
They are arranged in 3sets, each set containing a permutation for all 9 di-alogues.
We ensured that not all permutations in agiven set are particularly very good or very bad.
Weused Kendall?s ?
to balance the permutations across2For more on the relationship between b2, b3 and ?
see row3,4 of table 1 and figure 4.the given set as well as across the given dialogue.Unlike Lapata (2006) who chose to remove thepronouns and discourse connectives, we decided notdo any pre-processing on the text like removingdisfluencies or removing cohesive devices such asanaphora, ellipsis, discourse connectives, etc.
Oneof the reason is such pre-processing if done manu-ally defeats the purpose of removing humans fromthe evaluation procedure.
Moreover it is very diffi-cult to remove certain cohesive devices such as dis-course deixis without affecting the coherence levelof the original dialogues.6 Experiment 1In our first experiment, we divided a total of 9 hu-man judges among the 3 sets (3 judges per set).
Eachjudge was presented with 9 dialogue permutations.They were asked to assign a single coherence rat-ing for each dialogue permutation.
The ratings wereon a scale of 1 to 7, with 1 being very incoherentand 7 being perfectly coherent.
We did not provideany additional instructions or examples of scale aswe wanted to capture the intuitive idea of coherencefrom our judges.
Within each set the dialogue per-mutations were presented in random order.We compute the inter-rater agreement by usingPearson?s correlation analysis.
We correlate the rat-ings given by each judge with the average ratingsgiven by the judges who were assigned the same set.For inter-rater agreement we report the average of 9such correlations which is 0.73 (std dev = 0.07).
Art-stein and Poesio (2008) have argued that Krippen-dorff?s ?
(Krippendorff, 2004) can be used for inter-rater agreement with interval scales like the one wehave.
In our case for the three sets ?
values were0.49, 0.58, 0.64.
These moderate values of alpha in-dicate that the task of judging coherence is indeed adifficult task, especially when detailed instructionsor examples of scales are not given.In order to assess whether Kendall?s ?
can be usedas an automatic measure of dialogue coherence, weperform a correlation analysis of ?
values againstthe average ratings by human judges.
The Pearson?scorrelation coefficient is 0.35 and it is statisticallynot significant (P=0.07).
Fig 1(a) shows the rela-tionship between coherence judgments and ?
val-ues.
This experiment fails to support the suitability175(a) Kendall?s ?
does not correlate well with humanjudgments for dialogue coherence.
(b) Fraction of bigram & trigram counts correlate wellwith human judgments for dialogue coherence.Figure 1: Experiment 1 - single coherence rating per permutationof Kendall?s ?
as an evaluation understudy.We also analyzed the correlation of human judg-ments against simple n-gram statistics, specifically(b2 + b3) /2.
Fig 1(b) shows the relationship be-tween human judgments and the average of fractionof bigrams and fraction of trigrams that were pre-served in the permutation.
The Pearson?s correlationcoefficient is 0.62 and it is statistically significant(P<0.01).7 Experiment 2Since human judges found it relatively hard to as-sign a single rating to a dialogue permutation, wedecided to repeat experiment 1 with some modifica-tions.
In our second experiment we asked the judgesto provide coherence ratings at every turn, based onthe dialogue that preceded that turn.
The dialoguepermutations were presented to the judges through aweb interface in an incremental fashion turn by turnas they rated each turn for coherence (see Fig 5 inthe appendix for the screenshot of this interface).
Weused a scale from 1 to 5 with 1 being completely in-coherent and 5 as perfectly coherent.
3 A total of 11judges participated in this experiment with the firstset being judged by 5 judges and the remaining twosets by 3 judges each.3We believe this is a less complex task than experiment 1and hence a narrower scale is used.For the rest of the analysis, we use the averagecoherence rating from all turns as a coherence rat-ing for the dialogue permutation.
We performedthe inter-rater agreement analysis as in experiment1.
The average of 11 correlations is 0.83 (std dev =0.09).
Although the correlation has improved, Krip-pendorff?s ?
values for the three sets are 0.49, 0.35,0.63.
This shows that coherence rating is still a hardtask even when judged turn by turn.We assessed the relationship between the aver-age coherence rating for dialogue permutations withKendall?s ?
(see Fig 2(a)).
The Pearson?s correlationcoefficient is 0.33 and is statistically not significant(P=0.09).Fig 2(b) shows high correlation of average coher-ence ratings with the fraction of bigrams and tri-grams that were preserved in permutation.
The Pear-son?s correlation coefficient is 0.75 and is statisti-cally significant (P<0.01).Results of both experiments suggest that,(b2 + b3) /2 correlates very well with human judg-ments and can be used for evaluating informationordering when applied to dialogues.8 Experiment 3We wanted to know whether information ordering asapplied to dialogues is a valid task or not.
In this ex-periment we seek to establish a higher baseline for176(a) Kendall?s ?
does not correlate well with humanjudgments for dialogue coherence.
(b) Fraction of bigram & trigram counts correlate wellwith human judgments for dialogue coherence.Figure 2: Experiment 2 - turn-by-turn coherence ratingthe task of information ordering in dialogues.
Wepresented the dialogue permutations to our humanjudges and asked them to reorder the turns so thatthe resulting order is as coherent as possible.
All 11judges who participated in experiment 2 also partic-ipated in this experiment.
They were presented witha drag and drop interface over the web that allowedthem to reorder the dialogue permutations.
The re-ordering was constrained to keep the first speakerof the reordering same as that of the original di-alogue and the re-orderings must have strictly al-ternating turns.
We computed the Kendall?s ?
andfraction of bigrams and trigrams (b2 + b3) /2 forthese re-orderings.
There were a total of 11 ?
9= 99 reordered dialogue permutations.
Fig 3(a)and 3(b) shows the frequency distribution of ?
and(b2 + b3) /2 values respectively.Humans achieve high values for the reorderingtask.
For Kendall?s ?
, the mean of the reordered dia-logues is 0.82 (std dev = 0.25) and for (b2 + b3) /2,the mean is 0.71 (std dev = 0.28).
These values es-tablish an upper baseline for the information order-ing task.
These can be compared against the randombaseline.
For ?
random performance is 0.02 4 and4Theoretically this should be zero.
The slight positive biasis the result of the constraints imposed on the re-orderings -like only allowing the permutations that have the correct startingspeaker.for (b2 + b3) /2 it is 0.11.
59 DiscussionResults show that (b2 + b3) /2 correlates well withhuman judgments for dialogue coherence better thanKendall?s ?
.
?
encodes long distance relationshipsin orderings where as (b2 + b3) /2 only looks at lo-cal context.
Fig 4 shows the relationship betweenthese two measures.
Notice that most of the order-ings have ?
values around zero (i.e.
in the middlerange for ?
), whereas majority of orderings will havea low value for (b2 + b3) /2.
?
seems to overesti-mate the coherence even in the absence of immedi-ate local coherence (See third entry in table 1).
Itseems that local context is more important for dia-logues than for discourse, which may follow fromthe fact that dialogues are produced by two speakerswho must react to each other, while discourse can beplanned by one speaker from the beginning.
Traumand Allen (1994) point out that such social obliga-tions to respond and address the contributions of theother should be an important factor in building dia-logue systems.The information ordering paradigm does not takeinto account the content of the information-bearingitems, e.g.
the fact that turns like ?yes?, ?I agree?,5This value is calculated by considering all 14400 permuta-tions as equally likely.177(a) Histogram of Kendall?s ?
for reordered se-quences(b) Histogram of fraction of bigrams & tri-grams values for reordered sequencesFigure 3: Experiment 3 - upper baseline for information ordering task (human performance)?okay?
perform the same function and should betreated as replaceable.
This may suggest a need tomodify some of the objective measures to evaluatethe information ordering specially for dialogue sys-tems that involve more of such utterances.Human judges can find the optimal sequenceswith relatively high frequency, at least for shortdialogues.
It remains to be seen how this varieswith longer dialogue lengths which may containsub-dialogues that can be arranged independently ofeach other.10 Conclusion & Future WorkEvaluating dialogue systems has always been a ma-jor challenge in dialogue systems research.
The corecomponent of dialogue systems, the dialogue model,has usually been only indirectly evaluated.
Suchevaluations involve too much human effort and are abottleneck for the use of data-driven machine learn-ing models for dialogue coherence.
The informationordering task, widely used in discourse coherencemodeling, can be adopted as a testbed for evaluatingdialogue coherence models as well.
Here we haveshown that simple n-gram statistics that are sensi-tive to local features correlate well with human judg-ments for coherence and can be used as an evalua-tion understudy for dialogue coherence models.
Aswith any evaluation understudy, one must be carefulwhile using it as the correlation with human judg-ments is not perfect and may be inaccurate in somecases ?
it can not completely replace the need forfull evaluation with human judges in all cases (see(Callison-Burch et al, 2006) for a critique of BLUEalong these lines).In the future, we would like to perform more ex-periments with larger data sets and different typesof dialogues.
It will also be interesting to see therole cohesive devices play in coherence ratings.
Wewould like to see if there are any other measures orcertain modifications to the current ones that corre-late better with human judgments.
We also plan toemploy this evaluation metric as feedback in build-ing dialogue coherence models as is done in ma-chine translation (Och, 2003).AcknowledgmentsThe effort described here has been sponsored by the U.S. ArmyResearch, Development, and Engineering Command (RDE-COM).
Statements and opinions expressed do not necessarilyreflect the position or the policy of the United States Govern-ment, and no official endorsement should be inferred.
We wouldlike to thank Radu Soricut, Ron Artstein, and the anonymousSIGdial reviewers for helpful comments.ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
In To appearin Computational Linguistics.Regina Barzilay and Mirella Lapata.
2005.
Modelinglocal coherence: An entity-based approach.
In Proc.ACL-05.178Regina Barzilay, Noemie Elhadad, and Kathleen McKe-own.
2002.
Inferring strategies for sentence orderingin multidocument summarization.
JAIR, 17:35?55.Harry Bratt, John Dowding, and Kate Hunicke-Smith.1995.
The sri telephone-based atis system.
In Pro-ceedings of the Spoken Language Systems TechnologyWorkshop, January.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
In proceedings of EACL-2006.Wieland Eckert, Esther Levin, and Roberto Pieraccini.1997.
User modeling for spoken dialogue system eval-uation.
In Automatic Speech Recognition and Under-standing, pages 80?87, Dec.Sudeep Gandhe and David Traum.
2007.
Creating spo-ken dialogue characters from corpora without annota-tions.
In Proceedings of Interspeech-07.Sudeep Gandhe, Andrew Gordon, and David Traum.2006.
Improving question-answering with linking di-alogues.
In International Conference on IntelligentUser Interfaces (IUI), January.Kalliroi Georgila, James Henderson, and Oliver Lemon.2006.
User simulation for spoken dialogue systems:Learning and evaluation.
In proceedings of Inter-speech.James Henderson, Oliver Lemon, and Kallirroi Georgila.2005.
Hybrid reinforcement/supervised learning fordialogue policies from communicator data.
In pro-ceedings of IJCAI workshop.Kate S. Hone and Robert Graham.
2000.
Towards a toolfor the subjective assessment of speech system inter-faces (SASSI).
Natural Language Engineering: Spe-cial Issue on Best Practice in Spoken Dialogue Sys-tems.Daniel Jurafsky and James H. Martin.
2000.
SPEECHand LANGUAGE PROCESSING: An Introduction toNatural Language Processing, Computational Lin-guistics, and Speech Recognition.
Prentice-Hall.Maurice G. Kendall.
1938.
A new measure of rank cor-relation.
Biometrika, 30:81?93.Klaus Krippendorff.
2004.
Content Analysis, An Intro-duction to Its Methodology 2nd Edition.
Sage Publi-cations.Mirella Lapata.
2003.
Probabilistic text structuring: Ex-periments with sentence ordering.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics, Sapporo, Japan.Mirella Lapata.
2006.
Automatic evaluation of informa-tion ordering.
Computational Linguistics, 32(4):471?484.Esther Levin, Roberto Pieraccini, and Wieland Eckert.1997.
Learning dialogue strategies within the markovdecision process framework.
In Automatic SpeechRecognition and Understanding, pages 72?79, Dec.Chin-Yew Lin.
2004.
ROUGE: a package for automaticevaluation of summaries.
In Proceedings of the Work-shop on Text Summarization Branches Out.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In In ACL 2003: Proc.of the 41st Annual Meeting of the Association for Com-putational Linguistics, July.Kishore A. Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2001.
BLEU: a method for automaticevaluation of machine translation.
In Technical Re-port RC22176 (W0109-022), IBM Research Division,September.Amruta Purandare and Diane Litman.
2008.
Analyz-ing dialog coherence using transition patterns in lexi-cal and semantic features.
In Proceedings 21st Inter-national FLAIRS Conference, May.Antonio Roque, Hua Ai, and David Traum.
2006.
Evalu-ation of an information state-based dialogue manager.In Brandial 2006: The 10th Workshop on the Seman-tics and Pragmatics of Dialogue.Jost Schatzmann, Blaise Thomson, Karl Weilhammer,Hui Ye, and Steve Young.
2007.
Agenda-based usersimulation for bootstrapping a pomdp dialogue sys-tem.
In proceedings of HLT/NAACL, Rochester, NY.Radu Soricut and Daniel Marcu.
2006.
Discourse gener-ation using utility-trained coherence models.
In Proc.ACL-06.David R. Traum and James F. Allen.
1994.
Discourseobligations in dialogue processing.
In proceedings ofthe 32nd Annual Meeting of the Association for Com-putational Linguistics (ACL-94), pages 1?8.David R. Traum, Susan Robinson, and Jens Stephan.2004.
Evaluation of multi-party virtual reality dia-logue interaction.
In In Proceedings of Fourth Interna-tional Conference on Language Resources and Evalu-ation (LREC), pages 1699?1702.David Traum, William Swartout, Jonathan Gratch, andStacy Marsella.
2005.
Virtual humans for non-teaminteraction training.
In AAMAS-05 Workshop on Cre-ating Bonds with Humanoids, July.M.
Walker, C. Kamm, and D. Litman.
2000.
Towards de-veloping general models of usability with PARADISE.Natural Language Engineering: Special Issue on BestPractice in Spoken Dialogue Systems.Richard Wallace.
2003.
Be Your Own Botmaster, 2ndEdition.
ALICE A. I. Foundation.Joseph Weizenbaum.
1966.
Eliza?a computer programfor the study of natural language communication be-tween man and machine.
Communications of theACM, 9(1):36?45, January.Jason D. Williams and Steve Young.
2006.
Partially ob-servable markov decision processes for spoken dialogsystems.
Computer Speech and Language, 21:393?422.Jason D. Williams.
2007.
A method for evaluating andcomparing user simulations: The cramer-von mises di-vergence.
In IEEE Workshop on Automatic SpeechRecognition and Understanding (ASRU).179Appendix(a) (b) (c)Figure 4: Distributions for Kendall?s ?
, (b2 + b3) /2 and the relationship between them for all possible dialoguepermutations with 10 turns and earlier mentioned constraints.Figure 5: Screenshot of the interface used for collecting coherence rating for dialogue permutations.180Agent AAA at American Express may I help you?User yeah this is BBB BBB I need to make some travel arrangementsAgent ok and what do you need to do?User ok on June sixth from San Jose to Denver, UnitedAgent leaving at what time?User I believe there?s one leaving at eleven o?clock in the morningAgent leaves at eleven a.m. and arrives Denver at two twenty p.m. out of San JoseUser okAgent yeah that?s United flight four seventyUser that?s the oneDoctor hello i?m doctor perezhow can i help youCaptain uh well i?m with uh the locali?m i?m the commander of the local companyand uh i?d like to talk to you about some options you have for relocating your clinicDoctor uh we?re not uh planning to relocate the clinic captainwhat uh what is this aboutCaptain well have you noticed that there?s been an awful lot of fighting in the area recentlyDoctor yes yes i havewe?re very busywe?ve had many more casual+ casualties many more patients than than uh usual in thelast monthbut uh what what is this about relocating our clinichave have uh you been instructed to move usCaptain nobut uh we just have some concerns about the increase in fighting xxDoctor are you suggesting that we relocate the clinicbecause we had no planswe uh we uh we?re located here and we?ve been uhwe are located where the patients need usCaptain yeah butyeah actually it is a suggestion that you would be a lot safer if you moved away fromthis areawe can put you in an area where there?s n+ no insurgentsand we have the area completely under control with our troopsDoctor i see captainis this a is this a suggestion from your commanderCaptain i?m uh the company commanderFigure 6: Examples of the dialogues used to elicit human judgments for coherence181
