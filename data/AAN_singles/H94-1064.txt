The LIMSI Continuous Speech Dictation SystemtJ.L.
Gauvain, L.F. Lamel, G. Adda, M. Adda-DeckerLIMSI-CNRS,  BP 13391403 Orsay cedex, FRANCE{gauvain, lamel,adda,madda}@limsi.frABSTRACTA major axis of research at LIMSI is directed at multilingual,speaker-independent, large vocabulary speech dictation.
In this pa-per the LIMSI recognizer which was evaluated in the ARPA NOV93CSR test is described, and experimental results on the WSJ andBREF corpora under closely matched conditions are reported.
Forboth corpora word recognition expenrnents were carried out withvocabularies containing up to 20k words.
The recognizer makesuse of continuous density HMM with Gaussian mixture for acous-tic modeling and n-gram statistics estimated on the newspaper textsfor language modeling.
The recognizer uses a time-synchronousgraph-search strategy which is shown to still be viable with a 20k-word vocabulary when used with bigram back-off language models.A second forward pass, which makes use of a word graph generatedwith the bigram, incorporates a trigram language model.
Acousticmodeling uses cepstrum-based f atures, context-dependent phonemodels (intra and interword), phone duration models, and sex-dependent models.INTRODUCTIONSpeech recognition research at LIMSI aims to develop rec-ognizers that are task-, speaker-, and vocabulary-independentso as to be easily adapted to a variety of applications.
Theapplicability of speech recognition techniques used for onelanguage to other languages is of particular importance inEurope.
The multilingual aspects are in part carried out inthe context of the LRE SQALE (Speech recognizer Qual-ity Assessment for Linguistic Engineering) project, whichis aimed at assessing language dependent issues in multilin-gual recognizer evaluation.
In this project, the same systemwill be evaluated on comparable tasks in different languages(English, French and German) to determine cross-lingual dif-ferences, and different recognizers will be compared on thesame language to compare advantages of different recogni-tion strategies.In this paper some of the primary issues in large vocabu-lary, speaker-independent, continuous speech recognition fordictation are addressed.
These issues include language mod-eling, acoustic modeling, lexical representation, and search.Acoustic modeling makes use of continuous density HMMwith Gaussian mixture of context-dependent phone models.For language modeling n-gram statistics are estimated ontThis work is partially funded by the LRE project 62-058 SQALE.319text material.
To deal with phonological variability alter-nate pronunciations are included in the lexicon, and optionalphonological rules are applied during training and recogni-tion.
The recognizer uses a time-synchronous graph-searchstrategy\[16\] for a first pass with a bigram back-off languagemodel (LM)\[10\].
A trigram LM is used in a second acousticdecoding pass which makes use of the word graph generatedusing the bigram LM\[6\].
Experimental results are reportedon the ARPA Wall Street Journal (WSJ)\[19\] and BREF\[14\]corpora, using for both corpora over 37k utterances for acous-tic training and more than 37 million words of newspaper textfor language model training.
While the number of speakersis larger for WSJ, the total amount of acoustic training ma-terial is about the same (see Table 1).
It is shown that forboth corpora increasing the amount of training utterances byan order of magnitude reduces the word error by about 30%.The use of a trigram LM in a second pass also gives an errorreduction of 20% to 30%.
The combined error reduction ison the order of 50%.LANGUAGE MODEL INGLanguage modeling entails incorporating constraints onthe allowable sequences of words which form a sentence.Statistical n-gram models attempt o capture the syntacticand semantic onstraints by estimating the frequencies ofsequences of n words.
In this work bigram and trigram lan-guage models are estimated on the training text material foreach corpus.
This data consists of 37M words of the WSJ 1and 38M words of Le Monde.
A backoff mechanism\[10\]is used to smooth the estimates of the probabilities of raren-grams by relying on a lower order n-gram when there isinsufficient training data, and to provide a means of model-ing unobserved n-grams.
Another advantage of the backoffmechanism is that LM size can be arbitrarily reduced by re-lying more on the backoff, by increasing the minimum num-ber of required n-gram observations needed to include then-gram.
This property can be used in the first bigram decod-1While we have built n-gram-backoff LMs directly from the 37M-wordstandardized WSJ training text material, inthese xperiments all results arereported using the 5k or 20k, bigram and tfigram backoff LMs provided byLincoln Labs\[ 19\] as required by ARPA so as to be compatible with the othersites participating in the tests.ing pass to reduce computational requirements.
The trigramlangage model is used in the second pass of the decodingprocess:.In order to be able to constnact LMs for BREF, it wasnecessary to normalize the text material of Le Monde newpa-per, which entailed apre-treatment rather different from thatused to normalize the WSJ texts\[19\].
The main differencesare in the treatment of compound words, abbreviations, andcase.
In BREF the distinction between the cases is kept ifit designates a distinctive graphemic feature, but not whenthe upper case is simply due to the fact that the word oc-curs at the beginning of the sentence.
Thus, the first wordof each sentence was semi-automatically verified to deter-mine if a transformation to lower case was needed.
Specialtreatment is also needed for the symbols hyphen (-), quote('), and period (.)
which can lead to ambiguous separations.For example, the hyphen in compound words like beaux-artsand au-dessus i considered word-internal.
Alternatively thehyphen may be associated with the first word as in ex-, oranti-, or with the second word as in -Id or -nL Finally, it mayappear in the text even though it is not associated with anyword.
The quote can have two different separations: it canbe word internal (aujourd' hui, o'Donnel, hors-d'oeuvre), ormay be part of the first word (l'aml).
Similarly the periodmay be part of a word, for instance, L.A., sec.
(secondes), p.(page), or simply an end-of-sentence mark.Table 1 compares ome characteristics of the WSJ and LeMonde text corpora.
In the same size training texts, thereare almost 60% more distinct words for Le Monde than forWSJ without aking case into account.
2 As a consequence,the lexical coverage for a given size lexicon is smaller forLe Monde than for WSJ.
For example, the 20k WSJ lexiconaccounts for 97.5% of word occurrences, but the 20k BREFlexicon only covers 94.9% of word occurrences in the trainingtexts.
For lexicons in the range of 5k to 40k words, thenumber of words must be doubled for Le Monde in order toobtain the same word coverage as for WSJ.The lexical ambiguity is also higher for French than forEnglish.
The homophone rate (the number of words whichhave a homophone divided by the total number of words) inthe 20k BREF lexicon is 57% compared to 9% in 20k-openWSJ lexicon.
This effect is even greater if the word fre-quencies are taken into account.
Given a perfect phonemictranscription, 23% of words in the WSJ training texts is am-biguous, whereas 75% of the words in the Le Monde trainingtexts have an ambiguous phonemic transcription.
Not onlydoes one phonemic form correspond to different orthographicforms, there can also be a relatively large number of possi-ble pronunciations for a given word.
In French, the alternatepronunciations arise mainly from optional word-final phones,due to liaison and optional word-final consonant cluster e-2If case is kept when distinctive, there are 280k words in the Le Mondetraining material.Corpus \]\[ WSJ Le Monde# training speakers 284 80# training utterances 37.5k 38.5kTraining text size 37.2M 37.7M#distinct words 165k 259k (280)5k coverage 90.6% 85.5% (85.2)20k coverage 97.5% 94.9% (94.7)Homophone rate 20k lexicon 9% 57%Homophone rate 20k text 23% 75%Monophone words (2Ok) 3% 17%Table 1: Comparison of WSJ and BREF corpora.ruction (see Figure 1).
There are also a larger number offrequent, monophone words for Le Monde than for WSJ, ac-counting for about 17% and 3% of all word occurrences inthe respective training texts.ACOUSTIC-PHONETIC  MODEL INGThe recognizer makes use of continuous density HMM(CDHMM) with Gaussian mixture for acoustic modeling.The main advantage continuous density modeling offersover discrete or semi-continuous (or tied-mixture) observa-tion density modeling is that the number of parameters usedto model an HMM observation distribution can easily beadapted to the amount of available training data associatedto this state.
As a consequence, high precision modeling canbe achieved for highly frequented states without he explicitneed of smoothing techniques for the densities of less fre-quented states.
Discrete and semi-continuous modeling usea fixed number of parameters torepresent a given observationdensity and therefore cannot achieve high precision withoutthe use of smoothing techniques.
This problem can be alle-viated by tying some states of the Markov models in orderto have more training data to estimate ach state distribution.However, since this kind of tying requires careful design andsome a priori assumptions, these techniques are primarily ofinterest when the training data is limited and cannot easily beincreased.
In the experimental section we demonstrate heimprovement in performance obtained on the same test databy simply using additional training material.A 48-component feature vector is computed every 10 ms.This feature vector consists of 16 Bark-frequency scale cep-strum coefficients computed on the 8kHz bandwidth and theirfirst and second order derivatives.
For each frame (30 mswindow), a 15 channel Bark power spectrum is obtained byapplying triangular windows to the DFT output.
The cep-strum coefficients are then computed using a cosinus trans-form \[2\].The acoustic models are sets of context-dependent(CD),position independent phone models, which include bothintra-word and cross-word contexts.
The contexts are au-tomatically selected based on their frequencies in the train-ing data.
The models include tfiphone models, fight- and320left-context phone models, and context-independent phonemodels.
Each phone model is a left-to-right CDHMM withGaussian mixture observation densities (typically 32 com-ponents).
The covariance matrices of all the Gaussians arediagonal.
Duration is modeled with a gamma distributionper phone model.
The HMM and duration parameters are es-timated separately and combined in the recognition processfor the Viterbi search.
Maximum a postedori estimators areused for the HMM parameters\[8\] and moment estimators forthe gamma distributions.
Separate male and female modelsare used to more accurately model the speech data.Dunng system development phone recognition has beenused to evaluate different acoustic model sets.
It has beenshown that improvements in phone accuracy are directly in-dicative of improvements in word accuracy when the samephone models are used for recognition\[12\].
Phone recog-nition provides the added benefit hat the recognized phonestring can be used to understand word recognition errors andproblems in the lexical representation.LEX ICAL  REPRESENTATIONLexicons containing 5k, 20k, and 64k words have beenused in these experiments.
The lexicons are representedphonemically, using language-specific sets of phonemes.Each lexicon has alternate pronunciations for some of thewords, and allows some of the phones to be optional) Apronunciation graph is generated for each word from thebaseform transcription to which word internal phonologicalrules are optionally applied during training and recognitionto account for some of the phonological variations observedin fluent speech.
The WSJ lexicons are represented usinga set of 46 phonemes, including 21 vowels, 24 consonants,and silence.
Training and test lexicons were created at LIMSIand include some input from modified versions of the TIMIT,Pocket and Moby lexicons.
Missing forms were generatedby rule when possible, or added by hand.
Some pronoun-ciations for proper names were kindly provided by MurraySpiegel at Bellcore from the Orator system.
The BREF lexi-cons, corresponding to the 5k and 20k most common wordsin the Le Monde texts are represented with 35 phonemes in-cluding 14 vowels, 20 consonants, and silence\[3\].
The basepronunciations, obtained using text-to-phoneme rules\[20\],were extended to annotate potential liaisons and pronunci-ation variants.
Some example lexical entries are given inFigure 1.Word boundary phonological rules are applied in build-ing the phone graph used by the recognizer so as to allowfor some of the phonological variations observed in fluentspeech\[11\].
The principle behind the phonological rules isto modify the phone network to take into account such vari-3About 10% of the lexical entries have multiple transcriptions, if theword final optional phonemes marking possible liaisons for BREF are notincluded.
Including these raises the number of entries with multiple tran-scriptions to almost 40%.Example ntries for WSJ:INTEREST In t r I s t  In{t}XIstEXCUSE Ekskyu  \[sz\]CORP. kcrp  kcrpXeSxnGAMBLING g@mb \[ L1 \] I Gph.rtdeAREA \[@e\] r ix  --~ \[@e\] r iyxExample ntries for BREF:sont sO sOt(V)les le(C.) lez(V)mon mO mOn (V)ma ma (C.)aut res  ot(C.)  o t rx  otr(V) otrxz(V)Figure 1: Example lexical entries for WSJ and BREE Phones in{ } are optional, phones in \[ \] are alternates.
0 specify a contextconstraint and V stands for vowel, C for consonant and the periodrepresents silence.ations.
These rules are optionally applied during trainingand recognition.
Using optional phonological rules duringtraining results in better acoustic models, as they are less"polluted" by wrong transcriptions.
Their use during recog-nition reduces the number of mismatches.
For English, only?
well known phonological rules, such as glide insertion, stopdeletion, homorganic stop insertion, palatalization, and voic-ing assimilation have been incorporated in the system.
Thesame mechanism has been used to handle liaisons, mute-e,and final consonant cluster eduction for French.SEARCH STRATEGYOne of the most important problems in implementing alarge vocabulary speech recognizer is the design of an effi-cient search algorithm to deal with the huge search space,especially when using language models with a longer spanthan two successive words, such as trigrams.
The most com-monly used approach for small and medium vocabulary sizesis the one-pass frame-synchronous beam search \[16\] whichuses a dynamic programming procedure.
This basic strategyhas been recently extended by adding other features uchas "fast match"\[9, 1\], N-best rescoring\[21\], and progressivesearch\[15\].
The two-pass approach used in our system isbased on the idea of progressive search where the informa-tion between levels is transmitted via word graphs.
Priorto word recognition, sex identification is performed for eachsentence using phone-based ergodic HMMs\[13\].
The wordrecognizer is then run with a bigram LM using the acousticmodel set corresponding to the identified sex.The first pass uses a bigram-backoff LM with a tree or-ganization of the lexicon for the backoff component.
Thisone-pass frame-synchronous beam search, which includesintra- and inter-word CD phone models, intra- and inter-word phonological rules, phone duration models, and gender-dependent models, generates a list of word hypotheses result-ing in a word lattice.
Two problems need to be considered321at this level.
The first is whether or not the dynamic pro-gramming procedure used in the first pass, which guaranteesthe optimality of the search for the bigram, generates an "op-timal" lattice to be used with a trigram LM.
For example,any giwen word in the lattice will have many possible ndingpoints, but only a few starting points.
This problem was infact less severe than expected since the time information isnot critical to generate an "optimal" word graph from thelattice, i.e.
the multiple word endings provide nough flexi-bility to compensate for single word beginnings.
The secondconsideration is that the lattice generated in this way cannotbe too large or there is no interest in a two pass approach.
Tosolve this second problem, two pruning thresholds are usedduring r, he first pass, a beam search pruning threshold whichis kept to a level insuring almost no search errors (from thebigram point of view) and a word lattice pruning thresholdused to control the lattice size.A description of the exact procedure used to generate theword graph from the word lattice is beyond the scope of thispaper.
The following steps give the key elements behind theprocedure.
4 First, a word graph is generated from the latticeby merging three consecutive frames (i.e.
the minimumduration for a word in our system).
Then, "similar" graphnodes are merged with the goal of reducing the overall graphsize and generalizing the word lattice.
This step is reiterateduntil no further eductions are possible.
Finally, based onthe trigram backoff language model a trigram word graphis then generated by duplicating the nodes having multiplelanguage model contexts.
Bigram backoff nodes are createdwhen possible to limit the graph expansion.To fix these ideas, let us consider some numbers for theWSJ 5k-closed vocabulary.
With the pruning threshold setat a level such that there are only a negligible number ofsearch errors, the first pass generates a word lattice contain-ing on average 10,000 word hypotheses per sentence.
Thegenerated word graph before trigram expansion contains onaverage 1400 arcs.
After expansion with the trigram backoffLM, there are on average 3900 word instanciations includingsilences which are treated the same way as words.It should be noted that this decoding strategy based ontwo forward passes can in fact be implemented in a singleforward pass using one or two processors.
We are using atwo pass solution because it is conceptually simpler, and alsodue to memory constraints.EXPERIMENTAL RESULTSWSJ: The ARPA WSJ corpus\[19\] was designed to providegeneral-purpose speech data with large vocabularies.
Textmaterials were selected to provide training and test data for5k and 20k word, closed and open vocabularies, and withboth verbalized (VP) and non-verbalized (NVP) punctuation.41n our implementation, a word lattice differs from a word graph onlybecause it includes word endpoint information.5k- WSJ Corr.
Subs.
Del.
Ins.
I Err.Nov92, si84, bg 94.4 5.0 0.6 0.9 6.6Nov92, si284, bg 96.0 3.6 0.3 0.9 4.8Nov92, si284, tg 97.7 2.1 0.2 0.8 3.
INov93, si84, bg 91.9 6.2 1.9 1.3 9.4Nov93, si284, bg 94.1 4.8 1.2 0.9 6.8Nov93, si284, tg 95.5 3.5 1.1 0.8 \[ 5.3Table 2: 5k results - Word recognition results on the WSJ corpuswith bigram/trigram (bg/tg) grammars estimated on WSJ text data.2Ok- WSJ Corr.
Subs.
Del.Nov92, si84c, bg 88.3 10.
I 1.5Nov92+, si84c, bg 86.8 11.7 1.5Nov92+, si284, bg 91.6 7.6 0.8Nov92+, si284, tg 93.2 6.2 0.6Nov93+, si284, bg 87.1 11.0 1.9Nov93+, si284, tg 90.1 8.5 1.4Ins.2.02.72.62.32.31.9Err.13.615.911.09.115.211.8Table 3: 20k/64K results - Word recognition results with 20,000word lexicon on the WSJ corpus.
Bigram/trigram (bg/tg) grammarsestimated on WSJ text data.
+: 20,000 word lexicon with open test.For testing purposes, the 20k closed vocabulary includes allthe words in the test data whereas the 20k open vocabularycontains only the 20k most common words in the WSJ texts.The 20k open test is also referred to as a 64k test sinceall of the words in these sentences occur in the 63,495 mostfrequent words in the normalized WSJ text material\[ 19\].
Twosets of standard training material have been used for theseexperiments: The standard WSJ0 SI84 training data whichinclude 7240 sentences from 84 speakers, and the standard setof 37,518 WSJ0/WSJ1 SI284 sentences from 284 speakers.Only the primary microphone data were used for training.The WSJ corpus provides a wealth of material that can beused for system development.
We have worked primarilywith the WSJ0-Dev (410 sentences, 10 speakers), and theWSJ1-Dev from spokes 5 and s6 (394 sentences, 10 speak-ers).
Development of the word recognizer was done with the5k closed vocabulary system in order to reduce the compu-tational requirements.
The Nov92 5k and 20k nvp test setswere used to assess progress during this development phase.The WSJ system was evaluated in the Nov92 ARPA evalu-ation test\[17\] for the 5k-closed vocabulary and in the Nov93ARPA evaluation test\[18\] for the 5k and 64k hubs.
Exceptwhen explicitly stated otherwise, all of the results reportedfor WSJ use the standard language models\[19\].
Using a setof 1084 CD models trained with the WSJ0 si84 training data,the word error is 6.6% on the Nov92 5k test data and 9.4%on the Nov93 test data.
Using the combined WSJ0\]WSJ1si284 training data reduces the error by about 27% for bothtests.
When a trigram LM is used in the second pass, theword error is reduced by an addition 35% on the Nov92 testand by 22% on the Nov93 test.Results are given in the Table 3 for the Nov92 nvp 64K322test data using both closed and open 20k vocabularies.
Withsi84 training (si84c, a slightly smaller model set than si84)the word error ate is doubled when the vocabulary increasesfrom 5k to 20k words and the test perplexity goes from 111to 244.
The higher error rate with the 20k open lexiconcan be largely attributed to the out-of-vocabulary (OOV)words, which account for almost 2% of the words in thetest sentences.
Processing the same test data with a systemtrained on the si284 training data, reduces the word error by30%.
The word error on the Nov93 20k test is 15.2% withthe si284 system.
Using the trigrarn LM reduces the errorrate by 18% on the Nov92 test and 22% on the Nov93 test.The 20k trigram sentence error ates for Nov92 and Nov93are 60% and 62% respectively.
Since this is an open vocab-ulary test, the lower bound for the sentence rror is givenby the percent of sentences with OOV words, which is 26%for Nov92 and 21% for Nov93.
In addition there are errorsintroduced by the use of word graphs generated by the firstpass.
The graph error rate (ie.
the correct solution was notin the graph) was 6% and 12% respectively for Nov92 andNov93.
In fact, in most of these cases the errors should notbe considered search errors as the recognized string has ahigher likelihood than the correct string.A final test was run using a 64k lexicon in an attempt toeliminate rrors due to unknown words.
(In principle, all ofthe read WSJ prompts are found in the 64k most frequentwords, however, since the WSJ1 data were recorded withnon-normalized prompts, additional OOV words can occur.
)Running a full 64k system was not possible with the com-puting facilities available, so we added a third decoding passto extend the vocabulary size.
Starting with the phone stringcorresponding to the hypothesis of the trigram 20k system,an A* algorithm isused to generate a word graph using phoneconfusion statistics and the 64k lexicon.
This word graph isthen used by the recognizer with a 64k trigram LM trainedon the standard WSJ training texts (37M words).
Using thisapproach only about 30% of the errors due to OOV words onthe Nov93 64k test are recovered, reducing the word error to11.2% from 11.8%.BREF: BREF\[14\] is a large read-speech corpus, contain-ing over 100 hours of speech material, from 120 speakers(55m/65f).
The text materials were selected verbatim fromthe French newspaper Le Monde, so as to provide alarge vo-cabulary (over 20,000 words) and a wide range of phoneticenvironments\[7\].
The material in BREF was selected tomaximize the number of different phonemic contexts.
5 Con-taining 1115 distinct diphones and over 17,500 triphones,BREF can be used to train vocabulary-independent acous-tic models.
The text material was read without verbalizedpunctuation using the verbatim prompts.
65This is in contrast to the WSJ texts which were selected so as to containonly words in the most frequent 64,000 words in the original text material.6Another difference between BREF and WSJ0 is that the prompts for3235k-  BREF Corr.
Subs.
Del.
Ins. '
Err.Feb94, si57, bg 88.7 7.5 3.7 1.4 12.6Feb94, si80, bg 92.0 5.9 2.1 1.1 9.1Feb94, si80, tg 95.2 3.7 1.1 1.0 5.8Table 4: 5k word recognit ion results on the Feb94 test data withbigram/trigrarn grammars  est imated on Le Monde text data.2Ok- BREF Corr.
Subs.
Del.
Ins.
i Err.Feb94, si57, bg 85.5 11.9 2.6 1.8 16.3Feb94, si80, bg 88.6 9.7 1.7 1.6 13.0Feb94, si80, tg 91.6 7.5 0.9 1.2 9.6Feb94+, si80, bg 84.6 14.2 1.3 4.6 20.0Feb94+, si80, tg 87.4 11.6 1.0 4.3 I 16.9Table 5: 20k word recognition results on the Feb94 test data withbigram/trigram grammars estimated on Le Monde text data.
+: 20kword lexicon with open test.We have previously reported results using only a smallportion (2770 sentences from 57 speakers) of the availabletraining material for BREF\[3, 5, 4\].
In these experiments,the amount of training data has been extended to 38,550 sen-tences from 80 speakers.
The amount of text material usedfor LM training has also been increased to 38M words, en-abling us to estimate trigram LMs.
Vocabularies containingthe most frequent 5k and 20k words in the training mate-fial are used and bigram and trigram LMs were estimatedfor both vocabularies.
200 test sentences (25 from each of8 speakers) for each vocabulary were selected from the de-velopment test material for a closed vocabulary test.
Theperplexity of all the within vocabulary sentences of the de-velopment test data using the 5k/20k LM is 106/178 (whichcan be compared to 96/196 for WSJ computed under thesame conditions with the 5k/20k-open LM).
An additional200 sentences were used for a 20k-open test set.
As en-sured by the prompt selection process, the prompt exts weredistinct from the training prompts.Word recognition results for the 5k test are given in Table 4with bigram and trigram LMs estimated on the 38M-wordnormalized text material from Le Monde.
With 428 CDmodels trained on the si57 sentences, the word error is 12.6%.Using an order of magnitude more training data (si80) and1747 CD models, the word error with the bigram is reducedby 28% to 9.1%.
The use ofa trigram LM gives an additional36% reduction of error.Results for the 20k test are given in Table 5 using thesame acoustic model sets and LMs, for both closed and openvocabulary test sets.
For the closed vocabulary test, the si80training data gives an error reduction of 20% over the si57WSJ0 were normalized, where for BREF the prompts were presented asthey appeared in the original text.
This latter approach as since beenadopted for the recordings of WSJ1.
However, while for WSJ1 orthographictranscriptions are provided, for BREF the only reference currently availableis the prompt ext.training.
The use of the trigram LM reduces the word errorby an additional 26%.
The 20k-open test results are givenin the lower part of the table.
3.9% of the words are OOVand occur in 72 of the 200 sentences.
We observe almost a50% increase in word error, with a three-fold increase in theword insertions compared with the closed vocabulary test.Thus apparently the OOV words are not simply replaced byanother word, but are more often replaced by a sequence ofwords.
The trigram LM only reduces the word error by 15%on this test.D ISCUSSION AND SUMMARYThe recognizer has been evaluated on 5k and 20k testdata for the English and French languages using similar stylecorpora.
It should be pointed out however, that althoughthe Nov92 5k WSJ test data and the BREF 5k test data wereclosed-vocabulary, the conditions are not quite the same.
ForWSJ, paragraphs were selected ensuring not more than oneword was out of the 5.6k most frequent words \[ 19\], and theseadditional words were then included as part of the vocabulary.For BREF, a lexicon was first constructed containing the5k/20k most frequent words, and sentences covered by thisvocabulary were selected from the development test material.The situation was slightly different for the Nov93 5k test inthat the prompt texts were not normalized, and thereforeseveral OOV words (0.3%) occurred in the test data despiteit being a closed-vocabulary test.However, looking at the recognition results for individualspeakers, it appears that interspeaker differences are muchmore important than differences in perplexity, and perhapsmore than language differences.
Just considering the rela-tionship between speaking rate and word accurracy, in gen-eral, speakers that are faster or slower than the average have ahigher word error.
It has been observed that the better/worsespeakers are the same on both the 5k and 20k tests.We have observed some language dependencies, such asthe higher number of homophones in BREF, which has theeffect of reducing the efficiency of the search and the largenumber of frequent monophone words which results in largernetworks.
At the same time, the phone accuracy for BREFis better than that for WSJ, which speeds up the search.Improving the model accuracy, at the acoustic level and atthe language model evel, by taking advantage of the avail-able gaining data, has led to better system performance.
Forboth WSJ and BREF increasing the amount of training ut-terances by an order of magnitude reduces the word errorby about 30%.
By using larger training text materials it ispossible to train a trigram LM which was incorporated in asecond acoustic pass.
The trigram pass gives an error ratereduction of 20% to 30%.
The combined error reduction ison the order of 50%.It remains a general problem of how to define comparabletest conditions for different languages.
This may even de-pend on the definition of a word in a given language, whichis linked to the lexical coverage.
A primary aim of the afore-mentioned LRE Sqale project is to address this issue.REFERENCES\[1\] L.R.
Bahl et al "A Fast Match for Continuous Speech Recog-nition Using Allophonic Models," ICASSP-92.\[2\] S.B.
Davis, P. Mermelstein, "Comparison ofparametric repre-sentations for monosyllabic word recognition in continuouslyspoken sentences,"lEEE Trans.ASSP, 28(4), 1980.\[3\] J.L.
Gauvain, L.F. Lamel, "Speaker-Independent Phone Re-cognition Using BREF," DARPA S&NL Workshop, 1992.\[4\] J.L.
Gauvain, L.F. Lamel, G. Adda, J. Mariani, "Speech-to-Text Conversion i French," Int.
J. Pat.
Rec.
& A.L, 1994.\[5\] J.L.
Gauvain et al," Speaker-Independent Co inuous SpeechDictation," Eurospeech-93.\[6\] J.L.
Gauvain et al, "The LIMSI Continuous Speech DictationSystem: Evaluation on the ARPA Wall Street Journal Task,"ICASSP-94.\[7\] J.L.Gauvain, L.F. Lamel, M. Esk6nazi, "Design considera-tions & text selection for BREF, a large French read-speechcorpus," 1CSLP-90.\[8\] J.L.Gauvain, C.H.Lee, "Bayesian Learning for HiddenMarkov Model with Gaussian Mixture State Observation Den-sities," Speech Communication, 11(2-3), 1992.\[9\] L. Gilliek, R. Roth,"A Rapid Match Algorithm for ContinuousSpeech Recognition," DARPA Sp&NL Workshop, 1990.\[10\] S.M.
Katz, "Estimation of Probabilities from Sparse Data forthe Language Model Component of a Speech Recognizer,"1EEE Trans.ASSP, 35(3), 1987.\[11\] L. Lamel, J.L.
Gauvain, "Continuous Speech Recognition atLIMSI," Final review DARPA ANNTSpeech Prog., Sep. 1992.\[12\] L. Lamel, J.L.Gauvain,"High Performance Speaker-Indepen-dent Phone Recognition Using CDHMM," Eurospeech-93.\[13\] L. Lamel, J.L.
Gauvain, "Identifying Non-Linguistic SpeechFeatures," Eurospeech-93.\[14\] L.F. Lamel, J.L.
Gauvain, M. Esk~nazi, "BREF, a Large Vo-cabulary Spoken Corpus for French," Eurospeech-91.\[15\] H. Murveit et al "Large-VocabularyDictation usi g SRI's De-cipher Speech Recognition System: Progressive Search Tech-niques," ICASSP-93.\[16\] H. Ney, "The Use of a One-Stage Dynamic ProgrammingAlgorithm for Connected Word Recognition," IEEE Trans.ASSP, 32(2), pp.
263-271, April 1984.\[17\] D.S.Pallett etal., "Benchmark Tests for the DARPA SpokenLanguage Program," ARPA HLT Workshop, 1993.\[18\] D.S.Pallett etal., "1993 Benchmark Tests for the ARPA Spo-ken Language Program," ARPA HLT Workshop, 1994.\[19\] D.B.Paul, J.M.
Baker, "The Design for the Wall StreetJournal-based CSR Corpus," 1CSLP-92.\[20\] B.Prouts,"Contribution ~ la synth~se de la parole ~ partir dutexte: Transcription graph~me-phon~meen temps reel sur mi-croprocesseur", Th~se de docteur-ing~nieur, Universit6 PadsXI, Nov. 1980.\[21\] R. Schwartz et al,"New uses for N-Best Sentence Hypothesis,within the BYBLOS Speech Recognition System," ICASSP-92.324
