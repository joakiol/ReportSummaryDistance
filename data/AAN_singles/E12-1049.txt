Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 482?491,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsExperimenting with Distant Supervision for Emotion ClassificationMatthew Purver?
and Stuart Battersby?
?Interaction Media and Communication Group?Chatterbox AnalyticsSchool of Electronic Engineering and Computer ScienceQueen Mary University of LondonMile End Road, London E1 4NS, UKm.purver@qmul.ac.uk stuart@cbanalytics.co.ukAbstractWe describe a set of experiments using au-tomatically labelled data to train supervisedclassifiers for multi-class emotion detectionin Twitter messages with no manual inter-vention.
By cross-validating between mod-els trained on different labellings for thesame six basic emotion classes, and testingon manually labelled data, we conclude thatthe method is suitable for some emotions(happiness, sadness and anger) but less ableto distinguish others; and that different la-belling conventions are more suitable forsome emotions than others.1 IntroductionWe present a set of experiments into classify-ing Twitter messages into the six basic emotionclasses of (Ekman, 1972).
The motivation behindthis work is twofold: firstly, to investigate the pos-sibility of detecting emotions of multiple classes(rather than purely positive or negative sentiment)in such short texts; and secondly, to investigatethe use of distant supervision to quickly bootstraplarge datasets and classifiers without the need formanual annotation.Text classification according to emotion andsentiment is a well-established research area.
Inthis and other areas of text analysis and classifica-tion, recent years have seen a rise in use of datafrom online sources and social media, as theseprovide very large, often freely available datasets(see e.g.
(Eisenstein et al 2010; Go et al 2009;Pak and Paroubek, 2010) amongst many others).However, one of the challenges this poses is thatof data annotation: given very large amounts ofdata, often consisting of very short texts, writtenin unconventional style and without accompany-ing metadata, audio/video signals or access to theauthor for disambiguation, how can we easily pro-duce a gold-standard labelling for training and/orfor evaluation and test?
One possible solutionthat is becoming popular is crowd-sourcing the la-belling task, as the easy access to very large num-bers of annotators provided by tools such as Ama-zon?s Mechanical Turk can help with the problemof dataset size; however, this has its own attendantproblems of annotator reliability (see e.g.
(Hsuehet al 2009)), and cannot directly help with the in-herent problem of ambiguity ?
using many anno-tators does not guarantee that they can understandor correctly assign the author?s intended interpre-tation or emotional state.In this paper, we investigate a different ap-proach via distant supervision (see e.g.
(Mintzet al 2009)).
By using conventional markers ofemotional content within the texts themselves asa surrogate for explicit labels, we can quickly re-trieve large subsets of (noisily) labelled data.
Thisapproach has the advantage of giving us directaccess to the authors?
own intended interpreta-tion or emotional state, without relying on third-party annotators.
Of course, the labels themselvesmay be noisy: ambiguous, vague or not havinga direct correspondence with the desired classi-fication.
We therefore experiment with multiplesuch conventions with apparently similar mean-ings ?
here, emoticons (following (Read, 2005))and Twitter hashtags ?
allowing us to examine thesimilarity of classifiers trained on independent la-bels but intended to detect the same underlyingclass.
We also investigate the precision and cor-respondence of particular labels with the desiredemotion classes by testing on a small set of man-482ually labelled data.We show that the success of this approach de-pends on both the conventional markers chosenand the emotion classes themselves.
Some emo-tions are both reliably marked by different con-ventions and distinguishable from other emotions;this seems particularly true for happiness, sadnessand anger, indicating that this approach can pro-vide not only the basic distinction required forsentiment analysis but some more finer-grainedinformation.
Others are either less distinguishablefrom short text messages, or less reliably marked.2 Related Work2.1 Emotion and Sentiment ClassificationMuch research in this area has concentrated on therelated tasks of subjectivity classification (distin-guishing objective from subjective texts ?
see e.g.
(Wiebe and Riloff, 2005)); and sentiment classifi-cation (classifying subjective texts into those thatconvey positive, negative and neutral sentiment ?see e.g.
(Pang and Lee, 2008)).
We are interestedin emotion detection: classifying subjective textsaccording to a finer-grained classification of theemotions they convey, and thus providing richerand more informative data for social media anal-ysis than simple positive/negative sentiment.
Inthis study we confine ourselves to the six basicemotions identified by Ekman (1972) as beingcommon across cultures; other finer-grained clas-sifications are of course available.2.1.1 Emotion ClassificationThe task of emotion classification is by naturea multi-class problem, and classification experi-ments have therefore achieved lower accuraciesthan seen in the binary problems of sentiment andsubjectivity classification.
Danisman and Alpko-cak (2008) used vector space models for the samesix-way emotion classification we examine here,and achieved F-measures around 32%; Seol et al(2008) used neural networks for an 8-way clas-sification (hope, love, thank, neutral, happy, sad,fear, anger) and achieved per-class accuracies of45% to 65%.
Chuang and Wu (2004) used su-pervised classifiers (SVMs) and manually definedkeyword features over a seven-way classificationconsisting of the same six-class taxonomy plus aneutral category, and achieved an average accu-racy of 65.5%, varying from 56% for disgust to74% for anger.
However, they achieved signifi-cant improvements using acoustic features avail-able in their speech data, improving accuracies upto a maximum of 81.5%.2.2 ConventionsAs we are using text data, such intonational andprosodic cues are unavailable, as are the otherrich sources of emotional cues we obtain fromgesture, posture and facial expression in face-to-face communication.
However, the prevalence ofonline text-based communication has led to theemergence of textual conventions understood bythe users to perform some of the same functionsas these acoustic and non-verbal cues.
The mostfamiliar of these is the use of emoticons, eitherWestern-style (e.g.
:), :-( etc.)
or Eastern-style(e.g.
(?_?
), (>_<) etc.).
Other conventionshave emerged more recently for particular inter-faces or domains; in Twitter data, one commonconvention is the use of hashtags to add or em-phasise emotional content ?
see (1).
(1) a.
Best day in ages!
#Happy :)b.
Gets so #angry when tutors don?t emailback... Do you job idiots!Linguistic and social research into the use ofsuch conventions suggests that their function isgenerally to emphasise or strengthen the emo-tion or sentiment conveyed by a message, ratherthan to add emotional content which would nototherwise be present.
Walther and D?Addario(2001) found that the contribution of emoticonstowards the sentiment of a message was out-weighed by the verbal content, although nega-tive ones tended to shift interpretation towards thenegative.
Ip (2002) experimented with emoticonsin instant messaging, with the results suggestingthat emoticons do not add positivity or negativ-ity but rather increase valence (making positivemessages more positive and vice versa).
SimilarlyDerks et al(2008a; 2008b) found that emoticonsare used in strengthening the intensity of a ver-bal message (although they serve other functionssuch as expressing humour), and hypothesizedthat they serve similar functions to actual non-verbal behavior; Provine et al(2007) also foundthat emoticons are used to ?punctuate?
messagesrather than replace lexical content, appearing insimilar grammatical locations to verbal laughterand preserving phrase structure.4832.3 Distant SupervisionThese findings suggest, of course, that emoticonsand related conventional markers are likely to beuseful features for sentiment and emotion classifi-cation.
They also suggest, though, that they mightbe used as surrogates for manual emotion class la-bels: if their function is often to complement theverbal content available in messages, they shouldgive us a way to automatically label messages ac-cording to emotional class, while leaving us withmessages with enough verbal content to achievereasonable classification.This approach has been exploited in severalways in recent work; Tanaka et al(2005) usedJapanese-style emoticons as classification labels,and Go et al(2009) and Pak and Paroubek (2010)used Western-style emoticons to label and classifyTwitter messages according to positive and nega-tive sentiment, using traditional supervised clas-sification methods.
The highest accuracies ap-pear to have been achieved by Go et al(2009),who used various combinations of features (un-igrams, bigrams, part-of-speech tags) and clas-sifiers (Na?
?ve Bayes, maximum entropy, andSVMs), achieving their best accuracy of 83.0%with unigram and bigram features and a maxi-mum entropy; using only unigrams with a SVMclassifier achieved only slightly lower accuracy at82.2%.
Ansari (2010) then provides an initial in-vestigation into applying the same methods to six-way emotion classification, treating each emotionindependently as a binary classification problemand showing that accuracy varied with emotionclass as well as with dataset size.
The highest ac-curacies achieved were up to 81%, but these wereon very small datasets (e.g.
81.0% accuracy onfear, but with only around 200 positive and nega-tive data instances).We view this approach as having several ad-vantages; apart from the ease of data collectionit allows by avoiding manual annotation, it givesus access to the author?s own intended interpeta-tions, as the markers are of course added by theauthors themselves at time of writing.
In somecases such as the examples of (1) above, the emo-tion conveyed may be clear to a third-party anno-tator; but in others it may not be clear at all with-out the marker ?
see (2):(2) a.
Still trying to recover from seeing the#bluewaffle on my TL #disgusted #sickb.
Leftover ToeJams with Kettle Salt andVinegar chips.
#stress #sadness #comfort#letsturnthisfrownupsidedown3 MethodologyWe used a collection of Twitter messages, allmarked with emoticons or hashtags correspond-ing to one of Ekman (1972)?s six emotion classes.For emoticons, we used Ansari (2010)?s taxon-omy, taken from the Yahoo messenger classifica-tion.
For hashtags, we used emotion names them-selves together with the main related adjective ?both are used commonly on Twitter in slightlydifferent ways as shown in (3); note that emo-tion names are often used as marked verbs as wellas nouns.
Details of the classes and markers aregiven in Table 1.
(3) a.
Gets so #angry when tutors don?t emailback... Do you job idiots!b.
I?m going to say it, Paranormal Activity2 scared me and I didn?t sleep well lastnight because of it.
#fear #demonsc.
Girls that sleep w guys without even fullygetting to know them #disgust meMessages with multiple conventions (see (4))were collected and used in the experiments, ensur-ing that the marker being used as a label in a par-ticular experiment was not available as a feature inthat experiment.
Messages with no markers werenot collected.
While this prevents us from exper-imenting with the classification of neutral or ob-jective messages, it would require manual anno-tation to distinguish these from emotion-carryingmessages which are not marked.
We assume thatany implementation of the techniques we investi-gate here would be able to use a preliminary stageof subjectivity and/or sentiment detection to iden-tify these messages, and leave this aside here.
(4) a. just because people are celebs they dontreply to your tweets!
NOT FAIR #Angry:( I wish They would reply!
#PleaseData was collected from Twitter?s StreamingAPI service.1 This provides a 1-2% random sam-ple of all tweets with no constraints on language1See http://dev.twitter.com/docs/streaming-api.484Table 1: Conventional markers used for emotionclasses.happy :-) :) ;-) :D :P 8) 8-| <@osad :-( :( ;-( :-< :?
(anger :-@ :@fear :| :-o :-Osurprise :s :Sdisgust :$ +o(happy #happy #happinesssad #sad #sadnessanger #angry #angerfear #scared #fearsurprise #surprised #surprisedisgust #disgusted #disgustor location.
These are collected in near real timeand stored in a local database.
An English lan-guage selection filter was applied; scripts collect-ing each conventional marker set were alternatedthroughout different times of day and days of theweek to avoid any bias associated with e.g.
week-ends or mornings.
The numbers of messages col-lected varied with the popularity of the markersthemselves: for emoticons, we obtained a max-imum of 837,849 (for happy) and a minimumof 10,539 for anger; for hashtags, a maximumof 10,219 for happy and a minimum of 536 fordisgust.
2Classification in all experiments was using sup-port vector machines (SVMs) (Vapnik, 1995) viathe LIBSVM implementation of Chang and Lin(2001) with a linear kernel and unigram features.Unigram features included all words and hashtags(other than those used as labels in relevant exper-iments) after removal of URLs and Twitter user-names.
Some improvement in performance mightbe available using more advanced features (e.g.n-grams), other classification methods (e.g.
maxi-mum entropy, as lexical features are unlikely to beindependent) and/or feature weightings (e.g.
thevariant of TFIDF used for sentiment classificationby Martineau (2009)).
Here, our interest is morein the difference between the emotion and con-vention marker classes - we leave investigation of2One possible way to increase dataset sizes for the rarermarkers might be to include synonyms in the hashtag namesused; however, people?s use and understanding of hashtags isnot straightforwardly predictable from lexical form.
Instead,we intend to run a longer-term data gathering exercise.absolute performance for future work.4 ExperimentsThroughout, the markers (emoticons and/or hash-tags) used as labels in any experiment were re-moved before feature extraction in that experi-ment ?
labels were not used as features.4.1 Experiment 1: Emotion detectionTo simulate the task of detecting emotion classesfrom a general stream of messages, we first builtfor each convention type C and each emotionclass E a dataset DCE of size N containing (a)as positive instances, N/2 messages containingmarkers of the emotion class E and no othermarkers of type C, and (b) as negative instances,N/2 messages containing markers of type C ofany other emotion class.
For example, the posi-tive instance set for emoticon-marked anger wasbased on those tweets which contained :-@ or:@, but none of the emoticons from the happy,sad, surprise, disgust or fear classes;any hashtags were allowed, including those as-sociated with emotion classes.
The negative in-stance set contained a representative sample ofthe same number of instances, with each havingat least one of the happy, sad, surprise,disgust or fear emoticons but not containing:-@ or :@.This of course excludes messages with no emo-tional markers; for this to act as an approximationof the general task therefore requires a assump-tion that unmarked messages reflect the same dis-tribution over emotion classes as marked mes-sages.
For emotion-carrying but unmarked mes-sages, this does seem intuitively likely, but re-quires investigation.
For neutral objective mes-sages it is clearly false, but as stated above we as-sume a preliminary stage of subjectivity detectionin any practical application.Performance was evaluated using 10-foldcross-validation.
Results are shown as the boldfigures in Table 2; despite the small datasetsizes in some cases, a ?2 test shows all to besignificantly different from chance.
The best-performing classes show accuracies very similarto those achieved by Go et al(2009) for their bi-nary positive/negative classification, as might beexpected; for emoticon markers, the best classesare happy, sad and anger; interestingly thebest classes for hashtag markers are not the same485Table 2: Experiment 1: Within-class results.
Same-convention (bold) figures are accuracies over 10-foldcross-validation; cross-convention (italic) figures areaccuracies over full sets.TrainConvention Test emoticon hashtagemoticon happy 79.8% 63.5%emoticon sad 79.9% 65.5%emoticon anger 80.1% 62.9%emoticon fear 76.2% 58.5%emoticon surprise 77.4% 48.2%emoticon disgust 75.2% 54.6%hashtag happy 67.7% 82.5%hashtag sad 67.1% 74.6%hashtag anger 62.8% 74.7%hashtag fear 60.6% 77.2%hashtag surprise 51.9% 67.4%hashtag disgust 64.6% 78.3%?
happy performs best, but disgust and fearoutperform sad and anger, and surpriseperforms particularly badly.
For sad, one reasonmay be a dual meaning of the tag #sad (one emo-tional and one expressing ridicule); for angerone possibility is the popularity on Twitter of thegame ?Angry Birds?
; for surprise, the dataseems split between two rather distinct usages,ones expressing the author?s emotion, but one ex-pressing an intended effect on the audience (see(5)).
However, deeper analysis is needed to estab-lish the exact causes.
(5) a. broke 100 followers.
#surprised im gladthat the HOFF is one of them.b.
Who?s excited for the Big Game?
Weknow we are AND we have a #surprisefor you!To investigate whether the different conven-tion types actually convey similar properties (andhence are used to mark similar messages) we thencompared these accuracies to those obtained bytraining classifiers on the dataset for a differentconvention: in other words, for each emotionclass E, train a classifier on dataset DC1E and teston DC2E .
As the training and testing sets are dif-ferent, we now test on the entire dataset ratherthan using cross-validation.
Results are shown asthe italic figures in Table 2; a ?2 test shows allto be significantly different from the bold same-convention results.
Accuracies are lower overall,but the highest figures (between 63% and 68%)are achieved for happy, sad and anger; hereperhaps we can have some confidence that notonly are the markers acting as predictable labelsthemselves, but also seem to be labelling the samething (and therefore perhaps are actually labellingthe emotion we are hoping to label).4.2 Experiment 2: Emotion discriminationTo investigate whether these independent clas-sifiers can be used in multi-class classification(distinguishing emotion classes from each otherrather than just distinguishing one class from ageneral ?other?
set), we next cross-tested the clas-sifiers between emotion classes: training modelson one emotion and testing on the others ?
foreach convention type C and each emotion classE1, train a classifier on dataset DCE1 and test onDCE2, DCE3 etc.
The datasets in Experiment 1 hadan uneven balance of emotion classes (including ahigh proportion of happy instances) which couldbias results; for this experiment, therefore, we cre-ated datasets with an even balance of emotionsamong the negative instances.
For each conven-tion type C and each emotion class E1, we builta dataset DCE1 of size N containing (a) as pos-itive instances, N/2 messages containing mark-ers of the emotion class E1 and no other mark-ers of type C, and (b) as negative instances, N/2messages consisting of N/10 messages contain-ing only markers of class E2, N/10 messagescontaining only markers of class E3 etc.
Resultswere then generated as in Experiment 1.Within-class results are shown in Table 3 andare similar to those obtained in Experiment 1;again, differences between bold/italic results arestatistically significant.
Cross-class results areshown in Table 4.
The happy class was welldistinguished from other emotion classes for bothconvention types (i.e.
cross-class classificationaccuracy is low compared to the within-class fig-ures in italics and parentheses).
The sad classalso seems well distinguished when using hash-tags as labels, although less so when using emoti-cons.
However, other emotion classes show a sur-prisingly high cross-class performance in manycases ?
in other words, they are producing dis-appointingly similar classifiers.This poor discrimination for negative emotionclasses may be due to ambiguity or vagueness inthe label, similarity of the verbal content associ-486Table 4: Experiment 2: Cross-class results.
Same-class figures from 10-fold cross-validation are shown in(italics) for comparison; all other figures are accuracies over full sets.TrainConvention Test happy sad anger fear surprise disgustemoticon happy (78.1%) 17.3% 39.6% 26.7% 28.3% 42.8%emoticon sad 16.5% (78.9%) 59.1% 71.9% 69.9% 55.5%emoticon anger 29.8% 67.0% (79.7%) 74.2% 76.4% 67.5%emoticon fear 27.0% 69.9% 64.4% (75.3%) 74.0% 61.2%emoticon surprise 25.4% 69.9% 67.7% 76.3% (78.1%) 66.4%emoticon disgust 42.2% 54.4% 61.1% 64.2% 64.1% (73.9%)hashtag happy (81.1%) 10.7% 45.3% 47.8% 52.7% 43.4%hashtag sad 13.8% (77.9%) 47.7% 49.7% 46.5% 54.2%hashtag anger 44.6% 45.2% (74.3%) 72.0% 63.0% 62.9%hashtag fear 45.0% 50.4% 68.6% (74.7%) 63.9% 60.7%hashtag surprise 51.5% 45.7% 67.4% 70.7% (70.2%) 64.2%hashtag disgust 40.4% 53.5% 74.7% 71.8% 70.8% (74.2%)Table 3: Experiment 2: Within-class results.
Same-convention (bold) figures are accuracies over 10-foldcross-validation; cross-convention (italic) figures areaccuracies over full sets.TrainConvention Test emoticon hashtagemoticon happy 78.1% 61.2%emoticon sad 78.9% 60.2%emoticon anger 79.7% 63.7%emoticon fear 75.3% 55.9%emoticon surprise 78.1% 53.1%emoticon disgust 73.9% 51.5%hashtag happy 68.7% 81.1%hashtag sad 65.4% 77.9%hashtag anger 63.9% 74.3%hashtag fear 58.9% 74.7%hashtag surprise 51.8% 70.2%hashtag disgust 55.4% 74.2%ated with the emotions, or of genuine frequent co-presence of the emotions.
Given the close lex-ical specification of emotions in hashtag labels,the latter reasons seem more likely; however, withemoticon labels, we suspect that the emoticonsthemselves are often used in ambiguous or vagueways.As one way of investigating this directly, wetested classifiers across labelling conventions aswell as across emotion classes, to determinewhether the (lack of) cross-class discriminationholds across convention marker types.
In thecase of ambiguity or vagueness of emoticons,we would expect emoticon-trained models to failto discriminate hashtag-labelled test sets, buthashtag-trained models to discriminate emoticon-labelled test sets well; if on the other hand thecause lies in the overlap of verbal content or theemotions themselves, the effect should be simi-lar in either direction.
This experiment also helpsdetermine in more detail whether the labels usedlabel similar underlying properties.Table 5 shows the results.
For the three classeshappy, sad and perhaps anger, models trainedusing emoticon labels do a reasonable job of dis-tinguishing classes in hashtag-labelled data, andvice versa.
However, for the other classes, dis-crimination is worse.
Emoticon-trained mod-els appear to give (undesirably) higher perfor-mance across emotion classes in hashtag-labelleddata (for the problematic non-happy classes).Hashtag-trained models perform around the ran-dom 50% level on emoticon-labelled data forthose classes, even when tested on nominallythe same emotion as they are trained on.
Forboth label types, then, the lower within-class andhigher cross-class performance with these nega-tive classes (fear, surprise, disgust) sug-gests that these emotion classes are genuinelyhard to tell apart (they are all negative emotions,and may use similar words), or are simply of-ten expressed simultaneously.
The higher perfor-mance of emoticon-trained classifiers comparedto hashtag-trained classifiers, though, also sug-gests vagueness or ambiguity in emoticons: datalabelled with emoticons nominally thought to be487Table 5: Experiment 2: Cross-class, cross-convention results (train on hashtags, test on emoticons and viceversa).
All figures are accuracies over full sets.
Accuracies over 60% are shown in bold.TrainConvention Test happy sad anger fear surprise disgustemoticon happy 61.2% 40.4% 44.1% 47.4% 52.0% 45.9%emoticon sad 38.3% 60.2% 55.1% 51.5% 47.1% 53.9%emoticon anger 47.0% 48.0% 63.7% 56.2% 50.9% 56.6%emoticon fear 39.8% 57.7% 57.1% 55.9% 50.8% 56.1%emoticon surprise 43.7% 55.2% 59.2% 58.4% 53.1% 54.0%emoticon disgust 51.5% 48.0% 53.5% 55.1% 53.1% 51.5%hashtag happy 68.7% 32.5% 43.6% 32.1% 35.4% 50.4%hashtag sad 33.8% 65.4% 53.2% 65.0% 61.8% 48.8%hashtag anger 43.9% 55.5% 63.9% 59.6% 60.4% 53.0%hashtag fear 44.3% 54.6% 56.1% 58.9% 61.5% 54.3%hashtag surprise 54.2% 45.3% 49.8% 49.9% 51.8% 52.3%hashtag disgust 41.5% 57.6% 61.6% 62.2% 59.3% 55.4%associated with surprise produces classifierswhich perform well on data labelled with manyother hashtag classes, suggesting that those emo-tions were present in the training data.
Con-versely, the more specific hashtag labels produceclassifiers which perform poorly on data labelledwith emoticons and which thus contains a rangeof actual emotions.4.3 Experiment 3: Manual labellingTo confirm whether either (or both) set of auto-matic (distant) labels do in fact label the under-lying emotion class intended, we used human an-notators via Amazon?s Mechanical Turk to labela set of 1,000 instances.
These instances were alllabelled with emoticons (we did not use hashtag-labelled data: as hashtags are so lexically close tothe names of the emotion classes being labelled,their presence may influence labellers unduly)3and were evenly distributed across the 6 classes,in so far as indicated by the emoticons.
Labellerswere asked to choose the primary emotion class(from the fixed set of six) associated with the mes-sage; they were also allowed to specify if anyother classes were also present.
Each data in-stance was labelled by three different annotators.Agreement between labellers was poor over-all.
The three annotators unanimously agreed inonly 47% of cases overall; although two of threeagreed in 83% of cases.
Agreement was worst3Although, of course, one may argue that they do thesame for their intended audience of readers ?
in which case,such an effect is legitimate.for the three classes already seen to be prob-lematic: surprise, fear and disgust.
Tocreate our dataset for this experiment, we there-fore took only instances which were given thesame primary label by all labellers ?
i.e.
onlythose examples which we could take as reliablyand unambiguously labelled.
This gave an un-balanced dataset, with numbers varying from 266instances for happy to only 12 instances foreach of surprise and fear.
Classifiers weretrained using the datasets from Experiment 2.
Per-formance is shown in Table 6; given the imbal-ance between class numbers in the test dataset,evaluation is given as recall, precision and F-scorefor the class in question rather than a simple accu-racy figure (which is biased by the high proportionof happy examples).Table 6: Experiment 3: Results on manual labels.Train Class Precision Recall F-scoreemoticon happy 79.4% 75.6% 77.5%emoticon sad 43.5% 73.2% 54.5%emoticon anger 62.2% 37.3% 46.7%emoticon fear 6.8% 63.6% 12.3%emoticon surprise 15.0% 90.0% 25.7%emoticon disgust 8.3% 25.0% 12.5%hashtag happy 78.9% 51.9% 62.6%hashtag sad 47.9% 81.7% 60.4%hashtag anger 58.2% 76.0% 65.9%hashtag fear 10.1% 81.8% 18.0%hashtag surprise 5.9% 60.0% 10.7%hashtag disgust 6.7% 66.7% 11.8%488Again, results for happy are good, and cor-respond fairly closely to the levels of accuracyreported by Go et al(2009) and others for thebinary positive/negative sentiment detection task.Emoticons give significantly better performancethan hashtags here.
Results for sad and angerare reasonable, and provide a baseline for fur-ther experiments with more advanced features andclassification methods once more manually anno-tated data is available for these classes.
In con-trast, hashtags give much better performance withthese classes than the (perhaps vague or ambigu-ous) emoticons.The remaining emotion classes, however, showpoor performance for both labelling conventions.The observed low precision and high recall can beadjusted using classifier parameters, but F-scoresare not improved.
Note that Experiment 1 showsthat both emoticon and hashtag labels are to someextent predictable, even for these classes; how-ever, Experiment 2 shows that they may not bereliably different to each other, and Experiment 3tells us that they do not appear to coincide wellwith human annotator judgements of emotions.More reliable labels may therefore be required;although we do note that the low reliability ofthe human annotations for these classes, and thecorrespondingly small amount of annotated dataused in this evaluation, means we hesitate to drawstrong conclusions about fear, surprise anddisgust.
An approach which considers multi-ple classes to be associated with individual mes-sages may also be beneficial: using majority-decision labels rather than unanimous labels im-proves F-scores for surprise to 23-35% by in-cluding many examples also labelled as happy(although this gives no improvements for otherclasses).5 SurveyTo further detemine whether emoticons usedas emotion class labels are ambiguous or vaguein meaning, we set up a web survey to exam-ine whether people could reliably classify theseemoticons.5.1 MethodOur survey asked people to match up which ofthe six emotion classes (selected from a drop-down menu) best matched each emoticon.
Eachdrop-down menu included a ?Not Sure?
option.To avoid any effect of ordering, the order of theemoticon list and each drop-down menu was ran-domised every time the survey page was loaded.The survey was distributed via Twitter, Facebookand academic mailing lists.
Respondents were notgiven the opportunity to give their own definitionsor to provide finer-grained classifications, as wewanted to establish purely whether they would re-liably associate labels with the six emotions in ourtaxonomy.5.2 ResultsThe survey was completed by 492 individuals;full results are shown in Table 7.
It demonstratedagreement with the predefined emoticons for sadand most of the emoticons for happy (peoplewere unsure what 8-| and <@o meant).
For allthe emoticons listed as anger, surprise anddisgust, the survey showed that people are reli-ably unsure as to what these mean.
For the emoti-con :-o there was a direct contrast between thedefined meaning and the survey meaning; the def-inition of this emoticon following Ansari (2010)was fear, but the survey reliably assigned this tosurprise.Given the small scale of the survey, we hesi-tate to draw strong conclusions about the emoti-con meanings themselves (in fact, recent conver-sations with schoolchildren ?
see below ?
have in-dicated very different interpretations from theseadult survey respondents).
However, we do con-clude that for most emotions outside happy andsad, emoticons may indeed be an unreliable la-bel; as hashtags also appear more reliable in theclassification experiments, we expect these to bea more promising approach for fine-grained emo-tion discrimination in future.6 ConclusionsThe approach shows reasonable performance atindividual emotion label prediction, for bothemoticons and hashtags.
For some emotions (hap-piness, sadness and anger), performance acrosslabel conventions (training on one, and testing onthe other) is encouraging; for these classes, per-formance on those manually labelled exampleswhere annotators agree is also reasonable.
Thisgives us confidence not only that the approachproduces reliable classifiers which can predict thelabels, but that these classifiers are actually de-tecting the desired underlying emotional classes,489Table 7: Survey results showing the defined emotion, the most popular emotion from the survey, the percentageof votes this emotion received, and the ?2 significance test for the distribution of votes.
These are indexed byemoticon.Emoticon Defined Emotion Survey Emotion % of votes Significance of votes distribution:-) Happy Happy 94.9 ?2 = 3051.7 (p < 0.001):) Happy Happy 95.5 ?2 = 3098.2 (p < 0.001);-) Happy Happy 87.4 ?2 = 2541 (p < 0.001):D Happy Happy 85.7 ?2 = 2427.2 (p < 0.001):P Happy Happy 59.1 ?2 = 1225.4 (p < 0.001)8) Happy Happy 61.9 ?2 = 1297.4 (p < 0.001)8-| Happy Not Sure 52.2 ?2 = 748.6 (p < 0.001)<@o Happy Not Sure 84.6 ?2 = 2335.1 (p < 0.001):-( Sad Sad 91.3 ?2 = 2784.2 (p < 0.001):( Sad Sad 89.0 ?2 = 2632.1 (p < 0.001);-( Sad Sad 67.9 ?2 = 1504.9 (p < 0.001):-< Sad Sad 56.1 ?2 = 972.59 (p < 0.001):?
( Sad Sad 80.7 ?2 = 2116 (p < 0.001):-@ Anger Not Sure 47.8 ?2 = 642.47 (p < 0.001):@ Anger Not Sure 50.4 ?2 = 691.6 (p < 0.001):s Surprise Not Sure 52.2 ?2 = 757.7 (p < 0.001):$ Disgust Not Sure 62.8 ?2 = 1136 (p < 0.001)+o( Disgust Not Sure 64.2 ?2 = 1298.1 (p < 0.001):| Fear Not Sure 55.1 ?2 = 803.41 (p < 0.001):-o Fear Surprise 89.2 ?2 = 2647.8 (p < 0.001)without requiring manual annotation.
We there-fore plan to pursue this approach with a view toimproving performance by investigating trainingwith combined mixed-convention datasets, andcross-training between classifiers trained on sepa-rate conventions.However, this cross-convention performance ismuch better for some emotions (happiness, sad-ness and anger) than others (fear, surprise and dis-gust).
Indications are that the poor performanceon these latter emotion classes is to a large de-gree an effect of ambiguity or vagueness of theemoticon and hashtag conventions we have usedas labels here; we therefore intend to investi-gate other conventions with more specific and/orless ambiguous meanings, and the combinationof multiple conventions to provide more accu-rately/specifically labelled data.
Another possi-bility might be to investigate approaches to anal-yse emoticons semantically on the basis of theirshape, or use features of such an analysis ?
see(Ptaszynski et al 2010; Radulovic and Milikic,2009) for some interesting recent work in this di-rection.AcknowledgementsThe authors are supported in part by the Engi-neering and Physical Sciences Research Council(grants EP/J010383/1 and EP/J501360/1) and theTechnology Strategy Board (R&D grant 700081).We thank the reviewers for their comments.ReferencesSaad Ansari.
2010.
Automatic emotion tone detectionin twitter.
Master?s thesis, Queen Mary Universityof London.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a library for Support Vector Machines.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Ze-Jing Chuang and Chung-Hsien Wu.
2004.
Multi-modal emotion recognition from speech and text.Computational Linguistics and Chinese LanguageProcessing, 9(2):45?62, August.Taner Danisman and Adil Alpkocak.
2008.
Feeler:Emotion classification of text using vector spacemodel.
In AISB 2008 Convention, Communication,Interaction and Social Intelligence, volume 2, pages53?59, Aberdeen.490Daantje Derks, Arjan Bos, and Jasper von Grumbkow.2008a.
Emoticons and online message interpreta-tion.
Social Science Computer Review, 26(3):379?388.Daantje Derks, Arjan Bos, and Jasper von Grumbkow.2008b.
Emoticons in computer-mediated commu-nication: Social motives and social context.
Cy-berPsychology & Behavior, 11(1):99?101, Febru-ary.Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,and Eric P. Xing.
2010.
A latent variable modelfor geographic lexical variation.
In Proceedingsof the 2010 Conference on Empirical Methods inNatural Language Processing, pages 1277?1287,Cambridge, MA, October.
Association for Compu-tational Linguistics.Paul Ekman.
1972.
Universals and cultural differ-ences in facial expressions of emotion.
In J. Cole,editor, Nebraska Symposium on Motivation 1971,volume 19.
University of Nebraska Press.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervi-sion.
Master?s thesis, Stanford University.Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.2009.
Data quality from crowdsourcing: A study ofannotation selection criteria.
In Proceedings of theNAACL HLT 2009 Workshop on Active Learning forNatural Language Processing, pages 27?35, Boul-der, Colorado, June.
Association for ComputationalLinguistics.Amy Ip.
2002.
The impact of emoticons on affect in-terpretation in instant messaging.
Carnegie MellonUniversity.Justin Martineau.
2009.
Delta TFIDF: An improvedfeature space for sentiment analysis.
Artificial In-telligence, 29:258?261.Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In Proceedings of ACL-IJCNLP 2009.Alexander Pak and Patrick Paroubek.
2010.
Twitteras a corpus for sentiment analysis and opinion min-ing.
In Proceedings of the 7th conference on Inter-national Language Resources and Evaluation.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1?2):1?135.Robert Provine, Robert Spencer, and Darcy Mandell.2007.
Emotional expression online: Emoticonspunctuate website text messages.
Journal of Lan-guage and Social Psychology, 26(3):299?307.M.
Ptaszynski, J. Maciejewski, P. Dybala, R. Rzepka,and K Araki.
2010.
CAO: A fully automatic emoti-con analysis system based on theory of kinesics.
InProceedings of The 24th AAAI Conference on Arti-ficial Intelligence (AAAI-10), pages 1026?1032, At-lanta, GA.F.
Radulovic and N. Milikic.
2009.
Smiley ontology.In Proceedings of The 1st International WorkshopOn Social Networks Interoperability.Jonathon Read.
2005.
Using emoticons to reduce de-pendency in machine learning techniques for sen-timent classification.
In Proceedings of the 43rdMeeting of the Association for Computational Lin-guistics.
Association for Computational Linguis-tics.Young-Soo Seol, Dong-Joo Kim, and Han-Woo Kim.2008.
Emotion recognition from text using knowl-edge based ANN.
In Proceedings of ITC-CSCC.Y.
Tanaka, H. Takamura, and M. Okumura.
2005.
Ex-traction and classification of facemarks with kernelmethods.
In Proceedings of IUI.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer.Joseph Walther and Kyle D?Addario.
2001.
Theimpacts of emoticons on message interpretation incomputer-mediated communication.
Social ScienceComputer Review, 19(3):324?347.J.
Wiebe and E. Riloff.
2005.
Creating subjectiveand objective sentence classifiers from unannotatedtexts .
In Proceedings of the 6th International Con-ference on Computational Linguistics and Intelli-gent Text Processing (CICLing-05), volume 3406 ofSpringer LNCS.
Springer-Verlag.491
