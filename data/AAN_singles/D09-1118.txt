Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1133?1141,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPReal-time decision detection in multi-party dialogueMatthew Frampton, Jia Huang, Trung Huu Bui and Stanley PetersCenter for the Study of Language and InformationStanford UniversityStanford, CA, 94305, USA{frampton|jiahuang|thbui|peters}@stanford.eduAbstractWe describe a process for automaticallydetecting decision-making sub-dialoguesin multi-party, human-human meetings inreal-time.
Our basic approach to decisiondetection involves distinguishing betweendifferent utterance types based on the rolesthat they play in the formulation of a de-cision.
In this paper, we describe how thisapproach can be implemented in real-time,and show that the resulting system?s per-formance compares well with other detec-tors, including an off-line version.1 IntroductionIn collaborative and organized work environ-ments, people share information and make de-cisions through multi-party conversations, com-monly referred to as meetings.
The demand forautomatic methods that process, understand andsummarize information contained in audio andvideo recordings of meetings is growing rapidly,as evidenced by on-going projects which are fo-cused on this goal, (Waibel et al, 2003; Janin etal., 2004).
Our research is part of a general effortto develop a system that can automatically extractand summarize information such as conversationaltopics, action items, and decisions.This paper concerns the development of a real-time decision detector ?
a system which can de-tect and summarize decisions as they are madeduring a meeting.
Such a system could providea summary of all of the decisions which have beenmade up until the current point in the meeting,and this is something which we expect will helpusers to enjoy more productive meetings.
Cer-tainly, good decision-making relies on access torelevant information, and decisions made earlierin a meeting often have a bearing on the currenttopic of discussion, and so form part of this rele-vant information.
However, in a long and windingmeeting, participants might not have these earlierdecisions at the forefront of their minds, and soan accurate and succinct reminder, as provided bya real-time decision detector, could potentially bevery useful.
A record of earlier decisions couldalso help users to identify outstanding issues fordiscussion, and to therefore make better use of theremainder of the meeting.Our approach to decision detection uses an an-notation scheme which distinguishes between dif-ferent types of utterance based on the roles whichthey play in the decision-making process.
Such ascheme facilitates the detection of decision discus-sions (Fern?andez et al, 2008), and by indicatingwhich utterances contain particular types of infor-mation, it also aids their summarization.
To auto-matically detect decision discussions, we use whatwe refer to as hierarchical classification.
Here, in-dependent binary sub-classifiers detect the differ-ent decision dialogue acts, and then based on thesub-classifier hypotheses, a super-classifier deter-mines which dialogue regions are decision discus-sions.
In this paper then, we address the chal-lenges for applying this approach in real-time, andproduce a system which is able to detect decisionssoon after they are made, (for example within aminute).
We conduct tests and compare this sys-tem?s performance with other detectors, includingan off-line equivalent.The remainder of the paper proceeds as follows.Section 2 describes related work, and Section 3 de-scribes our annotation scheme for decision discus-sions, and our experimental data.
Next, Section4 explains the hierarchical classification approachin more detail, and Section 5 considers how it canbe applied in real-time.
Section 6 describes theexperiments in which we test the real-time detec-tor, and finally, Section 7 presents conclusions andideas for future work.11332 Related WorkDecisions are one of the most important meet-ing outputs.
User studies (Lisowska et al, 2004;Banerjee et al, 2005) have confirmed that meetingparticipants consider this to be the case, and Whit-taker et al (2006) found that the development ofan automatic decision detection component is crit-ical to the re-use of meeting archives.
As a result,with the new availability of substantial meetingcorpora such as the ISL (Burger et al, 2002), ICSI(Janin et al, 2004) and AMI (McCowan et al,2005) Meeting Corpora, recent years have seen anincreasing amount of research on decision-makingdialogue.This recent research has tackled issues suchas the automatic detection of agreement and dis-agreement (Hillard et al, 2003; Galley et al,2004), and of the level of involvement of conver-sational participants (Wrede and Shriberg, 2003;Gatica-Perez et al, 2005).
In addition, Verbreeet al (2006) created an argumentation scheme in-tended to support automatic production of argu-ment structure diagrams from decision-orientedmeeting transcripts.
Only very recent research hasspecifically investigated the automatic detection ofdecisions, namely (Hsueh and Moore, 2007) and(Fern?andez et al, 2008).Hsueh and Moore (2007) used the AMI MeetingCorpus, and attempted to automatically identifydialogue acts (DAs) in meeting transcripts whichare ?decision-related?.
Within any meeting, theauthors decided which DAs were decision-relatedbased on two different kinds of manually createdsummary: the first was an extractive summary ofthe whole meeting, and the second, an abstrac-tive summary of the decisions which were made.Those DAs in the extractive summary which sup-port any of the decisions in the abstractive sum-mary were manually tagged as decision-related.Hsueh and Moore (2007) then trained a Maxi-mum Entropy classifier to recognize this singleDA class, using a variety of lexical, prosodic, dia-logue act and conversational topic features.
Theyachieved an F-score of 0.35, which gives an indi-cation of the difficulty of this task.Unlike Hsueh and Moore (2007), Fern?andezet al (2008) made an attempt at modelling thestructure of decision-making dialogue.
They de-signed an annotation scheme that takes account ofthe different roles which different utterances playin the decision-making process ?
for example,their scheme distinguishes between decision DAs(DDAs) which initiate a discussion by raising atopic/issue, those which propose a resolution, andthose which express agreement for a proposed res-olution and cause it to be accepted as a decision.The authors applied the annotation scheme to aportion of the AMI corpus, and then took whatthey refer to as a hierarchical classification ap-proach in order to automatically identify decisiondiscussions and their component DAs.
Here, onebinary Support Vector Machine (SVM) per DDAclass hypothesized occurrences of that DDA class,and then based on the hypotheses of these so-called sub-classifiers, a super-classifier, (a furtherSVM), determined which regions of dialogue rep-resented decision discussions.
This approach pro-duced better results than the kind of ?flat classi-fication?
approach pursued by Hsueh and Moore(2007) where a single classifier looks for exam-ples of a single decision-related DA class.
Usingmanual transcripts, and a variety of lexical, utter-ance, speaker, DA and prosodic features for thesub-classifiers, the super-classifier?s F1-score was0.58 according to a lenient match metric.
Note that(Purver et al, 2007) had previously pursued thesame basic approach as Fern?andez et al (2008) inorder to detect action items.While both Hsueh and Moore (2007), andFern?andez et al (2008) attempted off-line decisiondetection, in this paper, we attempt real-time deci-sion detection.
We take the same basic approachas Fern?andez et al (2008), and make changes toits implementation so that it can work effectivelyin real-time.3 DataThe AMI corpus (McCowan et al, 2005), is afreely available corpus of multi-party meetingscontaining both audio and video recordings, aswell as a wide range of annotated informationincluding dialogue acts and topic segmentation.Conversations are all in English, but participantscan include non-native English speakers.
All ofthe meetings in our sub-corpus last around 30 min-utes, and are scenario-driven, wherein four partic-ipants play different roles in a company?s designteam: project manager, marketing expert, inter-face designer and industrial designer.
The discus-sions concern how to design a remote control.We used the off-line version of the Decipherspeech recognition engine (Stolcke et al, 2008) in1134order to obtain off-line ASR transcripts for these17 meetings, and the real-time version, to ob-tain real-time ASR transcripts.
Decipher gener-ates the transcripts by first producing Word Con-fusion Networks (WCNs) and then extracting theirbest paths.
The real-time recognizer generates?live?
transcripts with 5 to 15 seconds of latencyfor immediate display.
In processing completedmeetings, the off-line system makes seven recog-nition passes, including acoustic adaptation andlanguage model rescoring, in about 4.2 times real-time (on a 4-score 2.6 GHz Opteron server).
Ingeneral usage with multi-party dialogue, the worderror rate (WER) for the off-line version of De-cipher is approximately 23%, and for the real-time version, approximately 35%1.
Stolcke et al(2008) report a WER of 26.9% for the off-line ver-sion on AMI meetings.The real-time ASR transcripts for the 17 meet-ings contain a total of 8440 utterances/dialogueacts, (around 496 per meeting), and the off-lineASR transcripts, 7495 utterances/dialogue acts,(around 441 per meeting).3.1 Modelling Decision DiscussionsWe use the same annotation scheme as(Fern?andez et al, 2008) in order to modeldecision-making dialogue.
As stated in Section 2,this scheme distinguishes between a small numberof dialogue act types based on the role which theyperform in the formulation of a decision.
Recallthat using this scheme in conjunction with hierar-chical classification produced better decision de-tection than a ?flat classification?
approach with asingle ?decision-related?
DA class.
Since it indi-cates which utterances contain particular types ofinformation, such a scheme also aids the summa-rization of decision discussions.The annotation scheme (see Table 1 for a sum-mary) is based on the observation that a decisiondiscussion contains the following main structuralcomponents: (a) a topic or issue requiring resolu-tion is raised, (b) one or more possible resolutionsare considered, (c) a particular resolution is agreedupon, that is, it becomes the decision.
Hence thescheme distinguishes between three correspond-ing decision dialogue act (DDA) classes: Issue (I),Resolution (R), and Agreement (A).
Class R is fur-ther subdivided into Resolution Proposal (RP) and1This information was obtained through personal commu-nication.Resolution Restatement (RR).
Note that an utter-ance can be assigned to more than one of theseDDA classes, and that within a decision discus-sion, more than one utterance may correspond to aparticular DDA class.Here we use the sample decision discussionbelow in 1 in order to provide examples of thedifferent DDA types.
I utterances introduce thetopic of the decision discussion, examples be-ing ?Are we going to have a backup??
and ?Butwould a backup really be necessary??
On theother hand, R utterances specify the resolutionwhich is ultimately adopted as the decision.
RPutterances propose this resolution (e.g.
?I thinkmaybe we could just go for the kinetic energy.
.
.
?
),while RR utterances close the discussion by con-firming/summarizing the decision (e.g.
?Okay,fully kinetic energy?).
Finally, A utterances agreewith the proposed resolution, so causing it to beadopted as the decision, (e.g.
?Yeah?, ?Good?and ?Okay?.
(1) A: Are we going to have a backup?Or we do just?B: But would a backup really be necessary?A: I think maybe we could just go for thekinetic energy and be bold and innovative.C: Yeah.B: I think?
yeah.A: It could even be one of our selling points.C: Yeah ?laugh?.D: Environmentally conscious or something.A: Yeah.B: Okay, fully kinetic energy.D: Good.23.2 Experimental data for real-time decisiondetectionOriginally, two individuals used the annotationscheme described above in order to annotate themanual transcripts of 9 and 10 meetings respec-tively.
The annotators overlapped on two meet-ings, and their kappa inter-annotator agreementranged from 0.63 to 0.73 for the four DDA classes.The highest agreement was obtained for class RP,and the lowest for class A.
Although these kappavalues are not extremely high, if we used a single,less homogeneous ?decision-related?
DA classlike Hsueh and Moore (2007), then its kappa score2This example was extracted from the AMI dialogueES2015c and has been modified slightly for presentation pur-poses.1135key DDA class descriptionI issue utterances introducing the issue or topic under discussionR resolution utterances containing the resolution adopted as the decisionRP ?
proposal ?
utterances where the decision is originally proposedRR ?
restatement ?
utterances where the decision is confirmed or restatedA agreement utterances explicitly signalling agreement with the decisionTable 1: Set of decision dialogue act (DDA) classeswould probably be significantly lower.
The de-cision discussion annotations used by Hsueh andMoore (2007) are part of the AMI corpus, and arefor the manual transcriptions.
The reader can finda comparison between these annotations and ourown manual transcript annotations in (Fern?andezet al, 2008).After obtaining the new off-line and real-timeASR transcripts, we transferred the DDA annota-tions from the manual transcripts.
In both sets ofASR transcripts, each meeting contains on aver-age around 26 DAs tagged with one or more of theDDA sub-classes in Table 1.
DDAs are thus verysparse, corresponding to only 5.3% of utterancesin the real-time transcripts, and 6.0% in the off-line.
In the real-time transcripts, Issue utterancesmake up less than 1.2% of the total number of ut-terances in a meeting, while Resolution utterancesare around 1.6%: 1.2% are RP and less than 0.4%are RR on average.
Almost half of DDA utterances(slightly over 2.6% of all utterances on average)are tagged as belonging to class Agreement.
In theoff-line transcripts, the percentages are fairly sim-ilar: 1.6% of utterances are Issue DDAs, 2.0% areRP, 0.5% are RR, and 2.4% are A.We now move on to describe the hierarchicalclassification approach which we use to try to au-tomatically detect decision sub-dialogues and theircomponent DDAs.4 Hierarchical ClassificationHierarchical classification is designed to exploitthe fact that within decision discussions, ourDDAs can be expected to co-occur in particulartypes of patterns.
It involves two different types ofclassifier:1.
Sub-classifier: One independent binary sub-classifier per DDA class classifies each utter-ance.2.
Super-classifier: A sliding window shiftsthrough the meeting one utterance at a time,and following each shift, a binary super-classifier determines whether the region ofdialogue within the window is part of a de-cision discussion.In our decision detectors, the sub-classifiers runin parallel in order to reduce processing time.For each utterance, the sub-classifiers use fea-tures which are derived from the properties ofthat utterance in context.
On the other hand,the super-classifier?s features are the hypothesizedclass labels and confidence scores for the utter-ances within the window.
In various experiments,we have found that a suitable size for the window,is the average length of a decision discussion inour data in utterances.
The super-classifier also?corrects?
the sub-classifiers.
This means that if aDA is classified as positive by a sub-classifier, butdoes not fall within a region classified as part ofa decision discussion by the super-classifier, thenthe sub-classifier?s hypothesis is changed to nega-tive.We now move on to consider how this basic ap-proach to decision detection can be implementedin a real-time system.5 Design considerations for our real-timesystemA real-time decision detector should detect deci-sions as soon after they are made as possible.
It isfor this reason that we have set our real-time de-tector to automatically run at frequent and regularintervals during a meeting.
An alternative wouldbe to give the user (a meeting participant) respon-sibility for instructing the detector when to run.However, a user may sometimes leave substantialgaps between giving run commands.
When thishappens, the detector will have to process a largenumber of utterances in a single run, and so theuser may wait some time before being presentedwith any results.
In addition, giving the user re-sponsibility for instructing the detector when to1136Figure 1: Decision discussion regions hypothesized byconsecutive runs overlap (D1to D3and D2to D4) and soare merged.run means burdening the user with an extra task toperform during the meeting, and this goes againstthe general philosophy behind the system?s devel-opment.
The system is intended to be as unobtru-sive as possible during the meeting, and to relieveusers of tasks which distract their attention awayfrom the current discussion (e.g.
note-taking), notto create new tasks, however small.Obviously, on the first occasion that the detectorruns during a meeting, it can only process ?new?
(previously unprocessed) utterances, but on sub-sequent runs, it has the option to reprocess somenumber of ?old?
utterances (utterances which ithas already processed in a previous run).
Cer-tainly, the detector should reprocess some of themost recent old utterances because it is possiblethat a decision discussion straddles these utter-ances and new utterances.
However, the number ofold utterances that are reprocessed should be lim-ited.
If the meeting has lasted a while already, thenthe processing of a large portion of the earlier oldutterances is likely to be redundant ?
it will sim-ply produce the same results for these utterancesas the previous run.The fact that the real-time detector processes re-cent old utterances means that consecutive runscan produce hypotheses for decision discussion re-gions which overlap, or which are duplicates.
Fig-ure 1 gives an example of the former.
We deal withoverlapping hypotheses by merging them into one,so forming a larger single decision discussion re-gion.
Figure 2 gives an example of duplicate hy-potheses.
Here, on run n, the detector hypothe-sizes decision discussion D1to D2, and then onrun n+1, since the bounds of this original hypoth-esis are now wholly contained within the region ofFigure 2: Consecutive runs hypothesize the same decisiondiscussion region D1to D2, and so one of the duplicates isdiscarded.old reprocessed utterances, the detector hypothe-sizes a duplicate.
We deal with such cases by dis-carding the duplicate.6 ExperimentsWe conducted various experiments related to real-time decision detection, our goal being to producea system which:?
relative to alternative versions, detects deci-sion discussions accurately,?
generates results for any portion of dialoguevery soon after that portion of dialogue hasended.The current version of our real-time detector is setto process the same number of old and new utter-ances on each run.
Here, we refer to this value as i,and hence on each run the system processes a totalof 2i utterances (i old and i new).
Another of thesystem?s characteristics is that runs take place ev-ery i utterances, meaning that as we decrease i, thesystem provides new results more frequently andis hence ?more real-time?.
One of the things weinvestigate here then, is what to set i to in orderto best satisfy the two design goals given above.Having found this value, we compare the hierar-chical real-time detector?s performance with alter-native detectors, these being:?
an off-line detector applied to off-line ASRtranscripts,?
a flat real-time detector,?
an off-line detector applied to the real-timeASR transcripts.1137Lexical unigrams after text normalizationUtterance length in words, duration inword rateSpeaker speaker ID & AMI speaker roleContext features as above for utterancesu +/- 1. .
.u +/- 5Table 2: Features for decision DA detectionNote that the off-line detectors use hierarchicalclassification, and that the flat real-time detec-tor uses a single binary classifier which treats allDDAs as members of a single merged DDA class.6.1 Classifiers and featuresAll classifiers (sub and super-classifiers) in all de-tectors are linear-kernel Support Vector Machines(SVMs), produced using SVMlight (Joachims,1999).
For the sub-classifiers, we are obviously re-stricted to using features which can be computedin a very short period of time, and in the experi-ments here, we use lexical, utterance and speakerfeatures.
These are summarized in Table 2.
Anutterance?s lexical features are the words in itstranscription, its utterance features are its dura-tion, number of words, and word rate (number ofwords divided by duration), and its speaker fea-tures are the speaker?s role (see Section 3) and ID.We also use lexical features for the previous andwhere available, next utterances: the I, RP and RRsub-classifiers use the lexical features for the pre-vious/next utterance and the A sub-classifier, thosefrom the previous/next 5 utterances.
These set-tings produced the best results in preliminary ex-periments.
We do not use DA features becausewe lack an automatic DA tagger, nor do we useprosodic features because (Fern?andez et al, 2008)was unable to derive any value from them withSVMs.6.2 EvaluationWe evaluate each of our decision detectors in 17-fold cross validations, where in each fold, the de-tector trains on 16 meetings and then tests on theremaining one.
Evaluation can be made at threelevels:1.
The sub-classifiers?
detection of each of theDDA classes.2.
The sub-classifiers?
detection of each of theDDA classes after correction by the super-classifier.Figure 3: The relationship between the number of old/newutterances processed in a single run, and the super-classifier?sF1-score.
Here the sub-classifiers use only lexical features.3.
The super-classifier?s detection of decisiondiscussion regions.For 1 and 2, we use the same lenient-match met-ric as (Fern?andez et al, 2008; Hsueh and Moore,2007), which allows a margin of 20 seconds pre-ceding and following a hypothesized DDA.
Notethat here we only give credit for hypotheses basedon a 1-1 mapping with the gold-standard labels.For 3, we follow (Fern?andez et al, 2008; Purver etal., 2007) and use a windowed metric that dividesthe dialogue into 30-second windows and evalu-ates on a per window basis.6.3 Results and analysisHere, Section 6.3.1 will present results for differ-ent values of i, the number of old/new utterancesprocessed in a single run.
Section 6.3.2 then com-pares the performance of the real-time and off-linesystems, (and also real-time systems which use hi-erarchical vs. flat classification), and Section 6.3.3presents some feature analysis.6.3.1 Varying the number of old/newutterances processed in a runFigure 3 shows the relationship between i, the set-ting for the number of old/new utterances pro-cessed in a single run, and the super-classifier?sF1-score.
Here, the sub-classifiers are using onlylexical features.
We can see from the graph thatas i increases to 15, the super-classifier?s F1-scorealso increases, but thereafter, it plateaus.
Hence15 is apparently the value which best satisfies thetwo design goals given at the start of Section 6.It should also be noted that 15 is the mean lengthof a decision discussion in our data, and so per-1138sub-classifiers superI RP RR A classifierRe .73 .73 .84 .71 .82Pr .08 .09 .03 .15 .40F1 .15 .16 .06 .25 .54Table 3: Results for the hierarchical real-timedecision detector, using lexical, utterance andspeaker features.sub-classifiers superI RP RR A classifierRe .51 .51 .10 .63 .83Pr .12 .11 .04 .15 .41F1 .19 .19 .05 .24 .55Table 4: Results for the hierarchical off-line de-cision detector on off-line ASR transcripts, usinglexical, utterance and speaker features.haps this is a transferable finding.
The mean du-ration of a run when i = 15 is approximately 4seconds, while the mean duration of 15 utterancesin our data-set is approximately 60 seconds, mean-ing that for the average case, the detector returnsthe results for the current run, long before it isdue to make the next.
Significant lee-way is per-haps necessary here, because the final version ofthe real-time detector will include a summariza-tion component which extracts key phrases fromIssue/Resolution utterances, and its processing canlast some time, even for a single decision.We should say then, that the system is notstrictly real-time because in general, it detects de-cisions soon after they are made (for examplewithin a minute), rather than immediately after.
Inthe future we intend to modify the system so thatit can run more frequently than once every i ut-terances.
However it is important that runs do notoccur too frequently ?
for example, if i = 15 andthe system runs after every utterance, then the ex-tra processing will cause it to gradually fall furtherand further behind the meeting.6.3.2 Real-time vs. off-line resultsTable 3 shows the results achieved by a hierarchi-cal real-time decision detector whose run settingsare as described above, and whose sub-classifiers3use lexical, utterance and speaker features.
Theseresults compare well with those of an equivalent3In Tables 3 to 6, sub-classifier results are post-correction(see Section 6.2).sub-classifiers superI RP RR A classifierRe .50 .51 .09 .63 .83Pr .11 .11 .03 .14 .41F1 .19 .18 .05 .23 .55Table 5: Results for the hierarchical off-line de-tector on real-time ASR transcripts, using lexical,utterance and speaker features.sub-classifiers superI RP RR A classifierRe .67 .74 .84 .66 .85Pr .07 .08 .03 .14 .41F1 .13 .15 .05 .24 .55Table 6: Results for the hierarchical real-time de-cision detector, using lexical features only.off-line detector, which are shown in Table 4.
TheF1-scores for the real-time and off-line decisionsuper-classifiers are .54 and .55 respectively, andthe difference is not statistically significant.
Thismay indicate that the hierarchical classification ap-proach is fairly robust to increasing ASR WordError Rates (WERs).
Combining the output fromeach of the independent sub-classifiers might com-pensate somewhat for any decreases in their indi-vidual accuracy, as there was here for the I and RPsub-classifiers.The hierarchical real-time detector?s F1-score isalso 10 points higher than a flat classifier (.54 vs..44).
Hence, while Fern?andez et al (2008) demon-strated that the hierarchical classification approachcould improve off-line decision detection, we havedemonstrated here that it can also improve real-time decision detection.Table 5 shows the results when an off-linedetector is applied to real-time ASR transcripts.Here, the super-classifier obtains an F1-score of.55, one point higher than the real-time detector,but again, the difference is not statistically signifi-cant.6.3.3 Feature analysisWe also investigated the contribution of the ut-terance and speaker features.
Table 6 shows theresults for the hierarchical real-time decision de-tector when its sub-classifiers use only lexical fea-tures.
The sub-classifier F1-scores are all slightlylower than when utterance and speaker featuresare used (see Table 3), and the super-classifier1139score is only 1 point different.
None of these dif-ferences are statistically significant.Since lexical features are important, we used in-formation gain in order to investigate which wordsare predictive of each DDA type.
Due to differ-ences in the transcripts, the predictive words forthe off-line and real-time systems are not the same,but we can find commonalities, and these com-monalities make sense given the DDA definitions.Firstly in Resolution and particularly Issue DAs,some of the most predictive words could be usedto define discussion topics, and so we might ex-pect to find them in the meeting agenda.
Exam-ples are ?energy?, and ?color?.
Predictive wordsfor Resolutions also include semantically-relatedwords which are key in defining the decision (?ki-netic?,?green?).
Additional predictive words forRPs are the personal pronouns ?I?
and ?we?,and the verbs, ?think?
and ?like?, and for RRs,words which we would associate with summingup (?consensus?, ?definitely?, and ?okay?).
Un-surprisingly, for Agreements, ?yeah?
and ?okay?both score very highly.7 Conclusion(Fern?andez et al, 2008) described an approachto decision detection in multi-party meetings anddemonstrated how it could work relatively well inan off-line system.
The approach has two definingcharacteristics.
The first is its use of an annota-tion scheme which distinguishes between differ-ent utterance types based on the roles which theyplay in the decision-making process.
The secondis its use of hierarchical classification, wherebybinary sub-classifiers detect instances of each ofthe decision DAs (DDAs), and then based on thesub-classifier hypotheses, a super-classifier deter-mines which regions of dialogue are decision dis-cussions.In this paper then, we have taken the same ba-sic approach to decision detection as Fern?andez etal.
(2008), but changed the way in which it is im-plemented so that it can work effectively in real-time.
Our implementation changes include run-ning the detector at regular and frequent intervalsduring the meeting, and reprocessing recent utter-ances in case a decision discussion straddles theseand brand new utterances.
The fact that the de-tector reprocesses utterances means that on con-secutive runs, overlapping and duplicate hypothe-sized decision discussions are possible.
We havetherefore added facilities to merge overlapping hy-potheses and to remove duplicates.In general, the resulting system is able to detectdecisions soon after they are made (for examplewithin a minute), rather than immediately after.
Ithas performed well in testing, achieving an F1-score of .54, which is only one point lower thanan equivalent off-line system, and in any case, thedifference was not statistically significant.
A flatreal-time detector achieved .44.In future work, we plan to extend the decisiondiscussion annotation scheme and try to extractsupporting arguments for decisions.
We will alsoexperiment with using sequential models in orderto try to exploit any sequential ordering patterns inthe occurrence of the DDAs.Acknowledgements This material is basedupon work supported by the Defense AdvancedResearch Projects Agency (DARPA) under Con-tract No.
FA8750-07-D-0185/0004, and by theDepartment of the Navy Office of Naval Research(ONR) under Grants No.
N00014-05-1-0187 andN00014-09-1-0106.
Any opinions, findings andconclusions or recommendations expressed inthis material are those of the authors and do notnecessarily reflect the views of DARPA or ONR.We are grateful to the three anonymous EMNLPreviewers for their helpful comments and sugges-tions, and to our partners at SRI International whoprovided us with off-line and real-time transcriptsfor our meeting data.ReferencesSatanjeev Banerjee, Carolyn Ros?e, and Alex Rudnicky.2005.
The necessity of a meeting recording andplayback system, and the benefit of topic-level anno-tations to meeting browsing.
In Proceedings of the10th International Conference on Human-ComputerInteraction.Susanne Burger, Victoria MacLaren, and Hua Yu.2002.
The ISL Meeting Corpus: The impact ofmeeting type on speech style.
In Proceedings of the7th International Conference on Spoken LanguageProcessing (INTERSPEECH - ICSLP), Denver, Col-orado.Raquel Fern?andez, Matthew Frampton, Patrick Ehlen,Matthew Purver, and Stanley Peters.
2008.
Mod-elling and detecting decisions in multi-party dia-logue.
In Proc.
of the 9th SIGdial Workshop on Dis-course and Dialogue.Michel Galley, Kathleen McKeown, Julia Hirschberg,and Elizabeth Shriberg.
2004.
Identifying agree-1140ment and disagreement in conversational speech:Use of Bayesian networks to model pragmatic de-pendencies.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Lin-guistics (ACL).Daniel Gatica-Perez, Ian McCowan, Dong Zhang, andSamy Bengio.
2005.
Detecting group interest levelin meetings.
In IEEE International Conference onAcoustics, Speech, and Signal Processing (ICASSP).Dustin Hillard, Mari Ostendorf, and ElisabethShriberg.
2003.
Detection of agreement vs. dis-agreement in meetings: Training with unlabeleddata.
In Companion Volume of the Proceedings ofHLT-NAACL 2003 - Short Papers, Edmonton, Al-berta, May.Pey-Yun Hsueh and Johanna Moore.
2007.
Automaticdecision detection in meeting speech.
In Proceed-ings of MLMI 2007, Lecture Notes in Computer Sci-ence.
Springer-Verlag.Adam Janin, Jeremy Ang, Sonali Bhagat, RajdipDhillon, Jane Edwards, Javier Marc?
?as-Guarasa,Nelson Morgan, Barbara Peskin, Elizabeth Shriberg,Andreas Stolcke, Chuck Wooters, and Britta Wrede.2004.
The ICSI meeting project: Resources and re-search.
In Proceedings of the 2004 ICASSP NISTMeeting Recognition Workshop.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In B. Sch?olkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods ?Support Vector Learning.
MIT Press.Agnes Lisowska, Andrei Popescu-Belis, and SusanArmstrong.
2004.
User query analysis for the spec-ification and evaluation of a dialogue processing andretrieval system.
In Proceedings of the 4th Interna-tional Conference on Language Resources and Eval-uation.Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,S.
Bourban, M. Flynn, M. Guillemot, T. Hain,J.
Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,M.
Lincoln, A. Lisowska, W. Post, D. Reidsma, andP.
Wellner.
2005.
The AMI Meeting Corpus.
InProceedings of Measuring Behavior, the 5th Inter-national Conference on Methods and Techniques inBehavioral Research, Wageningen, Netherlands.Matthew Purver, John Dowding, John Niekrasz,Patrick Ehlen, Sharareh Noorbaloochi, and StanleyPeters.
2007.
Detecting and summarizing actionitems in multi-party dialogue.
In Proceedings of the8th SIGdial Workshop on Discourse and Dialogue,Antwerp, Belgium.Andreas Stolcke, Xavier Anguera, Kofi Boakye,?Ozg?urC?etin, Adam Janin, Matthew Magimai-Doss, ChuckWooters, and Jing Zheng.
2008.
The ICSI-SRISpring 2007 meeting and lecture recognition system.In Proc.
of CLEAR 2007 and RT2007.Daan Verbree, Rutger Rienks, and Dirk Heylen.
2006.First steps towards the automatic construction ofargument-diagrams from real discussions.
In Pro-ceedings of the 1st International Conference onComputational Models of Argument, volume 144,pages 183?194.
IOS press.A.
Waibel, T. Schultz, M. Bett, M. Denecke, R. Malkin,I.
Rogina, R. Stiefelhagen, and J. Yang.
2003.SMaRT: The smart meeting room task at ISL.
InICASSP.Steve Whittaker, Rachel Laban, and Simon Tucker.2006.
Analysing meeting records: An ethnographicstudy and technological implications.
In S. Renalsand S. Bengio, editors, Machine Learning for Multi-modal Interaction: Second International Workshop,MLMI 2005, Revised Selected Papers, volume 3869of Lecture Notes in Computer Science, pages 101?113.
Springer.Britta Wrede and Elizabeth Shriberg.
2003.
Spotting?hot spots?
in meetings: Human judgements andprosodic cues.
In Proceedings of the 9th EuropeanConference on Speech Communication and Technol-ogy, Geneva, Switzerland.1141
