Proceedings of ACL-08: HLT, pages 389?397,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsName Translation in Statistical Machine TranslationLearning When to TransliterateUlf Hermjakob and Kevin KnightUniversity of Southern CaliforniaInformation Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292, USAfulf,knightg@isi.eduHal Daume?
IIIUniversity of UtahSchool of Computing50 S Central Campus DriveSalt Lake City, UT 84112, USAme@hal3.nameAbstractWe present a method to transliterate namesin the framework of end-to-end statisticalmachine translation.
The system is trainedto learn when to transliterate.
For Arabicto English MT, we developed and trained atransliterator on a bitext of 7 million sen-tences and Google?s English terabyte ngramsand achieved better name translation accuracythan 3 out of 4 professional translators.
Thepaper also includes a discussion of challengesin name translation evaluation.1 IntroductionState-of-the-art statistical machine translation(SMT) is bad at translating names that are not verycommon, particularly across languages with differ-ent character sets and sound systems.
For example,consider the following automatic translation:1Arabic input 	?
AK.???
P@P???
pAK.?J??JJ?J????J?@P???JJKA?kP??A????????JJK.??JJ????QK.
?SMT output musicians such as BachCorrect translation composers such as Bach,Mozart, Chopin, Beethoven, Schumann,Rachmaninoff, Ravel and ProkofievThe SMT system drops most names in this ex-ample.
?Name dropping?
and mis-translation hap-pens when the system encounters an unknown word,mistakes a name for a common noun, or trains onnoisy parallel data.
The state-of-the-art is poor for1taken from NIST02-05 corporatwo reasons.
First, although names are important tohuman readers, automatic MT scoring metrics (suchas BLEU) do not encourage researchers to improvename translation in the context of MT.
Names arevastly outnumbered by prepositions, articles, adjec-tives, common nouns, etc.
Second, name translationis a hard problem ?
even professional human trans-lators have trouble with names.
Here are four refer-ence translations taken from the same corpus, withmistakes underlined:Ref1 composers such as Bach, missing nameChopin, Beethoven, Shumann, Rakmaninov,Ravel and ProkovievRef2 musicians such as Bach, Mozart, Chopin,Bethoven, Shuman, Rachmaninoff, Rafael andBrokovievRef3 composers including Bach, Mozart, Schopen,Beethoven, missing name Raphael, Rahmanievand BrokofienRef4 composers such as Bach, Mozart, missingname Beethoven, Schumann, Rachmaninov,Raphael and ProkofievThe task of transliterating names (independent ofend-to-end MT) has received a significant amountof research, e.g., (Knight and Graehl, 1997; Chen etal., 1998; Al-Onaizan, 2002).
One approach is to?sound out?
words and create new, plausible target-language spellings that preserve the sounds of thesource-language name as much as possible.
Anotherapproach is to phonetically match source-languagenames against a large list of target-language words389and phrases.
Most of this work has been discon-nected from end-to-end MT, a problem which weaddress head-on in this paper.The simplest way to integrate name handling intoSMT is: (1) run a named-entity identification systemon the source sentence, (2) transliterate identifiedentities with a special-purpose transliteration com-ponent, and (3) run the SMT system on the sourcesentence, as usual, but when looking up phrasaltranslations for the words identified in step 1, insteaduse the transliterations from step 2.Many researchers have attempted this, and it doesnot work.
Typically, translation quality is degradedrather than improved, for the following reasons: Automatic named-entity identification makeserrors.
Some words and phrases that shouldnot be transliterated are nonetheless sent to thetransliteration component, which returns a badtranslation. Not all named entities should be transliterated.Many named entities require a mix of translit-eration and translation.
For example, in the pairA JKP??
J?A ?
H.?J k./jnub kalyfurnya/SouthernCalifornia, the first Arabic word is translated,and the second word is transliterated. Transliteration components make errors.
Thebase SMT system may translate a commonly-occurring name just fine, due to the bitext it wastrained on, while the transliteration componentcan easily supply a worse answer. Integration hobbles SMT?s use of longerphrases.
Even if the named-entity identifi-cation and transliteration components operateperfectly, adopting their translations means thatthe SMT system may no longer have access tolonger phrases that include the name.
For ex-ample, our base SMT system translates ?JKP?J K.?
?
Z@PP?
?
@ (as a whole phrase) to ?Pre-mier Li Peng?, based on its bitext knowledge.However, if we force 	?
J K.?
?
to translate asa separate phrase to ?Li Peng?, then the termZ @PP??
@ ?JKP becomes ambiguous (with trans-lations including ?Prime Minister?, ?Premier?,etc.
), and we observe incorrect choices beingsubsequently made.To spur better work in name handling, an ACEentity-translation pilot evaluation was recently de-veloped (Day, 2007).
This evaluation involvesa mixture of entity identification and translationconcerns?for example, the scoring system asks forcoreference determination, which may or may not beof interest for improving machine translation output.In this paper, we adopt a simpler metric.
We ask:what percentage of source-language named entitiesare translated correctly?
This is a precision metric.We can readily apply it to any base SMT system, andto human translations as well.
Our goal in augment-ing a base SMT system is to increase this percentage.A secondary goal is to make sure that our overalltranslation quality (as measured by BLEU) does notdegrade as a result of the name-handling techniqueswe introduce.
We make all our measurements on anArabic/English newswire translation task.Our overall technical approach is summarizedhere, along with references to sections of this paper: We build a component for transliterating be-tween Arabic and English (Section 3). We automatically learn to tag those words andphrases in Arabic text, which we believe thetransliteration component will translate cor-rectly (Section 4). We integrate suggested transliterations into thebase SMT search space, with their use con-trolled by a feature function (Section 5). We evaluate both the base SMT system and theaugmented system in terms of entity translationaccuracy and BLEU (Sections 2 and 6).2 EvaluationIn this section we present the evaluation method thatwe use to measure our system and also discuss chal-lenges in name transliteration evaluation.2.1 NEWA Evaluation MetricGeneral MT metrics such as BLEU, TER, METEORare not suitable for evaluating named entity transla-tion and transliteration, because they are not focusedon named entities (NEs).
Dropping a comma or a theis penalized as much as dropping a name.
We there-fore use another metric, jointly developed with BBNand LanguageWeaver.390The general idea of the Named Entity Weak Ac-curacy (NEWA) metric is to Count number of NEs in source text: N Count number of correctly translated NEs: C Divide C/N to get an accuracy figureIn NEWA, an NE is counted as correctly translatedif the target reference NE is found in the MT out-put.
The metric has the advantage that it is easy tocompute, has no special requirements on an MT sys-tem (such as depending on source-target word align-ment) and is tokenization independent.In the result section of this paper, we will use theNEWA metric to measure and compare the accuracyof NE translations in our end-to-end SMT transla-tions and four human reference translations.2.2 Annotated CorpusBBN kindly provided us with an annotated Arabictext corpus, in which named entities were markedup with their type (e.g.
GPE for Geopolitical Entity)and one or more English translations.
Example:?
?<GPE alt=?Termoli?>???
?QJK</GPE><PER alt=?Abdullah II j Abdallah II?> ?
?
?
@ Y J.??KAJ?
@</PER>The BBN annotations exhibit a number of issues.For the English translations of the NEs, BBN anno-tators looked at human reference translations, whichmay introduce a bias towards those human transla-tions.
Specifically, the BBN annotations are some-times wrong, because the reference translations werewrong.
Consider for example the Arabic phrase?
??
?Q JK ???
@QKP?
K.?J ?
?
(mSn?
burtranfY tyrmulY), which means Powertrain plant in Ter-moli.
The mapping from tyrmulY to Termoli is notobvious, and even less the one from burtran to Pow-ertrain.
The human reference translations for thisphrase are1.
Portran site in Tremolo2.
Termoli plant (one name dropped)3.
Portran in Tirnoli4.
Portran assembly plant, in TirmoliThe BBN annotators adopted the correct transla-tion Termoli, but also the incorrect Portran.
Inother cases the BBN annotators adopted both a cor-rect (Khatami) and an incorrect translation (Kha-timi) when referring to the former Iranian president,which would reward a translation with such an in-correct spelling. <PER alt=?KhatamijKhatimi?>?
?KA 	k</PER> <GPE alt=?the American?> ?J?QJ?A?
@</GPE>In other cases, all translations are correct, but ad-ditional correct translations are missing, as for ?theAmerican?
above, for which ?the US?
is an equallyvalid alternative in the specific sentence it was anno-tated in.All this raises the question of what is a correctanswer.
For most Western names, there is normallyonly one correct spelling.
We follow the same con-ventions as standard media, paying attention to howan organization or individual spells its own name,e.g.
Senator Jon Kyl, not Senator John Kyle.
ForArabic names, variation is generally acceptable ifthere is no one clearly dominant spelling in English,e.g.
GaddafijGadhafijQaddafijQadhafi, as long as agiven variant is not radically rarer than the most con-ventional or popular form.2.3 Re-AnnotationBased on the issues we found with the BBN annota-tions, we re-annotated a sub-corpus of 637 sentencesof the BBN gold standard.We based this re-annotation on detailed annota-tion guidelines and sample annotations that had pre-viously been developed in cooperation with Lan-guageWeaver, building on three iterations of test an-notations with three annotators.We checked each NE in every sentence, usinghuman reference translations, automatic translitera-tor output, performing substantial Web research formany rare names, and checked Google ngrams andcounts for the general Web and news archives to de-termine whether a variant form met our threshold ofoccurring at least 20% as often as the most dominantform.3 TransliteratorThis section describes how we transliterate Arabicwords or phrases.
Given a word such as 	??
JJK A?kPor a phrase such as ?J?
@P ?KP?
?, we want to findthe English transliteration for it.
This is not just a391romanization like rHmanynuf and murys rafyl forthe examples above, but a properly spelled Englishname such as Rachmaninoff and Maurice Ravel.
Thetransliteration result can contain several alternatives,e.g.
RachmaninoffjRachmaninov.
Unlike variousgenerative approaches (Knight and Graehl, 1997;Stalls and Knight, 1998; Li et al, 2004; Matthews,2007; Sherif and Kondrak, 2007; Kashani et al,2007), we do not synthesize an English spellingfrom scratch, but rather find a translation in verylarge lists of English words (3.4 million) and phrases(47 million).We develop a similarity metric for Arabic and En-glish words.
Since matching against millions of can-didates is computationally prohibitive, we store theEnglish words and phrases in an index, such thatgiven an Arabic word or phrase, we quickly retrievea much smaller set of likely candidates and applyour similarity metric to that smaller list.We divide the task of transliteration into twosteps: given an Arabic word or phrase to translit-erate, we (1) identify a list of English translitera-tion candidates from indexed lists of English wordsand phrases with counts (section 3.1) and (2) com-pute for each English name candidate the cost forthe Arabic/English name pair (transliteration scor-ing model, section 3.2).We then combine the count information with thetransliteration cost according to the formula:score(e) = log(count(e))/20 - translit cost(e,f)3.1 Indexing with consonant skeletonsWe identify a list of English transliteration candi-dates through what we call a consonant skeleton in-dex.
Arabic consonants are divided into 11 classes,represented by letters b,f,g,j,k,l,m,n,r,s,t.
In a one-time pre-processing step, all 3,420,339 (unique) En-glish words from our English unigram languagemodel (based on Google?s Web terabyte ngram col-lection) that might be names or part of names(mostly based on capitalization) are mapped to oneor more skeletons, e.g.Rachmaninoff !
rkmnnf, rmnnf, rsmnnf, rtsmnnfThis yields 10,381,377 skeletons (average of 3.0 perword) for which a reverse index is created (withcounts).
At run time, an Arabic word to be translit-erated is mapped to its skeleton, e.g.?
?JJK A?kP !
rmnnfThis skeleton serves as a key for the previously builtreverse index, which then yields the list of Englishcandidates with counts:rmnnf !
Rachmaninov (186,216), Rachmaninoff(179,666), Armenonville (3,445), Rachmaninow(1,636), plus 8 others.Shorter words tend to produce more candidates, re-sulting in slower transliteration, but since there arerelatively few unique short words, this can be ad-dressed by caching transliteration results.The same consonant skeleton indexing process isapplied to name bigrams (47,700,548 unique with167,398,054 skeletons) and trigrams (46,543,712unique with 165,536,451 skeletons).3.2 Transliteration scoring modelThe cost of an Arabic/English name pair is com-puted based on 732 rules that assign a cost to a pairof Arabic and English substrings, allowing for oneor more context restrictions.1.
?
::q == ::02.
??
::ough == ::03. h::ch == :[aou],::0.14.
?
::k == ,$:,$::0.1 ; ::0.25.
Z:: == :,EC::0.1The first example rule above assigns to thestraightforward pair ?/q a cost of 0.
The second ruleincludes 2 letters on the Arabic and 4 on the Englishside.
The third rule restricts application to substringpairs where the English side is preceded by the let-ters a, o, or u.
The fourth rule specifies a cost of 0.1if the substrings occur at the end of (both) names,0.2 otherwise.
According to the fifth rule, the Ara-bic letter Z may match an empty string on the En-glish side, if there is an English consonant (EC) inthe right context of the English side.The total cost is computed by always applying thelongest applicable rule, without branching, result-ing in a linear complexity with respect to word-pairlength.
Rules may include left and/or right contextfor both Arabic and English.
The match fails if norule applies or the accumulated cost exceeds a presetlimit.Names may have n words on the English and m onthe Arabic side.
For example, New York is one wordin Arabic and Abdullah is two words in Arabic.
The392rules handle spaces (as well as digits, apostrophesand other non-alphabetic material) just like regularalphabetic characters, so that our system can handlecases like where words in English and Arabic namesdo not match one to one.The French name Beaujolais ( ?J??k.
?K./bujulyh)deviates from standard English spelling conventionsin several places.
The accumulative cost from therules handling these deviations could become pro-hibitive, with each cost element penalizing the sameunderlying offense ?
being French.
We solve thisproblem by allowing for additional context in theform of style flags.
The rule for matching eau/?specifies, in addition to a cost, an (output) style flag+fr (as in French), which in turn serves as an ad-ditional context for the rule that matches ais/ ?Kata much reduced cost.
Style flags are also used forsome Arabic dialects.
Extended characters such ase?, o?, and s?
and spelling idiosyncrasies in names onthe English side of the bitext that come from variousthird languages account for a significant portion ofthe rule set.Casting the transliteration model as a scoringproblem thus allows for very powerful rules withstrong contexts.
The current set of rules has beenbuilt by hand based on a bitext development corpus;future work might include deriving such rules auto-matically from a training set of transliterated names.This transliteration scoring model described inthis section is used in two ways: (1) to transliter-ate names at SMT decoding time, and (2) to identifytransliteration pairs in a bitext.4 Learning what to transliterateAs already mentioned in the introduction, namedentity (NE) identification followed by MT is a badidea.
We don?t want to identify NEs per se anyway?
we want to identify things that our transliteratorwill be good at handling, i.e., things that should betransliterated.
This might even include loanwordslike bnk (bank) and brlman (parliament), but wouldexclude names such as National Basketball Associ-ation that are often translated rather transliterated.Our method follows these steps:1.
Take a bitext.2.
Mark the Arabic words and phrases that have arecognizable transliteration on the English side.3.
Remove the English side of the bitext.4.
Divide the annotated Arabic corpus into a train-ing and test corpus.5.
Train a monolingual Arabic tagger to identifywhich words and phrases (in running Arabic)are good candidates for transliteration (section4.2)6.
Apply the tagger to test data and evaluate itsaccuracy.4.1 Mark-up of bitextGiven a tokenized (but unaligned and mixed-case)bitext, we mark up that bitext with links betweenArabic and English words that appear to be translit-erations.
In the following example, linked words areunderlined, with numbers indicating what is linked.English The meeting was attended by Omani (1)Secretary of State for Foreign Affairs Yusif (2)bin (3) Alawi (6) bin (8) Abdallah (10) andSpecial Advisor to Sultan (12) Qabus (13)for Foreign Affairs Umar (14) bin (17)Abdul Munim (19) al-Zawawi (21).Arabic (translit.)
uHDr allqa?
uzyr alduleal?manY (1) llsh?uun alkharjye yusf (2) bn (3)?luY (6) bn (8) ?bd allh (10) ualmstshar alkhaSllslTan (12) qabus (13) ll?laqat alkharjye ?mr (14)bn (17) ?bd almn?m (19) alzuauY (21) .For each Arabic word, the linking algorithm triesto find a matching word on the English side, usingthe transliteration scoring model described in sec-tion 3.
If the matcher reaches the end of an Arabicor English word before reaching the end of the other,it continues to ?consume?
additional words until aword-boundary observing match is found or the costthreshold exceeded.When there are several viable linking alternatives,the algorithm considers the cost provided by thetransliteration scoring model, as well as context toeliminate inferior alternatives, so that for examplethe different occurrences of the name particle binin the example above are linked to the proper Ara-bic words, based on the names next to them.
Thenumber of links depends, of course, on the specificcorpus, but we typically identify about 3.0 links persentence.The algorithm is enhanced by a number of heuris-tics:393 English match candidates are restricted to cap-italized words (with a few exceptions). We use a list of about 200 Arabic and Englishstopwords and stopword pairs. We use lists of countries and their adjectiveforms to bridge cross-POS translations suchas Italy?s president on the English and ?JKP??A?KA?
@ (?Italian president?)
on the Arabic side. Arabic prefixes such as ?/l- (?to?)
are treatedin a special way, because they are translated,not transliterated like the rest of the word.
Link(12) above is an example.In this bitext mark-up process, we achieve 99.5%precision and 95% recall based on a manualvisualization-tool based evaluation.
Of the 5% re-call error, 3% are due to noisy data in the bitext suchas typos, incorrect translations, or names missing onone side of the bitext.4.2 Training of Arabic name taggerThe task of the Arabic name tagger (or moreprecisely, ?transliterate-me?
tagger) is to predictwhether or not a word in an Arabic text should betransliterated, and if so, whether it includes a prefix.Prefixes such as ?/u- (?and?)
have to be translatedrather than transliterated, so it is important to splitoff any prefix from a name before transliterating thatname.
This monolingual tagging task is not trivial,as many Arabic words can be both a name and a non-name.
For example, ?QKQj.?
@ (aljzyre) can mean bothAl-Jazeera and the island (or peninsula).Features include the word itself plus two wordsto the left and right, along with various prefixes,suffixes and other characteristics of all of them, to-talling about 250 features.Some of our features depend on large corpusstatistics.
For this, we divide the tagged Arabicside of our training corpus into a stat section anda core training section.
From the stat section we col-lect statistics as to how often every word, bigram ortrigram occurs, and what distribution of name/non-name patterns these ngrams have.
The name distri-bution bigram?KP???@?QKQj.?
@ 3327 00:133 01:3193 11:1(aljzyre alkurye/?peninsula Korean?)
for exampletells us that in 3193 out of 3327 occurrences in thestat corpus bitext, the first word is a marked up asa non-name (?0?)
and the second as a name (?1?
),which strongly suggests that in such a bigram con-text, aljzyre better be translated as island or penin-sula, and not be transliterated as Al-Jazeera.We train our system on a corpus of 6 million statsentences, and 500; 000 core training sentences.
Weemploy a sequential tagger trained using the SEARNalgorithm (Daume?
III et al, 2006) with aggressiveupdates ( = 1).
Our base learning algorithmis an averaged perceptron, as implemented in theMEGAM package2.Reference Precision Recall F-meas.Raw test corpus 87.4% 95.7% 91.4%Adjusted for GS 92.1% 95.9% 94.0%deficienciesTable 1: Accuracy of ?transliterate-me?
taggerTesting on 10,000 sentences, we achieve preci-sion of 87.4% and a recall of 95.7% with respect tothe automatically marked-up Gold Standard as de-scribed in section 4.1.
A manual error analysis of500 sentences shows that a large portion are not er-rors after all, but have been marked as errors becauseof noise in the bitext and errors in the bitext mark-up.
After adjusting for these deficiencies in the goldstandard, we achieve precision of 92.1% and recallof 95.9% in the name tagging task.5 Integration with SMTWe use the following method to integrate ourtransliterator into the overall SMT system:1.
We tag the Arabic source text using the taggerdescribed in the previous section.2.
We apply the transliterator described in section3 to the tagged items.
We limit this transliter-ation to words that occur up to 50 times in thetraining corpus for single token names (or upto 100 and 150 times for two and three-wordnames).
We do this because the general SMTmechanism tends to do well on more commonnames, but does poorly on rare names (and will2Freely available at http://hal3.name/megam394always drop names it has never seen in thetraining bitext).3.
On the fly, we add transliterations to SMTphrase table.
Instead of a phrasal probability,the transliterations have a special binary featureset to 1.
In a tuning step, the Minimim ErrorRate Training component of our SMT systemiteratively adjusts the set of rule weights, in-cluding the weight associated with the translit-eration feature, such that the English transla-tions are optimized with respect to a set ofknown reference translations according to theBLEU translation metric.4.
At run-time, the transliterations then competewith the translations generated by the gen-eral SMT system.
This means that the MTsystem will not always use the transliteratorsuggestions, depending on the combination oflanguage model, translation model, and othercomponent scores.5.1 Multi-token namesWe try to transliterate names as much as possible incontext.
Consider for example the Arabic name:?J??
?K.@??
?K(?yusf abu Sfye?
)If transliterated as single words without context,the top results would be JosephjJosefjYusufjYosefjYoussef, AbujAbojIvojApojIbo, and SephiajSofiajSophiajSafiehjSafia respectively.
However, whentransliterating the three words together against ourlist of 47 million English trigrams (section 3), thetransliterator will select the (correct) translationYousef Abu Safieh.
Note that Yousef was not amongthe top 5 choices, and that Safieh was only choice 4.Similarly, when transliterating 	?
A K.???
P@P?
?
?/umuzar ushuban (?and Mozart and Chopin?)
with-out context, the top results would be MoserjMauserjMozerjMozartjMouser and ShuppanjShoppingjSchwabenjSchuppanjShobana (with Chopin waydown on place 22).
Checking our large English listsfor a matching name, name pattern, the transliteratoridentifies the correct translation ?, Mozart, Chopin?.Note that the transliteration module provides theoverall SMT system with up to 5 alternatives,augmented with a choice of English translationsfor the Arabic prefixes like the comma and theconjunction and in the last example.6 End-to-End resultsWe applied the NEWA metric (section 2) to bothour SMT translations as well as the four human ref-erence translations, using both the original named-entity translation annotation and the re-annotation:Gold Standard BBN GS Re-annotated GSHuman 1 87.0% 85.0%Human 2 85.3% 86.9%Human 3 90.4% 91.8%Human 4 86.5% 88.3%SMT System 80.4% 89.7%Table 2: Name translation accuracy with respect to BBNand re-annotated Gold Standard on 1730 named entitiesin 637 sentences.Almost all scores went up with re-annotations, be-cause the re-annotations more properly reward cor-rect answers.Based on the original annotations, all humanname translations were much better than our SMTsystem.
However, based on our re-annotation, theresults are quite different: our system has a higherNEWA score and better name translations than 3 outof 4 human annotators.The evaluation results confirm that the originalannotation method produced a relative bias towardsthe human translation its annotations were largelybased on, compared to other translations.Table 3 provides more detailed NEWA results.The addition of the transliteration module improvesour overall NEWA score from 87.8% to 89.7%, arelative gain of 16% over base SMT system.
Fornames of persons (PER) and facilities (FAC), oursystem outperforms all human translators.
Hu-mans performed much better on Person Nominals(PER.Nom) such as Swede, Dutchmen, Americans.Note that name translation quality varies greatlybetween human translators, with error rates rangingfrom 8.2-15.0% (absolute).To make sure our name transliterator does not de-grade the overall translation quality, we evaluatedour base SMT system with BLEU, as well as ourtransliteration-augmented SMT system.
Our stan-dard newswire training set consists of 10.5 millionwords of bitext (English side) and 1491 test sen-395NE Type Count Baseline SMT with Human 1 Human 2 Human 3 Human 4SMT TransliterationPER 342 266 (77.8%) 280 (81.9%) 210 (61.4%) 265 (77.5%) 278 (81.3%) 275 (80.4%)GPE 910 863 (94.8%) 877 (96.4%) 867 (95.3%) 849 (93.3%) 885 (97.3%) 852 (93.6%)ORG 332 280 (84.3%) 282 (84.9%) 263 (79.2%) 265 (79.8%) 293 (88.3%) 281 (84.6%)FAC 27 18 (66.7%) 24 (88.9%) 21 (77.8%) 20 (74.1%) 22 (81.5%) 20 (74.1%)PER.Nom 61 49 (80.3%) 48 (78.7%) 61 (100.0%) 56 (91.8%) 60 (98.4%) 57 (93.4%)LOC 58 43 (74.1%) 41 (70.7%) 48 (82.8%) 48 (82.8%) 51 (87.9%) 43 (74.1%)All types 1730 1519 (87.8%) 1552 (89.7%) 1470 (85.0%) 1503 (86.9%) 1589 (91.8%) 1528 (88.3%)Table 3: Name translation accuracy in end-to-end statistical machine translation (SMT) system for different namedentity (NE) types: Person (PER), Geopolitical Entity, which includes countries, provinces and towns (GPE), Organi-zation (ORG), Facility (FAC), Nominal Person, e.g.
Swede (PER.Nom), other location (LOC).tences.
The BLEU scores for the two systems were50.70 and 50.96 respectively.Finally, here are end-to-end machine translationresults for three sentences, with and without thetransliteration module, along with a human refer-ence translation.Old: Al-Basha leads a broad list of musicians suchas Bach.New: Al-Basha leads a broad list of musical actssuch as Bach, Mozart, Beethoven, Chopin, Schu-mann, Rachmaninoff, Ravel and Prokofiev.Ref: Al-Bacha performs a long list of works bycomposers such as Bach, Chopin, Beethoven,Shumann, Rakmaninov, Ravel and Prokoviev.Old: Earlier Israeli military correspondent turnintroduction programme ?Entertainment Bui?New: Earlier Israeli military correspondent turn tointroduction of the programme ?Play Boy?Ref: Former Israeli military correspondent turnshost for ?Playboy?
programOld: The Nikkei president company De Beers saidthat ...New: The company De Beers chairman Nicky Op-penheimer said that ...Ref: Nicky Oppenheimer, chairman of the De Beerscompany, stated that ...7 DiscussionWe have shown that a state-of-the-art statistical ma-chine translation system can benefit from a dedi-cated transliteration module to improve the transla-tion of rare names.
Improved named entity transla-tion accuracy as measured by the NEWA metric ingeneral, and a reduction in dropped names in par-ticular is clearly valuable to the human reader ofmachine translated documents as well as for sys-tems using machine translation for further informa-tion processing.
At the same time, there has been nonegative impact on overall quality as measured byBLEU.We believe that all components can be further im-proved, e.g. Automatically retune the weights in thetransliteration scoring model. Improve robustness with respect to typos, in-correct or missing translations, and badlyaligned sentences when marking up bitexts. Add more features for learning whether or nota word should be transliterated, possibly usingsource language morphology to better identifynon-name words never or rarely seen duringtraining.Additionally, our transliteration method could be ap-plied to other language pairs.We find it encouraging that we already outper-form some professional translators in name transla-tion accuracy.
The potential to exceed human trans-lator performance arises from the patience requiredto translate names right.AcknowledgmentThis research was supported under DARPA ContractNo.
HR0011-06-C-0022.396ReferencesYaser Al-Onaizan and Kevin Knight.
2002.
MachineTransliteration of Names in Arabic Text.
In Proceed-ings of the Association for Computational LinguisticsWorkshop on Computational Approaches to SemiticLanguages.Thorsten Brants, Alex Franz.
2006.
Web 1T 5-gramVersion 1.
Released by Google through the Linguis-tic Data Consortium, Philadelphia, as LDC2006T13.Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding, andShih-Chung Tsai.
1998.
Proper Name Translation inCross-Language Information Retrieval.
In Proceed-ings of the 36th Annual Meeting of the Association forComputational Linguistics and the 17th InternationalConference on Computational Linguistics.Hal Daume?
III, John Langford, and Daniel Marcu.2006.
Search-based Structured Prediction.Submitted to the Machine Learning Journal.http://pub.hal3.name/#daume06searnDavid Day.
2007.
Entity Translation 2007 Pilot Evalua-tion (ET07).
In proceedings of the Workshop on Auto-matic Content Extraction (ACE).
College Park, Mary-land.Byung-Ju Kang and Key-Sun Choi.
2000.
AutomaticTransliteration and Back-transliteration by DecisionTree Learning.
In Conference on Language Resourcesand Evaluation.Mehdi M. Kashani, Fred Popowich, and Fatiha Sadat.2007.
Automatic Transliteration of Proper Nounsfrom Arabic to English.
The Challenge of Arabic ForNLP/MT, 76-84.Alexandre Klementiev and Dan Roth.
2006.
Namedentity transliteration and discovery from multilingualcomparable corpora.
In Proceedings of the HumanLanguage Technology Conference of the North Ameri-can Chapter of the Association of Computational Lin-guistics.Kevin Knight and Jonathan Graehl.
1997.
MachineTransliteration.
In Proceedings of the 35th AnnualMeeting of the Association for Computational Linguis-tics.Li Haizhou, Zhang Min, and Su Jian.
2004.
A JointSource-Channel Model for Machine Transliteration.In Proceedings of the 42nd Annual Meeting on Asso-ciation for Computational Linguistics.Wei-Hao Lin and Hsin-Hsi Chen.
2002.
Backward Ma-chine Transliteration by Learning Phonetic Similar-ity.
Sixth Conference on Natural Language Learning,Taipei, Taiwan, 2002.David Matthews.
2007.
Machine Transliteration ofProper Names.
Master?s Thesis.
School of Informat-ics.
University of Edinburgh.Masaaki Nagata, Teruka Saito, and Kenji Suzuki.
2001.Using the Web as a Bilingual Dictionary.
In Proceed-ings of the Workshop on Data-driven Methods in Ma-chine Translation.Bruno Pouliquen, Ralf Steinberger, Camelia Ignat, IrinaTemnikova, Anna Widiger, Wajdi Zaghouani, and JanZizka.
2006.
Multilingual Person Name Recognitionand Transliteration.
CORELA - COgnition, REpre-sentation, LAnguage, Poitiers, France.
Volume 3/3,number 2, pp.
115-123.Tarek Sherif and Grzegorz Kondrak.
2007.
Substring-Based Transliteration.
In Proceedings of the 45th An-nual Meeting on Association for Computational Lin-guistics.Richard Sproat, ChengXiang Zhai, and Tao Tao.
2006.Named Entity Transliteration with Comparable Cor-pora.
In Proceedings of the 21st International Confer-ence on Computational Linguistics and the 44th An-nual Meeting on Association for Computational Lin-guistics.Bonnie Glover Stalls and Kevin Knight.
1998.
Trans-lating Names and Technical Terms in Arabic Text.
InProceedings of the COLING/ACL Workshop on Com-putational Approaches to Semitic Languages.Stephen Wan and Cornelia Verspoor.
1998.
AutomaticEnglish-Chinese Name Transliteration for Develop-ment of Multilingual Resources.
In Proceedings of the36th Annual Meeting of the Association for Computa-tional Linguistics.
Montreal, Canada.397
