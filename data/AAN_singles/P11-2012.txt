Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 65?70,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLost in Translation: Authorship Attribution using Frame SemanticsSteffen HedegaardDepartment of Computer Science,University of CopenhagenNjalsgade 128,2300 Copenhagen S, Denmarksteffenh@diku.dkJakob Grue SimonsenDepartment of Computer Science,University of CopenhagenNjalsgade 128,2300 Copenhagen S, Denmarksimonsen@diku.dkAbstractWe investigate authorship attribution usingclassifiers based on frame semantics.
The pur-pose is to discover whether adding semanticinformation to lexical and syntactic methodsfor authorship attribution will improve them,specifically to address the difficult problem ofauthorship attribution of translated texts.
Ourresults suggest (i) that frame-based classifiersare usable for author attribution of both trans-lated and untranslated texts; (ii) that frame-based classifiers generally perform worse thanthe baseline classifiers for untranslated texts,but (iii) perform as well as, or superior tothe baseline classifiers on translated texts; (iv)that?contrary to current belief?na?ve clas-sifiers based on lexical markers may performtolerably on translated texts if the combinationof author and translator is present in the train-ing set of a classifier.1 IntroductionAuthorship attribution is the following problem: Fora given text, determine the author of said text amonga list of candidate authors.
Determining author-ship is difficult, and a host of methods have beenproposed: As of 1998 Rudman estimated the num-ber of metrics used in such methods to be at least1000 (Rudman, 1997).
For comprehensive recentsurveys see e.g.
(Juola, 2006; Koppel et al, 2008;Stamatatos, 2009).
The process of authorship at-tribution consists of selecting markers (features thatprovide an indication of the author), and classifyinga text by assigning it to an author using some appro-priate machine learning technique.1.1 Attribution of translated textsIn contrast to the general authorship attributionproblem, the specific problem of attributing trans-lated texts to their original author has received littleattention.
Conceivably, this is due to the commonintuition that the impact of the translator may addenough noise that proper attribution to the originalauthor will be very difficult; for example, in (Arunet al, 2009) it was found that the imprint of thetranslator was significantly greater than that of theoriginal author.
The volume of resources for nat-ural language processing in English appears to bemuch larger than for any other language, and it isthus, conceivably, convenient to use the resources athand for a translated version of the text, rather thanthe original.To appreciate the difficulty of purely lexical orsyntactic characterization of authors based on trans-lation, consider the following excerpts from threedifferent translations of the first few paragraphs ofTurgenev?s Dvornskoe Gnezdo:Liza "A nest of nobles" Translated by W. R. Shedden-RalstonA beautiful spring day was drawing to a close.
Highaloft in the clear sky floated small rosy clouds,which seemed never to drift past, but to be slowlyabsorbed into the blue depths beyond.At an open window, in a handsome mansion situ-ated in one of the outlying streets of O., the chieftown of the government of that name?it was in theyear 1842?there were sitting two ladies, the oneabout fifty years old, the other an old woman ofseventy.A Nobleman?s Nest Translated by I. F. HapgoodThe brilliant, spring day was inclining toward the65evening, tiny rose-tinted cloudlets hung high in theheavens, and seemed not to be floating past, but re-treating into the very depths of the azure.In front of the open window of a handsome house,in one of the outlying streets of O * * * the capitalof a Government, sat two women; one fifty years ofage, the other seventy years old, and already aged.A House of Gentlefolk Translated by C. GarnettA bright spring day was fading into evening.
Highoverhead in the clear heavens small rosy cloudsseemed hardly to move across the sky but to besinking into its depths of blue.In a handsome house in one of the outlying streetsof the government town of O?- (it was in the year1842) two women were sitting at an open window;one was about fifty, the other an old lady of seventy.As translators express the same semantic contentin different ways the syntax and style of differenttranslations of the same text will differ greatly dueto the footprint of the translators; this footprint mayaffect the classification process in different ways de-pending on the features.For markers based on language structure such asgrammar or function words it is to be expected thatthe footprint of the translator has such a high im-pact on the resulting text that attribution to the au-thor may not be possible.
However, it is possi-ble that a specific author/translator combination hasits own unique footprint discernible from other au-thor/translator combinations: A specific translatormay often translate often used phrases in the sameway.
Ideally, the footprint of the author is (more orless) unaffected by the process of translation, for ex-ample if the languages are very similar or the markeris not based solely on lexical or syntactic features.In contrast to purely lexical or syntactic features,the semantic content is expected to be, roughly, thesame in translations and originals.
This leads us tohypothesize that a marker based on semantic framessuch as found in the FrameNet database (Ruppen-hofer et al, 2006), will be largely unaffected bytranslations, whereas traditional lexical markers willbe severely impacted by the footprint of the transla-tor.The FrameNet project is a database of annotatedexemplar frames, their relations to other frames andobligatory as well as optional frame elements foreach frame.
FrameNet currently numbers approxi-mately 1000 different frames annotated with naturallanguage examples.
In this paper, we combine thedata from FrameNet with the LTH semantic parser(Johansson and Nugues, 2007), until very recently(Das et al, 2010) the semantic parser with best ex-perimental performance (note that the performanceof LTH on our corpora is unknown and may dif-fer from the numbers reported in (Johansson andNugues, 2007)).1.2 Related workThe research on authorship attribution is too volu-minous to include; see the excellent surveys (Juola,2006; Koppel et al, 2008; Stamatatos, 2009) foran overview of the plethora of lexical and syntac-tic markers used.
The literature on the use of se-mantic markers is much scarcer: Gamon (Gamon,2004) developed a tool for producing semantic de-pendency graphs and using the resulting informationin conjunction with lexical and syntactic markers toimprove the accuracy of classification.
McCarthyet al (McCarthy et al, 2006) employed WordNetand latent semantic analysis to lexical features withthe purpose of finding semantic similarities betweenwords; it is not clear whether the use of semanticfeatures improved the classification.
Argamon etal.
(Argamon, 2007) used systemic functional gram-mars to define a feature set associating single wordsor phrases with semantic information (an approachreminiscent of frames); Experiments of authorshipidentification on a corpus of English novels of the19th century showed that the features could improvethe classification results when combined with tra-ditional function word features.
Apart from a fewstudies (Arun et al, 2009; Holmes, 1992; Archer etal., 1997), the problem of attributing translated textsappears to be fairly untouched.2 Corpus and resource selectionAs pointed out in (Luyckx and Daelemans, 2010) thesize of data set and number of authors may cruciallyaffect the efficiency of author attribution methods,and evaluation of the method on some standard cor-pus is essential (Stamatatos, 2009).Closest to a standard corpus for author attribu-tion is The Federalist Papers (Juola, 2006), origi-nally used by Mosteller and Wallace (Mosteller andWallace, 1964), and we employ the subset of this66corpus consisting of the 71 undisputed single-authordocuments as our Corpus I.For translated texts, a mix of authors and transla-tors across authors is needed to ensure that the at-tribution methods do not attribute to the translatorinstead of the author.
However, there does not ap-pear to be a large corpus of texts publicly availablethat satisfy this demand.Based on this, we elected to compile a fresh cor-pus of translated texts; our Corpus II consists of En-glish translations of 19th century Russian romanticliterature chosen from Project Gutenberg for whicha number of different versions, with different trans-lators existed.
The corpus primarily consists of nov-els, but is slightly polluted by a few collections ofshort stories and two nonfiction works by Tolstoydue to the necessity of including a reasonable mixof authors and translators.
The corpus consists of 30texts by 4 different authors and 12 different transla-tors of which some have translated several differentauthors.
The texts range in size from 200 (Turgenev:The Rendezvous) to 33000 (Tolstoy: War and Peace)sentences.The option of splitting the corpus into an artifi-cially larger corpus by sampling sentences for eachauthor and collating these into a large number of newdocuments was discarded; we deemed that the sam-pling could inadvertently both smooth differencesbetween the original texts and smooth differences inthe translators?
footprints.
This could have resultedin an inaccurate positive bias in the evaluation re-sults.3 Experiment designFor both corpora, authorship attribution experimentswere performed using six classifiers, each employ-ing a distinct feature set.
For each feature set themarkers were counted in the text and their relativefrequencies calculated.
Feature selection was basedsolely on training data in the inner loop of the cross-validation cycle.
Two sets of experiments were per-formed, each with with X = 200 and X = 400features; the size of the feature vector was kept con-stant across comparison of methods, due to spaceconstraints only results for 400 features are reported.The feature sets were:Frequent Words (FW): Frequencies in the text ofthe X most frequent words1.
Classificationwith this feature set is used as baseline.Character N-grams: The X most frequent N-grams for N = 3, 4, 5.Frames: The relative frequencies of the X mostfrequently occurring semantic frames.Frequent Words and Frames (FWaF): The X/2most frequent features; words and frames resp.combined to a single feature vector of size X .In order to gauge the impact of translation upon anauthor?s footprint, three different experiments wereperformed on subsets of Corpus II:The full corpus of 30 texts [Corpus IIa] was usedfor authorship attribution with an ample mix of au-thors an translators, several translators having trans-lated texts by more than one author.
To ascertainhow heavily each marker is influenced by translationwe also performed translator attribution on a sub-set of 11 texts [Corpus IIb] with 3 different transla-tors each having translated 3 different authors.
If thetranslator leaves a heavy footprint on the marker, themarker is expected to score better when attributingto translator than to author.
Finally, we reduced thecorpus to a set of 18 texts [Corpus IIc] that only in-cludes unique author/translator combinations to seeif each marker could attribute correctly to an authorif the translator/author combination was not presentin the training set.All classification experiments were conductedusing a multi-class winner-takes-all (Duan andKeerthi, 2005) support vector machine (SVM).
Forcross-validation, all experiments used leave-one-out(i.e.
N -fold for N texts in the corpus) validation.All features were scaled to lie in the range [0, 1] be-fore different types of features were combined.
Ineach step of the cross-validation process, the mostfrequently occurring features were selected from thetraining data, and to minimize the effect of skewedtraining data on the results, oversampling with sub-stitution was used on the training data.1The most frequent words, is from a list of word frequenciesin the BNC compiled by (Leech et al, 2001)674 Results and evaluationWe tested our results for statistical significance us-ing McNemar?s test (McNemar, 1947) with Yates?correction for continuity (Yates, 1934) against thenull hypothesis that the classifier is indistinguishablefrom a random attribution weighted by the numberof author texts in the corpus.Random Weighted AttributionCorpus I IIa IIb IIcAccuracy 57.6 28.7 33.9 26.5Table 1: Accuracy of a random weighted attribution.FWaF performed better than FW for attribution ofauthor on translated texts.
However, the differencefailed to be statistically significant.Results of the experiments are reported in the ta-ble below.
For each corpus results are given forexperiments with 400 features.
We report macro2precision/recall, and the corresponding F1 and ac-curacy scores; the best scoring result in each row isshown in boldface.
For each corpus the bottom rowindicates whether each classifier is significantly dis-cernible from a weighted random attribution.400 FeaturesCorpus Measure FW 3-grams 4-grams 5-grams Frames FWaFI precision 96.4 97.0 97.0 99.4 80.7 92.0recall 90.3 97.0 91.0 97.6 66.8 93.3F1 93.3 97.0 93.9 98.5 73.1 92.7Accuracy 95.8 97.2 97.2 98.6 80.3 93.0p<0.05: X X X X X XIIa precision 63.8 61.9 59.1 57.9 82.7 81.9recall 66.4 60.4 60.4 60.4 70.8 80.8F1 65.1 61.1 59.7 59.1 76.3 81.3Accuracy 80.0 73.3 73.3 73.3 76.7 90.0p<0.05: X X X X X XIIb precision 91.7 47.2 47.2 38.9 70.0 70.0recall 91.7 58.3 58.3 50.0 63.9 63.9F1 91.7 52.2 52.2 43.8 66.8 66.8Accuracy 90.9 63.6 63.6 54.5 63.6 63.6p<0.05: X ?
?
?
?
?IIc precision 42.9 43.8 42.4 51.0 60.1 75.0recall 52.1 42.1 42.1 50.4 59.6 75.0F1 47.0 42.9 42.2 50.7 59.8 75.0Accuracy 55.6 50.0 44.4 55.6 61.1 72.2p<0.05: ?
?
?
?
?
XTable 2: Authorship attribution results2each author is given equal weight, regardless of the numberof documents4.1 Corpus I: The Federalist PapersFor the Federalist Papers the traditional authorshipattribution markers all lie in the 95+ range in accu-racy as expected.
However, the frame-based mark-ers achieved statistically significant results, and canhence be used for authorship attribution on untrans-lated documents (but performs worse than the base-line).
FWaF did not result in an improvement overFW.4.2 Corpus II: Attribution of translated textsFor Corpus IIa?the entire corpus of translated texts?all methods achieve results significantly better thanrandom, and FWaF is the best-scoring method, fol-lowed by FW.The results for Corpus IIb (three authors, threetranslators) clearly suggest that the footprint of thetranslator is evident in the translated texts, and thatthe FW (function word) classifier is particularly sen-sitive to the footprint.
In fact, FW was the only oneachieving a significant result over random assign-ment, giving an indication that this marker may beparticularly vulnerable to translator influence whenattempting to attribute authors.For Corpus IIc (unique author/translator combina-tions) decreased performance of all methods is evi-dent.
Some of this can be attributed to a smaller(training) corpus, but we also suspect the lack ofseveral instances of the same author/translator com-binations in the corpus.Observe that the FWaF classifier is the onlyclassifier with significantly better performance thanweighted random assignment, and outperforms theother methods.
Frames alone also outperform tradi-tional markers, albeit not by much.The experiments on the collected corpora stronglysuggest the feasibility of using Frames as markersfor authorship attribution, in particular in combina-tion with traditional lexical approaches.Our inability to obtain demonstrably significantimprovement of FWaF over the approach based onFrequent Words is likely an artifact of the fairlysmall corpus we employ.
However, computation ofsignificance is generally woefully absent from stud-ies of automated author attribution, so it is conceiv-able that the apparent improvement shown in manysuch studies fail to be statistically significant under68closer scrutiny (note that the exact tests to employfor statistical significance in information retrieval?including text categorization?is a subject of con-tention (Smucker et al, 2007)).5 Conclusions, caveats, and future workWe have investigated the use of semantic frames asmarkers for author attribution and tested their appli-cability to attribution of translated texts.
Our resultsshow that frames are potentially useful, especiallyso for translated texts, and suggest that a combinedmethod of frequent words and frames can outper-form methods based solely on traditional markers,on translated texts.
For attribution of untranslatedtexts and attribution to translator traditional markerssuch as frequent words and n-grams are still to bepreferred.Our test corpora consist of a limited number ofauthors, from a limited time period, with translatorsfrom a similar limited time period and cultural con-text.
Furthermore, our translations are all from a sin-gle language.
Thus, further work is needed beforefirm conclusions regarding the general applicabilityof the methods can be made.It is well known that effectiveness of authorshipmarkers may be influenced by topics (Stein et al,2007; Schein et al, 2010); while we have endeav-ored to design our corpora to minimize such influ-ence, we do not currently know the quantitative im-pact on topicality on the attribution methods in thispaper.
Furthermore, traditional investigations of au-thorship attribution have focused on the case of at-tributing texts among a small (N < 10) class ofauthors at the time, albeit with recent, notable ex-ceptions (Luyckx and Daelemans, 2010; Koppel etal., 2010).
We test our methods on similarly re-stricted sets of authors; the scalability of the meth-ods to larger numbers of authors is currently un-known.
Combining several classification methodsinto an ensemble method may yield improvementsin precision (Raghavan et al, 2010); it would beinteresting to see whether a classifier using framesyields significant improvements in ensemble withother methods.
Finally, the distribution of frames intexts is distinctly different from the distribution ofwords: While there are function words, there are no?function frames?, and certain frames that are com-mon in a corpus may fail to occur in the trainingmaterial of a given author; it is thus conceivable thatsmoothing would improve classification by framesmore than by words or N-grams.ReferencesJohn B. Archer, John L. Hilton, and G. Bruce Schaalje.1997.
Comparative power of three author-attributiontechniques for differentiating authors.
Journal of Bookof Mormon Studies, 6(1):47?63.Shlomo Argamon.
2007.
Interpreting Burrows?
Delta:Geometric and probabilistic foundations.
Literary andLinguistic Computing, 23(2):131?147.R.
Arun, V. Suresh, and C. E. Veni Madhaven.
2009.Stopword graphs and authorship attribution in text cor-pora.
In Proceedings of the 3rd IEEE InternationalConference on Semantic Computing (ICSC 2009),pages 192?196, Berkeley, CA, USA, sep. IEEE Com-puter Society Press.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A. Smith.
2010.
Probabilistic frame-semanticparsing.
In Proceedings of the North American Chap-ter of the Association for Compututional LinguisticsHuman Language Technologies Conference (NAACLHLT ?10).Kai-Bo Duan and S. Sathiya Keerthi.
2005.
Which isthe best multiclass svm method?
an empirical study.In Proceedings of the Sixth International Workshop onMultiple Classifier Systems, pages 278?285.Michael Gamon.
2004.
Linguistic correlates of style:Authorship classification with deep linguistic analy-sis features.
In Proceedings of the 20th InternationalConference on Computational Linguistics (COLING?04), pages 611?617.David I. Holmes.
1992.
A stylometric analysis of mor-mon scripture and related texts.
Journal of the RoyalStatistical Society, Series A, 155(1):91?120.Richard Johansson and Pierre Nugues.
2007.
Semanticstructure extraction using nonprojective dependencytrees.
In Proceedings of SemEval-2007, Prague, CzechRepublic, June 23-24.Patrick Juola.
2006.
Authorship attribution.
Found.Trends Inf.
Retr., 1(3):233?334.Moshe Koppel, Jonathan Schler, and Shlomo Argamon.2008.
Computational methods for authorship attribu-tion.
Journal of the American Society for InformationSciences and Technology, 60(1):9?25.Moshe Koppel, Jonathan Schler, and Shlomo Arga-mon.
2010.
Authorship attribution in the wild.Language Resources and Evaluation, pages 1?12.10.1007/s10579-009-9111-2.69Geoffrey Leech, Paul Rayson, and Andrew Wilson.2001.
Word Frequencies in Written and Spoken En-glish: Based on the British National Corpus.
Long-man, London.Kim Luyckx and Walter Daelemans.
2010.
The effect ofauthor set size and data size in authorship attribution.Literary and Linguistic Computing.
To appear.Philip M. McCarthy, Gwyneth A. Lewis, David F. Dufty,and Danielle S. McNamara.
2006.
Analyzing writingstyles with coh-metrix.
In Proceedings of the Interna-tional Conference of the Florida Artificial IntelligenceResearch Society, pages 764?769.Quinn McNemar.
1947.
Note on the sampling error ofthe difference between correlated proportions or per-centages.
Psychometrika, 12:153?157.Frederick Mosteller and David L. Wallace.
1964.
In-ference and Disputed Authorship: The Federalist.Springer-Verlag, New York.
2nd Edition appeared in1984 and was called Applied Bayesian and ClassicalInference.Sindhu Raghavan, Adriana Kovashka, and RaymondMooney.
2010.
Authorship attribution using proba-bilistic context-free grammars.
In Proceedings of theACL 2010 Conference Short Papers, pages 38?42.
As-sociation for Computational Linguistics.Joseph Rudman.
1997.
The state of authorship attribu-tion studies: Some problems and solutions.
Comput-ers and the Humanities, 31(4):351?365.Joseph Ruppenhofer, Michael Ellsworth, Miriam R. L.Petruck, Christopher R. Johnson, and Jan Scheffczyk.2006.
FrameNet II: Extended Theory and Practice.The Framenet Project.Andrew I. Schein, Johnnie F. Caver, Randale J. Honaker,and Craig H. Martell.
2010.
Author attribution evalua-tion with novel topic cross-validation.
In Proceedingsof the 2010 International Conference on KnowledgeDiscovery and Information Retrieval (KDIR ?10).Mark D. Smucker, James Allan, and Ben Carterette.2007.
A comparison of statistical significance testsfor information retrieval evaluation.
In Proceedings ofthe sixteenth ACM conference on Conference on infor-mation and knowledge management, CIKM ?07, pages623?632, New York, NY, USA.
ACM.Efstathios Stamatatos.
2009.
A survey of modern au-thorship attribution methods.
Journal of the Ameri-can Society for Information Science and Technology,60(3):538?556.Benno Stein, Moshe Koppel, and Efstathios Stamatatos,editors.
2007.
Proceedings of the SIGIR 2007 In-ternational Workshop on Plagiarism Analysis, Au-thorship Identification, and Near-Duplicate Detection,PAN 2007, Amsterdam, Netherlands, July 27, 2007,volume 276 of CEUR Workshop Proceedings.
CEUR-WS.org.Frank Yates.
1934.
Contingency tables involving smallnumbers and the ?2 test.
Supplement to the Journal ofthe Royal Statistical Society, 1(2):pp.
217?235.70
