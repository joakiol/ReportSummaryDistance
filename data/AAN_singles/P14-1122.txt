Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1294?1304,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsValidating and Extending Semantic Knowledge Basesusing Video Games with a PurposeDaniele Vannella, David Jurgens, Daniele Scarfini, Domenico Toscani and Roberto NavigliDepartment of Computer ScienceSapienza University of Romesurname@di.uniroma1.itAbstractLarge-scale knowledge bases are impor-tant assets in NLP.
Frequently, such re-sources are constructed through automaticmergers of complementary resources, suchas WordNet and Wikipedia.
However,manually validating these resources is pro-hibitively expensive, even when usingmethods such as crowdsourcing.
We pro-pose a cost-effective method of validat-ing and extending knowledge bases usingvideo games with a purpose.
Two videogames were created to validate concept-concept and concept-image relations.
Inexperiments comparing with crowdsourc-ing, we show that video game-based vali-dation consistently leads to higher-qualityannotations, even when players are notcompensated.1 IntroductionLarge-scale knowledge bases are an essentialcomponent of many approaches in Natural Lan-guage Processing (NLP).
Semantic knowledgebases such as WordNet (Fellbaum, 1998), YAGO(Suchanek et al, 2007), and BabelNet (Navigliand Ponzetto, 2010) provide ontological struc-ture that enables a wide range of tasks, such asmeasuring semantic relatedness (Budanitsky andHirst, 2006) and similarity (Pilehvar et al, 2013),paraphrasing (Kauchak and Barzilay, 2006), andword sense disambiguation (Navigli and Ponzetto,2012; Moro et al, 2014).
Furthermore, suchknowledge bases are essential for building unsu-pervised algorithms when training data is sparseor unavailable.
However, constructing and updat-ing semantic knowledge bases is often limited bythe significant time and human resources required.Recent approaches have attempted to build orextend these knowledge bases automatically.
Forexample, Snow et al (2006) and Navigli (2005)extend WordNet using distributional or structuralfeatures to identify novel semantic connectionsbetween concepts.
The recent advent of largesemi-structured resources has enabled the creationof new semantic knowledge bases (Medelyan etal., 2009; Hovy et al, 2013) through automati-cally merging WordNet and Wikipedia (Suchaneket al, 2007; Navigli and Ponzetto, 2010; Nie-mann and Gurevych, 2011).
While these auto-matic approaches offer the scale needed for open-domain applications, the automatic processes of-ten introduce errors, which can prove detrimentalto downstream applications.
To overcome issuesfrom fully-automatic construction methods, sev-eral works have proposed validating or extendingknowledge bases using crowdsourcing (Biemannand Nygaard, 2010; Eom et al, 2012; Sarasua etal., 2012).
However, these methods, too, are lim-ited by the resources required for acquiring largenumbers of responses.In this paper, we propose validating and extend-ing semantic knowledge bases using video gameswith a purpose.
Here, the annotation tasks aretransformed into elements of a video game whereplayers accomplish their jobs by virtue of playingthe game, rather than by performing a more tradi-tional annotation task.
While prior efforts in NLPhave incorporated games for performing annota-tion and validation (Siorpaes and Hepp, 2008b;Herda?gdelen and Baroni, 2012; Poesio et al,2013), these games have largely been text-based,adding game-like features such as high-scores ontop of an existing annotation task.
In contrast,we introduce two video games with graphical 2Dgameplay that is similar to what game players arefamiliar with.
The fun nature of the games pro-vides an intrinsic motivation for players to keepplaying, which can increase the quality of theirwork and lower the cost per annotation.Our work provides the following three contribu-tions.
First, we demonstrate effective video game-based methods for both validating and extending1294semantic networks, using two games that operateon complementary sources of information: seman-tic relations and sense-image mappings.
In con-trast to previous work, the annotation quality isdetermined in a fully automatic way.
Second, wedemonstrate that converting games with a purposeinto more traditional video games creates an in-creased player incentive such that players annotatefor free, thereby significantly lowering annotationcosts below that of crowdsourcing.
Third, for bothgames, we show that games produce better qualityannotations than crowdsourcing.2 Related WorkMultiple works have proposed linguisticannotation-based games with a purpose fortasks such as anaphora resolution (Hladk?a etal., 2009; Poesio et al, 2013), paraphrasing(Chklovski and Gil, 2005), term associations(Artignan et al, 2009; Lafourcade and Joubert,2010), query expansion (Simko et al, 2011), andword sense disambiguation (Chklovski and Mi-halcea, 2002; Seemakurty et al, 2010; Venhuizenet al, 2013).
Notably, all of these linguistic gamesfocus on users interacting with text, in contrastto other highly successful games with a purposein other domains, such as Foldit (Cooper et al,2010), in which players fold protein sequences,and the ESP game (von Ahn and Dabbish, 2004),where players label images with words.Most similar to our work are games that createor validate common sense knowledge.
Two gameswith a purpose have incorporated video game-like mechanics for annotation.
First, Herda?gdelenand Baroni (2012) validate automatically acquiredcommon sense relations using a slot machinegame where players must identify valid relationsand arguments from randomly aligned data withina time limit.
Although the validation is embeddedin a game-like setting, players are limited to oneaction (pulling the lever) unlike our games, whichfeature a variety of actions and rich gameplay ex-perience to keep players interested longer.
Sec-ond, Kuo et al (2009) describe a pet-raising gamewhere players must answer common sense ques-tions in order to obtain pet food.
While their gameis among the most video game-like, the annotationtask is a chore the player must perform in order toreturn to the game, rather than an integrated, funpart of the game?s objectives, which potentiallydecreases motivation for answering correctly.Several works have proposed adapting existingword-based board game designs to create or val-idate common sense knowledge.
von Ahn et al(2006) generate common sense facts by using agame similar to TabooTM, where one player mustlist facts about a computer-selected lemma and asecond player must guess the original lemma hav-ing seen only the facts.
Similarly, Vickrey et al(2008) gather free associations to a target wordwith the constraint, similar to TabooTM, whereplayers cannot enter a small set of banned words.Vickrey et al (2008) also present two games simi-lar to the ScattergoriesTM, where players are givena category and then must list things in that cate-gory.
The two variants differ in the constraints im-posed on the players, such as beginning all itemswith a specific letter.
For all three games, twoplayers play the same game under time limits andthen are rewarded if their answers match.Last, three two-player games have focusedon validating and extending knowledge bases.Rzeniewicz and Szyma?nski (2013) extend Word-Net with common-sense knowledge using a 20Questions-like game.
In a rapid-play style game,OntoPronto attempts to classify Wikipedia pagesas either categories or individuals (Siorpaes andHepp, 2008a).
SpotTheLink uses a similar rapidquestion format to have players align the DBpediaand PROTON ontologies by agreeing on the dis-tinctions between classes (Thaler et al, 2011).Unlike dynamic gaming elements common inour video games, the above games are all focusedon interacting with textual items.
Another majorlimitation is their need for always having two play-ers, which requires them to sustain enough inter-est to always maintain an active pool of players.While the computer can potentially act as a secondplayer, such a simulated player is often limited tousing preexisting knowledge or responses, whichmakes it difficult to validate new types of entitiesor create novel answers.
In contrast, we drop thisrequirement thanks to a new strategy for assign-ing confidence scores to the annotations based onnegative associations.3 Video Game with a Purpose DesignTo create video games, our development processfocused on a common design philosophy and acommon data set.3.1 Design ObjectivesThree design objectives were used to develop thevideo games.
First, the annotation task should bea central and natural action with familiar videogame mechanics.
That is, the annotation should1295be supplied by common actions such as collectingitems, puzzles, or destroying objects, rather thanthrough extrinsic tasks that players must completein order to return to the game.
This design hasthe benefits of (1) growing the annotator pool withvideo games players, and (2) potentially increas-ing annotator enjoyment.Second, the game should be playable by a singleplayer, with reinforcement for correct game playcoming from gold standard examples.1We notethat gold standard examples may come from bothtrue positive and true negative items.Third, the game design should be sufficientlygeneral to annotate a variety of linguistic phenom-ena, such that only the game data need be changedto accomplish a different annotation task.
Whilesome complex linguistic annotation tasks such aspreposition attachment may be difficult to inte-grate directly into gameplay, many simpler but stillnecessary annotation tasks such as word and im-age associations can be easily modeled with tradi-tional video game mechanics.3.2 Annotation SetupTasks We focused on two annotation tasks: (1)validating associations between two concepts, and(2) validating associations between a concept andan image.
For each task we developed a videogame with a purpose that integrates the task withinthe game, as illustrated in Sections 4 and 5.Knowledge base As the reference knowledgebase, we chose BabelNet2(Navigli and Ponzetto,2010), a large-scale multilingual semantic ontol-ogy created by automatically merging WordNetwith other collaboratively-constructed resourcessuch as Wikipedia and OmegaWiki.
BabelNetdata offers two necessary features for generat-ing the games?
datasets.
First, by connectingWordNet synsets to Wikipedia pages, most synsetsare associated with a set of pictures; while oftennoisy, these pictures sometimes illustrate the tar-get concept and are an ideal case for validation.Second, BabelNet contains the semantic relationsfrom both WordNet and hyperlinks in Wikipedia;these relations are again an ideal case of valida-tion, as not all hyperlinks connect semantically-related pages in Wikipedia.
Last, we stress thatwhile our games use BabelNet data, they couldeasily validate or extend other knowledge basessuch as YAGO (Suchanek et al, 2007) as well.1This design is in contrast to two-player games where mu-tual agreement reinforces correct behavior.2http://babelnet.orgData We created a common set of concepts, C,used in both games, containing sixty synsets se-lected from all BabelNet synsets with at least fiftyassociated images.
Using the same set of synsets,separate datasets were created for the two valida-tion tasks.
In each dataset, a concept c ?
C isassociated with two sets: a set Vccontaining itemsto validate, and a setNcwith examples of true neg-ative items (i.e., items where the relation to c doesnot hold).
We use the notation V and N when re-ferring to the to-validate and true negative sets forall concepts in a dataset, respectively.For the concept-concept dataset, Vcis the unionof VBc, which contains the lemmas of all synsetsincident to c in BabelNet, and Vnc, which con-tains novel lemmas derived from statistical asso-ciations.
Specifically, novel lemmas were selectedby computing the ?2statistic for co-occurrencesbetween the lemmas of c and all other part ofspeech-tagged lemmas in Wikipedia.
The 30 lem-mas with the highest ?2are included in Vc.
Toenable concept-to-concept annotations, we disam-biguate novel lemmas using a simple heuristicbased on link co-occurrence count (Navigli andPonzetto, 2012).
Each set Vccontains 77.6 lem-mas on average.For the concept-image data, Vcis the union ofVBc, which contains all images associated with c inBabelNet, and Vnc, which contains web-gatheredimages using a lemma of c as the query.
Web-gathered images were retrieved using Yahoo!
Bossimage search and the first result set (35 images)was added to Vc.
Each set Vccontains 77.0 imageson average.For both datasets, each negative set Ncis con-structed as ?c?
?C\{c}VBc?, i.e., from the items re-lated in BabelNet to all other concepts in C. ByconstructingNcdirectly from the knowledge base,play actions may be validated based on recogni-tion of true negatives, removing the heavy burdenfor ever manually creating a gold standard test set.Annotation Aggregation In each game, an itemis annotated when players make a binary choice asto whether the item?s relation is true (e.g., whetheran image is related to a concept).
To produce afinal annotation, a rating of p ?
n is computed,where p and n denote the number of times playershave marked the item?s relation as true or false, re-spectively.
Items with a positive rating after aggre-gating are marked as true examples of the relationand false otherwise.1296(a) The passphrase shown at the start (b) Main gameplay screen with a close-up of a player?s interaction with two humansFigure 1: Screenshots of the key elements of Infection4 Game 1: InfectionThe first game, Infection, validates the concept-concept relation dataset.Design Infection is designed as a top-downshooter game in the style of Commando.
Infectionfeatures the classic game premise that a virus haspartially infected humanity, turning people intozombies.
The player?s responsibility is to stopzombies from reaching the city and rescue humansthat are fleeing to the city.
Both zombies and hu-mans appear at the top of the screen, advance tothe bottom and, upon reaching it, enter the city.In the game, some humans are infected, buthave not yet become zombies; these infected hu-mans must be stopped before reaching the city.Because infected and uninfected humans lookidentical, the player uses a passphrase call-and-response mechanism to distinguish between thetwo.
Each level features a randomly-chosenpassphrase that the player?s character shouts.
Un-infected humans are expected to respond with aword or phrase related to the passphrase; in con-trast, infected humans have become confused dueto the infection and will say something completelyunrelated in an attempt to sneak past.
When an in-fected human reaches the city, the city?s total in-fection level increases; should the infection levelincrease beyond a certain threshold, the playerfails the stage and must replay it to advance thegame.
Furthermore, if any time after ten humanshave been seen, the player has killed more than80% of the uninfected humans, the player?s gun istaken by the survivors and she loses the stage.Figure 1a shows instructions for the passphrase?medicine.?
In the corresponding gameplay,shown in the close up of Figure 1b, a hu-man shouts a valid response, ?radiology?
for thelevel?s passphrase, while the nearby infected hu-man shouts an incorrect response ?longitude.
?Gameplay is divided into eight stages, each withincreasing difficulty.
Each stage has a goal ofsaving a specific number of uninfected humans.Infection incorporates common game mechanics,such as unlockable weapons, power-ups that re-store health, and achievements.
Scoring is basedon both the number of zombies killed and the per-centage of uninfected humans saved, motivatingplayers to kill infected humans in order to increasetheir score.
Importantly, Infection also includes aleaderboard where players compete for top posi-tions based on their total scores.Annotation Each human is assigned a responseselected uniformly from V or N .
Humans withresponses from N are treated as infected.
Playersannotate by selecting which humans are infected:Allowing a human with a response from V to enterthe city is treated as a positive annotation; killingthat human is treated as a negative annotation.The design of Infection enables annotating mul-tiple types of conceptual relations such as syn-onymy or antonymy by changing only the descrip-tion of the passphrase and how uninfected humansare expected to respond.Quality Enforcement Mechanisms Infection in-cludes two game mechanics to limit adversarialplayers from creating many low quality annota-tions.
Specifically, the game prevents playersfrom both (1) allowing all humans to live, via thecity infection level and (2) killing all humans, viasurvivors taking the player?s gun; these actionswould both generate many false positives and falsenegatives, respectively.
These mechanics ensurethe game naturally produces better quality anno-tations; in contrast, common crowdsourcing plat-forms do not support analogous mechanics for en-forcing this type of correctness at annotation time.5 Game 2: The Knowledge TowersThe second game, The Knowledge Towers (TKT),validates the concept-image dataset.Design TKT is designed as a single-player roleplaying game (RPG) where the player explores a1297(a) An example tower?s concept (b) Image selection screen (c) GameplayFigure 2: Screenshots of the key elements of The Knowledge Towers.series of towers to unlock long-forgotten knowl-edge.
At the start of each tower, a target con-cept is shown, e.g., the tower of ?tango,?
alongwith a description of the concept (Figure 2a).
Theplayer must then recover the knowledge of the tar-get concept by acquiring pictures of it.
Pictures areobtained through defeating monsters and openingtreasure chests, such as those shown in Figure 2c.However, players must distinguish pictures of thetower?s concept from unrelated pictures.
When animage is picked up, the player may keep or discardit, as shown in Figure 2b.
A player?s inventory islimited to eight pictures to encourage them to se-lect the most relevant pictures only.Once the player has collected enough pictures,the door to the boss room is unlocked and theplayer may enter to defeat the boss and completethe tower.
Pictures may also be deposited in spe-cial reward chests that grant experience bonuses ifthe deposited pictures are from V .
Gathering un-related pictures has adverse effects on the player.If the player finishes the level with a majority ofunrelated pictures, the player?s journey is unsuc-cessful and she must replay the tower.TKT includes RPG game elements commonlyfound in game series such as Diablo and the Leg-end of Zelda: players begin with a specific charac-ter class that has class-specific skills, such as War-rior or Thief, but will unlock the ability to play asother classes by successfully completing the tow-ers.
Last, TKT includes a leaderboard where play-ers can compete for positions; a player?s score isbased on increasing her character?s abilities andher accuracy at discarding images from N .Annotation Players annotate by deciding whichimages to keep in their inventory.
Images receivepositive rating annotations from: (1) depositingthe image in a reward chest, and (2) ending thelevel with the image still in the inventory.
Con-versely, images receive a negative rating when aplayer (1) views the image but intentionally avoidspicking it up or (2) drops the image from her in-ventory.TKT is designed to assist in the validation andextension of automatically-created image librariesthat link to semantic concepts, such as ImageNet(Deng et al, 2009) and that of Torralba et al(2008).
However, its general design allows forother types of annotations, such as image labeling,by changing the tower?s instructions and pictures.Quality Enforcement Mechanisms Similar toInfection, TKT includes analogous mechanismsfor limiting adversarial player annotations.
Play-ers who collect no images are prevented from en-tering the boss room, limiting their ability to gen-erate false negative annotations.
Similarly, playerswho collect all images are likely to have half oftheir images from N and therefore fail the tower?squality-check after defeating the boss.6 ExperimentsTwo experiments were performed with Infectionand TKT: (1) an evaluation of players?
ability toplay accurately and to validate semantic relationsand image associations and (2) a comprehensivecost comparison.
Each experiment compared (a)free and financially-incentivized versions of eachgame, (b) crowdsourcing, and (c) a non-videogame with a purpose.6.1 Experimental SetupGold Standard Data To compare the quality ofannotation from games and crowdsourcing, a goldstandard annotation was produced for a 10% sam-ple of each dataset (cf.
Section 3.2).
Two annota-tors independently rated the items and, in cases ofdisagreement, a third expert annotator adjudicated.Unlike in the game setting, annotators were free toconsult additional resources such as Wikipedia.To measure inter-annotator agreement (IAA) onthe gold standard annotations, we calculated Krip-1298pendorff?s ?
(Krippendorff, 2004; Artstein andPoesio, 2008); ?
ranges between [-1,1] where 1indicates complete agreement, -1 indicates sys-tematic disagreement, and values near 0 indicateagreement at chance levels.
Gold standard an-notators had high agreement, 0.774, for concept-concept relations.
However, image-concept agree-ment was only moderate, 0.549.
A further analy-sis revealed differences in the annotators?
thresh-olds for determining association, with one anno-tator permitting more abstract relations.
However,the adjudication process resolved these disputes,resulting in substantial agreement by all annota-tors on the final gold annotations.Incentives At the start of each game, players wereshown brief descriptions of the game and a de-scription of a contest where the top-ranked playerswould win either (1) monetary prizes in the formof gift cards, or (2) a mention and thanks in thispaper.
We refer to these as the paid and free ver-sions of the game, respectively.
In the paid setting,the five top-ranking players were offered gift cardsvalued at 25, 15, 15, 10, and 10 USD, starting fromfirst place (a total of 75 USD per game).
To in-crease competition among players and to performa fairer time comparison with crowdsourcing, thecontest period was limited to two weeks.6.2 Comparison MethodsTo compare with the video games, items wereannotated using two additional methods: crowd-sourcing and a non-video game with a purpose.Crowdsourcing Setup Crowdsourcing was per-formed using the CrowdFlower platform.
Anno-tation tasks were designed to closely match eachgame?s annotation process.
A task begins with adescription of a target synset and its textual def-inition; following, ten annotation questions areshown.
Separate tasks were used for validat-ing concept-concept and concept-image relations.Each tasks?
questions were shown as a binarychoice of whether the item is related to the task?sconcept.
Workers were paid 0.05 USD per task.Each question was answered by three workers.Following common practices for guardingagainst adversarial workers (Mason and Suri,2012), the tasks for concept c include qualitycheck questions using items from Nc.
Workerswho rate too many relations from Ncas valid areremoved by CrowdFlower and prevented from par-ticipating further.
One of the ten questions in atask used an item fromNc, resulting in a task mix-ture of 90% annotation questions and 10% quality-check questions.
However, we note that both ofour video games use data that is 50% annotation,50% quality-check.
While the crowdsourcing taskcould be adjusted to use an increased number ofquality-check options, such a design is uncommonand artificially inflates the cost of the crowdsourc-ing comparison beyond what would be expected.Therefore, although the crowdsourcing and game-based annotation tasks differ slightly, we chose touse the common setup in order to create a fair cost-comparison between the two.Non-video Game with a Purpose To measurethe impact of the video game itself on the anno-tation process, we developed a non-video gamewith a purpose, referred to as SuchGame.
Playersperform a single action in SuchGame: after be-ing shown a concept c and its textual definition, aplayer answers whether an item is related to theconcept.
Items are drawn equally from Vcand Nc,with players scoring a point each time they selectthat an item from N is not related.
A round ofgameplay contains ten questions.
After the roundends, players see their score for that round and thecurrent leaderboard.
Two versions of SuchGamewere released, one for each dataset.
SuchGamewas promoted with same free recognition incen-tive as Infection and TKT.6.3 Game ReleaseBoth video games were released to multiple on-line forums, social media sites, and Facebookgroups.
SuchGame was released to separate Face-book groups promoting free webgames and groupsfor indie games.
For each release, we estimatedan upper-bound of the audience sizes using avail-able statistics such as Facebook group sites, web-site analytics, and view counts.
The free and paidversions had sizes of 21,546 and 14,842 people,respectively; SuchGame had an upper bound of569,131 people.
Notices promoting the game wereseparated so that audiences saw promotions forone of either the paid or free incentive version.Games were also released in such a way as to pre-serve the anonymity of the study, which limitedour ability to advertise to public venues where theanonymity might be compromised.7 Results and Discussion7.1 Gameplay AnalysisIn this section we analyze the games in terms ofparticipation and player?s ability to correctly play.Players completed over 1388 games during the12990 50 100150 200 250300 350 400450Number of ItemsPlayerCorrectIncorrect(a) Infection (free)0 50100 150200 250300 350400Number of ItemsPlayerCorrectIncorrect(b) Infection (paid)0 100200 300400 500600 700Number of ItemsPlayerCorrectIncorrect(c) TKT (free)0 200400 600800 10001200 14001600Number of ItemsPlayerCorrectIncorrect(d) TKT (paid)Figure 3: Accuracy of the top-40 players in rejecting true negative items during gameplay.G.S.
Agreement# Players # Anno.
N -Acc.
Krip.
?s ?
True Pos.
True Neg.
All Cost per Ann.TKT free 100 3005 97.0 0.333 82.5 82.5 82.5 $0.000TKT paid 97 3318 95.4 0.304 69.0 92.1 74.0 $0.023Crowdflower 290 13854 - 0.478 59.5 93.7 66.2 $0.008Infection free 89 3150 71.0 0.445 67.8 68.4 68.1 $0.000Infection paid 163 3355 65.9 0.330 69.1 54.8 61.1 $0.022Crowdflower 1097 13764 - 0.167 16.9 96.4 59.6 $0.008Table 1: Annotation statistics from all sources.
N -Accuracy denotes accuracy at rejecting items fromN ;G.S.
Agreement denotes percentage agreement of the aggregated annotations with the gold standard.study period.
The paid and free versions of TKThad similar numbers of players, while the paid ver-sion of Infection attracted nearly twice the play-ers compared to the free version, shown in Ta-ble 1, Column 1.
However, both versions createdapproximately the same number of annotations,shown in Column 2.
Surprisingly, SuchGame re-ceived little attention, with only a few playerscompleting a full round of game play.
We believethis emphasizes the strength of video game-basedannotation; adding incentives and game-like fea-tures to an annotation task will not necessarily in-crease its appeal.
Given SuchGame?s minimal in-terest, we omit it from further analysis.Second, the type of incentive did not change thepercentage of items from N that players correctlyreject, shown for all players as N -accuracy in Ta-ble 1 Column 3 and per-player in Figure 3.
How-ever, players were much more accurate at reject-ing items from N in TKT than in Infection.
Weattribute this difference to the nature of the itemsand the format of the games.
The images usedby TKT provide concrete examples of a concept,which can be easily compared with the game?s cur-rent concept; in addition, TKT allows players toinspect items as long as a player prefers.
In con-trast, concept-concept associations require morebackground knowledge to determine if a relationexists; furthermore, Infection gives players limitedtime to decide (due to board length) and also con-tains cognitive distractors (zombies).
Neverthe-less, player accuracy remains high for both games(Table 1, Col. 3) indicating the games represent aviable medium for making annotation decisions.Last, the distribution of player annotation fre-quencies (Figure 3) suggests that the leaderboardand incentives motivated players.
Especially in thepaid condition, a clear group appears in the topfive positions, which were advertised as receivingprizes.
The close proximity of players in the paidpositions is a result of continued competition asplayers jostled for higher-paying prizes.7.2 Annotation QualityThis section assesses the annotation quality ofboth games and of CrowdFlower in terms of (1)the IAA of the participants, measured using Krip-pendorff?s ?, and (2) the percentage agreement ofthe resulting annotations with the gold standard.Players in both free and paid games had similarIAA, though the free version is consistently higher(Table 1, Col. 4).3For images, crowdsourcingworkers have a higher IAA than game players;however, this increased agreement is due to ad-versarial workers consistently selecting the same,incorrect answer.
In contrast, both video gamescontain mechanisms for limiting such behavior.The strength of both crowdsourcing and gameswith a purpose comes from aggregating multipleannotations of a single item; i.e., while IAA may3In conversations with players after the contest ended,several mentioned that being aware their play was contribut-ing to research motivated them to play more accurately.1300Lemma Abbreviated Definition Most-selected ItemsatomThe smallest possibleparticle of a chemicalelementspectrum, nonparticulate radiation, molecule, hydrogen, electron?
?
?chordA combination of threeor more notesvoicing, triad, tonality,?
strum, note, harmony?colorAn attribute from re-flected or emitted lightorange, brown,?
video, sadness, RGB, pigment?
?
?
?fireThe state of combustionin which inflammablematerial burnssprinkler, machine gun, chemical reduction, volcano, organic chemistry?
?
?religionThe expression ofman?s belief in andreverence for a super-human powerpolytheistic,?
monotheistic, Jainism, Christianity,?
Freedom of religion?
?
?Table 2: Examples of the most-selected words and images from the free version of both games.
Boldedwords and images with a dashed border denote items not in BabelNet.
Only the items marked with a ?were rated as valid in the aggregated CrowdFlower annotations.be low, the majority annotation of an item may becorrect.
Therefore, in Table 1, we calculate thepercentage agreement of the aggregated annota-tions with the gold standard annotations for ap-proving valid relations (true positives; Col. 5), re-jecting invalid relations (true negatives; Col. 6),and for both combined (Col. 7).
On average, bothvideo games in all settings produce more accurateannotations than crowdsourcing.
Indeed, despitehaving lower IAA for images, the free version ofTKT provides an absolute 16.3% improvement ingold standard agreement over crowdsourcing.Examining the difference in annotation qualityfor true positives and negatives, we see a strongbias with crowdsourcing towards rejecting allitems.
This bias leads to annotations with few falsepositives, but as Column 5 shows, crowdflowerworkers consistently performed much worse thangame players at identifying valid relations, pro-ducing many false negative annotations.
Indeed,for concept-concept relations, workers identifiedonly 16.9% of the valid relations.In contrast to crowdsourcing, both games wereeffective at identifying valid relations.
Table2 shows examples of the most frequently cho-sen items from V for the free versions of bothgames.
For both games, players were equallylikely to select novel items, suggesting the gamescan serve a useful purpose of adding these miss-ing relations in automatically constructed knowl-edge bases.
Highlighting one example, the fivemost selected concept-concept relations for chordwere all novel; BabelNet included many relationsto highly-specific concepts (e.g., ?Circle of fifths?
)but did not include relations to more commonly-associated concepts, like note and harmony.7.3 Cost AnalysisThis section provides a cost-comparison betweenthe video games and crowdsourcing.
The freeversions of both games proved highly success-ful, yielding high-quality annotations at no directcost.
Both free and paid conditions produced sim-ilar volumes of annotations, suggesting that play-ers do not need financial incentives provided thatthe games are fun to play.
It could be argued thatthe recognition incentive was motivating playersin the free condition and thus some incentive wasrequired.
However, player behavior indicates oth-erwise: After the contest period ended, no playersin the free setting registered for being acknowl-edged by name, which strongly suggests the in-centive was not contributing to their motivation forplaying.
Furthermore, a minority of players con-tinued to play even after the contest period ended,suggesting that enjoyment was a driving factor.1301Last, while crowdsourcing has seen different qual-ity and volume from workers in paid and unpaidsettings (Rogstadius et al, 2011), in contrast, ourgames produced approximately-equivalent resultsfrom players in both settings.Crowdsourcing was slightly more cost-effectivethan both games in the paid condition, as shownin Table 1, Column 8.
However, three additionalfactors need to be considered.
First, both gamesintentionally uniformly sample between V and Nto increase player engagement,4which generates alarger number of annotations for items in N thanare produced by crowdsourcing.
When annota-tions on items in N are included for both gamesand crowdsourcing, the costs per annotation dropto comparable levels: $0.007 for CrowdFlowertasks, $0.008 for TKT, and $0.011 for Infection.Second, for both annotation tasks, crowdsourc-ing produced lower quality annotations, especiallyfor valid relations.
Based on agreement with thegold standard (Table 1, Col. 5), the estimated costfor crowdsourcing a correct true positive annota-tion increases to $0.014 for a concept-image anda $0.048 for concepts-concept annotation.
In con-trast, the cost when using video games increasesonly to $0.033 for concept-image and $0.031 forconcept-concept.
These cost increases suggestthat crowdsourcing is not always cheaper with re-spect to quality.Third, we note that both video games in the paidsetting incur a fixed cost (for the prizes) and there-fore additional games played can only further de-crease the cost per annotation.
Indeed, the presentstudy divided the audience pool into two separategroups which effectively halved the potential num-ber of annotations per game.
Assuming combiningthe audiences would produce the same number ofannotations, both our games?
costs per annotationdrop to $0.012.Last, video games can potentially come withindirect costs due to software development andmaintenance.
Indeed, Poesio et al (2013) reportspending 60,000?
in developing their Phrase De-tectives game with a purpose over a two-year pe-riod.
In contrast, both games here were developedas a part of student projects using open source soft-ware and assets and thus incurred no cost; fur-thermore, games were created in a few months,rather than years.
Given that few online gamesattain significant sustained interest, we argue that4Earlier versions that used mostly items from V provedless engaging due to players frequently performing the sameaction, e.g., saving most humans or collecting most pictures.our lightweight model is preferable for producingvideo games with a purpose.
While using studentsis not always possible, the development processis fast enough to sufficiently reduce costs belowthose reported for Phrase Detectives.8 ConclusionTwo video games have been presented for vali-dating and extending knowledge bases.
The firstgame, Infection, validates concept-concept rela-tions, and the second, The Knowledge Towers,validates image-concept relations.
In experimentsinvolving online players, we demonstrate threecontributions.
First, games were released in twoconditions whereby players either saw financialincentives for playing or a personal satisfactionincentive where they were thanked by us.
Wedemonstrated that both conditions produced nearlyidentical numbers of annotations and, moreover,that players were disinterested in the satisfactionincentive, suggesting they played out of interestin the game itself.
Furthermore, we demonstratedthe effectiveness of a novel design for games witha purpose which does not require two players forvalidation and instead reinforces behavior onlyusing true negative items that required no man-ual annotation.
Second, in a comparison withcrowdsourcing, we demonstrate that video game-based annotations consistently generated higher-quality annotations.
Last, we demonstrate thatvideo game-based annotation can be more cost-effective than crowdsourcing or annotation taskswith game-like features: The significant numberof annotations generated by the satisfaction incen-tive condition shows that a fun game can generatehigh-quality annotations at virtually no cost.
Allannotated resources, demos of the games, and alive version of the top-ranking items for each con-cept are currently available online.5In the future we will apply our video gamesto the validation of more data, such as the newWikipedia bitaxonomy (Flati et al, 2014).AcknowledgmentsThe authors gratefully acknowl-edge the support of the ERC Start-ing Grant MultiJEDI No.
259234.We thank Francesco Cecconi for his supportwith the websites and the many video game play-ers without whose enjoyment this work would notbe possible.5http://lcl.uniroma1.it/games/1302ReferencesGuillaume Artignan, Mountaz Hasco?et, and Math-ieu Lafourcade.
2009.
Multiscale visual analysisof lexical networks.
In Proceedings of the Inter-national Conference on Information Visualisation,pages 685?690.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Chris Biemann and Valerie Nygaard.
2010.
Crowd-sourcing wordnet.
In Proceedings of the 5th GlobalWordNet conference.Alexander Budanitsky and Graeme Hirst.
2006.Evaluating WordNet-based measures of Lexical Se-mantic Relatedness.
Computational Linguistics,32(1):13?47.Timothy Chklovski and Yolanda Gil.
2005.
Improv-ing the design of intelligent acquisition interfaces forcollecting world knowledge from web contributors.In Proceedings of the International Conference onKnowledge Capture, pages 35?42.
ACM.Tim Chklovski and Rada Mihalcea.
2002.
Building aSense Tagged Corpus with Open Mind Word Expert.In Proceedings of ACL 2002 Workshop on WSD: Re-cent Successes and Future Directions, Philadelphia,PA, USA.Seth Cooper, Firas Khatib, Adrien Treuille, JanosBarbero, Jeehyung Lee, Michael Beenen, AndrewLeaver-Fay, David Baker, Zoran Popovi?c, and Folditplayers.
2010.
Predicting protein structures with amultiplayer online game.
Nature, 466(7307):756?760.Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, KaiLi, and Li Fei-Fei.
2009.
ImageNet: A large-scalehierarchical image database.
In Proceedings of theConference on Computer Vision and Pattern Recog-nition (CVPR), pages 248?255.Soojeong Eom, Markus Dickinson, and Graham Katz.2012.
Using semi-experts to derive judgments onword sense alignment: a pilot study.
In Proceed-ings of the Conference on Language Resources andEvaluation (LREC), pages 605?611.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Database.
MIT Press, Cambridge, MA.Tiziano Flati, Daniele Vannella, Tommaso Pasini, andRoberto Navigli.
2014.
Two is bigger (and better)than one: the Wikipedia Bitaxonomy Project.
InProceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics (ACL), Bal-timore, Maryland.Amac?
Herda?gdelen and Marco Baroni.
2012.
Boot-strapping a game with a purpose for common sensecollection.
ACM Transactions on Intelligent Sys-tems and Technology, 3(4):1?24.Barbora Hladk?a, Ji?r??
M?
?rovsk`y, and Pavel Schlesinger.2009.
Play the language: Play coreference.
InProceedings of the Joint Conference of the Asso-ciation for Computational Linguistics and Inter-national Joint Conference of the Asian Federationof Natural Language Processing (ACL-IJCNLP),pages 209?212.
Association for Computational Lin-guistics.Eduard H. Hovy, Roberto Navigli, and Simone PaoloPonzetto.
2013.
Collaboratively built semi-structured content and Artificial Intelligence: Thestory so far.
Artificial Intelligence, 194:2?27.David Kauchak and Regina Barzilay.
2006.
Paraphras-ing for automatic evaluation.
In Proceedings of theConference of the North American Chapter of theAssociation of Computational Linguistics (NAACL),pages 455?462.Klaus Krippendorff.
2004.
Content Analysis: AnIntroduction to Its Methodology.
Sage, ThousandOaks, CA, second edition.Yen-ling Kuo, Jong-Chuan Lee, Kai-yang Chiang, RexWang, Edward Shen, Cheng-wei Chan, and JaneYung-jen Hsu.
2009.
Community-based game de-sign: experiments on social games for common-sense data collection.
In Proceedings of the ACMSIGKDD Workshop on Human Computation, pages15?22.Mathieu Lafourcade and Alain Joubert.
2010.
Com-puting trees of named word usages from a crowd-sourced lexical network.
In Proceedings of the In-ternational Multiconference on Computer Scienceand Information Technology (IMCSIT), pages 439?446, Wisla, Poland.Winter Mason and Siddharth Suri.
2012.
Conductingbehavioral research on amazons mechanical turk.Behavior Research Methods, 44(1):1?23.Olena Medelyan, David Milne, Catherine Legg, andIan H. Witten.
2009.
Mining meaning fromWikipedia.
International Journal of Human-Computer Studies, 67(9):716?754.Andrea Moro, Alessandro Raganato, and Roberto Nav-igli.
2014.
Entity Linking meets Word Sense Dis-ambiguation: A Unified Approach.
Transactions ofthe Association for Computational Linguistics.Roberto Navigli and Simone Paolo Ponzetto.
2010.BabelNet: Building a very large multilingual se-mantic network.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics (ACL), Uppsala, Sweden, pages 216?225.Roberto Navigli and Simone Paolo Ponzetto.
2012.Joining forces pays off: Multilingual Joint WordSense Disambiguation.
In Proceedings of the JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 1399?1410, Jeju, Korea.1303Roberto Navigli.
2005.
Semi-automatic extension oflarge-scale linguistic knowledge bases.
In Proceed-ings of the 18th Internationa Florida AI ResearchSymposium Conference, Clearwater Beach, Florida,15?17 May 2005, pages 548?553.Elisabeth Niemann and Iryna Gurevych.
2011.
Thepeople?s web meets linguistic knowledge: Auto-matic sense alignment of Wikipedia and WordNet.In Proceedings of the International Conference onComputational Semantics (IWCS), pages 205?214.Mohammad Taher Pilehvar, David Jurgens, andRoberto Navigli.
2013.
Align, Disambiguate andWalk: a Unified Approach for Measuring SemanticSimilarity.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 1341?1351, Sofia, Bulgaria.Massimo Poesio, Jon Chamberlain, Udo Kruschwitz,Livio Robaldo, and Luca Ducceschi.
2013.
Phrasedetectives: Utilizing collective intelligence forinternet-scale language resource creation.
ACMTransactions on Interactive Intelligent Systems,3(1):3:1?3:44, April.Jakob Rogstadius, Vassilis Kostakos, Aniket Kittur,Boris Smus, Jim Laredo, and Maja Vukovic.
2011.An assessment of intrinsic and extrinsic motivationon task performance in crowdsourcing markets.
InProceedings of the International AAAI Conferenceon Weblogs and Social Media (ICWSM).Jacek Rzeniewicz and Julian Szyma?nski.
2013.Bringing Common Sense to WordNet with a WordGame.
In Computational Collective Intelligence.Technologies and Applications, volume 8083 of Lec-ture Notes in Computer Science, pages 296?305.Springer.Cristina Sarasua, Elena Simperl, and Natalya F Noy.2012.
CrowdMap: Crowdsourcing ontology align-ment with microtasks.
In Proceedings of the Inter-national Semantic Web Conference (ISWC), pages525?541.Nitin Seemakurty, Jonathan Chu, Luis Von Ahn, andAnthony Tomasic.
2010.
Word sense disambigua-tion via human computation.
In Proceedings of theACM SIGKDD Workshop on Human Computation,pages 60?63.
ACM.Jakub Simko, Michal Tvarozek, and Maria Bielikova.2011.
Little search game: term network acquisitionvia a human computation game.
In Proceedings ofthe ACM conference on Hypertext and Hypermedia,pages 57?62.Katharina Siorpaes and Martin Hepp.
2008a.
Gameswith a purpose for the semantic web.
IEEE Intelli-gent Systems, 23(3):50?60.Katharina Siorpaes and Martin Hepp.
2008b.
On-togame: Weaving the semantic web by onlinegames.
In Sean Bechhofer, Manfred Hauswirth, JrgHoffmann, and Manolis Koubarakis, editors, TheSemantic Web: Research and Applications, volume5021 of Lecture Notes in Computer Science, pages751?766.
Springer Berlin Heidelberg.Rion Snow, Dan Jurafsky, and Andrew Ng.
2006.Semantic taxonomy induction from heterogeneousevidence.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Com-putational Linguistics (COLING-ACL), Sydney, Aus-tralia, pages 801?808.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
YAGO: A core of semantic knowl-edge.
unifying WordNet and Wikipedia.
In Proceed-ings of the 16th World Wide Web Conference, Banff,Canada, 8?12 May 2007, pages 697?706.Stefan Thaler, Elena Paslaru Bontas Simperl, andKatharina Siorpaes.
2011.
SpotTheLink: A Gamefor Ontology Alignment.
In Proceedings of the6th Conference on Professional Knowledge Man-agement: From Knowledge to Action, pages 246?253.Antonio Torralba, Robert Fergus, and William T Free-man.
2008.
80 million tiny images: A large dataset for nonparametric object and scene recognition.IEEE Transactions on Pattern Analysis and MachineIntelligence, 30(11):1958?1970.Noortje J. Venhuizen, Valerio Basile, Kilian Evang, andJohan Bos.
2013.
Gamification for word sense la-beling.
In Proceedings of the International Confer-ence on Computational Semantics (IWCS).David Vickrey, Aaron Bronzan, William Choi, AmanKumar, Jason Turner-Maier, Arthur Wang, andDaphne Koller.
2008.
Online word games for se-mantic data collection.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 533?542.Luis von Ahn and Laura Dabbish.
2004.
Labeling im-ages with a computer game.
In Proceedings of theConference on Human Factors in Computing Sys-tems (CHI), pages 319?326.Luis von Ahn, Mihir Kedia, and Manuel Blum.
2006.Verbosity: a game for collecting common-sensefacts.
In Proceedings of the Conference on HumanFactors in Computing Systems (CHI), pages 75?78.1304
