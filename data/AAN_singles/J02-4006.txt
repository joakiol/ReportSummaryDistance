c?
2002 Association for Computational LinguisticsUsing Hidden Markov Modeling toDecompose Human-Written SummariesHongyan Jing?Lucent Technologies, Bell LaboratoriesProfessional summarizers often reuse original documents to generate summaries.
The task of sum-mary sentence decomposition is to deduce whether a summary sentence is constructed by reusingthe original text and to identify reused phrases.
Specifically, the decomposition program needs toanswer three questions for a given summary sentence: (1) Is this summary sentence constructedby reusing the text in the original document?
(2) If so, what phrases in the sentence come fromthe original document?
and (3) From where in the document do the phrases come?
Solving thedecomposition problem can lead to better text generation techniques for summarization.
Decom-position can also provide large training and testing corpora for extraction-based summarizers.We propose a hidden Markov model solution to the decomposition problem.
Evaluations showthat the proposed algorithm performs well.1.
IntroductionWe define a problem referred to as summary sentence decomposition.
The goal of a de-composition program is to determine the relations between phrases in a summaryand phrases in the corresponding original document.
Our analysis of a set of human-written summaries has indicated that professional summarizers often rely on cuttingand pasting text from the original document to produce summaries.
Unlike most cur-rent automatic summarizers, however, which extract sentences or paragraphs withoutany modification, professional summarizers edit the extracted text using a number ofrevision operations.Decomposition of human-written summaries involves analyzing a summary sen-tence to determine how it is constructed by humans.
Specifically, we define the sum-mary sentence decomposition problem as follows: Given a human-written summarysentence, a decomposition program needs to answer three questions: (1) Is this sum-mary sentence constructed by reusing the text in the original document?
(2) If so, whatphrases in the sentence come from the original document?
and (3) From where in thedocument do the phrases come?
Here, the term phrase refers to any sentence compo-nent that is cut from the original document and reused in the summary.
A phrase canbe at any granularity, from a single word to a complicated verb phrase to a completesentence.There are two primary benefits of solving the summary sentence decompositionproblem.
First, decomposition can lead to better text generation techniques in summa-rization.
Most domain-independent summarizers rely on simple extraction to producesummaries, even though extracted sentences can be incoherent, redundant, or mis-leading.
By decomposing human-written sentences, we can deduce how summary sen-?
600 Mountain Avenue, Murray Hill, NJ 07974.
E-mail: hjing@research.bell-labs.com.
The work reportedhere was completed while the author attended Columbia University.528Computational Linguistics Volume 28, Number 4tences are constructed by humans.
By learning how humans use revision operations toedit extracted sentences, we can develop automatic programs to simulate these revisionoperations and build a better text generation system for summarization.
Second, thedecomposition result also provides large corpora for extraction-based summarizers.
Byaligning summary sentences with original-document sentences, we can automaticallyannotate the most important sentences in an input document.
By doing this automat-ically, we can afford to mark content importance for a large set of documents, therebyproviding valuable training and testing data sets for extraction-based summarizers.We propose a hidden Markov model solution to the summary sentence decompo-sition problem.
In the next section, we show by example the revision operations usedby professional summarizers.
In Section 3, we present our solution to the decompo-sition problem by first mathematically formulating the decomposition problem andthen presenting the Hidden Markov Model.
In Section 4, we present three evaluationexperiments and their results.
Section 5 describes applications, and Section 6 discussesrelated work.2.
Revision OperationsWe analyzed a set of articles to observe how they were summarized by human abstrac-tors.
This set included 15 news articles on telecommunications, 5 articles on medicalissues, and 10 articles in the legal domain.
Although individual articles related to spe-cific domains, they covered a broad range of topics and differed in writing style andstructure even within the same domain.
The telecommunications articles were collectedusing the free daily news service Communications-Related Headlines provided by theBenton Foundation ?http://www.benton.org?.
The abstracts of these articles from var-ious newspapers were written by staff writers at Benton.
The medical news articleswere collected from HIV/STD/TB Prevention News Update, provided by the Cen-ter for Disease Control (CDC) ?http://www.cdcnpin.org/news/prevnews.htm?.
As apublic service, CDC provides daily staff-written synopses of key scientific articles andlay media reports on HIV/AIDS.
The legal articles from the New York Law Journal de-scribe court decisions on lawsuits that have been summarized by the journal?s editors.From the corpus studied, we found that human abstractors almost universallyreuse text in the original document for producing a summary of that document.
Thisfinding is consistent with Endres-Niggemeyer et al (1998), which stated that pro-fessional abstractors often rely on cutting and pasting the original text to producesummaries.Based on careful analysis of human-written summaries, we have defined six re-vision operations that can be used to transform a sentence in an article into a sum-mary sentence in a human-written abstract: sentence reduction, sentence combination,syntactic transformation, lexical paraphrasing, generalization or specification, and re-ordering.
The following sections examine each of these operations in turn.1.
Sentence reduction.
In sentence reduction, nonessential phrases areremoved from a sentence, as in the following example (italics in thesource sentence mark material that is removed):1Document sentence: When it arrives sometime next year in new TVsets, the V-chip will give parents a new and potentially revolu-1 All the examples in this section were taken from the 30 articles we analyzed; the summary sentencesare actual examples found in human-written abstracts.529Jing Decomposing Human-Written Summariestionary device to block out programs they don?t want their chil-dren to see.Summary sentence: The V-chip will give parents a device toblock out programs they don?t want their children to see.The deleted material can be at any granularity: a word, a phrase, or aclause.
Multiple components can be removed from a single sentence.2.
Sentence combination.
In sentence combination, material from a fewsentences is merged into a single sentence.
This operation is typicallyused together with sentence reduction, as illustrated in the followingexample, which also employs paraphrasing (italics in the sourcesentences mark material that is removed; italics in the summary sentencemark material that is added):Document sentence 1: But it also raises serious questions aboutthe privacy of such highly personal information wafting about thedigital world.Document sentence 2: The issue thus fits squarely into thebroader debate about privacy and security on the Internet,whether it involves protecting credit card numbers or keeping childrenfrom offensive information.Summary sentence: But it also raises the issue of privacy of suchpersonal information and this issue hits the nail on the head inthe broader debate about privacy and security on the Internet.3.
Syntactic transformation.
Syntactic transformation involves changing thesyntactic structure of a sentence.
In both sentence reduction and sentencecombination, syntactic transformations may also be involved.
In thefollowing example, the sentence structure was changed from thecausative clause structure in the original to the conjunctive structure inthe summary.
The subject of the causative clause and the subject of themain clause were combined during this operation.Document sentence: Since annoy.com enables visitors to sendunvarnished opinions to political and other figures in the news,the company was concerned that its activities would be bannedby the statute.Summary sentence: Annoy.com enables visitors to send unvar-nished opinions to political and other figures in the news andfeared the law could put them out of business.4.
Lexical paraphrasing.
In lexical paraphrasing, phrases are replaced withtheir paraphrases.
For instance, in the example in item (2), the summarysentences substituted fit squarely into with a more picturesque descriptionhits the nail on the head.5.
Generalization or specification.
In generalization (specification), phrases orclauses are replaced with more general (specific) descriptions, as in the530Computational Linguistics Volume 28, Number 4following examples:Generalization: a proposed new law that would require Webpublishers to obtain parental consent before collecting personalinformation from children ?
legislation to protect children?sprivacy on-lineSpecification: the White House?s top drug official ?
Gen. BarryR.
McCaffrey, the White House?s top drug official6.
Reordering.
In reordering, the order of extracted sentences is changedwith respect to the original.
For instance, the ending sentence of anarticle may be placed at the beginning of an abstract.Not all revision operations are listed here, because some operations are used infre-quently.
Note that multiple revision operations are often involved in order to producea single summary sentence.In human-written abstracts, some sentences are not based on cut and paste butare written from scratch.
The main criterion we used to distinguish a sentence thatwas cut and pasted from a sentence written from scratch was whether more thanhalf the words in a summary sentence were composed of phrases borrowed fromthe original document, in which case the sentence was considered to have beenconstructed by cut and paste; otherwise, it was considered to have been writtenfrom scratch.23.
Using a Hidden Markov Model for DecompositionTo answer the three questions of the decomposition problem is difficult.
Because thephrases that are borrowed from the original document can be at any granularity,determining phrase boundaries is not easy.
Determining the origin of a phrase isalso difficult, since the phrase may occur multiple times in the document in slightlydifferent forms.
Moreover, multiple revision operations may have been performed onthe reused text.
The resulting summary sentence can therefore differ significantly fromthe source document sentences from which it has been developed.
All these factorscomplicate the decomposition problem.We propose a hidden Markov model (HMM) (Baum 1972) solution to the decom-position problem.
The model has three steps.
First, we formulate the decompositionproblem as an equivalent problem; that is, for each word in a summary sentence, weidentify a document position as its likely source.
This step is important, since only af-ter this transformation can we apply the HMM to solve the problem.
Second, we buildthe HMM on a set of general heuristic rules observed from the text-reusing practice ofhumans.
Although this is unconventional in applications that use HMMs, we believeit is appropriate in our particular application.
Evaluations show that this unconven-tional HMM is effective for decomposition.
In the last step, a dynamic programmingtechnique, the Viterbi algorithm (Viterbi 1967), is used to find the most likely docu-ment position for each word in a summary sentence and the best decomposition forthe sentence.2 It is, of course, possible that a summary sentence has not been constructed by cut and paste even ifmore than half of the words in the sentence are from the original document.531Jing Decomposing Human-Written Summaries3.1 Formulating the ProblemWe first mathematically formulate the summary sentence decomposition problem.
Aninput summary sentence can be represented as a word sequence: (I1, .
.
.
, IN), where I1is the first word of the sentence and IN is the last word.
The position of a word in adocument can be uniquely represented by the sentence position and the word positionwithin the sentence: (SNUM, WNUM).
For example, (4, 8) uniquely refers to the eighthword in the fourth sentence.
Multiple occurrences of a word in the document can berepresented by a set of word positions: {(SNUM1, WNUM1), .
.
.
, (SNUMm, WNUMm)}.Using the above notation, we formulate the decomposition problem as follows: Given aword sequence (I1, .
.
.
, IN) and the positions {(SNUM1, WNUM1), .
.
.
, (SNUMM,WNUMM)} for each word in the sequence, determine the most likely document posi-tion for each word.Through this formulation, we transform the difficult tasks of identifying phraseboundaries and determining phrase origins into the problem of finding a most likelydocument position for each word.
As shown in Figure 1, when a position has beenchosen for each word in the summary sequence, we obtain a sequence of positions.For example, ((0,21), (2,40), (2,41), (0,31)) is our position sequence when the first occur-rence of the same word in the document has been chosen for every summary word;((0,26), (2,40), (2,41), (0,31)) is another position sequence.
Every time a different po-sition is chosen for a summary word, we obtain a different position sequence.
Theword the in the sequence occurs 44 times in the document, communication occurs once,subcommittee occurs twice, and of occurs 22 times.
This four-word sequence thereforehas a total of 1,936 (44 ?
1 ?
2 ?
22) possible position sequences.3 Morphological anal-ysis or stemming can be performed to associate morphologically related words, butit is optional.
In our experiments, applying stemming improved system performancewhen the human-written summaries included many words that were morphologicalvariants of original-document words.
Many human-written summaries in our exper-iments, however, contained few cases of morphological transformation of words andphrases borrowed from original documents, so stemming did not improve the perfor-mance for these summaries.Finding the most likely document position for each word is equivalent to findingthe most likely position sequence among all possible position sequences.
For the ex-ample in Figure 1, the most likely position sequence should be ((2,39), (2,40), (2,41),(2,42)); that is, the fragment comes from document sentence 2 and its position withinthe sentence is word number 39 to word number 42.
How can we automatically findthis sequence, however, among 1,936 possible sequences?3.2 The Hidden Markov ModelThe exact document position from which a word in a summary comes depends on theword positions surrounding it.
Using the bigram model, we assume that the probabilityof a word?s coming from a certain position in the document depends only on the worddirectly before it in the sequence.
Suppose Ii and Ii+1 are two adjacent words in asummary sentence and Ii is before Ii+1.
We use PROB(Ii+1 = (S2, W2) | Ii = (S1, W1)) torepresent the probability that Ii+1 comes from sentence number S2 and word numberW2 of the document when Ii comes from sentence number S1 and word number W1.To decompose a summary sentence, we must consider how humans are likelyto generate it; we draw here on the revision operations discussed in section 2.
Two3 Given an N-word sequence (I1, .
.
.
, IN), supposing Ii occurs Fi times in the document, for i = 1 ?
?
?N,then the total number of possible position sequences is F1 ?
F2 ?
?
?
?
?
FN .532Computational Linguistics Volume 28, Number 4the                 communication       subcommittee                of(2,41)...(0,32)(0,21)(0,26)...(2,39)...(23,44)(2,40)...(4,1)(0,31)(1,10)(2,30)(2,42)(23,43)(4,16)Figure 1The sequences of positions in summary sentence decomposition.general heuristic rules can be safely assumed: First, humans are more likely to cutphrases than single, isolated words; second, humans are more likely to combine nearbysentences into a single sentence than those far apart.
These two rules guide us in thedecomposition process.We translate the heuristic rules into the bigram probability PROB(Ii+1 = (S2, W2) |Ii = (S1, W1)), where Ii, Ii+1 represent two adjacent words in the input summarysentence (abbreviated henceforth as PROB(Ii+1 | Ii)).
The values of PROB(Ii+1 | Ii) areassigned as follows:?
If ((S1 = S2) and (W1 = W2 ?
1)) (i.e., words in two adjacent positions inthe document), then PROB(Ii+1 | Ii) is assigned the maximal value P1.For example, PROB((subcommittee = (2, 41) | communications = (2, 40)) inFigure 1 will be assigned the maximal value.
(Rule: Two adjacent wordsin a summary are most likely to come from two adjacent words in thedocument.)?
If ((S1 = S2) and (W1 < W2 ?
1)), then PROB(Ii+1 | Ii) is assigned thesecond-highest value P2.
For example, PROB(of = (4, 16) | subcommittee =(4, 1)) will be assigned a high probability.
(Rule: Adjacent words in asummary are highly likely to come from the same sentence in thedocument, retaining their relative order, as in the case of sentencereduction.
This rule can be further refined by adding restrictions ondistance between words.)?
If ((S1 = S2) and (W1 > W2)), then PROB(Ii+1 | Ii) is assigned thethird-highest value P3.
For example, PROB(of = (2, 30) | subcommittee =(2, 41)).
(Rule: Adjacent words in a summary can come from the samesentence in the document but change their relative order.
For example, asubject can be moved from the end of the sentence to the front, as insyntactic transformation.)?
If (S2 ?
CONST < S1 < S2), then PROB(Ii+1 | Ii) is assigned thefourth-highest value P4.
For example, PROB(of = (3, 5) | subcommittee =533Jing Decomposing Human-Written Summaries(2, 41)).
(Rule: Adjacent words in a summary can come from nearbysentences in the document and retain their relative order, suchas in sentence combination.
CONST is a small constant suchas 3 or 5.)?
If (S2 < S1 < S2 + CONST ), then PROB(Ii+1 | Ii) is assigned thefifth-highest value P5.
For example, PROB(of = (1, 10) | subcommittee =(2, 41)).
(Rule: Adjacent words in a summary can come from nearbysentences in the document but reverse their relative orders.)?
If (|S2 ?
S1| >= CONST ), then PROB(Ii+1 | Ii) is assigned the smallestvalue P6.
For example, PROB(of = (23, 43) | subcommittee = (2, 41)).
(Rule:Adjacent words in a summary are not very likely to come from sentencesfar apart.
)Figure 2 shows a graphical representation of the above rules for assigning bi-gram probabilities.
The nodes in the figure represent possible positions in the doc-ument, and the edges output the probability of moving from one node to another.These bigram probabilities are used to find the most likely position sequence in thenext step.
Assigning values to P1?P6 is experimental.
In our experiments, the max-imal value is assigned 1 and others are usually assigned evenly decreasing values:0.9, 0.8, and so on.
These values, however, can be experimentally adjusted for dif-ferent corpora.
We decide the approximate optimal values of P1?P6 by testing dif-ferent values for P1?P6 and choosing the values that give the best performance inthe tests.Figure 2 is considered a very abstract representation of our HMM for decompo-sition.
Each word position in the figure represents a state in the HMM.
For example,(S, W) is a state, and (S, W + 1) is another state.
Note that (S, W) and (S, W + 1) arerelative values; the S and W in the state (S, W) have different values based on the(S,W) (S,W+1) (S,W+n)(n>=1) (n>=2)(S+i,W+j)(S+i,W+j)i>=CONST(S?i,W+j)(S?i,W+j)i>=CONSTSentence (S?CONST)Sentence (S+CONST)P1P2P3P6P60< i<CONST0<i<CONSTP5P4(S,W?n)Sentence SFigure 2Assigning transition probabilities in the HMM.534Computational Linguistics Volume 28, Number 4particular word position under consideration.
This relative model can be easily trans-formed, however, into an absolute model.
(S, W) can be replaced by every possibleword position in the document; transition probabilities between every possible pair ofpositions can be assigned in the same way as in Figure 2.
In section 3.6, we describehow the abstract model can be transformed into the absolute model and give a formaldescription of our HMM.3.3 The Viterbi AlgorithmTo find the most likely sequence, we must find a sequence of positions that maxi-mizes the probability PROB(I1, .
.
.
, IN).
Using the bigram model, this probability canbe approximated asPROB(I1, .
.
.
, IN) =N?1?i=0PROB(Ii+1 | Ii).Because PROB(Ii+1 | Ii) has been assigned as indicated earlier, we therefore have allthe information needed to solve the problem.
We use the Viterbi algorithm (Viterbi1967) to find the most likely sequence.
For an N-word sequence, supposing each wordoccurs M times in the document, the Viterbi algorithm is guaranteed to find the mostlikely sequence using k ?
N ?
M2 steps for some constant k, compared to MN for thebrute force search algorithm.We have slightly revised the Viterbi algorithm for our application.
In the initial-ization step, equal chance is assumed for each possible document position of the firstword in the sequence.
In the iteration step, we take special measures to handle thecase when a summary word does not appear in the document (i.e., has an emptyposition list).
We mark the word as nonexistent in the original document and continuethe computation as if it did not appear in the sequence.3.4 PosteditingAfter the phrases are identified, the program postedits to cancel mismatchings thatarise because the Viterbi algorithm assigns each word in the input sequence to aposition in the document, as long as the word appears at least once.
For instance, in theexample of sentence combination given in section 2, the summary sentence combinedtwo reduced document sentences by adding the conjunction and.
The word and wasinserted by the human writer, but the Viterbi algorithm assigned it to a documentposition, since it occurred in the original document.
The goal of the postedit step is toannul such mismatchings.The postedit step deals with two types of mismatchings: wrong assignment ofdocument positions for inserted stop words in a summary sentence and wrong as-signment of document positions for isolated content words in a summary sentence.
Tocorrect the first type of mismatching, if any document sentence contributes only stopwords for the summary, the matching is canceled, since the stop words are more likelyto have been inserted by humans rather than coming from the original document.
Thisis the case for the example just discussed.
To correct the second type of mismatching,if a document sentence provides only a single non?stop word, we also cancel suchmatching, since humans rarely cut single words from the original text to generate asummary sentence.535Jing Decomposing Human-Written Summaries3.5 An ExampleTo demonstrate the program, we now present an example from beginning to end.
Thefollowing input sample summary sentence is also shown in Figure 3:Arthur B. Sackler, vice president for law and public policy of Time Warner Inc.and a member of the Direct Marketing Association, told the communicationssubcommittee of the Senate Commerce Committee that legislation to protectchildren?s privacy online could destroy the spontaneous nature that makes theInternet unique.We first indexed the document, listing for each word its possible positions in thedocument.4 Stemming was not used in this example.
Upon augmenting each summaryword with its possible document positions, we obtained the following input for theViterbi program:arthur : 1,0b : 1,1sackler : 1,2 2,34 .
.
.
15,6. .
.the : 0,21 0,26 .
.
.
23,44internet : 0,27 1,39 .
.
.
18,16unique : 0,28This 48-word sentence has a total of 5.08?1027 possible position sequences.
Using thebigram probabilities as assigned in section 3.2, we ran the Viterbi algorithm to find themost likely position sequence.
After every word was assigned a most likely documentposition, we marked the phrases in the sentence by conjoining words from adjacentdocument positions.Figure 3 shows the final result for the sample input summary sentence.
The phrasesin the summary are tagged (FNUM:SNUM actual-text), where FNUM is the sequentialnumber of the phrase and SNUM is the number of the document sentence in whichthe phrase originates.
SNUM = ?1 means that the phrase is not derived from the orig-inal document.
The borrowed phrases are tagged (FNUM actual-text) in the documentsentences.In this example, the program correctly concluded that the summary sentence wasconstructed by reusing the original text.
It identified the four document sentences thatwere combined into the summary sentence; it also correctly divided the summarysentence into phrases, pinpointing the exact document origin of each.
In this example,the phrases that were borrowed from the document ranged from single words to longclauses.
Certain borrowed phrases were also syntactically transformed; despite these,the program successfully decomposed the sentence.The decomposition outputs such as shown in Figure 3 were then used for buildingthe training corpora for sentence reduction and sentence combination.
The outputshown in Figure 3 was included in the corpus for sentence combination, since thesummary sentence was constructed by merging document sentences.
If a summarysentence was constructed by removing phrases from a single document sentence, thenit was included in the training corpus for sentence reduction.4 The original document contained 25 sentences and 727 words in total.536Computational Linguistics Volume 28, Number 4Summary Sentence:(F0:S1 arthur b sackler vice president for law and public policy of time warnerinc) (F1:S-1 and) (F2:S0 a member of the direct marketing association told) (F3:S2the communications subcommittee of the senate commerce committee) (F4:S-1 thatlegislation) (F5:S1 to protect) (F6:S4 children?s) (F7:S4 privacy) (F8:S4 online) (F9:S0could destroy the spontaneous nature that makes the internet unique)Source Document Sentences:Sentence 0: a proposed new law that would require web publishers to obtain parentalconsent before collecting personal information from children (F9 could destroy thespontaneous nature that makes the internet unique) (F2 a member of the directmarketing association told) a senate panel thursdaySentence 1: (F0 arthur b sackler vice president for law and public policy of timewarner inc) said the association supported efforts (F5 to protect) children online buthe urged lawmakers to find some middle ground that also allows for interactivity onthe internetSentence 2: for example a child?s e-mail address is necessary in order to respond toinquiries such as updates on mark mcguire?s and sammy sosa?s home run figuresthis year or updates of an online magazine sackler said in testimony to (F3 thecommunications subcommittee of the senate commerce committee)Sentence 4: the subcommittee is considering the (F6 children?s) (F8 online) (F7 pri-vacy) protection act which was drafted on the recommendation of the federal tradecommissionFigure 3A sample output of the summary sentence decomposition program (boldface text indicatesmaterial that was cut from the original document and reused in the summary, and italic text inthe summary sentence indicates material that was added by the human writer).3.6 Formal Description of Our Hidden Markov ModelWe first illustrate how an absolute model can be created from the relative modelrepresented in Figure 2.
For simplicity, suppose there are only two sentences in theoriginal document and each sentence has two words.
From the relative model inFigure 2, we can build an absolute model as shown in Figure 4.In the absolute model, there are four states, (1,1), (1,2), (2,1), and (2,2), each cor-responding to a word position.
Each state has only one observation symbol (i.e., out-(1, 2)(1, 1)(2, 1) (2, 2)?Figure 4Example of the absolute hidden Markov model.537Jing Decomposing Human-Written Summariesput): the word in that position.
Each state is interconnected with the other states inthe model.
The state transition probabilities, which represent the probabilities of tran-sitioning from one state to another state, can be assigned following the rules shownin Figure 2.
In this case, however, we need to normalize the values of {P1, P2, .
.
.
, P6}so that for each state the sum of the transition probabilities is one, which is a basicrequirement for an HMM.
This normalization is needed in theory in order to conformour relative model to a formal model, but in practice it is not needed in the decom-position process, since it does not affect the final result.
The initial state distributionis uniform; that is, the initial state, labeled as ?
in Figure 4, has an equal chance toreach any state in the model.We give a formal description of our HMM for decomposition as follows.
Foreach original document, we can build an absolute model based on the relative modelin Figure 2.
In the absolute model, each state corresponds to a word position, andeach word position corresponds to a state.
The observation symbol set includes allthe words in the document, and the observation symbol probabilities are defined asP(Wi | Pi) = 1, if word Wi is in position Pi, and P(Wi | Pi) = 0, if word Wi isnot in position Pi.
The transition probabilities P(Pj | Pi) are defined as we described inFigure 2, with every word position linked to every other word position, and state initialprobabilities are uniform as we mentioned.
This Markov model is hidden because onesymbol sequence can correspond to many state sequences, meaning that many positionsequences can correspond to a word sequence, as shown in Figure 1.
Generally, ina hidden Markov model, one state sequence can also corrrespond to many symbolsequences.
Our HMM does not have this attribute.4.
EvaluationsThree experiments were performed to evaluate the decomposition module.
In the firstexperiment, we evaluated decomposition in a task called summary alignment.
Thismeasured how successfully the decomposition program can align sentences in thesummary with document sentences that are semantically equivalent.
In the secondexperiment, we asked humans to judge whether the decomposition results were cor-rect.
Compared to the first experiment, this was a more direct evaluation, using alarger collection of documents.
The third experiment evaluated the portability of theprogram.The corpus used in the first experiment consisted of 10 documents from the Ziff-Davis corpus, which contains articles related to computer products and is availableon TIPSTER discs from Linguistic Data Consortium (LDC) (Harman and Liberman1993).
The corpus used in the second experiment consisted of 50 documents related totelecommunications issues.
The corpus used in the third experiment consisted of legaldocuments on court cases, provided by the Westlaw Group.4.1 Summary AlignmentThe goal of the summary alignment task was to find sentences in the document thatwere semantically equivalent to the summary sentences.
We used a small collectionof 10 documents, gathered by Marcu (1999).
Marcu presented these 10 documentstogether with their human-written summaries from the Ziff-Davis corpus to 14 humanjudges.
These human judges were instructed to extract sentences from the originaldocument that were semantically equivalent to the summary sentences.
Sentencesselected by the majority of human judges were collected to build an extract (i.e.,extraction-based summary) of the document.
This resulting extract was used as thegold standard in our evaluation.
Note that this evaluation will be biased against the538Computational Linguistics Volume 28, Number 4Table 1Evaluation of decomposition program using the Ziff-Davis corpus.Doc.
No.
Precision Recall F-MeasureZF109-601-903 0.67 0.67 0.67ZF109-685-555 0.75 1 0.86ZF109-631-813 1 1 1ZF109-712-593 0.86 0.55 0.67ZF109-645-951 1 1 1ZF109-714-915 0.56 0.64 0.6ZF109-662-269 0.79 0.79 0.79ZF109-715-629 0.67 0.67 0.67ZF109-666-869 0.86 0.55 0.67ZF109-754-223 1 1 1Average 0.815 0.785 0.791decomposition model, as Marcu?s semantic equivalence is a broader concept than ourcut-and-paste equivalence.Decomposition provides a list of source document sentences for each summarysentence, as shown in Figure 3.
We can build an automatic extract for the documentby selecting all the source document sentences identified by the decomposition pro-gram.
We compared this automatic extract with the gold-standard extract.
The pro-gram achieved an average 81.5% precision, 78.5% recall, and 79.1% F-measure for 10documents.
By comparison, the average performance of 14 human judges was 88.8%precision, 84.4% recall, and 85.7% F-measure.
Detailed results for each document areshown in Table 1.
Precision, recall, and F-measure are computed as follows:Precision =# of sentences in the automatic extract and in the gold-standard extracttotal # of sentences in the automatic extractRecall =# of sentences in the automatic extract and in the gold-standard extracttotal # of sentences in the gold-standard extractF-measure =2 ?
Recall ?
PrecisionRecall + PrecisionFurther analysis indicates two types of errors made by the program.
The first isthat the program failed to find semantically equivalent sentences with very differentwordings.
For example, it did not find the correspondence between the summarysentence Running Higgins is much easier than installing it and the document sentenceThe program is very easy to use, although the installation procedure is somewhat complex.
Thisis not really an ?error,?
since the program is not designed to find such paraphrases.For decomposition purposes, the program needs only to indicate that the summarysentence is not produced by cutting and pasting text from the original document.
Theprogram correctly indicated this by returning no matching sentence.The second problem is that the program may identify a nonrelevant documentsentence as relevant if it contains some words common to the summary sentence.This typically occurs when a summary sentence is not constructed by cutting andpasting text from the document but shares words with certain document sentences.For example, the decomposition program mistakenly linked the summary sentenceThe program is very easy to use, although the installation procedure is somewhat complex with539Jing Decomposing Human-Written Summariesthe document sentence All you need to decide during the easy installation is where you wantto put the Higgins files and associated directories; this must be a directory available to all e-mail users, because they had a number of words in common, including the, is, easy,to, and installation.
Our postediting steps are designed to cancel such false matchings,although we cannot remove them completely.It is worth noting that the extract based on human judgments, considered the goldstandard in this evaluation, is not perfect.
For example, two document sentences mayexpress the same information (i.e., they are semantic paraphrases), and all human sub-jects may consider this information important enough to be in the summary, but halfof the subjects selected one sentence and half selected the other; thus, both sentenceswill be included in the extract although they are semantic paraphrases.
Precisely thishappened in the extract of document ZF109-601-903.
The document sentence This groupproductivity package includes e-mail, group scheduling and alerting, keyword cross-reference fil-ing, to-do lists, and expense reporting and the document sentence At $695 for 8 users, thisintegrated software package combines LAN-based e-mail with a variety of personal informationmanagement functions, including group scheduling, personal calendars, to-do lists, expense re-ports, and a cross-referenced key-word database were both included in the extract, althoughthey contain very similar information.
The program picked up only the second doc-ument sentence, yet this correct decision was penalized in the evaluation because ofthe mistake in the gold standard.The program won perfect scores for 3 out of 10 documents.
We checked the threesummaries and found that their texts were largely produced by cut and paste, com-pared to other summaries with sentences written completely from scratch by humans.This indicates that when only the decomposition task is considered, the algorithmperforms very well.4.2 Human Judgments of Decomposition ResultsSince the first experiment did not directly assess the program?s performance for thedecomposition task, we conducted another experiment to evaluate the correctness ofthe decomposition results.
First, we selected 50 summaries from a telecommunicationscorpus and ran the decomposition program.A human subject was asked to judge whether the decomposition results werecorrect.
A result was considered correct when all three questions posed in the decom-position problem were correctly answered.
As stated in section 1, the decompositionprogram needs to answer the following three questions: (1) Is a summary sentenceconstructed by reusing the text from the original document?
(2) If so, what phrasesin the sentence come from the original document?
and (3) From where in the doc-ument do the phrases come?
The 50 summaries contained a total of 305 sentences.Eighteen (6.2%) sentences were wrongly decomposed, for an accuracy rate of 93.8%.Most errors occurred when a summary sentence was not constructed by cutting andpasting but contained many overlapping words with certain sentences in the docu-ment.
The accuracy rate here was much higher than the precision and recall results inthe first experiment.
An important factor here is that we did not require the programto find semantically equivalent document sentence(s) if a summary sentence used verydifferent wordings.4.3 PortabilityIn the third and final evaluation of decomposition, we tested the program on legaldocuments in a joint experiment with the Westlaw Group, which provides lawyerswith court case documents.
Such documents start with a ?synopsis?
of the case, writ-ten by attorneys, followed by ?headnotes,?
which are points of law also written by540Computational Linguistics Volume 28, Number 4HEADNOTE:(F0:S0 A motion for issuance of a peremptory writ) (F1:S-1 of mandamus) (F2:S0notwithstanding) (F3:S0 the return) (F4:S1 operates as an admission by relator oftruth of facts well pleaded), (F5:S1 but claims that in law the return presents nosufficient) (F6:S-1 ground) (F7:S1 why relief sought) (F8:S1 should not be granted).OPINION:Sentence 0: As to the effect to be given (F0 the motion for the issuance of the peremp-tory writ) (F3 the return) of the respondents (F2 notwithstanding), it is well to stateat the outset that under our decided cases such a motion stands as the equivalent ofa demurrer to a pleading in a law action.Sentence 1: It (F4 operates as an admission by the relator of the truth of the factswell pleaded) by the respondent (F5 but claims that in law the return presents nosufficient) reason (F7 why the relief sought) in the alternative writ (F8 should notbe granted).Figure 5A sample output of legal document decomposition (boldface text indicates material that wascut from the original document and reused in the summary, and italic text in the summarysentence indicates material that was added by the human writer).attorneys and summarized from the discussions.
The last part is the discussion, called?opinion.
?The task here was to match each headnote entry with the corresponding text inthe opinion.
When lawyers study a legal case document, they can see not only theimportant points of law, but also where these points are discussed in the opinion.
Weapplied our decomposition program to this task.
We did not adjust our HMM param-eters.
A sample decomposition result is shown in Figure 5.
Similar to the notationused in Figure 3, the phrases in the headnote are tagged (FNUM:SNUM actual-text),where FNUM is the sequential number of the phrase and SNUM is the number of thedocument sentence where the phrase comes from.
SNUM = ?1 means that the phrasedid not come from the original document.
The borrowed phrases are tagged (FNUMactual-text) in the opinion.
Note that in this example, we ignored the difference of thedeterminers (?a,?
?the,?
etc.)
in the phrases, so the summary phrase ?a motion for is-suance of a peremptory writ?
was considered to originate from the document phrase?the motion for the issuance of the peremptory writ,?
although the two phrases werenot identical.We received 11 headnotes from Westlaw and examined the decomposition resultsfor all of them.
The program found the correct source sentences and identified thecorrect origins of the phrases for every headnote.In summary, we performed three experiments in three different domains?compu-ter, telecommunications news, and legal?and in each case achieved good results, withno change or minimal parameter adjustment to the HMM.
This demonstrates that ourproposed decomposition approach is portable.
The reason for this portability may bethat the heuristic rules that we used to build the HMM are indeed general and remaintrue for different humans and for articles from different domains.5.
Applications of Decomposition Results5.1 Providing Training and Testing Corpora for SummarizationWe have used the decomposition results in our development of a text generationsystem for domain-independent summarization.
The generation system mimics tworevision operations presented in section 2: sentence reduction and sentence combina-tion.
The decomposition program is used to build corpora for training and evaluating541Jing Decomposing Human-Written Summariesthe sentence reduction and combination modules.
The corpora contained examples asshown in Figure 3.
Details of the summarization system can be found in Jing (2001).5.2 Corpus AnalysisWe performed a corpus analysis using the decomposition program.
We automaticallyanalyzed 300 human-written summaries of news articles on telecommunications, pro-vided by the Benton Foundation.
The number of sentences in each summary rangedfrom 2 to 21; the corpus contained a total of 1,642 summary sentences.
The resultsindicated that 315 summary sentences (19%) did not have matching sentences in thedocument: They were written from scratch by humans rather than by cutting andpasting phrases from the original text.
Of the summary sentences, 686 (42%) matcheda single sentence in the document.
These sentences were constructed by sentence re-duction, sometimes together with other operations such as lexical paraphrasing andsyntactic transformation.
In addition, 592 sentences (36%) matched two or three sen-tences in the document and 49 sentences (3%) matched more than three sentences inthe document.
These sentences were constructed by sentence combination, often to-gether with other operations, especially sentence reduction, since the sentences wereusually reduced before they were combined.
These results suggested that a significantportion (81%) of summary sentences produced by humans were based on cutting andpasting the original text.
Sentence reduction was applied in at least 42% of the cases.Sentence combination was applied in 39% of the cases.5.3 Improving User InterfacesThe decomposition result can be used in applications other than summarization.
Forexample, in the experiment we performed jointly with Westlaw (see section 4.3), wefound that linking summaries and original documents can potentially improve userinterfaces, helping users to easily browse and find relations between portions of thetext.6.
Related WorkResearchers have previously tried to align summary sentences with sentences in adocument, mostly by manual effort (Edmundson 1969; Kupiec, Pedersen, and Chen1995; Teufel and Moens 1997).
Given the cost of this manual annotation process, onlysmall collections of text have been annotated.
Decomposition provides a means ofperforming this alignment automatically, building large corpora for summarizationresearch.Marcu (1999) presented an approach for aligning summary sentences with seman-tically equivalent sentences in a document.
It adopted an information retrieval basedapproach, coupled with discourse processing.
Although our decomposition also aimsto link summaries with the original documents, major differences exist between thetwo approaches.
While Marcu?s algorithm operates at the sentence or clause level, ourdecomposition program deals with phrases at various granularities (anything from aword to a complicated phrase to a complete sentence).
Furthermore, the approachesused by the two systems are distinct.
Marcu?s approach first breaks sentences intoclauses, then uses rhetorical structure to decide which clauses should be considered,and finally employs an IR-based similarity measure to decide which clauses in the doc-ument are similar to those in human-written abstracts.
Our HMM solution first buildsthe HMM, then uses a dynamic programming technique to find the optimal answer.Marcu reported a performance of 77.45%, 80.06%, and 78.15% for precision, recall, andF-measure, respectively, when the system was evaluated at the sentence level in the542Computational Linguistics Volume 28, Number 4summary alignment task described in section 3.1.
When tested on the same set of testdocuments and for the same task, our system averaged 81.5% precision, 78.5% recall,and 79.1% F-measure, as shown in Table 1.We transformed the decomposition problem into a problem of finding the mostlikely document position for each word in the summary, which is, in some sense, simi-lar to the problem of aligning parallel bilingual corpora (Brown, Lai, and Mercer 1991;Gale and Church 1991).
Whereas Brown, Lai, and Mercer and Gale and Church alignedsentences in a parallel bilingual corpus, we aligned phrases in a summary with phrasesin a document.
Brown, Lai, and Mercer (1991) also used an HMM in their solutionfor bilingual corpora alignment.
Their model and our model, however, differ greatly:Their model used sentence length as a feature, whereas ours used word position asa feature; they used an aligned training corpus to compute transition probabilities,whereas we did not use any annotated training data.7.
ConclusionsWe defined the problem of decomposing human-written summaries and proposed ahidden Markov model solution to the problem.
The decomposition program can auto-matically determine whether a summary sentence is constructed by reusing text fromthe original document; it can accurately recognize the reused phrases in a summarysentence despite their different granularities; it can also pinpoint the exact origin inthe document for a phrase.
The algorithm is fast and straightforward.
It does notneed other tools such as a tagger or parser as preprocessors.
It does not have complexprocessing steps.
The evaluations show that the program performs very well for thedecomposition task.AcknowledgmentsThe material in this article is based uponwork supported by the National ScienceFoundation under Grant No.
IRI 96-19124and IRI 96-18797.
Any opinions, findings,and conclusions or recommendationsexpressed in this material are those of theauthor and do not necessarily reflect theviews of the National Science Foundation.ReferencesBaum, Leonard E. 1972.
An inequality andassociated maximization technique instatistical estimation of probabilisticfunctions of a Markov process.Inequalities, 3:1?8.Brown, Peter F., Jennifer C. Lai, and RobertL.
Mercer.
1991.
Aligning sentences inparallel corpora.
In Proceedings of the 29thAnnual Meeting of the Association forComputational Linguistics, pages 169?176,Berkeley, June.Edmundson, H. P. 1969.
New methods inautomatic abstracting.
Journal of the ACM,16(2):264?285.Endres-Niggemeyer, Brigitte, Kai Haseloh,Jens Mu?ller, Simone Peist, Irene Santinide Sigel, Alexander Sigel, ElisabethWansorra, Jan Wheeler, and Bru?njaWollny.
1998.
Summarizing Information.Springer, Berlin.Gale, William A. and Kenneth W. Church.1991.
A program for aligning sentences inparallel corpora.
In Proceedings of the 29thAnnual Meeting of the Association forComputational Linguistics, pages 177?184,Berkeley, June.Harman, Donna and Mark Liberman.
1993.TIPSTER Complete.
Linguistic DataConsortium, University of Pennsylvania.Jing, Hongyan.
2001.
Cut-and-Paste TextSummarization.
Ph.D. thesis, Departmentof Computer Science, ColumbiaUniversity, New York.Kupiec, Julian, Jan Pedersen, and FrancineChen.
1995.
A trainable documentsummarizer.
In Proceedings of the 18thInternational Conference on Research andDevelopment in Information Retrieval,pages 68?73, Seattle.543Jing Decomposing Human-Written SummariesMarcu, Daniel.
1999.
The automaticconstruction of large-scale corpora forsummarization research.
In Proceedings ofthe 22nd International Conference on Researchand Development and Information Retrieval,pages 137?144, University of California,Berkeley, August.Teufel, Simone and Mark Moens.
1997.Sentence extraction as a classificationtask.
In Proceedings of the ACL/EACL?97Workshop on Intelligent Scalable TextSummarization, pages 58?65, Madrid.Viterbi, Andrew J.
1967.
Error bounds forconvolution codes and an asymptoticallyoptimal decoding algorithm.
IEEETransactions on Information Theory,13:260?269.
