Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 30?36,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsHITS-based Seed Selection and Stop List Construction for BootstrappingTetsuo Kiso Masashi Shimbo Mamoru Komachi Yuji MatsumotoGraduate School of Information ScienceNara Institute of Science and TechnologyIkoma, Nara 630-0192, Japan{tetsuo-s,shimbo,komachi,matsu}@is.naist.jpAbstractIn bootstrapping (seed set expansion), select-ing good seeds and creating stop lists are twoeffective ways to reduce semantic drift, butthese methods generally need human super-vision.
In this paper, we propose a graph-based approach to helping editors choose ef-fective seeds and stop list instances, appli-cable to Pantel and Pennacchiotti?s Espressobootstrapping algorithm.
The idea is to selectseeds and create a stop list using the rankingsof instances and patterns computed by Klein-berg?s HITS algorithm.
Experimental resultson a variation of the lexical sample task showthe effectiveness of our method.1 IntroductionBootstrapping (Yarowsky, 1995; Abney, 2004) is atechnique frequently used in natural language pro-cessing to expand limited resources with minimalsupervision.
Given a small amount of sample data(seeds) representing a particular semantic class ofinterest, bootstrapping first trains a classifier (whichoften is a weighted list of surface patterns character-izing the seeds) using the seeds, and then apply it onthe remaining data to select instances most likely tobe of the same class as the seeds.
These selected in-stances are added to the seed set, and the process isiterated until sufficient labeled data are acquired.Many bootstrapping algorithms have been pro-posed for a variety of tasks: word sense disambigua-tion (Yarowsky, 1995; Abney, 2004), informationextraction (Hearst, 1992; Riloff and Jones, 1999;Thelen and Riloff, 2002; Pantel and Pennacchiotti,2006), named entity recognition (Collins and Singer,1999), part-of-speech tagging (Clark et al, 2003),and statistical parsing (Steedman et al, 2003; Mc-Closky et al, 2006).Bootstrapping algorithms, however, are known tosuffer from the problem called semantic drift: as theiteration proceeds, the algorithms tend to select in-stances increasingly irrelevant to the seed instances(Curran et al, 2007).
For example, suppose we wantto collect the names of common tourist sites from aweb corpus.
Given seed instances {New York City,Maldives Islands}, bootstrapping might learn, at onepoint of the iteration, patterns like ?pictures of X?and ?photos of X,?
which also co-occur with manyirrelevant instances.
In this case, a later iterationwould likely acquire frequent words co-occurringwith these generic patterns, such as Michael Jack-son.Previous work has tried to reduce the effect of se-mantic drift by making the stop list of instances thatmust not be extracted (Curran et al, 2007; McIntoshand Curran, 2009).
Drift can also be reduced withcarefully selected seeds.
However, both of these ap-proaches require expert knowledge.In this paper, we propose a graph-based approachto seed selection and stop list creation for the state-of-the-art bootstrapping algorithm Espresso (Panteland Pennacchiotti, 2006).
An advantage of this ap-proach is that it requires zero or minimal super-vision.
The idea is to use the hubness score ofinstances and patterns computed from the point-wise mutual information matrix with the HITS al-gorithm (Kleinberg, 1999).
Komachi et al (2008)pointed out that semantic drift in Espresso has thesame root as topic drift (Bharat and Henzinger,1998) observed with HITS, noting the algorithmicsimilarity between them.
While Komachi et al pro-posed to use algorithms different from Espresso to30avoid semantic drift, in this paper we take advantageof this similarity to make better use of Espresso.We demonstrate the effectiveness of our approachon a word sense disambiguation task.2 BackgroundIn this section, we review related work on seed se-lection and stop list construction.
We also briefly in-troduce the Espresso bootstrapping algorithm (Pan-tel and Pennacchiotti, 2006) for which we build ourseed selection and stop list construction methods.2.1 Seed SelectionThe performance of bootstrapping can be greatly in-fluenced by a number of factors such as the size ofthe seed set, the composition of the seed set and thecoherence of the concept being expanded (Vyas etal., 2009).
Vyas et al (2009) studied the impact ofthe composition of the seed sets on the expansionperformance, confirming that seed set compositionhas a significant impact on the quality of expansions.They also found that the seeds chosen by non-experteditors are often worse than randomly chosen ones.A similar observation was made by McIntosh andCurran (2009), who reported that randomly chosenseeds from the gold-standard set often outperformedseeds chosen by domain experts.
These results sug-gest that even for humans, selecting good seeds is anon-trivial task.2.2 Stop ListsYangarber et al (2002) proposed to run multiplebootstrapping sessions in parallel, with each sessiontrying to extract one of several mutually exclusivesemantic classes.
Thus, the instances harvested inone bootstrapping session can be used as the stoplist of the other sessions.
Curran et al (2007) pur-sued a similar idea in their Mutual Exclusion Boot-strapping, which uses multiple semantic classes inaddition to hand-crafted stop lists.
While multi-classbootstrapping is a clever way to reduce human su-pervision in stop list construction, it is not generallyapplicable to bootstrapping for a single class.
To ap-ply the idea of multi-class bootstrapping to single-class bootstrapping, one has to first find appropri-ate competing semantic classes and good seeds forthem, which is in itself a difficult problem.
Alongthis line of research, McIntosh (2010) recently usedAlgorithm 1 Espresso algorithm1: Input: Seed vector i02: Instance-pattern co-occurrence matrix A3: Instance cutoff parameter k4: Pattern cutoff parameter m5: Number of iterations ?6: Output: Instance score vector i7: Pattern score vector p8: function ESPRESSO(i0,A,k,m,?
)9: i?
i010: for t = 1,2, ...,?
do11: p?
AT i12: Scale p so that the components sum to one.13: p?
SELECTKBEST(p,k)14: i?
Ap15: Scale i so that the components sum to one.16: i?
SELECTKBEST(i,m)17: return i and p18: function SELECTKBEST(v,k)19: Retain only the k largest components of v, resetting theremaining components to 0.20: return vclustering to find competing semantic classes (nega-tive categories).2.3 EspressoEspresso (Pantel and Pennacchiotti, 2006) is one ofthe state-of-the-art bootstrapping algorithms used inmany natural language tasks (Komachi and Suzuki,2008; Abe et al, 2008; Ittoo and Bouma, 2010;Yoshida et al, 2010).
Espresso takes advantage ofpointwise mutual information (pmi) (Manning andSchu?tze, 1999) between instances and patterns toevaluate their reliability.
Let n be the number of allinstances in the corpus, and p the number of all pos-sible patterns.
We denote all pmi values as an n?
pinstance-pattern matrix A, with the (i, j) element ofA holding the value of pmi between the ith instanceand the jth pattern.
Let AT denote the matrix trans-pose of A.Algorithm 1 shows the pseudocode of Espresso.The input vector i0 (called seed vector) is an n-dimensional binary vector with 1 at the ith com-ponent for every seed instance i, and 0 elsewhere.The algorithm outputs an n-dimensional vector i andan p-dimensional vector p, respectively representingthe final scores of instances and patterns.
Note thatfor brevity, the pseudocode assumes fixed numbers(k and m) of components in i and p are carried overto the subsequent iteration, but the original Espresso31allows them to gradually increase with the numberof iterations.3 HITS-based Approach to Seed Selectionand Stop List Construction3.1 Espresso and HITSKomachi et al (2008) pointed out the similaritybetween Espresso and Kleinberg?s HITS web pageranking algorithm (Kleinberg, 1999).
Indeed, if weremove the pattern/instance selection steps of Algo-rithm 1 (lines 13 and 16), the algorithm essentiallyreduces to HITS.
In this case, the outputs i and pmatch respectively the hubness and authority scorevectors of HITS, computed on the bipartite graph ofinstances and patterns induced by matrix A.An implication of this algorithmic similarity isthat the outputs of Espresso are inherently biasedtowards the HITS vectors, which is likely to bethe cause of semantic drift.
Even though the pat-tern/instance selection steps in Espresso reduce sucha bias to some extent, the bias still persists, as em-pirically verified by Komachi et al (2008).
In otherwords, the expansion process does not drift in ran-dom directions, but tend towards the set of instancesand patterns with the highest HITS scores, regard-less of the target semantic class.
We exploit this ob-servation in seed selection and stop list constructionfor Espresso, in order to reduce semantic drift.3.2 The ProcedureOur strategy is extremely simple, and can be sum-marized as follows.1.
First, compute the HITS ranking of instancesin the graph induced by the pmi matrix A. Thiscan be done by calling Algorithm 1 with k =m = ?
and a sufficiently large ?
.2.
Next, check the top instances in the HITS rank-ing list manually, and see if these belong to thetarget class.3.
The third step depends on the outcome of thesecond step.
(a) If the top instances are of the target class,use them as the seeds.
We do not use astop list in this case.
(b) If not, these instances are likely to make avector for which semantic drift is directed;hence, use them as the stop list.
In thiscase, the seed set must be prepared manu-ally, just like the usual bootstrapping pro-cedure.4.
Run Espresso with the seeds or stop list foundin the last step.4 Experimental SetupWe evaluate our methods on a variant of the lexi-cal sample word sense disambiguation task.
In thelexical sample task, a small pre-selected set of a tar-get word is given, along with an inventory of sensesfor each word (Jurafsky and Martin, 2008).
Eachword comes with a number of instances (contextsentences) in which the target word occur, and someof these sentences are manually labeled with the cor-rect sense of the target word in each context.
Thegoal of the task is to classify unlabeled context sen-tences by the sense of the target word in each con-text, using the set of labeled sentences.To apply Espresso for this task, we reformulatethe task to be that of seed set expansion, and notclassification.
That is, the hand-labeled sentenceshaving the same sense label are used as the seed set,and it is expanded over all the remaining (unlabeled)sentences.The reason we use the lexical sample task is thatevery sentence (instance) belongs to one of the pre-defined senses (classes), and we can expect the mostfrequent sense in the corpus to form the highestHITS ranking instances.
This allows us to com-pletely automate our experiments, without the needto manually check the HITS ranking in Step 2 ofSection 3.2.
That is, for the most frequent sense(majority sense), we take Step 3a and use the highestranked instances as seeds; for the rest of the senses(minority senses), we take Step 3b and use them asthe stop list.4.1 DatasetsWe used the seven most frequent polysemous nouns(arm, bank, degree, difference, paper, party andshelter) in the SENSEVAL-3 dataset, and line (Lea-cock et al, 1993) and interest (Bruce and Wiebe,32Task Method MAP AUC R-Precision P@30 P@50 P@100arm Random 84.3 ?4.1 59.6 ?8.1 80.9 ?2.2 89.5 ?10.8 87.7 ?9.6 85.4 ?7.2HITS 85.9 59.7 79.3 100 98.0 89.0bank Random 74.8 ?6.5 61.6 ?9.6 72.6 ?4.5 82.9 ?14.8 80.1 ?13.5 76.6 ?10.9HITS 84.8 77.6 78.0 100 100 94.0degree Random 69.4 ?3.0 54.3 ?4.2 66.7 ?2.3 76.8 ?9.5 73.8 ?7.5 70.5 ?5.3HITS 62.4 49.3 63.2 56.7 64.0 66.0difference Random 48.3 ?3.8 54.5 ?5.0 47.0 ?4.4 53.9 ?10.7 50.7 ?8.8 47.9 ?6.1HITS 50.2 60.1 51.1 60.0 60.0 48.0paper Random 75.2 ?4.1 56.4 ?7.1 71.6 ?3.3 82.3 ?9.8 79.6 ?8.8 76.9 ?6.1HITS 75.2 61.0 75.2 73.3 80.0 78.0party Random 79.1 ?5.0 57.0 ?9.7 76.6 ?3.1 84.5 ?10.7 82.7 ?9.2 80.2 ?7.5HITS 85.2 68.2 78.5 100 96.0 87.0shelter Random 74.9 ?2.3 51.5 ?3.3 73.2 ?1.3 77.3 ?7.8 76.0 ?5.6 74.5 ?3.5HITS 77.0 54.6 72.0 76.7 84.0 79.0line Random 44.5 ?15.1 36.3 ?16.9 40.1 ?14.6 75.0 ?21.0 69.8 ?24.1 62.3 ?27.9HITS 72.2 68.6 68.5 100 100 100interest Random 64.9 ?8.3 64.9 ?12.0 63.7 ?10.2 87.6 ?13.2 85.3 ?13.7 81.2 ?13.9HITS 75.3 83.0 80.1 100 94.0 77.0Avg.
Random 68.4 55.1 65.8 78.9 76.2 72.8HITS 74.2 64.7 71.8 85.2 86.2 79.8Table 1: Comparison of seed selection for Espresso (?
= 5, nseed = 7).
For Random, results are reported as (mean ?standard deviation).
All figures are expressed in percentage terms.
The row labeled ?Avg.?
lists the values macro-averaged over the nine tasks.1994) datasets1 for our experiments.
We lowercasedwords in the sentence and pre-processed them withthe Porter stemmer (Porter, 1980) to get the stems ofwords.Following (Komachi et al, 2008), we used twotypes of features extracted from neighboring con-texts: collocational features and bag-of-words fea-tures.
For collocational features, we set a window ofthree words to the right and left of the target word.4.2 Evaluation methodologyWe run Espresso on the above datasets using differ-ent seed selection methods (for majority sense of tar-get words), and with or without stop lists created byour method (for minority senses of target words).We evaluate the performance of the systems ac-cording to the following evaluation metrics: meanaverage precision (MAP), area under the ROC curve(AUC), R-precision, and precision@n (P@n) (Man-ning et al, 2008).
The output of Espresso may con-tain seed instances input to the system, but seeds areexcluded from the evaluation.1http://www.d.umn.edu/?tpederse/data.html5 Results and Discussion5.1 Effect of Seed SelectionWe first evaluate the performance of our seed se-lection method for the majority sense of the ninepolysemous nouns.
Table 1 shows the performanceof Espresso with the seeds chosen by the proposedHITS-based seed selection method (HITS), and withthe seed sets randomly chosen from the gold stan-dard sets (Random; baseline).
The results for Ran-dom were averaged over 1000 runs.
We set the num-ber of seeds nseed = 7 and number of iterations ?
= 5in this experiment.As shown in the table, HITS outperforms thebaseline systems except degree.
Especially, theMAP reported in Table 1 shows that our approachachieved improvements of 10 percentage points onbank, 6.1 points on party, 27.7 points on line, and10.4 points on interest over the baseline, respec-tively.
AUC and R-precision mostly exhibit a trendsimilar to MAP, except R-precision in arm and shel-ter, for which the baseline is better.
It can be seenfrom the P@n (P@30, P@50 and P@100) reportedin Table 1 that our approach performed considerablybetter than baseline, e.g., around 17?20 points above33Task Method MAP AUC R-Precision P@10 P@20 P@30arm NoStop 12.7 ?4.3 51.8 ?10.8 13.9 ?9.8 21.4 ?19.1 15.1 ?12.0 14.1 ?10.4HITS 13.4 ?4.1 53.7 ?10.5 15.0 ?9.5 23.8 ?17.7 17.5 ?12.0 15.5 ?10.2bank NoStop 32.5 ?5.1 73.0 ?8.5 45.1 ?10.3 80.4 ?21.8 70.3 ?21.2 62.6 ?18.1HITS 33.7 ?3.7 75.4 ?5.7 47.6 ?8.1 82.6 ?18.1 72.7 ?18.5 65.3 ?15.5degree NoStop 34.7 ?4.2 69.7 ?5.6 43.0 ?7.1 70.0 ?18.7 62.8 ?15.7 55.8 ?14.3HITS 35.7 ?4.3 71.7 ?5.6 44.3 ?7.6 72.4 ?16.4 64.4 ?15.9 58.3 ?16.2difference NoStop 20.2 ?3.9 57.1 ?6.7 22.3 ?8.3 35.8 ?18.7 27.7 ?14.0 25.5 ?11.9HITS 21.2 ?3.8 59.1 ?6.3 24.2 ?8.4 38.2 ?20.5 30.2 ?14.0 28.0 ?11.9paper NoStop 25.9 ?6.6 53.1 ?10.0 27.7 ?9.8 55.2 ?34.7 42.4 ?25.4 36.0 ?17.8HITS 27.2 ?6.3 56.3 ?9.1 29.4 ?9.5 57.4 ?35.3 45.6 ?25.3 38.7 ?17.5party NoStop 23.0 ?5.3 59.4 ?10.8 30.5 ?9.1 59.6 ?25.8 46.8 ?17.4 38.7 ?12.7HITS 24.1 ?5.0 62.5 ?9.8 32.1 ?9.4 61.6 ?26.4 47.9 ?16.6 40.8 ?12.7shelter NoStop 24.3 ?2.4 50.6 ?3.2 25.1 ?4.6 25.4 ?11.7 26.9 ?10.3 25.9 ?8.7HITS 25.6 ?2.3 53.4 ?3.0 26.5 ?4.8 28.8 ?12.9 29.0 ?10.4 28.1 ?8.2line NoStop 6.5 ?1.8 38.3 ?5.3 2.1 ?4.1 0.8 ?4.4 1.8 ?8.9 2.3 ?11.0HITS 6.7 ?1.9 38.8 ?5.8 2.4 ?4.4 1.0 ?4.6 2.0 ?8.9 2.5 ?11.1interest NoStop 29.4 ?7.6 61.0 ?12.1 33.7 ?13.2 69.6 ?40.3 67.0 ?39.1 65.7 ?37.8HITS 31.2 ?5.6 63.6 ?9.1 36.1 ?10.5 81.0 ?29.4 78.1 ?27.0 77.4 ?24.3Avg.
NoStop 23.2 57.1 27.0 46.5 40.1 36.3HITS 24.3 59.4 28.6 49.6 43.0 39.4Table 2: Effect of stop lists for Espresso (nstop = 10, nseed = 10, ?
= 20).
Results are reported as (mean ?
standarddeviation).
All figures are expressed in percentage.
The row labeled ?Avg.?
shows the values macro-averaged over allnine tasks.the baseline on bank and 25?37 points on line.5.2 Effect of Stop ListTable 2 shows the performance of Espresso usingthe stop list built with our proposed method (HITS),compared with the vanilla Espresso not using anystop list (NoStop).In this case, the size of the stop list is set to nstop =10, and the number of seeds nseed = 10 and iterations?
= 20.
For both HITS and NoStop, the seeds areselected at random from the gold standard data, andthe reported results were averaged over 50 runs ofeach system.
Due to lack of space, only the resultsfor the second most frequent sense for each word arereported; i.e., the results for more minor senses arenot in the table.
However, they also showed a similartrend.As shown in the table, our method (HITS) outper-forms the baseline not using a stop list (NoStop), inall evaluation metrics.
In particular, the P@n listedin Table 2 shows that our method provides about11 percentage points absolute improvement over thebaseline on interest, for all n = 10, 20, and 30.6 ConclusionsWe have proposed a HITS-based method for allevi-ating semantic drift in the bootstrapping algorithmEspresso.
Our idea is built around the concept ofhubs in the sense of Kleinberg?s HITS algorithm, aswell as the algorithmic similarity between Espressoand HITS.
Hub instances are influential and hencemake good seeds if they are of the target seman-tic class, but otherwise, they may trigger semanticdrift.
We have demonstrated that our method workseffectively on lexical sample tasks.
We are currentlyevaluating our method on other bootstrapping tasks,including named entity extraction.AcknowledgementsWe thank Masayuki Asahara and Kazuo Hara forhelpful discussions and the anonymous reviewersfor valuable comments.
MS was partially supportedby Kakenhi Grant-in-Aid for Scientific Research C21500141.34ReferencesShuya Abe, Kentaro Inui, and Yuji Matsumoto.
2008.Acquiring event relation knowledge by learning cooc-currence patterns and fertilizing cooccurrence sampleswith verbal nouns.
In Proceedings of the 3rd Interna-tional Joint Conference on Natural Language Process-ing (IJCNLP ?08), pages 497?504.Steven Abney.
2004.
Understanding the Yarowsky algo-rithm.
Computational Linguistics, 30:365?395.Krishna Bharat and Monika R. Henzinger.
1998.
Im-proved algorithms for topic distillation environment ina hyperlinked.
In Proceedings of the 21st Annual In-ternational ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR ?98),pages 104?111.Rebecca Bruce and Janyce Wiebe.
1994.
Word-sensedisambiguation using decomposable models.
In Pro-ceedings of the 32nd Annual Meeting of the Associa-tion for Computational Linguistics (ACL ?94), pages139?146.Stephen Clark, James R. Curran, and Miles Osborne.2003.
Bootstrapping POS taggers using unlabelleddata.
In Proceedings of the 7th Conference on NaturalLanguage Learning (CoNLL ?03), pages 49?55.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In Proceedingsof the Joint SIGDAT Conference on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora (EMNLP-VLC ?99), pages 189?196.James R. Curran, Tara Murphy, and Bernhard Scholz.2007.
Minimising semantic drift with mutual exclu-sion bootstrapping.
In Proceedings of the 10th Con-ference of the Pacific Association for ComputationalLinguistics (PACLING ?07), pages 172?180.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings of the14th Conference on Computational Linguistics (COL-ING ?92), pages 539?545.Ashwin Ittoo and Gosse Bouma.
2010.
On learningsubtypes of the part-whole relation: do not mix yourseeds.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics (ACL?10), pages 1328?1336.Daniel Jurafsky and James H. Martin.
2008.
Speech andLanguage Processing.
Prentice Hall, 2nd edition.Jon M. Kleinberg.
1999.
Authoritative sources ina hyperlinked environment.
Journal of the ACM,46(5):604?632.Mamoru Komachi and Hisami Suzuki.
2008.
Minimallysupervised learning of semantic knowledge from querylogs.
In Proceedings of the 3rd International JointConference on Natural Language Processing (IJCNLP?08), pages 358?365.Mamoru Komachi, Taku Kudo, Masashi Shimbo, andYuji Matsumoto.
2008.
Graph-based analysis of se-mantic drift in Espresso-like bootstrapping algorithms.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP ?08),pages 1011?1020.Claudia Leacock, Geoffrey Towell, and Ellen Voorhees.1993.
Corpus-based statistical sense resolution.
InProceedings of the ARPA Workshop on Human Lan-guage Technology (HLT ?93), pages 260?265.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Process-ing.
MIT Press.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the Human Language Technology Conferenceof the North American Chapter of the Association ofComputational Linguistics (HLT-NAACL ?06), pages152?159.Tara McIntosh and James R. Curran.
2009.
Reducingsemantic drift with bagging and distributional similar-ity.
In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP (ACL-IJCNLP ?09), volume 1, pages 396?404.Tara McIntosh.
2010.
Unsupervised discovery of nega-tive categories in lexicon bootstrapping.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing (EMNLP ?10), pages356?365.Patrick Pantel and Marco Pennacchiotti.
2006.
Espresso:Leveraging generic patterns for automatically harvest-ing semantic relations.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand the 44th Annual Meeting of the Association forComputational Linguistics (COLING-ACL ?06), pages113?120.M.
F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Ellen Riloff and Rosie Jones.
1999.
Learning dictio-naries for information extraction by multi-level boot-strapping.
In Proceedings of the 16th National Confer-ence on Artificial Intelligence and the 11th InnovativeApplications of Artificial Intelligence (AAAI/IAAI ?99),pages 474?479.Mark Steedman, Rebecca Hwa, Stephen Clark, Miles Os-borne, Anoop Sarkar, Julia Hockenmaier, Paul Ruhlen,Steven Baker, and Jeremiah Crim.
2003.
Example35selection for bootstrapping statistical parsers.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology (HLT-NAACL ?03), volume 1, pages 157?164.Michael Thelen and Ellen Riloff.
2002.
A bootstrappingmethod for learning semantic lexicons using extractionpattern contexts.
In Proceedings of the ACL-02 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP ?02), pages 214?221.Vishnu Vyas, Patrick Pantel, and Eric Crestan.
2009.Helping editors choose better seed sets for entity setexpansion.
In Proceeding of the 18th ACM Conferenceon Information and Knowledge Management (CIKM?09), pages 225?234.Roman Yangarber, Winston Lin, and Ralph Grishman.2002.
Unsupervised learning of generalized names.In Proceedings of the 19th International Conferenceon Computational Linguistics (COLING ?02).David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Proceed-ings of the 33rd Annual Meeting on Association forComputational Linguistics (ACL ?95), pages 189?196.Minoru Yoshida, Masaki Ikeda, Shingo Ono, Issei Sato,and Hiroshi Nakagawa.
2010.
Person name dis-ambiguation by bootstrapping.
In Proceeding of the33rd International ACM SIGIR Conference on Re-search and Development in Information Retrieval (SI-GIR ?10), pages 10?17.36
