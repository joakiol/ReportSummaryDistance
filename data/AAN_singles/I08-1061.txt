Minimally Supervised Multilingual Taxonomy andTranslation Lexicon InductionNikesh Garera and David YarowskyDepartment of Computer ScienceCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218, USA{ngarera,yarowsky}@cs.jhu.eduAbstractWe present a novel algorithm for the acqui-sition of multilingual lexical taxonomies (in-cluding hyponymy/hypernymy, meronymyand taxonomic cousinhood), from monolin-gual corpora with minimal supervision in theform of seed exemplars using discriminativelearning across the major WordNet seman-tic relationships.
This capability is also ex-tended robustly and effectively to a secondlanguage (Hindi) via cross-language projec-tion of the various seed exemplars.
We alsopresent a novel model of translation dic-tionary induction via multilingual transitivemodels of hypernymy and hyponymy, us-ing these induced taxonomies.
Candidatelexical translation probabilities are based onthe probability that their induced hyponymsand/or hypernyms are translations of one an-other.
We evaluate all of the above modelson English and Hindi.1 IntroductionTaxonomy resources such as WordNet are limitedor non-existent for most of the world?s languages.Building a WordNet manually from scratch requiresa huge amount of human effort and for rare lan-guages the required human and linguistic resourcesmay simply not be available.
Most of the automaticapproaches for extracting semantic relations (such ashyponyms) have been demonstrated for English andsome of them rely on various language-specific re-sources (such as supervised training data, language-specific lexicosyntactic patterns, shallow parsers,(grenade)haathagolaa (explosive)baaruuda(bomb)bama(gun)banduukaexplosivegrenade bomb gunweaponInduced Hindi Hypernymy (with glosses)Induced English Hypernymyhathiyaara(weapon)Figure 1: Goal: To induce multilingual taxonomy relation-ships in parallel in multiple languages (such as Hindi and En-glish) for information extraction and machine translation pur-poses.etc.).
This paper presents a language independentapproach for inducing taxonomies such as shownin Figure 1 using limited supervision and linguis-tic resources.
We propose a seed learning based ap-proach for extracting semantic relations (hyponyms,meronyms and cousins) that improves upon existinginduction frameworks by combining evidence frommultiple semantic relation types.
We show that us-ing a joint model for extracting different semanticrelations helps to induce more relation-specific pat-terns and filter out the generic patterns1 .
The pat-1By generic patterns, we mean patterns that cannot distin-guish between different semantic relations.
For example, the465terns can then be used for extracting new wordpairsexpressing the relation.
Note that the only trainingdata used in the algorithm are the few seed pairs re-quired to start the bootstrapping process, which arerelatively easy to obtain.
We evaluate the taxonomyinduction algorithm on English and a second lan-guage (Hindi) and show that it can reliably and accu-rately induce taxonomies in two diverse languages.We further show how having induced parallel tax-onomies in two languages can be used for augment-ing a translation dictionary between those two lan-guages.
We make use of the automatically inducedhyponym/hypernym relations in each language tocreate a transitive ?bridge?
for dictionary induction.Specifically, the dictionary induction task relies onthe key observation that words in two languages (e.g.English and Hindi) have increased probabilities ofbeing translations of each other if their hypernymsor hyponyms are translations of one another.2 Related WorkWhile manually created WordNets for English (Fell-baum, 1998) and Hindi (Narayan, 2002) have beenmade available, a lot of time and effort is requiredin building such semantic taxonomies from scratch.Hence several automatic corpus based approachesfor acquiring lexical knowledge have been proposedin the literature.
Much of this work has been donefor English based on using a few evocative fixedpatterns including ?X and other Ys?, ?Y such asX?, as in the classic work by Hearst (1992).
Theproblems with using a few fixed patterns is the of-ten low coverage of such patterns; thus there is aneed for discovering additional informative patternsautomatically.
There has been a plethora of workin the area of information extraction using automat-ically derived patterns contextual patterns for se-mantic categories (e.g.
companies, locations, time,person-names, etc.)
based on bootstrapping froma small set of seed words (Riloff and Jones, 1999;Agichtein and Gravano, 2000; Thelen and Riloff,2002; Ravichandran and Hovy, 2002; Hasegawa etal.
2004; Etzioni et al 2005; Pas?ca et al 2006).This framework has been also shown to work for ex-tracting semantic relations between entities: Pantelet al (2004) proposed an approach based on edit-pattern ?X and Y?
is a generic pattern whereas the pattern ?Ysuch as X?
is a hyponym-specific patterndistance to learn lexico-POS patterns for is-a andpart-of relations.
Girju et al (2003) used 100 seedwords from WordNet to extract patterns for part-ofrelations.
While most of the above pattern inductionwork has been shown to work well for specific rela-tions (such as ?birthdates, companies, etc.?
), Section3.1 explains why directly applying seed learning forsemantic relations can result in high recall but lowprecision patterns, a problem also noted by Panteland Pennacchiotti (2006).
Furthermore, much ofthe semantic relation extraction work has focusedon extracting a particular relation independently ofother relations.
We show how this problem can besolved by combining evidence from multiple rela-tions in Section 3.2.
Snow et al(2006) also de-scribe a probablistic framework for combining ev-idence using constraints from hyponymy and cousinrelations.
However, they use a supervised logisticregression model.
Moreover, their features rely onparsing dependency trees which may not be avail-able for most languages.The key contribution of this work is using evidencefrom multiple relationship types in the seed learningframework for inducing these relationships and con-ducting a multilingual evaluation for the same.
Wefurther show how extraction of semantic relations inmultiple languages can be applied to the task of im-proving a dictionary between those languages.3 ApproachTo be able to automatically create taxonomies suchas WordNet, it is useful to be able to learn not onlyhyponymy/hyponymy directly, but also the addi-tional semantic relationships of meronymy and tax-onomic cousinhood.
Specifically, given a pair ofwords (X, Y), the task is to answer the followingquestions: 1.
Is X a hyponym of Y (e.g.
weapon,gun)?
2.
Is X a part/member of Y (e.g.
trigger, gun)?3.
Is X a cousin/sibling2 of Y (e.g.
gun, missile)?
4.Do none of the above 3 relations apply but X is ob-served in the context of Y (e.g.
airplane,accident)?3We will refer to class 4 as ?other?.2Cousins/siblings are words that share a close common hy-pernym3Note that this does not imply X is unrelated or indepen-dent of Y.
On the contrary, the required sentential co-occurenceimplies a topic similarity.
Thus, this is a much harder class todistinguish from classes 1-3 than non co-occuring unrelatedness(such as gun, protazoa) and hence was included in the evalua-tion.466Rank English Hindi1 Y, the X Y aura X(Gloss: Y and X)2 Y and X Y va X(Gloss: Y in addition to X)3 X and other Y Y ne X(Gloss: Y (case marker) X)4 X and Y X ke Y(Gloss: X?s Y)5 Y, X Y me.n X(Gloss: Y in X)Table 1: Naive pattern scoring: Hyponymy patterns ranked bytheir raw corpus frequency scores.3.1 Independently Bootstrapping LexicalRelationship ModelsFollowing the pattern induction framework ofRavichandran and Hovy (2002), one of the waysof extracting different semantic relations is to learnpatterns for each relation independently using seedsof that relation and extract new pairs using thelearned patterns.
For example, to build an inde-pendent model of hyponymy using this framework,we collected approximately 50 seed exemplars ofhyponym pairs and extracted all the patterns thatmatch with the seed pairs4.
As in Ravichandranand Hovy (2002), the patterns were ranked by cor-pus frequency and a frequency threshold was set toselect the final patterns.
These patterns were thenused to extract new word pairs expressing the hy-ponymy relation by finding word pairs that occurwith these patterns in an unlabeled corpus.
How-ever, the problem with this approach is that genericpatterns (like ?X and Y?)
occur many times in acorpus and thus low-precision patterns may end upwith high cumulative scores.
This problem is illus-trated more clearly in Table 1, which shows a listof top five hyponymy patterns (ranked by their cor-pus frequency) using this approach.
We overcomethis problem by exploiting the multi-class nature ofour task and combine evidence from multiple rela-tions in order to learn high precision patterns (withhigh conditional probabilities) for each relation.
Thekey idea is to weed out the patterns that occur in4A pattern is the ngrams occurring between the seedpair(also called gluetext).
The length of the pattern was thresholdedto 15 words.Rank English Hindi1 Y like X X aura anya Y(Gloss: X and other Y)2 Y such as X Y, X(Gloss: Y, X)3 X and other Y X jaise Y(Gloss: X like Y)4 Y and X Y tathaa X(Gloss: Y or X)5 Y, including X X va anya Y(Gloss: X and other Y)Table 2: Patterns for hypernymy class reranked using ev-idence from other classes.
Patterns distributed fairly evenlyacross multiple relationship types (e.g.
?X and Y?)
are dep-recated more than patterns focused predominantly on a singlerelationship type (e.g.
?Y such as X?
).more than one semantic relation and keep the onesthat are relation-specific5 , thus using the relationsmeronymy, cousins and other as negative evidencefor hyponymy and vice versa.
Table 2 shows the pat-tern ranking by using the model developed in Sec-tion 3.2 that makes use of evidence from differentclasses.
We can see more hyponymy specific pat-terns ranked at the top6 suggesting the usefulness ofthis method in finding class-specific patterns.3.2 A minimally supervised multi-classclassifier for identifying different semanticrelationsFirst, we extract a list of patterns from an unla-beled corpus7 independently for each relationshiptype (class) using the seeds8 for the respective classas in Section 3.1.9 In order to develop a multi-5In the actual algorithm, we will not be entirely weedingout the common patterns but will estimate the conditional classprobabilities for each pattern: p(class|pattern)6It is interesting to see in Table 2 that the top learned Hindihyponymy patterns seem to be translations of the English pat-terns suggested by Hearst (1992).
This leads to an interestingfuture work question: Are the most effective hyponym patternsin other languages usually translations of the English hyponympatterns proposed by Hearst (1992) and what are frequent ex-ceptions?7Unlabeled monolingual corpora were used for this task, theEnglish corpus was the LDC Gigaword corpus and the Hindicorpus was newswire text extracted from the web containing atotal of 64 million words.8The number of seeds used for classes {hyponym,meronym, cousin, other} were {48,40,49,50} for English andwere {32,58,31,35} for Hindi respectively.
A sample of seedsused is shown in Table 5.9We retained only the patterns that had seed frequencygreater one for extracting new word pairs.
The total number467Hypo.
Mero.
Cous.
OtherX of the Y 0 0.66 0.04 0.3Y, especially X 1 0 0 0Y, whose X 0 1 0 0X and other Y 0.63 0.08 0.18 0.11X and Y 0.23 0.3 0.33 0.14Table 3: A sample of patterns and their relationship typeprobabilities P (class|pattern) extracted at the end of trainingphase for English.Hypo.
Mero.
Cous.
OtherX aura anya Y 1 0 0 0(X and other Y)X aura Y 0.09 0.09 0.71 0.11(X and Y)X jaise Y 1 0 0 0(X like Y)X va Y 0.11 0 0.89 0(X and Y)Y kii X 0.33 0.67 0 0(Y?s X)Table 4: A sample of patterns and their class probabilitiesP (class|pattern) extracted at the end of training phase forHindi.class probabilistic model, we obtain the probabilityof each class c given the pattern p as follows:P (c|p) = seedfreq(p,c)?c?
seedfreq(p,c?
)where seedfreq(p, c) is the number of seeds of classc that were found with the pattern p in an unlabeledcorpus.
A sample of the P (class|pattern) tablesfor English and Hindi are shown in the Tables 3 and4 respectively.
It is clear how occurrence of a patternin multiple classes can be used for finding reliablepatterns for a particular class.
For example, in Table3: although the pattern ?X and Y?
will get a higherseed frequency than the pattern ?Y, especially X?,the probability P (?X and Y ?
?|hyponymy) is muchlower than P (?Y, especially X ?
?|hyponymy),since the pattern ?Y, especially X?
is unlikely to oc-cur with seeds of other relations.Now, instead of using the seedfreq(p, c) as thescore for a particular pattern with respect to aclass, we can rescore patterns using the probabilitiesP (class|pattern).
Thus the final score for a patternof retained patterns across all classes for {English,Hindi} were{455,117} respectively.p with respect to class c is obtained as:score(p, c) = seedfreq(p, c) ?
P (c|p)We can view this equation as balancing recall andprecision, where the first term is the frequency ofthe pattern with respect to seeds of class c (repre-senting recall), and the second term represents therelation-specificness of the pattern with respect toclass c (representing precision).
We recomputed thescore for each pattern in the above manner and ob-tain a ranked list of patterns for each of the classesfor English and Hindi.
Now, to extract new pairsfor each class, we take all the patterns with a seedfrequency greater than 2 and use them to extractword pairs from an unlabeled corpus.
The semanticclass for each extracted pair is then predicted usingthe multi-class classifier as follows: Given a pair ofwords (X1, X2), note all the patterns that matchedwith this pair in the unlabeled corpus, denote this setas P. Choose the predicted class c?
for this pair as:c?
= argmaxc?p?P score(p, c)3.3 Evaluation of the Classification TaskOver 10,000 new word relationship pairs were ex-tracted based on the above algorithm.
While it ishard to evaluate all the extracted pairs manually, onecan certainly create a representative smaller test setand evaluate performance on that set.
The test setwas created by randomly identifying word pairs inWordNet and newswire corpora and annotating theircorrect semantic class relationships.
Test set con-struction was done entirely independently from thealgorithm application, and hence some of the testpairs were missed entirely by the learning algorithm,yielding only partial coverage.The total number of test examples including allclasses were 200 and 140 for English and Hindi test-sets respectively.
The overall coverage10 on thesetest-sets was 81% and 79% for English and Hindirespectively.
Table 6 reports the overall accuracy11for the 4-way classification using different patternsscoring methods.
Baseline 1 is scoring patterns bytheir corpus frequency as in Ravichandran and Hovy(2002), Baseline 2 is another intutive method of10Coverage is defined as the percentage of the test cases thatwere present in the unlabeled corpus, that is, cases for which ananswer was given.11Accuracy on a particular set of pairs is defined as the per-centage of pairs in that set whose class was correctly predicted.468English HindiSeed Pairs Model Predictions Seed Pairs Model Predictionstool,hammer gun,weapon khela,Tenisa kaa.ngresa,paarTii(game,tennis) (congress,party)Hypernym currency,yen hockey,sport appraadha,hatyaa passporTa,kaagajaata(crime,murder) (passport,document)metal,copper cancer,disease jaanvara,bhaaga a.ngrejii,bhaashhaa(animal,tiger) (English,language)wheel,truck room,hotel u.ngalii,haatha jeba,sharTa(finger,hand) (pocket,shirt)Meronym headline,newspaper bark,tree kamaraa,aspataala kaptaana,Tiima(room,hospital) (captain,team)wing,bird lens,camera ma.njila,imaarata darvaaja,makaana(floor,building) (door,house)dollar,euro guitar,drum bhaajapa,kaa.ngresa peTrola,Diijala(bjp,congress) (petrol,diesel)Cousin heroin,cocaine history, geography Hindii,a.ngrejii Daalara,rupayaa(Hindi,English) (dollar,rupee)helicopter,submarine diabetes,arthritis basa,Traka talaaba,nadii(bus,truck) (pond,river)Table 5: A sample of seeds used and model predictions for each class for the taxonomy induction task.
For each of the modelpredictions shown above, its Hyponym/Meronym/Cousin classification was correctly assigned by the model.scoring patterns by the number of seeds they ex-tract.
The third row in Table 6 indicates the resultof rescoring patterns by their class conditional prob-abilties, giving the best accuracy.While this method yields some improvement overother baselines, the main point to note here is thatthe pattern-based methods which have been shownto work well for English also perform reasonablywell on Hindi, inspite of the fact that the size of theunlabeled corpus available for Hindi was 15 timessmaller than for English.Table 7 shows detailed accuracy results for each re-lationship type using the model developed in sec-tion 3.2.
It is also interesting to see in Table 8 thatmost of the confusion is due to ?other?
class beingclassified as ?cousin?
which is expected as cousinwords are only weakly semantically related and usesmore generic patterns such as ?X and Y?
which canoften be associated with the ?other?
class as well.Strongly semantically clear classes like Hypernymyand Meronymy seem to be well discriminated astheir induced patterns are less likely to occur in otherrelationship types.Model English HindiAccuracy AccuracyBaseline 1[RH02] 65% 63%Baseline 2 seedfreq 70% 65%seedfreq ?
P (c|p) 73% 66%Table 6: Overall accuracy for 4-way classification{hypernym,meronym,cousin,other} using different patternscoring methods.English HindiTotal Cover.
Acc.
Total Cover.
Acc.Hypr.
83 74% 97% 59 82% 75%Mero.
41 81% 88% 33 63% 81%Cous.
42 91% 55% 23 91% 71%Other 34 85% 31% 25 80% 20%Overall 200 81% 73% 140 79% 66%Table 7: Test set coverage and accuracy results for inducingdifferent semantic relationship types.English HindiHypo.
Mero.
Cous.
Oth.
Hypo.
Mero.
Cous.
Oth.Hypo.
59 1 1 0 36 1 10 1Mero.
1 28 1 3 0 17 4 0Cous.
14 3 21 0 6 0 15 0Other 7 3 10 9 1 4 11 4Table 8: Confusion matrix for English (left) Hindi (right) forthe four-way classification task469baaruudahathiyaarabama[via inducedhypernymy]bomb explosive grenadegunweaponbanduukahyponymy][via inducedGoal: To learn this translationhaathagolaa[via existing dictionary entries or previous induced translations]EnglishHindiFigure 2: Illustration of the models of using induced hyponymy and hypernymy for translation lexicon induction.4 Improving a partial translationdictionaryIn this section, we explore the application ofautomatically generated multilingual taxonomiesto the task of translation dictionary induction.
Thehypothesis is that a pair of words in two languageswould have increased probability of being transla-tions of each other if their hypernyms or hyponymsare translations of one another.As illustrated in Figure 2, the probability thatweapon is a translation of the Hindi word hathiyaaracan be decomposed into the sum of the probabilitiesthat their hyponyms in both languages (as inducedin Section 3.2) are translations of each other.
Thus:PH?>E (WE |WH) =?i Phyper (WE |Eng(Hi)) Phypo(Hi|WH)for induced hyponyms Hi of the source wordWH , and using an existing (and likely very incom-plete) Hindi-English dictionary to generate Eng(Hi)for these hyponyms, and the corresponding inducedhypernyms of these translations in English.12.
Weconducted a very preliminary evaluation of this ideafor obtaining English translations of a set of 2512One of the challenges of inducing a dictionary via using acorpus based taxonomy is sense disambiguation of the words tobe translated.
In the current model, the more dominant sense(in terms of corpus frequency of its hyponyms) is likely to getselected by this approach.
While the current model can stillhelp in getting translations of the dominant sense, possible fu-ture work would be to cluster all the hyponyms according tocontextual features such that each cluster can represent the hy-ponyms for a particular sense.
The current dictionary inductionmodel can then be applied again using the hyponym clusters todistinguish different senses for translation.Hindi words.
The Hindi candidate hyponym spacehad been pruned of function words and non-nounwords.
The likely English translation candidatesfor each Hindi word were ranked according to theprobability PH?>E(WE|WH).The first column of Table 9 shows the stand-aloneperformance for this model on the dictionary induc-tion task.
This standalone model has a reasonablygood accuracy for finding the correct translation inthe Top 10 and Top 20 English candidates.Accuracy Accuracy Accuracy(uni-d) (bi-d) bi-d + OtherTop 1 20% 36% 36%Top 5 56% 64% 72%Top 10 72% 72% 80%Top 20 84% 84% 84%Table 9: Accuracy on Hindi to English word translation usingdifferent transitive hypernym algorithms.
The additional modelcomponents in the bi-d(irectional) plus Other model are onlyused to rerank the top 20 candidates of the bidirectional model,and are hence limited to its top-20 performance.This approach can be further improved by also im-plementing the above model in the reverse directionand computing the P (WH |WEi) for each of theEnglish candidates Ei.
We did so and computedP (WH |WEi) for top 20 English candidate trans-lations.
The final score for an English candidatetranslation given a Hindi word was combined bya simple average of the two directions, that is, bysumming P (WEi |WH) + P (WH |WEi).The second column of Table 9 shows how thisbidirectional approach helps in getting the right470translations in Top 1 and Top 5 as compared to theunidirectional approach.
Table 10 shows a sampleCorrectly translated Incorrectly translatedaujaara vishaya(tool) (topic)biimaarii saamana(disease) (stuff)hathiyaara dala(weapon) (group,union)dastaaveja tyohaara(documents) (festival)aparaadha jagaha(crime) (position,location)Table 10: A sample of correct and incorrect translations usingtransitive hypernymy/hyponym word translation inductionof correct and incorrect translations generatedby the above model.
It is interesting to see thatthe incorrect translations seem to be the wordsthat are very general (like ?topic?, ?stuff?, etc.
)and hence their hyponym space is very large anddiffuse, resulting in incorrect translations.While thecolumns 1 and 2 of Table 9 show the standaloneapplication of our translation dictionary inductionmethod, we can also combine our model withexisting work on dictionary induction using othertranslation induction measures such as using relativefrequency similarity in multilingual corpora andusing cross-language context similarity betweenword co-occurrence vectors (Schafer and Yarowsky,2002).We implemented the above dictionary induc-tion measures and combined the taxonomy baseddictionary induction model with other measures byjust summing the two scores13.
The preliminaryresults for bidirectional hypernym/hyponym +other features are shown in column 3 of Table9.
The results show that the hypernym/hyponymfeatures can be a useful orthogonal source of lexicalsimilarity in the translation-induction model space.While the model shown in Figure 2 proposesinducing translations of hypernyms, one can also goin the other direction and induce likely translationcandidates for hyponyms by knowing the translationof hypernyms.
For example, to learn that rifle isa likely translation candidate of the Hindi word13after renormalizing each of the individual score to be in therange 0 to 1.raaiphala, is illustrated in Figure 3.
But becausethere is a much larger space of hyponyms forweapon in this direction, the output serves more toreduce the entropy of the translation candidate spacewhen used in conjunction with other translationinduction similarity measures.
We would expect theapplication of additional similarity measures to thisgreatly narrowed and ranked hypothesis space toyield improvement in future work.5 ConclusionThis paper has presented a novel minimal-resourcealgorithm for the acquisition of multilingual lex-ical taxonomies (including hyponymy/hypernymyand meronymy).
The algorithm is based on crosslanguage projection of various monolingual indica-tors of these taxonomic relationships in free textand via bootstrapping thereof.
Using only 31-58seed examples, the algorithm achieves accuracies of73% and 66% for English and Hindi respectively onthe tasks of hyponymy/meronomy/cousinhood/othermodel induction.
The robustness of this approachis shown by the fact that the unannotated Hindi de-velopment corpus was only 1/15th the size of theutilized English corpus.
We also present a novelmodel of unsupervised translation dictionary induc-tion via multilingual transitive models of hypernymyand hyponymy, using these induced taxonomies andevaluated on Hindi-English.
Performance startingfrom no multilingual dictionary supervision is quitepromising.ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: extract-ing relations from large plain-text collections.
In Pro-ceedings of the 5th ACM International Conference onDigital Libraries, pages 85?94.M.
J. Cafarella, D. Downey, S. Soderland, and O. Etzioni.2005.
Knowitnow: Fast, scalable information extrac-tion from the web.
In Proceedings of EMNLP/HLT-05,pages 563?570.S.
Caraballo.
1999.
Automatic construction of ahypernym-labeled noun hierarchy from text.
In Pro-ceedings of ACL-99, pages 120?126.B.
Carterette, R. Jones, W. Greiner, and C. Barr.
2006.
Nsemantic classes are harder than two.
In Proceedingsof ACL/COLING-06, pages 49?56.471raaiphala missile grenade bombrifleweapon(hypothesis space)[via inducedhathiyaaraor previous induced translations][via existing dictionary entrieshypernymy][via inducedhyponymy]Hindi EnglishGoal: To learn this translationFigure 3: Reducing the space of likely translation candidates of the word raaiphala by inducing its hypernym, using a partialdictionary to look up the translation of hypernym and generating the candidate translations as induced hyponyms in English space.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. S. Weld, and A. Yates.2005.
Unsupervised named-entity extraction from theweb: an experimental study.
Artif.
Intell., 165(1):91?134.C.
Fellbaum.
1998.
WordNet: An electronic lexicaldatabase.R.
Girju, A. Badulescu, and D. Moldovan.
2003.Learning semantic constraints for the automatic dis-covery of part-whole relations.
In Proceedings ofHLT/NAACL-03, pages 1?8.R.
Girju, A. Badulescu, and D. Moldovan.
2006.
Au-tomatic discovery of part-whole relations.
Computa-tional Linguistics, 21(1):83?135.D.
Graff, J. Kong, K. Chen, and K. Maeda.
2005.
En-glish Gigaword Second Edition.
Linguistic Data Con-sortium, catalog number LDC2005T12.T.
Hasegawa, S. Sekine, and R. Grishman.
2004.
Discov-ering relations among named entities from large cor-pora.
In Proceedings of ACL-04, pages 415?422.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of COLING-92, pages 539?545.D.
Narayan, D. Chakrabarty, P. Pande, and P. Bhat-tacharyya.
2002.
An Experience in Building the IndoWordNet-a WordNet for Hindi.
International Confer-ence on Global WordNet.Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-chits, and Alpa Jain.
2006.
Names and similarities onthe web: Fact extraction in the fast lane.
In Proceed-ings of ACL/COLING-06, pages 809?816.P.
Pantel and M. Pennacchiotti.
2006.
Espresso: Lever-aging generic patterns for automatically harvestingsemantic relations.
In Proceedings of ACL/COLING-06, pages 113?120.P.
Pantel and D. Ravichandran.
2004.
Automati-cally labeling semantic classes.
In Proceedings ofHLT/NAACL-04, pages 321?328.P.
Pantel, D. Ravichandran, and E. Hovy.
2004.
Towardsterascale knowledge acquisition.
In Proceedings ofCOLING-04.D.
Ravichandran and E. Hovy.
2002.
Learning surfacetext patterns for a question answering system.
In Pro-ceedings of ACL-02, pages 41?47.E.
Riloff and R. Jones.
1999.
Learning dictionaries forinformation extraction by multi-level bootstrapping.In Proceedings of AAAI/IAAI-99, pages 474?479.E.
Riloff and J. Shepherd.
1997.
A corpus-based ap-proach for building semantic lexicons.
CoRR, cmp-lg/9706013.C.
Schafer and D. Yarowsky.
2002.
Inducing translationlexicons via diverse similarity measures and bridgelanguages.
In Proceedings of CONLL-02, pages 146?152.R.
Snow, D. Jurafsky, and A. Y. Ng.
2006.
Semantic tax-onomy induction from heterogenous evidence.
In Pro-ceedings of ACL/COLING-06, pages 801?808.M.
Thelen and E. Riloff.
2002.
A bootstrapping methodfor learning semantic lexicons using extraction patterncontexts.
In Proceedings of EMNLP-02, pages 214?221.D.
Widdows.
2003.
Unsupervised methods for devel-oping taxonomies by combining syntactic and statisti-cal information.
In Proceedings of HLT/NAACL-03,pages 197?204.472
