Word Extraction from Corpora and Its Part-of-SpeechEstimation Using Distributional AnalysisShinsuke Mori and Makoto Nagaol)ept, of Electrical Engineering Kyoto UniversityYoshida-honmachi, Sakyo, Kyoto, 606-01 Japan{mor ?, nagao } ~kuee.
kyoto-u,  ac.
j pAbst ractUnknown words are inevitable at anystep of analysis in natural anguage pro-cessing.
Wc propose a method to ex-tract words from a corl)us and estimatethe probability that each word belongsto given parts of speech (POSs), using adistributional analysis.
Our experimentshave shown that this method is etfectivefor inferring the POS of unknown words.1 IntroductionDictionaries are indispensable in NLP in order todetermine the grammatical functions and mean-ings of words, but the coutinuous increase of newwords and technical terms make unknown wordsan ongoing problem.
A good <teal of research asbeen directed to finding efficient and effective waysof expanding the lexicon.
With agglutinative lan-guages like Japanese, tile problem is even greater,since even word boundaries are ambiguous.
Tosolve these problems, we propose a method thatuses distributional analysis to extract words froma corpus and estimate the probability distributionof their usc a.s ditferent parts of speech.Distributional analysis was originally proposedby Harris (1951), a structural inguist, tLs a tech-nique to uncover the structure of a language.
Llar-ris intended it ~Ls a substitute for what he per-ceived ~ts unscientific information-gathering by lin-guists doing field work at that time.
Thus, lin-guists determine whether two words belong tothe same class by observing the environments inwhich the words occur.
Recently, this techniquehas been mathematically refined and used to dis-cover phrase structure from a corpus annotatedwith POS tags (Brill and Marcus, 1992; Mori andNagao, 1995).
Schiltze (1995) used the techniqueto induce POSs.
However, in these researches,the problem of catcgorial ambiguity (the fact thatsome words or POS sequences can belong to moretitan one category), has been ignored.In this paper, we propose a method that as-sumes that a word may belong to more than onePOS, and provides estimates of the relative i)rol) -ability that it may belong to each of a number ofPOSs.
Our method decomposes an observed prob-ability distribution into a particular linear sum-mation of a given set of model l)robability distri-butions.
'1't1?.'
resulting set of coefficients rcl)rc-sents tim probability that the observed event be-longs to each model event.
The application d:ls-cussed here is word extraction fron\] a Japanesecorpus.
First we calculate the model probabilitydistribution of each POS by observing the contextof each occurrence in a tagged corpus.
Then, foreach unknown word, we similarly calculate its en-vironment by collecting all occurren('es from a rawcorpus.
Finally, wc coml)ute tile probability dis-tribution of POSs for a word by comparing its ob-served environment with tile model environmentsrepresented by the set of POS distributions.1,1 subsequent sections, first we discuss the hy-pothesis, secondly describe the algorithm, thirdlypresent results of the exl)eriments on the ED\]{ cor-pus and journal articles, and finally conclude thisrcsearch.2 HypothesisIn this section, first we define environment of astring occurring ix, a corpus.
Next, we prol)osea hypothesis which gives foundation to our wordextraction method.2.1 Env i ronment  of  a Str ing in a CorpusWe detine tile "environment" of a type (characterstring, group of morl)hemes , or as tile prol)abilitydistribution of the elements preceding and follow?ing occurrences of that type in a corpus.
The ele-ments which precede tile type a.re described by theleft probability distribution, and those which fol-low it, by the right probability distribution.
Forinstance, Table 1 shows the one-character nvioromnent of the string ".~-L" in the I~DR corpus(Jap, 1993).
This string occurs 181 times, wittl 12different characters appearing to its left and l0 toits right.In general, a probability distribution can be re-garded a.s a vector, so the concatenatiori of two1119vectors is also a vector.
Thus, the concatenationof the left and right probability distributions fora type is what we call the "environment" of thattype, and we represent his by D in the subse-quent part of this paper.Table 1: Environment of the string "~-1_."freq.
prob.
str.
str.
freq.
prob.13 7.2% , :~b  v,, 16 8.9%6 3.3% o < 3 1.6%13 7.2% ~ ~ 8 4.4%10 5.6% ~ ~: 10 5.6%8 4.4% <" J\[ 7 3.8%14 7.8% if- & 41 22.6%19 10.4% m ~ 38 21.0%4 2.2% ?& ab 16 8.9%7 3.8% ~ % 4 2.2%4 2.2% ?0 L/ 38 21.0%83 45.9%181 100.0% total 181 100.0%2.2 Hypothes is  Concern ing  Env i ronmentIn general, if a string a is a word which belongs toa POS, it is expected that the environment D(a)of the string in a particular corpus will be similarto the environment D(pos) of that POS.
Sincea word can belong to more than one POS, it isexpected that the environment of tire string willbe similar to the summation across all POSs ofthe environment of each POS multiplied by tileprobability that the string occurs ms that POS.Therefore, we obtain the following formula:D(et) ..~ Zp(posk\[a)D(posk ) (1)kwhere p(poskla) is the probability that the stringa belongs to posk, and D(posk) is tire environ-ment of posk.
In this formula, summation iscalculated for the set of POSs in consideration.As an example, let us take the string "~.1_.
",which is used in tile corpus only as a verb andan adjective.
Ifp(Adjl'Z~l..) and p(Verbl~-b ) arethe probabilities that a particular instance of thestring is used as an adjective and a verb respec-tively, then the enviromnent of the string "x~-I~"is described by the tbllowing formula: D(-x~-U)p(Adjl~- L )D(Adj) + p( VerblX~ - L )D(Verb),In most cases, however, formula (1) cannot besolved as a linear equation, since the dimension ofprobability distribution vector D is greater thanthat of the independent variables.
In addition, weneed to minimize the effects of sample bias inher-ent in statistical estimates of this sort.
We there-fore reason that the question is to find the set ofp(posk let) which minimizes the difference betweenboth sides of formula (1) in terms of some mea-sure.
We use, as this measure, the square of Eu-clidean distance betwen vectors.
Then it followsthat the problem is formalized as an optimizationproblem (minimize).
The decision variables arethe elements of tile probability distribution vectorp which expresses tile likelihood that the string isused as each POS:F(p) = \]D(et) - ZpkD(posk) l  2 (2)kwhere p = (Pl,P2,...,P,~), Pk = p(poskl o~) andn is tile number of POSs in consideration.
Sinceeach element o fp  represents a probability, the fea-sible region V is given as follows:V={plO<_pk<_ l ,~p~=l}  (3/kThe minimum value of F(p) will be relativelysmall when tile environment of the string canbe decomposed into a linear summation of somePOS environments, while it will be relatively largewhen such a decomposition does not exist.
Sinceall true words must belong to one or more POSs,the minimum value of F(p) can be used to decidewhether a string is a word or not.
We call thisvalue the "word measure," and accept as wordsall strings with word measure Less than a certainthreshold.3 A lgor i thmIn this section we describe the algorithm used tocalcnlate tile word rneasure of all arbitrary stringand tire probabilities that the string belongs toeach of a set of POSs.
We used observations fromtile EDI{ corpus, which is divided into words andtagged as to POS, to calculate tile POS environ-ments, and then used a raw corpus (no indicationof word or morpheme boundaries, and no POStags) for calculating the string environments.3.1 Ca lcu lat ing POS Env i ronmentsThe environment of each POS is obtained by cal-culating statistics on all contexts that precede andfollow the POS in a tagged corpus, as follows:1.
Let al elements of left and right probabilityvectors be 0.2.
For each occurrence of the POS in the corpus,iucrement the left vector elenmnt correspond-ing to the context preceding this occurrenceof the POS, and increment the right vector el-ement corresponding to the context followingthe POS.3.
Divide each vector element by the total num-ber of o<:currences of the POS.Figure 1 shows a sample sentence from the EI)Rcorpus, and Table 2 shows the computation ofthe one-character nvironment of Noun in the tinycorpus consisting of this single sentence.In practice, instead of a single character, weused as contexts the preceding or following POS-tagged string (a morpheme or word).
Thus the11201 2 a 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18L :b , t  , 5~H 03 ~IN  tJ: ~ iL  w q~.
--- ?
~N ?g k~ J- q~Y~@ tff oconj.
sign noun pp noun pp adj.
intl.
noun pp pp noun pp verb in\[I. noun aux.
signFigure 1: An Example of EDR Corpusprobabil ity vectors, which consisted of all the con-texts found for any POS, were so sparse that weused a hash algorithm.Table 2: Environment of the Nounfreq.
prob.
str.
str.
freq.
prob.1 20% , noun td 1 20%1 20% m a.)
1 20%1 20% ~J- a 1 20%2 40% a) ~.
1 20%1 20%5 100% total 5 100%3.2 Ca lcu la t ing  S t r ing  Env i ronmentsThe cMculation of the enviromnent of an arbi-trary string (possible word) in a corpus is basi-cally identical to tire POS algorithm above, ex-cept that because Japanese has no blank spacebetween words arr(t a raw (unsegmented) corpusis used, the extent of the environment is ambigu-ous.
There are two ways to determine the extentof the left and right environment: one is to spec-ify a fixed number of characters, and the otheris to use a look-up-and-match procedure to iden-tify specific morphenms.
We adol)ted the secondmethod, and used as a mort)henm lexicon the setof hash keys representing the POS envirouments.Where there was a conflict between two or morepossible matches of a string context with tire POShash keys, the longest match was selected.
For in-stance, although a right context zi, ro 'kate'  couhtmatch either the postposition 'ka' or the postl)osi-lion 'kara', the longer match 'kara' would alwaysbe chosen.3.3 Opt imizat ionThe environments for a string and for each POSwhich it represents become the parameters of theobjective flmction defined I)y formula (2), and theoptimization of this flmction then yields the prob-abilities that the string belongs to each l)OS.
The.problem can be solved e~sily by the optimal gra-dient method because both the objective functionand the feasible region are convex.4 Resu l tsWe conducted two experiments, in each using arange of different thresholds for word measure.One experiment used the El)\]{ corl)us a.s a rawcorpus (ignoring the POS tags) in order to cal-('ulate recall and precision.
The other experimentTable 3: Recall and precision on EI)I~ corpusthreshold recall precisionof l"min tokens types tokens types0.10 26.2% 12.6% 96.8% 86.2%0.15 46.4% 28.3% 94.0% 80.4%0.20 61.8% 44.0% 90.7% 74.9%0.25 73.2% 57.1% 87.4% 69.1%0.30 79.8% 66.7%0.35 84.4% 73.7%used articles fl'om one year of the J apanese versiorlof Scientific A meT~ican i  order to test whether wecould incre~Lse the accuracy of the morphologicalanalyzer (tagger) by this method.4.1 Cond i t ions  o f  the Ex i )e r imentsFor I)oth experiments, we considered tire fivePOSs to which almost all unknown words inJapanese belong:I. verbal noun, e.g.
N '~ (-~- 6)  'benkyou(.~,,'u)'"to study"2. nora,s, e.g. '
- "~ 'gakkou'  "school"3. re-type verb, e.g.
:1~ -'e.
( 5 ) 'tal)e(ru)' "to eat"4. i-type adjective, e.g.
-~- (v,) 'samu(i) '  "cold"5. na-type adjective, e.g.
~'bv, (~av) 'kirei(na)'"cleaI|"POS environments were defined as one POS-tagged string (assumed to be one morpheme), andwere limited to strings made up only of h*raganacharacters plus comma and period.
The aim ofthis limitation was to reduce computational timeduring inatehing, and it was \['ell, that morl)hemesusing kanji and katakana characters are too infre-quent ~s contexts to exert much intluence on theresults.Candidate for unknown words were limited tostrings of two or more characters appearing inthe corpus at least ten times and not containingany symbols such as parentheses.
Since there arevery few unknown words which consist of only onecharacter, this limitation will not have much effecton the recall.4.2 Exper iment  1: Word  Ext rac t ionFor evaluation purposes, we conducted a wordextraction ext)eriment using the El)l{.
corpus asa raw corpus, and calculated recall and precision\['or each threshold value (see Table 3).
First, wecalculated f'mi,~and p for all character n-grams,1121Table 4: Examples of extracted words from "Science"string Action noun Noun R,a-type verb !-type adj.
Na-type adj.N~; 0.00 0.31 0.00 0.00 0.69J~Y'} O.O0 0.00 0.00 0.00 1.00:-'~'~  ~ * 0.00 1.00 0.00 0.00 0.00MH C 3r)-f~ * 0.00 1.00 O.O0 0.00 0.00?
means unknown word.F~i, freq.0.04 1150.05 1790.08 1030.11 632 < n < 20, excluding strings which consisted onlyof hiragana characters.
Then, for each thresholdlevel, our algorithm decided which of the candi-date strings were words, and assigned a POS toeach instance of the word-strings.Recall was computed as the percent, of all POS-tagged strings in the EDR corpus that were suc-cessfully identified by our algorithm as wordsand as belonging to the correct POS.
In calcu-lation of the recalls and the precisions, both POSand string is distinguished.
Precision was calcu-lated using the estimated frequency f((~,pos) =p(posl~ ) .f(tx) where f( ,x) is the frequency of thestring ~t in the corpus, and p(poslot) is the esti-mated probability that ct belongs to the pos.Judgement whether the string ~ belongs to posor not was made by hand.
The recalls are calcu-lated for ones with the estimated probability morethan or equal to 0.1.
The reason for this is thatthe amount of the output is too enormous to checkby hand.
For the same reason we did not calcu-late the precisions for thresholds more than 0.25in Table 3.
This table tells us that the lower thethreshold is, the higher the precision is.
This re-sult is consistent with the result derived from thehypothesis that we described in section 2.2.
Be-sides, there is a tendency that in proportion ,as thefrequency increases the precision rises.4.3 Exper iment  2: Improvement  ofS tochast ic  TaggingIn order to test how much the accuracy of a tag-ger could be improved by adding extracted wordsto its dictionary, we developed a tagger based ona simple Markov model and analyzed one journalarticle 1.
Using statistical parameters estimatedfrom the EDR corpus, and an unknown wordmodel based on character set heuristics (any kaujisequence is a noun, etc.
), tagging accuracy was95.9% (the percent of output morphemes whichwere correctly segmented and tagged).Next, we extracted words from the Japaneseversion of Scientific American (1990; 617,837characters) using a threshold of 0.25.
Unknownwords were considered to he those which could notbe divided into morphemes appearing in the learn-ing corpus of the Markov model.
Table 4 showsexamples of extracted words, with unknown wordsa"Progress in Gallium Arsenide Semiconductors"(Scientific American; February, 1990)starred.
Notice that some extracted words consistof more than one type of character, such as "3~ :/- '~ '~ (protein)."
This is one of the advantagesof our method over heuristics based on charactertype, which can never recognize mixed-characterwords.
Another advantage is that our method isapplicable to words belonging to more than onePOS.
For example, in Table 4 "Et;~ (nature)" isboth a noun and the stem of a na-type adjective.We added the extracted unknown words to thedictionary of the stochastic tagger, where theyare recorded with a frequency calculated by thefollowing fo,'mula: (size~/size,)f(c~,pos), wheresize~ and size, are the size of the EDR corpusand the size of the Scientific A merican corpus re-spectively.
Using this expanded dictionary, thetagger's accuracy improved to 98.2%.
This resulttells us that our method is useful as a preprocessorfor a tagger.5 Conc lus ionWe have described a new method to extract wordsfrom a corpus and estimate their POSs using dis-tributional analysis.
Our method is based on thehypothesis that sets of strings preceding or fol-lowing two arbitrary words belonging to the samePOS are similar to each other.
We have proposeda mathematically well-founded method to com-pute probability distrii)ution in which a string be-longs to given POSs.
The results of word extrac-tion experiments attested the correctness of ourhypothesis.
Adding extracted words to the dic-tionary, the accuracy of a morphological nalyzeraugmented considerably.Re ferencesEric Brill and Mitchell Marcus.
1992.
Automati-cally acquiring phrase structure using distribu-tional analysis.
In Proc.
of the DARPA Speechand Natural Language WorkshopZellig Ilarris.
1951.
Structural Linguistics.
Uni-versity of Chicago Press.Japan Electronic Dictionary Research Institute,Ltd., 1993.
EDR Electronic Diclwnary Techni-cal Guide.Shinsuke Mori and Makoto Nagao.
1995.
Parsingwitilout grammar.
In Proc.
of the IWPT95Hinrich Schiitze.
1995.
Distributional part-of-speech tagging.
In Proc.
of the EA CL95.1122
