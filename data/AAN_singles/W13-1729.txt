Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 224?231,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsNative Language Identification: a Simple n-gram Based ApproachBinod Gyawali and Gabriela Ramirez and Thamar SolorioCoRAL LabDepartment of Computer and Information SciencesUniversity of Alabama at BirminghamBirmingham, Alabama, USA{bgyawali,gabyrr,solorio}@cis.uab.eduAbstractThis paper describes our approaches to Na-tive Language Identification (NLI) for the NLIshared task 2013.
NLI as a sub area of au-thor profiling focuses on identifying the firstlanguage of an author given a text in his sec-ond language.
Researchers have reported sev-eral sets of features that have achieved rel-atively good performance in this task.
Thetype of features used in such works are: lex-ical, syntactic and stylistic features, depen-dency parsers, psycholinguistic features andgrammatical errors.
In our approaches, we se-lected lexical and syntactic features based onn-grams of characters, words, Penn TreeBank(PTB) and Universal Parts Of Speech (POS)tagsets, and perplexity values of character ofn-grams to build four different models.
Wealso combine all the four models using an en-semble based approach to get the final result.We evaluated our approach over a set of 11 na-tive languages reaching 75% accuracy.1 IntroductionRecently, a growing number of applications are tak-ing advantage of author profiling to improve theirservices.
For instance, in security applications (Ab-basi and Chen, 2005; Estival et al 2007) to helplimit the search space of, for example, the author ofan email threat, or in marketing where the demog-raphy information about customers is important topredict behaviors or to develop new products.Particularly, author profiling is a task of identi-fying several demographic characteristics of an au-thor from a written text.
Demographic groups can beidentified by age, gender, geographic origin, level ofeducation and native language.
The idea of identi-fying the native language based on the manner ofspeaking and writing a second language is borrowedfrom Second Language Acquisition (SLA), wherethis is known as language transfer.
The theory oflanguage transfer says that the first language (L1)influences the way that a second language (L2) islearned (Ahn, 2011; Tsur and Rappoport, 2007).According to this theory, if we learn to identify whatis being transfered from one language to another,then it is possible to identify the native language ofan author given a text written in L2.
For instance,a Korean native speaker can be identified by the er-rors in the use of articles a and the in his Englishwritings due to the lack of similar function words inKorean.
As we see, error identification is very com-mon in automatic approaches, however, a previousanalysis and understanding of linguistic markers areoften required in such approaches.In this paper we investigate if it is possible to buildnative language classifiers that are not based on theanalysis of common grammatical errors or in deepersemantic analysis.
On the contrary, we want to finda simple set of features related to n-grams of words,characters, and POS tags that can be used in an ef-fective way.
To the best of our knowledge, almostall the works related to L1 identification use finegrained POS tags, but do not look into whether acoarse grained POS tagset could help in their work.Here, we explore the use of coarse grained Univer-sal POS tags with 12 POS categories in the NLI taskand compare the result with the fine grained PennTreeBank (PTB) POS tags with 36 POS categories.224Moreover, we also investigate how the system workswhen perplexity values are used as features in iden-tifying native languages.
Using an ensemble basedapproach that combines four different models builtby various combinations of feature sets of n-gramsof words, characters, and POS tags, and perplexityvalues, we identify the native language of the author,over 11 different languages, with an accuracy closeto 80% and 75% in development and test dataset re-spectively.2 Related WorkThe first known work about native language identifi-cation appears in 2005 (Koppel et al 2005).
In theirstudy, the authors experimented with three types offeatures, i.e.
function words, letter n-grams, er-rors and idiosyncrasies.
But their analysis was fo-cused on the identification of common errors.
Theyfound that using a combination of all the features ina Support Vector Machine (SVM), they can obtainan accuracy of 80% in the classification of 5 differ-ent native languages.
As in this first study, analyz-ing errors is common in native language identifica-tion methods, since it is a straightforward adapta-tion of how this task is performed in SLA.
For in-stance, Wong and Dras (2009) investigate the useof error types such as disagreement on subject-verband noun-number, as well as misuse of determin-ers to show that error analysis is helpful in this task.But their results could not outperform the results ob-tained by Koppel et al(2005).
They also suggestedthat analyzing other types of errors might help to im-prove their approach.
In the same path, Jarvis et al(2012) investigate a larger variety of errors, for ex-ample lexical words and phrase errors, determinererrors, spelling errors, adjective order errors and er-rors in the use of punctuation marks, among others.But they also could not achieve results comparableto the previous results in this task.Since language transfer occurs when grammati-cal structures from a first language determine thegrammatical structures of a second language, the in-clusion of function words and dependency parsersas features seem to be helpful to find such trans-fers as well as error types (Tetreault et al 2012;Brooke and Hirst, 2011; Wong et al 2012).
Itis common that the analysis of the structure ofcertain grammatical patterns is also informative tofind the use or misuse of well-established gram-matical structures (e.g.
to distinguish between theuse of verb-subject-object, subject-verb-object, andsubject-object-verb), in such cases n-grams of POStags can be used.
Finally, according to Tsur andRappoport (2007), the transfer of phonemes is use-ful in identifying the native language.
Even thoughthe phonemes are usually speech features, the au-thors suggest that this transfer can be captured bythe use of character n-grams in the text.
Charactern-grams have been proved to be a good feature inauthor profiling as well since they also capture hintsof style, lexical information, use of punctuation andcapitalization.In sum, there are varieties of feature types usedin native language identification, most of them com-bine three to nine types.
Each type aims to capturespecific information such as lexical and syntactic in-formation, structural information, idiosyncrasies, orerrors.3 Shared Task DescriptionThe Native Language Identification (NLI) sharedtask focuses on identifying the L1 of an author basedon his writing in a second language.
In this case,the second language is English.
The shared task hadthree sub-tasks: one closed training and two opentraining.
The details about the tasks are describedby Tetreault et al(2013).
For each subtask, the par-ticipants were allowed to submit up to five runs.
Weparticipated in the closed training sub-task and sub-mitted five runs.The data sets provided for the shared task weregenerated from the TOEFL corpus (Blanchard et al2013) that contains 12, 100 English essays.
Thecorpus comprised 11 native languages (L1s): Ara-bic (ARA), Chinese (CHI), French (FRE), German(GER), Hindi (HIN), Italian (ITA), Japanese (JPN),Korean (KOR), Spanish (SPA), Telugu (TEL), andTurkish (TUR), each containing 1100 essays.
Thecorpus was divided into training, development, andtest datasets with 9900, 1100, and 1100 essays re-spectively.
Each L1 contained an equal number ofessays in each dataset.225Feature Sets N-gramsError rates for top k features500 800 1000 3000 6000Character n-grams2 grams 78.27 77.64 77.18 75.82 -3 grams 78.55 60.55 64.27 43.73 44.36Word n-grams2 grams 66.55 58.36 55.64 44.91 38.733 grams 75.55 69.18 76.36 67.09 54.18PTB POS n-grams2 grams 69.73 76.73 69.55 72.09 -3 grams 72.82 72.45 67.27 56.18 62.27Universal POS n-grams2 grams 85.36 - - - -3 grams 78.1818 79.55 72.36 85.27 -Table 1: Error rates in L1 identification using various feature sets with different number of features4 General System DescriptionIn this paper we describe two sets of experiments.We performed a first set of experiments to evaluatethe accuracy of different sets of features in order tofind the best selection.
This set was also intended todetermine the threshold of the number of top fea-tures in each set needed to obtain a good perfor-mance in the classification task.
These experimentsare described in Section 5.In the second set, we performed five different ex-periments for five runs.
Four of the five modelsused different combinations of feature sets to trainthe classifier.
The major goal of these experimentswas to find out how good the results achieved canbe by using lower level lexical and shallow syntacticfeatures.
We also compared the accuracy obtainedby using the fine grained POS tags and the coarsegrained POS tags.
In one of these experiments, weused perplexity values as features to see how effec-tive these features can be in NLI tasks.
Finally, thefifth experiment was an ensemble based approachwhere we applied a voting scheme to the predictionsof the four approaches to get the final result.
The de-tails of these experiments are described in Section 6.In our experiments, we trained the classifier usingthe training dataset, and using the model we testedthe accuracy on the development and test dataset.We used an SVM multiclass classifier (Crammer andSinger, 2002) with default parameter settings for themachine learning tasks.
We used character n-grams,word n-grams, Parts of Speech (POS) tag n-grams,and perplexity of character trigrams as features.
Forall the features except perplexity, we used a TF-IDFweighting scheme.
To reduce the number of fea-tures, we selected only the top k features based onthe document frequency in the training data.The provided dataset contained all the sentencesin the essays tokenized by using ETS?s proprietarytokenizers.
For the POS tags based features, weused two tagsets: Penn TreeBank (PTB) and Uni-versal POS tags.
For PTB POS tags, we tagged thetext with the Stanford parser (Klein and Manning,2003).
In order to tag the sentences with UniversalPOS tags, we mapped the PTB POS tags to universalPOS tags using the mapping described by Petrov etal.
(2011).We also used perplexity values from languagemodels in our experiments.
To generate the lan-guage models and compute perplexity, we used theSRILM toolkit (Stolcke et al 2011).
We used train-ing data to generate the language models and trainthe classifier.
Finally, all the sentences were con-verted into lower case before finding the word andcharacter n-grams.5 Feature Sets EvaluationWe performed a series of experiments using a sin-gle feature set per experiment in order to find thebest combinations of features to use in classificationmodels.
All of the feature sets were based on n-grams.
We ranked the n-grams by their frequencieson the training set and then used the development setto find out the best top k features in the training set.We used the values of k as 500, 800, 1000, 3000,and 6000 for this set of experiments.
The error ratesof these experiments are shown in Table 1.
Since thetotal number of features in character bigrams, PTB226Exp-W2,3PTB3C3 Exp-W2,3Univ3C3 Exp ClassBased Exp Perplexity Exp EnsembleL1 P R F1 P R F1 P R F1 P R F1 P R F1ARA 90.7 68.0 77.7 87.1 54.0 66.7 72.2 70.0 71.1 70.8 51.0 59.3 90.9 70.0 79.1CHI 79.0 83.0 81.0 57.9 84.0 68.6 75.0 78.0 76.5 71.7 66.0 68.8 78.4 87.0 82.5FRE 91.5 75.0 82.4 75.7 81.0 78.3 92.8 64.0 75.7 71.2 74.0 72.5 90.8 79.0 84.5GRE 86.0 92.0 88.9 77.5 86.0 81.5 84.2 85.0 84.6 63.8 83.0 72.2 88.3 91.0 89.7HIN 67.3 66.0 66.7 70.0 63.0 66.3 66.3 63.0 64.6 52.3 45.0 48.4 70.2 66.0 68.0ITA 72.3 94.0 81.7 76.9 83.0 79.8 66.4 89.0 76.1 65.3 77.0 70.6 74.6 94.0 83.2JPN 86.6 71.0 78.0 76.0 76.0 76.0 64.3 81.0 71.7 51.7 60.0 55.6 85.2 75.0 79.8KOR 78.3 83.0 80.6 65.0 80.0 71.7 68.1 64.0 66.0 55.1 49.0 51.9 78.8 82.0 80.4SPA 72.3 68.0 70.1 90.9 50.0 64.5 65.4 68.0 66.7 58.5 38.0 46.1 74.5 70.0 72.2TEL 68.4 80.0 73.7 66.9 83.0 74.1 68.2 75.0 71.4 53.4 71.0 60.9 69.2 81.0 74.7TUR 77.9 81.0 79.4 84.0 63.0 72.0 83.3 55.0 66.3 69.5 66.0 67.7 81.8 81.0 81.4Overall 78.3 73.0 72.0 61.8 79.6Table 2: L1 identification accuracy in development dataPOS bigrams, Universal POS bigrams, and Univer-sal POS trigrams were 1275, 1386, 144, and 1602respectively, some fields in the table are blank.A trivial baseline for this task is to classify all theinstances to a single class, which gives 9.09% ac-curacy.
The table above shows that the results ob-tained in all cases is better than the baseline.
In fivecases, better results were obtained when using thetop 3000 or 6000 features compared to other featurecounts.
In the case of the character trigram featureset, though the result using top 3000 features is bet-ter than the others, the difference is very small com-pared to the experiment using top 6000 features.
Theaccuracy obtained by using top 3000 features in PTBPOS tags is 6% higher than that with top 6000 fea-tures.
In case of Universal POS tags trigrams, betterresults were obtained with top 1000 features.Results show that bigram and trigram feature setsof words give higher accuracy compared to bigramsand trigrams of characters and POS tags.
Comparingthe results of n-grams of two different POS tagsets,the results obtained when using the PTB tagset arebetter than those when using the Universal tagsets.In the case of character, PTB POS tag, and Univer-sal POS tag bigram feature sets, the overall accu-racy is less than 30%.
Based on these results, we de-cided to use the following sets of features: trigramsof characters and POS tags (PTB and Universal) andbigrams of words in our experiments below.6 Final EvaluationWe submitted five runs for the task based on fiveclassifiers.
We named the experiments based on thefeatures used and the approaches used for feature se-lection.
Details about the experiments and their re-sults are described below.1.
Exp-W2,3PTB3C3: In this experiment, weused bigrams at the word level, and trigrams atthe word, character level, as well as PTB POStag trigrams as feature sets.
We selected thesefeature sets based on the accuracies obtainedin the experiments described in Section 5.
Wetried to use a consistent number of features ineach feature set.
As seen in Table 1, thoughthe results obtained by using top 3000 and 6000features are better in equal number of cases (2and 2), the difference in accuracies when us-ing 6000 features is higher than that when us-ing 3000 features.
Thus, we decided to use thetop 6000 features in all the four feature sets.2.
Exp-W2,3Univ3C3: The PTB POS tagset con-tains 36 fine grained POS categories while theUniversal POS tagset contains only 12 coarsePOS categories.
In the second experiment, wetried to see how the performance changes whenusing coarse grained Universal POS categoriesinstead of fine grained PTB POS tags.
Thus,we performed the second experiment with thesame settings as the first experiment except weused Universal POS tags instead of PTB POStags.
Since the total number of Universal POS227Exp-W2,3PTB3C3 Exp-W2,3Univ3C3 Exp ClassBased Exp Perplexity Exp EnsembleL1 P R F1 P R F1 P R F1 P R F1 P R F1ARA 74.3 55.0 63.2 90.9 50.0 64.5 67.9 74.0 70.8 54.3 44.0 48.6 79.7 63.0 70.4CHI 76.2 80.0 78.0 65.9 81.0 72.6 74.5 73.0 73.7 69.3 61.0 64.9 80.2 81.0 80.6FRE 86.4 70.0 77.3 75.8 75.0 75.4 90.6 58.0 70.7 54.5 54.0 54.3 85.7 72.0 78.3GRE 83.2 89.0 86.0 79.1 91.0 84.7 82.7 86.0 84.3 65.2 86.0 74.1 87.6 92.0 89.8HIN 63.7 65.0 64.4 64.5 69.0 66.7 59.6 56.0 57.7 60.0 54.0 56.8 67.0 67.0 67.0ITA 62.5 90.0 73.8 70.0 84.0 76.4 61.4 86.0 71.7 52.5 64.0 57.7 62.5 90.0 73.8JPN 85.7 72.0 78.3 67.2 78.0 72.2 62.1 87.0 72.5 52.6 50.0 51.3 81.9 77.0 79.4KOR 75.0 75.0 75.0 60.3 73.0 66.1 68.1 62.0 64.9 52.6 50.0 51.3 72.8 75.0 73.9SPA 60.0 57.0 58.5 81.1 43.0 56.2 57.6 57.0 57.3 55.6 45.0 49.7 67.1 57.0 61.6TEL 75.3 67.0 70.9 70.0 77.0 73.3 71.7 71.0 71.4 66.1 74.0 69.8 73.0 73.0 73.0TUR 66.4 79.0 72.1 79.0 64.0 70.7 80.6 50.0 61.7 61.4 51.0 55.7 72.4 76.0 74.1Accuracy 72.6 71.4 69.1 58.6 74.8Table 3: L1 identification accuracy in test datatrigrams was only 1602, we replaced 6000 PTBPOS trigrams with 1602 Universal POS tri-grams.3.
Exp ClassBased: The difference in this exper-iment from the first one lies in the process offeature selection.
Instead of selecting the top kfeatures from the whole training data, the se-lection was done considering the top m fea-tures for each L1 class present in the trainingdataset, i.e., we first selected the top m featuresfrom each L1 class and combined them for atotal of p where p is greater than or equal tom and k. After a number of experiments per-formed with different combinations of featuresto train the classifier and testing on the develop-ment dataset, we obtained the best result usingcharacter trigrams, PTB POS tag bigrams andtrigrams, and word bigrams feature sets with3000, 1000, 1000, and 6000 features from eachL1 respectively.
This makes the total numberof features in character trigrams, POS tag bi-grams, POS tag trigrams, and word bigrams as3781, 1278, 1475, and 15592 respectively.4.
Exp Perplexity: In this experiment, we usedthe perplexity values as the features that werecomputed from character trigram languagemodels.
Language models define the proba-bility distribution of a sequence of tokens ina given text.
We used perplexity values sincethese have been successfully used in some au-thorship attribution tasks (Sapkota et al 2013).5.
Exp Ensemble: In the fifth experiment, weused an ensemble based approach with ourabove mentioned four different models.
Weallowed each of the four models to have twovotes.
The first vote is a weighted votingschema in which the models were ranked ac-cording to their results in the developmentdataset and the weight for each model wasgiven by wc = 1/rank(c), where rank(c) isthe position of c in the ranked list.
The finaloutput was based on the second vote that useda majority voting schema.
In the second vote,the output of the first voting schema was alsoused along with the output of four models.The results obtained by the above mentioned fiveexperiments on the development and test datasets areshown in Tables 2 and 3 respectively.
The tablesshow that the results obtained in the developmentdataset are better than those in the test dataset forall the approaches.
In both datasets, we achieved thebest results using the ensemble based approach, i.e.79.2% and 74.8% accuracies in the development andtest dataset respectively.
Considering the accuraciesof individual L1s, this approach achieved the high-est accuracy in 10 L1s in the development datasetand in 7 L1s in the test dataset.
Our system has thebest accuracy for German in both development andtest dataset.
The other classes with higher accura-cies in both datasets are French and Chinese.
In bothdatasets, our system had the lowest accuracy for theHindi and Spanish classes.
Arabic and Telugu have228ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TURARA 63 2 1 0 6 8 1 5 6 4 4CHI 2 81 0 1 2 1 5 4 0 0 4FRE 2 0 72 7 1 11 0 0 4 0 3GER 0 2 2 92 1 1 0 0 1 0 1HIN 2 2 0 0 67 2 0 2 3 19 3ITA 0 0 2 2 0 90 0 0 3 0 3JPN 3 3 1 1 0 3 77 9 1 1 1KOR 1 7 1 0 0 0 8 75 4 1 3SPA 1 1 3 0 2 25 1 4 57 0 6TEL 1 0 0 0 21 0 1 0 3 73 1TUR 4 3 2 2 0 3 1 4 3 2 76Table 4: Confusion Matrix3rd and 4th lowest accuracies.Besides the ensemble based approach, the sec-ond best result was obtained by the first experiment(Exp W2,3PTB3C3).
Comparing the overall accura-cies of the first and second (Exp-W2,3Univ3C3) ex-periments, though the difference between them doesnot seem very high in the test dataset, there is a dif-ference of more than 5% in the development dataset.In the test dataset, the second experiment has thebest results among all the approaches for classesItalian and Telugu, and has better results than thefirst experiment for classes Arabic and Hindi.
Thedifference in the approaches used in the first and sec-ond experiments was the use of n-grams of differentPOS tagsets.
The use of coarse grained UniversalPOS tagset features generalizes the information andloses the discriminating features that the fine grainedPTB POS tagset features captures.
For instance, thePTB POS tagset differentiates verbs into six cate-gories while the Universal POS tagset has only onecategory for that grammatical class.
Because of this,the fine grained POS tagset seems better for identify-ing the native languages than using a coarse grainedPOS tagset in most of the cases.
More studies areneeded to analyze the cases where Universal POStagset works better than the fine grained PTB POStagset.The difference in accuracies obtained between thefirst experiment (Exp W2,3PTB3C3) and the thirdexperiment (Exp ClassBased) is more than 6% inthe development dataset and more than 3% in the testdataset.
In the test dataset, the third experiment hasthe highest accuracy for Arabic class and has betteraccuracy than the first experiment for Telugu class.The difference between these approaches was thefeature selection approach used to create the featurevector.
The results show that in most of the cases se-lecting the features from the whole dataset achievesbetter accuracy in identifying native languages com-pared to using the stratified approach of selecting thefeatures from individual classes.
The main reasonbehind using the class based feature selection wasthat we tried to capture some features that are specif-ically present in one class and not in others.
Since allthe texts in our dataset were about one of the eightprompts, and we have a balanced dataset, there wasno benefit of doing the class based feature selectionapproach.The fourth experiment (Exp Perplexity) usingperplexity values as features did not achieve accu-racy comparable to the first three experiments.
Be-cause of the time constraint, we calculated perplex-ity based on only character trigram language mod-els.
Though the result we achieved is not promis-ing, this approach could be an interesting work in fu-ture experiments where we could use other languagemodels or the combination of various language mod-els to compute the perplexity.7 Error AnalysisThe confusion matrix of the results obtained in thetest dataset by using the ensemble based approachis shown in Table 4.
The table shows the Germanclass has the best accuracy with only a small numberof texts of German mispredicted to other languages,while 7 texts of French class are mispredicted asGerman.
The German language is rich in morpohol-ogy and shares a common ancestor with English.
Italso has a different grammatical structure from the229other languages in the task.
The features we usedin our experiments are shallow syntactic and lexicalfeatures, which could discriminate the writing stylesand the structure of the German class texts, thus hav-ing a higher prediction accuracy.The table shows that French, Italian, and Spanishclasses seem to be confused with each other.
Thoughthe misclassification rate of texts in the Italian classis considerably low, a good number of texts in theFrench and Spanish classes are misclassified as Ital-ian.
The highest number of documents mispredictedis from Spanish to Italian, i.e.
25 texts of Span-ish class are mispredicted as Italian.
These threelanguages fall under the same language family i.e.Indo-European/Romance and have a similar gram-matical features.
The grammatical structure is a par-ticular example of the high rate of misclassificationamong these classes.
While English language is verystrict in the order of words (Subject-Verb-Object),Spanish, Italian and French allow more flexibility.For instance, in Spanish, the phrases ?the car red?
(el auto rojo) and ?the red car?
(el rojo auto) areboth correct although the later is a much less com-mon construction.
In this scenario, it is easy to seethat the n-grams of words and POS tags are benefi-cial to distinguish them from English, but these n-grams might be confusing to identify the differencesamong these three languages since the patterns oflanguage transfer might be similar.Though Hindi and Telugu languages do not fallunder the same language family, they are highly con-fused with each other.
After Spanish to Italian, thesecond highest number of misclassified texts is fromTelugu to Hindi.
Similarly, 19 texts from the classHindi are mispredicted as Telugu.
Both of these lan-guages are spoken in India.
Hindi is the Nationaland official language of India, while Telugu is an of-ficial language in some states of India.
Moreover,English is also one of the official languages.
So, itis very likely that the speakers are exposed to thesame English dialect and therefore their languagetransfer patterns might be very similar.
This mighthave caused our approach of lexical and syntacticfeatures to be unable to capture enough informationto identify the differences between the texts of theseclasses.Texts from Arabic class are equally misclassifiedto almost all the other classes, while misclassifica-tion to Arabic do not seem that high.
Texts of theJapanese, Korean, Chinese classes seem to be con-fused with each other, but the confusion does notseem very high thus having a good accuracy rate.8 Conclusion and Future WorkIn this paper, we describe our approaches to Na-tive Language identification for the NLI Shared Task2013.
We present four different models for L1 iden-tification, three of them using various combinationsof n-gram features at the word, character and POStag levels and a fourth one using perplexity values asfeatures.
Results show that all these approaches givea good accuracy in L1 identification.
We achievedthe best result among these by using the combina-tion of character, words, and PTB POS tags.
Fi-nally, we applied an ensemble based approach overthe results of the four different models that gave thehighest overall accuracy of 79.6% and 74.8% in thedevelopment and test dataset respectively.In our approaches, we use simple n-grams and donot consider grammatical errors in L1 identification.We would like to expand our approach by using theerrors such as misspelled words and subject-verb,and noun-number disagreements as features.
More-over, in our current work of using perplexity values,the result seems good but is not promising.
In thisapproach, we used the perplexity values based ononly character trigram language models.
We wouldlike to incorporate other word and character n-gramlanguage models to calculate perplexity values inour future work.AcknowledgementsWe would like to thank the organizers of NLI sharedtask 2013.
We would also like to thank CONACyTfor its partial support of this work under scholarship310473.ReferencesAhmed Abbasi and Hsinchun Chen.
2005.
Applyingauthorship analysis to Arabic web content.
In Pro-ceedings of the 2005 IEEE international conferenceon Intelligence and Security Informatics, ISI?05, pages183?197, Berlin, Heidelberg.
Springer-Verlag.Charles S. Ahn.
2011.
Automatically Detecting Authors?230Native Language.
Master?s thesis, Naval PostgraduateSchool, Monterey, CA.Daniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
TOEFL11: ACorpus of Non-Native English.
Technical report, Ed-ucational Testing Service.Julian Brooke and Graeme Hirst.
2011.
Native languagedetection with ?cheap?
learner corpora.
In Conferenceof Learner Corpus Research (LCR2011), Louvain-la-Neuve, Belgium.
Presses universitaires de Louvain.Koby Crammer and Yoram Singer.
2002.
On the al-gorithmic implementation of multiclass kernel-basedvector machines.
The Journal of Machine LearningResearch, 2:265?292.Dominique Estival, Tanja Gaustad, Son Bao Pham, WillRadford, and Ben Hutchinson.
2007.
Author profilingfor English emails.
In Proceedings of the 10th Con-ference of the Pacific Association for ComputationalLinguistics, pages 263?272, Melbourne, Australia.Scott Jarvis, Yves Bestgen, Scott A. Crossley, Syl-viane Granger, Magali Paquot, Jennifer Thewissen,and Danielle McNamara.
2012.
The Comparativeand Combined Contributions of n-Grams, Coh-MetrixIndices and Error Types in the L1 Classification ofLearner Texts.
In Scott Jarvis and Scott A. Crosley,editors, Approaching Language Transfer through TextClassification, pages 154?177.
Multilingual Matters.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Associ-ation for Computational Linguistics.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.Determining an author?s native language by mining atext for errors.
In Proceedings of the eleventh ACMSIGKDD international conference on Knowledge dis-covery in data mining, pages 624?628, Chicago, IL.ACM.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011.A universal part-of-speech tagset.
arXiv preprintarXiv:1104.2086.Upendra Sapkota, Thamar Solorio, Manuel Montes-yGo?mez, and Paolo Rosso.
2013.
The use of orthogo-nal similarity relations in the prediction of authorship.In Computational Linguistics and Intelligent Text Pro-cessing, pages 463?475.
Springer.Andreas Stolcke, Jing Zheng, Wen Wang, and VictorAbrash.
2011.
SRILM at sixteen: Update andoutlook.
In Proceedings of IEEE Automatic SpeechRecognition and Understanding Workshop.Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-tin Chodorow.
2012.
Native tongues, lost andfound: Resources and empirical evaluations in nativelanguage identification.
In Proceedings of COLING2012, pages 2585?2602, Mumbai, India, December.The COLING 2012 Organizing Committee.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.A report on the first native language identificationshared task.
In Proceedings of the Eighth Workshopon Building Educational Applications Using NLP, At-lanta, GA, USA, June.
Association for ComputationalLinguistics.Oren Tsur and Ari Rappoport.
2007.
Using classifier fea-tures for studying the effect of native language on thechoice of written second language words.
In Proceed-ings of the Workshop on Cognitive Aspects of Com-putational Language Acquisition, pages 9?16, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Sze-Meng Jojo Wong and Mark Dras.
2009.
ContrastiveAnalysis and Native Language Identification.
In Pro-ceedings of the Australasian Language Technology As-sociation Workshop 2009, pages 53?61, Sydney, Aus-tralia, December.Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.2012.
Exploring Adaptor Grammars for Native Lan-guage Identification.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 699?709, Jeju Island, Korea,July.
Association for Computational Linguistics.231
