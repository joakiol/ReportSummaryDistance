Construction of Structurally Annotated Spoken Dialogue CorpusShingo KatoGraduate School of Information Science,Dept.
of Information Engineering,Nagoya UniversityFuro-cho, Chikusa-ku, Nagoyagotyan@el.itc.nagoya-u.ac.jpShigeki MatsubaraYukiko YamaguchiNobuo KawaguchiInformation Technology Center,Nagoya UniversityFuro-cho, Chikusa-ku, NagoyaAbstractThis paper describes the structural an-notation of a spoken dialogue corpus.By statistically dealing with the corpus,the automatic acquisition of dialogue-structural rules is achieved.
The di-alogue structure is expressed as a bi-nary tree and 789 dialogues consist-ing of 8150 utterances in the CIAIRspeech corpus are annotated.
To eval-uate the scalability of the corpus forcreating dialogue-structural rules, a di-alogue parsing experiment was con-ducted.1 IntroductionWith the improvement of speech processing tech-nologies, spoken dialogue systems that appropri-ately respond to a user?s spontaneous utterancesand cooperatively execute a dialogue are desired.It is important for cooperative spoken dialoguesystems to understand the intentions of a user?sutterances, the purpose of the dialogue, and itsachievement state (Litman, 1990).
To solve thisissue, several approaches have been so far pro-posed.
One of them is an approach in which thesystem expresses the knowledge of the dialoguewith a frame and executes the dialogue accord-ing to that frame (Goddeau, 1996; Niimi, 2001;Oku, 2004).
However, it is difficult to make aframe that totally defines the content of the dia-logue.
Additionally, there is a tendency for thedialogue style to be greatly affected by the frame.Figure 1: The data collection vehicle(DCV)In this paper, we describe the construction ofa structurally annotated spoken dialogue corpus.By statistically dealing with the corpus, we canachieve the automatic acquisition of dialogue-structural rules.
We suppose that the system canfigure out the state of the dialogue through the in-cremental building of the dialogue structure.We use the CIAIR in-car spoken dialogue cor-pus (Kawaguchi, 2004; Kawaguchi, 2005), anddescribe the dialogue structure as a binary tree.The tree expresses the purpose of partial dia-logues and the relations between utterances orpartial dialogues.
The speaker?s intention tagswere provided in the transcription of the corpus.We annotated 789 dialogues consisting of 8150utterances.
Due to the advantages of the dialogue-400022 - 01:37:398-01:41:513 F:D:I:C:(F ) [FILLER:well] &(F )[delicious] & [Udon] & [restaurant] &	<SB> [want to go] & !
"#<SB>0023 - 01:42:368-01:49:961 F:O:I:C:$	 [well] &%& [this area] &'() [near] &*+,!
"-./ [SUWAYA] &"012345 [?CHIKUSAHOUGETSU?
]&*,6789#:;	<<SB> [there are ] &=>?
"#<SB>Figure 2: Transcription of in-car dialogue speechDiscourse act Action ObjectExpress(Exp)Propose(Pro)Request(Req)Statement(Sta)Suggest(Sug)Confirm(Con)Exhibit(Exh)Guide(Gui)ReSearch(ReS)Reserve(Rev)Search(Sea)Select(Sel)ExhibitDetail(ExD)Genre(Gen)IntentDetail(InD)Parking(Par)ParkingInfo(PaI)RequestDetail(ReD)ReserveInfo(ReI)SearchResult(SeR)SelectDetail(SeD)Shop(Sho)ShopInfo(ShI)Figure 3: A part of the LITstructural rules being represented by context freegrammars, we were able to use an existing tech-nique for natural language processing to reducethe annotation burden.In section 2, we explain the CIAIR in-car spo-ken dialogue corpus and the speaker?s intentiontags.
In sections 3 and 4, we discuss the designpolicy of a structurally annotated spoken dialoguecorpus and the construction of the corpus.
In sec-tion 5, we evaluate the corpus.2 Spoken Dialogue Corpus with LayeredIntention TagsThe Center for Integrated Acoustic InformationResearch (CIAIR), Nagoya University, has beencompiling a database of in-car speech and di-alogue since 1999, in order to achieve robustspoken dialogue systems in actual usage envi-ronments (Kawaguchi, 2004; Kawaguchi, 2005)?This corpus has been recorded using more than800 subjects.
Each subject had conversations withthree types of dialogue system: a human operator,the Wizard of OZ system, and the conversationalsystem.In this project, a system was specially built ina Data Collection Vehicle (DCV), shown in Fig-ure 1, and was used for the synchronous recordingof multi-channel audio data, multi-channel videodata, and vehicle related data.
All dialogue datawere transcribed according to transcription stan-dards in compliance with CSJ (Corpus of Spon-taneous Japanese) (Maekawa, 2000) and were as-signed discourse tags such as fillers, hesitations,and slips.
An example of a transcript is shown inFigure 2.
Utterances were divided into utteranceunits by a pause of 200 ms or more.These dialogues are annotated by speech acttags called Layered Intention Tags (LIT) (Irie,2004(a)), which indicate the intentions of thespeaker?s utterances.
LIT consists of four layers:?Discourse act?, ?Action?, ?Object?, and ?Argu-ment?.
Figure 3 shows a part of the organizationof LIT.
As Figure 3 shows, the lower layered in-tention tag depends on the upper layered one.
Inprinciple, one LIT is given to one utterance unit.35,421 utterance units have been tagged by hand(Irie, 2004(a)).In this research, we use parts of the restau-rant guide dialogues between a driver and a hu-man operator.
An example of the dialogue cor-pus with LIT is shown in Table 1.
In the col-umn called Speaker, ?D?
means a driver?s ut-terance and ?O?
means an operator?s one.
Weused the Discourse act, Action, and Object layersand extended them with speaker symbols such as?D+Request+Search+Shop?.
There are 41 typesof extended LIT.
Because the ?Argument?
layeris too detailed to express the dialogue structure,we omitted it.41Table 1: Example of the dialogue corpus with LITUtterance LITNumber Speaker Transcription First layer Second layer Third layer(Discourse Act) (Action) (Object)277 D kono hen de tai ga tabera reru tokoro naikana.Request Search Shop(I?d like to eat some sea bream.
)278 O hai.
Statement Exhibit IntentDetail(Let me see.
)279 O o ryori wa donna o ryouri ga yorosi kattadesuka.Request Select Genre(Which kind do you like?
)280 D nama kei ga ii kana.
Statement Select Genre(Fresh and roe.
)281 D Nabe ga tabe tai desu.
Statement Select Genre(I want to have a Hotpot.
)282 O hai kono tikaku desu to tyankonabe to odenkaiseki ato syabusyabu nado ga gozai masuga.Statement Exhibit SearchResult(Well, there are restaurants near here thatserve sumo wrestler?s stew, Japanese hot-pot, and sliced beef boiled with vegetables.
)283 D oden kaiseki ga ii.
Statement Select Genre(I love Japanese Hotpot.
)284 O hai sou simasu to ?MARU?
to iu omise ninari masu ga.Statement Exhibit SearchResult(?MARU?
restaurant is suitable.
)285 O yorosi katta de syou ka.
Request Exhibit IntentDetail(How about this?
)286 D yoyaku wa hituyou ari masu ka.
Request Exhibit ShopInfo(Should I make a reservation?
)287 O a yoyaku no hou wa yoyoku sare naku temoo mise ni wa hairu koto ga deki masu ga.Statement Exhibit ShopInfo(No, a reservation is not necessary.
)288 D a zya soko made annai onegai si masu.
Request Guide Shop(I see.
Please guide me there.
)289 O kasikomari masi ta.
Statement Exhibit IntentDetail(Sure.
)290 O sore dewa ?MARU?
made go annnai itasimasu.Express Guide Shop(Now, I?m navigating to ?MARU?
)291 D hai.
Statement Exhibit IntentDetail(Thanks.
)3 Description of Dialogue Structure3.1 Dialogue structureIn this research, we assume that the fundamentalunit of a dialogue is an utterance to which one LITis given.
To make the structural analysis of thedialogue more efficient, we express the dialoguestructure as a binary tree.
We defined a categorycalled POD (Part-Of-Dialogue), according to theobservations of the restaurant guide task, that wasespecially focused on what subject was dealt with.As a result of this, 11 types of POD were built(Table 2).
Each node of a structural tree is labeledwith a POD or LIT.
The dialogue structural treeof Table 1 is shown in Figure 4.3.2 The design policy of dialogue structureTo consider a dialogue as an LIT sequence, LITproviding process (Irie, 2004(b)) usually shouldbe done.
Furthermore, repairs and corrections areeliminated because they do not provide LIT.
Inthis research, we used an LIT sequence providedin the corpus.
After that, the annotation of thedialogue structure was done in the following way.Merging utterances: When two adjoining utter-ances such as request and answer, they seemto be able to pair up and merge with an42appropriate POD.
In Table 1, for example,the utterance ?Should I make a reservation??
(#286) is a request and the answer to #286 is?No, a reservation is not necessary?
(#287).In this way, utterances are combined with thePOD ?S INFO?.When the LIT?s of two adjacent utter-ances are corresponding, these utterancesare supposed to be paired and merged withthe same LIT.
Utterance ?Fresh and roe?
(#280) and ?I want to have Hotpot?
(#281)are related to choosing the style of restau-rant and are provided with the same LIT.Therefore they are combined with the LIT?D+Statement+Select+Genre?.Merging partial dialogues: When two adjoin-ing partial dialogues (i.e.
a partial tree) arecomposing another partial dialogue, they aremerged with a proper POD.
In Table 1, forexample, a search dialogue (from #277 to#285, SRCH) and a shop information dia-logue helping search (from #286 to #287,S INFO) are combined and labeled as thePOD ?SLCT?.When the POD?s of two adjacent partial di-alogues are corresponding, these dialoguesare merged with the same POD.
Two searchdialogues (one is from #277 to #282, otheris from #283 to #285) are combined with thesame POD ?SRCH?.The root of the tree: The POD of the root of thetree is ?GUIDE?, because the domain of thecorpus is restaurant guide task.4 Construction of StructurallyAnnotated Spoken Dialogue Corpus4.1 Work environment and proceduresWe made a dialogue parser as a supportive envi-ronment for annotating dialogue structures.Applying the dialogue-structural rules, whichare obtained from annotated structural trees (likeFigure 4.
), the parser analyzes the inputs ofthe LIT sequences and the outputs off all avail-able dialogue-structural trees.
An annotator thenchooses the correct tree from the outputs.
WhenTable 2: Type and substance of POD?sPOD SubstanceGENRE choosing style of cuisine.GUIDE guidance to restaurant or parking.P INFO extracting parking information suchas vacant space, neighborhood.P SRCH searching for a parking space.S INFO extracting shop information such asprice, reservation, menu, area, fixedholiday.SLCT selecting a restaurant or parkingspace.SRCH searching for a restaurant.SRCH RQST requesting a search.RSRV making a reservation.RSRV DTL extracting reservation informationsuch as time, number of people, etc.RSRV RQST requesting a reservation.the outputs don?t include the correct tree, the an-notator should rectify the wrong tree rewriting thelist form of the tree.
In this way, we make the an-notation more efficient.The dialogue parser was implemented using thebottom-up chart parsing (Kay, 1980).
The struc-tural rules were extracted from all annotated di-alogues.
In the environment outlined above, wehave worked at bootstrap building.
That is, we1.
outputed the dialogue structures through theparser.2.
chose and rectified the dialogue structure us-ing an annotator.3.
extracted some structural rules from somedialogue-structural trees.We repeated these procedures and increased thestructural rules incrementally, so that the dialogueparser improved it?s operational performance.4.2 Structurally annotated dialogue corpusWe built a structurally annotated dialogue corpusin the environment described in Section 4.1, us-ing the restaurant guide dialogues in the CIAIRcorpus.
The corpus includes 789 dialogues con-sisting of 8150 utterances.
One dialogue is com-posed of 11.61 utterances.
Table 3 shows them indetail.43       SpeakerNow, I'm navigating to "MARU"Sure.I see.
Please guide me there.No, reservation is not necessary.Should I make a reservation?How about this?"MARU?
restaurant is suitable.I love Japanese Hotpot.Well, there are restaurant near hearthat serve sumo wrestler's stew,Japanese hotchpotch and sliced beefboiled with vegetables.I want to have Hotpot.Fresh and row.Which kind do you like?Let me see.I'd like to eat sea bream.DescriptionD+Sta+Exh+InDD+Req+Sea+ShoO+Sta+Exh+InDO+Req+Sel+GenD+Sta+Sel+GenD+Sta+Sel+GenO+Sta+Exh+SeRD+Sta+Sel+GenO+Sta+Exh+SeRO+Req+Exh+InDD+Req+Exh+ShIO+Sta+Exh+ShID+Req+Gui+ShoO+Sta+Exh+InDO+Exp+Gui+ShoD+Req+Sea+ShoD+Sta+Sel+GenGENRESRCH_RQSTSRCHO+Sta+Exh+SeaSRCHSRCHS_INFOSLCTSLCTO+Exp+Gui+ShoD+Req+Gui+ShoGUIDESLCT  O+Exp+Gui+Sho GENREO+Req+Sel+Gen D+Sta+Sel+GenSLCTSLCT  D+Req+Gui+Sho S_INFOD+Req+Exh+ShI O+Sta+Exh+ShISLCTSRCH  S_INFO D+Sta+Sel+GenD+Sta+Sel+Gen D+Sta+Sel+GenSRCHSRCH  SRCH D+Req+Gui+ShoD+Req+Gui+Sho O+Sta+Exh+InDSRCHSRCH_RQST  O+Sta+Exh+SeR D+Req+Sea+ShoD+Req+Sea+Sho O+Sta+Exh+InDSRCH_RQSTD+Req+Sea+Sho GENRE O+Exp+Gui+ShoO+Exp+Gui+Sho D+Sta+Exh+InDD+Sta+Exh+SeRO+Sta+Exh+SeR O+Re+Exh+InDFigure4:Dialogue-structuraltreeandrulesforTable144Table 3: Corpus statisticsnumber of dialogues 789number of utterances 8150number of structural rules 297utterances per one dialogue 11.61number of dialogue-structural tree types 659number of LIT sequence types 6575 Evaluation of Structurally AnnotatedDialogue CorpusTo evaluate the scalability of the corpus for creat-ing dialogue-structural rules, a dialogue parsingexperiment was conducted.
In the experiment,all 789 dialogues were divided into two data sets.One of them is the test data consists of 100 dia-logues and the other is the training data consistsof 689 dialogues.
Furthermore, the training datawere divided into 10 training sets.By increasing the training data sets, we ex-tracted the probabilistic structural-rules from eachdata.
We then parsed the test data using the rulesand ranked their results by probability.In the evaluation, the coverage rate, the correctrate, and the N-best correct rate were used.Coverage rate      Correct rate     N-best correct rate         Number of the dialogues which can be parsed   Number of the dialogues which include the cor-rect tree in their parse trees   Number of the dialogues which include the cor-rect tree in their n-best parse trees   Number of the dialogues in the test dataThe results of the evaluation of the coveragerate and the correct rate are shown in Figure 5.The correct rates for each of the training sets,ranked from 1-best to 10-best, are shown in Fig-ure 6.In Figure 5, both the coverage rate and the cor-rect rate improved as the training data was in-creased.
The coverage rate of the training set con-sisting of 689 dialogues was 92%.
This means                  	  	   	                 	Figure 5: The relation between the size of training dataand coverage and correct rate.                 	         	    Figure 6: The relation between the size of training dataand the n-best correct rate.that the rules that were from the training set en-abled the parsing of a wide variety dialogues.
Thefact the correct rate was 86% shows that, using therules, the correct structures can be built for a largenumber of dialogues.Three in eight failure dialogues had continuedafter a guidance for a restaurant.
Therefore, weassume that offering guidance to a restaurant is atermination of the dialogue, in which case theycouldn?t be analyzed.
Another three dialoguescouldn?t be analyzed because they included someLIT which rarely appeared in the training data.The cause of failure in the other two dialogues isthat an utterance that should be combined with itsadjoining utterance is abbreviated.Figure 6 shows that the 10-best correct rate for45the training set consisting of 689 dialogues was80%.
Therefore the correct rate is 86%, and ap-proximately 93% (80/86) of the dialogues that canbe correctly analyzed include the correct tree intheir top-10.
According to Figure 5, the numberof average parse trees increased with the growthof the training data.
However, most of the di-alogues that can be analyzed correctly are sup-posed to include the correct tree in their top-10.Therefore, it is enough to refer to the top-10 in asituation where the correct one should be chosenfrom the set of candidates, such as in the speechprediction and the dialogue control.
As a result,the high-speed processing is achieved.6 ConclusionIn this paper, we described the construction ofa structurally annotated spoken dialogue corpus.From observating the restaurant guide dialogues,we designed the policy of the dialogue structureand annotated 789 dialogues consisting of 8150utterances.
Furthermore, we have evaluated thescalability of the corpus for creating dialogue-structural rules.We now introduce the application field of thestructurally annotated dialogue corpus.Discourse analysis: Using a POD labeled infor-mation for each partial structure of the dia-logue, we can obtain information such as thestructure of the domain, the user?s tasks, thedialogue formats, etc.Speech prediction and dialogue control: Asystem builds the structure of an input up todate and extracts the dialogue example thatis most similar to the structure of the inputfrom the corpus.
If the next utterance or LITof the extracted dialogue is the user?s, thesystem waits for the user?s utterance andpredicts its meaning and intention.
If thesystem?s utterance is next, the system usesthe utterance or LIT to control the dialogue.At the present time, we have run up the data of thecorpus and built probabilistic dialogue-structuraltrees.
Next, we will apply the trees to some com-ponents of the spoken dialogue systems such asspeech prediction and dialogue control.AcknowledgmentsThe authors would like to thank Ms. Yuki Irie forher valuable comments about the design of the di-alogue structure.
This research was partially sup-ported by the Grant-in-Aid for Scientific Research(No.
15300045) of JSPS.ReferencesDavid Goddeau, Helen Meng, Joe Poliformi,Stephanie Seneff, and Senis Busayapongchai: Aform-based dialogue manager for spoken languageapplications, Proc.
of ICSLP?96, pp.701-704, 1996.Diane J. Litman and James F. Allen : Discourse Pro-cessing and Commonsense Plans.
Phillip R. Cohen,Jerry Morgan, Martha E. Pollack, editors.
Intentionsin Communication.
pp.365-388, MIT Press, Cam-bridge, MA, 1990.Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, andHitoshi Isahara: Spontaneous speech corpus ofJapanese, LREC-2000, pp.947-952, 2000.Martin Kay: Algorithm Schemata and Data Struc-tures in Syntactic Processing, TR CSL-80-12, Xe-rox PARC, 1980.Nobuo Kawaguchi, Kazuya Takeda, and FumitadaItakura: Multimedia corpus of in-car speech com-munication.
J. VLSI Signal Processing, vol.36,no.2, pp.153-159, 2004.Nobuo Kawaguchi, Shigeki Matsubara, KazuyaTakeda, and Fumitada Itakura: CIAIR In-CarSpeech Corpus -Influence of Driving States-.
IE-ICE Trans.
on Information and System, E88-D(3),pp.578-582, 2005.Tomoki Oku, Takuya Nishimoto, Masahiro Araki,and Yasuhisa Niimi: A Task-Independent ControlMethod for Spoken Dialogs, Systems and Comput-ers in Japan, Vol.35, No.14, 2004.Yasuhisa Niimi, Tomoki Oku, Takuya Nishimoto, andMasahiro Araki: A rule based approach to extrac-tion of topic and dialog acts in a spoken dialog sys-tem, Proc.
of EUROSPEECH2001, vol.3, pp.2185-2188, 2001.46Yuki Irie, Shigeki Matsubara, Nobuo Kawaguchi,Yukiko Yamaguchi, and Yasuyoshi Inagaki: Designand Evaluation of Layered Intention Tag for In-CarSpeech Corpus, Proc.
of the INTERNATIONALSYMPOSIUM ON SPEECH TECHNOLOGYAND PROCESSING SYSTEMS iSTEPS-2004,pp.82-86, 2004.Yuki Irie, Shigeki Matsubara, Nobuo Kawaguchi,Yukiko Yamaguchi, and Yasuyoshi Inagaki: SpeechIntention Understanding based on Decision TreeLearning, Proceedings of 8th International Confer-ence on Spoken Language Processing, Cheju, Ko-rea, 2004.47
