Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 768?772,Dublin, Ireland, August 23-24, 2014.University of Warwick: SENTIADAPTRON - A Domain AdaptableSentiment Analyser for Tweets - Meets SemEvalRichard TownsendUniversity of WarwickRichard.Townsend@warwick.ac.ukAaron KalairUniversity of WarwickAaron.Kalair@warwick.ac.ukOjas KulkarniUniversity of WarwickOjas.Kulkarni@warwick.ac.ukRob ProcterUniversity of WarwickRob.Procter@warwick.ac.ukMaria LiakataUniversity of WarwickM.Liakata@warwick.ac.ukAbstractWe give a brief overview of our system,SentiAdaptron, a domain-sensitive and do-main adaptable system for twitter analy-sis in tweets, and discuss performance onSemEval (in both the constrained and un-constrained scenarios), as well as implica-tions arising from comparing the intra- andinter- domain performance on our twittercorpus.1 IntroductionA domain is broadly defined as a set of documentsdemonstrating a similar distribution of words andlinguistic patterns.
Task 9 of SemEval treats Twit-ter as a single domain with respect to sentimentanalysis.
However previous research has arguedfor the topic-specific treatment of sentiment givendomain-specific nuances and the over-generalityof current sentiment analysis systems with respectto applications in the social sciences (Thelwall andBuckley, 2013).
Thelwal?s method - manually ex-tending a sentiment lexicon for a particular topicor domain - highlights that expression of senti-ment varies from one domain to another.
Ratherthan relying on the manual extension of lexica, wedeveloped an approach to Twitter sentiment clas-sification that is domain sensitive.
To this effectwe gathered tweets from three primary domains -financial news, political opinion, technology com-panies and their products - and trained our systemon one domain while adapting to the other.
Us-ing this methodology we obtained both intrinsicas well as extrinsic evaluation of the system onreal world applications with promising results.
Asour approach to sentiment analysis has been influ-enced by the task description of SemEval 2013 weThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/Positive Negative Neutral ObjectiveSemEval 11610 6332 905 189887Our corpus 10725 17837 3514 36904(a) Contextual polarity.Positive Negative Neutral ObjectiveSemEval 4215 1798 4082 1243Our corpus 1090 1711 1191 -(b) Message polarity.Table 1: The distribution of sentiment classes be-tween SemEval and our corpus at word and tweetlevel.decided to also evaluate the system on SemEval?sdata, since it provides a well established bench-mark.
In the following we briefly describe oursystem and corpus and discuss our approach forthe SemEval submission.2 A Corpus of Three Domains: Source ofUnconstrained DataOur goal in developing SentiAdaptron was domainadaptive tweet-level classification.
We decided tofollow SemEval 2013 and collect both word-levelas well as message level annotations.
We prepareda corpus of 4000 tweets with a balanced cover-age of financial, political and technology relatedtweets.
Tweets were collected using keywordsfrom domain-specific websites: the final list waschosen after evaluating each candidate keyword?spopularity using a third-party service1.
Each tweetis tagged with multiple candidate domains, basedon a hierarchy of terms generated from the originalkeyword list and tweets are filtered using a clus-tering methodology based on the DBSCAN clus-tering algorithm to remove robotic and repetitivetweets.
We performed domain disambiguation forannotation through keyword filtering and also bypicking a number of synsets from WordNet andcomputing the tweet?s mean semantic distance us-ing the NLTK toolkit.
Tweets which didn?t contain1http://topsy.com768any words associated with a score of less than 0.3in SentiWordNet were removed, in a manner sim-ilar to Nakov et al (2013).
After further manualrelevance checks, the remaining tweets were sub-mitted to Amazon?s Mechanical Turk service formessage and phrase-level annotation.
We initiallyused the form demonstrated by Nakov et al., al-though we later redesigned it with a dramatic im-provement in annotator performance and annota-tion quality.
Each tweet was annotated by fourworkers and annotation sparsity at the phrase levelwas addressed by taking the majority of annota-tions following the precedence neutral > pos >neg > other.
This is in contrast to the approachused by Nakov et al.
for SemEval which intersectsannotations.
Annotations at the tweet level wereaggregated using a majority vote.
We found thatusing the proportion of positive, negative and neu-tral words in a tweet is a surprisingly robust fea-ture for cross-domain classification, and boostedour performance when using bigrams.3 Subjectivity Detection and ContextualPolarity Disambiguation (Subtask A)Sentiment lexicons such as SentiWordNet (Esuliand Sebastiani, 2006), the NRC emotions lexicon(Mohammad and Turney, 2010), the MPQA lex-icon (Wilson et al., 2005) and the Bing Liu Lex-icon (Hu and Liu, 2004) have been used for de-termining whether a phrase should be labelled aspositive, negative or neutral within a tweet or sen-tence (contextual polarity).
However, lexical re-sources are by nature non-contextual and may nothave good coverage over a given domain.
We in-stead considered how to infer contextual polaritypurely from the data available.To address the problem of class imbalance inthe tweets, we break the problem of contextual po-larity detection into two stages: (i) we first deter-mine whether a given word should be assigned apositive, negative or neutral annotation (subjectiv-ity detection) and (ii) distinguish subjective tweetsinto positive, negative neutral.3.1 Contextual Subjectivity DetectionTask A asks participants to predict the contextualsubjectivity annotation of a text span at a givenoffset: our extrinsic applications don?t have thisfundamental structure, so we considered whetherit was possible to automatically separate the con-tent of input documents into those regions whichshould be assigned an annotation and those whichshould not.
We considered a unigram and a bigrambaseline using a naive Bayes classifier, which gavean F1 score of 0.640 and 0.520 respectively on ourin domain data (under 10-fold cross-validation).We followed a number of approaches to subjec-tivity detection to try and improve on the baselineincluding sequential modelling using linear-chainConditional Random Fields (CRFs) with CRF-suite (Okazaki, 2007) and lexical inference us-ing semantically disambiguated WordNet (Miller,1995) synsets in conjunction with their occurrencein a subjective context.We found that the observed subjective pro-portion of a given word alongside its successorand predecessor2was a viable feature engineer-ing scheme, which we call neighbouring subjec-tivity proportions.
This gave the best subjectivityperformance on our in-domain data when fed to avoted perceptron (Freund and Schapire, 1999), anensemble approach which assigns particularly pre-dictive iterations of an incrementally trained per-ceptron a greater weight when deciding the finalclassification, and offers wide-margin classifica-tion akin to support vector machines whilst alsorequiring less parameter exploration.
We used theimplementation provided by the WEKA machine-learning environment (Hall et al., 2009), whichachieved an F1-score of 0.740 (again under 10-fold cross validation) for our in-domain data, butperformance dropped to an F1-score of 0.323 onthe SemEval 2013 training and development data.Table 1 indicates that the proportion of objectivefeatures in SemEval is much greater than that seenwithin our own corpus, likely due to the differ-ences in the way we processed annotations (out-lined in Section 2).3.2 Contextual PolarityWe considered a naive Bayes unigram baseline,(similar approaches have proven popular with Se-mEval 2013 participants for Task A) and achievedan F1-measure of 0.662 when training using Se-mEval?s 2014 training and development data andevaluating on SemEval?s 2013 gold-standard an-notations.
However we could not detect the neu-tral class, and the test did not consider the objec-tive class.2As an example, if we observe 14 total occurrences of theword ?heartbreaking?, and 13 of them appear with a positive,negative or neutral label, the subjective proportion computedwould be 13/14.769F1-score Precision RecallP N E Overall P N E P N ETask A (constrained) 85.5 49.0 9.84 67.3 91.1 41.4 3.8 80.6 60.2 16.7Task A (unconstrained) 85.1 49.2 12.2 67.2 89.8 43.4 8.0 80.9 59.8 25.9Finance 78.0 83.1 51.2 78.0 77.3 81.7 58.7 78.7 45.5 84.6Politics 67.3 83.3 42.1 74.3 70.9 79.7 50.3 64.1 87.2 36.3Technology 75.1 85.6 47.4 77.8 77.8 81.8 56.7 89.8 89.8 40.7(a) Performance on word-level contextual annotation tasks.F1-score Precision RecallP N E Overall P N E P N ETask B (constrained) 57.1 34.0 57.0 45.6 46.1 42.6 68.8 75.0 28.3 48.7Task B (unconstrained) 57.2 33.0 57.7 45.1 46.2 36.1 72.1 74.9 30.4 47.9Finance 78.7 78.9 64.6 73.8 77.9 79.5 64.8 78.3 79.5 64.5Politics 80.8 75.0 61.9 72.3 78.4 74.8 63.7 83.3 75.1 60.2Technology 73.2 78.2 58.0 70.1 69.7 76.3 62.9 76.9 80.2 53.7(b) Performance on document-level annotation tasks.Table 2: Classifier metrics obtained from 4-fold intra-domain cross-validation (using reference annota-tions) and results for subtasks A and B of SemEval task 9 (computed using reference scorer).Tech Politics FinanceUnigrams 0.53 0.35 0.53Bigrams 0.38 0.31 0.52Bigrams + SP 0.77 0.65 0.78Unigrams + SP 0.68 0.62 0.76Table 3: F1-scores achieved for each domain onour corpus (naive Bayes, 10-fold cross valida-tion) using reference annotations with and withoutsubjective (positive, negative, neutral) proportions(SP).We improved on this baseline by combining un-igrams with information from the wider context ofthe tweet.
The algorithm first runs subjectivity de-tection on the entire document and then, for eachword we need to classify (or otherwise each worddetected as subjective), effectively generates twobags of words consisting of the subjective wordsbefore and after that word (we also included anyadverbs as annotated by the Gimpel tagger (2011)in this bag to improve robustness).
We output theword itself as a further feature, and use a randomforest classifier (10 trees, log2N + 1 features) togenerate the annotation.
We found this approachoutperformed the other approaches we tried (in-cluding Naive Bayes and OneR) and also gave usbetter F1-scores on the neutral class.
Results fromthis approach for our in-domain data and the Se-mEval 2014 data can be seen in Table 2a.
Thedrop in performance from our in-domain data toSemEval 2014 can be explained by the differentclass distribution observed in SemEval (Table 1).Subjectivity detection was used to generate fea-tures for subtask B, but not subtask A, where thetarget subjective phrases are already given.4 Message Polarity Classification(Subtask B)We tried various different combinations of fea-tures to discover the best intra-domain classifica-tion approach for our corpus and found that theproportion of positive, negative and neutral wordswithin a tweet boosted performance using bigrambinary features (Table 3).
This involves first run-ning the contextual polarity detection componentas described in Section 3 and feeding in the resultsas features (together with bigrams) into a naiveBayes classifier for tweet level sentiment detec-tion.
However, one of our hypotheses was that do-main adaptation could help improve performancewhen moving from one domain to another, effec-tively allowing us to port our classifier from ourown corpus to SemEval.4.1 Cross-Domain AdaptationOur research in domain adaptation uses and ex-tends the technique described by Blitzer andPereira (2007) called Structural CorrespondenceLearning (SCL), which derives a relationship (orcorrespondence) between features from two dif-ferent domains.
This is done via pivot features se-lected from the intersection of features from bothdomains which have been ranked according to mu-tual information.
The technique then uses N pivotfeatures from both the seen and unseen domainsto learn a set of binary problems corresponding towhether a given pivot exists in a target document.A perceptron is then used to train each of the bi-nary problems, giving a matrix of weights (wherea weight represents covariance of non pivot fea-tures with pivot features).
We extended Blitzer?stechnique to encompass the neutral class and gavea wider notion of domain than previously found inthe literature.
As an example Liu et al.
(2013) use770Training domains Test Accuracy Precision Recall F-measureTech & Finance Politics 0.76 0.52 0.40 0.45Tech & Politics Finance 0.69 0.51 0.78 0.61Finance & Politics Tech 0.63 0.53 0.58 0.55Table 4: Binary class metrics with structural correspondence learning on our own corpus.F-measure Precision RecallTrain Test Accuracy(SP)SP Baseline(Bigrams) Loss SP Baseline (Bigrams) Loss SP Baseline (Bigrams) LossTech Politics 69.74% 0.67 0.24 0.43 0.64 0.19 0.45 0.71 0.33 0.38Tech Finance 72.28% 0.78 0.20 0.58 0.74 0.15 0.59 0.83 0.33 0.50Politics Tech 72.40% 0.76 0.21 0.55 0.71 0.25 0.46 0.82 0.33 0.49Politics Finance 73.06% 0.79 0.22 0.57 0.77 0.16 0.61 0.82 0.33 0.49Finance Politics 68.39% 0.66 0.23 0.43 0.67 0.18 0.49 0.66 0.33 0.33Finance Tech 71.92% 0.76 0.22 0.54 0.71 0.16 0.55 0.82 0.33 0.49Tech & Finance Politics 69.17% 0.63 0.23 0.40 0.61 0.18 0.43 0.67 0.33 0.34Tech & Politics Finance 70.49% 0.63 0.20 0.43 0.61 0.15 0.46 0.64 0.33 0.31Finance & Politics Tech 72.88% 0.77 0.22 0.55 0.72 0.16 0.56 0.84 0.33 0.51Table 5: Classifier metrics from training and testing on different domains, with and without proportionsof positive, negative and neutral phrases from the source domain (Subjective Proportions SP).an SVM-derived technique to adapt on domainscontaining terms relevant to Google and Twitter,which are both considered part of the technologydomain in our corpus, whereas we attempted toadapt from technology topics to financial news andpolitical opinions.We found that the amount of mutual informa-tion in our three domains was very low and waspractically zero for the three class version of ourproblem.
The results for the binary version of theclassifier generated poorer results (Table 4) thanthose produced by our back-up classifier (basedon the naive Bayes bigrams and subjective phraseproportions from the source domain, see Table5, last three rows).
Therefore we generated oursubmission to SemEval 2014 based on bigramsand subjective proportions rather than SCL, sincewe found that the proportion of pos/neg/neutralphrases is a robust feature across domains (as longas it can be reliably predicted during the contex-tual polarity prediction stage, which was the casefor our data).
Our results for SemEval task B us-ing the subjective phrase proportions can be foundin Table 2b.
Our unconstrained performance in-dicates that whilst this classifier provides reason-able cross domain performance for our own data(Table 5), it is very sensitive to the performanceof subjectivity and contextual polarity detection,which is lower for SemEval than it is for our owncorpus.
Presumably the reason for this is the dif-ferent assumptions in annotations in the two casesand the differences in the class distributions be-tween SemEval and our own data.
This meant thatour performance was lower than systems that hadspecifically trained on the SemEval data.5 ConclusionsOur goal was to demonstrate the potential of do-main sensitivity and domain adaptability for senti-ment analysis in tweets - a task which brings chal-lenges defying the use of fixed lexica.
We foundthat the proportions of positive, negative and neu-tral tweets are quite robust cross-domain features,although we do think that domain adaptation tech-niques such as Structural Correspondence Learn-ing merit further investigation in the context ofsentiment analysis for Twitter.Access to the source code of thissubmissionThe source code of the applications used togather and prepare our corpus, conduct CRF-suite and structural correspondence learning,and the Java-based environment used to gen-erate our final submission are available athttps://github.com/Sentimentron/Nebraska-public3and https://github.com/Sentimentron/PRJ90814.AcknowledgementsWarwick Research Development Fund grantRD13129 provided funding for crowdsourced an-notations.
We thank our partners at CUSP, NYUfor enabling us to use Amazon Mechanical Turkfor this process.3http://dx.doi.org/10.5281/zenodo.99064http://dx.doi.org/10.5281/zenodo.9904771ReferencesMartin Ester, Hans-Peter Kriegel, J?org Sander, and Xi-aowei Xu.
1996.
A density-based algorithm fordiscovering clusters in large spatial databases withnoise.
In KDD, volume 96, pages 226?231.Andrea Esuli and Fabrizio Sebastiani.
2006.
Senti-WordNet: A publicly available lexical resource foropinion mining.
In Proceedings of LREC, volume 6,pages 417?422.Yoav Freund and Robert E Schapire.
1999.
Largemargin classification using the perceptron algorithm.Machine learning, 37(3):277?296.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor Twitter: annotation, features, and experiments.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies: short papers - Volume 2,HLT ?11, pages 42?47.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The WEKA data mining software: an update.ACM SIGKDD explorations newsletter, 11(1):10?18.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 168?177.Mark Dredze John Blitzer and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InProceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 440?447.Shenghua Liu, Fuxin Li, Fangtao Li, Xueqi Cheng, andHuawei Shen.
2013.
Adaptive co-training SVM forsentiment classification on tweets.
In Proceedingsof the 22nd ACM international conference on Con-ference on information & knowledge management,pages 2079?2088.George A Miller.
1995.
WordNet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Saif M Mohammad and Peter D Turney.
2010.
Emo-tions evoked by common words and phrases: UsingMechanical Turk to create an emotion lexicon.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, pages 26?34.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
SemEval-2013 Task 2: Sentiment Analysis inTwitter.
pages 312?320, June.Naoaki Okazaki.
2007.
CRFsuite: a fastimplementation of Conditional Random Fields(CRFs).
[ONLINE] http://www.chokkan.org/software/crfsuite/ (Retrieved: July16, 2014).Mike Thelwall and Kevan Buckley.
2013.
Topic-basedsentiment analysis for the social web: The role ofmood and issue-related words.
Journal of the Amer-ican Society for Information Science and Technol-ogy, 64(8):1608?1617.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi,Claire Cardie, Ellen Riloff, and Siddharth Patward-han.
2005.
OpinionFinder: A system for subjec-tivity analysis.
In Proceedings of HLT/EMNLP onInteractive Demonstrations, pages 34?35.772
