Which Performs Better on In-Vocabulary Word Segmentation:Based on Word or Character?Zhenxing Wang1,2, Changning Huang2 and Jingbo Zhu11 Institute of Computer Software and Theory, Northeastern University,Shenyang, China, 1100042 Microsoft Research Asia, 49, Zhichun Road,Haidian District, Beijing, China, 100080zxwang@ics.neu.edu.cnv-cnh@microsoft.comzhujingbo@mail.neu.edu.cnAbstract*Since the first Chinese Word Segmenta-tion (CWS) Bakeoff on 2003, CWS hasexperienced a prominent flourish be-cause Bakeoff provides a platform forthe participants, which helps them rec-ognize the merits and drawbacks of theirsegmenters.
However, the evaluationmetric of bakeoff is not sufficientenough to measure the performance tho-roughly, sometimes even misleading.One typical example caused by this in-sufficiency is that there is a popular be-lief existing in the research field thatsegmentation based on word can yield abetter result than character-based tag-ging (CT) on in-vocabulary (IV) wordsegmentation even within closed tests ofBakeoff.
Many efforts were paid to bal-ance the performance on IV and out-of-vocabulary (OOV) words by combiningthese two methods according to this be-lief.
In this paper, we provide a more de-tailed evaluation metric of IV and OOVwords than Bakeoff to analyze CT me-thod and combination method, which isa typical way to seek such balance.
Ourevaluation metric shows that CT outper-forms dictionary-based (or so calledword-based in general) segmentation onboth IV and OOV words within Bakeoff* The work is done when the first author is workingin MSRA as an intern.closed tests.
Furthermore, our analysisshows that using confidence measure tocombine the two segmentation resultsshould be under certain limitation.1 IntroductionChinese Word Segmentation (CWS) has beenwitnessed a prominent progress in the last threeBakeoffs (Sproat and Emerson, 2003), (Emer-son, 2005), (Levow, 2006).
One of the reasonsfor this progress is that Bakeoff provides stan-dard corpora and objective metric, which makesthe result of each system comparable.
Throughthose evaluations researchers can recognize theadvantage and disadvantage of their methodsand improve their systems accordingly.
Howev-er, in the evaluation metric of Bakeoff, only theoverall F measure, precision, recall, IV (in-vocabulary) recall and OOV (out-of-vocabulary)recall are included and such a metric is not suffi-cient to give a completely measure on the per-formance, especially when the performance onIV and OOV word segmentation need to be eva-luated.
An important issue is that segmentationbased on which, word or character, can yield thebetter performance on IV words.
We give a de-tailed explanation about this issue as following.Since CWS was firstly treated as a character-based tagging task (we call it ?CT?
for short he-reafter) in (Xue and Converse, 2002), this me-thod has been widely accepted and further de-veloped by researchers (Peng et al, 2004),(Tseng et al, 2005), (Low et al, 2005), (Zhao etal., 2006).
Relatively to dictionary-based61Sixth SIGHAN Workshop on Chinese Language Processingsegmentation (we call it ?DS?
for short hereaf-ter), CT method can achieve a higher accuracyon OOV word recognition and a better perfor-mance of segmentation in whole.
Thus, CT hasdrawn more and more attention and became thedominant method in the Bakeoff 2005 and 2006.Although CT has shown its merits in wordsegmentation task, some researchers still holdthe belief that on IV words DS can perform bet-ter than CT even in the restriction of Bakeoffclosed test.
Consequently, many strategies areproposed to balance the IV and OOV perfor-mance (Goh et al, 2005), (Zhang et al, 2006a).Among these strategies, the confidence measureused to combine the results of CT and DS is astraight-forward one, which is introduced in(Zhang et al, 2006a).
The basic assumption ofsuch combination is that DS method performsbetter on IV words and Zhang derives this belieffrom the fact that DS achieves higher IV recallrate as Table 1 shows.
In which AS, CityU,MSRA and PKU are four corpora used in Ba-keoff 2005 (also see Table 2 for detail).
We pro-vide a more detailed evaluation metric to ana-lyze these two methods, including precision andF measure of IV and OOV respectively and ourexperiments show that CT outperforms DS onboth IV and OOV words within Bakeoff closedtest.
The precision and F measure are existingmetrics and the definitions of them are clear.Here we just employ them to evaluate segmenta-tion results.
Furthermore, our error analysis onthe results of combination reveals that confi-dence measure in (Zhang et al, 2006a) has arepresentation flaw and we propose an EIV tagmethod to revise it.
Finally, we give an empiri-cal comparison between existing pure CT me-thod and combination, which shows that pureCT method can produce state-of-the-art resultson both IV word and overall segmentation.CorpusRIV ROOVDS CT DS CTAS 0.982 0.967 0.038 0.647CityU 0.989 0.967 0.164 0.736MSRA 0.993 0.972 0.048 0.716PKU 0.981 0.955 0.408 0.754Table 1 IV and OOV recall in(Zhang et al,   2006a)The rest of this paper is organized as fol-lows.
In Section 2, we give a brief introductionto Zhang?s DS method and subword-based tag-ging, which is a special CT method.
And bycomparing the results of this special CT methodand DS according our detailed metric, we showthat CT performs better on both IV and OOV.We review in Section 3 how confidence measureworks and indicate its representation flaw.
Fur-thermore, an ?EIV?
tag method is proposed torevise the confidence measure.
In Section 4, theexperimental results of existing pure CT methodare demonstrated to compare with combinationresult, based on which we discuss the relatedwork.
In Section 5, we conclude the contribu-tions of this paper and discuss the future work.2 Comparison between DS and CTBased on Detailed MetricWe proposed a detailed evaluation metric for IVand OOV word identification in this section andexperiments based on the new metric show thatCT outperforms DS not only on OOV words butalso on IV words with F-measure of IV.
All theexperiments in this paper conform to theconstraints of closed test in Bakeoff 2005(Emerson, 2005).
It means that any resourcebeyond the training corpus is excluded.
We firstreview how DS and CT work and then presentour evaluation metric and experiment results.There is one thing should be emphasized, bycomparing DS and CT result we just want toverify that our new metric can show theperformance on IV words more objectively.Since either DS or CT implementation hasspecific setting here we should not extend thecomparison result to a general sense betweenthose generative models and discriminativemodels.2.1 Dictionary-based segmentationFor the dictionary-based word segmentation, wecollect a dictionary from training corpus first.Instead of Maximum Match, trigram languagemodel2 trained on training corpus is employedfor disambiguation.
During the disambiguationprocedure, a beam search decoder is used to seekthe most possible segmentation.
Since the settingin our paper is consistent with the closed test of2 Language model used in this paper is SLRIM fromhttp://www.speech.sri.com/projects/srilm/62Sixth SIGHAN Workshop on Chinese Language ProcessingBakeoff, we can only use the information welearn from training corpus though other openresources may be helpful to improve the perfor-mance further.
For detail, the decoder reads cha-racters from the input sentence one at a time,and generates candidate segmentations incre-mentally.
At each stage, the next incoming cha-racter is combined with an existing candidate intwo different ways to generate new candidates: itis either appended to the last word in the candi-date, or taken as the start of a new word.
Thismethod guarantees exhaustive generation ofpossible segmentations for any input sentence.However, the exponential time and space of thelength of the input sentence are needed for sucha search and it is always intractable in practice.Thus, we use the trigram language model to se-lect top B (B is a constant predefined beforesearch and in our experiment 3 is used) bestcandidates with highest probability at each stageso that the search algorithm can work in practice.Finally, when the whole sentence has been read,the best candidate with the highest probabilitywill be selected as the segmentation result.Here, the term ?dictionary-based?
is exactly themethod implemented in (Zhang et al, 2006a), itdoes not mean the generative language model ingeneral.2.2 Character-based taggingUnder CT scheme, each character in one sen-tence is labeled as ?B?
if it is the beginning of aword, ?O?
tag means the current character is asingle-character word, other character is labeledas ?I?.
For example, ????
(whole China)?
islabeled as ??
(whole)/O ?
(central)/B ?
(country)/I?.In (Zhang et al, 2006a), the above CT me-thod is developed as subword-based tagging.First, the most frequent multi-character wordsand all single characters in training corpus arecollected as subwords.
During the subword-based tagging, a subword is viewed as an unitinstead of several separate characters and givenonly one tag.
For example, in subword-basedtagging, ????
(whole China)?
is labeled as ??
(whole)/O ??
(China)/O?, if the word ???
(China)?
is collected as a subword.
As thepreprocessing, both training and test corpora aresegmented by maximum match with subword setas dictionary.
After this preprocessing, everysentence in both training and test corpora be-comes subword sequence.
Finally, the tagger istrained by CRFs approach3 on the training data.Although word information is integrated intothis method, it still works in the scheme of?IOB?
tagging.
Thus, we still call subword-based tagging as a special CT method and in thereminder of this paper ?CT?
means subword-based tagging in Zhang?s paper and ?Pure CT?means CT without subword.2.3 A detailed evaluation metricIn this paper, data provided by Bakeoff 2005 isused in our experiments in order to comparewith the published results in (Zhang et al,2006a).
The statistics of the corpora for Ba-keoff 2005 are listed in Table 2 (Emerson, 2005).Corpus Encoding#Trainingwords#TestwordsOOVrateAS Big5 5.45M 122K 0.043CityU Big5 1.46M 41K 0.074MSRA GB 2.37M 107K 0.026PKU GB 1.1M 104K 0.058Table 2 Corpora statistics of Bakeoff 2005Evaluation standard is also provided by Ba-keoff, including overall precision, recall, Fmeasure, IV recall and OOV recall (Sproat andEmerson, 2003), (Emerson, 2005).
However,some important metrics, such as F measure andprecision of both IV and OOV words are omit-ted, which are necessary when the performanceof IV or OOV word identification need to bejudged.
Thus, in order to judge the results ofeach experiment, a more detailed evaluationwith precision and F measure of both IV andOOV words included is used.
To calculate theIV and OOV precision and recall, we firstly di-vide words of the segmenter?s output and golddata into IV word and OOV word sets respec-tively with the dictionary collected from thetraining corpus.
Then, for IV and OOV wordsets respectively, the IV (or OOV) recall is theproportion of the correctly segmented IV (orOOV) word tokens to all IV (or OOV) word to-kens in the gold data, and IV (or OOV) precisionis the proportion of the correctly segmented IV3 CRF tagger in this paper  is implemented by CRF++downloaded from http://crfpp.sourceforge.net/63Sixth SIGHAN Workshop on Chinese Language Processing(or OOV) word tokens to all IV (or OOV) wordtokens in the segmenter?s output.
One thing haveto be emphasized is that the single character intest corpus will be defined as OOV if it does notappear in training corpus.
We will see later inthis section, by this evaluation, some facts cov-ered by the bakeoff evaluation can be illustratedby our new evaluation metric.Here, we repeat two experiments described in(Zhang et al, 2006a), namely dictionary-basedapproach and subword-based tagging.
For CTmethod, top 2000 most frequent multi-characterwords and all single characters in training corpusare selected as subwords and the feature tem-plates used for CRF model is listed in Table 3.We present all the segmentation results in Table6 to see the strength and weakness of each me-thod conveniently.Based on IV and OOV recall as we show inTable 1, Zhang argues that the DS performs bet-ter on IV word identification while CT performsbetter on OOV words.
But we can see from theresults in Table 6 (the lines about DS and CT),the IV precision of DS approach is much lowerthan that of CT on all the four corpora, whichalso causes a lower F measure of IV.
The reasonfor low IV precision of DS is that many OOVwords are segmented into two IV words by DS.For example, OOV word ????(choral)?
issegmented into???
(sing) ?(class)?
by DS.These wrongly identified IV words increase thenumber of all IV words in the segmenter?s out-put and cause the low IV precision of the DSresult.
Since the F measure of IV is a more rea-sonable metric of performance of IV than IVrecall only, Table 6 shows that CT method out-performs the DS on IV word segmentation overall four corpora.
The comparison also shows thatCT outperforms the DS on OOV and overallsegmentation as well.Type Feature FunctionUnigram C-2, C-1, C0, C1, C2 Previous two, current and next two subwordBigram C-2 C-1, C-1 C0, C0 C1, C1 C2 Two adjacent subwordsJump C-1 C1 Previous character and next subwordsTable 3 Feature templates used for CRF in our experiments3 Balance between IV and OOV Per-formanceThere are other strategies such as (Goh et al,2005) trying to seek balance between IV andOOV performance.
In (Goh et al 2005), infor-mation in a dictionary is used in a statisticalmodel.
In this way, the dictionary-based ap-proach and the statistical model are combined.We choose the confidence measure to study be-cause it is straight-forward.
We show in this sec-tion that there is a representation flaw in theformula of confidence measure in (Zhang et al,2006a).
And we propose an ?EIV?
tag method tosolve this problem.
Our experiments show thatconfidence measure with EIV tag outperformsCT and DS alone.3.1 Confidence measureConfidence Measure (CM) means to seek anoptimal tradeoff between performance on IV andOOV words.
The basic idea of CM comes fromthe belief that CT performs better on OOVwords while DS performs better on IV words.When both results of CT and DS are available,the CM can be calculated according to the fol-lowing formula in (Zhang et al, 2006a):ngiobwiobiobiob ttwtCMwt ),()1()|()|(CM ???
??
?Here, w  is a subword, iobt  is ?IOB?
tag givenby CT andwt  is ?IOB?
tag generated by DS.
Inthe first term of the right hand side of the formu-la, )|( wtCM iobiob  is the marginal probability ofiobt (we call this marginal probability ?MP?
forshort).
And in the second term,ngiobw tt ),(?
is aKronecker delta function, returning 1 if and onlyifwt  and  iobt  are identical, else returning 0.
Butif 1),( ?ngiobw tt?
, there is no requirement of re-placement at all.
While if 0),( ?ngiobw tt?
, wheniobw tt ?
, CM depends on the first term of itsright hand side only and ?
is unnecessary to beset as a weight.
Finally, ?
in the formula is aweight to seek balance between CT tag and DStag.
Another parameter here is a threshold t  forthe CM.
If CM is less than t , wt  replaces iobt as64Sixth SIGHAN Workshop on Chinese Language Processingthe final tag, otherwiseiobt will be remained asthe final tag.
However, two parameters in theCM, namely ?
and t , are unnecessary, becausewhen MP is greater than or equal to ?/t , iobtwill be kept, otherwise it will be replaced withwt .
Thus, the CM ultimately is the marginalprobability of the ?IOB?
tag (MP).
In the expe-riment of this paper, MP is used as CM becauseit is equivalent to Zhang?s CM but more conve-nient to express.3.2 Experiments and error analysis aboutcombinationWe repeat the experiments about CM in Zhang?spaper (Zhang et al, 2006a) and show that thereis a representation flaw in the CM formula.
Fur-thermore, we propose an EIV tag method tomake CM yield a better result.In this paper, ?
= 0.8 and t = 0.7 (Parametersin two papers, Zhang et al 2006a and Zhang etal.
2006b, are different.
And our parameters areconsistent with Zhang et al 2006b which is con-firmed by Dr Zhang through email) are used inCM, namely MP= 0.875 is the threshold.
Here,in Table 4, we provide some statistics on theresults of CT when MP is less than 0.875.
FromTable 4 we can see that even with MP less than0.875, most of the subwords are still tagged cor-rectly by CT and should not be revised by DSresult.
Besides, lots of the subwords with lowMP contained by OOV words in test data, espe-cially for the corpus whose OOV rate is high(i.e.
on CityU corpus more than one third sub-words with low MP belong to OOV word) andperformance on OOV recognition is the advan-tage of CT rather than that of DS approach.
Thuswhen combining the results of the two methods,it is theiobt should be maintained if the subwordis contained by an OOV word.
Therefore, theCM formula seems somewhat unreasonable.The error analysis about how many originalerrors are eliminated and how many new errorsare introduced by CM is provided in Table 5 (thecolumns about CM).
Table 5 illustrates that, af-ter combining the two results, most original er-rors on IV words are corrected because DS canachieve higher IV recall as described in Zhang?spaper.
But on OOV part, more new errors areintroduced by CM and these new errors decreasethe precision of the IV words.
For example, theOOV words ?????
(guard member)?
and ????
(design fee)?
is recognized correctly byCT but with low CM.
In the combining proce-dure, these words are wrongly split as IV errors:???
(guard) ??
(member)?
and ???
(de-sign) ?
(fee)?.
Thus, for two corpora (i.e.CityU and AS), F measure of IV and overall Fmeasure decreases since there are more new er-rors introduced than original ones eliminatedand only on the other two corpora (MSRA andPKU), overall F measure of combination methodis higher than CT alone, which is shown in Ta-ble 6 by the lines about combination.3.3 EIV tag methodSince combining the two results by CM mayproduce an even worse performance in somecase, it is worthy to study how to use this CM toget an enhanced result.
Intuitively, if we canchange only the CT tags of the subwords whichcontained in IV word while keep the CT tags ofthose contained in OOV words unchanged, wewill improve the final result according to ourerror analysis in Table 5.
Unfortunately, onlyfrom the test data, we can get the informationwhether a subword contained in an IV word, justas what we do to get Table 4.
However, we canget an approximate estimation from DS result.When using subwords to re-segment DS result4,all the fractions re-segmented out of multiple-character words, including both multiple-character words and single characters, will begiven an ?EIV?
tag, which means that the cur-rent multiple-character word or single characteris contained in an IV word with high probability.For example, ?????
(human resource)?
inDS result is a whole word.
However, only ???(resource)?
belongs to the subword set, so dur-ing the re-segmentation ?????
(human re-source)?
will be re-segmented as ??
(people) ?
(force) ??
(resource)?.
All these three frac-tions will be labeled with an ?EIV?
tag respec-tively.
It is reasonable because all the multiple-character words in the DS result can match anIV word.
After this procedure, when combining4 For the detail, please refer to (Zhang et al, 2006a).65Sixth SIGHAN Workshop on Chinese Language ProcessingCorpus AS CityU MSRA PKU# subword tokens belong to IV 10010 4404 9552 9619# subword tokens belong to IV and tagged correctly by CT 7452 3434 7452 7213# subword tokens belong to IV and tagged wrongly by CT 2558 970 2100 2406# subword tokens belong to OOV  5924 2524 2685 3580# subword tokens belong to OOV and tagged correctly by CT 3177 1656 1725 2208# subword tokens belong to OOV and tagged wrongly by CT 2747 868 960 1372Table 4 Results of CT when MP is less than 0.875Corpus AS CityU MSRA PKUMethod CM EIV CM EIV CM EIV CM EIV#original errors eliminated on IV  1905 1003 904 469 1959 1077 1923 1187#original errors eliminated on OOV 755 75 155 80 104 30 230 76#original errors eliminated totally 2660 1078 1059 549 2063 1107 2153 1263#new errors introduced on IV 441 185 80 50 148 68 211 118#new errors introduced on OOV 2487 77 1320 103 1517 57 1681 58# new errors introduced totally 2928 262 1400 153 1665 125 1892 176Table 5 Error analysis of confidence measure with and without EIV tagthe two results, only the CT tag with EIV tagsand low MP will be replaced by DS tag, other-wise the original CT tag will be maintained.
Un-der this condition the errors introduced by OOVwill not happen and enhanced results are listedin Table 6 lines about EIV.
We can see that onall four corpora the overall F measure of EIVresult is higher than that of CT alone, whichshow that our EIV method works well.
Now,let?s check what changes happened in the num-ber of error tags after EIV condition added intothe CM.
We can see from the Table 5 columnsabout EIV, there are more errors eliminated thanthe new errors introduced after EIV conditionadded into CM and most CT tags of subwordscontained in OOV words maintained unchangedas we supposed.
And then, our results (in Table6 lines about EIV) are comparable with that inZhang?s paper.
Thus, there may be some similarstrategies in Zhang?s CM too but not presentedin Zhang?s paper.4  Discussion and Related WorksAlthough the method such as confidence meas-ure can be helpful at some circumstance, ourexperiment shows that pure character-based tag-ging (pure CT) can work well with reasonablefeatures and tag set.
In (Zhao et al, 2006), anenhanced CRF tag set is proposed to distinguishdifferent positions in the multi-character wordswhen the word length is less than 6.
In this me-thod, feature templates are almost the same asshown in Table 3 with a 3-character window anda 6-tag set {B, B2, B3, M, E, O} is used.
Here,tag B and E stand for the first and the last posi-tion in a multi-character word, respectively.
Sstands up a single-character word.
B2 and B3stand for the second and the third position in amulti-character word, whose length is largerthan two-character or three-character.
M standsfor the fourth or more rear position in a multi-character word, whose length is larger than four-character.In Table 6, the lines about ?pure CT?
providethe results generated by pure CT with 6-tag set.We can see from the Table 6 this pure CT ap-proach achieves the state-of-the-art results on allthe corpora.
On three of the four corpora (AS,MSRA and PKU) this pure CT method gets thebest result.
Even on IV word, this pure CT ap-proach outperforms Zhang?s CT method andproduces comparable results with combinationwith EIV tags, which shows that pure CT me-thod can perform well on IV words too.
Moreo-ver, this character-based tagging approach ismore clear and simple than the confidencemeasure method.Although character-based tagging becamemainstream approach in the last two Bakeoffs, itdoes not mean that word information is valuelessin Chinese word segmentation.
A word-basedperceptron algorithm is proposed recently(Zhang and Clark, 2007), which views Chineseword segmentation task from a new angle in-stead of character-based tagging and gets com-parable results with the best results of Bakeoff.66Sixth SIGHAN Workshop on Chinese Language ProcessingCorpus Method R P F RIV PIV FIV ROOV POOV FOOVAS DS 0.943 0.881 0.911 0.984 0.892 0.935 0.044 0.217 0.076CT 0.954 0.938 0.946 0.967 0.960 0.964 0.666 0.606 0.635Combination 0.958 0.929 0.943 0.980 0.945 0.962 0.487 0.593 0.535EIV tag 0.960 0.942 0.951 0.973 0.962 0.968 0.667 0.624 0.645Pure CT 0.958 0.947 0.953 0.971 0.963 0.967 0.682 0.618 0.648CityU DS 0.928  0.848 0.886 0.989 0.865 0.923 0.162 0.353 0.223CT 0.947 0.940 0.944 0.963 0.964 0.964 0.739 0.717 0.728Combination 0.954 0.922 0.938 0.984 0.938 0.961 0.581 0.693 0.632EIV tag 0.953 0.949 0.951 0.970 0.968 0.969 0.744 0.750 0.747Pure CT 0.947 0.948 0.948 0.967 0.973 0.970 0.692 0.660 0.676MSRA DS 0.969 0.927 0.947 0.994 0.930 0.961 0.036 0.358 0.066CT 0.963 0.964 0.963 0.970 0.979 0.975 0.698 0.662 0.680Combination 0.977 0.961 0.969 0.990 0.970 0.980 0.511 0.653 0.574EIV tag 0.972 0.970 0.971 0.980 0.982 0.981 0.696 0.679 0.688Pure CT 0.972  0.975 0.973 0.978 0.986 0.982 0.750 0.632 0.686PKU DS 0.948 0.911 0.929 0.981 0.920 0.950 0.403 0.711 0.515CT 0.944 0.945 0.945 0.955 0.966 0.961 0.763 0.727 0.745Combination 0.955 0.942 0.949 0.973 0.953 0.963 0.664 0.782 0.718EIV tag 0.950 0.952 0.951 0.961 0.970 0.966 0.768 0.753 0.760Pure CT 0.946 0.957 0.951 0.956 0.973 0.964 0.672 0.580 0.623Table 6 Results of different approach used in our experiments (White background lines arethe results we repeat Zhang?s methods and they have some trivial difference with Table 1.
)Therefore, the most important thing worth to payattention in future study is how to integrate lin-guistic information into the statistical model effec-tively, no matter character or word information.5 Conclusions and Future WorkIn this paper, we first provided a detailed evalua-tion metric, which provides the necessary infor-mation to judge the performance of each methodon IV and OOV word identification.
Second, bythis evaluation metric, we show that character-based tagging outperforms dictionary-based seg-mentation not only on OOV words but also on IVwords within Bakeoff closed tests.
Furthermore,our experiments show that confidence measure inZhang?s paper has a representation flaw and wepropose an EIV tag method to revise the combina-tion.
Finally, our experiments show that pure cha-racter-based approach also can achieve good IVword and overall performance.
Perhaps, there aretwo reasons that existing combination resultsdon?t outperform the pure CT. One is that mostinformation contained in statistic language modelis already captured by the CT feature templates inCRF framework.
The other is that confidencemeasure may not be the effective way to combinethe DS and CT results.In the future work, our research will focus onhow to integrate word information into CRF fea-tures rather than using it to modify the results ofCRF tagging.
In this way, we can capture theword information meanwhile avoid destroying theoptimal output of CRF tagging.AcknowledgementThe authors appreciate Dr. Hai Zhao in City Uni-versity of Hong Kong and Dr. Ruiqiang Zhang inSpoken Language Communications Lab, ATR,Japan providing a lot of help for this paper.
Thankthose reviewers who gave the valuable commentto improve this paper.ReferencesThomas Emerson.
2005.
The Second International Chi-nese Word Segmentation Bakeoff.
In Proceedings of theFourth SIGHAN Workshop on Chinese LanguageProcessing, pages 123-133, Jeju Island, Korea:Chooi-Ling Goh, Masayuku Asahara and Yuji Matsu-moto.
2005.
Chinese Word Segmentatin by Classifi-cation of Characters.
Computational Linguistics and67Sixth SIGHAN Workshop on Chinese Language ProcessingChinese Language Processing, Vol.
10(3): pages381-396.Gina-Anne Levow.
2006.
The Third International Chi-nese Language Processing Bakeoff: Word Segmen-tation and Named Entity Recognition.
In  Proceed-ings of the Fifth SIGHAN Workshop on ChineseLanguage Processing , pages 108-117, Sydney: Ju-ly.Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.
2005.A Maximum Entropy Approach to Chinese WordSegmentation.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing, pages 161-164, Jeju Island, Korea.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detectionusing conditional random fields.
In COLING 2004,pages 562?568.
Geneva, Switzerland.Richard Sproat and Thomas Emerson.
2003.
The FirstInternational Chinese Word Segmentation Bakeoff.
InProceedings of the Second SIGHAN Workshop on Chi-nese Language Processing, pages 133-143, Sapporo, Ja-pan: July 11-12,Huihsin Tseng, Pichuan Chang et al 2005.
A ConditionalRandom Field Word Segmenter for SIGHAN Bakeoff2005.
In Proceedings of the Fourth SIGHAN Workshopon Chinese Language Processing, pages 168-171, JejuIsland, Korea.Neinwen Xue and Susan P. Converse.
2002.
Combin-ing Classifiers for Chinese Word Segmentation.
InProceedings of the First SIGHAN Workshop onChinese Language Processing, pages 63-70, Taipei,Taiwan.Ruiqiang Zhang, Genichiro Kikui and Eiichiro Sumita.2006a.
Subword-based Tagging by ConditionalRandom Fields for Chinese Word Segmentation.
InProceedings of the Human Language TechnologyConference of the NAACL, Companion volume, pag-es 193-196.
New York, USA.Ruiqiang Zhang, Genichiro Kikui and Eiichiro Sumita.2006b.
Subword-based Tagging for Confidence-dependent Chinese Word Segmentaion.
In Proceed-ings of the COLING/ACL, Main Conference PosterSessions, pages 961-968.
Sydney, Australia.Yue Zhang and Stephen Clark.
2007.
Chinese Segmenta-tion with a Word-Based Perceptron Algorithm.
InProceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics, pages 840-847.
Prague, Czech Republic.Hai Zhao, Changning Huang et al 2006.
Effective TagSet Selection in Chinese Word Segmentation viaConditional Random Field Modeling.
In Proceed-ings of PACLIC-20.
pages 87-94.
Wuhan, China,Novemeber.68Sixth SIGHAN Workshop on Chinese Language Processing
