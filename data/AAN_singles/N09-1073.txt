Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 647?655,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsLinear Complexity Context-Free Parsing Pipelines via Chart ConstraintsBrian Roark and Kristy HollingsheadCenter for Spoken Language UnderstandingDivision of Biomedical Computer ScienceOregon Health & Science University{roark,hollingk}@cslu.ogi.eduAbstractIn this paper, we extend methods from Roarkand Hollingshead (2008) for reducing theworst-case complexity of a context-free pars-ing pipeline via hard constraints derived fromfinite-state tagging pre-processing.
Methodsfrom our previous paper achieved quadraticworst-case complexity.
We prove here that al-ternate methods for choosing constraints canachieve either linear orO(N log2N) complex-ity.
These worst-case bounds on processingare demonstrated to be achieved without re-ducing the parsing accuracy, in fact in somecases improving the accuracy.
The new meth-ods achieve observed performance compara-ble to the previously published quadratic com-plexity method.
Finally, we demonstrate im-proved performance by combining complexitybounding methods with additional high preci-sion constraints.1 IntroductionFinite-state pre-processing for context-free parsingis very common as a means of reducing the amountof search required in the later stage.
For ex-ample, the well-known Ratnaparkhi parser (Ratna-parkhi, 1999) used a finite-state POS-tagger and NP-chunker to reduce the search space for his Maxi-mum Entropy parsing model, and achieved linearobserved-time performance.
Other recent examplesof the utility of finite-state constraints for parsingpipelines include Glaysher and Moldovan (2006),Djordjevic et al (2007), Hollingshead and Roark(2007), and Roark and Hollingshead (2008).
Notethat by making use of constraints derived from pre-processing, they are no longer performing full exactinference?these are approximate inference meth-ods, as are the methods presented in this paper.
Mostof these parsing pipeline papers show empiricallythat these techniques can improve pipeline efficiencyfor well-known parsing tasks.
In contrast, in Roarkand Hollingshead (2008), we derived and applied thefinite-state constraints so as to guarantee a reduc-tion in the worst-case complexity of the context-freeparsing pipeline from O(N3) in the length of thestring N to O(N2) by closing chart cells to entries.We demonstrated the application of such constraintsto the well-known Charniak parsing pipeline (Char-niak, 2000), which resulted in no accuracy loss whenthe constraints were applied.While it is important to demonstrate that thesesorts of complexity-reducing chart constraints do notinterfere with the operation of high-accuracy, state-of-the-art parsing approaches, existing pruning tech-niques used within such parsers can obscure the im-pact of these constraints on search.
For example, us-ing the default search parameterization of the Char-niak parser, the Roark and Hollingshead (2008) re-sults demonstrated no parser speedup using the tech-niques, rather an accuracy improvement, which weattributed to a better use of the amount of search per-mitted by that default parameterization.
We onlydemonstrated efficiency improvements by reducingthe amount of search via the Charniak search param-eterization.
There we showed a nice speedup of theparser versus the default, while maintaining accu-racy levels.
However, internal heuristics of the Char-niak search, such as attention shifting (Blaheta andCharniak, 1999; Hall and Johnson, 2004), can makethis accuracy/efficiency tradeoff somewhat difficultto interpret.Furthermore, one might ask whetherO(N2) com-plexity is as good as can be achieved through theparadigm of using finite-state constraints to closechart cells.
What methods of constraint would berequired to achieve O(N logN) or linear complex-647ity?
Would such constraints degrade performance,or can the finite-state models be applied with suffi-cient precision to allow for such constraints withoutsignificant loss of accuracy?In this paper, we adopt the same paradigm pur-sued in Roark and Hollingshead (2008), but applyit to an exact inference CYK parser (Cocke andSchwartz, 1970; Younger, 1967; Kasami, 1965).
Wedemonstrate that imposing constraints sufficient toachieve quadratic complexity in fact yields observedlinear parsing time, suggesting that tighter complex-ity bounds are possible.
We prove that a differ-ent method of imposing constraints on words be-ginning or ending multi-word constituents can giveO(N log2N) or O(N) worst-case complexity, andwe empirically evaluate the impact of such an ap-proach.The rest of the paper is structured as follows.
Webegin with a summary of the chart cell constrainttechniques from Roark and Hollingshead (2008),and some initial empirical trials applying these tech-niques to an exact inference CYK parser.
Complex-ity bounding approaches are contrasted (and com-bined) with high precision constraint selection meth-ods from that paper.
We then present a new approachto making use of the same sort of finite-state tag-ger output to achieve linear or N log2N complexity.This is followed with an empirical validation of thenew approach.2 Background: Chart Cell ConstraintsThe basic algorithm from Roark and Hollingshead(2008) is as follows.
Let B be the set of words in astring w1 .
.
.
wk that begin a multi-word constituent,and let E be the set of words in the string that end amulti-word constituent.
For chart parsing with, say,the CYK algorithm, cells in the chart represent sub-strings wi .
.
.
wj of the string, and can be indexedwith (i, j), the beginning and ending words of thesubstring.
If wi 6?
B, then we can close any cell(i, j) where i < j, i.e., no complete constituentsneed be stored in that cell.
Similarly, if wj 6?
E,then we can close any cell (i, j) where i < j.
A dis-criminatively trained finite-state tagger can be usedto classify words as being in or out of these setswith relatively high tagging accuracy, around 97%for both sets (B and E).
The output of the tagger isthen used to close cells, thus reducing the work forthe chart parser.An important caveat must be made about theseclosed cells, related to incomplete constituents.
Forsimplicity of exposition, we will describe incom-plete constituents in terms of factored categories ina Chomsky Normal Form grammar, e.g., the newnon-terminal Z:X+W that results when the ternaryrule production Z ?
Y X W is factored intothe two binary productions Z ?
Y Z:X+W andZ:X+W ?
X W .
A factored category suchas Z:X+W should be permitted in cell (i, j) ifwj ?
E, even if wi 6?
B, because the category couldsubsequently combine with an Y category to createa Z constituent that begins at some word wp ?
B.Hence there are three possible conditions for cell(i, j) in the chart:1. wj 6?
E: closing the cell affects all con-stituents, both complete and incomplete2.
wi 6?
B and wj ?
E: closing the cell affectsonly complete constituents3.
wi ?
B and wj ?
E: cell is not closed, i.e., itis ?open?In Roark and Hollingshead (2008), we provedthat, for the CYK algorithm, there is no work neces-sary for case 1 cells, a constant amount of work forcase 2 cells, and a linear amount of work for case3 cells.
Therefore, if the number of cells allowedto fall in case 3 is linear, the overall complexity ofsearch is O(N2).The amount of work for each case is relatedto how the CYK algorithm performs its search.Each cell in the chart (i, j) represents a substringwi .
.
.
wj , and building non-terminal categories inthat cell involves combining non-terminal categories(via rules in the context-free grammar) found in cellsof adjacent substrings wi .
.
.
wm and wm+1 .
.
.
wj .The length of substrings can be up to order N(length of the whole string), hence there are O(N)midpoint words wm in the standard algorithm, andin the case 3 cells above.
This accounts for the lin-ear amount of work for those cells.
Case 2 cellshave constant work because there is only one pos-sible midpoint, and that is wi, i.e., the first child ofany incomplete constituent placed in a case 2 cellmust be span 1, since wi 6?
B.
This is a very con-cise recap of the proof, and we refer the reader toour previous paper for more details.6483 Constraining Exact-Inference CYKDespite referring to the CYK algorithm in the proof,in Roark and Hollingshead (2008) we demonstratedour approach by constraining the Charniak parser(Charniak, 2000), and achieved an improvement inthe accuracy/efficiency tradeoff curve.
However, asmentioned earlier, the existing complicated systemof search heuristics in the Charniak parser makes in-terpretation of the results more difficult.
What canbe said from the previous results is that constrainingparsers in this way can improve performance of eventhe highest accuracy parsers.
Yet those results do notprovide much of an indication of how performanceis impacted for general context-free inference.For this paper, we use an exact inference (exhaus-tive search) CYK parser, using a simple probabilis-tic context-free grammar (PCFG) induced from thePenn WSJ Treebank (Marcus et al, 1993).
ThePCFG is transformed to Chomsky Normal Formthrough right-factorization, and is smoothed with aMarkov (order-2) transform.
Thus a production suchas Z ?
Y X W V becomes three rules: (1)Z ?
Y Z:X+W ; (2) Z:X+W ?
X Z:W+V ;and (3) Z:W+V ?
W V .
Note that only two childcategories are encoded within the new factored cate-gories, instead of all of the remaining children as inour previous factorization example.
This so-called?Markov?
grammar provides some smoothing of thePCFG; the resulting grammar is also smoothed us-ing lower order Markov grammars.We trained on sections 2-21 of the treebank, andall results except for the final table are on the devel-opment section (24).
The final table is on the testsection (23).
All results report F-measure labeledbracketing accuracy for all sentences in the section.To close cells, we use a discriminatively trainedfinite-state tagger to tag words as being either in Bor not, and also (in a separate pass) either in E ornot.
Note that the reference tags for each word canbe derived directly from the treebank, based on thespans of constituents beginning (or ending) at eachword.
Note also that these reference tags are basedon a non-factored grammar.For example, consider the chart in Figure 1 for thefive symbol string ?abcde?.
Each cell in the chart islabeled with the substring that the cell spans, alongwith the begin and end indices of the substring, e.g.,(3, 5) spans the third symbol to the fifth symbol:abcde(1, 5)abcd(1, 4)bcde(2, 5)abc(1, 3)bcd(2, 4)cde(3, 5)ab(1, 2)bc(2, 3)cd(3, 4)de(4, 5)a(1, 1)b(2, 2)c(3, 3)d(4, 4)e(5, 5)Figure 1: Fragment of a chart structure.
Each cell is labeledwith the substring spanned by that cell, along with the start andend word indices.
Cell shading reflects b 6?
E and d 6?
Econstraints: black denotes ?closed?
cells; white and gray are?open?
; gray cells have ?closed?
children cells, reducing thenumber of midpoints requiring processing.cde.
If our tagger output is such that b 6?
E andd 6?
E, then four cells will be closed: (1, 2), (1, 4),(2, 4) and (3, 4).
The gray shaded cells in the figurehave some midpoints that require no work, becausethey involve closed children cells.4 Constraint Selection4.1 High Precision vs Complexity BoundingThe chart constraints that are extracted from thefinite-state tagger come in the form of set exclu-sions, e.g., d 6?
E. Rather than selecting constraintsfrom the single, best-scoring tag sequence output bythe tagger, we instead rely on the whole distribu-tion over possible tag strings to select constraints.We have two separate tagging tasks, each with twopossible tags of each word wi in each string: (1) Bor ?B; and (2) E or ?E, where ?X signifies thatwi 6?
X for X ?
{B,E}.
The tagger (Holling-shead et al, 2005) uses log linear models trainedwith the perceptron algorithm, and derives, via theforward-backward algorithm, the posterior probabil-ity of each of the two tags at each word, so thatPr(B) + Pr(?B) = 1.
Then, for every word wiin the string, the tags B and E are associated with aposterior probability that gives us a score forwi ?
Band wi ?
E. All possible set memberships wi ?
Xin the string can be ranked by this score.
From thisranking, a decision boundary can be set, such thatall word/set pairs wi ?
B or wj ?
E with above-threshold probability are accepted, and all pairs be-low threshold are excluded from the set.The default decision boundary for this tagging649task is 0.5 posterior probability (more likely thannot), and tagging performance at that threshold isgood (around 97% accuracy, as mentioned previ-ously).
However, since this is a pre-processing step,we may want to reduce possible cascading errors byallowing more words into the sets B and E. Inother words, we may want more precision in ourset exclusion constraints.
One method for this is tocount the number c of word/set pairs below poste-rior probability of 0.5, then set the threshold so thatonly kc word/set pairs fall below threshold, where0 < k ?
1.
Note that the closer the parameter kis to 0, the fewer constraints will be applied to thechart.
We refer to the resulting constraints as ?highprecision?, since the selected constraints (set exclu-sions) have high precision.
This technique was alsoused in the previous paper.We also make use of the ranked list of word/setpairs to impose quadratic bounds on context-freeparsing.
Starting from the top of the list (high-est posterior probability for set inclusion), word/setpairs are selected and the number of open cells (case3 in Section 2) calculated.
When the accumulatednumber of open cells reaches kN for sentence lengthN , the decision threshold is set.
In such a way, thereare only a linear number of open, case 3 cells, hencethe parsing has quadratic worst-case complexity.For both of these methods, the parameter k canvary, allowing for more or less set inclusion.
Fig-ure 2 shows parse time versus F-measure parse ac-curacy on the development set for the baseline (un-constrained) exact-inference CYK parser, and forvarious parameterizations of both the high preci-sion constraints and the quadratic bound constraints.Note that accuracy actually improves with the im-position of these constraints.
This is not surpris-ing, since the finite-state tagger deriving the con-straints made use of lexical information that the sim-ple PCFG did not, hence there is complementary in-formation improving the model.
The best operatingpoints?fast parsing and relatively high accuracy?are achieved with 90% of the high precision con-straints, and 5N cells left open.
These achieve aroughly 20 times speedup over the baseline uncon-strained parser and achieve between 1.5 and 3 per-cent accuracy gains over the baseline.We can get a better picture of what is going on byconsidering the scatter plots in Figure 3, which plot0 500 1000 1500 2000 2500 3000 350065707580Seconds to parse sectionF?measure accuracyBaseline exact inferenceHigh precision constraintsO(N2) complexity boundsFigure 2: Time to parse (seconds) versus accuracy (F-measure)for the baseline of exact inference (no constraints) versustwo methods of imposing constraints with varying parameters:(1) High precision constraints; (2) Sufficient constraints to im-pose O(N2) complexity (the number of open cells ?
kN ).each sentence according to its length versus the pars-ing time for that sentence at three operating points:baseline (unconstrained); high precision at 90%; andquadratic with 5N open cells.
The top plot shows upto 120 words in the sentence, and up to 5 seconds ofparsing time.
The middle graph zooms in to under1 second and up to 60 words; and the lowest graphzooms in further to under 0.1 seconds and up to 20words.
It can be seen in each graph that the uncon-strained CYK parsing quickly leaves the graph via asteep cubic curve.Three points can be taken away from these plots.First, the high precision constraints are better forthe shorter strings than the quadratic bound con-straints (see bottom plot); yet with the longer strings,the quadratic constraints better control parsing timethan the high precision constraints (see top plot).Second, the quadratic bound constraints appear toactually result in roughly linear parsing time, notquadratic.
Finally, at the ?crossover?
point, wherequadratic constraints start out-performing the highprecision constraints (roughly 40-60 words, see mid-dle plot), there is quite high variance in high preci-sion constraints versus the quadratic bounds: somesentences process more quickly than the quadraticbounds, some quite a bit worse.
This illustratesthe difference between the two methods of select-ing constraints: the high precision constraints canprovide very strong gains, but there is no guaranteefor the worst case.
In such a way, the high preci-sion constraints are similar to other tagging-derived6500 20 40 60 80 100 120012345Sentence length in wordsParsing timein secondsNo constraintsHigh precision constraints (90%)O(N2) parsing (open cells ?
5N)0 10 20 30 40 50 6000.20.40.60.81Sentence length in wordsParsing timein secondsNo constraintsHigh precision constraints (90%)O(N2) parsing (open cells ?
5N)0 5 10 15 2000.020.040.060.080.1Sentence length in wordsParsing timein secondsNo constraintsHigh precision constraints (90%)O(N2) parsing (open cells ?
5N)Figure 3: Scatter plots of sentence length versus parsing timefor (1) baseline exact inference (no constraints); (2) high pre-cision begin- and end-constituent constraints (90% level); and(3) O(N2) constraints (5N open cells).constraints like POS-tags or chunks.4.2 Combining ConstraintsDepending on the length of the string, the quadraticconstraints may close more or fewer chart cellsthan the high precision constraints?more for longstrings, fewer for short strings.
We can achieveF-measure timeConstraints accuracy (seconds)None (baseline CYK) 74.1 3646High Precision (90%) 77.0 181Quadratic (5N ) 75.7 317Quad (5N ) + HiPrec (90%) 76.9 166Table 1: Speed and accuracy of exact-inference CYK parseron WSJ section 24 under various constraint conditions, includ-ing combining quadratic bound constraints and high precisionconstraints.worst-case bounds, along with superior typical casespeedups, by combining both methods as follows:first apply the quadratic bounds; then, if there areany high precision constraints that remain unap-plied, add them.
Table 1 shows F-measure accuracyand parsing time (in seconds) for four trials on thedevelopment set: the baseline CYK with no con-straints; high precision constraints at the 90% level;quadratic bound constraints at the 5N level; and acombination of the quadratic bound and high preci-sion constraints.
We can see that, indeed, the com-bination of the two yield speedups over both inde-pendently, with no significant drop in accuracy fromthe high precision constraints alone.
Further resultswith worst-case complexity bounds will be com-bined with high precision constraints in this way.The observed linear parsing time in Figure 3 withthe quadratic constraints raises the following ques-tion: can we apply these constraints in a way thatguarantees linear complexity?
The answer is yes,and this is the subject of the next section.5 Linear andN log2N Complexity BoundsGiven the two setsB and E, recall the three cases ofchart cells (i, j) presented in Section 2: 1) wj 6?
E(cell completely closed); 2) wj ?
E and wi 6?
B(cell open only for incomplete constituents); and 3)wi ?
B and wj ?
E (cell open for all constituents).Quadratic worst-case complexity is achieved withthese sets by limiting case 3 to hold for only O(N)cells?each with linear work?and the remainingO(N2) cells (cases 1 and 2) have none or constantwork, hence overall quadratic (Roark and Holling-shead, 2008).One might ask: why would imposing constraintsto achieve a quadratic bound give us linear observedparsing time?
One possibility is that the linear num-ber of case 3 cells don?t have a linear amount ofwork, but rather a constant bounded amount of work.651If there were a constant bounded number of mid-points, then the amount of work associated with case3 would be linear.
Note that a linear complexitybound would have to guarantee a linear number ofcase 2 cells as well since there is a constant amountof work associated with case 2 cells.To provide some intuition as to why the quadraticbound method resulted in linear observed parsingtime, consider again the chart structure in Figure 1.The black cells in the chart represent the cells thathave been closed when wj 6?
E (case 1 cells).
Inour example, w2 6?
E caused the cell spanning abto be closed, and w4 6?
E caused the cells span-ning abcd, bcd and cd to be closed.
Since there isno work required for these cells, the amount of workrequired to parse the sentence is reduced.
However,the quadratic bound does not include any potentialreduced work in the remaining open cells.
The graycells in the chart are cells with a reduced number ofpossible midpoints, as effected by the closed cellsin the chart.
For example, categories populating thecell spanning abc in position (1, 3) can be built intwo ways: either by combining entries in cell (1, 1)with entries in (2, 3) at midpoint m = 1; or by com-bining entries in (1, 2) and (3, 3) at midpointm = 2.However, cell (1, 2) is closed, hence there is onlyone midpoint at which (1, 3) can be built (m = 1).Thus the amount of work to parse the sentence willbe less than the worst-case quadratic bound based onthis processing savings in open cells.While imposition of the quadratic bound mayhave resulted (fortuitously) in constant boundedwork for case 3 cells and a linear number of case2 cells, there is no guarantee that this will be thecase.
One method to guarantee that both conditionsare met is the following: if |E| ?
k for some con-stant k, then both conditions will be met and parsingcomplexity will be linear.
We prove here that con-straining E to contain a constant number of wordsresults in linear complexity.Lemma 1: If |E| ?
k for some k, then theamount of work for any cell is bounded by ckfor some constant c (grammar constant).Proof: Recall from Section 2 that for each cell(i, j), there are j?i midpoints m that require com-bining entries in cells (i,m) and (m+1, j) to createentries in cell (i, j).
If m > i, then cell (i,m) isempty unless wm ?
E. If cell (i,m) is empty, thereis no work to be done at that midpoint.
If |E| ?
k,then there are a maximum of k midpoints for anycell, hence the amount of work is bounded by ck forsome constant c.2Lemma 2: If |E| ?
k for some k, then the num-ber of cells (i, j) such that wj ?
E is no morethan kN where N is the length of the string.Proof: For a string of length N , each word wj inthe string has at most N cells such that wj is thelast word in the substring spanned by that cell, sinceeach such cell must begin with a distinct word wi inthe string where i ?
j, of which there are at mostN .Therefore, if |E| ?
k for some k, then the numberof cells (i, j) such that wj ?
E would be no morethan kN .2Theorem: If |E| ?
k, then the parsing complex-ity is O(k2N).Proof: As stated earlier, each cell (i, j) falls in oneof three cases: 1) wj 6?
E; 2) wj ?
E and wi 6?
B;and 3) wi ?
B and wj ?
E. Case 1 cells are com-pletely closed, there is no work to be done in thosecells.
By Lemma 2, there are at maximum kN cellsthat fall in either case 2 or case 3.
By Lemma 1, theamount of work for each of these cells is boundedby ck for some constant c. Therefore, the theorem isproved.2If |E| ?
k for a constant k, the theorem provesthe complexity will be O(N).
If |E| ?
k logN ,then parsing complexity will be O(N log2N).
Fig-ure 4 shows sentence length versus parsing timeunder three different conditions1: baseline (uncon-strained); O(N log2N) at |E| ?
3 logN ; and linearat |E| ?
16.
The bottom graph zooms in to demon-strate that the O(N log2N) constraints can outper-form the linear constraints for shorter strings (seearound 20 words).
As the length of the string in-creases, though, the performance lines cross, and thelinear constraints demonstrate higher efficiency forthe longer strings, as expected.Unlike the method for imposing quadraticbounds, this method only makes use of set E, notB.
To select the constraints, we rank the word/Eposterior probabilities, and choose the top k (eitherconstant or scaled with a logN factor); the rest ofthe words fall outside of the set.
In this approach,1Selection of these particular operating points for theN log2N and linear methods is discussed in Section 6.6520 20 40 60 80 100 1200246810Sentence length in wordsParsing timein secondsNo constraintsO(Nlog2N) parsing (k=3)O(N) parsing (k=16)0 10 20 30 4000.20.40.60.81Sentence length in wordsParsing timein secondsNo constraintsO(Nlog2N) parsing (k=3)O(N) parsing (k=16)Figure 4: Scatter plots of sentence length versus pars-ing time for (1) baseline exact inference (no constraints);(2) O(N log2N) constraints; and (3) O(N) constraints.every word falls in the B set, hence no constraintson words beginning multi-word constituents are im-posed.6 ResultsFigure 5 plots F-measure accuracy versus time toparse the development set for four methods ofimposing constraints: the previously plotted highprecision and quadratic bound constraints, alongwith O(N log2N) and linear bound constraints us-ing methods described in this paper.
All meth-ods are employed at various parameterizations, fromvery lightly constrained to very heavily constrained.The complexity-bound constraints are not combinedwith the high-precision constraints for this plot.As can be seen from the plot, the linear andO(N log2N) methods do not, as applied, achieve asfavorable of an accuracy/efficiency tradeoff curve asthe quadratic bound method.
This is not surprising,0 200 400 600 800556065707580Seconds to parse sectionF?measure accuracyHigh precision constraintsO(N2) complexity boundsO(Nlog2N) complexity boundsO(N) complexity boundsFigure 5: Time to parse (seconds) versus accuracy (F-measure) for high precision constraints of various thresholdsversus three methods of imposing constraints with complexitybounds: (1) O(N2) complexity (number of open cells ?
kN );(2) O(N log2N) complexity (|E| ?
k logN ); and (3) linearcomplexity (|E| ?
k).given that no words are excluded from the set Bfor these methods, hence far fewer constraints over-all are applied with the new method than with thequadratic bound method.Of course, the high precision constraints can beapplied together with the complexity bound con-straints, as described in Section 4.2.
For combiningcomplexity-bound constraints with high-precisionconstraints, we first chose operating points for boththe linear and O(N log2N) complexity bound meth-ods at the points before accuracy begins to de-grade with over-constraint.
For the linear complex-ity method, the operating point is to constrain theset size of E to a maximum of 16 members, i.e.,|E| ?
16.
For the N log2N complexity method,|E| ?
3 logN .Table 2 presents results for these operating pointsused in conjunction with the 90% high precisionconstraints.
For these methods, this combinationis particularly important, since it includes all of thehigh precision constraints from the set B, which arecompletely ignored by both of the new methods.
Wecan see from the results in the table that the com-bination brings the new constraint methods to verysimilar accuracy levels as the quadratic constraints,yet with the guarantee of scaling linearly to longerand longer sentences.The efficiency benefits of combining constraints,shown in Table 2, are relatively small here becausethe dataset contains mostly shorter sentences.
Space653F-measure timeConstraints accuracy (seconds)None (baseline CYK) 74.1 3646High Precision (90%) 77.0 181Quad (5N ) + HiPrec (90%) 76.9 166N log2N (3logN ) + HP (90) 76.9 170Linear (16) + HiPrec (90%) 76.8 167Table 2: Speed and accuracy of exact-inference CYK parseron WSJ section 24 under various constraint conditions, includ-ing combining various complexity bound constraints and highprecision constraints.limitations prevent us from including scatter plotssimilar to those in Figure 3 for the constraint combi-nation trials, which show that the observed parsingtime of shorter sentences is typically identical undereach constraint set, while the parsing time of longersentences tends to differ more under each conditionand exhibit characteristics of the complexity bounds.Thus by combining high-precision and complexityconstraints, we combine typical-case efficiency ben-efits with worst-case complexity bounds.Note that these speedups are achieved with noadditional techniques for speeding up search, i.e.,modulo the cell closing mechanism, the CYK pars-ing is exhaustive?it explores all possible categorycombinations from the open cells.
Techniques suchas coarse-to-fine or A?
parsing, the use of an agenda,or setting of probability thresholds on entries incells?these are all orthogonal to the current ap-proach, and could be applied together with themto achieve additional speedups.
However, noneof these other techniques provide what the currentmethods do: a complexity bound that will hold evenin the worst case.To validate the selected operating points on a dif-ferent section, Table 3 presents speed and accuracyresults on the test set (WSJ section 23) for the exact-inference CYK parser.We also conducted similar preliminary trials forparsing the Penn Chinese Treebank (Xue et al,2004), which contains longer sentences and differ-ent branching characteristics in the induced gram-mar.
Results are similar to those shown here, withchart constraints providing both efficiency and ac-curacy gains.7 ConclusionWe have presented a method for constraining acontext-free parsing pipeline that provably achievesF-measure timeConstraints accuracy (seconds)None (baseline CYK) 73.8 5122High Precision (90%) 76.8 272Quad (5N ) + HiPrec (90%) 76.8 263N log2N (3logN ) + HP (90) 76.8 266Linear (16) + HiPrec (90%) 76.8 264Table 3: Speed and accuracy of exact-inference CYK parseron WSJ section 23 under various constraint conditions, includ-ing combining various complexity bound constraints and highprecision constraints.linear worst case complexity.
Our method achievescomparable observed performance to the quadraticcomplexity method previously published in Roarkand Hollingshead (2008).
We were motivated topursue this method by the observed linear parsingtime achieved with the quadratic bound constraints,which suggested that a tighter complexity boundcould be achieved without hurting performance.We have also shown that combining methods forachieving complexity bounds?which are of pri-mary utility for longer strings?with methods forachieving strong observed typical case speedups canbe profitable, even for shorter strings.
The result-ing combination achieves both typical speedups andworst-case bounds on processing.The presented methods may not be the only wayto achieve these bounds using tagger pre-processingof this sort, though they do have the virtue ofvery simple constraint selection.
More complicatedmethods that track, in fine detail, how many cellsare open versus closed, run the risk of a constraintselection process that is itself quadratic in the lengthof the string, given that there are a quadratic numberof chart cells.
Even so, the presented methods criti-cally control midpoints for all cells only via the setE (words that can end a multi-word constituent) andignoreB.
More complicated methods for using bothsets that also achieve linear complexity (perhapswith a smaller constant), or that achieve O(N logN)complexity rather than O(N log2N), may exist.AcknowledgmentsThis research was supported in part by NSF Grant#IIS-0447214 and DARPA grant #HR0011-08-1-0016.
Any opinions, findings, conclusions or recom-mendations expressed in this publication are those ofthe authors and do not necessarily reflect the viewsof the NSF or DARPA.654ReferencesD.
Blaheta and E. Charniak.
1999.
Automatic compen-sation for parser figure-of-merit flaws.
In Proceedingsof the 37th annual meeting of the Association for Com-putational Linguistics (ACL), pages 513?518.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In Proceedings of the 1st Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 132?139.J.
Cocke and J.T.
Schwartz.
1970.
Programming lan-guages and their compilers: Preliminary notes.
Tech-nical report, Courant Institute of Mathematical Sci-ences, NYU.B.
Djordjevic, J.R. Curran, and S. Clark.
2007.
Im-proving the efficiency of a wide-coverage CCG parser.In Proceedings of the 10th International Workshop onParsing Technologies (IWPT), pages 39?47.E.
Glaysher and D. Moldovan.
2006.
Speeding up fullsyntactic parsing by leveraging partial parsing deci-sions.
In Proceedings of the COLING/ACL 2006 MainConference Poster Sessions, pages 295?300.K.
Hall and M. Johnson.
2004.
Attention shifting forparsing speech.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 40?46.K.
Hollingshead and B. Roark.
2007.
Pipeline iteration.In Proceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages952?959.K.
Hollingshead, S. Fisher, and B. Roark.
2005.Comparing and combining finite-state and context-free parsers.
In Proceedings of the Human Lan-guage Technology Conference and the Conference onEmpirical Methods in Natural Language Processing(HLT/EMNLP), pages 787?794.T.
Kasami.
1965.
An efficient recognition and syntaxanalysis algorithm for context-free languages.
Techni-cal report, AFCRL-65-758, Air Force Cambridge Re-search Lab., Bedford, MA.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.A.
Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
MachineLearning, 34(1-3):151?175.B.
Roark and K. Hollingshead.
2008.
Classifying chartcells for quadratic complexity context-free inference.In Proceedings of the 22nd International Conferenceon Computational Linguistics (COLING), pages 745?752.N.
Xue, F. Xia, F. Chiou, and M. Palmer.
2004.
ThePenn Chinese treebank: Phrase structure annotationof a large corpus.
Natural Language Engineering,10(4):1?30.D.H.
Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information andControl, 10(2):189?208.655
