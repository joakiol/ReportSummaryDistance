Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 992?1002,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsEnvironment-Driven Lexicon Induction for High-Level InstructionsDipendra K. Misra?Kejia Tao?Percy Liang?
?Ashutosh Saxena?dkm@cs.cornell.edu kt454@cornell.edu pliang@cs.stanford.edu asaxena@cs.cornell.edu?Department of Computer Science, Cornell University?
?Department of Computer Science, Stanford UniversityAbstractWe focus on the task of interpreting com-plex natural language instructions to arobot, in which we must ground high-levelcommands such as microwave the cup tolow-level actions such as grasping.
Pre-vious approaches that learn a lexicon dur-ing training have inadequate coverage attest time, and pure search strategies can-not handle the exponential search space.We propose a new hybrid approach thatleverages the environment to induce newlexical entries at test time, even for newverbs.
Our semantic parsing model jointlyreasons about the text, logical forms, andenvironment over multi-stage instructionsequences.
We introduce a new datasetand show that our approach is able to suc-cessfully ground new verbs such as dis-tribute, mix, arrange to complex logicalforms, each containing up to four predi-cates.1 IntroductionThe task of mapping natural language instructionsto actions for a robot has been gaining momen-tum in recent years (Artzi and Zettlemoyer, 2013;Tellex et al, 2011; Misra et al, 2014; Bolliniet al, 2011; Guadarrama et al, 2013; Matuszeket al, 2012b; Fasola and Mataric, 2013).
Weare particularly interested in instructions contain-ing verbs such as ?microwave?
denoting high-levelconcepts, which correspond to more than 10 low-level symbolic actions such as grasp.
In thissetting, it is common to find new verbs requiringnew concepts at test time.
For example, in Fig-ure 1, suppose that we have never seen the verb?fill?.
Can we impute the correct interpretation,and moreover seize the opportunity to learn what?fill?
means in a way that generalizes to future in-structions?Text: ?get the cup, fill it with water and then microwave the cup?grasping cup3 ?near(robot1,cup3)in cup3,microwave ?state(microwave1,is-on)state cup3,water ?on(cup3,sink)Unseen verb ?
fill ?
is groundedat test time using environment.Figure 1: A lexicon learned on the training datacannot possibly cover all the verb-concept map-pings needed at test time.
Our algorithm learnsthe meaning of new verbs (e.g., ?fill?)
using theenvironment context.Previous work in semantic parsing handles lex-ical coverage in one of two ways.
Kwiatkowski etal.
(2010) induces a highly constrained CCG lex-icon capable of mapping words to complex log-ical forms, but it would have to skip new words(which in Figure 1 would lead to microwaving anempty cup).
Others (Berant and Liang, 2014) takea freer approach by performing a search over log-ical forms, which can handle new words, but thelogical forms there are much simpler than the oneswe consider.In this paper, we present an hybrid approachthat uses a lexicon to represent complex conceptsbut also strongly leverages the environment toguide the search space.
The environment can pro-vide helpful cues in several ways:?
Only a few environments are likely for a givenscenario?e.g., the text is unlikely to ask therobot to microwave an empty cup or put bookson the floor.?
The logical form of one segment of text con-strains that of the next segment?e.g., the text isunlikely to ask the robot to pick a cup and thenput it back immediately in the same spot.We show that this environment context provides992?1 ??
?1?1 ?
?planner?1simulatorShallow Parsing (Section 3.2)???planner??????
?Action Sequence:??????
???1?
??????
???1?
??????
?????_?????
?1 ???????
?????????
??????????(???,???1?)
?Environment: Logical Form ??(?,?
)Frame Node?:???
?????
?state v1,Kas-cG?
near v1,v,????
: ?
?1 ?
???1?????
?????1???
: old mapping???1?
???????????OnPower-off?
?
???
??planner?
?Semantic Parsing Model(Section 5)???
1planner???
1simulatorText: ?Turn on xbox.
Take Far Cry Game CD and put in xbox.Throw out beer, coke and sketchy stuff in bowl.?
??
: throw, :?
[ beer, coke, sketchy stuff, bowl ]?
: { in: sketchy stuff?
bowl?Figure 2: Graphical model overview: we first deterministically shallow parse the text x into a controlflow graph consisting of shallow structures {ci}.
Given an initial environment e1, our semantic parsingmodel maps these frame nodes to logical forms {zi} representing the postconditions.
From this, a plannerand simulator generate the action sequences {ai} and resulting environments {ei}.a signal for inducing new lexical entries that mappreviously unseen verbs to novel concepts.
In theexample in Figure 1, the algorithm learns that mi-crowaving an empty cup is unlikely and this sug-gests that the verb ?fill?
must map to actions thatend up making the cup not empty.Another contribution of this paper is using post-conditions as logical forms rather than actions, asin previous work (Artzi and Zettlemoyer, 2013;Misra et al, 2014).
Postconditions not only re-duce the search space of logical forms, but are alsoa more natural representation of verbs.
We definea conditional random field (CRF) model over post-conditions, and use a planner to convert postcon-ditions into action sequences and a simulator togenerate new environments.At test time, we use the lexicon induced fromthe training data, but also perform an environment-guided search over logical forms to induce newlexical entries on-the-fly.
If the predicted actionsequence uses a new lexical entry generated by thesearch, it is added to the lexicon, where it can bereused in subsequent test examples.We evaluate our algorithm on a new corpus con-taining text commands for a household robot.
Thetwo key findings of our experiments are: First, theenvironment and task context contain enough in-formation to allow us to learn lexical entries fornew verbs such as ?distribute?
and ?mix?
withcomplex semantics.
Second, using both lexicalentries generated by a test-time search and thosefrom the lexicon induced by the training data out-performs the two individual approaches.
This sug-gests that environment context can help allevi-ate the problem of having a limited lexicon forgrounded language acquisition.2 Problem StatementAt training time, we are given a set of examplesD = {(x(m), e(m), a(m), pi(m))}Mm=1, where x(m)is a text containing natural language instructions,e(m)is an initial environment, a(m)is a human-annotated sequence of actions, and pi(m)specifiesa monotonic alignment between segments of x(m)and segments of a(m).
For example, given wordsx(m)= x1x2and a(m)= a1a2a3, pi(m)mightspecify that x1aligns to a1a2and x2aligns to a3.At test time, given a sequence of text-environment pairs as input {(x(n), e(n))}Nn=1, wewish to generate a sequence of actions a(n)foreach input pair.
Note that our system is allowed touse information about one test example to improveperformance on subsequent ones.
We evaluate asystem on its ability to recover a human-annotatedsequence of actions.3 Approach OverviewFigure 2 shows our approach for mapping text xto actions a1:kgiven the initial environment e1.3.1 RepresentationWe use the following representation for the differ-ent variables in Figure 2.Environment.
An environment eiis representedby a graph whose nodes are objects and edgesrepresent spatial relations between these objects.We consider five basic spatial relations: near,grasping, on, in and below.
Each object has an993instance ID (e.g., book9), a category name (e.g.,chair, xbox), a set of properties such as graspable,pourable used for planning and a set of booleanstates such as has-water, at-channel3, whosevalues can be changed by robot actions.
The robotis also an object in the environment.
For example,the objects xbox1, snacktable2, are two objectsin e1in Figure 2 with relation on between them.Postconditions.
A postcondition is a conjunctionof atoms or their negations.
Each atom consists ofeither a spatial relation between two objects (e.g.,on(book9, shelf3)) or a state and a value (e.g.,state(cup4, has-water)).
Given an environmente, the postcondition evaluates to true or false.Actions.
Each action in an action sequence aiconsists of an action name with a list of argu-ments (e.g., grasp(xbox1)).
The action nameis one of 15 values (grasp, moveto, wait, etc.
),and each argument is either an object in the envi-ronment (e.g., xbox1), a spatial relation (e.g., infor keep(ramen2, in, kettle1), or a postcondi-tion (e.g., for wait(state(kettle1, boiling))).Logical Forms.
The logical form ziis a pair(`, ?)
containing a lexical entry ` and a map-ping ?.
The lexical entry ` contains a parameter-ized postcondition such as ?~v.grasping(v1, v2)?
?near(v3, v2), and ?
maps the variables~v to ob-jects in the environment.
Applying the parame-terized postcondition on ?
yields a postcondition;note that a postcondition can be represented bydifferent logical forms.
A lexical entry containsother information which are used for defining fea-tures, which is detailed in Section 4.Control Flow Graphs.
Following previous work(Tellex et al, 2011; Misra et al, 2014), we convertthe text x to a shallow representation.
The par-ticular representation we choose is a control flowgraph, which encodes the sequential relation be-tween atomic segments in the text.
Figure 3 showsthe control flow graph for an example text.
In acontrol flow graph, each node is either a framenode or a conditional node.
A frame node rep-resents a single clause (e.g., ?change the chan-nel to a movie?)
and has at most one successornode.
Specifically, a frame node consists of a verb?
(e.g., arrange, collect), a set of object descrip-tions {?i} which are the arguments of the verb(e.g., the guinness book, movie channel), and spa-tial relations r between the arguments (e.g., be-tween, near).
The object description ?
is either ananaphoric reference (such as ?it?)
or a tuple con-taining the main noun, associated modifiers, andrelative clauses.Text: ?/f an?
of the pots have food in them, then dump them out in thegarbage can and then put them on the sin?
else ?eep it on the table ??
?e categor\e,cup ?state(e,IooG)?
: dump?
: [them, the garbage can]?
: ?in: them?
garbage can??
: put?
: [them, the sink]?
: ?
?n: them?
the sink??
: keep?
: [it, the table]?
: ?
?n: it?
the table?Conditional node (????
)Frame Node ?,?, ??
:  verb?
: set of object description ?:?
(main noun or pronoun, modifiers)?
: relationship between descriptionsFigure 3: We deterministically parse text into ashallow structure called a control flow graph.A conditional node contains a logical postcon-dition with at most one existentially quantifiedvariable (in contrast to a frame node, which con-tains natural language).
For example, in Figure 3the conditional node contains the expression cor-responding to the text ?if any of the pots has food?There are two types of conditional nodes: branch-ing and temporal.
A branching conditional noderepresents an ?if ?
statement and has two succes-sor nodes corresponding to whether the conditionevaluates to true or false in the current environ-ment.
A temporal conditional node represents an?until?
statement and waits until the condition isfalse in the environment.3.2 Formal OverviewShallow Parsing.
We deterministically convertthe text x into its control flow graph G using a setof manual rules applied on its constituency parsetree from the Stanford parser (Klein and Manning,2003).
Conditionals in our dataset are simple andcan be converted into postconditions directly usinga few rules, unlike the action verbs (e.g., ?fill?
),which is the focus of this paper.
The details ofour shallow parsing procedure is described in theappendix.Given an environment e1, G is reduced to a sin-gle sequence of frame nodes c1, .
.
.
, ck, by evalu-ating all the branch conditionals on e1.Semantic Parsing Model.
For each frame node ciand given the current environment ei, the seman-tic parsing model (Section 5) places a distributionover logical forms zi.
This logical form zirep-resents a postcondition on the environment afterexecuting the instructions in ci.Planner and Simulator.
Since our semantic rep-resentations involve postconditions but our modelis based on the environment, we need to connectthe two.
We use planner and a simulator that to-994gether specify a deterministic mapping from thecurrent environment eiand a logical form zitoa new environment ei+1.
Specifically, the plan-ner takes the current environment eiand a logicalform ziand computes the action sequence ai=planner(ei, zi) for achieving the post conditionrepresented by zi.1The simulator takes the currentenvironment eiand an action sequence aiand re-turns a new environment ei+1= simulator(ei, ai).4 Anchored Verb LexiconsLike many semantic parsers, we use a lexicon tomap words to logical forms.
Since the environ-ment plays an central role in our approach, we pro-pose an anchored verb lexicon, in which we storeadditional information about the environment inwhich lexical entries were previously used.
Wefocus only on verbs since they have the most com-plex semantics; object references such as ?cup?can be mapped easily, as described in Section 5.More formally, an anchored verb lexicon ?
con-tains lexical entries ` of the following form: [?
?
(?~v.S, ?)]
where, ?
is a verb, S is a postconditionwith free variables ~v, and ?
is a mapping of thesevariables to objects.
An example lexical entry is:[ pour?
(?v1v2v3.S, ?
)], where:S = grasping(v1, v2)?
near(v1, v3)?
?state(v2, milk)?
state(v3, milk)?
= {v1?
robot1, v2?
cup1, v3?
bowl3} (anchoring)As Table 1 shows, a single verb will in generalhave multiple entries due to a combination of pol-ysemy and the fact that language is higher-levelthan postconditions.Advantages of Postconditions.
In contrast to pre-vious work (Artzi and Zettlemoyer, 2013; Misra etal., 2014), we use postconditions instead of actionsequence for two main reasons.
First, postcondi-tions generalize better.
To illustrate this, considerthe action sequence for the simple task of filling acup with water.
At the time of learning the lexi-con, the action sequence might correspond to us-ing a tap for filling the cup while at test time, theenvironment may not have a tap but instead havea pot with water.
Thus, if the lexicon maps to ac-tion sequence, then it will not be applicable at testtime whereas the postcondition state(z1, water)is valid in both cases.
We thus shift the load of in-ferring environment-specific actions onto planners1We use the symbolic planner of Rintanen (2012) whichcan perform complex planning.
For example, to pick up abottle that is blocked by a stack of books, the planner willfirst remove the books before grasping the bottle.
In contrast,Artzi and Zettlemoyer (2013) use a simple search over im-plicit actions.Table 1: Some lexical entries for the verb ?turn?Sentence Context Lexical entry [turn?
(?~v.S, ?
)]?turn on the TV?
state(v1, is-on) ?
near(v2, v1)?
: v1?
tv1, v2?
robot1?turn on the right state(v1, fire3) ?
near(v2, v1)back burner?
?
: v1?
stove1, v2?
robot1?turn off the water?
?state(v1, tap-on)?
: v1?
sink1?turn the television state(v1, channel6) ?
near(v1, v2)input to xbox?
?
: v1?
tv1, v2?
xbox1and use postconditions for representation, whichbetter captures the semantics of verbs.Second, because postconditions are higher-level, the number of atoms needed to repre-sent a verb is much less than the correspond-ing number of actions.
For example, thetext ?microwave a cup?, maps to action se-quence with 10?15 actions, the postconditiononly has two atoms: in(cup2, microwave1) ?state(microwave, is-on).
This makes search-ing for new logical forms more tractable.Advantages of Anchoring.
Similar to the VEILtemplates of Misra et al (2014), the free variables~v are associated with a mapping ?
to concrete ob-jects.
This is useful for resolving ellipsis.
Supposethe following lexical entry was created at train-ing time based on the text ?throw the drinks in thetrash bag?
:[`: throw?
?xyz.S(x, y, z)], whereS = in(x, y) ?
?grasping(z, x) ?
?state(z, closed)?
= {x?
coke1, y?
garbageBin1, z?
robot1}Now consider a new text at test time ?throwaway the chips?, which does not explicitly men-tion where to throw the chips.
Our semantic pars-ing algorithm (Section 5) will use the previousmapping y ?
garbabeBin1to choose an objectmost similar to a garbage bin.5 Semantic Parsing ModelGiven a sequence of frame nodes c1:kand an ini-tial environment e1, our semantic parsing modeldefines a joint distribution over logical forms z1:k.Specifically, we define a conditional random field(CRF) over z1:k, as shown in Figure 2:p?
(z1:k| c1:k, e1)?exp(k?i=1?
(ci, zi?1, zi, ei) ?
?
), (1)where ?
(ci, zi?1, zi, ei) is the feature vector and?
is the weight vector.
Note that the environ-ments e1:kare a deterministic function of the log-ical forms z1:kthrough the recurrence ei+1=simulator(ei, planner(ei, zi)), which couples thedifferent time steps.995Features.
The feature vector ?
(ci, zi?1, zi, ei)contains 16 features which capture the dependen-cies between text, logical forms, and environment.Recall that zi= ([?
?
(?~v.S, ?
)], ?i), where ?is the environment in which the lexical entry wascreated and ?iis the current environment.
Letfi= (?~v.S)(?i) be the current postcondition.Here we briefly describe the important features(see the supplemental material for the full list):?
Language and logical form: The logical formzishould generally reference objects mentionedin the text.
Assume we have computed a cor-relation ?
(?, o) between each object description?
and object o, whose construction is describedlater.
We then define two features: precision cor-relation, which encourages zito only use objectsreferred to in ci; and recall correlation, whichencourages zito use all the objects referred to inci.?
Logical form: The postcondition fishould bebased on previously seen environments.
For ex-ample, microwaving an empty cup and grasp-ing a couch are unlikely postconditions.
Wedefine features corresponding to the averageprobability (based on the training data) of allconjunctions of at most two atoms in thepostcondition (e.g., grasping(robot, cup)}).We do the same with their abstract versions({grasping(v1, v2)}).
In addition, we build thesame set of four probability tables conditionedon verbs in the training data.
For example, theabstract postcondition state(v1, water) has ahigher probability conditioned on the verb ?fill?.This gives us a total of 8 features of this type.?
Logical form and environment: Recall that an-choring helps us in dealing with ellipsis andnoise.
We add a feature based on the averagecorrelation between the objects of the new map-ping ?iwith the corresponding objects in the an-chored mapping ?.The other features are based on the relationshipbetween object descriptions, similarity between ?and ?iand transition probabilities between logi-cal forms zi?1and zi.
These probabilities are alsolearned from training data.Mapping Object Descriptions.
Our features relyon a mapping from object descriptions ?
(e.g.,?the red shiny cup?)
to objects o (e.g., cup8),which has been addressed in many recent works(Matuszek et al, 2012a; Guadarrama et al, 2014;Fasola and Matari?c, 2014).One key idea is: instead of computing rigid lex-ical entries such as cup?
cup1, we use a contin-uous correlation score ?
(?, o) ?
[0, 1] that mea-sures how well ?
describes o.
This flexibility al-lows the algorithm to use objects not explicitlymentioned in text.
Given ?get me a tank of wa-ter?, we might choose an approximate vessel (e.g.,cup2).Given an object description ?, an object o, and aset of previously seen objects (used for anaphoricresolution), we define the correlation ?
(?, o) usingthe following approach:?
If ?
is a pronoun, ?
(?, o) is the ratio of the posi-tion of the last reference of o to the length of theaction sequence computed so far, thus preferringrecent objects.?
Otherwise, we compute the correlation usingvarious sources: the object?s category; theobject?s state for handling metonymy (e.g.,the description ?coffee?
correlates well withthe object mug1if mug1contains coffee?state(mug1, has-coffee) is true), WordNet(Fellbaum, 1998) for dealing synonymy and hy-ponymy; and word alignments between the ob-jects and text from Giza++ (Och and Ney, 2003)to learn domain-specific references (e.g., ?Guin-ness book?
refers to book1, not book2).
Moredetails can be found in the supplemental mate-rial.6 Lexicon Induction from Training DataIn order to map text to logical forms, we first in-duce an initial anchored lexicon ?
from the train-ing data {(x(m), e(m), a(m), pi(m))}Mm=1.
At testtime, we add new lexical entries (Section 7) to ?.Recall that shallow parsing x(m)yields a list offrame nodes c1:k. For each frame node ciand itsaligned action sequence ai, we take the conjunc-tion of all the atoms (and their negations) whichare false in the current one eibut true in thenext environment ei+1.
We parametrize this con-junction by replacing each object with a variable,yielding a postcondition S parametrized by freevariables ~v and the mapping ?
from ~v to objectsin ei.
We then add the lexical entry [verb(ci) ?
(?~v.S, ?)]
to ?.Instantiating Lexical Entries.
At test time, fora given clause ciand environment ei, we generateset of logical forms zi= (`i, ?i).
To do this, weconsider the lexical entries in ?
with the same verbas ci.
For each such lexical entry `i, we can map itsfree variables ~v to objects in eiin an exponentialnumber of ways.
Therefore, for each `iwe onlyconsider the logical form (`i, ?i) where the map-ping ?iobtains the highest score under the current996model: ?i= arg max???
(ci, zi?1, (`i, ??
), ei) ??.
For the feature vector ?
that we consider,this approximately translates to solving an integerquadratic program with variables [yij] ?
{0, 1},where yij= 1 only if vimaps to object j.7 Environment-Driven LexiconInduction at Test TimeUnfortunately, we cannot expect the initial lexicon?
induced from the training set to have full cover-age of the required postconditions.
Even after us-ing 90% of the data for training, we encountered17% new postconditions on the remaining 10%.We therefore propose generating new lexical en-tries at test time and adding them to ?.Formally, for a given environment eiand framenode ci, we want to generate likely logical forms.Although the space of all possible logical formsis very large, the environment constrains the pos-sible interpretations.
We first compute the setof atoms that are false in eiand that only con-tain objects o that are ?referred?
to by eithercior ci?1, where ?refers?
means that there ex-ists some argument ?
in cifor which o ?arg maxo??
(?, o?).
For example, if cicorrespondsto the text ?distribute pillows among the couches?,we consider the atom on(pillow1, armchair1)but not on(pillow1, snacktable2) since the ob-ject armchair1has the highest correlation to thedescription ?couches?.Next, for each atom, we convert it into a logi-cal form z = (`, ?)
by replacing each object witha variable.
While this generalization gives us amapping ?, we create a lexical entry `i= [?
?
(?~v.S, ?)]
without it, where S is the parameter-ized atom.
Note that the anchored mapping isempty, representing the fact that this lexical en-try was unseen during training time.
For example,the atom state(tv1, mute) would be converted tothe logical form (`, ?
), where ` = [verb(ci) ?
(?v.state(v, mute), ?]
and ?
= {v ?
tv1}.
Wedo not generalize state names (e.g., mute) becausethey generally are part of the meaning of the verb.The score ?
(ci, zi?1, zi, ei) ?
?
is computedfor the logical form ziproduced by each post-condition.
We then take the conjunction of ev-ery pair of postconditions corresponding to the200 highest-scoring logical forms.
This gives usnew set of postconditions on which we repeatthe generalization-scoring-conjunction cycle.
Wekeep doing this while the scores of the new logi-cal forms is increasing or while there are logicalforms remaining.Train Time Anchored Lexicon ?
(Sec 6)???????
?(?????,?
)?such that ????
??(??
)Test Time Search for Logical Forms (Sec 7)Set of Logical Forms for ???????,??,??????(?,??)?
?is the new assignment ???
?,?
?where ???????
?(?????
, ?
)?is a test time lexical entryFigure 4: Logical forms for a given clause ci, en-vironment ei, and previous logical form zi?1aregenerated from both a lexicon induced from train-ing data and a test-time search procedure based onthe environment.If a logical form z = ([?
?
(?~v.S, ?
)], ?)
isused by the predicted action sequence, we add thelexical entry [?
?
(?~v.S, ?)]
to the lexicon ?.This is different to other lexicon induction proce-dures such as GENLEX (Zettlemoyer and Collins,2007) which are done at training time only andrequire more supervision.
Moreover, GENLEXdoes not use the environment context in creatingnew lexical entries and thus is not appropriate attest time, since it would vastly overgenerate lexi-cal entries compared to our approach.
For us, theenvironment thus provides implicit supervision forlexicon induction.8 Inference and Parameter EstimationInference.
Given a text x (which is converted toc1:kvia Section 3.2) and an initial environmente1, we wish to predict an action sequence a basedon p?
(a1:k| c1:k, e1), which marginalizes over alllogical forms z1:k(see Figure 2).To enumerate possible logical forms, semanticparsers typically lean heavily on a lexicon (Artziand Zettlemoyer, 2013), leading to high preci-sion but lower recall, or search more aggressively(Berant et al, 2013), leading to higher recall butlower precision.
We adopt the following hybridapproach: Given ei, ci?1, ciand zi?1, we use boththe lexical entries in ?
as explained in Section 6and the search procedure in Section 7 to generatethe set of possible logical forms for zi(see Fig-ure 4).
We use beam search, keeping only thehighest-scoring logical form with satisfiable post-conditions for each i ?
{1, .
.
.
, k} and resultingaction sequence a1:i.Parameter Estimation.
We split 10% of ourtraining data into a separate tuning set (the 90%was used to infer the lexicon).
On each examplein this set, we extracted the full sequence of logi-cal forms z1:kfrom the action sequence a1:kbasedon Section 6.
For efficiency, we used an objective997similar to pseudolikelihood to estimate the param-eters ?.
Specifically, we maximize the average log-likelihood over each adjacent pair of logical formsunder p??:p??
(zi| zi?1, ci, ei) ?
exp(?
(ci, zi?1, zi, ei)>?).
(2)The weights were initialized to 0.
We per-formed 300 iterations over the validation set witha learning rate of0.005N.9 Dataset and Experiments9.1 DatasetWe collected a dataset of 500 examples from 62people using a crowdsourcing system similar toMisra et al (2014).
We consider two different 3Dscenarios: a kitchen and a living room, each con-taining an average of 40 objects.
Both of thesescenarios have 10 environments consisting of dif-ferent sets of objects in different configurations.We define 10 high-level objectives, 5 per scenario,such as clean the room, make coffee, prepare roomfor movie night, etc.One group of users wrote natural language com-mands to achieve the high-level objectives.
An-other group controlled a virtual robot to accom-plish the commands given by the first group.
Thedataset contains considerable variety, consisting of148 different verbs, an average of 48.7 words pertext, and an average of 21.5 actions per action se-quence.
Users make spelling and grammar errorsin addition to occasionally taking random actionsnot relevant to the text.
The supplementary mate-rial contains more details.We filtered out 31 examples containing fewerthan two action sequences.
Of the remaining ex-amples, 378 were used for training and 91 wereused for test.
Our algorithm is tested on four newenvironments (two from each scenario).9.2 Experiments and ResultsEvaluation Metrics.
We consider two metrics,IED and END, which measure accuracy based onthe action sequence and environment, respectively.Specifically, the IED metric (Misra et al, 2014) isthe edit distance between predicted and true actionsequence.
The END metric is the Jaccard index ofsets A and B, where A is the set of atoms (e.g.,on(cup1,table1)) whose truth value changeddue to simulating the predicted action sequence,and B is that of the true action sequence.Baselines.
We compare our algorithm with thefollowing baselines:Table 3: Results on the metrics and baselines de-scribed in section 9.2.
The numbers are normal-ized to 100 with larger values being better.Algorithm IEDENDChance 0.3 0.5Manually Defined Templates 2.5 1.8UBL- Best Parse (Kwiatkowski et al, 2010) 5.3 6.9VEIL (Misra et al, 2014) 14.8 20.7Model with only train-time lexicon induction 20.8 26.8Model with only test-time lexicon induction 21.9 25.9Full Model 22.3 28.81.
Chance: Randomly selects a logical form forevery frame node from the set of logical formsgenerated by generalizing all possible postcon-ditions that do not hold in the current environ-ment.
These postconditions could contain up to93 atoms.2.
Manually Defined Templates: Defines a setof postcondition templates for verbs similar toGuadarrama (2013).3.
UBL-Best Parse (Kwiatkowski et al, 2010):UBL algorithm trained on text aligned with post-conditions and a noun-phrase seed lexicon.
Theplanner uses the highest scoring postconditiongiven by UBL to infer the action sequence.4.
VEIL (Misra et al, 2014): Uses action se-quences as logical forms and does not generatelexical entries at test time.We also consider two variations of our model: (i)using only lexical entries induced using the train-ing data, and (ii) using only the logical forms in-duced at test-time by the search procedure.The results are presented in Table 3.
We ob-serve that our full model outperforms the baselineand the two pure search- and lexicon-based varia-tions of our model.
We further observe that addingthe search procedure (Section 7) improved the ac-curacy by 1.5% on IED and 2% on END.
The log-ical forms generated by the search were able tosuccessfully map 48% of the new verbs.Table 2 shows new verbs and concepts that thealgorithm was able to induce at test time.
Thealgorithm was able to correctly learn the lexi-cal entries for the verbs ?distribute?
and ?mix?,while the ones for verbs ?change?
and ?boil?
wereonly partly correct.
The postconditions in Table 2are not structurally isomorphic to previously-seenlogical forms; hence they could not have beenhandled by using synonyms or factored lexicons(Kwiatkowski et al, 2011).
The poor performanceof UBL was because the best logical form oftenproduced an unsatisfiable postcondition.
This canbe remedied by joint modeling with the environ-998Table 2: New verbs and concepts induced at test time (Section 7).Text Postcondition represented by the learned logical form # Log.
forms explored?mix it with ice cream and syrup?state(cup2, ice-cream1) ?
state(cup2, vanilla) 15?distribute among the couches?
?j?
{1,3}on(pillowj, loveseat1) ?
on(pillowi+1, armchairi+1)386?boil it on the stove?
state(stove, stovefire1) ?
state(kettle, water) 109?change the channel to a movie?
state(tv1, channel4) ?
on(book1, loveseat1) 98ment.
The VEIL baseline used actions for repre-sentation and does not generalize as well as thepostconditions in our logical forms.It is also instructive to examine the alternatepostconditions that the search procedure consid-ers.
For the first example in Table 2, the followingpostcondition was considered by not selected:grasping(robot, icecream2)?grasping(robot, syrup1)While this postcondition uses all the objects de-scribed in the text, the environment-based featuressuggest it makes little sense for the task to end withthe robot eternally grasping objects.
For the sec-ond example, alternate postconditions consideredincluded:1. on(pillow1, pillow2) ?
on(pillow3, pillow4)2.
?4j=1on(pillowj, loveseat1)3.
?3j=1near(robot1, armchairj)The algorithm did not choose options 1 or 3since the environment-based features recognizesthese as unlikely configurations.
Option 2 wasruled out since the recall correlation feature real-izes that not all the couches are mentioned in thepostcondition.To test how much features on the environmenthelp, we removed all such features from our fullmodel.
We found that the accuracy fell to 16.0%on the IED metric and 16.6% on the END metric,showing that the environment is crucial.In this work, we relied on a simple deterministicshallow parsing step.
We found that shallow pars-ing was able to correctly process the text in only46% of the test examples, suggesting that improv-ing this initial component or at least modeling theuncertainty there would be beneficial.10 Related WorkOur work uses semantic parsing to map nat-ural language instructions to actions via novelconcepts, which brings together several themes:actions, semantic parsing, novel concepts, androbotics.Mapping Text to Actions.
Several works (Brana-van et al, 2009; Branavan et al, 2010; Vogel andJurafsky, 2010) use reinforcement learning to di-rectly map to text to actions, and do not even re-quire an explicit model of the environment.
How-ever, they can only handle simple actions, whereasour planner and simulator allows us to work withpostconditions, and thus tackle high-level instruc-tions.
Branavan et al (2012) extract precondi-tion relations from text, learn to map text to sub-goals (postconditions) for a planner.
However,their postconditions are atomic, whereas ours arecomplex conjunctions.Other works (Chen and Mooney, 2011; Kimand Mooney, 2012; Kollar et al, 2010; Fasola andMataric, 2013) have focused only on navigationalverbs and spatial relations, but do not handle high-level verbs.
Artzi and Zettlemoyer (2013) also fallinto the above category and offer a more composi-tional treatment.
They focus on how words com-pose; we focus on unraveling single words.The broader problem of grounded language ac-quisition, involving connecting words to aspects ofa situated context has been heavily studied (Duval-let et al, 2014; Yu and Siskind, 2013; Chu et al,2013; Chen and Mooney, 2008; Mooney, 2008;Fleischman and Roy, 2005; Liang et al, 2009).Semantic Parsing.
In semantic parsing, muchwork has leveraged CCG (Zettlemoyer andCollins, 2005; Zettlemoyer and Collins, 2007;Kwiatkowski et al, 2010).
One challenge behindlexically-heavy approaches is ensuring adequatelexical coverage.
Kwiatkowski et al (2011) en-hanced generalization by factoring a lexical entryinto a template plus a lexeme, but the rigidity ofthe template remains.
This is satisfactory whenwords map to one (or two) predicates, which is thecase in most existing semantic parsing tasks.
Forexample, in Artzi and Zettlemoyer (2013), verbsare associated with single predicates (?move?
tomove, ?walk?
to walk, etc.)
In our setting, verbscontain multi-predicate postconditions, for whichthese techniques would not be suitable.As annotated logical forms for training seman-tic parsers are expensive to obtain, several works(Clarke et al, 2010; Liang et al, 2011; Berant etal., 2013; Kwiatkowski et al, 2013) have devel-oped methods to learn from weaker supervision,and as in our work, use the execution of the logi-cal forms to guide the search.
Our supervision iseven weaker in that we are able to learn at test time999from partial environment constraints.Grounding to Novel Concepts.
Guadarrama etal.
(2014) map open vocabulary text to objects inan image using a large database.
Matuszek et al(2012a) create new predicates for every new ad-jective at test time.
Others (Kirk et al, 2014) askusers for clarification.
In contrast, we neither haveaccess to large databases for this problem, nor dowe do create new predicates or use explicit super-vision at test time.Robotic Applications.
Our motivation behind thiswork is to build robotic systems capable of takingcommands from users.
Other works in this areahave considered mapping text to a variety of ma-nipulation actions (Sung et al, 2015).
Levine etal.
(2015) and Lenz et al (2015) focus on spe-cific manipulation actions.
In order to build a rep-resentation of the environment, Ren et al (2012)and Wu et al (2014) present vision algorithms butonly output symbolic labels, which could act asinputs to our system.
In future work, we also planto integrate our work with RoboBrain (Saxena etal., 2014) to leverage these existing systems forbuilding a robotic system capable of working withphysical world data.11 ConclusionWe have presented an algorithm for mapping textto actions that induces lexical entries at test timeusing the environment.
Our algorithm couples thelexicon extracted from training data with a test-time search that uses the environment to reducethe space of logical forms.
Our results suggest thatusing the environment to provide lexical coverageof high-level concepts is a promising avenue forfurther research.Acknowledgements.
This research was supportedby the ONR (award N00014-14-1-0156), a SloanResearch Fellowship to the third author, and a Mi-crosoft Research Faculty Fellowship and NSF Ca-reer Award to the fourth author.
We thank AdityaJami and Jaeyong Sung for useful discussions.
Wealso thank Jiaqi Su for her help with data collec-tion and all the people who participated in the userstudy.Reproducibility.
Code, data, and experiments forthis paper are available on the CodaLab platformat https://www.codalab.org/worksheets/0x7f9151ec074f4f589e4d4786db7bb6de/.
De-mos can be found at http://tellmedave.com.Appendix: Parsing Text into Control FlowGraph.We first decompose the text x into its controlflow graph G using a simple set of rules:?
The parse tree of x is generated using the Stan-ford parser (Klein and Manning, 2003) and aframe node is created for each non-auxiliaryverb node in the tree.?
Conditional nodes are discovered by look-ing for the keywords until, if, after, when.The associated subtree is then parsed deter-ministically using a set of a rules.
Forexample, a rule parses ?for x minutes?
tofor(digit :x,unit :minutes).
We found thatall conditionals can be interpreted against theinitial environment e1, since our world is fully-observable, deterministic, and the user givingthe command has full view of the world.?
To find objects, we look for anaphoric terminalnodes or nominals whose parent is not a nominalor which have a PP sibling.
These are processedinto object descriptions ?.?
Object descriptions ?
are attached to the framenode, whose verb is nearest in the parse tree tothe main noun of ?.?
Nodes corresponding to {IN,TO,CC,?,?}
areadded as the relation between the correspondingargument objects.?
If there is a conjunction between two objects ina frame node and if these objects have the samerelation to other objects, then we split the framenode into two sequential frame nodes aroundthese objects.
For example, a frame node corre-sponding to the text segment ?take the cup andbowl from table?
is split into two frame nodescorresponding to ?take the cup from table?
and?take bowl from table?.?
A temporal edge is added between successiveframe nodes in the same branch of a condition.A temporal edge is added between a conditionalnode and head of the true and false branches ofthe condition.
The end of all branches in a sen-tence are joined to the starting node of the suc-cessive sentence.ReferencesY.
Artzi and L. Zettlemoyer.
2013.
Weakly supervisedlearning of semantic parsers for mapping instruc-tions to actions.
Transactions of the Association forComputational Linguistics (TACL), 1:49?62.J.
Berant and P. Liang.
2014.
Semantic parsing viaparaphrasing.
In Association for ComputationalLinguistics (ACL).J.
Berant, A. Chou, R. Frostig, and P. Liang.
2013.Semantic parsing on Freebase from question-answerpairs.
In Empirical Methods in Natural LanguageProcessing (EMNLP).1000M.
Bollini, J. Barry, and D. Rus.
2011.
Bakebot: Bak-ing cookies with the PR2.
In The PR2 Workshop,IROS.S.
Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzi-lay.
2009.
Reinforcement learning for mappinginstructions to actions.
In Association for Com-putational Linguistics and International Joint Con-ference on Natural Language Processing (ACL-IJCNLP), pages 82?90.S.
Branavan, L. Zettlemoyer, and R. Barzilay.
2010.Reading between the lines: Learning to map high-level instructions to commands.
In Associationfor Computational Linguistics (ACL), pages 1268?1277.S.
Branavan, N. Kushman, T. Lei, and R. Barzilay.2012.
Learning high-level planning from text.
InAssociation for Computational Linguistics (ACL),pages 126?135.D.
L. Chen and R. J. Mooney.
2008.
Learning tosportscast: A test of grounded language acquisition.In International Conference on Machine Learning(ICML), pages 128?135.D.
L. Chen and R. J. Mooney.
2011.
Learning to in-terpret natural language navigation instructions fromobservations.
In Association for the Advancement ofArtificial Intelligence (AAAI), pages 859?865.V.
Chu, I. McMahon, L. Riano, C. McDonald, Q. He,J.
Perez-Tejada, M. Arrigo, N. Fitter, J. Nappo,T.
Darrell, et al 2013.
Using robotic exploratoryprocedures to learn the meaning of haptic adjectives.In International Conference on Intelligent Robotsand Systems (IROS).J.
Clarke, D. Goldwasser, M. Chang, and D. Roth.2010.
Driving semantic parsing from the world?s re-sponse.
In Computational Natural Language Learn-ing (CoNLL), pages 18?27.F.
Duvallet, M. R. Walter, T. Howard, S. Hemachan-dra, J. Oh, S. Teller, N. Roy, and A. Stentz.
2014.Inferring maps and behaviors from natural languageinstructions.
In International Symposium on Exper-imental Robotics (ISER).J.
Fasola and M. Mataric.
2013.
Using semantic fieldsto model dynamic spatial relations in a robot archi-tecture for natural language instruction of servicerobots.
In International Conference on IntelligentRobots and Systems (IROS).J.
Fasola and M. J. Matari?c.
2014.
Interpretinginstruction sequences in spatial language discoursewith pragmatics towards natural human-robot inter-action.
In International Conference on Robotics andAutomation (ICRA), pages 6667?6672.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.M.
Fleischman and D. Roy.
2005.
Intentional contextin situated natural language learning.
In Computa-tional Natural Language Learning (CoNLL), pages104?111.S.
Guadarrama, L. Riano, D. Golland, D. Gouhring,Y.
Jia, D. Klein, P. Abbeel, and T. Darrell.
2013.Grounding spatial relations for human-robot inter-action.
In International Conference on IntelligentRobots and Systems (IROS).S.
Guadarrama, E. Rodner, K. Saenko, N. Zhang,R.
Farrell, J. Donahue, and T. Darrell.
2014.
Open-vocabulary object retrieval.
In Robotics: Scienceand Systems (RSS).J.
Kim and R. Mooney.
2012.
Unsupervised PCFG in-duction for grounded language learning with highlyambiguous supervision.
In Computational NaturalLanguage Learning (CoNLL), pages 433?444.N.
H. Kirk, D. Nyga, and M. Beetz.
2014.
Con-trolled natural languages for language generation inartificial cognition.
In International Conference onRobotics and Automation (ICRA), pages 6667?6672.D.
Klein and C. Manning.
2003.
Accurate unlexical-ized parsing.
In Association for Computational Lin-guistics (ACL), pages 423?430.T.
Kollar, S. Tellex, D. Roy, and N. Roy.
2010.Grounding verbs of motion in natural language com-mands to robots.
In International Symposium on Ex-perimental Robotics (ISER).T.
Kwiatkowski, L. Zettlemoyer, S. Goldwater, andM.
Steedman.
2010.
Inducing probabilistic CCGgrammars from logical form with higher-order uni-fication.
In Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1223?1233.T.
Kwiatkowski, L. Zettlemoyer, S. Goldwater, andM.
Steedman.
2011.
Lexical generalization inCCG grammar induction for semantic parsing.
InEmpirical Methods in Natural Language Processing(EMNLP), pages 1512?1523.T.
Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.2013.
Scaling semantic parsers with on-the-fly on-tology matching.
In Empirical Methods in NaturalLanguage Processing (EMNLP).I.
Lenz, R. Knepper, and A. Saxena.
2015.
Deepmpc:Learning deep latent features for model predictivecontrol.
In Robotics Science and Systems (RSS).S.
Levine, C. Finn, T. Darrell, and P. Abbeel.
2015.End-to-end training of deep visuomotor policies.arXiv preprint arXiv:1504.00702.P.
Liang, M. I. Jordan, and D. Klein.
2009.
Learningsemantic correspondences with less supervision.
InAssociation for Computational Linguistics and In-ternational Joint Conference on Natural LanguageProcessing (ACL-IJCNLP), pages 91?99.1001P.
Liang, M. I. Jordan, and D. Klein.
2011.
Learn-ing dependency-based compositional semantics.
InAssociation for Computational Linguistics (ACL),pages 590?599.C.
Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo,and D. Fox.
2012a.
A joint model of language andperception for grounded attribute learning.
In Inter-national Conference on Machine Learning (ICML),pages 1671?1678.C.
Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox.2012b.
Learning to parse natural language com-mands to a robot control system.
In InternationalSymposium on Experimental Robotics (ISER).D.
Misra, J.
Sung, K. Lee, and A. Saxena.
2014.
TellMe Dave: Context-sensitive grounding of naturallanguage to mobile manipulation instructions.
InRobotics: Science and Systems (RSS).R.
Mooney.
2008.
Learning to connect language andperception.
In Association for the Advancement ofArtificial Intelligence (AAAI), pages 1598?1601.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29:19?51.X.
Ren, L. Bo, and D. Fox.
2012.
Rgb-(d) scene label-ing: Features and algorithms.
In Computer Visionand Pattern Recognition (CVPR), pages 2759?2766.J.
Rintanen.
2012.
Planning as satisfiability: Heuris-tics.
Artificial Intelligence, 193.A.
Saxena, A. Jain, O. Sener, A. Jami, D. K. Misra,and H. S. Koppula.
2014.
Robobrain: Large-scale knowledge engine for robots.
arXiv preprintarXiv:1412.0691.J.
Sung, S. H. Jin, and A. Saxena.
2015.
Robobarista:Object part based transfer of manipulation trajecto-ries from crowd-sourcing in 3d pointclouds.
arXivpreprint arXiv:1504.03071.S.
Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G.Banerjee, S. J. Teller, and N. Roy.
2011.
Un-derstanding natural language commands for roboticnavigation and mobile manipulation.
In Associa-tion for the Advancement of Artificial Intelligence(AAAI).A.
Vogel and D. Jurafsky.
2010.
Learning to follownavigational directions.
In Association for Compu-tational Linguistics (ACL), pages 806?814.C.
Wu, I. Lenz, and A. Saxena.
2014.
Hierarchical se-mantic labeling for task-relevant RGB-D perception.In Robotics: Science and Systems (RSS).H.
Yu and J. M. Siskind.
2013.
Grounded languagelearning from video described with sentences.
InAssociation for Computational Linguistics (ACL),pages 53?63.L.
S. Zettlemoyer and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Un-certainty in Artificial Intelligence (UAI), pages 658?666.L.
S. Zettlemoyer and M. Collins.
2007.
Online learn-ing of relaxed CCG grammars for parsing to log-ical form.
In Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP/CoNLL), pages 678?687.1002
