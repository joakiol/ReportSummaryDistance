Feature Forest Models for ProbabilisticHPSG ParsingYusuke Miyao?University of TokyoJun?ichi Tsujii?
?University of TokyoUniversity of ManchesterProbabilistic modeling of lexicalized grammars is difficult because these grammars exploit com-plicated data structures, such as typed feature structures.
This prevents us from applyingcommon methods of probabilistic modeling in which a complete structure is divided into sub-structures under the assumption of statistical independence among sub-structures.
For example,part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsingis split into applications of CFG rules.
These methods have relied on the structure of the targetproblem, namely lattices or trees, and cannot be applied to graph structures including typed fea-ture structures.This article proposes the feature forest model as a solution to the problem of probabilisticmodeling of complex data structures including typed feature structures.
The feature forest modelprovides a method for probabilistic modeling without the independence assumption when prob-abilistic events are represented with feature forests.
Feature forests are generic data structuresthat represent ambiguous trees in a packed forest structure.
Feature forest models are maximumentropy models defined over feature forests.
A dynamic programming algorithm is proposed formaximum entropy estimation without unpacking feature forests.
Thus probabilistic modeling ofany data structures is possible when they are represented by feature forests.This article also describes methods for representing HPSG syntactic structures andpredicate?argument structures with feature forests.
Hence, we describe a complete strategy fordeveloping probabilistic models for HPSG parsing.
The effectiveness of the proposed methods isempirically evaluated through parsing experiments on the Penn Treebank, and the promise ofapplicability to parsing of real-world sentences is discussed.1.
IntroductionFollowing the successful development of wide-coverage lexicalized grammars (Riezleret al 2000; Hockenmaier and Steedman 2002; Burke et al 2004; Miyao, Ninomiya, and?
Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan.E-mail: yusuke@is.s.u-tokyo.ac.jp.??
Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan.E-mail: tsujii@is.s.u-tokyo.ac.jp.Submission received: 11 June 2006; revised submission received: 2 March 2007; accepted for publication:5 May 2007.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 1Tsujii 2005), statistical modeling of these grammars is attracting considerable attention.This is because natural language processing applications usually require disambiguatedor ranked parse results, and statistical modeling of syntactic/semantic preference is oneof the most promising methods for disambiguation.The focus of this article is the problem of probabilistic modeling of wide-coverageHPSG parsing.
Although previous studies have proposed maximum entropy mod-els (Berger, Della Pietra, and Della Pietra 1996) of HPSG-style parse trees (Oepen,Toutanova, et al 2002b; Toutanova and Manning 2002; Baldridge and Osborne 2003;Malouf and van Noord 2004), the straightforward application of maximum entropymodels to wide-coverage HPSG parsing is infeasible because estimation of maximumentropymodels is computationally expensive, especially when targeting wide-coverageparsing.
In general, complete structures, such as transition sequences inMarkovmodelsand parse trees, have an exponential number of ambiguities.
This causes an exponentialexplosion when estimating the parameters of maximum entropy models.
We thereforerequire solutions to make model estimation tractable.This article first proposes feature forest models, which are a general solution tothe problem of maximum entropy modeling of tree structures (Miyao and Tsujii 2002).Our algorithm avoids exponential explosion by representing probabilistic events withfeature forests, which are packed representations of tree structures.
When completestructures are represented with feature forests of a tractable size, the parameters ofmaximum entropy models are efficiently estimated without unpacking the featureforests.
This is due to dynamic programming similar to the algorithm for computinginside/outside probabilities in PCFG parsing.The latter half of this article (Section 4) is on the application of feature forestmodels to disambiguation in wide-coverage HPSG parsing.
We describe methods forrepresenting HPSG parse trees and predicate?argument structures using feature forests(Miyao, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005).
Together with theparameter estimation algorithm for feature forest models, these methods constitute acomplete procedure for the probabilistic modeling of wide-coverage HPSG parsing.The methods we propose here were applied to an English HPSG parser, Enju (TsujiiLaboratory 2004).
We report on an extensive evaluation of the parser through parsingexperiments on theWall Street Journal portion of the Penn Treebank (Marcus et al 1994).The content of this article is an extended version of our earlier work reportedin Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003).
Themajor contribution of this article is a strict mathematical definition of the feature forestmodel and the parameter estimation algorithm, which are substantially refined andextended from Miyao and Tsujii (2002).
Another contribution is that this article thor-oughly discusses the relationships between the feature forest model and its applicationto HPSG parsing.
We also provide an extensive empirical evaluation of the resultingHPSG parsing approach using real-world text.Section 2 discusses a problem of conventional probabilistic models for lexicalizedgrammars.
Section 3 proposes feature forest models for solving this problem.
Section 4describes the application of feature forest models to probabilistic HPSG parsing.
Sec-tion 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6introduces research related to our proposals.
Section 7 concludes.2.
ProblemMaximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now be-coming the de facto standard approach for disambiguation models for lexicalized or36Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsingfeature structure grammars (Johnson et al 1999; Riezler et al 2000, 2002; Geman andJohnson 2002; Clark and Curran 2003, 2004b; Kaplan et al 2004; Carroll and Oepen2005).
Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al 2002;Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord2004) have also adopted log-linear models.
This is because these grammar formalismsexploit feature structures to represent linguistic constraints.
Such constraints are knownto introduce inconsistencies in probabilistic models estimated using simple relativefrequency, as discussed in Abney (1997).
The maximum entropy model is a reasonablechoice for credible probabilistic models.
It also allows various overlapping features tobe incorporated, and we can expect higher accuracy in disambiguation.A maximum entropy model gives a probabilistic distribution that maximizes thelikelihood of training data under given feature functions.
Given training data E ={?x, y?
}, a maximum entropy model gives conditional probability p(y|x) as follows.Definition 1 (Maximum entropy model)A maximum entropy model is defined as the solution of the following optimizationproblem.pM(y|x) = argmaxp??????x,y??Ep?
(x, y) log p(y|x)??
?where:p(y|x) = 1Z(x)exp(?i?i fi(x, y))Z(x) =?y?Y(x)exp(?i?i fi(x, y))In this definition, p?
(x, y) is the relative frequency of ?x, y?
in the training data.
fi is afeature function, which represents a characteristic of probabilistic events by mappingan event into a real value.
?i is the model parameter of a corresponding feature functionfi, and is determined so as to maximize the likelihood of the training data (i.e., theoptimization in this definition).
Y(x) is a set of y for given x; for example, in parsing, x isa given sentence and Y(x) is a parse forest for x.
An advantage of maximum entropymodels is that feature functions can represent any characteristics of events.
That is,independence assumptions are unnecessary for the design of feature functions.
Hence,this method provides a principled solution for the estimation of consistent probabilisticdistributions over feature structure grammars.The remaining issue is how to estimate parameters.
Several numerical algorithms,such as Generalized Iterative Scaling (GIS) (Darroch and Ratcliff 1972), ImprovedIterative Scaling (IIS) (Della Pietra, Della Pietra, and Lafferty 1997), and the Limited-memory Broyden-Fletcher-Goldfarb-Shanno method (L-BFGS) (Nocedal and Wright1999), have been proposed for parameter estimation.
Although the algorithm proposedin the present article is applicable to all of the above algorithms, we used L-BFGS forexperiments.However, a computational problem arises in these parameter estimation algo-rithms.
The size of Y(x) (i.e., the number of parse trees for a sentence) is generally37Computational Linguistics Volume 34, Number 1very large.
This is because local ambiguities in parse trees potentially cause exponentialgrowth in the number of structures assigned to sub-sequences of words, resulting inbillions of structures for whole sentences.
For example, when we apply rewriting ruleS ?
NP VP, and the left NP and the right VP, respectively, have n and m ambiguoussubtrees, the result of the rule application generates n?m trees.This is problematic because the complexity of parameter estimation is proportionalto the size of Y(x).
The cost of the parameter estimation algorithms is bound by thecomputation ofmodel expectation, ?i, given as (Malouf 2002):?i =?x?Xp?
(x)?y?Y(x)fi(x, y)p(y|x)=?x?Xp?
(x)?y?Y(x)fi(x, y)1Z(x)exp??
?j?j fj(x, y)??
(1)As shown in this definition, the computation of model expectation requires the summa-tion over Y(x) for every x in the training data.
The complexity of the overall estimationalgorithm is O( ?|Y| ?|F||E|), where ?|Y| and ?|F| are the average numbers of y and activatedfeatures for an event, respectively, and |E| is the number of events.
When Y(x) growsexponentially, the parameter estimation becomes intractable.In PCFGs, the problem of computing probabilities of parse trees is avoided by usinga dynamic programming algorithm for computing inside/outside probabilities (Baker1979).
With the algorithm, the computation becomes tractable.
We can expect that thesame approach would be effective for maximum entropy models as well.This notion yields a novel algorithm for parameter estimation for maximum en-tropy models, as described in the next section.3.
Feature Forest ModelOur solution to the problem is a dynamic programming algorithm for computinginside/outside ?-products.
Inside/outside ?-products roughly correspond to inside/outside probabilities in PCFGs.
In maximum entropy models, a probability is definedas a normalized product of ?fjj (= exp(?j fj)).
Hence, similar to the algorithm of computinginside/outside probabilities, we can compute exp(?j ?j fj), which we define as the?-product, for each node in a tree structure.
If we can compute ?-products at a tractablecost, the model expectation ?i is also computed at a tractable cost.We first define the notion of a feature forest, a packed representation of a setof an exponential number of tree structures.
Feature forests correspond to packedcharts in CFG parsing.
Because feature forests are generalized representations of foreststructures, the notion is not only applicable to syntactic parsing but also to sequencetagging, such as POS tagging and named entity recognition (which will be discussed inSection 6).
We then define inside/outside ?-products that represent the ?-products ofpartial structures of a feature forest.
Inside?-products correspond to inside probabilitiesin PCFG, and represent the summation of ?-products of the daughter sub-trees.
Outside?-products correspond to outside probabilities in PCFG, and represent the summationof ?-products in the upper part of the feature forest.
Both can be computed incre-mentally by a dynamic programming algorithm similar to the algorithm for computing38Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsinginside/outside probabilities in PCFG.
Given inside/outside ?-products of all nodes ina feature forest, the model expectation ?i is easily computed by multiplying them foreach node.3.1 Feature ForestTo describe the algorithm, we first define the notion of a feature forest, the generalizedrepresentation of features in a packed forest structure.
Feature forests are used forenumerating possible structures of events, that is, they correspond to Y(x) in Equation 1.Definition 2 (Feature forest)A feature forest ?
is a tuple ?C,D, r,?, ?
?, where: C is a set of conjunctive nodes, D is a set of disjunctive nodes, r is the root node: r ?
C, ?
: D ?
2C is a conjunctive daughter function, ?
: C ?
2D is a disjunctive daughter function.We denote a feature forest for x as ?(x).
For example, ?
(x) can represent the set of allpossible tag sequences of a given sentence x, or the set of all parse trees of x.
A featureforest is an acyclic graph, and unpacked structures extracted from a feature forest aretrees.
We also assume that terminal nodes of feature forests are conjunctive nodes.
Thatis, disjunctive nodes must have daughters (i.e., ?
(d)= ?
for all d ?
D).A feature forest represents a set of trees of conjunctive nodes in a packed structure.Conjunctive nodes correspond to entities such as states in Markov chains and nodesin CFG trees.
Feature functions are assigned to conjunctive nodes and express theircharacteristics.
Disjunctive nodes are for enumerating alternative choices.
Conjunctive/disjunctive daughter functions represent immediate relations of conjunctive and dis-junctive nodes.
By selecting a conjunctive node as a child of each disjunctive node, wecan extract a tree consisting of conjunctive nodes from a feature forest.Figure 1 shows an example of a feature forest.
Each disjunctive node enumeratesalternative nodes, which are conjunctive nodes.
Each conjunctive node has disjunctiveFigure 1A feature forest.39Computational Linguistics Volume 34, Number 1Figure 2Unpacked trees.nodes as its daughters.
The feature forest in Figure 1 represents a set of 2?
2?
2 = 8unpacked trees shown in Figure 2.
For example, by selecting the left-most conjunctivenode at each disjunctive node, we extract an unpacked tree (c1, c2, c4, c6).
An unpackedtree is represented as a set of conjunctive nodes.
Generally, a feature forest representsan exponential number of trees with a polynomial number of nodes.
Thus, completestructures, such as tag sequences and parse trees with ambiguities, can be representedin a tractable form.Feature functions are defined over conjunctive nodes.1Definition 3 (Feature function for feature forests)A feature function for a feature forest is:fi : C ?
RHence, together with feature functions, a feature forest represents a set of trees offeatures.Feature forests may be regarded as a packed chart in CFG parsing.
Although featureforests have the same structure as PCFG parse forests, nodes in feature forests do notnecessarily correspond to nodes in PCFG parse forests.
In fact, in Sections 4.2 and 4.3, wewill demonstrate that syntactic structures and predicate?argument structures in HPSGcan be represented with tractable-size feature forests.
The actual interpretation of a nodein a feature forest may thus be ignored in the following discussion.
Our algorithm isapplicable whenever feature forests are of a tractable size.
The descriptive power offeature forests will be discussed again in Section 6.Asmentioned, a feature forest is a packed representation of trees of features.We firstdefine model expectations, ?i, on a set of unpacked trees, and then show that they canbe computed without unpacking feature forests.
We denote an unpacked tree as a set,c ?
C, of conjunctive nodes.
Our concern is only the set of features associated with eachconjunctive node, and the shape of the tree structure is irrelevant to the computation ofprobabilities of unpacked trees.
Hence, we do not distinguish an unpacked tree from aset of conjunctive nodes.The collection of unpacked trees represented by a feature forest is defined as amulti-set of unpacked trees because we allow multiple occurrences of equivalent unpacked1 Feature functions may also be conditioned on x.
In this case, feature functions can be written as fi(c, x).For simplicity, we omit x in the following discussion.40Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsingtrees in a feature forest.2 Given multisets of unpacked trees, A,B, we define the unionand the product as follows.A?
B ?
A ?
BA?
B ?
{a ?
b|a ?
A,b ?
B}Intuitively, the first operation is a collection of trees, and the second lists all combina-tions of trees in A and B.
It is trivial that they satisfy commutative, associative, anddistributive laws.A?
B = B?
AA?
B = B?
AA?
(B?
C) = (A?
B)?
CA?
(B?
C) = (A?
B)?
CA?
(B?
C) = (A?
B)?
(A?
C)We denote a set of unpacked trees rooted at node n ?
C ?D as ?(n).
?
(n) is de-fined recursively.
For a terminal node c ?
C, obviously ?
(c) = {{c}}.
For an internalconjunctive node c ?
C, an unpacked tree is a combination of trees, each of which isselected from a disjunctive daughter.
Hence, a set of all unpacked trees is representedas a product of trees from disjunctive daughters.?
(c) = {{c}} ??d??(c)?
(d)A disjunctive node d ?
D represents alternatives of packed trees, and obviously a setof its unpacked trees is represented as a union of the daughter trees, that is, ?
(d) =?c??(d)?
(c).To summarize, a set of unpacked trees is defined formally as follows.Definition 4 (Unpacked tree)Given a feature forest ?
= ?C,D, r,?, ?
?, a set ?
(n) of unpacked trees rooted at noden ?
C ?D is defined recursively as follows. If n ?
C is a terminal, that is, ?
(n) = ?,?
(n) ?
{{n}} If n ?
C,?
(n) ?
{{n}} ??d??(n)?
(d)2 In fact, no feature forests include equivalent unpacked trees if no disjunctive nodes have identicaldaughter nodes.
Thus we may define a set of unpacked trees as an ordinary set, although the detailsare omitted here for simplicity.41Computational Linguistics Volume 34, Number 1 If n ?
D,?
(n) ??c??(n)?
(c)Feature forests are directed acyclic graphs and, as such, this definition does not includea loop.
Hence, ?
(n) is properly defined.A set of all unpacked trees is then represented by ?
(r); henceforth, we denote ?
(r)as ?(?
), or just ?when it is not confusing in context.
Figure 3 shows ?(?)
of the featureforest in Figure 1.
Following Definition 4, the first element of each set is the root node,c1, and the rest are elements of the product of {c2, c3}, {c4, c5}, and {c6, c7}.
Each set inFigure 3 corresponds to a tree in Figure 2.Given this formalization, the feature function for an unpacked tree is defined asfollows.Definition 5 (Feature function for unpacked tree)The feature function fi for an unpacked tree, c ?
?(?)
is defined as:fi(c) =?c?cfi(c)Because c ?
?(?)
corresponds to y of the conventional maximum entropy model, thisfunction substitutes for fi(x, y) in the conventional model.
Once a feature function foran unpacked tree is given, a model expectation is defined as in the traditional model.Definition 6 (Model expectation of feature forests)The model expectation ?i for a set of feature forests {?
(x)} is defined as:?i =?x?Xp?(x)?c??(?(x))fi(c)p(c|x)=?x?Xp?(x)?c??(?(x))fi(c)1Z(x)exp??
?j?j fj(c)?
?where Z(x) =?c??(?(x))exp??
?j?j fj(c)?
?It is evident that the naive computation of model expectations requires exponentialtime complexity because the number of unpacked trees (i.e., |?(?
)|) is exponentiallyrelated to the number of nodes in the feature forest ?.
We therefore need an algorithmfor computing model expectations without unpacking a feature forest.Figure 3Unpacked trees represented as sets of conjunctive nodes.42Miyao and Tsujii Feature Forest Models for Probabilistic HPSG ParsingFigure 4Inside/outside at node c2 in a feature forest.3.2 Dynamic ProgrammingTo efficiently compute model expectations, we incorporate an approach similar to thedynamic programming algorithm for computing inside/outside probabilities in PCFGs.We first define the notion of inside/outside of a feature forest.
Figure 4 illustrates thisconcept, which is similar to the analogous concept in PCFGs.3 Inside denotes a set ofpartial trees (sets of conjunctive nodes) derived from node c2.
Outside denotes a set ofpartial trees that derive node c2.
That is, outside trees are partial trees of complementsof inside trees.We denote a set of inside trees at node n as ?
(n), and that of outside trees as o(n).Definition 7 (Inside trees)We define a set ?
(n) of inside trees rooted at node n ?
C ?D as a set of unpacked treesrooted at n.?
(n) ?
?
(n)Definition 8 (Outside trees)We define a set o(n) of outside trees rooted at node n ?
C ?D as follows.o(r) ?
{?
}o(c) ??d??
?1(c)o(d)o(d) ??c???1(d)???
{{c}} ?
o(c)??d???(c),d?
=d?(d?)??
?3 A node may have multiple outside trees in general as in the case of CFGs, although Figure 4 shows onlyone outside tree of c2 for simplicity.43Computational Linguistics Volume 34, Number 1In the definition, ?
?1 and ?
?1 denote mothers of conjunctive and disjunctive nodes,respectively.
Formally,?
?1(c) ?
{d|c ?
?(d)}?
?1(d) ?
{c|d ?
?
(c)}Next, inside/outside ?-products are defined for conjunctive and disjunctive nodes.The inside (or outside) ?-products are the summation of exp(?j ?j fj(c))of all inside (oroutside) trees c.Definition 9 (Inside/outside ?-product)An inside ?-product at conjunctive node c ?
C is?c =?c??(c)exp??
?j?j fj(c)?
?An outside ?-product is?c =?c?o(c)exp??
?j?j fj(c)?
?Similarly, inside/outside ?-products at disjunctive node d ?
D are defined as follows:?d =?c??(d)exp??
?j?j fj(c)??
?d =?c?o(d)exp??
?j?j fj(c)?
?We can derive that the model expectations of a feature forest are computed as theproduct of the inside and outside ?-products.Theorem 1 (Model expectation of feature forests)The model expectation ?i of a feature forest?
(x) = ?Cx,Dx, rx,?x, ?x?
is computed as theproduct of inside and outside ?-products as follows:?i =?x?Xp?
(x) 1Z(x)?c?Cxfi(c)?c?cwhere Z(x) = ?rx44Miyao and Tsujii Feature Forest Models for Probabilistic HPSG ParsingFigure 5Incremental computation of inside ?-products at conjunctive node c2.Figure 6Incremental computation of inside ?-products at disjunctive node d4.This equation shows a method for efficiently computing model expectations bytraversing conjunctive nodes without unpacking the forest, if the inside/outside?-products are given.
The remaining issue is how to efficiently compute inside/outside?-products.Fortunately, inside/outside ?-products can be incrementally computed by dynamicprogramming without unpacking feature forests.
Figure 5 shows the process of com-puting the inside ?-product at a conjunctive node from the inside ?-products of itsdaughter nodes.
Because the inside of a conjunctive node is a set of the combinations ofall of its descendants, the ?-product is computed by multiplying the ?-products of thedaughter trees.
The following equation is derived.
?c =???d??(c)?d??
exp??
?j?j fj(c)?
?The inside of a disjunctive node is the collection of the inside trees of its daughter nodes.Hence, the inside ?-product at disjunctive node d ?
D is computed as follows (Figure 6).
?d =?c??
(d)?c45Computational Linguistics Volume 34, Number 1Theorem 2 (Inside ?-product)The inside ?-product ?c at a conjunctive node c is computed by the following equationif ?d is given for all daughter disjunctive nodes d ?
?(c).
?c =???d??(c)?d??
exp??
?j?j fj(c)?
?The inside ?-product ?d at a disjunctive node d is computed by the following equationif ?c is given for all daughter conjunctive nodes c ?
?(d).
?d =?c??
(d)?cThe outside of a disjunctive node is equivalent to the outside of its daughter nodes.Hence, the outside ?-product of a disjunctive node is propagated to its daughter con-junctive nodes (Figure 7).
?c =?{d|c??
(d)}?dThe computation of the outside ?-product of a disjunctive node is somewhat com-plicated.
As shown in Figure 8, the outside trees of a disjunctive node are all com-binations of the outside trees of the mother nodes, and the inside trees of the sister nodes.Figure 7Incremental computation of outside ?-products at conjunctive node c2.46Miyao and Tsujii Feature Forest Models for Probabilistic HPSG ParsingFigure 8Incremental computation of outside ?-products at disjunctive node d4.From this, we find:?d =?{c|d??(c)}???????
?c exp??
?j?j fj(c)???d???(c)d?
=d?d???????
?We finally find the following theorem for the computation of outside ?-products.Theorem 3 (Outside ?-product)The outside ?-product ?c at conjunctive node c is computed by the following equationif ?d is given for all mother disjunctive nodes, that is, all d such that c ?
?(d).
?c =?{d|c??
(d)}?dThe outside ?-product ?d at disjunctive node d is computed by the following equationif ?c is given for all mother conjunctive nodes, that is, all c such that d ?
?
(c), and ?d?for all sibling disjunctive nodes d?.
?d =?{c|d??(c)}???????
?c exp??
?j?j fj(c)???d???(c)d?
=d?d???????
?Figure 9 shows the overall algorithm for estimating the parameters, given a setof feature forests.
The key point of the algorithm is to compute inside ?-products ?and outside ?-products ?
for each node in C, and not for all unpacked trees.
The func-tions inside product and outside product compute ?
and ?
efficiently by dynamicprogramming.Note that the order in which nodes are traversed is important for incremental com-putation, although it is not shown in Figure 9.
The computation for the daughternodes and mother nodes must be completed before computing the inside and outside47Computational Linguistics Volume 34, Number 1Figure 9Algorithm for computing model expectations of feature forests.
?-products, respectively.
This constraint is easily solved using any topological sortalgorithm.
A topological sort is applied once at the beginning.
The result of the sortingdoes not affect the cost and the result of estimation.
In our implementation, we assumethat conjunctive/disjunctive nodes are already ordered from the root node in input data.The complexity of this algorithm is O(( ?|C|+ ?|D|) ?|F||E|), where ?|C| and ?|D| are theaverage numbers of conjunctive and disjunctive nodes, respectively.
This is tractablewhen ?|C| and ?|D| are of a reasonable size.
As noted in this section, the number of48Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsingnodes in a feature forest is usually polynomial even when that of the unpacked treesis exponential.
Thus we can efficiently compute model expectations with polynomialcomputational complexity.4.
Probabilistic HPSG ParsingFollowing previous studies on probabilistic models for HPSG (Oepen, Toutanova, et al2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and vanNoord 2004), we apply a maximum entropy model to HPSG parse disambiguation.
Theprobability, p(t|w), of producing parse result t of a given sentence w is defined asp(t|w) = 1Zwp0(t|w) exp(?i?i fi(t,w))whereZw =?t?
?T(w)p0(t?|w) exp(?i?i fi(t?,w))where p0(t|w) is a reference distribution (usually assumed to be a uniform distribution)and T(w) is a set of parse candidates assigned to w. The feature function fi(t,w) rep-resents the characteristics of t and w, and the corresponding model parameter ?i is itsweight.
Model parameters that maximize the log-likelihood of the training data arecomputed using a numerical optimization method (Malouf 2002).Estimation of the model requires a set of pairs ?tw,T(w)?, where tw is the correctparse for a sentencew.
Whereas tw is provided by a treebank, T(w) has to be computedby parsing each w in the treebank.
Previous studies assumed T(w) could be enumer-ated; however, this assumption is impractical because the size of T(w) is exponentiallyrelated to the length of w.Our solution here is to apply the feature forest model of Section 3 to the probabilisticmodeling of HPSG parsing.
Section 4.1 briefly introduces HPSG.
Section 4.2 and 4.3describe how to represent HPSG parse trees and predicate?argument structures byfeature forests.
Together with the parameter estimation algorithm in Section 3, thesemethods constitute a complete method for probabilistic disambiguation.
We also ad-dress a method for accelerating the construction of feature forests for all treebanksentences in Section 4.4.
The design of feature functions will be given in Section 4.5.4.1 HPSGHPSG (Pollard and Sag 1994; Sag,Wasow, and Bender 2003) is a syntactic theory that fol-lows the lexicalist framework.
In HPSG, linguistic entities, such as words and phrases,are denoted by signs, which are represented by typed feature structures (Carpenter1992).
Signs are a formal representation of combinations of phonological forms andsyntactic/semantic structures, and express which phonological form signifies whichsyntactic/semantic structure.
Figure 10 shows the lexical sign for loves.
The geometryof signs follows Pollard and Sag: HEAD represents the part-of-speech of the head word,MOD denotes modifiee constraints, and SPR, SUBJ, and COMPS describe constraintsof a specifier, a syntactic subject, and complements, respectively.
CONT denotes the49Computational Linguistics Volume 34, Number 1Figure 10Lexical entry for the transitive verb loves.Figure 11Simplified representation of the lexical entry in Figure 10.predicate?argument structure of a phrase/sentence.
The notation of CONT in this articleis borrowed from that of Minimal Recursion Semantics (Copestake et al 2006): HOOKrepresents a structure accessed by other phrases, and RELS describes the remainingstructure of the semantics.
In what follows, we represent signs in a reduced form asshown in Figure 11, because of the large size of typical HPSG signs, which often includeinformation not immediately relevant to the point being discussed.
We will only showattributes that are relevant to an explanation, expecting that readers can fill in the valuesof suppressed attributes.50Miyao and Tsujii Feature Forest Models for Probabilistic HPSG ParsingIn our actual implementation of the HPSG grammar, lexical/phrasal signs containadditional attributes that are not defined in the standard HPSG theory but are usedby a disambiguation model.
Examples include the surface form of lexical heads, andthe type of lexical entry assigned to lexical heads, which are respectively used forcomputing the features WORD and LE introduced in Section 4.5.
By incorporating ad-ditional attributes into signs, we can straightforwardly compute feature functions foreach sign.
This allows for a simple mapping between a parsing chart and a feature forestas described subsequently.
However, this might increase the size of parse forests andtherefore decrease parsing efficiency, because differences between additional attributesinterfere with equivalence relations for ambiguity packing.4.2 Packed Representation of HPSG Parse TreesWe represent an HPSG parse tree with a set of tuples ?m, l, r?, where m, l, and r are thesigns of the mother, left daughter, and right daughter, respectively.4 In chart parsing,partial parse candidates are stored in a chart, in which phrasal signs are identified andpacked into equivalence classes if they are judged to be equivalent and dominate thesameword sequences.
A set of parse trees is then represented as a set of relations amongequivalence classes.5Figure 12 shows a chart for parsing he saw a girl with a telescope, where the modifieeof with is ambiguous (saw or girl).
Each feature structure expresses an equivalence class,and the arrows represent immediate-dominance relations.
The phrase, saw a girl witha telescope, has two trees (A in the figure).
Because the signs of the top-most nodes areequivalent, they are packed into an equivalence class.
The ambiguity is represented asthe two pairs of arrows leaving the node A.A set of HPSG parse trees is represented in a chart as a tuple ?E,Er,?
?, where E is aset of equivalence classes, Er ?
E is a set of root nodes, and ?
: E?
2E?E is a functionto represent immediate-dominance relations.Our representation of a chart can be interpreted as an instance of a feature forest.We map the tuple ?em, el, er?, which corresponds to ?m, l, r?, into a conjunctive node.Figure 13 shows (a part of) the HPSG parse trees in Figure 12 represented as a featureforest.
Square boxes (ci) are conjunctive nodes, and di disjunctive nodes.
A solid arrowrepresents a disjunctive daughter function, and a dotted line expresses a conjunctivedaughter function.Formally, a chart ?E,Er,??
is mapped into a feature forest ?C,D,R,?, ??
as follows.6 C = {?em, el, er?|em ?
E ?
(el, er) ?
?
(em)} ?
{w|w ?
w} D = E R = {?em, el, er?|em ?
Er ?
?em, el, er?
?
C}4 For simplicity, only binary trees are considered.
Extension to unary and n-ary (n > 2) trees is trivial.5 We assume that CONT and DTRS (a feature used to represent daughter signs) are restricted (Shieber 1985),and we will discuss a method for encoding CONT in a feature forest in Section 4.3.
We also assume thatparse trees are packed according to equivalence relations rather than subsumption relations (Oepen andCarroll 2000).
We cannot simply map parse forests packed under subsumption into feature forests,because they over-generate possible unpacked trees.6 For ease of explanation, the definition of the root node is different from the original definition givenin Section 3.
In this section, we define R as a set of conjunctive nodes rather than a single node r. Thedefinition here is translated into the original definition by introducing a dummy root node r?
that hasno features and only one disjunctive daughter whose daughters are R.51Computational Linguistics Volume 34, Number 1Figure 12Chart for parsing he saw a girl with a telescope.Figure 13Feature forest representation of HPSG parse trees in Figure 12.52Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing ?
(em) ={{?em, el, er?|(el, er) ?
?
(em)} if ?
(em)= ?
{w|em is a lexical entry for w} otherwise ?
(c) ={{el, er} if c = ?em, el, er??
if c ?
wOnemay claim that restricting the domain of feature functions to ?em, el, er?
limits theflexibility of feature design.
Although this is true to some extent, it does not necessarilymean the impossibility of incorporating features on nonlocal dependencies into themodel.
This is because a feature forest model does not assume probabilistic indepen-dence of conjunctive nodes.
This means that we can unpack a part of the forest withoutchanging the model.
Actually, we successfully developed a probabilistic model includ-ing features on nonlocal predicate?argument dependencies, as described subsequently.4.3 Packed Representation of Predicate?Argument StructuresWith the method previously described, we can represent an HPSG parsing chart witha feature forest.
However, equivalence classes in a chart might increase exponentiallybecause predicate?argument structures in HPSG signs represent the semantic relationsof all words that the phrase dominates.
For example, Figure 14 shows phrasal signs withpredicate?argument structures for saw a girl with a telescope.
In the chart in Figure 12,these signs are packed into an equivalence class.
However, Figure 14 shows that thevalues of CONT, that is, predicate?argument structures, have different values, and thesigns as they are cannot be equivalent.
As seen in this example, predicate?argumentstructures prevent us from packing signs into equivalence classes.In this section, we apply the feature forest model to predicate?argument structures,which may include reentrant structures and non-local dependencies.
It is theoreticallydifficult to apply the feature forest model to predicate?argument structures; a featureforest cannot represent graph structures that include reentrant structures in a straight-forward manner.
However, if predicate?argument structures are constructed as in themanner described subsequently, they can be represented by feature forests of a tracta-ble size.Feature forests can represent predicate?argument structures if we assume somelocality and monotonicity in the composition of predicate?argument structures.Locality: In each step of composition of a predicate?argument structure, only alimited depth of the daughters?
predicate?argument structures are referred to.That is, local structures in the deep descendent phrases may be ignored toconstruct larger phrases.
This assumption means that predicate?argumentstructures can be packed into conjunctive nodes by ignoring local structures.Figure 14Signs with predicate?argument structures.53Computational Linguistics Volume 34, Number 1Monotonicity: All relations in the daughters?
predicate?argument structuresare percolated to the mother.
That is, none of the predicate?argumentrelations in the daughter phrases disappear in the mother.
Thuspredicate?argument structures of descendent phrases can be located atlower nodes in a feature forest.Predicate?argument structures usually satisfy the above conditions, evenwhen theyinclude non-local dependencies.
For example, Figure 15 shows HPSG lexical entriesfor the wh-extraction of the object of love (left) and for the control construction of try(right).
The first condition is satisfied because both lexical entries refer to CONT|HOOKof argument signs in SUBJ, COMPS, and SLASH.
None of the lexical entries directlyaccess ARGX of the arguments.
The second condition is also satisfied because the valuesof CONT|HOOK of all of the argument signs are percolated to ARGX of the mother.
Inaddition, the elements in CONT|RELS are percolated to the mother by the Semantic Prin-ciple.
Compositional semantics usually satisfies the above conditions, including MRS(Copestake et al 1995, 2006).
The composition of MRS refers to HOOK, and no internalstructures of daughters.
The Semantic Principle of MRS also assures that all semanticrelations in RELS are percolated to the mother.
When these conditions are satisfied,semantics may include any constraints, such as selectional restrictions, although thegrammar we used in the experiments does not include semantic restrictions to constrainparse forests.Under these conditions, local structures of predicate?argument structures are en-coded into a conjunctive node when the values of all of its arguments have beeninstantiated.
We introduce the notion of inactives to denote such local structures.Definition 10 (Inactives)An inactive is a subset of predicate?argument structures in which all arguments havebeen instantiated.Because inactive parts will not change during the rest of the parsing process, they canbe placed in a conjunctive node.
By placing newly generated inactives into correspond-ing conjunctive nodes, a set of predicate?argument structures can be represented in afeature forest by packing local ambiguities, and non-local dependencies are preserved.Figure 16 illustrates a process of parsing the sentence She ignored the fact that I wantedto dispute, where dispute has an ambiguity (dispute1, intransitive, and dispute2, transitive)Figure 15Lexical entries including non-local relations.54Miyao and Tsujii Feature Forest Models for Probabilistic HPSG ParsingFigure 16Process of composing predicate?argument structures.Figure 17Predicate?argument structures of dispute.and factmay optionally take a complementizer phrase.7 The predicate?argument struc-tures for dispute1 and dispute2 are shown in Figure 17.
Curly braces express the am-biguities of partially constructed predicate?argument structures.
The resulting featureforest is shown in Figure 18.
The boxes denote conjunctive nodes and dx representdisjunctive nodes.The clause I wanted to dispute has two possible predicate?argument structures: onecorresponding to dispute1 (?
in Figure 16) and the other corresponding to dispute2 (?in Figure 16).
The nodes of the predicate?argument structure ?
are all instantiated, thatis, it contains only inactives.
The corresponding conjunctive node (??
in Figure 18) hastwo inactives, for want and dispute1.
The other structure ?
has an unfilled object in theargument (ARG28) of dispute2, which will be filled by the non-local dependency.
Hence,the corresponding conjunctive node ??
has only one inactive corresponding to want,and the remaining part that corresponds to dispute2 is passed on for further processing.When we process the phrase the fact that I wanted to dispute, the object of dispute2 is filledby fact (?
in Figure 16), and the predicate?argument structure of dispute2 is then placedinto a conjunctive node (??
in Figure 18).7 In Figure 16, feature structures of different nodes of parse trees are assigned distinct variables, even whenthey are from the same lexical entries.
This is because feature structures are copied during chart parsing.Although these variables are from the same lexical entry, it is copied to several chart items, and hencethere are no structure sharings among them.8 ?
(bottom) represents an uninstantiated value (Carpenter 1992).55Computational Linguistics Volume 34, Number 1Figure 18A feature forest representation of predicate?argument structures.One of the beneficial characteristics of this packed representation is that the rep-resentation is isomorphic to the parsing process, that is, a chart.
Hence, we can assignfeatures of HPSG parse trees to a conjunctive node, together with features of predicate?argument structures.
In Section 5, we will investigate the contribution of features onparse trees and predicate?argument structures to the disambiguation of HPSG parsing.4.4 Filtering by Preliminary DistributionThe method just described is the essence of our solution for the tractable estimationof maximum entropy models on exponentially many HPSG parse trees.
However,the problem of computational cost remains.
Construction of feature forests requiresparsing of all of the sentences in a treebank.
Despite the development of methods toimprove HPSG parsing efficiency (Oepen, Flickinger, et al 2002), exhaustive parsing ofall sentences is still expensive.We assume that computation of parse trees with low probabilities can be omittedin the estimation stage because T(w) can be approximated by parse trees with highprobabilities.
To achieve this, we first prepared a preliminary probabilistic model whoseestimation did not require the parsing of a treebank.
The preliminary model was usedto reduce the search space for parsing a training treebank.The preliminary model in this study is a unigram model, p?
(t|w) =?w?w p(l|w),where w ?
w is a word in the sentence w, and l is a lexical entry assigned to w. Thismodel is estimated by counting the relative frequencies of lexical entries used for w inthe training data.
Hence, the estimation does not require parsing of a treebank.
Actually,we use a maximum entropymodel to compute this probability as described in Section 5.56Miyao and Tsujii Feature Forest Models for Probabilistic HPSG ParsingThe preliminarymodel is used for filtering lexical entries whenwe parse a treebank.Given this model, we restrict the number of lexical entries used to parse a treebank.Witha threshold n for the number of lexical entries and a threshold  for the probability,lexical entries are assigned to a word in descending order of probability, until thenumber of assigned entries exceeds n, or the accumulated probability exceeds .
If thisprocedure does not assign a lexical entry necessary to produce a correct parse (i.e., anoracle lexical entry), it is added to the list of lexical entries.
It should be noted that oraclelexical entries are given by the HPSG treebank.
This assures that the filtering methoddoes not exclude correct parse trees from parse forests.Figure 19 shows an example of filtering the lexical entries assigned to saw.
With  =0.95, four lexical entries are assigned.
Although the lexicon includes other lexical entries,such as a verbal entry taking a sentential complement (p = 0.01 in the figure), they arefiltered out.
Although this method reduces the time required for parsing a treebank, thisapproximation causes bias in the training data and results in lower accuracy.
The trade-off between parsing cost and accuracy will be examined experimentally in Section 5.4.We have several ways to integrate p?
with the estimated model p(t|T(w)).
In theexperiments, we will empirically compare the following methods in terms of accuracyand estimation time.Filtering only: The unigram probability p?
is used only for filtering in training.Product: The probability is defined as the product of p?
and the estimated model p.Reference distribution: p?
is used as a reference distribution of p.Feature function: log p?
is used as a feature function of p. This method has beenshown to be a generalization of the reference distribution method (Johnsonand Riezler 2000).4.5 FeaturesFeature functions in maximum entropy models are designed to capture the characteris-tics of ?em, el, er?.
In this article, we investigate combinations of the atomic features listedFigure 19Filtering of lexical entries for saw.57Computational Linguistics Volume 34, Number 1Table 1Templates for atomic features.RULE name of the applied schemaDIST distance between the head words of the daughtersCOMMA whether a comma exists between daughters and/or inside of daughter phrasesSPAN number of words dominated by the phraseSYM symbol of the phrasal category (e.g., NP, VP)WORD surface form of the head wordPOS part-of-speech of the head wordLE lexical entry assigned to the head wordARG argument label of a predicatein Table 1.
The following combinations are used for representing the characteristics ofbinary/unary schema applications.fbinary =?RULE,DIST,COMMA,SPANl, SYMl, WORDl, POSl, LEl,SPANr, SYMr, WORDr, POSr, LEr?funary = ?RULE,SYM,WORD,POS,LE?where subscripts l and r denote left and right daughters.In addition, the following is used for expressing the condition of the root node ofthe parse tree.froot = ?SYM,WORD,POS,LE?Feature functions to capture predicate?argument dependencies are represented asfollows:fpa =?ARG, DIST, WORDp, POSp, LEp, WORDa, POSa, LEa?where subscripts p and a represent predicate and argument, respectively.Figure 20 shows examples: froot is for the root node, in which the phrase symbolis S and the surface form, part-of-speech, and lexical entry of the lexical head are saw,VBD, and a transitive verb, respectively.
fbinary is for the binary rule application to saw agirl and with a telescope, in which the applied schema is the Head-Modifier Schema, theleft daughter is VP headed by saw, and the right daughter is PP headed by with, whosepart-of-speech is IN and whose lexical entry is a VP-modifying preposition.Figure 21 shows example features for predicate?argument structures.
The figureshows features assigned to the conjunctive node denoted as ??
in Figure 18.
Becauseinactive structures in the node have three predicate?argument relations, three featuresare activated.
The first one is for the relation of want and I, where the label of the relationis ARG1, the distance between the head words is 1, the surface string and the POS of58Miyao and Tsujii Feature Forest Models for Probabilistic HPSG ParsingFigure 20Example features for binary schema application and root condition.Figure 21Example features for predicate?argument structures.the predicate are want and VBD, and those of the argument are I and PRP.
The secondand the third features are for the other two relations.
We may include features on morethan two relations, such as the dependencies among want, I, and dispute, although suchfeatures are not incorporated currently.In our implementation, some of the atomic features are abstracted (i.e., ignored) forsmoothing.
Tables 2, 3, and 4 show the full set of templates of combined features used inthe experiments.
Each row represents the template for a feature function.
A check indi-cates the atomic feature is incorporated, and a hyphen indicates the feature is ignored.59Computational Linguistics Volume 34, Number 1Table 2Feature templates for binary schema (left) and unary schema (right).RULE DIST COMMA SPAN SYM WORD POS LE?
?
??
??
?
??
?
??
??
???
?
??
?????
?
???
??
????
???
?
????
???
?????
????????
?
?
??
??
?
??
?
??
??
?
??
?
????
?
??
?
?
???
?
????
?
????
??
??
????
??
??????
??
?
?????
?
??
?
?RULE SYM WORD POS LE???
?
????
????????
?
??
???
??
???
?????
?
???
??
?
?Table 3Feature templates for root condition.SYM WORD POS LE??
?
???
???????
??
??
??
??
????
?
????
?
?Table 4Feature templates for predicate?argument dependencies.ARG DIST WORD POS LE?
?
?
?
??
?
????
???
????
?
????????
??
??
?
?
???
?
??
??
???????
??????
???
??
?60Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing5.
ExperimentsThis section presents experimental results on the parsing accuracy attained by thefeature forest models.
In all of the following experiments, we use the HPSG grammardeveloped by the method of Miyao, Ninomiya, and Tsujii (2005).
Section 5.1 describeshow this grammarwas developed.
Section 5.2 explains other aspects of the experimentalsettings.
In Sections 5.3 to 5.7, we report results of the experiments on HPSG parsing.5.1 The HPSG GrammarIn the following experiments, we use Enju 2.1 (Tsujii Laboratory 2004), which is a wide-coverage HPSG grammar extracted from the Penn Treebank by the method of Miyao,Ninomiya, and Tsujii (2005).
In this method, we convert the Penn Treebank into anHPSG treebank, and collect HPSG lexical entries from terminal nodes of the HPSGtreebank.
Figure 22 illustrates the process of treebank conversion and lexicon collection.We first convert and fertilize parse trees of the Penn Treebank.
This step identifiessyntactic constructions that require special treatment in HPSG, such as raising/controland long-distance dependencies.
These constructions are then annotated with typedfeature structures so that they conform to the HPSG analysis.
Next, we apply HPSGschemas and principles, and obtain fully specified HPSG parse trees.
This step solvesfeature structure constraints given in the previous step, and fills unspecified constraints.Failures of schema/principle applications indicate that the annotated constraints do notFigure 22Extracting HPSG lexical entries from the Penn Treebank.61Computational Linguistics Volume 34, Number 1conform to the HPSG analysis, and require revisions.
Finally, we obtain lexical entriesfrom the HPSG parse trees.
The terminal nodes of HPSG parse trees are collected, andthey are generalized by removing word-specific or context-specific constraints.An advantage of this method is that a wide-coverage HPSG lexicon is obtainedbecause lexical entries are extracted from real-world sentences.
Obtained lexical entriesare guaranteed to construct well-formed HPSG parse trees because HPSG schemasand principles are successfully applied during the development of the HPSG treebank.Another notable feature is that we can additionally obtain an HPSG treebank, whichcan be used as training data for disambiguation models.
In the following experiments,this HPSG treebank is used for the training of maximum entropy models.The lexicon used in the following experiments was extracted from Sections 02?21of the Wall Street Journal portion of the Penn Treebank.
This lexicon can assign correctlexical entries to 99.09% of words in the HPSG treebank converted from Penn TreebankSection 23.
This number expresses ?lexical coverage?
in the strong sense defined byHockenmaier and Steedman (2002).
In this notion of ?coverage,?
this lexicon has 84.1%sentential coverage, where this means that the lexicon can assign correct lexical entriesto all of the words in a sentence.
Although the parser might produce parse results foruncovered sentences, these parse results cannot be completely correct.5.2 Experimental SettingsThe data for the training of the disambiguation models was the HPSG treebank derivedfrom Sections 02?21 of the Wall Street Journal portion of the Penn Treebank, that is, thesame set used for lexicon extraction.
For training of the disambiguation models, weeliminated sentences of 40 words or more and sentences for which the parser could notproduce the correct parses.
The resulting training set consists of 33,604 sentences (whenn = 10 and  = 0.95; see Section 5.4 for details).
The treebanks derived from Sections22 and 23 were used as the development and final test sets, respectively.
Followingprevious studies on parsing with PCFG-based models (Collins 1997; Charniak 2000),accuracy is measured for sentences of less than 40 words and for those with less than100 words.
Table 5 shows the specifications of the test data.The measure for evaluating parsing accuracy is precision/recall of predicate?argument dependencies output by the parser.
A predicate?argument dependency isdefined as a tuple ?wh,wn,?,?
?, where wh is the head word of the predicate, wn is thehead word of the argument, ?
is the type of the predicate (e.g., adjective, intransitiveverb), and ?
is an argument label (MODARG, ARG1, .
.
., ARG4).
For example, He triedrunning has three dependencies as follows: ?tried, he, transitive verb,ARG1?Table 5Specification of test data for the evaluation of parsing accuracy.No.
of Sentences Avg.
LengthTest set (Section 23, < 40 words) 2,144 20.52Test set (Section 23, < 100 words) 2,299 22.23Development set (Section 22, < 40 words) 1,525 20.69Development set (Section 22, < 100 words) 1,641 22.4362Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing ?tried, running, transitive verb,ARG2? ?running, he, intransitive verb,ARG1?Labeled precision/recall (LP/LR) is the ratio of tuples correctly identified by the parser,and unlabeled precision/recall (UP/UR) is the ratio of wh and wn correctly identifiedregardless of ?
and ?.
F-score is the harmonic mean of LP and LR.
Sentence accuracyis the exact match accuracy of complete predicate?argument relations in a sentence.These measures correspond to those used in other studies measuring the accuracy ofpredicate?argument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al 2004),although exact figures cannot be compared directly because the definitions of depen-dencies are different.
All predicate?argument dependencies in a sentence are the targetof evaluation except quotation marks and periods.
The accuracy is measured by parsingtest sentences with gold-standard part-of-speech tags from the Penn Treebank unlessotherwise noted.The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and itshyper-parameter was tuned for each model to maximize F-score for the developmentset.
The algorithm for parameter estimation was the limited-memory BFGS method(Nocedal 1980; Nocedal and Wright 1999).
The parser was implemented in C++ withthe LiLFeS library (Makino et al 2002), and various speed-up techniques for HPSGparsing were used such as quick check and iterative beam search (Tsuruoka, Miyao, andTsujii 2004; Ninomiya et al 2005).
Other efficient parsing techniques, including globalthresholding, hybrid parsing with a chunk parser, and large constituent inhibition, werenot used.
The results obtained using these techniques are given in Ninomiya et al Alimit on the number of constituents was set for time-out; the parser stopped parsingwhen the number of constituents created during parsing exceeded 50,000.
In such acase, the parser output nothing, and the recall was computed as zero.Features occurring more than twice were included in the probabilistic models.
Amethod of filtering lexical entries was applied to the parsing of training data (Sec-tion 4.4).
Unless otherwise noted, parameters for filtering were n = 10 and  = 0.95, anda reference distribution method was applied.
The unigram model, p0(t|s), for filtering isa maximum entropy model with two feature templates, ?WORD, POS, LE?
and ?POS, LE?.The model includes 24,847 features.5.3 Efficacy of Feature Forest ModelsTables 6 and 7 show parsing accuracy for the test set.
In the tables, ?Syntactic features?denotes a model with syntactic features, that is, fbinary, funary, and froot introducedTable 6Accuracy of predicate?argument relations (test set, <40 words).LP LR UP UR F-score Sentence acc.Baseline 78.10 77.39 82.83 82.08 77.74 18.3Syntactic features 86.92 86.28 90.53 89.87 86.60 36.3Semantic features 84.29 83.74 88.32 87.75 84.01 30.9All 86.54 86.02 90.32 89.78 86.28 36.063Computational Linguistics Volume 34, Number 1Table 7Accuracy of predicate?argument relations (test set, <100 words).LP LR UP UR F-score Sentence acc.Baseline 77.58 76.84 82.22 81.43 77.21 17.1Syntactic features 86.47 85.83 90.06 89.40 86.15 34.1Semantic features 83.81 83.26 87.75 87.16 83.53 28.9All 86.13 85.59 89.85 89.29 85.86 33.8in Section 4.5.
?Semantic features?
represents a model with features on predicate?argument structures, that is, fpa given in Table 4.
?All?
is a model with both syntacticand semantic features.
The ?Baseline?
row shows the results for the reference model,p0(t|s), used for lexical entry filtering in the estimation of the other models.
This modelis considered as a simple application of a traditional PCFG-style model; that is, p(r) = 1for any rule r in the construction rules of the HPSG grammar.The results demonstrate that feature forest models have significantly higher ac-curacy than a baseline model.
Comparing ?Syntactic features?
with ?Semantic fea-tures,?
we see that the former model attained significantly higher accuracy than thelatter.
This indicates that syntactic features are more important for overall accuracy.We will examine the contributions of each atomic feature of the syntactic features inSection 5.5.Features on predicate?argument relations were generally considered as importantfor the accurate disambiguation of syntactic structures.
For example, PP-attachmentambiguity cannot be resolved with only syntactic preferences.
However, the resultsshow that a model with only semantic features performs significantly worse than onewith syntactic features.
Even when combined with syntactic features, semantic featuresdo not improve accuracy.
Obviously, semantic preferences are necessary for accurateparsing, but the features used in this work were not sufficient to capture semantic pref-erences.
A possible reason is that, as reported in Gildea (2001), bilexical dependenciesmay be too sparse to capture semantic preferences.For reference, our results are competitive with the best corresponding results re-ported in CCG parsing (LP/LR = 86.6/86.3) (Clark and Curran 2004b), although ourresults cannot be compared directly with other grammar formalisms because eachformalism represents predicate?argument dependencies differently.
In contrast with theresults of CCG and PCFG (Collins 1997, 1999, 2003; Charniak 2000), the recall is clearlylower than precision.
This may have resulted from the HPSG grammar having stricterfeature constraints and the parser not being able to produce parse results for around1% of the sentences.
To improve recall, we need techniques to deal with these 1% ofsentences.Table 8 gives the computation/space costs of model estimation.
?Estimation time?indicates user times required for running the parameter estimation algorithm.
?No.
offeature occurrences?
denotes the total number of occurrences of features in the trainingdata, and ?Data size?
gives the sizes of the compressed files of training data.
We canconclude that feature forest models are estimated at a tractable computational cost anda reasonable data size, even when a model includes semantic features including non-local dependencies.
The results reveal that feature forest models essentially solve theproblem of the estimation of probabilistic models of sentence structures.64Miyao and Tsujii Feature Forest Models for Probabilistic HPSG ParsingTable 8Computation/space costs of model estimation.No.
of features Estimation No.
of feature Data sizetime (sec.)
occurrences (MB)Baseline 24,847 499 6,948,364 21Syntactic features 599,104 511 127,497,615 727Semantic features 334,821 278 176,534,753 375All 933,925 716 304,032,368 1,093Table 9Estimation method vs. accuracy and estimation time.LP LR F-score Estimation time (sec.
)Filtering only 51.70 49.89 50.78 449Product 86.50 85.94 86.22 1,568Reference distribution 86.92 86.28 86.60 511Feature function 84.81 84.09 84.45 9455.4 Comparison of Filtering MethodsTable 9 compares the estimation methods introduced in Section 4.4.
In all of the follow-ing experiments, we show the accuracy for the test set (<40 words) only.
Table 9 revealsthat our method achieves significantly lower accuracy when it is used only for filteringin the training phrase.
One reason is that the feature forest model prefers lexical entriesthat are filtered out in the training phase, because they are always oracle lexical entriesin the training.
This means that we must incorporate the preference of filtering into thefinal parse selection.
As shown in Table 9, the models combined with a preliminarymodel achieved sufficient accuracy.
The reference distribution method achieved higheraccuracy and lower cost.
The feature function method achieved lower accuracy in ourexperiments.
A possible reason for this is that a hyper-parameter of the prior was set tothe same value for all the features including the feature of the log-probability given bythe preliminary distribution.Tables 10 and 11 show the results of changing the filtering threshold.
We candetermine the correlation between the estimation/parsing cost and accuracy.
In ourexperiment, n ?
10 and  ?
0.90 seem necessary to preserve the F-score over 86.0.5.5 Contribution of FeaturesTable 12 shows the accuracy with different feature sets.
Accuracy was measured for 15models with some atomic features removed from the final model.
The last row denotesthe accuracy attained by the unigram model (i.e., the reference distribution).
The num-bers in bold type represent a significant difference from the final model according tostratified shuffling tests with the Bonferroni correction (Cohen 1995) with p-value < .05for 32 pairwise comparisons.
The results indicate that DIST, COMMA, SPAN, WORD, and65Computational Linguistics Volume 34, Number 1Table 10Filtering threshold vs. accuracy.n, LP LR F-score Sentence acc.5, 0.80 85.09 84.30 84.69 32.45, 0.90 85.44 84.61 85.02 32.55, 0.95 85.52 84.66 85.09 32.75, 0.98 85.50 84.63 85.06 32.610, 0.80 85.60 84.65 85.12 32.510, 0.90 86.49 85.92 86.20 34.710, 0.95 86.92 86.28 86.60 36.310, 0.98 87.18 86.66 86.92 37.715, 0.80 85.59 84.63 85.11 32.415, 0.90 86.48 85.80 86.14 35.715, 0.95 87.21 86.68 86.94 37.015, 0.98 87.69 87.16 87.42 39.2Table 11Filtering threshold vs. estimation cost.n, Estimation time (sec.)
Parsing time (sec.)
Data size (MB)5, 0.80 108 5,103 3415, 0.90 150 6,242 4075, 0.95 190 7,724 4695, 0.98 259 9,604 54910, 0.80 130 6,003 37010, 0.90 268 8,855 51110, 0.95 511 15,393 72710, 0.98 1,395 36,009 1,23015, 0.80 123 6,298 37215, 0.90 259 9,543 52615, 0.95 735 20,508 85415, 0.98 3,777 86,844 2,031POS features contributed to the final accuracy, although the differences were slight.
Incontrast, RULE, SYM, and LE features did not affect accuracy.
However, when each wasremoved together with another feature, the accuracy decreased drastically.
This impliesthat such features carry overlapping information.5.6 Factors for Parsing AccuracyTable 13 shows parsing accuracy for covered and uncovered sentences.
As defined inSection 5.1, ?covered?
indicates that the HPSG lexicon has all correct lexical entries for asentence.
In other words, for covered sentences, exactly correct parse trees are obtainedif the disambiguation model worked perfectly.
The result reveals clear differences inaccuracy between covered and uncovered sentences.
The F-score for covered sentencesis around 2.5 points higher than the overall F-score, whereas the F-score is more than10 points lower for uncovered sentences.
This result indicates improvement of lexiconquality is an important factor for higher accuracy.66Miyao and Tsujii Feature Forest Models for Probabilistic HPSG ParsingTable 12Accuracy with different feature sets.Features LP LR F-score Sentence acc.
No.
of featuresAll 86.92 86.28 86.60 36.3 599,104?RULE 86.83 86.19 86.51 36.3 596,446?DIST 86.52 85.96 86.24 35.7 579,666?COMMA 86.31 85.81 86.06 34.4 584,040?SPAN 86.32 85.75 86.03 35.5 559,490?SYM 86.74 86.16 86.45 35.4 406,545?WORD 86.39 85.77 86.08 35.3 91,004?POS 86.18 85.61 85.89 34.1 406,545?LE 86.91 86.32 86.61 36.8 387,938?DIST,SPAN 85.39 84.82 85.10 33.1 270,467?DIST,SPAN,COMMA 83.75 83.25 83.50 28.9 261,968?RULE,DIST,SPAN,COMMA 83.44 82.93 83.18 27.6 259,372?WORD,LE 86.40 85.81 86.10 34.7 25,429?WORD,POS 85.44 84.87 85.15 32.7 40,102?WORD,POS,LE 84.68 84.12 84.40 31.1 8,899?SYM,WORD,POS,LE 82.77 82.14 82.45 24.9 1,914None 78.10 77.39 77.74 18.3 0Table 13Accuracy for covered/uncovered sentences.LP LR F-score Sentence acc.
No.
of sentencescovered sentences 89.36 88.96 89.16 42.2 1,825uncovered sentences 75.57 74.04 74.80 2.5 319Figure 23 shows the learning curve.
A feature set was fixed, and the parameter ofthe Gaussian prior was optimized for each model.
High accuracy is attained even with asmall training set, and the accuracy seems to be saturated.
This indicates that we cannotfurther improve the accuracy simply by increasing the size of the training data set.
Theexploration of new types of features is necessary for higher accuracy.
It should also benoted that the upper bound of the accuracy is not 100%, because the grammar cannotproduce completely correct parse results for uncovered sentences.Figure 24 shows the accuracy for each sentence length.
It is apparent from thisfigure that the accuracy is significantly higher for sentences with less than 10 words.This implies that experiments with only short sentences overestimate the performanceof parsers.
Sentences with at least 10 words are necessary to properly evaluate theperformance of parsing real-world texts.
The accuracies for the sentences with morethan 10 words are not very different, although data points for sentences with more than50 words are not reliable.Table 14 shows the accuracies for predicate?argument relations when parts-of-speech tags are assigned automatically by a maximum-entropy-based parts-of-speech tagger (Tsuruoka and Tsujii 2005).
The results indicate a drop of about threepoints in labeled precision/recall (a two-point drop in unlabeled precision/recall).A reason why we observed larger accuracy drops in labeled precision/recall is that67Computational Linguistics Volume 34, Number 1Figure 23Corpus size vs. accuracy.Figure 24Sentence length vs. accuracy.predicate?argument relations are fragile with respect to parts-of-speech errors becausepredicate types (e.g., adjective, intransitive verb) are determined depending on theparts-of-speech of predicate words.
Although our current parsing strategy assumes thatparts-of-speech are given beforehand, for higher accuracy in real application contexts,we will need a method for determining parts-of-speech and parse trees jointly.Table 14Accuracy with automatic parts-of-speech tags (test set).LP LR UP UR F-score Sentence acc.<40 words 83.88 82.84 88.83 87.73 83.36 30.1<100 words 83.45 82.40 88.37 87.26 82.92 28.268Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing5.7 Analysis of Disambiguation ErrorsTable 15 shows amanual classification of the causes of disambiguation errors in 100 sen-tences randomly chosen from Section 00.
In our evaluation, one error source may causemultiple dependency errors.
For example, if an incorrect lexical entry is assigned to averb, all of the argument dependencies of the verb are counted as errors.
The numbersin the table include such double-counting.
Figure 25 shows examples of disambiguationerrors.
The figure shows output from the parser.Major causes are classified into three types: attachment ambiguity, argument/modifier distinction, and lexical ambiguity.
As attachment ambiguities are well-knownerror sources, PP-attachment is the largest source of errors in our evaluation.
Ourdisambiguation model cannot accurately resolve PP-attachment ambiguities because itdoes not include dependencies among a modifiee and the argument of the preposition.Because previous studies revealed that such dependencies are effective features forPP-attachment resolution, we should incorporate them into our model.
Some of theattachment ambiguities, including adjective and adverb, should also be resolvedwith an extension of features.
However, we cannot identify any effective featuresfor the disambiguation of attachment of verbal phrases, including relative clauses,verb phrases, subordinate clauses, and to-infinitives.
For example, Figure 25 showsan example error of the attachment of a relative clause.
The correct answer is that theTable 15Classification of disambiguation errors.Error cause No.
of errorsAttachment ambiguity prepositional phrase 32relative clause 14adjective 7adverb 6verb phrase 5subordinate clause 3to-infinitive 3others 6Argument/modifier distinction to-infinitive 19noun phrase 7verb phrase 7subordinate clause 7others 9Lexical ambiguity preposition/modifier 13verb subcategorization frame 13participle/adjective 12others 6Test set errors errors of treebank conversion 18errors of Penn Treebank 4Comma 32Noun phrase identification 15Coordination/insertion 15Zero-pronoun resolution 9Others 469Computational Linguistics Volume 34, Number 1Figure 25Examples of disambiguation errors.subject of yielded is acre, but this cannot be determined only by the relation among yield,grapes, and acre.
The resolution of these errors requires a novel type of feature function.Errors of argument/modifier distinction are prominent in deep syntactic analysis,because arguments and modifiers are not explicitly distinguished in the evaluation ofCFG parsers.
Figure 25 shows an example of the argument/modifier distinction of ato-infinitive clause.
In this case, the to-infinitive clause is a complement of tempts.
Thesubcategorization frame of tempts seems responsible for this problem.
However, thedisambiguation model wrongly assigned a lexical entry for a transitive verb becauseof the sparseness of the training data (tempts occurred only once in the training data).The resolution of this sort of ambiguity requires the refinement of a probabilistic modelof lexical entries.
Errors of verb phrases and subordinate clauses are similar to thisexample.
Errors of argument/modifier distinction of noun phrases aremainly caused bytemporal nouns and cardinal numbers.
The resolution of these errors seems to requirethe identification of temporal expressions and usage of cardinal numbers.Errors of lexical ambiguities were mainly caused by idioms.
For example, in Fig-ure 25, compared with is a compound preposition, but the parser recognized it as averb phrase.
This indicates that the grammar or the disambiguation model requiresthe special treatment of idioms.
Errors of verb subcategorization frames were mainlycaused by difficult constructions such as insertions.
Figure 25 shows that the parsercould not identify the inserted clause (says John Siegel.
.
.)
and a lexical entry for adeclarative transitive verb was chosen.Attachment errors of commas are also significant.
It should be noted that commaswere ignored in the evaluation of CFG parsers.
We did not eliminate punctuationfrom the evaluation because punctuation sometimes contributes to semantics, asin coordination and insertion.
In this error analysis, errors of commas representingcoordination/insertion are classified into ?coordination/insertion,?
and ?comma?
in-dicates errors that do not contribute to the computation of semantics.Errors of noun phrase identification mean that a noun phrase was split into twophrases.
These errors were mainly caused by the indirect effects of other errors.70Miyao and Tsujii Feature Forest Models for Probabilistic HPSG ParsingErrors of identifying coordination/insertion structures sometimes resulted incatastrophic analyses.
While accurate analysis of such constructions is indispensable,it is also known to be difficult because disambiguation of coordination/insertionrequires the computation of preferences over global structures, such as the similarityof syntactic/semantic structure of coordinates.
Incorporating features for representingthe similarity of global structures is difficult for feature forest models.Zero-pronoun resolution is also a difficult problem.
However, we found thatmost were indirectly caused by errors of argument/modifier distinction in to-infinitiveclauses.A significant portion of the errors discussed above cannot be resolved by the fea-tures we investigated in this study, and the design of other features will be necessaryfor improving parsing accuracy.6.
Discussion6.1 Probabilistic Modeling of Complete StructuresThe model described in this article was first published in Miyao and Tsujii (2002), andhas been applied to probabilistic models for parsing with lexicalized grammars.
Appli-cations to CCG parsing (Clark and Curran 2003, 2004b) and LFG parsing (Kaplan et al2004; Riezler and Vasserman 2004) demonstrated that feature forest models attainedhigher accuracy than other models.
These researchers applied feature forests to repre-sentations of the packed parse results of LFG and the dependency/derivation structuresof CCG.
Their work demonstrated the applicability and effectiveness of feature forestmodels in parsing with wide-coverage lexicalized grammars.
Feature forest modelswere also shown to be effective for wide-coverage sentence realization (Nakanishi,Miyao, and Tsujii 2005).
This work demonstrated that feature forest models are genericenough to be applied to natural language processing tasks other than parsing.The work of Geman and Johnson (2002) independently developed a dynamic pro-gramming algorithm for maximum entropy models.
The solution was similar to ourapproach, although their method was designed to traverse LFG parse results repre-sented with disjunctive feature structures as proposed by Maxwell and Kaplan (1995).The difference between the two approaches is that feature forests use a simpler genericdata structure to represent packed forest structures.
Therefore, without assuming whatfeature forests represent, our algorithm can be applied to various tasks, includingtheirs.Another approach to the probabilistic modeling of complete structures is a methodof approximation.
The work on whole sentence maximum entropy models (Rosenfeld1997; Chen and Rosenfeld 1999b) proposed an approximation algorithm to estimateparameters of maximum entropy models on whole sentence structures.
However, thealgorithm suffered from slow convergence, and the model was basically a sequencemodel.
It could not produce a solution for complex structures as our model can.We should also mention Conditional Random Fields (CRFs) (Lafferty, McCallum,and Pereira 2001) for solving a similar problem in the context of maximum entropyMarkov models.
Their solution was an algorithm similar to the computation offorward/backward probabilities of hidden Markov models (HMMs).
Their algorithm isa special case of our algorithm in which each conjunctive node has only one daughter.This is obvious because feature forests can represent Markov chains.
In an analogy,CRFs correspond to HMMs, whereas feature forest models correspond to PCFGs.71Computational Linguistics Volume 34, Number 1Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are alsoregarded as instances of feature forest models.
This fact implies that our algorithm isapplicable to not only parsing but also to other tasks.
CRFs are now widely used forsequence-based tasks, such as parts-of-speech tagging and named entity recognition,and have been shown to achieve the best performance in various tasks (McCallum andLi 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al 2003; Sha and Pereira2003; Peng and McCallum 2004; Roark et al 2004; Settles 2004; Sutton, Rohanimanesh,and McCallum 2004).
These results suggest that the method proposed in the presentarticle will achieve high accuracy when applied to various statistical models withtree structures.
Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton,Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration forextending feature forest models.
The purpose of dynamic CRFs is to incorporate featurefunctions that are not represented locally, and the solution is to apply a variationalmethod, which is an algorithm of numerical computation, to obtain approximate so-lutions.
A similar method may be developed to overcome a bottleneck of feature forestmodels, that is, the fact that feature functions are localized to conjunctive nodes.The structure of feature forests is common in natural language processing andcomputational linguistics.
As is easily seen, lattices, Markov chains, and CFG parsetrees are represented by feature forests.
Furthermore, because conjunctive nodes donot necessarily represent CFG nodes or rules and terminals of feature forests neednot be words, feature forests can express any forest structures in which ambiguitiesare packed in local structures.
Examples include the derivation trees of LTAG andCCG.
Chiang (2003) proved that feature forests could be considered as the derivationforests of linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi1987; Weir 1988).
LCFRSs define a wide variety of grammars, including LTAG andCCG, while preserving polynomial-time complexity of parsing.
This demonstrates thatfeature forest models are applicable to probabilistic models far beyond PCFGs.
Featureforests are also isomorphic to support graphs (or explanation graphs) used in the graphicalEM algorithm (Kameya and Sato 2000).
In their framework, a program in a logic pro-gramming language, PRISM (Sato and Kameya 1997), is converted into support graphs,and parameters of probabilistic models are automatically learned by an EM algorithm.Support graphs have been proved to represent various statistical structural models, in-cluding HMMs, PCFGs, Bayesian networks, and many other graphical structures (Satoand Kameya 2001; Sato 2005).
Taken together, these results imply the high applicabilityof feature forest models to various real tasks.Because feature forests have a structure isomorphic to parse forests of PCFG, itmight seem that they can represent only immediate dominance relations of CFG rulesas in PCFG, resulting in only a slight, trivial extension of PCFG.
As described herein,however, feature forests can represent structures beyond CFG parse trees.
Furthermore,because feature forests are a generalized representation of ambiguous structures, eachnode in a feature forest need not correspond to a node in a PCFG parse forest.
That is,a node in a feature forest may represent any linguistic entity, including a fragment of asyntactic structure, a semantic relation, or other sentence-level information.The idea of feature forest models could be applied to non-probabilistic machinelearning methods.
Taskar et al (2004) proposed a dynamic programming algorithmfor the learning of large-margin classifiers including support vector machines (Vapnik1995), and presented its application to disambiguation in CFG parsing.
Their algorithmresembles feature forest models; an optimization function is computed by a dynamicprograming algorithmwithout unpacking packed forest structures.
From the discussionin this article, it is evident that if the main part of an update formula is represented72Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsingwith (the exponential of) linear combinations, a method similar to feature forest modelsshould be applicable.6.2 Probabilistic Parsing with Lexicalized GrammarsBefore the advent of feature forest models, studies on probabilistic models of HPSGadopted conventional maximum entropy models to select the most probable parse fromparse candidates given by HPSG grammars (Oepen, Toutanova, et al 2002; Toutanovaand Manning 2002; Baldridge and Osborne 2003).
The difference between these studiesand our work is that we used feature forests to avoid the exponential increase in thenumber of structures that results from unpacked parse results.
These studies ignoredthe problem of exponential explosion; in fact, training sets in these studies were verysmall and consisted only of short sentences.
A possible approach to avoid this problemis to develop a fully restrictive grammar that never causes an exponential explosion, al-though the development of such a grammar requires considerable effort and it cannot beacquired from treebanks using existing approaches.We think that exponential explosionis inevitable, particularly with the large-scale wide-coverage grammars required to an-alyze real-world texts.
In such cases, these methods of model estimation are intractable.Another approach to estimating log-linear models for HPSG was to extract a smallinformative sample from the original set T(w) (Osborne 2000).
The method was suc-cessfully applied to Dutch HPSG parsing (Malouf and van Noord 2004).
A possibleproblem with this method is in the approximation of exponentially many parse trees bya polynomial-size sample.
However, their method has an advantage in that any featureson parse results can be incorporated into a model, whereas our method forces featurefunctions to be defined locally on conjunctive nodes.
We will discuss the trade-offbetween the approximation solution and the locality of feature functions in Section 6.3.Non-probabilistic statistical classifiers have also been applied to disambiguation inHPSG parsing: voted perceptrons (Baldridge and Osborne 2003) and support vectormachines (Toutanova, Markova, and Manning 2004).
However, the problem of expo-nential explosion is also inevitable using their methods.
As described in Section 6.1, anapproach similar to ours may be applied, following the study of Taskar et al (2004).A series of studies on parsing with LFG (Johnson et al 1999; Riezler et al 2000,2002) also proposed a maximum entropy model for probabilistic modeling of LFG pars-ing.
However, similarly to the previous studies on HPSG parsing, these groups hadno solution to the problem of exponential explosion of unpacked parse results.
As dis-cussed in Section 6.1, Geman and Johnson (2002) proposed an algorithm for maximumentropy estimation for packed representations of LFG parses.Recent studies on CCG have proposed probabilistic models of dependency struc-tures or predicate?argument dependencies, which are essentially the same as thepredicate?argument structures described in the present article.
Clark, Hockenmaier, andSteedman (2002) attempted the modeling of dependency structures, but the model wasinconsistent because of the violation of the independence assumption.
Hockenmaier(2003) proposed a consistent generative model of predicate?argument structures.
Theprobability of a non-local dependency was conditioned on multiple words to preservethe consistency of the probability model; that is, probability p(I|want, dispute) in Sec-tion 4.3 was directly estimated.
The problem was that such probabilities could not beestimated directly from the data due to data sparseness, and a heuristic method hadto be employed.
Probabilities were therefore estimated as the average of individualprobabilities conditioned on a single word.
Another problem is that the model is nolonger consistent when unification constraints such as those in HPSG are introduced.73Computational Linguistics Volume 34, Number 1Our solution is free of these problems, and is applicable to various grammars, not onlyHPSG and CCG.Most of the state-of-the-art studies on parsing with lexicalized grammars haveadopted feature forest models (Clark and Curran 2003, 2004b; Kaplan et al 2004; Riezlerand Vasserman 2004).
Their methods of translating parse results into feature forests arebasically the same as our method described in Section 4, and details differ becausedifferent grammar theories represent syntactic structures differently.
They reportedhigher accuracy in parsing the Penn Treebank than the previous methods introducedherein, and these results attest the effectiveness of feature forest models in practicaldeep parsing.
A remaining problem is that no studies could provide empirical compar-isons across grammar theories.
The above studies and our research evaluated parsingaccuracy on their own test sets.
The construction of theory-independent standard testsets requires enormous effort because we must establish theory-independent criteriasuch as agreed definitions of phrases and headedness.
Although this issue is beyondthe scope of the present article, it is a fundamental obstacle to the transparency of thesestudies on parsing.Clark and Curran (2004a) described a method for reducing the cost of parsing atraining treebank without sacrificing accuracy in the context of CCG parsing.
They firstassigned each word a small number of supertags, corresponding to lexical entries inour case, and parsed supertagged sentences.
Because they did not use the probabilities ofsupertags in a parsing stage, their method corresponds to our ?filtering only?
method.The difference from our approach is that they also applied the supertagger in a parsingstage.
We suppose that this was crucial for high accuracy in their approach, althoughempirical investigation is necessary.6.3 Trade-Off between Dynamic Programming and Feature LocalityThe proposed algorithm is an essential solution to the problem of estimating probabilis-tic models on exponentially many complete structures.
However, the applicability ofthis algorithm relies on the constraint that features are defined locally in conjunctivenodes.
As discussed in Section 6.1, this does not necessarily mean that features in ourmodel can represent only the immediate-dominance relations of CFG rules, becauseconjunctive nodesmay encode any fragments of complete structures.
In fact, we demon-strated in Section 4.3 that certain assumptions allowed us to encode non-local predicate?argument dependencies in tractable-size feature forests.
In addition, although in theexperiments we used only features on bilexical dependencies, the method described inSection 4.3 allows us to define any features on a predicate and all of its arguments, suchas a ternary relation among a subject, a verb, and a complement (e.g., the relation amongI, want, and dispute1 in Figure 21), and a generalized relation among semantic classesof a predicate and its arguments.
This is because a predicate and all of its argumentsare included in a conjunctive node, and feature functions can represent any relationsexpressed within a conjunctive node.Whenwe definemore global features, such as co-occurrences of structures at distantplaces in a sentence, conjunctive nodes must be expanded so that they include allstructures that are necessary to define these features.
However, this obviously increasesthe number of conjunctive nodes, and consequently, the cost of parameter estimationincreases.
In an extreme case, for example, if we define features on any co-occurrencesof partial parse trees, the full unpacking of parse forests would be necessary, and pa-rameter estimation would be intractable.
This indicates that there is a trade-off betweenthe locality of features and the cost of estimation.
That is, larger context features might74Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsingcontribute to higher accuracy, while they inflate the size of feature forests and increasethe cost of parameter estimation.Sampling techniques (Rosenfeld 1997; Chen and Rosenfeld 1999b; Osborne 2000;Malouf and van Noord 2004) allow us to define any features on complete structureswithout any constraints.
However, they force us to employ approximation methodsfor tractable computation.
The effectiveness of those techniques therefore relies onconvergence speed and approximation errors, which may vary depending on the char-acteristics of target problems and features.It is an open research question whether dynamic programming or sampling candeliver a better balance of estimation efficiency and accuracy.
The answer will differ indifferent problems.
When most effective features can be represented locally in tractable-size feature forests, dynamic programming methods including ours are suitable.However, when global context features are indispensable for high accuracy, samplingmethods might be better.
We should also investigate compromise solutions such asdynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh,and McCallum 2004) and reranking techniques (Collins 2000; Charniak and Johnson2005).
There is no analytical way of predicting the best solution, and it must beinvestigated experimentally for each target task.7.
ConclusionA dynamic programming algorithm was presented for maximum entropy modelingand shown to provide a solution to the parameter estimation of probabilistic models ofcomplete structures without the independence assumption.
We first defined the notionof a feature forest, which is a packed representation of an exponential number of trees offeatures.
When training data is represented with feature forests, model parameters areestimated at a tractable cost without unpacking the forests.
Themethod provides a moreflexible modeling scheme than previous methods of application of maximum entropymodels to natural language processing.
Furthermore, it is applicable to complex datastructures where an event is difficult to decompose into independent sub-events.We also demonstrated that feature forest models are applicable to probabilistic mod-eling of linguistic structures such as the syntactic structures of HPSG and predicate?argument structures including non-local dependencies.
The presented approach canbe regarded as a general solution to the probabilistic modeling of syntactic analysiswith lexicalized grammars.
Table 16 summarizes the best performance of the HPSGparser described in this article.
The parser demonstrated impressively high coverageand accuracy for real-world texts.
We therefore conclude that the HPSG parser forEnglish is moving toward a practical level of use in real-world applications.
Recently,the applicability of the HPSG parser to practical applications, such as informationextraction and retrieval, has also been demonstrated (Miyao et al 2006; Yakushiji et al2006; Chun 2007).Table 16Final results.Parsing accuracy for Section 23 (<40 words)# parsed sentences 2,137/2,144 (99.7%)Precision/recall 87.69%/87.16%Sentential accuracy 39.2%75Computational Linguistics Volume 34, Number 1From our extensive investigation of HPSG parsing, we observed that explorationof new types of features is indispensable to further improvement of parsing accuracy.A possible research direction is to encode larger contexts of parse trees, which hasbeen shown to improve accuracy (Toutanova and Manning 2002; Toutanova, Markova,and Manning 2004).
Future work includes not only the investigation of these featuresbut also the abstraction of predicate?argument dependencies using semantic classes.Experimental results also suggest that an improvement in grammar coverage is crucialfor higher accuracy.
This indicates that an improvement in the quality of the grammaris a key factor for the improvement of parsing accuracy.The feature forest model provides new insight into the relationship between alinguistic structure and a unit of probability.
Traditionally, a unit of probability wasimplicitly assumed to correspond to a meaningful linguistic structure; a tagging of aword or an application of a rewriting rule.
One reason for the assumption is to enabledynamic programming algorithms, such as the Viterbi algorithm.
The probability of acomplete structure must be decomposed into atomic structures in which ambiguitiesare limited to a tractable size.
Another reason is to estimate plausible probabilities.Because a probability is defined over atomic structures, they should also be meaning-ful so as to be assigned a probability.
In feature forest models, however, conjunctivenodes are responsible for the former, whereas feature functions are responsible for thelatter.
Although feature functions must be defined locally in conjunctive nodes, theyare not necessarily equivalent.
Conjunctive nodes may represent any fragments of acomplete structure, which are not necessarily linguistically meaningful.
They shouldbe designed to pack ambiguities and enable us to define useful features.
Meanwhile,feature functions indicate an atomic unit of probability, and are designed to capturestatistical regularity of the target problem.
We expect the separation of a unit of prob-ability from linguistic structures to open up a new framework for flexible probabilisticmodeling.AcknowledgmentsThe authors wish to thank the anonymousreviewers of Computational Linguistics fortheir helpful comments and discussions.
Wewould also like to thank Takashi Ninomiyaand Kenji Sagae for their precious support.ReferencesAbney, Steven P. 1997.
Stochasticattribute-value grammars.
ComputationalLinguistics, 23(4):597?618.Baker, James K. 1979.
Trainable grammarsfor speech recognition.
In Jared J. Wolfand Dennis H. Klatt, editors, SpeechCommunication Papers Presented at the 97thMeeting of the Acoustical Society of America.MIT Press, Cambridge, MA, pages 547?550.Baldridge, Jason and Miles Osborne.
2003.Active learning for HPSG parse selection.In Proceedings of the Seventh Conference onNatural Language Learning at HLT-NAACL2003, pages 17?24, Edmonton, Canada.Berger, AdamL., StephenA.
Della Pietra, andVincent J. Della Pietra.
1996.
A maximumentropy approach to natural languageprocessing.
Computational Linguistics,22(1):39?71.Burke, Michael, Aoife Cahill, RuthO?Donovan, Josef van Genabith,and Andy Way.
2004.
Treebank-basedacquisition of wide-coverage, probabilisticLFG resources: Project overview,results and evaluation.
In Proceedingsof the IJCNLP-04 Workshop ?Beyond ShallowAnalyses?, Hainan Island.
Availableat www-tsujii.is.s.u-tokyo.ac.jp/bsa.Carpenter, Bob.
1992.
The Logic of TypedFeature Structures.
Cambridge UniversityPress, Cambridge, England.Carroll, John and Stephan Oepen.
2005.High efficiency realization for awide-coverage unification grammar.In Proceedings of the 2nd InternationalJoint Conference on Natural LanguageProcessing (IJCNLP-05), pages 165?176,Jeju Island.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.
In Proceedingsof the First Conference on North AmericanChapter of the Association for Computational76Miyao and Tsujii Feature Forest Models for Probabilistic HPSG ParsingLinguistics (NAACL 2000), pages 132?139,Seattle, WA.Charniak, Eugene and Mark Johnson.2005.
Coarse-to-fine n-best parsingand MaxEnt discriminative reranking.In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics(ACL 2005), pages 173?180, Ann Arbor, MI.Chen, Stanley and Ronald Rosenfeld.1999a.
A Gaussian prior for smoothingmaximum entropy models.
TechnicalReport CMUCS-99-108, CarnegieMellon University.Chen, Stanley F. and Ronald Rosenfeld.1999b.
Efficient sampling and featureselection in whole sentence maximumentropy language models.
In Proceedingsof the 1999 IEEE International Conference onAcoustics, Speech, and Signal Processing,pages 549?552, Phoenix, AZ.Chiang, David.
2003.
Mildly context sensitivegrammars for estimating maximumentropy parsing models.
In Proceedings ofthe 8th Conference on Formal Grammar,pages 19?31, Vienna.Chun, Hong-Woo.
2007.Mining Literature forDisease-Gene Relations.
Ph.D. thesis,University of Tokyo.Clark, Stephen and James R. Curran.
2003.Log-linear models for wide-coverageCCG parsing.
In Proceedings of the 2003Conference on Empirical Methods inNatural Language Processing (EMNLP 2003),pages 97?104, Sapporo.Clark, Stephen and James R. Curran.2004a.
The importance of supertaggingfor wide-coverage CCG parsing.In Proceedings of the 20th InternationalConference on Computational Linguistics(COLING 2004), pages 282?288, Geneva.Clark, Stephen and James R. Curran.
2004b.Parsing the WSJ using CCG and log-linearmodels.
In Proceedings of the 42nd AnnualMeeting of the Association for ComputationalLinguistics (ACL 2004), pages 104?111,Barcelona.Clark, Stephen, Julia Hockenmaier, andMark Steedman.
2002.
Building deepdependency structures with a wide-coverage CCG parser.
In Proceedings of the40th Annual Meeting of the Association forComputational Linguistics (ACL 2002),pages 327?334, Philadephia.Cohen, Paul R. 1995.
Empirical Methods forArtificial Intelligence.
The MIT Press,Cambridge, MA.Collins, Michael.
1997.
Three generative,lexicalised models for statistical parsing.In Proceedings of the 35th Annual Meetingof the Association for ComputationalLinguistics (ACL?97), pages 16?23,Madrid.Collins, Michael.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.Ph.D.
thesis, University of Pennsylvania.Collins, Michael.
2000.
Discriminativereranking for natural languageparsing.
In Proceedings of the SeventeenthInternational Conference on MachineLearning, pages 175?182, Palo Alto, CA.Collins,Michael.
2003.
Head-driven statisticalmodels for natural language parsing.Computational Linguistics, 29(4):589?637.Copestake, Ann, Dan Flickinger,Rob Malouf, Susanne Riehemann, andIvan Sag.
1995.
Translation using minimalrecursion semantics.
In Proceedings of theSixth International Conference on Theoreticaland Methodological Issues in MachineTranslation (TMI95), pages 15?32, Leuven.Copestake, Ann, Dan Flickinger, Ivan A. Sag,and Carl Pollard.
2006.
Minimal recursionsemantics: An introduction.
Researchon Language and Computation, 3(4):281?332.Darroch, J. N. and D. Ratcliff.
1972.Generalized iterative scaling for log-linearmodels.
The Annals of MathematicalStatistics, 43(5):1470?1480.Della Pietra, Stephen, Vincent Della Pietra,and John Lafferty.
1997.
Inducing featuresof random fields.
IEEE Transactions onPattern Analysis and Machine Intelligence,19(4):380?393.Geman, Stuart and Mark Johnson.
2002.Dynamic programming for parsing andestimation of stochastic unification-basedgrammars.
In Proceedings of the 40th AnnualMeeting of the Association for ComputationalLinguistics (ACL 2002), pages 279?286,Philadelphia, PA.Gildea, Daniel.
2001.
Corpus variation andparser performance.
In Proceedings of the2001 Conference on Empirical Methods inNatural Language Processing (EMNLP 2001),pages 167?202, Pittsburgh, PA.Hockenmaier, Julia.
2003.
Parsing withgenerative models of predicate-argumentstructure.
In Proceedings of the 41st AnnualMeeting of the Association for ComputationalLinguistics (ACL 2003), pages 359?366,Sapporo.Hockenmaier, Julia and Mark Steedman.2002.
Acquiring compact lexicalizedgrammars from a cleaner treebank.In Proceedings of the Third InternationalConference on Language Resources andEvaluation (LREC-2002), pages 1974?1981,Las Palmas.77Computational Linguistics Volume 34, Number 1Johnson, Mark, Stuart Geman, StephenCanon, Zhiyi Chi, and Stefan Riezler.1999.
Estimators for stochastic ?unification-based?
grammars.
In Proceedingsof the 37th Annual Meeting of the Associationfor Computational Linguistics (ACL?99),pages 535?541, College Park, Maryland.Johnson, Mark and Stefan Riezler.
2000.Exploiting auxiliary distributions instochastic unification-based grammars.In Proceedings of the First Conferenceon North American Chapter of theAssociation for Computational Linguistics,pages 154?161, Seattle, WA.Kameya, Yoshitaka and Taisuke Sato.2000.
Efficient EM learning with tabulationfor parameterized logic programs.In Proceedings of the 1st InternationalConference on Computational Logic(CL2000), volume 1861 of Lecture Notesin Artificial Intelligence (LNAI),pages 269?294, Imperial College, London.Kaplan, Ronald M., Stefan Riezler, Tracy H.King, John T. Maxwell, III, AlexanderVasserman, and Richard Crouch.2004.
Speed and accuracy in shallowand deep stochastic parsing.
In Proceedingsof the Human Language TechnologyConference and the North American Chapterof the Association for ComputationalLinguistics (HLT-NAACL 2004),pages 97?104, Boston, MA.Lafferty, John, Andrew McCallum, andFernando Pereira.
2001.
Conditionalrandom fields: Probabilistic models forsegmenting and labeling sequence data.In Proceedings of the International Conferenceon Machine Learning 2001, pages 282?289,Williams College, Williamstown, MA.Makino, Takaki, Yusuke Miyao, KentaroTorisawa, and Jun?ichi Tsujii.
2002.Native-code compilation of featurestructures.
In Stephen Oepen, DanFlickinger, Jun?ichi Tsujii, and HansUszkoreit, editors, CollaborativeLanguage Engineering: A Case Studyin Efficient Grammar-based Parsing.
CSLIPublications, Palo Alto, CA, pages 49?80.Malouf, Robert.
2002.
A comparisonof algorithms for maximum entropyparameter estimation.
In Proceedingsof the Sixth Conference on NaturalLanguage Learning (CoNLL-2002),pages 1?7, Taipei.Malouf, Robert and Gertjan van Noord.2004.
Wide coverage parsing withstochastic attribute value grammars.In Proceedings of the IJCNLP-04 Workshop?Beyond Shallow Analyses?, Hainan Island.Available at www.tsujii.is.s.u-tokyo.ac.jp/bsa.Marcus, Mitchell, Grace Kim, Mary AnnMarcinkiewicz, Robert MacIntyre,Ann Bies, Mark Ferguson, Karen Katz,and Britta Schasberger.
1994.
The PennTreebank: Annotating predicateargument structure.
In Proceedingsof the Workshop on Human LanguageTechnology, pages 114?119, Plainsboro, NJ.Maxwell John T., III and Ronald M. Kaplan.1995.
A method for disjunctive constraintsatisfaction.
In Mary Dalrymple, RonaldM.
Kaplan, John T. Maxwell, III, andAnnie Zaenen, editors, Formal Issues inLexical-Functional Grammar, number 47in CSLI Lecture Notes Series.
CSLIPublications, Palo Alto, CA, chapter 14,pages 381?481.McCallum, Andrew and Wei Li.
2003.Early results for named entity recognitionwith conditional random fields, featureinduction and web-enhanced lexicons.In Proceedings of the 7th Conferenceon Natural Language Learning (CoNLL),pages 188?191, Edmonton.McCallum, Andrew, KhashayarRohanimanesh, and Charles Sutton.2003.
Dynamic conditional random fieldsfor jointly labeling multiple sequences.
InProceedings of the Workshop on Syntax,Semantics, Statistics at the 16th AnnualConference on Neural Information ProcessingSystems, Vancouver.
Available atwww.cs.umasse.du/?mccallum/papers/derf-nips03.pdf.Miyao, Yusuke, Takashi Ninomiya, andJun?ichi Tsujii.
2003.
Probabilistic modelingof argument structures includingnon-local dependencies.
In Proceedingsof the International Conference on RecentAdvances in Natural Language Processing(RANLP 2003), pages 285?291, Borovets.Miyao, Yusuke, Takashi Ninomiya,and Jun?ichi Tsujii.
2005.
Corpus-orientedgrammar development for acquiring ahead-driven phrase structure grammarfrom the Penn Treebank.
In NaturalLanguage Processing - IJCNLP 2004,pages 684?693, Hainan Island.Miyao, Yusuke, Tomoko Ohta,Katsuya Masuda, Yoshimasa Tsuruoka,Kazuhiro Yoshida, Takashi Ninomiya,and Jun?ichi Tsujii.
2006.
Semantic retrievalfor the accurate identification of relationalconcepts in massive textbases.
InProceedings of the Joint Conference of the 21stInternational Conference on ComputationalLinguistics and the 44th Annual Meeting of the78Miyao and Tsujii Feature Forest Models for Probabilistic HPSG ParsingAssociation for Computational Linguistics(COLING-ACL 2006), pages 1017?1024,Sydney.Miyao, Yusuke and Jun?ichi Tsujii.
2002.Maximum entropy estimation for featureforests.
In Proceedings of the HumanLanguage Technology Conference (HLT-2002),pages 292?297, San Diego, CA.Miyao, Yusuke and Jun?ichi Tsujii.
2003.A model of syntactic disambiguationbased on lexicalized grammars.
InProceedings of the Seventh Conference onComputational Natural Language Learning(CoNLL-2003), pages 1?8, Edmonton.Miyao, Yusuke and Jun?ichi Tsujii.
2005.Probabilistic disambiguation modelsfor wide-coverage HPSG parsing.
InProceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics(ACL 2005), pages 83?90, Ann Arbor, MI.Nakanishi, Hiroko, Yusuke Miyao, andJun?ichi Tsujii.
2005.
Probabilistic modelsfor disambiguation of an HPSG-basedchart generator.
In Proceedings of the9th International Workshop on ParsingTechnologies (IWPT 2005), pages 93?102,Vancouver.Ninomiya, Takashi, Yoshimasa Tsuruoka,Yusuke Miyao, and Jun?ichi Tsujii.
2005.Efficacy of beam thresholding, unificationfiltering and hybrid parsing in probabilisticHPSG parsing.
In Proceedings of the 9thInternational Workshop on ParsingTechnologies, pages 103?114, Vancouver.Nocedal, Jorge.
1980.
Updatingquasi-Newton matrices with limitedstorage.Mathematics of Computation,35:773?782.Nocedal, Jorge and Stephen J. Wright.1999.
Numerical Optimization.
Springer,New York.Oepen, Stephan and John Carroll.
2000.Ambiguity packing in constraint-basedparsing: practical results.
In Proceedingsof the First Conference of the North AmericanChapter of the Association for ComputationalLinguistics (NAACL 2000), pages 162?169,Seattle, WA.Oepen, Stephan, Dan Flickinger, Jun?ichiTsujii, and Hans Uszkoreit, editors.
2002.Collaborative Language Engineering: A CaseStudy in Efficient Grammar-Based Processing.CSLI Publications, Palo Alto, CA.Oepen, Stephan, Kristina Toutanova,Stuart Shieber, Christopher Manning, DanFlickinger, and Thorsten Brants.
2002.
TheLinGO Redwoods treebankmotivation andpreliminary applications.
In Proceedingsof the 19th International Conference onComputational Linguistics (COLING 2002),volume 2, pages 1?5, Taipei.Osborne, Miles.
2000.
Estimation ofstochastic attribute-value grammar usingan informative sample.
In Proceedingsof the 18th International Conference onComputational Linguistics (COLING 2000),volume 1, pages 586?592, Saarbru?cken.Peng, Fuchun and Andrew McCallum.
2004.Accurate information extraction fromresearch papers using conditionalrandom fields.
In Proceedings of HumanLanguage Technology Conference andNorth American Chapter of the Associationfor Computational Linguistics (HLT/NAACL-04), pages 329?336, Boston, MA.Pinto, David, Andrew McCallum, Xen Lee,and W. Bruce Croft.
2003.
Table extractionusing conditional random fields.
InProceedings of the 26th Annual InternationalACM SIGIR Conference on Researchand Development in Information Retrieval(SIGIR 2003), pages 235?242, Toronto.Pollard, Carl and Ivan A.
Sag.
1994.Head-Driven Phrase Structure Grammar.University of Chicago Press, Chicago, IL.Riezler, Stefan, Tracy H. King, Ronald M.Kaplan, Richard Crouch, John T.Maxwell, III, and Mark Johnson.
2002.Parsing the Wall Street Journal usinga lexical-functional grammar anddiscriminative estimation techniques.In Proceedings of the 40th Annual Meetingof the Association for ComputationalLinguistics (ACL 2002), pages 271?278,Philadephia, PA.Riezler, Stefan, Detlef Prescher, Jonas Kuhn,and Mark Johnson.
2000.
Lexicalizedstochastic modeling of constraint-basedgrammars using log-linear measuresand EM training.
In Proceedings of the38th Annual Meeting of the Associationfor Computational Linguistics (ACL 2000),pages 480?487, Hong Kong.Riezler, Stefan and Alexander Vasserman.2004.
Incremental feature selection andl1 regularization for relaxed maximum-entropy modeling.
In Proceedingsof the 2004 Conference on EmpiricalMethods in Natural Language Processing(EMNLP 2004), pages 174?181, Barcelona.Roark, Brian, Murat Saraclar, Michael Collins,and Mark Johnson.
2004.
Discriminativelanguage modeling with conditionalrandom fields and the perceptronalgorithm.
In Proceedings of the 42nd AnnualMeeting of the Association for ComputationalLinguistics (ACL 2004), pages 47?54,Barcelona.79Computational Linguistics Volume 34, Number 1Rosenfeld, Ronald.
1997.
A whole sentencemaximum entropy language model.
InProceedings of the IEEE Workshop on AutomaticSpeech Recognition and Understanding,pages 230?237, Santa Barbara, CA.Sag, Ivan A., Thomas Wasow, and Emily M.Bender.
2003.
Syntactic Theory: A FormalIntroduction.
Number 152 in CSLI LectureNotes.
CSLI Publications, Standford, CA.Sarawagi, Sunita and William W. Cohen.2004.
Semi-Markov conditional randomfields for information extraction.
InProceedings of the 18th Annual Conferenceon Neural Information ProcessingSystems, pages 1185?1192, Vancouver.Sato, Taisuke.
2005.
A generic approach to emlearning for symbolic-statistical models.
InProceedings of the 4th Learning Language inLogic Workshop (LLL05), pages 21?28, Bonn.Sato, Taisuke and Yoshitaka Kameya.
1997.PRISM: a language for symbolic-statisticalmodeling.
In Proceedings of the 15thInternational Joint Conference on ArtificialIntelligence (IJCAI ?97), pages 1330?1335,Nagoya.Sato, Taisuke and Yoshitaka Kameya.
2001.Parameter learning of logic programs forsymbolic-statistical modeling.
Journal ofArtificial Intelligence Research, 15:391?454.Settles, Burr.
2004.
Biomedical named entityrecognition using conditional random fieldsand rich feature sets.
In Proceedings of theInternational Joint Workshop on NaturalLanguage Processing in Biomedicine and itsApplications (NLPBA), pages 104?107,Geneva.Sha, Fei and Fernando Pereira.
2003.Shallow parsing with conditionalrandom fields.
In Proceedings of the 2003Human Language Technology Conference andNorth American Chapter of the Associationfor Computational Linguistics (HLT-NAACL2003), pages 213?220, Edmonton.Shieber, Stuart M. 1985.
Using restrictionto extend parsing algorithms forcomplex-feature-based formalisms.
InProceedings of the 23rd Annual Meetingon Association for Computational Linguistics,pages 145?152, Chicago, IL.Sutton, Charles, Khashayar Rohanimanesh,and Andrew McCallum.
2004.
Dynamicconditional random fields: Factorizedprobabilistic models for labeling andsegmenting sequence data.
In Proceedingsof the 21st International Conferenceon Machine Learning (ICML 2004),pages 783?790, Alberta.Taskar, Ben, Dan Klein, Michael Collins,Daphne Koller, and Chris Manning.2004.
Max-margin parsing.
In Proceedingsof the 2004 Conference on EmpiricalMethods in Natural Language Processing(EMNLP 2004), pages 1?8, Barcelona.Toutanova, Kristina and ChristopherManning.
2002.
Feature selection for arich HPSG grammar using decision trees.In Proceedings of the Sixth Conference onNatural Language Lerning (CoNLL-2002),pages 77?83, Taipei.Toutanova, Kristina, Penka Markova,and Christopher Manning.
2004.
Theleaf projection path view of parse trees:Exploring string kernels for HPSG parseselection.
In Proceedings of the 2004Conference on Empirical Methods inNatural Language Processing (EMNLP2004), pages 166?173, Barcelona.Tsujii Laboratory.
2004.
Enju?A practicalHPSG parser.
Available at http://www.tsujii.is.s.u-tokyo.ac.jp/enju/.Tsuruoka, Yoshimasa, Yusuke Miyao, andJun?ichi Tsujii.
2004.
Towards efficientprobabilistic HPSG parsing: Integratingsemantic and syntatic preferenceto guide the parsing.
In Proceedingsof the IJCNLP-04 Workshop ?Beyond ShallowAnalyses?, Hainan Island.
Availableat www.tsujii.is.s.u-tokyo.ac.jp/bsa.Tsuruoka, Yoshimasa and Jun?ichi Tsujii.2005.
Bidirectional inference with theeasiest-first strategy for tagging sequencedata.
In Proceedings of Human LanguageTechnology Conference and Conference onEmpirical Methods in Natural LanguageProcessing (HLT/EMNLP 2005),pages 467?474, Vancouver.Vapnik, Vladimir N. 1995.
The Nature ofStatistical Learning Theory.
Springer-Verlag,New York.Vijay-Shanker, K., David J. Weir, andAravind K. Joshi.
1987.
Characterizingstructural descriptions produced byvarious grammatical formalisms.
InProceedings of the 25th Annual Meetingof the Association for ComputationalLinguistics, pages 104?111, Palo Alto, CA.Weir, David J.
1988.
Characterizing MildlyContext-Sensitive Grammar Formalisms.Ph.D.
thesis, University of Pennsylvania.Yakushiji, Akane, Yusuke Miyao,Tomoko Ohta, Yuka Tateisi, and Jun?ichiTsujii.
2006.
Automatic construction ofpredicate-argument structure patterns forbiomedical information extraction.In Proceedings of the 2006 Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP 2006), pages 284?292,Sydney.80
