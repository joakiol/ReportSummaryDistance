Proceedings of the TextGraphs-7 Workshop at ACL, pages 1?5,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsA New Parametric Estimation Method for Graph-based ClusteringAbstractRelational clustering has received muchattention from researchers in the last decade.
Inthis paper we present a parametric method thatemploys a combination of both hard and softclustering.
Based on the corresponding Markovchain of an affinity matrix, we simulate aprobability distribution on the states bydefining a conditional probability for eachsubpopulation of states.
This probabilisticmodel would enable us to use expectationmaximization for parameter estimation.
Theeffectiveness of the proposed approach isdemonstrated on several real datasets againstspectral clustering methods.1 IntroductionClustering methods based on pairwise similarity ofdata points have received much attention inmachine learning circles and have been shown to beeffective on a variety of tasks (Lin and Cohen,2010; Macropol, et al, 2009; Ng, et al, 2001).Apart from pure relational data e.g.
Biologicalnetworks (Jeong, et al, 2001), Social Networks(Kwak, et al, 2010), these methods can also beapplied to none relational data them e.g.
text (Ding,et al, 2001; Ng, et al, 2001), image (Shi and Malik2000), where the edges indicate the affinity of thedata points in the dataset.Relational clustering has been addressed fromdifferent perspectives e.g.
spectral learning (Ng, etal., 2001; Shi and Malik 2000), random walks(Meila and Shi 2000; Macropol, et al, 2009), tracemaximization (Bui and Jones, 1993) andprobabilistic models (Long, et al, 2007).
Someworks have proposed frameworks for a unifiedview of different approaches.
In (Meila and Shi2000) a random walk view of the spectral clusteringalgorithm in (Shi and Malik 2000) was presented.By selecting an appropriate kernel, kernel k-meansand spectral clustering are also proved to beequivalent (Dhillon, et al, 2004).
As shown in (vonLuxburg, 2007) the basic idea behind most methodsare somehow optimizing the normalized cutobjective function.We propose a new perspective on relationalclustering where we use the corresponding Markovchain of a similarity graph to iteratively cluster thenodes.
Starting from a random distribution ofnodes in groups and given the transitionprobabilities of the Markov chain, we useexpectation maximization (EM) to estimate themembership of nodes in each group to eventuallyfind the best partitioning.After a brief review of the literature in section 2,we present our clustering algorithm in detail(section 3) and report experiments and evaluation(section 4).2 Background and Related WorkDue to the wealth of literature on the subject, it?s aformidable task to give a thorough review of theresearch on relational clustering.
Here we give abrief review of the papers that are more well-known or related to our work and refer the readerto (Chen and Ji 2010; Schaeffer 2007; vonLuxburg, 2007) for more detailed surveys.Graph clustering can be defined as finding kdisjoint clusters 1?, .
.
??
?
?
in a graph G = (V,E) where the vertices within each clusters aresimilar to each other and dissimilar to vertices inJavid Ebrahimi  and  Mohammad Saniee AbadehFaculty of Electrical & Computer EngineeringTarbiat Modares UniversityTehran, Iran{j.ebrahimi,saniee}@modares.ac.ir1other clusters.
Cut based measures, among otherscan be used to identify high quality clusters.Minimum cut of a graph is a cut (1) with thelowest value.???
=  ?
?
???????
,????????
(1)Here ?
is the number of clusters and ??
is the ???cluster.
Normalized cut (2) is a better objectivefunction that evades minimum cut's bias towardsmaller clusters by incorporating total connectionfrom each cluster to all nodes in the graph.
In theirseminal work Shi and Malik (2000) transformedthe normalized cut to a constrained Rayleighquotient and solved it by a standard eigenvaluesystem.????_?????????
=  ?
???????
????
,????
,???????
(2)Spectral clustering makes use of the spectrum of agraph:  either the eigenvalues of its affinity matrixor its Laplacian matrix (Schaeffer 2007).
Forexample in (Ng, et al, 2001) the k largesteigenvectors of normalized graph Laplacian matrixis selected, the rows of the inverse of the resultantmatrix are unit normalized and are finally clusteredinto k clusters using k-means.
Roughly speaking,spectral clustering embeds data points in a low-dimensional subspace extracted from the similaritymatrix, however this dimension reduction mayensue poor results when the approximation is notgood (Lin and Cohen 2010).Meila and Shi (2000) showed that thecorresponding stochastic matrix of an affinitymatrix has the same eigenvectors as the normalizedLaplacian matrix of the graph, thus spectralclustering can be interpreted as trying to find apartition of the graph such that the random walkstays long within the same cluster and seldomjumps between clusters (von Luxburg, 2007).
TheMarkov clustering algorithm (MCL) (van Dongen2000) is another algorithm that addresses graphclustering from a random walk point of view.
MCLcalculates powers of associated stochastic matrixof the network and strengthens the degree ofconnectivity of densely linked nodes while thesparse connections are weakened.
Repeatedrandom walk (RRW) (Macropol, et al, 2009)addresses MCL?s sensitivity to large diameterclusters and uses random walk with restart methodto calculate relevant score of connectivity betweennodes in the network.
Then, it repeatedly expandsbased on relevant scores to find clusters in whichnodes are of high proximity.
We should bear inmind that most random walk based algorithmshave been designed primarily for biologicalnetworks where the number of clusters is unknownand some parameters e.g.
desired granularity,minimum or maximum size of clusters might beneeded for a meaningful interpretation ofbiological data.
On the other hand, spectralclustering methods need to know the number ofclusters beforehand but don?t need tuningparameters and are more practical.In this paper, we adopt an approach similar toprobabilistic and partitional clustering in Euclideanspace, where the algorithm starts from randomguesses for some parameters and iterativelyclusters the data and improves the guesses.
In otherwords instead of embedding data points in theEigen space or powering of the stochastic matrix,we?re looking for a probabilistic model that solelyemploys the relation between data points.3 Clustering Algorithm3.1 NotationGiven a dataset D = ??(?
), ?(?
), ?
??
?,   asimilarity function s(?(?
), ?(?))
is a function wheres(?(?
), ?(?))
= s(?(?)
, ?(?))
,  s ?
0 and  s = 0if    i = j .
An affinity matrix ?
?
????
is anundirected weighted graph defined by  ???
=s(?(?
), ?(?))
.
after row-normalizing the affinitymatrix, we find the stochastic matrix ?
?
????
ofthe corresponding Markov chain (MC) with states??(?
), ?(?
), ????
where ?
???
= 1????
.3.2 Hard-Soft ClusteringThe basic idea behind Hard-Soft clustering (HSC)is to put nodes in clusters where within clustertransitions are more probable and between clustertransitions are minimal.
HSC makes use of bothhard and soft guesses for cluster membership.
Themethod is parametric such that it estimates the hardguesses and uses the hard partition for soft(probabilistic) clustering of data.
The mixture used2to model hard guesses could be described by amixture of multinomial model where theparameters (probabilities), are discretized {0, 1}.We start from random hard guesses and iterativelyimprove them by maximizing the likelihood usingEM.
Let ??(?
), ?(?),??(?)?
denote the states ofthe MC and given the number of clusters, what isthe maximum likelihood of hard partitioning  ?
ofnodes?
Having ?
as the number of clusters and ?as number of nodes ?
is a ?
?
?
matrix thatshows which node belongs to which cluster i.e.
onein the corresponding element and zero otherwise.The likelihood function is as follows:?(?)
= ?
????
????(?)??(?
); ??(???(?);??
??(?)??????
(4)In (4),  ?(?
)~ ???????????(?)
is our latent randomvariable where the mixing coefficient??
gives ??(?(?)
= ?).
For the soft clustering part ofHSC, we define the prior distribution ????(?)??(?)
=?
; ?)
as the probability of transitioning from  ?(?)
tostates marked by row vector ??
(?
?????????? )
.
Thisconditional prior distribution simulates aprobability distribution on the states in the MCbecause Pr (?(?))
along with the joint distribution?
Pr (?(?))
????
barely have any real worldinterpretation.The E-step is computed using the Bayes rule:??(?)
?= ????(?)
= ???(?
); ?)
=????(?)??(?)
= ?
; ??
(?
(?(?)
= ?
; ?)?
??(?(?)|?(?)
= ?
; ??
(?
(?(?)
= ?
; ?)????
(5)The M-step (6) is intractable because of thelogarithm of the weighted sum of parameters.max??)?)
=??
??(?)log(?
??????????
)????(?)????????
(6)s.t ?
???
= 1???
?However since the weights are transitionprobabilities and  ?
???
= 1 ????
, we can useweighted Jensen?s inequality to find a lower boundfor  ?)?
), get rid of logarithm of sums and convertit to sum of logarithms.?)?)
???(?)
=  ????????(?)
?
?log ??????????+log??
?
log ??(?)????
?The weighted Jensen?s inequality ?)?)
?
??(?
)holds with equality if and only if for all the  ??
?with  ???
?
0  are equal (Poonen 1999), which isnot applicable to our case since taking theconstraint into account, all nodes would havemembership degrees to all clusters ( ???
=??)
,therefore the inequality changes to a strictinequality ( note that we have relaxed the problemso that ???
can take fractional values that willeventually be discretized {0, 1}, for examplesetting one for the maximum and zero for the rest),Nevertheless maximizing the lower bound stillimproves previous estimates and iscomputationally more efficient than maximizing?)?)
itself which would require none linearoptimization.
Taking the constraint into accountwe use Lagrange multipliers to derive theparameters.?(?)
= ????????(?)
?
?log ??????????+log??
?
log ??(?)??????
?(????
?
1????)??????(?)
=?
??(?)???????????
?
= 0???
=?
??(?)????
????
???(?)????
???????
(7)To avoid bias toward larger clusters ?
is furtherrow-normalized.
Similarly ??
can be calculated:??
=1??
??(?)????
(8)Algorithm: HSCInput: The stochastic matrix P and thenumber of clusters cPick an initial ?
and ?.repeatE-step:  ??(?)
=????(?)??(?)??;?)
???
????(?)??(?)??;?)
?????
?M-Step:  ???
=?
??(?)????
????
?
??(?)????
???????
;   ??
=???
??(?)???
?Row-normalize and then discretize H.until ?
does not changeOutput: the set of hard assignments H34 Experiments4.1 DatasetsWe use datasets provided in (Lin and Cohen 2010).UbmcBlog (Kale, et.al, 2007) is a connectednetwork dataset of 404 liberal and conservativepolitical blogs mined from blog posts.
AgBlog(Adamic and Glance 2005) is a connected networkdataset of 1222 liberal and conservative politicalblogs mined from blog home pages.
20ng* aresubsets of the 20 newsgroups text dataset.
20ngAcontains 100 documents from misc.forsale andsoc.religion.christian.
20ngB adds 100 documentsto each category in 20ngA.
20ngC adds 200 fromtalk.politics.guns to 20ngB.
20ngD adds 200 fromrec.sport.baseball to 20ngC.
For the socialnetwork datasets (UbmcBlog, AgBlog), theaffinity matrix is simply  ???
= 1  if blog i has alink to j or vice versa, otherwise  ???
= 0 .
Fortext data, the affinity matrix is simply the cosinesimilarity between feature vectors.4.2 EvaluationSince the ground truth for the datasets we haveused is available, we evaluate the clustering resultsagainst the labels using three measures: clusterpurity (Purity), normalized mutual information(NMI), and Rand index (RI).
All three metrics areused to guarantee a more comprehensiveevaluation of clustering results (for example, NMItakes into account cluster size distribution, whichis disregarded by Purity).
We refer the reader to(Manning, et.
al 2008) for details regarding allthese measures.
In order to find the most likelyresult, each algorithm is run 100 times and theaverage in each criterion is reported.4.3 DiscussionWe compared the results of HSC against those oftwo state of the art spectral clustering methodsNcut (Shi and Malik 2000) and NJW (Ng, et al,2001) and one recent method Pic (Lin and Cohen2010) that uses truncated power iteration on anormalized affinity matrix, see Table 1.
HSCscores highest on all text datasets, on all threeevaluation metrics and just well on social networkdata.
The main reason for the effectiveness of HSCis in its use of both local and global structure of thegraph.
While the conditional probability????(?)??(?)
= ?
; ?)
looks at the immediatetransitions of state  ?(?)
, it uses ??
for the targetstates which denotes a group of nodes that arebeing refined throughout the process.
Using thestochastic matrix instead of embedding data pointsin the Eigen space or powering of the stochasticmatrix may also be a contributing factor thatdemands future research.As for convergence analysis of the algorithm, weresort to EM?s convergence (Bormann 2004).
Therunning complexity of spectral clustering methodsis known to be of  ?
(|?||?|) (Chen and Ji 2010),HSC is in  ?
( |?|???? )
where |?| the number ofnodes, ?
is the number of clusters and ?
is thenumber of iterations to converge.
Figure 1 showsthe average number of iterations that HSC took toconverge.Table : Clustering performance of HSC and threeclustering algorithms on several datasets, for eachdataset bold numbers are the highest in a column.EvaluationMethodDataSet(clusters)Algorithm Purity NMI RINcut 0.9530 0.7488 0.9104UbmcBlog NJW 0.9530 0.7375 0.9104(2) Pic 0.9480 0.7193 0.9014HSC 0.9532 0.7393 0.9108Ncut 0.5205 0.0060 0.5006AgBlog NJW 0.5205 0.0006 0.5007(2) Pic 0.9574 0.7465 0.9185HSC 0.9520 0.7243 0.9085Ncut 0.9600 0.7594 0.923220ngA NJW 0.9600 0.7594 0.9232(2) Pic 0.9600 0.7594 0.9232HSC 0.9640 0.7772 0.9306Ncut 0.5050 0.0096 0.500120ngB NJW 0.5525 0.0842 0.5055(2) Pic 0.8700 0.5230 0.7738HSC 0.9475 0.7097 0.9005Ncut 0.6183 0.3295 0.675020ngC NJW 0.6317 0.3488 0.6860(3) Pic 0.6933 0.4450 0.7363HSC 0.7082 0.4471 0.7448Ncut 0.4750 0.2385 0.631220ngD NJW 0.5150 0.2959 0.6820(4) Pic 0.5825 0.3133 0.7149HSC 0.6181 0.3795 0.7482Ncut 0.6719 0.3486 0.6900Average NJW 0.6887 0.3710 0.7013Pic 0.8352 0.5844 0.8280HSC 0.8571 0.6295 0.85724Figure 1: Average number of iterations to converge5 Conclusion and Future WorkWe propose a novel and simple clustering method,HSC, based on approximate estimation of the hardassignments of nodes to clusters.
The hardgrouping of the data is used to simulate aprobability distribution on the correspondingMarkov chain.
It is easy to understand, implementand is parallelizable.
Experiments on a number ofdifferent types of labeled datasets show that with areasonable cost of time HSC is able to obtain highquality clusters, compared to three spectralclustering methods.
One advantage of our methodis its applicability to directed graphs that will beaddressed in future works.ReferencesA.
Kale, A. Karandikar, P. Kolari, A. Java, T. Finin andA.
Joshi.
2007.
Modeling trust and influence in theblogosphere using link polarity.
In proceedings of theInternational Conference on Weblogs and SocialMedia, ICWSM.A.
Ng, M. Jordan, and Y. Weiss.
2001.
On spectralclustering: Analysis   and an algorithm.
In Dietterich,T., Becker, S., and Ghahramani, Z., editors,Advances in Neural Information Processing Systems,pages   849?856.B.
Long, Z. M. Zhang, and P. S. Yu.
2007.
Aprobabilistic framework for relational clustering.
InKDD ?07: Proceedings of the 13th ACM SIGKDDinternational conference on Knowledge discoveryand data mining, pages 470?479.B.
Poonen.
1999.
Inequalities.
Berkeley Math Circle.C.
D. Manning, P. Raghavan and H.  Schu tze.
2008.Introduction to Information Retrieval.
CambridgeUniversity Press.C.
H. Q. Ding, X.
He, H. Zha, M. Gu and H. D. Simon.2001.
A min-max cut algorithm for graphpartitioning and data clustering.
In Proceedings ofICDM 2001, pages 107?114.F.
Lin and W. Cohen.
2010.
Power iteration clustering.ICML.H.
Jeong, S. P. Mason, A. L. Barabasi and Z. N. Oltvai.2001.
Lethality and centrality in protein networks.Nature, 411(6833):41?42H.
Kwak, C. Lee, H. Park and S. Moon.
2010.
What istwitter, a social network or a news media?
InProceedings of the 19th international conference onWorld Wide Web, WWW ?10, pages 591?600.I.
Dhillon, Y. Guan, and B. Kulis.
2004.
A unified viewof kernel k-means, spectral clustering and graph cuts.Technical Report TR-04-25.
UTCS.J.
Shi and J.  Malik, 2000.
Normalized cuts and imagesegmentation.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 22(8):888?905.K.
Macropol, T. Can and A. Singh.
2009.
RRW:repeated random walks on genome-scale proteinnetworks for local cluster discovery.
BMCBioinformatics, 10(1):283+.L.
A. Adamic and N. Glance.
2005.
The politicalblogosphere and the 2004 U.S. election: divided theyblog.
In Proceedings of the 3rd internationalworkshop on Link discovery, LinkKDD ?05, pages36?43.M.
Meila and J. Shi.
2000.
Learning segmentation byrandom walks.
In NIPS, pages 873?879.S.
Bormann.
2004.
The expectation maximizationalgorithm: A short tutorial.S.
Schaeffer, 2007.
Graph clustering.
Computer ScienceReview, 1(1):27?64.S.
M. van Dongen, 2000.
Graph Clustering by FlowSimulation.
PhD   thesis, University of Utrecht, TheNetherlands.T.
N. Bui and C. Jones,   1993.
A Heuristic forReducing Fill-In in Sparse Matrix Factorization",  inProc.
PPSC, pp.445-452.U.
von Luxburg, 2007.
A tutorial on spectral clustering.Statistics and Computing, 17(4):395?416.Z.
Chen and H. Ji.
2010.
Graph-based clustering forcomputational linguistics: A survey.
In Proceedingsof the 2010 Workshop on Graph-based Methods forNatural Language Processing, TextGraphs-5, pages1?9, ACL.5
