Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 373?381,Sydney, July 2006. c?2006 Association for Computational LinguisticsStyle & Topic Language Model Adaptation Using HMM-LDABo-June (Paul) Hsu, James GlassMIT Computer Science and Artificial Intelligence Laboratory32 Vassar Street, Cambridge, MA 02139, USA{bohsu,glass}@mit.eduAbstractAdapting language models across stylesand topics, such as for lecture transcrip-tion, involves combining generic stylemodels with topic-specific content rele-vant to the target document.
In thiswork, we investigate the use of the Hid-den Markov Model with Latent DirichletAllocation (HMM-LDA) to obtain syn-tactic state and semantic topic assign-ments to word instances in the trainingcorpus.
From these context-dependentlabels, we construct style and topic mod-els that better model the target document,and extend the traditional bag-of-wordstopic models to n-grams.
Experimentswith static model interpolation yielded aperplexity and relative word error rate(WER) reduction of 7.1% and 2.1%, re-spectively, over an adapted trigram base-line.
Adaptive interpolation of mixturecomponents further reduced perplexityby 9.5% and WER by a modest 0.3%.1 IntroductionWith the rapid growth of audio-visual materialsavailable over the web, effective language mod-eling of the diverse content, both in style andtopic, becomes essential for efficient access andmanagement of this information.
As a primeexample, successful language modeling for aca-demic lectures not only enables the initial tran-scription via automatic speech recognition, butalso assists educators and students in the creationand navigation of these materials through annota-tion, retrieval, summarization, and even transla-tion of the embedded content.Compared with other types of audio content,lecture speech often exhibits a high degree ofspontaneity and focuses on narrow topics withspecific terminology (Furui, 2003; Glass et al2004).
Unfortunately, training corpora availablefor language modeling rarely match the targetlecture in both style and topic.
While transcriptsfrom other lectures better match the style of thetarget lecture than written text, it is often difficultto find transcripts on the target topic.
On theother hand, although topic-specific vocabularycan be gleaned from related text materials, suchas the textbook and lecture slides, written lan-guage is a poor predictor of how words are actu-ally spoken.
Furthermore, given that the precisetopic of a target lecture is often unknown a prioriand may even shift over time, it is generally dif-ficult to identify topically related documents.Thus, an effective language model (LM) need tonot only account for the casual speaking style oflectures, but also accommodate the topic-specificvocabulary of the subject matter.
Moreover, theability of the language model to dynamicallyadapt over the course of the lecture could proveextremely useful for both increasing transcriptionaccuracy, as well as providing evidence for lec-ture segmentation and information retrieval.In this paper, we investigate the application ofthe syntactic state and semantic topic assign-ments from the Hidden Markov Model with La-tent Dirichlet Allocation model to the problem oflanguage modeling.
We explore the use of thesecontext-dependent labels to identify style andlearn topics from both a large number of spokenlectures as well as written text.
By dynamicallyinterpolating lecture style models with topic-specific models, we obtain language models thatbetter describe the subtopic structure within alecture.
Initial experiments demonstrate a 16.1%perplexity reduction and a 2.4% WER reductionover an adapted trigram baseline.373In the following sections, we first summarizerelated research on adaptive and topic-mixturelanguage models, and describe previous work onthe HMM-LDA model.
We then examine theability of the model to learn syntactic classes aswell as topics from textbook materials and lec-ture transcripts.
Next, we describe a variety oflanguage model experiments we performed tocombine style and topic models constructed fromthe state and topic labels with conventional tri-gram models trained from both spoken and writ-ten materials.
We also demonstrate the use ofthe combined model in an on-line adaptive mode.Finally, we summarize the results of this researchand suggest future opportunities for related mod-eling techniques in spoken lecture and other con-tent processing research.2 Adaptive and Topic-Mixture LMsThe concept of adaptive and topic-mixture lan-guage models has been previously explored bymany researchers.
Adaptive language modelingexploits the property that words appearing earlierin a document are likely to appear again.
Cachelanguage models (Kuhn and De Mori, 1990;Clarkson and Robinson, 1997) leverage this ob-servation and increase the probability of previ-ously observed words in a document when pre-dicting the next word.
By interpolating with aconditional trigram cache model, Goodman(2001) demonstrated up to 34% decrease in per-plexity over a trigram baseline for small trainingsets.The cache intuition has been extended by at-tempting to increase the probability of unob-served but topically related words.
Specifically,given a mixture model with topic-specific com-ponents, we can increase the mixture weights ofthe topics corresponding to previously observedwords to better predict the next word.
Some ofthe early work in this area used a maximum en-tropy language model framework to trigger in-creases in likelihood of related words (Lau et al,1993; Rosenfeld, 1996).A variety of methods has been used to exploretopic-mixture models.
To model a mixture oftopics within a document, the sentence mixturemodel (Iyer and Ostendorf, 1999) builds multipletopic models from clusters of training sentencesand defines the probability of a target sentence asa weighted combination of its probability undereach topic model.
Latent Semantic Analysis(LSA) has been used to cluster topically relatedwords and has demonstrated significant reduc-tion in perplexity and word error rate (Belle-garda, 2000).
Probabilistic LSA (PLSA) hasbeen used to decompose documents into compo-nent word distributions and create unigram topicmodels from these distributions.
Gildea andHofmann (1999) demonstrated noticeable per-plexity reduction via dynamic combination ofthese unigram topic models with a generic tri-gram model.To identify topics from an unlabeled corpus,(Blei et al, 2003) extends PLSA with the LatentDirichlet Allocation (LDA) model that describeseach document in a corpus as generated from amixture of topics, each characterized by a wordunigram distribution.
Hidden Markov Modelwith LDA (HMM-LDA) (Griffiths et al, 2004)further extends this topic mixture model to sepa-rate syntactic words from content words whosedistributions depend primarily on local contextand document topic, respectively.In the specific area of lecture processing, pre-vious work in language model adaptation hasprimarily focused on customizing a fixed n-gramlanguage model for each lecture by combining n-gram statistics from general conversationalspeech, other lectures, textbooks, and other re-sources related to the target lecture (Nanjo andKawahara, 2002, 2004; Leeuwis et al, 2003;Park et al, 2005).Most of the previous work on topic-mixturemodels focuses on in-domain adaptation usinglarge amounts of matched training data.
How-ever, most, if not all, of the data available to traina lecture language model are either cross-domainor cross-style.
Furthermore, although adaptivemodels have been shown to yield significant per-plexity reduction on clean transcripts, the im-provements tend to diminish when working withspeech recognizer hypotheses with high WER.In this work, we apply the concept of dynamictopic adaptation to the lecture transcription task.Unlike previous work, we first construct a stylemodel and a topic-domain model using the clas-sification of word instances into syntactic statesand topics provided by HMM-LDA.
Further-more, we leverage the context-dependent labelsto extend topic models from unigrams to n-grams, allowing for better prediction of transi-tions involving topic words.
Note that althoughthis work focuses on the use of HMM-LDA togenerate the state and topic labels, any methodthat yields such labels suffices for the purpose ofthe language modeling experiments.
The follow-ing section describes the HMM-LDA frameworkin more detail.3743 HMM-LDA3.1 Latent Dirichlet AllocationDiscrete Principal Component Analysis describesa family of models that decompose a set of fea-ture vectors into its principal components (Bun-tine and Jakulin, 2005).
Describing feature vec-tors via their components reduces the number ofparameters required to model the data, hence im-proving the quality of the estimated parameterswhen given limited training data.
LSA, PLSA,and LDA are all examples from this family.Given a predefined number of desired compo-nents, LSA models feature vectors by finding aset of orthonormal components that maximizethe variance using singular value decomposition(Deerwester et al, 1990).
Unfortunately, thecomponent vectors may contain non-interpret-able negative values when working with wordoccurrence counts as feature vectors.
PLSAeliminates this problem by using non-negativematrix factorization to model each document as aweighted combination of a set of non-negativefeature vectors (Hofmann, 1999).
However, be-cause the number of parameters grows linearlywith the number of documents, the model isprone to overfitting.
Furthermore, because eachtraining document has its own set of topic weightparameters, PLSA does not provide a generativeframework for describing the probability of anunseen document (Blei et al, 2003).To address the shortcomings of PLSA, Blei etal.
(2003) introduced the LDA model, which fur-ther imposes a Dirichlet distribution on the topicmixture weights corresponding to the documentsin the corpus.
With the number of model pa-rameters dependent only on the number of topicmixtures and vocabulary size, LDA is less proneto overfitting and is capable of estimating theprobability of unobserved test documents.Empirically, LDA has been shown to outper-form PLSA in corpus perplexity, collaborativefiltering, and text classification experiments (Bleiet al, 2003).
Various extensions to the basicLDA model have since been proposed.
The Au-thor Topic model adds an additional dependencyon the author(s) to the topic mixture weights ofeach document (Rosen-Zvi et al, 2005).
TheHierarchical Dirichlet Process is a nonparametricmodel that generalizes distribution parametermodeling to multiple levels.
Without having toestimate the number of mixture components, thismodel has been shown to match the best resultfrom LDA on a document modeling task (Teh etal., 2004).3.2 Hidden Markov Model with LDAHMM-LDA model proposed by Griffiths et al(2004) combines the HMM and LDA models toseparate syntactic words with local dependenciesfrom topic-dependent content words without re-quiring any labeled data.
Similar to HMM-basedpart-of-speech taggers, HMM-LDA maps eachword in the document to a hidden syntactic state.Each state generates words according to a uni-gram distribution except the special topic state,where words are modeled by document-specificmixtures of topic distributions, as in LDA.Figure 1 describes this generative process inmore detail.Figure 1: Generative framework and graphicalmodel representation of HMM-LDA.
The num-ber of states and topics are pre-specified.
Thetopic mixture for each document is modeled witha Dirichlet distribution.
Each word wi in the n-word document is generated from its hidden statesi or hidden topic zi if si is the special topic state.Unlike vocabulary selection techniques thatseparate domain-independent words from topic-specific keywords using word collocation statis-tics, HMM-LDA classifies each word instanceaccording to its context.
Thus, an instance of theword ?return?
may be assigned to a syntacticstate in ?to return a?, but classified as a topickeyword in ?expected return for?.
By labelingeach word in the training set with its syntacticstate and mixture topic, HMM-LDA not onlyseparates stylistic words from content words in acontext-dependent manner, but also decomposesthe corpus into a set of topic word distributions.This form of soft, context-dependent classifica-For each document d in the corpus:1.
Draw topic weights d?
from )(Dirichlet ?2.
For each word wi in document d:a.
Draw topic zi from )l(Multinomia d?b.
Draw state si from )(ultinomialM 1?ispic.
Draw word wi from:=otherwisesiisiz)(lMultinomia)(lMultinomia??
topicsD  dz1 w1z2 w2zn wns1s2sn?
?
?375tion has many potential uses for language model-ing, topic segmentation, and indexing.3.3 TrainingTo train an HMM-LDA model, we employ theMATLAB Topic Modeling Toolbox 1.3 (Grif-fiths and Steyvers, 2004; Griffiths et al, 2004).This particular implementation performs Gibbssampling, a form of Markov chain Monte Carlo(MCMC), to estimate the optimal model parame-ters fitted to the training data.
Specifically, thealgorithm creates a Markov chain whose station-ary distribution matches the expected distributionof the state and topic labels for each word in thetraining corpus.
Starting from random labels,Gibbs sampling sequentially samples the labelfor each hidden variable conditioned on the cur-rent value of all other variables.
After a suffi-cient number of iterations, the Markov chainconverges to the stationary distribution.
We caneasily compute the posterior word distributionfor each state and topic from a single sample byaveraging over the label counts and prior pa-rameters.
With a sufficiently large training set,we will have enough words assigned to eachstate and topic to yield a reasonable approxima-tion to the underlying distribution.In the following sections, we examine the ap-plication of models derived from the HMM-LDAlabels to the task of spoken lecture transcriptionand explore techniques on adaptive topic model-ing to construct a better lecture language model.4 HMM-LDA AnalysisOur language modeling experiments have beenconducted on high-fidelity transcripts of ap-proximately 168 hours of lectures from three un-dergraduate subjects in math, physics, and com-puter science (CS), as well as 79 seminars cover-ing a wide range of topics (Glass et al, 2004).For evaluation, we withheld the set of 20 CS lec-tures and used the first 10 lectures as a develop-ment set and the last 10 lectures for the test set.The remainder of these data was used for trainingand will be referred to as the Lectures dataset.To supplement the out-of-domain lecture tran-scripts with topic-specific textual resources, weadded the CS course textbook (Textbook) as ad-ditional training data for learning the target top-ics.
To create topic-cohesive documents, thetextbook is divided at every section heading toform 271 documents.
Next, the text is heuristi-cally segmented at sentence-like boundaries andnormalized into the words corresponding to thespoken form of the text.
Table 1 summarizes thedata used in this evaluation.Dataset Documents Sentences Vocabulary WordsLectures 150 58,626 25,654 1,390,039Textbook 271 6,762 4,686 131,280CS Dev 10 4,102 3,285 93,348CS Test 10 3,595 3,357 87,518Table 1: Summary of evaluation datasets.In the following analysis, we ran the Gibbssampler against the Lectures dataset for a total of2800 iterations, computing a model every 10 it-erations, and took the model with the lowest per-plexity as the final model.
We built the modelwith 20 states and 100 topics based on prelimi-nary experiments.
We also trained an HMM-LDA model on the Textbook dataset using thesame model parameters.
We ran the sampler fora total of 2000 iterations, computing the perplex-ity every 100 iterations.
Again, we selected thelowest perplexity model as the final model.4.1 Semantic TopicsHMM-LDA extracts words whose distributionsvary across documents and clusters them into aset of components.
In Figure 2, we list the top10 words from a random selection of 10 topicscomputed from the Lectures dataset.
As shown,the words assigned to the LDA topic state arerepresentative of content words and are groupedinto broad semantic topics.
For example, topic 4,8, and 9 correspond to machine learning, linearalgebra, and magnetism, respectively.Since the Lectures dataset consists of speechtranscripts with disfluencies, it is interesting to1 2 3 4 5 6 7 8 9 10centerworldandideasnewtechnologyinnovationcommunityplacebuildingworkresearchrightpeoplecomputingnetworksysteminformationsoftwarecomputersrightshumanU.S.governmentinternationalcountriespresidentworldsupportsystemthingsrobotsystemsworkexamplepersonrobotslearningmachine<laugh>herchildrenbookCambridgebooksstreetcitylibrarybrother<partial>memoryahbrainanimalokayeyesynapticreceptorsmouseclasspeopletaxwealthsocialAmericanpowerworld<unintelligible>societybasisv<eh>vectormatrixtransformationlineareightoutputtmagneticcurrentfieldloopsurfacedirectionelawfluxmlightredwatercolorswhiteanglebluehererainbowsunFigure 2: The top 10 words from 10 randomly selected topics computed from the Lectures dataset.376observe that ?<laugh>?
is the top word in atopic corresponding to childhood memories.Cursory examination of the data suggests that thespeakers talking about children tend to laughmore during the lecture.
Although it may not bedesirable to capture speaker idiosyncrasies in thetopic mixtures, HMM-LDA has clearly demon-strated its ability to capture distinctive semantictopics in a corpus.
By leveraging all documentsin the corpus, the model yields smoother topicword distributions that are less vulnerable tooverfitting.Since HMM-LDA labels the state and topic ofeach word in the training corpus, we can alsovisualize the results by color-coding the wordsby their topic assignments.
Figure 3 shows acolor-coded excerpt from a topically coherentparagraph in the Textbook dataset.
Notice howmost of the content words (uppercase) are as-signed to the same topic/color.
Furthermore, ofthe 7 instances of the words ?and?
and ?or?
(underlined), 6 are correctly classified as syntac-tic or topic words, demonstrating the context-dependent labeling capabilities of the HMM-LDA model.
Moreover, from these labels, wecan identify multi-word topic key phrases (e.g.output signals, input signal, ?and?
gate) in addi-tion to standalone keywords, an observation wewill leverage later on with n-gram topic models.Figure 3: Color-coded excerpt from the Textbookdataset showing the context-dependent topic la-bels.
Syntactic words appear black in lowercase.Topic words are shown in uppercase with theirrespective topic colors.
All instances of thewords ?and?
and ?or?
are underlined.4.2 Syntactic StatesSince the syntactic states are shared across alldocuments, we expect words associated with thesyntactic states when applying HMM-LDA to theLectures dataset to reflect the lecture style vo-cabulary.In Figure 4, we list the top 10 words from eachof the 19 syntactic states (state 20 is the topicstate).
Note that each state plays a clear syntacticrole.
For example, state 2 contains prepositionswhile state 7 contains verbs.
Since the model istrained on transcriptions of spontaneous speech,hesitation disfluencies (<uh>, <um>, <partial>)are all grouped in state 3 along with other words(so, if, okay) that frequently indicate hesitation.While many of these hesitation words are con-junctions, the words in state 6 show that mostconjunctions are actually assigned to a differentstate representing different syntactic behaviorfrom hesitations.
As demonstrated with sponta-neous speech, HMM-LDA yields syntactic statesthat have a good correspondence to part-of-speech labels, without requiring any labeledtraining data.4.3 DiscussionsAlthough MCMC techniques converge to theglobal stationary distribution, we cannot guaran-tee convergence from observation of the perplex-ity alone.
Unlike EM algorithms, random sam-pling may actually temporarily decrease themodel likelihood.
Thus, in the above analysis,the number of iterations was chosen to be at leastdouble the point at which the perplexity first ap-peared to converge.In addition to the number of iterations, thechoice of the number of states and topics, as wellas the values of the hyper-parameters on theDirichlet prior, also impact the quality and effec-tiveness of the resulting model.
Ideally, we runthe algorithm with different combinations of theparameter values and perform model selection tochoose the model with the best complexity-penalized likelihood.
However, given finitecomputing resources, this approach is often im-We draw an INVERTER SYMBOLICALLY as in Figure 3.24.An AND GATE, also shown in Figure 3.24, is a PRIMITIVEFUNCTION box with two INPUTS and ONE OUTPUT.
Itdrives its OUTPUT SIGNAL to a value that is the LOGICALAND of the INPUTS.
That is, if both of its INPUT SIGNALSBECOME 1.
Then ONE and GATE DELAY time later the ANDGATE will force its OUTPUT SIGNAL TO be 1; otherwise theOUTPUT will be 0.
An OR GATE is a SIMILAR two INPUTPRIMITIVE FUNCTION box that drives its OUTPUT SIGNALto a value that is the LOGICAL OR of the INPUTS.
That is, theOUTPUT will BECOME 1 if at least ONE of the INPUTSIGNALS is 1; otherwise the OUTPUT will BECOME 0.1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19thethisathatthesemyouryourthosetheirofinforonwithatfrombyaboutasso<uh>if<um><partial>nowthenokaywellbutknowseedothinkgogetsaymakelooktakeIyouwetheyletlet'sheI'llpeopleI'dandbutorbecauseasthatwherethankwhichisisarewashasweregoeshadcomesmeanssaysit'snotthat'sI'mjustthere's<uh>we'realsoyou'reityououtupthemthatmeabouthereallaansomeonenointwoanythisanotherwaytimethinglotquestionkindpointcaseideaproblemitthisthattherewhichheherecoursewhotheytwoonethreehundredmtfivedyearsfourgoingdoingonelookingsortdoneablecomingtalkingtryingthatwhathowwherewhenifwhywhichasbecausecanwillwoulddon'tcoulddojustmeshouldmayverymorelittlemuchgooddifferentthanimportantlongastojustlongerdoesn'tnevergophysicallythat'llanybody'swithhavebewanthadgetlikegotneedtrytakeFigure 4: The top 10 words from the 19 syntactic states computed from the Lectures dataset.377practical.
As an alternative for future work, wewould like to perform Gibbs sampling on thehyper-parameters (Griffiths et al, 2004) and ap-ply the Dirichlet process to estimate the numberof states and topics (Teh et al, 2004).Despite the suboptimal choice of parametersand potential lack of convergence, the labels de-rived from HMM-LDA are still effective for lan-guage modeling applications, as described next.5 Language Modeling ExperimentsTo evaluate the effectiveness of models derivedfrom the separation of syntax from content, weperformed experiments that compare the per-plexities and WERs of various model combina-tions.
For a baseline, we used an adapted model(L+T) that linearly interpolates trigram modelstrained on the Lectures (L) and Textbook (T)datasets.
In all models, all interpolation weightsand additional parameters are tuned on a devel-opment set consisting of the first half of the CSlectures and tested on the second half.
Unlessotherwise noted, modified Kneser-Ney discount-ing (Chen and Goodman, 1998) is applied withthe respective training set vocabulary using theSRILM Toolkit (Stolcke, 2002).To compute the word error rates associatedwith a specific language model, we used aspeaker-independent speech recognizer (Glass,2003).
The lectures were pre-segmented intoutterances by forced alignment of the referencetranscription.5.1 Lecture StyleIn general, an n-gram model trained on a limitedset of topic-specific documents tends to overem-phasize words from the observed topics insteadof evenly distributing weights over all potentialtopics.
Specifically, given the list of words fol-lowing an n-gram context, we would like todeemphasize the observed occurrences of topicwords and ideally redistribute these counts to allpotential topic words.
As an approximation, wecan build such a topic-deemphasized style tri-gram model (S) by using counts of only n-gramsequences that do not end on a topic word,smoothed over the Lectures vocabulary.
Figure5 shows the n-grams corresponding to an utter-ance used to build the style trigram model.
Notethat the counts of topic to style word transitionsare not altered as these probabilities are mostlyindependent of the observed topic distribution.By interpolating the style model (S) fromabove with the smoothed trigram model based onthe Lectures dataset (L), the combined model(L+S) achieves a 3.6% perplexity reduction and1.0% WER reduction over (L), as shown in Table2.
Without introducing topic-specific trainingdata, we can already improve the generic lectureLM performance using the HMM-LDA labels.<s> for the SPATIAL MEMORY </s>unigrams: for, the, spatial, memory, </s>bigrams: <s> for, for the, the spatial, spatial memory, memory </s>trigrams: <s> <s> for, <s> for the, for the spatial,the spatial memory, spatial memory </s>Figure 5: Style model n-grams.
Topic words inthe utterance are in uppercase.5.2 Topic DomainUnlike Lectures, the Textbook dataset containscontent words relevant to the target lectures, butin a mismatched style.
Commonly, the Textbooktrigram model is interpolated with the genericmodel to improve the probability estimates of thetransitions involving topic words.
The interpola-tion weight is chosen to best fit the probabilitiesof these n-gram sequences while minimizing themismatch in style.
However, with only one pa-rameter, all n-gram contexts must share the samemixture weight.
Because transitions from con-texts containing topic words are rarely observedin the off-topic Lectures, the Textbook model (T)should ideally have higher weight in these con-texts than contexts that are more equally ob-served in both datasets.One heuristic approach for adjusting theweight in these contexts is to build a topic-domain trigram model (D) from the Textbook n-gram counts with Witten-Bell smoothing (Chenand Goodman, 1998) where we emphasize thesequences containing a topic word in the contextby doubling their counts.
In effect, this reducesthe smoothing on words following topic contextswith respect to lower-order models without sig-nificantly affecting the transitions from non-topicwords.
Figure 6 shows the adjusted counts for anutterance used to build the domain trigrammodel.<s> HUFFMAN CODE can be represented as a BINARY TREE ?unigrams: huffman, code, can, be, represented, as, binary, tree, ?bigrams: <s> huffman, huffman code (2?
), code can (2?
),can be, be represented, represented as, a binary,binary tree (2?
), ?trigrams: <s> <s> hufmann, <s> hufmann code (2?
),hufmann code can (2?
), code can be (2?
),can be represented, be represented as,represented as a, as a binary, a binary tree (2?
), ...Figure 6: Domain model n-grams.
Topic wordsin the utterance are in uppercase.378Empirically, interpolating the lectures, text-book, and style models with the domain model(L+T+S+D) further decreases the perplexity by1.4% and WER by 0.3% over (L+T+S), validat-ing our intuition.
Overall, the addition of thestyle and domain models reduces perplexity andWER by a noticeable 7.1% and 2.1%, respec-tively, as shown in Table 2.PerplexityModel Development TestL: Lectures Trigram 180.2 (0.0%) 199.6 (0.0%)T: Textbook Trigram 291.7 (+61.8%) 331.7 (+66.2%)S: Style Trigram 207.0 (+14.9%) 224.6 (+12.5%)D: Domain Trigram 354.1 (+96.5%) 411.6 (+106.3%)L+S 174.2 (?3.3%) 192.4 (?3.6%)L+T: Baseline 138.3 (0.0%) 154.4 (0.0%)L+T+S 131.0 (?5.3%) 145.6 (?5.7%)L+T+S+D 128.8 (?6.9%) 143.6 (?7.1%)L+T+S+D+Topic100?
Static Mixture (cheat)?
Dynamic Mixture118.1 (?14.6%)115.7 (?16.4%)131.3 (?15.0%)129.5 (?16.1%)Word Error RateModel Development TestL: Lectures Trigram 49.5% (0.0%) 50.2% (0.0%)L+S 49.2% (?0.7%) 49.7% (?1.0%)L+T: Baseline 46.6% (0.0%) 46.7% (0.0%)L+T+S 46.0% (?1.2%) 45.8% (?1.8%)L+T+S+D 45.8% (?1.8%) 45.7% (?2.1%)L+T+S+D+Topic100?
Static Mixture (cheat)?
Dynamic Mixture45.5% (?2.4%)45.4% (?2.6%)45.4% (?2.8%)45.6% (?2.4%)Table 2: Perplexity (top) and WER (bottom) per-formance of various model combinations.
Rela-tive reduction is shown in parentheses.5.3 Textbook TopicsIn addition to identifying content words, HMM-LDA also assigns words to a topic based on theirdistribution across documents.
Thus, we canapply HMM-LDA with 100 topics to the Text-book dataset to identify representative words andtheir associated contexts for each topic.
Fromthese labels, we can build unsmoothed trigramlanguage models (Topic100) for each topic fromthe counts of observed n-gram sequences thatend in a word assigned to the respective topic.Figure 7 shows a sample of the word n-gramsidentified via this approach for a few topics.Note that some of the n-grams are key phrasesfor the topic while others contain a mixture ofsyntactic and topic words.
Unlike bag-of-wordsmodels that only identify the unigram distribu-tion for each topic, the use of context-dependentlabels enables the construction of n-gram topicmodels that not only characterize the frequenciesof topic words, but also describe the transitioncontexts leading up to these words.Huffman treerelative frequencyrelative frequenciesthe treeone hundredMonte Carlorand updaterandom numberstrials remainingtrials passedtime segmentthe agendasegment timecurrent timefirst agendaassoc keythe tablelocal tablea tableof recordsFigure 7: Sample of n-grams from select topics.5.4 Topic MixturesSince each target lecture generally only covers asubset of the available topics, it will be ideal toidentify the specific topics corresponding to atarget lecture and assign those topic models moreweight in a linearly interpolated mixture model.As an ideal case, we performed a cheating ex-periment to measure the best performance of astatically interpolated topic mixture model(L+T+S+D+Topic100) where we tuned themixture weights of all mixture components, in-cluding the lectures, textbook, style, domain, andthe 100 individual topic trigram models on indi-vidual target lectures.Table 2 shows that by weighting the compo-nent models appropriately, we can reduce theperplexity and WER by an additional 7.9% and0.7%, respectively, over the (L+T+S+D) modeleven with simple linear interpolation for modelcombination.To gain further insight into the topic mixturemodel, we examine the breakdown of the nor-malized topic weights for a specific lecture.
Asshown in Figure 8, of the 100 topic models, 15 ofthem account for over 90% of the total weight.Thus, lectures tend to show a significant topicskew which topic adaptation approaches canmodel effectively.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Figure 8: Topic mixture weight breakdown.5.5 Topic AdaptationUnfortunately, since different lectures cover dif-ferent topics, we generally cannot tune the topicmixture weights ahead of time.
One approach,without any a priori knowledge of the target lec-ture, is to adaptively estimate the optimal mix-ture weights as we process the lecture (Gildeaand Hofmann, 1999).
However, since the topicdistribution shifts over a long lecture, modeling alecture as an interpolation of components withfixed weights may not be the most optimal.
In-stead, we employ an exponential decay strategywhere we update the current mixture distributionby linearly interpolating it with the posteriortopic distribution given the current word.
Spe-cifically, applying Bayes?
rule, the probability oftopic t generating the current word w is given by:379( ) ( ) ( )( ) ( ) ??
?= t tPtwPtPtwPwtP |||To achieve the exponential decay, we update thetopic distribution after each word according toPi+1(t) = (1 ?
  )Pi(t) +   P(t | wi), where    is theadaptation rate.We evaluated this approach of dynamic mix-ture weight adaptation on the (L+T+S+D+Topic100) model, with the same set of components asthe cheating experiment with static weights.
Asshown in Table 2, the dynamic model actuallyoutperforms the static model by more than 1% inperplexity, by better modeling the dynamic topicsubstructure within the lecture.To run the recognizer with a dynamic LM, werescored the top 100 hypotheses generated withthe (L+T+S+D) model using the dynamic LM.The WER obtained through such n-best rescoringyielded noticeable improvements over the(L+T+S+D) model without a priori knowledgeof the topic distribution, but did not beat the op-timal static model on the test set.To further gain an intuition for mixture weightadaptation, we plotted the normalized adaptedweights of the topic models across the first lec-ture of the test set in Figure 9.
Note that thetopic mixture varies greatly across the lecture.
Inthis particular lecture, the lecturer starts out witha review of the previous lecture.
Subsequently,he shows an example of computation using ac-cumulators.
Finally, he focuses the lecture onstream as a data structure, with an interveningexample that finds pairs of i and j that sum up toa prime.
By comparing the topic labels in Figure9 with the top words from the corresponding top-ics in Figure 10, we observe that the topicweights obtained via dynamic adaptation matchthe subject matter of the lecture fairly closely.Finally, to assess the effect that word error ratehas on adaptation performance, we applied theadaptation algorithm to the corresponding tran-script from the automatic speech recognizer(ASR).
Traditional cache language models tendto be vulnerable to recognition errors since incor-rect words in the history negatively bias the pre-diction of the current word.
However, by adapt-ing at a topic level, which reduces the number ofdynamic parameters, the dynamic topic model isless sensitive to recognition errors.
As seen inFigure 9, even with a word error rate around40%, the normalized topic mixture weights fromthe ASR transcript still show a strong resem-blance to the original weights from the manualreference transcript.Figure 9: Adaptation of topic model weights onmanual and ASR transcription of a single lecture.T12 T35 T98 T99streamsstreamsintegersseriesprimefilterdelayedinterleaveinfinitepairsijkpairsintegerssumqueenstsequenceenumerateaccumulatemapintervalfiltersequencesoperationsoddnilofseeandinforvsregisterdataasmakeFigure 10: Top 10 words from select Textbooktopics appearing in Figure 9.6 Summary and ConclusionsIn this paper, we have shown how to leveragecontext-dependent state and topic labels, such asthe ones generated by the HMM-LDA model, toconstruct better language models for lecture tran-scription and extend topic models beyond tradi-tional unigrams.
Although the WER of the toprecognizer hypotheses exceeds 45%, by dynami-cally updating the mixture weights to model thetopic substructure within individual lectures, weare able to reduce the test set perplexity andWER by over 16% and 2.4%, respectively, rela-tive to the combined Lectures and Textbook(L+T) baseline.Although we primarily focused on lecturetranscription in this work, the techniques extendto language modeling scenarios where exactlymatched training data are often limited or non-existent.
Instead, we have to rely on appropriatecombination of models derived from partiallymatched data.
HMM-LDA and related tech-niques show great promise for finding structurein unlabeled data, from which we can build moresophisticated models.The experiments in this paper combine modelsprimarily through simple linear interpolation.
Asmotivated in section 5.2, allowing for context-dependent interpolation weights based on topic380labels may yield significant improvement forboth perplexity and WER.
Thus, in future work,we would like to study algorithms for automati-cally learning appropriate context-dependent in-terpolation weights.
Furthermore, we hope toimprove the convergence properties of the dy-namic adaptation scheme at the start of lecturesand across topic transitions.
Lastly, we wouldlike to extend the LDA framework to supportspeaker-specific adaptation and apply the result-ing topic distributions to lecture segmentation.AcknowledgementsWe would like to thank the anonymous review-ers for their useful comments and feedback.Support for this research was provided in part bythe National Science Foundation under grant#IIS-0415865.
Any opinions, findings, and con-clusions, or recommendations expressed in thismaterial are those of the authors and do not nec-essarily reflect the views of the NSF.ReferenceY.
Akita and T. Kawahara.
2004.
Language ModelAdaptation Based on PLSA of Topics and Speak-ers.
In Proc.
ICSLP.J.
Bellegarda.
2000.
Exploiting Latent Semantic In-formation in Statistical Language Modeling.
InProc.
IEEE, 88(8):1279-1296.D.
Blei, A. Ng, and M. Jordan.
1993.
LatentDirichlet Allocation.
Journal of Machine LearningResearch, 3:993-1022.W.
Buntine and A. Jakulin.
2005.
Discrete PrincipalComponent Analysis.
Technical Report, HelsinkiInstitute for Information Technology.S.
Chen and J. Goodman.
1996.
An Empirical Studyof Smoothing Techniques for Language Modeling.In Proc.
ACL, 310-318.P.
Clarkson and A. Robinson.
1997.
LanguageModel Adaptation Using Mixtures and an Expo-nentially Decaying Cache.
In Proc.
ICASSP.S.
Deerwester, S. Dumais, G. Furnas, T. Landauer, R.Harshman.
1990.
Indexing by Latent SemanticAnalysis.
Journal of the American Society for In-formation Science, 41(6):391-407.S.
Furui.
2003.
Recent Advances in SpontaneousSpeech Recognition and Understanding.
In Proc.IEEE Workshop on Spontaneous Speech Proc.
andRec, 1-6.D.
Gildea and T. Hofmann.
1999.
Topic-Based Lan-guage Models Using EM.
In Proc.
Eurospeech.J.
Glass.
2003.
A Probabilistic Framework for Seg-ment-based Speech Recognition.
Computer, Speechand Language, 17:137-152.J.
Glass, T.J. Hazen, L. Hetherington, and C. Wang.2004.
Analysis and Processing of Lecture AudioData: Preliminary Investigations.
In Proc.
HLT-NAACL Workshop on Interdisciplinary Approachesto Speech Indexing and Retrieval, 9-12.J.
Goodman.
2001.
A Bit of Progress in LanguageModeling (Extended Version).
Technical Report,Microsoft Research.T.
Griffiths and M. Steyvers.
2004.
Finding Scien-tific Topics.
In Proc.
National Academy of Sci-ence, 101(Suppl.
1):5228-5235.T.
Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum.2004.
Integrating Topics and Syntax.
Adv.
in Neu-ral Information Processing Systems, 17:537-544.R.
Iyer and M. Ostendorf.
1999.
Modeling LongDistance Dependence in Language: Topic MixturesVersus Dynamic Cache.
In IEEE Transactions onSpeech and Audio Processing, 7:30-39.R.
Kuhn and R. De Mori.
1990.
A Cache-BasedNatural Language Model for Speech Recognition.In IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 12:570-583.R.
Lau, R. Rosenfeld, S. Roukos.
1993.
Trigger-Based Language Models: a Maximum EntropyApproach.
In Proc.
ICASSP.E.
Leeuwis, M. Federico, and M. Cettolo.
2003.
Lan-guage Modeling and Transcription of the TEDCorpus Lectures.
In Proc.
ICASSP.H.
Nanjo and T. Kawahara.
2002.
UnsupervisedLanguage Model Adaptation for Lecture SpeechRecognition.
In Proc.
ICSLP.H.
Nanjo and T. Kawahara.
2004.
Language Modeland Speaking Rate Adaptation for SpontaneousPresentation Speech Recognition.
In IEEE Trans.SAP, 12(4):391-400.A.
Park, T. Hazen, and J.
Glass.
2005.
AutomaticProcessing of Audio Lectures for Information Re-trieval: Vocabulary Selection and Language Mod-eling.
In Proc.
ICASSP.M.
Rosen-Zvi, T. Griffiths, M. Steyvers, and P.Smyth.
2004.
The Author-Topic Model for Au-thors and Documents.
20th Conference on Uncer-tainty in Artificial Intelligence.R.
Rosenfeld.
1996.
A Maximum Entropy Approachto Adaptive Statistical Language Modeling.
Com-puter, Speech and Language, 10:187-228.A.
Stolcke.
2002.
SRILM ?
An Extensible LanguageModeling Toolkit.
In Proc.
ICSLP.Y.
Teh, M. Jordan, M. Beal, and D.  Blei.
2006.
Hi-erarchical Dirichlet Processes.
To appear in Jour-nal of the American Statistical Association.381
