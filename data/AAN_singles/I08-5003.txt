Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 5?16,Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language ProcessingNamed Entity Recognition for South and South East Asian Languages:Taking StockAnil Kumar SinghLanguage Technologies Research CentreIIIT, Hyderabad, Indiaanil@research.iiit.ac.inAbstractIn this paper we first present a brief discus-sion of the problem of Named Entity Recog-nition (NER) in the context of the IJCNLPworkshop on NER for South and South EastAsian (SSEA) languages1 .
We also present ashort report on the development of a namedentity annotated corpus in five South Asianlanguage, namely Hindi, Bengali, Telugu,Oriya and Urdu.
We present some detailsabout a new named entity tagset used for thiscorpus and describe the annotation guide-lines.
Since the corpus was used for a sharedtask, we also explain the evaluation mea-sures used for the task.
We then presentthe results of our experiments on a baselinewhich uses a maximum entropy based ap-proach.
Finally, we give an overview of thepapers to be presented at the workshop, in-cluding those from the shared task track.
Wediscuss the results obtained by teams partic-ipating in the task and compare their resultswith the baseline results.1 IntroductionOne of the motivations for organizing a workshop(NERSSEAL-08) focused on named entities (NEs)was that they have a special status in Natural Lan-guage Processing (NLP) because they have someproperties which other elements of human languagesdo not have, e.g.
they refer to specific things or con-cepts in the world and are not listed in the grammars1http://ltrc.iiit.ac.in/ner-ssea-08or the lexicons.
Identifying and classifying them au-tomatically can help us in processing text becausethey form a significant portion of the types and to-kens occurring in a corpus.
Also, because of theirvery nature, machine learning techniques have beenfound to be very useful in identifying them.
In orderto use these machine learning techniques, we needcorpus annotated with named entities.
In this paperwe describe such a corpus developed for five SouthAsian languages.
These languages are Hindi, Ben-gali, Oriya, Telugu and Urdu.This paper also presents an overview of the workdone for the IJCNLP workshop on NER for SSEAlanguages.
The workshop included two tracks.
Thefirst track was for regular research papers, while thesecond was organized on the lines of a shared task.Fairly mature named entity recognition systemsare now available for European languages (Sang,2002; Sang and De Meulder, 2003), especially En-glish, and even for East Asian languages (Sassanoand Utsuro, 2000).
However, for South and SouthEast Asian languages, the problem of NER is still farfrom being solved.
Even though we can gain muchinsight from the methods used for English, there aremany issues which make the nature of the problemdifferent for SSEA languages.
For example, theselanguages do not have capitalization, which is a ma-jor feature used by NER systems for European lan-guages.Another characteristic of these languages is thatmost of them use scripts of Brahmi origin, whichhave highly phonetic characteristics that could beutilized for multilingual NER.
For some languages,there are additional issues like word segmentation5(e.g.
for Thai).
Large gazetteers are not avail-able for most of these languages.
There is alsothe problem of lack of standardization and spellingvariation.
The number of frequently used words(common nouns) which can also be used as names(proper nouns) is very large for, unlike for Euro-pean languages where a larger proportion of the firstnames are not used as common words.
For exam-ple, ?Smith?, ?John?, ?Thomas?
and ?George?
etc.
arealmost always used as person names, but ?Anand?,?Vijay?, ?Kiran?
and even ?Manmohan?
can be (morethan often) used as common nouns.
And the fre-quency with which they can be used as commonnouns as against person names is more or less unpre-dictable.
The context might help in disambiguating,but this issue does make the problem much harderthan for English.Among other problems, one example is that of thevarious ways of representing abbreviations.
Becauseof the alpha-syllabic nature of the SSEA scripts, ab-breviation can be expressed through a sequence ofletters or syllables.
In the latter case, the syllablesare often combined together to form a pseudo-word,e.g.
BAjapA (bhaajapaa) for Bharatiya Janata Partyor BJP.But most importantly, there is a serious lack oflabeled data for machine learning.
As part of thisworkshop, we have tried to prepare some data but wewill need much more data for really accurate NERsystems.Since most of the South and South East Asian lan-guages are scarce in resources as well as tools, it isvery important that good systems for NER be avail-able, because many problems in information extrac-tion and machine translation (among others) are de-pendent on accurate NER.The need for a workshop specifically for SSEAlanguages was felt because the South and South EastAsian region has many major and numerous minorlanguages.
In terms of the number of speakers thereare at least four in any list of top ten languages of theworld.
For practical reasons, we focus only on themajor languages in the workshop (and in this paper).Most of the major languages belong to two families:Indo-European and Dravidian.
There are a lot of dif-ferences among these languages, but there are a lotof similarities too, even across families (Emeneau,1956; Emeneau, 1980).
For the reasons mentionedabove, NER is perhaps more difficult for SSEA lan-guages than for European languages.
For better orfor worse, there too many languages and too few re-sources.
Moreover, these languages are also com-paratively less studied by researchers.
However, wecan benefit from the similarities across these lan-guages to build multilingual systems so as to reducethe overall cost and effort required.All the issues mentioned above show that wemight need different methods for solving the NERproblem for SSEA languages.
However, for com-paring the results of these different methods, we willneed a reasonably good baseline.
A mature systemtuned for English but trained on SSEA language datacan become such a baseline.
We will describe such abaseline in a later section.
This baseline system hasbeen tested on the data provided for the shared task.We present the results for all five languages underthe settings required for the shared task.2 Related WorkVarious techniques have been used for solving theNER problem (Mikheev et al, 1999; Borthwick,1999; Cucerzan and Yarowsky, 1999; Chieu and Ng,2003; Klein et al, 2003; Kim and Woodland, 2000)ranging from naively using gazetteers to rules basedtechniques to purely statistical techniques, even hy-brid approaches.
Several workshops consisting ofshared tasks (Sang, 2002; Sang and De Meulder,2003) have been held with specific focus on thisproblem.
In this section we will mention some oftechniques used previously.Most of the approaches can be classified based onthe features they use, whether they are rule based ormachine learning based or hybrid approaches.
Someof the commonly used features are:?
Word form and part of speech (POS) tags?
Orthographic features like capitalization, deci-mal, digits?
Word type patterns?
Conjunction of types like capitalization,quotes, functional words etc.?
Bag of words?
Trigger words like New York City6Tag Name DescriptionNEP Person Bob Dylan, Mohandas GandhiNED Designation General Manager, CommissionerNEO Organization Municipal CorporationNEA Abbreviation NLP, B.J.P.NEB Brand Pepsi, Nike (ambiguous)NETP Title-Person Mahatma, Dr., Mr.NETO Title-Object Pride and Prejudice, OthelloNEL Location New Delhi, ParisNETI Time 3rd September, 1991 (ambiguous)NEN Number 3.14, 4,500NEM Measure Rs.
4,500, 5 kgNETE Terms Maximum Entropy, ArcheologyTable 1: The named entity tagset used for the shared task?
Affixes like Hyderabad, Rampur,Mehdipatnam, Lingampally?
Gazetteer features: class in the gazetteer?
Left and right context?
Token length, e.g.
the number of letters in aword?
Previous history in the document or the corpus?
Classes of preceding NEsThe machine learning techniques tried for NERinclude the following:?
Hidden Markov Models or HMM (Zhou andSu, 2001)?
Decision Trees (Isozaki, 2001)?
Maximum Entropy (Borthwick et al, 1998)?
Support Vector Machines or SVM (Takeuchiand Collier, 2002)?
Conditional Random Fields or CRF (Settles,2004)Different ways of classifying named entities havebeen used, i.e., there are more than one tagsets forNER.
For example, the CoNLL 2003 shared task2had only four tags: persons, locations, organizations2http://www.cnts.ua.ac.be/conll2003/ner/and miscellaneous.
On the other hand, MUC-63 hasa near ontology for information extraction purposes.In this (MUC-6) tagset, there are three4 main kindsof NEs: ENAMEX (persons, locations and organi-zations), TIMES (time expressions) and NUMEX(number expresssions).There has been some previous work on NERfor SSEA languages (McCallum and Li, 2003;Cucerzan and Yarowsky, 1999), but most of the timesuch work was an offshoot of the work done for Eu-ropean languages.
Even including the current work-shop, the work on NER for SSEA languages is stillin the initial stages as the results reported by papersin this workshop clearly show.3 A New Named Entity TagsetThe tagset being used for the NERSSEAL-08 sharedtask consists of more tags than the four tags usedfor the CoNLL 2003 shared task.
The reason weopted for these tags was that we needed a slightlyfiner tagset for machine translation (MT).
The ini-tial aim was to improve the performance of the MTsystem.As annotation progressed, we realized that therewere some problems that we had not anticipated.Some classes were hard to distinguish in some con-texts, making the task hard for annotators and bring-ing in inconsistencies.
For example, it was not al-ways clear whether something should be marked as3http://cs.nyu.edu/cs/faculty/grishman/muc6.html4http://cs.nyu.edu/cs/faculty/grishman/NEtask20.book 6.html7Number or as Measure.
Similarly for Time and Mea-sure.
Another difficult class was that of (technical)terms.
Is ?agriculture?
a term or not?
If no (as mostpeople would say), is ?horticulture?
a term or not?
Infact, Term was the most difficult class to mark.An option that we explored was to merge theabove mentioned confusable classes and ignore theTerm class.
But we already had a relatively largecorpus marked up with these classes.
If we mergedsome classes and ignored the Term class (which hada very large coverage and is definitely going to beuseful for MT), we would be throwing away a lotof information.
And we also had some corpus an-notated by others which was based on a differenttagset.
So some problems were inevitable.
Finally,we decided to keep the original tagset, with onemodification.
The initial tagset had only eleven tags.The problem was that there was one Title tag butit had two different meanings: ?Mr.?
is a Title, but?The Seven Year Itch?
is also a Title.
This tag clearlyneeded to be split into two: Title-Person and Title-ObjectWe should mention here that we considered usinganother tagset developed at AUKBC, Chennai.
Thiswas based on ENAMEX, TIMEX and NUMEX.
Thetotal number of tags in this tagset is more than a hun-dred and it is meant specifically for MT and only forcertain domains (health, tourism).
Moreover, this isa tagset for entities in general, not just named enti-ties.The twelve tags in our tagset are briefly explainedin Table-1.
In the next section we mention the con-straints under which the annotated corpus was cre-ated, using this tagset.4 Annotation ConstraintsThe annotated corpus was created under severe con-straints.
The annotation was to be for five languagesby different teams, sometimes with very little com-munication during the process of annotation.
As aresult, there were many logistical problems.There were other practical constraints like the factthat this was not a funded project and all the workwas mainly voluntary.
Another major constraint forall the languages except Hindi was time.
There wasnot enough time for cross validation as the corpuswas required by a deadline.
To keep annotation rea-sonably consistent, annotation guidelines were cre-ated and a common format was specified.5 Annotation GuidelinesThe annotation guidelines were of two kinds.
Onewas meant for preparing training data through man-ual annotation.
The other one was meant for prepar-ing reference data as well as for automatic annota-tion.
The main guidelines for preparing the trainingdata are as follows:?
Specificity: The most important criterion whiledeciding whether some expression is a namedentity or not is to see whether that expressionspecifies something definite and identifiable asif by a name or not.
This decision will have tobe based on the context.
For example, ?aanand?
(in South Asian languages, where there is nocapitalization) is not a named entity in ?sabaaanand hii aanand hai?
(?There is bliss every-where?).
But it is a named entity in ?aanandkaa yaha aakhiri saala hai?
(?Anand is in thelast year (of his studies)?).
Number, Measureand Term may be seen as exceptions (see be-low).?
Maximal Entity: Only the maximal entitieshave to be annotated for training data.
Struc-ture of entities will not be annotated by theannotators, even though it has to be learnt bythe NER systems.
For example, ?One HundredYears of Solitude?
has to be annotated as oneentity.
?One Hundred?
is not to be marked asa Number here, nor is ?One Hundred Years?
tobe made marked as a Measure in this case.
Thepurpose of this guideline is to make the task ofannotation for several languages feasible, giventhe constraints.?
Ambiguity: In cases where an entity can havetwo valid tags, the more appropriate one is tobe used.
The annotator has to make the deci-sion in such cases.
It is recommended that theannotation be validated by another person, oreven more preferably, two different annotatorshave to work on the same data independentlyand inconsistencies have to be resolved by anadjudicator.
Abbreviation is an exception to theAmbiguity guideline (see below).8Some other guidelines for specific tags are listedbelow:?
Abbreviations: All abbreviations have to bemarked as Abbreviations, Even though everyabbreviation is also some other kind of namedentity.
For example, APJ is an Abbreviation,but also a Person.
IBM is also an Organiza-tion.
Such ambiguity cannot be resolved fromthe context because it is due to the (wrong?
)assumption that a named entity can have onlyone tag.
Multiple annotations were not al-lowed.
This is an exception to the third guide-line above.?
Designation and Title-Person: An entity is aDesignation if it represents something formaland official status with certain responsibilities.If it is just something honorary, then it is aTitle-Object.
For example, ?Event Coordina-tor?
or ?Research Assistant?
is a Designation,but ?Chakravarti?
or ?Mahatma?
are Titles.?
Organization and Brand: The distinction be-tween these two has to be made based on thecontext.
For example, ?Pepsi?
could mean anOrganization, but it is more likely to mean aBrand.?
Time and Location: Whether something is tobe marked as Time or Location or not is to bedecided based on the Specificity guideline andthe context.?
Number, Measure and Term: These three maynot be strictly named entities in the way a per-son name is.
However, we have included thembecause they are different from other words ofthe language.
For problems like machine trans-lation, they can be treated like named entities.For example, a Term is a word which can be di-rectly translated into some language if we havea dictionary of technical terms.
Once we knowa word is a Term, there is likely to be less am-biguity about the intended sense of the word,unlike for other normal words.The second set of guidelines are different from thefirst set mainly in one respect: the corpus has to beannotated with not just the maximal NEs, but withall levels of NEs, i.e., nested NEs also have to bemarked.Nested entities were introduced because one ofthe requirements was that the corpus be useful forbuilding systems which can become parts of a ma-chine translation (MT) system.
Nested entities canbe useful for MT systems because, quite often, partsof the entities can need to be translated, while theothers can just be transliterated.
An example of anested named entity is ?Mahatma Gandhi Interna-tional Hindi University?.
This would be translatedin Hindi as mahaatmaa gaandhii antarraashtriyahindii vishvavidyaalaya.
Only ?International?
and?University?
are to be translated, while the otherwords are to be transliterated.
The nested named en-tities in this case are: ?Mahatma?
(NETO), ?Gandhi?
(NEP), ?Mahatma Gandhi?
(NEP), and ?MahatmaGandhi International Hindi University?
(NEO).6 Named Entity Annotated CorpusFor Hindi, Oriya and Telugu, all the annotation wasperformed at IIIT, Hyderabad.
For Bengali, the cor-pus was developed at IIIT, Hyderabad and JadavpurUniversity (Ekbal and Bandyopadhyay, 2008b), Cal-cutta.
For Urdu, annotation was performed atCRULP, Lahore (Hussain, 2008) and IIIT, Allahabd.Even though all the annotation was done by nativespeakers of respective languages, named entity an-notation was a new task for everyone involved.
Thiswas because of practical constraints as explained inan earlier section.The corpus was divided into two parts, one fortraining and one for testing.
The testing corpuswas annotated with nested named entities, while thetraining corpus was only annotated with ?maximal?named entities.Since different teams were working on differentlanguages, in some cases even the same language,and also because most of the corpus was created onshort notice, each team made its own decisions re-garding the kind of corpus to be annotated.
As a re-sult, the characteristics of the corpus differ widelyamong the five languages.
The Hindi and Ben-gali (partly) text that was annotated was from themultilingual comparable corpus known as the CIIL(Central Institute of Indian Languages) corpus.
TheOriya corpus was part of the Gyan Nidhi corpus.9NE Hindi Bengali Oriya Telugu UrduTrn Tst Trn Tst Trn Tst Trn Tst Trn TstNEP 4025 199 1299 728 2079 698 1757 330 365 145NED 935 61 185 11 67 216 87 77 98 41NEO 1225 44 264 20 87 200 86 12 155 40NEA 345 7 111 9 8 20 97 112 39 3NEB 5 0 22 0 11 1 1 6 9 18NETP 1 5 68 57 54 201 103 2 36 15NETO 964 88 204 46 37 28 276 118 4 147NEL 4089 211 634 202 525 564 258 751 1118 468NETI 1760 50 285 46 102 122 244 982 279 59NEN 6116 497 407 144 124 232 1444 391 310 47NEM 1287 17 352 146 280 139 315 53 140 40NETE 5658 843 1165 314 5 0 3498 138 30 4NEs 26432 2022 5000 1723 3381 2421 8178 3153 2584 1027Words 503179 32796 112845 38708 93173 27007 64026 8006 35447 12805Sentences 19998 2242 6030 1835 1801 452 5285 337 1508 498Trn: Training Data, Tst: Testing DataTable 2: Statistics about the corpus: counts of various named entity classes and the size of the corpus as thenumber of words and the number of sentences.
Note that the values for the testing part are of nested NEs.Also, the number of sentences, especially in the case or Oriya is not accurate because the sentences were notcorrectly segmented as there was no automatic sentence splitter available for these languages and manualsplitting would have been too costly: without much benefit for the NER task.Both of these (CIIL and Gyan Nidhi) corpora con-sist of text from educational books written on vari-ous topics for common readers.
The Urdu text waspartly news corpus.
The same was the case with Tel-ugu, but the text for both these languages includedtext from other domains too.Admittedly, the texts selected for annotation werenot the ideal ones.
For example, many documentshad very few named entities.
Also, the distributionof domains as well as the classes of NEs was notrepresentative.
The size of the annotated corporafor different languages is also widely varying, withHindi having the largest corpus and Urdu the small-est.
However, this corpus is hopefully just a startingpoint for much more work in the near future.Some statistics about the annotated corpus aregiven in Table-2.7 Shared TaskIn the shared task, the contestants having their ownNER systems were given some annotated test data.The contestants had the freedom to use any tech-nique for NER, e.g.
a purely rule based techniqueor a purely statistical technique.The contestants could build NER systems targetedfor a specific language, but they were required to re-port results for their systems on all the languagesfor which training data had been provided.
Thiscondition was meant to provide a somewhat fairground for comparison of systems, since the amountof training data is different for different languages.The data released for the shared task has beenmade accessible to all for non-profit research word,not just for the shared task participants, with thehope others will contribute in improving this dataand adding to it.The task in this contest was different in one im-portant way.
The NER systems also had to identifynested named entities.
For example, in the sentenceThe Lal Bahadur Shastri National Academy of Ad-ministration is located in Mussoorie, ?Lal BahadurShastri?
is a Person, but ?Lal Bahadur Shastri Na-tional Academy of Administration?
is an Organiza-tion.
In this case, the NER systems had to identifyboth ?Person?
and ?Organization?
in the given sen-tence.An evaluation script was also provided to evaluatethe performance of different systems in a uniformway.108 Evaluation MeasuresAs part of the evaluation process for the shared task,precision, recall and F-measure had to be calcu-lated for three cases: maximal named entities, nestednamed entities and lexical matches.
Thus, therewere nine measures of performance:?
Maximal Precision: Pm = cmrm?
Maximal Recall: Rm = cmtm?
Maximal F-Measure: Fm = 2?Pm?RmPm+Rm?
Nested Precision: Pn = cnrn?
Nested Recall: Rn = cntn?
Nested F-Measure: Fn = 2PnRnPn+Rn?
Lexical Precision: Pl = clrl?
Lexical Recall: Rl = cltl?
Lexical F-Measure: Fl = 2PlRlPl+Rlwhere c is the number of correctly retrieved (iden-tified) named entities, r is the total number of namedentities retrieved by the system being evaluated (cor-rect plus incorrect) and t is the total number ofnamed entities in the reference data.The participants were encouraged to report resultsfor specific classes of NEs.
Evaluation was auto-matic and was against the manually prepared refer-ence data given to the participants.
An evaluationscript for this purpose was also provided.
This scriptassumes that there are single test and reference fileand the number and order of sentences is the same inboth.
The format accepted by the evaluation script(which was also the format used for annotated data)was explained in an online tutorial5.9 Experiments on a BaselineFor our baseline experiments, we used an opensource implementation of maximum entropy basedNatural Languages Processing tools which are partof the OpenNLP6 package.
This package includes aname finder tool.5http://ltrc.iiit.ac.in/ner-ssea-08/NER-SAL-TUT.pdf6http://opennlp.sourceforge.net/This name finder was trained for all the twelveclasses of NEs and for all the five languages.
Thetest data, which was the same as that given to theshared task participants, was run through this namefinder.
Note that this NER tool is tuned for En-glish in terms of the features used, even though itwas trained on different SSEA languages in our case.Since the goal of the shared task was to encourageinvestigation of techniques (especially features) spe-cific to the SSEA languages, this fairly mature NERsystem (for English) could be used as a baselineagainst which to evaluate systems tuned (or speciallydesigned) for the five South Asian languages.The overall results of the baseline experiments areshown in Table-3.
The performance on specific NEclasses is given in Table-4.
It can be seen from thetables that the results are drastically low in compar-ison to the state of the art results reported for En-glish.
These results clearly show that even a ma-chine learning based system cannot be directly usedfor SSEA languages even when it has been trainedwith annotated data for these languages.In the next section we present a brief overview ofthe papers selected for the workshop including theshared task papers.10 An Overview of the PapersIn all, twelve papers were selected for the workshop,out of which four were in the shared task track.
Sahaet al, who were able to achieve the best results inthe shared task, describe a hybrid system that ap-plies maximum entropy models, language specificrules, and gazetteers.
For Hindi, the features theyutilized include orthographic features, informationabout suffixes and prefixes, morphological features,part of speech information, and information aboutthe surrounding words.
They used rules for num-bers, measures and time classes.
For designation,title-person and some terms (NETE), they built listsor gazetteers.
They also used gazetteers for personand location.
They did not use rules or gazetteers forOriya, Urdu and Telugu.
To identify some kinds ofnested entities, they applied a set of rules.Gali et al also combined machine learning withlanguage specific heuristics.
In a separate section,they discussed at some length the issues relevant toNER for SSEA languages.
Some of these have al-11Measure ?
Precision Recall F-MeasureLanguage ?
Pm Pn Pl Rm Rn Rl Fm Fn FlBengali 50.00 44.90 52.20 07.14 06.90 06.97 12.50 11.97 12.30Hindi 75.05 73.61 73.99 18.16 17.66 15.53 29.24 28.48 25.68Oriya 29.63 27.46 48.25 09.11 07.60 12.18 13.94 11.91 19.44Telugu 00.89 02.83 22.85 00.20 00.67 5.41 00.32 01.08 08.75Urdu 47.14 43.50 51.72 18.35 16.94 18.94 26.41 24.39 27.73m: Maximal, n: Nested, l: LexicalTable 3: Results for the experiments on a baseline for the five South Asian languagesBengali Hindi Oriya Telugu UrduNEP 06.62 26.23 28.48 00.00 04.39NED 00.00 12.20 00.00 00.00 00.00NEO 00.00 15.50 03.30 00.00 11.98NEA 00.00 00.00 00.00 00.00 00.00NEB NP NP 00.00 00.00 00.00NETP 00.00 NP 11.62 00.00 00.00NETO 00.00 05.92 04.08 00.00 00.00NEL 03.03 44.79 25.49 00.00 40.21NETI 34.00 47.41 22.38 01.51 38.38NEN 62.63 62.22 10.65 03.51 09.52NEM 13.61 24.39 08.03 00.71 07.15NETE 00.00 00.18 00.00 00.00 00.00NP: Not present in the reference dataTable 4: Baseline results for specific named entity classes (F-Measures for nested lexical match)ready been mentioned, but two others are the ag-glutinative property of these (especially Dravidian)languages and the low accuracy of available part ofspeech taggers, particularly for nouns.
They useda Conditional Random Fields (CRF) based methodfor machine learning and applied heuristics to takecare of the language specific issues.
They also pointout that a very high percentage of NEs in the Hindicorpus were marked as NETE and machine learningfailed to take care of this class of NEs.
This has beenvalidated by our results on the baseline too (Table-4) and is understandable because terms are hard toidentify even for humans.Ekbal et al also used an approach based on CRFs.They also used some language specific features forHindi and Bengali.
Srikanth and Murthy describethe results of their experiments on NER using CRFsfor Telugu.
They concentrated only on person, placeand organization names and used newspaper textas the corpus.
In this focused setting, they wereable to achieve overall F-measures between 80% and97% in various experiments.
Chaudhuri and Bhat-tacharya also experimented on a news corpus forBengali using a three stage NER system.
The threestages were based on an NE dictionary, rules andcontextual co-occurrence statistics.
They only triedto identify the NEs, not classify them.
For this task,they were able to achieve an overall F-measure of89.51%.Praveen and Ravi Kumar present the results ofexperiments (as part of the shared task) using twoapproaches: Hidden Markov Models (HMM) andCRF.
Surprisingly, they obtained better results withHMM for all the five languages.
Goyal described ex-periments using a CRF based model.
He also usedpart of speech information.
He experimented onlyon Hindi and was able to achieve results above 60%.One notable fact about this paper is that it also de-12Language ?
BL IK IH1 IH2 JUBengali 12.30 65.96 40.63 39.77 59.39Hindi 25.68 65.13 50.06 46.84 33.12Oriya 19.44 44.65 39.04 45.84 28.71Telugu 08.75 18.74 40.94 46.58 04.75Urdu 27.73 35.47 43.46 44.73 35.52Average 18.78 45.99 42.83 44.75 32.30BL: Baseline, IK: IIT KharagpurJU: Jadavpur University, CalcuttaIH1: Karthik et al, IIIT HyderabadIH2: Praveen and Ravi Kiran, IIIT HyderabadTable 5: Comparison of NER systems which participated in the NERSSEAL-08 shared task against a base-line that uses maximum entropy based name finder tuned for English but trained on data from five SouthAsian languagesscribes experiments on the CoNLL 2003 shared taskdata for English, which shows that the significantlyhigher results for English are mainly due to the factthat the CoNLL 2003 data is already POS tagged andchunked with high accuracy.
Goyal was also able toshow that capitalization is a major clue for English,either directly or indirectly (e.g., for accurate POStagging and chunking).
He also indicated that thecharacteristics of the Hindi annotated corpus werepartly responsible for the low results on Hindi.Nayan et al mainly describe how an NER systemcan benefit from approximate string matching basedon phonetic edit distance, both for a single language(to account for spelling variations) and for cross-lingual NER.
Shishtla et al (?Experiments in Tel-ugu NER?)
experimented only on Telugu and usedthe CoNLL shared task tagset.
Using a CRF basedapproach, they were able to achieve an F-measureof 44.91%.
Ekbal and Bandyopadhyay describe amethod based on Support Vector Machines (SVMs)for Bengali NER.
On a news corpus and with sixteenNE classes, they were able to achieve an F-measureof 91.8%.
Vijayakrishna and Sobha describe a CRFbased system for Tamil using 106 NE classes.
Theirsystem is a multi-level system which gave an over-all F-measure of 80.44%.
They also mention thattheir system achieved this level of performance on adomain focused corpus.
Shishtla et al (?Charactern-gram Based Approach?)
used a character n-grambased method to identify NEs.
They experimentedon Hindi as well as English and achieved F-measurevalues up to 45.48% for Hindi and 68.46% for En-glish.Apart from the paper presentations, the workshopwill also have two invited talks.
The first one is titled?Named Entity Recognition: Different Approaches?by Sobha L. and the second one is ?MultilingualNamed Entity Recognition?
by Sivaji Bandyopad-hyay.11 Shared Task ResultsFive teams participated in the shared task.
However,only four submitted papers for the workshop.
All theteams tried to combine machine learning with somelanguage specific heuristics, at least for one of thelanguages.
The results obtained by the four teamsare summarized in Table-5, which shows only the F-measure for lexical match.
It can be seen from thetable that all the teams were able to get significantlybetter results than the baseline.
Overall, the perfor-mance of the IIT Kharagpur team was the best, fol-lowed by the two teams from IIIT Hyderabad.Even though all the teams obtained results muchbetter than the baseline, it is still quite evident thatthe state of the art for NER for SSEA languagesleaves much to be desired.
At around 46% max-imum F-measure on lexical matching, the resultsmean that the NER systems built so far for SSEAlanguages are not quite practically useful.
But, afterthis workshop, we at least know where we stand andhow far we still have to go.However, it may be noted that the conditions for13the shared task were very stringent compared to theprevious shared tasks on NER, e.g.
neither the cor-pus was tagged with parts of speech or chunks, norwere good POS taggers or chunkers available forthe languages involved.
This indicates that withprogress in building better resources and basic toolsfor these languages, the accuracy of NER systemsshould also increase.
Already, some very high accu-racies are being reported under less stringent condi-tions, e.g.
for domain focused NER.12 ConclusionsWe started by discussing the problem of NER forSouth and South East Asian languages and the moti-vations for organizing a workshop on this topic.
Wealso described a named entity annotated corpus forfive South Asian languages used for this workshop.We presented some statistics about the corpus andalso the problems we encountered in getting the cor-pus annotated by teams located in distant places.
Wealso presented a new named entity tagset that wasdeveloped for annotation of this corpus.
Then wepresented the results for our experiments on a rea-sonable baseline.
Finally we gave an overview ofthe papers selected for the NERSSEAL-08 work-shop and discussed the systems described in thesepapers and the results obtained, including those forthe shared task which was one of the two tracks inthe workshop.ReferencesSivaji Bandyopadhyay.
2008.
Invited talk: Multilin-gual named entity recognition.
In Proceedings of theIJCNLP-08 Workshop on NER for South and SouthEast Asian Languages, pages 15?17, Hyderabad, In-dia, January.A.
Borthwick, J.
Sterling, E. Agichtein, and R. Grishman.1998.
Exploiting diverse knowledge sources via max-imum entropy in named entity recognition.
Proceed-ings of the Sixth Workshop on Very Large Corpora,pages 152?160.A.
Borthwick.
1999.
A Maximum Entropy Approach toNamed Entity Recognition.
Ph.D. thesis, New YorkUniversity.Bidyut Baran Chaudhuri and Suvankar Bhattacharya.2008.
An experiment on automatic detection of namedentities in bangla.
In Proceedings of the IJCNLP-08Workshop on NER for South and South East AsianLanguages, pages 51?58, Hyderabad, India, January.H.L.
Chieu and H.T.
Ng.
2003.
Named entity recognitionwith a maximum entropy approach.
Proceedings ofthe seventh conference on Natural language learningat HLT-NAACL 2003-Volume 4, pages 160?163.S.
Cucerzan and D. Yarowsky.
1999.
Language indepen-dent named entity recognition combining morpholog-ical and contextual evidence.
Proceedings of the JointSIGDAT Conference on EMNLP and VLC 1999, pages90?99.Asif Ekbal and Sivaji Bandyopadhyay.
2008a.
Ben-gali named entity recognition using support vector ma-chine.
In Proceedings of the IJCNLP-08 Workshopon NER for South and South East Asian Languages,pages 85?92, Hyderabad, India, January.
Associationfor Computational Linguistics.Asif Ekbal and Sivaji Bandyopadhyay.
2008b.
Devel-opment of bengali named entity tagged corpus and itsuse in ner systems.
In Proceedings of the Sixth Work-shop on Asian Language Resources, Hyderabad, India,January.Asif Ekbal, Rejwanul Haque, Amitava Das,Venkateswarlu Poka, and Sivaji Bandyopadhyay.2008.
Language independent named entity recog-nition in indian languages.
In Proceedings of theIJCNLP-08 Workshop on NER for South and SouthEast Asian Languages, pages 33?40, Hyderabad,India, January.M.
B. Emeneau.
1956.
India as a linguistic area.
Lin-guistics, 32:3-16.M.
B. Emeneau.
1980.
Language and linguistic area.
Es-says by Murray B. Emeneau.
Selected and introducedby Anwar S. Dil.
Stanford University Press.Karthik Gali, Harshit Surana, Ashwini Vaidya, PraneethShishtla, and Dipti Misra Sharma.
2008.
Aggregatingmachine learning and rule based heuristics for namedentity recognition.
In Proceedings of the IJCNLP-08Workshop on NER for South and South East AsianLanguages, pages 25?32, Hyderabad, India, January.Amit Goyal.
2008.
Named entity recognition for southasian languages.
In Proceedings of the IJCNLP-08Workshop on NER for South and South East AsianLanguages, pages 63?70, Hyderabad, India, January.Sarmad Hussain.
2008.
Resources for urdu languageprocessing.
In Proceedings of the Sixth Workshopon Asian Language Resources, Hyderabad, India, Jan-uary.Hideki Isozaki.
2001.
Japanese named entity recogni-tion based on a simple rule generator and decision tree14learning.
In Meeting of the Association for Computa-tional Linguistics, pages 306?313.J.H.
Kim and PC Woodland.
2000.
A rule-based namedentity recognition system for speech input.
Proc.
ofICSLP, pages 521?524.D.
Klein, J. Smarr, H. Nguyen, and C.D.
Manning.
2003.Named entity recognition with character-level models.Proceedings of CoNLL, 3.Sobha L. 2008.
Invited talk: Named entity recognition:Different approaches.
In Proceedings of the IJCNLP-08 Workshop on NER for South and South East AsianLanguages, pages 13?14, Hyderabad, India, January.A.
McCallum and W. Li.
2003.
Early results for namedentity recognition with conditional random fields, fea-ture induction and web-enhanced lexicons.
SeventhConference on Natural Language Learning (CoNLL).A.
Mikheev, M. Moens, and C. Grover.
1999.
NamedEntity recognition without gazetteers.
Proceedings ofthe ninth conference on European chapter of the Asso-ciation for Computational Linguistics, pages 1?8.Animesh Nayan, B. Ravi Kiran Rao, Pawandeep Singh,Sudip Sanyal, and Ratna Sanyal.
2008.
Named entityrecognition for indian languages.
In Proceedings ofthe IJCNLP-08 Workshop on NER for South and SouthEast Asian Languages, pages 71?78, Hyderabad, In-dia, January.
Association for Computational Linguis-tics.Praveen P and Ravi Kiran V. 2008.
Hybrid named entityrecognition system for south and south east asian lan-guages.
In Proceedings of the IJCNLP-08 Workshopon NER for South and South East Asian Languages,pages 59?62, Hyderabad, India, January.Vijayakrishna R and Sobha L. 2008.
Domain focusednamed entity recognizer for tamil using conditionalrandom fields.
In Proceedings of the IJCNLP-08Workshop on NER for South and South East AsianLanguages, pages 93?100, Hyderabad, India, January.Association for Computational Linguistics.Sujan Kumar Saha, Sanjay Chatterji, Sandipan Dandapat,Sudeshna Sarkar, and Pabitra Mitra.
2008.
A hybridnamed entity recognition system for south and southeast asian languages.
In Proceedings of the IJCNLP-08 Workshop on NER for South and South East AsianLanguages, pages 17?24, Hyderabad, India, January.E.F.T.K.
Sang and F. De Meulder.
2003.
Introduction tothe CoNLL-2003 Shared Task: Language-IndependentNamed Entity Recognition.
Development, 922:1341.Erik F. Tjong Kim Sang.
2002.
Introduction to the conll-2002 shared task: Language-independentnamed entityrecognition.
In Proceedings of CoNLL-2002, pages155?158.
Taipei, Taiwan.Manabu Sassano and Takehito Utsuro.
2000.
Namedentity chunking techniques in supervised learning forjapanese named entity recognition.
In Proceedingsof the 18th conference on Computational linguistics,pages 705?711, Morristown, NJ, USA.
Association forComputational Linguistics.B.
Settles.
2004.
Biomedical Named Entity RecognitionUsing Conditional Random Fields and Rich FeatureSets.
log, 1:1.Praneeth M Shishtla, Karthik Gali, Prasad Pingali, andVasudeva Varma.
2008a.
Experiments in telugu ner:A conditional random field approach.
In Proceed-ings of the IJCNLP-08 Workshop on NER for Southand South East Asian Languages, pages 79?84, Hyder-abad, India, January.
Association for ComputationalLinguistics.Praneeth M Shishtla, Prasad Pingali, and VasudevaVarma.
2008b.
A character n-gram based approachfor improved recall in indian language ner.
In Pro-ceedings of the IJCNLP-08 Workshop on NER forSouth and South East Asian Languages, pages 101?108, Hyderabad, India, January.
Association for Com-putational Linguistics.P Srikanth and Kavi Narayana Murthy.
2008.
Namedentity recognition for telugu.
In Proceedings of theIJCNLP-08 Workshop on NER for South and SouthEast Asian Languages, pages 41?50, Hyderabad, In-dia, January.K.
Takeuchi and N. Collier.
2002.
Use of support vec-tor machines in extended named entity recognition.
InProceedings of the sixth Conference on Natural Lan-guage Learning (CoNLL-2002).G.D.
Zhou and J. Su.
2001.
Named entity recognitionusing an HMM-based chunk tagger.
Proceedings ofthe 40th Annual Meeting on Association for Computa-tional Linguistics, pages 473?480.1516
