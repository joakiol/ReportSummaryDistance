THE AUDITORY PROCESSING AND RECOGNIT ION OF SPEECHWilliam Byrne, John Robinson, Shihab ShammaDepartment of Electrical EngineeringUniversity of Maryland College Park, MD 20742ABSTRACTWe are carrying out investigations into the application of biophysical and computational models to speechprocessing.
Described here are studies of the robustness of a speech representation using a biophysical modelof the cochlea; experimental results on the representation f speech and complex sounds in the mammalianauditory cortex; and descriptions of computational sequential processing networks capable of recognizingsequences of phonemes.INTRODUCTIONSystems for the automatic recognition of speech ave in recent years derived many ideas and strategies fromobservations ofthe structure and processing modes of the nervous ystem, and specifically of the mammalianauditory system.
Examples range from the adoption of cochlear-like processing as front-end analysis tages,to the use of artificial neural networks as adaptive pattern recognizers.
For the last five years, we havebeen studying the functions and algorithms that facilitate the remarkable abilities of the auditory systemto analyze, recognize, and localize complex sounds uch as speech, music, and other environmental sounds.We have developed and used biophysical and computational models of the peripheral cochlear stages, of theintermediate c ntral neural networks that extract various feature representations of the acoustic stimulus(e.g., as in speech phonemes), and of networks for the recognition of temporally ordered sequences ( uch aswords and sentences).
These models have been described in detail in \[1,2,3,4,5\].
Here, we shall outline a fewof our recent investigations and the results that we have obtained.NOISE  ROBUSTNESSCochlear models in various forms are now commonly used in speech recognition systems.
In many cases,they are severely simplified to reduce computational complexity, preserving only salient features of theoriginal models, e.g., the psuedo-logarithmic frequency axis, critical-band filters, and the fast and/or slowadaptation (as AGCs).
These and other processing steps have been justified in many elaborate and detailedexperiments.
One of the most desired features of cochlear processing has been robustness to noise, specifically,their supposed ability to provide a stable representation f the speech signal over a wide range of signal-to-noise ratios.
Results from a few studies have so far been equivocal for many reasons, primary amongthem is the complexity of the systems tested which precluded clear separation of the causes of improvementsand degradations.
We have compared the noise immunity of cochlear epresentations to that of linearpredictor coefficients (LPC), LPC cepstral coefficients, and discrete time Fourier Transform (DFT) spectra.Specifically, three investigations are performed: first, the distortion of each representation due to additivewhite noise is measured; in the second experiment, the robustness i measured through the deterioration ithe vector quantizer performance ofeach representation; and finally, in the third experiment we measure theability of each representation to discriminate speech sounds in noise.Ninety sentences spoken by ten male speakers are taken from the phonetically abeled Icecream databaseand transformed into each of the four representations after upsampling from 16KHz to 20Kiiz (the cochlearmodel requires a 20KItz sampling rate).
The cochlear model followed by two stages of lateral inhibition325\[1\] produces vectors of 128 tonotopically ordered elements from the 100ttz to 10KHz region of the basilarmembrane.
For all representations, a 20ms frame and 8ms step size are used, and, except for the cochlearmodel, a preemphasis of 1.0 and a Hamming window are applied.The Log Area Ratios are obtained from the LPC coefficients of an order 28 predictor found via the auto-correlation method.
The Log Area Ratios are used because of their appropriateness forvector quantizationand mean square distortion measurements.
The LPC cepstral coefficients are also computed via autocorre-lation and the quefrency ranged from 0.0625 ms to 3.0625 ms.
The spectrum is computed by a 256 pointFFT with zero padding.In the tests performed, noisy speech is obtained by adding white gaussian noise of the appropriateamplitude to the clean speech.
The slight oversampling of the speech is taken into account in determiningthe noise amplitude.
The signal to noise levels investigated are 24dB - 0dB in steps of 3dB.FEATURE D ISTORTIONThe actual effect of additive noise on the various representations is measured first as1/N N E j=I  \[\[K(F(sj)) - K (F (s j  + n/))l\[2Dpe,.
?~,~t Di,to,.tion = 1/NE~r=i IIK(F(*D)II2where F(s j )  is the representation f frame j of the clean speech, N is the number of frames, and sj + njrefers to a frame of speech with additive noise.The Karhunen-Loeve transform, K, is computed for each representation from the autocovariance ofthe clean speech features.
It is chosen as a means of reducing the dimension of the cochlear model in anoptimum fashion.
Since it also can be used to restrict measured ata to a known signal space, it is applied toall representations so as not to give the cochlear model an unfair advantage.
The eigenvectors correspondingto the 48 largest eigenvalues of the autocovariance matrix are chosen to form the transform kernel for boththe spectral and cochlear epresentations.
For the LPC and LPC cepstrum, all eigenvectors are retained.Note that if all eigenvectors are retained I \ [g(F(s)) - g (F (s  + n))\[\[2 = HE(s) - F(s  + n)\[\[2 so the transformdoes not affect the distortion computation for the LPC and LPC cepstrum, and in practice, the spectraldistortion is not reduced by the change in dimension.The cochlear model suffers less distortion than the other representations at noise levels less than 9db, atwhich point it becomes parallel to the parametric models (Figure l(a)).VECTOR QUANTIZER D ISTORTIONAnother comparison among the different representations is through the effects of noise on the performanceof vector quantizers (VQs) trained with clean speech.
The effect of noise on the both VQ class distributionsfor each phoneme and the increase in codebook distortion are used as the measuring criteria.Codebooks of 64 symbols are trained on clean speech and sample distributions of the VQ classes areformed for each phoneme at all noise levels.
The similarity between the class distribution of the quantizedclean speech, f, ,  and the distribution of the quantized noisy speech, fs+n, is measured byDDi,t~ib~tlo.
Di.to~tio.
= 1 -- Ei6_41 fs(i)" f~+n(i)64 ?2 (iS ~...~64For presentation and comparison, the measurement of each representation is normalized by its 0db value.Only the results for the most frequently occurring vowel,/ey/, are given (Figure l(b)), but the results forother phonemes, with the exception of stops, are essentially the same.
In the case of the stops (and duringsilences), all representations seem to perform similarly.Since the distribution of the VQ classes for particular phonemes i important to many statistical methodsof speech recognition, the superior performance of the cochlear epresentation is significant.
The class326(3) Percent  .
xtll Fs- Fs.n IIDlstortl0n Etll ~,11spectralIO0'80'60'40'20,I " : " : " .S /N(dB) .24 21 18 15 12 9 3 0I S.I s .
.
Intra- Class Scatter (d) Contusion = I Score ~-- log i s?l s?
= Inter-  ClassScatter7S COChleI .
.
.
.
.
.
SIN (dB)24 21 18 15 12 9 3 0Norma}lzedit)} Distortion InDistribution of lay/I.8.6l.4 I.2,t - <ts,rs.n >I - <q , rs .oae>Spect ra l~ r ~ ( d B  )24 21 18 15 12 9 3 0Normallze~ .
ztll Ks- VQ?F,.,~ II(c) Distortion Ztll Ks- Vaffs.oae) II.6,.4,I " " " : " .
s /N  cdB)24 21 IO 15 12 9 3 0Figure 1: Distortion Due to Noise: (a) Percent Distortion (b) Change in VQ Class Distribution o f /ey / (c )Normalized VQ Distortion (d) Intra-Class vs Inter-Class Scatterdistributions show that the LPC and cepstrum, as noise increases, model all the speech sounds as noise,which the VQ labels as one of three or four classes.
This happens also to the cochlear representation, but athigher noise levels.An alternative way of measuring VQ performance is through the codebook distortion, defined asNDvQ Distortion = 1/N ~ IIF(sj) - VQ(F(sj + n~))l12j= lThis is also computed for each phoneme, but only the composite results are presented here, normalized bythe 0db distortion (Figure l(c)).A similar measure based on 1/g~;=l  IlYQ(F(sj))- YQ(F(sj + nj))ll2 is also computed.
The resultsclosely resemble those in Figure l(c), but include a common bias due to the codebook distortion.
Themeasures  DDistribution Distortion and DVQ Distortion show that the cochlear model performs well at noiselevels below 9db.D ISCRIMINAT ION AB IL ITYThe ability of the LPC cepstrum and cochlear model to discriminate between different phonemes in thepresence of additive noise is an important performance measure in speech recognition.
The phonetic labels327in the database are used to compute avariant of the Fischer Discrimination tocompare the intra-class scatterto the inter-class catter at each noise level.
This measure favors representations i  which features assignedto any particular phoneme are tightly clustered and distant from features assigned to other phonemes.
Theevaluation is given byDcon\]usion Score = 1/n log det Swdet SBwhere Swand SB are the intra-class and inter-class catter matrices, respectivelySw = ~ ~ (x - mi)(x - rni) ti= l  xExicSB = n , (m,  -- m) (m,  -- m) 'i=1and c is the number of phonemes, Xi is the collection of all representations, x, labeled as the i ~h phoneme,ni is the cardinality of Xi, and m and mi are found by averaging all features and averaging all the featuresin Xi, respectively.Both the cepstrum and the cochlear model have similar discrimination performance at low noise levels(Figure l(d)), but the cochlear model retains its performance better as the additive noise level increases.DISCUSSIONWhy is the cochlear epresentation performance superior to other representations?
There are probably twosources: the first is the compression by the hair cell models; the second is the spectral extraction strategy- the lateral inhibitory network (LIN) - applied to the cochlear model output.
Compression produces awell know effect of enhancing a signal in a noisy background (see \[3\]).
In the cochlear models it is possibleto apply strong compression without loss of spectral detail because the spectral information is encoded inthe phase locked responses.
The LIN utilizes this phase locking to extract a robust spectral estimate thatcan tolerate xtreme compression.
Such compression is not feasible for spectrogram representations since itcompletely destroys the spectral peaks and valleys.AUDITORY NEUROPHYSIOLOGYIn the central auditory system, we are investigating the nature of the representation f complex acousticspectra in the auditory cortex \[4\].
Recordings of unit responses along the isofrequency contours of theferret primary auditory cortex reveal systematic changes in the symmetry of their receptive fields.
At thecenter, units with narrow and symmetric inhibitory sidebands predominate.
These give way gradually toasymmetric nhibition, with high frequencies (relative to the best frequency of the units) becoming moreeffective caudally, and weaker ostrally.
This organization gives rise to a new columner organization i theprimary auditory cortex that seems to encode spectral slopes and the symmetry of spectral peaks, edges, andenvelopes.
These columns are analogous to the well known orientation columns of the visual system.
Theimplication of these findings is that in the perception and recognition of complex sounds pecial attentionmust be given to the representation f spectral gradients.
We have simulated the receptive fields obtainedin neurophysiological experiments and are in the process of examining in detail the representation f naturaland synthetic stationary speech tokens in the responses of the cortex (Figure 2).WORD RECOGNIT IONFinally, we have been developing models of networks that can be used for the recognition of temporally-ordered sequences (e.g., phoneme sequences in a word) \[5\].
These networks are biologically plausible in thatthey do not require delay-lines to memorize the word prior to recognition.
Instead, they function in a manner328hf!
i IfR C R Ct hf Iii :th!
- ~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.hfIfR C R CIVowel /aa/ Vowel /i/ Vowel /uw/Figure 2: Spectral Representation i the Auditory Cortex: (left top) Profiles of Receptive Fields in AIAlong the Iso-Frequency Planes.
(left bottom) Response Patterns Elicited by Different Spectral Peaks.
(right) Examples of the Distribution of Activity Produced by Speech Stimuli in a Model of AI with SpectralOrientation Columns.
The Input Profiles are Shown to the Right of Each Figure.analogous to phase-locked loops, where the network locks onto an incoming sequence and predicts one stateahead.
An error signal between the network state and the input is fed back to control the rate of progressionin the network states (Figure 3).The system is based on a nonlinear ecurrent lateral inhibitory network operating in a hysteresis modewhich functions as a pattern generator.
The network consists of a single layer of reciprocally and stronglyinhibited neurons.
The profile of connectivities is designed such that the patterns of the desired sequence arestable states of the network outputs.
It can be shown that, when equally activated, the network settles inany one of its stable states depending on its initial conditions, i.e.
displays a hysteresis behavior.
A networkgenerates a sequence when it cycles through its stable states.
In order to control the order and rate of thisprocess, integrating excitatory connections are formed that project from the elements of one pattern to theelements of the succeeding pattern.
Only one time-constant of integration is used for all connections in thenetwork.
The varying durations of the sequence patterns are encoded not as different ime constants but asdifferent widths of the hysteresis loops between the different patterns, i.e.
through the magnitudes of theinhibitory connectivities in the network.329ErrorSignalComparatorIState - Phoneme IMapping ItDynamical State Network IControl SignalsPhonem IcClassirlcatlonNetworkIL~_A__A_2LA~_A.___n11:.
F-~-l F1,:, , , M " ',:._A__~ \[~n_~__L_0__?
~l_7t_J!I,.
JL_fl.AI_;LAI__f~_5_At ;L_~t_;LJl ~ ~ L_L_I_3__~ fi ~ RJL_ ~l B'J, 'Figure 3: Temporal Sequence Recognition: (left) Network Block Diagram.
(right) Correct Detection of"four" and Rejection of "nine" and "two".The proposed network can be readily used as a recognizer of sequences applied to its input.
The keyconcept here is the degree of correspondence b tween the applied input and the internally predicted state ofthe network.
This measure is used to modulate the mode of operation in the network between a free-cyclingmode when the correspondence is high, and an input-dominated mode when it is low.
The measure is astate-dependent function derived during training, similar to a likelihood function.
Thus, this measure canalso be used as an indicator of the match between the applied sequence and the sequence generated by thenetwork.330When the confidence is relatively high and the network is free-cycling, it automatically substitutes missingpatterns and is rather insensitive to small irregularities of the input temporal durations.
Therefore, in sucha scheme, no time-warping is needed.REFERENCES\[1\] S. Shamma, The Acoustic Features of Speech Sounds in a Model of Auditory Processing: Vowels andVoiceless Fricatives Journal of Phonetics 16, 77 (1988)\[2\] S. Shamma, Spatial and Temporal Processing in Central Auditory Networks in Methods in NeuronalModeling, Koch and Segev, 247 (1989)\[3\] S. Shamma nd K. Morrish, Synehrony Suppression i Complex Stimulus Responses of a BiophysicalModel of the Cochlea Journal of the Acoustical Society of America 81, 1486 (1987)\[4\] S. Shamma, Spectral Orientation Columns in the Primary Auditory Cortex, University of MarylandInstitute for Advanced Computer Studies Technical Report (1989)\[5\] W. Byrne and S. Shamma, Dynamical Networks for Speech Recognition Proceedings of the AnnualMeeting of International Neural Network Society, 291 (1988)331
