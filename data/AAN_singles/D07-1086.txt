Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
819?826, Prague, June 2007. c?2007 Association for Computational LinguisticsLearning Noun Phrase Query SegmentationShane Bergsma and Qin Iris WangDepartment of Computing ScienceUniversity of AlbertaEdmonton, Alberta, Canada, T6G 2E8{bergsma,wqin}@cs.ualberta.caAbstractQuery segmentation is the process of tak-ing a user?s search-engine query and di-viding the tokens into individual phrasesor semantic units.
Identification of thesequery segments can potentially improveboth document-retrieval precision, by firstreturning pages which contain the exactquery segments, and document-retrieval re-call, by allowing query expansion or substi-tution via the segmented units.
We train andevaluate a machine-learned query segmenta-tion system that achieves 86% segmentation-decision accuracy on a gold standard set ofsegmented noun phrase queries, well aboverecently published approaches.
Key en-ablers of this high performance are featuresderived from previous natural language pro-cessing work in noun compound bracketing.For example, token association features be-yond simple N-gram counts provide power-ful indicators of segmentation.1 IntroductionBillions of times every day, people around the worldcommunicate with Internet search engines via asmall text box on a web page.
The user providesa sequence of words to the search engine, and thesearch engine interprets the query and tries to returnweb pages that not only contain the query tokens,but that are also somehow about the topic or ideathat the query terms describe.Recent years have seen a widespread recognitionthat the user is indeed providing natural languagetext to the search engine; query tokens are not inde-pendent, unordered symbols to be matched on a webdocument but rather ordered words and phrases withsyntactic relationships.
For example, Zhai (1997)pointed out that indexing on single-word symbols isnot able to distinguish a search for ?bank terminol-ogy?
from one for ?terminology bank.?
The readercan submit these queries to a current search engineto confirm that modern indexing does recognize theeffect of token order on query meaning in some way.Accurately interpreting query semantics also de-pends on establishing relationships between thequery tokens.
For example, consider the query ?twoman power saw.?
There are a number of possibleinterpretations of this query, and these can be ex-pressed through a number of different segmentationsor bracketings of the query terms:1.
[two man power saw]2.
[two man] [power saw]3.
[two] [man] [power saw]4.
[two] [man power] [saw], etc.One simple way to make use of these interpretationsin search would be to put quotation marks around thephrasal segments to require the search engine to onlyfind pages with exact phrase matches.
If, as seemslikely, the searcher is seeking pages about the large,mechanically-powered two-man saws used by lum-berjacks and sawyers to cut big trees, then the firstsegmentation is correct.
Indeed, a phrasal searchfor ?two man power saw?
on Google does find thedevice of interest.
So does the second interpreta-tion, but along with other, less-relevant pages dis-cussing competitions involving ?two-man handsaw,819two-woman handsaw, power saw log bucking, etc.
?The top document returned for the third interpreta-tion, meanwhile, describes a man on a rampage at asubway station with two cordless power saws, whilethe fourth interpretation finds pages about topicsranging from hockey?s thrilling two-man power playadvantage to the man power situation during theSecond World War.
Clearly, choosing the right seg-mentation means finding the right documents faster.Query segmentation can also help if insufficientpages are returned for the original query.
A tech-nique such as query substitution or expansion (Joneset al, 2006) can be employed using the segmentedunits.
For example, we could replace the sexist ?twoman?
modifier with the politically-correct ?two per-son?
phrase in order to find additional relevant doc-uments.
Without segmentation, expanding via theindividual words ?two,?
?man,?
?power,?
or ?saw?could produce less sensible results.In this paper, we propose a data-driven, machine-learned approach to query segmentation.
Similar toprevious segmentation approaches described in Sec-tion 2, we make a decision to segment or not to seg-ment between each pair of tokens in the query.
Un-like previous work, we view this as a classificationtask where the decision parameters are learned dis-criminatively from gold standard data.
In Section 3,we describe our approach and the features we use.Section 4 describes our labelled data, as well as thespecific tools used for our experiments.
Section 5provides the results of our evaluation, and shows thestrong gains in performance possible using a wideset of features within a discriminative framework.2 Related WorkQuery segmentation has previously been ap-proached in an unsupervised manner.
Risvik etal.
(2003) combine the frequency count of a seg-ment and the mutual information (MI) between pairsof words in the segment in a heuristic scoring func-tion.
The system chooses the segmentation with thehighest score as the output segmentation.
Jones etal.
(2006) use MI between pairs of tokens as the solefactor in deciding on segmentation breaks.
If the MIis above a threshold (optimized on a small trainingset), the pair of tokens is joined in a segment.
Oth-erwise, a segmentation break is made.Query segmentation is related to the task of nouncompound (NC) bracketing.
NC bracketing deter-mines the syntactic structure of an NC as expressedby a binary tree, or, equivalently, a binary bracket-ing (Nakov and Hearst, 2005a).
Zhai (1997) firstidentified the importance of syntactic query/corpusparsing for information retrieval, but did not con-sider query segmentation itself.
In principle, asN increases, the number of binary trees for an N -token compound is much greater than the 2N?1 pos-sible segmentations.
In practice, empirical NC re-search has focused on three-word compounds.
Thecomputational problem is thus deciding whether thethree-word NC has a left or right-bracketing struc-ture (Lauer, 1995).
For the segmentation task,analysing a three-word NC requires deciding be-tween four different segmentations.
For example,there are two bracketings for ?used car parts,?
theleft-bracketing ?
[[used car] parts]?
and the right-bracketing ?
[used [car parts]],?
while there are foursegmentations, including the case where there isonly one segment, ?
[used car parts]?
and the basecase where each token forms its own segment,?
[used] [car] [parts].?
Query segmentation thus nat-urally handles the case where the query consists ofmultiple, separate noun phrases that should not beanalysed with a single binary tree.Despite the differences between the tasks, it isworth investigating whether the information thathelps disambiguate left and right-bracketings canalso be useful for segmentation.
In particular, weexplored many of the sources of information usedby Nakov and Hearst (2005a), as well as severalnovel features that aid segmentation performanceand should also prove useful for NC analysis re-searchers.
Unlike all previous approaches that weare aware of, we apply our features in a flexiblediscriminative framework rather than a classificationbased on a vote or average of features.NC analysis has benefited from the recent trendof using web-derived features rather than corpus-based counts (Keller and Lapata, 2003).
Lapata andKeller (2004) first used web-based co-occurrencecounts for the bracketing of NCs.
Recent inno-vations have been to use statistics ?beyond the N-gram,?
such as counting the number of web pageswhere a pair of words w, x participate in a genitiverelationship (?w?s x?
), occur collapsed as a single820phrase (?wx?)
(Nakov and Hearst, 2005a) or havea definite article as a left-boundary marker (?thew x?)
(Nicholson and Baldwin, 2006).
We showstrong performance gains when such features areemployed for query segmentation.NC bracketing is part of a larger field of researchon multiword expressions including general NC in-terpretation.
NC interpretation explores not justthe syntactic dependencies among compound con-stituents, but the semantics of the nominal relation-ships (Girju et al, 2005).
Web-based statistics havealso had an impact on these wider analysis tasks, in-cluding work on interpretation of verb nominalisa-tions (Nicholson and Baldwin, 2006) and NC coor-dination (Nakov and Hearst, 2005b).3 Methodology3.1 Segmentation ClassificationConsider a query x = {x1, x2, ..., xN} consistingof N query tokens.
Segmentation is a mapping S :x ?
y ?
YN , where y is a segmentation from theset YN .
Since we can either have or not have a seg-mentation break at each of the N?1 spaces betweenthe N tokens, |YN | = 2N?1.
Supervised machinelearning can be applied to derive the mapping S au-tomatically, given a set of training examples con-sisting of pairs of queries and their segmentationsT = {(xi,yi)}.
Typically this would be done via aset of features ?
(x,y) for the structured examples.A set of weights w can be learned discriminativelysuch that each training example (xi,yi) has a higherscore, Scorew(x,y) = w ?
?
(x,y), than alterna-tive query-segmentation pairs, (xi, zi), zi 6= yi.1 Attest time, the classifier chooses the segmentation forx that has the highest score according to the learnedparameterization: y?
= argmaxy Scorew(x,y).Unlike many problems in NLP such as parsing orpart-of-speech tagging, the small cardinality of YNmakes enumerating all the alternative query segmen-tations computationally feasible.In our preliminary experiments, we used a Sup-port Vector Machine (SVM) ranker (Joachims,2002) to learn the structured classifier.2 We also in-1See e.g.
Collins (2002) for a popular training algorithm.2A ranking approach was also used previously by Daume?
IIIand Marcu (2004) for the CoNLL-99 nested noun phrase iden-tification task.vestigated a Hidden Markov Model SVM (Altun etal., 2003) to label the segmentation breaks using in-formation from past segmentation decisions.
Ulti-mately, the mappings produced by these approacheswere not as accurate as a simple formulation thatcreates a full query segmentation y as the combi-nation of independent classification decisions madebetween each pair of tokens in the query.3In the classification framework, the input is aquery, x, a position in the query, i, where 0<i<N ,and the output is a segmentation decision yes/no.The training set of segmented queries is convertedinto examples of decisions between tokens andlearning is performed on this set.
At test time, N ?1segmentation decisions are made for the N -lengthquery and an output segmentation y is produced.Here, features depend only on the input query x andthe position in the query i.
For a decision at positioni, we use features from tokens up to three positionsto the left and to the right of the decision location.That is, for a decision between xL0 and xR0, we ex-tract features from a window of six tokens in thequery: {..., xL2, xL1, xL0, xR0, xR1, xR2, ...}.
Wenow detail the features derived from this window.3.2 FeaturesThere are a number of possible indicators of whethera segmentation break occurs between a pair of to-kens.
Some of these features fire separately for eachtoken x in our feature window, while others are de-fined over pairs or sets of tokens in the window.
Wefirst describe the features that are defined for the to-kens around the decision boundary, xL0 and xR0,before describing how these same features are ex-tended to longer phrases and other token pairs.3.2.1 Decision-boundary featuresTable 1 lists the binary features that fire if partic-ular aspects of a token or pair of tokens are present.For example, one of the POS-tags features will fireif the pair?s part-of-speech tags are DT JJ , anotherfeature will fire if the position of the pair in the to-3The structured learners did show large gains over the clas-sification framework on the dev-set when using only the basicfeatures for the decision-boundary tokens (see Section 3.2.1),but not when the full feature set was deployed.
Also, featuresonly available to structured learners, e.g.
number of segmentsin query, etc., did improve the performance of the structuredapproaches, but not above that of the simpler classifier.821Table 1: Indicator features.Name Descriptionis-the token x = ?the?is-free token x = ?free?POS-tags Part-of-speech tags of pair xL0 xR0fwd-pos position from beginning, irev-pos position from end N ?
iken is 2, etc.
The two lexical features (for when thetoken is ?the?
and when the token is ?free?)
fire sep-arately for the left and right tokens around the deci-sion boundary.
They are designed to add discrimi-nation for these common query words, motivated byexamples in our training set.
For example, in thetraining set, ?free?
often occurs in its own segmentwhen it?s on the left-hand-side of a decision bound-ary (e.g.
?free?
?online?
...), but may join into alarger segment when it?s on the right-hand-side of acollocation (e.g.
?sulfite free?
or ?sugar free?).
Theclassifier can use the feature weights to encourage ordiscourage segmentation in these specific situations.For statistical features, previous work (Section 2)suggests that the mutual information between the de-cision tokens xL0 and xR0 may be appropriate.
Thelog of the pointwise mutual information (Church andHanks, 1989) between the decision-boundary tokensxL0, xR0 is:MI(xL0, xR0) = log Pr(xL0xR0)Pr(xL0)Pr(xR0)This is equivalent to the sum: log C(xL0xR0) +log K ?
log C(xL0) ?
log C(xR0).
For web-basedfeatures, the counts C(.)
can be taken as a search en-gine?s count of the number of pages containing theterm.
The normalizer K is thus the total number ofpages on the Internet.Represented as a summation, we can see that pro-viding MI as the feature effectively ties the weightson the logarithmic counts C(xL0xR0), C(xL0), andC(xR0).
Another approach would be to providethese logarithmic counts as separate features to ourlearning algorithm, which can then set the weightsoptimally for segmentation.
We call this set ofcounts the ?Basic?
features.
In Section 5, we con-firm results on our development set that showed us-ing the basic features untied increased segmentationTable 2: Statistical features.Name Descriptionweb-count count of ?x?
on the webpair-count web count ?w x?definite web count ?the w x?collapsed web count ?wx?
(one word)and-count web count ?w and x?genitive web count ?w?s x?Qcount-1 Counts of ?x?
in query databaseQcounts-2 Counts of ?w x?
in databaseperformance by up to 4% over using MI ?
an impor-tant observation for all researchers using associationmodels as features in their discriminative classifiers.Furthermore, with this technique, we do not needto normalize the counts for the other pairwise statis-tical features given in Table 2.
We can simply relyon our learning algorithm to increase or decrease theweights on the logarithm of the counts as needed.To illustrate how the statistical features work,consider a query from our development set: ?starwars weapons guns.?
The phrase ?star wars?
caneasily be interpreted as a phrase; there is a highco-occurrence count (pair-count), and many pageswhere they occur as a single phrase (collapsed),e.g.
?starwars.com.?
?Weapons?
and ?guns,?
on theother hand, should not be joined together.
Althoughthey may have a high co-occurrence count, the coor-dination feature (and-count) is high (?weapons andguns?)
showing these to be related concepts but notphrasal constituents.
Including this novel feature re-sulted in noticeable gains on the development set.Since this is a query-based segmentation, featuresthat consider whether sets of tokens occurred else-where in the query database may provide domain-specific discrimination.
For each of the Qcount fea-tures, we look for two quantities: the number oftimes the phrase occurs as a query on its own and thenumber of times the phrase occurs within anotherquery.4 Including both of these counts also resultedin performance gains on the development set.We also extensively investigated other corpus-based features, such as the number of times thephrase occurred hyphenated or capitalized, and the4We exclude counts from the training, development, andtesting queries discussed in Section 4.1.822corpus-based distributional similarity (Lin, 1998)between a pair of tokens.
These features arenot available from search-engine statistics becausesearch engines disregard punctuation and capitaliza-tion, and collecting page-count-based distributionalsimilarity statistics is computationally infeasible.Unfortunately, none of the corpus-based featuresimproved performance on the development set andare thus excluded from further consideration.
Thisis perhaps not surprising.
For such a task that in-volves real user queries, with arbitrary spellings andsometimes exotic vocabulary, gathering counts fromweb search engines is the only way to procure reli-able and broad-coverage statistics.3.2.2 Context FeaturesAlthough the tokens at the decision boundaryare of paramount importance, information from theneighbouring tokens is also critical for segmentationdecision discrimination.
We thus include featuresthat take into consideration the preceding and fol-lowing tokens, xL1 and xR1, as context information.We gather all the token indicator features for each ofthese tokens, as well as all pairwise features betweenxL1 and xL0, and then xR0 and xR1.
If context to-kens are not available at this position in the query,a feature fires to indicate this.
Also, if the contextfeatures are available, we include trigram web andquery-database counts of ?xL1 xL0 xR0?
and ?xL0xR0 xR1?, and a fourgram spanning both contexts.Furthermore, if tokens xL2 and xR2 are available, wecollect relevant token-level, pairwise, trigram, andfourgram counts including these tokens as well.In Section 5, we show that context features arevery important.
They allow our system to implic-itly leverage surrounding segmentation decisions,which cannot be accessed directly in an independentsegmentation-decision classifier.
For example, con-sider the query ?bank loan amoritization schedule.
?Although ?loan amoritization?
has a strong connec-tion, we may nevertheless insert a break betweenthem because ?bank loan?
and ?amoritization sched-ule?
each have even stronger association.3.2.3 Dependency FeaturesMotivated by work in noun phrase parsing, itmight be beneficial to check if, for example, tokenxL0 is more likely to modify a later token, such asxR1.
For example, in ?female bus driver?, we mightnot wish to segment ?female bus?
because ?female?has a much stronger association with ?driver?
thanwith ?bus?.
Thus, as features, we include the pair-wise counts between xL0 and xR1, and then xL1 andxR0.
Features from longer range dependencies didnot improve performance on the development set.4 Experimental Setup4.1 DataOur dataset was taken from the AOL search querydatabase (Pass et al, 2006), a collection of 35million queries submitted to the AOL search en-gine.
Most punctuation has been removed from thequeries.5 Along with the query, each entry in thedatabase contains an anonymous user ID and the do-main of the URL the user clicked on, if they selectedone of the returned pages.
For our data, we used onlythose queries with a click-URL.
This subset has ahigher proportion of correctly-spelled queries, andfacilitates annotation (described below).We then tagged the search queries using a max-imum entropy part-of-speech tagger (Ratnaparkhi,1996).
As our approach was designed particularlyfor noun phrase queries, we selected for our final ex-periments those AOL queries containing only deter-miners, adjectives, and nouns.
We also only consid-ered phrases of length four or greater, since queriesof these lengths are most likely to benefit from a seg-mentation, but our approach works for queries of anylength.
Future experiments will investigate applyingthe current approach to phrasal verbs, prepositionalidioms and segments with other parts of speech.We randomly selected 500 queries for training,500 for development, and 500 for final testing.These were all manually segmented by our annota-tors.
Manual segmentation was done with improv-ing search precision in mind.
Annotators were askedto analyze each query and form an idea of what theuser was searching for, taking into consideration theclick-URL or performing their own online searches,if needed.
The annotators were then asked to seg-ment the query to improve search retrieval, by forc-ing a search engine to find pages with the segments5Including, unfortunately, all quotation marks, precludingour use of users?
own segmentations as additional labelled ex-amples or feature data for our system823occurring as unbroken units.One annotator segmented all three data sets, andthese were used for all the experiments.
Two ad-ditional annotators also segmented the final test setto allow inter-annotator agreement calculation.
Thepairwise agreement on segmentation decisions (be-tween each pair of tokens) was between 84.0% and84.6%.
The agreement on entire queries was be-tween 57.6% and 60.8%.
All three agreed com-pletely on 219 of the 500 queries, and we use this?intersected?
set for a separate evaluation in our ex-periments.6 If we take the proportion of segmenta-tion decisions the annotators would be expected toagree on by chance to be 50%, the Kappa statis-tic (Jurafsky and Martin, 2000, page 315) is around.69, below the .8 considered to be good reliability.This observed agreement was lower than we an-ticipated, and reflects both differences in query in-terpretation and in the perceived value of differ-ent segmentations for retrieval performance.
An-notators agreed that terms like ?real estate,?
?workforce,?
?west palm beach,?
and ?private investiga-tor?
should be separate segments.
These are colloca-tions in the linguistics sense (Manning and Schu?tze,1999, pages 183-187); we cannot substitute relatedwords for terms in these expressions nor apply syn-tactic transformations or paraphrases (e.g.
we don?tsay ?investigator of privates?).
However, for a querysuch as ?bank manager,?
should we exclude webpages that discuss ?manager of the bank?
or ?branchmanager for XYZ bank??
If a user is searching for aparticular webpage, excluding such results could beharmful.
However, for query substitution or expan-sion, identifying that ?bank manager?
is a single unitmay be useful.
We can resolve the conflicting objec-tives of our two motivating applications by movingto a multi-layer query bracketing scheme, first seg-menting unbreakable collocations and then buildingthem into semantic units with a query segmentationgrammar.
This will be the subject of future research.4.2 ExperimentsAll of our statistical feature information was col-lected using the Google SOAP Search API.7 Fortraining and classifying our data, we use the popular6All queries and statistical feature information is availableat http://www.cs.ualberta.ca/?bergsma/QuerySegmentation/7http://code.google.com/apis/soapsearch/Support Vector Machine (SVM) learning packageSVMlight (Joachims, 1999).
SVMs are maximum-margin classifiers that achieve good performance ona range of tasks.
In each case, we learn a linear ker-nel on the training set segmentation decisions andtune the parameter that trades-off training error andmargin on the development set.We use the following two evaluation criteria:1.
Seg-Acc: Segmentation decision accuracy: theproportion of times our classifier?s decision toinsert a segment break or not between a pair oftokens agrees with the gold standard decision.2.
Qry-Acc: Query segmentation accuracy: theproportion of queries for which the completesegmentation derived from our classificationsagrees with the gold standard segmentation.5 ResultsTable 3 provides our results for various configu-rations of features and token-combinations as de-scribed in Section 3.8 For comparison, a baselinethat always chooses a segmentation break achieves44.8% Seg-Acc and 4.2% Qry-Acc, while a systemthat inserts no breaks achieves 55.2% Seg-Acc and4.0% Qry-Acc.
Our comparison system is the MIapproach used by Jones et al (2006), which achieves68% Seg-Acc and 26.6% Qry-Acc (Table 3).
We letthe SVM set the threshold for MI on the training set.Note that the Basic, Decision-Boundary system(Section 3.2.1), which uses exactly the same co-occurrence information as the MI system (in theform of the Basic features) but allows the SVM todiscriminatively weight the logarithmic counts, im-mediately increases Seg-Acc performance by 3.7%.Even more strikingly, adding the Basic count infor-mation for the Context tokens (Section 3.2.2) boostsperformance by another 8.5%, increasing Qry-Accby over 22%.
Smaller, further gains arise by addingDependency token information (Section 3.2.3).Also, notice that moving from Basic features forthe Decision-Boundary tokens to all of our indica-tor (Table 1) and statistical (Table 2) features (re-ferred to as All features) increases performance from71.7% to 84.3%.
These gains convincingly justify8Statistically significant intra-row differences in Qry-Accare marked with an asterix (McNemar?s test, p<0.05)824Table 3: Segmentation Performance (%)Feature Type Feature Span Test Set Intersection SetSeg-Acc Qry-Acc Seg-Acc Qry-AccMI Decision-Boundary 68.0 26.6 73.8 34.7Basic Decision-Boundary 71.7 29.2 77.6 39.7Basic Decision-Boundary, Context 80.2 52.0* 85.6 62.1*Basic Decision-Boundary, Context, Dependency 81.1 53.2 86.2 64.8All Decision-Boundary 84.3 57.8* 86.6 63.5All Decision-Boundary, Context 86.3 63.8* 89.2 71.7*All Decision-Boundary, Context, Dependency 85.8 61.0 88.7 69.4our use of an expanded feature set for this task.Including Context with the expanded features addsanother 2%, while adding Dependency informationactually seems to hinder performance slightly, al-though gains were seen when adding Dependencyinformation on the development set.Note, however, that these results must also beconsidered in light of the low inter-annotator agree-ment (Section 4.1).
Indeed, results are lower if weevaluate using the test-set labels from another an-notator (necessarily training on the original anno-tator?s labels).
On the intersected set of the threeannotators, however, results are better still: 88.7%Seg-Acc and 69.4% Qry-Acc on the intersectedqueries for the full-featured system (Table 3).
Sincehigh performance is dependent on consistent train-ing and test labellings, it seems likely that develop-ing more-explicit annotation instructions may allowfurther improvements in performance as within-setand between-set annotation agreement increases.It would also be theoretically interesting, and ofsignificant practical importance, to develop a learn-ing approach that embraces the agreement of theannotations as part of the learning algorithm.
Ourinitial ranking formulation (Section 3.1), for exam-ple, could learn a model that prefers segmentationswith higher agreement, but still prefers any anno-tated segmentation to alternative, unobserved struc-tures.
As there is growing interest in making max-imal use of annotation resources within discrimina-tive learning techniques (Zaidan et al, 2007), devel-oping a general empirical approach to learning fromambiguously-labelled examples would be both animportant contribution to this trend and a potentiallyhelpful technique in a number of NLP domains.6 ConclusionWe have developed a novel approach to search querysegmentation and evaluated this approach on actualuser queries, reducing error by 56% over a recentcomparison approach.
Gains in performance weremade possible by both leveraging recent progress infeature engineering for noun compound bracketing,as well as using a flexible, discriminative incorpora-tion of association information, beyond the decision-boundary tokens.
We have created and made avail-able a set of manually-segmented user queries, andthus provided a new testing platform for other re-searchers in this area.
Our initial formulation ofquery segmentation as a structured learning prob-lem, and our leveraging of association statistics be-yond the decision boundary, also provides power-ful tools for noun compound bracketing researchersto both move beyond three-word compounds and toadopt discriminative feature weighting techniques.The positive results achieved on this important ap-plication should encourage further inter-disciplinarycollaboration between noun compound interpreta-tion and information retrieval researchers.
For ex-ample, analysing the semantics of multiword expres-sions may allow for more-focused query expansion;knowing to expand ?bank manager?
to include pagesdescribing a ?manager of the bank,?
but not doingthe same for non-compositional phrases like ?realestate?
or ?private investigator,?
requires exactly thekind of techniques being developed in the noun com-pound interpretation community.
Thus for query ex-pansion, as for query segmentation, work in naturallanguage processing has the potential to make a realand immediate impact on search-engine technology.825The next step in this research is to directly inves-tigate how query segmentation affects search perfor-mance.
For such an evaluation, we would need toknow, for each possible segmentation (including nosegmentation), the document retrieval performance.This could be the proportion of returned documentsthat are deemed to be relevant to the original query.Exactly such an evaluation was recently used by Ku-maran and Allan (2007) for the related task of querycontraction.
Of course, a dataset with queries andretrieval scores may serve for more than evaluation;it may provide the examples used by the learningmodule.
That is, the parameters of the contractionor segmentation scoring function could be discrim-inatively set to optimize the retrieval of the trainingset queries.
A unified framework for query contrac-tion, segmentation, and expansion, all based on dis-criminatively optimizing retrieval performance, isa very appealing future research direction.
In thisframework, the size of the training sets would notbe limited by human annotation resources, but bythe number of queries for which retrieved-documentrelevance judgments are available.
Generating moretraining examples would allow the use of more pow-erful, finer-grained lexical features for classification.AcknowledgmentsWe gratefully acknowledge support from the Natu-ral Sciences and Engineering Research Council ofCanada, the Alberta Ingenuity Fund, the Alberta In-genuity Center for Machine Learning, and the Al-berta Informatics Circle of Research Excellence.ReferencesYasemin Altun, Ioannis Tsochantaridis, and Thomas Hofmann.2003.
Hidden markov support vector machines.
In ICML.Kenneth Ward Church and Patrick Hanks.
1989.
Word associ-ation norms, mutual information, and lexicography.
In ACL,pages 76?83.Michael Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments with per-ceptron algorithms.
In EMNLP, pages 1?8.Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe.2005.
On the semantics of noun compounds.
ComputerSpeech and Language, 19(4):479?496.Hal Daume?
III and Daniel Marcu.
2004.
NP bracketing bymaximum entropy tagging and SVM reranking.
In EMNLP,pages 254?261.Thorsten Joachims.
1999.
Making large-scale Support VectorMachine learning practical.
In B. Scho?lkopf and C. Burges,editors, Advances in Kernel Methods: Support Vector Ma-chines, pages 169?184.
MIT-Press.Thorsten Joachims.
2002.
Optimizing search engines usingclickthrough data.
In ACM Conference on Knowledge Dis-covery and Data Mining, pages 133?142.Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner.2006.
Generating query substitutions.
In WWW, pages 387?396.Daniel Jurafsky and James H. Martin.
2000.
Speech and lan-guage processing.
Prentice Hall.Frank Keller and Mirella Lapata.
2003.
Using the web to obtainfrequencies for unseen bigrams.
Computational Linguistics,29(3):459?484.Giridhar Kumaran and James Allan.
2007.
A case for shorterqueries, and helping users create them.
In NAACL-HLT,pages 220?227.Mirella Lapata and Frank Keller.
2004.
The web as a base-line: Evaluating the performance of unsupervised web-basedmodels for a range of NLP tasks.
In HLT-NAACL, pages121?128.Mark Lauer.
1995.
Corpus statistics meet the noun compound:Some empirical results.
In ACL, pages 47?54.Dekang Lin.
1998.
Automatic retrieval and clustering of simi-lar words.
In COLING/ACL, pages 768?773.Christopher D. Manning and Hinrich Schu?tze.
1999.
Foun-dations of Statistical Natural Language Processing.
MITPress.Preslav Nakov and Marti Hearst.
2005a.
Search engine statis-tics beyond the n-gram: Application to noun compoundbracketing.
In CoNLL, pages 17?24.Preslav Nakov and Marti Hearst.
2005b.
Using the web asan implicit training set: application to structural ambiguityresolution.
In HLT/EMNLP, pages 835?842.Jeremy Nicholson and Timothy Baldwin.
2006.
Interpretationof compound nominalisations using corpus and web statis-tics.
In ACL Workshop on Multiword Expressions, pages54?61.Greg Pass, Abdur Chowdhury, and Cayley Torgeson.
2006.
Apicture of search.
In The First International Conference onScalable Information Systems.Adwait Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In EMNLP, pages 133?142.Knut Magne Risvik, Tomasz Mikolajewski, and Peter Boros.2003.
Query segmentation for web search.
In WWW (PosterSession).Omar Zaidan, Jason Eisner, and Christine Piatko.
2007.
Using?annotator rationales?
to improve machine learning for textcategorization.
In NAACL-HLT, pages 260?267.Chengxiang Zhai.
1997.
Fast statistical parsing of noun phrasesfor document indexing.
In ANLP, pages 312?319.826
