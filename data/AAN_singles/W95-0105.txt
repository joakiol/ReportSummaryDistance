Disambiguating Noun Groupings with Respect o WordNet SensesPhilip ResnikSun Microsystems LaboratoriesTwo Elizabeth DriveChelmsford, MA 01824-4195 USAph i l ip ,  resn ik@east ,  sun.
comAbstractWord groupings useful for language processing tasks are increasingly available, as thesauri appearon-line, and as distributional word clustering techniques improve.
However, for many tasks, one isinterested inrelationships among word senses, not words.
This paper presents amethod for automaticsense disambiguafion fnouns appearing within sets of related nouns -- the kind of data one finds inon-line thesauri, or as the output of distributional clustering algorithms.
Disambiguation is performedwith respect o WordNet senses, which are fairly fine-gained; however, the method also permits theassignment of higher-level WordNet categories rather than sense labels.
The method is illustratedprimarily by example, though results of a more rigorous evaluation are also presented.1 IntroductionWord groupings useful for language processing tasks are increasingly available, as thesauri appear on-line,and as distributional techniques become increasingly widespread (e.g.
(Bensch and Savitch, 1992; Brill,1991; Brown et al, 1992; Grefenstette, 1994; McKcown and Hatzivassiloglou, 1993; Pereira et al, 1993;Schtltze, 1993)).
However, for many tasks, one is interested inrelationships among word senses, not words.Consider, for example, the cluster containing attorney, counsel, trial, court, and judge, used by Brown etal.
(1992) to illustrate a "semantically sticky" group of words.
As is often the case where sense ambiguityis involved, we as readers impose the most coherent interpretation the words within the group withoutbeing aware that we are doing so.
Yet a computational system has no choice but to consider other, moreawkward possibilities - -  for example, this cluster might be capturing a distributional relationship betweenadvice (as one sense of counsel) and royalty (as one sense of court).
This would be a mistake for manyapplications, uch as query expansion in information retrieval, where a surfeit of false connections canoutweigh the benefits obtained by using lexical knowledge.One obvious olution to this problem would be to extend istributional grouping methods to word senses.For example, one could construct vector epresentations of senses on the basis of their co-occurrence withwords or with other senses.
Unfortunately, there are few corpora nnotated with word sense information,and computing reliable statistics on word senses rather than words will require more data, rather thanless.
1 Furthermore, one widely available xample of a large, manually sense-tagged corpus - -  the WordNetgroup's annotated subset of the Brown corpus 2- -  vividly illustrates the difficulty in obtaining suitable data.1Actually, this depends on the fine-grainedness of sense distinctions; clearly one could annotate corpora with very high levelsemantic distinctions For example, Basili et al (1994) take such a coarse-grained approach, utilizing on the order of 10 to 15semantic tags for a given domain.
I assume throughout this paper that finer-grained distinctions than that are necessary.2Available by anonymous ftp to clarity.princeton.edu as pub/wnl .
4seracor.
ta r .
Z; Word_Net is described by Miller et al(1990).54It is quite small, by current corpus standards (on the order of hundreds of thousands of words, rather thanmillions or tens of millions); the direct annotation methodology used to create it is labor intensive (Marcuset al (1993) found that direct annotation takes twice as long as automatic tagging plus correction, for part-of-speech annotation); and the output quality reflects the difficulty of the task (inter-annotator disagreementis on the order of 10%, as contrasted with the approximately 3% error rate reported for part-of-speechannotation by Marcus et al).There have been some attempts to capture the behavior of semantic ategories in a distributionalsetting, despite the unavailability of sense-annotated corpora.
For example, Hearst and Schtltze (1993)take steps toward a distributional treatment of WordNet-based classes, using Schtltze's (1993) approachto constructing vector representations from a large co-occurrence matrix.
Yarowsky's (1992) algorithmfor sense disambiguation can be thought of as a way of determining how Roget's thesaurus categoriesbehave with respect to contextual features.
And my own treatment ofselectional constraints (Resnik, 1993)provides away to describe the plausibility of co-occuffence in terms of WordNet's emantic categories, usingco-occurrence r lationships mediated by syntactic structure.
In each case, one begins with known semanticcategories (WordNet synsets, Roget's numbered classes) and non-sense-annotated text, and proceeds to adistributional characterization f semantic category behavior using co-occurrence r lationships.This paper begins from a rather different starting point.
As in the above-cited work, there is nopresupposition that sense-annotated t xt is available.
Here, however, I make the assumption that wordgroupings have been obtained through some black box procedure, .g.
from analysis of unannotated text,and the goal is to annotate the words within the groupings post hoc using a knowledge-based catalogue ofsenses.
If successful, such an approach as obvious benefits: one can use whatever sources of good wordgroupings are available - -  primarily unsupervised word clustering methods, but also on-line thesauri andthe like - -  without folding in the complexity of dealing with word senses at the same time) The resultingsense groupings hould be useful for a variety of purposes, although ultimately this work is motivated bythe goal of sense disarnbiguation for unrestricted text using unsupervised methods.2 Disambiguation of Word Groups2.1 Problem statementLet us state the problem as follows.
We are given a set of words W = {wl,.
?., wn}, with each word wihaving an associated set Si = {si,1,..., si,m} of possible senses.
We assume that there exists some setW' C_ U Si, representing the set of word senses that an ideal human judge would conclude belong to thegroup of senses corresponding to the word grouping W. The goal is then to define a membership function qothat takes si,j, wi, and W as its arguments and computes a value in \[0, 1\], representing the confidence withwhich one can state that sense si,j belongs in sense grouping W'.4 Note that, in principle, nothing precludesthe possibility that multiple senses of a word are included in W'.Example.
Consider the following word group: 5burglars thief rob mugging stray robbing lookout chase crate thieves3An alternative worth mentioning, however, is the distributional approach of Pereira et al (1993): within their representationalscheme, distributionaUy defined word senses emerge automatically inthe form of cluster centroids.4One could also say that ~ defines a fuzzy set.5This example comes from Schfitze's (1993) iUustration of how his algorithm determines nearest neighbors in a sublexicalrepresentation space; these are the ten words representationally most similar to burglar, based on a corpus of newspaper text.55Restricting our attention to noun senses in WordNet, only lookout and crate are polysemous.Treating this word group as W, one would expect ~ to assign a value of 1 to the unique sensesof the monosemous words, and to assign a high value to lookout's ense aslookout, lookout man, sentinel, sentry, watch, scout: a person employed to watch forsomething to happen.Low (or at least lower) values of q; would be expected for the senses of lookout hat correspondto an observation tower, or to the activity of watching.
Crate's two WordNet senses correspondto the physical object and the quantity (i.e., crateful, as in "a crateful of oranges"); my ownintuition is that the first of these would more properly be included in W' than the second, andshould therefore receive ahigher value of ~, though of course neither I nor any other individualreally constitutes an "ideal human judge.
"2.2 Computation of Semantic SimilarityThe core of the disambiguation algorithm is a computation of semantic similarity using the WordNettaxonomy, a topic recently investigated by a number of people (Leacock and Chodorow, 1994; Resnik,1995; Sussna, 1993).
In this paper, I restrict my attention to WordNet's IS-A taxonomy for nouns, and takean approach in which semantic similarity is evaluated on the basis of the information content shared by theitems being compared.The intuition behind the approach is simple: the more similar two words are, the more informative willbe the most specific concept that subsumes them both.
(That is, their least upper bound in the taxonomy;here a concept corresponds to a WordNet synset.)
The traditional method of evaluating similarity in asemantic network by measuring the path length between two nodes (Lee et al, 1993; Rada et al, 1989)also captures this, albeit indirectly, when the semantic network is just an IS-A hierarchy: if the minimal pathof IS-A links between two nodes is long, that means it is necessary to go high in the taxonomy, to moreabstract concepts, in order to find their least upper bound.
However, there are problems with the simplepath-length definition of semantic similarity, and experiments u ing WordNet show that other measures ofsemantic similarity, such as the one employed here, provide a better match to human similarity judgmentsthan simple path length does (Resnik, 1995).Given two words wl and w2, their semantic similarity is calculated assim(wl,WE) = max \ [ -  logPr(c)\], (1)c e subsumers(wl,w2)where subsumers(wl, WE) is the set of WordNet synsets that subsume (i.e., are ancestors of) both w~ andw2, in any sense of either word.
The concept e that maximizes the expression i  (1) will be referred to asthe most informative subsumer of Wl and w2.
Although there are many ways to associate probabilities withtaxonomic classes, it is reasonable torequire that concept probability be non-decreasing as one moves higherin the taxonomy; i.e., that el IS-A c2 implies Pr(c2) _> Pr(el).
This guarantees that "more abstract" doesindeed mean "less informative," defining informativeness in the traditional way in terms of log likelihood.Probability estimates are derived from a corpus by computingfreq(c) = ~ count(n), (2)newords(c)where words(c) is the set of nouns having a sense subsumed by concept c. Probabilities are then computedsimply as relative frequency:~(e) -  freq(c)N ' (3)56where N is the total number of noun instances observed.
Singular and plural forms are counted as the samenoun, and nouns not covered by WordNet are ignored.
Although the WordNet noun taxonomy has multipleroot nodes, a single, "virtual" root node is assumed to exist, with the original root nodes as its children.Note that by equations (1) through (3), if two senses have the virtual root node as their only upper boundthen their similarity value is 0.Example.
The following table shows the semantic similarity computed for several word pairs,in each case shown with the most informative subsumer.
6 Probabifities were estimated usingthe Penn Treebank version of the Brown corpus.
The pairs come from an example given byChurch and Hanks (1989), illustrating the words that human subjects most frequently judged asbeing associated with the word doctor.
(The word sick also appeared on the list, but is excludedhere because it is not a noun.
)Word 1 Word 2doctor nursedoctor lawyerdoctor mandoctor medicinedoctor hospitaldoctor healthdoctor sicknessSimilarity Most Informative Subsumer9.4823 (health professional)7.2240 (professional person}2.9683 (person, individual)1.0105 <entity}1.0105 <entity}0.0 virtual root0.0 virtual rootDoctors are minimally similar to medicine and hospitals, since these things are all instancesof "something having concrete xistence, riving or nonliving" (WordNet class (ent ?ty)), butthey are much more similar to lawyers, since both are kinds of professional people, and evenmore similar to nurses, since both are professional people working specifically within the healthprofessions.
Notice that similarity is a more specialized notion than association or relatedness:doctors and sickness may be highly associated, but one would not judge them to be particularlysimilar.2.3 Disambiguation AlgorithmThe disambiguation algorithm for noun groups is inspired by the observation that when two polysemouswords are similar, their most informative subsumer provides information about which sense of each word isthe relevant one.
In the above table, for example, both doctor and nurse are polysemous: WordNet recordsdoctor not only as a kind of health professional, but also as someone who holds a Ph.D., and nurse can meannot only a health professional but also a nanny.
When the two words are considered together, however,the shared element of meaning for the two relevant senses emerges in the form of the most informativesubsumer.
It may be that other pairings of possible senses also share elements of meaning (for example,doctor~Ph.D, and nurse~nanny are both descendants of (person,  ind iv idua l}) .
However, in cases likethose illustrated above, the more specific or informative the shared ancestor is, the more strongly it suggestswhich senses come to mind when the words are considered together.
The working hypothesis in this paperis that this holds U'ue in general.Turning that observation i to an algorithm requires two things: a way to assign credit to word sensesbased on similarity with co-occurring words, and a tractable way to generalize to the case where more thantwo polysemous words are involved.
The algorithm given in Figure 1 does both quite slraighfforwardly.6For readability, WordNet synsets are described either by symbolic labels (such as (person)) or by long descriptions(constructed from the words in the synset and/or the immediate parent synset and/or the description field from the lexical database).However, in implementations described here, all WordNet synsets are identified by a unique numerical identifier.57Algor i thm.
Given W = {w\[1\] .
.
.
.
.
w\[n\]}, a set of nouns:for i and j = 1 to n, with i < j{vii, j\] = sirn(w\[i\], w\[j\])e\[i, j\] = the most informative subsumer for w\[i\] and w\[j\]for k = 1 to num_senses(w\[i\])if c\[i, j\] is an ancestor of sense\[i, k\]increment support\[i, k\] by v\[i, j\]for k '  = 1 to num_senses(w\[j\])if e\[i, j\] is an ancestor of sense\[j, k'\]increment support\[j, k' \]  by vii, j\]increment normalization\[i\] by v\[i, j\]increment normalization\[j\] by v \[i, j\]fo r i=  1 tonfor k = 1 to num_senses(w\[i\]){i f  (normalization\[il > 0.0)phi\[i, k\] = support\[i, k\] / normalization\[i\]elsephi\[i, k\] = 1 / num_senses(w\[i\])}Figure 1: Disambiguation algorithm for noun groupingsThis algorithm considers the words in W pairwise, avoiding the tractability problems in considering allpossible combinations of senses for the group (0 (m ~) if each word had m senses).
For each pair considered,the most informative subsumer is identified, and this pair is only considered assupporting evidence for thosesenses that are descendants of that concept.
Notice that by equation (1), suppor t  \ [ i ,  k\] is a sum of logprobabilities, and therefore preferring senses with high support is equivalent to optimizing a product ofprobabilities.
Thus considering words pairwise in the algorithm reflects a probabilistic independenceassumption.Example.
The most informative subsumer for doctor and nurse is <health pro fess iona l ) ,and therefore that pairing contributes support to the sense of doctor as an M.D., but not a Ph.D.Similarly, it contributes support to the sense of nurse as a health professional, but not a nanny.The amount of support contributed by a pairwise comparison is proportional to how informative themost informative subsumer is.
Therefore the evidence for the senses of a word will be influenced more bymore similar words and less by less similar words.
By the time this process is completed over all pairs,each sense of each word in the group has had the potential of receiving supporting evidence from a pairingwith every other word in the group.
The value assigned to that sense is then the proportion of support itdid receive, out of the support possible.
(The latter is kept track of by array normal i za t ion  in thepseudocode.)Discussion.
The intuition behind this algorithm is essentially the same intuition exploited by Lesk (1986),Sussna (1993), and others: the most plausible assignment of senses to multiple co-occurring words is the58one that maximizes relatedness of meaning among the senses chosen.
Here I make an explicit comparisonwith Sussna's approach, since it is the most similar of previous work.Sussna gives as an example of the problem he is solving the following paragraph from the corpus of1963 Time magazine articles used in information retrieval research (uppercase inthe Time corpus, lowercasehere for readability; punctuation is as it appears in the original corpus):the allies after nassau in december 1960, the u.s. first proposed to help nato develop its ownnuclear strike force, but europe made no attempt to devise a plan.
last week, as they studiedthe nassau accord between president kennedy and prime minister macmillan, europeans sawemerging the first outlines of the nuclear nato that he u.s. wants and will support, it all sprangfrom the anglo-u.s, crisis over cancellation of the bug-ridden skybolt missile, and the u.s.offer to supply britain and france with the proved polaris (time, dec. 28)From this, Sussna extracts the following noun grouping to disambiguate:allies strike force attempt plan week accord president prime minister outlines upport crisiscancellation bug missile france polaris timeThese are the non-stopword nouns in the paragraph that appear in WordNet (he used version 1.2).The description of Sussna's algorithm for disambiguating oun groupings like this one is similar tothe one proposed here, in a number of ways: relatedness is characterized in terms of a semantic network(specifically WordNet); the focus is on nouns only; and evaluations of semantic similarity (or, in Sussna'scase, semantic distance) are the basis for sense selection.
However, there are some important differences,as well.
First, unlike Sussna's proposal, this algorithm aims to disambiguate groupings of nouns alreadyestablished (e.g.
by clustering, or by manual effort) to be related, as opposed to groupings of nouns thathappen to appear near each other in running text (which may or may not reflect relatedness based onmeaning).
This provides ome justification for restricting attention to similarity (reflected by the scaffoldingof IS-A links in the taxonomy), as opposed to the more general notion of association.
Second, this differenceis reflected algonthmically by the fact that Sussna uses not only IS-A links but also other WordNet linkssuch as PART-OF.
Third, unlike Sussna's algorithm, the semantic similarity/distance computation here isnot based on path length, but on information content, a choice that I have argued for elsewhere (Resnik,1993; Resnik, 1995).
Fourth, the combinatorics are handled ifferently: Sussna explores analyzing allsense combinations (and living with the exponential complexity), as well as the alternative of sequentially"freezing" a single sense for each of Wl, .
.
.
,  W~_l and using those choices, assumed to be correct, as thebasis for disambiguating wi.
The algorithm presented here falls between those two alternatives.A final, important difference between this algorithm and previous algorithms for sense disambiguationis that it offers the possibility of assigning higher-level WordNet categories rather than lowest-level senselabels.
It is a simple modification to the algorithm to assign values of ~ not only to synsets directly containingwords in W, but to any anccestors of those synsets - -  one need only let the list of synsets associated witheach word wi (i.e,, Si in the problem statement ofSection 2.1) also include any synset that is an ancestor ofany synset containing word wi.
Assuming that nura sen s e s (w \[ 5_ \] ) and sen  s e \[ 5_, k \] are reinterpretedaccordingly, the algorithm will compute qo not only for the synsets directly including words in W, but alsofor any higher-level abstractions of them.Example.
Consider the word group doctor, nurse, lawyer.
If one were to include all subsumingconcepts for each word, rather than just the synsets of which they are directly members, theconcepts with non-zero values of ~ would be as follows:?
For doctor:591.00 doctor, doc, physician, MD, Dr.: subconcept ofmedical practitioner1.00 medical practitioner: someone who practices medicine1.00 health professional: subconcept ofprofessional0.43 professional: a person engaged in one of the learned professions?
For nurse:1.001.000.43?
For lawyer:1.001.00nurse: one skilled in caring for the sickhealth professional: subconcept ofprofessionalprofessional: a person engaged in one of the learned professionslawyer, attorney: a professional person authorized to practice lawprofessional: a person engaged in one of the learned professionsGiven assignments of ~ at all levels of abstraction, one obvious method of semantic annotation is to assignthe highest-level concept for which ~ is at least as large as the sense-specific value of ~.
For instance, in theprevious example, one would assign the annotation (hea l th  pro fess iona l )  to both doctor and nurse(thus explicitly capturing a generalization about heir presence in the word group, at the appropriate l velof abstraction), and the annotation (professional) to lawyer.3 ExamplesIn this section I present a number of examples for evaluation by inspection.
In each case, I give the sourceof the noun grouping, the grouping itself, and for each word a description of word senses together with theirvalues of ~.3.1 Distr ibutionally derived groupingsDistributional cluster (Brown et al, 1992): head, body, hands, eye, voice, arm, seat, hair, mouthWord 'head' (17 alternatives)0.0000 crown, peak, summit, head, top: subconceptofupperbound0.0000 principaL school principal, head teacher, head: educator who has executive authority0.0000 head, chief, top dog: subeoncept of leader0.0000 head: a user of (usually soft) drugs0.1983 head: "the head of the page"; "the head of the fist"0.1983 beginning, head, origin, root, source: the point or place where something begins0.0000 pass, head, straits: a difficult juncture; "a pretty pass"0.0000 headway, head: subconcept ofprogress, progression, advance0.0903 point, hod :  a V-shaped mark at one end of an arrow pointer0.0000 heading, head: a line of text serving to indicate what the passage below it is about0.0000 mind, head, intellect, psyche: that which is responsible for your thoughts and feelings0.5428 head: the upper or front part of the body that contains the faee and brains0.0000 toilet, lavatory, can, head, facility, john, privy, bathroom0.0000 head: the striking part of a tool; "hammerhead"0.1685 head: a part that projects out from the rest; "the head of the nail", "pinhead"0.0000 drumhead, head: stretched taut0.0000 oral sex, head: oral-genital stimulationWord 'body' (8 alternatives)0.0000 body: an individual 3-dimensional object hat has mass0.0000 gathering, assemblage, assembly, body, confluence: group of people together in one place0.0000 body: people associated by some common tie or occupation0.0000 body: the centralmessage of acommunication0.9178 torso, trunk, body: subconcept ofbody part, member0.0000 body, organic structure: the entire physical structure of an animal or human being600.0822 consistency, consistence, body: subeoncept of property0.0000 fuselage, body: the central portion of an airplaneWord 'hands'0.00000.06530.06530.00000.00000.00000.21510.71960.00000.0000(10 alternatives)hand: subconeept of linear unithired hand, hand, hired man: a hired laborer on a farm or ranchbridge player, hand: "we need a 4th hand for bridge"hand, deal: the cards held in a card game by a given player at any given timehand: a round of applause to signify approval; "give the little lady a great big hand"handwriting, cursive, hand, script: something written by handhand: ability; "he wanted to try his hand at singing"hand, manus, hook, mauler, mitt, paw: the distal extremity of the superior limbhand: subconcept ofpointerhand: physical assistance; "give me a hand with the chores"Word 'eye' (4 alternatives)0.1479 center, centre, middle, heart, eye: approximately central within some region0.1547 eye: good judgment; "she has an eye for fresh talent"0.6432 eye, eyeball, oculus, optic, peeper, organ of sight0.0542 eye: a sanall hole or loop (as in a needle)Word 'voice' (7 alternatives)0.00000.14140.11220.20290.38950.00000.1539voice: the relation of the subject of a verb to the action that he verb denotesspokesperson, spokesman, i terpreter, epresentative, mouthpiece, voicevoice, vocalization: the sound made by the vibration of vocal foldsarticulation, voice: expressing in coherent verbal form; "I gave voice to my feelings"part, voice: the melody carried by a particular voice or instrument inpolyphonic musicvoice: the ability to speak; "he lost his voice"voice: the distinctive sound of a person's speech; "I recognized her voice"Word 'arm' (6 alternatives)0.0000 branch, subdivision, arm: an administrative division: "a branch of Congress"0.6131 arm: eornrnonly used to refer to the whole superior limb0.0346 weapon, arm, weapon system: used in fighting or hunting0.2265 sleeve, arm: attached at armhole0.1950 arm: any proj~tion that is thought to resemble an arm; "the arm of the record player"0.0346 arm: the part of an armchair that supports he elbow and forearm of a seated personWord 'seat' (6 alternatives)0.0000 seat: a city from which authority is exercised0.0000 seat, place: a space reserved for sitting0.7369 buttocks, arse, butt, backside, burn, buns, can ....0.2631 seat: covers the buttocks0.0402 seat: designed for sitting on0.0402 seat: where one sitsWord 'hair' (50.03230.23131.00001.00001.0000alternatives)hair, pilus: threadlike keratinous filaments growing from the skin of mammalshair, tomentum: filamentous hairlike growth on a planthair, follicular growth: subeoncept of externalbody parthair, mane, head of hair: hair on the headhair: hairy covering of an animal or body partWord 'mouth' (5 alternatives)0.0000 mouth: the point where a stream issues into a larger body of water0.0000 mouth: an opening that resembles a mouth (as of a cave or a gorge)0.0613 sass, sassing, baektalk, lip, mouth: an impudent or insolent rejoinder0.9387 mouth, oral cavity: subconcept ofcavity, body cavity, bodily cavity0.9387 mouth, trap, hole, maw, yap, muzzle, suout: list includes informal terms for "mouth"This group was among classes hand-selected by Brown et al as "particularly interesting.
"61Distributional cluster (Brown et al, 1992): tie, jacket, suitWord 'tie' (7 alternatives)0.00000.00000.00001.00000.00000.00000.0000draw, standoff, tie, stalemateaffiliation, association, tie, tie-up: a social or business relationshiptie, crosstie, sleeper: subconcept ofbrace, bracingnecktie, tielink, linkup, tie, tie-in: something that serves to join or linkdrawstring, string, tie: cord used as a fastenertie, tie beam: used to prevent two rafters, e.g., from spreading apartWord 'jacket' (4 alternatives)0.0000 book jacket, dust cover: subeoncept ofpromotional material0.0000 jacket crown, jacket: artificial crown fitted over a broken or decayed tooth0.0000 jacket: subconceptofwrapping, wrap, wrapper1.0000 jacket: a short coatWord 'suit' (4 alternatives)0.0000 suit, suing: subconcept of entreaty, prayer, appeal1.0000 suit, suit of clothes: subconcept of garment0.0000 suit: any of four sets of13" cards in a paek0.0000 legal action, action, case, lawsuit, suit: a judicial proceedingThis cluster was derived by Brown et al using a modification of their algorithm, designed to uncover"semantically sticky" clusters.Distributional cluster (Brown et al, 1992): cost, expense, risk, profitability, deferral, earmarks, capstone,cardinality, mintage, resellerWord 'cost' (2 alternatives)0.5426 cost, price, terms, damage: the amount of money paid for something0.4574 monetary value, price, cost: the amount of money it would bring if soldWord 'expense' (2 alternatives)1.0000 expense, expenditure, outlay, outgo, spending, disbursal, disbursement0.0000 expense: a detriment or sacrifice; "at the expense of"Word 'risk' (2 alternatives)0.6267 hazard, jeopardy, peril risk: subconeept ofdanger0.3733 risk, peril danger: subeonceptofventureWord 'profitability' (1 alternatives)1.0000 profitableness, profitability: subconcept of advantage, benefit, usefulnessWord 'deferral' (3 alternatives)0.6267 abeyance, deferral, recess: subconcept of inaction, inactivity, inactiveness0.3733 postponement, deferment, deferral, moratorium: an agreed suspension ofactivity0.3733 deferral: subconeeptofpause, waitWord 'earmarks' (2 alternatives)0.2898 earmark: identification mark on the ear of a domestic animal0.7102 hallma.k, trademark, earmark: a distinguishing characteristic or attributeWord 'capstone' (1 alternatives)1.0000 capstone, coping stone, stretcher: used at top of wallWord 'eardinality'Not in WordNetWord 'mintage' (1 alternatives)621.0000 coinage, mintage, specie, metal money: subconcept ofcashWord 'reseller'Not in WordNetThis cluster was one presented by Brown et al as a randomly-selected class, rather than one hand-pickedfor its coherence.
(I hand-selected it from that group forpresentation here, however.
)Distributional neighborhood (Sch0tze, 1993): burglars, thief, rob, mugging, stray, robbing, lookout,chase, crateWord 'burglars' (1 alternatives)1.0000 burglar: subconceptofthief, robberWord 'thief' (1 alternatives)1.0000 thief, robber: subconceptof erirninal, felon, crook, outlawWord 'mugging' (1 aRernatives)1.0000 battering, beating, mugging, whipping: subconcept of fight, fightingWord 'stray' (1 alternatives)1.0000 alley cat, stray: homeless catWord 'lookout'0.64630.00000.12690.2268(4 alternatives)lookout, lookout man.
sentinel, sentry, watch, scoutlookout, observation post: an elevated post affording a wide viewlookout, observation tower, lookout g.ation, observatory:lookout, outlook: wabconcept of look.
looking atWord 'chase' (1 aRernatives)1.0000 pursuit, chase, follow, following: the act of pursuingWord 'orate' (2 aRernatiVes)0.0000 crate, eratefi.d: subconcept of containerful1.0000 crate: a ragged box (usually made of wood); used for shippingAs noted in Section 2.1, this group represents a set of words similar to burglar, according to Schtltze'smethod for deriving vector epresentation from corpus behavior.
In this case, words rob and robbing wereexcluded because they were not nouns in WordNet.
The word stray probably should be excluded also, sinceit most likely appears on this list as an adjective (as in "stray bullet").Machine-generated thesaurus entry (Grefenstette, 1994): method, test, mean, procedure, techniqueWord 'method' (2 alternatives)1.0000 method: a way of doing something, esp.
a systematic one0.0000 wise, method: a way of doing or being: "in no wise"; "in this wise"Word 'test' (7 alternatives)0.6817 trial, test, tryout: trying something to find out about it; "ten days free trial"0.6817 assay, check, test: subeoncept ofappraisal assessanent0.0000 examination, exam, test: a set of questions or exercises evaluating skill or knowledge0.3183 test, mental test, mental testing, psychometric test0.0000 test: a hard outer covering as of some amoebas and sea urchins630.3183 test, trial: the act ofundergoingtesting; "he survived the great est of battle"0.3183 test, trial run: the act of testing somethingWord 'mean' (1 alternatives)1.0000 mean: an average ofn numbers computed by...Word 'proeedure' (4 alternatives)1.0000 procedure, process: aparticular course of action intended to achieve a results1.0000 operation, procedure: a process or series of acts ,.. involved in a particular form of work0.0000 routine, subroutine, subprogram, procedure, function0.0000 procedure: a mode of conducting legal and parliamentary proceedingsWord 'technique' (2 alternatives)1.0000 technique: a tecfiniealmethod0.0000 profieieney, facility, technique: skillfulness deriving from practice and familiarityI chose this grouping at random from a thesaurus created automatically by Grefenstette's yntactico-distributional methods, using the MED corpus of medical abstracts as its source.
The group comes fromfrom the thesaurus entry for the word method.
Note that mean probably should be means.3.2 Thesaurus  ClassesThere is a tradition in sense disambiguation of taking particularly ambiguous words and evaluating asystem'sperformance on those words.
Here I look at one such case, the word line; the goal is to see what sense thealgorithm chooses when considering the word in the contexts of each of the Roget's Thesaurus classes inwhich it appears, where a "class" includes all the nouns in one of the numbered categories.7 The followinglist provides brief descriptions of the 25 senses of line in WordNet:1. wrinkle, furrow, crease, crinkle, seam, line: "His faeehas many wrinkles"2. line: a length (straight or curved) without breadth or thickness3.
line, dividing line: "there is a narrow line between sanity and insanity"4. agate line, line: space for one line of print used to measure advertising5.
credit line, line of credit, line: the maximum credit that a customer is allowed6.
line: in games or sports; a mark indicating positions or bounds of the playing area7.
line: a spatial ocation defined by a real or imaginary unidimensional extent8.
eourse, line: a connected series of events or actions or developments9.
fine: a formation of people or things one after (or beside) another10.
lineage, line, line of descent, descent, bloodline, blood line, blood, pedigreeI 1. tune, melody, air, strain, melodic fine, line, melodic phrase: a succession of notes12.
line: a linear string of words expressing some idea13.
line: a mark that is long relative to its width; "He drew a line on the chart"14. note, short letter, line: "drop me a line when you get there"15. argumentation, logical argument, fine of thought, fine of reasoning, fine16.
telephone fine, phone line, fine: a telephone connection71 am grateful to Mark Lauer  for his k ind assistance with the thesaurus.6417.
production fine, assembly fine, fine: a factory system18.
pipeline, line: a long pipeused to transport liquids or gases19.
line: a cornmereial organization serving as a common carrier20.
fine, railway fine, rail line: railroad track and roadbed21.
fine: something long and thin and flexible22.
cable, line, transmission fi e: electrical conductor connecting telephones ortelevision23.
line, product fine, line of products, line of merchandise, business fine, fine of business24.
fine: acting in conformity; "in fine with" or "he got out of line" or "toe the fine"25. occupation, business, line of work, line: the principal activity in your lifeSince line appears in 13 of the numbered categories in Roget's thesaurus, a full description of the valuesof qo would be too large for the present paper.
Indeed, showing all the nouns in the numbered categorieswould take up too much space: they average about 70 nouns apiece.
Instead, I identify the numberedcategory, and give the three WordNet senses of line for which ~o was greatest.\[#4S.\] [Connecting medium.\] Connection.0.4280 cable, line, transmission li e0.2966 telephone line, phone line, line: a telephone connection0.2838 fine: something long and thin and flexible\[#69.\] \[Uninterrupted sequence.\] Continuity.0.3027 lineage, line, fine of descent0.2172 fine: a formation of people or things one after (or beside) another0.1953 course, line: a connected series of events or actions or developments\[#166.\] Paternity.0.5417 lineage, line, line of descent, descent, bloodfine, blood line, blood, pedigree0.2292 pipeline, line: a long pipe used to transport liquids or gases0.1559 llne, product line, line of products, line of merchandise\[#167.\] Posterity.0.3633 lineage, line, line of descent, descent, bloodline, blood line, blood, pedigree0.2904 line, product line, line of products, line of merchandise0.2464 cable, line, transmission li e: electrical conductor connecting telephones ortelevision\[#200.\] Length.0.5541 agateline, line: spaeeforonelineofprintusedtomeasoreadvertising0.0906 cable, line, transmission li e: electrical conductor connecting telephones ortelevision0.0894 telephone line, phone line, line: a telephone connection\[#203.\] Narrowness.
Thinness.0.2496 pipeline, line: a long pipe used to transport liquids or gases0.2141 line: a linear string of words expressing some idea0.2141 note, short letter, line: "drop me a line when you get there"\[#205.\] Filament.0.5724 line: something long and thin and flexible0.I 805 cable, line, transmission li e: electrical conductor connecting telephones ortelevision0.1425 line: in games or sports; amark indicating positions or bounds of the playing area\[#278.\] Direction.0.2083 line: a spatial location defined by a real or imaginary unidimensionalextent0.1089 wrinkle, furrow, crease, crinkle, seam, line: "His face has many wrinkles"0.1031 line: alength without breadth or thickness; the trace of a moving point\[#413.\] Melody.
Concord.650.34740.13370.1030note, short letter, line: "drop me a line when you get there"agate line, line: space for one line of print used to measure advertisingtune, melody, air, strain, melodic line, line, melodic phrase\[#466.\] Measurement.0.5423 cable, line, transmission li e: electrical conductor connecting telephones or television0.1110 argumentation, logical argument, line of thought, line of reasoning, line0.0969 agate line, line: space for one line of print used to measure advertising\[#590.\] Writing.0.4743 note, short letter, line: "drop me a line when you get there"0.1734 cable, line, transmission line: electrical conductor connecting telephones or television0.1648 tune, melody, air, strain, melodic line, line, melodic phrase\[#597.\] Poetry0.37170.26890.2272note, short letter, line: "drop me a line when you get there"tune, melody, air, strain, melodic line, line, melodic phraseline: a linear string of words expressing some idea\[#625.\] Business.0.4684 occupation, business, line of work, line: the principal activity in your life0.1043 line: a commercial organization serving as a common carrier0.0790 tune, melody, air, strain, melodic line, line, melodic phraseQualitatively, the algorithm does a good job in most of the categories.
The reader might find it an interestingexercise to try to decide which of the 25 senses he or she would choose, especially in the cases where thealgorithm did less well (e.g.
categories #200, #203, #466).4 Formal EvaluationThe previous section provided illustrative examples, demonstrating the performance of the algorithm onsome interesting cases.
In this section, I present experimental results using a more rigorous evaluationmethodology.Input for this evaluation came from the numbered categories of Roget's.
Test instances consisted ofa noun group (i.e., all the nouns in a numbered category) together with a single word in that group tobe disambiguated.
To use an example from the previous section, category #590 ("Writing") contains thefollowing:writing, chirography, penman ship, quill driving, typewriting, writing, manuscript, MS, these presents, troke of the pen,dash of the pen, coupe de plume, line, headline, pen and ink, letter, uncial writing, cuneiform character, arrowhead,Ogham, Runes, hieroglyphic, ontraction, Devanagari, Nagari, script, shorthand, stenography, secret writing, writ-ing in cipher, cryptography, stenography, copy, transcript, rescript, rough copy, fair copy, handwriting, signature,sign manual, autograph, monograph, olograph, hand, fist, calligraphy, good hand, running hand, flowing hand, cur-sive hand, legible hand, bold hand, bad hand, cramped hand, crabbed hand, illegible hand, scribble, ill-formed letters,pothooks and hangers, stationery, pen, quill, goose quill, pencil, style, paper, foolscap, parchment, veUum, papyrus,tablet, slate, marble, pillar, table, blackboard, ink bottle, ink horn, ink pot, ink stand, ink well, typewriter, tran-scription, inscription, superscription, graphology, composition, authorship, writer, scribe, amanuensis, crivener,secretary, clerk, penman, copyist, transcriber, quill driver, stenographer, typewriter, typist, writer for the pressAny word or phrase in that group that appears in the noun taxonomy for WordNet would be a candidate asa test instance - -  for example, line, or secret writing.The test set, chosen at random, contained 125 test cases.
(Note that because of the random choice, therewere some cases where more than one test instance came from the same numbered category.)
Two humanjudges were independently given the test cases to disambiguate.
For each case, they were given the full setof nouns in the numbered category (as shown above) together with descriptions of the WordNet senses for66the word to be disambiguated (as, for example, the list of 25 senses for line given in the previous ection,though thankfully few words have that many senses!).
It was a forced-choice task; that is, the judge wasrequired to choose xactly one sense.
In addition, for each judgment, he judge was required to provide aconfidence value for this decision, ranging from 0 (not at all confident) o 4 (highly confident).Results are presented here individually by judge.
For purposes of evaluation, test instances for whichthe judge had low confidence (i.e.
confidence ratings o f  0 or 1) were excluded.For Judge 1, there were 99 test instances with sufficiently high confidence to be considered.
As abaseline, ten runs were done selecting senses by random choice, with the average percent correct being34.8%, standard eviation 3.58.
As an upper bound, Judge 2 was correct on 65.7% of those test instances.The disambiguation algorithm shows considerable progress toward this upper bound, with 58.6% correct.For Judge 2, there were 86 test instances with sufficiently high confidence to be considered.
As abaseline, ten runs were done selecting senses by random choice, with the average percent correct being33.3%, standard eviation 3.83.
As an upper bound, Judge 1 was correct on 68.6% of those test instances.Again, the disambiguation algorithm performs well, with 60.5% correct.5 Conclusions and Future WorkThe results of the evaluation are exlremely encouraging, especially considering that disambiguating wordsenses to the level of fine-grainedness found in WordNet is quite a bit more difficult han disambiguationto the level of homographs (Hearst, 1991; Cowie et al, 1992).
A note worth adding: it is not clear thatthe "exact match" criterion - -  that is, evaluating algorithms by the percentage of exact matches of senseselection against a human-judged baseline - -  is the right task.
In particular, in many tasks it is at least asimportant to avoid inappropriate senses than to select exactly the right one.
This would be the case in queryexpansion for information retrieval, for example, where indiscriminately adding inappropriate words to aquery can degrade performance (Voorhees, 1994).
The examples presented in Section 3 are encouraging inthis regard: in addition to performing well at the task of assigning a high score to the best sense, it does agood job of assigning low scores to senses that are clearly inappropriate.Regardless of the criterion for success, the algorithm does need further evaluation.
Immediate plansinclude a larger scale version of the experiment presented here, involving thesaurus classes, as well as asimilarly designed evaluation of how the algorithm fares when presented with noun groups produced bydistributional c ustering.
In addition, I plan to explore alternative measures of semantic similarity, forexample an improved variant on simple path length that has been proposed by Leacock and Chodorow(1994).Ultimately, this algorithm is intended to be part of a suite of techniques used for disambiguating words inrunning text with respect to WordNet senses.
I would argue that success at that task will require combiningknowledge of the kind that WordNet provides, primarily about relatedness of meaning, with knowledgeof the kind best provided by corpora, primarily about usage in context.
The difficulty with the latter kindof knowledge is that, until now, the widespread success in characterizing lexical behavior in terms ofdistributional relationships has applied at the level of words - -  indeed, word forms - -  as opposed to senses.This paper epresents a step toward getting as much leverage as possible out of work within that paradigm,and then using it to help determine r lationships among word senses, which is really where the action is.BibliographyBasili, R., Pazienza, M. T., and Velardi, P. (1994).
The noisy channel and the braying donkey.
In Klavans, J. andResnik, P., editors, Proceedings of the ACL Workshop on Combining Symbolic and Statistical Approaches to67Language (The Balancing Act), pages 21-28.
Association for Computational Linguistics.Bensch, P. A. and Savitch, W. J.
(1992).
An occu~ence-based model of word categorization.
Presented at3rd Meetingon Mathematics ofLanguage (MOL3).Bfill, E. (1991).
Discovering the lexical features of a language.
In Proceedings of the 29th Annual Meeting of theAssociation for Computational Linguistics, Berkeley, CA.Brown, P. E, Della Pietra, V. J., deSouza, P. V., Lai, J. C., and Mercer, R. L. (1992).
Class-based n-gram models ofnatural language.
ComputationalLinguistics, 18(4):467-480.Church, K. and Hanks, P. (1989).
Word association norms, mutual information, and lexicography.
In Proceedings ofthe 27th Meeting of the Association for Computational Linguistics.
Vancouver, B.C.Cowie, J., Guthrie, J., and Guthrie, L. (1992).
Lexical disambiguation using simulated annealing.
In Proceedings ofCOLING-92, pages 359-365, Nantes, France.Grefenstette, G. (1994).
Explorations inAutomatic Thesaurus Discovery.
Kluwer.Hearst, M. (1991).
Noun homograph disambiguafion using local context in large corpora.
In Proceedings of the 7thAnnual Conference of the University of Waterloo Centre for the New OED and Text Research, Oxford.Hearst, M. and Schiitze, H. (1993).
Customizing a lexicon to better suit a computational t sk.
In Proceedings of theACL SIGLEX Workshop, Columbus, Ohio.Leacock, C. and Chodorow, M. (1994).
Filling in a sparse training space for word sense identification, ms.Lee, J. H., Kim, M. H., and Lee, Y. J.
(1993).
Information retrieval based on conceptual distance in IS-A hierarchies.Journal of Documentation, 49(2): 188-207.Lesk, M. (1986).
Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone froman ice cream cone.
In Proceedings of the 1986 SIGDOC Conference, pages 24-26.Marcus, M. P., Santorini, B., and Marcinkiewicz, M. (1993).
Building a large annotated corpus of English: the PennTreebank.
ComputationalLinguistics, 19:313-330.McKeown, K. and Hatzivassiloglou, V. (1993).
Augmenting lexicons automatically: Clustering semantically relatedadjectives.
In Bates, M., editor, ARPA Workshop on Human Language Technology.Miller, G. (1990).
WordNet: An on-line lexical database.
International Journal of Lexicography, 3(4).
(Special Issue).Pereira, E, Tishby, N., and Lee, L. (1993).
Distributional c ustering of English words.
In Proceedings of ACL-93.Rada, R., Mili, H., Bicknell, E., and Blettuer, M. (1989).
Development and application of a metric on semantic nets.IEEE Transaction on Systems, Man, and Cybernetics, 19(1): 17-30.Resnik, P. (1993).
Selection and Information: A Class-Based Approach to Lexical Relationships.
PhD thesis,University of Pennsylvania.Resnik, P. (1995).
Using information content to evaluate semantic similarity in a taxonomy.
In IJCAI-95.Schiitze, H. (1993).
Word space.
In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, Advances in NeuralInformation Processing Systems 5,pages 895-902.
Morgan Kaufnaann Publishers, San Mateo CA.Sussna, M. (1993).
Word sense disambiguation for free-text indexing using a massive semantic network.
In Proceed-ings of the Second International Conference on Information and Knowledge Management (CIKM-93 ), Arlington,Vkginia.Voorhees, E. M. (1994).
Query expansion using lexical-semantic relations.
In 17th International Conference onResearch and Development i  Information Retrieval (SIGIR '94), Dublin, Ireland.Yarowsky, D. (1992).
Word-sense disambiguation using statistical models of Roget's categories trained on largecorpora.
In Proceedings of COLING-92, pages 454-460.
Nantes, France.68
