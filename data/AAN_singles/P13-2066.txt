Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 370?374,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Novel Translation Framework Based on Rhetorical Structure TheoryMei Tu             Yu Zhou           Chengqing ZongNational Laboratory of Pattern Recognition, Institute of Automation,Chinese Academy of Sciences{mtu,yzhou,cqzong}@nlpr.ia.ac.cnAbstractRhetorical structure theory (RST) is widelyused for discourse understanding, which repre-sents a discourse as a hierarchically semanticstructure.
In this paper, we propose a noveltranslation framework with the help of RST.
Inour framework, the translation process mainlyincludes three steps: 1) Source RST-tree ac-quisition: a source sentence is parsed into anRST tree; 2) Rule extraction: translation rulesare extracted from the source tree and the tar-get string via bilingual word alignment; 3)RST-based translation: the source RST-treeis translated with translation rules.
Experi-ments on Chinese-to-English show that ourRST-based approach achieves improvementsof 2.3/0.77/1.43 BLEU points onNIST04/NIST05/CWMT2008 respectively.1 IntroductionFor statistical machine translation (SMT), a cru-cial issue is how to build a translation model toextract as much accurate and generative transla-tion knowledge as possible.
The existing SMTmodels have made much progress.
However,they still suffer from the bad performance of un-natural or even unreadable translation, especiallywhen the sentences become complicated.
Wethink the deep reason is that those models onlyextract translation information on lexical or syn-tactic level, but fail to give an overall under-standing of source sentences on semantic level ofdiscourse.
In order to solve such problem, (Gonget al, 2011; Xiao et al, 2011; Wong and Kit,2012) build discourse-based translation modelsto ensure the lexical coherence or consistency.Although some lexicons can be translated betterby their models, the overall structure still re-mains unnatural.
Marcu et al  (2000) design adiscourse structure transferring module, but leavemuch work to do, especially on how to integratethis module into SMT and how to automaticallyanalyze the structures.
Those reasons urge us toseek a new translation framework under the ideaof ?translation with overall understanding?.Rhetorical structure theory (RST) (Mann andThompson, 1988) provides us with a good per-spective and inspiration to build such a frame-work.
Generally, an RST tree can explicitly showthe minimal spans with semantic functional in-tegrity, which are called elementary discourseunits (edus) (Marcu et al, 2000), and it also de-picts the hierarchical relations among edus.
Fur-thermore, since different languages?
edus areusually equivalent on semantic level, it is intui-tive to create a new framework based on RST bydirectly mapping the source edus to target ones.Taking the Chinese-to-English translation asan example, our translation framework works asthe following steps:1) Source RST-tree acquisition: a sourcesentence is parsed into an RST-tree;2) Rule extraction: translation rules are ex-tracted from the source tree and the target stringvia bilingual word alignment;3) RST-based translation:  the source RST-tree is translated into target sentence with ex-tracted translation rules.Experiments on Chinese-to-English sentence-level discourses demonstrate that this methodachieves significant improvements.2 Chinese RST Parser2.1 Annotation of Chinese RST TreeSimilar to (Soricut and Marcu, 2003), a node ofRST tree is represented as a tuple R-[s, m, e],which means the relation R controls two seman-tic spans U1 and U2 , U1 starts from word positions and stops at word position m. U2 starts fromm+1 and ends with e. Under the guidance of def-inition of RST, Yue (2008) defined 12 groups1 of1They are Parallel, Alternative, Condition, Reason, Elabo-ration, Means, Preparation, Enablement, Antithesis, Back-ground, Evidences, Others.370rhetorical relations for Chinese particularly, uponwhich our Chinese RST parser is developed.Figure 1 illustrates an example of ChineseRST tree and its alignment to the English string.There are two levels in this tree.
The Antithesisrelation controls U1 from 0 to 9 and U2 from 10to 21.
Thus it is written as Antithesis-[0,9,21].Different shadow blocks denote the alignmentsof different edus.
Links between source and tar-get words are alignments of cue words.
Cuewords are viewed as the strongest clues for rhe-torical relation recognition and always found atthe beginning of text (Reitter, 2003), such as ???
(although), ??
(because of)?.
With the cuewords included, the relations are much easier tobe analyzed.
So we focus on the explicit relationswith cue words in this paper as our first try.2.2 Bayesian Method for Chinese RST ParserFor Chinese RST parser, there are two tasks.
Oneis the segmentation of edu and the other is therelation tagging between two semantic spans.Feature MeaningF1(F6) left(right) child is a syntactic sub-tree?F2(F5) left(right) child ends with a punctuation?F3(F4) cue words of left (right) child.F7 left and right children are sibling nodes?F8(F9) syntactic head symbol of left(right) child.Table 1: 9 features used in our Bayesian modelInspired by the features used in English RSTparser (Soricut and Marcu, 2003; Reitter, 2003;Duverle and Prendinger, 2009; Hernault et al,2010a), we design a Bayesian model to build ajoint parser for segmentation and tagging simul-taneously.
In this model, 9 features in Table 1 areused.
In the table, punctuations include comma,semicolons, period and question mark.
We viewexplicit connectives as cue words in this paper.Figure 2 illustrates the conditional independ-ences of 9 features which are denoted with F1~F9.The segmentation and parsing conditionalprobabilities are computed as follows:P (mjF 91 ) = P (mjF 31 ; F8) (1)P (ejF 91 ) = P (ejF 74 ; F9) (2)P (ReljF 91 ) = P (ReljF 43 ) (3)where Fn represents the nth  feature , F ln meansfeatures from n  to l .
Rel  is short for relation.
(1)and (2) describe the conditional probabilities ofm and e. When using Formula (3) to predict therelation, we search all the cue-words pair, asshown in Figure 1, to get the best match.
Whentraining, we use maximum likelihood estimationto get al the associated probabilities.
For decod-ing, the pseudo codes are given as below.m eF1 F2 F3RelF4 F5 F6 F7F8 F9Figure 2: The graph for conditional independencesof 9 features.1: Nodes={[]}2: Parser(0,End)3: Parser(s,e): // recursive parser function4:    if s > e or e is -1: return -1;5:    m = GetMaxM(s,e)  //compute m through Formu-la(1);if no cue words found,then m=-1;6:      e?
= GetMaxE(s,m,e)  //compute e?
through F (2);7:    if m or e?
equals to -1: return -1;8:   Rel=GetRelation(s,m,e?)
//compute relation by F(3)9:    push [Rel,s,m,e?]
into Nodes10:  Parser(s,m)11:  Parser(m+1,e?
)12:  Parser(e?+1,e)13:   Rel=GetRelation(s,e?,e)14:   push [Rel,s,e?,e] into Nodes15:   return eJ?sh?
l?b?
du?
me?yu?n  de m?ngy?
hu?l?
xi?ji?ng  le    ,??
??
?
??
?
??
??
??
?
?0               1           2         3              4        5                6            7          8     9y?uy?
g?o t?ngzh?ng ,??
?
??
?10          11      12            13q?
sh?j?
hu?l?
y?
sh?
sh?ngsh?ng de    .?
??
??
?
?
??
?
?14      15           16        17   18             19            20   21Although the rupee's nominal rate against the dollar was held down , India's real exchange rate rose because of  high inflation .ReasonAntithesisU1:[0,9] U2:[10,21]U1:[10,13] U2:[14,21]Cue-words pair matching set of cue words for span [0,9] and [10,21]:{??/??,??/NULL,NULL/??
}Cue-words pair matching set of cue words for span [10,13] and [14,21]:{?
?/NULL}RST-based Rules:  Antithesis:: ??
[X]/[Y] => Although[X]/[Y] ;  Reason::??
[X]/[Y] => [Y]/because of[X]Example 1:Figure 1: An example of Chinese RST tree and its word alignment of the corresponding English string.371For example in Figure 1, for the first iteration,s=0 and m will be chosen from {1-20}.
We getm=9 through Formula (1).
Then, similar with m,we get e=21 through Formula (2).
Finally, therelation is figured out by Formula (3).
Thus, anode is generated.
A complete RST tree con-structs until the end of the iterative process forthis sentence.
This method can run fast due to thesimple greedy algorithm.
It is plausible in ourcases, because we only have a small scale ofmanually-annotated Chinese RST corpus, whichprefers simple rather than complicated models.3 Translation Model3.1 Rule ExtractionAs shown in Figure 1, the RST tree-to-stringalignment provides us with two types of transla-tion rules.
One is common phrase-based rules,which are just like those in phrase-based model(Koehn et al, 2003).
The other is RST tree-to-string rule, and it?s defined as,relation ::U1(?;X)=U2(?
; Y )) U1(tr(?
); tr(X)) ?
U2(tr(?
); tr(Y ))where the terminal characters ?
and ?
representthe cue words which are optimum match formaximizing Formula (3).
While the non-terminals X and Y represent the rest of the se-quence.
Function tr(? )
means the translationof ?.
The operator ~ is an operator to indicatethat the order of tr(U1) and tr(U2) is monotone orreverse.
During rules?
extraction, if the meanposition of all the words in tr(U1) precedes thatin tr(U2), ~ is monotone.
Otherwise, ~ is reverse.For example in Figure 1, the Reason relationcontrols U1:[10,13] and U2:[14,21].
Because themean position of tr(U2) is before that of tr(U1),the reverse order is selected.
We list the RST-based rules for Example 1 in Figure 1.3.2 Probabilities EstimationFor the phrase-based translation rules, we usefour common probabilities and the probabilities?estimation is the same with those in (Koehn et al,2003).
While the probabilities of RST-basedtranslation rules are given as follows,(1) P(rejrf ;Rel) = Count(re;rf ;relation)Count(rf ;relation):  where reis the target side of the rule, ignorance of the or-der, i.e.
U1(tr(?
); tr(X)) ?
U2(tr(?
); tr(Y )) withtwo directions, rf is the source side, i.e.U1(?;X)=U2(?
;Y ), and Rel  means the relationtype.
(2) P(?jre; rf ;Rel) = Count(?
;re;rf ;relation)Count(re;rf ;relation):?
2 fmonotone; reverseg.
It is the conditionalprobability of re-ordering.4 DecodingThe decoding procedure of a discourse can bederived from the original decoding formulaeI1 = argmaxeI1P (eI1jfJ1 ).
Given the rhetoricalstructure of a source sentence and the corre-sponding rule-table, the translating process is tofind an optimal path to get the highest score un-der structure constrains, which is,argmaxesfP (esj; ft)g= argmaxesfYfn2ftP (eu1; eu2; ?
jfn)gwhere ft  is a source  RST tree combined by a setof node fn .
es is the target string combined byseries of en  (translations of fn ).
fn  consists ofU1 and U2.
eu1  and eu2  are translations of U1 andU2 respectively.
This global optimization prob-lem is approximately simplified to local optimi-zation to reduce the complexity,Yfn2ftargmaxenfP(eu1; eu2; ?
jfn)gIn our paper, we have the following two waysto factorize the above formula,Decoder 1:P (eu1; eu2; ?
jfn)= P (ecp; eX ; eY ; ?
jfcp; fX ; fY )= P (ecpjfcp)P (?
jecp; fcp)P (eX jfX )P (eY jfY )= P (rejrf ;Rel)P (?
jre; rf ; Rel)P (eX jfX )P (eY jfY )where eX, eY are the translation of non-terminalparts.
fcp  and ecp  are cue-words pair of sourceand target sides.
The first and second factors arejust the probabilities introduced in Section 3.2.After approximately simplified to local optimiza-tion, the final formulae are re-written as,argmaxrfP (rejrf ; Rel)P (?
jre; rf ; Rel)g (4)argmaxeX fP (eX jfX )g (5)argmaxeY fP (eY jfY )g (6)Taking the source sentence with its RST treein Figure 1 for instance, we adopt a bottom-upmanner to do translation recursively.
Suppose thebest rules selected by (4) are just those written inthe figure, Then span [11,13] and [14,21] arefirstly translated by (5) and (6).
Their translationsare then re-packaged by the rule of Reason-[10,13,21].
Iteratively, the translations of span[1,9] and [10,21] are re-packaged by the rule ofAntithesis-[0,9,21] to form the final translation.372Decoder 2 : Suppose that the translating processof two spans U1 and U2 are independent of eachother, we rewrite P(eu1; eu2; ?
jfn)   as follows,P (eu1; eu2; ?
jfn)= P (eu1; eu2; ?
jfu1; fu2)= P (eu1jfu1)P (eu2jfu2)P (?
jrf ; Rel)= P (eu1jfu1)P (eu2jfu2)XreP (?
jre; rf ; Rel)P (rejrf ; Rel)after approximately simplified to local optimization,the final formulae are re-written as below,argmaxeu1fPr(eu1jfu1)g (7)argmaxeu2fPr(eu2jfu2)g (8)argmaxrfXePr(?
jre; rf ; Rel)Pr(rejrf ; Rel)g (9)We also adopt the bottom-up manner similarto Decoder 1.
In Figure 1, U1 and U2 of Reasonnode are firstly translated.
Their translations arethen re-ordered.
Then the translations of twospans of Antithesis node are re-ordered and con-structed into the final translation.
In Decoder 2,the minimal translation-unit is edu.
While in De-coder 1, an edu is further split into cue-word partand the rest part to obtain the respective transla-tion.In our decoders, language model(LM) is usedfor translating edus in Formula(5),(6),(7),(8), butnot for reordering the upper spans because withthe bottom-to-up combination, the spans becomelonger and harder to be judged by a traditionallanguage model.
So we only use RST rules toguide the reordering.
But LM will be properlyconsidered in our future work.5 Experiment5.1 SetupIn order to do Chinese RST parser, we annotatedover 1,000 complicated sentences on CTB (Xueet al, 2005), among which 1,107 sentences areused for training, and 500 sentences are used fortesting.
Berkeley parser2 is used for getting thesyntactic trees.The translation experiment is conducted onChinese-to-English direction.
The bilingual train-ing data is from the LDC corpus3.
The trainingcorpus contains 2.1M sentence pairs.
We obtainthe word alignment with the grow-diag-final-andstrategy by GIZA++4.
A 5-gram language modelis trained on the Xinhua portion of the English2http://code.google.com/p/berkeleyparser/3LDC category number : LDC2000T50, LDC2002E18,LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27,LDC2005T10 and LDC2005T344 http://code.google.com/p/giza-pp/Gigaword corpus.
For tuning and testing, we useNIST03 evaluation data as the development set,and extract the relatively long and complicatedsentences from NIST04, NIST05 and CWMT085evaluation data as the test set.
The number andaverage word-length of sentences are 511/36,320/34, 590/38 respectively.
We use case-insensitive BLEU-4 with the shortest length pen-alty for evaluation.To create the baseline system, we use thetoolkit Moses6 to build a phrase-based translationsystem.
Meanwhile, considering that Xiong et al(2009) have presented good results by dividinglong and complicated sentences into sub-sentences only by punctuations during decoding,we re-implement their method for comparison.5.2 Results of Chinese RST ParserTable 2 shows the results of RST parsing.
Onaverage, our RS trees are 2 layers deep.
Theparsing errors mostly result from the segmenta-tion errors, which are mainly caused by syntacticparsing errors.
On the other hand, the polyse-mous cue words, such as ??
(but, and, thus)?may lead ambiguity for relation recognition, be-cause they can be clues for different relations.Task Precision Recall F1Segmentation 0.74 0.83 0.78Labeling 0.71 0.78 0.75Table 2: Segmentation and labeling result.5.3 Results of TranslationTable 3 presents the translation comparison re-sults.
In this table, XD represents the method in(Xiong et al, 2009).
D1 stands for Decoder-1,and D2 for Decoder-2.
Values with boldface arethe highest scores in comparison.
D2 performsbest on the test data with 2.3/0.77/1.43/1.16points.
Compared with XD, our results also out-perform by 0.52 points on the whole test data.Observing and comparing the translation re-sults, we find that our translation results are morereadable by maintaining the semantic integralityof the edus and by giving more appreciate reor-ganization of the translated edus.Testing Set Baseline XD D1 D2NIST04 29.39 31.52 31.34 31.69NIST05 29.86 29.80 30.28 30.63CWMT08 24.31 25.24 25.74 25.74ALL 27.85 28.49 28.66 29.01Table 3: Comparison with related models.5China Workshop on Machine Translation 20086www.statmt.org/moses/index.php?n=Main.HomePage3736 Conclusion and Future WorkIn this paper, we present an RST-based transla-tion framework for modeling semantic structuresin translation model, so as to maintain the se-mantically functional integrity and hierarchicalrelations of edus during translating.
With respectto the existing models, we think our translationframework works more similarly to what humandoes, and we believe that this research is a cru-cial step towards discourse-oriented translation.In the next step, we will study on the implicitdiscourse relations for Chinese and further modi-fy the RST-based framework.
Besides, we willtry to combine other current translation modelssuch as syntactic model and hierarchical modelinto our framework.
Furthermore, the more accu-rate evaluation metric for discourse-orientedtranslation will be further studied.AcknowledgmentsThe research work has been funded by the Hi-TechResearch and Development Program (?863?
Pro-gram) of China under Grant No.
2011AA01A207,2012AA011101, and 2012AA011102 and also sup-ported by the Key Project of Knowledge Innova-tion Program of Chinese Academy of Sciences un-der Grant No.KGZD-EW-501.ReferencesDavid A Duverle and Helmut Prendinger.
2009.
Anovel discourse parser based on support vector ma-chine classification.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Nat-ural Language Processing of the AFNLP: Volume2-Volume 2, pages 665?673.
Association for Com-putational Linguistics.Zhengxian Gong, Min Zhang, and Guodong Zhou.2011.
Cache-based document-level statistical ma-chine translation.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 909?919.
Association for Computa-tional Linguistics.Hugo Hernault, Danushka Bollegala, and MitsuruIshizuka.
2010a.
A sequential model for discoursesegmentation.
Computational Linguistics and Intel-ligent Text Processing, pages 315?326.Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka,et al 2010b.
Hilda: A discourse parser using sup-port vector machine classification.
Dialogue &Discourse, 1(3).Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language TechnologyVolume 1, pages 48?54.
Association for Computa-tional Linguistics.William C Mann and Sandra A Thompson.
1986.Rhetorical structure theory: Description and con-struction of text structures.
Technical report, DTICDocument.William C Mann and Sandra A Thompson.
1987.Rhetorical structure theory: A framework for theanalysis of texts.
Technical report, DTIC Docu-ment.William C Mann and Sandra A Thompson.
1988.Rhetorical structure theory: Toward a functionaltheory of text organization.
Text, 8(3):243?281.Daniel Marcu, Lynn Carlson, and Maki Watanabe.2000.
The automatic translation of discourse struc-tures.
In Proceedings of the 1st North Americanchapter of the Association for Computational Lin-guistics conference, pages 9?17.
Morgan Kauf-mann Publishers Inc.David Reitter.
2003.
Simple signals for complex rhet-orics: On rhetorical analysis with rich-feature sup-port vector models.
Language, 18:52.Radu Soricut and Daniel Marcu.
2003.
Sentence leveldiscourse parsing using syntactic and lexical in-formation.
In Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human Lan-guage Technology-Volume 1, pages 149?156.
As-sociation for Computational Linguistics.Billy TM Wong and Chunyu Kit.
2012.
Extendingmachine translation evaluation metrics with lexicalcohesion to document level.
In Proceedings of the2012 Joint Conference on Empirical Methods inNatural Language Processing and ComputationalNatural Language Learning, page 1060?1068.
As-sociation for Computational Linguistics.Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.2011.
Document-level consistency verification inmachine translation.
In Machine Translation Sum-mit, volume 13, pages 131?138.Hao Xiong, Wenwen Xu, Haitao Mi, Yang Liu, andQun Liu.
2009.
Sub-sentence division for tree-based machine translation.
In Proceedings of theACL-IJCNLP 2009 Conference Short Papers, pag-es 137?140.
Association for Computational Lin-guistics.Naiwen Xue, Fei Xia, Fu-Dong Chiou, and MartaPalmer.
2005.
The Penn Chinese treebank: Phrasestructure annotation of a large corpus.
NaturalLanguage Engineering, 11(2):207.Ming Yue.
2008.
Rhetorical structure annotation ofChinese news commentaries.
Journal of ChineseInformation Processing, 4:002.374
