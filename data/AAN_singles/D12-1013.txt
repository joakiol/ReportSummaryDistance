Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 139?148, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsActive Learning for Imbalanced Sentiment ClassificationShoushan Li?, Shengfeng Ju?, Guodong Zhou??
Xiaojun Li?
?Natural Language Processing LabSchool of Computer Science and Technology?College of Computer andInformation EngineeringSoochow University, Suzhou, 215006, China Zhejiang Gongshang University{shoushan.li, shengfeng.ju}@gmail.com, Hangzhou, 310035, Chinagdzhou@suda.edu.cn lixj@mail.zjgsu.edu.cnAbstractActive learning is a promising way forsentiment classification to reduce theannotation cost.
In this paper, we focus onthe imbalanced class distribution scenariofor sentiment classification, wherein thenumber of positive samples is quitedifferent from that of negative samples.This scenario posits new challenges toactive learning.
To address thesechallenges, we propose a novel activelearning approach, named co-selecting, bytaking both the imbalanced classdistribution issue and uncertainty intoaccount.
Specifically, our co-selectingapproach employs two feature subspaceclassifiers to collectively select mostinformative minority-class samples formanual annotation by leveraging acertainty measurement and an uncertaintymeasurement, and in the meanwhile,automatically label most informativemajority-class samples, to reduce human-annotation efforts.
Extensive experimentsacross four domains demonstrate greatpotential and effectiveness of our proposedco-selecting approach to active learning forimbalanced sentiment classification.
11 IntroductionSentiment classification is the task of identifyingthe sentiment polarity (e.g., positive or negative) of*1 Corresponding authora natural language text towards a given topic (Panget al2002; Turney, 2002) and has become thecore component of many important applications inopinion analysis (Cui et al2006; Li et al2009;Lloret et al2009; Zhang and Ye, 2008).Most of previous studies in sentimentclassification focus on learning models from alarge number of labeled data.
However, in manyreal-world applications, manual annotation isexpensive and time-consuming.
In these situations,active learning approaches could be helpful byactively selecting most informative samples formanual annotation.
Compared to traditional activelearning for sentiment classification, activelearning for imbalanced sentiment classificationfaces some unique challenges.As a specific type of sentiment classification,imbalanced sentiment classification deals with thesituation in which there are many more samples ofone class (called majority class) than the otherclass (called minority class), and has attractedmuch attention due to its high realistic value inreal-world applications (Li et al2011a).
Inimbalanced sentiment classification, since theminority-class samples (denoted as MI samples)are normally much sparse and thus more preciousand informative for learning compared to themajority-class ones (denoted as MA samples), it isworthwhile to spend more on manually annotatingMI samples to  guarantee both the quality andquantity of MI samples.
Traditionally, uncertaintyhas been popularly used as a basic measurement inactive learning (Lewis and Gale, 2004).
Therefore,how to select most informative MI samples formanual annotation without violating the basic139uncertainty requirement in active learning ischallenging in imbalanced sentiment classification.In this paper, we address above challenges inactive learning for imbalanced sentimentclassification.
The basic idea of our active learningapproach is to use two complementary classifiersfor collectively selecting most informative MIsamples: one to adopt a certainty measurement forselecting most possible MI samples and the otherto adopt an uncertainty measurement for selectingmost uncertain MI samples from the most possibleMI samples returned from the first classifier.Specifically, the two classifiers are trained withtwo disjoint feature subspaces to guarantee theircomplementariness.
This also applies to selectingmost informative MA samples.
We call our novelactive learning approach co-selecting due to itscollectively selecting informative samples throughtwo disjoint feature subspace classifiers.
To furtherreduce the annotation efforts, we only manuallyannotate those most informative MI samples whilethose most informative MA samples areautomatically labeled using the predicted labelsprovided by the first classifier.In principle, our active learning approach differsfrom existing ones in two main aspects.
First, acertainty measurement and an uncertaintymeasurement are employed in two complementarysubspace classifiers respectively to collectivelyselect most informative MI samples for manualannotation.
Second, most informative MA samplesare automatically labeled to further reduce theannotation cost.
Evaluation across four domainsshows that our active learning approach is effectivefor imbalanced sentiment classification andsignificantly outperforms the state-of-the-art activelearning alternatives, such as uncertainty sampling(Lewis and Gale, 2004) and co-testing (Muslea etal., 2006).The remainder of this paper is organized asfollows.
Section 2 overviews the related work onsentiment classification and active learning.Section 3 proposes our active learning approachfor imbalanced sentiment classification.
Section 4reports the experimental results.
Finally, Section 5draws the conclusion and outlines the future work.2 Related WorkIn this section, we give a brief overview onsentiment classification and active learning.2.1 Sentiment ClassificationSentiment classification has become a hot researchtopic in NLP community and various kinds ofclassification methods have been proposed, such asunsupervised learning methods (Turney, 2002),supervised learning methods (Pang et al2002),semi-supervised learning methods (Wan, 2009; Liet al2010), and cross-domain classificationmethods (Blitzer et al2007; Li and Zong, 2008;He et al2011).
However, imbalanced sentimentclassification is relatively new and there are only afew studies in the literature.Li et al2011a) pioneer the research inimbalanced sentiment classification and propose aco-training algorithm to perform semi-supervisedlearning for imbalanced sentiment classificationwith the help of a great amount of unlabeledsamples.
However, their semi-supervised approachto imbalanced sentiment classification suffers fromthe problem that their balanced selection strategyin co-training would generate many errors in lateiterations due to the imbalanced nature of theunbalanced data.
In comparison, our proposedactive learning approach can effectively avoid thisproblem.
By the way, it is worth to note that theexperiments therein show the superiority of under-sampling over other alternatives such as cost-sensitive and one-class classification forimbalanced sentiment classification.Li et al2011b) focus on supervised learningfor imbalanced sentiment classification andpropose a clustering-based approach to improvetraditional under-sampling approaches.
However,the improvement of the proposed clustering-basedapproach over under-sampling is very limited.Unlike all the studies mentioned above, ourstudy pioneers active learning on imbalancedsentiment classification.2.2 Active LearningActive leaning, as a standard machine learningproblem, has been extensively studied in manyresearch communities and several approaches havebeen proposed to address this problem (Settles,2009).
Based on different sample selectionstrategies, they can be grouped into two maincategories: (1) uncertainty sampling (Lewis andGale, 2004) where the active learner iterativelyselect most uncertain unlabeled samples formanual annotation; and (2) committee-based140sampling where the active learner selects thoseunlabeled samples which have the largestdisagreement among several committee classifiers.Besides query by committee (QBC) as the first ofsuch type (Freund et al1997), co-testing learns acommittee of member classifiers from differentviews and selects those contention points (i.e.,unlabeled examples on which the views predictdifferent labels) for manual annotation (Muslea etal., 2006).However, most previous studies focus on thescenario of balanced class distribution and only afew recent studies address the active learning issueon imbalanced classification problems includingYang and Ma (2010), Zhu and Hovy (2007),Ertekin et al2007a) and Ertekin et al2007b)2.Unfortunately, they straightly adopt the uncertaintysampling as the active selection strategy to addressactive learning in imbalanced classification, whichcompletely ignores the class imbalance problem inthe selected samples.Attenberg and Provost (2010) highlights theimportance of selecting samples by considering theproportion of the classes.
Their simulationexperiment on text categorization confirms thatselecting class-balanced samples is more importantthan traditional active selection strategies likeuncertainty.
However, the proposed experiment issimulated and non real strategy is proposed tobalance the class distribution of the selectedsamples.Doyle et al2011) propose a real strategy toselect balanced samples.
They first select a set ofuncertainty samples and then randomly selectbalanced samples from the uncertainty-sample set.However, the classifier used for selecting balancedsamples is the same as the one for supervisinguncertainty, which makes the balance controlunreliable (the selected uncertainty samples takevery low confidences which are unreliable tocorrectly predict the class label for controlling thebalance).
Different from their study, our approachpossesses two merits: First, two feature subspaceclassifiers are trained to finely integrate thecertainty and uncertainty measurements.
Second,the MA samples are automatically annotated,2  Ertekin et al2007a) and Ertekin et al2007b) selectsamples closest to the hyperplane provided by the SVMclassifier (within the margin).
Their strategy can be seen as aspecial case of uncertainty sampling.which reduces the annotation cost in a furthereffort.3 Active Learning for ImbalancedSentiment ClassificationGenerally, active learning can be either stream-based or pool-based (Sassano, 2002).
The maindifference between the two is that the former scansthrough the data sequentially and selectsinformative samples individually, whereas thelatter evaluates and ranks the entire collectionbefore selecting most informative samples at batch.As a large collection of samples can easilygathered once in sentiment classification, pool-based active learning is adopted in this study.Figure 1 illustrates a standard pool-based activelearning approach, where the most important issueis the sampling strategy, which evaluates theinformativeness of one sample.Input:Labeled data L;Unlabeled pool U;Output:New Labeled data LProcedure:Loop for N iterations:(1).
Learn a classifier using current L(2).
Use current classifier to label all unlabeledsamples(3).
Use the sampling strategy to select n mostinformative samples for manual annotation(4).
Move newly-labeled samples from U to LFigure 1: Pool-based active learning3.1 Sampling Strategy: Uncertainty vs.CertaintyAs one of the most popular selection strategies inactive learning, uncertainty sampling depends onan uncertainty measurement to select informativesamples.
Since sentiment classification is a binaryclassification problem, the uncertaintymeasurement of a document d can be simplydefined as follows:{ , }( ) min ( | )y pos negUncer d P y d?
?Where ( | )P y d denotes the posterior probability ofthe document d belonging to the class y and {pos,141neg} denotes the class labels of positive andnegative.In imbalanced sentiment classification, MIsamples are much sparse yet precious for learningand thus are believed to be more valuable formanual annotation.
The key in active learning forimbalanced sentiment classification is to guaranteeboth the quality and quantity of newly-added MIsamples.
To guarantee the selection of MI samples,a certainty measurement is necessary.
In this study,the certainty measurement is defined as follows:{ , }( ) max ( | )y pos negCer d P y d?
?Meanwhile, in order to balance the samples inthe two classes, once an informative MI sample ismanually annotated, an informative MA sample isautomatically labeled.
In this way, the annotateddata become more balanced than a randomselection strategy.However, the two sampling strategies discussedabove are apparently contradicted: while theuncertainty measurement is prone to selecting thesamples whose posterior probabilities are nearestto 0.5, the certainty measurement is prone toselecting the samples whose posterior probabilitiesare nearest to 1.
Therefore, it is essential to find asolution to balance uncertainty sampling andcertainty sampling in imbalanced sentimentclassification,3.2 Co-selecting with Feature SubspaceClassifiersIn sentiment classification, a document isrepresented as a feature vector generated from thefeature set ?
?1,..., mF f f?
.
When a feature subset,i.e., ?
?1 ,...,S S SrF f f?
( r m?
), is used, theoriginal m-dimensional feature space becomes anr-dimensional feature subspace.
In this study, wecall a classifier trained with a feature subspace afeature subspace classifier.Our basic idea of balancing both the uncertaintymeasurement and the certainty measurement is totrain two subspace classifiers to adopt themrespectively.
In our implementation, we randomlyselect two disjoint feature subspaces, each ofwhich is used to train a subspace classifier.
On oneside, one subspace classifier is employed to selectsome certain samples; on the other side, the otherclassifier is employed to select the most uncertainsample from those certain samples for manualannotation.
In this way, the selected samples arecertain in terms of one feature subspace forselecting more possible MI samples.
Meanwhile,the selected sample remains uncertain in terms ofthe other feature subspace to introduce uncertainknowledge into current learning model.
We namethis approach as co-selecting because itcollectively selects informative samples by twoseparate classifiers.
Figure 2 illustrates the co-selecting algorithm.
In our algorithm, we strictlyconstrain the balance of the samples between thetwo classes, i.e., positive and negative.
Therefore,once two samples are annotated with the sameclass label, they will not be added to the labeleddata, as shown in step (7) in Figure 2.Input:Labeled data L with balanced samples over the two classesUnlabeled pool U  Output:New Labeled data LProcedure:Loop for N iterations:(1).
Randomly select a feature subset SF  withsize r (with the proportion /r m?
? )
from F(2).
Generate a feature subspace from SF  andtrain a corresponding feature subspaceclassifier CerC  with L(3).
Generate another feature subspace from thecomplement set of SF , i.e., SF F?
and traina corresponding feature subspace classifierUncerC  with L(4).
Use CerC  to select top certain k positive and knegative samples, denoted as a sample set1CER(5).
Use UncerC  to select the most uncertainpositive sample and negative sample from1CER(6).
Manually annotate the two selected samples(7).
If the annotated labels of the two selectedsamples are different from each other:Add the two newly-annotated samples into LFigure 2: The co-selecting algorithmThere are two parameters in the algorithm: thesize of the feature subspace for training the firstsubspace classifier, i.e., ?
and the number of142selected certain samples, i.e., k. Both of the twoparameters will be empirically studied in ourexperiments.3.3 Co-selecting with Selected MA SamplesAutomatically LabeledInput:Labeled data L with balanced samples over the two classesUnlabeled pool U MA and MI Label (positive or negative)Output:New Labeled data LProcedure:Loop for N iterations:(1).
Randomly select a proportion of features(with the proportion ? )
from F to get afeature subset SF(2).
Generate a feature subspace from SF  andtrain a corresponding subspace classifier CerCwith L(3).
Generate another feature subspace from thecomplement set of SF , i.e., SF F?
and traina corresponding subspace classifier UncerCwith L(4).
Use CerC  to select top certain k positive and knegative samples, denoted as a sample set1CER(5).
Use UncerC  to select the most uncertainpositive sample and negative sample from1CER(6).
Manually annotate the sample that is predictedas a MI sample by CerC  and automaticallyannotate the sample that is predicted asmajority class(7).
If the annotated labels of the two selectedsamples are different from each other:Add the two newly-annotated samples into LFigure 3: The co-selecting algorithm with selectedMA samples automatically labeledTo minimize manual annotation, it is a good choiceto automatically label those selected MA samples.In our co-selecting approach, automaticallylabeling those selected MA samples is easy andstraightforward: the subspace classifier formonitoring the certainty measurement provides anideal solution to annotate the samples that havebeen predicted as majority class.
Figure 3 showsthe co-selecting algorithm with those selected MAsamples automatically labeled.
The maindifference from the original co-selecting is shownin Step (6) in Figure 3.
Another difference is theinput where a prior knowledge of which class ismajority class or minority class should be known.In real applications, it is not difficult to know this.We first use a classifier trained with the initiallabeled data to test all unlabeled data.
If thepredicted labels in the classification results aregreatly imbalanced, we can assume that theunlabeled data is imbalanced, and consider thedominated class as majority class.4 ExperimentationIn this section, we will systematically evaluate ouractive learning approach for imbalanced sentimentclassification and compare it with the state-of-the-art active learning alternatives.4.1 Experimental SettingDatasetWe use the same data as used by Li et al2011a).The data collection consists of four domains: Book,DVD, Electronic, and Kitchen ?Blitzer et al2007).
For each domain, we randomly select aninitial balanced labeled data with 50 negativesamples and 50 positive samples.
For the unlabeleddata, we randomly select 2000 negative samples,and 14580/12160/7140/7560 positive samples fromthe four domains respectively, keeping the sameimbalanced ratio as the whole data.
For the testdata in each domain, we randomly extract 800negative samples and 800 positive samples.Classification algorithmThe Maximum Entropy (ME) classifierimplemented with the Mallet 3  tool is mainlyadopted, except that in the margin-based activelearning approach (Ertekin et al2007a) whereSVM is implemented with light-SVM 4 .
Thefeatures for classification are unigram words withBoolean weights.3 http://mallet.cs.umass.edu/4 http://www.cs.cornell.edu/people/tj/svm_light/1430.620.640.660.680.70.720.740.760.78Book DVD Electronic KitchenG-meanRandom SVM-based Uncertainty CertaintyCo-testing Self-selecting Co-selecting-basic Co-selecting-plusFigure 4: Performance comparison of different active learning approaches on imbalanced sentimentclassificationEvaluation metricsThe popular geometric mean= rate rateG - mean TP TN?
is adopted, where rateTPis the true positive rate (also called positive recallor sensitivity) and rateTN  is the true negative rate(also called negative recall or specificity) (Kubatand Matwin, 1997).4.2 Experimental ResultsFor thorough comparison, various kinds of activelearning approaches are implemented including:?
Random: randomly select the samples from theunlabeled data for manual annotation;?
Margin-based: iteratively select samplesclosest to the hyperplane provided by the SVMclassifier, which is suggested by Ertekin et al(2007a) and Ertekin et al2007b).
One sampleis selected in each iteration;?
Uncertainty: iteratively select samples usingthe uncertainty measurement according to theoutput of ME classifier.
One sample is selectedin each iteration;?
Certainty: iteratively select class-balancedsamples using the certainty measurementaccording to the output of ME classifier.
Onepositive and negative sample (the positive andnegative label is provided by the ME classifier)are selected in each iteration;?
Co-testing: first get contention samples (i.e.,unlabeled examples on which the memberclassifiers predict different labels) and thenselect the least confidence one among thehypotheses of different member classifiers, i.e.,the aggressive strategy as described Muslea etal.
(2006).
Specifically, the member classifiersare two subspace classifiers trained by splittingthe whole feature space into two disjointsubspaces of same size;?
Self-selecting: first select k uncertainty samplesand then randomly select a positive andnegative sample from the uncertainty-sampleset, which is suggested by Doyle et al2011).We call it self-selecting since only oneclassifier is involved to measure uncertaintyand predict class labels.For those approaches involving randomselection of features, we run 5 times for them andreport the average results.
Note that the samplesselected by these approaches are imbalanced.
Toaddress the problem of classification onimbalanced data, we adopt the under-samplingstrategy which has been shown effective forsupervised imbalanced sentiment classification (Liet al2011a).
Our active learning approachincludes two versions: the co-selecting algorithmas described in Section 3.2 and the co-selectingwith selected MA samples automatically labeled asdescribed in Section 3.3.
For clarity, we refer theformer as co-selecting-basic and the latter as co-selecting-plus in the following.144Comparison with other active learningapproachesFigure 4 compares different active learningapproaches to imbalanced sentiment classificationwhen 600 unlabeled samples are selected forannotation.
Specifically, the parameters ?
and k isset to be 1/16 and 50 respectively.
Figure 4justifies that it is challenging to perform activelearning in imbalanced sentiment classification: theapproaches of margin-based, uncertainty-basedand self-selecting perform no better than randomselection while co-testing only outperformsrandom selection in two domains: DVD andElectronic with only a small improvement (about1%).
In comparison, our approaches, both co-selecting-basic and co-selecting-plus significantlyoutperform the random selection approach on allthe four domains.
It also shows that co-selecting-plus is preferable over co-selecting-basic.
Thisverifies the effectiveness of automatically labelingthose selected MA samples in imbalancedsentiment classification.Specifically, we notice that only using thecertainty measurement (i.e., certainty) performsworst, which reflects that only considering samplebalance factor in imbalanced sentimentclassification is not helpful.Figure 5 compares our approach to other activelearning approaches by varying the number of theselected samples for manually annotation.
Forclarity, we only include random selection and co-testing in comparison and do not show theperformances of the other active learningapproaches due to their similar behavior to randomselection.
From this figure, we can see that co-testing is effective on Book and Electronic whenless than 1500 samples are selected for manualannotation but it fails to outperform randomselection in the other two domains.
In contract, ourco-selecting-plus approach is apparently moreadvantageous and significantly outperformsrandom selection across all domains (p-value<0.05)when less than 4800 samples are selected formanual annotation.Sensitiveness of the parameters ?The size of the feature subspace is an importantparameter in our approach.
Figure 6 shows theperformance of co-selecting-plus with varyingsizes of the feature subspaces for the first subspaceElectronic0.680.70.720.740.760.780.8300 600 900 1200 1500 2400 4800 7000Number of the manually annoated samplesRandom Co-testing Co-selecting-plusDVD0.620.640.660.680.70.720.740.760.78300 600 900 1200 1500 2400 4800 9600Nubmer of the manually annotated samplesRandom Co-testing Co-selecting-plusBook0.620.640.660.680.70.720.740.760.78300 600 900 1200 1500 2400 4800 9600Nubmer of the manually annotated samplesRandom Co-testing Co-selecting-plusKitchen0.70.720.740.760.780.80.82300 600 900 1200 1500 2400 4800 7000Number of the manually annoated samplesRandom Co-testing Co-selecting-plusFigure 5:  Performance comparison of three active learning approaches:  random selection, co-testing and co-selecting-plus, by varying the number of the selected samples for manually annotation145classifier CerC .
From Figure 6, we can see that achoice of the proportion ?
between 1/8 and 1/32 isrecommended.
This result also shows that the sizeof the feature subspace for selecting certainsamples should be much less than that for selectinguncertain samples, which indicates the moreimportant role of the uncertainty measurement inactive learning.0.640.660.680.70.720.740.760.780.81/2  1/4  1/8 1/16 1/32 1/64 1/128Proportion of the Selected Features for Subspace(r /m )G-meanBook DVD Electornic KitchenFigure 6: Performance of co-selecting-plus overvarying sizes of feature subspaces (?
)Figure 7: Performance of co-selecting-plus overvarying numbers of the selected certain samples (k)Sensitiveness of parameter kFigure 7 presents the performance of co-selecting-plus with different numbers of the selected certainsamples in each iteration, i.e., parameter k.Empirical studies suggest that setting k between 20and 100 could get a stable performance.
Also, thisfigure demonstrates that using certainty as the onlyquery strategy is much less effective (see the resultwhen k=1).
This once again verifies the importanceof the uncertainty strategy in active learning.Number of MI samples selected for manualannotationIn Table 1, we investigate the number of the MIsamples selected for manual annotation usingdifferent active learning approaches when a total of600 unlabeled samples are selected for annotation.From this table, we can see that almost all theexisting active learning approaches can only selecta small amount of MI samples, taking similarimbalanced ratios as the whole unlabeled data.Although the certainty approach could selectmany MI samples for annotation, this approachperforms worst due to its totally ignoring theuncertainty factor.
When our approach is applied,especially co-selecting-plus, more MI samples areselected for manual annotation and finally includedto learn the models.
This greatly improves theeffectiveness of our active learning approach.Table 1: The number of MI samples selected formanual annotation when 600 samples are annotated on the whole.Book DVD Electronic KitchenRandom 71 82 131 123SVM-based 65 72 135 106Uncertainty 78 93 137 136Certainty 160 200 236 227Co-testing 89 84 136 109Self-selecting 87 95 141 126Co-selecting-basic101 112 179 174Co-selecting-plus161 156 250 272Precision of automatically labeled MA samplesIn co-selecting-plus, all the added MA samples areautomatically labeled by the first subspaceclassifier.
It is encouraging to observe that 92.5%,91.25%, 92%, and 93.5% of automatically labeledMA samples are correctly annotated in Book, DVD,Electronic, and Kitchen respectively.
This suggeststhat the subspace classifiers are able to predict theMA samples with a high precision.
This indicatesthe rationality of automatically annotating MAsamples.5 ConclusionIn this paper, we propose a novel active learningapproach, named co-selecting, to reduce theannotation cost for imbalanced sentimentclassification.
It first trains two complementary0.660.680.70.720.740.760.780.81 5 20 50 100 150Number of the selected certainty samplesG-meanBook DVD Electornic Kitchen146classifiers with two disjoint feature subspaces andthen uses them to collectively select mostinformative MI samples for manual annotation,leaving most informative MA samples forautomatic annotation.
Empirical studies show thatour co-selecting approach is capable of greatlyreducing the annotation cost and in the meanwhile,significantly outperforms several active learningalternativesFor the future work, we are interested inapplying our co-selecting approach to activelearning for other imbalanced classification tasks,especially those with much higher imbalanced ratio.AcknowledgmentsThe research work described in this paper has beenpartially supported by three NSFC grants,No.61003155, No.60873150 and No.90920004,one National High-tech Research andDevelopment Program of ChinaNo.2012AA011102, Open Projects Program ofNational Laboratory of Pattern Recognition, andthe NSF grant of Zhejiang Province No.Z1110551.We also thank the three anonymous reviewers fortheir helpful comments.ReferencesAttenberg J. and F. Provost.
2010.
Why Label when youcan Search?
Alternatives to Active Learning forApplying Human Resources to Build ClassificationModels Under Extreme Class Imbalance.
InProceeding of KDD-10, 423-432.Blitzer J., M. Dredze and F. Pereira.
2007.
Biographies,Bollywood, Boom-boxes and Blenders: DomainAdaptation for Sentiment Classification.
InProceedings of ACL-07, 440-447.Cui H., V. Mittal, and M. Datar.
2006.
ComparativeExperiments on Sentiment Classification for OnlineProduct Reviews.
In Proceedings of AAAI-06,pp.1265-1270.Doyle S., J. Monaco, M. Feldman, J. Tomaszewski andA.
Madabhushi.
2011.
An Active Learning basedClassification Strategy for the Minority ClassProblem: Application to Histopathology Annotation.BMC Bioinformatics, 12: 424, 1471-2105.Ertekin S., J. Huang, L. Bottou and C. Giles.
2007a.Learning on the Border: Active Learning inImbalanced Data Classification.
In Proceedings ofCIKM-07, 127-136.Ertekin S., J. Huang, L. Bottou and C. Giles.
2007b.Active Learning in Class Imbalanced Problem.
InProceedings of SIGIR-07, 823-824.Freund Y., H. Seung, E. Shamir and N. Tishby.
1997.Selective Sampling using the Query by Committeealgorithm.
Machine Learning, 28(2-3), 133-168.He Y., C. Lin and H. Alani.
2011.
AutomaticallyExtracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification.
In Proceeding ofACL-11, 123-131.Lewis D. and W. Gale.
1994.
Training Text Classifiersby Uncertainty Sampling.
In Proceedings of SIGIR-94, 3-12.Li F., Y. Tang, M. Huang and X. Zhu.
2009.
AnsweringOpinion Questions with Random Walks on Graphs.In Proceedings of ACL-IJCNLP-09, 737-745.Li S. and C. Zong.
2008.
Multi-domain SentimentClassification.
In Proceedings of ACL-08, short paper,pp.257-260.Li S., C. Huang, G. Zhou and S. Lee.
2010.
EmployingPersonal/Impersonal Views in Supervised and Semi-supervised Sentiment Classification.
In Proceedingsof ACL-10,  pp.414-423.Li S., Z. Wang, G. Zhou and S. Lee.
2011a.
Semi-supervised Learning for Imbalanced SentimentClassification.
In Proceeding of IJCAI-11, 826-1831.Li S., G. Zhou, Z. Wang, S. Lee and R. Wang.
2011b.Imbalanced Sentiment Classification.
In Proceedingsof CIKM-11,  poster paper, 2469-2472.Lloret E., A. Balahur, M. Palomar, and A. Montoyo.2009.
Towards Building a Competitive OpinionSummarization System.
In Proceedings of NAACL-09 Student Research Workshop and DoctoralConsortium, 72-77.Kubat M. and S. Matwin.
1997.
Addressing the Curse ofImbalanced Training Sets: One-Sided Selection.
InProceedings of ICML-97, 179?186.Muslea I., S. Minton and C. Knoblock .
2006.
ActiveLearning with Multiple Views.
Journal of ArtificialIntelligence Research, vol.27, 203-233.Pang B. and L. Lee.
2008.
Opinion Mining andSentiment Analysis: Foundations and Trends.Information Retrieval, vol.2(12), 1-135.Pang B., L. Lee and S. Vaithyanathan.
2002.Thumbs up?Sentiment Classification using Machine LearningTechniques.
In Proceedings of EMNLP-02, 79-86.147Settles B.
2009.
Active Learning Literature Survey.Computer Sciences Technical Report 1648,University of Wisconsin, Madison, 2009.Turney P. 2002.
Thumbs up or Thumbs down?Semantic Orientation Applied to UnsupervisedClassification of reviews.
In Proceedings of ACL-02,417-424.Wan X.
2009.
Co-Training for Cross-Lingual SentimentClassification.
In Proceedings of ACL-IJCNLP-09,235?243.Yang Y. and G. Ma.
2010.
Ensemble-based ActiveLearning for Class Imbalance Problem.
J. BiomedicalScience and Engineering,  vol.3,1021-1028.Zhang M. and X. Ye.
2008.
A Generation Model toUnify Topic Relevance and Lexicon-based Sentimentfor Opinion Retrieval.
In Proceedings of SIGIR-08,411-418.Zhu J. and E. Hovy.
2007.
Active Learning for WordSense Disambiguation with Methods for Addressingthe Class Imbalance Problem.
In Proceedings ofACL-07, 783-793.148
